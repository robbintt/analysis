---
title: Investigating Modality Contribution in Audio LLMs for Music
arxiv_id: '2509.20641'
source_url: https://arxiv.org/abs/2509.20641
generated_at: '2026-02-06T04:18:40'
quality_score: 8
citation_count: 0
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Investigating Modality Contribution in Audio LLMs for Music
*Giovana Morais; Magdalena Fuentes*

---

### ðŸ“Š Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **Methodology** | MM-SHAP Framework (Shapley Values) |
| **Models Analyzed** | Qwen-Audio, MU-LLaMA |
| **Benchmark** | MuChoMusic (MC-PI, MusicCaps) |
| **Key Insight** | High accuracy â‰  Deep audio processing |

---

## Executive Summary

> Current Audio Large Language Models (LLMs) demonstrate impressive capabilities in music understanding tasks, yet a critical ambiguity remains regarding their internal processing mechanisms. Specifically, it is unclear whether these models genuinely analyze and "listen" to audio signals or if they rely primarily on textual reasoning and language biases to achieve high benchmark scores. This distinction is vital because reliance on text shortcuts rather than auditory processing undermines the robustness of these models in real-world applications where audio cues are paramount. The paper addresses the limitation of standard performance metrics, which fail to distinguish between true multimodal integration and text-based problem-solving, creating a need for more rigorous evaluation methods.
>
> The authors introduce the first application of the MM-SHAP (Multi-Modal SHapley Additive exPlanations) framework to Audio LLMs to quantify modality contributions. This performance-agnostic explainable AI (XAI) technique utilizes Shapley values to calculate the relative contribution ($\Phi$) of audio versus text inputs for specific output tokens. By decomposing these contributions into absolute, positive, and negative components, the framework can isolate exactly how much each modality influences the model's decision-making process. The study applies this methodology to two distinct Audio LLMsâ€”Qwen-Audio and MU-LLaMAâ€”evaluating them on the MuChoMusic benchmark across MC-PI (multiple-choice) and MusicCaps (audio description) tasks to rigorously measure audio reliance.
>
> The analysis reveals that high benchmark accuracy is frequently achieved with surprisingly low reliance on audio data. In the MusicCaps task, the average Audio-SHAP (A-SHAP) score rose to 0.73%, indicating a statistically significant increase in audio usage compared to multiple-choice tasks, yet text remained the dominant modality. Qwen-Audio, despite being the best-performing model, demonstrated significantly lower audio magnitude than text, whereas MU-LLaMA exhibited similar magnitudes for both but with widespread, sometimes negative, audio activations. Crucially, the data showed that successful sound event localization could occur even with minimal overall audio contribution, and audio usage magnitude remained consistent regardless of whether the model's answer was correct or incorrect.
>
> This research fundamentally challenges the evaluation protocols for Audio LLMs by demonstrating that high performance metrics alone are insufficient to prove deep audio processing. The study establishes MM-SHAP as a foundational tool for the audio XAI community, providing a rigorous method for validating whether models are truly "listening." By exposing the discrepancy between benchmark success and modality reliance, these findings encourage the development of more robust multimodal systems that genuinely integrate audio data, ensuring future models are reliable for applications requiring actual auditory understanding rather than just textual correlation.

---

## Key Findings

*   **Textual Dominance:** High-performing models rely more heavily on textual information than audio input to generate answers.
*   **Selective Processing:** Models demonstrate selective audio processing, successfully localizing sound events despite a low overall contribution of the audio modality.
*   **Accuracy vs. Understanding:** High benchmark accuracy does not necessarily imply deep audio processing, as models may rely primarily on textual reasoning.
*   **Metric Limitations:** Variable modality contribution suggests that performance metrics alone are insufficient to determine if a model is truly 'listening'.

## Methodology

The study utilizes an adapted **MM-SHAP framework**, a performance-agnostic scoring method based on Shapley values, to quantify the relative contribution of each modality (audio and text). The authors evaluated two distinct Audio LLMs using the **MuChoMusic benchmark** to isolate and measure the specific impact of audio versus text inputs on the final output.

## Contributions

*   **Novel Application of Explainable AI:** First application of the MM-SHAP framework to Audio LLMs, introducing a rigorous method for interpreting modality contributions.
*   **Insight into Model Behavior:** Addresses the ambiguity regarding whether Audio LLMs perform genuine audio analysis or rely on textual reasoning.
*   **Foundation for Future Research:** Establishes a foundational step for future work in explainable AI (XAI) within the audio domain.

## Technical Details

The approach utilizes Shapley values to quantify modality contribution ($\Phi$) for specific output tokens, calculating separate values for audio and text contributions. It decomposes contributions into absolute values, positive contributions, and negative components.

**Visualization & Analysis:**
*   Highlights text tokens with a contribution â‰¥ 80% of the maximum Shapley value.
*   Maps audio contributions on waveforms where darker colors indicate higher magnitude.
*   Analyzes **Qwen-Audio** (best-performing) and **MU-LLaMA** models.
*   Evaluates on **MC-PI** (multiple-choice) and **MusicCaps** (audio description) tasks.

## Results

*   **MusicCaps Task:** The average A-SHAP increased to **0.73%**, indicating a significant increase in audio usage compared to multiple-choice tasks. Audio usage magnitude showed little change between correct and incorrect examples.
*   **Qwen-Audio:** Demonstrated lower audio magnitude than text, with positive contributions aligning with ground truth regions but also strong activations in non-relevant regions.
*   **MU-LLaMA:** Exhibited similar magnitudes for audio and text, with widespread high audio activations and negative contributions as strong as positive ones.
*   **Conclusion:** The results conclude that high benchmark accuracy does not imply deep audio processing, as localization occurred even with low overall audio contribution.