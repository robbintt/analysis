# Make Some Noise: Towards LLM audio reasoning and generation using sound tokens

*Shivam Mehta; Nebojsa Jojic; Hannes Gamper*

---

> ###  Quick Facts
> * **Target Bitrate:** 0.23 kbps
> * **Core Architecture:** VAE with Vector Quantization + Conditional Flow Matching (VQ + FM)
> * **Optimization:** LoRA (Rank 64, Alpha 128)
> * **Training Scale:** ~2M text-audio pairs (pretraining) + ~5M pairs (full fine-tuning)
> * **Compute Resources:** 4x Nvidia A6000 $\rightarrow$ 24x Nvidia A100
> * **Quality Score:** 8/10
> * **References:** 40 citations

---

## Executive Summary

### Problem
The research addresses the fundamental challenge of efficiently representing audio for Large Language Models (LLMs). Current multimodal approaches struggle with the computational burden of high-bitrate audio codecs, making the processing and reasoning over long audio sequences expensive. The paper investigates whether audio can be compressed into discrete tokens at an ultra-low bitrate without sacrificing the model's ability to understand semantic content. This is crucial for scalable "audio reasoning" that bridges continuous audio signals and discrete token spaces.

### Innovation
The key innovation is a novel codec architecture combining Variational Autoencoders (VAE) with Vector Quantization (VQ) and Conditional Flow Matching (FM). Unlike standard deterministic approaches (VQ + MSE) that suffer from over-averaging, this probabilistic method transforms continuous audio into discrete tokens at an extreme bitrate of **0.23 kbps**. These "sound tokens" are semantically compatible with standard text tokens. The authors fine-tuned a pre-trained text-based LLM using Low-Rank Adaptation (LoRA) on these combined streams, enabling the processing of millions of text-audio pairs without full retraining.

### Results
The proposed **VQ + FM** method outperformed VQ + MSE in reconstruction error on datasets (ESC50, FMA) and achieved superior Fr√©chet Audio Distance (FAD) scores regarding semantic retention. However, due to aggressive compression, raw reconstruction fidelity (FAD and MOS) remained inferior to state-of-the-art codecs like Encodec. Crucially, comprehension tasks remained robust: the model achieved competitive results in audio captioning (Clotho, AudioCaps, ESC50). Conversely, the model performed poorly on generation tasks, and speech quality (VCTK) was notably degraded, likely due to the underlying VAE being trained predominantly on music.

### Impact
This research demonstrates that LLMs can maintain high-level audio comprehension capabilities even when audio representations are compressed to ultra-low bitrates. The findings highlight a critical performance gap between understanding audio and generating it within a highly compressed discrete space. By identifying this disparity, the authors outline a path for future research emphasizing the need for diverse training datasets and improved evaluation metrics, establishing a foundation for efficient "audio reasoning" systems.

---

## Key Findings

*   **Ultra-Low Bitrate Efficiency:** The proposed tokenizer successfully converts audio into discrete tokens at **0.23kbps**, outperforming traditional models across various datasets.
*   **Competitive Audio Comprehension:** Despite aggressive compression, the multimodal LLM achieved results comparable to state-of-the-art methods in comprehension tasks.
*   **Limitations in Audio Generation:** The model exhibits poor performance on generation tasks, revealing a significant disparity between understanding and synthesizing audio.
*   **Data and Metric Deficiencies:** Current limitations are partly attributed to a lack of diverse datasets and the need for improved evaluation metrics.

---

## Methodology

1.  **Audio Tokenization**
    A hybrid approach is utilized, combining Variational Quantization with Conditional Flow Matching. This transforms continuous audio signals into ultra-low bitrate discrete tokens.

2.  **LLM Integration**
    Audio tokens are specifically designed to integrate seamlessly with standard text tokens, ensuring compatibility with existing transformer architectures.

3.  **Model Training**
    A pre-trained text-based LLM is fine-tuned using Low-Rank Adaptation (LoRA). This allows the model to effectively process and reason over combined audio-text token streams.

---

## Contributions

*   **Novel Codec Architecture:** Introduces a new audio tokenization method merging Variational Quantization with Conditional Flow Matching to achieve significantly lower bitrates (**0.23kbps**).
*   **Demonstration of Comprehension Robustness:** Provides evidence that audio comprehension capabilities remain competitive even when audio representation is heavily compressed.
*   **Identification of Performance Gaps:** Highlights specific challenges in audio generation compared to comprehension, outlining clear directions for future research regarding data and metrics.

---

## Technical Details

| Component | Specification |
| :--- | :--- |
| **Core Architecture** | VAE with Vector Quantization (VQ). Compares VQ + MSE (deterministic) vs. VQ + FM (probabilistic) objectives. |
| **Objective Function** | VQ + FM is utilized to address over-averaging issues found in deterministic methods. |
| **Operating Bitrate** | **0.23 kbps** (Ultra-Low Bitrate). |
| **Fine-Tuning Strategy** | LoRA (Rank: 64, Alpha: 128). |
| **Training Data** | ~2M text-audio pairs (pretraining); ~5M pairs (full fine-tuning). |
| **Optimization** | AdamW optimizer with a **1e-4** learning rate. |
| **Compute Infrastructure** | **Pretraining:** 4x Nvidia A6000 GPUs <br> **Full Fine-tuning:** 24x Nvidia A100 GPUs |

---

## Results

*   **Reconstruction Quality:** VQ + FM outperformed VQ + MSE in reconstruction error on **ESC50** and **FMA** datasets; achieved superior FAD scores for semantic retention.
*   **Fidelity Comparison:** The method performed inferiorly to **EnCodec** and baseline VAE in raw reconstruction fidelity (FAD and MOS) due to extreme compression.
*   **Modality Specifics:** Speech (**VCTK dataset**) was the most adversely affected modality, likely because the underlying VAE was trained primarily on music.
*   **Captioning Performance:** Evaluated using **SPICE** and **FENSE** metrics on **Clotho**, **AudioCaps**, and **ESC50**.

---

**Quality Score:** 8/10  
**References:** 40 citations