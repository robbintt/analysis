# Enabling Autoregressive Models to Fill In Masked Tokens

*Daniel Israel; Aditya Grover; Guy Van den Broeck*

---

> ### 4CA Quick Facts
>
> *   **Model Name:** MARIA (Masked and Autoregressive Infilling Architecture)
> *   **Primary Benchmark:** ROCStories Cloze Test
> *   **Key Metric:** BLEU-4 Score of **17.6** (vs 13.8 for Diffusion)
> *   **Efficiency Gain:** ~**100x faster** inference than discrete diffusion models
> *   **Accuracy Improvement:** **32% relative improvement** over baselines
> *   **Architecture:** Hybrid AR + MLM with linear decoder fusion

---

## Executive Summary

This research addresses a fundamental architectural trade-off in Large Language Modeling (LLM) between the generative efficiency of Autoregressive (AR) models and the contextual richness of Masked Language Models (MLM). While AR models (e.g., GPT) are the industry standard for scalable text generation—leveraging Key-Value (KV) caching for high inference speeds—they are natively unable to perform masked infilling (filling missing text based on bidirectional context). Conversely, MLM models (e.g., BERT) excel at understanding bidirectional context for infilling but suffer from severe computational inefficiencies during generation, as they lack the causal structure required for fast, cached decoding.

The authors propose **MARIA** (Masked and Autoregressive Infilling Architecture), a novel framework that hybridizes pre-trained AR and MLM models without requiring expensive re-training or architectural overhauls. Technically, MARIA processes inputs in parallel through both a pre-trained AR model and a pre-trained MLM model to capture unidirectional generative fluency and bidirectional context, respectively. These dual representations are fused by concatenating their hidden states and processing them through a lightweight, trainable linear decoder.

MARIA achieves state-of-the-art (SOTA) performance on standard masked infilling benchmarks, significantly outperforming existing baselines. Specifically, on the **ROCStories** cloze test, MARIA achieved a **BLEU-4 score of 17.6**, substantially surpassing discrete diffusion models (13.8 BLEU-4). Furthermore, the model demonstrated a **32% relative improvement** in accuracy over discrete diffusion baselines. Critically, the study confirmed that MARIA retains the inference efficiency of AR models; by maintaining the standard causal structure, the system continues to benefit from KV caching. This results in generation speeds that are approximately **100x faster** than discrete diffusion approaches, effectively resolving the historical conflict between infilling capability and computational cost.

---

## Key Findings

*   **Limitation Identification:** AR models are unable to perform masked infilling, while MLM models possess this capability but suffer from computational inefficiencies that hinder scalability.
*   **Superior Performance:** The proposed MARIA model achieves state-of-the-art performance on masked infilling tasks, significantly outperforming existing baselines such as discrete diffusion models.
*   **Efficiency Retention:** Despite adding infilling capabilities, the approach retains the inherent advantages of AR models, specifically enabling faster inference through Key-Value (KV) caching.
*   **Effective Fusion:** Simply concatenating the hidden states of a pre-trained MLM and AR model and processing them through a trained linear decoder is sufficient to enable high-quality masked infilling.

---

## Methodology

The study introduces **MARIA** (Masked and Autoregressive Infilling Architecture), a hybrid framework designed to combine the strengths of pre-trained Autoregressive (AR) and Masked Language Modeling (MLM) models.

The methodology follows a dual-input process:
1.  **Context Acquisition:** The method takes inputs processed by both a pre-trained MLM (providing bidirectional context) and a pre-trained AR model (providing generative fluency).
2.  **Fusion:** Integration is achieved by concatenating the hidden states from both models.
3.  **Decoding:** These concatenated inputs are passed through a lightweight linear decoder.

This approach allows the system to leverage the strengths of both paradigms without complex architectural overhauls, relying on a trained linear decoder rather than massive re-training.

---

## Technical Details

| Component | Description |
| :--- | :--- |
| **Architecture Type** | Hybrid Fusion (Parallel AR + MLM) |
| **Input Processing** | Dual-input: Processes text simultaneously through frozen AR and MLM backbones. |
| **Fusion Mechanism** | Concatenation of hidden states from both AR and MLM models. |
| **Decoder** | Trained Linear Decoder (No cross-attention mechanisms required). |
| **Optimization** | Retains AR model's Key-Value (KV) caching capability. |
| **Memory Footprint** | Low; benefits from the efficiency of standard causal generation structures. |

---

## Contributions

*   **Bridging Paradigms:** Addresses a critical trade-off in Large Language Modeling by successfully enabling AR models to perform masked infilling, a task previously exclusive to computationally inefficient MLM architectures.
*   **Architectural Innovation:** Proposes a novel, minimal-disturbance architecture (MARIA) that repurposes existing pre-trained models via a linear decoder, rather than requiring massive re-training or completely new model structures.
*   **Performance Benchmarking:** Establishes a new benchmark for masked infilling, demonstrating that the hybrid approach surpasses discrete diffusion models while maintaining the scalability and inference speed (via KV caching) required for modern LLM applications.

---

## Results

*   **Benchmark Dominance:** Achieved State-of-the-Art (SOTA) performance on masked infilling benchmarks.
*   **Metric Comparison:** Demonstrated significant improvements over discrete diffusion models (e.g., ROCStories BLEU-4 17.6 vs 13.8).
*   **Validation:** Experimental results validated that simple concatenation of hidden states enables high-quality infilling.
*   **Speed & Efficiency:** Successfully retained AR inference speed advantages through KV caching.
*   **Cost-Benefit:** Outperformed standard MLM efficiency, resolving the trade-off between infilling capability and scalability cost.

---
**Quality Score:** 9/10 &nbsp;|&nbsp; **References:** 17 citations