# Where AI Assurance Might Go Wrong: Initial lessons from engineering of critical systems

*Robin Bloomfield; John Rushby*

---

##  Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **References** | 40 Citations |
| **Study Type** | Position Paper / Comparative Analysis |
| **Key Standard** | Failure rates "not anticipated in fleet lifetime" |
| **Core Concept** | "Assurance 2.0" & Decision-Centric Assurance |
| **Risk Scope** | Artificial Fairly General Intelligence (AFGI) |

---

##  Executive Summary

> **Overview**
> Current AI safety frameworks are fundamentally inadequate for the deployment of high-stakes, critical systems. They are characterized by narrow boundaries, insufficient risk elaboration, and a lack of theoretical foundations. The central problem is a qualitative **"orders of magnitude" gap** in the confidence required for critical systems (such as aircraft flight control) compared to everyday consumer applications. This discrepancy prevents standard assurance techniques from scaling effectively, meaning that existing methods cannot guarantee the safety necessary for AI systems where failure results in catastrophic consequences.

> **Proposed Solution**
> To address these deficiencies, the authors propose **"Assurance 2.0,"** a structured, decision-centric shift grounded in Critical Systems Engineering and a "Dependability Perspective." This approach moves the focus from verifying the AI component in isolation to assuring the entire system environment through a technical architecture that includes a "World Model" of external factors and operators. The research introduces **Artificial Fairly General Intelligence (AFGI)** to characterize near-term societal risks that fall short of AGI but remain dangerous, and proposes a seven-step process spanning Environment Definition, Hazard Analysis, and Verification.

> **Conclusion**
> As a position paper based on comparative analysis rather than empirical experimentation, the study provides analytical benchmarks and specific metrics derived from aviation standards. It establishes the aviation standard for failure rates as events "not anticipated in fleet lifetime" and mandates a strict fault tolerance metric where "no single fault causes catastrophic failure." The analysis concludes that current frameworks fail to meet these standards due to their narrow focus, specifically noting that they ignore non-malign societal risks and lack a robust "Design Basis" for worst-case environmental challenges.

---

##  Key Findings

*   **Inadequate Frameworks:** Current AI safety frameworks are inadequate, characterized by narrow boundaries, insufficient risk elaboration, and assurance methods lacking theoretical foundations.
*   **The Confidence Gap:** There is an **'orders of magnitude' gap** in confidence required for critical systems versus everyday systems. This prevents standard techniques from scaling effectively.
*   **Decision-Centricity:** Effective assurance must be **decision-centric**, evaluating the specific criticality of individual decisions rather than just the overall system.
*   **Collaborative Evolution:** The evolution of critical systems engineering relies on open and diverse discussion.

---

##  Methodology

The authors employ a **comparative analysis approach** based on professional experience in traditional critical system assurance, specifically within aircraft flight control.

The research structure is defined by:
1.  **Three Technical Domains:**
    *   System Engineering
    *   Safety and Risk Analysis
    *   Decision Analysis and Support
2.  **Four Foundational Questions:**
    *   System Definition
    *   Performance Requirements
    *   Criticality Impact
    *   Trust Levels

---

##  Technical Details

The proposed approach utilizes **Critical Systems Engineering** with a 'Dependability Perspective' and **'Assurance 2.0'**, shifting focus from the AI component to the whole system environment.

*   **Architectural Components:**
    *   **World Model:** Defines external factors and operators.
    *   **Scope Shift:** Moves from component assurance to holistic system assurance.

*   **The Seven-Step Process:**
    1.  Environment Definition
    2.  System Requirements
    3.  Hazard Analysis (Design Basis)
    4.  Safety Requirements (defence in depth)
    5.  Requirements Validation
    6.  System Specification
    7.  Verification

*   **Key Concepts:**
    *   **Decision-Centric Assurance:** Evaluating individual decisions based on their specific criticality.
    *   **AFGI (Artificial Fairly General Intelligence):** A classification for near-term societal risks that pose significant dangers without being full AGI.

---

##  Results

As this is a position paper, no empirical experiments were conducted. Instead, the paper establishes analytical benchmarks and standards derived from aviation industries:

*   **Failure Rate Standard:** Failures should be events **"not anticipated in fleet lifetime."**
*   **Fault Tolerance Metric:** **"No single fault causes catastrophic failure."**
*   **Design Basis:** A defined standard for worst-case environmental challenges.

**Analytical Conclusions:**
*   Current AI safety frameworks are inadequate due to narrow boundaries.
*   Frameworks frequently ignore **Narrow AI** and **non-malign societal risks**.

---

##  Contributions

*   **Assurance 2.0 Advocacy:** Promotes the adoption of 'Assurance 2.0' as a structured method to support decision-making in high-stakes environments.
*   **Gap Identification:** Clearly identifies specific gaps in AI safety frameworks regarding boundaries and risk theories when compared to traditional critical systems engineering.
*   **Regulatory Guidance:** Maps findings to FAISC organizer questions to guide future dialogues and regulatory development.