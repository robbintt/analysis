# Occam's model: Selecting simpler representations for better transferability estimation

*Prabhant Singh; Sibylle Hess; Joaquin Vanschoren*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 8/10
> *   **Citations:** 40 references
> *   **Key Performance Indicator:** Up to **32% increase** in Kendall's Tau ($\tau$)
> *   **Primary Focus:** Transferability Estimation & Model Selection

---

## Executive Summary

Selecting the optimal pre-trained model for a specific downstream task is a critical component of transfer learning, yet identifying the best candidate without expensive full fine-tuning remains a challenge. Accurate transferability estimation is necessary to avoid the prohibitive computational costs associated with training multiple large-scale models merely to determine which performs best. While existing proxy metrics attempt to predict performance, they often lack robustness and theoretical grounding, resulting in sub-optimal model selection across diverse scenarios.

This paper introduces **"Occam's model,"** a novel framework grounded in the principle of Occamâ€™s Razor, which posits that simpler representations are inherently more transferable. The key innovation lies in redefining transferability as the **"trainability"** of a pre-trained model's internal representationsâ€”specifically, measuring how easily a linear classifier can separate target classes within the latent space. Rather than calculating gradients or feature distances, the authors propose two distinct metrics that assess linear separability and geometric density. This approach allows for the evaluation of how well a model's features can be adapted to a new task without the overhead of weight updates.

In rigorous empirical evaluations, the proposed metrics significantly outperformed state-of-the-art baselines, including LEEP, LogME, OTCE, and NLEEP. The primary measure of success was Kendallâ€™s Tau ($\tau$), which ranks the correlation between estimated transferability and actual fine-tuning performance. The authors achieved an increase in Kendall's Tau of up to **32%** compared to existing methods. These results were validated across a broad spectrum of architecturesâ€”such as Transformers (ViT, Swin), CNNs (ResNet), and self-supervised models (DINO, MoCo)â€”and diverse datasets ranging from ImageNet to remote sensing scene classification.

This work establishes a significant theoretical and practical shift in transferability estimation by validating the relationship between representation simplicity and model adaptability. The accompanying theoretical framework not only justifies the efficacy of the new metrics but also provides insights into their adaptability across different problem settings. By offering a computationally efficient and robust method for model selection, this research enables practitioners to make more informed decisions when deploying deep learning models, ultimately reducing resource waste and improving iteration times in model development pipelines.

---

## Key Findings

*   **Significant Performance Improvement:** The proposed metrics increase **Kendall's Tau by up to 32%** when compared to state-of-the-art baselines.
*   **Robustness Across Scenarios:** The metrics demonstrate robustness and practical utility when rigorously evaluated against existing methods across diverse problem settings.
*   **Efficacy via Separability:** Transferability can be effectively estimated by assessing how easily a pre-trained model's representations can be trained to separate target classes.
*   **Theoretical Validation:** Theoretical insights support the experimental results, explaining the metrics' efficacy and their adaptability to various scenarios.

---

## Methodology

The authors propose a novel framework for transferability estimation grounded in the perspective that transferability is a measure of the **'trainability'** of a pre-trained model's representations.

*   **Core Concept:** The approach conceptualizes transferability as the ease with which a model's internal representations can be adapted to linearly separate the classes of a target task.
*   **Theoretical Foundation:** This foundation is used to develop two distinct metrics.
*   **Validation:** These metrics are rigorously evaluated against state-of-the-art alternatives to test their correlation with actual fine-tuning performance.

---

## Contributions

*   **Novel Metrics:** Introduction of two new, effective metrics designed to estimate the transferability of pre-trained models without the computational cost of full fine-tuning.
*   **New Theoretical Perspective:** A unique conceptualization of transferability that focuses on class separation within representation spaces, differing from previous estimation approaches.
*   **Theoretical Framework:** Provision of theoretical insights that justify why the metrics work and explain their adaptability.
*   **Benchmarking Results:** Comprehensive empirical evidence demonstrating a **32% improvement** in ranking correlation (Kendall's Tau) over existing state-of-the-art methods.

---

## Technical Details

| Aspect | Description |
| :--- | :--- |
| **Underlying Principle** | Based on **Occam's Razor**; posits that simpler representations are more transferable. |
| **Estimation Method** | Estimates transferability by assessing the **separability** of target classes (specifically linear separability or geometric density) in the latent space rather than calculating gradients or feature distances directly. |
| **Theoretical Basis** | Theoretically grounded approach. |
| **Baselines Compared** | LEEP, LogME, OTCE, NLEEP, Etran, and Potential Energy. |
| **Architectures Covered** | **Transformers:** ViT, Swin, Xcit, MaxViT <br> **CNNs:** ResNet <br> **Self-supervised:** MoCo, SwAV, DINO |
| **Evaluation Metric** | Kendall's Tau ($\tau$) â€” a ranking correlation measure between estimated transferability and actual performance. |

---

## Results

The primary evaluation metric is Kendall's Tau ($\tau$).

*   **Performance:** The proposed metrics demonstrate an increase in Kendall's Tau of up to **32%** compared to state-of-the-art baselines.
*   **Robustness:** Results indicate robustness across diverse problem settings and practical utility in real-world model selection.
*   **Datasets:** Benchmarks likely include ImageNet, KTH-TIPS, Flower Classification, remote sensing scene classification, and Meta-Album.

---

## References

40 citations