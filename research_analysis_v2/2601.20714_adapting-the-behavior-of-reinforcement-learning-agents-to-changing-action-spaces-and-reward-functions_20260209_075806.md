---
ver: rpa2
title: Adapting the Behavior of Reinforcement Learning Agents to Changing Action Spaces
  and Reward Functions
arxiv_id: '2601.20714'
source_url: https://arxiv.org/abs/2601.20714
tags:
- learning
- drift
- action
- reward
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces MORPHIN, a self-adaptive Q-learning framework\
  \ that enables agents to dynamically adapt to non-stationary environments, specifically\
  \ addressing changes in reward functions (goals) and on-the-fly expansions of the\
  \ action space. The core innovation lies in integrating concept drift detection\
  \ using the Page-Hinkley test with dynamic adjustments to both exploration (\u03B5\
  ) and learning (\u03B1) rates."
---

# Adapting the Behavior of Reinforcement Learning Agents to Changing Action Spaces and Reward Functions

## Quick Facts
- arXiv ID: 2601.20714
- Source URL: https://arxiv.org/abs/2601.20714
- Reference count: 22
- Primary result: MORPHIN achieves 1.7x improvement in learning efficiency over standard Q-learning in non-stationary environments

## Executive Summary
This paper introduces MORPHIN, a self-adaptive Q-learning framework that enables agents to dynamically adapt to non-stationary environments, specifically addressing changes in reward functions (goals) and on-the-fly expansions of the action space. The core innovation lies in integrating concept drift detection using the Page-Hinkley test with dynamic adjustments to both exploration (ε) and learning (α) rates. MORPHIN detects environmental changes and triggers re-exploration while maintaining Q-table structure to prevent catastrophic forgetting. Experimental results demonstrate superior performance compared to standard Q-learning baselines, with MORPHIN achieving a 1.7x improvement in learning efficiency (measured in total steps) and successfully converging after each induced drift in both Gridworld benchmarks and traffic signal control simulations.

## Method Summary
MORPHIN combines concept drift detection via the Page-Hinkley test with adaptive exploration and learning rate modulation to handle non-stationary environments. The system monitors cumulative episode rewards to detect drifts, triggering exploration resets and dynamically adjusting learning rates based on temporal difference error magnitude. Unlike approaches that retrain from scratch, MORPHIN preserves existing Q-values while extending table dimensions for new actions, enabling rapid adaptation without catastrophic forgetting. The framework was tested on 9×9 Gridworld environments with goal relocations and action expansions, as well as traffic signal control scenarios with changing arrival rates.

## Key Results
- MORPHIN achieves 1.7x improvement in learning efficiency (total steps) compared to standard Q-learning baselines
- Successfully converges after each induced drift in both Gridworld benchmarks and traffic signal control simulations
- Preserves knowledge of previously learned goals while adapting to new ones, and effectively incorporates new actions to discover more optimal policies

## Why This Works (Mechanism)

### Mechanism 1: Concept Drift Detection via Page-Hinkley Test
Detecting environmental changes through monitoring cumulative episode rewards enables timely adaptive responses. The Page-Hinkley (PH) test monitors the stream of cumulative episode rewards (R_ep). A drift is flagged when the cumulative difference between R_ep and its running mean exceeds a threshold H. Upon detection, the exploration decay counter is reset, forcing re-exploration. This assumes shifts in reward distribution reliably signal meaningful environmental changes that require policy adaptation. The approach may fail when drift produces reward distributions that are subsets of previously seen values (undetected drift at episode 8,000 in traffic scenario), or when H and δ are poorly tuned to the reward scale.

### Mechanism 2: Adaptive Learning Rate Modulation via TD-Error
Temporal Difference error magnitude provides a reliable signal for dynamically scaling learning rate to accelerate adaptation after drift. When concept drift occurs, the agent's existing Q-values become mismatched with new environmental dynamics, producing large TD-errors. These large errors drive α* toward α_max via the sigmoid function, accelerating knowledge update. As the policy stabilizes, TD-errors diminish and α* decays toward base α. This assumes high TD-errors indicate genuine environment-policy mismatch rather than noise, and rapid overwriting of obsolete Q-values is beneficial. The mechanism may fail if TD-error sensitivity parameter k is poorly calibrated, or if environment noise produces sustained high TD-errors unrelated to actual drift.

### Mechanism 3: Q-Table Structure Preservation with Dimensional Extension
Preserving existing Q-values while extending table dimensions for new actions enables rapid adaptation without catastrophic forgetting. The Q-table is never reset upon drift detection. For action space expansion, a new row is added for the new action (initialized to zero), while existing Q-values serve as informed starting points. Combined with forced re-exploration, this allows discovery of whether new actions improve upon existing policies. This assumes prior learned state-action values remain partially relevant after environmental changes, and obsolete values will be corrected through re-exploration. The approach may fail when prior Q-values are actively harmful (opposite optimal direction) and re-exploration is insufficient to correct them within the adaptation window.

## Foundational Learning

- **Temporal Difference (TD) Error**
  - Why needed here: Core signal for adaptive learning rate; quantifies prediction-reality mismatch.
  - Quick check question: Can you explain why TD-error would spike immediately after a goal relocation?

- **Exploration-Exploitation Tradeoff in Non-Stationary Environments**
  - Why needed here: Central problem MORPHIN addresses; standard ε-decay fails when environment changes.
  - Quick check question: Why does a fixed exponential ε-decay prevent adaptation to new goals?

- **Page-Hinkley Statistical Test**
  - Why needed here: Drift detection mechanism; understanding its parameters (δ, H) is critical for tuning.
  - Quick check question: What happens to false positive rate if threshold H is set too low?

## Architecture Onboarding

- Component map:
  - Drift Detector (PH-test) -> Exploration Controller -> Learning Rate Controller -> Q-Table Manager -> Standard Q-Learning Core
  - Critical path: R_ep stream → PH-test → drift flag → e=0 reset → ε*→1 → exploration → TD-error spike → α*→α_max → rapid Q-update

- Design tradeoffs:
  - Detection sensitivity (δ, H): Lower thresholds catch subtle drifts but increase false positives
  - TD-error sensitivity (k): Controls how aggressively α* responds to prediction errors
  - Single Q-table vs. context models: Lower memory overhead but potential for knowledge interference

- Failure signatures:
  - Undetected drifts (episode 8,000): Reward distribution subset of prior values
  - Slow convergence: k or H poorly calibrated to environment reward scale
  - Catastrophic forgetting: Exploration reset insufficient; Q-values overwritten before new policy stabilized

- First 3 experiments:
  1. **Gridworld goal relocation (single drift)**: Set up 9×9 grid, train to goal A for 300 episodes, relocate to goal B. Verify PH-test triggers and convergence time < baseline. Log ε*, α*, and TD-error trajectories.
  2. **Gridworld action expansion**: Train for 300 episodes with 4 actions (N/S/E/W), add "jump" action. Confirm Q-table extension and new action discovery. Compare path optimality before/after.
  3. **Traffic control with arrival rate shift**: Replicate two-lane intersection scenario. Induce λ increase at episode 3,000. Measure recovery time vs. baseline. Test parameter sweep on H (100–500) to observe detection sensitivity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the MORPHIN framework be effectively generalized to Deep Reinforcement Learning architectures to handle high-dimensional state spaces?
- Basis in paper: The authors explicitly state future work should focus on "Extending the core principles of MORPHIN from tabular methods to deep RL architectures."
- Why unresolved: The current implementation relies on tabular Q-learning with a structural Q-table that preserves specific state-action values, a mechanism not directly transferable to neural network weights.
- What evidence would resolve it: Successful implementation and convergence analysis of MORPHIN within a Deep Q-Network (DQN) or similar architecture in a non-stationary environment.

### Open Question 2
- Question: How can drift detection be modified to identify environmental shifts where new reward distributions are subsets of previously observed values?
- Basis in paper: The paper notes a specific failure case where a second drift was "not detected because the new reward distribution is a subset of previously seen values," and suggests exploring "more robust drift detection methods."
- Why unresolved: The Page-Hinkley test relies on cumulative differences exceeding a threshold, which subset distributions may fail to trigger, causing the agent to miss changes.
- What evidence would resolve it: A modified detection mechanism that successfully identifies and triggers adaptation for "subset" drifts in the traffic simulation scenario without manual threshold adjustment.

### Open Question 3
- Question: What is the impact of the TD-error sensitivity (k) and Page-Hinkley threshold (H) hyperparameters on the stability and speed of adaptation?
- Basis in paper: The authors identify the need for "conducting a rigorous sensitivity analysis of the introduced hyperparameters, particularly the TD-error sensitivity (k) and H threshold."
- Why unresolved: Current parameters were determined empirically based on the reward scales of specific environments, leaving their broader robustness untested.
- What evidence would resolve it: A systematic ablation study plotting convergence times and failure rates across a wide range of values for k and H.

## Limitations
- Performance improvements are demonstrated only in tabular RL settings and may not generalize to high-dimensional or continuous state-action spaces
- The method relies on hand-tuned hyperparameters (δ, H, k) whose optimal values may vary significantly across different domains
- While the framework avoids explicit Q-table resets, it cannot guarantee preservation of useful knowledge when drift produces reward distributions that are statistical subsets of prior values

## Confidence
- **High Confidence**: Q-table structure preservation prevents catastrophic forgetting (directly observed in experiments); concept drift detection via Page-Hinkley test reliably triggers exploration resets in standard goal-relocation scenarios
- **Medium Confidence**: TD-error modulated learning rate accelerates adaptation (mechanism sound but sensitivity parameter k requires careful calibration); MORPHIN maintains knowledge of previously learned goals while adapting to new ones (observed but limited to tested environments)
- **Low Confidence**: Performance improvements generalize to high-dimensional or continuous state-action spaces (methodology currently limited to tabular RL); the approach scales to multiple concurrent drift types (tested only on sequential single-drift scenarios)

## Next Checks
1. **Parameter Sensitivity Analysis**: Systematically vary PH-test parameters (δ, H) and TD-error sensitivity (k) across multiple runs to establish robust hyperparameter ranges and quantify performance variance
2. **Multi-Drift Scenario**: Implement a benchmark with simultaneous goal relocation and action space expansion occurring within the same episode sequence to test MORPHIN's ability to handle compound non-stationarity
3. **Knowledge Interference Test**: Design an experiment where the optimal action for a previously learned goal becomes actively detrimental under new conditions (opposite optimal direction), then measure whether MORPHIN can recover without catastrophic interference