---
ver: rpa2
title: Temporal Analysis of Adversarial Attacks in Federated Learning
arxiv_id: '2501.11054'
source_url: https://arxiv.org/abs/2501.11054
tags:
- data
- learning
- clients
- outlier
- federated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper studies how adversarial clients affect the performance
  of Federated Learning (FL) systems over time. It examines eight different FL models,
  including classical classifiers like MLR and SVC, tree-based models like Random
  Forest and XGBoost, and neural networks such as MLP, CNN, RNN, and LSTM, under three
  types of attacks: label flipping, model poisoning, and GAN reconstruction.'
---

# Temporal Analysis of Adversarial Attacks in Federated Learning

## Quick Facts
- arXiv ID: 2501.11054
- Source URL: https://arxiv.org/abs/2501.11054
- Reference count: 40
- Primary result: Temporal attacks, especially in later rounds, significantly degrade FL model performance, with outlier detection providing substantial defense improvement.

## Executive Summary
This paper investigates how the timing of adversarial attacks affects Federated Learning model performance. Using eight different FL models and three attack types (label flipping, model poisoning, GAN reconstruction), the authors systematically vary attack timing across FL rounds. Their experiments demonstrate that attacks in later rounds cause the most severe accuracy degradation, particularly for neural network architectures under model poisoning attacks. Outlier detection methods, especially One-Class SVM, provide effective defense by identifying and excluding malicious client updates, substantially improving model resilience across most attack scenarios.

## Method Summary
The study evaluates eight FL models (MLR, SVC, MLP, CNN, RNN, LSTM, Random Forest, XGBoost) using MNIST dataset with 10 FL rounds and 100 clients (50 for RNN/LSTM/Trees). Attacks are executed with 25% malicious clients in three temporal patterns: throughout all rounds (FULL), middle rounds only (MID), or final rounds only (END). Three attack types are implemented: label flipping (modifying labels), model poisoning (using MPAF method with magnified weight differences), and GAN reconstruction (conditional GAN generating adversarial samples). Outlier detection employs One-Class SVM trained on normalized client metrics (precision, recall, F1, loss) to identify malicious updates.

## Key Results
- Later-stage attacks (END condition) cause the most severe accuracy degradation across all models
- CNN, RNN, and LSTM models are particularly vulnerable to model poisoning attacks, dropping to near-random accuracy without defense
- One-Class SVM outlier detection achieves 97% accuracy in identifying malicious clients and substantially improves model resilience
- GAN reconstruction attacks are somewhat effective on tree-based algorithms (Random Forest and XGBoost)
- Attack timing significantly impacts model performance, with temporal attacks most effective when executed throughout or during later rounds

## Why This Works (Mechanism)

### Mechanism 1: Late-Stage Attack Amplification
By the final 30% of rounds (END condition), the global model has largely converged toward an optimum. Malicious updates at this stage directly corrupt an already-stabilized weight configuration, with fewer remaining honest updates available to correct the trajectory. Early-round attacks allow subsequent training rounds to "recover" the model.

### Mechanism 2: Outlier Detection via Metric-Based Client Profiling
The defense extracts per-client evaluation metrics (precision, recall, F1-score, loss), normalizes them to [0,1], and trains a One-Class SVM on honest client patterns. Malicious clients—whose poisoned data or tampered weights produce anomalous metric distributions—are flagged and excluded from aggregation.

### Mechanism 3: Architecture-Dependent Attack Vulnerability
Neural network architectures exhibit differential vulnerability to specific attack types based on how they aggregate and propagate weight updates. CNNs, RNNs, and LSTMs are highly sensitive to model poisoning because their learned feature hierarchies and temporal dependencies amplify corrupted weight contributions during aggregation.

## Foundational Learning

- **Federated Learning Training Loop:** Understanding the round-based structure (client selection → broadcast → local training → aggregation → global update) is essential to grasp why attack timing matters and where defenses can be inserted.
  - Quick check: In a 10-round FL process with END attacks, during which rounds would malicious clients be active?

- **FedAvg Aggregation:** The paper uses FedAvg (weighted averaging of client weights) for all non-tree models; attacks exploit this averaging to inject corrupted gradients.
  - Quick check: How does FedAvg's weighted averaging formula give adversarial clients influence over the global model?

- **Outlier Detection Fundamentals:** The defense mechanism relies on identifying statistical anomalies in client behavior; understanding One-Class SVM decision boundaries is critical to interpreting why it outperforms Isolation Forest and Robust Covariance.
  - Quick check: Why might One-Class SVM struggle if malicious clients deliberately mimic honest metric distributions?

## Architecture Onboarding

- **Component map:** Server -> Client selection -> Client training -> Client update submission -> Server aggregation with outlier detection -> Global model update
- **Critical path:** 1) Server initializes global model and selects clients 2) Clients receive global weights and perform local training 3) Adversarial clients inject attacks during designated rounds 4) Clients send updates to server 5) Server applies outlier detection (if enabled) to filter suspicious updates 6) Server aggregates remaining updates and updates global model 7) Loop repeats for configured rounds
- **Design tradeoffs:** Outlier detection threshold (higher sensitivity catches more attacks but risks excluding honest clients), attack timing (END attacks maximize damage but provide early-round baseline), model selection (LSTM achieves highest baseline accuracy but is most vulnerable)
- **Failure signatures:** Sudden accuracy collapse in final rounds with normal early performance, outlier detection accuracy <80%, high variance across FL runs
- **First 3 experiments:** 1) Baseline validation: Run all 8 models without attacks to reproduce reported baseline accuracies 2) Core vulnerability test: Execute model poisoning END attack on CNN/RNN/LSTM with and without One-Class SVM outlier detection 3) Defense breaking point: Vary malicious client ratio under model poisoning END to determine when outlier detection effectiveness degrades

## Open Questions the Paper Calls Out

- **Question:** How do temporal adversarial attacks affect model performance in fully decentralized Federated Learning architectures compared to the centralized setup?
  - Basis: The conclusion states that "a fully decentralized FL structure would be an interesting case study"
  - Why unresolved: The experimental design relied exclusively on a central server to coordinate training and aggregate updates
  - What evidence would resolve it: Comparative accuracy metrics for decentralized FL models subjected to FULL, MID, and END attack strategies

- **Question:** Can differential privacy serve as an effective defense against temporal attacks without excessively compromising the utility of the global model?
  - Basis: The authors suggest exploring "the effectiveness of more advanced defense mechanisms, such as differential privacy"
  - Why unresolved: The study only evaluated basic outlier detection methods
  - What evidence would resolve it: Accuracy and loss results of FL models employing differential privacy when subjected to model poisoning and label flipping attacks

- **Question:** Can outlier detection maintain high efficacy in "blind" scenarios where the defense mechanism has no prior labeled knowledge of which clients are malicious?
  - Basis: Section 5.2 states "We employ a supervised approach to create a classifier that attempts to distinguish between honest and malicious clients"
  - Why unresolved: The reported 97% accuracy for One-Class SVM relies on training with known malicious examples
  - What evidence would resolve it: Performance metrics of unsupervised anomaly detectors trained strictly on benign data or unlabeled mixed data

## Limitations
- Temporal attack dynamics rely on assumptions about convergence curves that need empirical validation across different FL configurations
- Outlier detection effectiveness is contingent on attackers not adaptively optimizing their updates to evade detection
- Architecture-specific vulnerabilities are based on limited experimental scope (10 FL rounds, 100 clients, MNIST)

## Confidence
- High: Temporal attacks cause greater accuracy degradation when applied in later FL rounds
- High: One-Class SVM outlier detection substantially improves resilience to model poisoning attacks
- Medium: Tree-based models are particularly vulnerable to GAN reconstruction attacks
- Low: Attack impact scales systematically with architectural complexity

## Next Checks
1. Vary the total number of FL rounds post-model poisoning END attack to determine if early-stage attacks can be compensated with additional honest client updates
2. Test adaptive attackers who optimize poisoned updates to produce metric profiles within the normal distribution detected by One-Class SVM
3. Evaluate model poisoning attacks across diverse datasets (CIFAR-10, FEMNIST) and non-IID distributions to assess generalizability of architecture-specific vulnerability patterns