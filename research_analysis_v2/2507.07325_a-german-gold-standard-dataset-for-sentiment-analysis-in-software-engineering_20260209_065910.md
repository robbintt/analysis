---
ver: rpa2
title: A German Gold-Standard Dataset for Sentiment Analysis in Software Engineering
arxiv_id: '2507.07325'
source_url: https://arxiv.org/abs/2507.07325
tags:
- sentiment
- dataset
- software
- german
- statements
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the lack of a German-language gold-standard
  dataset for sentiment analysis in software engineering (SE). The authors created
  a dataset of 5,949 unique German developer statements extracted from the Android-Hilfe.de
  forum, each annotated with one of six basic emotions using the Shaver et al.
---

# A German Gold-Standard Dataset for Sentiment Analysis in Software Engineering

## Quick Facts
- **arXiv ID:** 2507.07325
- **Source URL:** https://arxiv.org/abs/2507.07325
- **Reference count:** 40
- **Key result:** Created a 5,949-statement German SE sentiment dataset; existing tools perform poorly, especially on negative sentiment

## Executive Summary
This study addresses the critical gap in German-language sentiment analysis resources for software engineering by creating the first gold-standard dataset of 5,949 German developer statements annotated with six basic emotions using the Shaver et al. model. The dataset was built from Android-Hilfe.de forum posts and annotated by four German-speaking computer science students through a rigorous two-round process with intermediate discussions. The resulting dataset demonstrates that existing German sentiment analysis tools (GerVADER, SentiStrength DE, TextBlobDE, BertDE) perform poorly on SE domain text, particularly struggling to detect negative sentiment, highlighting the need for domain-specific sentiment analysis solutions for German-speaking developers.

## Method Summary
The authors created a German SE sentiment dataset by crawling 20,380 statements from Android-Hilfe.de forum, then pre-sorting with GerVADER to select 2,000 statements per polarity class. Four German-speaking CS students (one withdrew) annotated the statements using Shaver et al.'s 6-emotion model (Love, Joy, Surprise, Anger, Sadness, Fear + Neutral) with three raters per statement and intermediate discussion rounds. The final dataset contains 5,949 statements with high interrater agreement (Fleiss' Kappa: 0.71 for Neutral, 0.52 overall). The dataset was evaluated against four German sentiment tools using precision, recall, F1 (micro/macro), and accuracy metrics.

## Key Results
- Final dataset: 69.78% Neutral, 21.36% Positive, 8.85% Negative (vs. GerVADER's pre-sorting: 33.33% each)
- Highest tool accuracy: SentiStrength DE (0.72) vs. BertDE (0.36)
- All tools struggled with negative sentiment (F1 scores: 0.16-0.42)
- Fleiss' Kappa improved from 0.50 to 0.71 for Neutral through intermediate discussions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative annotation with intermediate discussions improves interrater reliability compared to single-pass labeling
- Mechanism: The annotation process involved two rounds with a structured intermediate discussion phase where disagreements were explicitly resolved. This reduced subjectivity, particularly for ambiguous emotions like "Love" which showed a Fleiss' Kappa improvement of +0.81 between rounds. The discussion established objective criteria—for example, statements must explicitly express emotion rather than raters inferring what they "would feel" in that situation.
- Core assumption: Subjectivity in emotion annotation can be systematically reduced through structured consensus-building, and this reduction translates to higher dataset validity
- Evidence anchors: Section on labeling process shows agreement increased from 0.63 to 0.82 and Fleiss' Kappa from 0.5 to 0.71 for Neutral; related work by Novielli et al. emphasizes high interrater reliability as crucial for gold-standard validity

### Mechanism 2
- Claim: Pre-sorting with existing sentiment tools (GerVADER) does not guarantee balanced polarity distributions in domain-specific datasets
- Mechanism: The authors pre-sorted 20,380 statements using GerVADER to select 2,000 statements per polarity class (positive, neutral, negative). However, final manual annotation revealed 69.78% neutral, 21.36% positive, and only 8.85% negative—a severe imbalance. GerVADER appears to detect polarity in statements that humans classify as neutral, particularly for negative sentiment (F1 = 0.34 for negative class)
- Core assumption: Existing German sentiment tools trained on non-SE domains conflate technical language patterns with emotional expression
- Evidence anchors: Figure 1 shows the dramatic shift from GerVADER's pre-sorting (33.33% each class) to final manual results (69.78% neutral); "GerVADER tends to detect polarity even in neutral developer statements, particularly for the Negative class"

### Mechanism 3
- Claim: Domain-specific training data is prerequisite for acceptable sentiment analysis performance; cross-domain transfer fails for German SE text
- Mechanism: BertDE (machine-learning-based) achieved only 0.36 accuracy versus SentiStrength DE's 0.72, despite ML approaches typically outperforming lexicon methods. The authors attribute this to BertDE lacking SE-domain training. English SE-specific tools like Senti4SD achieve ~0.89 accuracy on in-domain data but perform poorly out-of-domain
- Core assumption: The language patterns in developer forums (technical jargon, issue descriptions, solution proposals) carry domain-specific semantic structures that general-purpose sentiment models cannot capture
- Evidence anchors: "BertDE was not trained for the SE domain...it is possible that BertDE would also perform well if trained on the dataset created in this study"; Table IV shows BertDE macro-average F1 of 0.39 vs. SentiStrength DE's 0.58; all tools show lowest F1 for Negative class (0.16-0.42)

## Foundational Learning

- Concept: **Fleiss' Kappa vs. Cohen's Kappa for multi-rater agreement**
  - Why needed here: The paper uses both metrics—Fleiss' Kappa for 3+ rater agreement across all statements, Cohen's Kappa for pairwise comparisons between tools and human labels. Understanding when each applies is essential for interpreting validity claims
  - Quick check question: If you have 4 raters labeling 100 statements each, which metric do you use for overall agreement?

- Concept: **Emotion classification models (basic vs. dimensional vs. hierarchical)**
  - Why needed here: The paper chose Shaver et al.'s hierarchical model (6 basic emotions with subcategories) over dimensional models (valence-arousal-dominance). This choice affects label granularity and annotation difficulty
  - Quick check question: Why might "surprise" be problematic as a basic emotion category (hint: consider valence ambiguity)?

- Concept: **Class imbalance effects on F1-scores (micro vs. macro averaging)**
  - Why needed here: With 69.78% neutral statements, accuracy can be misleading. Macro F1 equally weights classes (revealing poor negative detection), while micro F1 weights by class size (inflated by neutral majority). The paper reports both
  - Quick check question: If a dataset is 90% neutral and a model always predicts "neutral," what's the accuracy? What's the macro F1 for positive/negative classes?

## Architecture Onboarding

- Component map:
```
Data Pipeline:
Source Selection (Android-Hilfe.de forum) -> Crawler (Scrapy framework) -> Pre-sorting (GerVADER sentiment scoring) -> Manual Annotation (3 raters per statement) -> Gold-Standard Output (5,949 statements)

Evaluation Pipeline:
Tool Input (statements) -> Metric Calculation (precision, recall, F1, accuracy) -> Agreement Analysis (Cohen's Kappa vs. human labels) -> Performance Reporting
```

- Critical path: Crawler accuracy → Pre-sorting balance → Annotation guideline clarity → Intermediate discussion quality → Final interrater reliability. Errors propagate; poor filtering (e.g., missing signatures) requires manual post-processing
- Design tradeoffs:
  - Single data source (Android-Hilfe) vs. multiple sources: Chosen for language consistency but limits generalizability
  - 6-emotion model vs. 3-polarity: Richer annotations but lower per-class sample sizes (Fear: 0.13%, Positive Surprise: 0.07%)
  - 3 raters vs. more: Resource-efficient; Ortu et al. found >2 raters yields diminishing returns, but this study had one rater withdraw
- Failure signatures:
  - Low Fleiss' Kappa for specific emotions (Round 1 Love: 0.04) → indicates unclear guidelines for that category
  - Tool accuracy >> Cohen's Kappa (e.g., SentiStrength DE: 0.72 accuracy but 0.35 Kappa) → tool is right often but for wrong reasons; systematically biased
  - Large gap between micro and macro F1 → class imbalance masking poor minority-class performance
- First 3 experiments:
  1. **Baseline replication**: Run all four German tools (GerVADER, SentiStrength DE, TextBlobDE, BertDE) on the gold-standard dataset. Verify you can reproduce reported F1-scores (Table IV) and Kappa values (Table V). Deviation >5% suggests data processing issues
  2. **Annotation stability test**: Sample 100 statements from the dataset and have 2-3 new annotators label them independently using the Shaver et al. guidelines. Calculate Fleiss' Kappa against original labels. Values <0.6 suggest the gold-standard may not generalize across annotator pools
  3. **Domain transfer experiment**: Fine-tune a German BERT model on this dataset (80/20 train/test split). Compare test accuracy against BertDE baseline. If improvement <10 percentage points, the dataset size (5,949) may be insufficient for ML approaches—consider this before investing in model development

## Open Questions the Paper Calls Out

- **Open Question 1**: Does a more diverse group of raters, considering gender, age, and background, yield different annotation results compared to the homogeneous group used in this study?
  - Basis: Section V-C (External Validity) explicitly states this question
  - Why unresolved: The study utilized a convenience sample of five computer science students who were all male and aged 20–25
  - What evidence would resolve it: Replication with demographically diverse raters, statistical comparison of interrater agreement and label distribution

- **Open Question 2**: To what extent can generative Large Language Models (LLMs) be used for filtering datasets, and how does their performance compare to traditional methods?
  - Basis: Section V-B (Future Work) proposes investigating LLM filtering
  - Why unresolved: The current study relied on lexicon-based tool GerVADER for pre-sorting and filtering phase
  - What evidence would resolve it: Empirical study where LLM filters raw forum data, comparing quality of resulting gold standard and time investment against GerVADER-based approach

- **Open Question 3**: Can the performance of machine-learning-based tools like BertDE be significantly improved by fine-tuning them on this specific German SE dataset?
  - Basis: Inferred from Section V-A, authors note BertDE performed poorly because not trained for SE domain, speculating it "would also perform well if trained on the dataset created in this study"
  - Why unresolved: Paper only evaluates existing tools "off-the-shelf" to demonstrate lack of domain-specific solutions
  - What evidence would resolve it: Experiment fine-tuning BertDE (or similar transformer) on 5,949 labeled statements, measuring improvement in F1-scores and accuracy compared to baseline scores

## Limitations
- Single data source (Android-Hilfe.de) limits generalizability across different SE communities
- Dataset size (5,949 statements) may be insufficient for training robust ML models
- Withdrawal of one annotator during process could have affected group dynamics and consensus-building

## Confidence
- **High**: Claims about necessity of German SE sentiment datasets (given no existing resources)
- **Medium**: Claims about domain-specific tool performance gaps (supported by systematic tool evaluation)
- **Low**: Claims about optimal annotation methodology (based on single study with limited annotators)

## Next Checks
1. Replicate the annotation process with a different German SE forum to test generalizability
2. Fine-tune German BERT models on this dataset and evaluate on held-out test data
3. Compare performance with English SE sentiment tools when applied to German-translated statements