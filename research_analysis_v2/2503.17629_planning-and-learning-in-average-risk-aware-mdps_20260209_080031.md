---
ver: rpa2
title: Planning and Learning in Average Risk-aware MDPs
arxiv_id: '2503.17629'
source_url: https://arxiv.org/abs/2503.17629
tags:
- risk
- algorithm
- q-learning
- assumption
- average
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces planning and learning algorithms for average
  cost Markov decision processes (MDPs) with general dynamic risk measures. The authors
  propose a relative value iteration (RVI) algorithm for planning and two model-free
  Q-learning algorithms: a multi-level Monte Carlo (MLMC) based approach and an off-policy
  algorithm specialized for utility-based shortfall risk (UBSR) measures.'
---

# Planning and Learning in Average Risk-aware MDPs

## Quick Facts
- **arXiv ID:** 2503.17629
- **Source URL:** https://arxiv.org/abs/2503.17629
- **Reference count:** 40
- **Primary result:** Planning and learning algorithms for average cost MDPs with general dynamic risk measures, with convergence guarantees and empirical validation

## Executive Summary
This paper develops planning and learning algorithms for average cost Markov decision processes (MDPs) under general dynamic risk measures. The authors propose a relative value iteration (RVI) algorithm for planning and two model-free Q-learning approaches: a multi-level Monte Carlo (MLMC) method for general risk measures and an off-policy algorithm specialized for utility-based shortfall risk (UBSR). Both RVI and MLMC-based Q-learning are proven to converge to optimality. Empirical results validate convergence across different risk measures, demonstrate faster convergence and lower variance for UBSR Q-learning compared to MLMC Q-learning, and show that risk-aware policies can be finely tuned to agent preferences in practical applications like machine replacement and inventory management.

## Method Summary
The paper addresses average cost MDPs with dynamic risk measures by proposing three main algorithms. The RVI algorithm extends value iteration to the average cost setting by subtracting a reference function at each iteration, ensuring convergence under ergodicity assumptions. The MLMC Q-learning algorithm constructs an unbiased estimator of the risk-aware Bellman operator using geometric sampling to enable model-free learning. The UBSR Q-learning algorithm exploits the specific structure of utility-based shortfall risk to reformulate the problem as root-finding, enabling single-sample updates without resampling. The algorithms are supported by theoretical convergence proofs under specific assumptions about the risk measures and MDP structure.

## Key Results
- RVI algorithm converges to optimal policy for general dynamic risk measures under ergodicity conditions
- MLMC Q-learning converges to optimality for general risk measures using unbiased estimation
- UBSR Q-learning shows faster convergence and lower variance than MLMC Q-learning in empirical tests
- Risk-aware policies can be tuned to different agent preferences in practical applications

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Relative Value Iteration (RVI) converges to an optimal risk-aware policy for general dynamic risk measures, provided the risk map satisfies a specific ergodicity condition.
- **Mechanism:** The algorithm mitigates the value overflow common in average-cost MDPs by subtracting a reference function $f(V_n)$ at every iteration. It replaces the standard Bellman operator with a risk-aware operator $G(v)(x) = \min_a R_{x,a}(c(x,a) + v)$. Convergence is ensured because this operator acts as a contraction mapping under the *span-seminorm* rather than the standard infinity norm.
- **Core assumption:** Assumption 2.12 (Doeblin type condition), which requires a specific form of ergodicity for the risk map to guarantee the optimal average risk is independent of the initial state.
- **Evidence anchors:**
  - [abstract] "We propose a relative value iteration (RVI) algorithm... [proven] to converge."
  - [section 3.1] Theorem 3.2 states convergence under Assumptions 2.12 and 3.1.
  - [corpus] Weak direct evidence; corpus neighbors focus on risk-neutral or specific risk measures rather than the general RVI contraction proof.
- **Break condition:** The mechanism fails if the underlying Markov chain does not satisfy the unichain/ergodicity requirements or if the risk measure does not satisfy the translation invariance properties required for the span-seminorm contraction.

### Mechanism 2
- **Claim:** Multi-Level Monte Carlo (MLMC) Q-learning enables model-free learning for general risk measures by constructing an unbiased estimator of the Bellman operator.
- **Mechanism:** Standard Q-learning fails for risk measures because single samples cannot directly estimate the distributional property $R(v)$. This mechanism uses a randomized truncation parameter $N$ (geometric distribution) to sample $2^{N+1}$ transitions. It combines estimates from coarse (few samples) and fine (many samples) levels to construct $\hat{H}$, an unbiased estimator of the risk-aware Bellman operator. This allows the use of standard stochastic approximation (Robbins-Monro) for convergence.
- **Core assumption:** Assumption 4.6 (Hölder continuity) ensures the risk map is sufficiently smooth with respect to the Wasserstein distance between distributions; Assumption 4.1 (unbiasedness/controllable variance).
- **Evidence anchors:**
  - [abstract] "...MLMC-based Q-learning... proven to converge to optimality."
  - [section 4.2] Theorem 4.7 proves the unbiasedness of the MLMC estimator $\hat{H}$ defined in Eq (4.2).
  - [corpus] Weak; related papers in corpus discuss different estimation techniques (e.g., fixed-point methods) rather than the MLMC approach.
- **Break condition:** The mechanism breaks if the geometric parameter $r$ is too small (leading to infinite expected samples) or if the risk measure is not law-invariant/Hölder continuous (e.g., pure CVaR without the mean component may violate assumptions).

### Mechanism 3
- **Claim:** An off-policy Q-learning algorithm specialized for Utility-Based Shortfall Risk (UBSR) can converge empirically without the costly resampling required by MLMC, by reformulating the objective as root-finding.
- **Mechanism:** This algorithm exploits the specific definition of UBSR where $E[\ell(v - SR(v))] = 0$. Instead of estimating the risk measure explicitly, the update rule uses the loss function $\ell$ applied to the temporal difference error. This transforms the problem into finding the root of a non-linear equation via stochastic approximation, allowing standard single-sample updates.
- **Core assumption:** The risk measure must be UBSR; the update rule relies on the property that the derivative of the risk measure can be implicitly handled by the loss function.
- **Evidence anchors:**
  - [abstract] "...off-policy algorithm specialized for utility-based shortfall risk... confirm empirically the convergence..."
  - [section 4.3] Eq (4.3) defines the update rule; the text explicitly states "theoretical convergence remains an open question."
  - [corpus] Weak; while corpus mentions "Risk-Averse Total-Reward RL" and other risk-aware methods, none appear to use this specific root-finding loss-function update.
- **Break condition:** If the theoretical convergence proof does not hold (currently an open question), the algorithm might diverge or oscillate in certain MDPs. Empirically, it relies on the loss function $\ell$ being well-behaved.

## Foundational Learning

- **Concept: Average Reward / Cost Criterion**
  - **Why needed here:** The paper focuses on "continuing tasks" where the agent runs forever (e.g., network control). Unlike discounted settings, the value function is not bounded by definition, necessitating the "Relative" aspect of the RVI algorithms.
  - **Quick check question:** Can you explain why simply subtracting $f(V_n)$ prevents the values from exploding to infinity during iteration?

- **Concept: Dynamic Risk Measures**
  - **Why needed here:** The paper moves beyond risk-neutrality. You must understand that a "risk map" $R_{x,a}$ evaluates a random cost variable $v$ based on its distribution, not just its expectation. Key properties include translation invariance (adding constant $c$ to costs adds $c$ to risk) and convexity.
  - **Quick check question:** How does a coherent risk measure differ from a general convex one, and which class does Entropic risk belong to?

- **Concept: Stochastic Approximation (ODE Method)**
  - **Why needed here:** This is the mathematical bedrock for proving Q-learning convergence. It maps the discrete update steps to a continuous Ordinary Differential Equation (ODE). If the ODE has a stable equilibrium and the noise is controlled (unbiased/variance-bounded), the algorithm converges.
  - **Quick check question:** What are the Robbins-Monro conditions for the step size $\gamma(n)$, and why must $\sum \gamma(n)^2 < \infty$?

## Architecture Onboarding

- **Component map:** Planner (Model-Based) -> Risk-aware RVI; Learner (Model-Free) -> MLMC Q-learning or UBSR Q-learning
- **Critical path:**
  1. Verify the MDP satisfies the Doeblin-type condition (Assumption 2.12)
  2. Select the risk measure (e.g., UBSR, OCE) and verify it is "asymptotically coherent" (Assumption 4.2) if using MLMC
  3. If model-free, choose between the generic but sample-heavy MLMC approach or the specialized UBSR approach
- **Design tradeoffs:**
  - **MLMC vs. UBSR Q-learning:** MLMC is theoretically proven for a broader class of risk measures but requires a "simulator" or the ability to reset and resample transitions (off-policy is difficult). UBSR Q-learning is standard off-policy (like standard Q-learning) and highly sample efficient but is currently theoretically limited to UBSR and lacks a rigorous convergence proof
- **Failure signatures:**
  - **Infinite Variance (MLMC):** If the parameter $r$ in the geometric distribution is $< 0.5$, the expected number of samples per iteration is infinite, potentially causing memory issues or extreme variance
  - **Non-Convergence (UBSR):** Theoretical instability of the ODE; empirical trajectories might show oscillations or divergence if the step size $\gamma(n)$ decays too slowly or the loss function is non-convex
  - **Constraint Violation:** Using a measure like pure CVaR (which has a non-strict slope at 0) violates the "bounded slope" assumption (4.9), potentially breaking the Doeblin condition
- **First 3 experiments:**
  1. **Benchmark RVI:** Implement `Algorithm 1` on a known average-reward MDP (e.g., Gridworld) to verify baseline convergence against risk-neutral RVI
  2. **MLMC Tuning:** Implement `Algorithm 3` and vary the geometric parameter $r$ (e.g., 0.3 to 0.8) to plot the trade-off between sample complexity and variance
  3. **Algorithm Comparison:** Compare `Algorithm 3` (MLMC) vs. `Algorithm 4` (UBSR) on a simple machine maintenance task to validate the paper's claim that UBSR Q-learning has faster convergence and lower variance

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the off-policy Q-learning algorithm for utility-based shortfall risk (UBSR) converge almost surely to the optimal policy?
- **Basis in paper:** [explicit] The paper states, "While the theoretical convergence remains open, we validate it empirically" (Section 4.3) and explicitly lists "the almost sure convergence of UBSR Q-learning should be addressed" as future research (Section 6)
- **Why unresolved:** The standard ODE analysis for this specific update rule leads to a high-dimensional, nonlinear system that is difficult to analyze for stability using existing techniques like Lemma B.10
- **What evidence would resolve it:** A rigorous proof demonstrating the existence of a unique globally asymptotically stable equilibrium for the associated ODE, or a formal counter-example demonstrating divergence

### Open Question 2
- **Question:** What are the finite-sample complexity guarantees for the Multi-Level Monte Carlo (MLMC) Q-learning algorithm?
- **Basis in paper:** [explicit] The authors identify "establishing finite-sample guarantees for MLMC Q-learning remains an open problem" in the conclusion (Section 6)
- **Why unresolved:** The current work proves asymptotic convergence but does not bound the estimation error or sample complexity relative to the geometric parameter $r$
- **What evidence would resolve it:** A theoretical derivation providing a high-probability bound for $||Q_n - Q^*||$ after $n$ samples, potentially leveraging conditions from distributionally robust MDP literature

### Open Question 3
- **Question:** Can the strong ergodicity assumption (Assumption 4.8) be relaxed for the proposed planning and learning algorithms?
- **Basis in paper:** [explicit] The conclusion notes, "First, we conjecture that the strong ergodicity Assumption 4.8 could be weakened" (Section 6)
- **Why unresolved:** The current convergence proofs rely on Assumption 4.8 to ensure the risk maps satisfy the Doeblin-type condition (Assumption 2.12), which is stronger than the standard unichain assumption used in risk-neutral settings
- **What evidence would resolve it:** A modified proof of convergence (Theorem 4.5) that holds under weaker conditions, such as standard irreducibility or aperiodicity, without requiring a positive probability of transitioning to a specific state $\bar{x}$ from all state-action pairs

## Limitations
- Strong ergodicity assumptions required for theoretical guarantees may not hold in all practical MDPs
- MLMC Q-learning can be computationally expensive due to geometric sampling requirements
- UBSR Q-learning lacks theoretical convergence proof despite strong empirical performance

## Confidence

**Confidence Levels:**
- **High Confidence:** Convergence proofs for RVI algorithm and MLMC Q-learning (Theorems 3.2 and 4.7)
- **Medium Confidence:** Empirical convergence of UBSR Q-learning and its performance advantages
- **Medium Confidence:** Risk measure assumptions (asymptotic coherence, Hölder continuity) are practically verifiable

## Next Checks
1. **Convergence robustness test:** Implement RVI on a non-unichain MDP to verify whether the algorithm fails gracefully or produces incorrect results when ergodicity assumptions are violated
2. **MLMC parameter sensitivity analysis:** Systematically vary the geometric parameter r across the theoretical bound (0.49) and empirical choice (0.6) to quantify the trade-off between computational cost and convergence stability
3. **UBSR theoretical gap investigation:** Construct a simple MDP where UBSR Q-learning exhibits oscillatory behavior, and analyze whether the lack of convergence proof manifests in pathological cases beyond the empirical evaluation