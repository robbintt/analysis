---
ver: rpa2
title: 'Quantifying True Robustness: Synonymity-Weighted Similarity for Trustworthy
  XAI Evaluation'
arxiv_id: '2501.01516'
source_url: https://arxiv.org/abs/2501.01516
tags:
- similarity
- synonymity
- weighting
- measures
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the issue of robustness evaluation in Explainable
  AI (XAI) by addressing the inadequacy of standard similarity measures that treat
  all word perturbations equally, ignoring synonymity. The core method introduces
  synonymity-weighted similarity, modifying common information retrieval metrics like
  Jaccard, Kendall's Tau, Spearman's footrule, and Rank-biased Overlap (RBO) by incorporating
  semantic similarity between perturbed words using word embeddings.
---

# Quantifying True Robustness: Synonymity-Weighted Similarity for Trustworthy XAI Evaluation

## Quick Facts
- arXiv ID: 2501.01516
- Source URL: https://arxiv.org/abs/2501.01516
- Reference count: 9
- Primary result: Synonymity-weighted similarity measures substantially reduce reported attack success rates in XAI robustness evaluation, with Jaccard dropping from 54% to 1% and Spearman from 64% to 8% on benchmark datasets.

## Executive Summary
This paper addresses a critical flaw in XAI robustness evaluation where standard similarity measures treat all word perturbations equally, failing to account for semantic synonymity. The authors introduce synonymity-weighted similarity metrics that incorporate semantic similarity between perturbed words, modifying standard information retrieval measures like Jaccard, Kendall's Tau, Spearman's footrule, and Rank-biased Overlap. The primary result shows that applying this weighting dramatically reduces reported attack success rates for sensitive measures (Jaccard and Spearman), providing a more accurate assessment of true system robustness. The method was validated across three different synonymity estimation approaches and two datasets, demonstrating consistent improvements and general applicability.

## Method Summary
The method extends standard similarity measures by incorporating synonymity weighting between words in original and perturbed explanations. For each pair of corresponding words (based on the perturbation process), a synonymity score between 0 and 1 is computed using either word embeddings (GloVe, fastText) or thesaurus membership (WordNet). This score is then integrated into the similarity calculation for each measure according to specific formulas. The approach was applied post-hoc to adversarial examples generated using the Burger et al. (2023) attack algorithm across two datasets (GB and S2D) and evaluated using multiple thresholds. The evaluation compared attack success rates before and after applying synonymity weighting to quantify the reduction in false-positive robustness failures.

## Key Results
- Jaccard success rates dropped from 54% to 1% on GB dataset and from 64% to 8% on S2D dataset when synonymity weighting was applied
- Spearman's footrule showed similar dramatic reductions (from 64% to 8% on S2D)
- Kendall's Tau remained highly sensitive with success rates staying at 88% even after weighting
- Results were consistent across three synonymity estimation methods (GloVe, fastText, WordNet)
- RBO showed minimal change regardless of weighting, suggesting inherent robustness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Standard similarity measures overestimate adversarial attack success because they treat all word perturbations as equivalent, ignoring that synonym substitutions preserve semantic meaning.
- Mechanism: Synonymity weighting modifies similarity calculations by incorporating a semantic similarity function `Syn(a,b) → [0,1]` between mapped word pairs from original and perturbed explanations. For highly synonymous replacements, the penalty for difference is reduced proportionally.
- Core assumption: The perturbation process replaces words with semantically close synonyms, creating a one-to-one mapping between original and perturbed tokens that can be exploited for weighted comparison.
- Evidence anchors:
  - [abstract] "We argue these measures are poorly suited in the evaluation of trustworthiness, as they treat all word perturbations equally while ignoring synonymity, which can misrepresent an attack's true impact."
  - [section 4] "We consider two intuitive possibilities, one where the synonymity estimate is within an interval and the second where synonymity is a dichotomy."
  - [corpus] Related work "Towards Robust and Accurate Stability Estimation" examines LIME vulnerability but does not address the measurement artifact problem this paper targets.
- Break condition: If perturbations involve multi-word substitutions or substantial meaning changes, the implicit mapping assumption fails and weighting becomes unreliable.

### Mechanism 2
- Claim: Different similarity measures respond differently to synonymity weighting based on their structural properties.
- Mechanism: Set-intersection measures (Jaccard, RBO) show large improvements because weighting effectively increases the "soft intersection" size. Rank-order measures (Kendall's Tau) show minimal improvement because small position swaps still register as full inversions regardless of word similarity.
- Core assumption: The relationship between measure structure and weighting effectiveness is consistent across text domains and explanation types.
- Evidence anchors:
  - [section 6] "Jaccard & Spearman: Immediately seen is the drastic reduction in the attack success rate for the Jaccard index and Spearman's footrule... Kendall's Tau also appears to gain less from synonymity weighting."
  - [table 3] Jaccard success rates dropped from 54% to 1% on GB dataset; Kendall remained at 88% even after weighting.
  - [corpus] Related papers on adversarial XAI do not systematically compare multiple similarity measures.
- Break condition: If explanation rankings are highly unstable (frequent position swaps among top features), even weighted rank measures may remain too sensitive.

### Mechanism 3
- Claim: The choice of synonymity estimation method (embeddings vs. thesaurus) produces consistent qualitative conclusions about measure sensitivity.
- Mechanism: GloVe, fastText, and WordNet-based similarity functions were tested. All three showed that Jaccard and Spearman benefit substantially from weighting while Kendall's Tau does not, confirming the finding is not an artifact of a specific embedding.
- Core assumption: The tested synonymity measures adequately capture semantic closeness for the perturbation types used in adversarial XAI.
- Evidence anchors:
  - [section 6.1] "There is little difference between the original GloVe embedding used and fastText... The thesaurus-based option using WordNet showed more difference as expected due to its dichotomous construction."
  - [table 5-6] All three methods show similar patterns: substantial reduction for Jaccard/Spearman, minimal for Kendall.
  - [corpus] No corpus papers validated synonymity-weighted measures across multiple embedding types.
- Break condition: If domain-specific vocabulary or technical terms are poorly represented in pre-trained embeddings, thesaurus or domain-adapted embeddings may be required.

## Foundational Learning

- Concept: **Information Retrieval Similarity Measures (Jaccard, Kendall's Tau, Spearman's footrule, RBO)**
  - Why needed here: These are the standard metrics used to compare ranked explanations in adversarial XAI. Understanding how they calculate similarity/distance is essential to see why they fail and how weighting fixes them.
  - Quick check question: Given two ranked lists `A=[a,b,c]` and `B=[a,c,b]`, which measures would show high similarity and which would show low similarity?

- Concept: **Word Embeddings and Semantic Similarity**
  - Why needed here: The synonymity weighting mechanism relies on embedding-based cosine similarity or thesaurus lookups to quantify how close two words are semantically.
  - Quick check question: Why might "good" and "great" have higher embedding similarity than "good" and "frog," and how would this affect the weighted Jaccard calculation?

- Concept: **Adversarial XAI Attack Process**
  - Why needed here: The paper evaluates attacks that perturb input text to change explanations while preserving model output. Understanding this process clarifies why the mapping between original and perturbed words exists.
  - Quick check question: In an adversarial attack that replaces "worried" with "alarmed," what happens to the explanation ranking and how would standard vs. weighted Jaccard differ in scoring this?

## Architecture Onboarding

- Component map: Explanation generation (LIME) -> Mapping construction -> Synonymity computation -> Weighted similarity calculation -> Success/failure classification

- Critical path: Explanation generation (LIME) → Mapping construction → Synonymity computation → Weighted similarity calculation → Success/failure classification

- Design tradeoffs:
  - **Interval vs. dichotomous synonymity**: Interval [0,1] preserves gradations but introduces subjectivity; dichotomous (thesaurus) is cleaner but loses nuance.
  - **Post-hoc vs. integrated weighting**: Paper applies weighting after attack completes (reducing computation). Integration into search would be more accurate but requires substantial re-computation.
  - **Penalty value selection**: For measures handling disjoint elements, penalty `p` affects sensitivity. Paper uses `k²` for Spearman based on prior work.

- Failure signatures:
  - **Near-100% attack success even after weighting**: Likely using Kendall's Tau on unstable explanations (measure inherently too sensitive).
  - **Large discrepancy between embedding types**: May indicate domain vocabulary is poorly covered by general-purpose embeddings.
  - **Weighted similarity lower than expected**: Check if perturbation process violated single-word replacement assumption, breaking the mapping.

- First 3 experiments:
  1. **Baseline replication**: Apply standard Jaccard, Kendall, Spearman, and RBO (p=0.5, 0.7, 0.9) to existing adversarial examples from the paper's GitHub repository. Verify you reproduce Table 3 success rates before any weighting.
  2. **Single-measure weighting implementation**: Implement synonymity-weighted Jaccard (Eq. 3) using GloVe-Twitter-25 embeddings. Apply to the same examples and confirm attack success rates drop substantially (GB: ~54% → ~1%).
  3. **Cross-embedding validation**: Replace GloVe with fastText embeddings in your weighted Jaccard implementation. Confirm results are within 1-2% of GloVe results, demonstrating embedding-choice robustness.

## Open Questions the Paper Calls Out
None

## Limitations
- The method depends critically on the assumption that adversarial perturbations replace individual words with single-word synonyms, which may not hold for all attack types.
- The effectiveness depends on the quality of pre-trained embeddings for domain-specific vocabulary, which may not capture technical terms adequately.
- The choice of penalty values for disjoint elements introduces some arbitrariness, though the paper reports minimal sensitivity to this parameter.

## Confidence
- **High confidence**: The core empirical finding that synonymity-weighted Jaccard and Spearman substantially reduce attack success rates compared to standard measures is well-supported by consistent results across two datasets and three synonymity estimation methods.
- **Medium confidence**: The characterization of Kendall's Tau as inherently too sensitive to small position changes is reasonable but may depend on the stability of the underlying explanations and the specific attack parameters used.
- **Medium confidence**: The generalizability of results across different embedding types is supported but limited by the use of only two embedding approaches and one thesaurus method.

## Next Checks
1. **Multi-word perturbation validation**: Test the synonymity-weighted measures on adversarial examples that employ phrase-level substitutions or multi-word paraphrases to verify the method's limitations when the single-word replacement assumption is violated.

2. **Domain adaptation test**: Apply the synonymity-weighted measures to a technical or domain-specific text classification task (e.g., medical or legal text) where general-purpose embeddings may perform poorly, and compare results with domain-adapted embeddings.

3. **Attack integration experiment**: Modify the Burger et al. (2023) attack algorithm to incorporate synonymity weighting during the search process rather than applying it post-hoc, and measure whether this changes the nature or success rates of generated attacks.