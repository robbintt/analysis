---
ver: rpa2
title: 'NepaliGPT: A Generative Language Model for the Nepali Language'
arxiv_id: '2506.16399'
source_url: https://arxiv.org/abs/2506.16399
tags:
- nepali
- corpus
- language
- data
- nepaligpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NepaliGPT, the first generative large language
  model for the Nepali language. The authors address the lack of NLP resources for
  Nepali by creating a new corpus called Devanagari Corpus, collected from news portals,
  existing datasets, and translated English sources.
---

# NepaliGPT: A Generative Language Model for the Nepali Language

## Quick Facts
- arXiv ID: 2506.16399
- Source URL: https://arxiv.org/abs/2506.16399
- Authors: Shushanta Pudasaini; Aman Shakya; Siddhartha Shrestha; Sahil Bhatta; Sunil Thapa; Sushmita Palikhe
- Reference count: 13
- Primary result: First generative large language model for Nepali language, achieving perplexity of 26.32245 and ROUGE-1 score of 0.2604

## Executive Summary
NepaliGPT addresses the lack of generative NLP resources for the Nepali language by introducing a custom-built large language model based on GPT-2 architecture. The authors collected a 9.6 GB corpus from news portals, existing datasets, and translated English sources, then developed a custom BPE tokenizer trained specifically on Nepali text. After pretraining on 383M Nepali tokens, the model achieves competitive performance metrics and demonstrates the ability to generate coherent Nepali text and answer questions through fine-tuning on a newly created QA dataset.

## Method Summary
The approach involves three main stages: corpus collection and preprocessing from diverse Nepali sources, custom BPE tokenizer training (10,000 vocab), and GPT-2 pretraining on the Devanagari Corpus followed by fine-tuning on question-answer pairs. The model uses causal language modeling with a learning rate of 5e-5 for pretraining and 0.01 for fine-tuning, trained on NVIDIA Tesla T4 hardware. The QA dataset contains 4,296 pairs combining scraped Nepali sources and translated English content.

## Key Results
- Perplexity of 26.32245 on test set after pretraining
- ROUGE-1 score of 0.2604 on QA task
- Human evaluation shows 81.25% causal coherence and 85.41% causal consistency
- First generative model demonstrating Nepali text generation capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A custom BPE tokenizer trained on Nepali-specific corpus reduces vocabulary fragmentation and improves subword representation quality.
- Mechanism: The tokenizer learns the 10,000 most common consecutive character sequences from the Devanagari Corpus, merging frequently co-occurring Nepali characters into meaningful subword units rather than relying on generic multilingual tokenizers that may over-segment Nepali text.
- Core assumption: Nepali's complex morphology (36 consonants, 13 vowels, half-alphabets, and multiple valid spellings for the same word) benefits from language-specific subword optimization rather than character-level or borrowed tokenizers.
- Evidence anchors:
  - [section 3.2.1]: "We trained a custom BPE tokeniser utilising the collected Nepali Corpus from scratch using Sentencepiece Library... collected the 10,000 most common consecutive characters or subwords"
  - [section 1]: "As the same word can be written in different ways in the Nepali language, and there are multiple numbers of alphabets compared to the English language. This increases the vocabulary size which makes the Nepali language complicated for NLP tasks."
  - [corpus]: Related paper "Towards Nepali-language LLMs: Efficient GPT training with a Nepali BPE tokenizer" confirms tokenizer design is an active research area for Nepali, though citation count is low (0 citations).
- Break condition: If generic multilingual tokenizers (e.g., GPT-2's original BPE) achieve comparable perplexity on Nepali text, the custom tokenizer's contribution diminishes; this comparison is not provided in the paper.

### Mechanism 2
- Claim: Causal Language Modeling (CLM) pretraining on large Nepali corpus enables generative capabilities that encoder-only models (BERT-style) cannot provide.
- Mechanism: The GPT-2 architecture learns autoregressive next-token prediction on 383M Nepali tokens, building internal representations of Nepali syntax, semantics, and discourse patterns that can be sampled for text generation—unlike masked language models that only learn to fill gaps.
- Core assumption: The corpus quality and diversity (news portals, translated datasets, existing corpora) are sufficient to capture productive Nepali language patterns without severe domain bias.
- Evidence anchors:
  - [abstract]: "NepaliGPT achieves... Perplexity of 26.32245... causal coherence was 81.25%, and causal consistency was 85.41%"
  - [section 2]: "There are few language models that has been developed for the Nepali Corpus major, such as NepaliBERT, and NepBERTa... they are more of Natural Language Understanding (NLU) models i.e. cannot generate Nepali tokens to communicate"
  - [corpus]: Weak corpus evidence—no direct comparisons with encoder-only models on generation tasks; related papers focus on NER and other discriminative tasks.
- Break condition: If the training corpus contains significant noise, translation artifacts, or domain skew, the learned representations may not generalize to diverse Nepali use cases; manual verification was performed but systematic quality metrics are not reported.

### Mechanism 3
- Claim: Two-stage training (pretraining → fine-tuning on QA pairs) transfers general language understanding to task-specific generation.
- Mechanism: After pretraining on the Devanagari Corpus, fine-tuning on 4,296 question-answer pairs updates all model weights to learn the response generation pattern, conditioning the model on query-answer structure rather than just continuation.
- Core assumption: The QA dataset quality (scraped Nepali GK quizzes + machine-translated English QA with manual verification) provides accurate signal for the response generation task.
- Evidence anchors:
  - [section 3.4]: "We updated the entire weights during this fine-tuning step, making sure the model learns to answer the user's query"
  - [section 5.2]: "ROUGE-1 score of 0.2604... causal coherence was 81.25%, and causal consistency was 85.41%"
  - [corpus]: No comparable QA benchmarks exist for Nepali; this is the first such dataset according to the paper.
- Break condition: If the QA pairs contain factual errors, translation mistakes, or inconsistent formatting, the model may learn incorrect patterns; the paper mentions manual verification but does not quantify inter-annotator agreement or error rates.

## Foundational Learning

- Concept: Byte Pair Encoding (BPE) Tokenization
  - Why needed here: Nepali's Devanagari script and morphological complexity require subword tokenization that balances vocabulary size with sequence length; a custom BPE tokenizer captures language-specific patterns.
  - Quick check question: Can you explain why a tokenizer trained on English text would likely over-segment Nepali words, and how this affects model efficiency?

- Concept: Causal Language Modeling vs. Masked Language Modeling
  - Why needed here: The paper explicitly contrasts its generative approach with prior Nepali BERT-style models; understanding this distinction is essential for appreciating why GPT-2 was chosen.
  - Quick check question: If you wanted to build a Nepali text classifier, would CLM or MLM be more appropriate? What about a chatbot?

- Concept: Perplexity and ROUGE Metrics
  - Why needed here: The paper reports perplexity (26.32245) and ROUGE scores (0.2604) as primary evaluation metrics; interpreting these values requires understanding what they measure and their limitations.
  - Quick check question: A lower perplexity indicates what property of the model? What does a ROUGE-1 score of 0.26 suggest about the overlap between generated and reference text?

## Architecture Onboarding

- Component map:
  Data Collection → Preprocessing → Custom BPE Tokenizer (10k vocab)
                                          ↓
                                GPT-2 Pretraining (9.6 GB corpus)
                                          ↓
                                QA Dataset Construction (4,296 pairs)
                                          ↓
                                Fine-tuning (10 epochs, lr=0.01)
                                          ↓
                                Evaluation (Perplexity, ROUGE, Human)

- Critical path:
  1. Corpus quality directly determines tokenizer quality and pretraining effectiveness
  2. Tokenizer vocabulary size (10,000) affects both representation capacity and inference speed
  3. Fine-tuning learning rate (0.01) is unusually high for transformer fine-tuning and may require careful monitoring

- Design tradeoffs:
  - **Corpus size vs. quality**: Larger corpus (9.6 GB) improved perplexity from 87.2 to 26.3, but mixed-source data (scraped news + translated English) may introduce style inconsistency
  - **Vocabulary size**: 10,000 tokens balances coverage and model size; larger vocab may improve rare word handling but increases embedding matrix
  - **Human evaluation cost**: Three annotators with different profiles provide diversity but small sample (50 data points) limits statistical reliability

- Failure signatures:
  - **High perplexity after pretraining**: Indicates insufficient corpus size, poor tokenization, or hyperparameter issues; compare Table 6 (105.6 → 87.2) vs Table 7 (37.7 → 26.3)
  - **Low ROUGE scores with high human ratings**: May indicate evaluation metric mismatch rather than poor generation quality
  - **Incoherent outputs on out-of-domain queries**: Fine-tuning on GK questions may overfit to that domain; test with diverse prompts

- First 3 experiments:
  1. **Tokenizer ablation**: Compare custom Nepali BPE (10k vocab) against GPT-2's original tokenizer on the same corpus; measure vocabulary coverage and average sequence length
  2. **Corpus scaling**: Train identical architectures on the 3.2 GB subset vs. full 9.6 GB corpus; plot perplexity curves to validate scaling behavior
  3. **Fine-tuning domain transfer**: After QA fine-tuning, evaluate on held-out domains (e.g., news continuation, creative writing) to assess overfitting to GK-style responses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of a native Nepali generative model compare to approaches that utilize translation pipelines with existing English models?
- Basis in paper: [explicit] The conclusion states that "evaluation of different datasets and approaches like translation+English models might provide more insights into the model’s capability."
- Why unresolved: The authors focused on building a native model (NepaliGPT) and did not provide a comparative baseline against translation-based methods (e.g., translating Nepali queries to English, processing, and translating back).
- What evidence would resolve it: A comparative study benchmarking NepaliGPT against a pipeline model (e.g., GPT-4 with translation) on the proposed NepaliQA dataset using metrics like ROUGE and human evaluation.

### Open Question 2
- Question: Can NepaliGPT be effectively adapted into an instruction-following model through fine-tuning on specific instruction-based datasets?
- Basis in paper: [explicit] The authors note in the Future Work section that "finetuning on instruction-based datasets might lead to an instruction-following model."
- Why unresolved: The current model is fine-tuned primarily on question-answer pairs for specific queries, but its ability to follow complex, open-ended instructions has not been tested or trained.
- What evidence would resolve it: Creating a Nepali instruction dataset, fine-tuning the model, and evaluating its performance on instruction-following tasks (similar to Alpaca or FLAN evaluations).

### Open Question 3
- Question: How does expanding the training corpus to include diverse non-news domains (e.g., literature, technical manuals) affect the model's grasp of specialized terminology?
- Basis in paper: [explicit] The paper suggests "usages of bigger and more diverse datasets, including text from different sectors so that the model has more insights on different terminologies."
- Why unresolved: The current Devanagari Corpus relies heavily on scraped news portals, potentially limiting the model's effectiveness in technical or creative writing domains.
- What evidence would resolve it: Pre-training a new version of the model with added data from diverse sectors and testing for perplexity and semantic accuracy in those specific domains.

### Open Question 4
- Question: Does the limited vocabulary size of 10,000 tokens in the custom BPE tokenizer restrict the model's ability to handle the high morphological complexity of the Nepali language?
- Basis in paper: [inferred] The authors acknowledge Nepali has a large vocabulary due to "multiple numbers of alphabets" and different word writings (Section 1), yet they trained a custom tokenizer with only 10,000 tokens (Section 3.2.1).
- Why unresolved: The paper does not ablate different vocabulary sizes or compare the custom tokenizer against standard larger vocabularies (e.g., 32k or 50k tokens) to see if 10k is sufficient for generative tasks.
- What evidence would resolve it: An ablation study comparing the tokenization efficiency (tokens per word) and downstream generation quality between the 10k tokenizer and larger vocabulary variants.

## Limitations

- Corpus quality concerns from mixed sources (news portals, translated English) may introduce domain bias and translation artifacts
- Evaluation methodology relies on custom metrics without comparative baselines or established benchmarks
- QA dataset construction combines scraped Nepali sources with translated English pairs, introducing potential quality issues
- Fine-tuning learning rate (0.01) is unusually high for transformer fine-tuning and may indicate optimization challenges

## Confidence

- **High confidence**: The core claim that NepaliGPT can generate Nepali text and answer questions is supported by multiple evaluation metrics and human assessments
- **Medium confidence**: The claim that NepaliGPT achieves state-of-the-art performance for Nepali generative tasks is limited by the absence of comparative baselines
- **Low confidence**: The assertion that NepaliGPT fills a "critical gap" in Nepali NLP resources is overstated given the limited scope of evaluation

## Next Checks

1. **Comparative Baseline Evaluation**: Implement and evaluate a NepaliGPT model using GPT-2's original tokenizer against the custom Nepali BPE tokenizer on identical pretraining data, measuring both perplexity and generation quality on held-out Nepali text.

2. **Cross-Domain Generation Testing**: After QA fine-tuning, evaluate NepaliGPT's generation capabilities on diverse Nepali text types (news article continuation, creative writing, dialogue) to assess whether the model overfits to the GK question-answering domain.

3. **Encoder-Only Model Comparison**: Train and evaluate a Nepali BERT-style model on the same QA task to determine whether the added complexity of generative modeling provides meaningful advantages over discriminative approaches for question-answering in low-resource languages like Nepali.