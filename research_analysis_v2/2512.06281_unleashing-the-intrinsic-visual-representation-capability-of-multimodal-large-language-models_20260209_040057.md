---
ver: rpa2
title: Unleashing the Intrinsic Visual Representation Capability of Multimodal Large
  Language Models
arxiv_id: '2512.06281'
source_url: https://arxiv.org/abs/2512.06281
tags:
- visual
- laver
- zhang
- vision
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses modality imbalance in multimodal large language
  models (MLLMs), where visual information is underutilized compared to textual representations,
  particularly in deeper layers. The authors propose Latent Visual Reconstruction
  (LaVer), a training framework that enhances visual representation learning through
  masked image modeling in the LLM's joint latent semantic space.
---

# Unleashing the Intrinsic Visual Representation Capability of Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2512.06281
- Source URL: https://arxiv.org/abs/2512.06281
- Authors: Hengzhuang Li; Xinsong Zhang; Qiming Peng; Bin Luo; Han Hu; Dengyang Jiang; Han-Jia Ye; Teng Zhang; Hai Jin
- Reference count: 40
- One-line primary result: Addresses modality imbalance in MLLMs by enhancing visual representation learning through masked image modeling in the LLM's joint latent semantic space.

## Executive Summary
This paper addresses the critical issue of modality imbalance in multimodal large language models (MLLMs), where visual information is progressively underutilized compared to textual representations, particularly in deeper layers. The authors propose Latent Visual Reconstruction (LaVer), a training framework that enhances visual representation learning through masked image modeling in the LLM's joint latent semantic space. Experiments across diverse benchmarks show LaVer consistently outperforms baselines, especially on dense visual tasks like OCR and vision-centric benchmarks, achieving 19.22% improvement on OCR-Bench while maintaining language performance.

## Method Summary
LaVer is a three-stage training framework that addresses modality imbalance in MLLMs through latent visual reconstruction. It uses a student-teacher framework where the model reconstructs masked vision tokens using an online teacher's visual embeddings, regularized by Clipped Gram-Anchoring to prevent feature inconsistency. The method applies mixed attention (bidirectional for vision, causal for text) with 2D-RoPE encoding to enable spatial context integration. Key hyperparameters include mask ratio 0.05 with cosine schedule, EMA decay 0.95 updated every 100 steps, and vision head with 3-layer MLP (8192 hidden dim).

## Key Results
- LaVer achieves 19.22% improvement on OCR-Bench and 6.72% on MMVP
- Demonstrates strong scalability across different model sizes and datasets
- Improves visual attention allocation and enhances complex visual reasoning capabilities
- Maintains competitive language performance (IFEval, MMLU, BBH) with <1% change

## Why This Works (Mechanism)

### Mechanism 1: Direct Visual Supervision via Latent Space Reconstruction
The predominant reliance on next-text-token-prediction during training fails to provide direct visual supervisory signals, causing progressive visual representation homogenization. LaVer addresses this by reconstructing masked vision tokens using teacher visual embeddings in the LLM's latent space.

### Mechanism 2: Clipped Gram-Anchoring Prevents Feature Collapse Shortcut
Naive MIM creates an exploitable shortcut where the model outputs nearly identical visual features. The Clipped Gram-Anchoring computes Gram matrices of student and teacher features, clipping differences to penalize only when student features become more homogeneous than teacher.

### Mechanism 3: Mixed Attention + 2D-RoPE Enable Spatial Context Integration
Visual reconstruction requires global spatial context that causal attention and 1D positional embeddings fundamentally impede. Bidirectional full attention for vision-vision interactions with 2D-RoPE encoding patch grid coordinates enables the model to aggregate information from neighboring regions for reconstruction.

## Foundational Learning

- **Concept: Progressive Visual Feature Homogenization**
  - Why needed here: The core diagnosis is that visual representations degrade through layers, not that they're absent initially
  - Quick check question: Given Figure 2d showing cosine similarity increasing from layers 4→28, what does this imply about how the LLM processes visual vs. textual information differently?

- **Concept: Student-Teacher Self-Distillation with EMA**
  - Why needed here: LaVer uses an online teacher rather than frozen targets or the student's own outputs
  - Quick check question: Why might an EMA teacher (updated slowly, not trained directly) produce more stable reconstruction targets than training a separate encoder?

- **Concept: Gram Matrix for Relational Feature Structure**
  - Why needed here: CGA operates on G(Z) = Norm(Z)·Norm(Z)^T, not individual features
  - Quick check question: If two vision tokens have identical embeddings, what happens to their row/column in the Gram matrix? How does clipping the difference detect this?

## Architecture Onboarding

**Component map:**
Vision Encoder (frozen) → Connector → LLM Backbone F_θ → Language Head + Vision Head V_ψ → Visual logits Z̃ → Compare with Teacher's Ẑ

**Critical path:**
1. Stage 2: Pack vision tokens from multiple images → apply mask (r=0.05, cosine schedule from 0.0)
2. Student forward: Masked tokens → LLM → Vision Head → predicted logits
3. Teacher forward: Unmasked tokens → EMA-copy of LLM → EMA-copy of Vision Head → target logits
4. Loss computation: L_MIM (cross-entropy at masked positions) + L_CGA (clipped Gram difference) + L_LM (standard language modeling)
5. Teacher update: Every 100 steps, Ẇ(t) = λ·Ẇ(t-1) + (1-λ)·W(t)

**Design tradeoffs:**
- Mask ratio (0.05 vs. 0.3): Lower ratios work with limited training (6K steps); higher ratios may need extensive pretraining
- EMA update frequency (100 steps vs. every step): Too frequent propagates student inconsistencies into teacher; 100 steps is robust
- Computational cost: 13-16% longer training, 14-26% more memory (teacher forward pass + packed sequences)

**Failure signatures:**
- Visual feature inconsistency: Training with L_MIM alone shows cosine similarity dropping then exceeding baseline (Figure 4b pattern)
- Performance collapse on general VQA: If CGA omitted, average score drops below baseline (Table 2c: 53.76% vs 55.72%)
- Language degradation: Monitor MMLU/IFEval; LaVer maintains parity (Table 14 shows ±0.5% change)

**First 3 experiments:**
1. Replicate Figure 2d diagnostic: Run baseline model on MMVP images, compute vision-token cosine similarity per layer; verify homogenization pattern before applying LaVer
2. Ablate CGA to observe collapse: Train with L_MIM only, log cosine similarity during training (should see early drop then reversal per Figure 4b); confirms CGA necessity
3. Validate on dense visual tasks: Evaluate OCR-Bench and MMVP (claim: +103 points OCR, +6.72% MMVP); these require preserved fine-grained visual information that homogenization destroys

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does LaVer demonstrate continued performance scaling when applied to datasets significantly larger than the 4M samples tested?
- Basis in paper: Supplementary material states that "Scaling to larger datasets remains an avenue for future investigation" and notes the full FineVision 23M dataset was not utilized due to computational constraints
- Why unresolved: The paper validates data scaling only up to 4M samples; the performance trajectory on the full 23M dataset remains unknown
- What evidence would resolve it: Training LaVer on the complete FineVision 23M dataset and evaluating if the observed linear improvements persist or saturate

### Open Question 2
- Question: Can the inferior performance of high masking ratios (e.g., 0.3) be recovered through extended training schedules?
- Basis in paper: The paper hypothesizes that high ratios failed because they "may require longer training process to converge," but the ablation study maintained a fixed iteration count
- Why unresolved: It is undetermined if high masking ratios are fundamentally flawed for this architecture or simply under-trained in the current setup
- What evidence would resolve it: A comparative ablation study where models with high masking ratios are trained for 2x-3x the standard duration to check for delayed convergence

### Open Question 3
- Question: Can the Latent Visual Reconstruction framework be effectively adapted for temporal domains such as video understanding?
- Basis in paper: The method is built upon Masked Image Modeling (MIM) exploiting "spatial redundancy," leaving the application to temporal redundancy in video unexplored
- Why unresolved: While the architecture is generic, the specific masking strategy and Clipped Gram-Anchoring loss are designed and validated exclusively for static image patches
- What evidence would resolve it: Applying LaVer to a Video-LLM (e.g., LLaVA-OneVision video models) and evaluating performance on video-based QA benchmarks

## Limitations
- Limited ablation studies with no external validation of the Clipped Gram-Anchoring technique
- Underspecified architectural details for 2D-RoPE and packed sequence handling
- Performance claims depend on relatively small evaluation set without validation on larger-scale datasets
- Mixed attention benefits show incremental improvements without comparative studies with alternatives

## Confidence

- **High Confidence:** The experimental results showing consistent performance improvements across 17 benchmarks, particularly the large gains on OCR-Bench (+19.22%) and MMVP (+6.72%). The three-stage training methodology and overall implementation framework appear well-specified.
- **Medium Confidence:** The core claim that progressive visual representation homogenization causes modality imbalance, supported by the diagnostic visualization but lacking direct ablation validation. The necessity of Clipped Gram-Anchoring is well-supported by the MIM-only degradation but the technique itself lacks external validation.
- **Low Confidence:** The mechanism-specific claims regarding Clipped Gram-Anchoring (no corpus validation), the architectural necessity of mixed attention and 2D-RoPE (no comparative studies with alternatives), and the scalability claims across different model sizes without systematic experiments.

## Next Checks

1. **Ablation on Progressive Homogenization:** Train a baseline model without any LaVer components and monitor vision-token cosine similarity through layers to confirm the homogenization pattern exists independently of LaVer implementation details.

2. **External Validation of Clipped Gram-Anchoring:** Implement the CGA technique in a simpler MIM setup (e.g., BERT pretraining on images) to verify that asymmetric Gram-anchoring is necessary and sufficient for preventing feature collapse in latent space reconstruction tasks.

3. **Scalability Stress Test:** Evaluate LaVer across at least three different model scales (7B, 13B, 33B) and training dataset sizes (200K, 800K, 4M) to rigorously test the claimed "consistently superior" performance and identify breaking points where the method degrades.