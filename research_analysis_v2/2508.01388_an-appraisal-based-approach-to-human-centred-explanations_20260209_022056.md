---
ver: rpa2
title: An Appraisal-Based Approach to Human-Centred Explanations
arxiv_id: '2508.01388'
source_url: https://arxiv.org/abs/2508.01388
tags:
- appraisal
- explanations
- user
- event
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a cognitive framework for generating human-centred
  explanations in AI systems, leveraging the appraisal component of the Component
  Process Model (CPM). The framework uses appraisal dimensions such as predictability,
  relevance, valence, urgency, agency, and normative significance to create context-aware
  explanations that align with human reasoning.
---

# An Appraisal-Based Approach to Human-Centred Explanations

## Quick Facts
- **arXiv ID:** 2508.01388
- **Source URL:** https://arxiv.org/abs/2508.01388
- **Reference count:** 36
- **Primary result:** Introduces a cognitive framework using CPM appraisal dimensions to generate emotionally resonant, context-aware AI explanations that improve transparency and trust over baseline methods.

## Executive Summary
This paper presents a novel framework for generating human-centred explanations in AI systems by leveraging the appraisal component of the Component Process Model (CPM). The framework maps AI decisions to six appraisal dimensions—predictability, relevance, valence, urgency, agency, and normative significance—derived from the CoreGRID questionnaire. Through a meal recommendation case study, the approach demonstrates how appraisal-based reasoning can produce explanations that are cognitively accessible and emotionally aligned with user needs. The framework addresses limitations in traditional XAI methods by integrating cognitive and emotional factors, offering a novel direction for developing explainable AI systems.

## Method Summary
The framework implements a multi-stage pipeline: user context is built from profile and query, classified to appraisal dimensions using BART-large-MNLI NLI model, candidate items are scored against prioritized appraisals using custom scoring functions, the top item is selected, and an explanation is generated via GPT-4o explicitly naming the appraisal factors. The approach is demonstrated via a meal recommendation case study comparing appraisal-conditioned explanations against a baseline without appraisal conditioning.

## Key Results
- Appraisal-conditioned explanations are more personalized and cognitively aligned with user needs than baseline methods
- Dynamic appraisal prioritization enables context-aware explanation selection that adapts to situational user requirements
- Explicitly surfacing the reasoning chain linking decisions to appraisal factors enhances perceived transparency and trust

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structuring explanations around appraisal dimensions produces more cognitively accessible justifications than feature-importance methods.
- Mechanism: The framework treats appraisals as "explanation primitives" (Page 3), mapping AI decisions to six appraisal dimensions—predictability, goal relevance, valence, urgency, agency, and normative significance—derived from the CoreGRID questionnaire's 21 items. Instead of post-hoc technical justifications, the system explains decisions using cognitive evaluations that mirror how humans assess events.
- Core assumption: Humans naturally evaluate events through structured appraisal dimensions, so explanations framed in these terms require less cognitive translation.
- Evidence anchors:
  - [abstract]: "By structuring explanations around key appraisal dimensions—such as relevance, implications, coping potential, and normative significance—our framework provides context-sensitive, cognitively meaningful justifications for AI decisions."
  - [section]: Page 3 describes appraisals as "cognitively grounded building blocks that allow autonomous systems to communicate their reasoning in terms familiar to human users."
  - [corpus]: Related work PHAX (arXiv:2507.22009) similarly argues that "explanations that are clear, contextual, and socially accountable" are needed beyond feature attribution, supporting the direction but not validating the specific appraisal mechanism.
- Break condition: If users do not find appraisal-framed language intuitive (e.g., "this aligns with your goal relevance"), personalization gains may reverse. The paper acknowledges this risk, noting "the subjectivity of appraisal-based explanations requires validation across diverse user groups" (Page 7).

### Mechanism 2
- Claim: Dynamic appraisal prioritization enables context-aware explanation selection that adapts to situational user needs.
- Mechanism: A natural language inference model (BART-large-MNLI) compares the composite user input against statements representing each appraisal dimension, returning normalized confidence scores. These scores weight recipe selection, ensuring the final recommendation and its explanation address the most salient concerns (e.g., Sarah's urgency vs. Alex's novelty-seeking).
- Core assumption: User queries and profiles contain sufficient signal to infer which appraisal dimensions are currently most important.
- Evidence anchors:
  - [section]: Page 5 describes using BART NLI to "assess which appraisal dimensions are most salient in the current context" with confidence scores that are "normalised and used to prioritise the appraisal dimensions."
  - [section]: Sarah's case study (Figure 3) shows high urgency and goal relevance driving the 5-minute Greek Yogurt Parfait recommendation, while Alex's case (Figure 6) prioritizes normative significance and predictability for Sushi Rolls.
  - [corpus]: Weak direct validation. Neighbor papers focus on argumentation frameworks and justifications but do not test dynamic appraisal prioritization.
- Break condition: If user queries are ambiguous or low-information, NLI-based classification may misidentify salient dimensions, producing misaligned explanations. The paper uses simplified scoring methods and acknowledges this as a limitation (Page 7).

### Mechanism 3
- Claim: Explicitly surfacing the reasoning chain—linking decisions to appraisal factors—enhances perceived transparency and trust.
- Mechanism: The GPT-4o explanation generator is prompted with the scenario, dominant appraisals, and selected item, explicitly naming the appraisal factors in the output (e.g., "Since you're in a hurry tonight... [Urgency]. ...supporting your goal to eat healthier [Goal Relevance]"). This makes the why behind the recommendation inspectable.
- Core assumption: Users value and engage with explicit reasoning traces rather than just the final recommendation.
- Evidence anchors:
  - [section]: Figure 3 shows the explanation explicitly labeling appraisal dimensions in brackets, demonstrating the "transparent justification" mechanism.
  - [section]: Page 6 notes the comparison with baseline highlights "enhanced interpretability and emotional alignment achieved by integrating CPM-based appraisals."
  - [corpus]: Human-Centered Explainability in AI-Enhanced UI Security (arXiv:2601.22653) emphasizes that "the degree to which users can understand" system reasoning affects effectiveness, aligning conceptually but not providing direct evidence for this framework.
- Break condition: If explicit reasoning increases cognitive load without corresponding trust gains—particularly for expert users who prefer concise outputs—the mechanism may backfire. No user study is presented; this remains unvalidated.

## Foundational Learning

- Concept: Component Process Model (CPM) and its appraisal component
  - Why needed here: The entire framework is built on CPM's appraisal component as a proxy for human-like evaluation. You must understand that appraisals are cognitive evaluations of events across dimensions like relevance, implications, coping potential, and normative significance—not emotions themselves, but the precursors to emotional responses.
  - Quick check question: Can you distinguish between "valence appraisal" (positive/negative evaluation) and "emotional feeling" (the resulting affective state)?

- Concept: CoreGRID appraisal questionnaire structure
  - Why needed here: The framework operationalizes 21 CoreGRID items grouped into six dimensions. Understanding this mapping is essential for implementing the scoring functions (e.g., which items belong to "Agency" vs. "Normative Significance").
  - Quick check question: Which CoreGRID items would you group under "Agency" versus "Urgency"?

- Concept: Natural Language Inference (NLI) for classification
  - Why needed here: The framework uses BART-large-MNLI to classify user context against appraisal dimension statements. You need to understand NLI as determining entailment/contradiction relationships between a premise (user context) and hypotheses (appraisal statements).
  - Quick check question: Given a user query "I need this done immediately," which appraisal dimension statement would an NLI model most likely entail: "There was no urgency in the situation" or "The event required an immediate response"?

## Architecture Onboarding

- **Component map:**
  User Context Builder → NLI Appraisal Classification → Candidate Generation → Appraisal-Aligned Scoring → Selection → Explanation Generation

- **Critical path:**
  User input → Context encoding → NLI appraisal classification → Candidate scoring → Selection → Explanation generation. The NLI classification and subsequent scoring directly determine both the recommendation and the explanation framing.

- **Design tradeoffs:**
  - **Scoring method simplicity vs. accuracy**: The paper uses "simplified scoring" (keyword matching, basic sentiment) for demonstration. Production systems would need more sophisticated analyzers (deep learning sentiment, semantic similarity), increasing complexity.
  - **Fixed vs. dynamic appraisal dimensions**: Current framework uses 6 predefined dimensions. Dynamic discovery of domain-specific dimensions could improve fit but requires additional infrastructure.
  - **LLM explanation generator vs. template-based**: LLM provides fluency and adaptability but adds latency, cost, and potential inconsistency. Templates are faster but less personalized.

- **Failure signatures:**
  - **NLI misclassification**: User query "I want something exciting" classified as high urgency rather than low predictability/novelty-seeking → wrong appraisal prioritization → misaligned recommendation.
  - **Scoring function drift**: Keyword-based scoring fails on synonyms or negation (e.g., "not in a rush" still triggers urgency keywords).
  - **Explanation hallucination**: LLM generates appraisal labels not actually supported by the scoring (claiming "Goal Relevance" when scores were low).
  - **Profile-query mismatch**: Stored profile conflicts with immediate query (profile says "health-conscious," query says "I want comfort food") without reconciliation logic.

- **First 3 experiments:**
  1. **Ablation study on appraisal dimensions**: Run the pipeline with each dimension removed individually to measure impact on user-rated explanation quality (if user study possible) or on recommendation-explanation alignment scores.
  2. **NLI classification accuracy audit**: Manually label a test set of user queries with ground-truth appraisal salience, compare against BART NLI outputs to quantify classification accuracy per dimension.
  3. **Baseline comparison with structured prompts**: Compare against a baseline where GPT-4o is directly prompted to "explain this recommendation in a personalized way" without explicit appraisal conditioning, to isolate the appraisal framework's contribution.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do appraisal-conditioned explanations affect user trust and decision-making performance in empirical studies with real human participants compared to baseline methods?
- **Basis in paper:** [explicit] The authors state that "user studies with actual participants are necessary to assess better the framework’s effectiveness, which is considered future research work."
- **Why unresolved:** The current results rely on synthetic case studies (Sarah and Alex) and simulated data rather than live human evaluation.
- **What evidence would resolve it:** Quantitative results from controlled user trials measuring trust scores, task completion times, and subjective satisfaction ratings.

### Open Question 2
- **Question:** To what extent does the effectiveness of appraisal-based explanations vary across different cultural contexts and diverse user demographics?
- **Basis in paper:** [explicit] The paper notes that "the subjectivity of appraisal-based explanations requires validation across diverse user groups, cultural contexts, and domains."
- **Why unresolved:** Appraisal dimensions like "normative significance" are inherently subjective, and the current framework has not been tested on populations with varying cultural norms.
- **What evidence would resolve it:** Cross-cultural experimental data showing consistent or adaptable explanation utility across different sociodemographic groups.

### Open Question 3
- **Question:** Can the proposed multi-stage pipeline be optimized to scale computationally for real-time, high-stakes applications without excessive latency?
- **Basis in paper:** [explicit] The authors acknowledge that "scaling this framework to more complex systems could be computationally demanding" due to the reliance on large language models and NLI.
- **Why unresolved:** The framework currently utilizes computationally heavy models (e.g., GPT-4o, BART), and its viability in low-latency environments remains unproven.
- **What evidence would resolve it:** Performance benchmarks showing system latency and throughput in complex, real-time environments like autonomous driving or high-frequency trading.

## Limitations

- **Major unknown:** The framework lacks empirical validation through user studies; all claims about enhanced trust and transparency remain theoretical
- **Computational overhead:** Heavy reliance on large language models (GPT-4o, BART) creates scalability concerns for real-time, high-stakes applications
- **NLI classification risk:** Classification errors in the NLI model will propagate directly into misaligned recommendations and explanations

## Confidence

- **Medium confidence**: The theoretical mechanism linking CPM appraisal dimensions to explanation quality (Mechanism 1) is well-grounded in cognitive science literature, though untested empirically in this framework.
- **Low confidence**: Claims about dynamic appraisal prioritization (Mechanism 2) and explicit reasoning chain transparency (Mechanism 3) lack direct validation; success depends entirely on NLI classification accuracy and user acceptance of appraisal-framed language.
- **Medium confidence**: The architectural design is coherent and implementable, but the simplified scoring approach may not scale to complex domains.

## Next Checks

1. Conduct a user study comparing appraisal-conditioned explanations against baseline explanations on trust, transparency, and cognitive accessibility metrics across diverse user groups.
2. Perform ablation studies removing individual appraisal dimensions to quantify their contribution to explanation quality and recommendation alignment.
3. Audit NLI classification accuracy by creating a labeled test set of user queries with ground-truth appraisal salience, measuring classification precision per dimension.