---
ver: rpa2
title: 'SpeakEasy: Enhancing Text-to-Speech Interactions for Expressive Content Creation'
arxiv_id: '2504.05106'
source_url: https://arxiv.org/abs/2504.05106
tags:
- speech
- participants
- speakeasy
- performance
- voice
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces SpeakEasy, a Wizard-of-Oz system designed
  to simplify expressive text-to-speech (TTS) generation for content creators. Through
  two formative studies with creators and voice actors, the authors identify that
  users struggle with unintuitive TTS controls and benefit from high-level context
  and iterative feedback.
---

# SpeakEasy: Enhancing Text-to-Speech Interactions for Expressive Content Creation

## Quick Facts
- arXiv ID: 2504.05106
- Source URL: https://arxiv.org/abs/2504.05106
- Authors: Stephen Brade; Sam Anderson; Rithesh Kumar; Zeyu Jin; Anh Truong
- Reference count: 40
- Primary result: Wizard-of-Oz TTS system with high-level controls significantly outperforms industry tools on expressiveness and ease of use

## Executive Summary
SpeakEasy introduces a novel approach to text-to-speech generation by allowing users to provide high-level context alongside scripts and iterate with semantic feedback rather than low-level controls. The system is designed around insights from formative studies with content creators and voice actors, who found existing TTS tools unintuitive and benefited from iterative feedback and exploration of diverse performances. Through a Wizard-of-Oz evaluation with 12 participants, SpeakEasy demonstrated significant improvements over ElevenLabs and Speechify in generating expressive performances while reducing mental and temporal demand.

## Method Summary
The authors conducted two formative studies to understand user needs for expressive TTS generation, followed by a within-subjects evaluation comparing SpeakEasy against industry-leading TTS tools. SpeakEasy uses a Wizard-of-Oz approach where GPT-4 Turbo selects from a database of 230 pre-recorded human performances based on user-provided context and feedback. The evaluation involved 12 participants completing identical TTS generation tasks using SpeakEasy, ElevenLabs, and Speechify, with measurements including task completion time, subjective ratings of suitability and effort, and qualitative feedback on the interaction experience.

## Key Results
- SpeakEasy's first generations were rated significantly more suitable than both ElevenLabs and Speechify (p = .007 and p = .006 respectively)
- Participants reported significantly less mental and temporal demand when using SpeakEasy compared to both baselines
- SpeakEasy exposed users to significantly more creative ideas than either baseline tool (p = 0.012 for ElevenLabs, p = 0.02 for Speechify)

## Why This Works (Mechanism)

### Mechanism 1: Context-Conditioned Generation via Human-in-the-Loop
Providing high-level context alongside scripts improves initial generation suitability by aligning output with user intent from the outset. SpeakEasy simulates a future TTS model that conditions directly on user context through GPT-4 Turbo selecting appropriate pre-recorded performances.

### Mechanism 2: Iterative Refinement with High-Level Feedback
Refining individual sentences with semantic feedback (adjectives, phrases) reduces cognitive load compared to manipulating low-level controls. The system abstracts away acoustic complexity by accepting natural language feedback that mirrors how directors communicate with voice actors.

### Mechanism 3: Diverse Performance Variations for Exploration and Selection
Presenting diverse performances and facilitating comparison helps users discover desirable outputs and expand creative possibilities. The "Surprise Me" feature and contrasting adjective recommendations expose users to unanticipated but fitting performances, mimicking the voice actor practice of providing multiple "takes."

## Foundational Learning

- **Concept: Wizard-of-Oz Prototyping**
  - **Why needed here:** SpeakEasy simulates an AI capability not yet available at required quality, making results about interaction design rather than a fully automated algorithm
  - **Quick check question:** In SpeakEasy, what is simulated by humans/pre-recordings versus what a production TTS model would handle?

- **Concept: User Mental Models in AI Interactions**
  - **Why needed here:** The paper identifies a mismatch between users' mental models (expressive, semantic) and existing tool controls (quantitative, granular)
  - **Quick check question:** Why did participants in the formative study find ElevenLabs' sliders difficult to interpret?

- **Concept: The Director-Actor Analogy for Human-AI Interaction**
  - **Why needed here:** The design philosophy is modeled after professional director-voice actor workflow, shaping every feature from context provision to iterative feedback
  - **Quick check question:** According to expert interviews, how do directors communicate intentions to voice actors, and how does SpeakEasy replicate this?

## Architecture Onboarding

- **Component map:** User Input -> Script Context View/Editing View -> Sentence Panel/Playback Bar/Sentence Iteration Menu -> Take Selection Service -> GPT-4 Turbo -> Performance Database -> Audio Output

- **Critical path:**
  1. User enters script and optional context
  2. Take Selection Service queries GPT-4 to select performance ID based on context; retrieves corresponding audio file
  3. User clicks sentence and provides feedback (adjective, freeform text, "Surprise Me")
  4. Take Selection Service queried again for new performance; UI creates comparison tab
  5. User downloads stitched performance

- **Design tradeoffs:**
  - High-Level Control vs. Granular Precision: Prioritizes ease of use over fine-grained control
  - Fixed Performance Set vs. Dynamic Generation: Pre-recorded set guarantees quality but limits variety
  - Semantic Feedback vs. Acoustic Parameters: Relies on natural language, which is intuitive but may be lossy

- **Failure signatures:**
  - User stuck on prompt: Cannot articulate intent, leading to poor initial generation
  - Iteration feels random: Poor feedback-to-performance mapping makes outcomes unpredictable
  - Lack of suitable variety: Pre-recorded performances too similar
  - Comparison fatigue: Too many tabs overwhelms user

- **First 3 experiments:**
  1. Implement live "Take Selection" prompt: Integrate with real prompt-based TTS API to test if GPT-4-based selection works with dynamically generated audio
  2. A/B test control granularity: Add "Advanced Controls" panel with low-level parameters alongside high-level adjectives for expert users
  3. Test prompt bootstrapping: Implement "interactive prompt exploration" feature to help stuck users generate initial context

## Open Questions the Paper Calls Out
The paper acknowledges that the Wizard-of-Oz nature means results are findings about interaction design rather than fully automated system capabilities. The effectiveness of GPT-4 Turbo for mapping free-form context to appropriate speech characteristics has not been validated in production settings. Additionally, the evaluation sample size (N=12) and task complexity (single sentence generation) may not capture challenges in longer-form content creation.

## Limitations
- Results are based on Wizard-of-Oz setup using pre-recorded performances rather than real-time TTS generation
- Evaluation sample size (N=12) may not capture challenges in longer-form content creation
- Effectiveness of GPT-4 Turbo for mapping free-form context to speech characteristics not validated in production

## Confidence

- **High confidence**: Formative study findings about user struggles with current TTS tools are well-supported by qualitative data
- **Medium confidence**: Within-subjects evaluation results are statistically significant but may not generalize to real TTS generation
- **Medium confidence**: Claim that high-level semantic feedback reduces cognitive load is supported by user ratings, though some experts desired more granular controls

## Next Checks
1. Replace Wizard-of-Oz take selection with real-time prompt-based TTS generation and measure if interaction benefits persist
2. Conduct longitudinal study with content creators working on longer scripts to assess scalability
3. Test system with professional voice directors and experienced TTS users to validate expert needs