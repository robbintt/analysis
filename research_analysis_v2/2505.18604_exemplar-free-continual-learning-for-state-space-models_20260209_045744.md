---
ver: rpa2
title: Exemplar-Free Continual Learning for State Space Models
arxiv_id: '2505.18604'
source_url: https://arxiv.org/abs/2505.18604
tags:
- inf-ssm
- task
- learning
- ssms
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of catastrophic forgetting in
  State-Space Models (SSMs) under exemplar-free continual learning. It proposes Inf-SSM,
  a geometry-aware regularization method that constrains the evolution of SSM states
  by leveraging the extended observability subspace and the infinite-dimensional Grassmannian.
---

# Exemplar-Free Continual Learning for State Space Models

## Quick Facts
- arXiv ID: 2505.18604
- Source URL: https://arxiv.org/abs/2505.18604
- Reference count: 40
- Key outcome: Reduces forgetting by up to 33.11% and improves average accuracy by up to 29.19% compared to existing methods

## Executive Summary
This paper addresses catastrophic forgetting in State-Space Models (SSMs) under exemplar-free continual learning. It proposes Inf-SSM, a geometry-aware regularization method that constrains the evolution of SSM states by leveraging the extended observability subspace and the infinite-dimensional Grassmannian. By solving a Sylvester equation efficiently in O(n²) time, Inf-SSM regularizes the model without relying on past exemplars. Experiments on ImageNet-R and Caltech-256 demonstrate significant improvements over existing methods.

## Method Summary
Inf-SSM regularizes SSMs by constraining the extended observability subspace using a geometry-aware loss computed on the infinite-dimensional Grassmannian. The method extracts time-averaged state matrices (A, B, C) from the model, computes Gram matrices using a diagonal Sylvester solver, and minimizes the chordal distance between current and previous subspaces. Soft-Normalization ensures Schur stability, while the O(n²) solver exploits the diagonal structure of modern SSMs. The approach can be combined with replay methods for additional performance gains.

## Key Results
- Reduces forgetting by up to 33.11% on ImageNet-R compared to existing methods
- Improves average accuracy by up to 29.19% on benchmark datasets
- Achieves up to 1.94% higher average accuracy when combined with replay-based methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Constraining the extended observability subspace (S_∞) preserves past knowledge more effectively than direct weight regularization in SSMs.
- **Mechanism:** The method leverages the invariance of the extended observability matrix (O_∞) to P-equivalence, minimizing chordal distance between subspaces spanned by observability matrices rather than raw weights.
- **Core assumption:** The model's past knowledge is primarily encoded in the evolution of the hidden state defined by (A, C).
- **Evidence anchors:** Abstract mentions regularizing "extended observability subspace" and "geometry of the infinite-dimensional Grassmannian"; Section 3.1 argues previous methods suffer by "Ignoring the geometry of SSMs."
- **Break condition:** If the SSM is not observable or state dimension n is too small, the subspace constraint may become trivial.

### Mechanism 2
- **Claim:** Reducing computational complexity from O(n³) to O(n²) via the Sylvester equation makes the method viable for training.
- **Mechanism:** Exploits diagonal structure of state matrix A in modern SSMs like Mamba and S4D, computing solution G via Hadamard product instead of standard cubic-time methods.
- **Core assumption:** The state matrix A remains effectively diagonal or structured.
- **Evidence anchors:** Abstract explicitly claims "$O(n²) solution by exploiting the structure and properties of SSMs"; Section 3.3 details simplification using diagonal structure.
- **Break condition:** Cannot use efficient solver on non-diagonal SSM architectures without specific structural properties.

### Mechanism 3
- **Claim:** Regularizing state transition dynamics (A, C) is more critical for stability than regularizing input mapping (B).
- **Mechanism:** Focuses geometric regularization on (A, C) defining system's autonomous evolution, with Inf-SSM+ adding Frobenius norm on B to close input path gap.
- **Core assumption:** Controllability defined by B is secondary to observability defined by (A, C) in retaining learned representations.
- **Evidence anchors:** Section 3.5 defines Inf-SSM+ with γ term for B; Section H.1 provides empirical analysis showing CKD values are highest for A.
- **Break condition:** In tasks where input distribution shifts are primary driver of performance collapse, ignoring B regularization might lead to suboptimal stability.

## Foundational Learning

- **Concept:** Grassmann Manifold (Gr(n, d))
  - **Why needed here:** Core of treating SSM state as subspace on manifold rather than point in Euclidean space. Explains why two different matrices can represent same subspace via P-equivalence.
  - **Quick check question:** Can you explain why two different matrices, X and X·R (where R is orthogonal), might represent the exact same subspace?

- **Concept:** Extended Observability Matrix (O_∞)
  - **Why needed here:** Mathematical object being regularized, representing cumulative output of system over infinite horizon.
  - **Quick check question:** If matrix A has eigenvalues with magnitude >1, why does O_∞ = [C, CA, CA², ...] calculation become unstable?

- **Concept:** P-Equivalence (System Invariance)
  - **Why needed here:** Explains why standard weight regularization fails for SSMs - penalizing moves along "orbit" of equivalent solutions constrains learning without preserving knowledge.
  - **Quick check question:** If I transform system parameters (A, B, C) to (PA P⁻¹, PB, CP⁻¹), does output sequence y(t) change for same input x(t)?

## Architecture Onboarding

- **Component map:** Vision Mamba (Vim-Small) using S6 blocks -> State Extractor (intercepts forward pass, extracts A,B,C, applies Soft-Normalization) -> Geometry Module (computes Gram matrices using diagonal solver) -> Loss Aggregator (calculates Chordal distance, combines with Frobenius norm on B and Cross-Entropy)

- **Critical path:** Extraction of Ã and C̃ and subsequent solving of Sylvester equation. Soft-Normalization (SN) must enforce Schur stability to prevent Gram matrix divergence.

- **Design tradeoffs:**
  - Inf-SSM vs. Inf-SSM+: Inf-SSM is theoretically cleaner (geometry-aware) using only (A,C), while Inf-SSM+ adds crude Frobenius norm on B for empirical stability
  - Efficiency: O(n²) solver is extremely fast but strictly relies on diagonal structure of A

- **Failure signatures:**
  - Instability: Exploding loss values during training - check Soft-Normalization of Ã; eigenvalues approaching 1 cause division-by-zero errors
  - Scan Breaking: Appendix notes regularization interferes with efficient CUDA scan, increasing VRAM usage

- **First 3 experiments:**
  1. Sanity Check (P-Equivalence): Train small SSM on one task, continue training on second task using only L2 weight regularization, compare against Inf-SSM
  2. Efficiency Validation: Benchmark custom O(n²) Sylvester solver against standard Bartels-Stewart solver on dummy diagonal SSM to confirm 100x FLOP reduction
  3. Ablation on B: Compare Inf-SSM vs. Inf-SSM+ on dataset with high input noise to determine if B regularization is practically necessary

## Open Questions the Paper Calls Out

- **Question:** Can the input state matrix B be regularized in a manner that is invariant to the system orbit, unlike the Frobenius norm used in Inf-SSM+?
- **Basis in paper:** Section 6 states Inf-SSM+ uses Frobenius norm for B which "is not invariant to the orbit as well."
- **Why unresolved:** Frobenius norm breaks geometric consistency used for (A, C), introducing trade-off between regularizing input path and maintaining P-equivalence
- **What evidence would resolve it:** Derived geometric loss function for B based on controllability subspace that preserves P-equivalence

- **Question:** Can computational efficiency of Inf-SSM be improved to avoid "scan-breaking" overhead by extracting states directly from optimized CUDA kernels?
- **Basis in paper:** Appendix E notes method is "scan-breaking" because intermediate states must be recomputed outside scan, increasing VRAM and training time
- **Why unresolved:** Current SSM libraries fuse operations for speed; extracting intermediate states forces fallback to slower computation
- **What evidence would resolve it:** Custom kernel implementation extracting necessary states without disrupting fused scan operation

- **Question:** Do advanced geometric measures like distances integrating Schubert Varieties or kernel methods in RKHS offer better knowledge retention than standard chordal distance?
- **Basis in paper:** Section 7 suggests future work should "explore Grassmannian-based distances by integrating them with Schubert Varieties... and kernel methods"
- **Why unresolved:** Paper successfully uses chordal distance, but more complex geometric approaches remain untested
- **What evidence would resolve it:** Empirical benchmarks showing improved AA or FM using advanced metrics on ImageNet-R or Caltech-256

## Limitations

- **Architecture dependency:** Method relies on diagonal structure of modern SSMs; applying to dense SSMs reverts to O(n³) complexity
- **Numerical stability concerns:** Soft-Normalization claims to ensure stability but edge cases with eigenvalues approaching 1 could cause Gram matrix instability
- **Dataset specificity:** Results primarily demonstrated on ImageNet-R and Caltech-256; performance on different dataset characteristics may vary

## Confidence

- **High confidence:** Geometric regularization mechanism works as described for diagonal SSMs; P-equivalence argument and subspace distance calculation are mathematically sound
- **Medium confidence:** Efficiency claims (100x speedup) and necessity of regularizing B are empirically validated but could depend on architectural details
- **Low confidence:** Claim that Inf-SSM "outperforms" replay-based methods when combined is based on limited ablation studies; integration mechanism isn't fully explored

## Next Checks

1. **Architecture robustness test:** Apply Inf-SSM to non-diagonal SSM variant (e.g., dense recurrent layers) and measure computational overhead and forgetting performance compared to diagonal case
2. **Stability boundary analysis:** Systematically vary Soft-Normalization parameters and measure point at which numerical instability occurs in Gram matrix computation
3. **Cross-dataset generalization:** Evaluate Inf-SSM on dataset with fundamentally different characteristics (e.g., fine-grained species classification) to test whether 33% forgetting reduction generalizes beyond ImageNet-R/Caltech-256