---
ver: rpa2
title: Labels Generated by Large Language Models Help Measure People's Empathy in
  Vitro
arxiv_id: '2501.00691'
source_url: https://arxiv.org/abs/2501.00691
tags:
- empathy
- labels
- data
- training
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how large language models (LLMs) can enhance
  the training of smaller pre-trained language models (PLMs) for empathy prediction
  tasks. The key innovation lies in using LLM-generated labels in two in-vitro applications:
  (1) correcting noisy labels in crowdsourced datasets and (2) augmenting training
  data with additional LLM-labelled samples.'
---

# Labels Generated by Large Language Models Help Measure People's Empathy in Vitro

## Quick Facts
- arXiv ID: 2501.00691
- Source URL: https://arxiv.org/abs/2501.00691
- Reference count: 40
- Primary result: RoBERTa achieves PCC 0.648 on NewsEmp24 using LLM-generated labels

## Executive Summary
This paper demonstrates that large language models (LLMs) can significantly improve empathy prediction by generating high-quality labels for training smaller pre-trained language models (PLMs). The approach uses two in-vitro applications: correcting noisy crowdsourced labels by replacing them with LLM labels when disagreement exceeds a threshold, and augmenting training data with LLM-labeled additional samples. By leveraging scale-aware prompting aligned with psychological annotation protocols, the method achieves state-of-the-art performance on the NewsEmp24 benchmark with a Pearson correlation coefficient of 0.648. The approach also shows robust improvements across multiple evaluation metrics and datasets, with statistically significant gains over baseline models.

## Method Summary
The method employs two complementary applications of LLM-generated labels for empathy prediction. First, it corrects noisy crowdsourced labels by replacing them with LLM labels when the absolute difference exceeds threshold α (typically 4.0 out of 6-point scale). Second, it augments training data by labeling additional unlabeled essays with LLMs. Both applications use scale-aware prompting that mirrors the Batson empathy scale's six subscales (sympathetic, moved, compassionate, tender, warm, soft-hearted). The PLMs (RoBERTa-base or DeBERTa-v3-base) are fine-tuned with linear regression heads on [CLS] tokens, using early stopping based on concordance correlation coefficient rather than Pearson correlation alone.

## Key Results
- RoBERTa achieves state-of-the-art PCC 0.648 on NewsEmp24 benchmark
- Combined applications (correction + augmentation) show statistically significant improvements (p<0.0001)
- DeBERTa-v3-base slightly outperforms RoBERTa-base across all configurations
- Scale-aware prompting outperforms plain prompting (PCC: 0.405→0.441 on NewsEmp24 test)

## Why This Works (Mechanism)

### Mechanism 1: Scale-Aware Prompting Aligns LLM Annotations with Psychological Protocols
Structuring prompts to mirror the original psychological annotation protocol produces LLM labels that better match the construct being measured. The Batson empathy scale has six subscales (sympathetic, moved, compassionate, tender, warm, soft-hearted). By instructing the LLM to output scores for each subscale rather than a single aggregate score, the prompt constrains the LLM's reasoning to follow the same decomposition used by human annotators. This natural alignment with psychology-grounded labeling protocols improves label quality and downstream model performance.

### Mechanism 2: Threshold-Based Selective Replacement Reduces Noise Without Breaking Distribution Alignment
Selectively replacing only highly divergent labels preserves train-test distribution similarity while correcting the most damaging noise. When LLM and crowdsourced labels differ by more than threshold α, the original label is replaced; otherwise retained. Smaller α increases LLM label proportion (more correction but larger distribution shift); larger α is more conservative. The paper finds α≈3.5-4.5 optimal, balancing noise reduction against distribution preservation.

### Mechanism 3: LLM-Labeled Additional Data Extends Supervision Beyond Available Human Annotations
LLMs can label new data points that lack human annotations, providing additional supervision that improves generalization. Unlabeled essays similar to the target domain are labeled using the same scale-aware prompt. This extended dataset exposes the PLM to more diverse examples without requiring additional human annotation effort. The additional data provides meaningful supervision, with performance improvements plateauing after 30% additional data.

## Foundational Learning

- **Concept: Label Noise in Crowdsourced Annotations**
  - Why needed here: The entire method is motivated by noise in self-report questionnaire data collected via platforms like Mechanical Turk (inattentiveness, multitasking).
  - Quick check question: Can you explain why standard regularization (dropout, weight decay) doesn't fully address label noise in supervised learning?

- **Concept: Regression vs. Classification for Subjective Constructs**
  - Why needed here: Empathy is measured on a continuous 1-7 scale. Many noise-robust learning methods are designed for classification (cross-entropy, pseudo-labels from softmax) and don't transfer directly to regression.
  - Quick check question: Why can't you directly apply semi-supervised classification techniques like FixMatch to this regression problem?

- **Concept: Evaluation Metrics for Correlation and Agreement (PCC, CCC, RMSE)**
  - Why needed here: The paper argues PCC alone is insufficient—it measures linear relationship but not error magnitude. CCC captures both. Understanding this distinction is essential for proper model selection.
  - Quick check question: Why might a model achieve perfect PCC (1.0) while having large prediction errors?

## Architecture Onboarding

- **Component map:** Scale-aware prompt module → LLM API layer → Label aggregation → Annotation selector → PLM fine-tuning → Early stopping
- **Critical path:** Data → LLM labeling (one-time cost) → Label selection/augmentation → PLM fine-tuning → Inference (runs on PLM only, no LLM needed)
- **Design tradeoffs:**
  - Application choice: Application 1 (noise correction) provides moderate gains with lower data requirements; Application 2 (additional data) provides larger gains (p<0.0001) but requires finding similar unlabeled essays
  - Threshold α: Values 3.5-4.5 balance noise correction against distribution shift; α too small = over-reliance on LLM labels, α too large = insufficient correction
  - LLM choice: Llama (open, free) and GPT-4 (proprietary, paid) perform comparably
  - PLM choice: DeBERTa slightly outperforms RoBERTa but has more parameters (184M vs. 126M)
- **Failure signatures:**
  - Performance degrades with α<3.0 (excessive distribution shift)
  - High variance across random seeds suggests overfitting—check early stopping is working
  - CCC much lower than PCC indicates systematic bias in predictions (magnitude errors)
  - Negative CCC on specific demographic groups signals potential bias—investigate data representation
- **First 3 experiments:**
  1. **Baseline establishment:** Fine-tune RoBERTa on original crowdsourced NewsEmp24 training data. Report PCC, CCC, RMSE on test set across 5 seeds.
  2. **Scale-aware prompt validation:** Compare plain vs. scale-aware prompting on a 100-sample subset. Verify LLM outputs are parseable and subscale scores average correctly.
  3. **Threshold sweep:** With Application 1, test α ∈ {3.0, 3.5, 4.0, 4.5, 5.0} on NewsEmp24. Plot validation CCC vs. α to find optimal threshold.

## Open Questions the Paper Calls Out

### Open Question 1
Can chain-of-thought prompting or explicit reasoning requirements improve LLM-generated empathy labels beyond scale-aware prompting alone? Current experiments used only scale-aware prompting without requiring reasoning justification; no comparison to chain-of-thought approaches was conducted.

### Open Question 2
Does the proposed in-vitro LLM labeling approach transfer effectively to multilingual empathy computing? All empirical validation was conducted exclusively on English-language datasets using English-capable LLMs.

### Open Question 3
What prompting strategies can mitigate demographic biases in LLM-generated empathy annotations across diverse populations? Current demographic-unaware prompting showed inconsistent agreement across groups; the paper tested neither demographic-aware prompts nor systematic bias mitigation strategies.

## Limitations
- Results are specific to the Batson empathy scale's six-subscale structure and may not generalize to constructs lacking clear dimensional decomposition
- Threshold-based correction creates a trade-off between noise reduction and train-test distribution alignment that may vary with different noise characteristics
- Performance disparities across demographic groups indicate potential bias in both crowdsourced and LLM-generated labels

## Confidence
- **High Confidence**: Scale-aware prompting improves LLM label quality (PCC: 0.405→0.441 on NewsEmp24); combined applications achieve statistically significant improvements (p<0.0001); DeBERTa-v3-base outperforms RoBERTa-base
- **Medium Confidence**: Threshold α=4.0 is optimal for these datasets; LLM-labeled additional data provides meaningful supervision; LLM labels are more consistent than crowdsourced labels (K-Alpha: 0.80 vs 0.19-0.27)
- **Low Confidence**: Generalization to other empathy scales or psychological constructs; cost-effectiveness of LLM labeling at scale; long-term stability of LLM label quality across model versions

## Next Checks
1. **Distribution Alignment Test**: Run experiments with α∈{3.0, 3.5, 4.0, 4.5, 5.0} and measure train-test distribution similarity to validate the trade-off between noise correction and distribution shift.

2. **Cross-Construct Generalization**: Apply the same method to a different psychological construct (e.g., anxiety or depression scales) with different dimensional structure to test scale-aware prompting's generalizability beyond the Batson empathy scale.

3. **Cost-Benefit Analysis**: Calculate the total cost of LLM labeling versus the performance improvement achieved, comparing against alternative noise-robust learning approaches that don't require LLM access.