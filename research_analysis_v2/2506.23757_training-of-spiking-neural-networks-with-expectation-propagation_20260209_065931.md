---
ver: rpa2
title: Training of Spiking Neural Networks with Expectation-Propagation
arxiv_id: '2506.23757'
source_url: https://arxiv.org/abs/2506.23757
tags:
- training
- spiking
- networks
- neural
- weights
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces an Expectation-Propagation (EP) framework
  for training spiking neural networks (SNNs) using mini-batches, enabling the first
  unified approach to train deterministic and stochastic SNNs with both continuous
  and discrete weights. The method uses forward and backward message-passing to update
  approximating distributions for weights and hidden layer outputs, preserving Bayesian
  inference while handling large datasets via stochastic EP.
---

# Training of Spiking Neural Networks with Expectation-Propagation

## Quick Facts
- **arXiv ID:** 2506.23757
- **Source URL:** https://arxiv.org/abs/2506.23757
- **Reference count:** 40
- **One-line primary result:** EP framework enables first unified training of deterministic and stochastic SNNs with continuous/discrete weights, matching MLE performance while providing uncertainty estimates.

## Executive Summary
This paper introduces an Expectation-Propagation (EP) framework for training spiking neural networks using mini-batches, enabling the first unified approach to train deterministic and stochastic SNNs with both continuous and discrete weights. The method uses forward and backward message-passing to update approximating distributions for weights and hidden layer outputs, preserving Bayesian inference while handling large datasets via stochastic EP. Experiments on MNIST classification and 1D regression tasks show performance on par with maximum likelihood estimation, with the added benefit of providing weight uncertainties.

## Method Summary
The EP framework decomposes SNN training into modular factor blocks (mixing and activation) and iteratively refines approximating distributions through forward and backward passes. It approximates intractable Bayesian posteriors by treating mini-batches as incremental evidence, using cavity distributions to isolate each batch's contribution. Moment matching via KL divergence minimization yields closed-form updates without gradients. The framework supports inference without Monte Carlo sampling and enables efficient, scalable training with uncertainty quantification.

## Key Results
- MNIST classification accuracy around 0.97–1.00 on single-layer and two-layer architectures
- Regression MSE matching or slightly outperforming MLE baseline with meaningful uncertainty estimates
- Faster convergence than MLE without requiring early stopping
- Successful handling of both continuous and discrete weight types with appropriate uncertainty quantification

## Why This Works (Mechanism)

### Mechanism 1: Factor Graph Decomposition with Bidirectional Message-Passing
The EP framework approximates intractable Bayesian posteriors by decomposing the network into modular factor blocks and iteratively refining approximating distributions through forward and backward passes. Each layer contains mixing and activation blocks that update q1(U(ℓ)), q0(V(ℓ)), q0(U(ℓ)), q1(W(ℓ)), and q1(V(ℓ-1)). The separable approximation assumption enables tractable moment updates but may degrade if true posterior correlations are strong.

### Mechanism 2: Stochastic Expectation-Propagation for Mini-Batch Scalability
SEP enables training on large datasets by treating mini-batches as incremental evidence. For each batch, a cavity distribution removes that batch's previous contribution, treats it as a local prior, runs EP, and updates the global q1(W) via geometric averaging. This allows multiple epochs without overfitting by implicitly accounting for total data quantity. Small or unrepresentative batches can destabilize cavity distributions, requiring damping for stability.

### Mechanism 3: Moment Matching via KL Divergence Minimization
Each local update minimizes KL divergence between a "tilted" distribution and the approximating distribution, yielding closed-form moment updates without gradients. For mixing blocks, delta functions enforcing u(ℓ) = W(ℓ)v(ℓ-1) are approximated as Gaussian, then marginal tilted moments are computed in closed-form. For activation blocks, independent 1D KL problems reduce to computing mean/variance of tilted distributions. The approach breaks down when spike activations are rare, causing division instability.

## Foundational Learning

- **Concept: Expectation-Propagation (EP)**
  - Why needed: EP replaces backpropagation with iterative distribution refinement via local KL minimizations
  - Quick check: Explain why EP uses KL[q||p] (reverse KL) versus variational inference's KL[p||q], and what this implies for mode-seeking vs. coverage behavior

- **Concept: Factor Graphs and Message-Passing**
  - Why needed: The paper structures SNN training as inference on a factor graph; understanding message flow between factor nodes is essential
  - Quick check: Given variables A, B, C and factors f1(A,B), f2(B,C), what messages would node B receive during a forward pass from A to C?

- **Concept: Spike Response Model (SRM) and Binary Spiking**
  - Why needed: The neuron model defines the likelihood; distinguishing deterministic from stochastic spiking determines which update rules apply
  - Quick check: For a deterministic SRM neuron with membrane potential u = 0.3, what is the spike output? For a stochastic neuron with sigmoid activation, what is the probability of spiking?

## Architecture Onboarding

- **Component map:**
  ```
  Input v(0) → [Layer 1: Mixing Block → Activation Block] → ... → [Layer L: Mixing Block → Activation Block] → Output v(L)
                    ↓                           ↓
               q1(U(1)), q0(V(1))         q1(U(L)), q0(V(L))
                    ↑                           ↑
              Backward pass updates q0(U), q1(W), q1(V_prev)
  ```
  Each layer maintains three variable groups: U(ℓ) (membrane potentials), V(ℓ) (spike outputs), W(ℓ) (weights).

- **Critical path:**
  1. Initialization: Set q0(W) = p0(W); initialize q1(W), q0(Θ), q1(Θ)
  2. Forward pass: Update q1(U(ℓ)) via mixing block → update q0(V(ℓ)) via activation block
  3. Backward pass: Update q0(U(ℓ)) → update q1(W(ℓ)), q1(V(ℓ-1)) via mixing block
  4. Mini-batch handling: Compute cavity qc(W) → run Algo. 1 → update q1(W) via Eq. 24
  5. Inference: Single forward pass with fixed q(W) from training

- **Design tradeoffs:**
  - Continuous vs. discrete weights: Gaussian vs. Bernoulli/categorical approximating factors
  - Heaviside vs. sigmoid activation: Sigmoid yields better numerical stability
  - Batch size M: Larger batches stabilize cavity estimates but increase memory
  - Convergence vs. speed: EP lacks guaranteed convergence; damping and variance bounding stabilize but may slow learning

- **Failure signatures:**
  - Negative variances in rarely-activated regions
  - Divergent test losses showing plateauing rather than improvement
  - Cavity instability from near-deterministic q1,old(W)
  - Sparse activation issues causing weight update degradation

- **First 3 experiments:**
  1. Single-layer MNIST classification (784-10) with continuous weights, sigmoid activation, M=1000
  2. Convergence speed benchmark comparing EPSNN vs. MLE (Adam) on N=10^4 MNIST samples
  3. Regression with uncertainty quantification (100-128-1) on 1D regression with population encoding

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what theoretical conditions can convergence guarantees be established for the combined stochastic/average EP training method?
- Basis in paper: The abstract states "Although its convergence is not ensured, the algorithm converges in practice," and Section V-B notes "While convergence of the proposed method cannot be ensured, a series of tools are available to stabilize Bayesian learning."
- Why unresolved: The combination of stochastic EP with average EP introduces approximation cascades whose fixed-point behavior is difficult to characterize analytically
- What evidence would resolve it: Formal analysis establishing sufficient conditions (on damping parameters, batch sizes, network architecture) that guarantee convergence to a fixed point

### Open Question 2
- Question: How can EP update rules be derived for convolutional layers in SNNs, and how do they compare to fully-connected variants on image classification benchmarks?
- Basis in paper: The conclusion states: "For computer vision tasks, it would also be interesting to derive update rules for convolutional layers."
- Why unresolved: The factor graph and mixing block derivations assume dense weight matrices; convolutional weight sharing requires a different factorization structure that remains undefined
- What evidence would resolve it: Derivation of convolutional mixing block updates with moment-matching rules, plus comparative experiments on CIFAR-10/100 showing accuracy and uncertainty calibration

### Open Question 3
- Question: Can the framework be extended to recurrent SNNs with feedback connections and winner-take-all circuits?
- Basis in paper: The conclusion states: "The neuron models and architectures investigated in this paper need to be generalized in the future, in particular for use with recurrent neurons and to include feedback mechanisms and winner-take-all units."
- Why unresolved: Recurrent connections introduce temporal dependencies that break the layer-wise factorization, requiring loopy belief propagation or alternative schedule strategies
- What evidence would resolve it: Extended factor graph with recurrent edges, modified message-passing schedule, and demonstrations on temporal sequence tasks with performance and uncertainty metrics

## Limitations

- Convergence is not guaranteed; the method relies on damping and variance bounding to stabilize training, which may slow learning
- The separable posterior factorization assumption may break down for strongly correlated weight-spike dynamics
- Exact initialization values for approximating distributions are unspecified, creating reproducibility challenges
- Computational complexity per iteration is higher than gradient-based methods due to message-passing overhead

## Confidence

- **High:** MNIST classification performance claims (0.97–1.00 accuracy) are well-supported by Table I and reproducible architecture specifications
- **Medium:** Uncertainty quantification in regression tasks, as visual evidence shows expected behavior but quantitative calibration metrics are not provided
- **Low:** Scalability claims for large datasets rely on SEP, but corpus lacks direct comparisons of mini-batch EP variants under identical conditions

## Next Checks

1. Replicate the 784-10 MNIST architecture with continuous weights and sigmoid activation to verify baseline accuracy (≥0.93) and compare Heaviside vs. sigmoid stability
2. Implement the 1D regression task (100-128-1) and assess uncertainty estimates in unobserved regions ([0,1] ∪ [2.5,4]) against MLE predictions
3. Test cavity distribution stability by systematically varying batch sizes (M=100 to 10,000) and measuring variance degradation in rarely-activated neurons