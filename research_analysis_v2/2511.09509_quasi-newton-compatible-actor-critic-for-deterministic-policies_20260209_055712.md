---
ver: rpa2
title: Quasi-Newton Compatible Actor-Critic for Deterministic Policies
arxiv_id: '2511.09509'
source_url: https://arxiv.org/abs/2511.09509
tags:
- policy
- critic
- gradient
- function
- quasi-newton
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a quasi-Newton compatible actor-critic method
  for deterministic policies that leverages curvature information to accelerate convergence.
  The key idea is a quadratic critic that simultaneously preserves both the true policy
  gradient and an approximation of the performance Hessian, enabling second-order
  policy updates.
---

# Quasi-Newton Compatible Actor-Critic for Deterministic Policies

## Quick Facts
- arXiv ID: 2511.09509
- Source URL: https://arxiv.org/abs/2511.09509
- Reference count: 28
- One-line primary result: Quadratic critic preserving both gradient and Hessian enables second-order policy updates with faster convergence than first-order DPG baselines.

## Executive Summary
This paper proposes a quasi-Newton compatible actor-critic method for deterministic policies that leverages curvature information to accelerate convergence. The key idea is a quadratic critic that simultaneously preserves both the true policy gradient and an approximation of the performance Hessian, enabling second-order policy updates. A least-squares temporal difference learning algorithm is developed to efficiently estimate the critic parameters. The method is demonstrated on LQR and cart-pendulum balancing tasks, showing faster convergence and improved sample efficiency compared to first-order deterministic actor-critic baselines. The approach is applicable to any differentiable policy class and maintains consistency with the deterministic policy gradient theorem through careful compatibility design.

## Method Summary
The method combines deterministic policy gradient theory with quasi-Newton optimization by introducing a quadratic critic structure. The critic is parameterized as Q_w(s,a) = ψ(s,a)^T W ψ(s,a) + ψ(s,a)^T g + V_v(s), where ψ(s,a) = ∇_θπ_θ(s)(a - π_θ(s)). This structure ensures compatibility with the deterministic policy gradient while also approximating the performance Hessian. Critic parameters are estimated via decoupled least-squares temporal difference learning, solving sequential regression problems for baseline value, gradient, and curvature components. The resulting quasi-Newton update uses the estimated Hessian to precondition policy gradient steps, accelerating convergence while maintaining descent directions through positive semi-definite projection.

## Key Results
- Quasi-Newton updates converge faster than first-order DPG on LQR and cart-pendulum tasks
- Sample efficiency improves through batch LSTD estimation versus online TD updates
- Quadratic critic maintains compatibility conditions ensuring consistent gradient estimates
- Curvature approximation captures true Hessian at optimal policy parameters

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A quadratic critic structure can simultaneously preserve the deterministic policy gradient and approximate the performance Hessian, enabling consistent second-order updates.
- **Mechanism:** The critic is parameterized as Q_w(s,a) = ψ(s,a)^T W ψ(s,a) + ψ(s,a)^T g + V_v(s), where ψ(s,a) = ∇_θπ_θ(s)(a - π_θ(s)). This structure ensures that when substituted into the policy gradient and Hessian expressions, the critic derivatives match the true Q^π derivatives at the current policy, satisfying the compatibility conditions in Theorem 2.
- **Core assumption:** The true action-value function Q^π is twice continuously differentiable, and the policy π_θ is differentiable with respect to θ.
- **Evidence anchors:** [abstract] "Building on the concept of compatible function approximation for the critic, we introduce a quadratic critic that simultaneously preserves the true policy gradient and an approximation of the performance Hessian." [Section III-A, Definition 1] Formally defines quasi-Newton compatible critic requiring both (6a) and (6b). [corpus] Weak corpus support—no neighboring papers directly address quadratic compatible critics for second-order policy updates.
- **Break condition:** If the critic function class is too restrictive to represent the true Q^π curvature, or if exploration is insufficient to observe action-value variations, gradient and Hessian estimates become biased.

### Mechanism 2
- **Claim:** Decoupled least-squares temporal difference (LSTD) estimation of critic parameters provides lower-variance, more sample-efficient learning than incremental TD updates.
- **Mechanism:** The LSTD algorithm solves three sequential regression problems in closed form: (1) baseline value parameters v via (11), (2) gradient parameters g via (13), and (3) curvature matrix W via (15) with PSD projection. Batch processing over entire trajectories reduces variance inherent in online TD updates.
- **Core assumption:** Sufficient exploration coverage across state-action space, and linear parameterization of the critic in features ϕ(s) and ψ(s,a).
- **Evidence anchors:** [Section III-C] "LSTD belongs to the family of batch RL algorithms that estimate the critic parameters from an entire set of collected transitions, offering higher sample efficiency, lower variance, and improved numerical robustness." [Algorithm 1, steps 2-4] Explicit sequential computation of v, g, and W using accumulated statistics. [corpus] Limited corpus support—standard LSTD is known, but decoupled gradient/curvature estimation is not addressed in neighbors.
- **Break condition:** If matrices A_v, A_g, or A_W become ill-conditioned or singular, closed-form solutions fail; pseudoinverse or regularization is required.

### Mechanism 3
- **Claim:** The approximated Hessian H(θ) = E_s[∇_θπ_θ(s) ∇²_a Q^π(s,a)|_{a=π_θ(s)} ∇_θπ_θ(s)^T] becomes exact at optimal parameters, enabling superlinear convergence.
- **Mechanism:** Theorem 1 (from prior work [10]) shows this approximation captures the true Hessian at θ*. During learning, it provides a preconditioner that rescales gradient steps along directions of high/low curvature, improving conditioning of the optimization landscape.
- **Core assumption:** The optimal policy θ* exists and the true Q^π Hessian is well-defined at θ*; the approximation error diminishes as θ approaches θ*.
- **Evidence anchors:** [Section II-C, Theorem 1] "This approximation captures the exact Hessian at the optimal policy parameters, that is, H(θ*) = ∇²_θ J(θ)|_{θ=θ*}." [Figure 4] Shows faster convergence of ||θ - θ*|| for quasi-Newton vs. first-order DPG in LQR. [corpus] No corpus papers address this specific Hessian approximation or its optimality property.
- **Break condition:** If W is poorly estimated (high variance, non-PSD without projection), the quasi-Newton step may point in a non-descent direction or have excessive magnitude.

## Foundational Learning

- **Concept: Deterministic Policy Gradient (DPG) Theorem**
  - Why needed here: The entire framework builds on the DPG theorem (Equation 3), which expresses the policy gradient in terms of ∇_θπ and ∇_a Q^π. Without understanding why the gradient depends only on action-value derivatives, the critic compatibility conditions are unmotivated.
  - Quick check question: Can you explain why the policy gradient involves ∇_a Q^π evaluated at the policy's action rather than the full Q^π function?

- **Concept: Compatible Function Approximation**
  - Why needed here: The paper extends the standard (first-order) compatible critic concept to second-order. Understanding that compatibility requires ∇_a Q_w = ∇_θπ^T g at the policy action is prerequisite to grasping the additional Hessian constraint (ii) in Theorem 2.
  - Quick check question: Why does substituting an arbitrary critic Q_w into the DPG formula generally introduce bias, and what condition eliminates this bias?

- **Concept: Least-Squares Temporal Difference (LSTD)**
  - Why needed here: The critic parameters are estimated via LSTD (Equations 11, 13, 15). Understanding how LSTD minimizes TD error in closed form via correlation matrices is essential for implementing the algorithm and diagnosing numerical issues.
  - Quick check question: What is the relationship between the TD error δ_k and the Bellman equation, and how does LSTD use δ_k to form a least-squares problem?

## Architecture Onboarding

- **Component map:**
  Actor (policy π_θ): parameterized deterministic policy π_θ: S → A computes ∇_θπ_θ(s) for critic features and actor update
  Critic (quadratic Q_w): baseline V_v(s) = v^T ϕ(s), linear advantage term: ψ(s,a)^T g, quadratic advantage term: ψ(s,a)^T W ψ(s,a)
  LSTD Estimator: accumulates A_v, b_v for v (Equation 11), accumulates A_g, b_g for g (Equation 13), accumulates A_W, b_W for W (Equation 15)
  Quasi-Newton Updater: assembles gradient ∇J = E[∇_θπ ∇_θπ^T g], assembles Hessian H = E[∇_θπ (∇_θπ^T W ∇_θπ) ∇_θπ^T], updates θ ← θ - α_θ H^{-1} ∇J

- **Critical path:**
  1. Collect E episodes of horizon K with exploration noise
  2. Compute baseline v (Equation 11)
  3. Compute TD residuals δ_k using v
  4. Compute gradient g (Equation 13)
  5. Compute modified residuals δ̂_k and features ψ̂_k
  6. Compute unconstrained Ŵ (Equation 15), project to PSD cone
  7. Assemble ∇J and H from g and W
  8. Perform quasi-Newton step H^{-1} ∇J
  The sequential dependency v → g → W is explicit; errors propagate forward.

- **Design tradeoffs:**
  - Sample efficiency vs. computation: Each quasi-Newton update is O(n_θ^3) for matrix inversion, but converges in fewer iterations than first-order methods. Paper claims this is favorable overall (Section V).
  - Exploration magnitude σ: Larger σ improves Hessian estimation but increases trajectory variance and can destabilize learning. Paper uses σ = 0.01 for cart-pendulum.
  - Batch size E × K: Larger batches reduce variance but increase memory and latency. Paper uses E=500, K=50 for LQR; E=50, K=100 for cart-pendulum.
  - PSD projection: Enforcing W ⪰ 0 via eigendecomposition adds O(n_θ^3) cost but ensures descent directions.

- **Failure signatures:**
  - Singular A matrices: Indicates insufficient exploration or feature rank deficiency. Symptoms: NaN/Inf in v, g, or W. Fix: Use pseudoinverse, add regularization λI, or increase exploration σ.
  - Non-increasing cost J(θ): May indicate poorly estimated Hessian (W nearly singular or indefinite) causing oversized steps. Fix: Reduce α_θ, check PSD projection, increase batch size.
  - High variance in Hessian across updates: Exploration noise insufficient to observe action-value curvature. Fix: Increase σ or use structured critics (e.g., MPC-based as noted in conclusion).
  - Constraint violations in cart-pendulum (soft constraints): Policy not learning to satisfy constraints. Fix: Increase constraint penalty coefficients in MPC stage cost.

- **First 3 experiments:**
  1. Reproduce LQR benchmark: Use the exact A, B matrices, policy π_θ(s) = -K(θ)s with K_0 as specified. Verify that ||θ - θ*|| converges faster than first-order DPG. This validates the full pipeline with known optimal solution.
  2. Ablate PSD projection: Run LQR with W projection disabled (use unconstrained Ŵ directly). Compare convergence rate and stability. Hypothesis: without PSD projection, updates may occasionally increase cost or diverge.
  3. Vary exploration scale σ: Test σ ∈ {0.001, 0.01, 0.1} on cart-pendulum. Measure gradient variance, Hessian condition number, and final cost J(θ*). Identify the σ range where Hessian estimation is accurate without excessive trajectory variance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the quasi-Newton actor-critic framework be effectively extended to discrete and hybrid state-action MDPs?
- Basis in paper: [explicit] The conclusion explicitly lists extending the framework to "discrete and hybrid state–action MDPs" as a primary direction for future research.
- Why unresolved: The current theoretical derivation relies on the deterministic policy gradient theorem and requires twice continuously differentiable action-value functions and policies, assumptions that do not hold for discrete action spaces or hybrid dynamics.
- What evidence would resolve it: A modified compatibility definition and update rule that handles non-differentiable or hybrid transitions, demonstrated empirically on control tasks with discrete action sets or hybrid system dynamics.

### Open Question 2
- Question: Can smoother action-value approximators, such as MPC-based critics, significantly reduce the variance observed in Hessian estimation?
- Basis in paper: [explicit] The conclusion suggests "reducing variance in Hessian estimation by leveraging smoother action–value approximators, such as model predictive control–based critics."
- Why unresolved: The simulations (Fig. 4) note that results exhibit increased variance due to second-order exploration terms, and standard function approximators may lack the regularity required for stable curvature estimation.
- What evidence would resolve it: Comparative experiments showing that integrating structured, smooth critics (like MPC) reduces the variance of the estimated curvature matrix $W$ compared to generic quadratic approximators, leading to more stable convergence.

### Open Question 3
- Question: Is the proposed quadratic compatible critic computationally scalable to high-dimensional policy parameterizations, such as deep neural networks?
- Basis in paper: [inferred] The method requires estimating a curvature matrix $W$ of size $n_\theta \times n_\theta$ and solving linear systems involving its vectorized form, but the experiments are restricted to low-dimensional policies (linear and MPC-based).
- Why unresolved: The computational and memory costs of estimating and inverting matrices that scale quadratically with the number of policy parameters become prohibitive for deep neural networks with thousands or millions of parameters.
- What evidence would resolve it: Successful application of the algorithm to high-dimensional continuous control benchmarks (e.g., humanoid locomotion) using deep networks, potentially utilizing efficient approximations like Kronecker-factoring or diagonal constraints on $W$.

## Limitations
- Empirical evaluation limited to two deterministic control tasks (LQR, cart-pendulum) without testing on nonlinear or stochastic environments
- Computational complexity scales cubically with policy dimension due to Hessian estimation and inversion
- Fixed exploration noise scale selected heuristically without systematic sensitivity analysis
- No statistical significance testing or confidence intervals provided for performance comparisons

## Confidence
- **High**: Theoretical compatibility framework and quadratic critic structure derivation (Theorem 2)
- **Medium**: LSTD estimation procedure and sequential decoupled updates implementation
- **Low**: Empirical performance claims and sample efficiency gains across diverse tasks

## Next Checks
1. **Ablate PSD projection:** Run LQR with W projection disabled (use unconstrained Ŵ directly). Compare convergence rate and stability. Hypothesis: without PSD projection, updates may occasionally increase cost or diverge.
2. **Vary exploration scale σ:** Test σ ∈ {0.001, 0.01, 0.1} on cart-pendulum. Measure gradient variance, Hessian condition number, and final cost J(θ*). Identify the σ range where Hessian estimation is accurate without excessive trajectory variance.
3. **Evaluate on nonlinear, stochastic task:** Apply the method to a pendulum swing-up or inverted pendulum with stochastic dynamics. Assess whether the quasi-Newton updates still provide faster convergence than first-order DPG, and whether the quadratic critic remains well-conditioned in the presence of noise.