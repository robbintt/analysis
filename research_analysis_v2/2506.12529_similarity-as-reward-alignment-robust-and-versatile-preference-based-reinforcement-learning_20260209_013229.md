---
ver: rpa2
title: 'Similarity as Reward Alignment: Robust and Versatile Preference-based Reinforcement
  Learning'
arxiv_id: '2506.12529'
source_url: https://arxiv.org/abs/2506.12529
tags:
- learning
- preference
- reward
- sara
- latexit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces SARA (Similarity as Reward Alignment), a preference-based
  reinforcement learning framework that learns a latent representation of preferred
  behaviors using contrastive learning and computes rewards as cosine similarities
  to this learned latent. SARA is designed to be robust to label noise and adaptable
  to various feedback formats and training paradigms.
---

# Similarity as Reward Alignment: Robust and Versatile Preference-based Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2506.12529
- **Source URL**: https://arxiv.org/abs/2506.12529
- **Reference count**: 40
- **Primary result**: Achieves 31% improvement in policy evaluation rewards over baselines with 20% label noise

## Executive Summary
SARA introduces a preference-based reinforcement learning framework that learns a latent representation of preferred behaviors using contrastive learning. Instead of predicting pairwise preference probabilities with Bradley-Terry models, SARA encodes trajectories into a latent space and computes rewards as cosine similarities to a learned preferred prototype. This approach demonstrates robustness to label noise (31% improvement over baselines at 20% noise) and versatility across different feedback formats and training paradigms.

## Method Summary
SARA uses a two-stage transformer architecture to learn a preferred behavior latent representation from noisy preference labels. Trajectories are encoded and partitioned into subsets, with SimCLR loss pulling preferred subset latents together while separating non-preferred latents. The encoder is trained with randomized subset composition each epoch for noise robustness. At inference, rewards are computed as cosine similarities to the preferred latent prototype, enabling downstream policy training with IQL on offline datasets. The method shows strong performance on D4RL benchmarks and adaptability to trajectory filtering, cross-task transfer, and online reward shaping.

## Key Results
- Outperforms or matches state-of-the-art baselines on 6 out of 8 D4RL datasets
- Achieves 31% average improvement in policy evaluation rewards over baselines with 20% label noise
- Shows up to 73% performance fluctuation reduction compared to baseline models across dataset variants
- Demonstrates versatility through trajectory filtering for imitation learning, cross-task preference transfer, and online RL reward shaping

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Representation Learning Enables Noise Tolerance
Learning a latent representation of the preferred set as a whole provides natural robustness to individual labeling errors. Trajectories are encoded into latent space and partitioned into subsets, with SimCLR loss pushing preferred subset latents together while separating non-preferred latents. Randomizing subset composition each epoch forces the encoder to learn general patterns rather than memorizing specific (potentially mislabeled) trajectories. This works when noise rate doesn't exceed the signal-to-noise threshold where preferred and non-preferred sets become statistically indistinguishable.

### Mechanism 2: Cosine Similarity Provides Dense Reward Signal Without BT Overfitting
Computing rewards as trajectory-to-preferred-prototype similarity avoids Bradley-Terry model limitations. After encoder training, a fixed preferred latent is inferred from the full preferred set, and each trajectory segment is rewarded proportional to cosine similarity. This yields dense rewards reflecting alignment with learned preferences rather than relative pairwise rankings. The approach works when the preferred set is internally consistent and the learned latent captures essential features of preferred behavior.

### Mechanism 3: Set-Based Formulation Decouples from Pairwise Dependencies
Breaking trajectory pairs into sets enables flexibility across feedback formats and training paradigms. This removes dependency on pairwise rankings, allowing integration of neutral preferences, handling sets of different sizes, and enabling non-pairwise feedback formats. The approach works when useful information in preferences is categorical (preferred vs. non-preferred) rather than ordinal (relative ranking).

## Foundational Learning

- **Bradley-Terry Model for Preferences**: Understanding why BT-based methods overfit helps motivate SARA's design choice to avoid it entirely. Quick check: Can you explain why predicting P(A ≻ B) might overfit when labels are noisy, compared to learning a direct representation of "good" trajectories?

- **SimCLR Contrastive Learning**: SARA's encoder training uses SimCLR loss; understanding the pull-apart dynamic is essential for debugging representation quality. Quick check: Given two latent vectors z_p and z_n, how does the SimCLR loss change their relative positions in embedding space?

- **Offline RL with IQL (Implicit Q-Learning)**: SARA provides rewards for downstream IQL training; understanding IQL's in-sample learning helps diagnose reward-quality issues. Quick check: Why does IQL avoid querying out-of-distribution actions, and how does this interact with learned preference rewards?

## Architecture Onboarding

- **Component map**: Transformer Encoder 1 → Subset Partitioner → Transformer Encoder 2 → SimCLR Loss → Freeze encoder → Infer z*_p → Reward computation → Downstream IQL

- **Critical path**: Preference dataset → Encoder 1 → Subset partition → Encoder 2 → SimCLR loss → Freeze encoder → Infer z*_p → Reward computation → Downstream IQL

- **Design tradeoffs**: k=2 subsets (default) trades compute for robustness; α=0 in reward formula simplifies rewards but may lose signal when preferred behaviors have high variance; no causal masking in Encoder 1 allows future information to inform current-timestep encoding.

- **Failure signatures**: Latent collapse (all z_p and z_n converge to similar regions), high variance across seeds (indicates insufficient encoder training), rewards near zero for all trajectories (check if z*_p is computed correctly).

- **First 3 experiments**: 
  1. Sanity check on synthetic data: Create small dataset with clear preferred vs. non-preferred trajectories and verify encoder separates latents in t-SNE space.
  2. Noise ablation: Sweep label noise from 0% to 30% on single D4RL dataset and plot policy evaluation rewards vs. noise rate for SARA vs. PT baseline.
  3. Hyperparameter sensitivity: Vary k (1, 2, 4, 8) and SimCLR temperature (0.05, 0.1, 0.2) on walker2d-replay and report both final rewards and training stability.

## Open Questions the Paper Calls Out

1. **Theoretical connections**: Does the cosine-similarity reward formulation have provable theoretical connections to preference modeling that extend beyond the motivational argument in Appendix A?

2. **Online adaptation**: Can SARA be effectively adapted for iterative online preference learning where the encoder must be updated continuously as new human feedback arrives?

3. **Imitation learning impact**: Does trajectory filtering using SARA's learned representations improve downstream imitation learning performance compared to unfiltered datasets?

4. **LLM alignment**: Can SARA be extended to Large Language Model alignment settings where the current formulation's assumptions about trajectory structure may not directly apply?

## Limitations

- Exact preference dataset files and preprocessing methodology are not fully specified, making exact reproduction challenging
- Reward normalization implementation details for IQL training are referenced but not provided
- Subset shuffling implementation specifics are conceptually described but not detailed

## Confidence

- **High confidence** in the core mechanism: Contrastive learning approach with subset randomization provides theoretical noise robustness, and transformer architecture is standard
- **Medium confidence** in performance claims: 31% improvement with 20% noise depends on exact noise injection methodology; 73% fluctuation observation requires verification with same baseline implementations
- **Low confidence** in generalization claims: Versatility demonstrations are briefly mentioned but not extensively validated; claim that set-based formulation enables all applications needs more empirical support

## Next Checks

1. **Noise robustness ablation**: Implement controlled experiment varying label noise from 0% to 30% on single D4RL dataset and plot SARA's performance degradation curve against PT and BT baselines to verify 31% improvement claim at 20% noise.

2. **Latent space visualization**: For small synthetic preference dataset with clear preferred vs. non-preferred patterns, visualize learned latent space (t-SNE or UMAP) before and after encoder training to verify contrastive mechanism.

3. **Subset shuffling ablation**: Run encoder training with and without subset shuffling mechanism (fixed subset assignments) and measure difference in robustness to label noise and latent space separation quality.