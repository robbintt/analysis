---
ver: rpa2
title: 'MOTOR: Multimodal Optimal Transport via Grounded Retrieval in Medical Visual
  Question Answering'
arxiv_id: '2506.22900'
source_url: https://arxiv.org/abs/2506.22900
tags:
- medical
- motor
- re-ranking
- multimodal
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces MOTOR, a multimodal retrieval and re-ranking
  approach for medical visual question answering (MedVQA). Existing retrieval-augmented
  generation (RAG) methods in MedVQA often retrieve irrelevant context, impairing
  reasoning.
---

# MOTOR: Multimodal Optimal Transport via Grounded Retrieval in Medical Visual Question Answering

## Quick Facts
- arXiv ID: 2506.22900
- Source URL: https://arxiv.org/abs/2506.22900
- Reference count: 40
- Primary result: Achieves 56.34% and 60.38% accuracy on MIMIC-CXR-VQA and Medical-Diff-VQA datasets, with 6.45% average improvement over state-of-the-art.

## Executive Summary
MOTOR introduces a retrieval-augmented generation approach for medical visual question answering that addresses the challenge of irrelevant context retrieval. The method combines grounded captions and optimal transport to align textual and visual features between queries and retrieved elements. By generating structured captions with spatial localization and re-ranking retrieved contexts using OT, MOTOR improves the factual accuracy of answers without requiring additional training. The approach demonstrates significant performance gains on two medical VQA benchmarks while maintaining interpretability through human expert validation.

## Method Summary
MOTOR operates through a four-stage pipeline: (1) generates grounded captions for query and database images using MAIRA-2, pairing textual abnormalities with bounding boxes; (2) retrieves top-k similar elements using RAD-DINO embeddings and cosine similarity; (3) re-ranks retrieved elements using optimal transport that combines text similarity (Clinical-BERT), visual similarity, and question-report similarity; (4) generates answers using frozen VLMs (LLaVA-Med or Dragonfly-Med). The method selects top-s reports based on minimal OT cost and passes them to the VLM with the query for final answer generation.

## Key Results
- Achieves 56.34% accuracy on MIMIC-CXR-VQA dataset
- Achieves 60.38% accuracy on Medical-Diff-VQA dataset
- Demonstrates 6.45% average improvement over state-of-the-art methods
- Human expert review confirms high-quality grounded captions and clinically relevant retrievals

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Grounded captions provide structured intermediate representation that reduces ambiguity in medical image retrieval.
- **Mechanism:** MOTOR uses MAIRA-2 to generate grounded captions G = {(t₁, b₁), ..., (tₙ, bₙ)}, linking textual abnormalities to spatial coordinates, allowing comparison of visual evidence rather than word co-occurrence.
- **Core assumption:** Captioning model accurately localizes and describes abnormalities in both query and database images.
- **Evidence anchors:** Abstract mentions leveraging grounded captions to align textual and visual features; section 2 describes utilizing G in re-ranking to compare query image against candidates.

### Mechanism 2
- **Claim:** Optimal Transport captures underlying cross-modal relationships better than standard cosine similarity.
- **Mechanism:** Constructs transportation cost matrix C combining question-report similarity, grounded text similarity, and visual similarity. Sinkhorn algorithm finds minimal cost to transport mass from query to retrieved element distributions.
- **Core assumption:** Lower transport cost correlates with higher clinical relevance for VLM.
- **Evidence anchors:** Abstract states re-ranks using OT to prioritize clinically relevant contexts; section 2 explains OT's significance in optimizing transportation cost matrix capturing cross-modal relationships.

### Mechanism 3
- **Claim:** Filtering irrelevant context reduces VLM hallucination and improves factual accuracy.
- **Mechanism:** Reranks and selects only top-s reports with minimal OT cost, removing distractor context that might mislead VLM.
- **Core assumption:** Initial retrieval set contains at least some relevant elements that can be surfaced by reranking.
- **Evidence anchors:** Abstract notes risks of retrieving irrelevant context degrading VLM reasoning; section 4 discusses MOTOR's success indicating necessity of fine-grained optimization techniques.

## Foundational Learning

- **Concept: Optimal Transport (Sinkhorn Algorithm)**
  - **Why needed here:** Essential for understanding reranking logic and how OT measures "distance" between probability distributions and how entropy regularization speeds this up.
  - **Quick check question:** How does Sinkhorn distance differ from standard Euclidean distance in terms of handling distribution mass?

- **Concept: Grounded Captioning**
  - **Why needed here:** Core data structure relies on pairing text with bounding boxes (tᵢ, bᵢ). Need to understand this is explicit localization task, not just global image description.
  - **Quick check question:** If a model outputs "Cardiomegaly" without a bounding box, is it a "grounded" caption in the context of MOTOR?

- **Concept: Retrieval-Augmented Generation (RAG)**
  - **Why needed here:** MOTOR is wrapper around VLM. Need to understand baseline RAG workflow (Retrieve → Generate) to see where and why "Rerank" step is inserted.
  - **Quick check question:** In standard RAG pipeline, what is risk of increasing number of retrieved documents (k) without reranking?

## Architecture Onboarding

- **Component map:** Knowledge Base (MIMIC-CXR-JPG) -> Captioning Module (MAIRA-2) -> Retriever (RAD-DINO + Cosine) -> Reranker (Sinkhorn OT) -> Generator (LLaVA-Med/Dragonfly-Med)
- **Critical path:** Optimal Transport calculation (Algorithm 1, Line 6). If cost matrix construction or Sinkhorn iteration is slow, inference pipeline stalls.
- **Design tradeoffs:**
  - Weights (α, β, δ): Tuning changes modality priority. Paper defaults to δ=0.5 (visual priority).
  - k vs s: Retrieving 10 items and keeping 5. Increasing k increases recall but adds compute overhead for OT; reducing s too much might starve VLM of context.
- **Failure signatures:**
  - Hallucinated Grounding: Captioner invents findings → OT aligns on noise → VLM generates false positives.
  - Low Re-ranking Change Rate: If OT doesn't change order much, weights may be too uniform or initial retrieval already optimal.
- **First 3 experiments:**
  1. **Sanity Check (Retrieval Only):** Run pipeline with Re-ranking = False to establish baseline accuracy.
  2. **Modality Ablation:** Set δ=0 (text only) vs β=0 (visual only) to verify multimodal combination yields reported ~6% gain.
  3. **Qualitative Inspection:** Visualize grounded bounding boxes for query vs. retrieved images for 5 random samples to ensure MAIRA-2 localizes same anatomical regions.

## Open Questions the Paper Calls Out

- **Question 1:** Can MOTOR be effectively adapted to non-radiological medical imaging modalities (e.g., CT, MRI) and generative tasks such as medical report generation?
  - **Basis in paper:** Conclusion states "Future work could explore adapting MOTOR to other medical imaging modalities and tasks, such as medical report generation."
  - **Why unresolved:** Current study validates approach exclusively on chest X-ray datasets for VQA tasks, leaving efficacy on 3D data or generation tasks unknown.

- **Question 2:** To what extent does error rate of grounded caption generator propagate through Optimal Transport re-ranking mechanism?
  - **Basis in paper:** Method relies on MAIRA-2 for grounded captions, which paper notes achieves 74% verified accuracy. Impact of remaining 26% erroneous captions on OT cost matrix calculation is not analyzed.
  - **Why unresolved:** System treats caption generator as frozen component without quantifying how caption hallucinations or localization errors affect final re-ranking relevance.

- **Question 3:** Are fixed weighting parameters (α, β, δ) optimal across varying query types, or would dynamic weighting scheme improve performance?
  - **Basis in paper:** Authors use fixed weights (0.2, 0.3, 0.5) for text, visual, and question features, identified as best configuration. However, different clinical questions may rely differently on visual vs. textual alignment.
  - **Why unresolved:** Paper presents global optimum but does not explore query-conditioned or adaptive weighting strategies.

## Limitations
- Effectiveness depends on MAIRA-2's ability to accurately localize and describe abnormalities; no quantitative evaluation of caption quality beyond human expert review of small sample.
- Optimal weights (α=0.2, β=0.3, δ=0.5) and entropy regularization (γ=1) were not extensively validated through ablation studies.
- 560-image subset of MIMIC-CXR-JPG may not capture full distribution of clinical scenarios, potentially limiting generalizability.

## Confidence

- **High confidence:** Core claim that re-ranking improves factual accuracy is well-supported by 6.45% average accuracy gain across two datasets and human expert validation of retrieved contexts.
- **Medium confidence:** Mechanism by which grounded captions enable better alignment is plausible but not empirically isolated from other factors.
- **Low confidence:** Specific OT hyperparameter choices and their optimality remain unverified beyond reported configuration.

## Next Checks
1. **Cross-dataset robustness:** Test MOTOR on completely independent medical imaging dataset (e.g., ChestX-ray14) to assess generalizability beyond MIMIC-CXR.
2. **Ablation of grounding quality:** Replace MAIRA-2 with baseline global image captioning model to quantify contribution of spatial grounding to performance gains.
3. **OT sensitivity analysis:** Systematically vary α, β, δ and γ across grid to identify optimal configurations and assess stability of performance improvements.