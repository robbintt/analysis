---
ver: rpa2
title: Model-Free Output Feedback Stabilization via Policy Gradient Methods
arxiv_id: '2601.19284'
source_url: https://arxiv.org/abs/2601.19284
tags:
- lemma
- system
- where
- algorithm
- follows
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a model-free algorithm for learning stabilizing
  static output feedback (SOF) controllers for unknown discrete-time linear systems.
  The method leverages zeroth-order policy gradient updates and a discount mechanism
  to iteratively improve the policy.
---

# Model-Free Output Feedback Stabilization via Policy Gradient Methods

## Quick Facts
- arXiv ID: 2601.19284
- Source URL: https://arxiv.org/abs/2601.19284
- Reference count: 40
- One-line primary result: A model-free algorithm using zeroth-order policy gradients and a discount mechanism to learn stabilizing static output feedback controllers for unknown discrete-time linear systems, with sample complexity $O(\log(\rho(A)) \cdot m^2 p^2 / \epsilon^4 \cdot \text{poly}(\|A\|, \|B\|, \|Q\|, \|R\|, J))$.

## Executive Summary
This paper addresses the problem of stabilizing an unknown discrete-time linear system using a static output feedback controller, where the controller only has access to system outputs (not the full state). The proposed method is model-free, learning the controller without knowing the system matrices. It leverages a discount mechanism to transform the potentially unstable stabilization problem into a sequence of finite-horizon optimizations, and uses zeroth-order policy gradient updates to iteratively improve the controller. The algorithm is proven to converge to a stabilizing policy with a sample complexity that scales polynomially with the system dimensions and the desired accuracy.

## Method Summary
The core method combines a zeroth-order policy gradient estimator with a discount annealing strategy. The zeroth-order estimator (Algorithm 1) approximates the gradient of the cost function by simulating the system with perturbed policies and using two-point function queries, thus avoiding the need for system identification. The discount mechanism starts with a small discount factor $\gamma_0 < 1/\rho(A)^2$ to ensure the initial cost is finite, even for unstable systems. After the policy gradient finds a locally optimal policy for the current $\gamma$, the discount factor is gradually increased (Algorithm 2) using a rate constrained by the current cost, effectively "annealing" the system back to its original dynamics until a stabilizing controller for the undiscounted system is found.

## Key Results
- The algorithm successfully learns a stabilizing static output feedback controller for an unknown linear system without requiring system identification.
- Theoretical analysis provides a sample complexity bound that scales as $O(\log(\rho(A)) \cdot m^2 p^2 / \epsilon^4 \cdot \text{poly}(\|A\|, \|B\|, \|Q\|, \|R\|, J))$.
- Numerical experiments on a synthetic system and a cart-pole system demonstrate the effectiveness of the approach, showing the spectral radius of the closed-loop system converging to a value below 1.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The algorithm enables learning from a trivial (zero) initial policy by transforming the stabilization problem into a sequence of finite-horizon optimizations on damped systems.
- **Mechanism:** A discount factor $\gamma < 1/\rho(A)^2$ is introduced to define a "damped" closed-loop system $\tilde{x}_{t+1} = \sqrt{\gamma}(A-BKC)\tilde{x}_t$. By keeping $\gamma$ small initially, the spectral radius of the damped system is forced below 1, rendering the initial cost finite and the policy gradient well-defined, even if the original system is unstable.
- **Core assumption:** An upper bound on the open-loop spectral radius $\rho(A)$ is known to initialize $\gamma$, and the set of stabilizing static output feedback (SOF) policies is non-empty (Assumption 2).
- **Evidence anchors:**
  - [abstract] Mentions leveraging a "discount mechanism to iteratively improve the policy."
  - [section 2.2] Defines the damped system dynamics and the cost $J_\gamma(K)$ equivalence.
  - [corpus] Related work (e.g., "Stabilizing dynamical systems via policy gradient methods") establishes the precedent of discounting for stabilization, which this paper extends to output feedback.
- **Break condition:** If the initial $\gamma$ is too large (underestimating $\rho(A)$), the initial cost may be infinite, causing gradient estimation to fail immediately.

### Mechanism 2
- **Claim:** The system approximates the gradient of the cost function $J_\gamma(K)$ without knowing the system matrices $(A, B, C)$ by utilizing system trajectory data.
- **Mechanism:** The method employs a "two-point" zeroth-order optimization scheme (Algorithm 1). It samples random policy perturbations $U_i$, simulates the system with perturbed policies $K \pm rU_i$, and uses the difference in resulting costs to construct a gradient estimate $\hat{\nabla}J_\gamma(K)$.
- **Core assumption:** The system allows for multiple rollouts (trajectories) from specific initial conditions, and the cost function is locally smooth (Lemmas 3, 5).
- **Evidence anchors:**
  - [abstract] States the method "leverages zeroth-order policy gradient updates."
  - [section 3.1] Details Algorithm 1 and Lemma 5, bounding the estimation error based on simulation time $\tau_e$ and number of trajectories $N_e$.
  - [corpus] "Model-free" approaches in neighbors (e.g., "Optimal Output Feedback Learning Control") similarly rely on trajectory data to bypass system identification.
- **Break condition:** If the number of trajectories $N_e$ is too low or perturbation $r$ is poorly scaled, gradient noise will dominate, preventing convergence to a stationary point.

### Mechanism 3
- **Claim:** The algorithm guarantees convergence to a stabilizing policy for the *original* system by iteratively "annealing" the discount factor towards 1.
- **Mechanism:** After the Policy Gradient (PG) step finds a policy $K$ with a small gradient for the current $\gamma_k$, the algorithm increases $\gamma$ via $\gamma_{k+1} = (1 + \zeta \alpha_k)\gamma_k$. The rate $\alpha_k$ is constrained by the current estimated cost to ensure that the policy remains within the stability region of the *new* (less damped) system.
- **Core assumption:** The optimization landscape, while lacking global gradient dominance, allows convergence to a stationary point that maintains stability as $\gamma \to 1$.
- **Evidence anchors:**
  - [abstract] Notes the algorithm "returns a stabilizing output feedback policy" by stretching PG boundaries without global convergence guarantees.
  - [section 3.3] Lemma 6 proves that if $\gamma'$ is sufficiently close to $\gamma$, stability is preserved; Algorithm 2 implements this update.
  - [corpus] Weak explicit discussion in neighbors on the specific *rate* of annealing for output feedback, making the paper's Lemma 6 and Theorem 2 analysis the primary evidence.
- **Break condition:** If the step size $\alpha_k$ is too aggressive (violating Lemma 6 conditions), the policy may fall outside the stability region of the next $\gamma$, causing the cost to diverge.

## Foundational Learning

- **Concept: Static Output Feedback (SOF) vs. Full State Feedback**
  - **Why needed here:** SOF assumes the controller only observes outputs $y_t = Cx_t$, not the full state $x_t$. This removes the "gradient dominance" property found in standard LQR, creating a non-convex landscape where standard PG might get stuck or fail.
  - **Quick check question:** Can you explain why a disconnected set of stabilizing policies (possible in SOF) invalidates standard global convergence proofs used in state-feedback LQR?

- **Concept: Spectral Radius and Schur Stability**
  - **Why needed here:** The core objective is to drive the spectral radius $\rho(A - BKC)$ below 1. The discount mechanism relies on explicitly manipulating the effective spectral radius via the $\sqrt{\gamma}$ factor.
  - **Quick check question:** If a system matrix $A$ has spectral radius 1.5, what is the maximum discount factor $\gamma$ such that $\sqrt{\gamma}A$ is stable?

- **Concept: Zeroth-Order Optimization (Random Search)**
  - **Why needed here:** This is the engine of the "model-free" approach. It replaces analytical derivatives with finite differences estimated from simulation data.
  - **Quick check question:** How does the "two-point" estimation method (sampling $K+rU$ and $K-rU$) reduce variance compared to a "one-point" method, specifically regarding the baseline cost?

## Architecture Onboarding

- **Component map:** System -> Two-Point Estimator -> Gradient Estimate -> Policy Gradient Update -> New Policy $K$ -> Annealer ($\gamma$ update) -> New $\gamma$ -> System

- **Critical path:**
  1. Initialize $K=0$ and $\gamma_0 < 1/\rho(A)^2$.
  2. **Estimate** cost $J$ and gradient $\nabla J$ via rollouts.
  3. **Update** $K$ using gradient descent (Inner Loop).
  4. **Check** if gradient is small enough (stationary point found).
  5. **Update** $\gamma$ (Outer Loop) and repeat until $\gamma \ge 1$.

- **Design tradeoffs:**
  - **Accuracy ($\epsilon$) vs. Samples:** Tighter convergence tolerance $\epsilon$ significantly increases sample complexity ($O(1/\epsilon^4)$).
  - **Rollout Length ($\tau_e$) vs. Stability:** Longer rollouts reduce bias in the gradient estimate but risk state explosion if the policy briefly becomes unstable during search.
  - **Annealing Rate ($\zeta$):** A higher $\zeta$ speeds up the algorithm but increases the risk of "breaking" stability during the discount factor update.

- **Failure signatures:**
  - **Gradient Explosion:** $K$ diverges rapidly; likely caused by $\gamma$ being too large for the current policy capability.
  - **Stagnation:** $\gamma$ stops increasing; likely caused by the gradient estimate variance being higher than the threshold $\epsilon$, preventing the "stationary point" condition from triggering.
  - **High Variance:** Fluctuating costs across iterations; indicates $N_e$ (number of trajectories) is too low.

- **First 3 experiments:**
  1. **Sanity Check (Known System):** Implement Algorithm 1 on a stable linear system with known matrices. Verify that the zeroth-order gradient points in the same direction as the analytical gradient.
  2. **Annealing Test (Synthetic Unstable):** Run the full Algorithm 2 on the synthetic system (Eq 31). Plot $\rho(A-BKC)$ vs. iteration to verify it stays below 1 as $\gamma$ increases.
  3. **Robustness Check (Cart-Pole):** Apply the algorithm to the cart-pole system (Eq 32). Test sensitivity by reducing the number of trajectories $N_e$ to find the breaking point where variance prevents stabilization.

## Open Questions the Paper Calls Out

- **Question:** Can the proposed model-free policy gradient framework be extended to learn stabilizing *dynamic* output feedback controllers?
  - **Basis in paper:** [explicit] The conclusion states, "Future research will explore extensions to studying the stabilization problem with dynamic output feedback controllers..."
  - **Why unresolved:** The current work is restricted to static output feedback (SOF), and the analysis relies on the specific structure of static gains.
  - **What evidence would resolve it:** Convergence guarantees and sample complexity analysis for a model-free algorithm utilizing dynamic controllers.

- **Question:** Can the discount method and zeroth-order policy gradient approach be effectively applied to stabilize unknown nonlinear dynamical systems?
  - **Basis in paper:** [explicit] The conclusion identifies the "stabilization of unknown nonlinear systems" as a direction for future research.
  - **Why unresolved:** The theoretical proofs in this paper strictly assume discrete-time linear time-invariant (LTI) system dynamics.
  - **What evidence would resolve it:** Theoretical analysis or empirical validation demonstrating stabilization of nonlinear systems without known models.

- **Question:** Is it possible to establish global convergence guarantees for policy gradient methods in this setting despite the lack of gradient dominance?
  - **Basis in paper:** [explicit] The abstract notes the proposed framework "stretches the boundary of PG methods to the problem without global convergence guarantees."
  - **Why unresolved:** The SOF landscape is non-convex and the set of stabilizing policies may be disconnected, preventing standard global convergence proofs.
  - **What evidence would resolve it:** A modified algorithm or analysis proving convergence to a global optimum or a stabilizing solution regardless of initialization.

## Limitations

- The theoretical framework relies on strong assumptions, including bounded initial states, knowledge of the system's spectral radius $\rho(A)$ for initialization, and strict constraints on the discount update rate $\zeta$.
- The sample complexity bound, while explicit, depends on unknown system constants ($L, G, \nu_k$) that make practical parameter tuning challenging without oracle access.
- The convergence guarantee is limited to finding a stationary point, not the global optimum, which is inherent to the non-convex SOF problem but not explicitly discussed as a limitation.

## Confidence

- **High:** The zeroth-order gradient estimation mechanism (Algorithm 1) and its bias/variance bounds are well-established in the literature.
- **Medium:** The convergence of the discount annealing process (Algorithm 2) to a stabilizing policy is theoretically sound but relies on strict conditions (e.g., $\zeta \alpha_k$ in Lemma 6) that may be violated in practice.
- **Low:** The practical performance on the cart-pole system is demonstrated, but the synthetic system results are limited to spectral radius tracking without showing actual state trajectories or cost convergence plots.

## Next Checks

1. **Sensitivity Analysis:** Systematically vary $\rho(A)$ and $\zeta$ to identify the breaking point where the discount update causes instability, validating Lemma 6's conditions.
2. **Oracle Comparison:** For a known system, compare the zeroth-order gradient direction from Algorithm 1 to the analytical gradient to quantify estimation error.
3. **Global Landscape Mapping:** For the synthetic system (Eq 31), compute the cost $J_\gamma(K)$ over a grid of $K$ values to visualize the non-convex landscape and identify potential stationary points that are not globally optimal.