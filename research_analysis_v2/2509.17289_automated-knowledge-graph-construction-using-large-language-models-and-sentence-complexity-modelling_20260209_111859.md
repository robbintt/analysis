---
ver: rpa2
title: Automated Knowledge Graph Construction using Large Language Models and Sentence
  Complexity Modelling
arxiv_id: '2509.17289'
source_url: https://arxiv.org/abs/2509.17289
tags:
- sentence
- sentences
- lung
- simple
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents CoDe-KG, a novel pipeline for automated knowledge
  graph construction that combines coreference resolution and syntactic sentence decomposition
  to improve relation extraction accuracy. The method systematically converts complex
  biomedical sentences into simpler forms and uses optimal prompt-model pairs for
  each sentence type.
---

# Automated Knowledge Graph Construction using Large Language Models and Sentence Complexity Modelling

## Quick Facts
- arXiv ID: 2509.17289
- Source URL: https://arxiv.org/abs/2509.17289
- Reference count: 40
- Primary result: CoDe-KG achieves 65.8% macro-F1 on REBEL, an 8-point gain over prior SOTA

## Executive Summary
This paper introduces CoDe-KG, a novel pipeline for automated knowledge graph construction that combines coreference resolution and syntactic sentence decomposition to improve relation extraction accuracy. The method systematically converts complex biomedical sentences into simpler forms using optimal prompt-model pairs for each sentence type. On benchmark datasets, CoDe-KG achieves 65.8% macro-F1 on REBEL (8 points above prior SOTA), 75.7% micro-F1 on WebNLG2, and demonstrates that integrating coreference and decomposition increases recall on rare relations by over 20%. The authors release 150K+ generated triples, 7.2K rows of sentence complexity data, 190 PubMed lung-cancer abstracts for co-reference resolution, and 900 rows for sentence conversion policies.

## Method Summary
CoDe-KG is a 4-stage pipeline: (1) Coreference resolution resolves pronouns to entities using LLM prompts with few-shot in-context learning; (2) Sentence classification labels each sentence by complexity (simple, complex, compound, compound-complex, incomplete) using a fine-tuned BERT classifier; (3) Sentence conversion decomposes non-simple sentences into simple ones using optimal prompt-model pairs with hybrid chain-of-thought and few-shot prompting; (4) Relation extraction extracts (Entity1, Relation, Entity2) triples from simple sentences using Mixtral-8x7B-Instruct with chain-of-thought prompts. The pipeline is trained and evaluated on biomedical text including 7,500 PubMed lung cancer abstracts and benchmark subsets of REBEL, WebNLG+, Wiki-NRE, and CaRB datasets.

## Key Results
- 65.8% macro-F1 on REBEL benchmark, 8 points above prior state of the art
- 75.7% micro-F1 on WebNLG2 dataset
- 99.8% exact-match accuracy on sentence simplification using hybrid prompting
- Over 20% increase in recall on rare relations when integrating coreference and decomposition
- Coreference resolution improves recall from 74.60% to 92.90% when enabled

## Why This Works (Mechanism)

### Mechanism 1: Sentence Complexity-Based Decomposition Improves Relation Extraction
LLMs struggle with complex biomedical sentences. By classifying sentences and converting complex forms into simple sentences using optimal prompt-model pairs, the pipeline reduces cognitive load on the extraction model. Each simple sentence ideally contains a single, explicit relation, making triple extraction more tractable. Evidence shows removing sentence decomposition reduces recall from 92.90% to 46.20%.

### Mechanism 2: Coreference Resolution Reduces Entity Ambiguity
Biomedical texts are dense with pronouns. The pipeline resolves these expressions to their canonical forms, ensuring extracted relations link correct entities rather than ambiguous pronouns. Evidence shows removing coreference resolution reduces recall from 92.90% to 74.60%.

### Mechanism 3: Hybrid Prompting Optimizes Decomposition Fidelity
A hybrid prompting strategy (Chain-of-Thought + Few-Shot In-Context Learning) yields higher fidelity in sentence decomposition than single-strategy prompts. CoT provides reasoning steps while FICL provides concrete output examples, grounding the model's reasoning in task-specific requirements. Evidence shows CoT+FICL achieving 99.78% exact match on compound-to-simple conversion vs. 45.81% for GIP.

## Foundational Learning

**Sentence Complexity Categories**
- Why needed: This is the core classification logic that determines downstream decomposition strategy
- Quick check: Given "The study showed positive results, and the team published a paper, although the data was limited," what is its complexity category? (Answer: Compound-Complex)

**Coreference Resolution**
- Why needed: This is a critical first stage that links pronouns to antecedents
- Quick check: In "Apple announced a new product. It will launch next month," what does "It" refer to? (Answer: The new product)

**Knowledge Graph Triples (Entity1, Relation, Entity2)**
- Why needed: This is the final output format all stages are designed to produce
- Quick check: From "Aspirin treats headaches," what is the most likely triple? (Answer: (Aspirin, treats, headaches))

## Architecture Onboarding

**Component map:** Coreference Resolution → Sentence Classification → Sentence Conversion → Relation Extraction

**Critical path:** The pipeline is serial. The most critical components for accuracy are Sentence Classification (determines downstream strategy) and Sentence Conversion (must preserve meaning). Ablation study shows removing these components crashes performance.

**Design tradeoffs:**
- Performance vs. Cost: CoT+FICL prompting is more accurate but computationally expensive
- Generalizability: Sentence classifier is fine-tuned; adapting to new domains requires retraining
- Open-source priority: Favors open-source models (Llama, Mistral) for reproducibility

**Failure signatures:**
- Low Recall: Often caused by sentence conversion step losing information
- Spurious Relations: Can result from poor coreference resolution linking wrong entities
- Pipeline Stall: If classifier fails to label a sentence, it may be dropped or mishandled

**First 3 experiments:**
1. Validate Classifier: Test fine-tuned sentence classifier on held-out set from target domain
2. Ablate Coreference: Run documents with coreference stage disabled, compare triple accuracy
3. Prompt Engineering: Experiment with few-shot examples in CoT+FICL prompt, verify clause identification

## Open Questions the Paper Calls Out

**Open Question 1:** Does CoDe-KG's sentence complexity modeling approach generalize to noisy, informal text domains such as legal documents, newswire, or social media where syntactic structures diverge from academic conventions?

**Open Question 2:** Can parameter-efficient fine-tuning (PEFT) substantially close the performance gap between open-source models and closed-source baselines (GPT-4o) on coreference resolution in dense biomedical texts?

**Open Question 3:** Can post-hoc filtering or agent-based validation loops reduce the 73% error rate attributable to spurious and missing relationship extractions without sacrificing the >20% recall gains from decomposition?

## Limitations
- Domain dependence: Sentence classifier and decomposition strategies are fine-tuned on biomedical text and may not generalize to other domains
- Benchmark representation: Datasets used may not fully represent complexity of real-world biomedical literature
- Prompt-only approach: Relies solely on prompting strategies rather than exploring parameter-efficient fine-tuning

## Confidence

**High Confidence:** The sentence decomposition mechanism's effectiveness (65.8% macro-F1 on REBEL, +8 over SOTA)
**Medium Confidence:** The coreference resolution contribution (recall increase from 74.60% to 92.90%)
**Medium Confidence:** The hybrid prompting strategy's superiority (99.8% exact-match accuracy)

## Next Checks

1. **Domain Generalization Test:** Apply CoDe-KG pipeline to non-biomedical domain (legal or news text) and measure performance degradation

2. **Error Analysis on Rare Relations:** Perform detailed error analysis on 20% recall improvement for rare relations to determine if improvement comes from genuine pattern capture vs. over-generation

3. **Ablation of Coreference Threshold:** Systematically vary cosine similarity threshold for coreference resolution and measure impact on precision-recall tradeoff