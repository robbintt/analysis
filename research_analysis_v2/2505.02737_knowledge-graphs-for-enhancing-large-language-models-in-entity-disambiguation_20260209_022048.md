---
ver: rpa2
title: Knowledge Graphs for Enhancing Large Language Models in Entity Disambiguation
arxiv_id: '2505.02737'
source_url: https://arxiv.org/abs/2505.02737
tags:
- entity
- which
- knowledge
- entities
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes enhancing Large Language Models (LLMs) with
  Knowledge Graphs (KGs) for Entity Disambiguation (ED). The core idea is to leverage
  the hierarchical class structure in KGs to progressively prune candidate entities,
  followed by adding entity descriptions to the prompt for final disambiguation.
---

# Knowledge Graphs for Enhancing Large Language Models in Entity Disambiguation

## Quick Facts
- **arXiv ID:** 2505.02737
- **Source URL:** https://arxiv.org/abs/2505.02737
- **Reference count:** 40
- **Primary result:** KG-enhanced LLM achieves 85.5% Gold F1 vs 79.3% baseline and 82.8% task-specific model

## Executive Summary
This paper proposes enhancing Large Language Models with Knowledge Graphs for Entity Disambiguation by leveraging hierarchical class structures to progressively prune candidate entities, followed by adding entity descriptions for final disambiguation. The method outperforms both non-enhanced and description-only enhanced LLMs on 10 popular ED datasets, achieving 85.5% Gold F1 compared to 82.8% for task-specific models. Using a more semantically expressive KG (YAGO vs DBpedia) further improves results by 2.4 percentage points. The approach shows better adaptability across domains than task-specific models, though error analysis reveals LLM errors as the most common failure mode.

## Method Summary
The method constructs a DAG from the KG with candidates as leaves, iteratively finding the Lowest Common Ancestor (LCA) of remaining candidates and prompting the LLM to select among the LCA's direct successors (classes or entities), pruning non-selected branches until one entity remains. When candidates share the same class, descriptions are retrieved from Wikipedia and appended to the prompt for final disambiguation. The approach uses zero-shot inference with GPT-3.5-turbo-1106, requiring no task-specific training while leveraging KG class taxonomies and entity descriptions for improved disambiguation accuracy.

## Key Results
- KG-enhanced LLM achieves 85.5% weighted Gold F1 vs 79.3% baseline and 82.8% task-specific model
- YAGO KG improves results by 2.4 percentage points over DBpedia due to deeper class hierarchy
- Better adaptability across domains compared to task-specific models
- Mean 2.2 pruning iterations required, with no significant increase for deeper graphs

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical class-based pruning of candidate entities via KG taxonomies improves disambiguation accuracy compared to direct LLM selection. The algorithm constructs a DAG from the KG with candidates as leaves, iteratively finding the LCA of remaining candidates, then prompting the LLM to select among the LCA's direct successors, pruning non-selected branches until one entity remains. Core assumption: LLMs can reliably distinguish between entity types/classes based on document context even when they struggle with direct entity selection. Break condition: When KG class annotations are missing, incorrect, or inconsistent for relevant entities.

### Mechanism 2
Augmenting prompts with entity descriptions at the final disambiguation stage reduces hallucination and handles unseen entities. When candidates share the same class, descriptions are retrieved from Wikipedia (truncated at 250 characters) and appended to a RAG-style prompt for entity-level selection. Core assumption: External descriptions contain disambiguating facts not reliably present in the LLM's training data. Break condition: When entities are genuinely ambiguous even with descriptions, or when descriptions lack distinguishing information.

### Mechanism 3
More semantically expressive KGs with deeper, finer-grained class hierarchies improve pruning effectiveness. YAGO (819,292 classes, avg depth 6.61) provides more branching points for disambiguation than DBpedia (760 classes, avg depth 3.51), enabling more granular class distinctions earlier in pruning. Core assumption: Finer class granularity maps better to how humans conceptually distinguish entities in context. Break condition: When granularity exceeds contextual cues (OKE datasets with generic occupations cause YAGO to underperform DBpedia).

## Foundational Learning

- **Concept: Entity Linking Pipeline (Mention Detection → Candidate Generation → Entity Disambiguation)**
  - Why needed here: The paper specifically targets the ED step, assuming candidates are pre-generated. Misunderstanding this leads to incorrectly scoping the method.
  - Quick check question: If you have a mention "Phoenix" in a sports article, which step determines whether it refers to the city or the basketball team?

- **Concept: Knowledge Graph Ontology vs. Instance Data**
  - Why needed here: The method leverages the class taxonomy (ontology structure), not entity relationship triples. This distinguishes it from other KG-enhanced LLM approaches.
  - Quick check question: Would querying "Paris is the capital of France" from a KG help this method? Why or why not?

- **Concept: Zero-shot Inference vs. Task-specific Training**
  - Why needed here: The paper positions itself against trained models like ReFinED. Understanding this tradeoff is critical for interpreting the adaptability vs. raw performance results.
  - Quick check question: Why might a zero-shot approach outperform a trained model on out-of-domain datasets like KORE?

## Architecture Onboarding

- **Component map:**
  DAG Constructor -> Pruning Engine -> LLM Interface -> Description Retriever -> KG Connector

- **Critical path:**
  1. Receive candidate set C (size k, typically 5-30) + mention m + document d
  2. Query KG for class taxonomy of all candidates
  3. Construct and preprocess DAG G
  4. While |candidates| > 1: Find LCA → Determine case → Prompt LLM → Prune G
  5. Return remaining entity

- **Design tradeoffs:**
  - YAGO vs. DBpedia: YAGO provides 2.4 pp improvement but may over-complicate generic occupation disambiguation
  - Iteration depth vs. accuracy: Mean 2.2 iterations; deeper graphs don't significantly increase calls
  - LLM choice: GPT-3.5 chosen for cost/reasoning balance; GPT-4 reduces LLM errors
  - Assumption: Graph manipulation overhead is "two orders of magnitude lower than LLM calls"

- **Failure signatures:**
  - LLM errors (12/21 analyzed): Missed context (5), wrong class interpretation (7)
  - KG errors (3/21): Missing or incorrect class annotations
  - Ambiguous context (5/21): Insufficient signal even with correct method
  - Ground truth errors (1/21): Dataset annotation issues

- **First 3 experiments:**
  1. Baseline validation: Run non-enhanced baseline vs. full method on KORE dataset to quantify improvement on highly ambiguous cases
  2. KG expressivity ablation: Compare YAGO vs. DBpedia on OKE15/OKE16 to observe where simpler ontologies outperform deeper ones
  3. Error categorization audit: Manually classify 50 failures across datasets to determine whether investing in stronger LLMs or better KG coverage would yield higher ROI

## Open Questions the Paper Calls Out

### Open Question 1
To what extent does using more powerful LLMs mitigate the reasoning errors identified as the primary failure mode in the hierarchical pruning approach? The Conclusion states that "the usage of more powerful LLMs could be studied" as a future line of work to help disambiguate difficult mentions. This is unresolved because the error analysis identified "LLM errors" as the most frequent failure, but the main experiments only utilized GPT-3.5.

### Open Question 2
Is there a point of diminishing returns where increased semantic granularity in the Knowledge Graph hinders the disambiguation process? Section 4.3 discusses the "KG Expressivity Impact," noting that YAGO's deeper taxonomy improves results over DBpedia, but requires preprocessing to manage complexity. This is unresolved because while deeper graphs provided better results here, the paper does not determine if excessive granularity could eventually confuse the LLM or unnecessarily increase the number of pruning iterations.

### Open Question 3
How robust is the hierarchical pruning method when the initial candidate set is noisy or fails to include the ground truth entity? The method relies on pre-computed candidate sets and the "inKB" metric assumes the gold entity is present in the graph. This is unresolved because the pruning algorithm is designed to select a leaf node; it is unclear how the method handles retrieval failures where the correct entity is missing from the initial candidate list.

## Limitations
- Relies heavily on pre-generated candidate sets from ChatEL and accurate KG class hierarchies
- Performance ceiling constrained by LLM capabilities rather than KG pruning approach
- 2.4 percentage point gain from YAGO over DBpedia is modest given YAGO's significantly more complex structure
- May struggle with domains requiring fine-grained semantic distinctions not captured in available KGs

## Confidence
- **High Confidence:** The hierarchical pruning mechanism works as described and provides measurable improvements over non-enhanced LLMs (85.5% Gold F1 vs 79.3% baseline)
- **Medium Confidence:** The claim that YAGO's deeper hierarchy consistently improves performance is supported but has domain-dependent exceptions (OKE datasets with generic occupations)
- **Low Confidence:** The assertion that LLM errors are the primary bottleneck is based on a small sample (21 errors analyzed)

## Next Checks
1. **LLM Scaling Validation:** Run the full method on identical datasets using GPT-4 instead of GPT-3.5-turbo-1106 to quantify the expected performance ceiling from LLM improvements alone
2. **KG Expressivity Stress Test:** Create controlled experiments varying class hierarchy depth and granularity on synthetic datasets to determine optimal pruning depth vs. accuracy tradeoffs
3. **Domain Adaptation Benchmark:** Test the method on datasets from underrepresented domains (medical, legal, technical) to validate the claimed adaptability advantage over task-specific models