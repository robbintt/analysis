---
ver: rpa2
title: 'RL from Physical Feedback: Aligning Large Motion Models with Humanoid Control'
arxiv_id: '2506.12769'
source_url: https://arxiv.org/abs/2506.12769
tags:
- motion
- tracking
- motions
- policy
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of translating text-driven human
  motions into executable actions for humanoid robots, focusing on generating physically
  feasible motions while maintaining semantic alignment with text instructions. The
  authors propose Reinforcement Learning from Physical Feedback (RLPF), a novel framework
  that fine-tunes text-conditioned motion generation models using physical feasibility
  feedback from a motion tracking policy and semantic alignment verification.
---

# RL from Physical Feedback: Aligning Large Motion Models with Humanoid Control

## Quick Facts
- arXiv ID: 2506.12769
- Source URL: https://arxiv.org/abs/2506.12769
- Reference count: 12
- Achieves 0.95 success rate on CMU test set in IsaacGym, compared to 0.43 for base model

## Executive Summary
This paper addresses the challenge of translating text-driven human motions into executable actions for humanoid robots, focusing on generating physically feasible motions while maintaining semantic alignment with text instructions. The authors propose Reinforcement Learning from Physical Feedback (RLPF), a novel framework that fine-tunes text-conditioned motion generation models using physical feasibility feedback from a motion tracking policy and semantic alignment verification. RLPF employs a motion tracking policy to assess feasibility in a physics simulator and generates rewards for fine-tuning the motion generator, while an alignment verification module preserves semantic fidelity to text instructions. Extensive experiments demonstrate that RLPF significantly outperforms baseline methods in generating physically feasible motions while maintaining semantic correspondence with text instructions, enabling successful deployment on real humanoid robots.

## Method Summary
The RLPF framework fine-tunes text-to-motion models through three stages: (1) pre-training motion tokenizer and Llama-2-7B on MotionX dataset, (2) training Exbody2-style teacher-student tracking policy in IsaacGym using PPO and DAgger, and (3) fine-tuning with GRPO using tracking reward from student policy and alignment verification from contrastive encoders. Motion retargeting converts SMPL parameters to robot kinematics via gradient optimization. The GRPO optimizer groups 20 samples per text, computing relative advantages from binary tracking success and alignment penalties to update LLM weights while preserving semantic meaning.

## Key Results
- RLPF achieves 0.95 success rate on CMU test set in IsaacGym vs 0.43 for base model
- RLPF-w/o track drops to 0.32 success rate, demonstrating importance of physics feedback
- RLPF-w/o align collapses to standing motions, showing alignment verification prevents semantic collapse
- Real-world deployment on Unitree G1 validates sim-to-real transfer for forward walking

## Why This Works (Mechanism)

### Mechanism 1: Physics-Grounding as Reward Signal
Routing generated motions through a physics simulator produces a feasibility signal that directly shapes the motion generator's output distribution. The motion tracking policy attempts to execute each retargeted motion in IsaacGym, yielding binary success/failure rewards that guide GRPO optimization toward physically plausible token sequences. This sparse signal averaged over groups provides relative advantages for fine-tuning.

### Mechanism 2: Contrastive Alignment Prevents Semantic Collapse
Joint optimization with alignment verification preserves text-motion correspondence while physical rewards improve feasibility. Pre-trained text and motion encoders embed both modalities into shared space, computing penalties that act as soft constraints during RL, preventing deviation from semantic intent while allowing physical modifications.

### Mechanism 3: Teacher-Student Distillation for Deployable Policies
A two-stage training pipeline yields tracking policies that transfer from simulation to real robots. Stage 1 trains teacher policy with privileged information via PPO, while stage 2 distills into student using only observable history via DAgger-style imitation learning, bridging the sim-to-real gap.

## Foundational Learning

- **Vector Quantization (VQ-VAE) for Motion Tokens**: Discretizes motion into codebook indices enabling LLM-backbone generation. Quick check: Can you explain why discretizing motion enables LLM-backbone generation but may limit motion resolution?

- **Proximal Policy Optimization (PPO)**: Baseline RL algorithm for training tracking policy. Quick check: What problem does the clipping term in PPO solve, and why does GRPO eliminate the critic?

- **Motion Retargeting (SMPL to Robot Kinematics)**: Converts human motions to robot-executable actions via hierarchical optimization. Quick check: Why is decoupling shape adaptation from pose transfer important for physical plausibility?

## Architecture Onboarding

- **Component map**: Text → LLM → motion tokens → tokenizer decode → retargeting → tracking policy (sim) → success/failure reward + alignment scores → GRPO update LLM weights

- **Critical path**: The offline evaluation step (tracking in IsaacGym) is the bottleneck—each reward query requires a full simulation rollout.

- **Design tradeoffs**: GRPO vs PPO (simpler but higher compute), binary vs dense tracking rewards (sparse but interpretable), frozen vs learnable alignment encoders (prevents reward hacking but may not adapt).

- **Failure signatures**: Semantic collapse (motions ignore text), low success despite training (tracking policy overfitting), sim-to-real gap (student policy fails on real robot).

- **First 3 experiments**: (1) Reproduce tracking policy baseline with teacher→student on AMASS subset, (2) Ablate alignment weight across β∈{0.5, 1, 2, 4} on CMU test set, (3) Cross-dataset generalization: train on CMU, evaluate on AMASS test split.

## Open Questions the Paper Calls Out

The paper identifies two key limitations: (1) the framework's generalization is constrained by the frozen tracking policy, suggesting joint training with adaptable tracking policy as future direction, and (2) scalability to arbitrary text instructions remains unverified beyond qualitative examples.

## Limitations

- Sim-to-real transfer scope limited to forward walking motions, with scalability to arbitrary text instructions unverified
- Binary success/failure reward may oversimplify feasibility landscape for complex contact-rich motions
- Cross-dataset generalization lacks systematic evaluation across held-out datasets

## Confidence

**High Confidence**: Physics simulation feedback mechanism (supported by ablation showing 0.95→0.32 success drop), teacher-student distillation framework (established methodology), motion retargeting pipeline (standard practice)

**Medium Confidence**: Contrastive alignment mechanism (primarily qualitative ablation evidence), GRPO optimization specifics (some details missing from main text)

**Low Confidence**: Cross-dataset generalization claims (lack systematic evaluation), scalability to arbitrary text instructions (supported only by qualitative examples)

## Next Checks

1. **Ablation of Reward Components**: Systematically vary tracking reward weight (currently 10) and alignment reward weight (currently 2) across full CMU test set. Plot success rate vs semantic alignment metrics to identify Pareto frontier.

2. **Cross-Dataset Generalization Test**: Train RLPF on CMU dataset, evaluate on held-out AMASS test split. Measure success rate, MPJPE, and semantic alignment to quantify dataset-specific vs general motion understanding.

3. **Complexity Scaling Analysis**: Generate motions with varying semantic complexity (simple: "walk forward"; medium: "walk while waving"; complex: "spin and jump while waving"). Measure success rate and MPJPE across complexity levels to establish operational boundaries.