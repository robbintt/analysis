---
ver: rpa2
title: Large language models struggle with ethnographic text annotation
arxiv_id: '2601.12099'
source_url: https://arxiv.org/abs/2601.12099
tags:
- features
- feature
- human
- llms
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) struggle to accurately annotate ethnographic
  texts, a critical bottleneck in cross-cultural research. We evaluated 7 state-of-the-art
  LLMs on their ability to annotate 121 ritual features across 567 ethnographic excerpts.
---

# Large language models struggle with ethnographic text annotation

## Quick Facts
- arXiv ID: 2601.12099
- Source URL: https://arxiv.org/abs/2601.12099
- Reference count: 40
- Primary result: LLMs achieved F1 scores below 0.41 on ethnographic annotation, well below levels required for reliable automated annotation

## Executive Summary
Large language models (LLMs) cannot reliably annotate ethnographic texts, a critical bottleneck for cross-cultural research automation. This study evaluated seven state-of-the-art LLMs on their ability to annotate 121 ritual features across 567 ethnographic excerpts from 73 cultures. Performance was consistently poor, with the best models achieving F1 scores below 0.41. Features requiring ordinal distinctions and interpretive inference proved particularly difficult, suggesting LLMs cannot yet substitute for human expertise in ethnographic annotation. The study highlights the need for cautious validation and reserving automated approaches for well-defined, explicitly described features.

## Method Summary
The study evaluated seven LLMs (including GPT-5 Nano, Claude Sonnet 4.5, Llama 3.2, and GPT-OSS 120B) on zero-shot annotation of 121 ritual features across 567 ethnographic texts from the eHRAF database. Texts were annotated using three prompting strategies: single-feature prompts, multi-task prompting, and ensemble sampling with 10 repetitions. Performance was measured against human expert annotations using macro-averaged F1 scores, precision, recall, and Cohen's κ for inter-coder reliability. Mixed-effects models analyzed variance sources including text length, feature type, and cultural region.

## Key Results
- All seven tested LLMs achieved F1 scores below 0.41, with small models (Llama 3.2) performing particularly poorly
- Text length negatively impacted performance (β=-0.015, p < .001), with longer excerpts yielding more false positives
- Multi-class features had 90% lower odds of correct detection compared to binary features
- Features requiring interpretive inference and ordinal distinctions proved especially difficult for models

## Why This Works (Mechanism)

### Mechanism 1
Human inter-coder reliability constrains LLM annotation performance as an approximate ceiling. Features that human experts disagree on (low Cohen's κ) are also difficult for LLMs. When annotation tasks are inherently ambiguous due to interpretive indeterminacy, no model improvement can resolve disagreements that stem from the task itself.

### Mechanism 2
Text length degrades annotation accuracy through spurious pattern matching. Longer texts provide more surface-level cues that LLMs match against feature descriptions without genuine understanding, leading to elevated false positive rates. Each SD increase in log-transformed character length reduced odds of correct rejection by 42%.

### Mechanism 3
Ensemble certainty (agreement across repetitions) predicts annotation accuracy for larger models. When models produce consistent outputs across multiple runs, this signals higher confidence that the annotation is correct. Certainty-accuracy correlation was r=0.36 for GPT-OSS 120B but near-zero for smaller models.

## Foundational Learning

**F1 Score as harmonic mean of precision and recall**
Why needed: Paper reports F1 ranging 0.12–0.41; understanding why low F1 matters requires grasping both false positives and false negatives.
Quick check: If a model predicts "present" for everything, what happens to precision vs. recall?

**Cohen's Kappa (κ) for inter-coder reliability**
Why needed: Human κ sets ceiling for LLM performance; κ≥0.80 is "substantial agreement," κ<0.40 is "poor."
Quick check: Why use κ instead of raw agreement percentage?

**Binary vs. multi-class classification difficulty**
Why needed: Multi-class features had 90% lower odds of correct detection than binary; ordinal distinctions proved particularly difficult.
Quick check: Why might distinguishing among 5 arousal levels be harder than presence/absence?

## Architecture Onboarding

**Component map:** Ethnographic texts → Feature definitions (115 morphospace + 6 synchrony) → LLM prompt → Annotation output → F1 evaluation against human ground truth

**Critical path:** Prompt design → Model selection → Multi-task prompting → Ensemble sampling → Certainty-weighted quality filtering

**Design tradeoffs:** Zero-shot accessibility vs. fine-tuning performance; open-source transparency vs. proprietary capability; single-feature focus vs. multi-task context

**Failure signatures:**
- Small models (Llama 3.2): High recall, near-zero precision (predicts present for everything)
- Long texts: False positive rates spike
- Multi-class features: 90% detection failure
- Regional bias: North America/Oceania underperform; Europe overperforms for web-enabled models

**First 3 experiments:**
1. Replicate with structured codebooks containing illustrative examples per feature category to test whether detailed definitions narrow human-LLM gap
2. Segment long texts into passages and annotate independently, then aggregate, to isolate whether length or passage-relevance drives errors
3. Fine-tune mid-sized open model (e.g., GPT-OSS 120B equivalent) on 10% of human-annotated data to establish upper bound for domain adaptation

## Open Questions the Paper Calls Out

**Open Question 1**
Does the implementation of structured codebooks with detailed examples significantly narrow the performance gap between LLMs and human annotators? The authors note they relied on zero-shot and multi-task prompting "without extensive prompt engineering or the use of structured codebooks" and suggest "future work should explore whether codebook-based prompting narrows the gap with human performance."

**Open Question 2**
To what extent does the cultural distance between LLM training data and the target ethnographic texts drive performance disparities across geographic regions? The authors found performance was significantly worse for texts from Oceania and North America, but state: "Whether this reflects differences in ethnographic writing style, ritual complexity, or cultural distance from training data remains unclear."

**Open Question 3**
Can domain-specific fine-tuning enable LLMs to reliably detect features requiring interpretive inference, such as "psychological discomfort" or "arousal"? The authors acknowledge they "did not fine-tune any models" and note that fine-tuning "has been shown to improve performance on domain-specific annotation tasks."

## Limitations
- Human ground truth annotations were not fully accessible, requiring "reasonable request" access that limits independent verification
- Proprietary model versions (GPT-5 Nano, Claude Sonnet 4.5) may have changed since the study
- Feature definitions for 115 morphospace features were incomplete in the paper

## Confidence
- High confidence: Core finding that LLMs struggle with ethnographic annotation (F1<0.41 across 7 models, p<0.001)
- Medium confidence: Mechanism linking human inter-coder reliability to LLM performance ceiling (r=0.61, but correlation doesn't prove causation)
- Medium confidence: Text length negatively impacts accuracy (β=-0.015, p<0.001) though mechanism remains speculative
- Low confidence: Ensemble certainty predicts accuracy for larger models (r=0.36 for GPT-OSS 120B but absent for smaller models)

## Next Checks
1. Replicate with complete feature definitions and stratified evaluation separating detection vs. specificity to confirm class imbalance effects
2. Segment long texts into passages and annotate independently to isolate whether length or passage-relevance drives false positives
3. Fine-tune mid-sized open model on 10% of human-annotated data to establish upper bound for domain adaptation