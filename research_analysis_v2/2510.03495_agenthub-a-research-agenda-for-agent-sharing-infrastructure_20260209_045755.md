---
ver: rpa2
title: 'AgentHub: A Research Agenda for Agent Sharing Infrastructure'
arxiv_id: '2510.03495'
source_url: https://arxiv.org/abs/2510.03495
tags:
- agents
- software
- agent
- https
- agenthub
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes AgentHub, a research agenda for building a\
  \ registry infrastructure to support the discovery, evaluation, and governance of\
  \ LLM-based agents. The authors identify key challenges unique to agents\u2014such\
  \ as capability clarity, lifecycle transparency, interoperability, governance, security,\
  \ and workflow integration\u2014that are not fully addressed by existing software\
  \ registries or emerging agent protocols."
---

# AgentHub: A Research Agenda for Agent Sharing Infrastructure

## Quick Facts
- **arXiv ID:** 2510.03495
- **Source URL:** https://arxiv.org/abs/2510.03495
- **Reference count:** 40
- **Key outcome:** Proposes AgentHub, a research agenda for building registry infrastructure to support discovery, evaluation, and governance of LLM-based agents, addressing challenges unique to agents like capability clarity, lifecycle transparency, and interoperability.

## Executive Summary
This paper identifies critical gaps in existing software registry ecosystems when applied to LLM-based agents. While npm, PyPI, and Hugging Face provide foundational models for package distribution, they fail to address the unique challenges of autonomous, evolving agentsâ€”including capability ambiguity, lifecycle management, and cross-protocol interoperability. AgentHub proposes a comprehensive research agenda centered on standardized capability schemas, signed manifests, verifiable evidence pipelines, and transparent governance to enable safe, scalable agent sharing. The vision extends beyond simple distribution to create an ecosystem where agents can be discovered, trusted, and composed as seamlessly as software libraries.

## Method Summary
The paper employs qualitative analysis and synthesis of existing software engineering practices applied to agentic systems. Rather than implementing a system, the authors conduct a conceptual analysis drawing from mature ecosystems like npm and PyPI while identifying gaps specific to autonomous agents. The methodology involves mapping challenges from traditional software registries to the agent domain and proposing theoretical solutions without empirical validation. The research agenda serves as a blueprint for future implementation efforts rather than presenting experimental results.

## Key Results
- Identifies three core mechanisms for trustworthy agent sharing: standardized capability schemas with evidence pipelines, signed manifests with lifecycle metadata, and canonical manifests for cross-protocol interoperability
- Highlights unique agent-specific challenges including autonomous composition risks, dependency confusion attacks, and the "Leftpad" cascade effect
- Proposes a hybrid governance model combining open publication with security vetting to balance growth and safety

## Why This Works (Mechanism)

### Mechanism 1: Standardized Capability Schemas with Evidence Pipelines
If implemented, this mechanism would reduce "abuse of ambiguity" (e.g., typosquatting, over-privileged agents) by enforcing machine-readable declarations of intent and behavior before distribution. The registry enforces publish-time validation where agents must submit a structured manifest (capabilities, I/O, permissions) linked to re-executable evidence traces (e.g., benchmark logs). This shifts trust from "popularity" to "verifiable proof," analogous to Android's permission model but with automated auditing.

**Core assumption:** Agents behave deterministically enough during evidence generation for benchmarks to serve as reliable proxies for runtime behavior.

**Evidence anchors:**
- [section 3.2.1] Proposes "standardizing machine-readable capability schemas" and "re-executable (idempotent) evidence pipelines"
- [section 3.2.5] Notes that "unclear manifests let adversaries mimic popular entries"
- [corpus] Related work (SAMEP, arXiv:2507.10562) emphasizes secure context sharing, supporting the need for rigorous permission schemas

**Break condition:** If agents exhibit high behavioral drift (non-determinism) where past evidence fails to predict future actions, the schema serves only as a weak signal rather than a guarantee.

### Mechanism 2: Signed Manifests and Lifecycle Metadata
Cryptographically signed manifests combined with explicit lifecycle states (active, deprecated, revoked) mitigate supply chain risks inherent in autonomous composition. Adapting standards like SLSA, the registry tracks the provenance of an agent's build, models, and datasets. Lifecycle metadata ensures that "zombie" dependencies are flagged, preventing agents from composing with abandoned or compromised predecessors.

**Core assumption:** The ecosystem adopts a unified standard for identity and key management (e.g., via ANS) so that revocation and deprecation signals propagate faster than agent execution cycles.

**Evidence anchors:**
- [section 3.2.2] Argues for "lifecycle metadata standards with clear states" to combat "dependency concentration and abandonment"
- [section 3.2.5] Recommends "signed manifests and provenance attestations" to secure the supply chain
- [corpus] *Infrastructure for AI Agents* (arXiv:2501.10114) highlights the general need for safety infrastructure

**Break condition:** If key management is centralized and compromised, or if mirrors fail to sync lifecycle states within the "freshness window," the system risks serving revoked agents.

### Mechanism 3: Canonical Manifests for Cross-Protocol Interoperability
A unified "core" manifest format enables discovery and composition across fragmented protocols (e.g., MCP, A2A, ANS). The registry uses a canonical schema (the "core") with protocol-specific extensions ("dialects"). Declarative adapters translate between the registry's internal representation and the native protocols of various agent frameworks, preventing vendor lock-in.

**Core assumption:** The rate of change in underlying agent protocols (MCP, A2A) slows sufficiently for stable adapters to be maintained by the community.

**Evidence anchors:**
- [section 3.2.3] Proposes a "compact capability ontology and a canonical manifest" to solve the "One Schema, Many Dialects" problem
- [abstract] Identifies that current infrastructure is "fragmented" and focuses narrowly on "protocol negotiation"
- [corpus] *Planet as a Brain* (arXiv:2504.14411) discusses "AgentSites," implying a future need for cross-protocol discovery hubs

**Break condition:** If a dominant protocol emerges that refuses to adhere to open standards, the registry risks becoming a niche archive rather than a central hub.

## Foundational Learning

- **Concept: Software Supply Chain Security (SBOM & Provenance)**
  - **Why needed here:** The paper builds directly on lessons from npm and PyPI. You cannot understand the proposed "Evidence" or "Trust" sections without understanding how Supply-chain Levels for Software Artifacts (SLSA) and Software Bill of Materials (SBOM) function in traditional software.
  - **Quick check question:** Can you explain the difference between a simple version lock-file and a signed provenance attestation?

- **Concept: Agent Protocols (MCP, A2A, ANS)**
  - **Why needed here:** Section 2.2 and 3.2.3 assume familiarity with the "stack" of agent infrastructure. MCP handles tool connectivity; ANS handles naming; A2A handles inter-agent behavior. AgentHub aims to sit *above* these as the registry layer.
  - **Quick check question:** If MCP is the "USB port" for tools, what is AgentHub's role in relation to it? (Answer: The library catalog where you find devices that use that port).

- **Concept: Typosquatting and Dependency Confusion**
  - **Why needed here:** The paper explicitly maps these software supply chain attacks to the agent domain (Section 3.2.1, 3.2.5). Agents autonomously installing other agents amplify these threats.
  - **Quick check question:** Why is a "typosquatting" attack potentially more dangerous in an autonomous agent ecosystem than in a standard npm environment? (Answer: Speed and scale of autonomous propagation).

## Architecture Onboarding

- **Component map:** The Manifest -> The Evidence Store -> The Resolver -> The Governance Engine

- **Critical path:** The definition of the **Canonical Capability Schema** (Section 3.2.3). If the schema is too rigid, it stifles innovation; if too loose, it fails to provide security guarantees. All other components (Resolver, Evidence Store) depend on this schema being stable and expressive.

- **Design tradeoffs:**
  - **Open vs. Curated:** The paper suggests a hybrid model (Section 3.2.4). You must decide: allow instant publication (npm style) to maximize growth, or enforce vetting (App Store style) to maximize safety?
  - **Centralized vs. Federated:** While the paper notes most registries are centralized, it acknowledges the need for "federated operations" to prevent single points of failure.

- **Failure signatures:**
  - **The "Leftpad" Cascade:** A base agent is revoked or abandoned, causing downstream multi-agent systems to fail spontaneously (Section 3.2.2).
  - **Popularity Feedback Loops:** Agents autonomously boosting the stats of mediocre or malicious agents (Section 3.2.6), creating a "blind trust" ecosystem.

- **First 3 experiments:**
  1. **Schema Drafting:** Create a minimal "AgentManifest" spec that maps one protocol (e.g., MCP) to a generic capability ontology. Test if humans and machines can accurately predict agent behavior from this manifest alone.
  2. **Evidence Pipeline Simulation:** Design a CI/CD pipeline for a simple agent that generates a "trace" of its execution. Attempt to verify this trace cryptographically to establish a "proof of behavior."
  3. **Discovery Benchmark:** Build a mock registry with 100 agents (some maliciously tagged) and measure precision/recall of a "Capability-Based Search" vs. "Keyword Search" to validate Section 3.2.6's concerns about popularity bias.

## Open Questions the Paper Calls Out
None

## Limitations
- The specific format and enforcement of the "evidence pipeline" for verifiable agent behavior is not defined
- The paper does not adequately address the economic or governance challenges of a federated registry system
- Lacks empirical validation of proposed mechanisms, relying instead on logical extrapolation from existing software registries

## Confidence
- **High:** The identification of unique challenges in the agent domain (non-determinism, autonomous composition) is well-founded
- **Medium:** The proposed mechanisms (signed manifests, lifecycle metadata) are sound in principle but lack implementation details
- **Low:** The specific format and enforcement of the "evidence pipeline" for verifiable agent behavior is not defined

## Next Checks
1. **Capability Schema Workshop:** Convene a cross-organizational working group to draft a minimal, actionable capability ontology. The goal is to produce a concrete JSON schema that can describe a simple agent (e.g., a web scraper) and be validated by both humans and machines. Measure success by whether two independent teams can accurately predict agent behavior from the manifest alone.

2. **Lifecycle State Simulation:** Build a simulation of a multi-agent system where base agents can be revoked or deprecated. Measure the "blast radius" of a single revocation event to quantify the risk of the "Leftpad" cascade described in Section 3.2.2. This will provide empirical grounding for the need for robust lifecycle metadata.

3. **Cross-Protocol Adapter Test:** Implement a simple declarative adapter that translates an MCP tool description into the proposed canonical manifest format. Attempt to round-trip this information (manifest back to MCP) without semantic loss. This will validate the feasibility of the "One Schema, Many Dialects" approach and expose potential brittleness in the abstraction layer.