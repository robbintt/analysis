---
ver: rpa2
title: Grounding and Enhancing Informativeness and Utility in Dataset Distillation
arxiv_id: '2601.21296'
source_url: https://arxiv.org/abs/2601.21296
tags:
- dataset
- infoutil
- shapley
- methods
- distillation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a principled framework for dataset distillation,
  addressing efficiency and interpretability challenges in existing methods. The proposed
  InfoUtil framework balances informativeness and utility through game-theoretic Shapley
  value attribution and gradient-norm-based sample selection.
---

# Grounding and Enhancing Informativeness and Utility in Dataset Distillation

## Quick Facts
- arXiv ID: 2601.21296
- Source URL: https://arxiv.org/abs/2601.21296
- Reference count: 40
- Primary result: InfoUtil achieves 6.1% performance improvement over state-of-the-art on ImageNet-1K with 50x lower time costs and 100x smaller memory usage.

## Executive Summary
This paper introduces InfoUtil, a principled framework for dataset distillation that addresses efficiency and interpretability challenges in existing methods. The framework balances informativeness and utility through game-theoretic Shapley value attribution and gradient-norm-based sample selection. By extracting key features from samples and selecting globally influential ones, InfoUtil achieves superior performance while maintaining strong cross-architecture generalization and dramatically reduced computational costs.

## Method Summary
InfoUtil operates in two stages: first, informativeness maximization using Shapley Value attribution to extract key information from samples through patch selection with noise injection for diversity; second, utility maximization using gradient-norm-based sample selection to identify the most influential training samples. The method generates compact synthetic datasets that enable comparable model performance to training on full datasets while achieving 50x lower time costs and 100x smaller memory usage compared to training-based approaches.

## Key Results
- 6.1% performance improvement over state-of-the-art on ImageNet-1K
- 50x lower time costs and 100x smaller memory usage compared to training-based approaches
- Strong cross-architecture generalization (e.g., 19.8% top-1 accuracy with ResNet-18→Swin-V2-Tiny transfer)
- Shapley attribution outperforms Grad-CAM by 13.49% on ImageNet-1K at IPC=10

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shapley Value attribution identifies the most semantically meaningful image patches better than random cropping or gradient-based heuristics.
- Mechanism: Each image patch is treated as a "player" in a cooperative game where the neural network is the characteristic function. The Shapley value fairly distributes attribution by computing each patch's marginal contribution across all possible coalitions, satisfying Linearity, Dummy, Symmetry, and Efficiency axioms. This yields an attribution heatmap guiding crop selection.
- Core assumption: The neural network's predictive behavior can be meaningfully decomposed into additive contributions from input patches.
- Evidence anchors:
  - [abstract]: "game-theoretic informativeness maximization using Shapley Value attribution to extract key information from samples"
  - [Page 4]: "the Shapley Value is the unique attribution method that satisfies the four key axioms"
  - [Page 9, Table 8]: Shapley outperforms Grad-CAM by 13.49% on ImageNet-1K at IPC=10

### Mechanism 2
- Claim: Gradient norm serves as a computationally tractable upper bound for sample utility, enabling efficient selection of training-influential samples.
- Mechanism: Utility (Definition 3) measures worst-case impact of removing a sample on gradient flow. Theorem 1 proves U(x,y) ≤ c·‖∇θℓ(x,y)‖ via Cauchy-Schwarz inequality under SGD dynamics. Samples with high gradient norms have larger potential influence on training trajectories, making them valuable for distillation.
- Core assumption: The learning rate η is small and gradient norms have bounded magnitude (assumed for converged models).
- Evidence anchors:
  - [Page 5, Theorem 1]: "Utility is bounded by Gradient Norm... there exists a constant c > 0 such that U(x_i, y_i) ≤ c‖∇ℓ‖"
  - [Page 8, Table 6]: Gradient Norm scoring alone improves over loss scoring by 1.5-4.6% across datasets

### Mechanism 3
- Claim: Noise injection during patch selection maintains synthetic data diversity, preventing redundant crop selection and improving generalization.
- Mechanism: Gaussian noise ε ~ N(0, σ²) is added to pooled Shapley attribution maps before selecting crop centers. This stochasticity prevents the greedy deterministic selection of identical high-value patches, ensuring feature space coverage.
- Core assumption: The optimal σ balances exploration of secondary informative regions without selecting uninformative patches.
- Evidence anchors:
  - [Page 5]: "random noise is employed on the average pooled attribution heatmap, resulting in diverse informative patches"
  - [Page 9, Table 7]: Removing noise drops accuracy by 15.6% on ImageNette at IPC=50 (86.2% → 70.6%)

## Foundational Learning

- Concept: **Cooperative Game Theory / Shapley Values**
  - Why needed here: Understanding why Shapley values are axiomatically justified for fair attribution requires grasping how marginal contributions are computed across coalitions.
  - Quick check question: Given 3 players, what's the Shapley value of player 1 if the characteristic function values for coalitions {1}, {2}, {3}, {1,2}, {1,3}, {2,3}, {1,2,3} are [3, 2, 2, 5, 6, 4, 8]?

- Concept: **Gradient Flow and Continuous-Time Training Dynamics**
  - Why needed here: The utility definition uses gradient flow ħ_t as a continuous approximation of discrete SGD, enabling analytical bounds.
  - Quick check question: How does gradient flow differ from the discrete gradient update in standard SGD?

- Concept: **Dataset Distillation as Bi-level vs. Knowledge Distillation-based Paradigms**
  - Why needed here: InfoUtil builds on knowledge-distillation-based DD (two-stage: compress to teacher, then invert), not matching-based methods.
  - Quick check question: Why does matching-based DD require significantly more GPU memory than knowledge-distillation-based approaches?

## Architecture Onboarding

- Component map:
  Original Dataset D → [Stage 1: Informativeness Maximization] Teacher Model f_θD → KernelShap Attribution → Avg Pooling → Noise Injection → Patch Extraction → Compressed Dataset D' (n samples, each d' < d) → [Stage 2: Utility Maximization] Teacher Model f_θD → Gradient Computation → Norm Scoring → Top-k Selection → Image Reconstruction + Soft Label Assignment → Distilled Dataset D̂ (m ≪ n samples)

- Critical path:
  1. Shapley value computation (dominant cost; uses KernelShap approximation)
  2. Gradient norm scoring (single forward-backward pass per sample)
  3. Soft label generation from intermediate vs. converged teacher checkpoints

- Design tradeoffs:
  - **Shapley vs. Grad-CAM**: Shapley is axiomatically principled but slower; Grad-CAM is fast but suffers gradient saturation (Table 8 shows 13.49% gap)
  - **Early-stage vs. converged teacher for soft labels**: Early (epoch 10) for low IPC provides high-entropy diverse labels; converged for high IPC provides precise labels (Figure 4)
  - **Noise intensity σ**: Set as α × std(Shapley_values) with α=2; controls diversity-quality tradeoff

- Failure signatures:
  - Identical repeated crops in output → noise injection disabled or σ too low
  - Unstable performance across seeds → check gradient norm computation for numerical issues
  - Poor cross-architecture transfer → teacher model may be overfitted; try early-stage checkpoint

- First 3 experiments:
  1. Reproduce ImageNet-1K IPC=1 baseline: Train ResNet-18 on distilled 1000 images, should achieve ~12.8% top-1 accuracy
  2. Ablate noise injection: Run with σ=0 on ImageNette IPC=50, expect ~15% accuracy drop per Table 7
  3. Cross-architecture test: Distill with ResNet-18 teacher, evaluate on Swin-V2-Tiny student, expect ~19.8% per Table 2

## Open Questions the Paper Calls Out
- Future work includes extending InfoUtil to more complex and diverse datasets, focusing on scalability and robustness in real-world applications.

## Limitations
- The method relies heavily on the assumption that gradient norms serve as reliable proxies for sample utility, but provides limited empirical validation across diverse dataset distributions.
- The KernelShap approximation for Shapley values introduces approximation errors that could affect patch selection quality.
- The noise injection mechanism lacks theoretical grounding for the specific α=2 scaling parameter.

## Confidence
- **High confidence**: The Shapley value attribution mechanism and its axiomatic justification (axioms of Linearity, Dummy, Symmetry, Efficiency are well-established)
- **Medium confidence**: Gradient norm as utility proxy - theoretically sound but empirically under-validated
- **Medium confidence**: The two-stage pipeline design and efficiency gains over training-based methods
- **Low confidence**: The specific hyperparameters (α=2 for noise, k=4 patches, temperature settings) and their robustness

## Next Checks
1. **Gradient norm bound validation**: Systematically test whether U(x,y) ≤ c·‖∇θℓ(x,y)‖ holds across different datasets by measuring actual utility impact when removing samples versus predicted gradient norm scores.

2. **Hyperparameter sensitivity analysis**: Run ablation studies varying α (noise scale) from 0.5 to 5.0 and k (patches per image) from 1 to 16 to identify optimal ranges and assess robustness to parameter choices.

3. **Cross-architecture generalization stress test**: Evaluate the distilled datasets on architectures with fundamentally different designs (e.g., ViT, MLP-Mixer, ConvNeXt) beyond the ResNet-Swin progression tested in the paper to verify the claimed broad applicability.