---
ver: rpa2
title: Patent Figure Classification using Large Vision-language Models
arxiv_id: '2501.12751'
source_url: https://arxiv.org/abs/2501.12751
tags:
- patent
- classification
- figure
- https
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first study of large vision-language
  models (LVLMs) for patent figure classification. The authors create two new datasets
  (PatFigVQA and PatFigCLS) covering four classification aspects (type, projection,
  object, USPC) and propose a tournament-style classification strategy to handle large
  numbers of classes efficiently.
---

# Patent Figure Classification using Large Vision-language Models

## Quick Facts
- arXiv ID: 2501.12751
- Source URL: https://arxiv.org/abs/2501.12751
- Reference count: 40
- This paper introduces the first study of large vision-language models (LVLMs) for patent figure classification.

## Executive Summary
This paper introduces the first study of large vision-language models (LVLMs) for patent figure classification. The authors create two new datasets (PatFigVQA and PatFigCLS) covering four classification aspects (type, projection, object, USPC) and propose a tournament-style classification strategy to handle large numbers of classes efficiently. Through experiments on these datasets, they demonstrate that the proposed LVLM-based approach outperforms CNN-based classifiers for two aspects (Type and USPC) and achieves competitive performance for the others. The work shows the feasibility of using LVLMs for patent figure classification in both zero-shot and few-shot settings.

## Method Summary
The authors introduce two datasets: PatFigCLS for classification and PatFigVQA for fine-tuning. They employ InstructBLIP with a frozen vision encoder (ViT-g/14) and LLM (FlanT5-XL), training only the Q-Former connector. The paper proposes a tournament-style classification strategy (MC-TS) that partitions large concept sets into multiple-choice questions to overcome context length limitations. Three classification strategies are implemented: Binary Classification (BC), Open-ended Classification (OC), and the proposed Tournament-style Strategy. Models are evaluated in zero-shot and few-shot settings using Top-1 accuracy and semantic equivalence matching (SemEq).

## Key Results
- LVLM-based approach outperforms CNN-based classifiers for Type and USPC classification aspects
- Tournament-style classification strategy (MC-TS) effectively handles large concept sets (1,447 object classes)
- Open-ended classification with semantic matching achieves competitive performance in "Seen" object categories

## Why This Works (Mechanism)

### Mechanism 1: Q-Former Domain Adaptation via VQA
If an LVLM's vision encoder is pre-trained primarily on natural images, fine-tuning the connector (Q-Former) on a domain-specific VQA dataset can bridge the representation gap for abstract patent figures. The Q-Former acts as a bottleneck that learns to extract visual features relevant to the text prompts, compensating for the lack of natural image features.

### Mechanism 2: Hierarchical Context Reduction (Tournament-Style)
If the number of classes is large, partitioning the classification task into a series of smaller multiple-choice questions improves performance over single-step classification. This strategy mitigates the "lost in the middle" phenomenon where LLMs struggle to attend to items in the middle of long contexts.

### Mechanism 3: Semantic Equivalence Matching
If using open-ended classification, exact string matching is insufficient; semantic similarity matching is required to align generated text with ground truth. The model generates free-form text which may differ syntactically from the ground truth label, requiring embedding-based similarity matching.

## Foundational Learning

- **InstructBLIP Architecture (Vision Encoder + Q-Former + LLM)**: The paper freezes the vision and LLM components and only trains the Q-Former. Understanding this modularity is essential to grasp how the model adapts to the patent domain without retraining the entire backbone.
- **Zero-shot vs. Few-shot Fine-tuning**: The paper explicitly evaluates the gap between out-of-the-box performance (zero-shot) and performance after training on PatFigVQA (few-shot). "Few-shot" refers to updating model weights through fine-tuning.
- **Context Window Limitations ("Lost in the Middle")**: This is the theoretical justification for the Tournament-Style Strategy. Without understanding that LLMs ignore items in the middle of long lists, the need for partitioning classes is unclear.

## Architecture Onboarding

- **Component map**: Vision Encoder (ViT-g/14, Frozen) -> Q-Former (Trainable) -> LLM (FlanT5-XL, Frozen) -> Semantic Matcher (PatentBERT + Cosine Similarity)
- **Critical path**: Data Prep (convert PatFigCLS labels to VQA format) -> Fine-Tuning (train Q-Former on PatFigVQA using InstructBLIP) -> Inference (implement MC-TS to classify figures into large label sets)
- **Design tradeoffs**: BC offers high accuracy but is computationally expensive; OC is fastest but requires complex semantic matching; MC-TS balances performance and efficiency for large concept sets
- **Failure signatures**: Low projection accuracy due to subtle class differences; semantic mismatch between generated text and ground truth; early round elimination in tournament strategy
- **First 3 experiments**: 1) Baseline zero-shot InstructBLIP on PatFigCLS; 2) Ablation comparing exact string matching vs semantic similarity on OC task; 3) Scaling test of MC-TS with varying subset sizes (k=5, 10, 20) on Object aspect

## Open Questions the Paper Calls Out

1. How does the tournament-style classification strategy perform when applied to other Large Vision-Language Model (LVLM) architectures besides InstructBLIP?
2. To what extent does unfreezing and fine-tuning deeper layers of an LVLM improve classification accuracy compared to the current method of only updating the Q-Former projection layer?
3. Can prompt compression techniques effectively mitigate the context length limitations inherent in handling large numbers of classification labels?

## Limitations
- Label Quality: USPC and Object labels are derived from automated extraction and clustering, potentially introducing noise
- CNN Baseline Fairness: ResNet50 and ResNext101 are strong but not state-of-the-art; modern architectures could provide more rigorous comparison
- Generalization Scope: Results may not transfer to other jurisdictions or non-patent technical diagrams beyond European/US patents

## Confidence
- High Confidence: Efficacy of Tournament-Style Strategy for large class sets and need for semantic matching in Open-ended classification
- Medium Confidence: Q-Former fine-tuning approach for bridging natural image to patent figure domain gap

## Next Checks
1. Conduct ablation study comparing InstructBLIP performance with and without Q-Former fine-tuning on PatFigVQA
2. Replace CNN baselines with modern architectures (e.g., ConvNeXt or DeiT) for more rigorous comparison
3. Evaluate trained model on patent figures from different patent offices (e.g., Japan or China) to assess cross-jurisdiction generalization