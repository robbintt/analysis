---
ver: rpa2
title: LLM-Guided Compositional Program Synthesis
arxiv_id: '2503.15540'
source_url: https://arxiv.org/abs/2503.15540
tags:
- program
- synthesis
- examples
- output
- values
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: S YMLLM tackles the challenge of improving program synthesis from
  input-output examples using large language models (LLMs), which can fail unpredictably.
  The core idea is compositional program synthesis, where the LLM guides the decomposition
  of the task into simpler subtasks and solves them.
---

# LLM-Guided Compositional Program Synthesis

## Quick Facts
- arXiv ID: 2503.15540
- Source URL: https://arxiv.org/abs/2503.15540
- Authors: Ruhma Khan; Sumit Gulwani; Vu Le; Arjun Radhakrishna; Ashish Tiwari; Gust Verbruggen
- Reference count: 6
- Key outcome: S YMLLM solves about 30% of tasks that fail with self-reflection alone by decomposing them into simpler subproblems using four strategies: Forward1, ForwardAll, Backward1, and IfThenElse.

## Executive Summary
S YMLLM addresses the challenge of improving program synthesis from input-output examples when large language models (LLMs) fail unpredictably. The core insight is compositional program synthesis, where an LLM guides the decomposition of complex synthesis tasks into simpler subtasks that are easier to solve. When an LLM generates an incorrect program, S YMLLM extracts meaningful parts of it, uses execution semantics to define new input-output examples for the remaining task, and recursively solves these subtasks. The approach introduces four decomposition strategies that handle different types of failures, achieving significant improvements on challenging benchmarks.

## Method Summary
S YMLLM uses GPT-4o to generate initial programs from input-output examples. When these programs fail, the system applies compositional decomposition strategies to salvage useful parts of the incorrect programs. The four strategies work as follows: Forward1 salvages the first operation and creates a new subtask with intermediate values as inputs; ForwardAll salvages the entire program and creates a subtask to convert intermediate values to final outputs; Backward1 salvages the last operation and uses inverse execution to hypothesize intermediate values; IfThenElse handles conditional cases by splitting examples into solved and unsolved subsets and synthesizing a condition to combine the resulting programs. The system limits recursion depth to one for efficiency.

## Key Results
- S YMLLM solves approximately 31.6% of Python tasks that fail with self-reflection alone
- The approach achieves a 19.7% success rate on previously unsolved Excel formula tasks
- Each decomposition strategy contributes uniquely to the overall success rate, with Forward1, ForwardAll, Backward1, and IfThenElse showing distinct effectiveness profiles

## Why This Works (Mechanism)

### Mechanism 1: Failure-Guided Task Decomposition (Forward Strategies)
When an LLM generates an incorrect program, the prefix or entirety may still correctly transform inputs to intermediate values, reducing the complexity of the remaining synthesis task. The system executes the partial (or full) incorrect program on the original inputs to generate intermediate values V, then constructs a new, simpler Programming by Example task where V are the inputs and the original outputs O are the targets.

### Mechanism 2: Inverse-Execution Decomposition (Backward Strategy)
Synthesis can be guided by fixing the final operation of a program and reverse-engineering the necessary inputs for that operation. The system extracts the last operation (e.g., `join(v1, v2)`) from the failed program, uses the target output to hypothesize concrete values for the operation's inputs that would produce the target, and creates new synthesis subtasks based on these hypothesized values.

### Mechanism 3: Conditional Composition (IfThenElse)
A program that passes some test cases but fails others often represents a correct branch of a larger conditional solution. If the candidate program solves a subset Ex1 of the examples, the system treats the remaining examples Ex2 as a separate synthesis task, then synthesizes a condition to switch between the two programs based on input features.

## Foundational Learning

**Concept: Programming by Example (PBE)**
*Why needed:* This is the core problem domain. Understanding that the goal is to infer a program solely from input-output pairs (without natural language description) is critical.
*Quick check:* Given input `[1, 2]` -> output `[2, 4]`, does the PBE system prefer `x * 2` or `x + 1`? (Neither is provably correct without more examples; this ambiguity is what S YMLLM tries to navigate.)

**Concept: Dataflow Graphs & Abstract Syntax Trees (AST)**
*Why needed:* The decomposition strategies (Forward1/Backward1) rely on parsing code into trees to identify "first operations" (leaves) and "last operations" (roots). You cannot implement the salvage logic without parsing.
*Quick check:* In the expression `y = f(g(x))`, which function is the "suffix" (last operation) and which is the "prefix"?

**Concept: Inverse Semantics**
*Why needed:* The Backward1 mechanism requires understanding how to invert a function. For `y = x + 1`, the inverse is `x = y - 1`. For `y = hash(x)`, there is no inverse. The paper uses LLMs to approximate this.
*Quick check:* If the last operation is `concat(a, b) -> "a,b"` and the output is `"cat,dog"`, what are the possible inverse values for `a` and `b`?

## Architecture Onboarding

**Component map:**
1. Synthesizer (LLM): Generates candidate programs, inverse values, and conditions
2. Executor: Runs the candidate programs on I/O examples to verify correctness
3. Parser/Analyzer: Extracts the AST/Dataflow graph to slice the program into Prefix and Suffix
4. Orchestrator: Manages the recursion logic, selecting strategies and constructing new prompts

**Critical path:**
1. Receive Ex (Input/Output examples)
2. Call LLM for program F
3. Execute F. If success, return
4. If failure:
   - Try Forward: Execute F (or prefix) on inputs -> new inputs -> call LLM
   - Try Backward: Parse F suffix -> LLM inverse execution -> new outputs -> call LLM
5. Compose results and verify

**Design tradeoffs:**
- Python vs. Domain Specific Language (DSL): Lower success rates for Excel formulas due to parsing challenges
- Recursion Depth: Limited to 1 for efficiency, though deeper recursion could solve harder problems
- Temperature: Inverse execution uses 0.4 for diverse candidate generation; deterministic steps use 0

**Failure signatures:**
- "Parser Error": LLM code cannot be sliced due to complex structures
- "Inverse Failure": LLM cannot guess intermediate values or guesses produce incorrect outputs
- "Condition Overfitting": Generated conditions only match specific samples rather than general patterns

**First 3 experiments:**
1. Baseline Check: Reproduce "Self-Reflection" baseline on Playgol dataset to identify hard tasks
2. Forward1 Validity: Implement only Forward1 and verify meaningful intermediate values are generated at least 20% of the time
3. Inverse Execution Accuracy: Isolate Backward1 and test LLM accuracy at recovering input arguments for known inverse functions

## Open Questions the Paper Calls Out
- Can a predictive model or heuristic be developed to determine the optimal order and trigger conditions for applying the four decomposition strategies? (Section 4.4 states the specific order and trigger conditions were left unspecified)
- Does increasing the recursion depth beyond a single iteration yield significant improvement in solve rates for complex tasks? (Section 5.2 notes recursion was restricted to 1 for efficiency)
- How sensitive is the synthesis success rate to the specific heuristics used to extract program slices for Forward1 and Backward1 strategies? (Section 7 acknowledges choices about program part extraction were not optimized)

## Limitations
- Inverse execution reliability depends heavily on LLM's ability to hallucinate correct intermediate values, which is empirically weak for complex transformations
- The "first operation" extraction is implementation-specific and may not generalize across diverse program structures
- Success metrics combine multiple strategies, making it difficult to isolate which mechanism is actually responsible for improvements

## Confidence
- **High confidence:** General decomposition framework (Forward1, IfThenElse) works by reducing search space through simpler subproblems
- **Medium confidence:** Backward1's inverse execution mechanism - theoretically sound but practically brittle due to LLM hallucination quality
- **Low confidence:** Claim that each strategy contributes "uniquely" - ablation studies don't sufficiently demonstrate independence

## Next Checks
1. **Ablation stress test:** Implement each strategy in isolation and measure individual success rates on the same benchmark to verify claimed unique contributions
2. **Inverse execution benchmark:** Create controlled dataset with known inverse values and measure LLM accuracy at this task independent of full synthesis pipeline
3. **Parser robustness audit:** Test program parser/analyzer on edge cases (nested comprehensions, imports, lambda functions) to quantify failure rates that could cascade into strategy failures