---
ver: rpa2
title: Latent Discrete Diffusion Models
arxiv_id: '2510.18114'
source_url: https://arxiv.org/abs/2510.18114
tags:
- latent
- diffusion
- discrete
- data
- continuous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Latent Discrete Diffusion Models (LDDMs) to
  address the limitation of standard masked discrete diffusion where reverse transitions
  factorize across positions, weakening joint structure and degrading quality in few-step
  generation. The core idea is to couple a masked discrete diffusion over tokens with
  a continuous diffusion over latent embeddings, where latents carry cross-token dependencies
  to resolve ambiguities.
---

# Latent Discrete Diffusion Models

## Quick Facts
- arXiv ID: 2510.18114
- Source URL: https://arxiv.org/abs/2510.18114
- Reference count: 40
- Primary result: LDDMs achieve lower generative perplexity than masked discrete diffusion baselines, especially at low sampling budgets

## Executive Summary
Latent Discrete Diffusion Models (LDDMs) address the factorized transition limitation in standard masked discrete diffusion by coupling discrete token denoising with continuous latent diffusion. The model introduces a trainable encoder that maps discrete tokens to continuous embeddings, which carry cross-token dependencies to resolve ambiguities during generation. Two variants are proposed: FUJI-LDDMs perform fully joint denoising of tokens and latents, while SEQ-LDDMs resolve the latent first and then conditionally denoise the discrete chain. Experiments show consistent improvements on LM1B, with the most pronounced gains at lower sampling budgets where unmasking many tokens per step is desirable.

## Method Summary
LDDMs combine a masked discrete diffusion process over tokens with a continuous diffusion over latent embeddings. A trainable encoder maps discrete tokens to continuous latents, which are then noised and denoised alongside the discrete chain. The objective combines discrete reconstruction loss and continuous latent reconstruction loss with weighting parameters. Two architectures are proposed: FUJI uses a multi-modal DiT with shared attention and modality-specific heads to jointly process tokens and latents, while SEQ separates the denoisers, resolving latents first before conditionally denoising the discrete chain. Training uses a two-stage schedule with latent weighting ramping up after warmup, and unit-norm normalization prevents latent blow-up.

## Key Results
- FUJI-LDDM with constant loss weighting achieves lowest generative perplexity across all budgets on LM1B
- SEQ-LDDM recovers binary sawtooth distribution in 1-2 steps (SWD metric)
- Most pronounced gains at lower sampling budgets where factorized unmasking degrades quality most severely

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Continuous latents improve few-step generation by capturing global cross-token dependencies that factorized reverse transitions miss
- **Mechanism**: The continuous latents carry a "soft" signal representing global structure, which conditions the discrete denoiser, tightening the ELBO lower bound
- **Core assumption**: The latent encoder can efficiently compress joint token dependencies into a continuous representation easier to denoise than discrete data directly
- **Evidence anchors**: Abstract mentions latents provide "softer signal and carry cross-token dependencies"; Section 2.3 constructs Y via encoder with ELBO tightening argument; Coevolutionary Continuous Discrete Diffusion explores similar co-evolution

### Mechanism 2
- **Claim**: FUJI-LDDMs stabilize hard token unmasking commitments by allowing continuous latents to amortize errors across steps
- **Mechanism**: Continuous and discrete channels co-evolve, with the continuous channel providing soft, reversible adjustments guiding discrete unmasking away from inconsistent states
- **Core assumption**: Interaction between channels during denoising trajectory provides corrective signal lacking in purely discrete factorized transitions
- **Evidence anchors**: Abstract describes FUJI as "fully joint denoising"; Section 1 contrasts soft reversible continuous denoising with hard discrete unmasking; Continuously Augmented Discrete Diffusion similarly augments discrete states with continuous hints

### Mechanism 3
- **Claim**: SEQ-LDDMs enable high-speed sampling by front-loading computational cost into latent resolution followed by rapid conditioned discrete phase
- **Mechanism**: Latent resolved first over T_Y steps, then discrete chain conditioned on fixed global context allows quick dependency resolution in few steps (1-2)
- **Core assumption**: Latent manifold is smooth enough to traverse efficiently and contains sufficient information to guide short discrete generation
- **Evidence anchors**: Abstract describes SEQ as "sequentially resolve the latent first"; Section 5.1 shows SEQ attains near-optimal SWD with 1-2 data steps; Variational Masked Diffusion Models addresses similar dependency capture

## Foundational Learning

- **Concept**: Masked Discrete Diffusion (MDLM)
  - **Why needed here**: LDDMs fix the "factorization bottleneck" inherent in standard MDLMs; understanding baseline forward/reverse process is essential
  - **Quick check question**: In standard masked diffusion reverse step, why does factorizing across positions lead to "jointly inconsistent" outputs when unmasking many tokens?

- **Concept**: Evidence Lower Bound (ELBO)
  - **Why needed here**: LDDMs derive new ELBO combining discrete reconstruction and continuous latent terms; understanding balance via weights λ_x, λ_y is critical
  - **Quick check question**: How does LDDM objective change if latent encoder variance σ²_lat is set too high vs too low?

- **Concept**: Variational Autoencoders (VAEs) & Latent Diffusion
  - **Why needed here**: LDDMs utilize trainable encoder E_φ mapping discrete data to continuous latent space; managing trade-off between reconstruction accuracy and latent regularity is core
  - **Quick check question**: What failure mode does paper identify when encoder not properly constrained (hint: "latent blow-up"), and what normalization trick prevents it?

## Architecture Onboarding

- **Component map**: Data tokens → Forward Masking → Discrete Denoiser; Data tokens → Encoder → Continuous Noising → Latent Denoiser; Joint Mechanism: FUJI (MM-DiT with shared attention), SEQ (Separate denoisers)

- **Critical path**:
  1. Training: Sample t → Noising (X_t, Y_t) → Predict (X_0, Y_0) → Compute Weighted Loss
  2. Inference (FUJI): Start from (X_T [masks], Y_T [noise]) → Iterate T→1 jointly updating X and Y
  3. Inference (SEQ): Start from Y_T → Denoise to Y_0 → Start X_T → Denoise X conditioned on fixed Y_0

- **Design tradeoffs**:
  - Loss Weighting: Standard ELBO weights often unstable; recommend constant weighting (λ=1) for latent term and two-stage ramp-up for λ_latent
  - Latent Normalization: Unit-norm normalization strictly required to prevent "latent blow-up" failure mode
  - Architecture: FUJI uses joint MM-DiT (heavier but better interaction), SEQ separates them (lighter/faster discrete steps but requires accurate latent prediction first)

- **Failure signatures**:
  - Latent Blow-Up: Gradients explode or loss drops to zero because encoder increases latent magnitude to infinite SNR; fix: unit norm normalization
  - Latent Collapse: Latent provides no signal (constant output); fix: verify two-stage schedule, start λ_latent=0 for ≥1K steps
  - Entropy Collapse: Generated samples are repetitive; fix: adjust λ_X weighting and check sampling precision (paper uses fp64)

- **First 3 experiments**:
  1. Synthetic Validation: Replicate "Binary Sawtooth" experiment; train SEQ-LDDM and verify it recovers distribution in 1-2 steps (SWD metric)
  2. Ablation on Weighting: On LM1B, compare λ_X = λ_ELBO vs λ_X = 1; verify constant weighting yields lower generative perplexity
  3. Encoder Comparison: Compare frozen pre-trained encoder vs training E_φ from scratch; verify if frozen encoder helps or trained encoder necessary

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can discrete latent spaces be effectively integrated into LDDM framework while maintaining benefits of softer cross-token signal propagation?
- **Basis**: "Exploring discrete latents is left to future work" (Section 3, p. 5)
- **Why unresolved**: Current formulation uses continuous latents for soft signal properties; discrete latents would require reformulating latent diffusion process
- **What evidence resolves it**: Experiments comparing continuous vs discrete latent variants on same benchmarks, measuring generative perplexity and entropy at equivalent sampling budgets

### Open Question 2
- **Question**: What is optimal interpolation between FUJI-style joint denoising and SEQ-style sequential denoising, and does hybrid schedule offer advantages?
- **Basis**: "We leave possible exploration of schedules interpolating between both core models to further work" (Section 3.1, p. 6)
- **Why unresolved**: FUJI and SEQ represent extremes in how tokens and latents co-evolve; space between unexplored
- **What evidence resolves it**: Systematic sweep of noise schedules varying relative noising order, reporting quality metrics and wall-clock sampling time

### Open Question 3
- **Question**: Would switching from y₀-prediction to ε-, v-, or rectified-flow objectives improve latent channel learning stability or generation quality?
- **Basis**: "One may also switch targets from y₀-prediction to ε-, v-, or rectified-flow prediction; we leave this exploration to future work" (Section 3.2, p. 7)
- **Why unresolved**: Different prediction targets induce different loss weighting dynamics and gradient properties
- **What evidence resolves it**: Ablation studies training LDDMs with each prediction target under matched conditions, comparing validation perplexity and latent reconstruction error curves

## Limitations

- **Domain generalization gap**: Performance gains may be tightly coupled to specific encoder architecture and tokenization scheme used in LM1B experiments
- **Computational overhead reality**: Claims of "minimal" overhead based on training hours, but inference-time costs vary significantly between FUJI and SEQ variants
- **Theoretical tightness gap**: ELBO improvement assumes latent makes X_t|Y closer to factorizable, but paper doesn't empirically validate this assumption or measure actual divergence reduction

## Confidence

**High Confidence** (4/5): Core empirical claim that LDDMs outperform MDLMs on LM1B generative perplexity across all sampling budgets, well-supported with multiple baselines and ablation studies

**Medium Confidence** (3/5): Mechanism explanations are theoretically sound but lack direct causal validation; paper shows correlation between design choices and performance but doesn't isolate which mechanism drives improvements

**Low Confidence** (2/5): Claim that LDDMs are broadly applicable to "any masked discrete diffusion model" is extrapolated from single dataset and architecture without testing robustness to different masking patterns, sequence lengths, or latent dimensionalities

## Next Checks

1. **Latent Dimensionality Sensitivity**: Systematically vary d' from 16 to 128 on LM1B and measure impact on generative perplexity and sample entropy to validate whether 32-dimensional latents are near-optimal

2. **Cross-Domain Transfer**: Train same LDDM architecture on C4 text corpus with SentencePiece tokenization and ImageNet-32x32 using discrete VQ codes; compare relative improvements against MDLM baselines to assess domain generalization

3. **Mechanism Isolation Test**: Create "broken" SEQ-LDDM where latent denoiser replaced with random projection (destroying dependency-capturing mechanism) but all other components remain; compare performance against full SEQ-LDDM and MDLM to quantify marginal contribution of latent channel beyond architectural complexity