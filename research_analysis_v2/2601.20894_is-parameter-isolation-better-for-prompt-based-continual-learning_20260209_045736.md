---
ver: rpa2
title: Is Parameter Isolation Better for Prompt-Based Continual Learning?
arxiv_id: '2601.20894'
source_url: https://arxiv.org/abs/2601.20894
tags:
- learning
- prompt
- prompts
- continual
- split
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a shared prompt pool with dynamic allocation
  for continual learning, using a task-aware gated routing mechanism to select sparse
  prompt subsets per input. A history-aware modulator protects frequently used prompts
  from excessive updates, balancing learning and forgetting.
---

# Is Parameter Isolation Better for Prompt-Based Continual Learning?

## Quick Facts
- **arXiv ID**: 2601.20894
- **Source URL**: https://arxiv.org/abs/2601.20894
- **Reference count**: 40
- **Primary result**: Shared prompt pool with dynamic allocation improves continual learning performance on CIFAR-100 (95.02% FAA) and ImageNet-R (79.02% FAA) while reducing forgetting.

## Executive Summary
This paper introduces a novel approach to class-incremental learning using a shared prompt pool with dynamic allocation. The method employs a task-aware gated routing mechanism that selects sparse prompt subsets per input, combined with a history-aware modulator that protects frequently used prompts from excessive updates. This architecture addresses the stability-plasticity dilemma in continual learning by balancing learning new tasks while preserving knowledge of previous ones. The proposed framework demonstrates consistent gains over static prompt allocation baselines while maintaining lower forgetting rates across standard benchmarks.

## Method Summary
The proposed "Hash" framework implements a shared prompt pool of $N_r=15$ experts, each with length $L=15$, which are dynamically selected through a task-aware gated routing mechanism. For each input, the model selects the top-K (K=2) experts and injects them into layers 1-4 of a ViT-B/16 backbone. A history-aware modulator tracks cumulative expert activations and applies a stepwise routing penalty (δ=0.4) to high-frequency experts while scaling their gradients by α=0.1 during backpropagation. The training follows a three-phase process (WTP, TII, TAP) with cross-entropy loss and contrastive regularization (τ=0.8), using Adam optimization.

## Key Results
- Achieves final average accuracy of 95.02% on CIFAR-100
- Achieves final average accuracy of 79.02% on ImageNet-R
- Demonstrates lower forgetting compared to static prompt allocation baselines

## Why This Works (Mechanism)
The approach works by dynamically allocating sparse prompt subsets per input through task-aware routing, preventing catastrophic forgetting through gradient scaling for frequently used prompts. The history-aware modulator creates a protective mechanism that reduces updates to well-established prompts while still allowing for adaptation. The shared pool with dynamic selection provides flexibility to handle new tasks without increasing model capacity per task, addressing the fundamental challenge of balancing stability and plasticity in continual learning.

## Foundational Learning
- **Prompt-based continual learning**: Modifying model behavior through input-dependent prompt sequences rather than full fine-tuning; needed to reduce catastrophic forgetting while maintaining efficiency.
- **Task-aware gated routing**: Dynamic selection of model components based on input features; required to allocate different prompts for different tasks without task-specific models.
- **History-aware modulator**: Mechanism to track and protect frequently used parameters; essential to prevent over-updating stable components while allowing adaptation.
- **Contrastive regularization**: Loss component that maintains feature distinctiveness; needed to preserve task boundaries in the embedding space.
- **Top-K expert selection**: Sparse activation pattern where only K experts are used per input; critical for computational efficiency and preventing prompt interference.

## Architecture Onboarding
- **Component map**: Input → Task Router → Top-K Selector → Prompt Pool → Feature Injection → Backbone → Prediction
- **Critical path**: Task Router → Top-K Selection → Prompt Injection into Backbone Layers
- **Design tradeoffs**: Shared vs. task-specific prompts (efficiency vs. specialization), dynamic vs. static allocation (flexibility vs. stability), gradient scaling vs. hard constraints (adaptability vs. protection)
- **Failure signatures**: High Forgetting Measure indicates prompt overwriting; Router Collapse shows skewed expert activation distribution
- **3 first experiments**: 1) Verify gradient scaling reduces updates to frequent experts, 2) Monitor expert activation diversity across tasks, 3) Test Top-K selection stability with varying K values

## Open Questions the Paper Calls Out
None

## Limitations
- Missing critical hyperparameters (learning rate, epochs per task, contrastive loss weight) that significantly impact CIL performance
- Implementation details of HiDe-Prompt training phases (WTP, TII, TAP) are referenced but not fully specified
- Performance may vary due to sensitivity of CIL to optimization details and phase-specific training procedures

## Confidence
- **High confidence**: The core architectural concept is clearly specified and implementable
- **Medium confidence**: The overall experimental setup is reproducible, but performance may vary due to missing hyperparameters
- **Low confidence**: Exact replication of training phases and specific loss weighting details

## Next Checks
1. **Verify Gradient Scaling**: Implement the history-aware modulator and validate that gradients for frequently activated experts are scaled down by α=0.1 during backpropagation. Check gradient magnitudes across training iterations.
2. **Monitor Expert Utilization**: Track the activation frequency of each expert prompt across all tasks and inputs. Ensure the routing mechanism maintains a diverse usage pattern and that no single expert dominates for all tasks.
3. **Hyperparameter Sensitivity**: Conduct a small-scale ablation study on CIFAR-100 to determine the impact of the unknown learning rate and epoch count on the Final Average Accuracy and Forgetting Measure.