---
ver: rpa2
title: 'DROGO: Default Representation Objective via Graph Optimization in Reinforcement
  Learning'
arxiv_id: '2602.00403'
source_url: https://arxiv.org/abs/2602.00403
tags:
- learning
- principal
- eigenvector
- reward
- drogo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DROGO, a method for directly approximating
  the principal eigenvector of the default representation (DR) in reinforcement learning
  using neural networks. Unlike prior approaches that first approximate the full DR
  matrix and then perform eigendecomposition, DROGO optimizes a graph-drawing objective
  in log-space with natural gradient to account for the inherent geometry of the parameter
  space.
---

# DROGO: Default Representation Objective via Graph Optimization in Reinforcement Learning

## Quick Facts
- arXiv ID: 2602.00403
- Source URL: https://arxiv.org/abs/2602.00403
- Reference count: 34
- Primary result: Direct approximation of principal eigenvector of default representation (DR) in RL using log-space parameterization and natural gradient, achieving cosine similarity approaching 1 and improving reward shaping

## Executive Summary
This paper introduces DROGO, a method for directly approximating the principal eigenvector of the default representation (DR) in reinforcement learning using neural networks. Unlike prior approaches that first approximate the full DR matrix and then perform eigendecomposition, DROGO optimizes a graph-drawing objective in log-space with natural gradient to account for the inherent geometry of the parameter space. The method includes an alternative pointwise constraint to prevent vector collapse. Experiments show that DROGO successfully learns the principal eigenvector across different state representations (one-hot, coordinates, pixels) with cosine similarity approaching 1. When applied for reward shaping, the learned eigenvectors outperform both the successor representation and no shaping, achieving faster convergence and fewer visits to low-reward states.

## Method Summary
DROGO learns the log of the principal eigenvector of the default representation matrix Z = [diag(exp(-r/λ)) - P^πd]^{-1} directly through neural networks. The method uses log-space parameterization (v = log e) to guarantee positive outputs, natural gradient descent to handle the exponential scaling instability, and a pointwise constraint v(s_T) = 0 for terminal states to prevent vector collapse. The loss function includes stop-gradient on the bracketed term to properly account for the natural gradient geometry. The approach is validated across grid world environments with one-hot, coordinate, and pixel state representations, showing improved reward shaping compared to SR-based methods.

## Key Results
- Cosine similarity between learned and ground-truth eigenvectors approaches 1 across all tested state representations
- DROGO-based reward shaping outperforms SR-based shaping and no shaping in terms of sample efficiency (fewer updates needed to reach optimal policy)
- Reward shaping with DROGO reduces visits to low-reward states compared to baselines
- The method scales to high-dimensional pixel inputs while maintaining performance

## Why This Works (Mechanism)

### Mechanism 1: Log-space Parameterization
Standard graph drawing objective can produce negative eigenvector estimates, preventing log transformation for practical use. By reparameterizing to learn v = log e directly and optimizing L(exp(v)), the network output is exponentiated before use, ensuring positivity regardless of v's sign. This guarantees the learned eigenvector is always positive while retaining reward-awareness.

### Mechanism 2: Natural Gradient Descent
Log-space parameterization introduces exponential scaling instability: standard gradients of L_log_GDO include exp(v) factors, causing updates to have vastly different effects depending on v's magnitude. Natural gradient multiplies by G(v)^(-1) = diag(exp(-2v)), canceling this instability by accounting for the non-Euclidean geometry induced by the exponential mapping.

### Mechanism 3: Pointwise Constraint
Without constraint, solutions can drift as αe with α→0 (all entries shift toward -∞ in log space). Fixing v(s_T) = 0 for a terminal state prevents this collapse without the hyperparameter tuning required for quadratic penalties. Theoretically, terminal state columns span the principal eigenspace as δ→0+, providing grounding.

## Foundational Learning

- **Concept**: Successor Representation (SR)
  - **Why needed here**: DR generalizes SR by incorporating rewards. Understanding SR as "expected discounted visits to successor states" provides the baseline for appreciating DR's reward-aware modification.
  - **Quick check question**: Given a 3-state chain with deterministic transitions, can you hand-compute the SR for the middle state?

- **Concept**: Graph Drawing Objective (GDO)
  - **Why needed here**: DROGO adapts spectral graph drawing to retrieve eigenvectors. The core idea—minimizing u^T M u subject to constraints—is the foundation upon which log-space and natural gradient modifications build.
  - **Quick check question**: Why does minimizing u^T (I - P) u retrieve the smallest eigenvector of the Laplacian?

- **Concept**: Natural Gradient / Riemannian Geometry
  - **Why needed here**: Log-space parameterization creates non-Euclidean geometry. Natural gradient corrects for this by using the Fisher information / metric tensor to adjust update directions.
  - **Quick check question**: In what situation would standard gradient descent and natural gradient descent give identical updates?

## Architecture Onboarding

- **Component map**: State representation (one-hot/coordinates/pixels) -> 4 FC layers (128 units) with ReLU / 3 conv layers + FC -> Output layer (single scalar v_φ(s)) -> Loss computation (ℓ_DROGO with stop-gradient) -> RMSProp optimizer

- **Critical path**:
  1. Sample transition (s, a, r, s') under default policy π_d
  2. Compute v_φ(s) and v_φ(s') via forward pass
  3. Mask s' contribution if s' is terminal: v_φ(s') × (1 - I{s'=s_T})
  4. Compute loss and backprop with stop-gradient on natural gradient estimator
  5. Apply gradient norm clipping before optimizer step

- **Design tradeoffs**:
  - λ hyperparameter: Larger λ reduces reward-awareness strength but improves stability (paper uses λ=20)
  - Pointwise vs. quadratic constraint: Pointwise avoids hyperparameter b but requires at least one terminal state
  - State representation: One-hot is most accurate; pixels require more capacity and may have approximation errors

- **Failure signatures**:
  - Cosine similarity not approaching 1: Check if λ is too small (exp(-r/λ) explodes) or learning rate too high
  - Learned vector has near-constant values: Natural gradient may not be applied correctly; verify stop-gradient
  - Negative outputs after exp: Should not happen with log-space param; check if exp is being applied

- **First 3 experiments**:
  1. **Tabular sanity check**: Use tabular (array) parameters with one-hot states; verify cosine similarity → 1 on ring environment. Confirms loss derivation is correct.
  2. **Ablation on natural gradient**: Compare ℓ_log_GDO vs. ℓ_NG_GDO vs. ℓ_DROGO on coordinates input. Expect ℓ_log_GDO to fail or converge slowly.
  3. **Reward shaping validation**: Train Q-learning with DR-based shaping vs. SR-based shaping vs. no shaping in FourRooms. Measure N_OPT and N_VISIT to confirm DR guides away from low-reward regions.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can DROGO be adapted for online learning where the principal eigenvector and policy are learned simultaneously?
- **Basis in paper**: [explicit] The authors state: "future work can adapt DROGO to learn both the principal eigenvector and the policy simultaneously," noting their two-stage approach serves only to demonstrate utility.
- **Why unresolved**: The current implementation requires pre-training on collected transitions before application, which limits real-time adaptability.
- **What evidence would resolve it**: An online variant that interleaves eigenvector learning with policy optimization, maintaining convergence properties while updating in real-time.

### Open Question 2
- **Question**: How does DROGO perform on downstream applications such as temporally-extended exploration and generalization in RL?
- **Basis in paper**: [explicit] The conclusion states: "it will be interesting to apply the learned eigenvectors for downstream applications, similar to how Klissarov & Machado (2023); Chandrasekar & Machado (2025) used GDO for exploration and generalization in RL."
- **Why unresolved**: The paper only demonstrates reward shaping; other applications of the DR eigenvector remain unexplored in the function approximation setting.
- **What evidence would resolve it**: Experiments applying DROGO-learned eigenvectors for option discovery, count-based exploration, or transfer learning in complex environments.

### Open Question 3
- **Question**: Can DROGO scale beyond grid world environments to high-dimensional control tasks and continuous state spaces?
- **Basis in paper**: [inferred] The authors acknowledge they "run experiments in grid world environments to be able to compare the eigenvectors learned using DROGO against the ground-truth computed in closed form." While DROGO handles pixels as input, complex continuous control domains remain untested.
- **Why unresolved**: No experiments in standard deep RL benchmarks (e.g., MuJoCo, Atari) where ground-truth eigenvectors cannot be computed.
- **What evidence would resolve it**: Demonstrating DROGO on high-dimensional continuous control tasks, comparing against SR-based baselines without access to ground-truth DR.

## Limitations

- Theoretical foundation assumes the ground-truth principal eigenvector is strictly positive, which may not hold in all environments with mixed-sign rewards or asymmetric transition structures
- Natural gradient derivation's practical sensitivity to approximation errors in the metric tensor remains unclear, with limited ablation studies
- Pointwise constraint relies on terminal states with near-zero rewards, limiting applicability to continuous or infinite-horizon settings without natural terminal states

## Confidence

- **High confidence**: Core experimental results showing DROGO learns principal eigenvectors with cosine similarity approaching 1 across multiple state representations; reward shaping experiments demonstrating improved sample efficiency
- **Medium confidence**: Theoretical derivations for log-space parameterization and natural gradient, which follow from established principles but lack extensive validation through ablations
- **Low confidence**: Claims about DROGO's performance in high-dimensional pixel inputs, as the paper lacks quantitative comparisons against other representation learning methods in this regime

## Next Checks

1. **Ablation study on natural gradient**: Train DROGO with standard gradient descent (varying learning rates) versus natural gradient on the same tasks. Measure convergence speed and final cosine similarity to determine if natural gradient provides consistent advantages or compensates for suboptimal learning rate choices.

2. **Robustness to terminal state selection**: Systematically vary which state is chosen as the terminal state (s_T) and observe the impact on learned eigenvectors and reward shaping performance. This tests whether the theoretical justification is practically important or if the constraint serves primarily as a numerical stabilizer.

3. **Failure mode characterization**: Construct environments where the principal eigenvector should contain negative entries (e.g., asymmetric rewards, absorbing states with negative reward) and verify whether DROGO fails gracefully or produces misleading positive outputs. This tests the limits of the log-space parameterization assumption.