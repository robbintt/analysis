---
ver: rpa2
title: Model Merging via Multi-Teacher Knowledge Distillation
arxiv_id: '2512.21288'
source_url: https://arxiv.org/abs/2512.21288
tags:
- arxiv
- merging
- merge
- task
- fine-tuned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of model merging in multi-task
  learning, where the goal is to combine independently fine-tuned models into a single
  model that performs well across all tasks. The authors propose a novel method called
  SAMerging that leverages sharpness-aware minimization (SAM) and multi-teacher knowledge
  distillation to find flatter minima and minimize the Kullback-Leibler divergence
  between the merged model and each fine-tuned model.
---

# Model Merging via Multi-Teacher Knowledge Distillation

## Quick Facts
- arXiv ID: 2512.21288
- Source URL: https://arxiv.org/abs/2512.21288
- Reference count: 40
- Primary result: State-of-the-art model merging via SAM + multi-teacher KD with 16-sample data efficiency

## Executive Summary
This paper introduces SAMerging, a model merging framework that combines independently fine-tuned models into a single multi-task model using sharpness-aware minimization (SAM) and multi-teacher knowledge distillation. The method minimizes KL divergence between the merged model and each fine-tuned model while seeking flat minima, achieving state-of-the-art results across vision and NLP benchmarks with remarkable data efficiency (as few as 16 examples per task).

## Method Summary
SAMerging optimizes layer-wise coefficients λ to construct a merged model θ_λ = θ₀ + Σₜ λₜ·τₜ from task vectors τₜ = θₜ - θ₀. The optimization minimizes KL divergence between the merged model and each fine-tuned model using SAM with a perturbation radius ρ=0.07, starting from λ=0 initialization. The method requires unlabeled calibration data from each task and operates with batch size 16, Adam optimizer (lr=0.001), and equal task weights.

## Key Results
- Achieves state-of-the-art merging performance across CLIP ViT-B/32/ViT-L/14 on TA-8, TALL-14, and TALL-20 benchmarks
- Demonstrates 16-sample data efficiency per task while maintaining high accuracy
- Outperforms baselines including AdaMerging, both theoretically (tighter PAC-Bayes bound) and empirically
- No additional inference or memory overhead compared to fine-tuned models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Minimizing KL divergence between merged model and fine-tuned models tightens excess-risk bound
- **Mechanism:** Theorem 3 shows average excess risk bounded by optimizable KL term plus fixed teacher error
- **Core assumption:** Pinsker's inequality relates TV distance to KL divergence for bounded losses
- **Evidence:** Abstract states KL minimization "directly tightens the upper bound on the merged model's excess risk"; Theorem 3 decomposes risk; sparse corpus support for multi-teacher KD theory
- **Break condition:** High intrinsic error in teacher models limits recovery via KL minimization

### Mechanism 2
- **Claim:** Seeking flat minima via SAM improves multi-task generalization
- **Mechanism:** Theorem 2's bound includes gradient-norm squared terms penalizing sharp basins; SAM pushes toward flatter regions
- **Core assumption:** NTK linearization holds locally around pretrained weights; loss is γ-smooth and convex in scores
- **Evidence:** Abstract mentions SAM for finding flatter minima; Theorem 2 explicitly includes flatness penalty; Figure 2-3 shows loss landscape visualizations; corpus support from related sharpness-merging work
- **Break condition:** Merged model straying far from θ₀ violates NTK approximation and degrades bound tightness

### Mechanism 3
- **Claim:** Cross-task heterogeneity term H_Q quantifies transfer mismatch and shrinks when parameters lie near consensus
- **Mechanism:** H_Q measures performance gap of fine-tuned model Q_j on other tasks vs. its own; bound includes dispersion and kernel-weighted penalties
- **Core assumption:** Tasks share structure via shared pretrained initialization; evaluation weights α may differ from merging weights β
- **Evidence:** Section 3.1 defines H_Q as measuring performance degradation; Theorem 2 includes dispersion and kernel-weighted penalty terms; corpus support from heterogeneous expert analysis
- **Break condition:** Disjoint task regions with minimal shared structure cause heterogeneity term to dominate

## Foundational Learning

- **Task Arithmetic / Model Merging**
  - Why needed: SAMerging builds on task vectors τ_t = θ_t - θ_0 and layer-wise coefficient optimization λ
  - Quick check: Can you explain why scaling and summing task vectors might yield multi-task capability, and when it fails?

- **Knowledge Distillation (KL Divergence)**
  - Why needed: Core objective L_KD minimizes KL(p_t||q_λ) between teacher and student output distributions
  - Quick check: Why use KL divergence rather than cross-entropy or MSE for distribution matching?

- **Sharpness-Aware Minimization (SAM)**
  - Why needed: SAM's min-max formulation finds flat basins through worst-case loss optimization
  - Quick check: How does SAM differ from standard SGD in its update rule, and what does ρ control?

## Architecture Onboarding

- **Component map:**
  Task Vector Extraction -> Merged Model Constructor -> KD Loss Module -> SAM Optimizer

- **Critical path:**
  1. Load pretrained θ_0 and fine-tuned {θ_t}
  2. Initialize λ (typically zeros or small values)
  3. For each epoch: construct θ_λ, compute KD loss, SAM ascent to find perturbation, SAM descent to update λ
  4. Return optimized λ; final model is θ_λ* with no runtime overhead

- **Design tradeoffs:**
  - KL objective vs. entropy (AdaMerging): KL provides explicit excess-risk bound; entropy is heuristic without guarantees
  - SAM vs. standard optimizer: SAM adds ~2× gradient computation cost but yields flatter minima and better generalization (Table 5: -1.0% drop without SAM)
  - Data budget k: Works with as few as 16 examples/task; more data helps but saturates quickly (Figure 4)

- **Failure signatures:**
  - Performance collapses when fine-tuned models have high mutual interference (disjoint task domains)
  - If λ diverges or merged model strays far from θ_0, NTK assumption breaks—monitor ‖θ_merge - θ_0‖
  - SAM sensitivity to ρ: too small → no flatness benefit; too large → optimization instability

- **First 3 experiments:**
  1. Reproduce TA-8 baseline: Merge 8 CLIP ViT-B/32 fine-tuned models with k=1600 unlabeled samples/task; verify ~87.1% average accuracy
  2. Ablate SAM vs. KL: Run (a) KL-only with Adam, (b) entropy+SAM, (c) KL+SAM to isolate contributions (expect Table 5 pattern)
  3. Data efficiency sweep: Test k ∈ {16, 160, 1600} on TALL-14; plot accuracy vs. samples to confirm 16-sample viability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SAMerging perform in settings with strong task interference, heavy domain shift, conflicting label spaces, or multi-label classification?
- Basis: "Even with more tasks, regimes with strong task interference or heavy domain shift (e.g., conflicting label spaces or multi-label settings) remain underexplored"
- Why unresolved: Only tested on standard multi-class classification benchmarks with non-overlapping label spaces
- Evidence needed: Experiments on benchmarks with overlapping/conflicting labels showing whether flatness-aware objective still tightens excess-risk bound under severe heterogeneity

### Open Question 2
- Question: Can SAMerging be extended to generative tasks beyond classification?
- Basis: "Our evaluation focuses solely on classification; extending it to generative tasks is left for future work"
- Why unresolved: Theoretical framework assumes bounded loss and convexity in scores; unclear if KL-based distillation transfers to sequential unbounded outputs
- Evidence needed: Application to merge instruction-tuned or code-generation LLMs with metrics like pass@k or human evaluation

### Open Question 3
- Question: Does the PAC-Bayes bound remain tight outside the NTK linearization regime, and how far can merged parameters deviate from the pretrained checkpoint before the bound loosens?
- Basis: "The analysis assumes a local NTK-style linearization, so behavior far from this regime is uncertain"
- Why unresolved: Lemma 2 and Lemma 3 rely on NTK approximations to pass from posterior-level to single-model bounds
- Evidence needed: Empirical measurement of gap between bound terms and actual generalization error as ||θ_merge – θ_0|| increases

### Open Question 4
- Question: Can lighter flatness proxies replace SAM to reduce calibration-time overhead without sacrificing generalization?
- Basis: "SAM adds calibration-time cost; lighter flatness proxies may reduce this overhead"
- Why unresolved: SAM requires additional forward-backward pass per step, doubling compute; alternative flatness measures unexplored
- Evidence needed: Ablation studies substituting SAM with cheaper approximations, reporting accuracy vs. calibration wall-clock time trade-offs

## Limitations

- NTK-based PAC-Bayes bound assumes local linearity around θ₀; breaks if merged models deviate substantially
- 16-sample data efficiency claim rests on unreported hyperparameter tuning with no ablation of calibration data quality
- Cross-task heterogeneity term H_Q is defined but empirical contribution is not quantified
- Experiments limited to CLIP and GPT-2 variants only, no coverage of dense encoder architectures or non-vision/nlp domains

## Confidence

- Mechanism 1 (KL tightening risk bound): High - bound is explicitly derived and loss is provably linked to risk
- Mechanism 2 (SAM finding flat minima): Medium - bound includes flatness term and loss landscape visuals support, but no ablations on SAM vs. standard optimizer outside Table 5
- Mechanism 3 (heterogeneity term): Low - term is theoretically defined but not empirically isolated or validated

## Next Checks

1. Reproduce TA-8 with varying ρ in SAM to confirm flatness vs. instability tradeoff
2. Ablate calibration data size (k=16, 160, 1600) on TALL-14 to quantify data efficiency claim
3. Compare SAMerging's bound terms to AdaMerging's entropy objective on same merged model to measure theoretical vs. empirical gap