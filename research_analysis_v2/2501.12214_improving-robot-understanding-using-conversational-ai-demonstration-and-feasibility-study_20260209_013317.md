---
ver: rpa2
title: 'Improving robot understanding using conversational AI: demonstration and feasibility
  study'
arxiv_id: '2501.12214'
source_url: https://arxiv.org/abs/2501.12214
tags:
- robot
- dialog
- error
- adaptive
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a conversational AI-based approach to improve
  robot understandability in human-robot interaction. The authors developed four levels
  of explanation (LOE) based on verbosity and justification to generate adaptive dialog
  when errors occur in a collaborative sorting task.
---

# Improving robot understanding using conversational AI: demonstration and feasibility study

## Quick Facts
- arXiv ID: 2501.12214
- Source URL: https://arxiv.org/abs/2501.12214
- Reference count: 7
- Primary result: 8 out of 10 participants resolved errors with adaptive dialog AD2 vs 5 in AD1

## Executive Summary
This paper presents a conversational AI-based approach to improve robot understandability in human-robot interaction. The authors developed four levels of explanation (LOE) based on verbosity and justification to generate adaptive dialog when errors occur in a collaborative sorting task. A Rasa-based conversational AI system was implemented to transition between LOEs based on user queries. In a feasibility study with 10 participants, the adaptive dialog successfully resolved errors, with 8 out of 10 participants resolving errors in the more advanced adaptive dialog (AD2) compared to 5 in AD1. The system demonstrates the potential of conversational AI to enhance robot understandability by providing context-appropriate explanations, though further user studies are needed to evaluate performance across different tasks and conditions.

## Method Summary
The study implemented a conversational AI system using Rasa to manage adaptive explanations during robot error scenarios in a collaborative sorting task. Four levels of explanation (LOE) were defined by combinations of verbosity (low/high) and justification (low/high). The system started with Low LOE on error detection and transitioned based on user queries ("what" or "why"). Two adaptive dialog configurations were tested: AD1 (progressing through Low→Medium1→Medium2) and AD2 (Low→Medium1→High). A Sawyer robot with RGB camera performed cube sorting using QR codes, with error types including incorrect items and out-of-range objects. User interactions occurred via tablet and GUI interface, with error resolution success as the primary metric.

## Key Results
- 8 out of 10 participants resolved errors with adaptive dialog AD2 compared to 5 in AD1
- The system successfully transitioned between LOE levels based on user queries
- Adaptive dialog approach demonstrated feasibility for improving robot understandability during error scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Progressive explanation levels reduce human-robot mental model disparity through query-triggered disclosure.
- Mechanism: When an error occurs, the system starts at low LOE (minimal information). User queries ("what" or "why") trigger transitions to higher LOEs, matching explanation depth to the user's information gap rather than providing uniform verbosity.
- Core assumption: Users have varying mental model gaps; some need only identification (verbosity), others need reasoning (justification), and the query type signals which gap exists.
- Evidence anchors:
  - [abstract] "The understandable robot requires a communicative action when there is disparity between the human's mental model of the robot and the robots state of mind."
  - [section] "To lessen the disparity between the state of mind of the robot and the human's mental model of the robot, we designed this verbal communication to be an adaptive system."
  - [corpus] Weak direct support; neighbor papers focus on demonstration learning alignment rather than explanation-based mental model repair.
- Break condition: If users cannot articulate their confusion as "what" or "why" queries, or if query interpretation fails, the adaptive transition logic cannot route to appropriate LOEs.

### Mechanism 2
- Claim: Combining high verbosity with high justification yields superior error resolution rates compared to progressing through intermediate levels.
- Mechanism: AD2 jumps directly to High LOE (high verbosity + high justification) on "why" queries, whereas AD1 stops at Medium2 (low verbosity + high justification). The additional verbosity in AD2 appears to provide actionable context missing from Medium2.
- Core assumption: When users ask "why," they benefit from both the reasoning and sufficient operational detail to act.
- Evidence anchors:
  - [abstract] "8 out of 10 participants resolving errors in the more advanced adaptive dialog (AD2) compared to 5 in AD1."
  - [section] Table II shows High LOE response: "Error I'm unable to reach the item on the table because it is outside my camera vision. Please move it inside the square" vs Medium2: "Error due to incorrect item. Swap the cube."
  - [corpus] No directly comparable mechanism in neighbors; related work on demonstration learning does not address explanation verbosity effects.
- Break condition: If the task requires minimal justification but high verbosity (or vice versa), the fixed LOE combinations may not match user needs. The paper does not test decomposed dimensions.

### Mechanism 3
- Claim: Conversational AI platforms (Rasa) can provide real-time, context-sensitive explanations without hardcoding all dialog paths.
- Mechanism: Rasa interprets user queries and maps them to predefined LOE responses via intent classification. This decouples error detection (robot system) from explanation generation (dialog system), enabling modular updates.
- Core assumption: Query phrasing maps reliably to "what" vs "why" intents; the paper notes fallback responses for unrecognized queries.
- Evidence anchors:
  - [section] "If the user asked, 'what is the error?' or a similar question, then the robot would respond with a medium1 LOE... If the user entered any other question or comment, the robot would respond by generating text such as: 'I am sorry, please ask different question.'"
  - [corpus] Neighbor paper on conversational alignment with AI (arXiv 2505.22907) discusses alignment in conversational agents but does not address HRI-specific explanation structures.
- Break condition: Intent classification errors could route users to wrong LOEs. The paper does not report classification accuracy or failure rates.

## Foundational Learning

- Concept: Mental models in HRI
  - Why needed here: The entire LOE framework is predicated on detecting and repairing gaps between the human's mental model of the robot and the robot's actual state.
  - Quick check question: Can you explain why a user might persistently misunderstand a robot's action even after seeing it repeated?

- Concept: Intent classification in conversational AI
  - Why needed here: The Rasa system must reliably distinguish "what" queries from "why" queries to trigger correct LOE transitions.
  - Quick check question: Given the query "What's going wrong here?", how would you classify its intent and what response strategy would it trigger?

- Concept: Explanation dimensions (verbosity vs justification)
  - Why needed here: The 2×2 LOE matrix is non-sequential; understanding that verbosity and justification are orthogonal design choices is essential for extending or modifying the system.
  - Quick check question: If a user asks "how do I fix this?", which existing LOE would address this, and what dimension might be missing?

## Architecture Onboarding

- Component map:
  - Robot layer: Sawyer cobot (7 DOF arm), RGB camera on end effector, hand-eye calibration for QR code detection
  - Dialog layer: Rasa conversational AI with intent classification for "what"/"why" queries
  - Interface layer: Tablet mounted on robot + GUI on user's computer
  - Logic layer: LOE selection module that maps error type + query intent → response template

- Critical path: Error detection → Rasa activation → Low LOE display → User query → Intent classification → LOE transition → User action → Error resolution

- Design tradeoffs:
  - AD1 vs AD2: AD1 provides more granular progression (4 levels accessible), AD2 provides faster path to full information but skips intermediate granularity
  - Fixed LOE combinations: Cannot independently tune verbosity and justification per task context
  - Small sample (n=10): Feasibility demonstration only; statistical significance not claimed

- Failure signatures:
  - Users asking unrecognized queries receive only fallback response ("I am sorry, please ask different question")
  - If QR code detection fails or camera is occluded, error type may be misclassified, triggering wrong explanation templates
  - No mechanism for users to request clarification if LOE response is insufficient

- First 3 experiments:
  1. Replicate the AD1 vs AD2 comparison with n≥30 participants to establish whether the 8/10 vs 5/10 difference is reproducible and statistically meaningful.
  2. Add logging for intent classification accuracy and unrecognized query rates to quantify conversational AI reliability under real user phrasing variability.
  3. Test a decomposed design where verbosity and justification are independently adjustable based on cumulative query history rather than single-query triggers.

## Open Questions the Paper Calls Out

- Does the adaptive dialog system with four LOEs improve robot understandability and task performance across different collaborative tasks beyond cube sorting?
- Would expanding LOEs to address additional question types (e.g., "when," "how") further enhance robot understandability and user satisfaction?
- What user characteristics (e.g., prior robotics experience, cognitive load preferences) predict which adaptive dialog type (AD1 vs. AD2) is most effective for error resolution?

## Limitations

- Small sample size (n=10) limits generalizability of results
- Intent classification accuracy is not reported, leaving uncertainty about real-world performance
- Fixed LOE combinations cannot adapt to tasks requiring independent tuning of verbosity and justification

## Confidence

- High confidence: The adaptive dialog successfully resolved errors (8/10 vs 5/10), and the 2×2 LOE matrix design is clearly specified with example responses.
- Medium confidence: The mechanism of query-triggered LOE transitions improving mental model alignment is plausible but not directly tested; the paper demonstrates feasibility rather than establishing causal relationships.
- Low confidence: The claim that combining high verbosity with high justification yields superior error resolution is based on a single comparison without statistical testing or investigation of intermediate levels' effectiveness.

## Next Checks

1. Replicate the AD1 vs AD2 comparison with n≥30 participants to establish whether the 8/10 vs 5/10 difference is reproducible and statistically meaningful.
2. Add logging for intent classification accuracy and unrecognized query rates to quantify conversational AI reliability under real user phrasing variability.
3. Test a decomposed design where verbosity and justification are independently adjustable based on cumulative query history rather than single-query triggers.