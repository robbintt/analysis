---
ver: rpa2
title: Peeking Into The Future For Contextual Biasing
arxiv_id: '2512.17657'
source_url: https://arxiv.org/abs/2512.17657
tags:
- entity
- biasing
- bias
- speech
- entities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a contextual biasing method for attention-based
  encoder-decoder (AED) speech recognition models that leverages multi-token prediction
  to improve recognition of rare named entities. The method predicts multiple future
  tokens in parallel, allowing the model to "peek into the future" and score potential
  candidate entities in a bias list.
---

# Peeking Into The Future For Contextual Biasing

## Quick Facts
- arXiv ID: 2512.17657
- Source URL: https://arxiv.org/abs/2512.17657
- Reference count: 0
- Primary result: 50.34% relative improvement in named entity word error rate on Librispeech

## Executive Summary
This paper introduces a contextual biasing method for attention-based encoder-decoder (AED) speech recognition that leverages multi-token prediction (MTP) to improve recognition of rare named entities. Instead of predicting only the next token, the model simultaneously predicts multiple future tokens, enabling it to "peek into the future" and score potential candidate entities in a bias list. The approach uses raw MTP logits directly for entity scoring without requiring additional entity encoders or cross-attention layers, significantly reducing architectural complexity. Experiments on Librispeech demonstrate substantial improvements in named entity recognition while maintaining general transcription accuracy.

## Method Summary
The method trains an AED model with K parallel MTP heads that predict tokens at positions s+1 through s+K, all conditioned on the same preceding context. For each named entity in the bias list, the model extracts logits from the appropriate prediction head for each subword and passes them through a learned scoring function f_θ to compute entity scores. During inference, the model uses a unified search space that combines the static vocabulary and dynamic bias list, with a gating mechanism based on the null entity probability to prevent over-biasing. The approach treats entities as indivisible candidates in the search space, mitigating the entity fragmentation problem common in traditional contextual biasing methods.

## Key Results
- 50.34% relative improvement in named entity word error rate (B-WER) on Librispeech
- 2.58% word error rate (WER) on general transcription, comparable to baseline AED model
- 4 MTP heads achieve optimal performance, with single-head showing no improvement

## Why This Works (Mechanism)

### Mechanism 1: Multi-Token Prediction Enables Lookahead for Entity Scoring
Predicting K future tokens in parallel provides the model with lookahead capability that improves scoring of candidate named entities by considering their fit within broader sequence context. At each decoding step s, K specialized prediction heads produce logits for tokens at positions s+1 through s+K, all conditioned on the same preceding context y_{≤s}. The conditional independence assumption P(y_{s+1:s+K}|y≤s, X) ≈ ∏_{k=1}^K P_k(y_{s+k}|y≤s, X) makes parallel prediction tractable. Evidence shows single-head MTP provides no improvement while 4-head achieves full gains.

### Mechanism 2: Direct Logit-to-Entity Score Mapping Eliminates Encoder Complexity
Raw multi-token prediction logits contain sufficient discriminative signal to score candidate entities without requiring separate entity encoders or cross-attention layers. For each entity E_n, the model constructs an entity logit vector by collecting the logit value corresponding to each subword from the appropriate prediction head. A learned scoring function f_θ maps this K-dimensional vector to a scalar entity score. Learned scoring substantially outperforms heuristic weighted sum approaches and prevents over-biasing that degrades overall WER.

### Mechanism 3: Unified Search Space with Gating Prevents Over-Biasing
Combining static vocabulary and dynamic bias list into a single search space with learned gating enables entity-level predictions while suppressing base model when entities are unlikely. The unified search space Q(i) uses P_e(∅) (null entity probability) as a gating mechanism: Q(i) = P_e(∅)·P_1(i) for vocabulary tokens, Q(i) = λ·P_e(i) for bias entities. When entity confidence is high, P_e(∅) decreases and suppresses base predictions. A confidence threshold γ prunes unlikely entities.

## Foundational Learning

- **Autoregressive Decoding in AED Models**: Understanding standard AED factorization P(y|X) = ∏ P(y_s|y_{0:s-1}, X) reveals why the model cannot natively consider future tokens when making current predictions—hence the need for MTP. Why needed: Shows limitation of standard decoding. Quick check: Why can't a standard autoregressive decoder use information about token y_{s+2} when predicting y_s?

- **Conditional Independence vs Autoregressive Dependencies**: The MTP mechanism trades autoregressive accuracy for parallel prediction capability. Understanding what information is lost helps evaluate when this approximation is acceptable. Why needed: Reveals tradeoff in MTP approximation. Quick check: What dependencies cannot be captured when approximating P(y_{s+1}, y_{s+2}|context) as P(y_{s+1}|context)·P(y_{s+2}|context)?

- **Logits vs Softmax Probabilities**: The method uses un-normalized logits for entity scoring rather than probabilities. Logits preserve relative magnitude information that softmax normalizes away, which matters for comparing scores across candidates. Why needed: Explains why logits are preferable to probabilities. Quick check: Why might raw logits be preferable to softmax probabilities when computing a learned combination of token scores?

## Architecture Onboarding

- **Component map**: Audio → Conformer Encoder → H_e → Decoder(y_{0:s-1}, H_e) → h_d^s → [4 MTP heads in parallel] → L_s ∈ R^{4×V} → Entity logit extraction p_n for each E_n → f_θ(p_n) → Softmax over entities → Unified search space Q → Greedy decode

- **Critical path**: The model processes audio through a 12-layer Conformer encoder, then uses a 6-layer transformer decoder with 4 parallel MTP heads to predict future tokens. Entity logits are extracted and scored via a learned FFN, then combined with vocabulary predictions in a unified search space for greedy decoding.

- **Design tradeoffs**: K=4 heads match data distribution (87% entities ≤4 tokens) but truncate longer entities; FFN heads outperform transformer heads for simplicity; decreasing loss weights for farther predictions reflect lower confidence in distant predictions.

- **Failure signatures**: No bias list causes WER degradation on test-other (6.25% vs 6.01% baseline); single MTP head shows no B-WER improvement; transformer MTP heads perform worse than baseline; heuristic scoring causes over-biasing and overall WER degradation.

- **First 3 experiments**: 1) Train AED-MTP baseline without biasing to verify MTP doesn't harm general recognition (2.58% WER). 2) Compare K=1, 2, 4 heads to quantify lookahead contribution. 3) Replace learned f_θ with weighted sum of logits to validate learned projection necessity.

## Open Questions the Paper Calls Out

- How can the performance degradation on general transcription be mitigated when the model is presented with an empty bias list?
- Does truncating entities longer than K tokens using only their first K tokens result in significant accuracy loss for those specific entities?
- Can the proposed unified search space be extended to support beam search decoding effectively?

## Limitations

- The K=4 MTP head configuration is optimized for Librispeech's entity length distribution and may not generalize to domains with longer entities or different entity type distributions.
- The model shows degraded WER on test-other when no bias list is provided, indicating learned dependency on biasing signals rather than maintaining general ASR robustness.
- Inference complexity scales linearly with bias list size, potentially creating computational bottlenecks for large entity lists.

## Confidence

- **High Confidence**: The core MTP lookahead mechanism is well-supported by ablation studies showing cumulative improvement from K=1 to K=4 heads.
- **Medium Confidence**: The 50.34% relative B-WER improvement is statistically significant within Librispeech but requires cross-domain validation.
- **Low Confidence**: Claims about broader applicability to Speech LLMs are speculative without experimental validation.

## Next Checks

1. Test the approach on conversational speech datasets (Switchboard, CHiME) and noisy environments to verify robustness beyond clean read speech.

2. Evaluate inference latency and memory usage with progressively larger bias lists (100, 1000, 10000 entities) to quantify computational overhead.

3. Train a variant where the model sees both bias-available and bias-unavailable utterances to measure whether this prevents the observed WER degradation when bias lists are absent.