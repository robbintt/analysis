---
ver: rpa2
title: Targeted perturbations reveal brain-like local coding axes in robustified,
  but not standard, ANN-based brain models
arxiv_id: '2509.23333'
source_url: https://arxiv.org/abs/2509.23333
tags:
- adversarial
- brain
- neural
- resnet-50
- robust
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the fragility of artificial neural network\
  \ (ANN) models used to predict brain responses, challenging the assumption that\
  \ high predictive accuracy alone indicates model quality. The authors systematically\
  \ characterize the local representational geometry of ANN-based brain models using\
  \ targeted adversarial probes\u2014small, imperceptible image perturbations designed\
  \ to maximally disrupt model predictions."
---

# Targeted perturbations reveal brain-like local coding axes in robustified, but not standard, ANN-based brain models

## Quick Facts
- arXiv ID: 2509.23333
- Source URL: https://arxiv.org/abs/2509.23333
- Reference count: 30
- Standard ANN-based brain models are fragile to small adversarial perturbations despite high predictive accuracy; robustified models exhibit stable, brain-aligned local coding axes.

## Executive Summary
This paper challenges the assumption that high predictive accuracy alone indicates quality in ANN-based brain encoding models. Through systematic adversarial probing, the authors reveal that standard models, despite high prediction scores, are unexpectedly fragile: small, imperceptible perturbations can drastically shift predicted neural responses, exposing unreliable local coding directions. In contrast, adversarially robustified models—trained to withstand such perturbations—exhibit stable local coding axes that better align with neural selectivity and produce semantically meaningful, transferable adversarial probes. These findings establish local representational geometry as a critical evaluation criterion, providing empirical support for favoring robust models whose stable coding axes not only align better with brain-like representations but also enable concrete, testable predictions for future neuroscience experiments.

## Method Summary
The authors evaluate the robustness of ANN-based brain encoding models by characterizing their local representational geometry using targeted adversarial perturbations. They extract features from 14 pre-trained ANNs (including ResNet-50, VGG16, CLIP, and robust variants) and fit linear ridge regression readouts to predict voxel responses in three high-level visual regions (FFA, PPA, EBA) from the Natural Scenes Dataset. For each voxel and image, they generate adversarial probes via iterative gradient descent to maximally perturb predicted responses within an L₂ budget (ε=5). They compute the Jacobian of the predicted response with respect to the input image, perform SVD to extract perturbation subspaces, and measure sensitivity and subspace overlap across models. The analysis reveals that standard models are highly sensitive to small perturbations and lack transferable perturbation subspaces, while robustified models exhibit stable, brain-aligned coding axes with semantically meaningful, transferable adversarial probes.

## Key Results
- Standard ANN-based brain models are unexpectedly fragile to small adversarial perturbations, despite high predictive accuracy.
- Sensitivity to adversarial probes is a stronger discriminator between candidate models than predictive accuracy alone.
- Robustified models exhibit stable local coding axes that better align with neural selectivity and produce semantically meaningful, transferable adversarial probes.

## Why This Works (Mechanism)

### Mechanism 1: Adversarial Probes Reveal Local Representational Geometry
Small, imperceptible perturbations can drastically shift predicted neural responses in standard ANN-based brain models, revealing unreliable local coding directions that diverge from biological stability. Adversarial probes optimize perturbation δ (bounded by ε) to maximize response change. In fragile models, the response landscape has steep, irregular local geometry—tiny input changes cause large output shifts. In robust models, the geometry is smooth, yielding stable predictions under perturbation. The Jacobian J = ∂r/∂x characterizes this: its singular values indicate sensitivity magnitude; its singular vectors indicate sensitive directions in pixel space. Biological visual representations are assumed to be locally smooth—perceptually invisible changes should not dramatically alter neural responses in high-level visual areas.

### Mechanism 2: Robust Training Aligns Local Coding Axes Across Architectures
Adversarially robustified models converge on shared local coding directions that transfer across architectures, whereas standard models occupy idiosyncratic, non-overlapping perturbation subspaces. Adversarial training flattens the loss landscape by exposing the model to perturbed examples during training. This regularizes the Jacobian structure—the top singular vectors (directions of maximal sensitivity) become more aligned across independently trained robust models. Standard models lack this constraint, so their sensitive directions are architecture-specific artifacts of optimization. The brain's true local coding axes are assumed to be not model- or architecture-specific; convergent representations across robust models reflect a closer approximation to biological computation.

### Mechanism 3: Linear Readout Inherits and Amplifies Base ANN Vulnerabilities
The linear mapping from ANN features to brain responses does not immunize the encoding model against adversarial fragility inherited from the underlying network. Encoding models use frozen features f(x) from a pretrained ANN and learn only a linear readout g via ridge regression. The Jacobian J = ∂g(f(x))/∂x = W·∂f(x)/∂x depends on both the learned weights W and the feature gradients. Adversarial directions in feature space project through W—if W amplifies features that are themselves fragile (high gradient magnitude), the brain model inherits that fragility. Ridge regression is assumed to prioritize predictive power, potentially upweighting features that happen to correlate with brain activity but are non-robust—idiosyncratic ANN artifacts rather than brain-like coding.

## Foundational Learning

- **L_p-bounded adversarial attacks**: The entire methodology hinges on generating perturbations δ constrained by ||δ||_p ≤ ε. L_2 bounds allow concentrated noise; L_∞ bounds force uniform per-pixel changes. Quick check: Given image x and L_2 budget ε=5, after a gradient step increases ||δ||_2 to 6.2, how do you project back to the feasible set?

- **Jacobian and singular value decomposition for sensitivity analysis**: The perturbation subspace P_i is spanned by the top-k right singular vectors of J = ∂r/∂x ∈ R^(m×p). These directions maximize ||Δr||_2 for bounded ||δ||. Quick check: If J has singular values [10, 3, 0.5, ...], what does the first right singular vector represent vs. the third?

- **Linear encoding models with ridge regression**: Brain models use ridge regression to map frozen ANN features to voxel responses. The regularization coefficient is chosen via nested cross-validation. Quick check: Why keep the encoder f frozen? What assumption does this make about the relationship between ANN features and brain representations?

## Architecture Onboarding

- **Component map**: Feature encoder f (pretrained ANN) → Linear readout g (ridge regression) → Adversarial probe generator (iterative gradient descent) → Jacobian analyzer (SVD) → Transfer evaluator (apply probe to target model)

- **Critical path**: Train encoding models by extracting features from ANN layers and fitting ridge regression readouts to NSD fMRI data; generate adversarial probes via iterative gradient descent to maximize/minimize predicted responses; compute sensitivity by recording maximum response change per voxel; test transfer by applying probes generated on source model to target model; analyze subspaces by computing Jacobian, performing SVD, and measuring overlap between model pairs.

- **Design tradeoffs**: Fixed budget vs. fixed effect (fixed ε reveals sensitivity differences but makes standard model probes uninterpretable; fixed target sensitivity yields interpretable robust-model probes but requires larger ε); L_2 vs. L_∞ constraints (L_2 concentrates perturbation on salient regions; L_∞ spreads uniformly; results highly correlated but model rankings shift slightly); single-step vs. iterative attacks (single-step probes dominant gradient direction; iterative explores curved regions but may exploit higher-order nonlinearities).

- **Failure signatures**: High predictivity, high sensitivity (standard model with fragile, non-brain-like coding axes); low transfer across models (distinct perturbation subspaces—models rely on architecture-specific features); probes appear as unstructured noise (model is fragile; ε too small to reveal semantic structure); robust model requires ε >> perceptual threshold to achieve target effect (over-regularized—smoothed past biologically relevant sensitivity).

- **First 3 experiments**: Train encoding models for ResNet-50 and robust ResNet-50 on NSD FFA voxels; compute adversarial sensitivity s_i at ε_attack=5 and verify robust model shows lower median sensitivity and shallower sensitivity-vs-ε curve; generate single-step probes on VGG16 for FFA and apply to CLIP ResNet-50 and robust ResNet-50 to quantify transfer; using robust ResNet-50, generate fixed-effect probes for PPA maximize/minimize conditions and visualize whether maximize probes enhance scene structure and minimize probes blur/remove scene elements.

## Open Questions the Paper Calls Out

- Do the semantically meaningful adversarial probes generated by robustified models (e.g., face-like transformations for FFA) actually modulate neural responses in human participants as predicted? The study is computational; the generated stimuli have not yet been tested in actual human or animal neuroimaging experiments.

- Can other model classes better capture large-scale representational structures in the brain, even if robustified models capture local coding directions? This study focused exclusively on local, small-scale geometry via adversarial probes; global representational structure was not assessed.

- What biological mechanisms produce robustness in the visual system, and do they resemble adversarial training in ANNs? The paper demonstrates behavioral/functional alignment but does not investigate mechanistic or developmental origins of that robustness.

## Limitations
- The findings are based on high-level ventral stream regions (FFA, PPA, EBA) with strong visual selectivity; sensitivity and transferability patterns might differ in early visual areas or non-visual regions.
- The L₂ budget ε=5 is chosen for technical tractability, but its correspondence to biologically relevant input variations remains unclear.
- The study does not account for potential confounds such as dataset biases, training hyperparameters, or implicit regularization effects in the linear readout that might influence sensitivity independently of representational geometry.

## Confidence
- **High confidence**: Standard ANN-based brain models are fragile to small adversarial perturbations despite high predictive accuracy. Robustified models exhibit more stable local coding axes that align better with neural selectivity and enable semantically meaningful, transferable adversarial probes.
- **Medium confidence**: The observed fragility and lack of transferability in standard models reflects non-brain-like representational geometry rather than mere overfitting or random noise.
- **Low confidence**: The specific choice of ε=5 optimally balances technical constraints and biological relevance.

## Next Checks
1. Replicate sensitivity and transferability analyses in early visual areas (V1, V2) and non-visual regions to test whether robustification consistently improves alignment with neural coding geometry across the brain.
2. Systematically vary ε to identify the smallest perturbation budget that yields interpretable, semantically meaningful probes in robust models, and verify these remain below human perceptual thresholds.
3. Test whether the same fragility patterns emerge when using different readout methods (e.g., deep linear networks, convolutional readouts) or training paradigms (e.g., supervised fine-tuning) to isolate whether linear regression specifically amplifies fragility.