---
ver: rpa2
title: Toward a Holistic Approach to Continual Model Merging
arxiv_id: '2509.23592'
source_url: https://arxiv.org/abs/2509.23592
tags:
- learning
- merging
- task
- arxiv
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses catastrophic forgetting in continual learning
  by proposing a holistic model merging framework that intervenes at three stages:
  pre-merging (fine-tuning in tangent space), during merging (leveraging optimizer
  states for functional information), and post-merging (representation alignment).
  The method avoids growing memory usage and data replay while improving task-specific
  weight disentanglement.'
---

# Toward a Holistic Approach to Continual Model Merging

## Quick Facts
- arXiv ID: 2509.23592
- Source URL: https://arxiv.org/abs/2509.23592
- Reference count: 40
- Primary result: Proposed holistic model merging framework for continual learning with 72.54% average accuracy on class-incremental tasks and 84.49% on domain-incremental ones

## Executive Summary
This paper introduces a three-stage framework for continual model merging that addresses catastrophic forgetting without requiring growing memory budgets or data replay. The method intervenes at three stages: pre-merging through tangent space fine-tuning, during merging via optimizer-based functional information, and post-merging with representation alignment. Evaluated across both class-incremental and domain-incremental benchmarks, the approach achieves competitive performance while maintaining constant memory usage.

## Method Summary
The framework operates on pre-trained models and sequentially processes tasks through three stages. First, task-specific fine-tuning is performed in the tangent space (linear regime) to amplify weight disentanglement. Second, merging combines task vectors using Adam's second-moment statistics as a proxy for Fisher Information Matrix, weighted by functional importance. Third, a post-hoc representation alignment step corrects feature space drift by optimizing a bias term between merged and previous representations. The entire process maintains constant memory Θ(1) and avoids data replay.

## Key Results
- Achieves 72.54% average accuracy on class-incremental learning benchmarks
- Achieves 84.49% average accuracy on domain-incremental learning benchmarks
- Outperforms or matches more memory-intensive approaches while maintaining constant memory usage
- Demonstrates competitive performance across multiple architectures and dataset combinations

## Why This Works (Mechanism)

### Mechanism 1: Linearization via Tangent Space Fine-Tuning (Pre-merging)
Constraining fine-tuning to the linear approximation of the network (tangent space) disentangles task-specific weights, reducing interference during merging. By optimizing in the Neural Tangent Kernel regime, the model evolves as a linear function of its parameters, preventing non-linear interactions that cause negative transfer when task vectors are added.

### Mechanism 2: Functional-Information Weighted Merging (During merging)
Leveraging Adam optimizer's second-moment statistics as a proxy for Fisher Information Matrix improves merging by preserving functional importance without data replay. The accumulated uncentered variance from Adam approximates parameter importance, allowing the merge to prioritize weights critical to the loss landscape of both previous and current tasks.

### Mechanism 3: Representation Bias Correction (Post-merging)
A post-hoc alignment step corrects representation drift caused by weight merging, ensuring feature consistency. By calculating the bias between pre-merge and post-merge feature representations on current task data and optimizing a correction, the model realigns the output space.

## Foundational Learning

- **Neural Tangent Kernel (NTK)**: Explains the pre-merging strategy. Understanding NTK is required to grasp why linearizing the network allows weight additions to sum functionally without non-linear interference. Quick check: How does the NTK assumption change the interpretation of "task vectors" compared to standard fine-tuning?

- **Fisher Information Matrix (FIM)**: Essential for the during merging stage. The method relies on approximating the FIM using optimizer states; understanding FIM explains why certain weights are preserved over others. Quick check: Why is the diagonal approximation of the FIM often preferred in scalable continual learning, and what information is lost?

- **Task Arithmetic**: Provides the basic operation (adding/subtracting weights) that this framework builds upon. Quick check: What is the "mode connectivity" assumption that allows merging to work, and how does this paper address its limitations?

## Architecture Onboarding

- **Component map**: Input (Pre-trained CLIP) -> Tangent Space Fine-Tuning -> State Capture (task vector + Adam moments) -> Fisher-weighted Merging -> Representation Alignment -> Output (merged model)

- **Critical path**: The implementation of Algorithm 1, Step 1 (Linear Fine-Tuning) is the most non-standard component. You must implement the forward pass as f(x; θ0 + τ) ≈ f(x; θ0) + τ⊤∇θf(x; θ0) rather than standard backprop on weights.

- **Design tradeoffs**: Constant Memory (Θ(1)) trades potentially higher accuracy of storing all past task vectors for fixed memory footprint. Adam-based approximation is faster but noisier than explicitly calculating the Fisher matrix on a replay buffer.

- **Failure signatures**: If t-SNE scatter plots show "triangle" (merged) and "circle" (old) points do not overlap, post-merging alignment is failing. Sudden accuracy drops after merging a specific task indicate linear fine-tuning implementation issues.

- **First 3 experiments**: 1) Sanity Check: Reproduce "Linear vs. NTK" plot on 2-task subset to verify linear fine-tuning reduces loss barriers. 2) Ablation: Run full pipeline with only "During Merging" to isolate Adam-weighted averaging contribution. 3) Visualization: Generate t-SNE plots before and after "Representation Alignment" to confirm representation mismatch reduction.

## Open Questions the Paper Calls Out

### Open Question 1
Can the method effectively handle downstream tasks requiring large parameter shifts that violate the linear fine-tuning assumption? The paper acknowledges that reliance on linear fine-tuning may limit modeling capacity for tasks with significant covariate shifts. This remains unresolved as evaluated benchmarks likely operate within the pre-trained model's "linear regime," masking potential performance collapse when non-linear updates are necessary.

### Open Question 2
How sensitive is the Fisher approximation to the optimizer's hyperparameters, specifically β2, in the continual setting? The method approximates the Hessian using Adam's second moment (vt) to determine merge weights, assuming it captures sufficient functional information. This remains unresolved as the paper assumes vt is sufficient but does not analyze how exponential moving average hyperparameters affect the accuracy of the merging priority compared to the true Fisher.

### Open Question 3
Does the sequential representation alignment accumulate error over very long task sequences? While effective for tested sequence lengths (up to 50 tasks), the paper does not analyze if small alignment errors compound over hundreds of tasks. This remains unresolved as a longitudinal analysis tracking representation bias magnitude and performance degradation across 100+ sequential tasks has not been conducted.

## Limitations

- Assumes NTK linear regime holds throughout fine-tuning, which may not generalize to large domain shifts or significant task differences
- Relies on Adam's second moments as a proxy for Fisher Information, which is computationally efficient but represents an approximation
- Post-merging representation alignment assumes current task data provides sufficient signal for correcting representation drift, which may be problematic in class-incremental scenarios

## Confidence

- **High Confidence**: The three-stage framework architecture and its sequential implementation are well-defined and reproducible
- **Medium Confidence**: Theoretical grounding for tangent space linearization and Adam-based Fisher approximation is sound, but practical performance may vary with task characteristics
- **Low Confidence**: Optimal λ value for variance accumulation and specific implementation details of representation alignment step remain unspecified

## Next Checks

1. Implement the linearization check: Reproduce NTK vs. standard fine-tuning comparison on a 2-task subset to verify the linear fine-tuning implementation reduces loss barriers as claimed

2. Test approximation fidelity: Run ablation experiments comparing Adam-weighted merging against both standard averaging and true Fisher-based weighting to quantify the approximation error

3. Validate representation correction: Generate t-SNE visualizations of feature embeddings before and after the post-merging alignment step to confirm the reduction of representation mismatch across multiple task pairs