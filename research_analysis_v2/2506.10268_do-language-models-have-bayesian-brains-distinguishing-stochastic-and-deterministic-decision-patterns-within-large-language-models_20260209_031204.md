---
ver: rpa2
title: Do Language Models Have Bayesian Brains? Distinguishing Stochastic and Deterministic
  Decision Patterns within Large Language Models
arxiv_id: '2506.10268'
source_url: https://arxiv.org/abs/2506.10268
tags:
- uni00000013
- uni00000011
- uni00000048
- language
- uni00000015
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper challenges the assumption that large language models\
  \ always make stochastic decisions during text generation. While prior work has\
  \ used iterated learning and simulated Gibbs sampling to infer LLM priors by assuming\
  \ models sample from posterior distributions, the authors show that under certain\
  \ conditions, models can behave near-deterministically\u2014converging to maximum\
  \ likelihood estimates even with non-zero sampling temperatures."
---

# Do Language Models Have Bayesian Brains? Distinguishing Stochastic and Deterministic Decision Patterns within Large Language Models

## Quick Facts
- arXiv ID: 2506.10268
- Source URL: https://arxiv.org/abs/2506.10268
- Reference count: 19
- Key finding: LLMs can behave near-deterministically during text generation, challenging methods that infer model priors by assuming stochastic sampling

## Executive Summary
This paper challenges the assumption that large language models always make stochastic decisions during text generation. While prior work has used iterated learning and simulated Gibbs sampling to infer LLM priors by assuming models sample from posterior distributions, the authors show that under certain conditions, models can behave near-deterministically—converging to maximum likelihood estimates even with non-zero sampling temperatures. This undermines previous methods for eliciting human-like priors, as deterministic processes can mimic stochastic sampling but produce misleading "false priors." To address this, the authors propose a simple method to distinguish stochastic from deterministic decision patterns by varying initial conditions and comparing resulting stationary distributions. Experiments across various models and tasks (proportion estimation and life expectancy prediction) reveal that models exhibit different decision patterns—some are stochastic, others deterministic, and some switch between modes.

## Method Summary
The paper introduces a diagnostic method to distinguish stochastic from deterministic decision patterns in LLMs by running iterated learning chains with varying initial conditions and comparing the resulting stationary distributions. For the CoinFlip task, chains are initialized with different numbers of heads (Ω₀ ∈ [0,10]) and iteratively updated by prompting the model to estimate heads in 100 tosses given the previous estimate, then sampling from the predicted proportion. For LifeExpectancy, chains start with different current ages and predict lifespan. The core hypothesis is that true Gibbs sampling should produce invariant stationary distributions regardless of initial conditions, while deterministic MLE-style processes will show distribution dependence on initial values. The method involves running multiple chains per initial condition, collecting θ values after burn-in, and comparing distributions visually and statistically.

## Key Results
- gpt-4o-mini exhibits deterministic decision-making on CoinFlip, with distributions diverging across different initial values
- claude-3-5-sonnet and LLaMA-3.1-70B-Instruct show stochastic behavior, with consistent distributions across initial conditions
- All models exhibit stochastic decision-making on LifeExpectancy, converging to similar distributions regardless of starting age
- Models can switch between stochastic and deterministic patterns depending on task semantics and structure

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deterministic decision-making in iterated learning produces stationary distributions that mimic Bayesian priors but are artifacts of the iterative procedure, not true model beliefs.
- Mechanism: Under MLE-style decision-making, the model outputs θᵢ = ωᵢ₋₁/N (proportion matching). The resulting Markov process ωᵢ ~ Binomial(N, ωᵢ₋₁/N) has absorbing states at ω=0 and ω=N, forcing convergence to extreme values regardless of the model's actual prior beliefs.
- Core assumption: The model generates outputs by deterministically matching observed frequencies rather than sampling from a posterior distribution.
- Evidence anchors:
  - [abstract]: "models can behave near-deterministically—converging to maximum likelihood estimates even with non-zero sampling temperatures"
  - [Section 3.1]: "This transition matrix has notable features. First, ωᵢ = 0 and ωᵢ = N are the only absorbing states"
  - [corpus]: Corpus neighbors focus on Bayesian inference methods but do not directly address the MLE artifact mechanism; weak external corroboration.
- Break condition: If models sample from true posteriors rather than applying MLE, the stationary distribution reflects actual priors and the artifact mechanism does not apply.

### Mechanism 2
- Claim: Varying initial conditions and comparing resulting stationary distributions reliably distinguishes stochastic from deterministic decision patterns in Gibbs sampling contexts.
- Mechanism: True Gibbs sampling converges to the same prior distribution p(θ) regardless of initial ω₀ (within reasonable ranges). Deterministic MLE processes converge to distributions dependent on E[ω₀], producing different stationary distributions for different initial conditions.
- Core assumption: Gibbs sampling guarantees invariant stationary distributions when initial values fall within reasonable ranges, and deterministic processes generally show dependence on initial conditions.
- Evidence anchors:
  - [Section 3.2]: "If the distribution remains consistent across different ω₀ values, the process is likely stochastic. If not, it is more likely deterministic."
  - [Section 4.2]: "gpt-4o-mini follows a deterministic decision-making pattern, with its distributions diverging across different initial values"
  - [corpus]: Related papers on Gibbs sampling (e.g., "Bregman geometry-aware split Gibbs sampling") assume standard convergence properties but do not address this diagnostic method.
- Break condition: If a deterministic process happens to converge to the same distribution for all initial values, this diagnostic would produce false negatives (though the paper notes this empirically signals specific priors).

### Mechanism 3
- Claim: Models can exhibit mixed decision patterns—stochastic on some tasks, deterministic or near-deterministic on others—with the same temperature setting.
- Mechanism: Task structure and prompt framing interact with model internals to produce varying decision modes. In LifeExpectancy, models avoid naïve MLE (predicting life expectancy = current age) by leveraging world knowledge, while in CoinFlip, some models default to frequency matching.
- Core assumption: Task semantics and model training influence whether outputs reflect probabilistic sampling or deterministic estimation strategies.
- Evidence anchors:
  - [Section 4.2]: "all models consistently converge to similar distributions [in LifeExpectancy], demonstrating stochastic decision-making"
  - [Section 4.2]: "gpt-4o consistently converges to a distribution focused at θ = 1...gpt-4o actively avoids predicting 0 heads"
  - [corpus]: No direct external evidence on task-dependent decision mode switching in LLMs.
- Break condition: If all tasks elicit consistent decision patterns from a given model, task-dependent mode switching does not occur.

## Foundational Learning

- Concept: **Gibbs sampling and stationary distributions**
  - Why needed here: The paper's core method relies on understanding how iterated learning approximates Gibbs sampling, and why invariant stationary distributions indicate true prior elicitation.
  - Quick check question: If you run Gibbs sampling from two different starting points, what should happen to the resulting distributions after sufficient iterations?

- Concept: **Maximum Likelihood Estimation vs Bayesian posterior sampling**
  - Why needed here: The paper shows MLE behavior produces different convergence properties than posterior sampling; distinguishing these is essential for interpreting results.
  - Quick check question: Given observed data, does MLE give you a single point estimate or a distribution over possible parameter values?

- Concept: **Markov chain absorbing states**
  - Why needed here: Understanding why MLE decision-making creates absorbing states at extremes (0, N) explains the bimodal "false prior" artifact.
  - Quick check question: In a Markov chain, what is an absorbing state and what happens to the chain once it enters one?

## Architecture Onboarding

- Component map:
  - Iterated learning loop: Prompt model with previous round's output → extract numerical response → sample new observation from Binomial → repeat
  - Stationary distribution estimator: Collect θ values after burn-in period (n > N threshold)
  - Initial condition diagnostic: Run multiple chains with different ω₀ values, compare resulting distributions
  - Model interface: Temperature=1.0 API calls with constrained output format (single numerical value)

- Critical path:
  1. Define task (CoinFlip: N=10, M=100; LifeExpectancy: age ranges)
  2. Run ≥10 chains per initial condition value (ω₀ ∈ [0,10] for CoinFlip)
  3. Discard early iterations (burn-in), collect stationary samples
  4. Compare distributions across initial conditions using visualization and statistical tests

- Design tradeoffs:
  - More initial condition values → better diagnostic power but higher API costs
  - Longer chains → more reliable convergence detection but diminishing returns
  - Temperature manipulation (not explored) could force more stochastic behavior but changes the phenomenon being studied

- Failure signatures:
  - Distributions that shift systematically with ω₀ (deterministic pattern detected)
  - Convergence to single absorbing state (e.g., always θ=1) suggests model bias against certain outputs
  - High variance across runs with same ω₀ indicates true stochasticity but requires more samples

- First 3 experiments:
  1. Replicate CoinFlip task on a small model (Gemma-2-2B-it) with ω₀ ∈ {0, 5, 10}, verify distributions differ (confirming deterministic pattern per paper findings)
  2. Run LifeExpectancy task on same model across multiple initial ages, verify distributions converge (confirming stochastic pattern)
  3. Test intermediate ω₀ values (1, 2, 3, 7, 8, 9) to map the full initial-condition-to-stationary-distribution relationship for deterministic-mode models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method for distinguishing stochastic from deterministic decision patterns generalize to tasks beyond proportion estimation and life expectancy prediction?
- Basis in paper: [explicit] "We leave further evaluations on the generalizability of our approach to other tasks for future work."
- Why unresolved: Experiments were limited to two tasks from prior work due to computational costs of experimenting with large language models.
- What evidence would resolve it: Application of the varying-initial-conditions method across diverse iterated learning tasks (e.g., causal strength estimation, mental representation recovery) showing consistent identification of decision patterns.

### Open Question 2
- Question: What architectural, training, or prompting factors cause different LLMs to exhibit stochastic versus deterministic decision patterns on the same task?
- Basis in paper: [inferred] The paper observes that gpt-4o-mini behaves deterministically while claude-3-5-sonnet and LLaMA-3.1-70B-Instruct behave stochastically on CoinFlip, but does not investigate why.
- Why unresolved: The paper focuses on identifying patterns rather than explaining their causes; model internals and training differences are not analyzed.
- What evidence would resolve it: Systematic experiments varying model size, training data, instruction-tuning procedures, and prompting formats to identify correlates of stochastic versus deterministic behavior.

### Open Question 3
- Question: Can valid priors be elicited from models that exhibit deterministic decision patterns, or must alternative methods be developed?
- Basis in paper: [inferred] The paper demonstrates that iterated learning produces "false priors" for deterministic models but does not propose corrective methods.
- Why unresolved: The work focuses on diagnosis rather than remediation; whether deterministic models possess recoverable priors at all remains unclear.
- What evidence would resolve it: Development and validation of alternative prior elicitation methods that account for or circumvent deterministic decision-making, producing results that align with known properties of the model's training distribution.

### Open Question 4
- Question: How does sampling temperature interact with the stochastic/deterministic distinction, given that near-deterministic behavior was observed even at temperature 1.0?
- Basis in paper: [inferred] All experiments used temperature 1.0, yet some models still exhibited near-deterministic behavior; the effect of lower or higher temperatures on this phenomenon is unexplored.
- Why unresolved: The experimental design held temperature constant, leaving the relationship between temperature settings and decision pattern classification unknown.
- What evidence would resolve it: Experiments systematically varying temperature across the same models and tasks, measuring whether higher temperatures induce stochastic behavior in otherwise deterministic models.

## Limitations

- The diagnostic method assumes deterministic processes always show initial-condition dependence, but pathological cases could converge to the same distribution despite being deterministic
- Task-specific behavior suggests models may use different reasoning strategies based on semantic content, but the boundary conditions for mode switching are unclear
- The study uses temperature=1.0 but doesn't explore how temperature tuning affects decision patterns

## Confidence

- **High confidence**: The mathematical mechanism of MLE producing absorbing states is well-established and clearly demonstrated
- **Medium confidence**: The initial-condition diagnostic reliably distinguishes stochastic from deterministic patterns, though edge cases may exist
- **Medium confidence**: Task-dependent mode switching is empirically observed but requires further investigation to understand underlying mechanisms

## Next Checks

1. Test the initial-condition diagnostic on synthetic Markov chains with known stochastic/deterministic properties to establish false positive/negative rates
2. Run controlled experiments varying temperature while holding task constant to map the stochastic-deterministic transition boundary
3. Implement the coinflip iterated learning procedure with multiple initial conditions on a small open-weight model to verify the divergence pattern predicted by the MLE artifact mechanism