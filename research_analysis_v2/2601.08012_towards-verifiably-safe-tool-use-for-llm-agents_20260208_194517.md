---
ver: rpa2
title: Towards Verifiably Safe Tool Use for LLM Agents
arxiv_id: '2601.08012'
source_url: https://arxiv.org/abs/2601.08012
tags:
- agents
- safety
- tool
- agent
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of guaranteeing safety in LLM-based
  AI agents that use external tools, focusing on preventing unintended or harmful
  interactions such as data leaks or unauthorized actions. The authors propose a process
  that combines safety engineering via System-Theoretic Process Analysis (STPA) to
  identify hazards and derive formal safety requirements, with information flow control
  (IFC) to enforce constraints on data flows and tool sequences.
---

# Towards Verifiably Safe Tool Use for LLM Agents

## Quick Facts
- arXiv ID: 2601.08012
- Source URL: https://arxiv.org/abs/2601.08012
- Reference count: 40
- Authors propose a process to formally verify safety specifications prevent unsafe data flows in LLM agents

## Executive Summary
This paper addresses the challenge of guaranteeing safety in LLM-based AI agents that use external tools, focusing on preventing unintended or harmful interactions such as data leaks or unauthorized actions. The authors propose a process that combines safety engineering via System-Theoretic Process Analysis (STPA) to identify hazards and derive formal safety requirements, with information flow control (IFC) to enforce constraints on data flows and tool sequences. They introduce an enhanced Model Context Protocol (MCP) framework requiring structured labels on tool capabilities, confidentiality, and trust. The approach formalizes safety requirements into symbolic specifications and enforces them through a four-tier structure (blocklist, mustlist, allowlist, confirmation) to balance safety and autonomy. Preliminary results using Alloy formal verification demonstrate that unsafe flows can be deterministically blocked while preserving agent capabilities.

## Method Summary
The authors combine STPA hazard analysis with IFC enforcement to create verifiable safety specifications for LLM agents. They adapt STPA to derive safety requirements from stakeholder values and potential losses, then formalize these as symbolic specifications. An enhanced MCP framework requires tools to be annotated with labels for capabilities, confidentiality, and trust. A four-tier enforcement structure (blocklist, mustlist, allowlist, confirmation) applies these specifications at runtime. The approach is validated through Alloy formal verification, which proves that unsafe information flows and temporal violations cannot occur while preserving safe agent behavior.

## Key Results
- Alloy verification confirms hazardous predicates (e.g., private data leaks) are unsatisfiable with enforced policies
- Four-tier enforcement structure balances safety guarantees with agent autonomy
- STPA-derived requirements successfully formalized as IFC and temporal specifications
- Preliminary results show deterministic blocking of unsafe flows while preserving safe execution traces

## Why This Works (Mechanism)

### Mechanism 1: STPA-Based Hazard Derivation
Systematically analyzing stakeholder values and inverting them into potential losses yields concrete safety requirements that can be formalized. The STPA process identifies stakeholders → derives their expected values → inverts values into potential losses → maps losses to system behaviors → prioritizes which losses to address as formal requirements. Core assumption: Task-specific agents have bounded enough scope that hazards can be comprehensively enumerated.

### Mechanism 2: Label-Enhanced Information Flow Control
Attaching structured labels (capabilities, confidentiality, trust) to tool outputs enables deterministic blocking of unsafe data flows. MCP tool declarations are extended with required key-value tags. Data inherits labels during propagation. A policy engine intercepts tool calls and checks label compatibility before execution. Core assumption: Labels are available and trustworthy at runtime; the enforcement entity is independent of the agent and cannot be bypassed.

### Mechanism 3: Four-Tier Enforcement with Formal Verification
A layered enforcement structure (blocklist → mustlist → allowlist → confirmation) combined with Alloy formal verification can guarantee unsafe flows are eliminated while preserving safe agent behavior. Specifications are enforced through four tiers. Blocklist auto-denies; mustlist requires actions; allowlist permits without confirmation; confirmation escalates to humans. Alloy verifies exhaustively within bounded traces that no hazardous predicates are satisfiable. Core assumption: Specifications correctly capture all safety-relevant constraints; the Alloy model faithfully represents system behavior.

## Foundational Learning

- **Information Flow Control (IFC)**
  - Why needed here: IFC is the core enforcement mechanism that tracks how data moves between tools and blocks unsafe propagations based on labels.
  - Quick check question: Can you explain why preventing private data from reaching `send_email` is an IFC constraint rather than an access control constraint?

- **STPA (System-Theoretic Process Analysis)**
  - Why needed here: STPA provides the systematic method for deriving safety requirements from stakeholder analysis, which is otherwise ad hoc in agent design.
  - Quick check question: What is the difference between a "loss" and a "hazard" in STPA terminology?

- **Temporal Logic Constraints**
  - Why needed here: SPEC2 ("each `update_event` must be followed by `send_email`") is a temporal ordering constraint, not a data flow constraint.
  - Quick check question: How would you express "action A must occur within 2 steps of action B" in temporal logic?

## Architecture Onboarding

- Component map:
  - Agent Core -> MCP Server -> Policy Engine -> Label Store
  - Agent Core: LLM that plans and invokes tools
  - MCP Server: Declares tools with required labels (capabilities, confidentiality, trust)
  - Policy Engine: Intercepts tool calls, checks SPECs, enforces four-tier structure
  - Label Store: Runtime label propagation and lookup

- Critical path:
  1. Developer performs STPA → derives REQs → formalizes as SPECs
  2. Tools are annotated with MCP labels at integration time
  3. At runtime, agent generates tool call → policy engine intercepts → checks labels against SPECs → applies appropriate tier (block/must/allow/confirm)
  4. Periodically, run Alloy verification to confirm policies cover all hazardous traces

- Design tradeoffs:
  - Safety vs. autonomy: Stricter policies (more blocklist, less allowlist) reduce risk but increase confirmation fatigue and limit capability.
  - Label precision vs. labeling effort: Richer labels enable finer-grained policies but require more annotation work.
  - Task-specific vs. general agents: Narrow scope enables comprehensive STPA; general agents make hazard enumeration intractable.

- Failure signatures:
  - Label propagation gaps: Private data appears in unexpected tool calls without blocking.
  - Specification incompleteness: New tool combinations enable hazardous flows not covered by existing SPECs.
  - Confirmation fatigue: Users auto-approve confirmations, undermining the escalation mechanism.

- First 3 experiments:
  1. Implement the calendar agent with the three tools (`list_events`, `update_event`, `send_email`), apply STPA to derive REQ1/REQ2, and verify Alloy blocks the Figure 1 leak.
  2. Extend MCP declarations for an existing tool (e.g., email API) with the three required labels and test label-based blocking of private-to-external flows.
  3. Compare three policy configurations (blocklist-heavy, allowlist-heavy, confirmation-heavy) on the same task, measuring task completion rate, confirmation count, and safety violations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can developers efficiently author and maintain structured labels for tool capabilities and confidentiality without incurring prohibitive manual overhead?
- Basis in paper: [explicit] The authors state they "will explore how developers can efficiently author and maintain labels through key-value tagging" as a primary next step.
- Why unresolved: While the framework requires structured metadata, the paper currently relies on manual labeling or assumptions, a known bottleneck in Information Flow Control systems.
- What evidence would resolve it: User studies or automated tooling demonstrations showing that accurate labels can be generated and updated with minimal developer effort.

### Open Question 2
- Question: What are the measurable tradeoffs between safety guarantees and agent utility when applying the four-tier enforcement structure (blocklist, mustlist, etc.) in real-world workflows?
- Basis in paper: [explicit] The authors plan to "evaluate our process... measuring coverage of risky interactions, usability impacts like notification fatigue, and tradeoffs between safety and utility."
- Why unresolved: The paper provides a formal Alloy model proving safety violations are blocked, but lacks empirical data on how often legitimate agent capabilities are restricted or interrupted.
- What evidence would resolve it: Empirical benchmarks showing task completion rates and frequency of user interruptions across diverse agent tasks.

### Open Question 3
- Question: Can an external policy engine effectively intercept tool calls and enforce specifications in real-world agent frameworks without introducing unacceptable latency?
- Basis in paper: [explicit] The authors plan to "design and implement an external policy engine that intercepts tool calls in agent frameworks."
- Why unresolved: The feasibility has been demonstrated only via offline formal modeling (Alloy), not in a live system where performance and integration constraints exist.
- What evidence would resolve it: A working implementation integrated into a standard agent framework (e.g., LangChain) with latency measurements.

## Limitations
- Empirical validation gap: The work relies entirely on Alloy formal verification rather than empirical deployment with real LLM agents and tool ecosystems.
- Labeling overhead: The proposed IFC approach requires comprehensive labeling of all tools and data flows, which may be impractical at scale.
- Task-specific scope: The STPA-based hazard analysis assumes bounded task scope; the approach may not scale to general-purpose agents.

## Confidence
- High confidence: The formal modeling approach and Alloy verification methodology are sound.
- Medium confidence: The four-tier enforcement structure represents a reasonable policy framework, but its practical effectiveness depends on implementation details.
- Medium confidence: The claim that hazardous flows can be "deterministically blocked" holds within the formal model but may face implementation challenges in real systems.

## Next Checks
1. Implement and deploy: Build the enhanced MCP framework with IFC enforcement and test it with an actual LLM agent (e.g., using OpenAI functions or LangChain) to validate the labeling and enforcement mechanisms work as specified.
2. Scale labeling experiment: Evaluate the labeling burden by applying the approach to a larger tool ecosystem (e.g., 20+ tools) and measure annotation time, maintenance overhead, and policy coverage.
3. Empirical safety evaluation: Compare the approach against baseline safety mechanisms (e.g., simple blocklists, prompt-based instructions) in realistic scenarios where agents must balance safety constraints with task completion.