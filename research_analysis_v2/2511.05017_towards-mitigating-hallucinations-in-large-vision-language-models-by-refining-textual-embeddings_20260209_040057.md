---
ver: rpa2
title: Towards Mitigating Hallucinations in Large Vision-Language Models by Refining
  Textual Embeddings
arxiv_id: '2511.05017'
source_url: https://arxiv.org/abs/2511.05017
tags:
- visual
- hallucinations
- visalign
- image
- textual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses hallucinations in large vision-language models
  (LVLMs), which arise from over-reliance on textual priors and underutilization of
  visual cues, leading to outputs that are linguistically fluent but visually inaccurate.
  The authors identify that the common practice of appending visual embeddings to
  pre-trained LLM input sequences introduces a modality imbalance, where the language-dominant
  LLM backbone under-attends to visual information during fine-tuning, resulting in
  hallucinations.
---

# Towards Mitigating Hallucinations in Large Vision-Language Models by Refining Textual Embeddings

## Quick Facts
- arXiv ID: 2511.05017
- Source URL: https://arxiv.org/abs/2511.05017
- Reference count: 15
- Achieves +9.33% accuracy on MMVP-MLLM and significant gains on hallucination benchmarks

## Executive Summary
This paper addresses hallucinations in large vision-language models (LVLMs) caused by over-reliance on textual priors and under-utilization of visual information. The authors identify that the standard practice of appending visual embeddings to pre-trained LLM input sequences creates a modality imbalance, where language-dominant LLM backbones under-attend to visual tokens during fine-tuning. They propose VisAlign, a simple yet effective method that refines textual embeddings by integrating average-pooled visual features, encouraging the model to learn visually-informed textual embeddings and promoting more balanced attention between text and vision.

## Method Summary
VisAlign addresses modality imbalance by refining textual embeddings before they enter the LLM. The method works by average-pooling projected visual embeddings into a global context vector, concatenating this vector to each textual token embedding, and projecting the fused representation back to the LLM's input dimension. This creates visually-conditioned textual tokens that promote balanced attention distribution. The training follows a two-stage process: first pretraining the projection layers while keeping the LLM frozen, then end-to-end fine-tuning.

## Key Results
- +9.33% accuracy improvement on MMVP-MLLM benchmark
- +2.99% accuracy and +1.76% precision on POPE-AOKVQA
- Up to +3.4% accuracy improvement on MERLIN
- Up to +3% accuracy improvement on hard split of HallusionBench
- Produces more balanced and structured attention patterns with consistent visual token attendance

## Why This Works (Mechanism)

### Mechanism 1
Injecting visual context directly into textual token embeddings mitigates the "modality imbalance" where LLM backbones inherently prioritize language priors. The method modifies the input representation space by concatenating a globally averaged visual vector to every textual token embedding before the LLM layers, forcing the self-attention mechanism to compute relevance based on fused visual-semantic features rather than text alone.

### Mechanism 2
Modifying textual embeddings improves visual grounding by redistributing attention weights across the multimodal sequence during inference. In standard LVLMs, attention maps show high concentration on text tokens and distinct "holes" where visual tokens are under-attended. VisAlign shifts this distribution by enriching text tokens with visual data, creating higher attention scores between text and vision tokens.

### Mechanism 3
Decoupling the projection of textual embeddings from the base LLM allows the model to learn a distinct "visual-language" manifold separate from the pure text space. The introduction of a specific projection layer after the visual-textual concatenation allows the model to reshape the input distribution specifically for multimodal tasks, preventing the "language-dominant" weights of the pre-trained LLM from projecting the visual features into a subspace that it effectively ignores.

## Foundational Learning

### Concept: Self-Attention Bias in Transformers
**Why needed here:** The paper's core diagnosis is that transformers naturally attend to "text-like" tokens. Understanding how Query/Key/Value matrices determine token importance is required to interpret the attention heatmap analysis.
**Quick check question:** Can you explain why appending visual tokens to the end of a sequence might result in them being "under-attended" by later text tokens in a standard decoder-only architecture?

### Concept: Modality Gap & Alignment
**Why needed here:** The authors propose "refining" embeddings to bridge the gap. You need to understand that visual embeddings (from CLIP/ViT) and text embeddings (from Llama/Vicuna) occupy different vector spaces and require projection/alignment.
**Quick check question:** What is the functional purpose of the "projection layer" mentioned in Equation 1, and how does VisAlign modify this pipeline?

### Concept: Feature Pooling Strategies
**Why needed here:** VisAlign relies on "average-pooled visual features" to create a global context vector. Understanding the trade-off between average pooling (global context, loss of spatial detail) versus patch-wise features is crucial.
**Quick check question:** Why would averaging all visual tokens be a "robust" choice for global grounding, but potentially insufficient for counting or localization tasks?

## Architecture Onboarding

### Component map:
Visual Encoder (Frozen) -> Vision Projector -> VisAlign Module (Aggregator + Fusion + Projector) -> LLM Backbone

### Critical path:
The specific data transformation in the VisAlign module: Input -> AvgPool(Vision) -> Concatenate(Repeat(Vision_Avg), Text) -> Linear_Project -> LLM_Input.

### Design tradeoffs:
- Avg Pooling vs. Attention: The authors admit average pooling is simple. It captures global semantics efficiently but risks losing spatial granularity.
- Compute: Minimal overhead (one linear layer and pooling op) compared to cross-attention models like Flamingo.

### Failure signatures:
- Hallucination of Absent Objects: If the model still fails on POPE benchmarks, check if the visual gradient is being dampened too much by the text embedding magnitude.
- Loss of Reasoning: If the model fails at logic tasks, the visual injection might be interfering with the LLM's pre-trained reasoning circuits.

### First 3 experiments:
1. Attention Visualization (Sanity Check): Run inference on a sample image-question pair and plot the attention heatmaps of the first 6 layers. Verify the "vertical bands" appear.
2. POPE Benchmark (Object Hallucination): Evaluate on POPE (Adversarial) to specifically test if the model learns to say "No" to objects not present in the image.
3. Ablation on Fusion Method: Replace "Average Pooling" with "Max Pooling" or "CLS token" extraction to determine if the gains come from the architecture or specifically the averaging.

## Open Questions the Paper Calls Out

### Open Question 1
Can advanced fusion mechanisms outperform the average-pooling approach used in VisAlign? The authors explicitly state that more sophisticated fusion methods could further enhance visual grounding and cross-modal alignment, leaving this exploration for future work.

### Open Question 2
Can larger-scale training datasets mitigate the performance trade-offs observed in knowledge-dependent tasks? The appendix notes VisAlign causes performance drops in categories like Landmark and Celebrity, speculating that these trade-offs could potentially be mitigated by training on larger-scale multimodal datasets.

### Open Question 3
Is VisAlign effective for LVLM architectures that utilize complex cross-attention rather than token concatenation? The paper focuses on concatenation-based models and contrasts them with cross-attention models like Flamingo, but does not validate the method on the latter architecture.

## Limitations
- The method relies on average pooling of visual features, which may not capture fine-grained spatial information necessary for counting or precise localization tasks
- Performance trade-offs observed in knowledge-dependent tasks (Landmark, Celebrity categories) suggest limitations in bridging visual grounding with parametric knowledge
- Lack of detailed hyperparameter specifications (learning rates, batch sizes, optimizer configurations) makes exact reproduction challenging

## Confidence

### High Confidence:
- The core mechanism of injecting averaged visual context into textual embeddings and its effectiveness in improving visual grounding (demonstrated through consistent performance gains across multiple benchmarks)

### Medium Confidence:
- The causal relationship between attention redistribution patterns and hallucination reduction
- The generalization across different LVLMs

## Next Checks

1. **Ablation on Fusion Method:** Replace average pooling with max pooling and CLS token extraction to determine if the gains come from the specific architecture or the general principle of visual-textual fusion.

2. **Cross-Model Validation:** Apply VisAlign to a different LVLM architecture (e.g., LLaVA-1.5 or MiniGPT-4) to test the claim of broad applicability across different model families.

3. **Fine-Grained Localization Test:** Evaluate on counting and localization-specific datasets (e.g., CLEVR or RefCOCO) to assess whether the average pooling approach retains sufficient spatial detail for precise visual reasoning tasks.