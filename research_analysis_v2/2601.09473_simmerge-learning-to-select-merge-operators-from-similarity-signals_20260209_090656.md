---
ver: rpa2
title: 'SimMerge: Learning to Select Merge Operators from Similarity Signals'
arxiv_id: '2601.09473'
source_url: https://arxiv.org/abs/2601.09473
tags:
- merge
- linear
- similarity
- operator
- slerp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "SimMerge learns to choose merge operators from inexpensive similarity\
  \ signals between checkpoints, avoiding costly merge-and-evaluate search. It predicts\
  \ which operator to apply (Linear, SLERP, TIES) and the merge order for multi-way\
  \ merges using KL divergence, cosine similarity, and \u21132 distance computed from\
  \ unlabeled probe sets."
---

# SimMerge: Learning to Select Merge Operators from Similarity Signals

## Quick Facts
- arXiv ID: 2601.09473
- Source URL: https://arxiv.org/abs/2601.09473
- Reference count: 40
- One-line primary result: SimMerge predicts optimal merge operators from similarity signals, closing 65.0% of the expert-off-domain performance gap versus 41.8% for best single fixed operator.

## Executive Summary
SimMerge addresses the challenge of selecting among different merge operators for combining fine-tuned LLMs by predicting operator utility from inexpensive similarity signals between checkpoints. Instead of costly merge-and-evaluate search, SimMerge computes KL divergence, cosine similarity, and ℓ2 distance on unlabeled probe data to predict which operator (Linear, SLERP, TIES) and merge order will yield the best downstream performance. The method generalizes across 2-, 3-, and 4-way merges of 7B-parameter models and transfers to 111B models without retraining. An online bandit variant adapts to new tasks and operators, closely tracking oracle performance while reducing regret.

## Method Summary
SimMerge learns to predict merge operator utility from similarity features computed on unlabeled probe data between model checkpoints. For pairwise merges, it computes KL divergence, activation/weight/attention cosine similarity, and ℓ2 distance from probe sets, then trains a lightweight MLP to map these features to per-operator utility scores. For multi-way merges, it decomposes plans into ordered pairwise steps and propagates similarity proxies using convexity bounds. An online contextual bandit variant uses a frozen feature encoder with per-operator Bayesian linear models for adaptation to new tasks. The approach avoids expensive iterative evaluation while achieving near-oracle merge performance.

## Key Results
- SimMerge closes 65.0% of the expert-off-domain performance gap versus 41.8% for the best single fixed operator across code, math, multilingual, and RAG tasks.
- Generalizes from 7B to 111B models without retraining, maintaining strong performance.
- Online bandit variant achieves near-oracle cumulative regret over 60 rounds, outperforming uniform random selection.
- Reduces expert degradation to −16.0% (3-way) and −17.9% (4-way) relative to fixed operators while maintaining positive auxiliary gains.

## Why This Works (Mechanism)

### Mechanism 1
Different merge operators succeed in distinct similarity regimes, and pre-merge signals can predict the optimal operator before evaluation. The paper computes four similarity metrics (KL divergence, weight cosine, attention cosine, ℓ2 distance) between model pairs on unlabeled probe data. Each metric exhibits operator-specific correlation patterns: e.g., high KL divergence favors SLERP while penalizing Linear; high weight cosine benefits TIES while hurting Linear in pairwise settings. A lightweight MLP learns to map these feature vectors to predicted per-operator utility scores.

### Mechanism 2
A predictor trained only on pairwise merges can score multi-way merge plans by decomposing them into ordered pairwise steps and propagating similarity proxies. A k-way merge plan π = (mi₁, …, miₖ) is scored by concatenating pairwise similarity features for each step. For intermediate merges involving previously-merged models, the paper uses convexity-based proxies (e.g., KL(a+b∥c) ≤ (1−α)KL(a∥c) + αKL(b∥c)) computed from the precomputed pairwise table.

### Mechanism 3
Online adaptation to new tasks and operators is achievable via a neural-linear contextual bandit with warm-start from offline data. The offline MLP encoder g_ϕ transforms similarity features into a representation z(s). During online deployment, this encoder is frozen, and per-operator Bayesian linear models (LinTS) are updated using observed rewards. Warm-start initializes posterals from full-information offline data; Thompson sampling balances exploration.

## Foundational Learning

**Concept: KL Divergence between predictive distributions**
- Why needed here: Primary functional similarity signal; asymmetric, sensitive to distributional differences in logits
- Quick check question: Given two models' softmax outputs p and q on a shared prompt, can you compute D_KL(p∥q) and explain why it's asymmetric?

**Concept: Merge operator properties (Linear vs. SLERP vs. TIES)**
- Why needed here: SimMerge selects among these; understanding their geometric assumptions clarifies regime dependence
- Quick check question: Why does SLERP preserve norms while Linear does not, and when might norm preservation help vs. hurt?

**Concept: Contextual bandits with Thompson sampling**
- Why needed here: Online variant uses LinTS for exploration-exploitation under partial feedback
- Quick check question: How does Thompson sampling differ from UCB in handling uncertainty, and why might it converge faster in small-action settings?

## Architecture Onboarding

**Component map:**
Feature extraction (probe data → KL, activation cos, attention cos, weight cos, ℓ2 distance) -> Offline selector (MLP) -> Per-operator utility scores -> Plan scorer (for k-way) -> Best plan selection

**Critical path:**
1. Gather probe data per task domain (unlabeled)
2. Cache per-model probe outputs (logits, activations, attention)
3. Compute pairwise similarity table for all model pairs
4. Train offline selector on observed 2-way merge outcomes
5. At inference: construct feature vector → predict best operator (and order for k>2)

**Design tradeoffs:**
- Task encoding vs. task-agnostic: Task encoding improves accuracy modestly (Appendix F.1: 87.5% vs 85.2% for Linear) but requires known task at inference
- Proxy strictness: Using upper bounds for KL and ℓ2 propagation is conservative; tighter approximations could improve plan ranking but add complexity
- Encoder freezing in bandit: Preserves offline knowledge but limits adaptation to genuinely novel similarity-outcome mappings

**Failure signatures:**
- Fixed operator severely underperforms auxiliary baseline (e.g., TIES at −10.2% GapClosed on Multilingual in Figure 2) → selector should avoid this regime
- Multi-way merge with random order degrades sharply vs. learned order (Figure 5: −47 pp on Code) → order prediction is critical
- Online bandit regret accumulates linearly → exploration may be insufficient or features uninformative for new domain

**First 3 experiments:**
1. **Sanity check**: On held-out 2-way merges, verify selector accuracy vs. random choice; expect >70% accuracy (paper reports 68–87% per operator)
2. **Ablation on proxy propagation**: Compare exact intermediate computation vs. proxy bounds on a small set of 3-way merges; measure ranking correlation
3. **Scale transfer test**: Apply 7B-trained selector to a few 3-way 111B merges; expect performance within 5–10% of retrained selector (paper shows successful transfer)

## Open Questions the Paper Calls Out

**Open Question 1**
Can SimMerge generalize its selection capability to merge operators outside the tested set (e.g., Task Arithmetic or Frankenstein merging)?
- Basis in paper: [explicit] The conclusion states future work should "expand the operator set."
- Why unresolved: The experiments were restricted to Linear, SLERP, and TIES; it is unclear if the similarity signals effectively predict the utility of structurally different merging algorithms.
- What evidence would resolve it: Empirical results showing SimMerge successfully selecting among a broader library of merge operators without retraining the underlying feature pipeline.

**Open Question 2**
Can the approximation methods for intermediate metrics be refined to improve accuracy in deeper merge trees?
- Basis in paper: [explicit] The conclusion identifies a need to "improve intermediate-metric propagation for deeper merge trees."
- Why unresolved: The current method relies on proxy upper bounds (e.g., using log-sum inequality for KL divergence) to estimate features for intermediate merged models, which may introduce noise in complex plans.
- What evidence would resolve it: A comparison of current proxy-based scoring against exact computation for intermediate steps, showing improved correlation with downstream utility in >4-way merges.

**Open Question 3**
Is it possible to achieve robust merge selection without task-specific encodings or calibration?
- Basis in paper: [explicit] The conclusion suggests a need to "explore task-agnostic selection that generalizes across evaluation suites with minimal calibration."
- Why unresolved: The current architecture appends a task encoding $c(t)$ to the input features, implying a dependency on knowing the target task domain.
- What evidence would resolve it: A study showing that a variant omitting $c(t)$ maintains high performance (comparable to the 65.0% gap closed) across entirely new, unlabeled task distributions.

## Limitations
- Feature extraction is highly engineered with multiple probe types and aggregation methods that are empirically tuned without theoretical justification.
- Reliance on probe data from specific task domains raises questions about performance on out-of-domain tasks or domains with limited unlabeled data.
- Proxy-based multi-way merge scoring assumes tight step-wise utility decomposition but lacks theoretical guarantees on bound tightness.

## Confidence
- **High confidence**: The core mechanism of using similarity signals to predict merge operator performance is well-supported by the correlation analysis (Appendix G, Figure 15) and ablation studies. The transfer of the selector from 7B to 111B models without retraining is a significant empirical result with clear evidence.
- **Medium confidence**: The proxy-based multi-way merge scoring works in practice but lacks theoretical guarantees on bound tightness. The online bandit variant's performance is promising (Figure 7) but relies on assumptions about feature transferability that are not fully validated.
- **Low confidence**: The optimal feature extraction strategy (which probe types, aggregation methods, and normalization) is not justified beyond empirical tuning. The paper does not address how sensitive the method is to probe data quality or distribution mismatch.

## Next Checks
1. **Probe data sensitivity**: Systematically vary probe set sizes and distributions across task domains to measure impact on selector accuracy and downstream merge quality. Compare performance when using task-specific vs. generic probe sets.
2. **Proxy bound tightness validation**: For a small set of 3-way and 4-way merges, compute exact intermediate merge utilities and compare rankings to those produced by the proxy-based method. Quantify the ranking correlation (e.g., Kendall tau) to assess proxy effectiveness.
3. **New operator adaptation test**: Introduce a novel merge operator (e.g., a weighted variant of SLERP) and evaluate whether the bandit variant can adapt to it using the existing feature encoder. Measure regret accumulation and final performance relative to retraining the encoder.