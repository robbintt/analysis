---
ver: rpa2
title: Separated Inter/Intra-Modal Fusion Prompts for Compositional Zero-Shot Learning
arxiv_id: '2501.17171'
source_url: https://arxiv.org/abs/2501.17171
tags:
- soft
- hard
- prompt
- pair
- fusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Separated Inter/Intra-Modal Fusion Prompts
  for Compositional Zero-Shot Learning (CZSL). CZSL aims to recognize novel compositions
  of state-object pairs, but existing methods using single pair prompts struggle with
  subtle semantic differences.
---

# Separated Inter/Intra-Modal Fusion Prompts for Compositional Zero-Shot Learning

## Quick Facts
- **arXiv ID:** 2501.17171
- **Source URL:** https://arxiv.org/abs/2501.17171
- **Authors:** Sua Jung
- **Reference count:** 0
- **Primary Result:** Introduces Separated Inter/Intra-Modal Fusion Prompts for CZSL, achieving 6.8% improvement in harmonic mean on MIT-States and state-of-the-art results on MIT-States, UT-Zappos, and C-GQA.

## Executive Summary
This paper introduces a novel method for Compositional Zero-Shot Learning (CZSL) that addresses the challenge of recognizing novel state-object compositions. The proposed approach employs separate soft prompts for states, objects, and pairs, combined with a Modal Fusion Synthesizer Block (MFSB) that performs Inter- and Intra-modal fusion through cross-attention mechanisms. The method significantly improves attribute recognition by providing dedicated representational channels for attribute and entity information, demonstrating state-of-the-art performance across three benchmark datasets.

## Method Summary
The method builds upon pre-trained CLIP models, introducing a three-pronged prompt system: hard pair prompts for compositional grounding, and separate soft prompts for individual states and objects. The Modal Fusion Synthesizer Block performs two-stage fusion - first Inter-modal fusion using cross-attention between visual and textual features, then Intra-modal fusion refining attributes and objects by exchanging relevant content between them. This architecture is trained with a multi-term loss function and demonstrates significant improvements in compositional generalization.

## Key Results
- Achieves 6.8% improvement in harmonic mean on MIT-States dataset compared to baseline
- Demonstrates state-of-the-art performance across MIT-States, UT-Zappos, and C-GQA datasets
- Shows optimal configuration uses "Hard Pair + Soft Object/Attribute" prompts with Inter-then-Intra fusion order

## Why This Works (Mechanism)

### Mechanism 1: Separated Prompt Granularity
- Claim: Decomposing a single pair prompt into distinct hard pair, soft state, and soft object prompts improves compositional generalization by providing dedicated representational channels for attribute and entity information.
- Core assumption: The semantic meaning of a state or object can be more effectively learned and generalized when isolated from its pair context via dedicated soft prompts.
- Evidence: Abstract mentions "separate soft prompts for states, objects, and pairs"; ablation study shows "Hard {Pair}, Soft {Obj}, Soft {Attr}" achieves highest HM (20.35) and AUC (7.33).

### Mechanism 2: Inter-Modal Fusion via Cross-Attention
- Claim: Fusing visual and textual features through cross-attention enriches representation of each modality by grounding text in visual evidence.
- Core assumption: Simple cosine similarity between encoded image and text is insufficient for CZSL; deeper bidirectional fusion via cross-attention is necessary.
- Evidence: Abstract mentions Inter-modal fusion uses cross-attention; ablation shows "Inter-Fusion Only" (HM 19.80) outperforms "No Fusion" (HM 19.10).

### Mechanism 3: Intra-Modal Fusion for Contextual Refinement
- Claim: Exchanging information between state and object features of same modality refines their representations by incorporating relational context.
- Core assumption: Attributes and objects are contextually related; explicitly modeling this relationship improves prediction of feasible compositions.
- Evidence: Abstract mentions Intra-modal fusion refines features by exchanging relevant content; ablation shows "1. Inter 2. Intra" order achieves best performance (HM 20.35).

## Foundational Learning

- **Vision-Language Models (VLMs) & CLIP**: Why needed - entire method built on pre-trained CLIP as foundational encoder. Quick check - explain how CLIP aligns images and text in shared latent space.
- **Prompt Learning (Hard vs. Soft Prompts)**: Why needed - core contribution involves specific configuration of hard and soft prompts. Quick check - explain difference between hard prompt "a photo of a dog" and soft prompt.
- **Compositional Zero-Shot Learning (CZSL)**: Why needed - specific problem being solved requiring recognition of novel combinations of known primitives. Quick check - explain how CZSL differs from standard Zero-Shot Learning.

## Architecture Onboarding

- **Component map**: Image & Labels → CLIP Encoders → Initial Features → MFSB (Inter-Modal Fusion) → Fused Features → MFSB (Intra-Modal Fusion) → Refined Features → Similarity Scoring → Cross-Entropy Loss Calculation → Total Loss Aggregation → Backpropagation
- **Critical path**: Image `x` & Labels `(s, o)` → CLIP Encoders → Initial Features (`v`, `t_pair`, `t_attr`, `t_obj`) → MFSB (Inter-Modal Fusion) → Fused Features → MFSB (Intra-Modal Fusion) → Refined Features → Similarity Scoring → Cross-Entropy Loss Calculation → Total Loss Aggregation → Backpropagation
- **Design tradeoffs**: Prompt Configuration (Hard Pair + Soft Obj/Attr optimal but sacrifices some flexibility), Fusion Complexity (both Inter and Intra increases cost), Loss Weighting (critical hyperparameters α, β, γ require tuning)
- **Failure signatures**: Overfitting to seen compositions, Attention Collapse in MFSB, Imbalanced Loss Contribution
- **First 3 experiments**: 1) Sanity Check - No Fusion vs. Full Model, 2) Ablation - Prompt Types (all hard vs. all soft), 3) Component-wise Evaluation (seen vs. unseen accuracy separation)

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions, but based on the analysis, several key uncertainties remain regarding the generalizability of the optimal fusion order, computational efficiency tradeoffs, and robustness to semantically ambiguous domains.

## Limitations

- **Dataset Dependency**: Reported improvements are primarily benchmarked on MIT-States, with smaller datasets like UT-Zappos showing only marginal gains, raising questions about robustness across diverse data regimes.
- **Implementation Sensitivity**: Method's effectiveness critically depends on unspecified architectural details like number of learnable tokens in soft prompts and cross-attention configurations, requiring manual tuning.
- **Generalizability Concerns**: The optimal "Hard Pair + Soft State/Object" prompt configuration's effectiveness in domains with high semantic ambiguity remains unproven.

## Confidence

- **High Confidence**: The core architectural claim that decomposing prompts into separate soft/hard variants and using two-stage fusion strategy is beneficial for CZSL.
- **Medium Confidence**: The specific quantitative improvements (6.8% HM on MIT-States, best AUC on C-GQA), though limited diversity of evaluation introduces uncertainty.
- **Medium Confidence**: The stated mechanism that Intra-modal fusion refines representations by exchanging attribute-object context, though the paper doesn't fully explain why the fusion order is critical.

## Next Checks

1. **Generalization Robustness Test**: Evaluate performance on held-out subset of MIT-States where state-object pairs have high semantic similarity but are unseen during training to verify model isn't relying on spurious correlations.
2. **Architectural Sensitivity Analysis**: Systematically vary number of learnable tokens in soft prompt prefixes and hidden dimensions/number of heads in MFSB cross-attention modules to quantify method's sensitivity to unspecified hyperparameters.
3. **Cross-Dataset Transfer Learning**: Train on one dataset (MIT-States) and directly evaluate on another (UT-Zappos) without fine-tuning to test whether learned soft prompts capture truly generalizable compositional primitives.