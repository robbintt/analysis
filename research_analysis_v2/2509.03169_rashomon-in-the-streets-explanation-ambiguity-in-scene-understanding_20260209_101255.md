---
ver: rpa2
title: 'Rashomon in the Streets: Explanation Ambiguity in Scene Understanding'
arxiv_id: '2509.03169'
source_url: https://arxiv.org/abs/2509.03169
tags:
- rashomon
- scene
- agreement
- action
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the Rashomon effect in action prediction
  for autonomous driving, where multiple equally accurate models provide divergent
  explanations for the same prediction. Using Qualitative Explainable Graphs (QXGs)
  as a symbolic scene representation, the authors train two model classes: interpretable
  gradient boosting models and complex graph neural networks (GNNs).'
---

# Rashomon in the Streets: Explanation Ambiguity in Scene Understanding

## Quick Facts
- arXiv ID: 2509.03169
- Source URL: https://arxiv.org/abs/2509.03169
- Reference count: 6
- This paper demonstrates that explanation ambiguity is an inherent property of action prediction in autonomous driving, not just a modeling artifact.

## Executive Summary
This paper investigates the Rashomon effect in action prediction for autonomous driving, where multiple equally accurate models provide divergent explanations for the same prediction. Using Qualitative Explainable Graphs (QXGs) as a symbolic scene representation, the authors train two model classes: interpretable gradient boosting models and complex graph neural networks (GNNs). They measure explanation agreement using Fleiss' kappa and Kendall's W across varying top-k features. Results show moderate agreement (kappa 0.6-0.9) for interpretable models on top features, but significantly lower agreement (kappa ~0.1-0.2) for GNNs. This demonstrates that explanation ambiguity is an inherent property of the problem, not just a modeling artifact, suggesting the need for new approaches that embrace explanation multiplicity rather than seeking single "ground-truth" rationales.

## Method Summary
The study uses nuScenes dataset combined with DriveLM relevance annotations, transforming scenes into Qualitative Explainable Graphs (QXGs) using three qualitative calculi: Qualitative Distance, Qualitative Trajectory, and Rectangle Algebra. The dataset yields 2,131 scenes with 5 frames each. Two model classes are trained: 100 LightGBM gradient boosting models (pair-based) and 116 Graph Attention Networks (GNNs) on the full QXG. Models are selected based on ε-Rashomon sets with ε=0.05, meaning they achieve within 95% of best validation performance. SHAP values are computed for feature importance, and explanation agreement is measured using Fleiss' kappa (for top-k features) and Kendall's W (for full feature rankings).

## Key Results
- Interpretable models show moderate agreement (kappa 0.6-0.9) on top features, while GNNs show significantly lower agreement (kappa ~0.1-0.2)
- For full feature rankings, interpretable models achieve Kendall's W of 0.32-0.40, while GNNs only reach 0.07-0.14
- Top-k feature agreement is higher than full ranking agreement, suggesting models converge on dominant features while diverging on secondary attributions
- Explanation ambiguity persists even among correct predictions, indicating it's an inherent problem property

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explanation ambiguity emerges from model multiplicity within equally-performing model sets.
- Mechanism: Multiple models trained on identical data with similar accuracy can encode different internal decision boundaries, producing divergent feature attributions for the same prediction. The Rashomon set captures this multiplicity by including all models within ε tolerance of a reference model's loss.
- Core assumption: The validation Rashomon set approximates the population Rashomon set, and feature attributions from SHAP faithfully represent model reasoning.
- Evidence anchors:
  - [abstract] "results reveal significant explanation disagreement... explanation ambiguity is an inherent property of the problem, not just a modeling artifact"
  - [Page 6, Table 1] Kendall's W ranges from 0.07 (GNN all predictions) to 0.40 (pair-based correct predictions), showing low-to-moderate agreement even among correct predictions
  - [corpus] Related work on Rashomon sets confirms explanations can disambiguate models but also reveals ambiguity challenges
- Break condition: If models achieve near-perfect agreement (κ > 0.95), the Rashomon effect is not the dominant source of explanation variance.

### Mechanism 2
- Claim: Interpretable models exhibit higher explanation agreement than black-box GNNs due to constrained capacity and training dynamics.
- Mechanism: Gradient boosting trees have capacity coupled to training performance via early stopping, limiting representational redundancy. GNNs have fixed upfront capacity with multiple internal pathways to represent the same function, enabling overparameterization-induced divergence.
- Core assumption: The difference in agreement reflects model class properties rather than implementation artifacts.
- Evidence anchors:
  - [Page 6] "Where tree-based model training has its capacity bundled to its performance when using early stopping, this is difficult for black-box models like the used GNNs"
  - [Page 5-6, Figure 5] Pair-based models achieve κ=0.6-0.9 on top features; GNNs achieve only κ~0.1-0.2
  - [corpus] Weak corpus signals directly comparing interpretable vs. black-box explanation stability in this domain
- Break condition: If early stopping is disabled for gradient boosting or capacity constraints are added to GNNs and agreement gaps persist, the mechanism is not capacity-driven.

### Mechanism 3
- Claim: Top-k feature agreement is higher than full ranking agreement because models converge on dominant features while diverging on secondary attributions.
- Mechanism: Primary causal features (e.g., nearby pedestrian) are strongly constrained by data; secondary features (e.g., distant objects, fine-grained trajectory details) have weaker supervision signals, allowing model-specific spurious correlations or symmetries to influence rankings.
- Core assumption: Human relevance annotations capture primary causes, and secondary feature disagreement reflects genuine ambiguity rather than noise.
- Evidence anchors:
  - [Page 5-6] "the more features we consider in the ranking, the more disagreement we observe... This could indicate that there are symmetries in the data"
  - [Page 6, Table 1] Full ranking Kendall's W is substantially lower than top-k Fleiss' kappa for both model classes
  - [corpus] No corpus evidence on top-k vs. full ranking stability trade-offs
- Break condition: If synthetic data with controlled symmetries shows agreement remaining high across all rankings, the mechanism is incorrect.

## Foundational Learning

- **Rashomon Sets and Model Multiplicity**
  - Why needed here: Understanding that ε-tolerance defines which models belong to the Rashomon set is essential for interpreting agreement metrics and selecting appropriate ε values.
  - Quick check question: Given a reference model with 85% validation accuracy and ε=0.05, what minimum accuracy qualifies a model for the Rashomon set?

- **Qualitative Spatial-Temporal Calculi**
  - Why needed here: QXG construction relies on qualitative calculi (distance, trajectory, rectangle algebra) to encode scene relationships; understanding threshold-based relation assignment is critical for feature engineering.
  - Quick check question: If two objects' centroids are at distances that fall between thresholds θ₂ and θ₃ under Qualitative Distance Calculi, what relation label applies?

- **Feature Attribution Agreement Metrics**
  - Why needed here: Fleiss' kappa measures categorical agreement on top-k features; Kendall's W measures rank correlation. Selecting the right metric determines what aspect of explanation stability you assess.
  - Quick check question: A Kendall's W of 0.14 indicates what level of ranking agreement among models?

## Architecture Onboarding

- **Component map**: QXG Builder -> Pair-based Pipeline (edge lists -> LightGBM -> SHAP) / Graph-based Pipeline (QXG -> GAT -> action classification -> Integrated Gradients) -> Agreement Analyzer (Fleiss' kappa/Kendall's W)

- **Critical path**: 1. Dataset preparation: nuScenes + DriveLM annotations -> QXG generation (5 frames per scene) 2. Model training: Train 100+ models per class with different seeds -> select Rashomon set (ε=0.05) 3. Explanation extraction: Run SHAP on all selected models for labeled scenes 4. Agreement computation: Aggregate κ and W statistics across scenes and model classes

- **Design tradeoffs**: Interpretable (pair-based) vs. black-box (GNN): Pair-based offers higher explanation stability (κ 0.6-0.9 vs. 0.1-0.2) but loses cross-object context during edge-level classification; ε selection: Smaller ε tightens Rashomon set, potentially increasing agreement but reducing diversity; k in top-k analysis: Higher k captures more features but introduces noise.

- **Failure signatures**: Negative Fleiss' kappa: Systematic disagreement beyond chance; possible class imbalance or feature scale issues; Kendall's W near 0 with high accuracy: Models learn equivalent functions via different internal representations; overparameterization suspected; Large κ gap between "all predictions" and "correct predictions": Wrong predictions have inconsistent reasoning.

- **First 3 experiments**: 1. Reproduce baseline agreement metrics: Train 10 pair-based and 10 GNN models, compute κ and W on 100 scenes; verify pair-based κ > 0.5 and GNN κ < 0.3 on top-5 features 2. Ablate model capacity: Reduce GNN hidden dimensions by 50%; hypothesize increased agreement (higher W) if overparameterization is causal 3. Inject controlled symmetries: Create synthetic scenes with two equivalent causes (e.g., two pedestrians at equal distance); measure whether models systematically diverge on which cause is ranked higher

## Open Questions the Paper Calls Out

- Can a "consensus explanation" be constructed that aggregates feature importance across a Rashomon set to provide a more robust rationale than any single model?
- Does the observed explanation ambiguity stem from true multi-causal symmetries in the data or from spurious correlations learned through overparameterization?
- Can explanation variance within a Rashomon set be utilized as a reliable metric for uncertainty quantification in safety-critical predictions?

## Limitations

- Explanation ambiguity was measured only on human-annotated "causative" features, which may not capture all dimensions of model disagreement
- The absolute magnitude of ambiguity (kappa ~0.1-0.2 for GNNs) depends heavily on the attribution method and scene representation
- The contrast between interpretable and black-box models may reflect implementation choices rather than inherent properties

## Confidence

- High: Explanation ambiguity exists and is measurable; interpretable models show higher agreement than GNNs on top features
- Medium: The Rashomon set captures meaningful model diversity; explanation multiplicity is an inherent problem property
- Low: The specific quantitative thresholds (kappa values, Kendall's W ranges) generalize beyond this dataset and attribution method

## Next Checks

1. Test explanation agreement on synthetic data with known symmetries to isolate whether SHAP or the underlying model causes observed divergence
2. Compare multiple attribution methods (Integrated Gradients, LIME) on the same model sets to verify SHAP-specific artifacts
3. Measure agreement across different ε-tolerance levels to determine if tighter performance bands reduce explanation ambiguity