---
ver: rpa2
title: 'Large Language Model-Based Agents for Software Engineering: A Survey'
arxiv_id: '2409.02977'
source_url: https://arxiv.org/abs/2409.02977
tags:
- agents
- code
- agent
- software
- llm-based
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey systematically analyzes 124 papers on Large Language
  Model (LLM)-based agents for Software Engineering (SE). It categorizes these agents
  from both SE and agent perspectives, examining their application across SE tasks
  like requirements engineering, code generation, testing, debugging, and end-to-end
  development.
---

# Large Language Model-Based Agents for Software Engineering: A Survey

## Quick Facts
- arXiv ID: 2409.02977
- Source URL: https://arxiv.org/abs/2409.02977
- Reference count: 40
- Key outcome: Systematic analysis of 124 papers on LLM-based agents for SE, categorizing by task and architecture, identifying iterative refinement and multi-agent collaboration as key success factors.

## Executive Summary
This survey systematically analyzes 124 papers on Large Language Model (LLM)-based agents for Software Engineering (SE). It categorizes these agents from both SE and agent perspectives, examining their application across SE tasks like requirements engineering, code generation, testing, debugging, and end-to-end development. The survey highlights that LLM-based agents extend standalone LLMs by incorporating planning, memory, perception, and action components, enabling them to tackle complex SE problems through multi-agent collaboration and human interaction.

## Method Summary
The survey employs an iterative keyword search on DBLP, backward/forward snowballing, author verification, and collaborative manual screening to identify and validate 124 relevant papers. It provides qualitative categorization and synthesis, quantitative distribution analysis of papers over time and venues, and identifies open challenges. The approach relies on meta-analysis of existing literature rather than primary empirical data.

## Key Results
- LLM-based agents extend standalone LLMs by incorporating planning, memory, perception, and action components
- Predominant applications in code generation and quality assurance tasks, with iterative refinement strategies proving crucial
- Challenges include evaluation standardization, integration with traditional SE tools, and the need for better human-agent collaboration

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLM-based agents achieve higher effectiveness than standalone LLMs in software engineering by decomposing complex tasks and iteratively refining outputs based on environmental feedback.
- **Mechanism:** The "Planning" component breaks down high-level goals into sub-tasks, while "Perception" and "Action" enable a feedback loop where the agent executes code, observes errors or test failures, and corrects its own outputs. This shifts from one-time generation to a "plan-generate-refine" cycle.
- **Core assumption:** The foundation LLM possesses sufficient reasoning capabilities to interpret error messages and adjust plans dynamically; the feedback from tools is accurate and interpretable.
- **Evidence anchors:**
  - [abstract] "The survey highlights that LLM-based agents extend standalone LLMs by incorporating planning... enabling them to tackle complex SE problems through... iterative refinement strategies."
  - [section 4.2] "Current studies primarily leverage the capabilities of agents to plan and take actions, thus transforming one-time code generation into a 'plan-generate-refine' model..."
  - [corpus] The neighbor paper "Agentic Software Issue Resolution with Large Language Models" supports the efficacy of this loop for maintenance tasks.

### Mechanism 2
- **Claim:** Integrating external tools via the "Action" component extends the utility of LLMs beyond text processing to actual software manipulation and validation.
- **Mechanism:** Agents utilize an "Action" interface to invoke external resources—such as static analysis tools, dynamic execution environments, or version control systems. This allows the agent to perform operations and retrieve precise state information that the LLM could not generate internally.
- **Core assumption:** The tool APIs are reliable, and the agent can correctly map natural language intents to specific function calls or shell commands.
- **Evidence anchors:**
  - [section 5.1.4] "The action component of existing LLM-based agents for SE primarily involves using external tools to extend their capabilities... including Searching Tools, File Operation, GUI Operation, Static Program Analysis..."
  - [abstract] "...enhancing LLMs with the capabilities of perceiving and utilizing external resources and tools."
  - [corpus] "A Survey on Code Generation with LLM-based Agents" corroborates the critical role of tool augmentation for code generation tasks.

### Mechanism 3
- **Claim:** Simulating software development teams via multi-agent collaboration improves task coverage and error detection in complex, end-to-end workflows.
- **Mechanism:** By assigning distinct roles (e.g., Product Manager, Developer, Tester) to different agents, the system emulates human team dynamics. This separation of concerns allows for specialized "horizontal" (peer review) or "vertical" (pipeline) collaboration.
- **Core assumption:** Agents can maintain consistent role personas and communicate effectively; the overhead of multi-agent coordination does not outweigh the benefits of specialization.
- **Evidence anchors:**
  - [section 4.7.2] "Imitating real-world software development teams, multi-agent systems... often assign different roles to tackle specialized sub-tasks... including Managers, Designers, Developers, and Quality Assurance Experts."
  - [section 5.2.2] "Collaboration mechanism... can generally be divided into... Layered Structure... [and] Circular Structure [for] generation-validation style loop."
  - [corpus] Weak specific evidence in neighbors for the "role-playing" taxonomy specifically, though "Agentic Software Issue Resolution" supports multi-agent resolution strategies.

## Foundational Learning

- **Concept:** **Agent Architecture Components (Planning, Memory, Perception, Action)**
  - **Why needed here:** The survey organizes the entire field around this taxonomy. Understanding that an agent is not just an LLM, but a system of modules that *plan*, *remember*, *see*, and *act*, is prerequisite to analyzing the 124 papers.
  - **Quick check question:** Can you distinguish between the "Perception" module (receiving GUI screenshots or code context) and the "Action" module (executing a shell command or calling an API)?

- **Concept:** **Feedback Loop Strategies (Reflexion, Self-Repair)**
  - **Why needed here:** A major finding of the survey is that iterative refinement drives success in code generation and debugging. You must understand how model feedback (self-correction) differs from tool feedback (compiler errors).
  - **Quick check question:** In a "Self-Debugging" scenario, does the agent rely on *model feedback* (reasoning about the code) or *tool feedback* (reading a stack trace)?

- **Concept:** **Software Engineering Lifecycle Stages**
  - **Why needed here:** The survey categorizes agents by the SE phase they address (e.g., Requirements Engineering vs. Static Checking). You need to map agent capabilities to the correct stage to understand their constraints.
  - **Quick check question:** If an agent is designed to generate UML diagrams from user stories, which SE phase is it automating?

## Architecture Onboarding

- **Component map:** Brain (LLM Core) -> Planner -> Memory -> Perception -> Action
- **Critical path:**
  1. **Input:** User Requirement (e.g., "Fix bug #42")
  2. **Preprocessing:** Agent parses the issue description and potentially screenshots (Perception)
  3. **Localization:** Agent uses Search/Static Analysis tools (Action) to find relevant files
  4. **Planning:** Agent formulates a patch strategy (Planning)
  5. **Execution:** Agent writes code and runs tests (Action)
  6. **Refinement:** If tests fail, Agent analyzes logs (Perception), retrieves history (Memory), and updates the plan (Planning)
  7. **Output:** Validated Patch
- **Design tradeoffs:**
  - **Single-Agent vs. Multi-Agent:** Single agents are simpler and cheaper; Multi-agents offer specialization but require complex orchestration and memory sharing
  - **General vs. Specialized LLMs:** General models (GPT-4) are versatile but may lack domain-specific knowledge; specialized models require fine-tuning data which may be scarce
  - **Tool Reliability:** Deep integration with static analysis tools improves precision but introduces fragility if tools crash or produce verbose output
- **Failure signatures:**
  - **Infinite Loops:** Agents repeating "thank you" or "I will fix it" without progress
  - **Hallucinated Tools:** Agent attempting to call a tool or function that does not exist
  - **Context Overflow:** Long debugging sessions exceeding the token limit, causing the agent to "forget" the original requirement
- **First 3 experiments:**
  1. **Simple Code Generation with Tool Feedback:** Implement a single-agent loop where the agent writes a Python function, runs it in a sandbox, receives the error trace, and attempts a fix. Test if *tool feedback* outperforms *zero-shot* generation.
  2. **Role-Based Code Review:** Set up a 2-agent system (Coder vs. Reviewer). Provide the Coder with a buggy snippet and the Reviewer with a style guide. Measure if the final code quality improves over a single agent.
  3. **Repository Navigation:** Give an agent a file system tool (`ls`, `cat`) and a bug report. Task it with finding the relevant file without reading the whole repo. This tests the *Localization* and *Perception* coupling.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can fine-grained evaluation metrics be designed to assess the intermediate states and trustworthiness (e.g., robustness, security) of LLM-based agents, rather than relying solely on final task success rates?
- **Basis in paper:** [Explicit] The paper states in Section 6 that current evaluations focus on success rates "without delving into the intermediate states," making it difficult to analyze failures, and that requirements like robustness and security remain "underexplored."
- **Why unresolved:** Existing metrics treat the agent workflow as a "black-box," failing to capture decision-making quality or potential safety violations during execution.
- **What evidence would resolve it:** A standardized evaluation framework that includes error-related metrics (e.g., ratios of erroneous actions) and trustworthiness scores alongside traditional pass rates.

### Open Question 2
- **Question:** How can high-quality benchmarks be constructed to reflect the scale and complexity of real-world software engineering, given the limitations of current datasets like SWE-bench?
- **Basis in paper:** [Explicit] Section 6 highlights that current benchmarks suffer from quality issues (e.g., vague or incomplete descriptions) and fail to capture real-world complexity because the generated software is typically "small in scale."
- **Why unresolved:** Real-world SE tasks involve complex dependencies and large codebases that existing benchmarks, often limited to single functions or small projects, do not simulate effectively.
- **What evidence would resolve it:** New benchmarks derived from large-scale industrial repositories that require multi-file editing and contain comprehensive, unambiguous issue descriptions.

### Open Question 3
- **Question:** To what extent does training foundation LLMs on full software development lifecycle (SDLC) data—such as design documents, developer discussions, and runtime logs—improve agent performance compared to training on code alone?
- **Basis in paper:** [Explicit] Section 6 notes that current agents rely on general or code-specific LLMs, but "software is not just about code" and valuable data from the entire development life cycle remains "largely untapped."
- **Why unresolved:** Agents currently lack the broader context and domain knowledge contained in non-code artifacts, which limits their ability to handle complex tasks like architecture design.
- **What evidence would resolve it:** Comparative studies showing performance improvements in end-to-end development tasks when agents utilize foundation models trained on enriched SDLC datasets.

## Limitations
- The survey is a meta-analysis that does not conduct original experiments to validate proposed mechanisms
- Taxonomy and categorizations may overemphasize trends from higher-profile venues or miss emerging patterns
- Rapid field evolution means some findings may become outdated quickly

## Confidence
- **High Confidence:** The categorization framework (Planning, Memory, Perception, Action) is well-grounded in surveyed literature and consistently applied
- **Medium Confidence:** Multi-agent collaboration benefits have theoretical support and some case studies, but empirical evidence for performance gains over single agents is limited
- **Low Confidence:** Specific evaluation metrics and their effectiveness across different SE tasks are poorly standardized in literature

## Next Checks
1. **Implementation Replication:** Attempt to reproduce a minimal agent system using the most common architectural patterns (iterative refinement with tool feedback) to verify the claimed effectiveness of the feedback loop mechanism.

2. **Evaluation Standardization:** Analyze whether the proposed evaluation frameworks (Section 5.3) can be consistently applied across different SE tasks by testing them on at least three distinct agent implementations from the literature.

3. **Multi-Agent Coordination:** Design a controlled experiment comparing single-agent vs. two-agent (specialized roles) systems on a concrete SE task to empirically validate the claimed benefits and overhead costs of role-based collaboration.