---
ver: rpa2
title: Metamorphic Testing of Large Language Models for Natural Language Processing
arxiv_id: '2511.02108'
source_url: https://arxiv.org/abs/2511.02108
tags:
- equivalence
- metamorphic
- testing
- llms
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first comprehensive study of metamorphic
  testing (MT) for large language models (LLMs) on natural language processing tasks.
  The authors conducted a systematic literature review, collecting 191 metamorphic
  relations (MRs) across 24 NLP tasks.
---

# Metamorphic Testing of Large Language Models for Natural Language Processing

## Quick Facts
- arXiv ID: 2511.02108
- Source URL: https://arxiv.org/abs/2511.02108
- Reference count: 40
- First comprehensive study of metamorphic testing (MT) for LLMs on NLP tasks, revealing 18% average failure rate across 24 tasks

## Executive Summary
This paper presents the first systematic study of metamorphic testing for large language models on natural language processing tasks. The authors conducted a literature review to collect 191 metamorphic relations across 24 NLP tasks and implemented LLMORPH, a framework applying MT to three popular LLMs (GPT-4, LLAMA3, HERMES2). Using four datasets and 36 representative MRs, they found MT effectively exposes faulty LLM behaviors without requiring labeled ground truth, detecting 11% of failures missed by traditional testing. The study demonstrates MT's value particularly when labeled data is limited, while highlighting challenges with false positives and the need for better input transformation quality.

## Method Summary
The study implemented LLMORPH to apply metamorphic testing to LLMs across four NLP tasks: question answering (SQuAD2), natural language inference (SNLI), sentiment analysis (SST2), and relation extraction (RE-DOCRED). They collected 191 metamorphic relations through literature review, implemented 36 representative MRs, and tested three LLMs using 1,000 samples per dataset. Input transformations were generated using either function-based methods (character swaps, synonym replacement) or LLM-based approaches (HERMES2 via few-shot prompting). Output relations were evaluated using BERT-based semantic similarity with calibrated thresholds (0.8 for equivalence, 0.4 for difference). Violations were flagged when input relations held but output relations failed, with manual validation determining true positive rates.

## Key Results
- MT detected faulty LLM behaviors with an average failure rate of 18% across all tasks and models
- MT identified 11% of failures that traditional testing with labeled data missed, demonstrating complementarity
- Manual validation revealed a 62% true positive rate for reported failures, with the remaining 38% being false positives
- Input transformation errors caused 43% of false positives, while output comparison errors caused 57%
- Failure rates varied significantly by task: QA showed highest sensitivity (λ=0.18-0.27) while sentiment analysis showed lowest (λ=0.06-0.11)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Metamorphic relations serve as pseudo-oracles that detect faulty LLM behaviors without requiring labeled ground truth.
- Mechanism: Define an input relation Ri (e.g., paraphrase) and an expected output relation Ro (e.g., equivalence). Execute the LLM on both source and follow-up inputs. If Ri holds but Ro fails, flag an oracle violation.
- Core assumption: LLMs should produce consistent outputs for semantically equivalent inputs under the same task prompt.
- Evidence anchors:
  - [abstract] "MT can expose faulty behaviors without the need for explicit oracles (e.g., labeled datasets)"
  - [Section II] "Whenever a given input relation Ri holds between two or more inputs, a corresponding output relation Ro is expected to hold between the outputs"
  - [corpus] Related work on bias detection in LLMs via MRs confirms generalizability of this approach (paper 77002)
- Break condition: When MR definitions are task-specific or when input transformations alter meaning beyond the intended semantic perturbation.

### Mechanism 2
- Claim: LLM-based input transformations produce more grammatically valid and task-appropriate follow-up inputs than rule-based methods.
- Mechanism: Use an auxiliary LLM (HERMES2 in this study) with few-shot prompting to apply transformations like paraphrasing or synonym substitution. The LLM generates contextually appropriate text that satisfies the input relation.
- Core assumption: The transformation LLM is sufficiently capable of preserving semantic content while modifying surface form.
- Evidence anchors:
  - [Section IV - LLMORPH implementation] "LLMs, unlike traditional techniques, consistently outputs text that is grammatically correct and thus viable test cases"
  - [Section V] False positives from input transformation errors were the most common category, indicating transformation quality directly impacts effectiveness
  - [corpus] Weak corpus evidence on transformation quality comparison; related work focuses on MR selection rather than transformation methods
- Break condition: When transformation LLM introduces unintended semantic drift or fails to satisfy the input relation (e.g., changing meaning too much in paraphrase tasks).

### Mechanism 3
- Claim: BERT-based semantic similarity thresholds can approximate equivalence and difference output relations for free-form text outputs.
- Mechanism: Compute cosine similarity between output embeddings using PARAPHRASE-MINILM-L6-V2. Apply threshold of 0.8 for equivalence relations and 0.4 for difference relations, calibrated to retain 75% of manually-validated true positives.
- Core assumption: Embedding similarity correlates with semantic equivalence for the specific NLP task domain.
- Evidence anchors:
  - [Section IV - LLMORPH implementation] "If the similarity score is above a threshold, we consider the texts to be semantically equivalent"
  - [Section V - Discussion] "In QA tasks, 31% of the false positives was due to the cosine similarity not being able to recognise the equivalence between 'unknown' and a similar response"
  - [corpus] No corpus evidence on alternative output comparison methods; related papers assume similar embedding-based approaches
- Break condition: When LLM outputs are valid but use different vocabulary or phrasing that embeddings fail to recognize as equivalent (e.g., "unknown" vs. "cannot determine").

## Foundational Learning

- Concept: **Oracle Problem in Software Testing**
  - Why needed here: The entire motivation for MT stems from the difficulty of determining correct outputs for NLP tasks without human labeling.
  - Quick check question: Given an LLM response to a question, can you programmatically determine if it's correct without knowing the expected answer?

- Concept: **Metamorphic Relations (Input/Output Relations)**
  - Why needed here: Understanding the formal structure Ri → Ro is essential for implementing and debugging MRs in LLMORPH.
  - Quick check question: If input x2 is a paraphrase of x1, what output relation should hold between f(x1) and f(x2) for a sentiment analysis task?

- Concept: **Semantic vs. Syntactic Equivalence in NLP**
  - Why needed here: Choosing between syntactic (exact string match) and semantic (embedding similarity) output comparison depends on task type.
  - Quick check question: For a translation task, would you use syntactic or semantic equivalence to compare outputs? What about a classification task with fixed labels?

## Architecture Onboarding

- Component map:
  Input sampler -> Transformation engine -> Task executor -> Output comparator -> Violation reporter

- Critical path:
  1. Sample source input from dataset
  2. Apply input transformation → generate follow-up input
  3. Verify input relation Ri holds (discard if not)
  4. Execute both inputs on target LLM with identical prompt
  5. Compare outputs against output relation Ro
  6. Report violation if Ri=True and Ro=False

- Design tradeoffs:
  - Function-based transformations are deterministic and fast but limited to simple perturbations
  - LLM-based transformations produce natural text but introduce nondeterminism and require rate limit management
  - Higher similarity thresholds reduce false positives but may miss genuine violations
  - Using the same LLM for transformation and testing (HERMES2 case) may introduce bias

- Failure signatures:
  - **FPi (Input transformation FP)**: Transformation changed too much/little relative to MR intent
  - **FPo (Output comparison FP)**: Semantic similarity failed to recognize equivalent/different outputs
  - **FPo:qa**: QA-specific—"unknown" responses not recognized as equivalent
  - **FPo:re**: RE-specific—multiple valid relation labels not recognized as equivalent
  - **Flakiness**: 28% of violations reproduced 10/10 runs; 38% reproduced ≤5/10 runs

- First 3 experiments:
  1. Run MR-9 (word insertion) and MR-152 (negation addition) on a small QA sample (100 inputs) to compare failure rates and validate your LLMORPH setup against reported λ=0.17 and λ=0.27
  2. Manually label 30 violations from MR-142 (asymmetric relation swap) to calibrate your true positive expectations—reported TP rate is ~5% with λ=0.80, indicating high FP risk
  3. Test semantic similarity thresholds on your specific task domain by comparing BERT similarity scores against human judgment for 50 output pairs to validate whether 0.8/0.4 thresholds are appropriate for your use case

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can false positive metamorphic violations be effectively detected and filtered automatically?
- Basis in paper: [explicit] The authors state in the Conclusion that "false positive metamorphic violations is still a major challenge" and "Future research should focus on detecting and filtering FPs to improve the effectiveness of metamorphic testing for LLMs."
- Why unresolved: The study found a significant false positive rate (~38%) caused by ambiguous input transformations or output comparison errors (e.g., semantic similarity misclassification), which currently requires costly manual validation.
- What evidence would resolve it: A method that automatically classifies violations as true or false positives with high accuracy, or a filtering technique that significantly reduces the human inspection burden without discarding true faults.

### Open Question 2
- Question: What is the impact of using a dedicated, high-performance LLM for input transformations on the reliability of metamorphic testing?
- Basis in paper: [explicit] The authors identify a threat to validity where input transformations were performed by HERMES 2, which was also a model under test. They note: "Performing more experiments possibly using a dedicated LLM for the input transformation is an important future work."
- Why unresolved: Using the same or a lower-capability model for generating test inputs may introduce biases or fail to generate grammatically correct and semantically precise transformations, potentially affecting the failure rates observed.
- What evidence would resolve it: Comparative experiments where input transformations are generated by a state-of-the-art model (e.g., GPT-4) versus the current approach, analyzing the resulting difference in failure rates and false positive rates.

### Open Question 3
- Question: Can a catalog of task-independent metamorphic relations be developed to effectively test fine-tuned LLMs across different domains?
- Basis in paper: [explicit] The Conclusion notes that "it is important to find more of such task-independent MRs to more effectively test these systems automatically" because LLMs are increasingly being fine-tuned for specific tasks.
- Why unresolved: While the study identified some MRs that work across tasks, it also found that MR effectiveness varies significantly depending on the specific NLP task (e.g., QA vs. RE), making universal applicability difficult.
- What evidence would resolve it: A study identifying a set of MRs that maintain high true positive rates and acceptable failure rates across a diverse set of NLP tasks without requiring task-specific tuning.

## Limitations

- High false positive rate in free-form tasks: Semantic similarity metrics struggle with QA and RE outputs, with TP rates dropping to 5-31% for certain MRs
- Transformation quality dependency: 43% of false positives stemmed from input transformation errors, heavily impacting MT effectiveness
- Limited generalizability: Results based on four NLP tasks and three LLMs may not extend to other model architectures or task domains

## Confidence

- **High confidence**: The fundamental mechanism of using metamorphic relations as pseudo-oracles without labeled data is well-supported. The 18% average failure rate and detection of 11% of failures missed by traditional testing are robust findings.
- **Medium confidence**: The reported true positive rate of 62% assumes the manual validation process was comprehensive, but the study doesn't specify the sample size or validation methodology in detail.
- **Low confidence**: The optimal threshold values (0.8 for equivalence, 0.4 for difference) are based on calibration for this specific study and may not generalize across different NLP tasks or embedding models.

## Next Checks

1. **Replicate failure rate variation**: Test MR-9 (word insertion) and MR-152 (negation addition) on a small QA sample to validate the reported failure rates of λ=0.17 and λ=0.27 against your LLMORPH implementation.
2. **Calibrate semantic similarity thresholds**: Compare BERT similarity scores against human judgment for 50 output pairs from your specific task domain to validate whether the 0.8/0.4 thresholds are appropriate.
3. **Assess transformation quality impact**: Manually inspect 30 violations from MR-142 (asymmetric relation swap) to determine if input transformation errors are driving false positives in your test environment.