---
ver: rpa2
title: 'Causal Understanding by LLMs: The Role of Uncertainty'
arxiv_id: '2509.20088'
source_url: https://arxiv.org/abs/2509.20088
tags:
- causal
- uncertainty
- accuracy
- entropy
- original
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) exhibit near-random performance in
  causal relation classification, raising questions about whether this stems from
  limited pretraining exposure or fundamental representational gaps. This work investigates
  these questions through uncertainty-based evaluation, examining whether pretraining
  exposure to causal examples improves causal understanding.
---

# Causal Understanding by LLMs: The Role of Uncertainty

## Quick Facts
- **arXiv ID:** 2509.20088
- **Source URL:** https://arxiv.org/abs/2509.20088
- **Reference count:** 11
- **Primary result:** Large language models (LLMs) exhibit near-random performance in causal relation classification, raising questions about whether this stems from limited pretraining exposure or fundamental representational gaps.

## Executive Summary
This work investigates whether large language models can understand causal relationships and whether pretraining exposure to causal examples improves this capability. Through uncertainty-based evaluation on over 18K PubMed sentences, the study tests seven models (Pythia, GPT-J, Dolly, Qwen) on causal classification and verbatim memorization probing tasks. The results reveal almost identical accuracy on seen versus unseen sentences, no memorization bias, and output distributions nearly flat with entropic values near maximum, confirming random guessing. Instruction-tuned models display severe miscalibration with high confidence despite low accuracy. These findings suggest causal understanding failures arise from lack of structured causal representation rather than insufficient exposure to causal examples during pretraining.

## Method Summary
The study employs a two-task evaluation framework: causal type classification (4-way: direct causal, conditional causal, correlational, no relationship) and verbatim memorization probing (original vs. paraphrase selection). The dataset consists of 18,366 PubMed sentences from Yu et al. (2019) filtered to ensure recoverable abstracts, plus 5,400 post-2024 sentences labeled by a BERT classifier (F1=0.97). Seven models (Pythia-1.4B/7B/12B, GPT-J-6B, Dolly-v2-7B/12B, Qwen-7b-base) are tested via VLLM API with temperature=0.0. Key metrics include accuracy, entropy (uncertainty measure), ECE/ACE (calibration), and consistency across paraphrases. Statistical tests (chi-square, t-tests, ANOVA) compare seen vs. unseen performance and across causal types.

## Key Results
- Pretraining exposure to causal text does not improve causal classification accuracy (accuracy differs by <1.5% between seen and unseen sentences, p > 0.05)
- Instruction-tuned models show severe miscalibration (Qwen: >95% confidence, 32.8% accuracy, ECE=0.49)
- Output distributions are nearly flat with entropic values near maximum (1.35/1.39), confirming random guessing
- Conditional relations induce highest entropy (+11% vs. direct), indicating difficulty with compositional reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pretraining exposure to causal text does not improve causal classification accuracy.
- Mechanism: If causal understanding were learned from exposure, models should perform better on sentences from their training corpus (The Pile) versus post-2024 unseen text. The paper tests this by comparing performance on MCQA (seen) vs MCQA-newer (unseen) datasets.
- Core assumption: Presence in The Pile implies meaningful exposure during training.
- Evidence anchors:
  - [abstract] "Results show almost identical accuracy on seen/unseen sentences (p > 0.05)"
  - [section 6.1] "Across all models trained on The Pile, accuracy differs by <1.5% between MCQA and MCQA-newer"
  - [corpus] Related work on training data coverage (arXiv:2511.17946) suggests lexical coverage alone is insufficient for reasoning tasks, supporting the exposure-performance independence finding.
- Break condition: If future work shows models with significantly more causal exposure (e.g., domain-specific pretraining) achieve higher accuracy, the exposure-independence claim would weaken.

### Mechanism 2
- Claim: Instruction tuning creates miscalibrated confidence without improving causal reasoning.
- Mechanism: Base pretrained models (Pythia, GPT-J) output near-uniform probability distributions (entropy ~1.35) reflecting appropriate uncertainty at random performance. Instruction-tuned models (Dolly, Qwen) learn to output confident predictions regardless of actual capability.
- Core assumption: The instruction tuning process, not architecture or base training, is responsible for the confidence shift.
- Evidence anchors:
  - [abstract] "Instruction-tuned models show severe miscalibration (Qwen: >95% confidence, 32.8% accuracy, ECE=0.49)"
  - [section 6.3] "Compare pythia-7b (ECE=0.13, entropy=1.32) with dolly-v2-7b (ECE=0.36, entropy=0.92)—instruction tuning does not even half the entropy while almost tripling the calibration error"
  - [corpus] arXiv:2507.07186 ("Planted in Pretraining, Swayed by Finetuning") finds cognitive biases can be amplified by instruction tuning, consistent with calibration degradation observed here.
- Break condition: If an instruction-tuned model were found that maintains base-model calibration while improving accuracy, the mechanism would require revision.

### Mechanism 3
- Claim: Causal complexity, not surface form, drives uncertainty patterns.
- Mechanism: Models exhibit highest entropy on conditional causal statements (+11% vs direct), suggesting difficulty with compositional reasoning about conditions and moderators rather than surface-level linguistic features.
- Core assumption: Entropy differences across causal types reflect conceptual difficulty, not dataset artifacts.
- Evidence anchors:
  - [abstract] "Conditional relations induce highest entropy (+11% vs. direct)"
  - [section 6.4] "ANOV A confirms significant differences in entropy across causal types (p < 0.001 for all models)"
  - [corpus] Limited direct corpus evidence on conditional causal reasoning specifically; this mechanism rests primarily on paper-internal evidence.
- Break condition: If controlled experiments showed entropy differences disappear when controlling for sentence length or lexical complexity, the conceptual-difficulty interpretation would be challenged.

## Foundational Learning

- Concept: **Entropy as uncertainty measure** (Shannon entropy over output distributions)
  - Why needed here: The paper uses entropy to distinguish random guessing (near-maximum entropy ~1.39 for 4 classes) from confident (but possibly wrong) predictions.
  - Quick check question: For a 4-class classification task, what entropy value indicates uniform random guessing?

- Concept: **Expected Calibration Error (ECE)** — weighted average of |accuracy − confidence| per bin
  - Why needed here: ECE quantifies overconfidence; the paper shows instruction-tuned models have 3-4x higher ECE than base models despite similar accuracy.
  - Quick check question: If a model outputs 90% confidence on all predictions but achieves 30% accuracy, is it well-calibrated?

- Concept: **Epistemic vs. aleatoric uncertainty**
  - Why needed here: The paper frames causal understanding failures as epistemic (representational gaps) rather than aleatoric (intrinsic ambiguity), though paraphrase consistency analysis suggests some questions may be inherently ambiguous.
  - Quick check question: Would collecting more training data reduce epistemic uncertainty, aleatoric uncertainty, or both?

## Architecture Onboarding

- Component map:
  Input layer (PubMed sentences) -> Task heads (4-way causal classifier, memorization probe) -> Uncertainty extraction (softmax → entropy + confidence) -> Calibration analysis (bin predictions → ECE/ACE)

- Critical path:
  1. Dataset construction: Filter Yu et al. (2019) to sentences with recoverable PubMed abstracts → MCQA
  2. Newer dataset: Train BERT classifier (F1=0.97) on original labels → label post-2024 abstracts → MCQA-newer
  3. Paraphrase generation: GPT-4o-mini generates 5 paraphrases + 1 negation + 2 questions per sentence
  4. Model inference: VLLM API, temperature=0.0, randomized option order to mitigate positional bias
  5. Uncertainty quantification: Extract per-prediction entropy, confidence, ECE, ACE, consistency metrics

- Design tradeoffs:
  - Dataset size vs. label quality: Filtering removed ~50% of original dataset but preserved label distribution (total variation distance = 0.026)
  - Memorization detection power: Single exposure in The Pile doesn't guarantee memorization (prior work shows 100+ repetitions needed); null results should be interpreted as "exposure insufficient" rather than "memorization tested and failed"
  - Model selection: Only Qwen not trained on The Pile serves as control; cannot fully isolate architecture vs. training data effects

- Failure signatures:
  - Random guessing with high confidence: ECE > 0.4 with accuracy near 25% indicates instruction-tuning-induced miscalibration
  - Identical seen/unseen performance: Accuracy difference <1.5% suggests no functional memorization for causal reasoning
  - Flat output distributions: Entropy > 1.3 (of max 1.39) indicates models have no meaningful signal

- First 3 experiments:
  1. Replicate entropy analysis on your model: Run 4-way causal classification on MCQA/MCQA-newer splits; compute entropy per prediction. If entropy < 0.5, check calibration before trusting predictions.
  2. Test paraphrase consistency: For each sentence, compare predictions across 5 paraphrases. High KL divergence (>0.5) or low correlation (ρ < 0.2) indicates unstable representations.
  3. Probe instruction tuning effects: Compare base vs. instruction-tuned variants of the same architecture (e.g., Pythia vs. Dolly). If calibration degrades without accuracy gain, the tuning process is introducing overconfidence.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs acquire causal patterns during pretraining but fail to apply or recall them during inference, and what methods could distinguish acquisition failure from retrieval failure?
- Basis in paper: Limitations section states: "we cannot determine whether the models truly failed to acquire causal patterns during training, or whether they learned them but are unable to apply or recall them during inference. Structured prompting, Causal probing with small datasets, pre-trained data inspections through sampling, probing representations, etc., can be possible approaches to tackle this problem."
- Why unresolved: Current evaluation only measures output behavior; internal representations remain unexamined.
- What evidence would resolve it: Representation probing studies detecting learned-but-inaccessible causal knowledge, or prompt engineering experiments showing whether contextual interventions enable recall of causal patterns.

### Open Question 2
- Question: What is the quantitative relationship between repetition frequency of causal examples in pretraining data and downstream causal understanding performance?
- Basis in paper: Limitations section notes: "our binary classification of 'seen' versus 'unseen' may oversimplify the memorization spectrum. Future work should examine the relationship between repetition frequency and causal understanding."
- Why unresolved: Study used binary seen/unseen classification; prior work suggests memorization requires 100+ repetitions, but the frequency-performance curve for causal reasoning is unknown.
- What evidence would resolve it: Controlled experiments varying repetition counts of causal statements during training, or corpus analysis correlating known frequencies with performance on held-out causal items.

### Open Question 3
- Question: What architectural modifications or training objectives beyond next-token prediction could enable structured causal representation in language models?
- Basis in paper: Discussion concludes: "causal understanding in LLMs requires fundamental advances beyond current pretraining paradigms. Memorization, even at scale, cannot substitute for genuine causal knowledge" and states "Models require architectural innovations or training objectives that explicitly target causal inference."
- Why unresolved: Performance is near-random regardless of data exposure, indicating the limitation is architectural rather than data-driven, but no alternative architectures were tested.
- What evidence would resolve it: Experiments with models incorporating explicit causal graph representations, intervention-aware training, or counterfactual reasoning modules showing improved accuracy and calibration on causal classification benchmarks.

## Limitations
- The causal reasoning evaluation uses a single dataset (Yu et al., 2019) filtered to PubMed abstracts, limiting generalizability to other domains and potentially introducing sampling bias.
- The BERT classifier used to label post-2024 abstracts has not been validated against human annotators for the newer data, creating potential label noise.
- Memorization probing with single exposure to sentences in The Pile provides only weak evidence against memorization; prior work suggests 100+ exposures are typically needed for reliable memorization detection.

## Confidence

**High confidence:** The finding that instruction-tuned models show severe miscalibration (Qwen: >95% confidence, 32.8% accuracy, ECE=0.49) is directly supported by clear numerical evidence and consistent across multiple metrics.

**Medium confidence:** The claim that pretraining exposure to causal examples does not improve causal understanding rests on a reasonable assumption (presence in The Pile implies meaningful exposure) but lacks absolute verification of actual training data coverage.

**Medium confidence:** The assertion that causal complexity drives uncertainty patterns is primarily supported by paper-internal entropy comparisons, with limited external validation of whether conditional causal reasoning is genuinely more difficult than other types.

## Next Checks
1. Test the same models on alternative causal reasoning datasets (e.g., CLUTRR, SocialIQA) to verify that near-random performance is not specific to the PubMed domain or filtering process.
2. Conduct controlled experiments varying the amount of causal examples in pretraining data to determine the exposure threshold needed for any causal reasoning improvement.
3. Implement a multi-round memorization probe with sentences exposed 10+, 50+, and 100+ times in training data to establish whether any memorization threshold exists for causal reasoning tasks.