---
ver: rpa2
title: 'Stay Unique, Stay Efficient: Preserving Model Personality in Multi-Task Merging'
arxiv_id: '2512.01461'
source_url: https://arxiv.org/abs/2512.01461
tags:
- merging
- task
- tasks
- performance
- task-specific
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies that model merging methods suffer significant
  performance degradation even when combining models fine-tuned on similar tasks,
  highlighting the need to preserve task-specific information. The authors propose
  Decomposition, Thresholding, and Scaling (DTS), a personalized model merging framework
  that applies singular value decomposition to task-specific information, retains
  only a small subset of singular values and vectors, and introduces a novel thresholding
  strategy that partitions singular vector elements into groups with assigned scaling
  factors.
---

# Stay Unique, Stay Efficient: Preserving Model Personality in Multi-Task Merging

## Quick Facts
- arXiv ID: 2512.01461
- Source URL: https://arxiv.org/abs/2512.01461
- Authors: Kuangpu Guo; Yuhe Ding; Jian Liang; Zilei Wang; Ran He
- Reference count: 40
- One-line result: Proposes DTS framework that preserves task-specific information during model merging with only 1% additional storage per task while achieving superior generalization to unseen tasks

## Executive Summary
This paper identifies a critical limitation in existing model merging methods: significant performance degradation when combining models fine-tuned on similar tasks. The authors introduce Decomposition, Thresholding, and Scaling (DTS), a personalized model merging framework that applies singular value decomposition to extract low-rank task-specific information, uses fine-grained thresholding with per-group scaling to preserve magnitude information, and extends to unseen tasks via semantic similarity-based fusion. DTS consistently outperforms state-of-the-art baselines while requiring minimal additional storage (1% per task), addressing both efficiency and personalization needs in multi-task learning.

## Method Summary
DTS operates by computing task vectors (fine-tuned weights minus pretrained weights) or difference vectors (fine-tuned minus merged base model), applying SVD to extract low-rank task-specific information, and reconstructing personalized parameters using thresholding and scaling. The framework retains only a small subset of singular values and vectors (default 30%), partitions singular vector elements into four groups based on magnitude and sign, and applies per-group scaling factors to preserve relative magnitudes. For unseen tasks, DTS uses semantic similarity of task class embeddings to weight contributions from seen tasks. The method achieves efficient personalization with minimal storage overhead while maintaining high accuracy across both seen and unseen tasks.

## Key Results
- DTS achieves 90.32% average accuracy on 8 visual tasks with only 3.68% additional memory rate (AMR)
- DTS-D variant achieves 90.40% accuracy with same AMR, outperforming baselines (Ties-Merging: 88.99%, T-Switch: 89.96%)
- On unseen tasks, DTS achieves 55.23% average accuracy compared to 50.67% for Ties-Merging
- DTS-T* and DTS-D* achieve only 0.98% AMR through adaptive sparsity selection while maintaining competitive accuracy
- Ablation studies confirm thresholding with scaling provides ~2% accuracy improvement over simple binarization

## Why This Works (Mechanism)

### Mechanism 1: Low-Rank Task-Specific Information Extraction via SVD
- Claim: Task vectors contain compressible low-rank structure that preserves most task-relevant information with minimal storage
- Mechanism: SVD isolates principal directions of parameter updates; retaining top-r singular values/vectors filters noise while keeping task-critical modifications
- Core assumption: Task-specific knowledge concentrates in a low-dimensional subspace of parameter space
- Evidence anchors:
  - [abstract]: "DTS first applies singular value decomposition to the task-specific information and retains only a small subset of singular values and vectors"
  - [Section 3.2, Eq. 2]: "U^n, Σ^n, V^n = SVD_r(τ^n), where r represents the proportion of singular values preserved"
  - [corpus]: Weak direct validation; neighboring papers like "No Task Left Behind" explore common/task-specific subspaces but don't validate SVD compression ratios
- Break condition: If tasks require high-rank modifications (r > 0.5), storage savings diminish with minimal performance gain (Table 14 shows plateauing at r > 0.5)

### Mechanism 2: Fine-Grained Thresholding with Group-Specific Scaling
- Claim: Partitioning singular vector elements into 4 groups with separate scaling factors approximates original task vectors more accurately than binary masking
- Mechanism: Median-based thresholds split values by magnitude; per-group L2-norm scaling factors preserve relative magnitudes within each partition
- Core assumption: Magnitude information within singular vectors carries task-relevant signal beyond sign alone
- Evidence anchors:
  - [Section 3.2, Fig. 3]: Toy example showing thresholding provides finer approximation than binarization
  - [Section 4.3, Table 13]: "Compared to simple binarization, our thresholding approach yields an accuracy improvement of approximately 2%"
  - [corpus]: No direct corpus validation of thresholding strategy specifically
- Break condition: Without scaling (Table 13), performance collapses to ~5% accuracy, indicating magnitude restoration is critical

### Mechanism 3: Semantic Similarity-Based Fusion for Unseen Task Generalization
- Claim: Weighted combination of seen tasks' difference vectors, weighted by semantic similarity between task class embeddings, transfers to unseen tasks without training data
- Mechanism: Text encoder generates embeddings from class names; cosine similarity determines interpolation weights; this selects relevant seen-task knowledge
- Core assumption: Tasks with semantically similar class labels share transferable parameter modifications
- Evidence anchors:
  - [Section 3.3, Eq. 8]: "γ_n = cos_sim(E_u, E_n) / Σ cos_sim(E_u, E_k)"
  - [Section 4.2, Table 4-5]: DTS variants achieve best unseen task performance across ViT and GPT-2 backbones
  - [corpus]: Related work (CALM, TALL-Mask) explores task-aware merging but doesn't validate semantic similarity specifically
- Break condition: If unseen task semantics diverge significantly from all seen tasks, weighted fusion provides no benefit over base merged model

## Foundational Learning

- **Concept: Task Vectors and Parameter Arithmetic**
  - Why needed here: DTS operates on task vectors (θ_finetuned - θ_pretrained) or difference vectors (θ_finetuned - θ_merged); understanding this abstraction is prerequisite
  - Quick check question: Given pretrained weights θ₀ and finetuned weights θ_f, what is the task vector τ?

- **Concept: Singular Value Decomposition for Matrix Compression**
  - Why needed here: Core mechanism relies on low-rank approximation; must understand what U, Σ, V represent and how truncation affects reconstruction
  - Quick check question: If you keep only 30% of singular values, what information is lost versus preserved?

- **Concept: Element-wise Thresholding vs. Quantization**
  - Why needed here: DTS uses 4-group thresholding (not binary masks); distinguishing this from simple binarization explains performance gains
  - Quick check question: Why would separating "large positive" and "small positive" values help more than just marking positive vs. negative?

## Architecture Onboarding

- **Component map:**
  1. Decomposition Module: SVD on task/difference vectors → produces U, Σ, V matrices
  2. Thresholding Module: Sign + magnitude-based partitioning → 3 binary masks per U/V
  3. Scaling Module: Per-group L2 norm computation → 8 scalar factors per matrix pair
  4. Storage: 6 masks (1-bit each) + 8 scalars + singular values per task
  5. Reconstruction Runtime: ˆθ = θ₀ + ˆU × Σ × ˆV (for seen tasks)
  6. Unseen Task Router: Text encoder → embedding similarity → weighted fusion

- **Critical path:**
  1. Obtain task vectors from fine-tuned models
  2. Apply SVD with r ∈ [0.05, 0.5] (default r=0.3)
  3. Compute thresholds (medians of positive/negative values)
  4. Generate masks and scaling factors
  5. Store compressed representation (~1% of model size)
  6. At inference: reconstruct per-task parameters dynamically

- **Design tradeoffs:**
  - **r (sparsity coefficient)**: Lower r → less storage, potential accuracy drop. Paper shows r=0.3 achieves ~1% AMR with <1% ADR
  - **DTS-T vs DTS-D**: Task vectors require pretrained model access; difference vectors work when pretrained is unavailable (Section 9.5)
  - **4 groups vs 8 groups**: 8 groups doubles mask storage with negligible gain (Table 13)

- **Failure signatures:**
  - Accuracy ~5%: Scaling factors not applied (Table 13)
  - Significant ADR on similar tasks: Parameter conflicts not resolved—check if personalized reconstruction is actually invoked
  - Poor unseen task performance: Semantic similarity not helping—verify text encoder quality and class name coverage

- **First 3 experiments:**
  1. **Sanity check**: Single-task reconstruction—apply DTS to one task vector, reconstruct, verify accuracy matches individual fine-tuned model within 0.5%
  2. **Storage/accuracy sweep**: Vary r ∈ {0.05, 0.1, 0.2, 0.3, 0.5} on 8-task ViT-B/32 benchmark; plot ADR vs AMR (replicate Fig. 4 pattern)
  3. **Unseen task test**: Train on 6 visual tasks, evaluate on RESISC45 and SVHN; verify DTS-D variant outperforms Ties-Merging baseline (target: >55% avg unseen accuracy per Table 4)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the DTS framework be adapted for task-agnostic inference where the task identity is unavailable?
- Basis in paper: [explicit] The paper states in the problem formulation that it assumes "the information of the test task (e.g., task ID) is available during evaluation" (Page 3)
- Why unresolved: The current method relies on the task ID to reconstruct the specific parameters $\hat{\theta}_n$; without it, the mechanism cannot select the correct personalized components
- What evidence would resolve it: A framework integrating DTS with a task classifier or routing mechanism, along with results in a blind merging setting

### Open Question 2
- Question: How robust is the semantic similarity-based weighting to the choice of text encoder or the ambiguity of task descriptions?
- Basis in paper: [inferred] The unseen task extension relies on encoding task characteristics (class names or descriptions) using a text encoder to compute cosine similarity (Page 5)
- Why unresolved: The method assumes the text encoder captures task relationships accurately, which may fail if class names are ambiguous or unrelated to the visual/logical task content
- What evidence would resolve it: Ablation studies using different text encoders or datasets with non-semantic labels (e.g., purely numeric classes) to test sensitivity

### Open Question 3
- Question: Is the fixed four-group thresholding strategy optimal for all layer types, or would a dynamic, layer-adaptive strategy improve efficiency?
- Basis in paper: [inferred] The authors set a fixed threshold of four groups (large/small positive/negative), noting that eight groups doubles storage without gain (Page 8)
- Why unresolved: While four groups is a strong heuristic, the paper does not verify if this fixed granularity is optimal across different architectures or layer depths
- What evidence would resolve it: Experiments treating the number of threshold groups as a learnable or searchable parameter per layer to identify optimal compression configurations

## Limitations

- The paper assumes task-specific knowledge concentrates in low-rank subspaces but does not empirically validate this assumption across diverse task combinations
- The semantic similarity-based generalization relies on class name embeddings, which may not capture true task relationships in all domains
- The computational overhead of SVD and thresholding at inference time is not fully characterized

## Confidence

- **High Confidence**: The core mechanism of SVD-based task vector compression with thresholding and scaling is well-validated through extensive experiments (Tables 4-6, 13)
- **Medium Confidence**: The 1% additional memory rate claim assumes specific sparsity settings (r=0.3) that may not generalize to all task combinations
- **Medium Confidence**: Unseen task generalization results depend heavily on quality of semantic embeddings and may not transfer to tasks with limited or ambiguous class descriptions

## Next Checks

1. **Rank Sensitivity Analysis**: Systematically vary the sparsity coefficient r ∈ {0.1, 0.2, 0.3, 0.4, 0.5} across different task combinations to map the relationship between storage efficiency and accuracy degradation
2. **Semantic Embedding Robustness**: Replace the current text encoder with alternative embedding methods (e.g., sentence transformers) and evaluate impact on unseen task performance to isolate embedding quality from the merging algorithm
3. **Cross-Domain Generalization**: Test the unseen task variant on cross-domain scenarios (e.g., medical imaging tasks with clinical descriptions) to assess limitations of semantic similarity-based fusion