---
ver: rpa2
title: Explainable AI for Correct Root Cause Analysis of Product Quality in Injection
  Moulding
arxiv_id: '2505.01445'
source_url: https://arxiv.org/abs/2505.01445
tags:
- machine
- product
- injection
- cause
- settings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper investigates the root cause analysis of product quality\
  \ deviations in injection moulding using explainable AI. It demonstrates that interactions\
  \ exist among machine settings (e.g., melt temperature and packing time) affecting\
  \ product weight, validated through Friedman\u2019s H-statistic."
---

# Explainable AI for Correct Root Cause Analysis of Product Quality in Injection Moulding

## Quick Facts
- arXiv ID: 2505.01445
- Source URL: https://arxiv.org/abs/2505.01445
- Reference count: 40
- The paper demonstrates that permutation-based SHAP correctly identifies root causes of product weight deviations in injection moulding by accounting for feature interactions, unlike ICE which consistently prioritizes packing time regardless of actual changes.

## Executive Summary
This paper addresses the critical challenge of identifying root causes of product quality deviations in injection moulding using explainable AI. The study demonstrates that feature interactions exist among machine settings (e.g., melt temperature and packing time) and that these interactions significantly affect product weight. Through controlled experiments using permutation-based SHAP versus ICE, the research shows SHAP correctly identifies the primary cause of quality deviations by considering feature interactions, while ICE consistently prioritizes packing time regardless of actual changes. Both RF and MLP models achieve MAPE < 0.05%, validating their use for analysis and establishing SHAP's interaction-aware explanations as more reliable for actionable process optimization.

## Method Summary
The study uses a Central Composite Design experimental dataset with 77 machine setting combinations, each repeated 20 times (1,540 cycles total), split into 60% training, 10% validation, and 30% test sets. Two black-box models are trained: Random Forest and Multi-Layer Perceptron (2 hidden layers with 8 and 4 neurons, tanh activation, L-BFGS solver). Feature interaction strength is quantified using Friedman's H-statistic, while root cause analysis employs permutation-based SHAP and ICE methods. The models achieve MAPE < 0.05% accuracy, and controlled experiments validate that SHAP correctly identifies which machine setting was actually changed from mid-value, whereas ICE always ranks packing time highest regardless of the actual perturbation.

## Key Results
- Feature interactions exist among machine settings in injection moulding, with melt temperature and packing time accounting for maximum interaction effects
- SHAP correctly identifies the primary cause of quality deviations by considering feature interactions, unlike ICE which always prioritizes packing time
- RF and MLP models achieve MAPE < 0.05%, validating their use for analysis
- SHAP's interaction-aware explanations provide more reliable, actionable insights for process optimization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Machine settings in injection moulding exhibit statistically significant interactions that affect product quality outcomes.
- Mechanism: Friedman's H-statistic quantifies the variance explained by interaction terms between features (e.g., melt temperature and packing time) versus their independent effects. When H-statistic values are non-zero, the prediction cannot be decomposed into independent feature contributions.
- Core assumption: The experimental design (Central Composite Design with 77 combinations, 20 repetitions each) adequately samples the process space to capture interaction effects.
- Evidence anchors:
  - [abstract]: "interactions among the multiple input machine settings do exist in real experimental data collected as per a central composite design"
  - [Section 4.1]: "There are interactions among the features, with melt temperature and packing time accounting for maximum of them"
  - [corpus]: Weak direct corpus support for H-statistic specifically; related papers address injection moulding XAI but do not validate interaction statistics.
- Break condition: If features were truly independent (H = 0), ICE and SHAP would produce identical rankings, invalidating the comparison.

### Mechanism 2
- Claim: Permutation-based SHAP identifies the correct root cause of quality deviations by accounting for feature interactions, whereas ICE consistently prioritizes the globally dominant feature regardless of the actual perturbation.
- Mechanism: SHAP computes Shapley values by averaging marginal contributions across all feature permutations (Equation 8), distributing interaction effects among contributing features. ICE measures local feature impact by varying one feature while holding others fixed (Equation 9), capturing only marginal effects without interaction attribution.
- Core assumption: The model has learned the true input-output relationship (MAPE < 0.05% supports this), so explanation differences stem from the explanation method, not model error.
- Evidence anchors:
  - [abstract]: "SHAP correctly identifies the primary cause of quality deviations by considering feature interactions, unlike ICE which always prioritizes packing time"
  - [Section 4.2]: "When a single machine setting changes from its mid value... SHAP suggests tuning that machine setting first... On the other hand, ICE will always suggest tuning the packing time"
  - [corpus]: Related papers apply XAI to injection moulding but do not compare SHAP vs. ICE head-to-head.
- Break condition: If models have high error (MAPE > 5%), explanations would be unreliable regardless of method.

### Mechanism 3
- Claim: Local, model-agnostic explanation methods applied to accurate black-box models can provide actionable root cause analysis for individual production cycles.
- Mechanism: Both RF (ensemble of decision trees via bagging) and MLP (feedforward network with backpropagation) achieve low error, enabling the explanation methods to operate on models that have captured the true process dynamics. Model-agnostic methods (SHAP, ICE) query only inputs and predictions, making explanations portable across architectures.
- Core assumption: Low MAPE on held-out test data (30% split) indicates the model generalizes and is not overfitting to training noise.
- Evidence anchors:
  - [abstract]: "RF and MLP models achieve MAPE < 0.05%, validating their use for analysis"
  - [Table 6]: RF MAPE = 0.0312% (95% CI), MLP MAPE = 0.0431% (95% CI)
  - [corpus]: Corpus papers on injection moulding XAI use different model classes but do not report comparable MAPE benchmarks.
- Break condition: If test distribution diverges from production (covariate shift), model accuracy and explanation reliability degrade.

## Foundational Learning

- Concept: **Friedman's H-statistic for interaction detection**
  - Why needed here: Determines whether features interact, which dictates whether interaction-aware explanation methods (SHAP) are necessary.
  - Quick check question: If H-statistic between two features is 0.5, what does that imply about their relationship?

- Concept: **Shapley values and permutation-based attribution**
  - Why needed here: Core mathematical foundation for SHAP; understanding how marginal contributions are averaged across permutations clarifies why SHAP handles interactions.
  - Quick check question: Why does averaging over all feature orderings matter for fair attribution?

- Concept: **Local vs. global interpretability**
  - Why needed here: Root cause analysis for individual defective products requires local explanations; global methods (PDP) explain average behavior only.
  - Quick check question: Can a global importance ranking identify which parameter caused a specific product to be underweight?

## Architecture Onboarding

- Component map:
  - Data layer: Central Composite Design experimental data (77 machine setting combinations × 20 repetitions = 1,540 cycles), stratified 60/10/30 train/validation/test split
  - Model layer: Random Forest (bootstrap aggregation of M trees) and MLP (2 hidden layers, 8/4 neurons, tanh activation, L-BFGS solver)
  - Explanation layer: Permutation-based SHAP (background dataset W, P permutations) and ICE (standard deviation of prediction across feature values)
  - Validation layer: Friedman's H-statistic for interaction detection; controlled experiments where single features are perturbed from mid values

- Critical path:
  1. Verify model accuracy (MAPE < 0.05%) on test set before trusting explanations
  2. Compute H-statistic to confirm interactions exist
  3. Apply SHAP and ICE to test instances; compare top-ranked features against known perturbations
  4. Deploy SHAP-based explainer in production for real-time root cause recommendations

- Design tradeoffs:
  - RF vs. MLP: RF easier to debug; MLP better for extrapolation but sensitive to initialization (20% stagnation observed)
  - SHAP vs. ICE: SHAP computationally heavier (requires P permutations) but interaction-aware; ICE faster but misleading under feature interactions
  - Model-agnostic vs. model-specific: Model-agnostic enables architecture flexibility; TreeSHAP is faster but RF-only

- Failure signatures:
  - ICE always ranks packing time first regardless of actual perturbation → indicates ICE is ignoring interactions
  - SHAP rankings inconsistent between RF and MLP → check model accuracy discrepancy or data standardization differences
  - H-statistic near zero → ICE and SHAP should converge; if not, re-examine model fit

- First 3 experiments:
  1. Reproduce H-statistic calculation on the dataset to confirm melt temperature–packing time interaction strength
  2. Train RF and MLP on the provided split; verify MAPE < 0.1% before proceeding
  3. Select 10 test instances where a single feature (e.g., melt temperature) is perturbed; compare SHAP vs. ICE top-ranked feature against ground truth

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does interaction-aware feature attribution (like SHAP) perform in root cause analysis when optimizing for multiple, potentially conflicting product quality characteristics simultaneously?
- Basis in paper: [explicit] The conclusion states it will be "interesting to explore how the cause analysis/feature attribution extends to the case of multiple product quality characteristics" because incorrect identification could deteriorate other unmeasured characteristics.
- Why unresolved: The study validates its approach on single outputs (weight and planarity) in isolation, rather than a multi-objective optimization scenario where adjusting a setting for weight might harm planarity.
- What evidence would resolve it: A study analyzing feature attributions on a multi-output model to see if SHAP consistently identifies the correct cause across all quality dimensions without trade-offs.

### Open Question 2
- Question: Does the reliability of permutation-based SHAP degrade when applied to high-dimensional, correlated sensor data compared to the independent machine settings used in this study?
- Basis in paper: [inferred] The paper notes that "machine settings are independently tuned... hence, the feature independence holds," which validates the permutation method. This implies the results rely on the lack of correlation among inputs.
- Why unresolved: Real-world process data often contains highly correlated sensor streams (e.g., pressure curves), and permutation-based methods are known to struggle with feature dependence, which was not tested here.
- What evidence would resolve it: Experiments applying SHAP and ICE to datasets with artificially injected feature correlations to measure the deviation in attribution accuracy.

### Open Question 3
- Question: Do other local, model-agnostic methods (e.g., LIME) outperform ICE in identifying root causes in injection molding, or is the superior performance specific to the Shapley value formulation?
- Basis in paper: [inferred] The paper compares only SHAP and ICE, concluding that SHAP's consideration of interactions makes it superior. It does not test if other surrogate-based explainers can similarly capture these interactions.
- Why unresolved: While SHAP worked best here, it is computationally heavier than some alternatives; a broader comparison is needed to determine if the "interaction-awareness" is unique to SHAP or a general property of more advanced explainers.
- What evidence would resolve it: A comparative benchmark including LIME or Anchors on the same experimental dataset to verify if they also avoid the "always packing time" bias shown by ICE.

## Limitations
- Lack of complete hyperparameter specifications for the RF model may affect reproducibility
- Reliance on synthetic data generation for validation may not fully capture real injection moulding process complexity
- Controlled experiment design (perturbing single features) may not reflect multi-factor variations common in production environments

## Confidence
- High confidence: MAPE < 0.05% validation and the fundamental mechanism of SHAP vs ICE comparison under feature interactions
- Medium confidence: The interaction strength quantification via H-statistic and the controlled experiment design
- Medium confidence: The generalizability of conclusions to other manufacturing processes beyond injection moulding

## Next Checks
1. Replicate the H-statistic calculation on the experimental dataset to verify interaction strength between melt temperature and packing time specifically
2. Train both RF and MLP models with exact specifications; verify MAPE < 0.1% on test set before proceeding to explanation analysis
3. Conduct ablation studies by disabling feature interactions in the synthetic data generator to confirm SHAP and ICE converge when interactions are absent