---
ver: rpa2
title: Are Modern Speech Enhancement Systems Vulnerable to Adversarial Attacks?
arxiv_id: '2509.21087'
source_url: https://arxiv.org/abs/2509.21087
tags:
- speech
- user
- adversarial
- attacks
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that modern speech enhancement (SE) systems
  can be vulnerable to targeted adversarial attacks. By adding psychoacoustically-masked
  adversarial noise, attackers can manipulate SE outputs to convey unintended semantic
  meanings.
---

# Are Modern Speech Enhancement Systems Vulnerable to Adversarial Attacks?

## Quick Facts
- arXiv ID: 2509.21087
- Source URL: https://arxiv.org/abs/2509.21087
- Reference count: 0
- Primary result: Modern speech enhancement systems can be manipulated via psychoacoustically-masked adversarial attacks to produce outputs with attacker-controlled semantic content.

## Executive Summary
This paper demonstrates that modern speech enhancement (SE) systems are vulnerable to targeted adversarial attacks that manipulate their outputs to convey unintended semantic meanings. The authors develop a white-box attack framework using projected gradient descent with psychoacoustic constraints, showing that predictive SE models (direct mapping and complex ratio mask) are highly susceptible while diffusion-based models exhibit greater inherent robustness. The attacks remain audibly perceptible despite psychoacoustic masking, raising significant security concerns for SE applications. The findings suggest that diffusion models offer stronger defense against targeted manipulation compared to traditional predictive approaches.

## Method Summary
The authors develop a white-box adversarial attack framework that optimizes complex STFT-domain perturbations using projected gradient descent with psychoacoustic constraints. The attack targets three SE architectures: direct mapping, complex ratio mask, and diffusion-based SGMSE+ models, all trained on the EARS-WHAM-v2 dataset. Psychoacoustic masking is achieved through an MPEG-1 model that gates gradient updates to frequency-time regions where human hearing is insensitive. Attack success is measured by word error rate (WER) between enhanced output and attacker target, while perturbation impact is evaluated using POLQA, ESTOI, and SNR metrics.

## Key Results
- Predictive SE models (Direct Mapping and CRM) are highly vulnerable, achieving low WER and high semantic similarity to targets while remaining audibly perceptible.
- Complex ratio mask variant performs similarly but slightly worse than direct mapping under attacks.
- Diffusion models with stochastic sampling exhibit inherent robustness, with deterministic sampling significantly reducing this protection (WER increases from 0.27 to 0.47).

## Why This Works (Mechanism)

### Mechanism 1
Targeted adversarial perturbations can redirect speech enhancement outputs toward attacker-specified semantic content. The attacker computes gradients through the fixed SE model via backpropagation, using projected gradient descent to iteratively update a complex STFT-domain perturbation δ that minimizes the mean squared error between the enhanced output and the attacker's target. The SE model's expressivity is repurposed to transform user speech into the desired semantic content. Non-differentiable components or quantized weights would prevent this gradient-based optimization.

### Mechanism 2
Psychoacoustic gating constrains adversarial perturbations to frequency-time regions masked by the source signal. An MPEG-1 psychoacoustic model estimates hearing thresholds from the input signal, and a gating mask scales gradients to suppress updates in audible regions while allowing aggressive perturbation where human hearing is insensitive. The effectiveness depends on accurate prediction of human auditory masking under adversarial conditions, and may be affected by hearing impairments or playback equipment characteristics.

### Mechanism 3
Stochastic sampling in diffusion-based SE provides inherent robustness against gradient-based adversarial attacks. The reverse stochastic differential equation injects fresh noise at each sampling step, making the mapping from input to output non-deterministic across different random seeds. This randomness makes gradient-based attacks unreliable, as computed gradients may not transfer between different noise trajectories. Deterministic samplers or insufficient noise variance would reduce this stochastic protection.

## Foundational Learning

- **Complex STFT representation**
  - Why needed here: All attacks operate in the complex spectrogram domain; understanding magnitude/phase coupling is essential for interpreting δ optimization.
  - Quick check question: Can you explain why perturbing real and imaginary STFT components jointly differs from perturbing magnitude only?

- **Psychoacoustic masking theory**
  - Why needed here: The attack's imperceptibility relies on simultaneous and temporal masking; understanding thresholds helps diagnose why certain λ values fail.
  - Quick check question: What is the difference between absolute threshold of hearing and signal-dependent masking threshold?

- **Diffusion model inference mechanics**
  - Why needed here: Understanding reverse SDE vs. ODE samplers, step count N, and noise schedule σ(t) is critical for interpreting robustness results and ablations.
  - Quick check question: Why does injecting noise during reverse sampling make the output distribution broader, and how does this affect gradient-based attacks?

## Architecture Onboarding

- **Component map:** EARS-WHAM-v2 dataset -> SE model (Direct Mapping / CRM / Diffusion) -> Complex STFT output -> Evaluation (WER, POLQA, ESTOI, DistillMOS)
- **Critical path:**
  1. Load pre-trained SE model (Direct Map / CRM / Diffusion)
  2. Sample (Y_user, S_attacker) pair from EARS-WHAM-v2 test set
  3. Initialize δ = 0; compute fixed hearing threshold H from Y_user
  4. For K=150 iterations: forward pass through SE with Y_user + δ, compute L_adv, backpropagate to δ, apply psychoacoustic gate, project onto ℓ₂ ball
  5. Evaluate attack success and perturbation impact

- **Design tradeoffs:**
  - λ (masking tolerance): Higher λ relaxes psychoacoustic constraints → stronger attack but more audible perturbation
  - ε (ℓ₂ budget): Larger ε allows more aggressive perturbations; SNR decreases predictably
  - Diffusion N (reverse steps): Fewer steps increase vulnerability; more steps improve robustness but increase inference cost
  - σ_max (noise scale): Higher σ_max weakens robustness; lower σ_max strengthens it but may degrade enhancement quality

- **Failure signatures:**
  - High WER (>1.0) with low SNR (>20 dB): Perturbation too weak; increase ε or relax λ
  - Low WER but PI-POLQA < 2.0: Attack succeeds but perturbation is highly audible; tighten λ or reduce ε
  - Diffusion attack unstable across runs: Stochastic sampling causing gradient mismatch
  - Memory overflow during diffusion attack: Missing activation checkpointing

- **First 3 experiments:**
  1. Reproduce Direct Map vulnerability curve: Sweep λ ∈ {0, 10, 20, 40} with fixed ε=10 on 20 random pairs
  2. Validate stochastic vs. fixed-noise diffusion: Run unconstrained attack on Diffusion SE with N=25 under both sampling modes
  3. Stress-test psychoacoustic hiding: For successful Direct Map attack, run AB listening test with 5 participants

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text provided, but several implicit questions arise from the findings regarding the limitations of white-box attacks, the effectiveness of psychoacoustic masking in real-world scenarios, and the generalizability of diffusion model robustness to adaptive attacks.

## Limitations
- The study assumes a white-box threat model with full access to model weights and architecture, which may not reflect realistic attack scenarios.
- Psychoacoustic masking effectiveness is borrowed from ASR attack literature without direct validation on enhancement models.
- The evaluation dataset uses only 6 speakers and clean/noisy pairs, potentially limiting generalizability to more diverse acoustic conditions.

## Confidence
- **High Confidence:** Predictive SE models are vulnerable to targeted adversarial attacks using psychoacoustic masking; diffusion models show inherent robustness through stochastic sampling.
- **Medium Confidence:** Psychoacoustic masking achieves claimed imperceptibility levels suggested by PI-POLQA scores.
- **Low Confidence:** Absolute robustness of diffusion models against adaptive attackers who might exploit deterministic samplers.

## Next Checks
1. Conduct formal AB listening tests with 15-20 participants across multiple attack configurations to validate psychoacoustic masking imperceptibility.
2. Design and evaluate adaptive attacks specifically targeting diffusion models to test robustness against determined adversaries.
3. Test the attack framework on real-world enhancement systems with different architectures to assess generalizability beyond controlled settings.