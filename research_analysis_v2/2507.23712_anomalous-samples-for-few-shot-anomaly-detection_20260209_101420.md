---
ver: rpa2
title: Anomalous Samples for Few-Shot Anomaly Detection
arxiv_id: '2507.23712'
source_url: https://arxiv.org/abs/2507.23712
tags:
- anomaly
- anomalous
- samples
- detection
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using anomalous samples in Few-Shot anomaly
  detection to improve performance over traditional approaches that rely solely on
  normal samples. The authors construct an anomalous memory bank from a single anomalous
  sample, combined with Zero-Shot and reference memory scores from the WinCLIP framework.
---

# Anomalous Samples for Few-Shot Anomaly Detection

## Quick Facts
- arXiv ID: 2507.23712
- Source URL: https://arxiv.org/abs/2507.23712
- Reference count: 31
- Key outcome: Using one anomalous sample improves AUROC scores compared to using only normal samples in few-shot anomaly detection, with gains up to 37% on some classes

## Executive Summary
This paper addresses few-shot anomaly detection by incorporating anomalous samples into the training process, challenging the traditional approach of using only normal samples. The authors build on the WinCLIP framework to create a multi-score anomaly detection system that combines Zero-Shot text similarity, normal reference memory scores, and anomalous memory scores. They demonstrate that using a single pixel-annotated anomalous sample can significantly improve detection performance on several classes, particularly when the training anomaly shares visual features with test anomalies. The paper also introduces a novel validation strategy using data-free augmentations to optimize score aggregation weights without requiring held-out data.

## Method Summary
The method constructs an anomalous memory bank from a single anomalous sample with pixel-level annotation, combined with Zero-Shot and reference memory scores from the WinCLIP framework. Multi-scale patch representations (16×16, 32×32, 48×48, 112×112 pixels) capture anomalies of varying sizes. The final anomaly score aggregates three components: text similarity via prompts, distance to normal reference patches, and similarity to anomalous patches. Weights for aggregation are optimized using Monte-Carlo sampling on augmented versions of training samples, avoiding the need for separate validation data.

## Key Results
- Using one anomalous sample improves AUROC scores compared to using only normal samples
- Performance gains of up to 37% observed on pcb1 class (53.51→91.03 AUROC)
- Multi-scale patch representations capture anomalies of varying sizes more effectively
- Augmentation-based validation yields statistically significant improvements in two of three datasets

## Why This Works (Mechanism)

### Mechanism 1
Incorporating a single pixel-annotated anomalous sample into a memory bank can improve few-shot anomaly detection over using only normal reference samples. The anomalous sample provides explicit representations of defect regions. At inference, patches from test images are compared to both normal reference patches (higher distance = more anomalous) AND anomalous patches (higher similarity = more anomalous). This dual-direction comparison captures defect information that may not be discernible from normal samples alone, particularly when the text prompt alignment is weak. The mechanism breaks when test anomalies are visually/semantically dissimilar from the training anomaly, as seen in classes like macaroni2 and capsules that performed worse with anomalous samples.

### Mechanism 2
Multi-scale patch representations capture anomalies of varying sizes better than single-scale approaches. The method stores patch embeddings at four scales (16×16, 32×32, 48×48, 112×112 pixels). Larger patches capture larger defects; smaller patches capture fine-grained anomalies. The final score aggregates across scales, allowing the model to respond to diverse defect sizes. This mechanism breaks when a class has highly variable defect sizes where no single scale dominates, potentially diluting the aggregated signal.

### Mechanism 3
Augmenting training samples can serve as a proxy validation set for optimizing score aggregation weights without requiring held-out data. The authors apply geometric augmentations (rotation, flipping, distortion, skewing) to the training samples, creating a pseudo-validation set. Monte-Carlo sampling with uniform distribution explores weight configurations, selecting those maximizing AUROC on augmented data. This approach works when score rankings on augmented training data correlate with rankings on unseen test data, but overfits with normal or Student-t sampling distributions.

## Foundational Learning

- **Concept: Vision-Language Models (VLMs) for Zero-Shot Detection**
  - Why needed here: The method builds on WinCLIP, which uses CLIP's pre-trained image-text alignments. Understanding that CLIP encodes images and text into a shared embedding space is essential for grasping why "scratch on a printed circuit board" text can detect actual scratches.
  - Quick check question: Can you explain why a text prompt like "damaged surface" might fail to detect a specific defect type, and how the anomalous memory bank compensates?

- **Concept: Memory-Based Anomaly Detection**
  - Why needed here: The core mechanism stores patch embeddings in memory banks and computes anomaly scores via nearest-neighbor distances. Without understanding k-NN style retrieval, the distance/similarity computations in Eq. 1-2 will be opaque.
  - Quick check question: Given a test patch embedding F, how would you compute its anomaly score against a reference memory bank of normal patches R? What about an anomalous memory bank?

- **Concept: AUROC for Imbalanced Classification**
  - Why needed here: Anomaly detection is inherently imbalanced (few anomalies, many normals). AUROC measures ranking quality across thresholds, which is why it's the standard metric here.
  - Quick check question: If a model assigns random scores, what AUROC would you expect? If it perfectly ranks all anomalies higher than all normals, what AUROC would you get?

## Architecture Onboarding

- **Component map:**
Input Image → CLIP Visual Encoder → Multi-scale Patch Embeddings
                                              ↓
                    ┌─────────────────────────┼─────────────────────────┐
                    ↓                         ↓                         ↓
            Zero-Shot Score          Reference Memory Score    Anomalous Memory Score
            (text similarity)        (distance to normal)      (similarity to anomalous)
                    └─────────────────────────┼─────────────────────────┘
                                              ↓
                                    Weighted Aggregation
                                              ↓
                                        Final Anomaly Score

- **Critical path:**
  1. **Memory bank construction** (training time): Extract multi-scale patch embeddings from the anomalous training image. Store normal patches (from non-defect regions) in reference memory R, anomalous patches (from defect regions using pixel mask) in anomalous memory R⁺.
  2. **Score computation** (inference): For each test image, compute three scores: (a) Zero-Shot text similarity via WinCLIP prompts, (b) reference memory distance, (c) anomalous memory similarity.
  3. **Weight optimization** (optional): Use augmented training images to validate different weight combinations via Monte-Carlo sampling.

- **Design tradeoffs:**
  - **Memory vs. speed:** Storing multi-scale patches increases memory footprint but enables faster inference (no gradient computation).
  - **Uniform vs. tailored weights:** Uniform weights work reasonably well; class-specific weights via validation can improve performance but require an additional normal sample and risk overfitting.
  - **Single vs. multiple anomalous samples:** The paper focuses on single-sample scenarios. Adding more anomalous samples could help but may also introduce conflicting signals if anomaly types diverge.

- **Failure signatures:**
  - **Performance drops vs. baseline:** Check Table 1 for classes like macaroni2 (70.49→68.29) or capsules (79.49→77.65)—these indicate the training anomaly doesn't generalize to test anomalies.
  - **Validation overfitting:** If using normal/Student-t sampling for weight optimization and seeing high validation scores but poor test scores, switch to uniform sampling or reduce sampling iterations.
  - **Text template mismatch:** If Zero-Shot score (azs) is low for a class (e.g., pcb2 in Table 3: 40.4), the text prompts don't align well—rely more on memory scores.

- **First 3 experiments:**
  1. **Reproduce single-sample comparison:** Take one anomalous sample per class from VisA/MVTec. Compare AUROC of (a) WinCLIP+ with one normal sample vs. (b) proposed method with one anomalous sample. Verify the pcb1 improvement is reproducible.
  2. **Ablate memory components:** Run three variants—(a) Zero-Shot only, (b) + reference memory, (c) + anomalous memory—to isolate each component's contribution.
  3. **Validate weight optimization:** Implement the augmentation-based validation with uniform sampling. Compare fixed baseline weights vs. optimized weights across multiple seeds to confirm statistical significance reported in Table 4.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the incorporation of anomalous memory banks and the proposed validation strategy impact pixel-level anomaly segmentation performance?
- **Basis in paper:** The authors state, "Future work will consider the anomaly segmentation scores" and explicitly mention investigating "defect segmentation quality."
- **Why unresolved:** The current evaluation is restricted to binary image-level anomaly classification (AUROC) and excludes segmentation metrics.
- **What evidence would resolve it:** Evaluation of pixel-level AUROC or PRO scores on MVTec and VisA datasets using the proposed score aggregation.

### Open Question 2
- **Question:** What more robust strategies exist for weighting the aggregation of multiple anomaly scores (Zero-Shot, normal, anomalous) beyond the proposed Monte Carlo validation on augmented data?
- **Basis in paper:** The authors suggest to "explore more strategies for weighting different anomaly scores," noting that current methods suffer from overfitting with non-uniform distributions.
- **Why unresolved:** The current validation relies on random uniform sampling; other distributions (Normal, Student-t) failed due to overfitting on the small augmented validation set.
- **What evidence would resolve it:** A proposed weighting mechanism (e.g., learnable attention or regularization) that remains stable across different data distributions or sampling methods.

### Open Question 3
- **Question:** How does the method perform when the single training anomaly is not visually representative of the anomalies found in the test set?
- **Basis in paper:** Section 4.2 notes that the memory bank can "mislead the model when unseen anomalies arise that are more dissimilar to the seen anomaly than to the normal sample."
- **Why unresolved:** The paper does not quantitatively analyze failure cases where the training anomaly differs significantly from test anomalies.
- **What evidence would resolve it:** An ablation study grouping test results by the similarity (e.g., feature distance) between the training anomaly and the test anomalies.

### Open Question 4
- **Question:** How does performance scale when utilizing more than one anomalous sample?
- **Basis in paper:** The paper is strictly limited to a single anomalous sample ("1 Anomalous Sample"), leaving the utility of multiple distinct anomaly examples unexplored.
- **Why unresolved:** It is uncertain if the memory bank effectively handles the increased variance of multiple anomaly types or if it introduces noise.
- **What evidence would resolve it:** Experimental results comparing performance when $K$ (number of anomalous samples) is increased from 1 to $N$.

## Limitations
- Reliance on a single anomalous sample per class constrains generalization when test anomalies differ visually from the training anomaly
- Performance improvements are highly class-dependent, with gains ranging from -2% to +37%
- The method inherits CLIP's limitations in domain-specific anomaly detection where visual-text alignment may be weak

## Confidence

- **High confidence:** The core mechanism of combining anomalous memory scores with normal reference scores is well-supported by controlled experiments showing consistent improvements in many classes (e.g., pcb1: 53.51→91.03 AUROC).
- **Medium confidence:** The weight optimization via augmented validation shows statistically significant improvements in two of three datasets, but the effect size is modest and the approach overfits with certain sampling distributions.
- **Medium confidence:** The multi-scale representation improves performance for classes with varying defect sizes, though the optimal scale varies by class and no single scale dominates for classes with highly variable defect sizes.

## Next Checks

1. **Cross-anomaly generalization test:** Run experiments where the single anomalous training sample is deliberately chosen to be visually dissimilar from test anomalies (different defect types within same class) to quantify the method's robustness to intra-class anomaly variation.

2. **Weight optimization sensitivity analysis:** Compare uniform vs. normal vs. Student-t sampling across 10 random seeds per class to confirm the overfitting behavior and determine optimal sampling strategy.

3. **Multi-sample ablation study:** Test scenarios with 2-3 anomalous training samples per class to determine whether adding samples beyond the single-sample case provides additional benefits or introduces conflicting signals.