---
ver: rpa2
title: Continual GUI Agents
arxiv_id: '2601.20732'
source_url: https://arxiv.org/abs/2601.20732
tags:
- agents
- continual
- gui-aif
- arxiv
- interaction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Continual GUI Agents, a new task that requires
  GUI agents to adapt to shifting domains and resolutions in dynamic digital environments.
  The authors identify that existing methods fail to maintain stable grounding as
  GUI distributions shift, due to the diversity of UI interaction points and regions
  in fluxing scenarios.
---

# Continual GUI Agents

## Quick Facts
- arXiv ID: 2601.20732
- Source URL: https://arxiv.org/abs/2601.20732
- Authors: Ziwei Liu, Borui Kang, Hangjie Yuan, Zixiang Zhao, Wei Li, Yifan Zhu, Tao Feng
- Reference count: 18
- Primary result: GUI-Anchoring in Flux (GUI-AiF) achieves 83.5% accuracy on mobile, 77.3% on desktop, and 95.7% on web tasks in continual domain learning, and 30.5% accuracy on CAD, 34.4% on Dev, and 24.2% on Creative tasks in continual resolution learning.

## Executive Summary
This paper introduces Continual GUI Agents, a new task requiring GUI agents to adapt to shifting domains and resolutions in dynamic digital environments. The authors identify that existing methods fail to maintain stable grounding as GUI distributions shift, due to the diversity of UI interaction points and regions in fluxing scenarios. To address this, they propose GUI-Anchoring in Flux (GUI-AiF), a reinforcement fine-tuning framework that stabilizes continual learning through two novel rewards: Anchoring Point Reward in Flux (APR-iF) and Anchoring Region Reward in Flux (ARR-iF). These rewards guide agents to align with shifting interaction points and regions, mitigating over-adaptation to static grounding cues.

## Method Summary
The paper proposes GUI-Anchoring in Flux (GUI-AiF), a reinforcement fine-tuning framework that stabilizes continual GUI agents through two novel rewards. APR-iF enhances generalization of interaction points by rewarding exploration of diverse locations, avoiding over-adapting to static coordinates. ARR-iF promotes robustness to scale variations by encouraging diversity in predicted element regions through distributional separation rewards. The method uses Qwen2.5VL-3B with GRPO optimization, incorporating a KL penalty term to prevent forgetting. The framework is trained sequentially on domain and resolution tasks, with extensive evaluation on ScreenSpot benchmarks showing significant improvements over state-of-the-art baselines.

## Key Results
- GUI-AiF achieves 83.5% accuracy on mobile, 77.3% on desktop, and 95.7% on web tasks in continual domain learning
- GUI-AiF achieves 30.5% accuracy on CAD, 34.4% on Dev, and 24.2% on Creative tasks in continual resolution learning
- Ablation studies confirm both APR-iF and ARR-iF contribute to performance, with APR-iF having larger impact on domain shifts

## Why This Works (Mechanism)

### Mechanism 1: Spatial Variance Reward for Interaction Point Generalization (APR-iF)
- Claim: Encouraging diverse predicted interaction points during training prevents over-adaptation to static coordinate patterns.
- Mechanism: For N predictions per instruction, APR-iF computes spatial variance by calculating the centroid of all predicted center points and measuring average squared Euclidean distance from this centroid. Higher variance signals exploration of diverse locations rather than clustering around memorized coordinates.
- Core assumption: GUI domain shifts primarily manifest as changes in element positions; diverse exploration during training transfers to better localization under distribution shift.
- Evidence anchors:
  - [abstract] "APR-iF enhances the agents' ability to generalize interaction points by rewarding exploration of diverse locations, avoiding over-adapting to coordinates from a static task."
  - [Section 3.2] "Intuitively, a higher variance indicates that center points are more spread out across the interface, rather than being clustered around a single coordinate."
  - [corpus] ScreenExplorer (arXiv:2505.19095) explores diverse exploration in GUI environments, supporting the exploration-generalization link.
- Break condition: If domain shifts primarily involve appearance changes (e.g., dark mode, localization) rather than position changes, APR-iF's variance reward may not provide meaningful signal.

### Mechanism 2: Distributional Separation Reward for Scale Robustness (ARR-iF)
- Claim: Modeling bounding boxes as Gaussian distributions and rewarding their separation improves adaptation to varying element scales across resolutions.
- Mechanism: Each predicted bounding box is transformed into a Gaussian with mean at the center and covariance proportional to width/height. Bhattacharyya distance measures separation between distribution pairs. Average pairwise distance becomes the reward—larger values indicate spatially distinct regions.
- Core assumption: Resolution shifts cause proportional changes to element scales; encouraging diverse region predictions prepares the agent for scale variations.
- Evidence anchors:
  - [abstract] "ARR-iF promotes robustness to scale variations by encouraging diversity in predicted element regions."
  - [Section 3.2] "ARR-iF computes the Bhattacharyya distance DB between them... This reward incentivizes the agents to generate spatially distinct prediction regions."
  - [corpus] Corpus evidence for Gaussian reward modeling exists (GUI-G2 cited in paper), but corpus lacks direct validation of Bhattacharyya distance for continual resolution learning specifically.
- Break condition: If high-resolution interfaces introduce qualitatively new element types rather than scaled versions of existing elements, the scale-diversity assumption weakens.

### Mechanism 3: KL-Regularized Policy Updates for Forgetting Mitigation
- Claim: RFT's implicit KL divergence minimization from a reference model provides a stability mechanism absent in SFT approaches.
- Mechanism: The objective function J(θ) includes a KL penalty term β·D_KL[π_ref||π_θ] that measures divergence between current and reference policies. This regularizes updates to prevent large parameter shifts that would overwrite prior knowledge, while the reward terms (APR-iF + ARR-iF) encourage exploration.
- Core assumption: Solutions with low KL from the reference model correlate with reduced forgetting; the exploration rewards do not collapse the policy into erratic behavior.
- Evidence anchors:
  - [Section 2.2] "RFT offers potential advantages as its on-policy updates inherently favor solutions with minimal KL divergence from the base model, which is strongly correlated with reduced forgetting."
  - [Table 5] Ablation without KL divergence shows degraded final average accuracy (77.1% vs 81.7% on SSv1 Mobile→Desktop→Web), confirming the regularization role.
  - [corpus] RL's Razor (Shenfeld et al., cited in paper) discusses why online RL forgets less, supporting the mechanism.
- Break condition: If the exploration rewards (α·Rp + γ·Rr) dominate the KL penalty (β too small), unconstrained exploration can collapse policy and overwrite prior knowledge—the paper's ablation confirms this risk.

## Foundational Learning

- Concept: **Reinforcement Fine-Tuning (RFT) vs. Supervised Fine-Tuning (SFT)**
  - Why needed here: The paper argues SFT's cross-entropy objective forces convergence to current task distribution, causing catastrophic forgetting. RFT's on-policy updates with KL regularization provide implicit continual learning capacity.
  - Quick check question: Can you explain why minimizing KL divergence from a reference model helps preserve prior knowledge compared to standard gradient descent on cross-entropy loss?

- Concept: **GRPO (Group Relative Policy Optimization)**
  - Why needed here: The paper uses GRPO to compute advantages by normalizing rewards against group mean/std. Understanding this is essential for implementing the reward integration (At + R_AiF).
  - Quick check question: How does GRPO's advantage normalization differ from standard PPO's advantage estimation, and why might it matter for GUI grounding tasks?

- Concept: **Bhattacharyya Distance between Gaussian Distributions**
  - Why needed here: ARR-iF requires computing distributional separation between predicted bounding boxes. The closed-form solution for Gaussian Bhattacharyya distance is used directly in the reward calculation.
  - Quick check question: Given two Gaussian distributions with means μ₁, μ₂ and covariances Σ₁, Σ₂, can you implement the Bhattacharyya distance calculation?

## Architecture Onboarding

- Component map: Input Processing -> Policy Model π_θ -> Reference Model π_ref -> Reward Computation -> GRPO Advantage -> KL Penalty
- Critical path:
  1. For each instruction, generate N=4 bounding box predictions
  2. Extract center points and compute APR-iF (spatial variance)
  3. Convert boxes to Gaussians and compute ARR-iF (pairwise Bhattacharyya)
  4. Combine with task reward via R_AiF = α·Rp + γ·Rr (hyperparameters: α=15, γ=0.5)
  5. Compute GRPO advantage: A_i = (r_i + R_AiF - mean) / std
  6. Optimize J(θ) with KL penalty (β=0.04)

- Design tradeoffs:
  - **α vs γ weighting**: Paper finds APR-iF (point diversity) more impactful for domain shifts; ARR-iF (region diversity) more relevant for resolution shifts. Task-specific tuning may be needed.
  - **N predictions per instruction**: More predictions increase reward estimation stability but raise compute cost. Paper uses N=4.
  - **KL penalty β**: Too high prevents adaptation; too low allows collapse. Paper's β=0.04 is a starting point.

- Failure signatures:
  - **Unconstrained exploration**: Removing KL penalty causes performance degradation (Table 5)—policy collapses.
  - **Reward imbalance**: Using only APR-iF or ARR-iF underperforms full GUI-AiF (Table 4)—both location and scale diversity matter.
  - **Text-icon bias**: Paper notes consistent performance gap favoring text over icon interactions across domains—visual branch may over-rely on OCR.

- First 3 experiments:
  1. **Baseline reproduction**: Implement GRPO with standard IoU/distance rewards on Mobile→Desktop→Web sequence; verify performance matches reported baselines (~77-78% final average on SSv2).
  2. **Ablation by reward component**: Add APR-iF alone, then ARR-iF alone, then both; confirm both contribute and APR-iF has larger impact on domain shifts.
  3. **KL sensitivity**: Sweep β ∈ {0.01, 0.02, 0.04, 0.08} on a held-out validation split; identify collapse threshold and stability plateau.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does GUI-AiF maintain its effectiveness when applied to vision-language models significantly larger than the 3B parameter scale tested?
- Basis in paper: [explicit] The authors explicitly state in the "Limitations" section that computing resource constraints restricted their exploration to a 3B model scale, which inherently restricts the absolute performance of the agents.
- Why unresolved: It is unclear if the proposed reward mechanisms (APR-iF and ARR-iF) scale linearly or suffer from diminishing returns when applied to higher-capacity models with different grounding priors.
- Evidence would resolve it: Comparative experiments applying GUI-AiF to 7B or 70B parameter models, demonstrating consistent performance improvements over RFT baselines on the ScreenSpot benchmarks.

### Open Question 2
- Question: Can the current anchoring reward framework handle non-spatial environmental shifts, such as appearance changes (e.g., dark mode) or localization updates?
- Basis in paper: [explicit] The authors identify that their exploration is limited to domain and resolution shifts, noting that real-world flux includes "layout shifts (app updates), appearance shifts (dark mode), and localization (language changes)."
- Why unresolved: The current rewards (APR-iF and ARR-iF) are mathematically designed to address spatial diversity (variance in points and region separation) and may not explicitly penalize errors stemming from semantic or stylistic visual changes.
- Evidence would resolve it: Evaluation results on a modified benchmark where GUI agents must adapt to dark mode interfaces or multi-language applications without fine-tuning on those specific variations.

### Open Question 3
- Question: Can the reward structure be modified to close the performance gap between text-based and icon-based grounding?
- Basis in paper: [inferred] The supplementary material (Section C) identifies a consistent bias where agents perform significantly better on text tasks than icon tasks, attributing this to a reliance on OCR capabilities over semantic visual understanding.
- Why unresolved: While GUI-AiF improves general grounding, the paper does not demonstrate that the proposed rewards specifically rectify the agent's weakness in interpreting semantically ambiguous icons compared to readable text.
- Evidence would resolve it: An ablation study specifically analyzing the performance delta between text and icon elements, showing a reduced gap when using GUI-AiF compared to standard RFT.

## Limitations

- Computing resource constraints limited exploration to 3B parameter models, restricting absolute performance
- Effectiveness of APR-iF assumes GUI shifts are primarily positional, but real-world scenarios often involve complex appearance changes
- Bhattacharyya distance formulation assumes Gaussian modeling of bounding boxes is appropriate, which may not capture true uncertainty for irregularly shaped UI elements

## Confidence

- **High confidence**: The core mechanism of using KL-regularized RFT for forgetting mitigation is well-supported by ablation studies and consistent with RL literature on continual learning
- **Medium confidence**: The spatial variance reward (APR-iF) shows clear benefits for domain shifts, but its effectiveness depends heavily on the nature of GUI transformations
- **Medium confidence**: The distributional separation reward (ARR-iF) is theoretically sound but lacks extensive ablation studies across different resolution ranges and UI element types
- **Low confidence**: Claims about generalization to unseen domains beyond the tested benchmark sequences require additional validation

## Next Checks

1. **Reward breakdown analysis**: Implement detailed tracking of APR-iF and ARR-iF values during training, comparing their correlation with task performance across different domain shift types (position vs. appearance changes)

2. **Distribution assumption validation**: Test alternative uncertainty modeling approaches (e.g., uniform distributions, mixture models) for ARR-iF to verify that Gaussian assumptions are optimal for bounding box uncertainty

3. **Long-horizon continual learning**: Design experiments with extended task sequences (>3 domains) and periodic revisiting of old tasks to assess true forgetting resistance under realistic usage patterns