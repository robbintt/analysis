---
ver: rpa2
title: Exploration by Random Reward Perturbation
arxiv_id: '2506.08737'
source_url: https://arxiv.org/abs/2506.08737
tags:
- uni00000013
- uni00000014
- uni00000015
- uni00000017
- uni00000019
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Random Reward Perturbation (RRP), a novel
  exploration strategy for reinforcement learning that injects zero-mean Gaussian
  noise into environmental rewards to enhance policy diversity. RRP is theoretically
  shown to increase the variance of trajectories sampled during learning, thereby
  expanding the exploration range.
---

# Exploration by Random Reward Perturbation

## Quick Facts
- arXiv ID: 2506.08737
- Source URL: https://arxiv.org/abs/2506.08737
- Reference count: 40
- One-line primary result: RRP achieves competitive performance with structured exploration methods while offering lower computational complexity through simple reward perturbation.

## Executive Summary
This paper introduces Random Reward Perturbation (RRP), a novel exploration strategy for reinforcement learning that injects zero-mean Gaussian noise into environmental rewards to enhance policy diversity. RRP is theoretically shown to increase the variance of trajectories sampled during learning, thereby expanding the exploration range. It is fully compatible with existing action-perturbation-based exploration strategies like ε-greedy and entropy regularization, providing additive improvements. The method is lightweight, requiring only simple modifications to the reward function, and is easy to integrate into existing RL algorithms with negligible computational overhead.

Experiments integrating RRP into Proximal Policy Optimization (PPO) and Soft Actor-Critic (SAC) across nine tasks in MuJoCo, Mobile Manipulator, and Dexterous Hand domains show consistent improvements. RRP-PPO and RRP-SAC achieve higher sample efficiency, faster convergence, and better ability to escape local optima, especially in sparse-reward settings. Performance is competitive with structured exploration methods like NovelD, RND, and ExploRS, while offering lower computational complexity. Ablation studies confirm robustness to noise scale and decay period. RRP effectively broadens exploration by preventing premature convergence in non-progressive optimization, making it a practical, resource-efficient exploration strategy.

## Method Summary
RRP injects zero-mean Gaussian noise ε ~ N(0, σ²) into environmental rewards: R_RRP(s) = R_env(s) + ε. The noise is linearly annealed over training: σ(t) = max{0, σ_max − (σ_max − σ_min)t/T}, with default σ_max=1.0 and λ=0.3 (decay as fraction of total steps). For PPO, noise is sampled in real-time during rollout collection; for SAC, noise is stored alongside transitions in the replay buffer and annealed at batch sampling time. The method requires minimal implementation changes to existing RL algorithms and adds negligible computational overhead.

## Key Results
- RRP-PPO and RRP-SAC achieve higher sample efficiency and faster convergence across 9 benchmark tasks
- Performance is competitive with structured exploration methods (NovelD, RND, ExploRS) while requiring less computational complexity
- RRP shows particular effectiveness in sparse-reward settings by preventing premature convergence to zero-gradient traps
- Ablation studies confirm robustness to noise scale and decay period hyperparameters

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Injecting zero-mean Gaussian noise into environmental rewards increases the variance of policy outputs and sampled trajectories.
- **Mechanism:** Theoretical analysis (Lemma 3.2) demonstrates that training neural networks with perturbed targets increases the trace of the output covariance matrix compared to training with clean targets. This variance in value/policy outputs propagates through the transition dynamics, ensuring V(T^RRP) > V(T^ori) (Theorem 3.5), thereby preventing the policy from collapsing into low-variance, repetitive behaviors.
- **Core assumption:** The noise distribution is zero-mean, ensuring the expected value of the policy updates remains aligned with the true environmental reward over time.
- **Evidence anchors:**
  - [Section 3.2, Eq. 11] "Tr(C(2)) = Tr(C(1)) + ... > Tr(C(1))"
  - [Abstract] "theoretical analyses demonstrate that adding zero-mean noise... effectively enhances policy diversity"
  - [Corpus] Weak direct support; neighbors like *IIB-LPO* discuss exploration collapse generally, but RRP's specific variance-injection via rewards is distinct.
- **Break condition:** If the noise scale is excessively large relative to the reward scale, gradient updates may become unstable, causing divergence rather than structured exploration.

### Mechanism 2
- **Claim:** In sparse-reward settings, RRP mitigates "non-progressive optimization" by replacing zero-dominated feedback with stochastic signals.
- **Mechanism:** In sparse environments, standard TD targets (r_env + γV') often lack informative gradients when r_env ≈ 0. RRP modifies the target to (r_env + ε + γV'). Although ε carries no semantic meaning, it breaks the constant-gradient trap, preventing the value function from prematurely converging to zero and encouraging the agent to visit diverse states to "explain" the random rewards.
- **Core assumption:** The agent's optimization process (e.g., SGD) treats the perturbed reward as a valid learning signal, driving action selection even in the absence of environmental feedback.
- **Evidence anchors:**
  - [Section 3.1] "In sparse-reward scenarios... perturbing the rewards modifies the TD-target... effectively prevents premature convergence."
  - [Figure 1b] Visual comparison showing increased action diversity in a grid-maze under RRP.
  - [Corpus] *Value of Information-Enhanced Exploration* notes standard methods struggle in sparse settings; RRP provides a lightweight alternative to complex novelty search.

### Mechanism 3
- **Claim:** RRP is orthogonal to action-space noise (e.g., entropy regularization), allowing for additive exploration improvements.
- **Mechanism:** Standard exploration perturbs the action selection (a ~ π(·|s)) while the objective remains fixed. RRP perturbs the objective (reward) itself. By combining stochastic policies (action variance) with RRP (objective variance), the agent explores a broader manifold of possible state-action-reward mappings than either method alone.
- **Core assumption:** The exploration benefits of perturbing the objective function do not cancel out or destabilize the exploration benefits of stochastic action sampling.
- **Evidence anchors:**
  - [Abstract] "fully compatible with the action-perturbation-based exploration strategies... providing additive improvements"
  - [Section 1] "providing controllable transitions from exploration to exploitation"
  - [Corpus] *Improving DAPO* mentions mixed-policy perspectives; RRP effectively mixes the "reward policy" randomly.

## Foundational Learning

- **Concept: Reward Shaping & Potential-Based Rewards**
  - **Why needed here:** RRP acts as a form of reward shaping. Understanding that valid shaping preserves the optimal policy (or in RRP's case, anneals back to it) is crucial to grasping why the agent doesn't just optimize for the noise.
  - **Quick check question:** If the noise ε were constant (non-zero mean), would the optimal policy for the perturbed reward still be optimal for the original environment?

- **Concept: Bias-Variance Trade-off in SGD**
  - **Why needed here:** RRP explicitly manipulates the variance of the optimization process. Learners must understand that high variance aids exploration but can hurt convergence stability (requiring annealing).
  - **Quick check question:** Why does the paper decay the noise σ over time rather than keeping it constant?

- **Concept: On-Policy vs. Off-Policy Implementation**
  - **Why needed here:** The implementation of RRP differs slightly for PPO (on-policy) vs. SAC (off-policy) regarding when the noise is sampled and annealed.
  - **Quick check question:** In the SAC implementation (Algorithm 1), is the noise annealed when the transition is stored in the buffer or when it is sampled for training?

## Architecture Onboarding

- **Component map:** Noise Generator -> Reward Wrapper -> Scheduler -> Buffer (for SAC) -> RL Algorithm
- **Critical path:**
  1. Environment step returns r_env
  2. Scheduler calculates current σ(t)
  3. Noise Generator produces ε (PPO) or stores raw ε for later annealing (SAC)
  4. Reward Wrapper computes r_RRP = r_env + ε
  5. Loss function uses r_RRP to calculate TD-error or Advantage
- **Design tradeoffs:**
  - **Noise Scale (σ):** Too low → no exploration gain; Too high → unstable gradient descent
  - **Decay Period (λ):** Short → premature convergence; Long → wasted samples optimizing noise
  - **On vs. Off Policy:** On-policy (PPO) anneals noise in real-time; Off-policy (SAC) must store noise and anneal during sampling to ensure correct temporal logic
- **Failure signatures:**
  - **Runaway Loss:** Loss fails to decrease or explodes; indicates σ is too high or decay is too slow
  - **Stagnation:** Agent performs similarly to baseline; indicates σ is too low or decayed too fast
  - **Replay Mismatch (SAC):** Using the wrong noise value for old data; indicates failure to store/retrieve ε correctly per transition
- **First 3 experiments:**
  1. **Sparse Reward Sanity Check:** Implement RRP-PPO on a simple sparse maze (e.g., MountainCar or GridWorld). Verify that the agent finds the goal faster than vanilla PPO by visualizing state visitation density.
  2. **Ablation on σ:** Test σ ∈ {0.5, 1.0, 1.5} on a dense reward task (e.g., HalfCheetah) to observe the transition from "no effect" to "instability" and identify the robust range.
  3. **Compatibility Test:** Enable both RRP and Entropy Regularization (if using SAC) on a complex manipulation task (e.g., HandBlock). Confirm that performance is additive and not degrading compared to either method alone.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can a hybrid reward shaping strategy that transitions from random perturbations to structured, value-based rewards improve convergence compared to the standard annealing approach?
- **Basis in paper:** [explicit] Section 7 (Discussion and Conclusion) states that "RRP's generality makes it a promising extension... one could design rewards that shift from random perturbations to structured and value-based rewards."
- **Why unresolved:** The current implementation only investigates annealing noise to zero; the proposed "shift" to a structured bonus is suggested as future work but not implemented or tested.
- **What evidence would resolve it:** Experiments comparing the performance of standard RRP against a variant that switches its perturbation strategy to an intrinsic motivation module (e.g., RND) after a set number of steps or based on a convergence metric.

### Open Question 2
- **Question:** How does RRP specifically fail in extremely long-horizon tasks compared to directional exploration methods?
- **Basis in paper:** [explicit] Section 7 explicitly notes that "RRP struggles in extremely sparse reward settings or extremely long-horizon tasks where directional exploration is more practical."
- **Why unresolved:** While the paper identifies this limitation, the experimental evaluation is limited to standard MuJoCo and manipulation tasks; it does not quantify performance on "extremely" sparse or long-horizon benchmarks where these failures would occur.
- **What evidence would resolve it:** Benchmarking RRP against directional exploration algorithms (e.g., Go-Explore) on hard-exploration games like Montezuma's Revenge or complex maze navigation to isolate failure modes.

### Open Question 3
- **Question:** Is the fixed linear decay schedule optimal compared to adaptive decay mechanisms based on agent uncertainty?
- **Basis in paper:** [inferred] The methodology (Eq. 2) relies on a manually tuned linear decay, and ablation studies show sensitivity to the decay period λ.
- **Why unresolved:** A fixed schedule assumes a constant exploration need decay, which may not align with the agent's actual learning progress or the specific reward landscape of the task.
- **What evidence would resolve it:** Implementing an adaptive decay rate tied to metrics such as value function error or policy entropy, and comparing the sample efficiency and stability against the fixed linear schedule.

## Limitations

- Theoretical analysis relies on assumptions about policy network size and data distribution that may not hold for practical architectures
- Performance improvements vary across domains, with more modest gains in dense-reward tasks
- Limited evaluation on extremely sparse or long-horizon tasks where directional exploration methods may be more effective

## Confidence

- **High**: The empirical performance improvements on benchmark tasks, the compatibility with existing exploration methods, and the computational efficiency claims are well-supported by experimental results
- **Medium**: The theoretical justification for variance increase and the mechanism preventing premature convergence are mathematically sound but rely on assumptions that may not fully translate to practice
- **Medium**: The claim of competitive performance with structured exploration methods is supported by comparison on 9 tasks, though the relative advantage varies by domain

## Next Checks

1. **Theoretical Bound Verification**: Implement a controlled experiment with a simple policy network (e.g., linear or small MLP) to empirically verify that RRP increases output covariance as predicted by Lemma 3.2
2. **Hyperparameter Sensitivity**: Conduct a systematic grid search over σ_max ∈ {0.5, 1.0, 1.5} and λ ∈ {0.1, 0.3, 0.5} on a subset of tasks to map the performance landscape and identify failure modes
3. **Compatibility Robustness**: Test RRP integration with additional exploration strategies (e.g., curiosity-driven exploration, count-based bonuses) on complex manipulation tasks to confirm additive improvements hold across method combinations