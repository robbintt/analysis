---
ver: rpa2
title: Multi-modal brain encoding models for multi-modal stimuli
arxiv_id: '2505.20027'
source_url: https://arxiv.org/abs/2505.20027
tags:
- brain
- alignment
- multi-modal
- unimodal
- video
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how multi-modal brain encoding models predict
  neural activity when participants engage with multi-modal stimuli, specifically
  movies with audio. Using fMRI data from subjects watching movies, the authors compare
  brain alignment of cross-modal and jointly pretrained multi-modal models against
  unimodal models across various brain regions.
---

# Multi-modal brain encoding models for multi-modal stimuli

## Quick Facts
- arXiv ID: 2505.20027
- Source URL: https://arxiv.org/abs/2505.20027
- Reference count: 40
- Multi-modal brain encoding models predict neural activity from movies with audio better than unimodal models

## Executive Summary
This study investigates how multi-modal brain encoding models predict neural activity when participants engage with multi-modal stimuli, specifically movies with audio. Using fMRI data from subjects watching movies, the authors compare brain alignment of cross-modal and jointly pretrained multi-modal models against unimodal models across various brain regions. They find that both multi-modal model types show improved alignment in language and visual regions compared to unimodal models, with cross-modal models particularly excelling in semantic regions. Through residual analysis, they demonstrate that multi-modal alignment partially depends on both video and audio features for jointly pretrained models, while for cross-modal models it primarily depends on video features. The results suggest that multi-modal models capture additional information beyond unimodal representations that is processed in visual and language brain regions.

## Method Summary
The study uses fMRI data from the Movie10 dataset where 6 subjects watched 4 movies. Brain encoding models were trained to predict voxel activity from multi-modal embeddings using voxel-wise ridge regression with L2 regularization. Three types of models were compared: unimodal (ViT-B, VideoMAE, ViViT for video; Wav2Vec2.0, AST for audio), cross-modal (ImageBind with separate video/audio encoders), and jointly pretrained (TVLT with token-level fusion). Features were temporally aligned using a 5-delay Finite Impulse Response filter to account for hemodynamic response. Brain alignment was measured as Pearson correlation between predicted and actual activity, normalized by cross-subject prediction accuracy ceiling.

## Key Results
- Multi-modal models (both cross-modal and joint) show significantly improved brain alignment compared to unimodal models across all ROIs
- Cross-modal models (ImageBind) show superior alignment in semantic regions (angular gyrus, posterior cingulate cortex) compared to jointly pretrained models
- Residual analysis reveals multi-modal alignment depends on both video and audio features for jointly pretrained models, but primarily on video features for cross-modal models
- No single model outperforms others across all brain regions, indicating region-specific representational advantages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-modal models capture cross-modal information transfer that improves brain alignment beyond what unimodal representations provide.
- Mechanism: Cross-modal pretraining (e.g., ImageBind) uses contrastive learning to align separate modality encoders into a shared representation space. When visual input is processed, the model implicitly activates semantically related auditory representations through learned cross-modal associations, creating richer embeddings that better match the brain's integrated multi-modal processing.
- Core assumption: The brain processes movie stimuli by simultaneously activating related visual and auditory semantic representations, which multi-modal models approximate through their pretraining objective.
- Evidence anchors:
  - [abstract]: "multi-modal alignment partially depends on both video and audio features for jointly pretrained models, while for cross-modal models it primarily depends on video features"
  - [section 6.3]: "cross-modal models, when transferring knowledge from one modality to another, the model relies more heavily on visual information"
  - [corpus]: Limited direct corpus support; neighbor papers confirm multi-modal models show higher brain alignment than unimodal models.

### Mechanism 2
- Claim: Jointly pretrained models learn balanced multi-modal representations through token-level fusion, making alignment robust to single-modality removal.
- Mechanism: TVLT processes video patches and audio spectrograms as tokens in a single transformer encoder using masked autoencoding. Early layers fuse modalities directly, distributing semantic information across both modalities rather than privileging one. Residual analysis shows both video and audio removal cause partial alignment drops, indicating integrated rather than modality-dominant representations.
- Core assumption: Joint early fusion creates distributed representations where neither modality dominates, mirroring how brain regions like AG integrate spatio-temporal multi-modal information.
- Evidence anchors:
  - [section 6.3]: "TVLT model learns a more balanced representation of both video and audio features... making the model less sensitive to the loss of features from a specific modality"
  - [figure 4]: TVLT Joint shows partial alignment drops after removal of either video or audio features

### Mechanism 3
- Claim: Improved alignment in language regions (AG, PCC, PTL, IFG) reflects multi-modal models capturing narrative-level semantics beyond low-level sensory features.
- Mechanism: Language regions process integrated semantic content (e.g., plot, dialogue). Multi-modal models, trained on video-audio-text data, internalize narrative structure. Cross-modal models' superior alignment in AG/PCC suggests their contrastive pretraining emphasizes semantic alignment over perceptual features, matching these regions' integrative function.
- Core assumption: Language regions like AG serve as "multi-modal convergent buffers" integrating narrative information across modalities, which multi-modal models approximate through their pretraining on semantically rich data.
- Evidence anchors:
  - [section 6.2]: "multi-modal embeddings show improvements in semantic regions such as the AG, PCC and dmPFC, and syntactic regions such as the PTL and IFG"
  - [section 7]: "AG serves as a multi-modal convergent buffer integrating spatio-temporal information from multiple sensory modalities to process narratives"

## Foundational Learning

- Concept: **Hemodynamic Response Function (HRF) and Temporal Delays**
  - Why needed here: fMRI measures blood oxygenation changes that lag neural activity by ~4-6 seconds. The paper models this using a Finite Impulse Response (FIR) filter with 5 temporal delays (~7.5s). Without this, stimulus-brain alignment would be temporally misaligned.
  - Quick check question: Why does the paper use 5 TR delays (~7.5s) rather than a single instantaneous mapping?

- Concept: **Residual Analysis for Causal Attribution**
  - Why needed here: To determine if multi-modal alignment comes from genuine integration or just better unimodal features, the paper removes unimodal features (via ridge regression projection) from multi-modal embeddings and measures remaining alignment. This isolates the contribution of cross-modal integration.
  - Quick check question: If removing video features from IB-Concat embeddings causes a large alignment drop in AG, what does this imply about the source of IB-Concat's alignment?

- Concept: **Normalized Brain Alignment (Ceiling-Normalized Correlation)**
  - Why needed here: Raw Pearson correlations are hard to interpret due to noise ceilings. The paper divides model predictions by cross-subject prediction accuracy (estimated from inter-subject reliability), giving a normalized score where 1.0 = perfect prediction given noise limits.
  - Quick check question: If a model achieves 0.5 normalized alignment and the cross-subject ceiling is 0.12 raw correlation, what is the model's raw correlation?

## Architecture Onboarding

- Component map: Input Stimuli (Video + Audio) -> Multi-modal Encoders (Cross-modal: ImageBind with separate ViT+AST encoders; Joint: TVLT with single transformer) -> Embeddings -> Temporal Alignment (5-delay FIR expansion) -> Ridge Regression (voxel-wise, L2 regularization) -> Predicted fMRI Activity -> Evaluation (Pearson correlation normalized by cross-subject ceiling)

- Critical path:
  1. **Stimulus preprocessing**: Segment video/audio into TR-aligned clips (1.49s windows)
  2. **Feature extraction**: Extract embeddings from last transformer layer (for IB: concatenate video+audio; for TVLT: use joint output)
  3. **HRF modeling**: Expand features with 5 temporal delays to account for hemodynamic lag
  4. **Residual analysis (optional)**: Train ridge to project unimodal features to multi-modal space, compute residuals, re-evaluate alignment
  5. **Voxel-wise encoding**: Train ridge regression per voxel; predict test movie brain activity

- Design tradeoffs:
  - **Ridge regression vs. deep encoder**: Paper uses linear ridge regression for interpretability and noise robustness. Tradeoff: cannot capture non-linear stimulus-brain relationships. Justified by fMRI's low SNR and prior literature.
  - **Last-layer embeddings only**: Paper uses final transformer layer. Tradeoff: misses potential hierarchical alignment (early sensory regions may align better with early layers). Appendix briefly shows layerwise analysis.
  - **Linear residual removal**: Removing unimodal features linearly limits ability to remove non-linear cross-modal interactions. Paper acknowledges this but argues it provides interpretable bounds.

- Failure signatures:
  - **Low alignment in early visual cortex (EVC)**: Multi-modal models show ~40% alignment vs. ~50% in high-level regions. This suggests multi-modal training may deprioritize low-level visual features (expected, as semantic learning dominates).
  - **No significant difference between cross-modal and joint models at whole-brain level**: Indicates macro-level similarity; differences emerge only at ROI level (AG, PCC, dmPFC).
  - **High variance across participants**: Error bars show substantial individual differences; cross-subject ceiling (~0.12 raw correlation) limits maximum achievable alignment.

- First 3 experiments:
  1. **Baseline sanity check**: Train encoding models with randomly initialized (untrained) multi-modal models vs. pretrained models. Verify pretrained models significantly outperform random (per Figure 2, they do: p ≤ 0.05).
  2. **ROI-specific comparison**: Compare IB-Concat vs. TVLT Joint alignment in AG, PCC, and dmPFC. Expect IB-Concat > TVLT in AG/PCC, TVLT > IB-Concat in dmPFC (per Figure 3).
  3. **Residual analysis validation**: Remove unimodal video features from IB-Concat, measure alignment drop in AG. Expect ~40-50% drop (per Figure 5), confirming video contribution. Repeat with audio removal—expect minimal drop for IB-Concat, moderate drop for TVLT Joint.

## Open Questions the Paper Calls Out

- **Open Question 1**: What specific mechanisms enable cross-modal models to capture additional brain-relevant information beyond what is present in unimodal embeddings, and does this reflect genuine multi-modal integration or transfer of modality-specific features?
  - Basis in paper: [explicit] The authors state that "there is additional information beyond the unimodal embeddings that is processed in the visual and language regions" and call for "the neuroscience community to investigate the interpretability of these models for deepening our understanding of multi-modal information processing in brain."

- **Open Question 2**: How does the timing of sensory experience during development affect the multi-modal representations formed in brain regions like the angular gyrus, and do late-onset sensory modalities produce brittle representations similar to those observed in cross-modal models?
  - Basis in paper: [explicit] The authors state: "It would be interesting to study patterns of activation in AG in patients who acquired visual or auditory function later in their life to see if one observes such brittleness in the representations acquired."

- **Open Question 3**: Can fine-tuning multi-modal models on brain-prediction tasks or ecologically valid tasks (e.g., video captioning) improve their alignment with neural activity, and would such improvements generalize across brain regions?
  - Basis in paper: [explicit] The limitations section states: "In the future, by fine-tuning these multi-modal models on specific tasks such as generating captions for videos, we can better leverage their alignment strengths."

- **Open Question 4**: To what extent do architectural differences versus pretraining objectives drive the observed differences between cross-modal and jointly pretrained models in their brain alignment patterns?
  - Basis in paper: [explicit] The limitations acknowledge that models differ in "architectural variability and variability in pretraining methods" and that "future work could benefit from more tightly controlled comparisons to better isolate the effects of these factors."

## Limitations

- **Single dataset limitation**: Results are based on one fMRI dataset (Movie10) with only 6 participants, limiting generalizability across populations.
- **Linear modeling constraint**: The study uses linear ridge regression, potentially missing non-linear brain representations and cross-modal interactions.
- **Noise ceiling uncertainty**: Cross-subject ceiling estimates introduce additional uncertainty in normalized alignment scores, affecting interpretation of model performance.

## Confidence

- **High Confidence**: Brain alignment improvements from multi-modal vs unimodal models (supported by multiple prior studies)
- **Medium Confidence**: Architecture-specific differences between cross-modal and joint models (limited comparative literature)
- **Medium Confidence**: Interpretation of alignment in language regions reflecting narrative semantics (plausible but requires further validation)

## Next Checks

1. **Replication on diverse datasets**: Test model alignment using different movie datasets and neuroimaging modalities (e.g., EEG, MEG) to verify generalizability beyond Movie10 fMRI.

2. **Layer-wise analysis**: Extend the layerwise encoding analysis to systematically map which transformer layers best predict activity in different brain regions, distinguishing sensory vs semantic processing.

3. **Ablation with matched semantics**: Conduct controlled experiments ablating either video or audio content while maintaining semantic coherence, to isolate whether alignment improvements stem from cross-modal integration or semantic richness.