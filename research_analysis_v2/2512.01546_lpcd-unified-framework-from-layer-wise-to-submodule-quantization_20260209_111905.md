---
ver: rpa2
title: 'LPCD: Unified Framework from Layer-Wise to Submodule Quantization'
arxiv_id: '2512.01546'
source_url: https://arxiv.org/abs/2512.01546
tags:
- quantization
- lpcd
- layer-wise
- loaq
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LPCD (Layer-Projected Coordinate Descent),
  a unified framework that extends post-training quantization (PTQ) beyond individual
  linear layers to arbitrary submodules while preserving layer-wise PTQ efficiency.
  LPCD optimizes relaxed objectives across Transformer submodules and projects the
  solutions back using standard layer-wise quantizers, generalizing existing methods
  like QEP and LoaQ as special cases.
---

# LPCD: Unified Framework from Layer-Wise to Submodule Quantization

## Quick Facts
- arXiv ID: 2512.01546
- Source URL: https://arxiv.org/abs/2512.01546
- Reference count: 40
- Key outcome: LPCD extends post-training quantization beyond individual layers to arbitrary submodules while preserving efficiency, consistently reducing quantization error and improving perplexity/accuracy across diverse models and bit-widths.

## Executive Summary
This paper introduces LPCD (Layer-Projected Coordinate Descent), a unified framework that extends post-training quantization (PTQ) beyond individual linear layers to arbitrary submodules while preserving layer-wise PTQ efficiency. LPCD optimizes relaxed objectives across Transformer submodules and projects the solutions back using standard layer-wise quantizers, generalizing existing methods like QEP and LoaQ as special cases. Applied to grouped-query KV, VO aggregation, and MLP up-down blocks, LPCD consistently reduces quantization error and improves perplexity and zero-shot accuracy across diverse models and bit-widths, particularly at low bit regimes. It provides a principled, memory-efficient approach for submodule quantization without altering existing PTQ pipelines.

## Method Summary
LPCD extends post-training quantization by treating quantization as a block-coordinate descent problem over submodule blocks rather than individual layers. The method alternates between continuous relaxation of quantization constraints and projection back to discrete values using existing layer-wise PTQ operators. This relaxation-projection decomposition enables optimization over arbitrary submodules (like QK attention or VO aggregation) while leveraging efficient solvers. LPCD generalizes prior methods like QEP and LoaQ as single-iteration special cases, providing a unified mathematical framework for error-compensation in quantization.

## Key Results
- LPCD consistently reduces quantization error and improves perplexity across LLaMA2-7B/13B, LLaMA3-8B, and Qwen3-8B/14B at 4/3/2 bits
- Submodule-level optimization (QK, VO, Up-Down) provides significant gains over layer-wise approaches, particularly at low bit-widths
- LPCD generalizes QEP and LoaQ as single-iteration special cases, establishing theoretical connection between these methods
- Optimal propagation strength parameters vary by model and bit-width, requiring careful hyperparameter tuning

## Why This Works (Mechanism)

### Mechanism 1: Relaxation-Projection Decomposition
LPCD approximately solves intractable discrete quantization by alternating between continuous relaxation and quantization projection steps. The method first relaxes quantization constraints to solve unconstrained optimization (often with closed-form least-squares solutions), then projects the continuous solution back to the discrete quantization domain using existing layer-wise PTQ operators as projectors. This preserves feasibility while leveraging efficient solvers.

### Mechanism 2: Submodule-Level Output Matching
Optimizing quantization at the submodule level (e.g., QK attention, VO aggregation, MLP up-down) reduces accumulated quantization error compared to layer-wise approaches. By defining losses over submodule outputs rather than individual layer outputs, LPCD captures interactions between layers within each block.

### Mechanism 3: Generalization of Prior Methods via Block-Coordinate Formulation
LPCD provides a unified mathematical framework where QEP and LoaQ emerge as single-iteration special cases. By formulating quantization as block-coordinate descent over multiple block variables (weights, activations, residuals), LPCD generalizes prior error-compensation methods.

## Foundational Learning

- **Concept: Activation-Aware Layer-Wise PTQ (e.g., GPTQ, AWQ)**
  - Why needed here: LPCD reuses existing layer-wise quantizers as projection operators; understanding how `Π_Q^(a) = argmin_{ĉW} ||X̂(ĉW - W)||_F^2` works is essential.
  - Quick check question: Can you explain why GPTQ uses the Hessian `H = X^T X` to weight quantization error?

- **Concept: Coordinate Descent and Block Relaxation Methods**
  - Why needed here: LPCD is fundamentally a block-coordinate scheme; the relaxation step solves `argmin_U L_r^(t)(U)` while holding other blocks fixed.
  - Quick check question: In block-coordinate descent, why does cyclic updates with Gauss-Seidel-style updates (using most recent values) typically converge faster than Jacobi-style (using previous-iteration values)?

- **Concept: Transformer Submodule Structure (Attention, MLP, Residual Connections)**
  - Why needed here: LPCD targets specific submodule compositions (QK, VO, Up-Down); understanding how attention scores, value aggregation, and gated MLPs compose is required to define appropriate losses.
  - Quick check question: For grouped-query attention, why does the key/value projection `W_K^(g)` serve multiple heads, and how does this affect the quantization objective?

## Architecture Onboarding

- **Component map:** Relaxation Solver -> Projection Operators -> Submodule Loss Definitions -> Block Scheduler
- **Critical path:**
  1. Define submodule structure and identify block variables (e.g., `cW_Q`, `cW_K` for QK module)
  2. Formulate submodule loss `L(cM_1, ..., cM_R)` matching full-precision output
  3. For each block `r`, derive relaxation step (closed-form or gradient-based)
  4. Apply appropriate projection operator based on block type (weight vs. activation)
  5. Iterate until convergence or fixed iteration count

- **Design tradeoffs:**
  - **Closed-form vs. gradient-based relaxation:** Closed-form is exact but memory-prohibitive for large design matrices; gradient-based is approximate but scalable
  - **Iteration count:** Single iteration recovers QEP/LoaQ; multiple iterations may improve performance but increase compute
  - **Submodule granularity:** Larger submodules capture more interactions but have higher memory/compute costs

- **Failure signatures:**
  - **Overfitting to calibration data:** Excessive correction strength (`α`, `β` near 1.0) causes degradation; the paper uses grid search on smaller models
  - **Memory overflow in relaxation step:** Explicit construction of design matrices is prohibitive for LLM-scale; must use gradient-based approximation
  - **Divergence at extreme low-bit (2-bit):** Even LPCD shows `>1e3` perplexity for some models at 2-bit, indicating fundamental limits

- **First 3 experiments:**
  1. **Reproduce QEP equivalence:** Implement two-block LPCD with `L(ĉW, X̂) = ||X̂ĉW - XW||_F^2`, run single iteration, and verify output matches QEP's corrected target `W* = (I + Ĥ^{-1}C)W`
  2. **Ablate iteration count:** Apply LPCD to QK module with 1, 3, 5 iterations on LLaMA-3-8B at 3-bit; measure layer output MSE and perplexity to assess convergence benefits
  3. **Submodule comparison:** Compare QK-only, VO-only, Up-Down-only, and combined LPCD on Qwen-3-8B at 3-bit to identify which submodule contributes most to perplexity reduction

## Open Questions the Paper Calls Out

- **Open Question 1:** Does adapting the block update sequence (e.g., randomized or greedy ordering) improve the convergence rate or final quantization error compared to the standard cyclic block-coordinate scheme? The current implementation utilizes a fixed cyclic order for updating blocks, leaving the potential benefits of alternative scheduling strategies unexplored.

- **Open Question 2:** Does optimizing the Gate projection weights (`W_G`) within the Up-Down MLP submodule yield better performance than the simplified approach of restricting optimization to only the Up (`W_U`) and Down (`W_D`) weights? The authors restricted optimization variables to `W_U` and `W_D` to simplify the minimization process, noting that applying LPCD to `W_G` requires approximate solvers which were not studied.

- **Open Question 3:** Can LPCD effectively and efficiently jointly quantize weights, activations, and KV caches simultaneously in large-scale models without destabilizing the optimization process? While the paper derives theoretical update rules for activations and KV caches, the experimental evaluation focuses primarily on weight-only quantization.

## Limitations
- Implementation details remain underspecified, particularly optimal propagation strength parameters (α, β) which vary by model and bit-width
- Computational complexity trade-offs are not fully characterized; memory-prohibitive closed-form solutions require gradient-based approximation without quality degradation analysis
- Generalization to non-Transformer architectures remains unproven as validation is limited to specific Transformer submodule structures

## Confidence
- **High confidence** in mathematical framework and special-case recovery (QEP/LoaQ equivalence)
- **Medium confidence** in empirical performance claims showing consistent improvements across multiple models
- **Low confidence** in practical deployment without extensive hyperparameter tuning due to sensitive dependence on α/β values

## Next Checks
1. **Quantify approximation error from gradient-based relaxation:** Implement both closed-form and gradient-based relaxation for QK module on LLaMA3-8B, measuring the difference in layer output MSE and downstream perplexity.

2. **Ablate submodule granularity systematically:** Apply LPCD to individual layers, pairs of adjacent layers, and full submodules (QK, VO, Up-Down) on Qwen3-8B at 3-bit to establish optimal granularity.

3. **Test extreme low-bit regime boundaries:** Evaluate LPCD on Qwen3-14B at 2-bit quantization with different initialization strategies and propagation strengths to document fundamental limits.