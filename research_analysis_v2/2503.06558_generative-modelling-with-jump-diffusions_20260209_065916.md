---
ver: rpa2
title: Generative modelling with jump-diffusions
arxiv_id: '2503.06558'
source_url: https://arxiv.org/abs/2503.06558
tags:
- function
- noise
- process
- score
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a generalization of generative diffusion models\
  \ to non-Gaussian noise processes with jumps, specifically finite activity L\xE9\
  vy processes. The key innovation is deriving a generalized score function that governs\
  \ the generative process when the forward process includes both Gaussian noise and\
  \ Poisson jumps with amplitudes drawn from an arbitrary normalizable distribution."
---

# Generative modelling with jump-diffusions

## Quick Facts
- arXiv ID: 2503.06558
- Source URL: https://arxiv.org/abs/2503.06558
- Reference count: 40
- Primary result: Jump-diffusion models with Laplace-distributed amplitudes outperform Gaussian models at high noise intensities with limited sampling steps

## Executive Summary
This paper presents a generalization of generative diffusion models to non-Gaussian noise processes with jumps, specifically finite activity Lévy processes. The key innovation is deriving a generalized score function that governs the generative process when the forward process includes both Gaussian noise and Poisson jumps with amplitudes drawn from an arbitrary normalizable distribution. The framework introduces a generalized score function S(x,t) that differs from the standard ∇logp(x,t) and can be estimated via a modified denoising score matching loss function. Both probability flow ODE and SDE formulations are derived, where the SDE includes jump-diffusion noise.

## Method Summary
The method combines Gaussian white noise with Poisson jumps in the forward diffusion process, deriving a generalized score function S(x,t) that replaces the standard score function for time-reversal. For pure jump processes with Laplace-distributed amplitudes, closed-form expressions are derived for both the conditional distribution and generalized score function using modified Bessel functions. The training procedure uses a modified denoising score matching loss that estimates the scaled score function, while sampling is performed via discretized ODE or SDE with exponential integrators. The JL model implementation is only slightly more computationally expensive than standard Gaussian models while providing performance benefits in specific parameter regimes.

## Key Results
- The JL model with Laplace-distributed jumps achieves F1 scores of 0.66 and 0.55 for swiss roll and Gaussian mixture datasets respectively with only 25 sampling steps at high noise intensity
- Performance advantage observed specifically at noise intensity D, σ² = 1 and small step numbers (≤50)
- The JL model outperforms both Gaussian models and LIM models (which use α-stable noise) in the high-noise, low-step regime
- Performance is highly dependent on noise intensity, with optimal range at σ² ≈ 1

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A generalized score function S(x,t) different from ∇logp(x,t) is required for consistent denoising when the forward process contains non-Gaussian jumps.
- Mechanism: The time-reversal of jump-diffusion processes introduces an additional drift term derived from the jump amplitude distribution. This term is captured by S(x,t) = ∇V(x,t)/p(x,t), where V is computed via an inverse Fourier transform involving ψ(k), which encodes both Gaussian noise intensity and jump characteristics through ϕ(k).
- Core assumption: The jump amplitude distribution ρ is isotropic and normalizable (finite activity Lévy process).
- Evidence anchors:
  - [abstract] "The generative process is shown to be governed by a generalized score function that depends on the jump amplitude distribution."
  - [Section 3, Eq. 9-10] Derivation of S(x,t) = ∇V(x,t)/p(x,t) with V defined through ψ(k)g(t)/k².
  - [corpus] Related work on generator matching [9] also recognizes that backward processes with jumps require adapted generators, though via different formalism.
- Break condition: If jump amplitudes follow a power-law distribution (infinite activity, α-stable), the derivation does not directly apply and requires fractional Laplacian operators instead.

### Mechanism 2
- Claim: The JL model with Laplace-distributed jump amplitudes provides closed-form expressions for both the conditional distribution and generalized score function.
- Mechanism: For pure jump processes (D=0) with λ=1 and Laplace-distributed amplitudes Ld(σ²), the characteristic function of jump increments admits an analytical form that yields closed-form expressions for p(x,t|x′) and S(x,t|x′) using modified Bessel functions K_ν.
- Core assumption: Pure jump regime (D=0) and isotropic Laplace distribution with ν=1.
- Evidence anchors:
  - [Section 3.1, Eq. 17-20] Explicit formulas for conditional PDF p(x,t|x′) and score function S(x,t|x′) involving Bessel functions.
  - [Section 3.1] "The resulting diffusion model is given in closed analytical form and is only slightly more computationally costly than the standard Gaussian case."
  - [corpus] Corpus evidence on this specific mechanism is weak; no directly comparable Laplace jump implementations found.
- Break condition: Adding Gaussian noise (D>0) or using non-Laplace amplitude distributions generally precludes closed-form solutions, requiring numerical Fourier inversion.

### Mechanism 3
- Claim: Jump-diffusion models can outperform Gaussian models in regimes with high noise intensity and limited sampling steps.
- Mechanism: The heavy-tailed nature of jump processes allows larger state transitions per step, enabling more efficient coverage of the target distribution when step count is constrained. The generalized score function balances these jumps in the reverse process.
- Core assumption: The advantage stems from the jump structure itself rather than hyperparameter tuning specific to non-Gaussian models.
- Evidence anchors:
  - [Section 5, Fig. 1] "For steps=25, JL-ODE achieves maxF1 of 0.66 for the swiss roll and 0.55 for the Gaussian mixture, while maxF1 <0.52 and <0.37, respectively, for the other methods."
  - [Section 5] Performance advantage observed specifically at D, σ² = 1 and small step numbers.
  - [corpus] Related work [27] on α-stable noise also reports faster sampling properties, supporting the broader hypothesis that non-Gaussian noise aids efficiency.
- Break condition: At low noise intensities (D, σ² < 0.1), Gaussian models outperform JL, achieving F1 ≥ 0.8. Performance gains are not universal across all parameter regimes.

## Foundational Learning

- Concept: **Score-based diffusion models (SDE formulation)**
  - Why needed here: The entire framework generalizes Song et al.'s score-based SDE approach. Without understanding how ∇logp(x,t) governs standard diffusion reversal, the meaning of the generalized score function S(x,t) will be opaque.
  - Quick check question: Can you explain why the score function appears in both the probability flow ODE and the time-reversed SDE, and what role it plays in directing samples toward the data distribution?

- Concept: **Lévy processes and Poisson jump processes**
  - Why needed here: The forward noise process combines Gaussian white noise with Poisson-distributed jumps. Understanding finite activity Lévy processes (versus infinite activity α-stable) is essential to grasp why this framework admits simpler mathematics than LIM.
  - Quick check question: What distinguishes a finite activity Lévy process from an infinite activity one, and why does this affect whether the jump amplitude distribution is normalizable?

- Concept: **Fourier transforms for solving Fokker-Planck equations**
  - Why needed here: The generalized score function V(x,t) is defined via inverse Fourier transform of terms involving ψ(k). Implementation requires comfort with characteristic functions and FT-based PDE solutions.
  - Quick check question: Given the characteristic function Ĝ(k,t) of a stochastic process, how would you recover the conditional PDF p(x,t|x′), and what numerical considerations arise in high dimensions?

## Architecture Onboarding

- Component map:
  Training Pipeline -> Forward process sampler (Eq. 23-24): Y(t) = Y(0)e^(-t/2) + J(t) -> Generalized score network s_θ(x,t): MLP predicting scaled score ŝ_θ -> Loss function (Eq. 25): MSE between predicted and analytical ŝ_θ vs. J(t)/||J(t)||·Ĝ(||J(t)||,t)
  Sampling Pipeline -> Initial sample: X₀ ~ L_d(σ²) via Γ·Z decomposition (Eq. 29) -> ODE path (Eq. 26): Exponential integrator with s_θ drift -> SDE path (Eq. 27): Adds backward jump increment J̃(t) with amplified variance σ²e^t

- Critical path:
  1. Implement Laplace sampler using Γ ~ Gamma(1), Z ~ N(0, σ²I) composition
  2. Implement forward jump increment sampler J(t) using simplified algorithm (ω ~ U(0,1) test, Section 3.3b)
  3. Implement analytical score target function Ĝ(x,t) using Bessel function K_{1-d/2}
  4. Train score network with time-weighted MSE loss
  5. Sample via discretized ODE/SDE with exponential integrator

- Design tradeoffs:
  - **ODE vs. SDE sampling**: ODE is deterministic and simpler; SDE adds stochastic jumps that may improve diversity but requires sampling J̃(t) with enlarged variance σ²e^t
  - **Pure jump (D=0) vs. mixed (D>0)**: Pure jump admits closed-form expressions; mixed requires numerical Fourier inversion
  - **Network architecture**: Paper uses simple 4-layer MLP (128 nodes, GELU). Assumption: Optimal architectures for non-Gaussian noise may differ from Gaussian-optimized designs.

- Failure signatures:
  - **Numerical instability at t→0**: Score function diverges; mitigated by scaling with (1-e^{-t}) factor in loss (Eq. 22)
  - **Poor performance at low noise intensity**: If σ² < 0.1, expect Gaussian models to outperform; this is expected behavior, not a bug
  - **High-dimensional scaling**: Variance of score estimator scales linearly with d (Appendix C.3), matching Gaussian case but requiring monitoring

- First 3 experiments:
  1. **2D validation on Swiss roll / Gaussian mixture**: Replicate Figure 1 results with σ² = 1 and 25 sampling steps to verify JL-ODE achieves F1 > 0.6; compare against Gaussian ODE baseline
  2. **Noise intensity sweep**: Test σ² ∈ {0.01, 0.1, 1.0, 10} with fixed 100 steps to identify crossover point where JL outperforms Gaussian (expected near σ² = 1)
  3. **Step ablation**: At σ² = 1, vary sampling steps {10, 25, 50, 100, 500} to quantify JL's advantage in low-step regime and verify convergence at high steps

## Open Questions the Paper Calls Out
- Can jump-diffusion models improve sample quality and sampling efficiency in high-dimensional image synthesis tasks compared to standard Gaussian diffusion models?
- What jump amplitude distributions beyond Laplace yield closed-form conditional scores while providing better performance characteristics?
- What neural network architectures and training protocols are optimal for non-Gaussian diffusion models?
- Does combining Gaussian noise with Poisson jumps (D > 0) provide performance benefits over pure jump or pure Gaussian processes?

## Limitations
- The framework is derived for finite activity Lévy processes with normalizable jump amplitude distributions, excluding infinite activity processes
- Closed-form solutions exist only for the pure jump case (D=0) with Laplace-distributed amplitudes
- Performance advantage is conditional on specific regimes—high noise intensity and limited sampling steps

## Confidence
- **High confidence**: The mathematical derivation of the generalized score function S(x,t) for jump-diffusion processes is rigorous and follows established stochastic calculus
- **Medium confidence**: The empirical performance claims are based on 2D synthetic datasets only, limiting generalization to high-dimensional real-world data
- **Low confidence**: The choice of Laplace distribution for jump amplitudes is motivated by analytical tractability rather than demonstrated superiority

## Next Checks
1. **High-dimensional scalability test**: Implement the JL model on CIFAR-10 or similar high-dimensional image dataset to verify whether the computational overhead remains manageable and whether performance gains persist beyond 2D synthetic data

2. **Distribution-specific ablation**: Compare JL against LIM and Gaussian models across multiple jump amplitude distributions (Laplace, Gaussian, uniform, exponential) to determine whether the Laplace choice is optimal for specific data characteristics or merely convenient

3. **Continuous noise intensity sweep**: Systematically vary σ² from 0.001 to 10.0 with fine-grained steps to precisely map the crossover region where JL transitions from underperforming to outperforming Gaussian models, validating the reported optimal range of 0.01-0.1