---
ver: rpa2
title: 'TyDi QA-WANA: A Benchmark for Information-Seeking Question Answering in Languages
  of West Asia and North Africa'
arxiv_id: '2507.17709'
source_url: https://arxiv.org/abs/2507.17709
tags:
- answer
- language
- question
- arabic
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces TyDi QA-WANA, a benchmark dataset for information-seeking
  question answering in 10 under-represented languages of West Asia and North Africa.
  The dataset contains 28,000 examples where questions are written directly in each
  language variety by native speakers genuinely curious about the answers, paired
  with entire Wikipedia articles that may or may not contain the answer.
---

# TyDi QA-WANA: A Benchmark for Information-Seeking Question Answering in Languages of West Asia and North Africa

## Quick Facts
- arXiv ID: 2507.17709
- Source URL: https://arxiv.org/abs/2507.17709
- Reference count: 7
- Introduces TyDi QA-WANA, a benchmark dataset for information-seeking QA in 10 under-represented languages of West Asia and North Africa.

## Executive Summary
This paper introduces TyDi QA-WANA, a benchmark dataset for information-seeking question answering in 10 under-represented languages of West Asia and North Africa. The dataset contains 28,000 examples where questions are written directly in each language variety by native speakers genuinely curious about the answers, paired with entire Wikipedia articles that may or may not contain the answer. This design tests models' ability to process long contexts and avoids cultural relevance issues from translation. Two baseline systems using Gemini 1.5 Pro and Gemini 2.0 Flash are evaluated, achieving varied performance across languages with exact match scores ranging from 34.6% to 74.9% and F1 scores from 38.7% to 76.2%. The dataset includes a specialized evaluation procedure with NULL consensus to handle unanswerable questions and provides comprehensive statistics on performance across language varieties. All code and data are publicly released to facilitate further research in low-resource multilingual question answering.

## Method Summary
The task is Minimal Answer Span Task (MinSpan)—given a full Wikipedia article and question, output either: (1) start/end byte indices of minimal answer span, (2) YES/NO for boolean questions, or (3) NULL if unanswerable. The dataset contains 28,197 examples across 10 language varieties (Algerian/Egyptian/Iraqi/Jordanian Arabic, Armenian, Azerbaijani, Hebrew, Farsi, Tajik, Turkish). Train: 16,200 (1 annotation), Dev: 5,995 (3 annotations), Test: 6,002 (3 annotations). Arabic varieties use dialect questions with Modern Standard Arabic articles. Primary metric is average per-example F1; also report Exact Match (EM). Uses NULL consensus (majority vote on answerability). Break down scores by NULL vs non-NULL consensus. Zero-shot inference with Gemini 1.5 Pro and 2.0 Flash. Uses preamble + 1 exemplar per question type + input question. Greedy sampling, max generation 1024 tokens. For span answers, model outputs text, first occurrence in article maps to byte range. No-answer critic (second inference pass) detects misformatted NULL responses.

## Key Results
- Baseline systems using Gemini 1.5 Pro and 2.0 Flash achieve varied performance across 10 languages with exact match scores ranging from 34.6% to 74.9% and F1 scores from 38.7% to 76.2%
- NULL consensus rates vary significantly across languages (47-83%), highlighting the challenging nature of determining answerability in long contexts
- Arabic varieties show substantial performance differences despite sharing the same MSA articles, suggesting dialectal variation impacts comprehension

## Why This Works (Mechanism)
The dataset design addresses key limitations in cross-lingual QA by using native-speaker-written questions that reflect genuine information needs, full Wikipedia articles that test long-context processing, and a NULL consensus evaluation that properly handles unanswerable questions. The inclusion of multiple Arabic dialects with MSA articles creates a controlled setting to study dialect-article compatibility.

## Foundational Learning
- **Information-seeking QA**: Why needed - to move beyond extractive QA to questions users genuinely want answered. Quick check - questions come from native speakers with authentic curiosity.
- **NULL consensus evaluation**: Why needed - to handle unanswerable questions without penalizing models for correctly identifying them. Quick check - majority vote of 3 annotations determines if question is answerable.
- **Long-context processing**: Why needed - real-world articles are lengthy and models must identify relevant passages. Quick check - full Wikipedia articles (up to 1024 tokens) are provided, not extracted passages.
- **Dialect-article compatibility**: Why needed - to study how well MSA articles serve dialectal questions. Quick check - Arabic varieties use dialect questions with MSA articles.
- **Cross-lingual transfer**: Why needed - to enable models to leverage knowledge across related languages. Quick check - language selection includes related varieties (Farsi/Tajik, Arabic dialects).

## Architecture Onboarding

**Component Map**: Dataset (questions + articles) -> LLM inference (preamble + exemplars + question) -> Answer extraction (text to byte range) -> NULL critic -> Evaluation (F1/EM with NULL consensus)

**Critical Path**: Question + Article context → LLM inference → Answer extraction → Evaluation metric calculation

**Design Tradeoffs**: Native questions vs. translation (authenticity vs. comparability); full articles vs. extracted passages (realism vs. tractability); zero-shot vs. fine-tuned baselines (accessibility vs. performance).

**Failure Signatures**: Misformatted NULL outputs; answer text appearing multiple times in article (first occurrence heuristic fails); high NULL consensus rates skewing overall scores.

**First Experiments**:
1. Implement NULL consensus logic and verify it correctly handles majority vote scenarios
2. Run single-language inference with Gemini 1.5 Pro using provided preamble and one exemplar
3. Test answer-to-byte-range mapping on articles with repeated answer text

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does supervised fine-tuning significantly improve performance on TyDi QA-WANA compared to the provided zero-shot baselines?
- Basis in paper: [explicit] Section 9 states, "these baselines were not finetuned for this task," implying the potential for improvement via training remains unquantified.
- Why unresolved: It is currently unclear if the performance gap between languages (e.g., Farsi vs. Armenian) is due to lack of task-specific adaptation or inherent pretraining deficits.
- What evidence would resolve it: Fine-tuning open-weight models on the 16,200 training examples and measuring the resulting performance delta.

### Open Question 2
- Question: To what extent does cross-lingual transfer learning improve performance among the related language varieties in the dataset?
- Basis in paper: [explicit] Section 5 notes that the selection includes related languages specifically to "facilitate transfer learning," but the paper provides no experimental results validating this transfer.
- Why unresolved: It is unknown if the linguistic relatedness (e.g., between Farsi and Tajik) is sufficient to overcome data scarcity in the low-resource varieties.
- What evidence would resolve it: Ablation studies showing performance changes when training on high-resource relatives (e.g., Turkish) and evaluating on low-resource counterparts (e.g., Azerbaijani).

### Open Question 3
- Question: Does the "first occurrence" heuristic used for answer mapping introduce systematic noise into the evaluation metrics?
- Basis in paper: [inferred] Section 8 admits the evaluation relies on a heuristic that selects the "first occurrence" of answer text because LLMs cannot natively output byte ranges.
- Why unresolved: If a model extracts a correct answer that happens to appear a second time in the text, the heuristic might match the first instance and incorrectly assign a zero score.
- What evidence would resolve it: A manual error analysis of false negatives to identify cases where the model predicted a valid answer span that was not the first occurrence.

## Limitations
- Baseline performance varies significantly across languages (EM 34.6%–74.9%), indicating the challenging nature of the task and potential data quality issues
- The "first occurrence" heuristic for answer mapping may introduce systematic evaluation errors when answer text appears multiple times
- Missing details about exemplar articles and preprocessing steps make exact baseline reproduction difficult

## Confidence
- **High**: Dataset novelty and value for cross-lingual QA research
- **Medium**: Fidelity of baseline reproduction given missing exemplar and preprocessing details
- **Medium**: Robustness of NULL consensus metric given sensitivity to annotation noise in low-resource settings

## Next Checks
1. Implement and run the exact baseline inference pipeline using the specified preamble and in-context exemplars; compare results with the reported scores
2. Validate the NULL consensus procedure by simulating annotation noise and measuring its effect on per-language performance
3. Conduct a linguistic analysis of answer spans across language varieties to confirm that Arabic MSA articles adequately cover dialectal question semantics