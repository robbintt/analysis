---
ver: rpa2
title: 'Global Optimality of Single-Timescale Actor-Critic under Continuous State-Action
  Space: A Study on Linear Quadratic Regulator'
arxiv_id: '2505.01041'
source_url: https://arxiv.org/abs/2505.01041
tags:
- have
- where
- policy
- proof
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the global optimality of the classic single-sample
  single-timescale actor-critic (AC) algorithm for solving the linear quadratic regulator
  (LQR) problem in continuous state-action space. Unlike prior works that focus on
  double-loop or two-timescale variants with local convergence guarantees in finite
  settings, this work analyzes the more practical single-sample single-timescale AC
  under continuous (infinite) state-action space.
---

# Global Optimality of Single-Timescale Actor-Critic under Continuous State-Action Space: A Study on Linear Quadratic Regulator

## Quick Facts
- arXiv ID: 2505.01041
- Source URL: https://arxiv.org/abs/2505.01041
- Reference count: 40
- Primary result: Establishes global optimality and O(ε⁻²) sample complexity for single-sample single-timescale actor-critic in continuous LQR

## Executive Summary
This paper proves that the single-sample single-timescale actor-critic algorithm achieves global optimality for solving linear quadratic regulator problems in continuous state-action spaces. Unlike previous works that required double-loop or two-timescale variants with only local convergence guarantees, this analysis establishes that the more practical single-timescale version converges globally with sample complexity O(ε⁻²). The key insight is treating the interconnected estimation errors of cost, critic, and actor as a coupled system rather than decoupling them conservatively.

## Method Summary
The method employs three parallel updates: a cost estimator, a critic, and an actor, all using proportional stepsizes. The critic uses TD(0) learning with quadratic feature mapping to estimate the Q-function, while the actor performs natural gradient descent based on the critic's parameters. The analysis bounds the interconnected system of estimation errors without decoupling, showing simultaneous O(T⁻¹/²) convergence when the stepsize ratio remains below a problem-dependent threshold. This approach bridges the gap between theory and practice for actor-critic methods in continuous control.

## Key Results
- First global optimality guarantee for single-sample single-timescale actor-critic in continuous state-action space
- Achieves ε-optimal solution with O(ε⁻²) sample complexity
- Improves upon prior AC-based LQR solvers in both sample efficiency and theoretical scope
- Experiments show faster convergence compared to zeroth-order and double-loop AC methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Under proportional stepsizes with ratio c below a threshold, the interconnected estimation errors in cost, critic, and actor all converge simultaneously at O(T⁻¹/²), provided policies remain stabilizing.
- **Mechanism:** Treats three update streams (cost estimator η, critic ω, actor K) as a coupled system. Bounds cross-dependencies—how critic error affects gradient estimation, which affects actor update, which affects critic's target—showing all errors contract together when stepsize ratio is small enough. Key inequalities form a system where average errors depend on each other, and solving this system yields simultaneous O(T⁻¹/²) convergence.
- **Core assumption:** Actor stepsize ratio c = α_t/β_t must satisfy h₂h²₄ + h₂h²₄h²₇ + 2h₅h²₇ < 1 (Eq. 51), which reduces to c being smaller than a problem-dependent threshold.
- **Evidence anchors:** Abstract states single-timescale can attain ε-optimal solution with O(ε⁻²) complexity; section 4.1 says they bound interconnected iteration system without resorting to conservative decoupled analysis.
- **Break condition:** If stepsize ratio exceeds threshold, or policy K exits stabilizing set (ρ(A-BK) ≥ 1), cross-terms can amplify rather than dampen errors, and coupled system may diverge.

### Mechanism 2
- **Claim:** The critic's parameter ω directly encodes the Q-function matrix Ω_K through the feature mapping ϕ(x,u) = svec([x;u][x;u]ᵀ), enabling natural gradient computation without model knowledge.
- **Mechanism:** For LQR, Q-function has quadratic form Q_K(x,u) = [x;u]ᵀΩ_K[x;u] - constants. Parameterizing critic as Q̂(x,u;ω) = ϕ(x,u)ᵀω where ϕ extracts quadratic features, optimal ω* equals svec(Ω_K). Natural gradient ∇ᴺK J(K) = Ω₂₂ K - Ω₂₁ becomes smat(ω)₂₂ K - smat(ω)₂₁—extracted directly from critic parameters. Bypasses explicit estimation of system matrices A, B or value function P_K.
- **Core assumption:** Policy class K is norm-bounded and stabilizing (Assumption 1); ensures stationary distribution exists and matrix A_K is invertible with σ_min(A_K) ≥ μ > 0.
- **Evidence anchors:** Section 3 shows ω*_K = svec(Ω_K) and ∇ᴺ_K J(K) = smat(ω*_K)₂₂ K - smat(ω*_K)₂₁; section 2.2 shows Q-function structure and feature mapping.
- **Break condition:** If feature representation is misspecified (non-quadratic true Q-function), or if numerical instability in svec/smat transformation occurs for ill-conditioned ω, gradient estimates become biased.

### Mechanism 3
- **Claim:** The gradient domination property of LQR (J(K) - J(K*) ≤ C·‖E_K‖²) ensures that driving natural gradient norm to zero guarantees global optimality, not just stationarity.
- **Mechanism:** Unlike general non-convex RL problems where gradient descent may find local optima, LQR has special structure: performance difference from optimality is bounded by squared norm of specific error term E_K = (R + BᵀP_K B)K - BᵀP_K A. When natural gradient (which equals E_K) approaches zero, policy approaches K* globally. Proved via almost-smoothness property (Lemma 12) and PL-like inequality in Lemma 13.
- **Core assumption:** Optimal policy K* exists uniquely and is stabilizing; (A,B) pair is stabilizable and (A,Q^½) is observable.
- **Evidence anchors:** Section 4.1 says they utilize gradient domination condition of LQR (Lemma 13); section A.3 establishes almost-smoothness and gradient domination.
- **Break condition:** This mechanism is specific to LQR; for general MDPs without gradient domination, single-timescale AC may only reach local optima or stationary points.

## Foundational Learning

- **Concept: Temporal Difference (TD) Learning**
  - Why needed here: Critic update uses TD(0) with bootstrap target. Understanding how δ_t = c_t - η_t + ϕ(x'_t,u'_t)ᵀω_t - ϕ(x_t,u_t)ᵀω_t forms unbiased estimate of critic's Bellman error is essential for following convergence proof.
  - Quick check question: Can you derive why E[δ_t · ϕ(x_t,u_t)] = b_K - A_K ω under stationary distribution?

- **Concept: Linear Quadratic Regulator (LQR) Basics**
  - Why needed here: Entire analysis exploits LQR-specific structure: quadratic costs, linear dynamics, and resulting closed-form expressions for J(K), ∇J(K), and P_K via Riccati equations. Without this background, lemmas (1-4, 12-13) appear unmotivated.
  - Quick check question: For a stabilizing K, can you explain why P_K satisfies the Lyapunov equation P_K = Q + KᵀRK + (A-BK)ᵀP_K(A-BK)?

- **Concept: Natural Policy Gradient**
  - Why needed here: Actor uses natural gradient (Eq. 13), which divides vanilla gradient by covariance D_K. For LQR, this simplifies to E_K = (R + BᵀP_K B)K - BᵀP_K A—no D_K estimation needed. This is why critic can directly provide update direction.
  - Quick check question: Why does natural gradient eliminate need to estimate stationary distribution covariance D_K?

## Architecture Onboarding

- **Component map:**
  ```
  Environment (LQR): x_{t+1} = Ax_t + Bu_t + ε_t, c_t = x_t'Q x_t + u_t' R u
              ↓ samples (x_t, u_t, c_t, x'_t, u'_t)
  ┌──────────────┬────────────────┬──────────────────────┐
  │ Cost Est. η  │   Critic ω     │     Actor K          │
  │ η ← η+γ(c-η) │ ω ← ω+β·δ·ϕ   │ K ← K-α·∇̂ᴺJ(K)     │
  │              │  δ=TD error    │  = K-α(smat(ω)₂₂K   │
  │              │                │     - smat(ω)₂₁)    │
  └──────────────┴────────────────┴──────────────────────┘
              │                │               │
              └────────────────┴───────────────┘
                      Proportional stepsizes: α_t/β_t = c
  ```

- **Critical path:**
  1. Initialize K_0 with stabilizing gain (ρ(A-BK_0) < 1 is essential; check via Lyapunov or random small norm)
  2. Choose stepsize ratio c below threshold (Eq. 51; if unknown, start with c = 0.001-0.01)
  3. Set T based on desired ε (need T ≥ O(ε⁻²) samples)
  4. Run single-sample loop: one transition per update
  5. Monitor ‖E_K‖² (estimate from ω) for convergence

- **Design tradeoffs:**
  - **Smaller stepsize ratio c:** More stable convergence but slower; may require more samples than theoretical minimum
  - **Larger c:** Faster updates but risk instability if threshold exceeded; threshold depends on problem constants (μ, λ, system dimensions)
  - **Projection radius ω̄:** Larger radius allows more expressiveness but Lemma 10's invertibility bound may weaken; smaller radius guarantees boundedness but may clip optimal critic parameters
  - **Exploration noise σ:** Higher σ improves coverage but increases variance in cost estimates and may slow convergence; too low risks numerical issues in A_K invertibility

- **Failure signatures:**
  - **Diverging ‖K‖:** Policy exited stabilizing set; reduce stepsize c or add stronger projection
  - **Critc oscillating:** β_t too large relative to noise; increase projection radius bounds or decrease β
  - **No progress in J(K):** c below threshold not satisfied, or initialization far from stabilizing; check ρ(A-BK_0)
  - **Numerical overflow in smat(ω):** ω ill-conditioned; check projection and stepsize

- **First 3 experiments:**
  1. **2D toy LQR** with known optimal K* (compute via ARE): Use paper's Example 1 parameters (A=[[0,1],[1,0]], B=[[0,1],[1,0]], Q,R as given). Run with c=0.005, β=0.01/√T, γ=0.1/√T, T=10⁵. Plot ‖K_t - K*‖_F and J(K_t)-J(K*) vs iterations. Expected: both decay as O(t⁻¹/²).
  2. **Stepsize ratio ablation:** On same 2D system, vary c ∈ {0.001, 0.005, 0.01, 0.05, 0.1}. Measure final J(K)-J(K*) at T=10⁵. Expect sharp transition around theoretical threshold—values below converge, above diverge or plateau.
  3. **Comparison with double-loop AC:** Implement baseline from [Yang et al. 2019] (Algorithm 3 in appendix) with inner loop T_inner=5000 vs proposed single-timescale. Measure wall-clock time and sample complexity to reach ε=0.01 error. Expected: single-timescale faster in wall-clock due to no inner loop, sample complexity similar.

## Open Questions the Paper Calls Out

- **Question:** Can a specific projection map be formulated to rigorously enforce the boundedness of the policy parameter K without compromising the finite-time convergence guarantees of single-timescale actor-critic?
  - **Basis in paper:** [explicit] The authors explicitly state that addressing the uniform boundedness assumption via a projection map "is deferred to future research endeavors."
  - **Why unresolved:** The current analysis relies on an assumption that the policy matrix K is norm-bounded, rather than proving the algorithm inherently stays within a bounded region or designing a specific projection mechanism.
  - **What evidence would resolve it:** A finite-time analysis that explicitly includes a projection step in the actor update and proves it maintains the O(ε⁻²) sample complexity.

- **Question:** Can the global optimality guarantees established for the Linear Quadratic Regulator (LQR) be extended to general non-linear continuous control tasks or general MDPs with infinite state-action spaces?
  - **Basis in paper:** [explicit] The paper describes the LQR analysis as a "case study" and suggests this work serves as a "first step towards understanding the limits of AC methods on continuous control tasks."
  - **Why unresolved:** The proof relies heavily on specific properties of LQR, such as the closed-form solution of the Riccati equation and the linear structure of the policy, which do not exist in general non-linear settings.
  - **What evidence would resolve it:** A theoretical proof demonstrating global convergence or characterizing the optimality gap for single-timescale AC in a non-linear control setting.

- **Question:** Does the global optimality guarantee hold for a strictly online implementation where the algorithm must learn from a single trajectory without resetting to samples from the stationary distribution?
  - **Basis in paper:** [inferred] The analysis relies on the assumption that samples are drawn from the stationary distribution (Line 3 of Algorithm 1), which the authors admit is done to "simplify the theoretical analysis" rather than reflecting a true online constraint.
  - **Why unresolved:** Theoretical bounds in this context often rely on mixing time assumptions or "restart" mechanisms; proving convergence for a single, continuously evolving trajectory in the single-timescale setting remains theoretically challenging.
  - **What evidence would resolve it:** A finite-time analysis that bounds the estimation error and convergence rate using only the Markovian noise inherent in a single trajectory without the stationarity assumption.

## Limitations

- The threshold for the stepsize ratio c depends on problem-specific constants that are hard to compute without knowing the optimal P_K, requiring heuristic tuning in practice.
- The analysis assumes exact quadratic features and bounded policy norms; real-world deviations may break the guarantees.
- The projection radii for ω and η are unspecified in experiments, making exact reproduction difficult.

## Confidence

- **High confidence:** Sample complexity O(ε⁻²) for single-timescale AC in continuous LQR (backed by Theorem 1 and structural lemmas).
- **Medium confidence:** Simultaneous convergence of all three components at O(T⁻¹/²) (the coupled error analysis is novel but relies on bounding chains of inequalities).
- **Low confidence:** Practical performance claims vs double-loop methods (only shown on two small LQR examples without ablation on hyperparameters like projection radii).

## Next Checks

1. **Stepsize ratio sweep:** On 2D LQR, test c from 0.001 to 0.1; verify convergence only below a critical threshold and measure final J(K)-J(K*) to identify transition point.
2. **Critic projection ablation:** Run same 2D system with and without projection; check if unbounded ω leads to divergence and quantify performance loss from projection clipping.
3. **Wall-clock vs double-loop:** Implement double-loop AC from [Yang et al. 2019] and compare time-to-ε on 4D LQR; measure both iterations and CPU time to isolate overhead from inner loop.