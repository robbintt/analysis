---
ver: rpa2
title: Neural Index Policies for Restless Multi-Action Bandits with Heterogeneous
  Budgets
arxiv_id: '2510.22069'
source_url: https://arxiv.org/abs/2510.22069
tags:
- policy
- index
- neural
- action
- budget
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of solving multi-action restless
  multi-armed bandits (RMABs) with heterogeneous budget constraints, which are common
  in real-world healthcare settings where multiple interventions have different costs
  and resource limits. The proposed method, Neural Index Policy (NIP), uses a neural
  network to predict budget-aware indices for arm-action pairs and employs a differentiable
  knapsack layer, formulated as an entropy-regularized optimal transport problem,
  to enforce heterogeneous budget constraints while maintaining end-to-end differentiability.
---

# Neural Index Policies for Restless Multi-Action Bandits with Heterogeneous Budgets

## Quick Facts
- **arXiv ID:** 2510.22069
- **Source URL:** https://arxiv.org/abs/2510.22069
- **Reference count:** 36
- **Primary result:** NIP achieves near-optimal performance within 5% of oracle occupancy-measure policy while strictly enforcing heterogeneous budgets and scaling to hundreds of arms.

## Executive Summary
This paper addresses the challenge of solving multi-action restless multi-armed bandits (RMABs) with heterogeneous budget constraints, which are common in real-world healthcare settings where multiple interventions have different costs and resource limits. The proposed method, Neural Index Policy (NIP), uses a neural network to predict budget-aware indices for arm-action pairs and employs a differentiable knapsack layer, formulated as an entropy-regularized optimal transport problem, to enforce heterogeneous budget constraints while maintaining end-to-end differentiability. The model is trained to align its induced occupancy measure with the theoretical upper bound from a linear programming relaxation, enabling gradient-based optimization of decision quality.

## Method Summary
The method generates synthetic RMAB environments with N arms, S states, and A actions. A neural network predicts index scores for each arm-action pair, which are then converted into feasible allocations via a differentiable knapsack layer formulated as an entropy-regularized optimal transport problem. The network is trained end-to-end to minimize KL divergence between the predicted transport plan and the oracle occupancy measure computed from a linear programming relaxation. The approach scales to hundreds of arms and achieves near-optimal performance within 5% of the oracle policy.

## Key Results
- NIP achieves near-optimal performance within 5% of oracle occupancy-measure policy
- The method strictly enforces heterogeneous budgets while maintaining end-to-end differentiability
- NIP scales to hundreds of arms (tested up to N=1000) while maintaining performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The system enables gradient-based optimization of discrete resource allocation by relaxing combinatorial constraints into a continuous space.
- **Mechanism:** The architecture replaces the non-differentiable Knapsack assignment with an entropy-regularized Optimal Transport (OT) problem (Sinkhorn algorithm). This converts hard binary constraints (assign action $a$ to arm $n$) into a soft transport plan $\Gamma$, allowing gradients to backpropagate from the final decision quality loss back to the neural network weights.
- **Core assumption:** The entropy-regularized solution provides a sufficiently accurate approximation of the original discrete Knapsack solution such that minimizing the loss on the relaxed problem translates to gains in the actual constrained setting.
- **Evidence anchors:**
  - [Abstract] "...converts them into feasible allocations via a differentiable knapsack layer formulated as an entropy-regularized optimal transport (OT) problem."
  - [Section 4.2] "...we relax the problem to an optimal transport formulation, enabling efficient gradient-based optimization."
  - [Corpus] Corpus neighbors (e.g., "Lagrangian Relaxation...") focus heavily on relaxation techniques; however, specific evidence for the Sinkhorn-Knapsack link is primarily anchored in the paper text rather than the provided neighbor abstracts.
- **Break condition:** If the regularization parameter $\epsilon$ is set too high, the transport plan becomes "too smooth" (losing discrete action fidelity); if too low, gradients vanish or become unstable.

### Mechanism 2
- **Claim:** The policy achieves near-optimality by anchoring learning to a theoretical upper bound rather than sparse reward signals alone.
- **Mechanism:** The model is trained to minimize the KL-divergence between its predicted occupancy measure (transport plan) and the solution of a Linear Programming (LP) relaxation of the RMAB. This "occupancy matching" allows the model to mimic the steady-state behavior of an optimal oracle without requiring expensive exploration of the full state space.
- **Core assumption:** The LP relaxation provides a tight bound on the optimal achievable reward, and matching the steady-state occupancy implies optimal per-step decision-making under the stationary policy class.
- **Evidence anchors:**
  - [Abstract] "The network is optimized to align its induced occupancy measure with the theoretical upper bound from a linear programming relaxation..."
  - [Section 4.3] "...model is trained to minimize the Kullback–Leibler (KL) divergence between the predicted transport plan $\Gamma$ and $\omega^*$..."
  - [Corpus] Theoretical foundations in neighbors (e.g., "Finite-Horizon Single-Pull") frequently cite LP relaxations as the standard for defining optimality bounds.

### Mechanism 3
- **Claim:** The indices learned are implicitly budget-aware, adjusting priorities based on resource scarcity.
- **Mechanism:** Instead of learning an absolute value index, the network learns indices through the differentiable constraint layer. This forces the network to depress the learned index values for actions that are highly effective but critically resource-constrained, distributing the budget efficiently across the cohort.
- **Core assumption:** The end-to-end training loop successfully propagates the "cost" of budget consumption back to the index prediction layer, effectively internalizing the constraint shadow price.
- **Evidence anchors:**
  - [Section 4.4] "The neural network implicitly captures how heterogeneous budget constraints affect the marginal value of each action... This makes the learned index budget-aware."
  - [Section 1] "...learns budget-aware indices to arm-action pairs..."
- **Break condition:** If the "Reward-based Loss" (model-free) is used exclusively without the "Occupancy-based Loss" (and the LP oracle is ignored), the model may struggle to infer these complex budget trade-offs without extensive simulation data.

## Foundational Learning

- **Concept:** **Restless Multi-Armed Bandits (RMABs)**
  - **Why needed here:** This is the problem substrate. You must understand that arms (e.g., patients) evolve stochastically regardless of whether you intervene, distinguishing this from standard contextual bandits.
  - **Quick check question:** If I pull arm A, does arm B's state stay the same? (In standard bandits, yes; in RMAB, it might change anyway—this is the "restless" property).

- **Concept:** **Whittle Index**
  - **Why needed here:** This paper generalizes the Whittle index. You need to know that an "index" typically represents the subsidy required to make an intervention passive, serving as a priority score for activation.
  - **Quick check question:** Does a higher Whittle index imply a higher or lower priority for receiving an intervention in a standard budget-constrained setting?

- **Concept:** **Optimal Transport (Sinkhorn Algorithm)**
  - **Why needed here:** This is the "Differentiable Knapsack" engine. You need to grasp that we are moving unit masses (arms) to bins (action budgets) with minimal cost, regularized by entropy to ensure differentiability.
  - **Quick check question:** Why does adding an entropy term ($\epsilon \Gamma \log \Gamma$) make the discrete optimization problem differentiable? (Answer: It smooths the solution space, preventing hard 0/1 jumps).

## Architecture Onboarding

- **Component map:** Input -> Neural Network -> Sinkhorn-Knapsack Solver -> Transport Plan -> Loss Module
- **Critical path:** The **Sinkhorn iteration loop** inside the constraint layer. This is where the discrete feasibility (budgets $b_a$) is enforced. If this loop diverges or $\epsilon$ is wrong, the gradients passed back to $f_\theta$ will be noise.
- **Design tradeoffs:**
  - **Epsilon ($\epsilon$) Tuning:** High $\epsilon$ = stable gradients but "soft" allocations (probabilities sum to budget, but no clear decision); Low $\epsilon$ = near-optimal discrete decisions but unstable/vanishing gradients. The paper suggests $\epsilon \in [0.05, 0.1]$ as a sweet spot.
  - **Training Mode:** Occupancy-based training requires solving an LP (requires known transition dynamics) but is sample-efficient. Reward-based training is model-free but requires simulation rollouts.
- **Failure signatures:**
  - **Constraint Leakage:** If $\epsilon$ is too high, the sum of assigned actions might strictly violate the budget $b_a$ during the actual discrete assignment step.
  - **Index Collapse:** The network learns to output uniform indices, causing the Sinkhorn layer to default to random allocation (check gradient flow if loss plateaus early).
- **First 3 experiments:**
  1.  **Sanity Check (Small N):** Run with $N=10$, known transitions. Compare against optimal DP (if feasible) or the LP Oracle to verify the "within 5%" claim.
  2.  **Ablation on $\epsilon$:** Sweep $\epsilon \in \{0.5, 0.1, 0.01\}$ and plot "Reward Gap" vs. "Convergence Speed" to validate the stability claim in Section 6.
  3.  **Generalization Test:** Train on a cohort of size $N=100$, test on $N=200$ with unseen arm parameters to verify if the policy generalizes using only local state features (as claimed in Section 4.4).

## Open Questions the Paper Calls Out

- **How can NIP be extended to non-stationary environments where reward distributions or transition dynamics evolve over time?**
  - **Basis in paper:** [explicit] "Our current approach is designed for stationary settings where the reward distribution remains constant over time. In scenarios where the reward dynamics change, such as non-stationary environments, our method would require an online learning adaptation to maintain performance."
  - **Why unresolved:** The current framework assumes fixed transition probabilities and reward structures, which limits applicability to dynamic real-world settings where patient behaviors or intervention effects may shift.
  - **What evidence would resolve it:** A modified NIP framework with online learning capabilities tested on environments with time-varying dynamics, showing maintained performance under distribution shift.

- **How does NIP perform on real-world healthcare datasets compared to synthetic simulations?**
  - **Basis in paper:** [explicit] "Future work should focus on validating the model's effectiveness on real-world datasets to assess its practical applicability."
  - **Why unresolved:** All empirical evaluations used simulated RMAB environments, which may not capture complexities like partial observability, irregular decision epochs, or data quality issues present in clinical settings.
  - **What evidence would resolve it:** Deployment and evaluation of NIP on actual healthcare datasets (e.g., maternal health programs) with comparison to synthetic benchmarks.

- **What is the performance gap when transition probabilities are unknown and the LP oracle cannot be computed exactly?**
  - **Basis in paper:** [inferred] The occupancy-based loss requires computing $\omega^*$ via LP relaxation with known transition probabilities; the paper does not quantify degradation when using reward-based loss alone.
  - **Why unresolved:** In practice, transition dynamics are often unknown, and the paper does not compare occupancy-based vs. reward-based training regimes comprehensively.
  - **What evidence would resolve it:** Ablation experiments comparing NIP trained with and without oracle access across varying estimation errors in transition probabilities.

## Limitations
- The neural architecture specifications (depth, width, activation functions) are not provided, requiring assumptions that may impact performance.
- Training hyperparameters (learning rate, batch size, epochs, optimizer) are unspecified, potentially affecting convergence and final results.
- The exact procedure for generating synthetic RMAB transition matrices and rewards is not described, making direct replication difficult.

## Confidence
- **High confidence** in the mechanism of using entropy-regularized OT as a differentiable knapsack layer (well-established in literature).
- **Medium confidence** in the occupancy-based training approach (depends on accurate LP oracle and transition dynamics).
- **Low confidence** in the exact neural architecture design and its ability to generalize without domain-specific feature engineering.

## Next Checks
1. **Sanity Check (Small N):** Run with N=10, known transitions. Compare against optimal DP (if feasible) or the LP Oracle to verify the "within 5%" claim.
2. **Ablation on $\epsilon$:** Sweep $\epsilon \in \{0.5, 0.1, 0.01\}$ and plot "Reward Gap" vs. "Convergence Speed" to validate the stability claim in Section 6.
3. **Generalization Test:** Train on a cohort of size N=100, test on N=200 with unseen arm parameters to verify if the policy generalizes using only local state features (as claimed in Section 4.4).