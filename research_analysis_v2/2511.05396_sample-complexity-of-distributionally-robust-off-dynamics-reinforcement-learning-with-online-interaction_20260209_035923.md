---
ver: rpa2
title: Sample Complexity of Distributionally Robust Off-Dynamics Reinforcement Learning
  with Online Interaction
arxiv_id: '2511.05396'
source_url: https://arxiv.org/abs/2511.05396
tags:
- theorem
- learning
- robust
- online
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of learning robust policies in Reinforcement
  Learning (RL) when the deployment environment differs from the training environment.
  The authors propose a framework called Constrained Robust MDPs (CRMDPs) and Regularized
  Robust MDPs (RRMDPs) to handle this setting.
---

# Sample Complexity of Distributionally Robust Off-Dynamics Reinforcement Learning with Online Interaction

## Quick Facts
- arXiv ID: 2511.05396
- Source URL: https://arxiv.org/abs/2511.05396
- Reference count: 40
- Primary result: Proposes ORBIT algorithm achieving sublinear regret in robust RL with dependence on supremal visitation ratio (Cvr)

## Executive Summary
This paper addresses robust reinforcement learning when deployment environments differ from training environments. The authors introduce a framework of Constrained Robust MDPs (CRMDPs) and Regularized Robust MDPs (RRMDPs) to handle this setting. The key theoretical contribution is identifying the "supremal visitation ratio" (Cvr) as the fundamental quantity governing sample complexity - when Cvr is unbounded, online learning becomes exponentially hard. They propose the Online Robust Bellman Iteration (ORBIT) algorithm that achieves sublinear regret with bounds explicitly depending on Cvr, and prove matching lower bounds showing their algorithm is optimal.

## Method Summary
The paper studies tabular finite-horizon RMDPs with f-divergence uncertainty sets. The Online Robust Bellman Iteration (ORBIT) algorithm operates in an episodic setting: at each episode, it performs backward value iteration using robust Bellman operators, executes the greedy policy, and updates empirical transition estimates. The robust updates are implemented via dual formulations - for TV divergence using a closed-form solution, and for KL/χ² divergences through parameterized optimization. The algorithm maintains optimistic Q-value estimates by adding bonus terms that depend on the uncertainty level and Cvr. Theoretical analysis provides regret bounds that scale as O(√(H⁴S²A K Cvr)) for CRMDP-TV and O(√(H⁴S²A K Cvr log K)) for RRMDP-KL, where K is the number of episodes.

## Key Results
- Introduces Cvr as the fundamental quantity governing sample complexity in off-dynamics RL
- Proves that unbounded Cvr implies exponential sample complexity (Theorem 5.14)
- Proposes ORBIT algorithm achieving sublinear regret for both CRMDPs and RRMDPs
- Establishes matching lower bounds showing optimal dependence on Cvr and K
- Demonstrates computational efficiency through dual formulations, especially for TV divergence

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Sample efficiency is fundamentally characterized by the mismatch between nominal and worst-case state visitations, quantified by Cvr = sup_{π,h,s} q^π_h(s)/d^π_h(s).
- **Mechanism:** Cvr captures the "information deficit" - risk that states rarely visited in training (small d^π) become critical in perturbed deployment (large q^π). If unbounded, exploration becomes exponentially difficult.
- **Core assumption:** Cvr is polynomial in horizon and state space dimensions (Assumption 5.5).
- **Evidence anchors:** Abstract introduces Cvr as difficulty measure; Section 1 discusses information deficit; corpus neighbors address similar off-dynamics challenges.
- **Break condition:** If Cvr is unbounded or exponential, no algorithm can achieve polynomial sample complexity (Theorem 5.14).

### Mechanism 2
- **Claim:** Optimistic estimation with robust Bellman updates enables efficient exploration under dynamic uncertainty.
- **Mechanism:** ORBIT updates Q-functions backward from horizon using empirical estimates plus bonus terms for optimism, while solving robust inner minimization over uncertainty set.
- **Core assumption:** Access to tabular finite horizon MDP with (s,a)-rectangular uncertainty set.
- **Evidence anchors:** Abstract proposes ORBIT for sublinear regret; Section 4.1 describes two-stage process; corpus validates general approach of efficient online robust learning.
- **Break condition:** If bonus term is misspecified (fails to account for variance in dual formulation), optimism may fail leading to suboptimal convergence.

### Mechanism 3
- **Claim:** Converting constrained inner optimization to dual formulation makes robust update computationally tractable.
- **Mechanism:** Instead of solving constrained minimization directly, derives dual forms for TV, KL, and χ² divergences. For example, KL constraint becomes log-sum-exp operation in dual space.
- **Core assumption:** Uncertainty set or regularization defined via f-divergence.
- **Evidence anchors:** Section 4.2 lists dual formulation estimators; Appendix C provides proofs for dual derivations; corpus shows standard approach in robust control literature.
- **Break condition:** Non-convex divergence measures might prevent closed-form dual, breaking computational efficiency.

## Foundational Learning

- **Concept:** Robust Markov Decision Processes (RMDPs)
  - **Why needed here:** Standard MDPs assume fixed dynamics; RMDPs assume uncertainty set of possible transitions to prepare for off-dynamics deployment.
  - **Quick check question:** How does the objective function in a CRMDP differ from a standard MDP? (Answer: Max-min over uncertainty set vs. Max over fixed transition kernel).

- **Concept:** Visitation Measures and Density Ratios
  - **Why needed here:** Understanding how often a state is visited (d^π) versus how often it could be visited in worst-case scenario (q^π) is core theoretical contribution (Cvr).
  - **Quick check question:** If a state has zero probability of being visited in nominal environment but high probability in deployment environment, what happens to Cvr? (Answer: It becomes unbounded/infinite).

- **Concept:** Regret Analysis and Sublinearity
  - **Why needed here:** Paper proves efficiency via regret (difference between optimal and learned policy performance); sublinear regret implies algorithm eventually learns optimal policy.
  - **Quick check question:** If regret scales as O(√K), does average regret per episode go to zero as episodes K→∞? (Answer: Yes).

## Architecture Onboarding

- **Component map:** Data Collector -> Statistics Engine -> Robust Solver -> Policy Selector
- **Critical path:** The backward update loop (Algorithm 1, Lines 3-10). Efficiency of "Robust Solver" at each step determines if system runs in polynomial time.
- **Design tradeoffs:**
  - Conservatism vs. Performance: Increasing uncertainty radius ρ makes policy more robust but potentially lowers average reward (pessimistic value)
  - Divergence Choice: TV distance allows hard constraints on probability shifts; KL divergence penalizes deviations smoothly (often yielding different worst-case transitions)
- **Failure signatures:**
  - Explosion of exploration: If bonus term b_k is too aggressive or Cvr is ignored, agent may waste resources exploring irrelevant states or fail to visit states critical for uncertainty set
  - Numerical instability: Logarithms in KL dual (Eq C.11) can be unstable if transition probabilities are near zero; requires smoothing or lower bounds (Assumption 5.7)
- **First 3 experiments:**
  1. Cvr Validation: Run ORBIT on gridworld where Cvr can be manually tuned (changing transition probabilities to make specific states rare vs. common). Plot Regret vs. Cvr to verify theoretical bound dependency.
  2. Perturbation Robustness: Train on "Frozen Lake" (Section 6.3). At test time, introduce stochastic slippage or wind (perturbations). Compare ORBIT (CRMDP/RRMDP) against non-robust Q-learning baseline.
  3. Divergence Sensitivity: Compare TV, KL, and χ² implementations on same environment with same uncertainty budget. Measure runtime of update step and final robustness achieved.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can ORBIT algorithm and Cvr concept be effectively extended to robust RL with function approximation (e.g., linear MDPs)?
- **Basis in paper:** Explicit statement that they "investigate tabular RMDPs with finite states and actions," limiting scope of theoretical guarantees to tabular setting.
- **Why unresolved:** Current regret bounds and bonus designs rely on state-action counts and epsilon-nets specific to finite spaces, which don't readily extend to continuous or high-dimensional function spaces.
- **What evidence would resolve it:** Derivation of regret bounds for ORBIT variant in linear MDP setting, or analysis of Cvr under linear function approximation.

### Open Question 2
- **Question:** Is it possible to design adaptive algorithm that achieves sublinear regret without prior knowledge of Cvr?
- **Basis in paper:** Theoretical results (Theorems 5.9 and 5.16) and algorithm's bonus design depend explicitly on Cvr. While Assumption 5.5 posits Cvr is polynomial, it's treated as known quantity for analysis.
- **Why unresolved:** Standard optimistic algorithms require inputting difficulty measure into bonus; unclear if "information deficit" can be overcome without explicitly parameterizing algorithm with Cvr.
- **What evidence would resolve it:** Algorithmic variant using adaptive estimates of visitation ratios to adjust bonus term dynamically while maintaining same regret guarantees.

### Open Question 3
- **Question:** Is there computationally efficient method to estimate or bound Cvr for given environment?
- **Basis in paper:** Cvr defined as supremum over all policies. Calculating this requires solving optimization problem over entire policy space to find maximum mismatch between nominal and worst-case visitation measures.
- **Why unresolved:** Paper introduces Cvr as fundamental measure of difficulty but doesn't provide method for computing it, limiting immediate practical application of theoretical findings.
- **What evidence would resolve it:** Polynomial-time algorithm or convex relaxation providing tight upper bound for Cvr.

## Limitations
- Reliance on Cvr being polynomially bounded (Assumption 5.5) is a worst-case theoretical quantity difficult to verify empirically
- Dual formulations for KL and χ² divergences introduce computational complexity that may not scale well to larger problems
- Theoretical claims about Cvr being fundamental quantity are mathematically sound but practically unverifiable without access to ground-truth worst-case transitions

## Confidence
- **High confidence:** Regret bounds for CRMDP-TV and RRMDP-TV are well-established through efficient dual formulation and matching lower bounds; algorithmic framework is clearly specified and implementable
- **Medium confidence:** Bounds for CRMDP-KL and CRMDP-χ² depend on unknown constant C_MSP (minimum transition probability), making practical verification difficult; computational efficiency claims are less certain due to inner optimization requirements
- **Low confidence:** Theoretical claims about Cvr being fundamental quantity governing sample complexity are mathematically sound but practically unverifiable without ground-truth worst-case transitions

## Next Checks
1. Implement synthetic environment where Cvr can be explicitly controlled through transition design, then verify ORBIT's regret scales as predicted with respect to Cvr
2. Test ORBIT's performance degradation when KL/χ² bonus terms are misspecified (e.g., underestimating minimum transition probability), comparing against TV baseline
3. Measure empirical wall-clock time for CRMDP-KL vs. CRMDP-TV on identical problem instances to validate computational efficiency claims in Figure 3(a)