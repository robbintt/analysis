---
ver: rpa2
title: Mitigating Estimation Bias with Representation Learning in TD Error-Driven
  Regularization
arxiv_id: '2511.16090'
source_url: https://arxiv.org/abs/2511.16090
tags:
- learning
- bias
- double
- dadc-r
- tddr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses estimation bias in deterministic policy gradient
  algorithms for continuous control, specifically the overestimation bias in Q-learning
  and underestimation bias in TD3. The authors propose an enhanced TD error-driven
  regularization (TDDR) framework with two key refinements: (1) a flexible convex
  combination mechanism that balances pessimistic estimates (to mitigate overestimation)
  and optimistic exploration (via double actors to alleviate underestimation), governed
  by a single hyperparameter; and (2) a representation learning module that generates
  augmented state and action features through a dynamics-based encoder, improving
  input quality for actor and critic networks.'
---

# Mitigating Estimation Bias with Representation Learning in TD Error-Driven Regularization

## Quick Facts
- arXiv ID: 2511.16090
- Source URL: https://arxiv.org/abs/2511.16090
- Reference count: 40
- One-line primary result: A novel TD error-driven regularization framework with representation learning achieves competitive performance against strong baselines by effectively mitigating both overestimation and underestimation biases in deterministic policy gradient algorithms.

## Executive Summary
This paper addresses estimation bias in deterministic policy gradient algorithms for continuous control, specifically the overestimation bias in Q-learning and underestimation bias in TD3. The authors propose an enhanced TD error-driven regularization (TDDR) framework with two key refinements: (1) a flexible convex combination mechanism that balances pessimistic estimates (to mitigate overestimation) and optimistic exploration (via double actors to alleviate underestimation), governed by a single hyperparameter; and (2) a representation learning module that generates augmented state and action features through a dynamics-based encoder, improving input quality for actor and critic networks. They introduce three core algorithms—DADC, DASC, and SASC—with different combinations of actors and critics, and their representation-enhanced counterparts (DADC-R, DASC-R, SASC-R). Extensive experiments on MuJoCo and Box2D environments show that the proposed methods achieve competitive or superior performance against strong baselines including DDPG, TD3, DARC, SD3, GD3, SAC, and PPO. The results demonstrate effective mitigation of both overestimation and underestimation biases, with tunable bias control and improved sample efficiency. The work highlights that overestimation and underestimation can each be beneficial depending on the environment, and combining them with high-quality representations significantly enhances overall performance.

## Method Summary
The paper introduces a TD error-driven regularization framework that extends deterministic actor-critic methods with double actors and critics, along with representation learning modules. Three algorithms are proposed: DADC (Double Actors Double Critics), DASC (Double Actors Single Critic), and SASC (Single Actor Single Critic). Each uses a convex combination of pessimistic CDQ estimates and optimistic Q-learning estimates, with the combination ratio υ governed by TD error magnitudes. The framework also incorporates dynamics-based encoders E_s and E_sa that generate augmented state and action representations, trained via MSE loss to predict next-state embeddings. Fixed and target-fixed encoders are updated every 250 steps. Actor inputs use [s, e_s], while critic inputs use [[s, e_s], a, e_sa].

## Key Results
- The proposed methods achieve competitive or superior performance against strong baselines (DDPG, TD3, DARC, SD3, GD3, SAC, PPO) on MuJoCo and Box2d environments.
- The TD error-driven convex combination effectively mitigates both overestimation and underestimation biases, with tunable bias control via hyperparameter υ.
- Representation learning significantly improves performance, with DADC-R achieving the highest average return of 5779 on the Ant environment.
- The framework demonstrates that both overestimation and underestimation biases can be beneficial depending on the environment, and their combination with high-quality representations enhances overall performance.

## Why This Works (Mechanism)

### Mechanism 1: TD Error-Driven Convex Combination for Tunable Bias
- **Claim:** A single hyperparameter υ enables predictable control across the bias spectrum, transitioning from underestimation to overestimation as υ decreases.
- **Mechanism:** The framework computes TD errors δ_i for each actor's Q-estimates, then selects the actor with smaller |δ_i| as the base. The TD target ψ is constructed as a convex combination: υ·(pessimistic estimate) + (1-υ)·(secondary estimate). Three variants differ in their secondary estimates—DADC uses another CDQ minimum, DASC uses an optimistic Q-value, SASC uses the same actor's Q-value.
- **Core assumption:** The TD error magnitude correlates with estimate stability; smaller |δ_i| indicates more reliable Q-estimates for that actor-state pair.
- **Evidence anchors:**
  - [Abstract] "A single hyperparameter governs this mechanism, enabling tunable control across the bias spectrum."
  - [Section IV-A] Equations (13)-(15) define the convex combinations; Remark 1 states convergence is preserved since "a linear convex combination of convergent operators preserves convergence."
  - [Corpus] ADDQ (arxiv:2506.19478) addresses bias through distributional methods, suggesting bias control is an active research direction, but does not validate the specific TD error-driven approach here.
- **Break condition:** If TD errors become uniformly large or noisy (e.g., in highly stochastic environments), the selection signal degrades and the mechanism may fail to distinguish actors meaningfully.

### Mechanism 2: Double Actors for Optimistic Exploration
- **Claim:** Double actors provide exploratory breadth that enables escaping local optima, and this exploration is more potent for inducing optimism than including a Q-learning component alone.
- **Mechanism:** Two actor networks π_φ1 and π_φ2 generate different actions a'_1, a'_2 for the same state. The DAC framework evaluates both, allowing selection of higher-return policies. This counters the "pessimistic underexploration" induced by CDQ's min operator.
- **Core assumption:** Diverse actions from independent actors span a richer region of the action space than single-actor exploration with noise.
- **Evidence anchors:**
  - [Section III-C] "Double actors can effectively enhance the agent's exploration ability, helping it maintain a balance between exploration and value estimation."
  - [Section V-B] "Experimental results consistently show that this transition occurs fastest in DASC, followed by DADC, and slowest in SASC...SASC, which relies on double actors. This observation highlights that exploration is a more potent mechanism for inducing optimism."
  - [Corpus] No direct validation found; neighbor papers do not address double-actor exploration specifically.
- **Break condition:** If actors converge to similar policies (mode collapse), exploration benefit diminishes. The paper does not analyze actor diversity metrics.

### Mechanism 3: Dynamics-Based Representation Learning
- **Claim:** Augmented state and action representations improve input quality for actor and critic networks, enhancing performance and learning stability.
- **Mechanism:** Two encoders—E_s (state) and E_sa (state-action)—are trained with MSE loss L(E_s, E_sa) = (e_sa - stopgrad(e'_s))². Encoders are decoupled from policy/value gradients. Fixed and target-fixed encoders are updated every 250 steps. Critic inputs: [s, e_s], a, e_sa. Actor inputs: [s, e_s].
- **Core assumption:** Representations trained to predict next-state embeddings capture useful structure for value estimation and policy improvement.
- **Evidence anchors:**
  - [Abstract] "To further improve performance, we integrate augmented state and action representations into the actor and critic networks."
  - [Section IV-B] Equation (16) defines the dynamics prediction loss; Equation (17) defines the 250-step update schedule.
  - [Corpus] TD7 (cited in paper) uses similar SALE architecture; no independent corpus validation of this specific augmentation strategy.
- **Break condition:** If encoder training destabilizes (e.g., representation collapse) or if dynamics prediction is uninformative (e.g., highly random environments), augmented features may add noise rather than signal.

## Foundational Learning

- **Concept: Temporal Difference (TD) Error**
  - **Why needed here:** The core mechanism uses |δ_i| to drive actor selection. TD error δ = r + γ·Q(s', a') - Q(s, a) measures prediction accuracy.
  - **Quick check question:** Can you explain why |δ_i| might indicate estimate reliability?

- **Concept: Estimation Bias in Q-Learning (Over/Underestimation)**
  - **Why needed here:** The paper's central problem is mitigating overestimation (from max operator) and underestimation (from min/CDQ operator).
  - **Quick check question:** Why does the min operator in CDQ produce systematic underestimation?

- **Concept: Actor-Critic Framework with Target Networks**
  - **Why needed here:** The DAC architecture extends AC with double actors/critics and soft target updates (θ' ← τθ + (1-τ)θ').
  - **Quick check question:** What role do target networks play in stabilizing TD learning?

## Architecture Onboarding

- **Component map:** s -> E_s -> [s, e_s] -> π_φ1/π_φ2 -> a, a' -> E_sa -> e_sa -> Q_θ1/Q_θ2 -> Q-values -> TD errors δ_1, δ_2 -> convex combination -> ψ

- **Critical path:**
  1. State s → E_s → augmented state [s, e_s]
  2. Actors generate actions; E_sa produces e_sa
  3. Critics compute Q-values and TD errors δ_1, δ_2
  4. TD target ψ constructed via convex combination (υ-governed)
  5. Critics updated via MSE; actors via deterministic policy gradient
  6. Encoders updated every step; fixed/target-fixed copies every 250 steps

- **Design tradeoffs:**
  - **υ selection:** Environment-dependent (Table II shows values 0.0-0.9 across tasks). Higher υ = more pessimistic; lower υ = more optimistic.
  - **Encoder decoupling:** Gradients from policy/value do not flow to encoders—trades representation quality for stability.
  - **Network depth:** Three-layer critic vs. TD7's four-layer—simpler but may limit expressiveness.

- **Failure signatures:**
  - **SASC underperformance in Ant:** Without double actors, exploration-heavy tasks suffer (Table III: SASC 4444 vs. DADC 5779).
  - **Underestimation persistence:** Even with υ tuning, some environments show residual bias (Table IV: HalfCheetah bias -47 to -245).
  - **Encoder instability:** If L(E_s, E_sa) diverges, augmented features degrade critic inputs.

- **First 3 experiments:**
  1. **Ablate representation learning:** Run DADC vs. DADC-R on Ant and HalfCheetah; expect performance gap per Fig. 2.
  2. **Sweep υ values:** Plot estimation bias vs. υ for each algorithm (replicate Fig. 3) to verify tunable bias claim.
  3. **Single vs. double actor comparison:** Compare SASC vs. DADC in Ant to quantify exploration contribution per Section V-B analysis.

## Open Questions the Paper Calls Out

- **Adaptive scheduling of hyperparameter υ:** The authors suggest it would be interesting to explore adaptive scheduling of υ during training, as the current framework requires manual tuning per environment.

## Limitations

- The TD error-driven actor selection mechanism assumes |δ_i| correlates with estimate reliability, but this relationship is not empirically validated across diverse environments.
- The double-actor exploration mechanism is claimed to be "more potent" than Q-learning for inducing optimism, but this is only supported by relative convergence speeds, not absolute performance gains.
- The representation learning module's contribution is not independently validated through ablation studies to confirm its necessity beyond standard RL features.

## Confidence

- **High Confidence:** Experimental results showing performance gains over baselines (Table III, Fig. 2). The quantitative improvements are directly observable and reproducible.
- **Medium Confidence:** The tunable bias control mechanism (υ parameter controlling overestimation/underestimation). While theoretically sound (convex combination preserves convergence per Remark 1), the practical effectiveness depends on environment-specific υ tuning not fully explained.
- **Low Confidence:** The claim that representation learning improves performance. The paper cites TD7 for similar architecture but provides no ablation demonstrating that encoders E_s and E_sa are necessary rather than incidental.

## Next Checks

1. **Validate TD error reliability:** Plot |δ_i| vs actual Q-value error across training steps for each actor. If correlation is weak or negative, the actor selection mechanism may be selecting suboptimally.
2. **Ablate representation learning:** Implement DADC without E_s/E_sa encoders. Compare performance against DADC-R on HalfCheetah and Ant. If performance gap is small, representation learning may not be essential.
3. **Test actor diversity preservation:** Track the cosine similarity between actor policies π_φ1 and π_φ2 over training. If similarity approaches 1.0, double actors provide no exploration benefit and DASC/DADC collapse to single-actor performance.