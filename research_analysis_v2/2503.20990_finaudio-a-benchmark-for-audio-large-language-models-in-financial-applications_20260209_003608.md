---
ver: rpa2
title: 'FinAudio: A Benchmark for Audio Large Language Models in Financial Applications'
arxiv_id: '2503.20990'
source_url: https://arxiv.org/abs/2503.20990
tags:
- audio
- financial
- audiollms
- long
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces FinAudio, the first benchmark for evaluating
  Audio Large Language Models (AudioLLMs) in financial applications. The authors address
  the gap in financial audio processing by defining three tasks: ASR for short financial
  audio clips, ASR for long financial audio recordings, and summarization of long
  financial audio.'
---

# FinAudio: A Benchmark for Audio Large Language Models in Financial Applications

## Quick Facts
- arXiv ID: 2503.20990
- Source URL: https://arxiv.org/abs/2503.20990
- Reference count: 40
- First benchmark for evaluating AudioLLMs in financial applications

## Executive Summary
This paper introduces FinAudio, the first comprehensive benchmark for evaluating Audio Large Language Models (AudioLLMs) in financial applications. The authors identify a critical gap in existing evaluations, which focus on general audio tasks while financial audio processing remains underexplored. They define three tasks: automatic speech recognition (ASR) for short and long financial audio recordings, and summarization of long financial audio. Through extensive evaluation of seven state-of-the-art AudioLLMs on five curated datasets totaling over 430 hours of financial audio, the study reveals significant performance gaps, particularly for long-form content and specialized financial terminology.

## Method Summary
The benchmark defines three tasks: (1) ASR for short financial audio clips (<1 minute), (2) ASR for long financial audio recordings (45-60 minutes earnings calls), and (3) summarization of long financial audio. The evaluation uses five datasets: MDRM-test (87h), SPGISpeech-test (130h), Earnings-21 (39h), Earnings-22 (120h), and FinAudioSum (55h). For long audio, the pipeline chunks recordings into 30-second segments, transcribes each independently, then concatenates results. Summarization uses a two-stage process: ASR output → GPT-4o (temperature=0) for text summarization. All experiments run on 2× NVIDIA RTX A6000 GPUs, repeated 3 times with averaged results.

## Key Results
- Whisper-v3 achieves the lowest WER (2-3%) on short financial audio and (12-16%) on long audio, outperforming closed-source models
- SALMONN models exhibit poor performance with WERs around 80-88%, suggesting overfitting to training data
- Summarization quality depends heavily on ASR accuracy, with Gemini-1.5-flash achieving the best results among evaluated models
- Long-form financial audio processing shows significant degradation due to chunk-based processing and context loss

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Whisper-v3 achieves strong ASR performance on short financial audio clips (2–3% WER) without domain-specific fine-tuning due to large-scale weak supervision during pre-training.
- **Mechanism:** The model's encoder-decoder architecture, trained on 680,000 hours of diverse web audio, generalizes to financial terminology through exposure to varied speaking styles, accents, and domain vocabulary during pre-training. The weak supervision approach (predicting transcripts without precise alignment) creates robustness to domain shift.
- **Core assumption:** Financial speech patterns and terminology distributions in earnings calls overlap sufficiently with the diverse audio-text pairs seen during Whisper's web-scale pre-training.
- **Evidence anchors:**
  - [abstract] "Whisper-v3 achieves the lowest WER (2-3%) on short audio... outperforming closed-source models"
  - [section 3.1] "Whisper-v3 achieves notably low WER scores (2%–3%) on both datasets, highlighting its robust speech recognition without domain-specific tuning"
  - [corpus] SPGISpeech 2.0 paper confirms that financial audio transcription benefits from formatted text training data, supporting the importance of transcript structure
- **Break condition:** Performance degrades significantly if: (1) financial jargon density exceeds pre-training distribution, (2) audio quality falls below web-scraped data standards, or (3) domain-specific numerical formats (e.g., "two point five billion") differ systematically from pre-training examples.

### Mechanism 2
- **Claim:** Long-form financial audio ASR performance degrades (WER increases from 2–3% to 12–16% for Whisper-v3) due to error accumulation across concatenated 30-second chunks and context fragmentation.
- **Mechanism:** Current AudioLLMs cannot process >30 seconds in a single forward pass. The benchmark chunks long audio (45–60 minute earnings calls) into 30-second segments, transcribes each independently, then concatenates outputs. Errors at chunk boundaries (word cutoffs, speaker changes) propagate, and cross-chunk context (speaker identity, topic continuity) is lost.
- **Core assumption:** Chunk-level transcription errors are independent and linearly additive; cross-chunk context provides minimal lexical disambiguation value for financial terminology.
- **Evidence anchors:**
  - [section 2.1.2] "To handle the varying window lengths and the inability of current AudioLLMs to process long audio files within a single window, we standardize audio inputs to 30-second segments"
  - [section 3.1] "These results reveal a decline in performance across all AudioLLMs, indicating that AudioLLMs face significant challenges when processing long financial audio data"
  - [corpus] VIBEVOICE-ASR technical report explicitly addresses "context fragmentation and multi-speaker complexity in long-form audio" as persistent challenges, validating this mechanism
- **Break condition:** Chunk-based ASR fails catastrophically when: (1) utterances span chunk boundaries with critical numerical data, (2) speaker diarization is required for attribution, or (3) cross-referencing between audio sections (e.g., "as I mentioned earlier") is necessary for accurate transcription.

### Mechanism 3
- **Claim:** Summarization quality in the two-stage pipeline (ASR → LLM summarization) is upper-bounded by ASR accuracy; transcription errors in numerical data and financial terminology directly propagate into summary hallucinations.
- **Mechanism:** The pipeline first transcribes audio chunks via AudioLLM, concatenates transcripts, then feeds text to a text-only LLM (GPT-4o with temperature=0) for summarization. ASR errors in financial entities (e.g., "NextEra Energy" → "Era Energy") and numerical values (e.g., "$2.5B" → "2.5") cannot be recovered by the summarization model, as it lacks access to original audio signals.
- **Core assumption:** Text-only LLMs cannot infer correct financial terminology or numerical values from corrupted transcripts without audio grounding; ASR errors are irrecoverable in the summarization stage.
- **Evidence anchors:**
  - [abstract] "Summarization quality depends heavily on ASR accuracy"
  - [section F] "Financial audio frequently contains specialized financial proper nouns, leading AudioLLMs to produce translation errors... NextEra Energy was incorrectly transcribed as Era Energy"
  - [corpus] Corpus evidence weak for this specific mechanism; no direct corpus papers address ASR error propagation to summarization
- **Break condition:** The pipeline fails when: (1) ASR WER exceeds ~20% (SALMONN's 80–88% WER makes summarization meaningless), (2) numerical errors compound (wrong billion/million units), or (3) entity coreference chains break across chunks.

## Foundational Learning

- **Concept: Word Error Rate (WER)**
  - **Why needed here:** Primary evaluation metric for ASR tasks; measures transcription accuracy via edit distance (substitutions + deletions + insertions / total words). Understanding WER components is essential for diagnosing whether models fail on financial terminology (substitutions), audio segmentation (deletions), or over-generation (insertions).
  - **Quick check question:** If a model transcribes "revenue of 2.5 billion" as "revenue of two point five million," what is the WER contribution, and which error type dominates?

- **Concept: Audio Context Window Limitations**
  - **Why needed here:** Current AudioLLMs process fixed-duration audio (typically 30 seconds) due to memory and attention complexity constraints. This architectural limitation forces chunking strategies for long-form content, introducing boundary artifacts and context loss that directly impact financial applications (earnings calls average 45–60 minutes).
  - **Quick check question:** Why does chunking long audio into 30-second segments for independent transcription inherently lose information, even if each segment is transcribed perfectly?

- **Concept: Two-Stage Cascaded Error Propagation**
  - **Why needed here:** The summarization task uses a pipeline (ASR → text concatenation → LLM summarization) where errors from the first stage irreversibly corrupt the second stage. Unlike end-to-end audio-to-summary models, this architecture cannot recover from ASR mistakes during summarization.
  - **Quick check question:** If ASR mis-transcribes a company name in 3 of 10 chunk transcripts, can the downstream summarization LLM correct this? Why or why not?

## Architecture Onboarding

- **Component map:** Raw audio → AudioLLM encoder (log-mel spectrogram → conformer/Vicuna) → AudioLLM decoder (transformer/LM) → transcript chunks → concatenation → GPT-4o → summary

- **Critical path:**
  1. Long audio file → 30-second chunk segmentation
  2. Each chunk → AudioLLM inference → transcript segment
  3. All transcript segments → string concatenation → full transcript
  4. Full transcript → text LLM → summary
  5. Summary vs. reference → ROUGE-L / BertScore computation

- **Design tradeoffs:**
  - **Chunk size (30s vs. shorter/longer):** 30s balances GPU memory constraints against boundary error frequency; shorter chunks increase boundary errors, longer chunks may exceed model context
  - **Whisper-v3 vs. closed-source models:** Whisper offers privacy (on-premise deployment) and cost advantages with superior ASR performance; closed-source models provide integrated pipelines but suffer higher WER
  - **Instruction-tuned vs. base models:** Qwen2-Audio-7B-Instruct shows stable performance across prompt variations; base Qwen2-Audio-7B is prompt-sensitive (WER increases with varied prompts)

- **Failure signatures:**
  - **SALMONN models (80–88% WER):** Frequent empty outputs or instruction misunderstanding; suggests overfitting to specific audio-task pairs during training; poor generalization to financial domain
  - **Numerical transcription errors:** Digit inconsistencies ("2.5" → "two point five") and missing units ("billion" → ""); critical for financial applications
  - **Financial terminology errors:** Named entity corruption ("NextEra Energy" → "Era Energy"); indicates insufficient financial vocabulary in pre-training

- **First 3 experiments:**
  1. **Reproduce Whisper-v3 baseline on MDRM-test:** Run Whisper-v3 (large-v3 checkpoint) on 100 random samples from MDRM-test; compute WER and categorize errors (substitutions/deletions/insertions); target: 2–3% WER per paper
  2. **Chunk boundary error analysis:** Take 5 earnings call recordings; manually annotate 10 chunk boundaries per file; measure WER specifically at boundaries vs. chunk centers; quantify context fragmentation impact
  3. **Prompt robustness stress test:** Evaluate Qwen2-Audio-7B and Qwen2-Audio-7B-Instruct on SPGISpeech-test using the 10-prompt set from Appendix E; compare WER variance; confirm instruction-tuning stabilizes performance

## Open Questions the Paper Calls Out
None

## Limitations
- The study relies on a cascaded two-stage pipeline (ASR → LLM summarization) rather than end-to-end audio-to-summary models, which may underestimate the potential of integrated architectures
- Long audio processing uses fixed 30-second chunking without overlap or speaker diarization, potentially missing cross-chunk context critical for financial terminology disambiguation
- Closed-source model evaluations depend on API access and may not reflect production deployment conditions

## Confidence
- High confidence: Whisper-v3 ASR performance on short financial audio (2-3% WER) - well-established through extensive prior research and reproducible with open models
- Medium confidence: Long audio ASR degradation pattern (12-16% WER) - mechanistically sound but dependent on chunking strategy implementation
- Medium confidence: Summarization quality correlation with ASR accuracy - demonstrated but limited by lack of end-to-end audio models for comparison
- Low confidence: SALMONN model failure modes - attributed to instruction misunderstanding but could reflect dataset distribution mismatches

## Next Checks
1. **End-to-end validation**: Implement an end-to-end audio-to-summary model (e.g., fine-tuned Whisper with summarization head) to compare against the cascaded pipeline and quantify context loss from chunking
2. **Chunk boundary analysis**: Systematically measure WER at chunk boundaries versus chunk centers across all models to quantify the exact contribution of context fragmentation to performance degradation
3. **Numerical entity robustness test**: Create a controlled test set with financial numerical entities (amounts, percentages, units) in varied contexts to measure specific failure modes in digit transcription and unit preservation across models