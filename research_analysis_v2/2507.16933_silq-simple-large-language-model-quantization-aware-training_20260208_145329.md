---
ver: rpa2
title: 'SiLQ: Simple Large Language Model Quantization-Aware Training'
arxiv_id: '2507.16933'
source_url: https://arxiv.org/abs/2507.16933
tags:
- training
- accuracy
- quantization
- silq
- spinquant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently quantizing large
  language models (LLMs) for deployment on low-precision inference accelerators, balancing
  accuracy retention with reduced latency and energy consumption. The authors introduce
  SiLQ, a simple, end-to-end quantization-aware training (QAT) approach that achieves
  superior accuracy compared to leading post-training quantization (PTQ) methods.
---

# SiLQ: Simple Large Language Model Quantization-Aware Training

## Quick Facts
- **arXiv ID**: 2507.16933
- **Source URL**: https://arxiv.org/abs/2507.16933
- **Reference count**: 40
- **Primary result**: SiLQ achieves near-lossless accuracy when quantizing LLMs to 8-bit activations and 4-bit weights with <0.1% increase in training budget.

## Executive Summary
SiLQ introduces a simple end-to-end quantization-aware training approach that outperforms leading post-training quantization methods on LLM accuracy benchmarks. The method employs three key practices: quantization with straight-through estimation, step size initialization via calibration and LSQ refinement, and end-to-end training with knowledge distillation. On benchmarks including Common Sense Reasoning and Open LLM v1/v2, SiLQ achieves superior accuracy compared to SmoothQuant and SpinQuant while maintaining a minimal training budget increase. The approach is particularly effective for instruction-tuned models where standard next-token prediction fails to recover accuracy.

## Method Summary
SiLQ implements symmetric uniform quantization with straight-through estimator for activations, KV cache, and weights. Step sizes are initialized using percentile calibration (99.91/99.99/99.995 for 4/8/16-bit activations) and convex MSE approximation for weights, then refined via LSQ. The method trains end-to-end using knowledge distillation with the original unquantized model as teacher. Key hyperparameters include AdamW optimizer (β1=0.9, β2=0.95, eps=1e-10), learning rate 5e-6 scaled ∝ 1/√steps, batch size 128, sequence length 1024, and weight decay 0.1. Activation quantizer step-size learning rate is boosted by 50× relative to weights.

## Key Results
- Outperforms SmoothQuant and SpinQuant by several percentage points on Common Sense Reasoning, Open LLM v1, and v2 benchmarks
- Achieves near-lossless accuracy (<0.1% increase in training budget) on both base and instruction-tuned models
- Generalizes across architectures without requiring complex model-specific optimizations
- Maintains accuracy superiority even with minimal training (8k steps vs 128k steps for full convergence)

## Why This Works (Mechanism)

### Mechanism 1: Teacher-Guided Error Recovery
Preserving the functional behavior of the original FP16 model is best achieved by training the quantized model against the teacher's output distribution rather than ground-truth tokens. Standard next-token prediction forces the model to re-learn ground truth from scratch within low-bit constraints. Distillation allows the student to mimic the reasoning of the teacher, smoothing the loss landscape and prioritizing preservation of semantic features over exact token matches. Ablation study shows KD ratio of 1.0 (pure distillation) outperforms mixed approach (0.5 ratio) which outperforms pure next-token prediction (0.0).

### Mechanism 2: Quantization-Resilient Initialization
Initializing step sizes using statistical percentiles (activations) and error-minimization (weights) prevents early-training divergence better than standard max-calibration. LLM activations contain outliers that skew max-based ranges, compressing the bulk of values into too few integer bins. Percentile initialization clips these outliers, ensuring high utilization of the integer range for typical values. Calibration dataset (e.g., 5 batches) must be representative of broader data distribution.

### Mechanism 3: Straight-Through Gradient Flow
The Straight-Through Estimator allows the optimizer to adjust weights to minimize quantization error, finding solutions inaccessible to PTQ. The rounding operation is non-differentiable. STE approximates the gradient as 1, effectively passing the error gradient directly through the quantizer. This allows the training process to "nudge" weights such that their quantized versions minimize the loss. The bias introduced by approximating the gradient of the round operation is negligible relative to the benefit of end-to-end optimization.

## Foundational Learning

- **Straight-Through Estimator (STE)**: Mathematical trick that makes QAT possible by allowing backpropagation through the integer rounding operation. *Quick check*: If you round a value from 2.3 to 2, what gradient does STE assign to the input 2.3?

- **Knowledge Distillation (KD)**: Superior loss function for recovering accuracy in quantized instruction-tuned models by training against teacher's output distribution. *Quick check*: Why would a soft label distribution from a teacher be more useful for a quantized student than a hard "one-hot" label?

- **Symmetric vs. Asymmetric Quantization**: SiLQ uses symmetric quantization (centered at zero) to match hardware constraints, requiring specific step-size handling. *Quick check*: In symmetric quantization, does the range [-127, 127] handle values the same way as asymmetric [0, 255]?

## Architecture Onboarding

- **Component map**: Input -> Activation Quantizer -> Linear(Weight Quantizer) -> Output. Linear layers (Up, Down, Gate, Q, K, V, O) are wrapped with Weight Quantizers. Inputs to these layers are wrapped with Activation Quantizers. Past Key/Value states are wrapped with Cache Quantizers.

- **Critical path**: Calibration -> Training Loop -> Evaluation. You must first run the calibration step to generate initial step_size tensors before starting the optimizer.

- **Design tradeoffs**:
  - Static vs. Dynamic: Static (A8s) is faster/cheaper but harder to tune than Dynamic (A8d). SiLQ makes Static viable.
  - Step Size Learning Rate: Activation step sizes need a learning rate boost (50x) compared to weights.

- **Failure signatures**:
  - Immediate Loss Spike: Likely due to poor step size initialization (e.g., Max calibration used instead of Percentile).
  - Slow Convergence: Learning rate for activation quantizers not boosted sufficiently.
  - Accuracy Gap on Instruct Models: Loss function likely using next-token prediction instead of pure Knowledge Distillation.

- **First 3 experiments**:
  1. Calibration Ablation: Run SiLQ on small model comparing "Max" vs. "Quantile" step initialization. Verify if Quantile converges faster.
  2. Loss Function Check: Train for 500 steps with KD_Ratio=1.0 vs. KD_Ratio=0.0. Confirm pure KD outperforms NTP for QAT.
  3. Weight Rotation Analysis: Visualize weight changes before and after training. Confirm if "non-rotational distance" increases with training steps.

## Open Questions the Paper Calls Out

### Open Question 1
Does SiLQ scale effectively to models significantly larger than 8 billion parameters without proportionally increasing training resource requirements? The paper states the method should generalize to larger models but only validated on 7-8B parameter models. What evidence would resolve it: Benchmarking on 70B or 405B parameter models to verify if training token requirements remain proportional.

### Open Question 2
Can SiLQ maintain near-lossless accuracy at sub-4-bit precision levels (e.g., 2- or 3-bit weights) where competing methods like BitDistiller operate? The paper only demonstrated results for 4-bit weights, not the extreme quantization regimes mentioned in related work. What evidence would resolve it: Applying SiLQ to W2 or W3 configurations and comparing accuracy trade-offs against low-bit baselines.

### Open Question 3
What specific representational adaptations does SiLQ learn beyond weight rotation to achieve high accuracy? The authors found only 43% of weight changes can be accounted for by rotation, contrasting with PTQ methods where rotation accounts for 90% of changes. What evidence would resolve it: Detailed layer-wise analysis of weight distribution changes or feature map behaviors orthogonal to rotational transformations.

## Limitations

- Focus on symmetric quantization for hardware compatibility (NorthPole) may not generalize to all deployment scenarios
- Scalability to models significantly larger than 8 billion parameters remains unproven
- Sub-4-bit precision performance not evaluated against specialized low-bit quantization methods

## Confidence

- **High Confidence**: End-to-end QAT with knowledge distillation outperforms PTQ methods, well-supported by ablation studies and benchmark comparisons
- **Medium Confidence**: Percentile calibration initialization strategy is supported by ablation results but effectiveness may vary with different data distributions
- **Low Confidence**: Numerical solver for convex MSE weight calibration not fully specified, making exact replication challenging

## Next Checks

1. **Calibration Method Ablation**: Implement and compare "Max" vs. "Quantile" step size initialization on a small model (e.g., Llama-2-7B) using 5 batches × 128 samples. Verify if the claimed accuracy gap (e.g., OLLMv1 68.65 vs 63.98) is reproducible.

2. **Loss Function Ablation**: Train a quantized model for 500 steps with KD_Ratio=1.0 (pure distillation) vs. KD_Ratio=0.0 (pure next-token prediction) on an instruction-tuned model. Confirm pure KD outperforms NTP for QAT.

3. **Weight Change Analysis**: Visualize and quantify the "non-rotational distance" of weights before and after training. Confirm that the optimizer finds specific weight adjustments rather than just global rotations.