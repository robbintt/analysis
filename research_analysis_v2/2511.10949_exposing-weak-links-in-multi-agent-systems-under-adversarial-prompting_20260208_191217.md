---
ver: rpa2
title: Exposing Weak Links in Multi-Agent Systems under Adversarial Prompting
arxiv_id: '2511.10949'
source_url: https://arxiv.org/abs/2511.10949
tags:
- task
- agent
- agents
- tool
- plan
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SafeAgents, a framework for evaluating multi-agent
  system (MAS) security against adversarial prompts, and Dharma, a fine-grained diagnostic
  metric that identifies weak links within MAS pipelines. Through extensive evaluation
  across five architectures (centralized, decentralized, and hybrid) on four benchmarks
  (AgentHarm, ASB, SafeArena, RedCode), the authors find that MAS design choices significantly
  influence vulnerability surfaces.
---

# Exposing Weak Links in Multi-Agent Systems under Adversarial Prompting

## Quick Facts
- arXiv ID: 2511.10949
- Source URL: https://arxiv.org/abs/2511.10949
- Reference count: 40
- Key outcome: MAS design choices significantly influence vulnerability surfaces, with centralized systems obscuring harmful objectives through atomic instruction delegation while decentralized systems show better context-aware refusal

## Executive Summary
This paper introduces SafeAgents, a framework for evaluating multi-agent system security against adversarial prompts, and Dharma, a fine-grained diagnostic metric that identifies weak links within MAS pipelines. Through extensive evaluation across five architectures on four benchmarks, the authors find that MAS design choices significantly influence vulnerability surfaces. Centralized systems often obscure harmful objectives through atomic instruction delegation, while decentralized systems show better context-aware refusal but with implementation-dependent performance. The study reveals that simple prompt-based mitigations can improve security, but existing LLM alignment techniques do not reliably transfer to multi-agent contexts.

## Method Summary
The SafeAgents framework orchestrates five agentic architectures (Magentic-One, LangGraph, OpenAI Agents in centralized/decentralized variants) to execute tasks from four benchmarks. An LLM-as-judge (GPT-4.1) processes execution logs using specific prompts to classify failure modes into the DHARMA metric. The framework evaluates security by measuring where failures occur in the pipeline - whether at the planner, sub-agent, or through unmitigated execution. The evaluation uses two frontier models (gpt-4o, Qwen3-30B-A3B-Instruct-2507) across different architectures and benchmarks.

## Key Results
- Centralized systems delegating atomic instructions obscure harmful intent and reduce robustness compared to high autonomy delegation
- Standard ASR metrics fail to identify specific weak links in MAS pipelines, necessitating hierarchical diagnostic metrics
- Model size significantly impacts vulnerability patterns, with smaller models (SLMs) showing higher unmitigated execution rates

## Why This Works (Mechanism)

### Mechanism 1: Sub-agent Autonomy vs. Context Fragmentation
Delegating atomic instructions to sub-agents can obscure harmful intent and bypass refusal mechanisms compared to delegating high-level subtasks. When an orchestrator decomposes a harmful task into granular, context-free steps, sub-agents execute them innocently. This works because sub-agents retain safety alignment capabilities when presented with full context, but lose this ability when context is fragmented.

### Mechanism 2: Stratified Planning and Blind Execution
Stratified planning creates a vulnerability when the orchestrator does not re-evaluate the plan's safety during delegation. A planner might generate a harmful but technically valid plan, and if the orchestrator treats it as a static directive to be executed mechanically, the system proceeds to unsafe execution. This assumes the planner component may not be as robustly aligned as execution agents.

### Mechanism 3: DHARMA Diagnostic Metric
Standard aggregate metrics like Attack Success Rate fail to identify specific weak links in MAS pipelines. The DHARMA metric classifies execution trajectories by where the refusal or failure occurred (Planner-Stop vs. Sub-agent-Ignore), distinguishing between systems that are safe because the planner refused versus those safe only because a sub-agent happened to refuse.

## Foundational Learning

- **Sub-agent Autonomy vs. Context Fragmentation**: To understand why giving agents more independence can improve safety by allowing them to "see" harmful intent, whereas strict control can blind them. Quick check: Does restricting an agent to "click a button" make it safer or less safe than asking it to "post a review"?

- **Stratified vs. Combined Planning**: To diagnose if vulnerability comes from a "bad plan" being blindly followed (stratified risk) or if safety checks happen dynamically during task decomposition (combined). Quick check: In this architecture, does one agent create the plan and another blindly execute it, or is planning integrated into the execution loop?

- **Alignment Transferability Gap**: The paper challenges the assumption that LLM safety alignment transfers to MAS. Agents may act safely in isolation but unsafely in coordination. Quick check: If Agent A (Planner) and Agent B (Executor) are both "safe" alone, is the system comprising A+B guaranteed to be safe?

## Architecture Onboarding

- **Component map**: User Prompt → Planner (Generate Plan) → Orchestrator (Delegate Step) → Sub-agent (Execute Tool) → Orchestrator (Aggregate/Next Step)
- **Critical path**: Safety checks typically occur at Planner (Refusal) or Sub-agent (Refusal)
- **Design tradeoffs**:
  - Magentic (Centralized, Stratified): High control via atomic steps, prone to context fragmentation
  - LangGraph (Centralized, Combined): High sub-agent autonomy, better context-aware refusal but risks mis-coordination
  - Swarm (Decentralized): Best for context-heavy tasks, but complex coordination logic
- **Failure signatures**:
  - Unmitigated Execution: Neither planner nor sub-agent refuses a harmful task
  - Planner-Ignore / Sub-agent-Ignore: Refusal generated but system lacks logic to halt
  - Fallback Loop: Planner fails, but Orchestrator invokes sub-agent with null instructions
- **First 3 experiments**:
  1. Vary Sub-agent Autonomy: Modify delegation prompt to pass "full task context" vs. "atomic step" and measure DHARMA "Sub-agent Stop" rates
  2. Test Fallback Robustness: Force planner refusal and verify if system halts or attempts to invoke sub-agents with empty plans
  3. Model Size Impact: Run same architecture with GPT-4o vs. Qwen3-30B on RedCode to observe shift from "Planner-Stop" to "Unmitigated Execution" failures

## Open Questions the Paper Calls Out
None

## Limitations
- LLM-as-judge dependency for DHARMA metric introduces potential subjectivity and reproducibility concerns
- Evaluation focuses on specific benchmarks and architectures, potentially missing broader real-world failure modes
- The assertion about alignment transferability gaps is based on negative results rather than positive demonstrations

## Confidence
- **High Confidence**: Centralized systems delegating atomic instructions obscure harmful intent
- **Medium Confidence**: Standard ASR metrics fail to identify weak links in MAS pipelines
- **Low Confidence**: Existing LLM alignment techniques do not reliably transfer to multi-agent contexts

## Next Checks
1. **Judge Consistency Test**: Run same execution logs through multiple LLM judges to measure classification consistency and identify potential bias
2. **Architecture Transferability**: Apply SafeAgents framework to a fundamentally different MAS architecture to validate if identified weak links generalize
3. **Real-world Scenario Validation**: Test framework on tasks involving longer execution chains (>50 steps) and more complex tool usage to assess scalability of identified vulnerabilities