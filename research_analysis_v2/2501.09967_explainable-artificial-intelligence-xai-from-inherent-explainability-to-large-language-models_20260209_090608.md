---
ver: rpa2
title: 'Explainable artificial intelligence (XAI): from inherent explainability to
  large language models'
arxiv_id: '2501.09967'
source_url: https://arxiv.org/abs/2501.09967
tags:
- methods
- language
- learning
- these
- interpretability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive survey of explainable AI (XAI)
  methods, covering the spectrum from inherently interpretable models to techniques
  for explaining black box models, including large language models (LLMs). The survey
  highlights the growing importance of XAI due to the increasing adoption of AI in
  mission-critical domains like healthcare and autonomous driving.
---

# Explainable artificial intelligence (XAI): from inherent explainability to large language models

## Quick Facts
- arXiv ID: 2501.09967
- Source URL: https://arxiv.org/abs/2501.09967
- Reference count: 40
- This paper presents a comprehensive survey of XAI methods covering inherent interpretability, post-hoc explanations, and the emerging role of LLMs/VLMs in automating and enhancing explainability.

## Executive Summary
This survey systematically reviews XAI techniques across the spectrum from inherently interpretable models to methods for explaining black-box systems, with special attention to vision-language models and large language models. It addresses the growing need for explainability in critical applications like healthcare and autonomous driving. The paper categorizes XAI approaches including gradient-based, model-agnostic, attention-based, counterfactual, and concept-based methods, while highlighting how VLMs and LLMs can enhance interpretability through improved attribution, natural language explanations, and automated concept discovery. It also discusses quantitative comparisons, challenges in evaluation, and future research directions emphasizing the need for reliable metrics and better integration of advanced AI models for interpretability.

## Method Summary
The survey synthesizes existing XAI literature by categorizing methods based on their technical approach and accessibility requirements. For inherent interpretability, it examines transparent model architectures like decision trees and linear models. For post-hoc explanations, it covers gradient-based methods (CAM variants, Integrated Gradients), perturbation-based approaches (SHAP, LIME), attention mechanisms, counterfactual explanations, and concept-based reasoning. The paper then explores how vision-language models (like CLIP) and large language models enhance these approaches - using VLMs to improve gradient-based attribution and discover concepts, and LLMs to generate natural language explanations and automate concept discovery in concept bottleneck models. Quantitative comparisons are drawn from published benchmarks across datasets like CIFAR-10/100, CUB-200, ImageNet, and Places365.

## Key Results
- Inherently interpretable models trade accuracy for transparency, while black-box models require post-hoc explanations to achieve both high performance and interpretability.
- Gradient-based methods like Grad-CAM and Integrated Gradients can localize important features but may suffer from gradient saturation and sensitivity issues.
- Model-agnostic methods like SHAP and LIME approximate feature importance through perturbations or Shapley values but face computational costs and assumption violations.
- VLMs and LLMs show promise in enhancing explainability through cross-modal alignment, natural language generation, and automated concept discovery, though domain generalization remains challenging.

## Why This Works (Mechanism)

### Mechanism 1: Gradient-Based Attribution
- **Claim:** Differentiable models can localize important features through backpropagation of output scores.
- **Mechanism:** Methods like Grad-CAM and Integrated Gradients compute gradients of predictions with respect to inputs or intermediate activations to create saliency maps.
- **Core assumption:** Gradient magnitude correlates with feature importance and the model's learned representations align with human-interpretable features.
- **Evidence anchors:** Section 3.2.5 describes Integrated Gradients computing integrals of average gradients; Section 3.2.6 details Grad-CAM using gradients to weight activation maps.
- **Break condition:** Gradient saturation (ReLU), discrete inputs, or models where gradients are uninformative; the paper notes these methods can violate sensitivity and produce noisy maps.

### Mechanism 2: Model-Agnostic Perturbation Methods
- **Claim:** Input-output access alone enables approximation of feature importance through observation of prediction changes.
- **Mechanism:** SHAP uses Shapley values to fairly allocate prediction contributions; LIME fits interpretable surrogate models locally by sampling perturbations.
- **Core assumption:** Local linearity (LIME) or feature independence/approximate Shapley assumptions (SHAP) hold, and perturbations stay on the data manifold.
- **Evidence anchors:** Section 3.4.1 explains SHAP computing outputs with and without features; Section 3.4.2 describes LIME learning behavior through interpretable surrogates.
- **Break condition:** High-dimensional inputs, strong feature interactions, or perturbations leaving the data distribution; noted as unstable for LIME and computationally expensive for SHAP.

### Mechanism 3: Vision-Language Model Enhancement
- **Claim:** VLMs like CLIP can refine attributions and discover human-interpretable concepts by aligning visual features with semantic descriptions.
- **Mechanism:** VLMs provide cross-modal alignment where image embeddings are compared to text embeddings of class labels or concept descriptions; methods like CLIMS use this to enhance target regions and suppress backgrounds.
- **Core assumption:** The VLM's pre-trained knowledge generalizes to the target domain and text descriptions meaningfully correspond to visual features.
- **Evidence anchors:** Section 5.1 describes CLIMS leveraging CLIP for multimodal understanding; Section 5.3 discusses LLMs determining which concepts correctly describe targets.
- **Break condition:** Domain shift where VLM knowledge is insufficient, ambiguous or non-visual concepts generated by LLMs, or bias in concept generation.

## Foundational Learning

- **Concept: Linear Models & Interpretability**
  - **Why needed here:** Many XAI methods build on linear model intuition where weights equal importance; understanding collinearity and regularization helps assess validity of linear interpretations.
  - **Quick check question:** Given a linear model with correlated features, can you trust coefficient magnitudes for feature importance? Why or why not?

- **Concept: Gradient-Based Optimization**
  - **Why needed here:** Gradient-based XAI methods rely on backpropagation; understanding gradient flow, saturation, and vanishing gradients is essential for diagnosing failures.
  - **Quick check question:** Why might a ReLU network produce a zero-gradient for a feature that is actually important to the prediction?

- **Concept: Black-Box vs. White-Box Models**
  - **Why needed here:** Distinguishes when to use post-hoc vs. ante-hoc methods and informs the accuracy-interpretability tradeoff.
  - **Quick check question:** For a mission-critical healthcare application, would you prioritize a highly accurate black-box model with post-hoc explanations or an inherently interpretable model with lower accuracy? What factors inform your choice?

## Architecture Onboarding

- **Component map:** Input → Black-box model (CNN, Transformer) → Prediction → XAI module → Explanation (heatmap, text, concept scores). XAI modules include gradient-based, perturbation-based, attention-based, counterfactual, concept-based, and LLM/VLM-assisted methods. Human-in-the-loop feedback enables concept editing and prototype debugging.

- **Critical path:**
  1. Define explainability requirements: local vs. global, audience, modality.
  2. Assess model access: white-box (gradients, weights) vs. black-box (input-output only).
  3. Select XAI method family based on constraints.
  4. Evaluate faithfulness using metrics but remain cautious of limitations.
  5. Iterate with domain experts to validate semantic meaningfulness.

- **Design tradeoffs:**
  - Accuracy vs. interpretability: Inherent models trade accuracy for transparency; black-box models need post-hoc explanations.
  - Fidelity vs. stability: LIME is flexible but unstable; SHAP is more stable but computationally heavy.
  - Automation vs. human-in-the-loop: LLM/VLM concept discovery scales but may introduce noise; human annotation ensures quality but is costly.

- **Failure signatures:**
  - Noisy/unfaithful heatmaps: Gradient saturation, low-resolution CAMs, or partial activation.
  - Inconsistent explanations across runs: LIME instability due to random sampling.
  - Non-discriminative concepts: LLM-generated concepts may be redundant or non-visual.
  - Background activation: CAM methods highlight irrelevant regions.

- **First 3 experiments:**
  1. Apply Grad-CAM, Integrated Gradients, and SHAP to a pre-trained CNN (ResNet50) on ImageNet; compare heatmaps using faithfulness metrics and assess localization qualitatively.
  2. Run LIME 10 times on the same instance (image or tabular data); measure variance in feature importance rankings to identify consistency.
  3. Prompt GPT-3.5 to generate 50 descriptive concepts for "axolotl"; use CLIP to align concepts with images and build a concept bottleneck model; evaluate accuracy vs. standard classifier and inspect concept quality.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can reliable, objective metrics be established to quantitatively evaluate and compare the faithfulness of different XAI methods? The authors note it's challenging to establish reliable metrics beyond subjective user studies, as current methods rely on concept count which fails to measure semantic quality.

- **Open Question 2:** Can integrating VLMs with diffusion models effectively generate accurate saliency maps for small or inconspicuous targets in zero-shot manner? The paper notes VLMs struggle with coarse image-level annotations for accurate pixel-level localization, particularly for fine-grained or rare objects.

- **Open Question 3:** How can LLM-based embodied agents be leveraged to automate and align prompt design with human needs to improve LLM explainability in specialized domains? The authors suggest future prompt designs could leverage LLM-based embodied agents to understand human preferences and align goals for effective prompts.

## Limitations

- Gradient-based and perturbation-based mechanisms rely on unproven assumptions about gradient fidelity and feature independence with limited empirical validation.
- VLM/LLM-assisted methods assume cross-modal alignment generalizes across domains but face domain shift risks in specialized applications.
- Evaluation metrics like Insertion/Deletion, ROAR, and ROAD are imperfect proxies for true interpretability and may not capture semantic meaningfulness.

## Confidence

- **High:** Inherently interpretable models (decision trees, linear models) provide transparent reasoning through straightforward mathematical mechanisms.
- **Medium:** Gradient-based (Grad-CAM, Integrated Gradients) and perturbation-based (SHAP, LIME) methods have established frameworks but face practical limitations like gradient saturation and sampling instability.
- **Low:** VLM/LLM-assisted methods are emerging with promising results but lack robust validation and face domain generalization challenges.

## Next Checks

1. **Replicate stability test:** Run LIME 10+ times on the same tabular instance and measure variance in feature rankings to quantify explanation consistency.

2. **Intervene on concept bottlenecks:** Swap generated concepts (e.g., "red tail" → "blue tail") at test time and observe if predictions change as expected, detecting information leakage.

3. **Evaluate domain generalization:** Apply CLIP-based concept discovery to a medical imaging dataset and measure concept quality (visual relevance, CLIP similarity) versus ImageNet to assess domain shift impact.