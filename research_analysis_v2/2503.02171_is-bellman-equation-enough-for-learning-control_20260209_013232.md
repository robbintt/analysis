---
ver: rpa2
title: Is Bellman Equation Enough for Learning Control?
arxiv_id: '2503.02171'
source_url: https://arxiv.org/abs/2503.02171
tags:
- equation
- solution
- bellman
- control
- solutions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the non-uniqueness of solutions to Bellman/HJB
  equations in continuous state spaces, a critical challenge for value-based reinforcement
  learning methods. The authors prove that for linear systems, the Bellman equation
  admits at least (2n choose n) solutions, with only one yielding both optimality
  and closed-loop stability.
---

# Is Bellman Equation Enough for Learning Control?

## Quick Facts
- arXiv ID: 2503.02171
- Source URL: https://arxiv.org/abs/2503.02171
- Reference count: 40
- The Bellman equation admits (2n choose n) solutions in linear systems, but only one yields both optimality and closed-loop stability.

## Executive Summary
This paper addresses the non-uniqueness of solutions to Bellman/HJB equations in continuous state spaces, a critical challenge for value-based reinforcement learning methods. The authors prove that for linear systems, the Bellman equation admits at least (2n choose n) solutions, with only one yielding both optimality and closed-loop stability. Through spectral analysis of the Hamiltonian matrix, they show this exponential growth in solution space creates a fundamental obstacle for learning stable controllers. A key finding is that standard neural network architectures can converge to unstable solutions, as demonstrated on LQR and nonlinear control problems (cartpole and drone). To address this, the authors propose a positive-definite neural architecture that guarantees convergence to the stable solution by construction, leveraging the relationship between value functions and Lyapunov functions. Experiments show this architecture consistently learns stable controllers while generic MLPs fail, achieving lower cumulative costs and stable trajectories.

## Method Summary
The authors propose a positive-definite neural architecture for learning value functions that guarantees convergence to the stable solution of the Bellman/HJB equation. The architecture enforces V(x_eq) = 0 and V(x) > 0 for x ≠ x_eq through V_θ(x) = h_N^T h_N + ε||x - x_eq||², where h_k are hidden layer activations with σ(0) = 0. This construction ensures the learned value function serves as a Lyapunov function, guaranteeing closed-loop stability. The method is compared against standard MLP baselines on LQR, cartpole, and drone tracking tasks, with experiments showing the positive-definite architecture consistently learns stable controllers while MLPs often converge to unstable solutions despite low TD error.

## Key Results
- The Bellman equation admits at least (2n choose n) solutions for linear systems, with only one yielding stable closed-loop control
- Standard MLPs can converge to unstable solutions with low TD error, as demonstrated on LQR and nonlinear control problems
- The positive-definite architecture guarantees convergence to the stable solution by construction, achieving lower cumulative costs and stable trajectories
- The method outperforms prior continuous-time RL approaches, particularly on challenging drone tracking tasks

## Why This Works (Mechanism)

### Mechanism 1: Spectral Constraint from Invariant Subspace Selection
- Claim: The closed-loop system's eigenvalues are determined by which invariant subspace of the Hamiltonian matrix H is chosen to construct the value function.
- Mechanism: For linear systems, solutions P to the Algebraic Riccati Equation are constructed from n-dimensional invariant subspaces of H ∈ R^{2n×2n}. Since H has symmetric eigenvalue pairs (±λ), selecting subspaces associated with positive eigenvalues yields unstable closed-loop dynamics (A_cl = A - BR^{-1}B^T P shares those eigenvalues). Only the subspace spanned by all negative-eigenvalue eigenvectors yields stable control.
- Core assumption: The system (A, B) is controllable and (Q, A) is observable (ensures no eigenvalues on imaginary axis).
- Evidence anchors:
  - [abstract] "the Bellman equation admits at least (2n choose n) solutions, with only one yielding both optimality and closed-loop stability"
  - [section 3.2, Theorem 5] "the characteristic matrix of closed-loop system A_cl = (A - BR^{-1}B^T P) has eigenvalues Λ_1" where Λ_1 are the eigenvalues of the selected invariant subspace
  - [corpus] Weak direct corpus support for this specific spectral mechanism; related work (arxiv 2505.21842) addresses HJB solution methods but not this non-uniqueness structure
- Break condition: System lacks controllability or observability → Lemma 3 fails → eigenvalues may lie on imaginary axis, invalidating the clean spectral partition

### Mechanism 2: Positive-Definite Architecture as Lyapunov Function Constraint
- Claim: Constraining the value function to be positive-definite by construction guarantees convergence to the stable solution.
- Mechanism: The proposed architecture V_θ(x) = h_N^T h_N + ε||x - x_eq||²_2, where h_{k+1} = σ(θ_k h_k) and h_1 = x - x_eq, enforces V(x_eq) = 0 and V(x) > 0 for x ≠ x_eq. Per Theorem 10, under the optimal policy derived from this V, we have V̇ = ∂V^T f(x, π*(x)) = -l(x, π*(x)) ≤ 0, making V a valid Lyapunov function. Unstable general solutions typically violate positive-definiteness, so this constraint eliminates them from the representable hypothesis class.
- Core assumption: No discounting (τ → ∞) or sufficiently large discount factor; the equilibrium x_eq is the unique minimizer of the value function; running cost l(x, u) > 0 for all (x, u) ≠ (x_eq, u_eq).
- Evidence anchors:
  - [abstract] "positive-definite neural architecture that guarantees convergence to the stable solution by construction, leveraging the relationship between value functions and Lyapunov functions"
  - [section 5.2, Theorem 10] Formal proof that positive-definite solutions to HJB yield asymptotically stable closed-loop systems
  - [corpus] Physics-informed neural networks for HJB (arxiv 2505.21842) use architectural constraints for PDE solutions but don't address this specific stability mechanism
- Break condition: Discount factor too small (τ << 1) → myopic policies that don't guarantee long-term stability; limit cycle objectives rather than equilibrium stabilization → Theorem 10 doesn't apply directly

### Mechanism 3: Exponential Solution Imbalance and Initialization Sensitivity
- Claim: Minimizing TD error alone is insufficient to distinguish stable from unstable solutions because the solution space grows exponentially while only one solution is admissible.
- Mechanism: The solution space scales as C(2n, n) ~ O(4^n/√(πn)). TD error minimization only enforces the necessary condition (Bellman equation), not the sufficient condition (stability). Standard initializations (LeCun normal, Kaiming) bias convergence toward solutions with small eigenvalues—often the unstable negative-definite solutions in LQR settings.
- Core assumption: The learned function class is sufficiently expressive to represent multiple solutions; no explicit stability constraints in the loss function.
- Evidence anchors:
  - [abstract] "convergence to unstable solutions due to the exponential imbalance between admissible and inadmissible solutions"
  - [section 4, Figure 2] MLP with LeCun normal initialization converges to unstable solution; TD error < 10^-4 but trajectories diverge
  - [corpus] Corpus papers on HJB/Bellman solutions (arxiv 2504.10865, 2505.21842) don't address this specific failure mode or solution imbalance
- Break condition: Adding sufficient boundary conditions (if available and consistent) or architectural constraints (positive-definiteness) that eliminate unstable solutions from the representable class

## Foundational Learning

- Concept: Lyapunov Stability Theory
  - Why needed here: The positive-definite architecture exploits the fact that value functions satisfying the HJB equation naturally serve as Lyapunov functions; understanding V(x) > 0 and V̇ < 0 → asymptotic stability is essential for interpreting Theorem 10
  - Quick check question: Given a scalar function V(x) with V(0) = 0, V(x) > 0 for x ≠ 0, and V̇ = -x^2, can you prove that lim_{t→∞} x(t) = 0 for any initial condition?

- Concept: Algebraic Riccati Equations and Hamiltonian Matrices
  - Why needed here: For LQR problems, the HJB equation reduces to the ARE; understanding that ARE solutions correspond to invariant subspaces of the Hamiltonian matrix is critical for grasping the non-uniqueness result
  - Quick check question: For a 2D system with Hamiltonian eigenvalues {+2, -2, +1, -1}, how many solutions P exist to the ARE, and which eigenvalues characterize the stable closed-loop system?

- Concept: Invariant Subspaces and Eigendecomposition
  - Why needed here: The constructive proof that C(2n, n) solutions exist relies on selecting different n-dimensional invariant subspaces from the 2n-dimensional eigenspace of H
  - Quick check question: If H = VΛV^{-1} with eigenvalues {λ_1, λ_2, -λ_1, -λ_2}, which column combinations of V yield valid invariant subspaces, and how many distinct subspaces exist?

## Architecture Onboarding

- Component map:
  Input: x ∈ R^n → Shift: h_1 = x - x_eq → Hidden layers: h_{k+1} = σ(θ_k · h_k) for k = 1, ..., N_layers → Positive-definite head: V_θ(x) = h_N^T · h_N + ε · ||x - x_eq||²_2

- Critical path:
  1. **Activation zero-point**: σ(0) = 0 is critical—ensures h_k = 0 for all k when x = x_eq, guaranteeing V(x_eq) = 0
  2. **Quadratic fallback term**: ε||x - x_eq||²_2 > 0 for x ≠ x_eq ensures positivity even if h_N^T h_N = 0 (prevents representation collapse)
  3. **Differentiability**: Network must be fully differentiable for computing ∂V/∂x in policy derivation (u* = -½R^{-1}f_2^T ∂V/∂x)

- Design tradeoffs:
  - ε selection: Too small (ε < 10^-5) → weak positive-definiteness guarantee, possible numerical issues; Too large (ε > 0.1) → quadratic term dominates, h_N^T h_N contributes little learned structure
  - Network depth: Deeper networks increase representational capacity but may have vanishing h_N activations; the ε term provides a safety net
  - Activation choice: tanh provides smooth gradients everywhere; ELU with shifted bias (ELU(x) + 1) ensures σ(0) = 0; ReLU loses gradient information at 0

- Failure signatures:
  1. **Converged but unstable**: TD error < 10^-4 but trajectories diverge or cumulative cost grows → network converged to unstable solution; diagnostic: check closed-loop eigenvalues or trajectory plots
  2. **High seed variance**: Standard MLP shows 10x+ variance in cumulative cost across seeds → indication of converging to different solutions; positive-definite architecture should reduce this
  3. **Boundary condition conflicts**: Adding Dirichlet boundaries causes no solution found → inconsistent BCs per Section 5.1; diagnostic: verify BC values match stable solution

- First 3 experiments:
  1. **2D LQR ground truth validation**: Implement on A = B = Q = R = I_2 system from Section 4; compare learned P matrix eigenvalues against analytical solutions; verify convergence to stable P = (1+√2)I rather than unstable P = (1-√2)I
  2. **Ablation on positive-definiteness components**: Test three variants on cartpole: (a) full architecture with h_N^T h_N + ε||x-x_eq||², (b) only h_N^T h_N, (c) only ε||x-x_eq||²; measure TD convergence rate, cumulative cost, and trajectory stability
  3. **Seed sensitivity comparison**: Run 20 random seeds comparing standard MLP (3-layer, 128 neurons, LeCun normal init) vs positive-definite architecture on the drone tracking task; report mean and std of cumulative cost; expect MLP to show high variance and frequent divergence, constrained architecture to show consistent stable convergence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the positive-definite architecture be extended to control problems with limit cycle objectives (e.g., locomotion) rather than equilibrium stabilization?
- Basis in paper: [explicit] "The primary limitation of this architecture is its applicability mainly to control problems that demand system stability. In problems such as locomotion, the objective is often to achieve a limit cycle rather than converging to an equilibrium point."
- Why unresolved: The proposed architecture enforces V(x_eq) = 0 and V(x) > 0, which presupposes a single equilibrium target; limit cycles require fundamentally different value function structures.
- What evidence would resolve it: A modified architecture or initialization scheme that successfully learns stable limit cycle behaviors on locomotion benchmarks.

### Open Question 2
- Question: What mechanisms govern which Bellman solution neural networks converge to, and can initialization be designed to reliably select the stable solution?
- Basis in paper: [inferred] The paper observes that "the solution to which the network converges is highly sensitive to weight initialization" and hypothesizes "this may be due to the negative definite solution having the smallest eigenvalues among the possible solutions, which are closer to the initial weights generated by these self-normalized methods."
- Why unresolved: The hypothesis remains untested; the spectral properties linking initialization distributions to specific solutions are not characterized.
- What evidence would resolve it: Theoretical analysis or systematic experiments mapping initialization schemes to convergence outcomes across solution classes.

### Open Question 3
- Question: Can hybrid policy-value methods avoid the exponential solution selection problem while retaining sample efficiency?
- Basis in paper: [explicit] The conclusion suggests: "Another alternative is to switch to policy-based approaches... These methods do not face the challenges of distinguishing solutions that value-based approaches encounter."
- Why unresolved: Policy-based methods avoid solution multiplicity but sacrifice offline learning benefits; whether hybrid approaches can combine advantages remains unexplored.
- What evidence would resolve it: Comparative analysis of hybrid algorithms (e.g., actor-critic variants with constrained critics) on the same control benchmarks.

## Limitations
- The spectral analysis of Hamiltonian matrices is rigorously proven only for linear LQR systems, with less rigorous treatment for nonlinear systems
- Experiments focus on three control benchmarks (LQR, cartpole, drone) without demonstrating failure modes on more complex systems
- The positive-definite architecture's guarantees assume no discounting or sufficiently large discount factors, with limited exploration of small discount factors

## Confidence
- **High confidence**: The spectral analysis of Hamiltonian matrices for linear systems is mathematically rigorous; the claim that C(2n,n) solutions exist and that only one yields stable control is well-supported
- **Medium confidence**: The claim that standard MLPs converge to unstable solutions in practice is supported by LQR experiments, but the mechanism may be initialization-dependent
- **Low confidence**: The generalization of these results to arbitrary nonlinear systems is asserted but not rigorously proven; nonlinear HJB equations may have different structural properties

## Next Checks
1. **Solution space characterization for nonlinear systems**: Implement the positive-definite architecture on a nonlinear system (e.g., Van der Pol oscillator or pendulum with large amplitudes) and systematically vary initialization to determine if multiple stable-looking solutions exist. Compare against grid search in function space to map the solution landscape.

2. **Discount factor sensitivity analysis**: Re-run LQR experiments with varying discount factors (τ = 0.1, 0.5, 0.9, 0.99, 1.0) to quantify how quickly myopic policies emerge and whether the positive-definite constraint remains beneficial under strong discounting.

3. **Alternative stabilization strategies comparison**: Implement and compare three approaches on the drone tracking task: (a) standard MLP with TD loss only, (b) standard MLP with explicit stability regularization (e.g., penalizing eigenvalues of closed-loop system), and (c) positive-definite architecture. Measure cumulative cost, convergence stability, and sensitivity to hyperparameters.