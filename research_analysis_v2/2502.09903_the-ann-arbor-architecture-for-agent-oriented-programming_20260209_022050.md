---
ver: rpa2
title: The Ann Arbor Architecture for Agent-Oriented Programming
arxiv_id: '2502.09903'
source_url: https://arxiv.org/abs/2502.09903
tags:
- agent
- language
- agents
- memory
- email
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper argues that large language models should be programmed
  in the same languages they accept, prompting a rethinking of software engineering.
  It introduces the Ann Arbor Architecture for agent-oriented programming as a higher-level
  abstraction over raw token generation, enabling in-context learning through persistent
  agent memory.
---

# The Ann Arbor Architecture for Agent-Oriented Programming

## Quick Facts
- arXiv ID: 2502.09903
- Source URL: https://arxiv.org/abs/2502.09903
- Authors: Wei Dong
- Reference count: 26
- One-line primary result: Agents initialized with static knowledge documents fail to follow protocols, while episodic memory through message journals enables in-context learning.

## Executive Summary
This paper introduces the Ann Arbor Architecture for agent-oriented programming, arguing that large language models should be programmed in the same natural languages they understand. The architecture uses email-based messaging (MBox format) where agents communicate via email addresses, execute shell commands, manage memory through Memory Segment Rewrite (MSR), and support dynamic in-context learning through conversation history. A prototype platform called Postline was developed to demonstrate these concepts, with experiments showing successful shell integration, code generation, binary data handling, and memory management via message-based communication.

## Method Summary
The method involves creating agents on-demand when messages are sent to new addresses @agents.localdomain, with memory stored as a dual representation: an append-only journal (Kafka) and an inference context (MinIO/S3). Agents communicate through email messages, execute shell commands via JSON to shell@localdomain, and manage memory through MSR operations sent to system@localdomain. The MSR primitive allows agents to rewrite context segments while preserving the full history in the journal. Experiments were run on gpt-4o to demonstrate shell integration, code generation, binary data handling, agent cloning, and MSR-based memory pruning.

## Key Results
- Shell robot integration successfully enabled agents to generate valid JSON commands and interpret stdout/stderr attachments
- Memory pruning via MSR reduced token counts from 17,727 to 16,919 while maintaining behavioral competence
- Static initialization documents ("Bible" approach) failed to reproduce working agents, demonstrating the necessity of episodic memory

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Persistent message journals enable in-context learning that static initialization documents cannot replicate.
- Mechanism: The agent's journal preserves the temporal sequence of task attempts, corrections, and explanations. This episodic structure allows the LLM to access not just factual knowledge but the *process* of skill acquisition—mirroring how humans learn under mentor guidance.
- Core assumption: LLMs generalize more effectively from demonstration trajectories than from condensed propositional knowledge. Assumption: The sequence and context of corrections carry information lost by summarization.
- Evidence anchors:
  - [Section 4.6] "Agents initialized with the Bible invariably failed to reproduce the expected behaviors... The journal is not merely a static repository of knowledge but an active record of evolving interactions."
  - [Section 1.3] "Effective training should involve opportunities for an AI model to make mistakes, receive feedback with explanations of why, and adjust its behavior accordingly – mirroring how humans refine their skills under the guidance of a mentor."
  - [corpus] Related work on promptware engineering (FMR 0.58) acknowledges prompts as programming interfaces but does not test episodic vs. static memory.
- Break condition: If context windows grow sufficiently large that full histories fit without compression, or if model training incorporates multi-turn interaction data directly, the distinction between episodic and condensed memory may diminish.

### Mechanism 2
- Claim: Modeling agent communication as email messages provides a universal protocol that LLMs already understand from training data exposure.
- Mechanism: Email/MBox format is text-based, well-represented in web corpora (forums, archives), and supports binary attachments via standard encodings. The LLM leverages its pre-existing knowledge of email conventions to parse and generate structured communications without additional training.
- Core assumption: LLMs have sufficient exposure to email-like formats in pre-training to fluently interpret and generate valid MBox structures. Assumption: Format familiarity transfers to correct protocol adherence.
- Evidence anchors:
  - [Section 2.2] "The MBox format is very likely to be well-represented within the training data of today's language models... This widespread exposure allows language models to more naturally understand and interact with messages stored in this format."
  - [Section 4.3] Successful generation of MIME multipart emails with base64-encoded image attachments after corrective feedback.
  - [corpus] No direct corpus evidence on MBox specifically; related work focuses on programming language translation, not message formats.
- Break condition: If task-specific protocols require domain knowledge absent from pre-training data, or if format complexity exceeds model's generalization capacity, the email abstraction may require explicit instruction or fine-tuning.

### Mechanism 3
- Claim: Memory Segment Rewrite (MSR) enables agents to modify their own context, reducing inference cost while preserving behavioral consistency.
- Mechanism: MSR allows an agent to replace a range of messages in its context with a summary or placeholder. The journal retains the full history for auditability, but the context (submitted for inference) reflects only the rewritten version. This functions as a form of self-directed prompt compression.
- Core assumption: Agents can reliably identify which memory segments are redundant without external guidance. Assumption: Summarization does not discard information critical for future tasks.
- Evidence anchors:
  - [Section 2.4] "Model inference is a special case of MSR, as it is equivalent to rewriting the empty segment at the end of the memory. Because the memory is the agent's program, MSR is a primitive that enables self-modifying programs."
  - [Section 4.5] Agent successfully identified and removed a binary data segment (X-Serial 29-35), reducing token count from 17,727 to 16,919.
  - [corpus] No corpus evidence on MSR specifically; related agent architectures focus on task decomposition rather than memory self-modification.
- Break condition: If agents prune memories too aggressively or incorrectly identify critical episodes as redundant, performance may degrade in ways difficult to diagnose.

## Foundational Learning

- Concept: **Automata Theory Basics (DFA/Turing Machines)**
  - Why needed here: The paper's theoretical foundation equates LLMs with automata that accept and generate tokens. Understanding states, transitions, and accepted languages clarifies why the authors argue for unified natural/formal language programming.
  - Quick check question: Can you explain why a language model generating tokens can be viewed as a finite automaton with hidden states?

- Concept: **Email/MBox Format Structure**
  - Why needed here: The entire architecture uses email as the primitive data structure. Engineers must understand headers, multipart MIME, and the "From " separator to debug agent communications.
  - Quick check question: What distinguishes the "From " envelope header from the "From:" header field in MBox format?

- Concept: **In-Context Learning vs. Fine-Tuning**
  - Why needed here: The paper redefines in-context learning as dynamic multi-turn training rather than few-shot prompting. Understanding this distinction is essential for correctly interpreting the Bible failure and the MSR approach.
  - Quick check question: Why does the paper argue that few-shot examples in prompts are insufficient for the kind of in-context learning it advocates?

## Architecture Onboarding

- Component map:
  - Agents (agents.localdomain) -> Realm Servers (process-level workers) -> Lock Servers (Redis for coordination) -> LLM API
  - Agents (agents.localdomain) -> Shell Robot (shell@localdomain) -> External tools
  - Agents (agents.localdomain) -> System (system@localdomain) -> MSR operations
  - Journal (Kafka) -> Context (MinIO/S3) -> Memory management

- Critical path:
  1. Message arrives addressed to agent
  2. Realm server acquires context lock; loads context if not in memory
  3. Context (MBox with X-Serial headers) sent to LLM API
  4. LLM generates response or MSR command
  5. If MSR: system modifies context, logs to journal, confirms to agent
  6. If response: message delivered, logged to journal

- Design tradeoffs:
  - **Dual representation (journal vs. context)**: Enables auditability and replay but requires synchronization logic
  - **Email as universal protocol**: Leverages LLM pre-training but may be verbose compared to binary protocols
  - **No automatic memory management**: Simplifies initial implementation but shifts burden to agents/users as journals grow

- Failure signatures:
  - **Protocol drift**: Agent generates malformed JSON for robots (observed in Bible-initialized agents)
  - **Memory bloat**: Token counts inflate; inference costs rise; attention may degrade
  - **Realm lock contention**: Frequent context switching between realms causes latency spikes
  - **Clone explosions**: Recursive agent creation without resource limits can overwhelm system

- First 3 experiments:
  1. **Shell robot integration**: Send a natural language request to an agent; verify it generates valid JSON commands for shell@localdomain and correctly interprets stdout/stderr attachments. (Section 4.1)
  2. **Memory pruning with MSR**: Instruct an agent to identify and remove a redundant memory segment; confirm token count reduction and retained behavioral competence. (Section 4.5)
  3. **Bible initialization failure test**: Create a condensed initialization document from a trained agent's journal; initialize a new agent with it; compare protocol adherence against the original. Expect degradation. (Section 4.6)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can an episodic memory system be designed to allow agents to autonomously manage context boundaries and retrieval?
- Basis in paper: [explicit] The conclusion identifies the need for mechanisms where agents "autonomously create episode boundaries," swap out older episodes via Memory Segment Rewrites (MSR), and retrieve out-of-core episodes.
- Why unresolved: The current Postline prototype assumes context size limits will expand and lacks automatic memory management or a higher-level structure for memory beyond the flat MBox journal.
- What evidence would resolve it: A system demonstration where an agent successfully summarizes and archives old interactions to stay within a context window, then accurately retrieves those memories when required by a new task.

### Open Question 2
- Question: What safety protocols and resource constraints are necessary to prevent runaway resource consumption during recursive agent cloning?
- Basis in paper: [explicit] Section 4.4 notes that while recursive cloning is possible, "stricter behavior and resource control are needed to make such flexibility safely applicable without accidentally overwhelming the system."
- Why unresolved: The architecture allows agents to create new agents dynamically, but the current implementation lacks the necessary guardrails to prevent a chain of recursive actions from degrading system performance.
- What evidence would resolve it: The successful execution of complex, multi-agent recursive tasks without system instability or resource exhaustion.

### Open Question 3
- Question: Can agent behavior be reliably transferred or initialized via static documents, or is the specific sequence of episodic interactions strictly necessary?
- Basis in paper: [inferred] Section 4.6 details the failure of the "Bible" approach, where condensing knowledge into a static document failed to reproduce behaviors. This leaves open the question of whether any equivalent static representation exists or if training must always be dynamic.
- Why unresolved: The authors found that static prompts lack the stability of formal code; however, relying solely on ever-growing dynamic journals may become computationally infeasible, creating a tension between efficiency and behavioral fidelity.
- What evidence would resolve it: The discovery of a compression or summarization technique that reduces memory size while preserving the protocol adherence and "personality" developed in the original dynamic journal.

## Limitations
- The Bible initialization failure, while demonstrated for one agent, needs broader testing across different agent types and domains to establish universality
- The dual memory representation (journal vs. context) introduces complexity that could lead to consistency issues if not carefully managed
- Reliance on email as a universal protocol may impose performance overhead compared to binary protocols and could face scalability challenges

## Confidence
- **High confidence**: The core claim that LLMs can be programmed in natural languages they understand, and that persistent message journals enable in-context learning through episodic memory
- **Medium confidence**: The assertion that condensed initialization documents cannot replace episodic learning; needs broader domain testing
- **Medium confidence**: The claim that email/MBox format provides a universal protocol LLMs can naturally interpret; evidence based on single test

## Next Checks
1. **Multi-domain Bible testing**: Create condensed initialization documents from agents trained in different domains (e.g., code generation, natural language reasoning, mathematical problem-solving) and measure protocol adherence degradation
2. **Memory retention validation**: After multiple MSR operations that reduce token count by 20-30%, measure whether agent performance on previously solved tasks degrades
3. **Protocol fluency benchmarking**: Compare agent performance on JSON command generation and email attachment handling when initialized with Bible documents versus trained with episodic memory across multiple agent instantiations