---
ver: rpa2
title: 'Beyond Imitation: Recovering Dense Rewards from Demonstrations'
arxiv_id: '2510.02493'
source_url: https://arxiv.org/abs/2510.02493
tags:
- reward
- learning
- policy
- token
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work establishes a formal equivalence between supervised fine-tuning
  (SFT) and inverse reinforcement learning (IRL), showing that SFT implicitly learns
  a dense, token-level reward model. Leveraging this insight, we recover the reward
  signal from SFT logits using a baseline-relative shaping approach and use it to
  improve the policy via token-level REINFORCE.
---

# Beyond Imitation: Recovering Dense Rewards from Demonstrations

## Quick Facts
- arXiv ID: 2510.02493
- Source URL: https://arxiv.org/abs/2510.02493
- Authors: Jiangnan Li; Thuy-Trang Vu; Ehsan Abbasnejad; Gholamreza Haffari
- Reference count: 20
- Key outcome: This work establishes a formal equivalence between supervised fine-tuning (SFT) and inverse reinforcement learning (IRL), showing that SFT implicitly learns a dense, token-level reward model. Leveraging this insight, we recover the reward signal from SFT logits using a baseline-relative shaping approach and use it to improve the policy via token-level REINFORCE. Evaluated across four pretrained backbones and four instruction-following benchmarks, the resulting Dense-Path REINFORCE method consistently outperforms the original SFT models in win rate and standardized scores, demonstrating that LfD can be enhanced through reward learning without requiring additional demonstrations or preference data.

## Executive Summary
This paper bridges supervised fine-tuning (SFT) and inverse reinforcement learning (IRL) by proving that SFT on token sequences is mathematically equivalent to inverse Q-learning under deterministic, undiscounted settings. This equivalence reveals that SFT implicitly learns a dense, token-level reward signal—specifically, the log-probabilities of expert tokens. The authors recover this implicit reward and apply a baseline-relative shaping technique to remove length bias and stabilize training. Using token-level REINFORCE with this recovered reward, they improve instruction-following models beyond their SFT baselines across multiple architectures and benchmarks.

## Method Summary
The method consists of three stages: (1) train an SFT model on 100k expert instruction-response pairs, saving a mid-training checkpoint as a reference; (2) compute a baseline-relative reward as the difference between log-probabilities under the final SFT and reference checkpoints for each token; (3) optimize the policy via token-level REINFORCE with undiscounted returns, using the recovered reward as the signal. The approach avoids learned critics and preference data, relying instead on the implicit reward embedded in SFT logits.

## Key Results
- Dense-Path REINFORCE consistently outperforms SFT baselines on AlpacaEval, Arena-Hard, LIMA, and MT-Bench across four pretrained backbones
- Token-level returns provide denser credit assignment than EOS-only sparse returns, yielding systematic win-rate gains
- The baseline-relative reward construction removes length bias and improves stability compared to raw log-probabilities
- Performance peaks when the baseline checkpoint is at ~50% training progress, balancing reward magnitude and discriminativeness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SFT is mathematically equivalent to inverse Q-learning on the token MDP, meaning it implicitly learns a reward function—not just a policy.
- Mechanism: The IQ-Learn reduced objective J*(Q) aggregates soft-advantage terms Q(s,a)−V(f(s,a)) along expert trajectories. On a deterministic token tree with γ=1, the V-terms telescope, and using logπ_Q = Q−V, this collapses to the SFT log-likelihood E[logπ_Q(a|s)]. Thus optimizing SFT is equivalent to optimizing the IQ-Learn Q-function.
- Core assumption: The environment is a finite-horizon token MDP with deterministic transitions f(s,a)=s∥a and discount γ=1. The reward regularizer ψ is convex (linear conjugate case in the equivalence).
- Evidence anchors:
  - [abstract] "We prove that the SFT objective is a special case of Inverse Q-Learning, which implies that the SFT process does not just learn a policy, but also an implicit, dense, token-level reward model."
  - [Section 4.1] "maximizing J*(Q) is equivalent to maximizing the teacher-forced log-likelihood on expert tokens"
  - [corpus] Limited direct corroboration; corpus papers focus on IRL/reward learning but do not explicitly replicate SFT↔IQ-Learn equivalence at the token level.
- Break condition: If discount γ<1 or if transitions are stochastic (breaking the telescoping collapse), the exact equivalence does not hold. Regularizer structure also matters: non-linear conjugates introduce additional terms.

### Mechanism 2
- Claim: SFT logits are a potential-based shaped reward, preserving optimal policies while providing dense per-token credit.
- Mechanism: The soft Bellman identity with deterministic transitions gives logπ_SFT(a_t|s_t) = r(s_t,a_t) + (V(s_{t+1})−V(s_t)). The second term is potential-based shaping, so logπ_SFT differs from r by a state-only potential. This makes logπ_SFT a valid dense reward yielding identical policy gradients (up to a state-dependent baseline that integrates to zero in expectation).
- Core assumption: γ=1 and deterministic transitions; finite-horizon with V(s_H)=0.
- Evidence anchors:
  - [Section 4.3.A] "logπ_SFT(a_t|s_t) = r(s_t, a_t) + (V_{SFT}(s_{t+1})−V_{SFT}(s_t)), so logπ_SFT is a shaped version of the task reward and shares the same optimal policies"
  - [Appendix A.8] Policy gradient under logπ_SFT equals that under r, up to a state-only baseline.
  - [corpus] Related work on reward shaping and IRL (e.g., MaxEnt IRL formulations) supports shaping invariance, but direct replication of this logit-as-shaped-reward construction is not reported.
- Break condition: If a critic-based method like PPO is used, the position-dependent baseline V(s_t) creates heteroscedastic return targets that are difficult to fit, destabilizing training.

### Mechanism 3
- Claim: A baseline-relative reward br(s,a)=logπ_SFT−logπ_ref removes length bias and stabilizes updates.
- Mechanism: Raw logπ_SFT is non-positive, favoring short sequences (EOS pathology). Subtracting a reference logπ_ref (an SFT checkpoint at ~50% training) cancels length bias, measures incremental competence, and empirically reduces variance. A return-shift bound shows |G̃_SFT−G̃_ref| ≤ ‖b_V‖_∞, with small b_V when π_ref lies on the SFT trajectory.
- Core assumption: π_ref is sufficiently close to π_SFT in value space so that b_V = V_SFT−V_ref has small ℓ∞ norm.
- Evidence anchors:
  - [Section 4.3.C] "br(s,a)=logπ_SFT(a_t|s_t)−logπ_ref(a_t|s_t) ... cancels length bias, measures incremental competence, and empirically reduces variance"
  - [Appendix A.9] Bounding argument for dynamic range reduction via checkpoint baseline.
  - [corpus] No direct replication found; corpus does not address baseline-relative log-likelihood rewards.
- Break condition: If the baseline is too early (incompetent) or too late (nearly identical to π_SFT), the signal magnitude or discriminativeness degrades, harming performance (see sensitivity analysis in Figure 3).

## Foundational Learning
- Concept: Soft optimality and Boltzmann policies in entropy-regularized RL
  - Why needed here: The paper uses soft Bellman equations V(s)=log∑_a exp Q(s,a) and π_Q(a|s)∝exp Q(s,a) as the foundation for both IQ-Learn reduction and reward recovery.
  - Quick check question: Given a Q-function, can you derive the corresponding soft value and Boltzmann policy?
- Concept: Occupancy measures and the IRL saddle-point formulation
  - Why needed here: The dual contraction theorem relates reward error to occupancy error via the convex-analytic IRL objective L(π,r)=⟨ρ_E−ρ_π, r⟩−H(π)−ψ(r).
  - Quick check question: What is the occupancy measure ρ^π(s,a), and how does the IRL saddle condition imply ρ_{π*}=ρ_E?
- Concept: Potential-based reward shaping invariance
  - Why needed here: The paper relies on shaping invariance to justify using logπ_SFT as a proxy reward without changing optimal policies.
  - Quick check question: Why does adding F(s′)−F(s) to the reward preserve the optimal policy under soft optimality?

## Architecture Onboarding
- Component map: SFT stage -> Reward extractor -> REINFORCE optimizer
- Critical path: (1) Train SFT → save midway checkpoint; (2) freeze π_SFT, π_ref; (3) roll out with current π_ϕ; (4) compute br and G_t; (5) update π_ϕ via REINFORCE. Errors in reward extraction (e.g., incorrect log-softmax) propagate directly into policy updates.
- Design tradeoffs:
  - Token-level vs sentence-level returns: Token-level provides denser credit assignment; the paper shows systematic gains vs EOS-only sparse returns (Table 1).
  - With vs without potential term V: Eliminating V (via shaping) avoids position-dependent return shifts; ablation shows +2–7 win-rate points vs w/V (Table 2).
  - Baseline selection: Mid-training checkpoint (~50%) balances magnitude and discriminativeness (Figure 3).
- Failure signatures:
  - Win rates degrade if baseline omitted (EOS/length pathology; Table 2 shows 10–15 pt drops).
  - Performance declines at γ<1 (Figure 2): early tokens over-weighted, late-token guidance weakened.
  - Using a critic (PPO-style) on shaped rewards is unstable due to heteroscedastic targets from the potential term.
- First 3 experiments:
  1. Replicate SFT↔IQ-Learn equivalence on a small language MDP: verify that gradients of log-likelihood match gradients of J*(Q) under γ=1.
  2. Ablate the baseline: compare br vs raw logπ_SFT as reward on a held-out validation set; measure length distribution shift and win rate.
  3. Sweep baseline checkpoint position (0.2–0.8 training progress) to validate the unimodal performance curve and correlate with ‖V_SFT−V_ref‖_∞.

## Open Questions the Paper Calls Out
- The paper does not explicitly call out open questions in the provided text.

## Limitations
- The theoretical equivalence between SFT and inverse Q-learning relies on deterministic transitions and γ=1; relaxing either condition introduces mismatch between the claimed and actual learned objectives.
- The claim that "no preference data" is needed is technically true but assumes access to a large expert demonstration set; on smaller datasets, the implicit reward may be too weak to drive meaningful improvement.
- The method's gains come from tuning an already strong pretrained backbone, raising questions about applicability to weaker initial policies or more challenging domains.

## Confidence
**High confidence:** The SFT→IQ-Learn reduction proof for deterministic, undiscounted token MDPs is mathematically sound and the empirical win-rate improvements over SFT baselines are robust across backbones and benchmarks. The shaping-invariance argument for using logπ_SFT as reward is standard and well-supported.

**Medium confidence:** The baseline-relative reward construction is well-motivated and improves empirical stability, but the exact sensitivity to checkpoint choice and the robustness of the small-ℓ∞ bound are not fully characterized. The claim of not requiring additional demonstrations is true given the 100k expert set, but the minimum dataset size for reliable reward extraction is unclear.

**Low confidence:** The assertion that "no preference data" is needed glosses over the fact that 100k expert demonstrations are still required; the implicit reward may not scale down to smaller datasets. The paper does not explore failure modes in stochastic or continuous action spaces, limiting generalizability.

## Next Checks
1. **Equivalence under perturbation:** Test the SFT↔IQ-Learn equivalence with γ<1 or stochastic transitions; measure deviation in gradients and policy outcomes.
2. **Baseline sensitivity:** Systematically sweep baseline checkpoints (0.1 to 0.9 of training) and correlate performance with ‖V_SFT−V_ref‖_∞ to validate the unimodal peak at ~50%.
3. **Dataset scaling:** Replicate the method on subsets of the expert data (10k, 30k, 50k samples) to identify the minimum size for reliable reward extraction and policy improvement.