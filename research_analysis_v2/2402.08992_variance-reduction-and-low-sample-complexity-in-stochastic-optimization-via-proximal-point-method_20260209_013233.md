---
ver: rpa2
title: Variance Reduction and Low Sample Complexity in Stochastic Optimization via
  Proximal Point Method
arxiv_id: '2402.08992'
source_url: https://arxiv.org/abs/2402.08992
tags:
- algorithm
- inequality
- stochastic
- follows
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of obtaining high-probability guarantees
  in stochastic convex composite optimization, which typically requires strong noise
  assumptions such as sub-Gaussian tails. The proposed stochastic proximal point method
  (SPPM) achieves such guarantees under only bounded variance assumptions by combining
  a proximal subproblem solver (PSS) with a probability booster (PB).
---

# Variance Reduction and Low Sample Complexity in Stochastic Optimization via Proximal Point Method

## Quick Facts
- arXiv ID: 2402.08992
- Source URL: https://arxiv.org/abs/2402.08992
- Authors: Jiaming Liang
- Reference count: 40
- One-line primary result: SPPM achieves high-probability convergence under bounded variance assumptions via proximal variance reduction and probability boosting, with sample complexity O(log(1/p) · log(1/ε))

## Executive Summary
This paper addresses the challenge of obtaining high-probability guarantees in stochastic convex composite optimization without requiring strong noise assumptions like sub-Gaussian tails. The proposed Stochastic Proximal Point Method (SPPM) combines a Proximal Subproblem Solver (PSS) that reduces variance through proximal regularization, with a Probability Booster (PB) that amplifies low per-iteration success probability to high confidence using Second Tertile Selection. The key innovation is achieving variance reduction (factor of I) without mini-batching, enabling high-probability bounds under only bounded variance assumptions.

## Method Summary
The method employs a three-level hierarchy: (1) PSS solves proximal subproblems using I iterations with momentum parameter α, reducing variance by a factor of I; (2) PB selects among n PSS outputs using STS and RGE to identify "good" candidates; (3) SPPM runs K outer iterations with fixed prox stepsize λ. The algorithm achieves high-probability guarantees with sample complexity O(log(1/p) · log(1/ε)), where p is failure probability and ε is target accuracy. Key parameters include K=O(log 1/ε), I=O(max{κ log(κ/ε), κσ²/(µε)}), and n=O(log 1/p).

## Key Results
- Achieves high-probability convergence guarantees under only bounded variance assumptions
- Sample complexity scales as O(log(1/p) · log(1/ε)), improving over methods requiring sub-Gaussian noise
- Variance reduction achieved through proximal regularization without relying on mini-batching
- Fixed prox stepsize λ prevents error accumulation while maintaining contraction

## Why This Works (Mechanism)

### Mechanism 1: Iterative Variance Dampening in Proximal Subproblem Solver (PSS)
The PSS oracle reduces variance term in expected primal gap by factor I through sequential composite updates rather than gradient averaging. Stability of proximal operator combined with recursive averaging filters out gradient noise over I steps, averaging variance to λσ²/I. Core assumption: stochastic gradient has bounded variance σ² and function is strongly convex and smooth.

### Mechanism 2: Probability Amplification via Second Tertile Selection (PB)
PB amplifies constant per-iteration success probability (3/4) to high confidence (1-p) using only logarithmic samples. Runs n independent PSS instances and uses STS to identify candidates clustering closely in Euclidean space and Bregman divergence, statistically filtering out "bad" random seeds.

### Mechanism 3: Contraction via Fixed Proximal Stepsize
Constant proximal stepsize λ prevents error accumulation across outer iterations while maintaining convergence. Strong convexity ensures distance to optimum contracts (θ-contraction) even with inexact solutions. Error term δ is constant but bounded, allowing geometric contraction to dominate.

## Foundational Learning

- **Concept: Proximal Point Method (PPM)**
  - Why needed here: Outer loop architecture solving regularized subproblems φ(x) + (1/2λ)||x-z̄||² rather than direct gradient steps
  - Quick check question: How does adding squared Euclidean distance term in subproblem improve stability compared to raw stochastic gradient step?

- **Concept: Bounded Variance vs. Sub-Gaussian Noise**
  - Why needed here: Paper's value proposition is achieving high-probability bounds under only bounded variance, making PB oracle necessary
  - Quick check question: Why does assuming only bounded variance make high-probability guarantees harder compared to sub-Gaussian assumptions?

- **Concept: Bregman Divergence**
  - Why needed here: PB uses metric d_h based on Bregman divergence to select candidates, accounting for geometry of composite function h
  - Quick check question: Why is metric d_h used instead of Euclidean distance when selecting final candidate in PB?

## Architecture Onboarding

- **Component map:** SPPM (Outer Loop) -> PSS (Inner Solver) -> PB (Selector) -> RGE (Gradient Estimator)
- **Critical path:** Initialize with z̄₀ → Generate: Call PSS n times → Filter (PB): Apply STS three times → Update: Set best candidate as new prox-center z̄_{k+1}
- **Design tradeoffs:** Constant λ vs. decaying λ prevents stagnation but requires λµ ≥ 3; Complexity vs. Confidence increasing n increases sample cost but decreases failure probability; Inner iterations I linearly increases sample cost but reduces error tolerance ε
- **Failure signatures:** Stalling if α misconfigured or I too small; Empty Selection if n too small relative to noise level; Drift if λµ < 3 causing cumulative error to dominate
- **First 3 experiments:** 1) Sanity Check on synthetic strongly convex quadratic with Gaussian noise; 2) Ablation on noise comparing SPPM vs vanilla SGD with heavy-tailed noise; 3) Parameter Sensitivity testing λ grid search given fixed µ

## Open Questions the Paper Calls Out

- **Can an accelerated variant of SPPM reduce the condition number dependence from κ to √κ?**
  - Basis: Section 7 asks whether restarted acceleration scheme could reduce κ dependence from linear to √κ
  - Why unresolved: Acceleration amplifies noise in stochastic optimization, making analysis non-trivial
  - What evidence would resolve it: Convergence proof with sample complexity O(√κ)

- **Can the algorithm be modified to adapt automatically to problem parameters without prior knowledge of strong convexity constant µ?**
  - Basis: Section 7 identifies removal of λµ ≥ 3 assumption as important extension
  - Why unresolved: Current guarantees rely on specific parameter settings assuming knowledge of µ
  - What evidence would resolve it: Modified SPPM maintaining high-probability convergence without requiring µ as input

- **Would implementing a variable prox stepsize λ_k improve the flexibility or theoretical guarantees?**
  - Basis: Section 7 suggests exploring variants with variable λ_k to expand framework flexibility
  - Why unresolved: Current analysis restricted to constant λ, impact of dynamic stepsizes unexplored
  - What evidence would resolve it: Analysis showing improved convergence rates or relaxed assumptions

## Limitations

- Computational overhead from O(log(1/p)) independent PSS calls per outer iteration may be prohibitive for practical applications
- Reliance on precise parameter tuning (λ ≥ 3/µ and α ≥ (I/2 + λL)/(1 + I/2 + λL)) creates potential brittleness
- STS-based probability booster requires multiple passes over samples and complex selection criteria, may not translate efficiently to parallel computing

## Confidence

- **High Confidence:** Variance reduction mechanism in PSS (1/I factor) and contraction property with constant λ are well-supported by theoretical analysis
- **Medium Confidence:** Probability amplification via STS is theoretically sound but relies on assumption that >2/3 of PSS outputs are "good"
- **Low Confidence:** Practical efficiency of three-level hierarchical structure compared to simpler alternatives not empirically demonstrated

## Next Checks

1. **Parameter Sensitivity Analysis:** Systematically vary λ, α, and I across multiple problem instances to identify robust operating regime and quantify impact of parameter misconfiguration

2. **STS Effectiveness Test:** Design experiments with controlled noise distributions to empirically measure actual probability of selecting "good" candidates through STS, comparing theoretical assumptions with observed performance

3. **Scaling Study:** Compare SPPM against baseline methods (SGD, SVRG) on problems with varying noise levels, measuring both sample complexity and wall-clock time to verify claimed O(log(1/p)·log(1/ε)) scaling