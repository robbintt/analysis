---
ver: rpa2
title: Demystifying the Paradox of Importance Sampling with an Estimated History-Dependent
  Behavior Policy in Off-Policy Evaluation
arxiv_id: '2505.22492'
source_url: https://arxiv.org/abs/2505.22492
tags:
- policy
- behavior
- learning
- estimator
- estimated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes how estimating history-dependent behavior policies
  affects off-policy evaluation (OPE) accuracy in reinforcement learning. It proves
  that while such estimation decreases asymptotic variance and improves large-sample
  accuracy, it introduces finite-sample bias that can be significant in small samples
  or long horizons.
---

# Demystifying the Paradox of Importance Sampling with an Estimated History-Dependent Behavior Policy in Off-Policy Evaluation

## Quick Facts
- arXiv ID: 2505.22492
- Source URL: https://arxiv.org/abs/2505.22492
- Authors: Hongyi Zhou; Josiah P. Hanna; Jin Zhu; Ying Yang; Chengchun Shi
- Reference count: 40
- Primary result: History-dependent behavior policy estimation reduces asymptotic variance in OPE but introduces finite-sample bias that grows exponentially with horizon.

## Executive Summary
This paper analyzes how estimating history-dependent behavior policies affects off-policy evaluation accuracy in reinforcement learning. The authors prove that while such estimation decreases asymptotic variance and improves large-sample accuracy, it introduces finite-sample bias that can be significant in small samples or long horizons. Through theoretical analysis and experiments on Cartpole and MuJoCo environments, they demonstrate a fundamental bias-variance tradeoff that holds across various OPE estimators.

## Method Summary
The paper investigates off-policy evaluation using history-dependent behavior policy estimation. For each timestep, the method conditions the estimated behavior policy on a history window $H_{t-k:t}$ of the previous $k$ steps rather than just the current state. The estimated policy $\hat{\pi}_b^{(k)}$ is trained via maximum likelihood (typically logistic regression or transformed Beta distributions) to predict actions given history. This estimated policy is then used to compute importance sampling ratios for standard OPE estimators (OIS, SIS, DR, MIS). The key innovation is analyzing how this history dependence affects estimator properties, revealing a bias-variance tradeoff where asymptotic variance decreases with history length while finite-sample bias increases.

## Key Results
- History-dependent behavior policy estimation consistently reduces asymptotic variance for OIS and SIS estimators compared to state-only estimation
- The same estimation introduces finite-sample bias that grows exponentially with horizon $T$ and linearly with history length $k$
- For MIS estimators, history-dependent estimation actually increases asymptotic variance, making it unique among analyzed estimators
- Experimental results on Cartpole and MuJoCo environments confirm theoretical predictions: improved accuracy with history-dependent estimation in large samples while confirming bias increase in smaller samples

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Estimating a history-dependent behavior policy reduces the asymptotic variance of Ordinary (OIS) and Sequential (SIS) Importance Sampling estimators compared to using the true Markovian policy.
- **Mechanism:** The estimation process effectively projects the empirical return onto the orthogonal complement of the tangent space spanned by the score function (derivatives of the log-likelihood). This projection removes variance components correlated with the policy parameters, functioning similarly to the control variates in a Doubly Robust estimator.
- **Core assumption:** The policy class is monotonic ($\Pi_0 \subseteq \Pi_1 \dots$) and the true policy lies within the base class (realizability).
- **Evidence anchors:**
  - [abstract] "...demonstrating that history-dependent behavior policy estimation decreases their asymptotic variances..."
  - [section 4.1] Equation (3) shows variance is a monotonically non-decreasing function with respect to history length; longer history equals a more constrained projection space.
  - [corpus] Weak direct support; neighbors focus on POMDPs or embeddings rather than the specific variance projection mechanism in MDPs.
- **Break condition:** Fails if the true behavior policy is not realizable by the assumed parametric class, potentially leading to inconsistent estimation.

### Mechanism 2
- **Claim:** History-dependent estimation introduces a finite-sample bias that grows exponentially with the horizon $T$ and linearly with history length $k$.
- **Mechanism:** The plug-in estimator for the behavior policy introduces estimation error. In finite samples, this error accumulates over the trajectory horizon and scales with the complexity of the history window, manifesting as a bias term in the MSE decomposition.
- **Core assumption:** Bounded rewards and bounded coverage (IS ratios).
- **Evidence anchors:**
  - [abstract] "...while such estimation decreases asymptotic variance... it introduces finite-sample bias that can be significant in small samples or long horizons."
  - [section 4.1] Theorem 2, Equation (2) upper bounds the finite-sample bias as $O((k + 1)C^T / n^{3/2})$.
  - [corpus] Neighbors like "Uncertainty Quantification..." allude to statistical uncertainty in OPE, consistent with finite-sample bias risks.
- **Break condition:** Small sample sizes ($n$) or long horizons ($T$) where the bias term dominates the $O(1/n)$ variance reduction.

### Mechanism 3
- **Claim:** For Marginalized Importance Sampling (MIS), incorporating history-dependent ratios increases asymptotic variance, reversing the trend seen in OIS/SIS.
- **Mechanism:** As history length increases, the history-dependent MIS ratio converges toward the sequential IS ratio (which suffers from the "curse of horizon"). Increasing history complexity increases the variability of the weight ratio, thereby increasing the estimator's variance.
- **Core assumption:** Linear function approximation for the ratio and correct specification of the Q-function for the DRL equivalence.
- **Evidence anchors:**
  - [abstract] "...except for the marginalized IS estimator, where performance worsens."
  - [section 4.4] Theorem 8 states that for $k' < k$, the asymptotic MSE of the shorter history is less than or equal to the longer history.
  - [corpus] Weak direct evidence regarding MIS specifically; neighbors focus on generic OPE challenges.
- **Break condition:** Using full history length in MIS estimation essentially degenerates the estimator into a high-variance SIS estimator.

## Foundational Learning

- **Concept:** **Bias-Variance Decomposition**
  - **Why needed here:** The central thesis relies on analyzing MSE as the sum of squared bias and variance. You cannot understand the "paradox" (improved MSE with estimated policies) without seeing that variance drops faster than bias rises in large samples.
  - **Quick check question:** If I increase the history length $k$, what happens to the *variance* term (decreases) vs. the *bias* term (increases)?

- **Concept:** **Importance Sampling (IS) & Propensity Scores**
  - **Why needed here:** The entire methodology is a specialized application of IS (reweighting returns by probability ratios). Understanding how to calculate $\frac{\pi_e(A|S)}{\pi_b(A|S)}$ is the baseline for the paper's modification (using history).
  - **Quick check question:** Why does standard IS with an *oracle* policy have zero bias but potentially high variance?

- **Concept:** **Markov Property vs. History Dependence**
  - **Why needed here:** The paper exploits the gap between the true generative process (Markov) and the estimation strategy (History-dependent). You must grasp that even if the true system is memoryless, treating it *as if* it has memory during estimation changes the statistics.
  - **Quick check question:** Does the paper assume the environment is non-Markovian? (No, it assumes the environment is an MDP, but the *estimated policy* conditions on history).

## Architecture Onboarding

- **Component map:** Offline trajectories $H = \{(S, A, R)\}$ -> History Encoder (constructs $k$-step history windows $H_{t-k:t}$) -> Behavior Policy Estimator ($\hat{\pi}_b$: parametric model trained to predict $A_t$ given $H_{t-k:t}$) -> IS Ratio Calculator (computes $\hat{\lambda}_t(k)$ using $\hat{\pi}_b$ and target policy $\pi_e$) -> Value Aggregator (OIS/SIS/DR/MIS formulas consuming ratios and rewards)

- **Critical path:** The training of the **Behavior Policy Estimator**. It must converge sufficiently fast so that the $O(n^{-3/2})$ bias term does not overpower the variance gains.

- **Design tradeoffs:**
  - **History Length ($k$):**
    - High $k$: Lower asymptotic variance (good for large data), higher finite-sample bias (bad for small data/long horizons)
    - Low $k$: Higher variance, lower bias
  - **Estimator Choice:**
    - Use OIS/SIS with history if you have large $n$
    - Avoid history for MIS (use state-only ratios)

- **Failure signatures:**
  - **Curse of Horizon:** If episode length $T$ is very large, the bias term $C^T$ explodes, causing MSE to spike despite variance reduction
  - **MIS Degradation:** If you plug history into an MIS estimator, MSE will likely increase relative to the standard state-based MIS

- **First 3 experiments:**
  1. **Variance vs. History Length:** Fix a large sample size. Plot MSE and Variance of OIS as $k$ increases. Verify variance decreases monotonically.
  2. **Bias Sensitivity:** Fix a small sample size ($n$) and long horizon ($T$). Plot MSE vs. $k$. Verify that MSE *increases* because the bias term dominates.
  3. **MIS Ablation:** Compare standard MIS (state-only) vs. History-MIS. Verify that standard MIS has lower MSE.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a theoretically grounded method be derived for optimally selecting the history length $k$ to minimize mean squared error (MSE) in finite samples?
- Basis in paper: [explicit] The Discussion section states that "selection of history length is crucial" and proposes a BIC-like heuristic, but acknowledges it lacks rigorous theoretical validation in the text.
- Why unresolved: While the paper derives the bias-variance trade-off, it does not provide a closed-form solution or consistent selection theory for the hyperparameter $k$.
- What evidence would resolve it: A derivation of the optimal history length $k$ as a function of sample size $n$ and horizon $T$, or consistency proofs for the proposed selection heuristic.

### Open Question 2
- Question: Can the Marginalized Importance Sampling (MIS) estimator be modified to benefit from history-dependent behavior policy estimation, unlike its current formulation?
- Basis in paper: [inferred] Table 1 and Theorem 8 show MIS is unique among the analyzed estimators because its asymptotic variance *increases* with history length, unlike OIS, SIS, and DR.
- Why unresolved: The paper proves this degradation for MIS but does not explore if this is a fundamental limitation of the marginalization approach or a specific artifact of the estimation method.
- What evidence would resolve it: A modified MIS estimator that incorporates history to reduce variance without incurring the performance penalty described in Theorem 8.

### Open Question 3
- Question: Does the history-dependent estimation paradox and the resulting bias-variance trade-off persist in Off-Policy Learning (OPL) settings?
- Basis in paper: [inferred] The Introduction notes that IS is widely used for policy learning (e.g., PPO), but the theoretical analysis is strictly confined to Off-Policy Evaluation (OPE).
- Why unresolved: It is unknown if the finite-sample bias introduced by history-dependent estimation destabilizes policy optimization or if the variance reduction translates to faster learning convergence.
- What evidence would resolve it: Theoretical analysis or empirical benchmarks showing the impact of history-dependent behavior policy estimation on the convergence rates of policy gradient algorithms.

## Limitations
- Theoretical analysis relies heavily on realizability assumption - true behavior policy must be within parametric class used for estimation
- Finite-sample bias analysis assumes bounded rewards and coverage, which may not hold in real-world applications
- Does not account for computational costs of longer history windows, which could be prohibitive in high-dimensional state spaces

## Confidence
- **High Confidence:** The bias-variance decomposition framework and the general observation that history-dependent estimation can improve large-sample accuracy while introducing finite-sample bias
- **Medium Confidence:** The specific variance reduction mechanisms for OIS/SIS estimators and the MIS degradation claim, as these depend on particular parametric assumptions
- **Medium Confidence:** The experimental results showing improved accuracy with history-dependent estimation in large samples, though the generalization to more complex environments remains to be seen

## Next Checks
1. Test the bias-variance trade-off on a non-linear policy class (e.g., neural network) to verify the variance reduction mechanism holds beyond linear/logistic models
2. Conduct experiments with unbounded rewards (e.g., continuous control with large reward scales) to validate the boundedness assumptions in the bias analysis
3. Compare the performance of history-dependent estimation against state-of-the-art variance reduction techniques (e.g., weighted IS, weighted doubly robust) to contextualize the improvements