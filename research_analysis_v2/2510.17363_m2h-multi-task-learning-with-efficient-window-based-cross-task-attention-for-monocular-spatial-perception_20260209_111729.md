---
ver: rpa2
title: 'M2H: Multi-Task Learning with Efficient Window-Based Cross-Task Attention
  for Monocular Spatial Perception'
arxiv_id: '2510.17363'
source_url: https://arxiv.org/abs/2510.17363
tags:
- depth
- multi-task
- block
- feature
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents M2H, a multi-task learning framework for dense
  spatial perception from monocular images, targeting tasks including semantic segmentation,
  depth estimation, edge detection, and surface normal estimation. M2H introduces
  a Window-Based Cross-Task Attention Module (WMCA) that efficiently exchanges local
  features among tasks within non-overlapping windows, complemented by a Global Gated
  Feature Merging (GGFM) module that aggregates global context per task.
---

# M2H: Multi-Task Learning with Efficient Window-Based Cross-Task Attention for Monocular Spatial Perception

## Quick Facts
- arXiv ID: 2510.17363
- Source URL: https://arxiv.org/abs/2510.17363
- Reference count: 0
- Primary result: Achieves 61.54% mIoU semantic segmentation and 0.4196 RMSE depth on NYUDv2, outperforming state-of-the-art multi-task models by 3.4% mIoU and 13% RMSE respectively.

## Executive Summary
M2H introduces a multi-task learning framework for dense spatial perception from monocular images, targeting semantic segmentation, depth estimation, edge detection, and surface normal estimation. The framework uses a lightweight DINOv2 backbone with an efficient Window-Based Cross-Task Attention Module (WMCA) that exchanges local features among tasks within non-overlapping windows, complemented by a Global Gated Feature Merging (GGFM) module that provides global context per task. This design enables real-time inference while achieving state-of-the-art performance on NYUDv2, Hypersim, and Cityscapes datasets.

## Method Summary
M2H processes monocular images through a DINOv2-small backbone, extracting multi-scale features that are reassembled into spatial maps. Task-specific features are generated through multi-scale fusion, then processed through parallel WMCA (local cross-task attention within windows) and GGFM (global task-specific gating) modules. The outputs are concatenated and fused before being passed to task-specific decoder heads. Dynamic Weight Averaging balances four task losses, with additional cross-task consistency losses (depth-normal, edge-segmentation) applied during fine-tuning to enforce geometric coherence.

## Key Results
- On NYUDv2: 61.54% mIoU semantic segmentation (3.4% improvement over prior multi-task methods), 0.4196 RMSE depth estimation (13% improvement)
- On Hypersim: 33% RMSE depth improvement over single-task baselines, 5.4 mIoU semantic segmentation improvement over single-task models
- On Cityscapes: 77.6% mIoU semantic segmentation, 6.10 RMSE depth estimation, outperforming prior multi-task methods
- Enables real-time 3D scene graph construction with Mono-Hydra for dynamic environment understanding

## Why This Works (Mechanism)

### Mechanism 1: Local Cross-Task Attention via WMCA
Partitioning task-specific features into non-overlapping windows enables efficient cross-task feature exchange while reducing computational overhead. Each window's features are concatenated across tasks and processed through multi-head cross-attention, allowing selective feature sharing based on local correlations (e.g., depth discontinuities aligning with semantic boundaries).

### Mechanism 2: Global Task-Specific Context via GGFM
A squeeze-and-excitation style gating mechanism captures global context per task without dense cross-task attention. Global average pooling produces task descriptors, passed through MLP layers to generate gating vectors that recalibrate feature maps, with residual fusion from backbone tokens providing additional context.

### Mechanism 3: Complementary Local-Global Fusion
Parallel processing through WMCA (local cross-task) and GGFM (global task-specific) provides complementary representations. Their concatenation and fusion via 1×1 convolution combines local interactions with global context, while cross-task consistency losses during fine-tuning enforce geometric and boundary coherence.

## Foundational Learning

- **Vision Transformers with window-based attention (Swin Transformer)**: Essential for understanding WMCA's efficiency gains from reducing quadratic attention complexity to linear through window partitioning. Quick check: For a 56×56 feature map with 7×7 windows, how many attention operations versus full self-attention?

- **DINOv2 self-supervised visual features**: Critical for understanding the pretrained backbone's patch tokens, their reassembly into spatial feature maps (MSTR), and how final layer tokens provide global context to GGFM. Quick check: How do DINOv2 features differ from supervised ViT features, and why might they transfer better to dense prediction?

- **Multi-task negative transfer and task balancing (DWA, GradNorm)**: Important for understanding how Dynamic Weight Averaging balances four task losses and two consistency losses to mitigate gradient interference. Quick check: If depth loss decreases slowly while segmentation improves rapidly, how would DWA adjust the loss weights?

## Architecture Onboarding

- **Component map:**
  Input Image (3×H×W) -> DINOv2-Small Backbone -> Multi-Scale Token Reassembly (MSTR) -> Multi-Scale Fusion (MSF) per task -> Parallel WMCA/GGFM processing -> Concatenate + 1×1 Conv fusion -> Task-Specific Heads

- **Critical path:** The MSF -> WMCA/GGFM -> fusion path is where cross-task synergy occurs. Errors here (e.g., incorrect window padding, misaligned feature dimensions) propagate to all task heads.

- **Design tradeoffs:**
  - Window size p=7 balances context capture with attention cost
  - 2 WMCA layers with 4 heads chosen for depth vs. efficiency tradeoff
  - DINOv2-small enables real-time inference but may limit feature richness vs. larger variants

- **Failure signatures:**
  - Blurred task boundaries: insufficient local cross-task attention or excessive global gating suppression
  - Inconsistent depth-normal alignment: check depth-normal consistency loss weighting (λ=0.1)
  - Temporal flickering in video: current architecture lacks explicit temporal modeling

- **First 3 experiments:**
  1. Reproduce ablation (Table VI): Train with WMCA only, GGFM only, and both to verify complementary contributions on NYUDv2 validation
  2. Window size sensitivity: Test p∈{5, 7, 9, 11} to find optimal efficiency-accuracy tradeoff
  3. Backbone substitution: Replace DINOv2-small with DINOv2-base to measure performance gain vs. FPS reduction

## Open Questions the Paper Calls Out

### Open Question 1
How can temporal stability be effectively integrated into the M2H framework to ensure consistent predictions for video-based 3D mapping? The Conclusion identifies "refining temporal stability" as a focus for future work, noting that the current architecture processes frames independently without mechanisms to enforce feature continuity across sequential frames.

### Open Question 2
Can M2H maintain its real-time performance (30 FPS) when deployed on actual resource-constrained edge hardware rather than a laptop GPU? While the Abstract targets "edge devices," experimental validation relies exclusively on an RTX 3080 Laptop GPU, which offers substantially higher compute capability than typical embedded processors.

### Open Question 3
Does the use of strictly non-overlapping windows in the WMCA block restrict the propagation of local cross-task features across window boundaries? The strict separation of local windows may cause discontinuities at window borders, limiting the model's ability to capture fine-grained cross-task interactions that span these boundaries.

## Limitations

- **Ablation granularity:** The paper reports aggregate metrics for removing WMCA or GGFM but lacks per-task breakdown, limiting understanding of which tasks benefit most from each module
- **Edge detection details:** Ground truth edge label generation method from NYUDv2 depth/normal/semantic maps is unspecified, creating reproducibility ambiguity
- **Real-time claims:** M2H claims real-time performance but does not specify hardware or exact FPS values across datasets, making deployment feasibility assessment difficult

## Confidence

- **High confidence:** Core architectural contributions (WMCA, GGFM) and their descriptions are well-specified and internally consistent
- **Medium confidence:** Effectiveness of combining local cross-task attention with global task-specific gating is supported by ablation studies, though lack of per-task breakdown limits detailed understanding
- **Low confidence:** Real-time deployment claims and 3D scene graph construction with Mono-Hydra are mentioned but not substantiated with hardware specifications or performance metrics

## Next Checks

1. **Reproduce ablation with per-task breakdown:** Train M2H with WMCA only, GGFM only, and both modules enabled on NYUDv2 validation set, reporting mIoU/RMSE for each task individually to verify complementary contributions

2. **Cross-task consistency sensitivity:** Train identical models with and without depth-normal and edge-segmentation consistency losses (λ=0 and λ=0.1) to isolate their contribution to overall performance gains

3. **Window size efficiency-accuracy tradeoff:** Systematically evaluate M2H performance on NYUDv2 with window sizes p∈{5, 7, 9, 11} to identify optimal balance between computational cost and task accuracy