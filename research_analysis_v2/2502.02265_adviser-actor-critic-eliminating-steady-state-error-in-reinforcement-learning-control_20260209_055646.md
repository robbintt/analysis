---
ver: rpa2
title: 'Adviser-Actor-Critic: Eliminating Steady-State Error in Reinforcement Learning
  Control'
arxiv_id: '2502.02265'
source_url: https://arxiv.org/abs/2502.02265
tags:
- control
- system
- learning
- adviser
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Adviser-Actor-Critic (AAC), a reinforcement
  learning framework that combines PID control theory with deep RL to address high-precision
  control tasks. The key innovation is an adviser module that mentors the actor to
  refine control actions by generating synthetic error signals, effectively reducing
  steady-state errors.
---

# Adviser-Actor-Critic: Eliminating Steady-State Error in Reinforcement Learning Control

## Quick Facts
- **arXiv ID**: 2502.02265
- **Source URL**: https://arxiv.org/abs/2502.02265
- **Reference count**: 15
- **One-line primary result**: AAC outperforms SAC and SAC+HER in reducing steady-state error across three goal-conditioned control tasks.

## Executive Summary
This paper introduces Adviser-Actor-Critic (AAC), a reinforcement learning framework that combines PID control theory with deep RL to address high-precision control tasks. The key innovation is an adviser module that mentors the actor to refine control actions by generating synthetic error signals, effectively reducing steady-state errors. AAC outperforms standard SAC and SAC+HER algorithms in three goal-conditioned environments—mass-spring-damper, robotic arm, and quadcopter—achieving lower steady-state errors and higher cumulative rewards. The framework also demonstrates successful real-world deployment on a quadcopter, showcasing its robustness and adaptability for complex control systems.

## Method Summary
Adviser-Actor-Critic (AAC) is built on Soft Actor-Critic (SAC) with Hindsight Experience Replay (HER). The core innovation is an Adviser module that acts as a PID controller, generating synthetic error signals (ε) based on the discrepancy between desired and achieved goals. This synthetic error is injected into the actor's observation space as a "fake goal," guiding the policy to compensate for systematic biases. The framework uses MLPs with SELU activation for actor and critic networks, trained with specified learning rates and hyperparameters. AAC is evaluated in three goal-conditioned environments, demonstrating superior steady-state error reduction compared to baselines.

## Key Results
- AAC reduces steady-state error in Mass-Spring-Damper, Robotics Arm, and Quadcopter environments compared to SAC and SAC+HER.
- Real-world quadcopter deployment validates the framework's robustness and adaptability.
- Performance gains are attributed to the Adviser's ability to eliminate systematic bias through synthetic error injection.

## Why This Works (Mechanism)

### Mechanism 1: Synthetic Error Injection via "Fake Goals"
The Adviser calculates a synthetic error signal (ε) based on the discrepancy between the desired and achieved goals. It shifts the target observed by the Actor. The Actor, attempting to reach this shifted target, applies control actions that, due to the system's bias, result in the agent converging on the true desired goal.

### Mechanism 2: PID-Based Integral Compensation
The Adviser implements the integral component of PID control, accumulating error over time to force a correction that a purely proportional policy might miss. This addresses the accumulation of past errors to minimize steady-state errors over time.

### Mechanism 3: Stability-Guaranteed Guidance
The integration of a PID Adviser with an RL Actor remains stable only if the combined system poles reside in the left half-plane, enforced via Routh-Hurwitz criteria. The Adviser is a structured controller with analytically derived constraints on gains to ensure closed-loop stability.

## Foundational Learning

- **PID Control (specifically Integral Action)**: Why needed here: The core value proposition of AAC is fixing the "steady-state error." This is classically solved by the Integral term in PID. You cannot understand why the "Adviser" works without understanding how integral accumulation biases the control signal until the error is zero. Quick check question: If a system has a constant disturbance, why is a Proportional-only controller insufficient to return the system to the exact setpoint?

- **Goal-Conditioned Reinforcement Learning (GCRL)**: Why needed here: The paper frames the problem as achieving a "desired goal" (g_d) vs. "achieved goal" (g_a). The architecture relies on Hindsight Experience Replay (HER) concepts to structure the observation space. Quick check question: How does the reward function change if the agent achieves a state that was not the original goal but is re-labeled as a goal during training?

- **Soft Actor-Critic (SAC)**: Why needed here: The AAC framework is built on top of SAC. Understanding the entropy regularization and the Q-function estimation is necessary to debug why the base RL agent might be settling for suboptimal precision before the Adviser is added. Quick check question: In SAC, what is the role of the entropy coefficient α, and how does it encourage exploration compared to DDPG?

## Architecture Onboarding

- **Component map**: Environment -> Actor ($\pi_\phi$) and Critic ($Q_\theta$) -> Adviser -> Actor (feedback loop)
- **Critical path**: 1. Observe true error e = g_d - g_a. 2. Pass e through PID Adviser to generate synthetic error ε. 3. Construct extended observation s_e. 4. Actor executes action based on s_e.
- **Design tradeoffs**: Train vs. Eval Adviser (simpler training vs. careful tuning), Observation Augmentation (increased state dimensions vs. direct corrective signals).
- **Failure signatures**: Oscillations (if K_d too low or K_i too high), Actuator Saturation (if correction too large), Non-convergence (if stability conditions not met).
- **First 3 experiments**: 1. Run standard SAC on Mass-Spring-Damper to quantify nominal steady-state error. 2. Implement Adviser with K_p=1.3, K_i=0.1, K_d=0.1 vs. no effect (K_i=0.0) to verify error reduction. 3. Deploy trained policy on Quadcopter with Adviser active; monitor fake goal vs. actual position tracking.

## Open Questions the Paper Calls Out

- Can alternative control strategies, such as Model Predictive Control (MPC) or H-infinity control, be effectively integrated into the Adviser module to handle transient and steady-state errors more effectively than the current PID-based implementation?
- Is the performance of the AAC framework sensitive to the manual tuning of the Adviser's parameters (K_p, K_i, K_d), and can these parameters be learned adaptively to remove the need for domain-specific manual design?
- Does the AAC framework maintain stability and performance guarantees in highly coupled Multi-Input Multi-Output (MIMO) systems where the "sufficiently decoupled" assumption used in the stability analysis is violated?

## Limitations

- **Observation Augmentation Ambiguity**: Unclear whether the input should be [s, g_a, -ε] or [s, g_a, -(e+ε)].
- **PID Implementation Details**: Specifics such as error normalization, integral term clamping, and reset conditions are not provided.
- **Decoupling Assumption**: Stability analysis assumes decoupled systems; may not hold for tightly coupled or highly nonlinear systems.

## Confidence

- **AAC Reduces Steady-State Error**: High Confidence (supported by quantitative results across multiple environments and real-world deployment)
- **PID-Based Adviser Eliminates Systematic Bias**: Medium Confidence (theoretical mechanism is sound, but implementation details introduce uncertainty)
- **Stability Guarantees via Routh-Hurwitz**: Medium Confidence (analytical derivation provided, but assumptions limit generalizability)

## Next Checks

1. **Observation Modification Clarification**: Implement and test both variants of observation augmentation ([s, g_a, -ε] vs. [s, g_a, -(e+ε)]) to determine which aligns with the paper's intended design and yields optimal performance.
2. **PID Parameter Sensitivity**: Conduct a systematic sweep of K_p, K_i, and K_d values to identify stability boundaries and optimal settings for each environment, ensuring the Adviser does not introduce instability or wind-up.
3. **Real-World Robustness Testing**: Deploy AAC on a physical robotic platform (e.g., a 3-DoF arm or quadcopter) under varying payload and environmental conditions to validate the framework's adaptability and robustness beyond the controlled simulation setup.