---
ver: rpa2
title: Evaluating Multimodal Large Language Models on Spoken Sarcasm Understanding
arxiv_id: '2509.15476'
source_url: https://arxiv.org/abs/2509.15476
tags:
- sarcasm
- multimodal
- detection
- mustard
- mcsd
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates multimodal large language models (MLLMs) for
  sarcasm detection across English and Chinese datasets using zero-shot, few-shot,
  and fine-tuning settings. The authors compare both direct LLM inference and feature-extraction
  approaches with collaborative gating fusion.
---

# Evaluating Multimodal Large Language Models on Spoken Sarcasm Understanding

## Quick Facts
- arXiv ID: 2509.15476
- Source URL: https://arxiv.org/abs/2509.15476
- Reference count: 0
- This paper evaluates multimodal large language models (MLLMs) for sarcasm detection across English and Chinese datasets using zero-shot, few-shot, and fine-tuning settings.

## Executive Summary
This paper systematically evaluates multimodal large language models for spoken sarcasm understanding across English (MUStARD++) and Chinese (MCSD 1.0) datasets. The authors compare direct LLM inference approaches with feature-extraction methods using collaborative gating fusion, examining zero-shot, few-shot, and fine-tuning settings. Results demonstrate that audio-based models achieve the strongest unimodal performance, while text-audio and audio-vision combinations consistently outperform unimodal and trimodal systems. Qwen-Omni delivers competitive results across all settings, and fine-tuning with LoRA yields substantial improvements. The study reveals that bimodal fusion approaches consistently outperform both unimodal and trimodal approaches, highlighting the importance of selective modality integration for sarcasm detection.

## Method Summary
The study evaluates MLLMs for sarcasm detection using three experimental settings: zero-shot (task instructions only), few-shot (2-6 in-context examples), and LoRA fine-tuning (rank=8, LR=1e-4). Four MLLMs are tested: LLaMA 3-8B (text), Qwen-Audio-7B, Qwen-VL-7B, and Qwen-Omni-7B. Two datasets are used: MUStARD++ (841 train/180 val/181 test English samples) and MCSD 1.0 (1893 train/406 val/406 test Chinese samples). The feature extraction approach uses modality-specific encoders (BERT/LLaMA3 for text, Wav2Vec2.0/Qwen-Audio for audio, ResNet50/Qwen-VL for video) with collaborative gating fusion that dynamically weights modality contributions. For direct inference, Qwen-Omni processes multimodal inputs end-to-end. Performance is measured using weighted F1 score.

## Key Results
- Audio-based models achieve the strongest unimodal performance (Wav2Vec2.0: 78.0% F1 on MCSD 1.0; Qwen-Audio: 75.1% F1 on MUStARD++)
- Bimodal fusion (text-audio, audio-vision) consistently outperforms unimodal and trimodal systems
- LoRA fine-tuning yields substantial improvements across all models and datasets
- Qwen-Omni delivers competitive zero-shot and fine-tuned results
- Vision modality consistently underperforms and can degrade trimodal performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Audio encodes the most discriminative unimodal signals for spoken sarcasm detection
- Mechanism: Prosodic features—pitch variation, speaking rate, intonation contours—are captured by audio encoders (Wav2Vec2.0, Qwen-Audio) and provide direct access to paralinguistic markers of ironic intent that text alone cannot convey.
- Core assumption: Sarcasm in spoken dialogue relies more heavily on how something is said than on lexical content or facial expression.
- Evidence anchors:
  - [abstract] "audio-based models achieve the strongest unimodal performance"
  - [section 3.6] "In unimodal settings, audio-based models consistently outperform text- or vision-based ones. On MCSD 1.0, Wav2Vec2.0 reaches the highest F1 score (78.0%). On MUStARD++, Qwen-Audio achieves the strongest performance with 75.1% F1."
  - [corpus] Related review confirms: "Linguistic research has highlighted the importance of prosodic cues, such as variations in pitch, speaking rate, and intonation, in conveying sarcastic intent"
- Break condition: When audio quality is degraded, or when sarcasm is conveyed primarily through lexical irony or visual gesture without prosodic marking.

### Mechanism 2
- Claim: Bimodal fusion outperforms both unimodal and trimodal approaches by capturing complementary incongruity signals while suppressing modality-specific noise
- Mechanism: Collaborative gating fusion dynamically weights modality contributions (α_m), allowing the model to emphasize stronger signals (e.g., prosody) and suppress weaker or noisy cues (e.g., vision in Chinese data), enabling cross-modal incongruity detection without information overload.
- Core assumption: Adding more modalities introduces noise that degrades performance; optimal fusion requires selective gating rather than full integration.
- Evidence anchors:
  - [abstract] "text-audio and audio-vision combinations outperform unimodal and trimodal systems"
  - [section 3.6] "Trimodal fusion: Adding all three modalities (T+A+V) does not yield further improvements over the strongest bimodal systems... On MUStARD++, Large (A+V) achieves the best overall performance with 77.9% F1... On MCSD 1.0, Base (T+A) attains the overall best result at 78.2% F1."
  - [corpus] Commander-GPT paper notes "growing evidence suggests that [LLMs] struggle with sarcasm understanding," supporting the need for targeted fusion approaches
- Break condition: When all three modalities provide equally strong complementary signals (dataset-dependent); when gating weights fail to adapt to modality quality.

### Mechanism 3
- Claim: LoRA fine-tuning enables efficient adaptation of pretrained MLLMs to sarcasm detection with limited labeled data
- Mechanism: Low-Rank Adaptation updates only projection matrices while freezing base model weights, transferring pretrained cross-modal reasoning capabilities to the sarcasm classification task without catastrophic forgetting or full parameter updates.
- Core assumption: Pretrained MLLMs encode generalizable cross-modal reasoning that only requires light task-specific adaptation.
- Evidence anchors:
  - [abstract] "Fine-tuning with LoRA yields substantial improvements"
  - [section 3.3] "Overall, fine-tuning with LoRA consistently leads to substantial improvements across all models and datasets. Qwen-Audio and Qwen-Omni reach the highest FT F1 on MCSD 1.0 (78.1% and 77.8%)"
  - [corpus] No direct corpus evidence for LoRA in sarcasm detection; related work focuses on prompt-tuning and attention-based fusion methods
- Break condition: When training data is extremely scarce (<100 samples); when full fine-tuning is computationally feasible and yields better results; when LoRA rank is insufficient for task complexity.

## Foundational Learning

- Concept: **Cross-modal incongruity detection**
  - Why needed here: Sarcasm frequently manifests as contradiction between literal text and nonverbal cues (prosody, facial expression); detecting this incongruity is central to the task.
  - Quick check question: Can you explain why the utterance "Oh, that's just great" with flat intonation and eye-rolling is sarcastic, and identify which modalities carry the incongruity signal?

- Concept: **Zero-shot / Few-shot / Fine-tuning evaluation paradigms**
  - Why needed here: The paper systematically compares these three settings to assess inherent MLLM capabilities versus adaptation potential.
  - Quick check question: What is the fundamental difference between providing k=4 examples in-context (few-shot) versus updating model weights with LoRA on those same examples?

- Concept: **Collaborative gating fusion**
  - Why needed here: The fusion mechanism dynamically weights modalities rather than using fixed weights or simple concatenation.
  - Quick check question: How does the gating network compute attention weights α_m for each modality, and why might this adaptively suppress noisy modalities?

## Architecture Onboarding

- Component map:
  - BERT/LLaMA3-8B (text) -> Average pooling -> 768-4096 dim embeddings
  - Wav2Vec2.0/Qwen-Audio-7B (audio) -> Average pooling -> 768-4096 dim embeddings
  - ResNet50/Qwen-VL-7B (video) -> Average pooling -> 2048-3584 dim embeddings
  - Collaborative gating module: Cross-modal gating functions g_θ(t,a), g_θ(t,v) compute dynamic attention weights; fusion: h_fusion = Σ α_m h_m
  - Classifier: Binary prediction head on fused representation
  - Alternative direct path: Qwen-Omni-7B for end-to-end zero-shot/few-shot/fine-tuned inference with instruction prompts

- Critical path:
  1. Input preprocessing: Extract aligned transcripts, audio waveforms, N_v video keyframes
  2. Feature extraction: Encode each modality; pool to sentence-level embeddings (768-4096 dim)
  3. Collaborative gating: Normalize embeddings → compute α_m → weighted fusion
  4. Classification: Pass h_fusion to classifier; output binary sarcasm label
  5. For direct MLLM: Instruction prompt → Qwen-Omni → classification output

- Design tradeoffs:
  - **Base vs. Large encoders**: Base (BERT, Wav2Vec2, ResNet50) faster inference but lower capacity; Large (LLaMA3, Qwen-Audio, Qwen-VL) richer representations at higher compute cost
  - **Bimodal vs. Trimodal**: Bimodal (T+A or A+V) consistently outperforms trimodal; vision adds noise in Chinese data
  - **Zero-shot vs. Fine-tuning**: Zero-shot requires no labels but underperforms (F1 ~50-63%); LoRA fine-tuning needs 500-1000+ samples for substantial gains (F1 72-78%)
  - **Model selection**: Qwen models (balanced Chinese-English pretraining) outperform LLaMA3 on MCSD 1.0

- Failure signatures:
  - **Cross-lingual degradation**: LLaMA3 (English-dominant) shows poor zero-shot on Chinese MCSD (F1: 30.1% vs. 49.7% on MUStARD++)
  - **Vision modality weakness**: ResNet50 and Qwen-VL achieve lowest unimodal F1 (~50-60%); vision adds noise in trimodal settings
  - **Trimodal performance ceiling**: T+A+V never exceeds best bimodal; often degrades
  - **Few-shot inconsistency**: Some models show minimal/negative few-shot gains (LLaMA3: 30.1% → 29.6% on MCSD)
  - **Overfitting risk**: Qwen-Omni MUStARD++ performance peaks at 500 samples, slight decrease at 1000

- First 3 experiments:
  1. Establish unimodal baselines on both datasets using Base encoders (BERT, Wav2Vec2.0, ResNet50) to confirm audio dominance and identify weak modalities.
  2. Run systematic bimodal fusion ablation (T+A, T+V, A+V) with collaborative gating, comparing Base vs. Large encoders to validate optimal pairings and encoder scaling.
  3. Execute LoRA fine-tuning sweep on Qwen-Omni across training sizes (0, 500, 1000, full dataset) on both MUStARD++ and MCSD 1.0 to establish scaling curves and detect overfitting thresholds.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can unified frameworks be designed to exploit MLLM reasoning capabilities while systematically mitigating modality-specific noise, particularly for the visual modality which consistently underperforms?
- Basis in paper: [explicit] Authors state in the conclusion: "Future work should explore... unified frameworks that exploit the reasoning capabilities of MLLMs while mitigating modality-specific noise."
- Why unresolved: The paper shows visual models are weakest and that adding vision to text-audio can hurt performance (T+A+V underperforms T+A), but the mechanism for selectively filtering noisy modalities within MLLMs remains unexplored.
- What evidence would resolve it: A framework that dynamically weights or filters visual features based on their discriminative utility, demonstrating improved trimodal performance over bimodal baselines.

### Open Question 2
- Question: What culturally adaptive training strategies would improve cross-lingual sarcasm detection, particularly for cultures where sarcasm relies on different prosodic or gestural marks?
- Basis in paper: [explicit] Conclusion states: "Future work should explore culturally adaptive training strategies."
- Why unresolved: LLaMA 3 shows a large gap between English (49.7% ZS F1) and Chinese (30.1% ZS F1) performance, suggesting language/culture-specific barriers that balanced multilingual pretraining alone does not fully address.
- What evidence would resolve it: Systematic evaluation of culture-specific fine-tuning, prosody-aware adaptation, or retrieval-augmented cultural knowledge on diverse linguistic datasets.

### Open Question 3
- Question: Why do trimodal models consistently underperform optimal bimodal combinations in sarcasm detection, and does this reflect dataset-specific noise or a fundamental limitation in current fusion mechanisms?
- Basis in paper: [inferred] Results show T+A+V (76.8% F1 on MCSD, 75.1% on MUStARD++) underperforms A+V (77.9% on MUStARD++) and T+A (78.2% on MCSD), but the paper does not explain this counterintuitive finding.
- Why unresolved: The collaborative gating mechanism should theoretically benefit from additional modalities, yet the opposite occurs, suggesting either noisy visual features or suboptimal fusion.
- What evidence would resolve it: Ablation studies analyzing learned attention weights across modalities, or modality dropout experiments revealing which features introduce interference.

### Open Question 4
- Question: Do findings from staged media (sitcoms and stand-up comedy) generalize to naturalistic, spontaneous conversational sarcasm?
- Basis in paper: [inferred] Both datasets are from performed content (TV dialogues and stand-up comedy), which may contain exaggerated prosodic and visual cues not representative of everyday sarcasm.
- Why unresolved: The paper does not discuss whether models trained on acted sarcasm transfer to spontaneous speech where cues are subtler.
- What evidence would resolve it: Evaluation on datasets of spontaneous conversational sarcasm, or analysis of prosodic cue magnitude differences between staged and naturalistic data.

## Limitations

- **Limited implementation details**: Collaborative gating mechanism specifics (network architecture, normalization methods) are not fully specified
- **Dataset domain constraints**: Both datasets are from staged media (sitcoms, stand-up comedy) that may not represent naturalistic conversational sarcasm
- **Pretraining data composition**: Cross-lingual performance differences attributed to pretraining data without controlled ablation studies

## Confidence

- **High confidence**: Audio-based unimodal superiority; bimodal fusion outperforming trimodal; LoRA fine-tuning efficacy
- **Medium confidence**: Cross-lingual robustness of Qwen models; collaborative gating mechanism
- **Low confidence**: Vision modality weakness; few-shot learning variability

## Next Checks

1. **Implement and validate collaborative gating architecture**: Reconstruct the cross-modal gating network (2-layer MLP with softmax normalization) and verify it produces interpretable attention weights that suppress vision modality on Chinese data while emphasizing audio signals
2. **Systematic few-shot learning analysis**: Conduct ablation studies varying k examples, example ordering, and prompt formatting across all MLLMs to identify conditions under which few-shot learning succeeds or fails, particularly for LLaMA3's performance degradation
3. **Controlled cross-lingual pretraining experiment**: Fine-tune Qwen-Omni with balanced English-Chinese pretraining data versus English-dominant pretraining data on MCSD 1.0 to isolate the effect of pretraining data composition on cross-lingual sarcasm detection performance