---
ver: rpa2
title: Generating Completions for Broca's Aphasic Sentences Using Large Language Models
arxiv_id: '2412.17669'
source_url: https://arxiv.org/abs/2412.17669
tags:
- data
- aphasic
- synthetic
- sentences
- broca
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study explores using Large Language Models (LLMs) to reconstruct
  fragmented Broca's aphasic sentences into complete, grammatically correct sentences.
  To address the lack of large aphasic speech datasets, the authors first generate
  synthetic Broca's aphasic data using a rule-based system that modifies neurotypical
  speech to mirror linguistic characteristics of Broca's aphasia.
---

# Generating Completions for Broca's Aphasic Sentences Using Large Language Models

## Quick Facts
- arXiv ID: 2412.17669
- Source URL: https://arxiv.org/abs/2412.17669
- Reference count: 0
- LLMs can reconstruct fragmented Broca's aphasic sentences, with performance improving for longer input utterances.

## Executive Summary
This study explores using Large Language Models (LLMs) to reconstruct fragmented Broca's aphasic sentences into complete, grammatically correct forms. Due to the lack of large aphasic speech datasets, the authors generate synthetic Broca's aphasic data using rule-based transformations of neurotypical speech. Four pre-trained sequence-to-sequence LLMs (T5 and FLAN-T5 variants) are fine-tuned on this synthetic data and evaluated on both synthetic and authentic Broca's aphasic sentences. The results demonstrate that LLMs can effectively reconstruct synthetic aphasic sentences, with performance improving for longer inputs. When tested on authentic aphasic sentences, the models show reasonable completions, particularly for more complex utterances, though challenges remain with very short or severely fragmented sentences.

## Method Summary
The study generates synthetic Broca's aphasic data by applying rule-based transformations to neurotypical spoken sentences from the SBCSAE corpus. These transformations probabilistically remove determiners, prepositions, adjectives, adverbs, and verbs based on established POS-specific rules. Four pre-trained sequence-to-sequence LLMs (T5-Base, T5-Large, FLAN-T5-Base, FLAN-T5-XL) are then fine-tuned on this synthetic data using QLoRA (4-bit quantization with LoRA adapters). The models are evaluated on held-out synthetic data using ChrF, RougeL, and cosine similarity metrics, and qualitatively assessed on authentic Broca's aphasic utterances from AphasiaBank. Fine-tuning uses early stopping with ChrF as the primary metric, and the best models are tested with and without instruction prefixes.

## Key Results
- T5-Base achieves ChrF 56.39 on synthetic test data, demonstrating effective reconstruction of rule-based aphasic fragments
- Model performance improves significantly with longer input utterances (3+ words) that provide more contextual information
- Qualitative evaluation on authentic AphasiaBank data shows reasonable completions, with better results for complex utterances than single-word fragments
- FLAN-T5-XL occasionally introduces semantic errors by adding negations to 2.99% of outputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rule-based linguistic transformations can synthesize training data that approximates authentic Broca's aphasic patterns.
- Mechanism: A PoS-tagged pipeline probabilistically modifies neurotypical sentences—discarding determiners/adpositions/particles (70% rate), adjectives/adverbs/verbs (50% rate), and altering morphological features—to produce agrammatic outputs that retain content words while stripping grammatical structure. Surprisal-based validation using GPT-2 and RoBERTa confirms synthetic utterances achieve predictability scores comparable to authentic aphasic speech (synthetic: 6.39/5.51 GPT-2/RoBERTa vs. authentic: 6.48/5.08).
- Core assumption: Broca's aphasic speech follows systematic, rule-governed simplification patterns that can be captured by deterministic transformations applied to normal speech.
- Evidence anchors:
  - [section II-A]: "probabilistically apply a series of PoS-specific rules to each word... established based on [30] and our manual inspection"
  - [Table III]: Synthetic aphasic surprisal (6.39/5.51) closely matches authentic aphasic surprisal (6.48/5.08), both substantially higher than neurotypical speech
  - [corpus]: Related work "Towards a Method for Synthetic Generation of Persons with Aphasia Transcripts" addresses similar synthetic generation challenges but focuses on CIU coding rather than sentence reconstruction
- Break condition: If authentic aphasic speech exhibits idiosyncratic, non-rule-governed patterns that vary substantially across individuals, synthetic data will underrepresent true clinical variability.

### Mechanism 2
- Claim: Pretrained sequence-to-sequence LLMs can leverage learned linguistic representations to reconstruct fragmented inputs into grammatically complete outputs.
- Mechanism: T5 and FLAN-T5 models, pretrained on large corpora, encode implicit knowledge of grammatical structure, argument roles, and lexical collocations. Fine-tuning on synthetic (fragment → complete) pairs adapts this knowledge to map agrammatic inputs to their probable original forms, with the decoder generating missing function words and morphological markers.
- Core assumption: The pretrained models' linguistic knowledge is sufficiently generalizable that exposure to synthetic fragments transfers to authentic clinical fragments without requiring authentic training data.
- Evidence anchors:
  - [abstract]: "We demonstrate LLMs' capability for reconstructing agrammatic sentences, with the models showing improved performance with longer input utterances"
  - [Table IV]: T5-BASE achieves ChrF 56.39, FLAN-T5-XL achieves ChrF 55.71 on synthetic test set, indicating successful reconstruction
  - [corpus]: "Component-Level Lesioning of Language Models" provides converging evidence that LLMs can exhibit aphasia-like behaviors when selectively lesioned, suggesting architectural plausibility for the reverse task
- Break condition: If the distributional gap between synthetic and authentic aphasic speech exceeds the model's generalization capacity, performance on real clinical data will degrade substantially.

### Mechanism 3
- Claim: Longer input utterances provide richer contextual constraints that disambiguate reconstruction targets.
- Mechanism: With more content words preserved, the model's attention mechanisms can access additional semantic and syntactic cues to infer missing grammatical elements. Three-word minimum requirements during synthetic training explicitly condition models on inputs with sufficient context, aligning test-time inference with training distribution.
- Core assumption: Contextual information is the primary limiting factor in accurate reconstruction; severely fragmented utterances are genuinely ambiguous even with optimal models.
- Evidence anchors:
  - [section III-B2]: "better results were observed for longer input sentences, which provide more contextual information, resulting in improved model performance"
  - [Table VI]: Longer authentic utterances (e.g., "Because kid in here" → "Because there is a kid in here") show more coherent completions than single-word fragments
  - [corpus]: No direct corpus evidence on context-length effects in aphasia reconstruction
- Break condition: If performance plateaus or degrades with additional context due to attention dilution or accumulated errors, the length-performance relationship will be non-monotonic.

## Foundational Learning

- Concept: **Broca's aphasia linguistic profile**
  - Why needed here: Understanding that Broca's aphasia selectively impairs grammatical markers (function words, bound morphemes, complex argument structures) while preserving content words and comprehension is essential for interpreting why specific rule-based transformations are linguistically motivated.
  - Quick check question: If a patient produces "Dog chase cat park," which grammatical elements are likely missing versus preserved?

- Concept: **Surprisal as linguistic predictability measure**
  - Why needed here: The paper validates synthetic data quality using surprisal (negative log-probability) assigned by pretrained language models. Understanding that higher surprisal indicates less predictable, more irregular language patterns is necessary to interpret why authentic and synthetic aphasic data cluster together.
  - Quick check question: Would you expect neurotypical speech to have higher or lower surprisal scores than aphasic speech, and why?

- Concept: **Parameter-efficient fine-tuning (QLoRA)**
  - Why needed here: The study uses 4-bit quantization and Low-Rank Adaptation to fine-tune models up to 3B parameters within computational constraints. Understanding that LoRA freezes base weights and learns small adapter matrices explains how large models can be adapted without full retraining.
  - Quick check question: What components of the model remain frozen versus trainable during QLoRA fine-tuning?

## Architecture Onboarding

- Component map:
  ```
  [SBCSAE Corpus] → [Preprocessing: remove annotations, unfold contractions]
                              ↓
  [Rule-Based Synthesizer: spaCy PoS tagging + probabilistic transformations]
                              ↓
  [Synthetic Fragment-Original Pairs] → [QLoRA Fine-tuning on T5/FLAN-T5]
                              ↓
  [Fine-tuned Model] → [Inference on AphasiaBank authentic fragments]
  ```

- Critical path:
  1. Data preprocessing quality directly impacts synthetic data realism
  2. Rule probabilities (Table I) determine fragment severity distribution
  3. Minimum 3-word filter in synthetic generation creates train-test distribution mismatch for 1-2 word authentic inputs

- Design tradeoffs:
  - **T5-BASE vs. FLAN-T5-XL**: T5-BASE shows highest ChrF on synthetic data but occasionally generates semantically incorrect completions; FLAN-T5-XL produces more conservative but contextually appropriate outputs. Instruction fine-tuning in FLAN variants may improve semantic grounding.
  - **Spoken vs. written source corpus**: Using SBCSAE (spoken) rather than C4 (written web text) better matches conversational contexts but limits training data scale.
  - **Prefix inclusion**: Minimal impact on T5-BASE; slight improvement for other models suggests task framing matters more for instruction-tuned variants.

- Failure signatures:
  - **Added negations**: FLAN-T5-XL inserts negations in 2.99% of outputs, potentially inverting meaning
  - **Minimal or no modification**: Models sometimes reproduce input unchanged when uncertainty is high
  - **Conversational fillers**: Outputs like "Yeah" or "I do not know" reflect generic continuations rather than semantic completions
  - **Very short inputs**: 1-2 word fragments below training distribution produce unreliable outputs

- First 3 experiments:
  1. **Validate synthetic-real alignment**: Compute surprisal scores for held-out authentic AphasiaBank samples versus synthetic samples; verify they fall within comparable ranges (target: <0.5 difference on GPT-2/RoBERTa).
  2. **Ablate input length bins**: Stratify authentic test set by word count (1-2, 3-5, 6+ words); report ChrF and manual acceptability per bin to quantify context-dependency effects.
  3. **Negation audit**: Run inference on 100 authentic samples with manual annotation; flag any semantic inversions due to added negation and measure incidence rate relative to total completions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a reliable benchmark of human-annotated completions for authentic aphasic speech be established to enable standardized quantitative evaluation?
- Basis in paper: [explicit] The authors explicitly state that future research should ask native speakers to produce target completions for authentic utterances to create a "reliable benchmark," as the current lack of ground-truth data limits evaluation to qualitative analysis.
- Why unresolved: Authentic aphasic datasets currently lack "target" sentences, making it impossible to calculate standard performance metrics (like ChrF) against a gold standard for real patient data.
- What evidence would resolve it: The creation and publication of a dataset pairing authentic aphasic utterances with valid, human-generated sentence completions.

### Open Question 2
- Question: To what extent does fine-tuning on specific thematic domains (e.g., a patient's hobbies or daily tasks) improve completion accuracy compared to general spoken language training?
- Basis in paper: [explicit] In the Future Work section, the authors propose "studying the domain influence" to create task-specific models, hypothesizing that this could improve performance for individual patients.
- Why unresolved: The current study utilizes a general corpus of spoken English (SBCSAE), and it remains untested whether narrowing the training domain to patient-specific interests yields better reconstructive performance.
- What evidence would resolve it: An experiment comparing models fine-tuned on general data versus those fine-tuned on domain-specific data, evaluated on aphasic speech within those specific domains.

### Open Question 3
- Question: Can integrating multimodal inputs, such as visual cues, resolve the semantic ambiguity of severely fragmented sentences where text-only context is insufficient?
- Basis in paper: [inferred] The authors note a key limitation where recovering meaning from short fragments like "He book table" is "inherently challenging" and often requires "additional grounding such as... visual cues" which the current text-only models lack.
- Why unresolved: The current text-only LLMs occasionally hallucinate or fail on short utterances because multiple distinct meanings are grammatically plausible without visual or situational context.
- What evidence would resolve it: A study evaluating the performance of a multimodal model against the current text-only baseline on a dataset of short, ambiguous utterances situated in visual contexts.

## Limitations

- The study relies on synthetic training data that may not fully capture the clinical heterogeneity of authentic Broca's aphasia, limiting the model's exposure to true clinical patterns
- The absence of authentic paired data (agrammatic → original) prevents standardized quantitative evaluation on real patient utterances
- The focus on isolated sentence completion may not reflect the complexity of actual conversational rehabilitation contexts where discourse coherence is crucial

## Confidence

**High confidence**: The mechanism by which pretrained LLMs leverage linguistic knowledge to reconstruct fragmented inputs is well-established through the quantitative performance on synthetic data and the consistency with related work on language model lesioning. The length-dependency effect (longer inputs yielding better performance) is clearly demonstrated and mechanistically sound.

**Medium confidence**: The quality of synthetic data generation and its alignment with authentic aphasic patterns, while validated through surprisal measures, remains somewhat uncertain due to the lack of direct comparative evaluation with large-scale authentic paired data. The transferability of performance from synthetic to authentic data is plausible but not definitively established.

**Low confidence**: The clinical utility and practical deployment of these models in real rehabilitation settings remains speculative. The study does not address critical factors such as user interface design, integration with speech-language pathology workflows, or the potential for introducing errors that could reinforce incorrect speech patterns.

## Next Checks

1. **Cross-institutional authentic data validation**: Evaluate the best-performing model on authentic paired Broca's aphasic data from a different institution or corpus than AphasiaBank to test generalization across clinical populations and recording conditions.

2. **Clinical expert blinded evaluation**: Conduct a randomized controlled trial where speech-language pathologists rate model completions versus baseline (no assistance) on authentic patient data, measuring both accuracy and potential harm from incorrect completions.

3. **Longitudinal intervention study**: Test whether LLM-assisted sentence completion during therapy sessions leads to measurable improvements in spontaneous speech production over multiple sessions, compared to standard therapy alone.