---
ver: rpa2
title: 'mKG-RAG: Multimodal Knowledge Graph-Enhanced RAG for Visual Question Answering'
arxiv_id: '2508.05318'
source_url: https://arxiv.org/abs/2508.05318
tags:
- knowledge
- multimodal
- retrieval
- visual
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces mKG-RAG, a framework that enhances multimodal
  large language models for knowledge-based visual question answering by integrating
  multimodal knowledge graphs with retrieval-augmented generation. It addresses the
  limitations of existing RAG methods that rely on unstructured text and overlook
  structural knowledge relationships, which can introduce noise and reduce accuracy.
---

# mKG-RAG: Multimodal Knowledge Graph-Enhanced RAG for Visual Question Answering

## Quick Facts
- arXiv ID: 2508.05318
- Source URL: https://arxiv.org/abs/2508.05318
- Reference count: 40
- 36.3% accuracy on E-VQA and 40.5% on InfoSeek benchmarks

## Executive Summary
mKG-RAG addresses the limitations of existing RAG methods for knowledge-based visual question answering by integrating multimodal knowledge graphs with retrieval-augmented generation. The framework constructs modality-aligned knowledge graphs from multimodal documents using MLLM-powered keyword extraction and vision-text matching, then applies a dual-stage retrieval strategy for precise, efficient knowledge retrieval. Experiments show significant performance improvements over state-of-the-art methods on E-VQA and InfoSeek benchmarks.

## Method Summary
The proposed framework follows a two-phase pipeline: (1) Offline multimodal KG construction using Llama-3.2-11B-Vision for textual extraction combined with EGTR for scene graphs and vision-text matching; (2) Dual-stage retrieval with QM-Retriever (BLIP-2 Q-Former backbone, 25 epochs, contrastive loss + KL divergence) followed by graph retrieval with 1-hop expansion. The system is fine-tuned on LLaVA-More using LoRA with retrieved context from the knowledge base.

## Key Results
- Achieves 36.3% accuracy on E-VQA benchmark (fine-tuned)
- Achieves 40.5% accuracy on InfoSeek benchmark (fine-tuned)
- Outperforms state-of-the-art methods through dual-stage retrieval and knowledge graph integration
- Ablation studies confirm effectiveness of each component

## Why This Works (Mechanism)
The framework's effectiveness stems from its ability to capture structural knowledge relationships that traditional RAG methods miss. By constructing multimodal knowledge graphs and using a dual-stage retrieval approach, mKG-RAG can access both semantic and structural information, reducing noise from unstructured text retrieval and improving answer accuracy.

## Foundational Learning
- **Multimodal KG Construction**: Why needed - Captures structural relationships between visual and textual elements; Quick check - Verify matching rates between visual and textual graphs exceed 80%
- **Dual-stage Retrieval**: Why needed - Combines document-level and entity-level retrieval for comprehensive coverage; Quick check - Monitor retrieval recall@1 on validation set
- **Vision-text Alignment**: Why needed - Ensures consistency between visual and textual knowledge representations; Quick check - Validate alignment accuracy on sample images
- **Contrastive Learning**: Why needed - Trains retriever to match queries with relevant evidence; Quick check - Verify KL loss convergence with coefficient α=2
- **LoRA Fine-tuning**: Why needed - Efficiently adapts large models to task-specific knowledge; Quick check - Monitor training loss and validation accuracy
- **Scene Graph Generation**: Why needed - Extracts structural relationships from images; Quick check - Assess graph density and coverage

## Architecture Onboarding
- **Component Map**: Wikipedia documents -> Multimodal KG construction -> QM-Retriever training -> Dual-stage retrieval -> LLaVA-More fine-tuning -> Answer generation
- **Critical Path**: Document segmentation → KG construction → QM-Retriever training → Dual-stage retrieval → Answer generation
- **Design Tradeoffs**: Single-stage retrieval (faster, less accurate) vs. dual-stage retrieval (slower, more accurate)
- **Failure Signatures**: Low matching rates in KG construction, poor retrieval recall, model hallucinations during fine-tuning
- **First Experiment 1**: Build KG on 100 sample documents and verify matching rates exceed 80%
- **First Experiment 2**: Train QM-Retriever on 1K query-evidence pairs and verify recall@1 > 70%
- **First Experiment 3**: Fine-tune LLaVA-More on 100 samples and verify answer accuracy > 30%

## Open Questions the Paper Calls Out
- **Open Question 1**: How can the framework be extended to model and retrieve hyper-relationships involving multiple entities? (Paper focuses exclusively on binary relationships)
- **Open Question 2**: How susceptible is the system to error propagation from MLLM hallucinations during knowledge graph construction? (Heavy reliance on MLLMs for entity extraction)
- **Open Question 3**: Does the dual-stage retrieval approach maintain its superiority in specialized domains with distinct document structures? (Experiments limited to Wikipedia-based datasets)

## Limitations
- Performance depends on quality of MLLM-generated entities and relationships
- Limited evaluation to Wikipedia-based datasets (E-VQA, InfoSeek)
- Heavy reliance on specific prompt engineering not fully specified in paper

## Confidence
- **High confidence**: Overall pipeline architecture, dataset specifications, QM-Retriever training procedure, LLaVA-More fine-tuning configuration
- **Medium confidence**: Scene graph generation with EGTR, multimodal alignment process, dual-stage retrieval mechanism
- **Low confidence**: Exact prompt templates for LLM-based extraction and vision-text matching, ground-truth evidence annotation format, document filtering criteria

## Next Checks
1. Implement and validate vision-text matching alignment using Llama-3.2-11B-Vision on sample images, verifying matching rates exceed 80%
2. Train QM-Retriever on sample query-evidence pairs and verify retrieval recall@1 exceeds 70% on validation set
3. Conduct ablation studies on dual-stage retrieval by removing document or graph retrieval to confirm expected performance impact