---
ver: rpa2
title: Representation-based Broad Hallucination Detectors Fail to Generalize Out of
  Distribution
arxiv_id: '2509.19372'
source_url: https://arxiv.org/abs/2509.19372
tags:
- hallucination
- table
- recall
- precision
- ragtruth
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether current state-of-the-art (SOTA)
  hallucination detection methods that use model internals generalize beyond their
  training data. The authors find that SOTA methods, specifically ReDeEP, largely
  perform due to a spurious correlation with task type in the RAGTruth dataset, rather
  than genuine hallucinatory signals.
---

# Representation-based Broad Hallucination Detectors Fail to Generalize Out of Distribution

## Quick Facts
- arXiv ID: 2509.19372
- Source URL: https://arxiv.org/abs/2509.19372
- Authors: Zuzanna Dubanowska; Maciej Żelaszczyk; Michał Brzozowski; Paolo Mandica; Michał Karpowicz
- Reference count: 12
- Current SOTA hallucination detection methods largely perform due to spurious correlations with task type rather than genuine hallucinatory signals

## Executive Summary
This paper critically examines whether current state-of-the-art hallucination detection methods that use model internals generalize beyond their training data. The authors find that methods like ReDeEP achieve high performance on the RAGTruth dataset primarily by exploiting a spurious correlation between the D2T task type and high hallucination rates, rather than detecting genuine hallucinatory signals. Simple classifiers based on task type achieve similar or better performance than complex attention-based methods. More concerningly, when evaluated on out-of-distribution data, all methods perform near random chance, indicating they learn dataset-specific features rather than transferable hallucinatory patterns. The paper proposes guidelines for future hallucination detection research emphasizing narrow hallucination definitions, proper evaluation protocols, and rigorous cross-distribution testing.

## Method Summary
The paper evaluates several representation-based hallucination detection methods including ReDeEP (attention-based with JS divergence), SAPLMA (hidden state probing), linear and logistic regression probes, Random Forest probes, and sparse autoencoder (SAE) probes. The authors extract activations from residual stream positions (resid_pre, resid_mid) across various layers and train classifiers to predict hallucination presence. Evaluation is performed on multiple datasets including RAGTruth (QA/D2T/Summary tasks), SQuAD, and Dolly, with particular emphasis on cross-dataset generalization where methods trained on one dataset are evaluated on another.

## Key Results
- Naïve classifiers using only task type achieve AUC 0.71-0.81, matching or exceeding SOTA methods like ReDeEP (AUC 0.73)
- Simple linear probes on hidden states match or outperform complex methods like ReDeEP on RAGTruth
- Out-of-distribution generalization fails completely, with all methods performing near random (AUC ~0.50) when evaluated across datasets
- The D2T task in RAGTruth shows artificially high hallucination rates (86-95%) compared to QA and Summary tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Current SOTA hallucination detection on RAGTruth achieves high scores by exploiting task-type correlation, not genuine hallucinatory signals.
- Mechanism: The D2T task in RAGTruth has a hallucination rate of ~86-95% across models, while QA and Summary tasks have much lower rates (~22-52%). A naïve classifier that simply predicts "hallucination" when task=D2T achieves AUC 0.71-0.81, matching or exceeding SOTA methods like ReDeEP (AUC 0.73), SEP (0.71), and SAPLMA (0.70).
- Core assumption: The high hallucination rate in D2T is a dataset artifact, not a fundamental property of the task type.
- Evidence anchors:
  - [abstract] "its performance on the RAGTruth dataset is largely driven by a spurious correlation with data"
  - [section 3, Table 3] Naïve classifier achieves AUC 0.7119 vs ReDeEP token 0.7325; naïve even outperforms on PCC (0.4494 vs 0.3979)
  - [corpus] Limited direct corpus support; related work EigenTrack focuses on spectral features but doesn't address this specific spurious correlation
- Break condition: If hallucination rates were balanced across tasks, or if evaluation were performed per-task rather than aggregated, this mechanism would not operate.

### Mechanism 2
- Claim: Simple linear probes on hidden states can match or outperform complex SOTA methods, suggesting hallucination signals (if they exist) may be linearly decodable.
- Mechanism: The authors extract activations from residual stream positions (resid_pre, resid_mid) and train logistic regression classifiers. On RAGTruth with LLaMA-2 7B, linear regression achieves overall AUC 0.7951 vs ReDeEP's 0.7324. This suggests complex attention-based methods may be overfitting to dataset artifacts rather than extracting superior signal.
- Core assumption: The linear representation hypothesis applies to hallucination-relevant features in hidden states.
- Evidence anchors:
  - [abstract] "state-of-the-art performs no better than supervised linear probes"
  - [section 4.1, Table 5] Logistic Regression achieves 0.7951 AUC overall, outperforming ReDeEP (0.7324)
  - [corpus] No corpus papers directly address linear probe sufficiency for hallucination detection
- Break condition: If non-linear structure were essential for hallucination detection, linear probes would consistently underperform. Current data is inconclusive due to dataset artifacts.

### Mechanism 3
- Claim: Out-of-distribution generalization fails because current methods learn dataset-specific features rather than transferable hallucinatory patterns.
- Mechanism: When methods trained on RAGTruth are evaluated on SQuAD (or vice versa), all approaches perform near random (AUC ~0.50). ReDeEP with RAGTruth hyperparameters evaluated on Dolly achieves AUC 0.5005 (essentially random). Cross-task evaluation within RAGTruth shows similar degradation.
- Core assumption: There exists some transferable hallucinatory signal that methods could in principle learn, but current approaches fail to capture it.
- Evidence anchors:
  - [abstract] "Out-of-distribution generalization is currently out of reach, with all of the analyzed methods performing close to random"
  - [section 4.1, Table 6] RAGTruth→SQuAD: all methods show AUC 0.48-0.55 (random range)
  - [corpus] "Can We Trust LLM Detectors?" (corpus) confirms detectors "are brittle under distribution shift, unseen generators"
- Break condition: If hallucinations were defined narrowly (e.g., specific entity hallucinations) rather than broadly, generalization might be achievable.

## Foundational Learning

- Concept: **Spurious correlation in evaluation**
  - Why needed here: Critical for understanding why reported SOTA performance may be misleading. Without controlling for confounds like task type, apparent success may reflect dataset artifacts.
  - Quick check question: If I shuffled the hallucination labels within each task, would my detector's performance change dramatically?

- Concept: **Linear probes on transformer activations**
  - Why needed here: The comparison baseline for all representation-based methods. Understanding how to extract and probe hidden states is foundational for this research direction.
  - Quick check question: From which layer and token position should I extract activations, and does this choice affect my conclusions?

- Concept: **Cross-distribution evaluation protocol**
  - Why needed here: The paper's central critique is that single-dataset evaluation is insufficient. Proper OOD evaluation requires train/tune separation from test data.
  - Quick check question: Am I tuning hyperparameters on the same distribution I'm evaluating on?

## Architecture Onboarding

- Component map:
  - Datasets: RAGTruth (QA/D2T/Summary tasks with span-level labels), SQuAD (QA only), Dolly (mixed)
  - Methods under test: ReDeEP (attention + JS divergence), SAPLMA (hidden state probing), linear/logistic regression probes, Random Forest probes, SAE-based probes
  - Activation extraction points: resid_pre (pre-attention), resid_mid (pre-MLP), various layers
  - Metrics: AUC, Pearson correlation (PCC), Precision, Recall, F1

- Critical path:
  1. Define narrow hallucination type (not "all hallucinations")
  2. Establish per-task baselines including naïve classifiers
  3. Train on one dataset, tune hyperparameters on another, evaluate on a third
  4. Verify logical consistency (e.g., negation handling)
  5. Attempt span-level localization

- Design tradeoffs:
  - Broad vs. narrow hallucination definition: Broader = more training data but noisier signal; narrower = clearer signal but less coverage
  - Layer selection: Earlier layers may capture surface features; later layers may capture task-specific reasoning
  - Token position: Last token vs. max activation vs. chunked—each captures different temporal dynamics

- Failure signatures:
  - Near-random OOD performance indicates overfitting to training distribution
  - Large performance gap between per-task and aggregated metrics signals spurious correlation
  - Insensitivity to negation/logical manipulation suggests detecting artifacts, not truthfulness

- First 3 experiments:
  1. **Naïve baseline check**: Train a classifier using only task type (or prompt format features like JSON presence) on your dataset. If this matches your method's performance, you haven't learned hallucination signal.
  2. **Cross-dataset transfer**: Train on RAGTruth, evaluate on SQuAD (same task type, different distribution). AUC < 0.55 indicates no genuine generalization.
  3. **Layer sweep with linear probe**: Train logistic regression on each layer's activations separately. If performance is uniform across layers, you may be detecting surface features rather than reasoning-level signal.

## Open Questions the Paper Calls Out
None

## Limitations
- Results critically depend on RAGTruth dataset quality and may not generalize to other datasets or model architectures
- Broad hallucination definition may encompass too many phenomena, preventing effective signal extraction
- Does not explore domain adaptation techniques that might enable cross-distribution generalization
- Limited to Llama-family models, leaving open questions about other architectures

## Confidence

**High Confidence Claims**:
- Current SOTA methods perform no better than linear probes on RAGTruth when controlling for task type (AUC differences < 0.02)
- All methods show near-random performance when evaluated OOD (AUC 0.48-0.55)
- The D2T task in RAGTruth has substantially higher hallucination rates than QA and Summary tasks

**Medium Confidence Claims**:
- The high D2T hallucination rate is primarily a dataset artifact rather than a fundamental property of the task type
- Linear probes can match complex methods because hallucination signals are linearly decodable in hidden states
- Current methods learn dataset-specific features rather than transferable hallucinatory patterns

**Low Confidence Claims**:
- OOD generalization is fundamentally impossible for representation-based hallucination detection
- The spurious correlation mechanism operates identically across all representation-based methods
- Task type is the only or primary confounding factor affecting current detection performance

## Next Checks

1. **Controlled Spurious Correlation Test**: Shuffle hallucination labels within each task type in RAGTruth and retrain ReDeEP and linear probes. If performance remains similar to unshuffled results, this confirms that methods are exploiting task-level artifacts rather than genuine hallucinatory signals. Compare the AUC degradation between methods.

2. **Cross-Distribution Adaptation**: Take a method trained on RAGTruth, fine-tune it on SQuAD (same task type, different distribution), then evaluate on Dolly. Measure whether fine-tuning on an intermediate distribution improves OOD generalization compared to direct RAGTruth→Dolly transfer. This tests whether methods can learn transferable features with appropriate adaptation.

3. **Per-Hallucination-Type Analysis**: Select three distinct hallucination sub-types (e.g., factual errors, logical contradictions, entity confusions) and annotate a small subset of RAGTruth. Train separate binary classifiers for each type and evaluate both in-distribution and OOD. This would reveal whether different hallucination types leave different traces in model internals and whether some are more amenable to generalization than others.