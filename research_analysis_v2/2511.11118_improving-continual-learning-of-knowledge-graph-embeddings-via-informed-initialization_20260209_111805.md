---
ver: rpa2
title: Improving Continual Learning of Knowledge Graph Embeddings via Informed Initialization
arxiv_id: '2511.11118'
source_url: https://arxiv.org/abs/2511.11118
tags:
- learning
- embeddings
- knowledge
- initialization
- schema
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of initializing embeddings for
  new entities in continually updated Knowledge Graphs (KGs). Current methods often
  use random initialization, which can be inefficient and lead to catastrophic forgetting
  of previously learned knowledge.
---

# Improving Continual Learning of Knowledge Graph Embeddings via Informed Initialization

## Quick Facts
- arXiv ID: 2511.11118
- Source URL: https://arxiv.org/abs/2511.11118
- Reference count: 10
- Primary result: Schema-based initialization improves continual KGE learning by reducing catastrophic forgetting and accelerating convergence.

## Executive Summary
This paper addresses the challenge of initializing embeddings for new entities in incrementally updated Knowledge Graphs (KGs). The authors propose a schema-based initialization strategy that leverages class centroids from the KG's ontology, significantly improving the performance of continual learning for KGEs compared to random initialization. The method reduces catastrophic forgetting, accelerates knowledge acquisition, and decreases the number of training epochs needed for convergence.

## Method Summary
The authors propose a model-agnostic initialization strategy for new entities in continually updated KGs. It leverages the KG schema and previously learned embeddings to initialize new entity embeddings based on the class centroids of their associated classes, with a random perturbation to avoid degeneracy. The method operates by computing class centroids and standard deviations from existing embeddings, then initializing new entities as the average of their class centroids plus a scaled random perturbation. This approach is integrated with any continual learning method and KGE model, requiring only class membership metadata from the schema.

## Key Results
- Schema-based initialization improved Hits@3 by up to 40% compared to random initialization on FBinc-M dataset.
- Reduced the number of training epochs needed for convergence by a factor of 2-2.67.
- Benefits observed across various continual learning methods (FT, EWC, EMR, LKGE, incDE) and KGE models (TransE, RotatE, QuatE).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Initializing new entity embeddings near their likely final positions reduces catastrophic forgetting of previously learned knowledge.
- Mechanism: When new embeddings start far from optimal (random initialization), training requires larger gradient updates that propagate through shared triples, disrupting existing embeddings. Schema-based centroid initialization positions new entities closer to their semantic neighborhood, minimizing the gradient magnitude needed during optimization.
- Core assumption: Entities of the same class share semantic properties that translate to proximity in embedding space.
- Evidence anchors:
  - [abstract] "enhances the acquisition of new knowledge while reducing catastrophic forgetting"
  - [section: Introduction] Figure 1 illustrates the "potential disruption... when the embedding of a new entity... is initialized in a distant (left) or close (right) position from the optimal"
  - [corpus] Limited direct corpus support for this specific schema-centroid mechanism; related work focuses on model architectures rather than initialization strategies.
- Break condition: If class memberships are noisy, incomplete, or semantically meaningless (entities share class labels but have unrelated relational patterns), centroids may misguide initialization.

### Mechanism 2
- Claim: Schema-based initialization accelerates convergence, reducing training epochs by a factor of 2-2.67.
- Mechanism: By encoding class-level semantic information at initialization, new embeddings require fewer optimization steps to reach their final positions. This directly reduces computational cost and the window during which existing embeddings can be perturbed.
- Core assumption: The centroid of existing class embeddings approximates a reasonable starting region for new class members.
- Evidence anchors:
  - [abstract] "reducing the number of training epochs needed for convergence by a factor of 2-2.67"
  - [section: Experiments - Metrics Evolution] "Compared to Random initialization... the number of epochs to convergence is reduced by a factor between 2.16 and 2.67"
  - [corpus] No direct corpus evidence on initialization speedup; neighboring papers focus on embedding quality rather than training efficiency.
- Break condition: If the perturbation parameter γ is too large, it may negate the centroid proximity benefit; if too small, embeddings of entities with identical class memberships become degenerate (identical).

### Mechanism 3
- Claim: The method is model-agnostic and integrates with any KGE architecture or continual learning strategy.
- Mechanism: Initialization operates purely on the embedding vectors themselves, independent of the scoring function or model architecture. It only requires: (1) pre-existing embeddings, and (2) class membership metadata from the schema.
- Core assumption: Schema/class information is available and maintained alongside the KG.
- Evidence anchors:
  - [abstract] "model-agnostic initialization strategy"
  - [section: Schema-Based Initialization] "our proposed initialization... remains applicable across all models, as it depends only on the generated embeddings and not on the characteristics of the underlying model"
  - [corpus] Weak corpus connection; neighboring KGE papers (QuatE-D, SMART) focus on geometric transformations, not initialization portability.
- Break condition: For KGE models with constrained embedding spaces (e.g., complex-valued, quaternion), the centroid computation and perturbation must respect those constraints—current formulation assumes real-valued vectors.

## Foundational Learning

- Concept: **Catastrophic forgetting in continual learning**
  - Why needed here: The paper's core motivation is mitigating forgetting when updating KGEs incrementally. Without understanding this problem, the value of informed initialization is unclear.
  - Quick check question: Can you explain why updating a neural model on new data can degrade performance on previously learned data?

- Concept: **Knowledge Graph Embeddings (KGEs)**
  - Why needed here: The method operates directly on embedding vectors. Understanding that KGEs map entities/relations to dense vectors for link prediction is essential.
  - Quick check question: What is the link prediction task, and what metrics (MRR, Hits@k) measure KGE quality?

- Concept: **KG schemas and entity typing**
  - Why needed here: The method requires class/ontology membership for each entity to compute centroids.
  - Quick check question: Given an entity "Dune" in a movie KG, what classes might it belong to in a schema (e.g., owl:Class hierarchy)?

## Architecture Onboarding

- Component map:
  - Centroid store -> Initializer -> Continual learner wrapper

- Critical path:
  1. Before first incremental update: Compute class centroids from base KG embeddings using Eq. 2.
  2. For each new entity: Retrieve classes Ce, compute initial embedding via Eq. 3 (centroid average + γ·σ⊙random).
  3. Pass initialized embeddings to chosen continual learning method.
  4. After training: Update centroid statistics to include newly trained entities.

- Design tradeoffs:
  - **Perturbation magnitude (γ)**: Too small → degenerate embeddings; too large → centroid benefit lost. Paper found γ∈{0, 0.1, 0.5} worked best depending on update size.
  - **Multi-class entities**: Averaging centroids across classes may dilute signal for entities with many diverse class memberships.
  - **Schema availability**: Requires ontology/class labels; not all KGs have rich typing.

- Failure signatures:
  - **Identical initializations**: Multiple new entities with same classes and γ=0 produce duplicate embeddings → training instability.
  - **No improvement over random**: Centroids may be uninformative if class assignments are too coarse or misaligned with relational structure.
  - **Slow convergence despite initialization**: Check if centroids are computed from too few class exemplars (|Ec| small).

- First 3 experiments:
  1. **Ablation on γ**: Fix a continual method (e.g., fine-tuning) and vary γ∈{0, 0.1, 0.5, 1.0}. Measure Ωbase and Ωnew to find the sweet spot between degeneracy avoidance and centroid fidelity.
  2. **Class granularity sensitivity**: If schema allows hierarchical types, compare initializing from leaf classes vs. root classes. Hypothesis: finer-grained classes yield more informative centroids.
  3. **Cross-model validation**: Replicate Table 3 results on at least one translational (TransH) and one semantic (DistMult) model to confirm model-agnostic claim before production integration.

## Open Questions the Paper Calls Out

- **Open Question 1**
  - Question: Can the centroid-based initialization strategy be effectively extended to new relations using relation hierarchies?
  - Basis in paper: [explicit] The authors state in the Method section: "Note that the method could be extended to relation embeddings (i.e., by leveraging relation hierarchies from S)... although in practical scenarios the introduction of new relations is infrequent."
  - Why unresolved: The current work focuses exclusively on entity embeddings because relation updates are less common; however, the schema-based logic (hierarchies) for relations was proposed but not implemented or tested.
  - What evidence would resolve it: An extension of the proposed algorithm to relations using hierarchy centroids, evaluated on a dataset specifically constructed to include frequent relation additions.

- **Open Question 2**
  - Question: How does the performance of schema-based initialization degrade in the absence of a complete or clean schema (e.g., missing or erroneous class labels for new entities)?
  - Basis in paper: [inferred] The method relies on "class centroids" computed from "previously learned embeddings" associated with classes in the schema.
  - Why unresolved: The experiments utilize DBpedia mappings which are treated as ground truth. The paper does not analyze scenarios where the schema is sparse (entities with no class) or noisy, which is common in real-world open KGs.
  - What evidence would resolve it: Experiments measuring performance drops when random subsets of class information are removed or corrupted during the initialization phase of new entities.

- **Open Question 3**
  - Question: Does the simple averaging of class centroids effectively initialize entities that belong to multiple, semantically distinct classes?
  - Basis in paper: [inferred] The method initializes an entity using "the average centroid of the classes it belongs to" combined with a random perturbation.
  - Why unresolved: Averaging centroids assumes that the optimal embedding lies at the geometric center of all parent classes. For entities with conflicting types (e.g., a class for "Vehicles" and a class for "FictionalObjects"), the average vector might fall in a meaningless region of the embedding space.
  - What evidence would resolve it: A comparative analysis of initialization quality for single-typed versus multi-typed entities, or using learned weighting schemes rather than simple averaging.

## Limitations
- The effectiveness of schema-based initialization hinges on the quality and granularity of class memberships; if entities are poorly typed or classes are semantically broad, the centroid initialization may provide limited benefit.
- The method assumes real-valued embeddings and may require adaptation for models using complex, quaternion, or hyperbolic spaces.
- The perturbation parameter γ is sensitive: too small leads to degenerate embeddings, too large negates the centroid proximity benefit.

## Confidence
- **High**: The claim that informed initialization improves link prediction metrics (MRR, Hits@3) is well-supported by the experimental results across multiple datasets and continual learning methods.
- **Medium**: The claim that the method is truly model-agnostic is supported by testing across different KGE models, but deeper architectural integration (e.g., with non-Euclidean embeddings) is not demonstrated.
- **Medium**: The claim of reduced training epochs (2-2.67x speedup) is quantified, but the mechanism (fewer gradient updates needed) is inferred rather than directly measured.

## Next Checks
1. **Ablation on Perturbation γ**: Fix a continual method (e.g., fine-tuning) and systematically vary γ∈{0, 0.1, 0.5, 1.0}. Measure Ω_base and Ω_new to find the optimal balance between avoiding degeneracy and maintaining centroid fidelity.
2. **Class Granularity Sensitivity**: If the schema allows hierarchical typing, compare initializing from leaf classes vs. root classes. Hypothesize that finer-grained classes yield more informative centroids.
3. **Cross-Model Validation**: Replicate the key results (e.g., Table 3) on at least one translational (TransH) and one semantic (DistMult) KGE model to further confirm the model-agnostic claim before production integration.