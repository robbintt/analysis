---
ver: rpa2
title: Thompson Sampling via Fine-Tuning of LLMs
arxiv_id: '2510.13328'
source_url: https://arxiv.org/abs/2510.13328
tags:
- optimization
- bayesian
- learning
- reward
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of scaling Bayesian optimization
  to large unstructured discrete spaces, where traditional methods face computational
  barriers due to the absence of gradients. The authors propose Thompson Sampling
  via Fine-Tuning (TOSFIT), a scalable approach that leverages pre-trained large language
  models (LLMs) to parameterize the probability of maximality directly, eliminating
  the need for acquisition function maximization.
---

# Thompson Sampling via Fine-Tuning of LLMs

## Quick Facts
- **arXiv ID:** 2510.13328
- **Source URL:** https://arxiv.org/abs/2510.13328
- **Reference count:** 40
- **Primary result:** Proposes Thompson Sampling via Fine-Tuning (TOSFIT), a scalable Bayesian optimization method for large unstructured discrete spaces that leverages pre-trained LLMs to parameterize the probability of maximality, eliminating acquisition function maximization and achieving state-of-the-art sample efficiency across three diverse tasks.

## Executive Summary
This paper addresses the fundamental challenge of scaling Bayesian optimization to large unstructured discrete spaces where traditional methods face computational barriers due to the absence of gradients. The authors propose TOSFIT, which leverages pre-trained large language models (LLMs) to parameterize the probability of maximality directly, eliminating the need for acquisition function maximization. The method initializes the policy using prior knowledge embedded in prompt-conditioned LLMs and incrementally adapts it toward the posterior using the Variational Bayesian Optimistic Sampling (VBOS) objective. Theoretically, the authors derive a novel regret bound for VBOS that matches Thompson sampling guarantees while accounting for reward correlation across the search space. Empirically, TOSFIT is validated on three diverse tasks—FAQ response refinement, thermally stable protein search, and quantum circuit design—demonstrating state-of-the-art sample efficiency and computational efficiency compared to Bayesian optimization, reinforcement learning, and evolutionary search methods.

## Method Summary
TOSFIT circumvents acquisition function maximization in discrete spaces by directly parameterizing the sampling policy using a pre-trained LLM. The method employs a Gaussian Process surrogate model with linear kernel features from task-specific embeddings, and optimizes the VBOS objective V(π) = E[μ_x + √(-2ln(π_θx))·σ_x] using vanilla SGD. The approach includes a burn-in phase where m=16 candidates are sampled to fit GP hyperparameters, followed by iterative fine-tuning where B=16 candidates are generated per round. The VBOS gradients are estimated using the Reinforce Leave-One-Out (RLOO) baseline for variance reduction. The method is validated across three tasks using different LLMs and embedding models: FAQ refinement with Qwen3-1.7B and cosine similarity rewards, protein stability search with ProtGPT2-0.7B and stability scores, and quantum circuit design with Qwen2.5-Coder-1.5B and Hamiltonian-based rewards.

## Key Results
- TOSFIT achieves state-of-the-art sample efficiency on FAQ response refinement, protein stability search, and quantum circuit design tasks compared to Bayesian optimization, reinforcement learning, and evolutionary search baselines
- The method demonstrates superior computational efficiency by avoiding acquisition function maximization, with wall-clock time improvements ranging from 2x to 10x across tasks
- Theoretical regret bounds show sublinear cumulative regret scaling with maximal information gain γ_T rather than search space size, improving from O(√(T|X|)) to O(√(Tγ_T))

## Why This Works (Mechanism)

### Mechanism 1: Direct Parameterization of Probability of Maximality
TOSFIT circumvents the intractability of acquisition function maximization in discrete spaces by parameterizing the sampling policy directly using a pre-trained LLM. Instead of maximizing a surrogate acquisition function (which requires gradients absent in discrete spaces), the method uses the LLM to parameterize the Probability of Maximality (PoM). Generating a candidate from this policy is equivalent to drawing a Thompson Sample. The pre-trained LLM provides a sufficient prior over the discrete space so that the policy can converge to the optimal region via fine-tuning. However, if the optimal solution lies outside the support of the pre-trained LLM's distribution, the initial policy probability P(x*) will be near-zero, potentially making discovery impossible regardless of fine-tuning.

### Mechanism 2: Variational Bayesian Optimistic Sampling (VBOS) Objective
Fine-tuning the LLM using the VBOS objective aligns the policy with the posterior reward distribution while retaining exploration properties. The algorithm maximizes V(π) = E[μ_x + σ_x√(-2ln(π_x))], which acts as an adaptive Upper Confidence Bound where the confidence bonus depends on the probability assigned by the policy. This effectively functions as an energy-based model, pushing probability mass toward regions with high predicted reward (μ_x) and high uncertainty (σ_x). The reward model (Gaussian Process) must accurately reflect the uncertainty σ_x of unseen candidates; underestimated uncertainty leads to premature convergence. The VBOS gradient estimation uses RLOO for variance reduction, which the paper shows is mathematically equivalent to Group Relative Policy Optimization (GRPO).

### Mechanism 3: Information-Theoretic Regret Bound
The theoretical framework guarantees sublinear cumulative regret even for approximate VBOS, scaling with information gain rather than search space size. The authors bound regret using the maximal information gain γ_T of the Gaussian Process kernel, accounting for correlation across the discrete search space. This improves the bound from Õ(√(T|X|)) (vacuous for large discrete spaces) to Õ(√(Tγ_T)). The theoretical guarantees assume the Bregman divergence D_σ(π_t, π̃_t) between the policy and the optimal VBOS solution is kept small through careful fine-tuning. However, if the "careful adaptation" fails (e.g., learning rate is too high), the Bregman divergence term dominates the regret bound, invalidating the sublinear guarantee.

## Foundational Learning

- **Concept: Thompson Sampling**
  - **Why needed here:** This is the core algorithmic strategy. Standard Thompson Sampling draws a sample function from the posterior and maximizes it. TOSFIT modifies this by directly sampling a candidate from a policy that approximates the Probability of Maximality (PoM).
  - **Quick check question:** How does generating a sample from the LLM policy replace the traditional step of "maximizing a sample function from the posterior"?

- **Concept: Gaussian Processes (GPs)**
  - **Why needed here:** The GP serves as the surrogate reward model. It provides the mean μ_x and uncertainty σ_x required to calculate the VBOS objective and guide the LLM fine-tuning.
  - **Quick check question:** Why is the uncertainty estimate σ_x from the GP critical for the VBOS exploration bonus?

- **Concept: Reinforce Leave-One-Out (RLOO) / GRPO**
  - **Why needed here:** Fine-tuning the LLM requires estimating gradients of the VBOS objective. RLOO is used as a variance reduction technique for these policy gradients, which the paper shows is mathematically equivalent to Group Relative Policy Optimization (GRPO).
  - **Quick check question:** How does the RLOO baseline stabilize the gradient estimation compared to standard REINFORCE?

## Architecture Onboarding

- **Component map:** Pre-trained LLM -> Gaussian Process with linear kernel -> Vanilla SGD optimizer
- **Critical path:**
  1. **Burn-in:** Sample initial candidates to fit GP hyperparameters
  2. **Generation:** Sample candidates x from the current LLM policy
  3. **Evaluation:** Observe rewards y and update the GP posterior (computing μ_x, σ_x)
  4. **Optimization:** Calculate the VBOS pseudo-reward and advantage function; perform one step of gradient ascent on the LLM weights

- **Design tradeoffs:**
  - **Sample Efficiency vs. Compute:** Increasing gradient steps (c) per round improves sample efficiency but increases wall-clock time (Section 5.5)
  - **Model Size:** Larger LLMs help natural language tasks significantly but offer diminishing returns for rigid domains like protein search or quantum circuit design (Section 5.3)
  - **Memory vs. Stability:** The authors use vanilla SGD instead of Adam to reduce memory overhead, trading off potentially faster convergence for lower RAM usage (Section A.3)

- **Failure signatures:**
  - **Diversity Collapse:** If the uncertainty term in VBOS is ignored or σ_x vanishes too quickly, the policy may collapse to a local optimum (contrast with Actor-Critic in Figure 5)
  - **Prior Forgetting:** High learning rates cause the model to forget the strong pre-trained prior, leading to stagnation (Figure 9)

- **First 3 experiments:**
  1. **Toy Task / Burn-in Check:** Verify that the GP hyperparameters (amplitude, noise) stabilize quickly during the burn-in phase (m=16 steps)
  2. **Gradient Variance Test:** Monitor the advantage function variance with and without the RLOO baseline to ensure stable gradients
  3. **Policy Divergence:** Run a sweep on the learning rate (η); plot generation entropy to confirm the policy explores before exploiting, ensuring it doesn't diverge from the pre-trained prior

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can deep, task-adaptive embeddings be learned jointly with the Gaussian process during optimization to improve upon the fixed feature maps used in current experiments?
- **Basis in paper:** Section 7.1 states, "A promising direction for future work is to learn deep, task-adaptive embeddings jointly with the Gaussian process... or replace the GP with more expressive reward models."
- **Why unresolved:** The authors evaluate ToSFiT using fixed feature maps derived from pre-trained embeddings or manually designed features, rather than updating the feature extractor online.
- **What evidence would resolve it:** Empirical results showing convergence speed and final reward when the kernel feature map φ is updated via gradient ascent alongside the LLM policy, compared to the fixed baselines.

### Open Question 2
- **Question:** Can more expressive probabilistic models, such as Bayesian neural networks or ensembles, replace the Gaussian Process surrogate without destabilizing the VBOS gradients?
- **Basis in paper:** Section 7.1 lists replacing "the GP with more expressive reward models such as Bayesian neural networks or ensembles" as a direction for future work.
- **Why unresolved:** The theoretical analysis and gradient derivations rely on the specific properties of Gaussian Processes (e.g., closed-form posterior variances σ_x).
- **What evidence would resolve it:** Derivation of the VBOS objective gradient using a BNN uncertainty estimate and subsequent experiments demonstrating if the regret bounds hold or if sample efficiency improves on complex, non-Gaussian reward landscapes.

### Open Question 3
- **Question:** Does restricting fine-tuning updates to the final layers of the LLM significantly reduce computational overhead while maintaining the regret guarantees of full fine-tuning?
- **Basis in paper:** Section 7.1 suggests that "to reduce the computational and memory overhead introduced by fine-tuning, one could restrict updates to the last few layers of the generator."
- **Why unresolved:** The paper utilizes weight adaptation (fine-tuning) which introduces latency overhead (Table 2), but it does not test if parameter-efficient fine-tuning (PEFT) methods preserve the alignment with the probability of maximality required by Theorem 1.
- **What evidence would resolve it:** A comparison of wall-clock time, memory usage, and cumulative regret between full-parameter fine-tuning and last-layer-only tuning across the three benchmark tasks.

### Open Question 4
- **Question:** Can in-context conditioning strategies optimize the VBOS objective as effectively as weight adaptation?
- **Basis in paper:** Section 7.1 proposes that "alternative strategies for optimizing the VBOS objective V(π)—such as in-context conditioning—may offer a lightweight alternative to weight adaptation."
- **Why unresolved:** ToSFiT currently modifies the policy weights θ to align π_θ with the posterior; the paper does not test if prompt-based updates (modifying context rather than weights) can achieve the same alignment without the computational cost of backpropagation.
- **What evidence would resolve it:** Implementation of a ToSFiT variant that optimizes a context vector rather than model weights, evaluated against the standard ToSFiT on the FAQ or Quantum circuit tasks.

## Limitations

- The theoretical regret bound's practical relevance depends heavily on the unvalidated assumption about Bregman divergence control during fine-tuning
- The method requires careful selection of learning rates and burn-in steps to prevent prior forgetting or diversity collapse
- Computational efficiency gains come at the cost of repeated LLM fine-tuning, which may limit scalability to very large models or real-time applications

## Confidence

- **High Confidence:** The core mechanism of using LLMs to parameterize the probability of maximality and avoiding acquisition function maximization is well-supported by the theoretical framework and empirical results across all three tasks
- **Medium Confidence:** The VBOS objective's effectiveness in balancing exploration and exploitation is demonstrated, but the variance reduction properties of RLOO relative to standard REINFORCE are primarily shown through internal consistency rather than direct comparison
- **Low Confidence:** The theoretical regret bound's practical relevance depends heavily on the unvalidated assumption about Bregman divergence control during fine-tuning

## Next Checks

1. **Bregman Divergence Monitoring:** Track the divergence between the policy distribution and the optimal VBOS solution during fine-tuning across multiple seeds to empirically validate the theoretical regret bound's assumptions

2. **Prior Coverage Analysis:** Systematically measure the probability mass assigned by the pre-trained LLM to optimal solutions in each task to quantify the risk of failure when optimal regions lie outside the initial support

3. **RLOO Variance Comparison:** Implement and compare TOSFIT with both RLOO and standard REINFORCE baselines on a simple discrete optimization task, measuring gradient variance and convergence stability to isolate the impact of the variance reduction technique