---
ver: rpa2
title: LLM-Driven Stationarity-Aware Expert Demonstrations for Multi-Agent Reinforcement
  Learning in Mobile Systems
arxiv_id: '2511.19368'
source_url: https://arxiv.org/abs/2511.19368
tags:
- uni00000013
- uni00000003
- agent
- policy
- uni00000048
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the non-stationarity problem in multi-agent
  reinforcement learning (MARL) systems caused by simultaneous policy updates among
  multiple agents. This non-stationarity leads to unstable training and poor policy
  convergence, particularly as the number of agents increases.
---

# LLM-Driven Stationarity-Aware Expert Demonstrations for Multi-Agent Reinforcement Learning in Mobile Systems

## Quick Facts
- **arXiv ID:** 2511.19368
- **Source URL:** https://arxiv.org/abs/2511.19368
- **Reference count:** 40
- **Primary result:** RELED achieves superior performance in multi-agent traffic navigation with improved sample and time efficiency across various traffic conditions.

## Executive Summary
This paper addresses the non-stationarity problem in multi-agent reinforcement learning (MARL) caused by simultaneous policy updates among multiple agents. The authors propose RELED, a scalable MARL framework that integrates large language model (LLM)-driven expert demonstrations with autonomous agent exploration. The key innovation is a Stationarity-Aware Expert Demonstration module that leverages theoretical non-stationarity bounds to refine LLM-generated expert trajectories, providing high-reward and training-stable samples for each agent. Extensive experiments with real city networks based on OpenStreetMap demonstrate that RELED achieves superior performance compared to state-of-the-art MARL methods.

## Method Summary
RELED combines LLM-generated expert demonstrations with agent learning in a hybrid framework. The method uses a Stationarity-Aware Expert Demonstration (SED) module that employs theoretical non-stationarity bounds (Reward Volatility Index and Policy Divergence Index) to refine LLM-generated trajectories. A Hybrid Expert-Agent Policy Optimization (HPO) module adaptively balances learning between expert and agent-generated trajectories using Dynamic Time Warping distance as a policy alignment metric. The system partitions agents into subsets to reduce computational complexity of bound estimation. Training uses a weighted loss combining expert and agent policies, with the weight dynamically adjusted based on trajectory alignment.

## Key Results
- RELED achieves higher average episode rewards and lower travel times compared to IPPO, MAPPO, and QMIX baselines
- The framework demonstrates improved sample efficiency, requiring fewer interaction steps to converge
- RELED scales better with increasing agent numbers, maintaining performance more gracefully than baseline methods
- The stationarity-aware refinement mechanism contributes significantly to performance gains

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Integrating theoretical non-stationarity bounds as feedback refines LLM-generated expert demonstrations to reduce environmental volatility.
- **Mechanism:** The SED module calculates Reward Volatility Index (RVI) and Policy Divergence Index (PDI) based on theoretical bounds. These metrics quantify worst-case impacts of policy changes and are fed back to the LLM via structured prompts to revise unstable instructions.
- **Core assumption:** The LLM can interpret quantitative constraints and adjust its semantic planning logic to produce executable trajectories that satisfy non-stationarity bounds.
- **Evidence anchors:** Abstract states SED "leverages theoretical non-stationarity bounds to enhance the quality of LLM-generated expert trajectories." Section III-D describes using RVI and PDI as feedback metrics. Corpus neighbor "LEED" supports LLM-empowered expert demonstrations.
- **Break condition:** If LLM fails to lower RVI after iterations or generated code fails execution, refinement loop stalls.

### Mechanism 2
- **Claim:** Adaptive weighting between expert and agent losses enables controlled transition from imitation to autonomous exploration.
- **Mechanism:** HPO module computes dynamic weight $\alpha = \exp(-\dots \cdot D_{DTW})$ based on Dynamic Time Warping distance between expert and agent trajectories. Large DTW reduces $\alpha$ (prioritizing expert), while alignment increases $\alpha$ (prioritizing agent exploration).
- **Core assumption:** DTW distance serves as valid proxy for "policy alignment" and readiness to shift from external guidance to internal reinforcement learning.
- **Evidence anchors:** Abstract mentions "adaptively balances each agent's learning from both expert-generated and agent-generated trajectories." Section III-E describes DTW-based weight adjustment. Corpus neighbor "LERO" supports hybrid approach with specific weighting mechanisms.
- **Break condition:** If agent learns "spam" behaviors that lower DTW without improving reward, or if $\alpha$ oscillates preventing convergence.

### Mechanism 3
- **Claim:** Theoretical decomposition of non-stationarity bounds into agent subsets makes stability estimation scalable.
- **Mechanism:** Instead of evaluating every agent's impact on every other agent, Theorem 1 partitions agents into $m$ disjoint subsets. Bounds the objective change of agents outside a subset based on policy change inside the subset, reducing complexity from $O(n)$ to $m$ subset-level checks.
- **Core assumption:** Randomly partitioning agents into subsets allows the bound to generalize sufficiently to stabilize joint policy updates.
- **Evidence anchors:** Section III-C states bound estimation reduces computational cost to $m$ evaluations. Section IV-A2 uses $m=2$. General MARL literature validates need for subset-based approximations.
- **Break condition:** If subset size is too large, bound becomes loose and uninformative; if too small, misses critical cross-agent dependencies.

## Foundational Learning

- **Concept: Non-Stationarity in MARL**
  - **Why needed here:** Standard RL assumes stationary environment, but in MARL, the "environment" includes other agents who are simultaneously learning and changing policies, destabilizing training.
  - **Quick check question:** If Agent A learns a policy that works well against Agent B's current policy, will it still work if Agent B updates its policy in the next epoch?

- **Concept: Dynamic Time Warping (DTW)**
  - **Why needed here:** This metric measures similarity between temporal sequences (trajectories) that may vary in speed. Used in RELED to determine how closely agent follows LLM expert's path.
  - **Quick check question:** Does high DTW distance imply agent is exploring new area or just moving slower/faster than expert?

- **Concept: Policy Divergence (KL Divergence)**
  - **Why needed here:** Measures difference between two probability distributions (policies). RELED uses maximum KL divergence to bound how much expert update can disrupt other agents.
  - **Quick check question:** If KL divergence between expert demonstration and agent's current policy is infinite, what does that imply about agent's probability of taking expert's preferred action?

## Architecture Onboarding

- **Component map:**
  1. Environment (SUMO/OSM) provides state/reward
  2. SED Module contains LLM and Feedback Calculator, generates expert trajectories
  3. HPO Module contains Agent Policy/Value Networks, calculates DTW and Hybrid Loss
  4. Trajectory Buffer stores expert ($\tau^e$) and agent ($\tau^a$) trajectories

- **Critical path:**
  1. Agent interacts with Environment → Agent Trajectory
  2. Partition Agents → SED generates Expert Trajectories for subsets
  3. Calculate RVI/PDI → Update LLM Prompt
  4. HPO calculates DTW between Agent/Expert trajectories
  5. Update weights ($\alpha$) → Optimize Hybrid Loss → Update Agent Policy

- **Design tradeoffs:**
  - Inference Cost: Single refinement takes ~7.35s (GPT-3.5), trading wall-clock time for sample efficiency
  - Subset Size ($m$): Smaller subsets mean tighter bounds but more frequent LLM calls/overhead
  - Model Size: Larger LLMs (GPT-4) yield higher rewards but drastically increase inference time

- **Failure signatures:**
  - Demonstration Failure: LLM hallucinates map nodes or creates invalid routing instructions, dropping "Execution Rate"
  - Stationary Collapse: RVI/PDI constraints too loose, system fails to mitigate non-stationarity, agent rewards oscillate or drop
  - Over-imitation: $\alpha$ remains low, agent fails to explore and generalizes poorly to new traffic conditions

- **First 3 experiments:**
  1. Baseline Convergence: Run RELED vs. IPPO/MAPPO/QMIX on grid city to verify if RELED reaches higher reward with fewer steps
  2. Ablation on Feedback: Run RELED without RVI/PDI feedback loop to measure specific contribution of stationarity-aware refinement
  3. Scalability Stress Test: Increase agents (5 to 20) to confirm RELED's performance degrades more gracefully than baselines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can RELED generalize effectively to continuous action spaces in autonomous driving and robotic control?
- Basis in paper: [explicit] Conclusion states future direction is extending method to applications like robot control and autonomous driving
- Why unresolved: Current evaluation limited to discrete action spaces in traffic navigation tasks using specific function calls
- What evidence would resolve it: Successful deployment and evaluation in high-fidelity robotic simulators (e.g., MuJoCo) or autonomous driving benchmarks

### Open Question 2
- Question: How does RELED perform in environments lacking "predefined navigation interfaces" used to guide LLM?
- Basis in paper: [inferred] Section IV-C defines specific function interfaces (e.g., `move_to_by_shortest_path`) that LLM must utilize to generate executable trajectories
- Why unresolved: Method relies on these high-level primitives to bridge LLM and environment; performance in primitive-sparse or raw-state environments untested
- What evidence would resolve it: Ablation studies in environments where LLM must infer low-level actions without provided utility functions

### Open Question 3
- Question: Does theoretical bound and partitioning mechanism remain computationally tractable with significantly more agents (e.g., >100)?
- Basis in paper: [inferred] Section V-D tests scalability only up to 20 agents, noting while RELED scales better than baselines, performance still degrades as agent count increases
- Why unresolved: Subset partitioning ($m$ subsets) reduces complexity, but interaction between increased group size and non-stationarity bounds at massive scale unverified
- What evidence would resolve it: Empirical results demonstrating performance stability and wall-clock training time in large-scale swarm scenarios

## Limitations

- **LLM dependency and execution rate:** The framework's performance critically depends on the LLM's ability to generate executable code, with execution rate dropping below 70% indicating potential failure
- **Computational overhead:** The stationarity-aware refinement adds significant inference time (~7.35s per refinement), creating a tradeoff between sample efficiency and wall-clock time
- **Domain specificity:** The method relies on predefined navigation interfaces and is currently validated only in urban traffic scenarios, limiting generalizability to other domains

## Confidence

- **High Confidence:** Problem statement (non-stationarity in MARL) is well-established and proposed solution (LLM-driven expert demonstrations with adaptive weighting) is logical extension of existing literature. Experimental setup using SUMO and OSM is standard and reproducible.
- **Medium Confidence:** Theoretical decomposition of non-stationarity bounds (Theorem 1) is sound but practical effectiveness depends on LLM's code generation ability and choice of subset size $m$. Adaptive weighting mechanism (DTW-based $\alpha$) is plausible but requires careful tuning.
- **Low Confidence:** Specific prompt templates and feedback representation used to integrate RVI/PDI indices into LLM are not detailed, making it difficult to assess robustness of stationarity-aware refinement. Generalizability to other domains beyond urban traffic navigation is uncertain.

## Next Checks

1. **Execution Rate Validation:** Measure execution rate of LLM-generated code in controlled environment. If execution rate drops below 70%, investigate LLM's ability to generate valid Python code using provided interfaces.

2. **DTW Alignment Monitoring:** Track DTW distance and adaptive weight $\alpha$ during training. If $\alpha$ remains low (indicating over-reliance on expert demonstrations), investigate calculation of adaptive weight and consider alternative alignment metrics.

3. **Subset Size Sensitivity:** Conduct sensitivity analysis on subset size $m$ used in stationarity-aware bound estimation. Measure impact of different subset sizes on computational overhead and effectiveness of non-stationarity mitigation.