---
ver: rpa2
title: 'FlowPure: Continuous Normalizing Flows for Adversarial Purification'
arxiv_id: '2505.13280'
source_url: https://arxiv.org/abs/2505.13280
tags:
- adversarial
- flowpure
- purification
- noise
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FlowPure introduces a novel adversarial purification method based
  on Continuous Normalizing Flows (CNFs) trained with Conditional Flow Matching (CFM)
  to directly map adversarial examples to their clean counterparts. Unlike diffusion-based
  approaches that rely on fixed noise processes, FlowPure can incorporate specific
  attack knowledge during training for improved robustness against known threats,
  while also supporting a stochastic variant trained on Gaussian perturbations for
  general defense.
---

# FlowPure: Continuous Normalizing Flows for Adversarial Purification

## Quick Facts
- arXiv ID: 2505.13280
- Source URL: https://arxiv.org/abs/2505.13280
- Authors: Elias Collaert; Abel Rodríguez; Sander Joos; Lieven Desmet; Vera Rimmer
- Reference count: 40
- Key outcome: FlowPure achieves state-of-the-art adversarial purification performance, preserving benign accuracy while significantly improving robustness against PGD and CW attacks on CIFAR-10 and CIFAR-100.

## Executive Summary
FlowPure introduces a novel adversarial purification method based on Continuous Normalizing Flows (CNFs) trained with Conditional Flow Matching (CFM) to directly map adversarial examples to their clean counterparts. Unlike diffusion-based approaches that rely on fixed noise processes, FlowPure can incorporate specific attack knowledge during training for improved robustness against known threats, while also supporting a stochastic variant trained on Gaussian perturbations for general defense. The method achieves state-of-the-art performance in preprocessor-blind scenarios, fully preserving benign accuracy while significantly improving robustness against PGD and CW attacks on CIFAR-10 and CIFAR-100. In white-box settings, FlowPure's Gaussian variant outperforms existing purification methods, though all approaches struggle under fully adaptive attacks. Additionally, FlowPure demonstrates strong potential as an adversarial detector, achieving near-perfect detection accuracy against PGD attacks across various perturbation levels.

## Method Summary
FlowPure is a neural network-based adversarial purification method that uses Continuous Normalizing Flows (CNFs) trained via Conditional Flow Matching (CFM) to map adversarial examples back to clean data distributions. The method trains a velocity field $v_\theta(t, x)$ that transports samples from adversarial distributions directly to clean distributions along optimal transport paths. FlowPure can be trained in deterministic mode (incorporating specific attack knowledge) or stochastic mode (using Gaussian noise). During inference, the method integrates an ODE to purify inputs before classification. The approach achieves state-of-the-art performance in preprocessor-blind scenarios while also demonstrating strong detection capabilities based on velocity magnitude.

## Key Results
- FlowPure achieves 92.1% robust accuracy against PGD attacks on CIFAR-10 while maintaining 96.1% standard accuracy in preprocessor-blind settings.
- The Gaussian variant of FlowPure outperforms existing purification methods in white-box settings, achieving 80.4% robust accuracy on CIFAR-10.
- FlowPure demonstrates near-perfect detection accuracy (AUC ≈ 1.0) against PGD attacks across various perturbation levels.
- All evaluated purification methods, including FlowPure, fail under fully adaptive white-box attacks, with robust accuracy dropping below 45% on CIFAR-10.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Direct distribution-to-distribution mapping can purify adversarial examples without intermediate noise injection.
- Mechanism: Conditional Flow Matching (CFM) learns a time-dependent velocity field $v_\theta(t, x)$ that transports samples from an adversarial distribution directly to the clean data distribution along optimal transport (OT) paths. The velocity field is trained via $L_{CFM} = \mathbb{E}_{t,z,x}[||v_\theta(t, x) - u_t(x|x^{adv}, x^{clean})||_2^2]$, where $u_t(x|x^{adv}, x^{clean}) = x^{adv} - x^{clean}$.
- Core assumption: Adversarial perturbations form a learnable distribution that can be mapped back to the clean manifold via continuous dynamics.
- Evidence anchors:
  - [abstract] "FlowPure...based on Continuous Normalizing Flows (CNFs) trained with Conditional Flow Matching (CFM) to learn mappings from adversarial examples to their clean counterparts."
  - [section 2.3] "Unlike diffusion models, which rely on Gaussian priors, CNFs can use any source distribution and are not limited to Gaussian sources."
  - [corpus] Related work on tensor network purification (FMR=0.65) addresses similar purification goals but through different representations; no direct corpus evidence validates CNF-specific purification claims.
- Break condition: If adversarial perturbations are too sparse or diverse for a single velocity field to capture, the ODE integration will fail to converge to clean samples.

### Mechanism 2
- Claim: Incorporating attack-specific knowledge during training improves robustness against known threats.
- Mechanism: The latent distribution $p(z) = p(x^{clean})p(x^{adv}|x^{clean})$ is constructed by generating adversarial examples using specific attacks (PGD, CW) with randomized parameters $\xi \sim p(\xi)$ during training. This conditions the flow on known perturbation patterns.
- Core assumption: The training attack distribution sufficiently covers the test attack distribution; attacks can be parametrically characterized.
- Evidence anchors:
  - [abstract] "FlowPure can incorporate specific attack knowledge during training for improved robustness against known threats."
  - [section 3] "Since the attacker's exact parameters are unknown, we randomly vary attack parameters during training, ensuring robustness across different attack settings."
  - [corpus] De-AntiFake (FMR=0.54) discusses protective perturbations being mitigated by determined attackers, suggesting attack-specific defenses have limitations.
- Break condition: If test attacks use fundamentally different perturbation patterns (e.g., different norms or optimization objectives), the learned velocity field may misdirect samples.

### Mechanism 3
- Claim: Stochastic Gaussian-variant provides broader robustness when attack knowledge is unavailable.
- Mechanism: Gaussian FlowPure trains the CNF to transport samples from $\mathcal{N}(x^{clean}, \sigma^2_{max})$ to clean data using OT paths. At inference, noise $\sigma \leq \sigma_{max}$ is injected, and ODE integration starts from $t = 1 - \sigma/\sigma_{max}$. The straighter OT paths (vs. curved diffusion trajectories) provide more stable integration.
- Core assumption: Gaussian noise adequately models or dilutes unknown adversarial perturbations; OT paths generalize better than diffusion paths.
- Evidence anchors:
  - [abstract] "a stochastic variant trained on Gaussian perturbations for general defense...outperforms existing purification methods [in white-box settings]."
  - [section 4.3] "Gaussian FlowPure consistently achieves higher robust accuracy than DiffPure across a range of noise levels, while incurring similar or lower degradation in benign accuracy."
  - [corpus] Corpus lacks direct comparisons between CNF and diffusion purification; related work focuses on tensor networks and manifold methods.
- Break condition: If adversarial perturbations are not diluted by Gaussian noise injection (e.g., imperceptible perturbations with large semantic impact), purification fails.

## Foundational Learning

- Concept: Continuous Normalizing Flows (CNFs)
  - Why needed here: FlowPure is built on CNFs, which define transformations via neural ODEs $dx = u_t(x)dt$. Understanding how to train and integrate these flows is essential.
  - Quick check question: Can you explain why CNFs can map between arbitrary distributions while diffusion models are constrained to Gaussian priors?

- Concept: Conditional Flow Matching (CFM)
  - Why needed here: CFM provides the tractable training objective $L_{CFM}$ that avoids expensive ODE simulation during training, enabling scalable learning of the velocity field.
  - Quick check question: How does conditioning on paired samples $(x^{adv}, x^{clean})$ enable learning a direct mapping instead of a denoising process?

- Concept: Optimal Transport (Rectified Flows)
  - Why needed here: FlowPure uses OT paths to define straight trajectories between distributions, which the paper claims improves integration stability and robustness.
  - Quick check question: Why would straighter ODE paths be more robust to adversarial perturbations than curved diffusion trajectories?

## Architecture Onboarding

- Component map:
  - Clean data and adversarial examples -> Velocity field $v_\theta$ -> ODE integration -> Purified output -> Classifier

- Critical path:
  1. Adversarial example generation during training dominates compute (PGD: 0.60s/batch, CW: 5.20s/batch vs. 0.31s for Gaussian)
  2. Pre-computing adversarial examples offline can reduce training time significantly
  3. ODE integration during inference takes ~1.2s/batch (comparable to DiffPure)

- Design tradeoffs:
  - **Deterministic vs. Stochastic**: Deterministic variants preserve benign accuracy but are vulnerable to adaptive white-box attacks; stochastic (Gaussian) variants trade benign accuracy for improved white-box robustness
  - **Attack-specific vs. General**: PGD-trained models transfer well to CW attacks; CW-trained models generalize poorly to PGD
  - **Noise level ($\sigma$)**: Higher $\sigma$ improves robustness but degrades benign accuracy (CIFAR-100: 72.8% → 55.2% standard accuracy as $\sigma$ increases 0.1 → 0.3)

- Failure signatures:
  - **Benign accuracy drops**: Check if training included small perturbation budgets ($\epsilon \sim U(0, 0.05)$) to ensure velocity field accuracy near data manifold
  - **White-box collapse**: All purification methods fail under fully adaptive attacks; verify attack has gradient access to both classifier and purifier
  - **Detection failures on CW**: CW perturbations are smaller and harder to distinguish; consider hybrid detection approaches

- First 3 experiments:
  1. **Reproduce preprocessor-blind results**: Train FlowPure-PGD on CIFAR-10, evaluate on PGD ($\epsilon=8/255$) with preprocessor-blind setting, verify ~92% robust accuracy and ~96% standard accuracy.
  2. **Ablate noise injection levels**: Train Gaussian FlowPure with $\sigma_{max}=0.3$, evaluate across $\sigma \in [0.1, 0.3]$ to plot standard/robust accuracy trade-off curve.
  3. **Detection baseline**: Compare velocity-based detection against BEYOND on PGD attacks at varying $\epsilon$; verify near-perfect AUC at higher perturbation budgets.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can FlowPure be extended to maintain robustness against fully adaptive white-box attacks without sacrificing benign accuracy?
- Basis in paper: [explicit] The authors acknowledge that "none of the evaluated methods demonstrate practical viability" under fully adaptive white-box attacks, with robust accuracy dropping below 45% on CIFAR-10 and 15% on CIFAR-100.
- Why unresolved: The deterministic variants are easily exploited via gradient-based attacks, while the stochastic Gaussian variant trades benign accuracy for white-box robustness. No current purification method solves this fundamental tension.
- What evidence would resolve it: A FlowPure variant achieving >70% robust accuracy under DiffHammer while maintaining >90% benign accuracy on CIFAR-10.

### Open Question 2
- Question: Why does FlowPure trained on PGD generalize well to CW attacks, but FlowPure trained on CW fails to generalize to PGD?
- Basis in paper: [explicit] The authors observe this asymmetry and hypothesize it relates to PGD perturbations being "larger in magnitude and more distributed" while CW perturbations are "smaller in magnitude" and "highly specialized," but this remains unverified.
- Why unresolved: The structural differences between attack perturbation distributions are noted but not systematically characterized or linked to flow learning dynamics.
- What evidence would resolve it: A theoretical analysis or empirical study mapping perturbation properties (magnitude, distribution, spectral characteristics) to transferability, coupled with experiments using additional attack types.

### Open Question 3
- Question: Can FlowPure's detection capability be extended to achieve near-perfect accuracy against subtle attacks like CW and adaptive threats?
- Basis in paper: [explicit] The authors note that detection "effectiveness diminishes on CW attacks, where the perturbations are more subtle and harder to distinguish," and the detection experiments are described as "preliminary."
- Why unresolved: The velocity magnitude heuristic works well for high-magnitude PGD perturbations but produces overlapping score distributions for CW attacks and was not evaluated under adaptive scenarios.
- What evidence would resolve it: Detection experiments on a broader attack suite (including AutoAttack, Square Attack, and adaptive attacks) with alternative detection signals beyond initial velocity magnitude.

## Limitations
- The paper lacks ablation studies on the impact of noise injection levels during inference for deterministic variants, leaving unclear whether deterministic FlowPure can be stabilized without stochastic elements.
- No analysis of computational overhead for real-time deployment, particularly the 1.2s/batch inference time compared to lightweight alternatives.
- Limited discussion of failure modes against adaptive attacks that could optimize over both the purifier and classifier simultaneously.

## Confidence
- **High Confidence**: Claims about FlowPure's superior performance in preprocessor-blind settings are well-supported by empirical results showing state-of-the-art robust accuracy while preserving benign accuracy.
- **Medium Confidence**: The mechanism explanation for why CNFs outperform diffusion models in adversarial purification is plausible but lacks direct comparative evidence in the paper or corpus.
- **Low Confidence**: The detection capabilities claim (near-perfect AUC against PGD) is based on a single experiment without validation against other attack types or detection baselines.

## Next Checks
1. **Reproduce Adaptive Attack Vulnerability**: Implement a white-box attack that optimizes over both the purifier and classifier to verify FlowPure's robustness collapse under fully adaptive threats.
2. **Cross-Dataset Generalization**: Train FlowPure on CIFAR-10 and evaluate on CIFAR-100 (or other datasets) to assess whether the learned velocity field generalizes beyond training data.
3. **Detection Benchmark Expansion**: Compare FlowPure's velocity-based detection against established methods (e.g., BEYOND, Mahalanobis) across multiple attack types and perturbation levels to validate detection claims.