---
ver: rpa2
title: Analysis of Off-Policy $n$-Step TD-Learning with Linear Function Approximation
arxiv_id: '2502.08941'
source_url: https://arxiv.org/abs/2502.08941
tags:
- learning
- policy
- matrix
- where
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the convergence of n-step TD-learning algorithms
  in the "deadly triad" scenario (linear function approximation, off-policy learning,
  and bootstrapping). The authors prove that n-step TD-learning converges to a solution
  when the sampling horizon n is sufficiently large.
---

# Analysis of Off-Policy $n$-Step TD-Learning with Linear Function Approximation

## Quick Facts
- arXiv ID: 2502.08941
- Source URL: https://arxiv.org/abs/2502.08941
- Reference count: 40
- This paper analyzes convergence of n-step TD-learning in the deadly triad scenario, proving convergence when sampling horizon n is sufficiently large.

## Executive Summary
This paper addresses the "deadly triad" problem in reinforcement learning - the divergence that can occur when combining linear function approximation, off-policy learning, and bootstrapping. The authors prove that n-step TD-learning algorithms converge when the sampling horizon n is sufficiently large, effectively mitigating this fundamental challenge. By analyzing deterministic counterparts and establishing contraction properties, they provide theoretical bounds on the required n value and demonstrate that these scale only logarithmically with key problem factors.

## Method Summary
The paper proposes two n-step TD-learning algorithms for off-policy policy evaluation with linear function approximation. Algorithm 1 uses i.i.d. sampling from the stationary distribution of the behavior policy, while Algorithm 2 samples from a Markov chain. Both algorithms use importance sampling ratios to correct for the behavior-target policy mismatch and update parameters using a stochastic approximation approach. The key insight is that increasing the sampling horizon n beyond certain thresholds ensures convergence by making the projected n-step Bellman operator a contraction and the associated matrix structures Hurwitz.

## Key Results
- n-step TD-learning converges when the sampling horizon n exceeds theoretical thresholds (n*_1, n*_2, n*_3)
- The required n scales logarithmically with discount factor γ, feature matrix conditioning, and stationary distribution ratios
- For linear function approximation with full column rank features, sufficiently large n guarantees convergence despite off-policy learning and bootstrapping
- Two practical algorithms are proposed and proven to converge asymptotically under both i.i.d. and Markov observation models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The projected n-step Bellman operator becomes a contraction mapping when the horizon n is sufficiently large, guaranteeing convergence to a unique fixed point.
- Mechanism: Increasing n reduces the influence of the bootstrapped term γ^n(P^π)^n through the discount factor, since γ^n decays exponentially. For large enough n, the composition ΠT^n satisfies ∥ΠT^n(x) - ΠT^n(y)∥ ≤ γ^n∥Π∥∥x - y∥, which becomes a contraction when γ^n∥Π∥ < 1.
- Core assumption: The feature matrix Φ has full column rank, and the behavior policy induces a well-defined stationary distribution d_β with d_β(s) > 0 for all states.
- Evidence anchors:
  - [abstract] "we prove that n-step TD-learning algorithms converge to a solution as the sampling horizon n increases sufficiently"
  - [section 3, Lemma 5] "There always exists a positive integer n*_2 < ∞ such that ΠT^n is a contraction with respect to ∥·∥_∞"
  - [corpus] Weak direct evidence; related papers focus on distributional TD variants rather than n-step contraction properties.
- Break condition: If n is too small (below threshold), ΠT^n may not be a contraction, and divergence can occur.

### Mechanism 2
- Claim: The matrix Φ^⊤D_β(γ^n(P^π)^n - I)Φ becomes negative definite (Hurwitz) for sufficiently large n, ensuring stability of the associated ODE and convergence of stochastic approximation.
- Mechanism: As n increases, the term γ^n(P^π)^n becomes negligible compared to the identity matrix I, since γ^n decays exponentially. This makes the overall matrix structure dominated by -Φ^⊤D_βΦ, which is negative definite when Φ has full column rank.
- Core assumption: Full column rank of Φ and bounded feature vectors with ϕ_max = max_s ∥ϕ(s)∥² < ∞.
- Evidence anchors:
  - [section 4, Theorem 4] "There exists a positive integer n*_3 < ∞ such that Φ^⊤D_β(γ^n(P^π)^n - I)Φ becomes negative definite and Hurwitz"
  - [section 4] Bound scales logarithmically with key problem factors: n_th = ⌈ln(max{...})/ln(γ)⌉
  - [corpus] No directly comparable mechanism; corpus papers address distributional robustness rather than n-step stability.
- Break condition: For n < n*_3, the matrix may not be Hurwitz, causing potential divergence of the gradient-based iteration.

### Mechanism 3
- Claim: The Schur stability of the matrix A := -(Φ^⊤D_βΦ)^(-1)Φ^⊤D_βγ^n(P^π)^n is equivalent to ΠT^n being a contraction, and holds for sufficiently large n.
- Mechanism: The n-step projected value iteration (n-PVI) can be rewritten as a discrete linear system θ_{k+1} - θ*_n = A(θ_k - θ*_n). Convergence to the fixed point requires A to have spectral radius < 1 (Schur stable). Large n ensures ∥A∥_∞ < 1.
- Core assumption: Same as Mechanism 1; additionally, the projected Bellman equation has a solution.
- Evidence anchors:
  - [section 3, Theorem 1] "The matrix A defined in (8) is Schur if and only if ΠT^n is a contraction"
  - [section 3, Lemma 5] Provides explicit bound: n*_1 ≤ ⌈ln(1/∥(Φ^⊤D_βΦ)^(-1)Φ^⊤D_β∥_∞∥Φ∥_∞)/ln(γ)⌉
  - [corpus] No direct corpus support for Schur stability mechanism in n-step TD.
- Break condition: For small n, A may have spectral radius ≥ 1, leading to divergence.

## Foundational Learning

- Concept: Contraction mappings and fixed-point iterations
  - Why needed here: The paper's core proof strategy relies on showing ΠT^n is a contraction for large n, which guarantees convergence to a unique fixed point via Banach fixed-point theorem.
  - Quick check question: Can you explain why ∥Bx∥ ≤ c∥x∥ with c < 1 implies iterating x_{k+1} = Bx_k + b converges regardless of starting point?

- Concept: Schur stability and Hurwitz matrices
  - Why needed here: Convergence of both the discrete n-PVI iteration and the continuous ODE counterpart depends on matrix stability properties (Schur for discrete, Hurwitz for continuous).
  - Quick check question: Given a 2×2 matrix with eigenvalues {0.3, 0.8}, is it Schur stable? What if eigenvalues were {0.9, 1.1}?

- Concept: Importance sampling for off-policy learning
  - Why needed here: Algorithms 1 and 2 correct for behavior-target policy mismatch using the ratio ρ = ∏π(a_k|s_k)/β(a_k|s_k), enabling off-policy convergence.
  - Quick check question: Why does the importance sampling ratio appear in the update but not in the on-policy case?

## Architecture Onboarding

- Component map:
  - MDP environment -> n-step return estimator -> importance sampling module -> linear value approximator -> update rule

- Critical path:
  1. Sample initial state s_0 ~ d_β (or from Markov chain in Algorithm 2)
  2. Generate n-step trajectory under behavior policy β
  3. Compute n-step return G and importance ratio ρ
  4. Apply TD update with step-size α_i satisfying Robbins-Monro conditions (Σα_k = ∞, Σα_k² < ∞)

- Design tradeoffs:
  - **Larger n**: More robust convergence (guaranteed contraction), but higher variance from longer trajectories and importance sampling ratios
  - **Smaller n**: Lower variance and faster updates, but may diverge if below threshold
  - **Step-size α**: Must be small enough for stability (α ≤ α* from Lemma 6), but large enough for reasonable convergence speed

- Failure signatures:
  - Divergence of θ (norm growing unboundedly): n is too small; increase horizon
  - High variance in updates causing oscillation: importance ratios too large; consider clipping ρ or reducing n
  - Convergence to wrong solution: feature matrix poorly conditioned; check Φ has full column rank

- First 3 experiments:
  1. **Validate n threshold**: Run Algorithm 1 on a simple 2-state MDP with known n*_3 bound; verify divergence for n < n*_3 and convergence for n ≥ n*_3
  2. **Test Markovian sampling**: Compare Algorithm 1 (i.i.d.) vs Algorithm 2 (Markov chain) on same MDP; measure convergence rate difference
  3. **Sensitivity to key factors**: Systematically vary γ, d_max/d_min ratio, and feature conditioning; verify that required n scales logarithmically as predicted by bounds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the sufficient bounds on n (n*₁, n*₂, n*₃) be tightened to necessary conditions?
- Basis in paper: [explicit] Remark 5 and Appendix E state "the bound on n*₃ in Theorem 4 may appear loose, further sharpening may be difficult" and provide an example showing negative-definiteness at n does not imply it at n+1.
- Why unresolved: The counterexample in Appendix E demonstrates that monotonicity of convergence properties with respect to n is not guaranteed, making systematic bound improvement challenging.
- What evidence would resolve it: A theorem characterizing necessary and sufficient conditions for Φ⊤Dβ(γⁿ(P^π)ⁿ − I)Φ being Hurwitz, or a proof that the logarithmic scaling is tight.

### Open Question 2
- Question: What is the finite-time convergence rate of n-step TD-learning algorithms?
- Basis in paper: [inferred] The paper proves asymptotic convergence (Theorems 6 and 7) but does not provide finite-sample analysis or convergence rates.
- Why unresolved: The ODE method used for asymptotic analysis does not yield finite-time guarantees, requiring different analytical techniques.
- What evidence would resolve it: A finite-time bound on ∥θₖ − θ*ₙ∥ as a function of k, n, αₖ, and problem parameters.

### Open Question 3
- Question: Can these results be extended to nonlinear function approximation (e.g., neural networks)?
- Basis in paper: [inferred] The analysis is restricted to linear function approximation with feature matrix Φ; extension to deep RL is not addressed despite motivation from deep RL successes.
- Why unresolved: The proofs rely heavily on matrix algebraic properties specific to linear function approximation (e.g., Π = Φ(Φ⊤DβΦ)⁻¹Φ⊤).
- What evidence would resolve it: A convergence proof for n-step TD with nonlinear approximation, or identification of conditions under which the linear analysis provides useful approximations.

## Limitations
- The theoretical bounds on required n values may be conservative and loose compared to empirical requirements
- Results critically depend on the feature matrix having full column rank and the behavior policy inducing a well-defined stationary distribution
- The analysis is limited to linear function approximation and does not extend to nonlinear approximation methods like neural networks

## Confidence
- **High confidence**: The core convergence results for n-step TD-learning when n exceeds theoretical thresholds (Theorems 3 and 4)
- **Medium confidence**: The logarithmic scaling bounds for required n with respect to problem parameters
- **Medium confidence**: The practical effectiveness of the proposed algorithms in mitigating the deadly triad

## Next Checks
1. **Bound verification**: Test the algorithm on a suite of MDPs with systematically varying γ, d_max/d_min ratios, and feature conditioning to empirically validate that required n scales logarithmically as predicted
2. **Sensitivity analysis**: Investigate algorithm robustness to violations of key assumptions (e.g., near-collinear features, near-zero stationary probabilities) to identify practical failure modes
3. **Performance benchmarking**: Compare convergence speed and stability of the proposed n-step algorithms against alternative approaches (e.g., emphatic TD, gradient TD) on standard RL benchmarks where the deadly triad is problematic