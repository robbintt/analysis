---
ver: rpa2
title: 'RoDiF: Robust Direct Fine-Tuning of Diffusion Policies with Corrupted Human
  Feedback'
arxiv_id: '2602.00886'
source_url: https://arxiv.org/abs/2602.00886
tags:
- diffusion
- preference
- policy
- human
- rodif
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RoDiF, a framework for robust direct fine-tuning
  of diffusion policies using noisy human preferences. It addresses the challenge
  of fine-tuning diffusion policies under corrupted preference labels by formulating
  a unified Markov decision process that integrates denoising steps with environmental
  dynamics, enabling reward-free direct preference optimization.
---

# RoDiF: Robust Direct Fine-Tuning of Diffusion Policies with Corrupted Human Feedback

## Quick Facts
- **arXiv ID:** 2602.00886
- **Source URL:** https://arxiv.org/abs/2602.00886
- **Reference count:** 31
- **Key outcome:** RoDiF achieves 86% mode alignment at 20% corrupted preferences versus 10% for SimPO, maintaining high performance even at 30% corruption while preserving task success rates.

## Executive Summary
This paper introduces RoDiF, a framework for robust direct fine-tuning of diffusion policies using noisy human preferences. It addresses the challenge of fine-tuning diffusion policies under corrupted preference labels by formulating a unified Markov decision process that integrates denoising steps with environmental dynamics, enabling reward-free direct preference optimization. The key innovation is interpreting the DPO objective through a geometric hypothesis-cutting lens and employing a conservative cutting strategy to achieve robustness without assuming any specific noise distribution. Experiments on five long-horizon manipulation tasks across three different diffusion architectures show that RoDiF consistently outperforms state-of-the-art baselines, maintaining high alignment performance even under 30% corrupted preference labels while preserving task success rates.

## Method Summary
RoDiF extends direct preference optimization to diffusion policies by integrating the denoising chain with environment dynamics into a unified MDP. This allows reward-free preference optimization by reparameterizing Q-values through policy likelihood ratios across all denoising steps. The method introduces a geometric interpretation where each preference pair creates a "cut" constraint on the hypothesis space, and corrupted labels create false cuts that permanently exclude the true aligned policy. To achieve robustness, RoDiF employs a conservative voting strategy that retains hypotheses satisfying at least (1-γ)N consistent cuts, where γ is the conservativeness level. The approach uses a softened robust loss that effectively ignores inconsistent feedback, enabling stable fine-tuning even with significant label corruption.

## Key Results
- RoDiF achieves 86% mode alignment at 20% corrupted preferences versus 10% for SimPO
- Maintains 75% mode alignment at 30% corruption while SimPO drops to 10%
- Preserves task success rates across all tested corruption levels (0%, 20%, 30%, 40%)
- Shows consistent performance across three different diffusion architectures (UNet, Transformer ACT, Image-Based MLP)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Integrating diffusion denoising steps with environment dynamics into a unified MDP enables reward-free preference optimization for diffusion policies.
- **Mechanism:** The diffusion policy's denoising chain $(a^K_t, a^{K-1}_t, ..., a^0_t)$ is nested within environment rollouts $(s_0, a_0, s_1, a_1, ...)$. By defining unified states $s^+_t = (a^K_t, s_t)$ and actions $a^+_t = (a^{K-1}_t, ..., a^0_t)$, the Markov property of both processes can be jointly exploited, allowing policy factorization: $\pi_\theta(a^+_t|s^+_t) = \prod_{k=1}^K p_\theta(a^{k-1}_t|a^k_t, s_t)$. This permits reparameterization of Q-values via policy likelihood ratios.
- **Core assumption:** The denoising chain exhibits Markov properties, and preferences can be modeled per control-step rather than requiring full trajectory rewards.
- **Evidence anchors:** [abstract] "unified Markov decision process that coherently integrates the diffusion denoising chain with environmental dynamics"; [Section 3.3] Defines unified MDP tuples and factorization in Eq. (3-6); [corpus] DiWA and related papers address diffusion policy fine-tuning but require reward models; RoDiF's unified MDP is a distinct contribution with weak corpus validation
- **Break condition:** If the denoising process lacks Markov structure or preferences fundamentally require trajectory-level reward modeling, the unified MDP formulation may not yield tractable DPO objectives.

### Mechanism 2
- **Claim:** Interpreting each preference label as a geometric "cut" on the hypothesis space reveals how corrupted labels irreversibly exclude the true aligned policy.
- **Mechanism:** Under low temperature, the Bradley-Terry model approximates a step function. Each preference pair $(a^+_w|s^+ \succ a^+_l|s^+)$ induces a constraint $C_i = \{\theta \in \Theta | \Delta Q_\theta(\cdot) \geq 0\}$. Standard DPO intersects all cuts: $\Theta \cap C_1 \cap C_2 ... \cap C_N$. A single corrupted label creates a "false cut" that removes $\theta_H$ from the feasible set permanently.
- **Core assumption:** Preference corruption manifests as label inversions that are inconsistent with the true Q-difference, and the temperature $\alpha \to 0$ approximation is valid for geometric interpretation.
- **Evidence anchors:** [abstract] "geometric hypothesis-cutting lens"; [Section 4.1] Eq. (16-19) formalize cuts and intersection; Figure 2 visualizes corrupted cut exclusion; [corpus] Weak direct evidence; hypothesis-cutting for robust RL appears in Xie et al. 2025 (cited) but not in corpus neighbors
- **Break condition:** If preferences are corrupted in ways other than label inversion (e.g., systematic bias, non-stationary criteria), the cut interpretation may not capture the failure mode.

### Mechanism 3
- **Claim:** A conservative voting strategy that retains hypotheses receiving at least $(1-\gamma)N$ consistent votes preserves the true aligned policy even under $\gamma N$ corrupted labels.
- **Mechanism:** Instead of requiring all cuts to be satisfied (which fails with any corruption), the robust objective maximizes $H(\sum_i V_{C_i}(\theta) - \lfloor(1-\gamma)N\rfloor + 0.5)$. This allows a hypothesis to survive even if it violates up to $\gamma N$ cuts. The Heaviside function is softened with sigmoid $\sigma_\alpha$ for gradient flow.
- **Core assumption:** $\gamma$ is a valid upper bound on the actual corruption rate; corrupted labels are a minority (< 50%).
- **Evidence anchors:** [abstract] "retaining hypotheses that satisfy at least $(1-\gamma)N$ consistent cuts"; [Section 4.2] Lemma 4.2 proves $\theta_H$ remains in solution set; Eq. (22-24) define the robust loss; [corpus] ADG addresses corruption in offline RL but via dataset recovery, not hypothesis cutting—different robustness mechanism
- **Break condition:** If $\gamma$ underestimates true corruption or corruption rate exceeds 50%, the conservative strategy may flip the preferred mode (confirmed in Appendix C: noise > 50% causes mode reversal).

## Foundational Learning

- **Concept: Diffusion Policy Denoising Process**
  - **Why needed here:** RoDiF operates on the denoising chain $(a^K, ..., a^0)$, not just final actions. Understanding that $a^k_t$ represents intermediate noisy states and that $\epsilon_\theta$ predicts injected noise is essential to grasp the unified MDP construction.
  - **Quick check question:** Can you explain why a diffusion policy requires $K$ denoising steps to generate an action, and what $\epsilon_\theta(a^k, k, s)$ computes?

- **Concept: Direct Preference Optimization (DPO) Reparameterization**
  - **Why needed here:** RoDiF extends DPO's key trick—expressing $Q^*$ via the log-likelihood ratio $\log \pi^*/\pi_{ref}$—to the diffusion setting. Without this, you cannot derive the reward-free objective in Eq. (12-13).
  - **Quick check question:** In standard DPO, how does the relationship $Q^*(s,a) = \beta \log \frac{\pi^*(a|s)}{\pi_{ref}(a|s)} + \beta \log Z(s)$ eliminate the need for explicit reward learning?

- **Concept: Bradley-Terry Preference Model with Temperature**
  - **Why needed here:** The temperature parameter $\alpha$ controls the sharpness of the preference probability. RoDiF's robust loss softens the step function using $\sigma_\alpha$, and ablation shows $\alpha=1$ outperforms $\alpha=0.1$ for gradient flow.
  - **Quick check question:** What happens to $P(a \succ b) = \frac{\exp(u_a/\alpha)}{\exp(u_a/\alpha) + \exp(u_b/\alpha)}$ as $\alpha \to 0$, and why does this create gradient issues in the robust loss?

## Architecture Onboarding

- **Component map:**
  ```
  Unified MDP Layer
  ├── State: s^+_t = (a^K_t, s_t)  [noisy action + env state]
  ├── Action: a^+_t = (a^{K-1}_t, ..., a^0_t)  [denoising trajectory]
  └── Policy: π_θ(a^+|s^+) = ∏_k p_θ(a^{k-1}|a^k, s)  [factorized over K steps]
  
  DPO Reparameterization
  ├── Q-function: ΔQ_θ computed via log-likelihood ratios over all K denoising steps
  └── Loss: L_DP-DPO aggregates per-step preference comparisons
  
  Robust Layer
  ├── Voting function: V̂_{C_i,t} = σ_α(ΔQ_θ(winner, loser))
  ├── Conservative threshold: ν(1-γ)MT  [M pairs × T timesteps]
  └── Final loss: L_RoDiF = -log σ_α(Σ V̂ - threshold)
  ```

- **Critical path:**
  1. **Pretrained diffusion policy** (behavior cloning on demonstrations) → generates multi-modal behaviors
  2. **Collect preference pairs** (winner/loser trajectories from different modes)
  3. **Inject corruption** (random label flips at rate γ_actual)
  4. **Fine-tune with L_RoDiF** using γ ≥ γ_actual for conservative cutting
  5. **Evaluate** mode alignment + success rate

- **Design tradeoffs:**
  - **α (temperature):** Low α (0.1) → sharp voting, gradient vanishing; High α (1.0) → smooth gradients, robust optimization
  - **β (KL weight):** High β (1.0) → policy trapped in pretrained mode; Low β (0.005) → aggressive updates, potential instability
  - **γ (conservativeness):** γ < γ_actual → insufficient robustness, corrupted labels degrade performance; γ >> γ_actual → overly conservative, discards valid preferences

- **Failure signatures:**
  - Mode alignment degrades sharply as corruption increases → γ is too low (underestimated corruption)
  - Policy remains at pretrained mode despite fine-tuning → β too high or α too low
  - Success rate drops → β too low, causing aggressive updates that break trajectory smoothness
  - Mode flips at high noise (> 50%) → expected per Lemma 4.2; corruption majority reverses preference signal

- **First 3 experiments:**
  1. **Sanity check:** Run RoDiF with 0% corruption on a simple task (Avoid). Verify mode alignment matches DP-DPO baseline and success rate is maintained.
  2. **Noise sensitivity:** Fix α=1, β=0.1, vary γ ∈ {0, 0.2, 0.3, 0.4} at 20% corruption. Confirm γ=0.3-0.4 maintains >80% alignment while γ=0 collapses.
  3. **Architecture agnosticism:** Apply RoDiF to both UNet (state-based) and Transformer (visual) backbones on different tasks. Verify robustness holds across architectures per Table 2.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the conservativeness level ($\gamma$) be dynamically estimated during training rather than manually tuned as a hyperparameter?
- **Basis:** [inferred] Section 5.4 (Ablation Study) demonstrates that performance is sensitive to $\gamma$, requiring it to match the actual corruption rate. However, the experiments assume prior knowledge of this rate to set $\gamma$, which is unavailable in realistic, open-ended learning scenarios.
- **Why unresolved:** The paper validates the method using pre-set $\gamma$ values corresponding to known noise levels (e.g., $\gamma=0.45$ for 40% noise) but does not propose a mechanism to adapt $\gamma$ online or estimate it from data statistics.
- **What evidence would resolve it:** An adaptive algorithm that analyzes gradient conflicts or hypothesis voting consistency to adjust $\gamma$ in real-time, showing comparable performance without ground-truth noise statistics.

### Open Question 2
- **Question:** Does the hypothesis-cutting strategy maintain robustness under correlated or adversarial noise patterns, as opposed to the random label flips tested?
- **Basis:** [inferred] Section 5.1 specifies that corrupted labels were generated by "randomly selecting a portion... [and] permanently inverted." This assumes independent noise, whereas Section 1 highlights real-world inconsistencies like "nonstationary criteria" or "annotator disagreement," which often introduce correlated errors.
- **Why unresolved:** The geometric cutting strategy assumes noisy constraints are spread such that the true hypothesis remains in the high-vote intersection. Systematic adversarial noise could theoretically "cut" the true hypothesis out of the space more efficiently than random noise.
- **What evidence would resolve it:** Experiments where label corruption is biased (e.g., flipping labels only for specific sub-tasks or based on trajectory features) showing the intersection of hypotheses remains stable.

### Open Question 3
- **Question:** How does the computational overhead of the Unified MDP formulation scale with increasing denoising steps ($K$) and horizon length ($T$)?
- **Basis:** [inferred] Section 4.3 describes the loss calculation requiring a summation over $K$ denoising steps for every step $t$ in a trajectory of length $T$ (Eq. 24). While effective for the tested manipulation tasks, the complexity of computing $\Delta Q_\theta$ across this flattened MDP may limit application in high-frequency control loops.
- **Why unresolved:** The paper focuses on alignment accuracy and success rates but does not provide wall-clock training time comparisons or an asymptotic complexity analysis relative to standard DPO baselines.
- **What evidence would resolve it:** Profiling of training time and memory usage on tasks with significantly longer horizons or higher denoising step counts compared to standard non-robust baselines.

## Limitations
- Requires accurate estimation of corruption rate γ, which is often unavailable in real-world scenarios
- Performance degrades significantly when corruption exceeds 50%, causing mode flipping
- Computational overhead increases with denoising steps K and trajectory length T due to the unified MDP formulation
- Preferential data generation and mode classification criteria are not fully specified, limiting reproducibility

## Confidence

- **Unified MDP formulation:** High - mathematical derivation is sound and consistent with diffusion policy mechanics
- **Geometric hypothesis-cutting interpretation:** Medium - elegant theory but limited empirical validation of the geometric claims
- **Robust loss effectiveness:** High - ablation studies clearly show performance degradation when γ is underestimated
- **Architecture agnosticism:** Medium - experiments cover multiple backbones but hyperparameter sensitivity is not thoroughly explored

## Next Checks

1. **Corruption rate estimation**: Implement a validation procedure to estimate γ_actual from corrupted preference data and compare against ground truth corruption rates used in training

2. **Gradient flow analysis**: Measure gradient magnitudes and effective learning rates across different α values (0.1 vs 1.0) to confirm the soft voting mechanism's impact on optimization dynamics

3. **Mode recovery under extreme corruption**: Systematically test RoDiF performance at corruption rates >50% to characterize the exact transition point where mode flipping occurs and validate the theoretical prediction from Lemma 4.2