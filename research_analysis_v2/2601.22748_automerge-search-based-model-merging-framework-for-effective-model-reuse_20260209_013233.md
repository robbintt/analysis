---
ver: rpa2
title: 'AutoMerge: Search-Based Model Merging Framework for Effective Model Reuse'
arxiv_id: '2601.22748'
source_url: https://arxiv.org/abs/2601.22748
tags:
- merging
- uni00000013
- uni00000048
- techniques
- uni00000044
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AutoMerge addresses the problem of effectively reusing multiple
  task-specific deep learning models with the same architecture across different domains.
  The core method involves segmenting complex models into heterogeneous blocks based
  on their structural properties and then using Bayesian optimization to search for
  optimal merging techniques and hyperparameters for each block pair.
---

# AutoMerge: Search-Based Model Merging Framework for Effective Model Reuse

## Quick Facts
- **arXiv ID:** 2601.22748
- **Source URL:** https://arxiv.org/abs/2601.22748
- **Reference count:** 40
- **Primary result:** AutoMerge improves average preservation rate by 23.55% and reduces preservation discrepancy by 51.94% across three domains compared to existing merging techniques.

## Executive Summary
AutoMerge addresses the challenge of effectively reusing multiple task-specific deep learning models with identical architectures by introducing a search-based framework that optimizes both merging techniques and hyperparameters at the block level. The framework segments complex models into heterogeneous blocks based on structural properties, then uses Bayesian optimization to search for optimal configurations that balance capability preservation across tasks while minimizing performance discrepancy. Results demonstrate significant improvements in preservation rates (23.55% average increase) and reduction in preservation discrepancy (51.94% average decrease) across three domains: large language models, image classification, and autonomous driving, while reducing computational time and cost by over 60% compared to fine-tuning approaches.

## Method Summary
AutoMerge segments source models into functional blocks by analyzing tensor shape transitions during inference, then applies Bayesian optimization to search for optimal merging configurations at the block level. The framework supports five state-of-the-art merging techniques (Linear, Task Arithmetic, TIES, DARE-Linear, DARE-TIES) and optimizes for the harmonic mean of approximated preservation scores across tasks. The search uses a random forest surrogate model with Log Expected Improvement acquisition, starting from 20 Sobol samples and running for 200 iterations. This approach balances capability preservation across tasks while minimizing preservation discrepancy, with the segmentation strategy enhancing effectiveness by preventing incompatible merging strategies from interfering with structurally distinct blocks.

## Key Results
- Achieves 23.55% average improvement in preservation rate across all three domains (LLMs, image classification, autonomous driving)
- Reduces average preservation discrepancy by 51.94% compared to existing merging techniques
- Outperforms whole-model hyperparameter search by 28.11% in merging effectiveness
- Cuts computational time by 62.43% and cost by 64.34% compared to fine-tuning while maintaining competitive capabilities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Segmenting heterogeneous structural blocks before merging improves capability preservation compared to whole-model merging
- **Mechanism:** The Model Segmentor identifies segmentation points by monitoring tensor shape transitions during inference (e.g., 4D tensors for CNNs → 3D tensors for Transformers), preventing incompatible merging strategies from degrading different architectural components
- **Core assumption:** Distinct architectural blocks have incompatible optimal merging strategies; a single global strategy causes interference
- **Evidence anchors:** [Section 3.1] describes segmentation based on tensor shape transitions; [Section 4.4] shows ablation study with 44.55% PR drop when removing segmentor on organism classes

### Mechanism 2
- **Claim:** Search-based approach using Bayesian optimization identifies superior configurations compared to manual fixed settings
- **Mechanism:** The Search-Based Merger treats merging as multi-objective optimization using Random Forest surrogate and Log Expected Improvement acquisition to balance exploration and exploitation of configuration space
- **Core assumption:** The relationship between merging hyperparameters and model capability is complex but smooth enough to be modeled by a surrogate function
- **Evidence anchors:** [Section 3.2] details Bayesian optimization with random forest; [Section 4.3] shows 13.6% time reduction compared to manual grid search

### Mechanism 3
- **Claim:** Optimizing for harmonic mean of loss-based preservation scores minimizes discrepancy between task capabilities
- **Mechanism:** The framework uses Approximated Preservation (AP) based on validation losses and maximizes harmonic mean of AP scores to inherently penalize large discrepancies between tasks
- **Core assumption:** Validation loss is a reliable proxy for downstream task capability during search phase
- **Evidence anchors:** [Section 3.2] defines harmonic mean objective; [Abstract] reports 51.94% reduction in preservation discrepancy

## Foundational Learning

- **Concept: Model Merging Techniques (Linear, Task Arithmetic, TIES, DARE)**
  - **Why needed here:** AutoMerge selects from these techniques as discrete choices in its search space; understanding "density" pruning in TIES/DARE is critical for interpreting search results
  - **Quick check question:** How does the "density" hyperparameter in TIES differ from the "weight" hyperparameter in Linear merging?

- **Concept: Bayesian Optimization (Surrogate Models & Acquisition Functions)**
  - **Why needed here:** The core engine of AutoMerge is the optimization loop; understanding exploration vs. exploitation trade-off is critical
  - **Quick check question:** Why might the authors choose a Random Forest surrogate over a Gaussian Process for this specific search space?

- **Concept: Structural Heterogeneity in Deep Learning**
  - **Why needed here:** The paper's primary insight is that CNNs (spatial, dense) and Transformers (sequential, sparse) cannot be merged with the same logic
  - **Quick check question:** Why does parameter pruning (setting weights to zero) damage CNN blocks more than Transformer blocks?

## Architecture Onboarding

- **Component map:**
  - Input: Source Models A & B + Validation Datasets
  - Model Segmentor: Analyzes inference trace → outputs Block Pairs (e.g., CNN block pair, Transformer block pair)
  - Search-Based Merger: Initializer (Sobol sampling) → Surrogate (Random Forest) → Acquisition (Log Expected Improvement) → Objective (Harmonic mean of AP)
  - Output: Merged Model with per-block configurations

- **Critical path:**
  1. Trace model inference to identify block boundaries based on tensor shape shifts
  2. Initialize search with 20 Sobol samples to train initial surrogate model
  3. Iterate: Select config via LEI → Merge blocks → Evaluate on validation set → Update surrogate

- **Design tradeoffs:**
  - Surrogate Choice: Random Forest is robust to high dimensions but may smooth over local optima compared to Gaussian Processes
  - Validation Proxy: Using loss instead of task-specific metrics speeds up search but assumes loss correlates perfectly with downstream utility

- **Failure signatures:**
  - High Preservation Discrepancy: Objective function failing to balance tasks; check validation set balance
  - Sudden PR Drop in CNNs: Search selecting low density values for CNN blocks; verify search space constraints
  - Search Stagnation: Surrogate overfitting to initial samples; increase exploration factor in acquisition function

- **First 3 experiments:**
  1. Segmentation Validation: Run segmentor on target architecture to verify correct separation of Convolutional and Transformer layers based on tensor shapes
  2. Baseline Reproduction (RQ1): Merge models using standard Linear/TIES with fixed hyperparameters to establish baseline
  3. Optimization Loop Dry Run: Execute 5 iterations of search-based merger on single block pair to verify pipeline

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can AutoMerge be extended to merge source models with fundamentally different architectures?
- **Basis in paper:** [explicit] Authors state in Section 8 (Threats to Validity) and Section 7 (Conclusion) that supporting different architectures is a current limitation and key future goal
- **Why unresolved:** Current framework relies on "Model Segmentor" that creates aligned "block pairs" based on identical structural properties; alignment breaks down when architectures differ
- **What evidence would resolve it:** Proposed extension using cross-architecture mapping or functional alignment techniques, evaluated on standard benchmarks

### Open Question 2
- **Question:** How robust is AutoMerge when required validation datasets are scarce, noisy, or suffer from distribution shift?
- **Basis in paper:** [inferred] Objective function relies on calculating loss values on "integrated validation datasets" to approximate capability preservation
- **Why unresolved:** Framework depends heavily on validation data despite claiming "zero-training" setting; paper does not analyze performance degradation with limited/unrepresentative validation data
- **What evidence would resolve it:** Sensitivity analysis measuring PR and PD while varying validation set volume and distribution integrity

### Open Question 3
- **Question:** Is the "shape-transition" heuristic for block segmentation optimal for capturing functional boundaries?
- **Basis in paper:** [inferred] Section 3.1 describes segmentor identifying boundaries based on tensor shape transitions
- **Why unresolved:** Structural shape changes may not perfectly align with functional independence; splitting based solely on tensor dimensions might separate layers that should be treated as cohesive unit
- **What evidence would resolve it:** Ablation study comparing current segmentation strategy against alternative partitioning methods like layer-wise granularity or gradient-based clustering

## Limitations
- Framework assumes identical architectures across source models, limiting applicability to cases where architectures naturally differ
- Validation datasets used during Bayesian optimization phase are not fully specified, making it difficult to assess search optimization quality
- Computational overhead of search-based approach (200 iterations × 5 seeds) is not compared to simpler hyperparameter tuning methods

## Confidence
- **High Confidence:** Mechanism of segmenting models based on tensor shape transitions is well-justified and empirically validated (44.55% PR improvement in ablation study)
- **Medium Confidence:** Bayesian optimization framework produces better results than manual hyperparameter selection, though superiority over simpler methods not established
- **Medium Confidence:** 51.94% reduction in preservation discrepancy is meaningful, but reliance on validation loss as proxy for capability may not perfectly correlate with actual task performance

## Next Checks
1. **Search Space Sensitivity:** Vary number of initial Sobol samples (10, 20, 50) and iterations (100, 200, 500) to determine if reported improvements are robust to search budget changes
2. **Baseline Comparison:** Implement random search baseline with same search space and compare convergence speed and final performance against Bayesian optimization approach
3. **Cross-Domain Generalization:** Apply framework to new domain (e.g., NLP models with different attention mechanisms) to test whether segmentation strategy generalizes beyond three studied domains