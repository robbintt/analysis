---
ver: rpa2
title: Log-Sum-Exponential Estimator for Off-Policy Evaluation and Learning
arxiv_id: '2506.06873'
source_url: https://arxiv.org/abs/2506.06873
tags:
- estimator
- learning
- reward
- off-policy
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the Log-Sum-Exponential (LSE) estimator for
  off-policy evaluation and learning from logged bandit feedback datasets. The LSE
  estimator addresses challenges of high variance, low-quality propensity scores,
  and heavy-tailed reward distributions by using a log-sum-exponential operator that
  effectively reduces variance and is robust to outliers.
---

# Log-Sum-Exponential Estimator for Off-Policy Evaluation and Learning

## Quick Facts
- arXiv ID: 2506.06873
- Source URL: https://arxiv.org/abs/2506.06873
- Reference count: 40
- Primary result: LSE estimator achieves O(n^(-ε/(1+ε))) regret convergence under bounded (1+ε)-th moment assumptions and outperforms state-of-the-art methods in heavy-tailed and noisy conditions.

## Executive Summary
This paper introduces the Log-Sum-Exponential (LSE) estimator for off-policy evaluation and learning from logged bandit feedback datasets. The LSE estimator addresses key challenges in off-policy methods including high variance, low-quality propensity scores, and heavy-tailed reward distributions by using a log-sum-exponential operator that effectively reduces variance and is robust to outliers. Theoretical analysis provides upper bounds on bias, variance, and regret for both off-policy evaluation and learning scenarios. Empirical evaluations on synthetic and real-world datasets demonstrate that LSE outperforms state-of-the-art estimators like IPS, PM, ES, and IX, particularly in noisy and heavy-tailed reward conditions.

## Method Summary
The LSE estimator computes (1/λ)log((1/n)∑exp(λ·r·w)) where λ < 0, providing a non-linear transformation of importance-weighted rewards that implicitly shrinks large values. For off-policy learning, gradients are computed as ∇θV̂_LSE = (1/n)∑rᵢ·exp(λ(rᵢwᵢ - V̂_LSE))·∇θwᵢ. The method includes a data-driven λ selection heuristic λ* = -n^(-1/(1+ε)) and can be combined with reward estimators for doubly robust variants. Implementation requires careful numerical handling including propensity score clipping and uses mini-batch SGD with early stopping for optimization.

## Key Results
- LSE achieves O(n^(-ε/(1+ε))) regret convergence rate under bounded (1+ε)-th moment assumptions
- Outperforms IPS, PM, ES, and IX estimators on EMNIST, FMNIST, and KUAIREC datasets
- Demonstrates robustness to noisy propensity scores with Gamma noise injection
- Shows superior performance in heavy-tailed reward distributions compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1: Variance Reduction via Non-Linear Shrinkage
The LSE operator (1/λ)log((1/n)∑exp(λ·r·w)) for λ < 0 applies non-linear shrinkage that reduces variance by implicitly ignoring extreme values. As r·w → +∞, exp(λ·r·w) → 0, causing the estimator to effectively bound above by the simple average.

### Mechanism 2: Robustness to Noisy/Estimated Propensity Scores
When propensity scores are corrupted by multiplicative inverse-Gamma noise (π̂₀ = (1/U)π₀ with U ~ Gamma(b,b)), LSE maintains bounded regret through its functional structure. The discrepancy between noisy and true estimators is controlled by TV distance between reward distributions divided by λ².

### Mechanism 3: Heavy-Tailed Reward Handling via Moment Boundedness
Under bounded (1+ε)-th moment of weighted reward (E[(w·R)^(1+ε)] ≤ ν), LSE achieves O(n^(-ε/(1+ε))) regret convergence by applying Bernstein's inequality to exponential-transformed weighted rewards, avoiding reliance on variance that may not exist.

## Foundational Learning

- **Concept: Logged Bandit Feedback (LBF) Dataset**
  - Why needed here: The methodology operates on LBF datasets containing (context, action, propensity score, reward) tuples collected from a logging policy π₀.
  - Quick check question: Can you distinguish between the logging policy (behavior policy that generated data) and target policy (policy being evaluated or learned)?

- **Concept: Inverse Propensity Score (IPS) Estimator**
  - Why needed here: LSE is designed to address IPS's limitations—unbiasedness under correct propensity scores but high variance, especially with heavy tails or noisy propensities.
  - Quick check question: Why does IPS have high variance when importance weights w = πθ/π₀ are large?

- **Concept: Bias-Variance Tradeoff in Off-Policy Estimation**
  - Why needed here: LSE introduces controlled bias to achieve variance reduction. The parameter λ governs this tradeoff—more negative λ increases bias but decreases variance.
  - Quick check question: How does the bias of LSE behave asymptotically as n→∞ with appropriate λ(n)?

## Architecture Onboarding

- **Component map:** LBF dataset S -> LSE Core computation -> Estimated value V̂(πθ) for OPE/OPL
- **Critical path:**
  1. Validate LBF dataset format and compute importance weights wᵢ = πθ(aᵢ|xᵢ)/pᵢ
  2. Handle numerical stability: clip propensity scores (e.g., to 0.001) to avoid overflow in exp(λ·r·w)
  3. For OPL: implement gradient ∇θV̂_LSE = (1/n)∑rᵢ·exp(λ(rᵢwᵢ - V̂_LSE))·∇θwᵢ
  4. Select λ: start with data-driven λ* = -n^(-1/2) (assuming ε≈1), validate on held-out set

- **Design tradeoffs:**
  - λ magnitude: More negative → lower variance, higher bias; closer to 0 → approaches IPS
  - Model-based combination: DR-LSE adds reward estimator for potential bias reduction at cost of model misspecification risk
  - Computational complexity: O(n) per evaluation (same as IPS), but requires exp() computation and careful numerical handling

- **Failure signatures:**
  - Exploding gradients/NaN: Caused by unclipped propensity scores → fix with pᵢ = max(pᵢ, 1e-3)
  - Bias dominates: When λ too negative or n small → reduce |λ| or increase data
  - Poor performance on bounded rewards: LSE may be overkill → check if standard IPS/PM/ES suffice

- **First 3 experiments:**
  1. Synthetic OPE validation: Generate data with known heavy-tailed reward (e.g., Lomax), compare LSE vs IPS/PM/ES on MSE decomposition (bias, variance) as per Table 9-10
  2. Propensity noise robustness test: Inject Gamma noise (varying b) into propensity scores, plot accuracy vs noise level as in Table 3 (EMNIST with b ∈ {5, 0.01})
  3. Real-world OPL sanity check: Apply to Kuairec or similar recommendation dataset, compare Precision@K/NDCG@K against baselines (Table 20), ensuring logging policy differs sufficiently from target

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the LSE estimator be theoretically and practically extended to sequential reinforcement learning settings where the i.i.d. assumption on logged data does not hold? The current theoretical guarantees rely on i.i.d. logged bandit feedback, while sequential data introduces temporal dependencies requiring adaptation to martingale difference sequences.

- **Open Question 2:** Can the LSE estimator be adapted for positive λ (λ > 0) to address data imbalance, and what are the resulting theoretical properties? The paper restricts analysis to λ < 0 to ensure robustness via implicit shrinkage, while positive λ increases influence of large weights and current proofs do not apply.

- **Open Question 3:** How can the LSE formulation be modified to handle partially missing rewards in the logged dataset? The current LSE operator assumes a reward value exists for every sample, necessitating integration with semi-supervised learning techniques when feedback is partially missed.

## Limitations

- Theoretical analysis relies heavily on bounded (1+ε)-th moment assumptions which may not hold in many real-world scenarios
- Empirical evaluation has limited comparison with some newer baselines like doubly robust estimators with neural network components
- The proposed data-driven λ selection heuristic requires knowledge of ε, which may be difficult to estimate in practice

## Confidence

- **High Confidence:** The variance reduction mechanism via non-linear shrinkage is mathematically sound and well-supported by theoretical analysis
- **Medium Confidence:** The heavy-tailed reward handling shows strong theoretical backing, but real-world applications may not perfectly match bounded moment assumptions
- **Medium Confidence:** The robustness to noisy propensity scores is demonstrated empirically but relies on specific noise models that may not generalize to all noise types

## Next Checks

1. **Robustness to Assumption Violations:** Test LSE performance when the (1+ε)-th moment assumption is violated (e.g., ε > 1 or unbounded moments) to validate the method's limitations

2. **Real-world Applicability:** Apply LSE to a production recommendation system with naturally heavy-tailed rewards to assess practical benefits beyond synthetic and benchmark datasets

3. **Hyperparameter Sensitivity:** Conduct a systematic ablation study on λ selection strategies across different data regimes (small vs. large n, different ε values) to validate the proposed data-driven approach