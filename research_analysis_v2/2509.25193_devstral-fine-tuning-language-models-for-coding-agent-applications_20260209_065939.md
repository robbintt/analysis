---
ver: rpa2
title: 'Devstral: Fine-tuning Language Models for Coding Agent Applications'
arxiv_id: '2509.25193'
source_url: https://arxiv.org/abs/2509.25193
tags:
- devstral-small
- performance
- code
- openhands
- temperature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Devstral-Small, a 24B parameter open-source
  language model specialized for code agent applications. The authors developed the
  model using a two-stage training process on agent trajectories from SWE-Gym environments,
  followed by policy optimization.
---

# Devstral: Fine-tuning Language Models for Coding Agent Applications

## Quick Facts
- arXiv ID: 2509.25193
- Source URL: https://arxiv.org/abs/2509.25193
- Authors: Abhinav Rastogi et al. (75 authors total)
- Reference count: 6
- Devstral-Small achieves 46.8% resolve rate on SWE-bench Verified at 50 max iterations

## Executive Summary
Devstral-Small is a 24B parameter open-source language model specialized for code agent applications. The authors developed the model using a two-stage training process on agent trajectories from SWE-Gym environments, followed by policy optimization. Devstral-Small achieves state-of-the-art performance among open models under 100B parameters on SWE-bench Verified, outperforming models like Qwen 3 235B and DeepSeek-V3 despite being significantly smaller.

## Method Summary
Devstral-Small is trained on agent trajectories from SWE-Gym environments using OpenHands CodeAct scaffold. The training follows a two-stage supervised fine-tuning approach with progressive quality filtering, followed by policy optimization. The model alternates between chain-of-thought reasoning and environment actions (bash execution, file editing). A natural language mixture is incorporated for general capabilities. The model uses Mistral Small 3 as base with 24B parameters, 40 layers, grouped query attention, and 128k context.

## Key Results
- Achieves 46.8% resolve rate on SWE-bench Verified with 50 max iterations
- Outperforms Qwen 3 235B and DeepSeek-V3 despite being significantly smaller
- 50 iterations optimal (36.8% at 30 iterations, no gain at 100 iterations)
- Lower temperatures (0.1-0.4) scale better with Pass@K than higher temperatures

## Why This Works (Mechanism)

### Mechanism 1
Training on agent trajectories from real software engineering environments yields superior code agent performance compared to general-purpose code models. The model learns the alternation pattern between chain-of-thought reasoning and environment actions through supervised trajectories generated in SWE-Gym environments using OpenHands CodeAct scaffold. This instills agentic behavior rather than just code completion. Software engineering tasks requiring multi-step reasoning over project contexts differ fundamentally from competitive programming tasks that traditional code models target.

### Mechanism 2
Two-stage progressive quality filtering enables efficient learning from noisy trajectory data. Stage 1 trains on heuristic-filtered larger subset; Stage 2 fine-tunes only on trajectories passing strictest filters. This curriculum allows the model to first acquire broad capabilities, then specialize on highest-quality demonstrations. The quality distribution of agent trajectories is highly skewed; strict filtering is necessary but reduces data volume too much for initial training.

### Mechanism 3
Iterative evaluation with controlled temperature escalation improves reliability by preferentially selecting later solutions. Three attempts per instance (temp=0, 0.1, 0.1) with preference for later solutions provides coverage while minimizing compute. Lower temperatures (0.1-0.4) scale better with Pass@K than higher temperatures for software engineering tasks. Deterministic generation benefits more from multiple attempts; higher temperatures introduce unhelpful variance in structured tasks.

## Foundational Learning

- **Agent-Computer Interface (ACI):** Why needed here: Devstral operates through OpenHands scaffold with bash execution and file editing tools; understanding ACI design is essential for effective deployment. Quick check: Can you explain why the paper disables web browsing and retrieval tools during evaluation?

- **Pass@K evaluation metric:** Why needed here: Temperature scaling experiments rely on Pass@K; the counter-intuitive finding that lower temperatures scale better requires understanding what Pass@K measures. Quick check: Why does Pass@4 at T=0.1 (62.0%) outperform Pass@4 at T=1.0 (60.4%) despite similar Pass@1 rates?

- **Policy optimization for code agents:** Why needed here: The training pipeline includes policy optimization after supervised fine-tuning; this is distinct from standard RLHF approaches. Quick check: What additional rollout data is generated between supervised fine-tuning and policy optimization stages?

## Architecture Onboarding

- **Component map:** Base model (Mistral Small 3 24B) -> SWE-Gym trajectories -> Stage 1 SFT (heuristic-filtered) -> Stage 2 SFT (strict-filtered) -> Policy optimization -> OpenHands scaffold (bash/file tools) -> vLLM server with paged attention

- **Critical path:** 1. Data generation → Agent rollouts in SWE-Gym with unit test validation 2. Stage 1 SFT → Heuristic-filtered trajectories 3. Stage 2 SFT → Strictly-filtered trajectories only 4. Policy optimization → Additional rollouts with finetuned model 5. Iterative evaluation → Up to 3 attempts with temperature escalation

- **Design tradeoffs:** 50 iterations optimal; 30 insufficient (36.8%), 100 wasteful (no gain). Lower temperatures preferred for multi-attempt strategies despite standard RL intuition. Pseudo-scaffolds and multi-format training (XML + function calling) improve generalization but add complexity.

- **Failure signatures:** Empty patches (7.6% in first iteration, 0% by iteration 2). Performance plateau at 46.8% regardless of iteration budget beyond 50. Stochasticity from GPU parallelism and file system state even at temp=0.

- **First 3 experiments:** 1. Reproduce iteration limit ablation (30/50/100) on SWE-bench Verified subset to validate 50-iteration optimum for your compute budget. 2. Test temperature scaling on your specific scaffold; verify if lower temperatures (0.1-0.4) consistently outperform higher temperatures at Pass@K. 3. Deploy with iterative evaluation protocol (3 attempts, prefer later solutions) and compare single-run vs. iterative resolve rates on held-out issues.

## Open Questions the Paper Calls Out

1. **Advanced refinement strategies:** Can advanced refinement strategies (e.g., Self-Refine) outperform the current temperature-based iterative evaluation protocol for code agents? The authors state they "leave exploration of more advanced refinement strategies (e.g. Madaan et al. [2023]) to future work" regarding their iterative protocol.

2. **Temperature scaling counter-intuition:** Why does lower temperature sampling improve Pass@K scaling in agentic tasks, contradicting trends seen in competitive programming? The authors note that lower temperatures consistently improve with increased K, calling the results "counter-intuitive" compared to competitive programming expectations.

3. **Fundamental iteration limits:** What are the "fundamental challenges" that prevent performance improvements beyond 50 agent iterations? Table 1 shows a performance plateau at 46.8% for both 50 and 100 iterations, which the authors suggest indicates the model "encounters fundamental challenges that additional iterations cannot overcome."

## Limitations

- Policy optimization details, filter definitions, and natural language mixture composition are unspecified
- 46.8% resolve rate still represents substantial room for improvement with limited analysis of failure cases
- Despite being "lightweight," 24B parameters still require significant computational resources

## Confidence

- **High confidence:** State-of-the-art performance among open models under 100B parameters on SWE-bench Verified; iteration limit optimization (50 iterations optimal) well-supported by ablation; counter-intuitive temperature scaling results empirically demonstrated
- **Medium confidence:** Two-stage training approach with progressive quality filtering likely contributes to performance; specialization claim supported by benchmark results but could benefit from more systematic ablation studies
- **Low confidence:** Exact contribution of policy optimization vs. supervised fine-tuning cannot be assessed without implementation details; natural language mixture's impact on agent capabilities not quantified

## Next Checks

1. Reproduce the iteration limit ablation: Run controlled experiments testing 30, 50, and 100 iteration limits on SWE-bench Verified to confirm the 50-iteration optimum for your specific deployment environment.

2. Validate temperature scaling on your scaffold: Systematically test Pass@K performance across temperature ranges (0.0, 0.1, 0.4, 0.7, 1.0) to verify if lower temperatures consistently outperform higher temperatures in your specific agent-computer interface configuration.

3. Analyze failure cases in detail: Deploy Devstral-Small on a held-out subset of SWE-bench issues and conduct systematic failure analysis to understand which problem types remain unsolved and whether these align with the paper's claims about multi-file edits and long-horizon reasoning challenges.