---
ver: rpa2
title: Reward Reasoning Model
arxiv_id: '2505.14674'
source_url: https://arxiv.org/abs/2505.14674
tags:
- reward
- reasoning
- rrms
- arxiv
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Reward Reasoning Models (RRMs) introduce chain-of-thought reasoning
  into reward modeling to improve accuracy on complex queries. By training via rule-based
  reinforcement learning without requiring explicit reasoning traces, RRMs learn to
  adaptively allocate test-time compute through multi-response rewarding strategies
  like ELO rating and knockout tournament.
---

# Reward Reasoning Model

## Quick Facts
- arXiv ID: 2505.14674
- Source URL: https://arxiv.org/abs/2505.14674
- Reference count: 40
- Primary result: RRMs achieve up to 98.6% accuracy on reasoning tasks and state-of-the-art results in reward-guided best-of-N inference

## Executive Summary
Reward Reasoning Models (RRMs) introduce chain-of-thought reasoning into reward modeling to improve accuracy on complex queries. By training via rule-based reinforcement learning without requiring explicit reasoning traces, RRMs learn to adaptively allocate test-time compute through multi-response rewarding strategies like ELO rating and knockout tournament. Experiments show RRMs outperform strong baselines on diverse reward modeling benchmarks, achieving up to 98.6% accuracy on reasoning tasks and state-of-the-art results in reward-guided best-of-N inference across MMLU-Pro, MATH, and GPQA. They also demonstrate superior post-training performance and consistently improve with increased test-time compute, indicating their ability to leverage additional reasoning steps for enhanced performance.

## Method Summary
RRMs formulate reward modeling as a text completion problem where the model autoregressively generates a reasoning trace before outputting the final judgment (`\boxed{Assistant 1}` or `\boxed{Assistant 2}`). The training uses Group Relative Policy Optimization (GRPO) with rule-based binary rewards (+1 for correct preference selection, -1 otherwise) on DeepSeek-R1-Distill-Qwen base models. The training data combines Skywork-Reward, Tülu 3 prompts, and synthesized data from WebInstruct, Skywork-OR1, Big-Math-RL, and DAPO-Math, totaling approximately 420K pairwise preference examples. At inference, RRMs use multi-response strategies (ELO rating, knockout tournament, majority voting) to scale test-time compute for best-of-N selection while maintaining pairwise input constraints.

## Key Results
- RRMs achieve 98.6% accuracy on reasoning tasks, outperforming strong baselines
- State-of-the-art results in reward-guided best-of-N inference across MMLU-Pro, MATH, and GPQA
- Superior post-training performance with consistent improvements as test-time compute increases
- ELO rating strategy slightly outperforms knockout tournament while both enable effective multi-candidate selection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicit chain-of-thought reasoning before reward generation improves judgment accuracy on complex queries
- Mechanism: RRM generates reasoning traces that decompose evaluation criteria (instruction fidelity, helpfulness, accuracy, harmlessness) and iterate on comparisons
- Core assumption: Additional compute allocated to reasoning translates to better reward accuracy
- Evidence anchors: Abstract mentions leveraging test-time compute for complex queries; Section 3.1 contrasts with existing reward models
- Break condition: If reasoning traces become redundant without addressing evaluation criteria

### Mechanism 2
- Claim: Rule-based RL with sparse outcome rewards can induce reward reasoning capabilities without explicit reasoning supervision
- Mechanism: Binary reward (+1/-1) shapes the policy to generate useful reasoning traces that maximize outcome correctness
- Core assumption: Base model has sufficient reasoning capacity to discover useful evaluation strategies through exploration
- Evidence anchors: Abstract describes fostering self-evolved reward reasoning capabilities; Section 3.2 shows rule-based rewards effectively supervise reasoning patterns
- Break condition: If base model lacks reasoning priors or exploration is insufficient

### Mechanism 3
- Claim: Multi-response strategies (ELO rating, knockout tournament) enable RRMs to scale test-time compute for best-of-N selection
- Mechanism: ELO runs round-robin pairwise comparisons (O(n²)) for full rankings; tournament runs elimination brackets (O(n)) to find best candidate
- Core assumption: Pairwise comparisons generalize to consistent multi-candidate rankings
- Evidence anchors: Section 3.3 describes combining strategies with majority voting; Section 4.5.1 shows ELO outperforms tournament with trade-offs
- Break condition: If pairwise comparisons are inconsistent (non-transitive preferences)

## Foundational Learning

- **Chain-of-thought (CoT) reasoning**
  - Why needed here: RRM's core innovation is generating reasoning traces before rewards
  - Quick check question: Can you explain why CoT helps on problems where the answer isn't immediately obvious?

- **Group Relative Policy Optimization (GRPO)**
  - Why needed here: RRM uses GRPO for RL training
  - Quick check question: How does GRPO differ from standard PPO in how it estimates advantages?

- **Reward modeling paradigms (scalar vs. generative)**
  - Why needed here: RRM is a generative reward model with explicit reasoning
  - Quick check question: What are the trade-offs between outputting a scalar score vs. natural language justification?

## Architecture Onboarding

- **Component map:**
  - Qwen2 Transformer-decoder (7B/32B) -> DeepSeek-R1-Distill-Qwen initialization -> GRPO training -> ELO/tournament inference
  - Input: Query + two candidate responses with system prompt
  - Output: Reasoning trace followed by `\boxed{Assistant 1}` or `\boxed{Assistant 2}`

- **Critical path:**
  1. Prepare preference pairs from Skywork-Reward + synthesized data
  2. Initialize from DeepSeek-R1-Distill-Qwen
  3. Train with GRPO, binary outcome reward
  4. Choose inference strategy based on compute budget

- **Design tradeoffs:**
  - ELO vs. tournament: ELO gives better accuracy but costs O(n²); tournament is O(n) with slight accuracy drop
  - Voting@k: Improves robustness but multiplies compute by k
  - Thinking budget: Longer traces improve accuracy but with diminishing returns and latency cost

- **Failure signatures:**
  - Model outputs `\boxed{Assistant 1}` without meaningful reasoning → likely undertrained or base model too weak
  - Inconsistent pairwise comparisons causing non-transitive rankings → may need more voting or calibration
  - Overfitting to response length/style heuristics → check if prompt properly instructs bias avoidance

- **First 3 experiments:**
  1. Ablate reasoning: Compare RRM vs. DirectJudge (same data, no reasoning) on RewardBench reasoning subset
  2. Scale test-time compute: Run RRM-32B on MATH with varying pairwise comparison counts and voting@k
  3. Post-training validation: Use RRM-32B ELO ratings as rewards for GRPO on unlabeled WebInstruct queries

## Open Questions the Paper Calls Out

- **Open Question 1:** To what extent does the RRM's reliance on synthetic preference data labeled by GPT-4o constrain its ability to surpass the performance ceiling or correct the biases inherent in the teacher model?
  - Basis: Section 4.1 describes training data composition with 80K Tülu 3 queries annotated by GPT-4o
  - Resolution evidence: Ablation comparing RRMs trained on human-annotated vs. GPT-4o annotated data

- **Open Question 2:** Can the computational complexity of the ELO rating strategy be reduced while maintaining its accuracy advantage over the knockout tournament for best-of-N inference?
  - Basis: Section 4.5.1 compares ELO (O(n²)) and tournament (O(n)) strategies
  - Resolution evidence: Experiments evaluating partial ELO rating systems with sampled pairwise matchups

- **Open Question 3:** How dependent is the emergence of effective reward reasoning capabilities on the selection of a pre-distilled reasoning model as the base architecture?
  - Basis: Section 3.2 states DeepSeek-R1 distilled models are used as base models in all experiments
  - Resolution evidence: Training ablation using a standard instruct model without R1 distillation

## Limitations

- Scalability of rule-based RL to truly complex reasoning tasks remains unproven
- Binary reward signal may create sparse learning signals limiting exploration of diverse reasoning strategies
- ELO and tournament inference strategies assume pairwise comparisons are consistent and transitive

## Confidence

- **High Confidence**: Core claim of outperforming strong baselines on RewardBench and PandaLM Test
- **Medium Confidence**: Claim that chain-of-thought reasoning specifically drives performance improvements
- **Low Confidence**: Assertion that RRMs can "effectively leverage additional test-time compute for complex queries"

## Next Checks

1. **Intransitivity Analysis**: Systematically test RRMs on preference pairs designed to exhibit known intransitivities to quantify logical inconsistencies and impact on ELO/tournament accuracy

2. **Reasoning Strategy Diversity**: Use clustering or similarity metrics to analyze whether reasoning traces converge to heuristics versus maintaining diverse approaches across query types

3. **Out-of-Distribution Transfer**: Evaluate RRMs on novel complex reasoning tasks (medical diagnosis, legal reasoning) to test generalization beyond benchmark optimization