---
ver: rpa2
title: Lower Bound on Howard Policy Iteration for Deterministic Markov Decision Processes
arxiv_id: '2506.12254'
source_url: https://arxiv.org/abs/2506.12254
tags:
- policy
- howard
- algorithm
- iteration
- edge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper presents an improved lower bound on Howard's policy\
  \ iteration algorithm for deterministic Markov decision processes (DMDPs) with mean-payoff\
  \ objectives. While previous work showed the algorithm requires sublinear iterations\
  \ with respect to input size, this work demonstrates that for input size I, the\
  \ algorithm requires \u03A9(I) iterations."
---

# Lower Bound on Howard Policy Iteration for Deterministic Markov Decision Processes

## Quick Facts
- **arXiv ID:** 2506.12254
- **Source URL:** https://arxiv.org/abs/2506.12254
- **Reference count:** 24
- **Primary result:** Improved lower bound of Ω(I) iterations for Howard's policy iteration algorithm on DMDPs with polynomial weights

## Executive Summary
This paper presents a significant improvement in the lower bound for Howard's policy iteration algorithm on deterministic Markov decision processes with mean-payoff objectives. The authors construct a family of DMDPs with 2n vertices and O(n²) edges where Howard's algorithm requires Ω(n²) iterations to converge. Unlike previous lower bounds that required exponential weights, this construction uses only polynomial weights (O(n²)), making the bound linear with respect to input size. The key technical insight is the "deceleration lane" technique that forces sequential discovery of optimal cycles rather than parallel exploration.

## Method Summary
The authors construct a specific family of DMDPs P_n with 2n vertices (b_1,...,b_n, t_1,...,t_n) and O(n²) edges. The construction uses polynomial edge weights w ≈ n² and implements Howard's policy iteration with a Bellman operator that performs lexicographic comparisons of appraisals (value, potential). The algorithm uses strict tie-breaking rules that prioritize the current policy choice followed by lowest vertex index. The method computes mean-payoff values using cycle detection in lasso-shaped runs and tracks potential functions for tie-breaking. The construction forces vertices to traverse a "deceleration lane" sequentially, requiring Ω(n²) iterations to discover all optimal cycles.

## Key Results
- Improved lower bound from sublinear to linear in input size: Ω(I) iterations required
- Construction uses only polynomial weights (O(n²)) rather than exponential weights
- Family of DMDPs with 2n vertices requires Ω(n²) iterations
- Lower bound holds for a class where best known upper bound is also polynomial
- Addresses fundamental open question about complexity of classical algorithm

## Why This Works (Mechanism)

### Mechanism 1: The Deceleration Lane Constraint
- Claim: The constructed graph forces sequential cycle discovery through a bottlenecked traversal
- Mechanism: Vertices b_1,...,b_n form a "deceleration lane" with high weights (n+1)² that tempt vertices away from current cycles, but tie-breaking rules restrict adding only one lane edge per iteration
- Core assumption: Lexicographical tie-breaking (current edge first, then lowest index) prevents skipping steps
- Evidence anchors: Section 4.3 explicitly describes how the technique forces Ω(i) iterations to find the ith cycle
- Break condition: Random or altered tie-breaking rules could reduce iterations by allowing parallel traversal

### Mechanism 2: Progress Reset on Cycle Discovery
- Claim: Finding new best cycles causes immediate reset of edge selection progress
- Mechanism: When a higher-value cycle is discovered, vertices discard current paths and switch directly to the new optimal cycle, forcing restart of slow traversal
- Core assumption: Direct edges exist from lane vertices to newly discovered optimal cycles
- Evidence anchors: Section 4.3 describes how "all progress in the deceleration lane is lost" when a new best cycle is found
- Break condition: Absence of direct edges to new cycle would delay reset, potentially changing iteration count

### Mechanism 3: Polynomial Weight Scaling
- Claim: Lower bound holds for input size I due to polynomial rather than exponential weights
- Mechanism: Previous bounds used exponential weights inflating input size; this construction uses w ≈ n² making I ≈ n², so Ω(n²) iterations = Ω(I)
- Core assumption: Inequalities w̄ > w_i and (k+1)w_i > kw̄ hold for chosen polynomial weights
- Evidence anchors: Abstract mentions improvement over exponential weight lower bounds; Page 2 discusses polynomial weight class
- Break condition: Weight truncation or insufficient precision breaks delicate inequality balance

## Foundational Learning

- Concept: **Mean-Payoff (Limit-Average) Objective**
  - Why needed here: Algorithm maximizes average reward over infinite horizon, justifying search for simple cycles rather than long paths
  - Quick check question: Why does the controller prefer a self-loop with weight 10 over a path of length 10 with total weight 99?

- Concept: **Positional (Stationary) Policies**
  - Why needed here: Policies are maps σ: V → V, allowing finite iteration over graph structures rather than histories
  - Quick check question: In a mean-payoff DMDP, does the optimal action at vertex v depend on the path taken to reach v?

- Concept: **The Bellman Operator & Appraisal**
  - Why needed here: The lexicographic appraisal tuple (value, potential) determines edge selection; understanding this comparison is critical to why the "deceleration lane" works
  - Quick check question: If two edges lead to the same mean-payoff value, what secondary metric determines the choice?

## Architecture Onboarding

- Component map: V_n = {t_1,...,t_n, b_1,...,b_n}; Target cycles (self-loops at t_i with weights w_i); Deceleration lane (edges between b and t vertices with weight (n+1)²); Initialization policies

- Critical path:
  1. **Initialization:** Start with π_1
  2. **Traversal (The Bottleneck):** Switch policies σ_{i,j} → σ_{i,j+1}, vertices slowly shift edges to include more b-vertices
  3. **Cycle Discovery:** Vertex t_{i+1} prefers its self-loop over path to t_i
  4. **Reset:** All vertices switch to point to t_{i+1}, repeat process

- Design tradeoffs:
  - Weights: Large enough to entice traversal (w̄ > w_i) but small enough to keep input size polynomial
  - Tie-breaking: Strictly defined to prevent "jumping" ahead in lane; real implementations might use different rules

- Failure signatures:
  - **Iteration Collapse:** Algorithm converges in o(n²) steps, likely due to weight truncation or random tie-breaking
  - **Non-termination:** Incorrect weight logic could cause cycling between policies if Bellman improvement property is violated

- First 3 experiments:
  1. **Baseline Replication:** Implement P_n for n=3,4,5; verify policy sequence matches Section 4.3 exactly
  2. **Scalability:** Run construction for n=10,50,100; plot iterations vs n² to confirm quadratic growth
  3. **Robustness Check:** Perturb weights by ±1; observe if structure collapses or bound is robust to noise

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is Hansen's conjecture true that Howard's algorithm performs at most |E| iterations on any DMDP?
- Basis in paper: Section 3 states Conjecture 3.1; Section 6 calls it "a major open problem"
- Why unresolved: Paper provides quadratic lower bound but edge-based upper bound remains unproven
- What evidence would resolve it: Proof establishing edge count as upper bound, or counterexample with more iterations than edges

### Open Question 2
- Question: Can Ω(n²) lower bound construction be extended to discounted-sum objectives with constant discount factor?
- Basis in paper: Section 5 states this extension only works when discount factor approaches 1 as function of n
- Why unresolved: Constant factors may require fundamentally different constructions
- What evidence would resolve it: Family of DMDPs with Ω(n²) iterations under fixed discount factor λ < 1, or proof such bound is impossible

### Open Question 3
- Question: What structural properties of real-world DMDPs explain practical efficiency of Howard's algorithm despite weak theoretical bounds?
- Basis in paper: Section 6 highlights practical performance raises relevant questions despite high theoretical worst-case complexity
- Why unresolved: Experimental studies show good performance but no structural characterization exists
- What evidence would resolve it: Identification of graph-theoretic or numeric properties guaranteeing fast convergence, supported by empirical validation

## Limitations

- Lower bound construction is specific to mean-payoff objectives and may not extend to other reward structures
- The result relies on specific tie-breaking rules that may not reflect all practical implementations
- Polynomial weight scaling shows the bound is linear in input size, but gap remains between lower and upper bounds

## Confidence

- **High Confidence:** Sublinear to linear iteration bound improvement - clearly stated and polynomial weight construction directly supports it
- **Medium Confidence:** Exact iteration count formula Ω(n²) for constructed family - requires careful implementation of Bellman operator and tie-breaking
- **Low Confidence:** Whether "deceleration lane" represents worst possible case - paper constructs one family achieving this bound, but proving optimality remains open

## Next Checks

1. **Critical Tie-Breaking Implementation:** Verify lexicographic tie-breaking (current edge first, then lowest index) is implemented exactly as specified, as this appears crucial to sequential traversal mechanism

2. **Weight Sensitivity Analysis:** Test whether small perturbations to polynomial weights (±1) preserve iteration count structure, determining if bound is robust or relies on precise weight relationships

3. **Alternative Policy Iteration Algorithms:** Compare Howard's algorithm with other policy iteration variants on same construction to see if they exhibit same Ω(n²) iteration behavior, suggesting lower bound is inherent to problem structure rather than specific to Howard's method