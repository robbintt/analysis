---
ver: rpa2
title: Mitigating Modality Imbalance in Multi-modal Learning via Multi-objective Optimization
arxiv_id: '2511.06686'
source_url: https://arxiv.org/abs/2511.06686
tags:
- multi-modal
- learning
- modality
- mimo
- uni-modal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses modality imbalance in multi-modal learning
  (MML), where some modalities learn faster than others, leading to suboptimal performance.
  The authors reformulate MML as a multi-objective optimization problem that prioritizes
  the worst-performing modality's objective while optimizing the overall multi-modal
  objective.
---

# Mitigating Modality Imbalance in Multi-modal Learning via Multi-objective Optimization

## Quick Facts
- **arXiv ID:** 2511.06686
- **Source URL:** https://arxiv.org/abs/2511.06686
- **Reference count:** 40
- **Key outcome:** MIMO algorithm achieves up to 20× speedup over baselines while improving multi-modal learning performance by prioritizing worst-performing modality's objective.

## Executive Summary
This paper addresses modality imbalance in multi-modal learning (MML), where some modalities learn faster than others, leading to suboptimal performance. The authors reformulate MML as a multi-objective optimization problem that prioritizes the worst-performing modality's objective while optimizing the overall multi-modal objective. They propose a gradient-based algorithm called MIMO that solves this modified problem without computationally intensive subroutines. The method achieves up to 20× speedup compared to baselines while improving performance across multiple MML benchmarks (CREMA-D, UR-Funny, Kinetics-Sound, VGGSound, AV-MNIST, CMU-MOSEI, AVE). MIMO consistently outperforms existing balanced MML and multi-objective optimization methods, with faster subroutine times close to vanilla MML. Theoretical analysis provides convergence guarantees for the proposed method.

## Method Summary
The method reformulates MML as a lexicographic multi-objective optimization problem where the worst-performing uni-modal objective must be optimized before the multi-modal objective. This is achieved through a penalty formulation using smoothed maximum of uni-modal optimality gaps, weighted by per-modality coefficients derived from softmax over these gaps. The MIMO algorithm uses auxiliary uni-modal heads during training to estimate learning progress and modulate gradients for balanced encoder updates, while maintaining inference efficiency with only the multi-modal head. The approach operates on late fusion architectures with shared encoders producing modality-specific and joint representations.

## Key Results
- Achieves up to 20× speedup compared to multi-objective optimization baselines on CREMA-D benchmark
- Consistently outperforms vanilla MML, OGM-GE, and MGDA across 7 multi-modal datasets
- Maintains subroutine computation times close to vanilla MML (0.037s vs 0.038s on CREMA-D)
- Improves multi-modal test accuracy while reducing uni-modal performance disparity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Requiring optimality of the worst-performing uni-modal objective prevents dominant modalities from overfitting and improves multi-modal generalization.
- **Mechanism:** The formulation in Equation (8) constrains the multi-modal objective to be optimized only after minimizing the maximum uni-modal optimality gap (max over k of f_mk - f*_mk). When one modality learns faster (smaller optimality gap), the penalty term automatically amplifies gradients for the slower modality via λ-weighting, forcing balanced encoder updates.
- **Core assumption:** Late fusion architecture with concatenation or sum fusion where uni-modal and multi-modal objectives share encoder parameters θ_mk but have separate heads.
- **Evidence anchors:**
  - [abstract]: "reformulate the MML problem as a multi-objective optimization (MOO) problem that overcomes the imbalanced learning issue"
  - [Section 3.1, Eq. 10]: Shows penalty reformulation where λ controls constraint enforcement on worst-performing modality
  - [Section 3.1]: "Assume modality m1 is quick to learn... f_m2 is larger compared to f_m2. Thus, λ1 = 0 and λ2 = λ... weights in m2 component of the model are updated rapidly"
- **Break condition:** If modalities have identical learning speeds (no optimality gap difference), the λ-weighting provides no correction; if the multi-modal head has no modality-specific partition, gradient decomposition fails.

### Mechanism 2
- **Claim:** LogSumExp smoothing of the max operator preserves gradient flow while maintaining O(1/ε) convergence to ε-stationary points.
- **Mechanism:** The non-smooth max(f_m1 - f*_m1, f_m2 - f*_m2) is replaced with μ·log(Σ_k exp((f_mk - f*_mk)/μ)). This provides smooth gradients via softmax-weighted combination (σ_mk terms in Algorithm 1), where smaller μ better approximates hard max.
- **Core assumption:** Objectives f_mm and f_mk are L-smooth (Assumption 1) and f_mk is Lipschitz continuous (Assumption 2).
- **Evidence anchors:**
  - [Section 3.2]: "g_μ(Θ) := μ log(Σ exp(μ^-1 g_m(Θ))) is a smoothing function for g(Θ) = max g_m(Θ)"
  - [Section 3.3, Theorem 1]: "Algorithm 1 reaches to an ε stationary point of f̂_mm with iteration complexity of O(1/ε)"
  - [corpus]: Weak - neighbor papers don't directly validate this smoothing mechanism
- **Break condition:** Large μ causes smoothed max to deviate from true max, failing to prioritize worst modality; numerical overflow when loss values are large (addressed by adaptive μ scaling in implementation).

### Mechanism 3
- **Claim:** Uni-modal auxiliary heads enable gradient modulation without requiring pre-trained uni-modal models or expensive bi-level optimization subroutines.
- **Mechanism:** Adding lightweight uni-modal heads (linear layers) at training time creates auxiliary objectives f_mk that expose per-modality learning progress. The encoder gradients combine multi-modal and uni-modal contributions (θ_mk update in Algorithm 1 includes both ∇f_mm and λ·σ_mk·∇f_mk), achieving balance with only forward/backward passes through added heads.
- **Core assumption:** Uni-modal heads are only needed during training; inference uses only multi-modal head.
- **Evidence anchors:**
  - [Figure 1b]: Shows uni-modal heads (ϑ_m1, ϑ_m2) as auxiliary branches for computing f_m1, f_m2
  - [Section 4, Table 1]: Subroutine times show MIMO (0.037s) comparable to vanilla MML (0.038s) on CREMA-D, ~20× faster than MMPareto (0.309s)
  - [corpus]: Neighbor papers (e.g., "Rebalanced Multimodal Learning with Data-aware Unimodal Sampling") also use auxiliary uni-modal signals but via sampling rather than gradient modulation
- **Break condition:** If uni-modal heads are too expressive (overfit), their loss may not reflect true modality learning progress; if f*_mk estimates are inaccurate, optimality gap mismeasurement corrupts balancing.

## Foundational Learning

- **Concept: Lexicographic Multi-objective Optimization**
  - Why needed here: The core reformulation (Eq. 2, 8) treats the worst uni-modal objective as a hard constraint that must be satisfied before optimizing the multi-modal objective. This differs from weighted sum or Pareto approaches.
  - Quick check question: Given objectives [f1=0.1, f2=0.8, f_mm=0.3], which does lexicographic MOO with priority order [max(f1,f2), f_mm] optimize first?

- **Concept: Polyak-Łojasiewicz (PL) Condition**
  - Why needed here: Theoretical analysis (Assumption 4, Proposition 2) uses PL condition to bound how fast the smoothed max objective can decrease, justifying convergence guarantees.
  - Quick check question: If ||∇f(θ)||² ≥ (1/μ)(f(θ) - f*), what does this imply about the relationship between gradient norm and suboptimality?

- **Concept: Late Fusion Architecture**
  - Why needed here: MIMO assumes separate uni-modal encoders producing features that are fused (concatenated/summed) before a shared head. The gradient decomposition in Eq. 5 and 11 relies on this structure.
  - Quick check question: In late fusion with concatenation, why can the multi-modal head gradient be partitioned into modality-specific components?

## Architecture Onboarding

- **Component map:**
  Input: x^(m1), x^(m2), ..., x^(mK) [K modalities] → Uni-modal Encoders: θ_m1, θ_m2, ..., θ_mK [shared parameters] → Uni-modal Heads (training-only): ϑ_m1, ϑ_m2, ..., ϑ_mK → Uni-modal losses: f_m1, f_m2, ..., f_mK → Fusion (concat/sum) → Multi-modal Head: ϑ_mm → Multi-modal loss: f_mm

- **Critical path:**
  1. Initialize all parameters near zero (standard for late fusion)
  2. For each batch, compute f_mm and all f_mk via forward pass through respective heads
  3. Compute σ_mk = exp(h_μ,k) / Σ exp(h_μ,k') where h_μ,k = (f_mk - f*_mk)/μ
  4. Update encoders: θ_mk ← θ_mk - η(∇_θ f_mm + λ·σ_mk·∇_θ f_mk)
  5. Update heads independently with respective objective gradients
  6. (Optional) Track f*_mk estimates via running minimum of f_mk

- **Design tradeoffs:**
  - λ (penalty): Larger λ enforces stronger balancing but may slow multi-modal objective optimization. Ablation shows λ ≥ 10 works well; λ = 1.0 is too weak.
  - μ (smoothing): Smaller μ better approximates hard max but risks numerical instability. Paper uses μ ∈ {0.001, 0.01, 0.1, 1.0} with adaptive scaling for stability.
  - f*_mk estimation: Using running minimum is cheap but may underestimate true minimum; could use holdout set for more accurate estimates.

- **Failure signatures:**
  - **Dominant modality persists:** Check if λ is too small or f*_mk estimates are too pessimistic (making optimality gap always small for one modality)
  - **Numerical overflow in softmax:** Loss values too large for exp(); increase μ or normalize losses
  - **No improvement over vanilla MML:** Dataset may not have modality imbalance (e.g., UR-Funny shows modest gains); verify by comparing uni-modal accuracy disparity

- **First 3 experiments:**
  1. **Toy validation:** Replicate Figure 1 two-layer regression on synthetic data satisfying superficial modality preference conditions (Eq. 16-18). Confirm encoder norms (||θ_m1||, ||θ_m2||) converge at similar rates with MIMO vs. divergent rates with vanilla MML.
  2. **CREMA-D baseline comparison:** Implement MIMO with audio (Mel-spectrogram) + visual (face frames) encoders. Compare multi-modal test accuracy and uni-modal accuracy disparity (|Acc_a - Acc_v|) against vanilla MML, OGM-GE, and MGDA. Target: MIMO should achieve ~75% accuracy with ~4% uni-modal disparity.
  3. **Hyperparameter sensitivity:** Grid search λ ∈ {1, 10, 100} and μ ∈ {0.001, 0.01, 0.1, 1.0} on CREMA-D validation set. Plot multi-modal accuracy vs. uni-modal disparity to identify Pareto frontier; validate that λ ≥ 10 and μ ≤ 0.1 produce best results.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the MIMO formulation be effectively adapted for early or hybrid fusion architectures where modalities are integrated before the final encoding stage?
- Basis in paper: [explicit] The conclusion states, "This work only focuses on late fusion cases... As future work, it would be interesting to see how MIMO-like algorithms... can be designed for early or hybrid fusion methods."
- Why unresolved: The current algorithm relies on separate gradients from distinct uni-modal encoders to calculate the smoothed max penalty, a structure absent in tightly coupled early fusion models.
- What evidence would resolve it: A theoretical extension of the lexicographic MOO formulation applicable to shared intermediate layers and empirical validation on standard early fusion benchmarks.

### Open Question 2
- Question: Under what specific quantitative metrics of modality imbalance does MIMO provide significant performance gains over standard joint training?
- Basis in paper: [inferred] Section 4 notes that on the UR-Funny dataset, "simple methods like vanilla MML also work reasonably well," which the authors attribute to the dataset potentially not satisfying "conditions for imbalance."
- Why unresolved: While the paper proves MIMO mitigates imbalance, it does not define the threshold or statistical properties required for a dataset to benefit from the method versus incurring its slight computational overhead.
- What evidence would resolve it: A correlation analysis mapping the performance gap between MIMO and vanilla MML against a quantifiable metric of modality learning speed disparity (e.g., gradient norms or uni-modal accuracy gaps) across diverse datasets.

### Open Question 3
- Question: Can the theoretical optimality guarantees for the worst-performing uni-modal objective be maintained without the restrictive Assumption 3 (fixing model heads)?
- Basis in paper: [inferred] Appendix D acknowledges that to prove the constraint satisfies the Polyak-Lojasiewicz (PL) condition (Theorem 2), the analysis must assume "ϑ_mm, ϑ_m1, ϑ_m2 are fixed," which decouples the heads from the optimization process.
- Why unresolved: The main algorithm (Algorithm 1) updates all parameters simultaneously, but the strongest theoretical guarantees rely on a bi-level simplification that does not fully reflect the end-to-end training dynamics.
- What evidence would resolve it: A proof establishing convergence to a Pareto optimal solution or satisfying the lexicographic constraint for the full parameter set $\hat{\Theta}_{mm}$ without decoupling the heads.

## Limitations
- MIMO formulation assumes late fusion architecture; extension to early or hybrid fusion requires significant modifications
- Theoretical convergence guarantees rely on strong assumptions (PL condition, convexity) that may not hold for deep neural networks
- Performance gains are most pronounced when datasets exhibit significant modality imbalance, showing modest improvements otherwise

## Confidence
- **High confidence:** 20× speedup claim (directly measured in Table 1), basic mechanism of gradient modulation via λ-weighted uni-modal gradients
- **Medium confidence:** Multi-objective reformulation prevents modality dominance (empirical evidence across 7 benchmarks, but theoretical guarantees require strong assumptions)
- **Low confidence:** Exact performance thresholds and dataset-specific hyperparameters (λ, μ values not fully specified per dataset)

## Next Checks
1. Verify convergence on synthetic data with known modality learning rates to isolate MIMO's balancing effect from other factors
2. Test MIMO on early fusion architectures to identify necessary gradient decomposition modifications
3. Conduct ablation studies varying uni-modal head sizes to quantify impact on optimality gap estimation accuracy