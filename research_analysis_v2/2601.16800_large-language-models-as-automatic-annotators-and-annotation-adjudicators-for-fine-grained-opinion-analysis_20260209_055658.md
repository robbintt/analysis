---
ver: rpa2
title: Large Language Models as Automatic Annotators and Annotation Adjudicators for
  Fine-Grained Opinion Analysis
arxiv_id: '2601.16800'
source_url: https://arxiv.org/abs/2601.16800
tags:
- opinion
- annotations
- aste
- annotation
- acos
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of Large Language Models (LLMs)
  for fine-grained opinion annotation, specifically addressing the shortage of domain-specific
  labelled datasets for tasks like Aspect Sentiment Triplet Extraction (ASTE) and
  Aspect-Category-Opinion-Sentiment (ACOS) analysis. The authors propose a declarative
  annotation pipeline using DSPy to reduce prompt variability and introduce a novel
  LLM-based adjudication approach that combines multiple annotations to resolve disagreements
  and produce final labels.
---

# Large Language Models as Automatic Annotators and Annotation Adjudicators for Fine-Grained Opinion Analysis

## Quick Facts
- arXiv ID: 2601.16800
- Source URL: https://arxiv.org/abs/2601.16800
- Reference count: 25
- Primary result: LLMs can serve as effective automatic annotators for fine-grained opinion analysis, with performance improving with model size and adjudication consistently improving ACOS results

## Executive Summary
This paper investigates the use of Large Language Models (LLMs) as automatic annotators and adjudicators for fine-grained opinion analysis tasks, specifically Aspect Sentiment Triplet Extraction (ASTE) and Aspect-Category-Opinion-Sentiment (ACOS) analysis. The authors propose a declarative annotation pipeline using DSPy to reduce prompt variability and introduce a novel LLM-based adjudication approach that combines multiple annotations to resolve disagreements. Experiments with models of different sizes (4B, 14B, and 32B parameters) demonstrate that LLMs can effectively annotate domain-specific opinion data, with performance scaling with model size. The Medium-sized models achieved the best individual alignment with human annotations, which was further improved through adjudication. While ASTE tasks showed high reliability (Krippendorff's α ≈ 0.7-0.8), ACOS tasks proved more challenging, particularly in identifying aspect categories across domains.

## Method Summary
The authors employ a declarative prompt programming approach using DSPy to systematically optimize prompts for ASTE and ACOS annotation tasks. The pipeline reserves half of the dev set for in-context learning examples and the other half for prompt evaluation. Multiple LLM annotators (Mini/Small/Medium scales) independently annotate test sets, followed by adjudication using the best-performing annotator to resolve disagreements. The system uses exact-match F1 for evaluating triplet/quadruple extraction accuracy and Krippendorff's α for measuring inter-annotator agreement. Temperature is set to 0.0, output length to 16384, and inference is served via vLLM. The approach aims to reduce the variability of manual prompt engineering and provide scalable, cost-effective annotation for fine-grained opinion analysis.

## Key Results
- LLMs of different sizes (4B, 14B, 32B) can effectively serve as automatic annotators for fine-grained opinion analysis tasks
- Medium-sized models achieved the best individual alignment with human annotations, which was further improved through adjudication
- Inter-Annotator Agreement scores indicated high reliability for ASTE tasks (α ≈ 0.7-0.8) and moderate reliability for ACOS tasks (α ≈ 0.3-0.5) across all model sizes
- ACOS task proved more challenging than ASTE, particularly in identifying aspect categories across domains

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Declarative prompt programming via DSPy reduces annotation variability compared to manual prompt engineering.
- **Mechanism:** DSPy uses a data model specification (inputs, outputs, types) and a pool of in-context learning examples to automatically compile optimized prompts, treating prompt construction as a programmatic optimization problem rather than string manipulation. The system evaluates prompts with 5, 10, or 15 ICL examples against a held-out evaluation set to select the configuration that maximizes alignment.
- **Core assumption:** LLMs are sensitive to prompt wording, and this sensitivity acts as a confounding variable when comparing annotation quality across models. A systematic optimization procedure can minimize this variance.
- **Evidence anchors:** Abstract states "This approach reduces the variability of manual prompt engineering"; Section 4.2 describes DSPy's role in generating optimal prompts using data models and ICL candidate pools; Related work "LLMs as Span Annotators" (arXiv:2504.08697) provides comparative evidence that LLMs can serve as span annotators.

### Mechanism 2
- **Claim:** LLM-based adjudication of multiple annotations improves final label quality by resolving inter-annotator disagreements through reasoning about conflicting evidence.
- **Mechanism:** The adjudication follows a stacking-inspired ensemble approach: k LLM annotators independently produce annotation sets O₁...Oₖ for input text T. A designated "best-performing" LLM (A1) then receives T plus all annotation sets and produces a final adjudicated set O. This differs from voting or averaging—the adjudicator LLM actively reasons about disagreements rather than applying a fixed aggregation rule.
- **Core assumption:** LLMs can perform meta-reasoning about annotation disagreements and apply annotation guidelines consistently when presented with conflicting labels.
- **Evidence anchors:** Abstract mentions "novel methodology for an LLM to adjudicate multiple labels"; Table 2 shows adjudication improves F1 for Mini and Med models on lap14 ASTE; Table 3 shows consistent ACOS improvements across all scales.

### Mechanism 3
- **Claim:** Annotation quality and inter-annotator agreement improve with model scale, with medium-sized models achieving the best cost-performance tradeoff for fine-grained opinion tasks.
- **Mechanism:** Larger models exhibit better instruction-following, reasoning about implicit aspects/opinions, and domain category recognition. Scale improves both individual annotation accuracy and the reliability of annotations across independent runs.
- **Core assumption:** Model parameter count correlates with the linguistic reasoning capabilities needed for fine-grained opinion analysis, including handling implicit expressions and cross-domain category assignment.
- **Evidence anchors:** Tables 4 and 5 show Med models achieving highest alignment on joint predictions; Table 6 shows α increasing with scale; Section 6 notes "Individual LLM-based opinion annotators generally achieve better alignment with human annotations as their parameter size increases."

## Foundational Learning

- **Concept: Aspect-Based Sentiment Analysis (ABSA) formulations—ASTE and ACOS**
  - Why needed here: The paper evaluates annotation quality on two fine-grained extraction tasks. ASTE extracts triplets (aspect term, sentiment, opinion term). ACOS extracts quadruples (aspect term, aspect category, opinion term, sentiment) and handles implicit cases where aspect or opinion spans are null. Understanding these formulations is essential for interpreting F1 scores and error patterns.
  - Quick check question: Given "The screen is gorgeous," what ASTE triplet would you extract? What if the sentence were "Overpriced and slow"—could you form an ACOS quadruple?

- **Concept: Inter-Annotator Agreement (IAA) and Krippendorff's α**
  - Why needed here: The paper uses Krippendorff's α to assess annotation reliability before adjudication. Unlike F1 (which measures alignment against a gold reference), α measures agreement among independent annotators, indicating whether the annotation task itself is well-defined enough for consistent replication.
  - Quick check question: Why might high F1 against a gold standard coexist with low IAA? What does α > 0.8 indicate versus α ≈ 0.4?

- **Concept: Declarative LLM programming (DSPy paradigm)**
  - Why needed here: The paper's pipeline relies on DSPy for prompt optimization rather than manual prompt engineering. Understanding DSPy's signature-based approach (defining input/output schemas) and its teleprompter optimization is necessary to replicate or extend the annotation pipeline.
  - Quick check question: How does DSPy's approach differ from few-shot prompting with hand-selected examples? What tradeoff does automatic prompt optimization introduce?

## Architecture Onboarding

- **Component map:** Data Model Specification -> ICL Candidates -> Prompt Optimization -> Annotation Generation -> Redundant Annotations -> Ensembled Annotations

- **Critical path:** Data Model → Prompt Optimization (Stages 0-2) determines annotation quality ceiling. If prompts are suboptimal, downstream annotations and adjudication inherit these errors. The adjudicator selection (assigning A1 as adjudicator based on evaluation performance) is the second critical decision point.

- **Design tradeoffs:**
  - Model scale vs. cost: 32B models achieve best results but require A40 GPUs and longer inference times. Mini (4B) models may suffice for simpler domains or when cost constraints dominate.
  - Task complexity vs. reliability: ASTE achieves α ≈ 0.7-0.8 (high reliability); ACOS achieves α ≈ 0.3-0.5 (moderate). Choose ASTE for high-throughput annotation; reserve ACOS for use cases requiring implicit aspect handling.
  - Adjudication benefit vs. risk: Adjudication consistently helps ACOS but harmed Small-scale ASTE (possibly due to non-"thinking" Qwen3-14B model). Verify adjudicator capability matches task difficulty.

- **Failure signatures:**
  - Domain mismatch: ACOS laptop (114 categories) yields F1 ≈ 15-19 even at 32B scale, versus restaurant (12 categories) at F1 ≈ 28-40. High category cardinality signals likely annotation difficulty.
  - Implicit aspect/opinion confusion: Table 7 shows models predicting explicit aspect terms when gold labels mark them implicit. If error analysis reveals this pattern, the task formulation may exceed model capacity.
  - Low IAA despite high individual F1: Indicates task subjectivity exceeds model consistency. Check α < 0.5 before relying on annotations for downstream training.

- **First 3 experiments:**
  1. **Baseline replication:** Run DSPy optimization on res14 ASTE with a single 14B model; compare F1 and α to paper's Small results (61.65 adjudicated F1, 0.78 α for at&s). Validate pipeline correctness before scaling.
  2. **Domain transfer test:** Train prompts on restaurant dev set, evaluate on laptop test set. Measure performance drop to quantify domain sensitivity—expect larger ACOS degradation due to category mismatch.
  3. **Adjudicator ablation:** Compare adjudication using best-performing vs. worst-performing annotator as adjudicator. Test hypothesis that adjudicator quality determines ensemble benefit; expect Small ASTE pattern (adjudication harm) to correlate with adjudicator weakness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does LLM-based adjudication improve annotation alignment for Mini (4B) and Medium (32B) models but reduce alignment for Small (14B) models on ASTE tasks?
- Basis in paper: The authors state: "Still, adjudication surprisingly reduces alignment, likely due to the inclusion of the non-'thinking' Qwen3-14B model."
- Why unresolved: The proposed explanation is tentative; the mechanism by which model "thinking" capability interacts with adjudication effectiveness remains untested.
- What evidence would resolve it: Ablation studies comparing adjudication performance across matched "thinking" vs. "non-thinking" model variants, or systematic analysis of adjudicator behavior on disagreeing annotations from different model types.

### Open Question 2
- Question: How would models larger than 32B parameters perform on fine-grained opinion annotation, particularly for the more challenging ACOS task?
- Basis in paper: The experiments only tested up to 32B models, and ACOS showed only "moderately reliable" IAA scores even at this scale, with the laptop domain (114 categories) proving substantially harder than restaurant (12 categories).
- Why unresolved: Scaling effects on annotation reliability and category prediction accuracy remain unknown beyond the tested range.
- What evidence would resolve it: Experiments with 70B+ parameter models on ACOS tasks, particularly measuring whether Krippendorff's α improves from moderate to high reliability and whether category prediction (ac) alignment increases.

### Open Question 3
- Question: How can the systematic difficulty in identifying implicit aspect terms and opinion spans in ACOS be mitigated?
- Basis in paper: The error analysis (Table 7) shows models fail to recognize implicit aspects, instead incorrectly predicting explicit spans, which "makes ACOS more challenging than the ASTE task."
- Why unresolved: No targeted intervention is proposed; the implicit aspect/opinion problem is identified but not addressed.
- What evidence would resolve it: Studies incorporating explicit training on implicit markers, prompt modifications emphasizing implicit targets, or architectural changes to better handle null span predictions.

## Limitations

- The non-public "openai/gpt-oss-20b" model used for Small/Medium adjudication creates reproducibility barriers
- Domain sensitivity poses significant limitations—ACOS laptop (114 categories) achieves substantially lower F1 than restaurant (12 categories), indicating high category cardinality creates annotation ceilings
- The systematic difficulty in identifying implicit aspect terms and opinion spans in ACOS remains unresolved, with models incorrectly predicting explicit spans when gold labels mark them implicit

## Confidence

**High Confidence** - The scaling trend showing improved annotation quality with model size is well-supported by Tables 4-5 and corroborated by related work on LLM span annotation.

**Medium Confidence** - The LLM adjudication mechanism's effectiveness depends heavily on adjudicator quality, as shown by Small ASTE results. The inter-annotator agreement improvements with scale (Table 6) are convincing for ASTE but more modest for ACOS.

**Low Confidence** - The reproducibility of results is questionable due to the non-public "openai/gpt-oss-20b" model and unspecified DSPy module definitions. The 114-category laptop ACOS dataset may be too complex for even 32B models to annotate reliably.

## Next Checks

1. **Adjudicator Ablation Study**: Compare adjudication results using best-performing vs. worst-performing annotators as adjudicators. Test whether adjudicator quality explains the Small ASTE adjudication failure.

2. **Domain Transfer Experiment**: Train DSPy prompts on restaurant dev set, evaluate on laptop test set. Measure performance degradation to quantify domain sensitivity, particularly for ACOS's 114 categories.

3. **Implicit Case Analysis**: Perform element-wise F1 breakdown for aspect terms, opinion terms, and categories on ACOS. Focus on implicit aspect/opinion cases to determine if models systematically confuse explicit/implicit spans.