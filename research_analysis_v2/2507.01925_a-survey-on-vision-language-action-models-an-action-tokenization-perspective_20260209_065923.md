---
ver: rpa2
title: 'A Survey on Vision-Language-Action Models: An Action Tokenization Perspective'
arxiv_id: '2507.01925'
source_url: https://arxiv.org/abs/2507.01925
tags:
- action
- data
- language
- learning
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This survey provides the first systematic overview of Vision-Language-Action\
  \ (VLA) models through the lens of action tokenization. It identifies eight primary\
  \ types of action tokens\u2014language description, code, affordance, trajectory,\
  \ goal state, latent representation, raw action, and reasoning\u2014and analyzes\
  \ how each shapes model design, training strategies, and task applicability."
---

# A Survey on Vision-Language-Action Models: An Action Tokenization Perspective

## Quick Facts
- **arXiv ID**: 2507.01925
- **Source URL**: https://arxiv.org/abs/2507.01925
- **Reference count**: 40
- **Primary result**: First systematic survey of VLA models through action tokenization lens, identifying 8 token types and proposing hierarchical architecture for future systems

## Executive Summary
This survey provides the first systematic overview of Vision-Language-Action (VLA) models through the lens of action tokenization. The authors identify eight primary types of action tokens—language description, code, affordance, trajectory, goal state, latent representation, raw action, and reasoning—and analyze how each shapes model design, training strategies, and task applicability. They propose that future VLA systems will likely adopt a hierarchical architecture combining language plans and code at the top layer with 3D affordances, trajectories, and goal states in the middle, and raw actions at the base. The work identifies key challenges including data scarcity, hardware limitations, and the need for stronger safety and alignment, while offering actionable insights for advancing embodied intelligence toward general-purpose agents.

## Method Summary
The survey conducts a comprehensive literature review of 40 papers on VLA models, organizing them by eight action token types. The methodology involves extracting key architectural patterns, training data sources, and embodiment types from each paper to build a unified framework. Rather than proposing a new computational method, the authors develop a taxonomic system for understanding how different action token choices influence VLA model design and performance. The framework is validated through analysis of existing architectures and proposals for future directions, with particular emphasis on hierarchical decomposition and the evolution from linear VLA models to more complex agent architectures.

## Key Results
- Identifies eight distinct action token types that serve as intermediate representations in VLA pipelines
- Proposes hierarchical architecture combining language plans/code (top), affordances/trajectories/goal states (middle), and raw actions (base)
- Demonstrates how action tokenization choices affect data requirements, generalization capabilities, and hardware constraints
- Highlights critical challenges including data scarcity, real-time control limitations, and safety concerns for embodied systems

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Decomposition via Action Token Chains
VLA models manage task complexity by decomposing the monolithic perception-to-action mapping into a chain of distinct modules, each generating a specific type of "action token" that serves as input for the next stage. The system processes vision and language inputs iteratively, with high-level modules generating abstract tokens (e.g., Language Plans or Code) to handle logic and semantics, mid-level modules producing spatially grounded tokens (e.g., Affordances, Trajectories, or Goal States), and low-level modules converting these into Raw Actions. This decomposition addresses the data scarcity and high dimensionality challenges of end-to-end learning.

### Mechanism 2: Generalization via Embodiment-Agnostic Representations
Using intermediate action tokens decoupled from specific robot kinematics (such as affordances or video trajectories) improves cross-embodiment generalization compared to predicting raw actions directly. Representations like keypoints or visual trajectories describe what to do in world space rather than how specific robot motors should move, allowing a single high-level policy to transfer knowledge across different robots by swapping the final, low-level decoder. This abstraction enables semantic and spatial requirements to be shared across embodiments while keeping motor control embodiment-specific.

### Mechanism 3: Data Scaling via Action-Free Pretraining
VLA performance scales by utilizing action tokens (like latent representations or goal states) that can be extracted from abundant "action-free" data sources such as internet videos. Instead of relying solely on scarce robot teleoperation data, models are pretrained on human videos using proxy tasks like predicting visual trajectories of human hands or latent representations of video clips. This builds a robust world model and policy prior before fine-tuning on robot-specific data, leveraging the transferability of visual dynamics and semantic logic from human manipulation videos to robotic manipulation.

## Foundational Learning

- **Concept: Action Tokenization**
  - Why needed here: The paper's central thesis is that the definition of the "output" of a VLA module (the action token) dictates the model's architecture and data needs. Understanding the difference between a *Language Description* ("pick cup") and a *Raw Action* (joint angle delta) is prerequisite to understanding the survey's taxonomy.
  - Quick check question: Can you distinguish between a "Goal State" token (an image of the desired future) and a "Trajectory" token (a sequence of points defining movement)?

- **Concept: The "Bitter Lesson" in Embodied AI**
  - Why needed here: The paper frequently contrasts manually engineered constraints (like code APIs) with scalable, end-to-end learning (Raw Actions). It posits that while intermediate tokens (hierarchy) help now, the long-term trend favors massive scaling of data and models (Raw Actions), a core theme in Section 2.1 and 10.8.
  - Quick check question: Why might a model predicting "Raw Actions" be preferred in the long run over one generating "Code," despite the current brittleness of the former?

- **Concept: World Models (Video Prediction)**
  - Why needed here: Sections 8 and 9 rely heavily on the ability of generative models to predict future visual states. "Goal State" tokens are essentially outputs of a world model. Without understanding video diffusion or VQ-VAEs, the mechanism of these tokens is opaque.
  - Quick check question: How does a "Goal Image" token differ from a standard text instruction in terms of the spatial information it provides to the low-level controller?

## Architecture Onboarding

- **Component map:** Vision + Language -> VLA Module 1 (Planner) -> Language Plan/Code -> VLA Module 2 (Grounding) -> Affordance/Goal State -> VLA Module 3 (Policy) -> Raw Action

- **Critical path:** The transition from Language/Code -> Affordance/Goal State -> Raw Action represents the most critical path, as failure often occurs at the translation boundary between high-level semantics and low-level geometry.

- **Design tradeoffs:**
  - **Interpretability vs. Scalability:** Code is highly interpretable and verifiable but brittle (relies on predefined APIs). Raw Action is scalable but acts as a "black box."
  - **Data Efficiency vs. Precision:** Language Plans allow sharing data across tasks but lack spatial precision. Trajectories are precise but require dense tracking data.

- **Failure signatures:**
  - **Language Plan:** "Hallucinated" plans where the LLM ignores visual constraints
  - **Code:** "Symbol grounding problem" where APIs fail due to environmental state mismatches
  - **Goal State:** "Overspecification" where the generated image includes irrelevant details that confuse the policy
  - **Raw Action:** "Catastrophic forgetting" of general visual knowledge during fine-tuning

- **First 3 experiments:**
  1. **Token Comparison:** Train two simple policies for a "pick-and-place" task. Condition one on a *Language Instruction* and the other on a *Goal Image*. Compare data efficiency and spatial precision.
  2. **Hierarchy Test:** Implement a minimal "Language-to-Affordance" pipeline. Use a VLM to identify a bounding box (Affordance) for a target object, and feed this crop into a pre-trained ACT policy for grasping. Verify if this improves robustness against distractors compared to a non-hierarchical baseline.
  3. **Latency Audit:** Measure the inference time of a "Reasoning" token module vs. a "Raw Action" module. Calculate the maximum control frequency achievable to determine suitability for real-time control.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can latent action representations achieve the granularity and task-centric alignment required for reliable hierarchical VLA architectures?
- Basis in paper: Section 13.1 notes latent representations are currently excluded from the proposed architecture due to training challenges regarding "granularity, comprehensiveness, and alignment."
- Why unresolved: Current methods suffer from inadequate reconstruction fidelity and often encode task-irrelevant information.
- Evidence would resolve it: A latent model matching explicit token fidelity in dexterous tasks without encoding noise.

### Open Question 2
- Question: How can efficient reinforcement learning (RL) algorithms be adapted for VLA models given real-world constraints like high reset costs?
- Basis in paper: Section 13.3 argues RL is needed to surpass imitation learning but states "real-world deployment requires more efficient RL algorithms."
- Why unresolved: Physical interaction for RL is dangerous and inefficient compared to simulation or digital data.
- Evidence would resolve it: An RL framework optimizing VLA success rates with significantly fewer real-world interactions.

### Open Question 3
- Question: What architectural shifts are required to evolve linear VLA models into proactive agents with memory and reflection?
- Basis in paper: Section 13.2 advocates evolving "from VLA models to VLA agents," requiring a transition to "complex, bidirectional, and graph-structured topologies."
- Why unresolved: Current linear processing lacks mechanisms for history and planning needed for long-horizon tasks.
- Evidence would resolve it: An agent architecture integrating memory modules that outperforms linear models on long-horizon benchmarks.

## Limitations

- Classification ambiguity for hybrid systems that blend multiple token types, with boundary conditions not formally defined
- 40-paper sample may underrepresent emerging architectures that don't fit cleanly into the eight categories
- Hardware constraints section relies on theoretical projections rather than empirical measurements of real-time inference bottlenecks

## Confidence

- **High**: Hierarchical decomposition as core architectural pattern (supported by explicit proposals in Sections 13.1 and 13.2, corroborated by neighbor paper)
- **Medium**: Generalization benefits of embodiment-agnostic representations (supported by UniAct and affordance sections, but limited by lack of cross-embodiment benchmarking data)
- **Low**: Data scaling projections via action-free pretraining (Section 12 "Data Pyramid" is conceptual; neighbor papers show mixed results on human-to-robot transfer)

## Next Checks

1. Test the taxonomy's boundaries by classifying a recent VLA paper (e.g., *VLA-Pruner*) that wasn't in the original 40-paper sample—does it require new token categories?
2. Replicate the latency audit (Section 13.2) on an actual VLA pipeline (e.g., Reasoning vs Raw Action modules) to verify claimed control frequency limitations
3. Conduct a cross-embodiment transfer study using the proposed affordance token (keypoints/boxes) to validate generalization claims beyond the survey's theoretical analysis