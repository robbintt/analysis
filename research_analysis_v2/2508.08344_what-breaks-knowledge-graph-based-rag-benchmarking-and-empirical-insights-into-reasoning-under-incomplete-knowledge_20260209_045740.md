---
ver: rpa2
title: What Breaks Knowledge Graph based RAG? Benchmarking and Empirical Insights
  into Reasoning under Incomplete Knowledge
arxiv_id: '2508.08344'
source_url: https://arxiv.org/abs/2508.08344
tags:
- answer
- knowledge
- reasoning
- rule
- entity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces BRINK, a benchmark designed to systematically
  evaluate KG-RAG methods under knowledge incompleteness. It addresses the problem
  that existing benchmarks include questions directly answerable from KGs, making
  it unclear whether models perform reasoning or simple retrieval.
---

# What Breaks Knowledge Graph based RAG? Benchmarking and Empirical Insights into Reasoning under Incomplete Knowledge

## Quick Facts
- arXiv ID: 2508.08344
- Source URL: https://arxiv.org/abs/2508.08344
- Reference count: 28
- Primary result: KG-RAG methods struggle with reasoning under missing knowledge, with training-based methods showing better robustness

## Executive Summary
This work introduces BRINK, a benchmark designed to systematically evaluate KG-RAG methods under knowledge incompleteness. It addresses the problem that existing benchmarks include questions directly answerable from KGs, making it unclear whether models perform reasoning or simple retrieval. The core method involves mining logical rules from KGs, removing triples that are inferable via these rules, and generating questions based on the removed triples. Empirical results show that current KG-RAG methods struggle with reasoning under missing knowledge, with training-based methods showing better robustness. Performance significantly drops when direct evidence is absent, highlighting limited reasoning capabilities. Textual entity labels substantially boost performance, suggesting models rely on internal memorization rather than structured reasoning.

## Method Summary
The BRINK benchmark systematically evaluates KG-RAG methods by creating incomplete knowledge graphs through logical rule mining. Horn rules are mined from KGs using AMIE3 with specific confidence thresholds (0.3 confidence, 0.4 PCA confidence). For each rule, groundings are extracted and head triples are removed while preserving body triples. Questions are generated via GPT-4 using a template that describes the KG structure and asks for the removed entity. The dataset is balanced through frequency-based downsampling with threshold τ, and split into 8:1:1 train/val/test sets. Three KG datasets are used: Family (17,615 triples), FB15k-237 (204,087 triples), and Wikidata5m (20.5M triples).

## Key Results
- KG-RAG methods struggle with reasoning under missing knowledge, with training-based methods showing better robustness
- Performance significantly drops when direct evidence is absent, highlighting limited reasoning capabilities
- Textual entity labels substantially boost performance, suggesting models rely on internal memorization rather than structured reasoning

## Why This Works (Mechanism)
None

## Foundational Learning
1. **Horn rule mining with AMIE3**: Extracting logical rules from KGs using confidence and head coverage thresholds to identify inferential relationships
   - Why needed: To systematically create knowledge incompleteness by removing inferable triples
   - Quick check: Verify mined rules have confidence ≥0.3 and head coverage ≥0.1

2. **Knowledge graph grounding**: Mapping logical rules to specific triples in the KG through variable substitution
   - Why needed: To identify which specific triples can be removed while preserving inferential structure
   - Quick check: Ensure all body atoms remain in KG after head triple removal

3. **Question generation with GPT-4**: Using LLM to create natural language questions from KG structures and removed triples
   - Why needed: To create realistic evaluation questions that test reasoning under incompleteness
   - Quick check: Verify generated questions follow the template and ask about removed triples

4. **Frequency-based downsampling**: Balancing the dataset by removing over-represented rule types
   - Why needed: To prevent models from overfitting to frequent reasoning patterns
   - Quick check: Confirm final dataset has balanced distribution across rule types

## Architecture Onboarding
- **Component map**: AMIE3 mining -> Rule grounding -> Triple removal -> GPT-4 question generation -> Dataset balancing -> Model evaluation
- **Critical path**: Rule mining → Triple removal → Question generation → Model inference → Metric computation
- **Design tradeoffs**: Rule-based incompleteness vs. random removal; text labels vs. anonymized IDs; frequency balancing vs. rule type coverage
- **Failure signatures**: Retrieval failures account for ~70% of errors; models overfit to frequent patterns; text labels enable memorization over reasoning
- **3 first experiments**:
  1. Test AMIE3 mining with different confidence thresholds (0.3 vs 0.4) and measure rule quality
  2. Verify triple removal preserves all body atoms for each grounding
  3. Compare model performance with text labels vs. anonymized numeric IDs

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can retrieval strategies be designed to explicitly identify and prioritize alternative reasoning paths when direct evidence is absent, rather than relying on heuristic path exploration?
- Basis in paper: [explicit] The authors state: "developing retrieval strategies that identify alternative paths when evidence is missing" as a key research direction.
- Why unresolved: Current retrievers fail to locate relevant facts when supporting triples are removed (e.g., missing paths like contains→source in the New York City example), with retrieval failures accounting for ~70% of errors.
- What evidence would resolve it: A retrieval module that, given the rule schema, systematically searches for body atom groundings rather than relying on semantic similarity or random walk heuristics.

### Open Question 2
- Question: How can training-based KG-RAG methods maintain robustness to incompleteness without overfitting to specific relational patterns observed during training?
- Basis in paper: [explicit] The authors note: "designing reasoning modules with stronger generalization to avoid overfitting to specific relation patterns" as a research direction.
- Why unresolved: RoG underperforms PoG on symmetric relations under incomplete KGs, suggesting training-based methods may overfit to patterns seen during training at the cost of simpler logical structures.
- What evidence would resolve it: Ablation studies showing consistent performance across held-out rule types, or regularization techniques that improve symmetry handling without degrading composition reasoning.

### Open Question 3
- Question: Can incorporating more expressive rule types (e.g., negation, disjunction, existential quantifiers) beyond Horn rules yield a more comprehensive evaluation of KG-RAG reasoning capabilities?
- Basis in paper: [explicit] The authors explicitly state: "Extending to richer rule types is a promising direction for future work, specifically, incorporating advanced rule-learning methods like DRUM and AnyBURL."
- Why unresolved: The current benchmark is restricted to Horn rules, which may not capture all real-world reasoning patterns; more expressive rules are challenging to mine reliably.
- What evidence would resolve it: A benchmark extension using DRUM or AnyBURL that produces valid, plausible rules, with empirical evaluation showing whether models struggle on these richer patterns.

### Open Question 4
- Question: To what extent do KG-RAG models perform genuine symbolic reasoning versus leveraging parametric knowledge when textual entity labels are provided?
- Basis in paper: [inferred] The dramatic performance boost from text labels (vs. random/official IDs) suggests models rely on internal memorization, but the exact mechanism—whether retrieval, generation, or both—is unclear.
- Why unresolved: The paper shows text labels boost F1 significantly but doesn't isolate whether improvements stem from better retrieval grounding, LLM prior knowledge activation, or both.
- What evidence would resolve it: Probing experiments with entities absent from LLM pre-training data, or controlled studies replacing text labels with synthetic but semantically meaningful identifiers.

## Limitations
- The specific τ value used for frequency-based downsampling (mentioned as tested between 0.01-0.2) significantly impacts dataset composition
- The relationship between AMIE3's confidence thresholds and their impact on rule quality is not fully characterized
- The extent to which models' performance gains from text labels represent memorization versus genuine reasoning remains unclear

## Confidence
- High confidence: KG-RAG methods struggle with reasoning under missing knowledge; training-based methods show better robustness
- Medium confidence: Performance drop without direct evidence demonstrates limited reasoning; text labels substantially boost performance
- Low confidence: The specific numerical thresholds for τ and their impact on results

## Next Checks
1. Reproduce the downsampling process using the exact τ value and verify dataset balance statistics
2. Test models on the anonymized Private ID setting to isolate true reasoning ability from text-based memorization
3. Compare AMIE3 rule mining results using different confidence thresholds to assess sensitivity of BRINK's construction