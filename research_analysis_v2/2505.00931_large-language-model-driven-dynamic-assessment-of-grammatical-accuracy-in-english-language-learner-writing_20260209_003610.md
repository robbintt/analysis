---
ver: rpa2
title: Large Language Model-Driven Dynamic Assessment of Grammatical Accuracy in English
  Language Learner Writing
arxiv_id: '2505.00931'
source_url: https://arxiv.org/abs/2505.00931
tags:
- feedback
- language
- dynamic
- assessment
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated the use of large language models (LLMs) to
  provide dynamic assessment of grammatical accuracy in English learner writing. DynaWrite,
  a modular, microservices-based tutoring application, was developed to support multiple
  LLMs in generating dynamic feedback.
---

# Large Language Model-Driven Dynamic Assessment of Grammatical Accuracy in English Language Learner Writing

## Quick Facts
- arXiv ID: 2505.00931
- Source URL: https://arxiv.org/abs/2505.00931
- Authors: Timur Jaganov; John Blake; Julián Villegas; Nicholas Carr
- Reference count: 40
- DynaWrite system achieves F1 scores around 0.74 for grammatical error detection using LLMs

## Executive Summary
This study evaluates large language models for dynamic assessment of grammatical accuracy in English learner writing. DynaWrite, a modular microservices-based tutoring application, was developed to support multiple LLMs in generating graduated feedback. Testing of 21 models identified GPT-4o and neural chat as most suitable, with GPT-4o providing higher quality feedback through clear, consistent, and progressively explicit hints. The system demonstrated sufficient real-time responsiveness and stability for practical deployment.

## Method Summary
The DynaWrite system employs a microservices architecture with Apache Kafka for asynchronous communication between components. The system processes learner sentences through REST API requests, LLM inference via OpenAI_Consumer or Ollama_Consumer microservices, and delivers graduated feedback through a four-level hierarchy. Testing involved 21 different LLMs on 200 sentences from 11 learners, with evaluation metrics including F1 score, precision, recall, and feedback quality assessment across three criteria: consistency, gradation, and resolution.

## Key Results
- GPT-4o and neural chat achieved F1 scores of approximately 0.74 for grammatical error detection
- GPT-4o provided usable feedback for 68% of true positive detections compared to 15% for neural chat
- Response times remained within acceptable thresholds for real-time applications across all test conditions

## Why This Works (Mechanism)

### Mechanism 1
LLMs can approximate teacher-like graduated feedback for grammatical error correction when explicitly prompted with a hierarchical hint structure. The system implements a four-level feedback taxonomy (implicit hint → probing question → error location → explicit correction) where each revision attempt triggers progressively more explicit guidance. GPT-4o successfully produced usable feedback meeting all three quality criteria (consistency, gradation, resolution) for 68% of true positive detections.

### Mechanism 2
Microservices architecture with asynchronous message passing enables real-time, scalable LLM-driven assessment that traditional monolithic systems cannot achieve. Apache Kafka decouples user requests from LLM processing through well-defined topics (nlp_request_text, nlp_response_text, cent_push_message). This allows horizontal scaling, model hot-swapping, and fault isolation without disrupting active user sessions.

### Mechanism 3
False positive tolerance can be mitigated through feedback quality evaluation rather than pure detection accuracy. When models incorrectly flag grammatically correct text, usable feedback that improves sentence quality (38/46 false positives for GPT-4o) may partially compensate for detection errors. The system treats false negatives as pedagogically acceptable since teachers routinely prioritize which errors to address.

## Foundational Learning

- Concept: **Dynamic Assessment (DA)**
  - Why needed here: This is the core pedagogical paradigm shift from static assessment (what learners can do independently) to mediated assessment (what learners can achieve with graduated hints). Understanding this explains why the system uses four feedback levels rather than direct error correction.
  - Quick check question: Can you explain why DA provides diagnostic information about "functions in the process of maturing" that conventional assessment cannot capture?

- Concept: **Sociocultural Theory (SCT) and Mediation**
  - Why needed here: SCT provides the theoretical justification for external mediation (teacher hints, LLM feedback) as a tool for cognitive development. This explains why the system prioritizes graduated hints over immediate corrections.
  - Quick check question: How does the concept of "symbolic tools mediating mental functions" relate to LLMs providing feedback on learner writing?

- Concept: **Microservices with Event-Driven Communication**
  - Why needed here: Understanding Apache Kafka's role as a message broker is essential for debugging latency issues, adding new LLM backends, or scaling the system. The pub/sub pattern decouples request handling from LLM inference.
  - Quick check question: What happens to in-flight user requests if the OpenAI_Consumer microservice crashes? How does Kafka help with recovery?

## Architecture Onboarding

- Component map: User Frontend → Nginx → REST_API → Kafka topics (nlp_request_text, nlp_response_text, cent_push_message) → OpenAI_Consumer/Ollama_Consumer → DynaWrite_Consumer → PostgreSQL → Centrifugo → User Frontend

- Critical path: User submits sentence → REST_API publishes to nlp_request_text → OpenAI_Consumer processes → DynaWrite_Consumer stores result → CPush_Consumer triggers Centrifugo → Frontend receives hint. Monitor response time at each hop.

- Design tradeoffs:
  - GPT-4o (higher feedback quality, paid API) vs. neural chat (lower quality, locally deployable)
  - Kafka complexity vs. simpler REST-based synchronous calls (chose async for scalability)
  - Four-level feedback vs. two-level (chose finer gradation to better approximate human DA)

- Failure signatures:
  - High response time variance: Check Kafka consumer lag, LLM API rate limits
  - Inconsistent feedback across levels: Prompt engineering issue, consider model temperature settings
  - False positive flood: Review prompt instructions for error detection threshold

- First 3 experiments:
  1. Replicate the 200-sentence benchmark with your chosen LLM to establish baseline F1, precision, recall before deployment
  2. Load test with 10 concurrent users submitting sentences to measure Kafka queue depth and response time degradation
  3. A/B test feedback level progression (provide all 4 levels vs. skip to level 3) to validate the graduated hint assumption with real learners

## Open Questions the Paper Calls Out

### Open Question 1
Do domain-specific LLMs tailored for specialized tasks, such as exam preparation, outperform general-purpose models like GPT-4o within the DynaWrite environment?
- Basis in paper: The Future Work section states that "future iterations will support a broader spectrum of models, including domain-specific models tailored to specialized tasks such as exam preparation."
- Why unresolved: The current study limited its evaluation to general-purpose models and did not assess models optimized for specific domains.
- What evidence would resolve it: Comparative benchmarking results of domain-specific LLMs against GPT-4o regarding accuracy and feedback appropriacy in specialized contexts.

### Open Question 2
How do structured usability studies with learners and educators inform the refinement of DynaWrite’s interface and the clarity of its dynamic hints?
- Basis in paper: The authors note limited attention to user experience research and state, "Usability and user experience studies will be conducted to inform future interface and functionality refinements."
- Why unresolved: Current validation focused primarily on technical functionality and model performance rather than detailed user interaction flows.
- What evidence would resolve it: Results from usability studies identifying specific friction points for non-technical users and subsequent improvements in hint clarity.

### Open Question 3
Can DynaWrite maintain real-time responsiveness and accuracy when deployed offline on resource-constrained mobile or edge devices?
- Basis in paper: The Limitations section identifies hardware dependency as a barrier, and Future Work proposes "optimizing local deployment for resource-constrained devices and enabling offline functionality."
- Why unresolved: The current system relies on high-performance hardware (e.g., MacBook Pro M2) or paid APIs, which limits accessibility.
- What evidence would resolve it: Performance benchmarks showing acceptable latency and error detection rates on low-resource devices running locally.

## Limitations

- Evaluation corpus of 200 sentences from 11 learners may not capture full diversity of learner errors and proficiency levels
- Feedback quality assessment relied on researcher judgment rather than learner outcomes, leaving pedagogical effectiveness question unanswered
- Four-level feedback hierarchy was predetermined rather than empirically derived from learner response patterns

## Confidence

- **High Confidence**: The architectural design enabling real-time, scalable LLM integration through Kafka messaging (Section IV-C results show stable response times under test conditions)
- **Medium Confidence**: GPT-4o's superior feedback quality compared to neural chat (supported by quantitative metrics but based on limited sentence samples)
- **Low Confidence**: The pedagogical benefit of graduated hints over immediate correction (mechanism assumes but does not empirically validate that learners benefit from progressive feedback levels)

## Next Checks

1. Conduct a controlled study with 50+ learners comparing DynaWrite's graduated hints against immediate correction and no-feedback control conditions, measuring error reduction and learner confidence after 2 weeks of use

2. Expand error detection evaluation to 1,000+ sentences across CEFR levels A1-C1 to establish model performance variance by proficiency level and error type

3. Implement a user feedback loop where learners can rate hint usefulness and request level changes, then use this data to optimize the feedback progression algorithm through reinforcement learning