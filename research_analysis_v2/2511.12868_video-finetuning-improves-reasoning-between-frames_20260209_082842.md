---
ver: rpa2
title: Video Finetuning Improves Reasoning Between Frames
arxiv_id: '2511.12868'
source_url: https://arxiv.org/abs/2511.12868
tags:
- video
- reasoning
- visual
- vcot
- llav
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether video fine-tuning enhances multimodal
  LLMs' ability to reason between frames. The authors introduce Visual Chain-of-Thought
  (vCoT), which generates explicit transitional captions between consecutive video
  frames.
---

# Video Finetuning Improves Reasoning Between Frames

## Quick Facts
- arXiv ID: 2511.12868
- Source URL: https://arxiv.org/abs/2511.12868
- Authors: Ruiqi Yang; Tian Yun; Zihan Wang; Ellie Pavlick
- Reference count: 10
- Primary result: Video fine-tuning induces implicit temporal reasoning that transfers to static relational reasoning tasks

## Executive Summary
This paper investigates whether video fine-tuning enhances multimodal LLMs' ability to reason between frames. The authors introduce Visual Chain-of-Thought (vCoT), which generates explicit transitional captions between consecutive video frames. Comparing image-only and video-finetuned models on long-form video QA (EgoSchema), they find vCoT significantly improves image models (+7.4% accuracy) but yields minimal gains for video models (+1.6%), suggesting video models already capture frame transitions implicitly. Further experiments show video models are more robust to textual noise and transfer temporal reasoning to static image tasks like I-RAVEN relational reasoning (+1.1% to +3.0% accuracy gains). The results demonstrate that video fine-tuning imparts implicit temporal reasoning capabilities that extend beyond the video domain.

## Method Summary
The study compares image-only and video-finetuned multimodal models (LLaVA-NeXT and InternVL variants) on long-form video question answering. Visual Chain-of-Thought (vCoT) is introduced as a two-step process: first identifying common visual attributes between consecutive frames, then inferring bridging events. The models are evaluated on EgoSchema (500 videos, 3 minutes each) and I-RAVEN relational reasoning tasks. Multiple-choice accuracy is measured using VLMEvalKit protocol with Qwen-2.5-7B-CHAT as judge. The critical comparison examines vCoT's marginal benefits on image versus video models to determine whether video fine-tuning induces implicit temporal reasoning.

## Key Results
- vCoT significantly improves image-only models on long-form video QA (+7.4% accuracy) but yields minimal gains for video-finetuned models (+1.6%)
- Video models show greater robustness to textual noise compared to image models, suggesting stronger visual modality reliance
- Video fine-tuning transfers temporal reasoning to static relational reasoning tasks, improving I-RAVEN performance by 1.1% to 3.0%
- The implicit temporal reasoning in video models reduces their need for explicit transitional scaffolding

## Why This Works (Mechanism)

### Mechanism 1
Video fine-tuning induces implicit inter-frame reasoning that reduces reliance on explicit transitional cues. Exposure to sequential video frames during training causes the model to internalize temporal transition patterns, allowing it to infer intermediate events without explicit textual scaffolding. Core assumption: The marginal vCoT gains for video models (+1.6%) versus image models (+7.4%) reflect learned temporal representations rather than architectural differences. Break condition: If additional training data scale drives the effect, implicit temporal reasoning attribution weakens.

### Mechanism 2
Temporal reasoning learned from video transfers to static relational reasoning tasks. Video fine-tuning strengthens inductive biases for relational structure detection—spatial layouts, relative positioning, object interactions—which generalize to non-temporal abstract reasoning. Core assumption: The I-RAVEN improvements reflect genuine reasoning transfer, not spurious correlations or better base visual features. Break condition: If video models have stronger base vision encoders rather than transferable reasoning, the mechanism is compromised.

### Mechanism 3
Video fine-tuning increases visual modality reliance and robustness to textual noise. Training on video sequences, where temporal continuity provides strong visual priors, shifts model attention toward visual evidence and reduces dependence on textual shortcuts. Core assumption: Text-shuffle degradation patterns reflect genuine modality preferences rather than training distribution artifacts. Break condition: If benchmark questions contain linguistic shortcuts, robustness differences may reflect shortcut exploitation patterns rather than genuine visual grounding.

## Foundational Learning

- **Concept: Chain-of-Thought (CoT) Prompting**
  - Why needed here: vCoT extends text-based CoT to visual sequences, requiring understanding of intermediate reasoning steps
  - Quick check question: Can you explain how prompting a model to "think step-by-step" differs from direct answer prompting?

- **Concept: Multimodal Tokenization and Frame Concatenation**
  - Why needed here: The paper critiques naive frame concatenation; understanding token-level video representation is essential
  - Quick check question: How do video LLMs typically convert video frames into sequences processable by language models?

- **Concept: Transfer Learning and Inductive Biases**
  - Why needed here: Core claim involves temporal reasoning transferring across domains; requires distinguishing task-specific learning from generalizable representations
  - Quick check question: What makes a learned representation "transferable" versus overfit to a specific task distribution?

## Architecture Onboarding

- **Component map:** Vision encoder -> Language backbone -> Cross-modal projector -> Temporal positional encoding (video variants only) -> vCoT generation module -> Qwen-2.5 rephrasing layer
- **Critical path:** 1) Frame extraction from video (#F = 5 or 10 frames) 2) For vCoT: Generate transitional captions between consecutive pairs 3) Interleave frames with text infills 4) Append task question 5) Model prediction
- **Design tradeoffs:** Dense sampling (#F=10) vs. sparse (#F=5): Dense improves base accuracy but vCoT gains vary inconsistently; vCoT inference cost: Two forward passes per frame pair increases latency substantially; Caption vs. event description: vCoT (+3.2 to +3.6 over captions) provides stronger guidance than static descriptions
- **Failure signatures:** vCoT performance degradation after LoRA fine-tuning on image data (–0.8% vs. +1.6% for video model); Negative vCoT gains for InternVL-Video at #F=5 (–2.2%), suggesting potential interference with implicit reasoning; Video model underperforming image model in some LLaVA configurations (LLaVA-NeXT 49.2% vs. LLaVA-NeXT-Video 49.0%)
- **First 3 experiments:** 1) Replicate the controlled comparison: Run image vs. video variants on EgoSchema subset with/without vCoT to validate the +7.4% vs. +1.6% gap 2) Ablate data scale: Implement LoRA fine-tuning on image data matching video training volume (100k samples) to isolate video-specific effects from scale effects 3) Test transfer on held-out relational task: Evaluate I-RAVEN sub-categories (especially spatial/directional rules) to confirm video-to-static transfer holds with controlled random seeds

## Open Questions the Paper Calls Out

### Open Question 1
Is the emergence of implicit temporal reasoning in video models driven by the video data itself, or is it a byproduct of the increased scale of training data? Basis: Appendix A.3 shows that when image models are LoRA-tuned on a data volume matching video models, the performance gap closes. Why unresolved: The primary experiments compare image models (trained on less data) against video models (trained on more data). What evidence would resolve it: A controlled study where image-only and video models are trained on identical token counts of image versus video data.

### Open Question 2
Which specific architectural inductive biases (e.g., temporal positional encodings) are strictly necessary for models to develop implicit inter-frame reasoning? Basis: Appendix A.4 cites the inability to train control models from scratch as a limitation. Why unresolved: The study relies on existing model pairs which differ in both training data and architectural configurations. What evidence would resolve it: Training models from scratch with ablations on specific temporal architectural components while holding training data constant.

### Open Question 3
Does video finetuning improve static relational reasoning by enhancing general spatial feature extraction or by instilling a specific "sequence-aware" logical structure? Basis: Section 3.3 observes that video models outperform image models on the static I-RAVEN benchmark. Why unresolved: While the results demonstrate transfer, the paper does not determine if the improvement stems from better spatial perception or abstract relational logic. What evidence would resolve it: A diagnostic evaluation separating pure spatial reasoning tasks from those requiring abstract rule extrapolation.

## Limitations
- Data scale confound: Video models receive 100k training samples while image models lack this additional exposure, making it difficult to isolate video-specific effects
- Benchmark shortcut exploitation: Text-shuffle robustness findings are vulnerable to linguistic shortcuts in benchmark questions that the authors acknowledge but do not systematically control for
- Vision encoder differences: The video-to-static transfer mechanism lacks direct corpus validation, and vision encoder differences between variants could explain I-RAVEN improvements

## Confidence
- **High confidence**: The controlled comparison showing vCoT benefits image models (+7.4%) more than video models (+1.6%) on EgoSchema is methodologically sound and the results are reproducible
- **Medium confidence**: The claim that video fine-tuning increases visual modality reliance and robustness to textual noise is supported by degradation patterns but vulnerable to benchmark shortcut artifacts
- **Low confidence**: The assertion that video models "transfer" temporal reasoning to static relational reasoning requires stronger causal evidence, as vision encoder differences could explain I-RAVEN improvements

## Next Checks
1. **Isolate Data Scale Effects**: Implement controlled experiments where image models receive matched training volume (100k samples) via LoRA fine-tuning, then re-evaluate vCoT marginal gains to distinguish video-specific from scale-driven effects
2. **Benchmark Shortcut Analysis**: Systematically analyze EgoSchema questions for linguistic shortcuts, then re-run text-shuffle robustness experiments on subsets known to lack shortcuts, isolating genuine visual grounding from shortcut exploitation
3. **Cross-Architecture Transfer Validation**: Test video-to-static reasoning transfer on additional static relational reasoning benchmarks (e.g., RAVEN, PGSS) using models with matched vision encoders but different temporal fine-tuning strategies to confirm the effect is not architecture-specific