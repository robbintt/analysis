---
ver: rpa2
title: 'Efficient Multimodal Large Language Models: A Survey'
arxiv_id: '2405.10739'
source_url: https://arxiv.org/abs/2405.10739
tags:
- arxiv
- vision
- efficient
- language
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey comprehensively reviews efficient multimodal large
  language models (MLLMs), addressing the challenges of computational cost and resource
  requirements that hinder their widespread adoption. The study systematically categorizes
  research advancements into six key areas: architecture, efficient vision, efficient
  LLMs, training methodologies, data and benchmarks, and applications.'
---

# Efficient Multimodal Large Language Models: A Survey

## Quick Facts
- **arXiv ID:** 2405.10739
- **Source URL:** https://arxiv.org/abs/2405.10739
- **Reference count:** 40
- **Key outcome:** Comprehensive survey of efficient MLLMs addressing computational bottlenecks through vision token compression, lightweight architectures, and parameter-efficient fine-tuning.

## Executive Summary
This survey systematically reviews advancements in efficient multimodal large language models (MLLMs), addressing the critical challenge of reducing computational costs while maintaining performance. The study categorizes research into six key areas: architecture, efficient vision, efficient LLMs, training methodologies, data and benchmarks, and applications. Core methods explored include vision token compression, Mixture-of-Experts, Mamba architectures, and parameter-efficient fine-tuning strategies. The survey demonstrates that efficient MLLMs can match or exceed larger models on various benchmarks while significantly reducing computational requirements, providing valuable insights for deployment in edge computing and real-world applications.

## Method Summary
The survey synthesizes research on efficient MLLMs through systematic categorization of architectural innovations, training methodologies, and evaluation frameworks. It analyzes vision token compression techniques, lightweight language models (SLMs < 3B parameters), and efficient attention mechanisms like Mamba. The two-stage training approach combines pre-training on image-caption pairs with instruction tuning on task-specific VQA data. Parameter-efficient fine-tuning methods like LoRA are highlighted for adapting frozen backbones. The study evaluates models across diverse benchmarks including VQAv2, GQA, and MMBench, emphasizing trade-offs between model size, performance, and computational efficiency.

## Key Results
- Efficient MLLMs using vision token compression and SLMs achieve state-of-the-art performance while reducing parameters to under 3B
- Mixture-of-Experts and Mamba architectures enable linear complexity processing of high-resolution inputs
- Parameter-efficient fine-tuning (LoRA) allows effective adaptation of frozen LLMs to multimodal tasks with minimal additional training
- Token pruning and compression techniques maintain reasoning accuracy while significantly reducing quadratic attention costs

## Why This Works (Mechanism)

### Mechanism 1: Redundant Visual Token Pruning
If visual tokens contain significant redundancy, removing low-information tokens in later layers preserves reasoning accuracy while reducing quadratic attention costs. Visual encoders generate dense patch tokens, but attention mechanisms often assign low weights to visual tokens after initial layers. By adaptively merging or pruning these tokens, the sequence length fed into the LLM is reduced, lowering computational complexity from O(N²) toward O(N). Core assumption: semantic content required for reasoning is concentrated in a subset of visual tokens; later LLM layers function primarily as textual reasoners rather than visual processors.

### Mechanism 2: Mixture-of-Experts (MoE) for Sparse Capacity
Decoupling total model parameters from inference-time compute allows efficient MLLMs to handle complex multimodal tasks without the latency of dense models. MoE architectures replace dense Feed-Forward Networks with expert layers, activating only a subset of experts per token. This increases model capacity to store multimodal knowledge while keeping activated parameters low, ensuring fast inference. Core assumption: multimodal tasks activate distinct sub-domains of knowledge, making sparse routing more efficient than dense computation for the same performance level.

### Mechanism 3: Linear Complexity Sequence Modeling (Mamba)
Replacing the quadratic self-attention mechanism with State Space Models allows efficient processing of high-resolution inputs (long visual sequences). Architectures like Cobra integrate Mamba blocks, which process sequences recurrently or with convolutional kernels (O(N)) instead of comparing every token pair (O(N²)). This is critical for high-resolution images where token counts are high. Core assumption: visual dependencies can be effectively compressed into a recurrent state without explicit pairwise comparison.

## Foundational Learning

- **Visual Tokenization & Quadratic Complexity:** Understanding that images convert to patch tokens explains why O(N²) attention makes high-resolution images expensive. *Quick check:* If you double image resolution (token count), how does self-attention cost change in standard Transformer? (Answer: Quadruples).

- **Modality Projection (Alignment):** Efficient MLLMs often freeze LLM and Vision Encoder, training only a "Projector" (e.g., MLP, Q-Former). Understanding this bottleneck explains why connector design is a major optimization target. *Quick check:* Which component maps 1024-dim ViT output to 2048-dim LLM input space? (Answer: Vision-Language Projector).

- **Parameter-Efficient Fine-Tuning (PEFT):** Full fine-tuning is expensive; methods like LoRA allow LLM adaptation to multimodal tasks by updating low-rank matrices, drastically reducing memory usage. *Quick check:* Why is LoRA preferred over full fine-tuning when adapting 7B LLM to visual instruction data? (Answer: Requires training only tiny fraction of parameters, saving GPU memory).

## Architecture Onboarding

- **Component map:** Image + Text Prompt → Vision Encoder (Frozen) → Projector (Trainable/Efficient) → LLM Backbone (SLM/MoE/Mamba) → Text Response

- **Critical path:** Input Resolution → Vision Encoder (FLOPs vs. Detail tradeoff) → **Projector (Token Compression point)** → LLM (Inference Latency bottleneck)

- **Design tradeoffs:**
  - **SLM vs. LLM:** Using Small Language Model (<3B params) drastically cuts memory but lowers reasoning capacity/chain-of-thought stability compared to 7B+ models
  - **Token Compression:** Aggressive compression (576 tokens → 64) speeds up inference but destroys fine-grained OCR capabilities
  - **Architecture:** Mamba offers speed but is newer/harder to train stable than Transformers; MoE offers capacity but requires complex routing logic

- **Failure signatures:**
  - **Hallucination:** SLM backbone lacks world knowledge to verify visual features (likely when using <1B models)
  - **Detail Loss:** Text/OCR tasks fail completely; usually indicates Projector or Token Compression module is too aggressive
  - **Modality Bias:** Model ignores image and answers based on text priors

- **First 3 experiments:**
  1. **Projector Ablation:** Replace simple Linear Projector with LDPv2 in TinyLLaVA setup. Measure accuracy drop vs. FLOPs reduction on VQAv2
  2. **Resolution vs. Compression:** Feed 336px vs 672px images into model with token pruning. Identify "sweet spot" where resolution increases don't improve accuracy due to pruning losses
  3. **SLM Baseline:** Compare Phi-2 (2.7B) vs Gemma-2B backbones on standard VQA benchmark to determine trade-off between parameter size and reasoning capability for hardware constraints

## Open Questions the Paper Calls Out

- **Open Question 1:** How can efficient MLLMs be adapted to effectively process extended-context multimodal information, such as lengthy videos or interleaved text-document streams, beyond single-image inputs? Current architectures and token compression techniques struggle to balance quadratic complexity of long visual sequences with strict memory constraints of efficiency.

- **Open Question 2:** What methodologies are required to expand efficient MLLMs to support broader diversity of input and output modalities beyond standard image-text pairing? Integrating additional modalities (e.g., audio, tactile) typically increases parameter counts and inference latency, conflicting with resource efficiency goals.

- **Open Question 3:** What specific architectural or training advancements are needed to bridge gap between current efficient MLLMs and functional embodied agents for edge deployment? Current efficient models lack necessary grounding and real-time interaction capabilities required for robotics and automation tasks on constrained hardware.

## Limitations

- Survey doesn't provide systematic ablation studies comparing efficiency mechanisms head-to-head under identical training conditions
- Efficiency claims rely heavily on parameter counts rather than detailed throughput measurements across hardware configurations
- Results aggregate from diverse training pipelines without standardizing evaluation protocols, making direct efficiency-performance trade-off analysis challenging

## Confidence

**High Confidence:** Architectural categorization and identification of efficiency bottlenecks (quadratic attention complexity, large parameter counts) are well-established in literature.

**Medium Confidence:** Claims about specific efficiency gains from token pruning and MoE are supported by cited works, but survey doesn't validate through independent experimentation or address potential negative interactions between efficiency methods.

**Low Confidence:** Assertion that efficient MLLMs can match or exceed larger models on various benchmarks is primarily based on cherry-picked comparisons from literature rather than comprehensive benchmarking.

## Next Checks

1. **Systematic Ablation Study:** Implement controlled experiment comparing token pruning, MoE routing, and Mamba sequence modeling in same training pipeline to quantify individual and combined efficiency gains on standardized VQA benchmark.

2. **Hardware-Agnostic Efficiency Measurement:** Benchmark efficient MLLMs across different hardware (CPU, GPU, edge devices) measuring actual throughput (samples/second) and memory usage, not just parameter counts, to validate claimed efficiency gains.

3. **Long-Term Stability Analysis:** Evaluate efficient MLLMs on temporally extended video tasks or long-context visual reasoning to identify where mechanisms like SSMs and token pruning fail compared to traditional attention-based approaches.