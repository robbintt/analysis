---
ver: rpa2
title: 'In Machina N400: Pinpointing Where a Causal Language Model Detects Semantic
  Violations'
arxiv_id: '2511.19232'
source_url: https://arxiv.org/abs/2511.19232
tags:
- layer
- semantic
- language
- each
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how and where transformer-based language
  models detect semantic violations, drawing parallels with human N400 brain responses.
  Using a carefully curated dataset of matched sentence pairs with plausible or implausible
  endings, the authors examine the Phi-2 causal language model layer by layer.
---

# In Machina N400: Pinpointing Where a Causal Language Model Detects Semantic Violations

## Quick Facts
- arXiv ID: 2511.19232
- Source URL: https://arxiv.org/abs/2511.19232
- Authors: Christos-Nikolaos Zacharopoulos; Revekka Kyriakoglou
- Reference count: 24
- Primary result: Phi-2 detects semantic anomalies only in mid-to-late layers (peak AUC=0.723 at layer 22), paralleling human N400 timing.

## Executive Summary
This study investigates where transformer-based language models detect semantic violations, drawing parallels with human N400 brain responses. Using a carefully curated dataset of matched sentence pairs with plausible or implausible endings, the authors examine the Phi-2 causal language model layer by layer. They employ logistic regression probes to detect anomaly signals and measure participation ratio to track representational dimensionality changes. Results show that semantic anomaly detection emerges in mid-to-late layers (peak at layer 22, AUC=0.723), not in early layers, paralleling human processing where semantic integration occurs after syntactic resolution. Dimensionality analysis reveals an initial expansion of representational space for violations followed by a late-stage contraction, suggesting an exploratory phase transitioning to consolidation. These findings suggest both artificial and biological language systems postpone full semantic evaluation until sufficient context has been integrated.

## Method Summary
The study uses Phi-2 (2.7B, 32 layers) to analyze semantic anomaly detection. A dataset of 1520 sentences (760 pairs) with plausible/implausible endings is processed through all layers. Hidden states at each layer are extracted, normalized via robust centering/scaling, and mean activation is computed. Logistic regression probes (ℓ2-regularized, λ=1) with 5-fold CV per layer assess anomaly discriminability via ROC-AUC. Participation ratio measures representational dimensionality through covariance eigenvalue decomposition. Cluster-based permutation testing (1000 permutations, p<0.01) identifies significant detection windows.

## Key Results
- Semantic anomaly detection emerges only in mid-to-late layers (peak AUC=0.723 at layer 22), with significant cluster spanning layers 18–30.
- Participation ratio shows initial expansion for violations (layers 1–6) followed by contraction after layer 12.
- Results parallel human N400 timing, suggesting both systems postpone semantic anomaly detection until sufficient context integration.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic anomaly detection in Phi-2 emerges only in mid-to-late layers, not early layers.
- Mechanism: A logistic regression probe trained on mean hidden-state activations at each layer fails to distinguish plausible from implausible sentence endings in early layers (1–10), then rises sharply to peak discriminability at layer 22 (AUC=0.723), with a significant cluster spanning layers 18–30.
- Core assumption: The linear probe's readability reflects where the model encodes the anomaly, not just where information is linearly accessible.
- Evidence anchors:
  - [abstract] "a simple linear decoder struggled to distinguish... in the lowest third... accuracy sharply increased in the middle blocks, reaching a peak just before the top layers"
  - [Section 3.1] "peak decoding occurred at layer 22 (mean AUC = 0.723, SEM = 0.014)" with significant cluster "layers 18–30" (p<0.01)
  - [corpus] Weak direct evidence; related work on concept circuits and semantic representations exists but does not replicate this layer-wise anomaly detection finding.
- Break condition: If early-layer probes could decode anomalies with non-linear classifiers, the claim would weaken to "linear readability" rather than "encoding emergence."

### Mechanism 2
- Claim: Semantic violations initially expand representational dimensionality, then contract after a mid-stack bottleneck.
- Mechanism: Participation ratio (PR) measures effective dimensionality via eigenvalue concentration in the covariance matrix of hidden states. Violation sentences show higher PR than controls in layers 1–6 (max difference 1.9 dimensions at layer 5), converge at layer 12, then fall below controls in deeper layers.
- Core assumption: PR differences reflect genuine representational reorganization for anomaly processing, not merely increased activation magnitude.
- Evidence anchors:
  - [abstract] "Initially, the violation widens the representational subspace, followed by a collapse after a mid-stack bottleneck"
  - [Section 3.2] "Across the lower stack (layers 1–6), the violation PR exceeded the control PR... converged at layer 12. The mean PR difference after the inflection point remained negative"
  - [corpus] Desbordes et al. (2023) use PR for sentence integration dynamics, supporting PR as a meaningful metric, but do not test anomaly-specific patterns.
- Break condition: If PR differences disappear after controlling for activation magnitude or sequence length, the expansion-contraction interpretation would require revision.

### Mechanism 3
- Claim: The late emergence of semantic anomaly detection parallels human N400 timing, where semantic integration follows syntactic resolution.
- Mechanism: The model processes input through successive layers that incrementally build context. Early layers may handle local/syntactic patterns while deeper layers integrate global semantic constraints, making anomaly signals detectable only after sufficient context accumulation.
- Core assumption: Layer depth in transformers maps meaningfully to processing stage in human comprehension; this is analogical, not proven mechanistic equivalence.
- Evidence anchors:
  - [Section 4.1] "semantic error detection emerges only after the network has traversed a substantial portion of its hierarchy... both artificial and biological language systems postpone semantic anomaly detection until a sufficiently broad context has been integrated"
  - [Section 4.2] "expansion–contraction profile aligns qualitatively with reanalysis signatures in human neurophysiology (N400–P600 sequence)"
  - [corpus] Goldstein et al. (2022) and Caucheteux & King (2022) show brain-model alignment in language processing, but do not specifically validate N400-homologous layer timing.
- Break condition: If bidirectional models (e.g., BERT) show early-layer anomaly detection, the claim would be specific to causal architectures rather than general semantic processing.

## Foundational Learning

- Concept: **Participation ratio (PR) as effective dimensionality**
  - Why needed here: PR quantifies how many principal components meaningfully contribute to variance, revealing whether representations are diffuse (high PR) or concentrated (low PR).
  - Quick check question: If a 100-dimensional activation has 95 eigenvalues near zero and 5 large ones, is PR closer to 5 or 100?

- Concept: **Linear probing and its limitations**
  - Why needed here: Logistic regression probes test whether information is linearly decodable; failure may indicate non-linear encoding or genuine absence.
  - Quick check question: If a linear probe fails but a non-linear probe succeeds, can you conclude the information is absent from the representation?

- Concept: **Causal vs. bidirectional attention in language models**
  - Why needed here: Phi-2's causal mask ensures each token attends only to preceding tokens, creating a left-to-right processing constraint analogous to human reading.
  - Quick check question: Would you expect anomaly detection to emerge at the same layer depth in a bidirectional model given the same stimulus? Why or why not?

## Architecture Onboarding

- Component map:
  Input sentences -> Tokenization -> Phi-2 forward pass -> Hidden state extraction -> Normalization -> Mean activation computation -> Logistic regression probe (AUC) and covariance decomposition (PR) -> Analysis

- Critical path:
  1. Sentence pairs (plausible/implausible) → tokenization
  2. Forward pass through all 32 layers
  3. Extract hidden states at each layer for final token
  4. Normalize across sentences within layer
  5. Compute mean activation → logistic regression probe (5-fold CV)
  6. Compute covariance across sentences → eigenvalues → PR
  7. Cluster-based permutation test for significance (layers 18–30 cluster, p<0.01)

- Design tradeoffs:
  - **Single model (Phi-2)**: Enables deep analysis but limits generalizability; replication across depths/scales needed
  - **Fixed violation position (final token)**: Maximizes control but creates positional confound; randomization needed in future work
  - **Linear probe only**: Conservative test; non-linear probes might reveal earlier anomaly signals
  - **Mean activation only**: Simplifies analysis; higher-order moments tested but added noise without signal gain

- Failure signatures:
  - **Early-layer decoding above chance**: Would contradict delayed semantic integration hypothesis
  - **PR difference reversal**: If violations never show early expansion, the exploratory-phase interpretation weakens
  - **No significant cluster after permutation correction**: Would indicate decoding is unreliable
  - **Cross-validation leakage**: If identical lexical items appear in train/test folds, AUC inflates spuriously

- First 3 experiments:
  1. **Replicate with violation at non-final positions**: Randomize anomaly location to verify position-invariance of layer-wise detection pattern.
  2. **Test non-linear probes (e.g., MLP, kernel methods)**: Determine whether early layers encode anomalies non-linearly or genuinely lack the signal.
  3. **Compare across model depths**: Run identical pipeline on shallower (e.g., 12-layer) and deeper (e.g., 48-layer) models to test whether mid-to-late emergence scales with depth proportionally.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the mid-to-late emergence of semantic anomaly detection generalize to bidirectional encoders and models of different scales?
- Basis in paper: [explicit] The authors state in the Limitations section that results are specific to Phi-2 and should be replicated in larger autoregressive LMs and bidirectional encoders.
- Why unresolved: Different model depths (layers) and training objectives (causal vs. masked) could fundamentally alter where and how semantic integration occurs.
- What evidence would resolve it: Layer-wise probing of architectures like BERT (bidirectional) and larger GPT variants using the same stimuli.

### Open Question 2
- Question: Is the anomaly detection mechanism invariant to the token's position within the sentence?
- Basis in paper: [explicit] The authors note that violations always appeared on the final token, creating a positional regularity that the network could exploit.
- Why unresolved: The fixed position prevents distinguishing between true semantic integration and end-of-sequence processing routines.
- What evidence would resolve it: Replicating the analysis with randomized violation positions and graded expectancy manipulations.

### Open Question 3
- Question: Do the model's internal representations share computational mechanisms with human neural processing, or merely correlational timing?
- Basis in paper: [explicit] The Discussion posits that shared latency (mid-late layers ~ N400) does not guarantee shared mechanisms due to the lack of multimodal grounding in models.
- Why unresolved: Behavioral similarity (fluency) masks the underlying algorithmic reality; the "In Machina N400" might be a superficial isomorphism.
- What evidence would resolve it: Representational Similarity Analysis (RSA) directly comparing model hidden states to time-resolved brain data (EEG/MEG) on identical stimuli.

## Limitations
- Results are based on a single causal transformer (Phi-2) with fixed architecture, limiting generalizability.
- Linear probes may miss non-linear encoding of anomalies in early layers.
- Fixed violation position prevents testing position-invariance of the detection mechanism.

## Confidence

| Claim | Confidence |
|-------|------------|
| Semantic anomaly detection emerges mid-to-late layers | High |
| Dimensionality expansion-contraction pattern | Medium |
| N400 analogy to human processing | Low |

- High confidence in layer-wise emergence due to consistent AUC rise and significant cluster.
- Medium confidence in PR patterns due to clear numerical differences but unclear mechanistic interpretation.
- Low confidence in N400 analogy as it remains purely correlational without direct neural data comparison.

## Next Checks
1. **Test non-linear probes** (e.g., multi-layer perceptron) on early layers to determine if anomalies are encoded non-linearly rather than absent.
2. **Randomize violation position** within sentences to confirm that the layer-wise emergence pattern is not an artifact of fixed final-token placement.
3. **Run identical pipeline on deeper models** (e.g., Phi-3 128B, Llama-3 70B) to test whether the 22-layer peak scales proportionally with model depth.