---
ver: rpa2
title: 'Learning to See Before Seeing: Demystifying LLM Visual Priors from Language
  Pre-training'
arxiv_id: '2509.26625'
source_url: https://arxiv.org/abs/2509.26625
tags:
- visual
- data
- reasoning
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically investigates the origins and structure
  of visual priors in Large Language Models (LLMs) trained solely on text. Through
  over 100 controlled experiments consuming 500,000 GPU-hours across five model scales,
  it reveals that visual priors decompose into separable perception and reasoning
  components.
---

# Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training

## Quick Facts
- arXiv ID: 2509.26625
- Source URL: https://arxiv.org/abs/2509.26625
- Authors: Junlin Han, Shengbang Tong, David Fan, Yufan Ren, Koustuv Sinha, Philip Torr, Filippos Kokkinos
- Reference count: 23
- Primary result: Visual priors in LLMs are separable into reasoning (cultivated by reasoning-centric data) and perception (diffusely from diverse corpora) components

## Executive Summary
This paper systematically investigates how visual priors emerge in Large Language Models trained solely on text. Through over 100 controlled experiments across five model scales consuming 500,000 GPU-hours, the authors discover that visual priors decompose into separable perception and reasoning components. Reasoning priors are predominantly cultivated by structured data (code, math, academia) and scale progressively, while perception priors emerge more diffusely from diverse corpora and depend heavily on vision encoder quality and visual instruction tuning. Based on these insights, they propose a data-centric recipe for pre-training vision-aware LLMs, validated at 1 trillion token scale, achieving competitive language proficiency while outperforming language-focused models on all visual tasks.

## Method Summary
The study systematically varies pre-training data mixtures across 16 sources (web-crawl, code, math, academia, literature, etc.) to isolate visual prior development. Controlled experiments at 5 scales (340M-13B parameters) test reasoning vs perception contributions using LLM and MLLM benchmarks. The MLLM adaptation pipeline uses two stages: (1) training an MLP projector to align frozen vision encoder features to LLM embeddings, and (2) supervised fine-tuning on mixed vision-language and language-only instructions. A 32B LLM classifier partitions data into reasoning-centric vs visual world categories. The final recipe uses ~50-60% reasoning-centric data and ~10-15% visual description data, validated at 1 trillion tokens.

## Key Results
- Reasoning priors scale progressively with reasoning-centric data (code, math, academia), while perception priors saturate quickly with visual description data
- Perception performance is highly sensitive to vision encoder choice and visual instruction tuning data
- The balanced pre-training mixture (50-60% reasoning, 10-15% visual) achieves competitive language perplexity while outperforming language-focused models on all 16 visual benchmarks
- Blind visual instruction tuning reveals models can produce visual reasoning outputs without images, confirming internalized visual priors

## Why This Works (Mechanism)

### Mechanism 1
Reasoning-centric language data (code, math, academia) cultivates a modality-agnostic reasoning prior that transfers to visual reasoning tasks. Structured reasoning text trains abstract logical patterns and compositional thinking that operate independently of input modality. When vision is later attached, the LLM applies these patterns to visual inputs without retraining the reasoning core.

### Mechanism 2
Perception prior emerges diffusely from diverse, broad corpora and depends heavily on vision encoder quality and visual instruction tuning—not just LLM pre-training. Diverse text exposes the model to wide vocabulary and fine-grained entity descriptions, creating receptive representations. However, actual visual grounding requires the vision encoder to map pixel patterns to these representations during alignment/tuning.

### Mechanism 3
A small amount of visually descriptive text is necessary but saturates quickly; reasoning data contribution scales progressively. Visual description text provides vocabulary grounding (names, attributes, relations) that connects language tokens to visual concepts. Once this mapping is established (~5-15% of data), additional visual text yields diminishing returns. Reasoning data, however, continues improving performance up to ~75% proportion.

## Foundational Learning

- Concept: **Separation of perception and reasoning in multimodal systems**
  - Why needed here: The paper's central finding is that these are distinct capabilities with different training requirements
  - Quick check question: Can you name one benchmark category that primarily tests perception vs. one that tests reasoning?

- Concept: **Transfer learning across modalities**
  - Why needed here: The paper claims reasoning learned from text transfers to vision without additional reasoning-specific training
  - Quick check question: What type of data would you expect to improve visual reasoning without any image exposure?

- Concept: **Data mixture optimization for pre-training**
  - Why needed here: The paper derives specific mixture ratios (not just "more data") through controlled experiments
  - Quick check question: If you had limited compute for pre-training an MLLM-bound LLM, what data categories would you prioritize?

## Architecture Onboarding

- Component map: LLM backbone (decoder-only Transformer, Llama-3 style) -> Vision encoder (MetaCLIP/DINOv2/MAE - swappable) -> MLP projector (aligns vision features to LLM embedding space) -> Visual instruction tuning data (perception-focused vs. reasoning-focused subsets)

- Critical path: 1) Pre-train LLM on balanced mixture (reasoning-heavy: ~50-60%, visual descriptions: ~10-15%) 2) Freeze LLM + vision encoder, train MLP projector on image-caption pairs 3) Supervised fine-tuning on mixed vision-language and language-only instructions

- Design tradeoffs: More reasoning data → better Knowledge/Vision-Centric VQA, slight language perplexity cost; more visual description → diminishing returns after initial vocabulary grounding; vision encoder choice → large impact on perception, minimal on reasoning

- Failure signatures: Strong language metrics but weak visual reasoning → insufficient reasoning-centric pre-training; weak OCR/Chart performance → check vision encoder quality and perception-tuning data; hallucination without images → model over-relying on language priors

- First 3 experiments: 1) Ablate reasoning data proportion (0%, 25%, 50%, 75%, 100%) and measure Knowledge VQA vs. language perplexity to verify tradeoff curve 2) Swap vision encoder (MetaCLIP → DINOv2 → MAE) while keeping LLM fixed to confirm perception sensitivity 3) Remove perception-tuning data incrementally (100% → 50% → 0%) and measure OCR & Chart VQA drop to validate perception-tuning dependency

## Open Questions the Paper Calls Out

- Do text-based visual priors encode biased visual associations that manifest as harmful behavior in downstream MLLMs?
- How do specific textual sources contribute to visual priors for dynamic modalities like video understanding?
- What is the precise interplay between abstract data structure and semantic grounding in forming cross-modal representations?

## Limitations
- The study focuses exclusively on static images, leaving temporal knowledge and video understanding as open questions
- Safety and fairness implications of text-based visual priors were not investigated
- The optimal data mixture ratios may not transfer across languages and domains
- Perception prior analysis relies heavily on specific vision encoder architectures that may not generalize

## Confidence

**High confidence**: The reasoning prior scales with reasoning-centric data proportion, and reasoning and perception priors are behaviorally separable based on task performance patterns.

**Medium confidence**: The perception prior emerges from diverse corpora and depends on vision encoder quality; the 50-60% reasoning / 10-15% visual description mixture is optimal for vision-aware LLMs.

**Low confidence**: The perception prior cannot be fully cultivated through text alone, and the exact saturation point for visual description data (~15%) is universal.

## Next Checks

1. **Architecture Transfer Test**: Reproduce the core findings using encoder-decoder or prefix-LM architectures to determine if the separable priors phenomenon is architecture-dependent or universal.

2. **Encoder-Agnostic Perception Test**: Implement the MLLM adaptation pipeline using three additional vision encoders (e.g., OpenCLIP, BLIP-2, SigLIP) beyond the three already tested to verify whether perception sensitivity is a general pattern or encoder-specific.

3. **Cross-Domain Mixture Validation**: Apply the proposed data mixture recipe to pre-train models on non-English corpora (e.g., Chinese, Japanese) and measure whether the same ~50-60% reasoning / ~10-15% visual description ratio yields optimal vision-aware performance, or if domain-specific adjustments are needed.