---
ver: rpa2
title: Experience-Evolving Multi-Turn Tool-Use Agent with Hybrid Episodic-Procedural
  Memory
arxiv_id: '2512.07287'
source_url: https://arxiv.org/abs/2512.07287
tags:
- tool
- memory
- agent
- h-epm
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: H-EPM is a hybrid episodic-procedural memory framework for multi-turn
  tool-use agents that adaptively reuses past experiences during both inference and
  reinforcement learning. It builds a tool graph where edges encode procedural tool-to-tool
  transition patterns and episodic state summaries, enabling context-aware retrieval
  of relevant past experiences.
---

# Experience-Evolving Multi-Turn Tool-Use Agent with Hybrid Episodic-Procedural Memory

## Quick Facts
- arXiv ID: 2512.07287
- Source URL: https://arxiv.org/abs/2512.07287
- Reference count: 26
- Primary result: Hybrid episodic-procedural memory improves inference accuracy by up to 50% and RL policy performance by up to 40% on multi-turn tool-use tasks.

## Executive Summary
H-EPM is a hybrid episodic-procedural memory framework for multi-turn tool-use agents that adaptively reuses past experiences during both inference and reinforcement learning. It builds a tool graph where edges encode procedural tool-to-tool transition patterns and episodic state summaries, enabling context-aware retrieval of relevant past experiences. At inference, the agent dynamically switches between episodic recall and procedural execution; during RL, it biases exploration toward historically successful tool transitions to improve policy learning. Experiments on diverse benchmarks show H-EPM improves inference accuracy by up to 50% and RL policy performance by up to 40% on out-of-distribution tasks, outperforming strong baselines by combining transferable tool patterns with state-specific episodic guidance.

## Method Summary
H-EPM constructs a tool graph from successful trajectories where edges carry procedural weights (transition frequency and efficiency) and episodic state summaries. During inference, the agent adaptively switches between episodic retrieval (when summarization is invoked) and procedural execution (based on edge weights). In RL, the framework guides exploration by suggesting historically successful tools while maintaining stochasticity via skip rate. The method integrates with GRPO, masking memory suggestions during loss computation to preserve unbiased learning.

## Key Results
- Inference accuracy improves by up to 50% over strong baselines on τ-Bench and τ²-Bench
- RL policy performance improves by up to 40% on out-of-distribution tasks in τ²-Bench
- H-EPM shows consistent improvements across Telecom, Retail, ToolSandbox, and ACEBench benchmarks

## Why This Works (Mechanism)

### Mechanism 1
Hybrid episodic-procedural memory enables transfer across partially overlapping task experiences. The tool graph encodes procedural patterns as weighted tool-to-tool edges while attaching compact state summaries to edges as episodic annotations. At inference, the agent retrieves procedural patterns for routine transitions and episodic context for state-specific decisions. Core assumption: Decision-relevant context can be compressed into summaries that generalize across similar states. Evidence: Abstract states "edges encode procedural tool-to-tool transition patterns and episodic state summaries, enabling context-aware retrieval." Break condition: If state summaries fail to capture decision-relevant information, episodic retrieval will match irrelevant experiences.

### Mechanism 2
Adaptive switching between episodic recall and procedural execution improves tool selection accuracy. The agent autonomously decides whether to invoke the state-summarization tool. If invoked, the resulting summary is compared against stored edge summaries via similarity matching (episodic). If not invoked, top-k tools are selected by edge weights (procedural). This preserves flexibility while providing context-aware guidance. Core assumption: The base LLM can reliably judge when summarization is necessary. Evidence: Section 4.3 states "H-EPM adaptively switches between episodic and procedural memory" and Table 4C shows ablation degradation when switching is removed. Break condition: If the summarization invocation decision is unreliable, the wrong memory type will be retrieved.

### Mechanism 3
Memory-guided exploration in RL improves policy learning for long-horizon tasks. During RL rollouts, H-EPM suggests historically successful next tools while injecting stochasticity via skip rate p_skip. The memory graph is updated online with newly successful trajectories, allowing guidance to evolve. This biases exploration toward productive regions of the action space without constraining discovery. Core assumption: Successful tool transitions in prior tasks correlate with productive actions in new tasks. Evidence: Section 4.3 states "By biasing exploration toward historically successful tool transitions, H-EPM learns a stronger policy that generalizes at inference time." Table 3 shows H-EPM with skip rates 0.8-1.0 consistently outperforms base GRPO and AgentEvolver across benchmarks. Break condition: If skip rate is too low, over-reliance on past experience prevents discovering novel strategies.

## Foundational Learning

- **POMDP (Partially Observable Markov Decision Process)**: Multi-turn tool-use is formulated as a POMDP where the agent observes dialogue and tool feedback but not the full environment state. Understanding belief states is essential for interpreting why state summarization matters. Quick check: Can you explain why the agent must maintain a belief over latent states rather than directly observing them?

- **Episodic vs. Procedural Memory (from cognitive science)**: H-EPM explicitly models these two memory types. Episodic stores context-conditioned transitions; procedural stores context-free patterns. The hybrid design requires understanding when each is appropriate. Quick check: Given a repetitive task (e.g., always calling get_customer_by_phone first), which memory type should dominate?

- **Group Relative Policy Optimization (GRPO)**: The RL integration uses GRPO as the base algorithm. Understanding advantage estimation and KL constraints is necessary to modify the rollout process correctly. Quick check: How does GRPO differ from PPO in how it computes advantages?

## Architecture Onboarding

- **Component map**: State-summarization tool -> Tool graph G = (V, E, W, S) -> Memory retrieval module -> RL rollout wrapper
- **Critical path**: 1) Collect successful trajectories from training set rollouts (with summarization tool enabled) 2) Construct tool graph: extract tool pairs, compute edge weights (accuracy + efficiency), attach state summaries 3) At inference/RL: locate previous tool in graph → retrieve candidates → (episodic) compare state similarity OR (procedural) compare weights → return top-k suggestions 4) For RL: apply skip rate, mask guidance during loss computation
- **Design tradeoffs**: Explicit vs. implicit summarization strategy: Explicit (per-turn prompting) achieves better performance but ~60% higher latency; implicit reduces cost with ~5% performance drop. Skip rate tuning: Lower p_skip (0.8) works when tool dependencies are strong; higher p_skip (1.0) when context dominates. Domain-specific. Top-k selection: k=2 is optimal; k=1 is too restrictive, k=3 introduces noise.
- **Failure signatures**: "Tool overuse trap": If edge weights only encode transition frequency without efficiency, agents invoke harmless but task-irrelevant tools repeatedly. Mismatched episodic retrieval: If state summaries are too specific, similarity matching returns irrelevant experiences. Exploration collapse in RL: If skip rate is too low and memory is sparse, the agent never discovers alternatives to stored patterns.
- **First 3 experiments**: 1) Memory construction sanity check: Build tool graph from 100 annotated trajectories; manually inspect edge weights and state summaries for plausibility. 2) Inference ablation: Run H-EPM on τ-Bench with (a) full method, (b) procedural-only (w/o state), (c) episodic-only (w/o weight). Confirm hybrid outperforms both. 3) Skip rate sweep: Train GRPO + H-EPM on τ²-Bench with p_skip ∈ {0.5, 0.8, 0.9, 1.0}. Identify optimal domain-specific setting and verify it matches tool-dependency strength.

## Open Questions the Paper Calls Out

### Open Question 1
How can external guidance mechanisms be designed to align with the internal reasoning processes of models that possess strong, inherent reasoning capabilities? Basis: "However, models with stronger reasoning capabilities often exhibit weaker adherence to external guidance during execution... Developing more effective mechanisms to align guidance with the reasoning processes of such models remains an important direction for future work." Why unresolved: The authors observe that while H-EPM aids smaller or instruction-tuned models significantly, highly capable models (e.g., GPT-4o, GPT-5.1) tend to ignore external memory guidance, limiting the framework's relative utility on the strongest backbones. Evidence: A modification to the guidance injection mechanism (e.g., implicit biasing rather than explicit prompting) that results in statistically significant performance gains for high-reasoning models over their baseline.

### Open Question 2
Can the optimal balance between procedural and episodic memory (skip rate p_skip) be determined adaptively by the agent rather than set as a domain-specific hyperparameter? Basis: "The effectiveness of H-EPM further depends on the skip rate p_skip... adjusting p_skip to match the degree of tool-to-tool reliance in a given domain enables H-EPM to achieve stronger task-specific performance." Why unresolved: The experiments demonstrate that different domains require vastly different skip rates (0.8 to 1.0) to maximize performance, implying that a static setting is insufficient for general-purpose agents operating in mixed environments. Evidence: An adaptive algorithm that dynamically adjusts the reliance on graph weights vs. state retrieval during inference, achieving optimal performance across benchmarks without manual tuning.

### Open Question 3
How robust is the H-EPM retrieval mechanism to noise or hallucinations generated by the autonomous state-summarization tool? Basis: "We compress each history into a summary-based state s_t = φ(h_t)... The resulting tuples (s_t, a_t, a_{t+1}) form the core of episodic experience." Why unresolved: The framework relies on the base model's ability to summarize states accurately. If the summarization tool omits critical details or hallucinates, the retrieval of "relevant" past experiences will be based on flawed state similarities, potentially misleading the agent. Evidence: An ablation study measuring performance degradation when synthetic noise is injected into the stored state summaries, or a comparative analysis using a ground-truth state oracle vs. the LLM summarizer.

## Limitations
- Memory construction relies on successful trajectories, but criteria for labeling "success" and exact tool definitions across benchmarks are underspecified, which could affect edge weights and state summaries.
- Skip rate tuning is domain-specific (0.8-1.0), but the paper does not provide clear guidance on how to determine the optimal rate without prior knowledge of task structure.
- The claim of 40% RL improvement on out-of-distribution tasks is based on τ²-Bench/Retail; generalization to other unseen domains remains unproven.
- Latency cost of explicit summarization (~60% higher) may limit practical deployment in low-latency scenarios.

## Confidence
- **High confidence**: Hybrid episodic-procedural memory framework improves inference accuracy by up to 50% (supported by ablation studies and clear mechanism).
- **Medium confidence**: RL policy performance improves by up to 40% with memory-guided exploration (based on single benchmark; skip rate sensitivity not fully explored).
- **Low confidence**: Out-of-distribution generalization without additional fine-tuning (limited evidence; benchmark overlap unclear).

## Next Checks
1. Reconstruct tool graph from a small subset of trajectories (e.g., 100); manually inspect edge weights and state summaries for plausibility before scaling.
2. Run H-EPM on τ-Bench with forced episodic-only and procedural-only modes to confirm hybrid outperforms both in controlled ablation.
3. Train GRPO + H-EPM on τ²-Bench with multiple skip rates (0.5, 0.8, 0.9, 1.0) to identify domain-specific optimal settings and verify alignment with tool-dependency strength.