---
ver: rpa2
title: Benchmarking Large Multimodal Models for Ophthalmic Visual Question Answering
  with OphthalWeChat
arxiv_id: '2505.19624'
source_url: https://arxiv.org/abs/2505.19624
tags:
- pairs
- question
- images
- open-ended
- ophthalmic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "OphthalWeChat is a bilingual (Chinese\u2013English) multimodal\
  \ visual question answering (VQA) benchmark for ophthalmology, built from WeChat\
  \ Official Account posts. It contains 3,469 images and 30,120 QA pairs spanning\
  \ 9 subspecialties, 29 imaging modalities, and 68 modality combinations."
---

# Benchmarking Large Multimodal Models for Ophthalmic Visual Question Answering with OphthalWeChat

## Quick Facts
- arXiv ID: 2505.19624
- Source URL: https://arxiv.org/abs/2505.19624
- Authors: Pusheng Xu; Xia Gong; Xiaolan Chen; Weiyi Zhang; Jiancheng Yang; Bingjie Yan; Meng Yuan; Yalin Zheng; Mingguang He; Danli Shi
- Reference count: 0
- Primary result: OphthalWeChat is a bilingual ophthalmic VQA benchmark with 3,469 images and 30,120 QA pairs; Gemini 2.0 Flash achieved highest accuracy (0.548) in zero-shot evaluation.

## Executive Summary
OphthalWeChat is a bilingual (Chinese–English) multimodal visual question answering (VQA) benchmark for ophthalmology, built from WeChat Official Account posts. It contains 3,469 images and 30,120 QA pairs spanning 9 subspecialties, 29 imaging modalities, and 68 modality combinations. The dataset was generated using GPT-4o-mini and includes binary, single-choice, and open-ended questions. It was used to evaluate three VLMs: GPT-4o, Gemini 2.0 Flash, and Qwen2.5-VL-72B-Instruct. Gemini 2.0 Flash achieved the highest overall accuracy (0.548), significantly outperforming the others. GPT-4o led in English and open-ended tasks, while Gemini 2.0 Flash excelled in closed-ended Chinese subsets. The benchmark addresses the need for diverse, clinically relevant ophthalmic VQA datasets to support AI development and evaluation.

## Method Summary
The study created a bilingual ophthalmic VQA benchmark by scraping 3,469 images and captions from WeChat Official Accounts (2016–2024), then using GPT-4o-mini to generate 30,120 bilingual QA pairs. The dataset covers 9 subspecialties and 29 imaging modalities, with answer distributions balanced to prevent model shortcut learning. Three VLMs (GPT-4o, Gemini 2.0 Flash, Qwen2.5-VL-72B) were evaluated zero-shot on six subsets (Binary/Single-choice/Open-ended × Chinese/English). Open-ended responses were automatically graded by DeepSeek-V3, validated against expert ophthalmologist consensus. Statistical analysis included McNemar's test for model comparison and Chi-square tests for language differences.

## Key Results
- Gemini 2.0 Flash achieved highest overall accuracy (0.548) across all six subsets
- GPT-4o performed best on English-language and open-ended tasks
- Gemini 2.0 Flash excelled in closed-ended Chinese subsets
- All models scored below 60% accuracy, indicating current VLMs are insufficient for clinical deployment
- Lowest accuracy observed in Oculoplastics/Orbit and Pediatric Ophthalmology subspecialties, and for Ultrasound B-scan and SLO+FFA composite modalities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-generated QA pairs from image captions can produce clinically relevant benchmark questions at scale.
- Mechanism: GPT-4o-mini converts heterogeneous image captions into structured QA pairs (binary, single-choice, open-ended) in both Chinese and English, using carefully designed prompts that restrict questions to image-derivable content.
- Core assumption: The source captions contain sufficient diagnostic detail for generating clinically meaningful questions without requiring direct image analysis.
- Evidence anchors:
  - [abstract] "Based on these captions, bilingual question-answer (QA) pairs in Chinese and English were generated using GPT-4o-mini."
  - [section] Supplementary Table 1 shows prompts explicitly instructing: "Focus only on the content of the images and their diagnosis."
  - [corpus] Weak direct evidence; DermaBench uses similar clinician-annotated approaches but with human verification as primary.
- Break condition: If captions contain diagnostic errors, hallucinated findings, or insufficient detail, generated QA pairs will inherit and amplify these errors.

### Mechanism 2
- Claim: Answer distribution balancing reduces evaluation bias from model shortcut learning.
- Mechanism: Initial binary distribution (71.2% True) was corrected by semantically inverting 21.2% of True questions to False. Single-choice distributions (A: 39.2%, B: 44.0%, C: 14.7%, D: 2.1%) were balanced via controlled option-swapping.
- Core assumption: Models may exploit imbalanced answer distributions without learning true visual reasoning.
- Evidence anchors:
  - [section] "To mitigate potential bias due to imbalanced answer distributions in binary and single-choice QA formats, corrective adjustments were implemented."
  - [section] Supplementary Table 3 documents the exact prompts for semantic inversion and option-swapping.
  - [corpus] No direct corpus evidence on answer balancing in medical VQA benchmarks.
- Break condition: If semantic inversion creates unnatural or contradictory questions, the balancing mechanism introduces noise rather than reducing bias.

### Mechanism 3
- Claim: Third-party LLM evaluation provides reliable automated assessment of open-ended medical responses.
- Mechanism: DeepSeek-V3 judges factual and semantic alignment between model outputs and reference answers, validated against an ophthalmologist on 100 samples (100% concordance).
- Core assumption: LLM judges can approximate expert clinical judgment for factual correctness assessment.
- Evidence anchors:
  - [section] "The model demonstrated 100% concordance with expert judgment, supporting its use as a valid and trustworthy evaluator in this study."
  - [section] Supplementary Table 5 shows the evaluation prompt requiring TRUE/FALSE judgment based on factual accuracy.
  - [corpus] DermaBench mentions similar automated evaluation approaches; Badshah et al. (cited) shows LLM ensembles improve evaluation reliability.
- Break condition: If validation sample (n=100) is not representative of full dataset complexity, concordance may not generalize to edge cases or ambiguous responses.

## Foundational Learning

- Concept: **Visual Question Answering (VQA)**
  - Why needed here: This is the core task format. Understanding that VQA requires joint visual encoding and textual reasoning is essential before interpreting benchmark results.
  - Quick check question: Given a fundus image and the question "Is there macular edema present?", what two capabilities must a model demonstrate?

- Concept: **Vision-Language Model Architecture**
  - Why needed here: The paper evaluates GPT-4o, Gemini 2.0 Flash, and Qwen2.5-VL-72B without explaining their differences. Understanding that these models integrate a visual encoder with an LLM decoder clarifies why they can process image-text pairs.
  - Quick check question: Why can't a text-only LLM (like GPT-3) solve the VQA tasks in this benchmark?

- Concept: **Evaluation Metrics (BLEU, BERTScore)**
  - Why needed here: Open-ended responses use different metrics than closed-ended. BLEU-1 measures lexical overlap while BERTScore captures semantic similarity. The paper notes Gemini ranked second in accuracy but lowest in BLEU-1, suggesting these metrics capture different aspects.
  - Quick check question: If a model answers "retinal thickening" when the reference is "macular edema," would BLEU-1 or BERTScore better capture semantic correctness?

## Architecture Onboarding

- Component map:
  - Data collection -> WeChat post scraping (Python scripts)
  - Image-caption extraction -> Manual ophthalmologist filtering (6+ years experience)
  - QA generation -> GPT-4o-mini with constrained prompts (Supplementary Table 1)
  - Quality control -> Keyword filtering (Supplementary Table 2) + balance adjustment (Supplementary Table 3)
  - Evaluation -> Three VLMs via API + DeepSeek-V3 as open-ended judge

- Critical path:
  1. Caption quality determines maximum QA quality (garbage-in, garbage-out)
  2. OCR cropping prevents shortcut learning from embedded text
  3. Answer balancing prevents distribution exploitation
  4. LLM judge validation ensures reliable automated evaluation

- Design tradeoffs:
  - **Composite vs. decomposed images**: Kept composite to reflect real-world presentation but introduces visual complexity that may challenge models
  - **Caption-based vs. image-based QA generation**: Faster and scalable but may miss visual-only findings not mentioned in captions
  - **Six subsets**: Increases coverage but smaller sample sizes per subset may reduce statistical power for subspecialty analysis

- Failure signatures:
  - All models <60% overall accuracy: Current VLMs insufficient for clinical deployment
  - Oculoplastics/Orbit and Pediatric Ophthalmology/Strabismus consistently lowest: Training data scarcity in these subspecialties
  - Ultrasound B-scan and SLO+FFA composites lowest by modality: Dataset scarcity + composite image complexity
  - Gemini: high accuracy but lowest BLEU-1 suggests different output style vs. training distribution mismatch

- First 3 experiments:
  1. **Baseline reproduction**: Run GPT-4o, Gemini 2.0 Flash, Qwen2.5-VL on Binary_EN subset; verify accuracy matches reported ~0.70 before expanding to full benchmark
  2. **Caption quality audit**: Sample 50 images and compare caption content against generated QA pairs to quantify what visual information is captured vs. lost
  3. **Judge validation extension**: Expand DeepSeek-V3 validation from 100 to 500 samples across different subspecialties to test generalization of concordance claim

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What architectural or training factors explain why Gemini 2.0 Flash achieves higher overall accuracy while GPT-4o excels on English-language and open-ended tasks?
- Basis in paper: [explicit] "The specific reasons behind these performance discrepancies remain unclear and warrant further investigation."
- Why unresolved: The paper evaluates outputs but does not analyze model architectures, training data composition, or training paradigms that might explain divergent performance patterns across languages and question types.
- What evidence would resolve it: Comparative analysis of training data distributions, ablation studies controlling for multilingual training, and architectural comparisons between models.

### Open Question 2
- Question: Would automated decomposition of composite multi-panel images into individual sub-figures improve VLM evaluation granularity and model performance?
- Basis in paper: [explicit] "Composite images were not decomposed into individual sub-figures... Implementing automated sub-figure detection and segmentation pipelines could support more granular and interpretable model evaluation."
- Why unresolved: The lowest open-ended accuracy was observed for composite images combining SLO and FFA, but it is unclear whether this reflects inherent task difficulty or the challenge of processing multiple simultaneous visual elements.
- What evidence would resolve it: Comparative evaluation of VLMs on segmented versus unsegmented composite images, measuring performance differences and error pattern changes.

### Open Question 3
- Question: Can expanding QA datasets to include next-step diagnostics, treatment planning, and prognostic reasoning improve VLM clinical utility?
- Basis in paper: [explicit] "Future VQA datasets should expand to cover broader clinical decision-making tasks, including next-step diagnostics, treatment planning, and prognostic reasoning."
- Why unresolved: The current dataset focuses primarily on lesion- and diagnosis-related questions, leaving higher-order clinical reasoning tasks untested and models' capabilities in these areas unknown.
- What evidence would resolve it: Creation of an expanded benchmark with clinical decision-making QA pairs and longitudinal evaluation of whether models trained on such data improve on downstream clinical workflow tasks.

### Open Question 4
- Question: What evaluation metrics best correlate with clinical expert judgment for open-ended VQA responses?
- Basis in paper: [inferred] "Gemini 2.0 Flash ranked second in open-ended accuracy, it recorded the lowest BLEU-1 score, suggesting a disconnect between correctness and surface-level text similarity... emphasize the need for comprehensive evaluations integrating both human judgment and diverse metrics."
- Why unresolved: The discordance between automated metrics (BLEU-1, BERTScore) and accuracy scores suggests current metrics may inadequately capture clinical relevance and semantic correctness.
- What evidence would resolve it: Systematic correlation analysis between automated metrics and expert clinician ratings across diverse response types, potentially identifying or developing metrics that better align with clinical judgment.

## Limitations

- Dataset construction bias: The benchmark relies entirely on LLM-generated QA pairs from WeChat captions, with no human expert validation of the generated questions beyond automated keyword filtering.
- Generalizability constraints: Results are based on zero-shot inference only, with no fine-tuning or few-shot evaluation.
- Evaluation methodology uncertainty: While DeepSeek-V3 showed 100% concordance on 100 validation samples, this represents only 0.3% of the dataset.

## Confidence

- **High confidence**: Dataset construction methodology, answer distribution balancing techniques, and basic accuracy reporting are well-documented and reproducible.
- **Medium confidence**: The automated evaluation pipeline using DeepSeek-V3 as judge is technically sound but may not generalize beyond the validation sample.
- **Low confidence**: Clinical relevance of generated questions and their diagnostic accuracy cannot be independently verified without access to the full dataset for manual expert review.

## Next Checks

1. **Dataset quality audit**: Conduct blind review of 500 randomly sampled QA pairs by independent ophthalmology experts to assess clinical accuracy, relevance, and potential hallucinations in both Chinese and English versions.

2. **Cross-benchmark validation**: Test the same VLMs on DermaBench and other established medical VQA benchmarks to determine if performance patterns are consistent across medical domains or specific to OphthalWeChat's construction methodology.

3. **Subspecialty reliability assessment**: Perform power analysis on subspecialty subsets (particularly Oculoplastics/Orbit and Pediatric Ophthalmology) to determine if current sample sizes provide sufficient statistical power for meaningful performance comparisons.