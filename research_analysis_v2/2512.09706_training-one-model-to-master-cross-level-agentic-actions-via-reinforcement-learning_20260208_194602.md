---
ver: rpa2
title: Training One Model to Master Cross-Level Agentic Actions via Reinforcement
  Learning
arxiv_id: '2512.09706'
source_url: https://arxiv.org/abs/2512.09706
tags:
- action
- arxiv
- training
- tasks
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces CrossAgent, a unified agentic model trained
  to dynamically switch across heterogeneous action spaces (from high-level APIs to
  low-level primitives) to optimize task execution in open-world environments. The
  authors propose a three-stage training pipeline: (1) Cold-Start Supervised Fine-Tuning
  on mixed-action-space data, (2) Single-Turn Reinforcement Learning to learn autonomous
  action-space selection, and (3) Multi-Turn Reinforcement Learning to optimize long-horizon
  task success and efficiency.'
---

# Training One Model to Master Cross-Level Agentic Actions via Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2512.09706
- **Source URL**: https://arxiv.org/abs/2512.09706
- **Reference count**: 24
- **Primary result**: Unified model (CrossAgent) trained to dynamically switch across heterogeneous action spaces (high-level APIs to low-level primitives) for optimizing task execution in Minecraft

## Executive Summary
This paper introduces CrossAgent, a unified agentic model trained to dynamically switch across heterogeneous action spaces to optimize task execution in open-world environments. The authors propose a three-stage training pipeline: (1) Cold-Start Supervised Fine-Tuning on mixed-action-space data, (2) Single-Turn Reinforcement Learning to learn autonomous action-space selection, and (3) Multi-Turn Reinforcement Learning to optimize long-horizon task success and efficiency. The approach uses Multi-Turn Group Relative Policy Optimization (GRPO) without human-specified rules. Extensive experiments on over 800 Minecraft tasks demonstrate that CrossAgent significantly outperforms fixed-action baselines, achieving state-of-the-art performance with strong generalization and efficiency. The model learns to balance high-level efficiency and low-level precision by contextually selecting the optimal interface for each action.

## Method Summary
The CrossAgent framework trains a unified vision-language-action model to dynamically switch across heterogeneous action spaces (Raw, Motion, Grounding, Language, Latent) for task execution in Minecraft. The training pipeline consists of three stages: (1) Cold-start supervised fine-tuning on a balanced mixed-action dataset to establish multi-space grounding, (2) Single-turn reinforcement learning with GRPO to learn strategic action-space selection using an action-space-agnostic reward based on parsed action equivalence, and (3) Multi-turn reinforcement learning with episodic rewards and token-length penalties to optimize trajectory-level efficiency and task success. The final model achieves state-of-the-art performance on over 800 Minecraft tasks by learning to contextually balance high-level API efficiency with low-level precision.

## Key Results
- CrossAgent achieves significant performance improvements over fixed-action baselines (RawHA-RL, MotionHA-RL) on Minecraft tasks
- The three-stage training pipeline demonstrates strong generalization from 30 training tasks to over 800 held-out tasks
- Action-space switching emerges as an emergent capability, with the model learning to select appropriate interfaces based on task requirements (e.g., Motion for navigation, Raw for GUI manipulation)
- Multi-turn GRPO optimization with token-length penalties successfully balances task success against execution efficiency

## Why This Works (Mechanism)

### Mechanism 1
Mixed-action-space supervised fine-tuning establishes a unified policy capable of decoding actions across heterogeneous interfaces without modal interference. By training on a balanced dataset combining trajectories from multiple action subspaces, the model learns shared syntax and semantics while preserving action-space-specific structure.

### Mechanism 2
Single-Turn RL with GRPO converts stochastic multi-space capability into strategic, context-aware action-space selection. GRPO samples multiple action candidates per query and computes advantages via group-relative rewards, reinforcing actions whose parsed raw representation matches ground truth regardless of surface form.

### Mechanism 3
Multi-Turn RL optimizes trajectory-level efficiency by balancing task success against execution cost, inducing emergent hierarchical action-space switching. Sparse episodic rewards propagate to all trajectory steps, while a token-length penalty biases the model toward concise high-level actions when viable.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**
  - Why needed here: Replaces PPO's critic network with group-level advantage estimation, reducing memory/compute while stabilizing multi-turn optimization
  - Quick check question: Can you explain how GRPO computes advantages from sampled outputs without a value function?

- **Hierarchical Action Spaces**
  - Why needed here: Enables the model to operate across Raw (keyboard/mouse), Motion (object-agnostic primitives), Grounding (spatially-targeted), Language (semantic commands), and Latent (learned embeddings) spaces
  - Quick check question: What is the tradeoff between action abstraction level and execution reliability?

- **Vision-Language-Action (VLA) Models**
  - Why needed here: The base model must ground visual observations into executable actions across modalities
  - Quick check question: How does a VLA model differ from a standard VLM in its output space?

## Architecture Onboarding

- **Component map**: Vision encoder (ViT) → Tokenizer → Causal Transformer → Action Router → Heterogeneous decoders (Motion/Grounding/Raw/etc.) → Environment (Minecraft simulator)

- **Critical path**: 1. Construct balanced D_mix from SAM-based grounding + MineCLIP motion annotations 2. Cold-start SFT (200 steps minimum) to obtain M_mix 3. STRL warm-up with rejection sampling → GRPO optimization 4. Self-training relabeling to create D_strl → M_cs2 initialization 5. Multi-Turn GRPO with episodic rewards + token cost penalty

- **Design tradeoffs**: STRL stage adds compute but significantly accelerates MTRL convergence; training on only 30 tasks yields OOD generalization but may underfit task-specific nuances; token-length penalty encourages API efficiency but may over-penalize necessary verbose actions

- **Failure signatures**: Mode collapse to single action space (check action distribution during inference); high ID performance but sharp OOD drop (indicates overfitting); stochastic rather than strategic switching post-STRL (suggests insufficient GRPO iterations)

- **First 3 experiments**:
  1. Ablate STRL: Train CrossAgent without Stage 2, compare MTRL convergence speed and final ASR against full pipeline
  2. Single-space baseline: Train MTRL from single-space initialization (Grounding-only or Motion-only) to quantify heterogeneous action-space benefit
  3. Generalization probe: Evaluate on 30 ID tasks vs. held-out OOD tasks to measure generalization gap; compare against RawHA-RL/MotionHA-RL baselines

## Open Questions the Paper Calls Out

### Open Question 1
Can the CrossAgent architecture be adapted for physical robotics deployment to handle safety constraints and real-time feedback latency? The conclusion identifies transferring capabilities to physical robotics as a primary future direction, noting that simulation-based frameworks don't model physics, latency, or safety risks of real-world hardware.

### Open Question 2
Can offline reinforcement learning algorithms be substituted for the online Multi-Turn GRPO to reduce computational costs? The authors note that the Multi-Turn RL stage entails "non-negligible computational costs" and suggest future investigations into "more sample-efficient or offline RL algorithms."

### Open Question 3
Is the CrossAgent framework robust to the addition of new, unseen action spaces without requiring full retraining on the Mixed-Space SFT dataset? The training pipeline constructs a fixed composite action space, and the paper doesn't address the model's adaptability to new action interfaces introduced after initial training.

## Limitations

- The paper assumes heterogeneous action spaces can be unified under a single transformer without significant interference, primarily validated empirically rather than theoretically
- The parser mapping heterogeneous action strings to canonical raw representations is critical but not explicitly defined, potentially impacting action-space-agnostic credit assignment
- The training pipeline relies on several hyperparameters that are presented as effective but not systematically optimized or analyzed for sensitivity

## Confidence

- **High Confidence**: Overall experimental methodology and evaluation protocol are clearly specified with sound methodology
- **Medium Confidence**: Three-stage training pipeline is logically coherent with statistically significant performance improvements, though mechanisms could benefit from more detailed analysis
- **Low Confidence**: Generalization claims to 800+ tasks from only 30 training tasks are impressive but not fully explained, with potential overfitting concerns

## Next Checks

1. **Parser Robustness Test**: Evaluate action-space-agnostic reward mechanism by introducing noisy or ambiguous action strings across different spaces to measure impact on STRL learning

2. **Action Space Distribution Analysis**: Systematically monitor action space distribution across task types and training iterations during MTRL to identify potential mode collapse

3. **Sample Efficiency Probe**: Replicate main results while varying number of training tasks (10, 20, 30, 50) to quantify relationship between training data diversity and OOD performance