---
ver: rpa2
title: 'MELAC: Massive Evaluation of Large Language Models with Alignment of Culture
  in Persian Language'
arxiv_id: '2508.00673'
source_url: https://arxiv.org/abs/2508.00673
tags:
- persian
- arxiv
- datasets
- language
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks 41 large language models (LLMs) on Persian
  language and Iranian cultural tasks, addressing the lack of evaluation resources
  for non-English languages. The authors created 19 new datasets, including 13 original
  datasets covering Persian linguistics, Iranian law, idioms, and university entrance
  exams, plus 6 adapted from well-known English benchmarks localized for Persian culture.
---

# MELAC: Massive Evaluation of Large Language Models with Alignment of Culture in Persian Language

## Quick Facts
- arXiv ID: 2508.00673
- Source URL: https://arxiv.org/abs/2508.00673
- Reference count: 24
- Primary result: 41 LLMs benchmarked on Persian language and Iranian culture; Persian fine-tuned models underperform multilingual baselines on cultural tasks

## Executive Summary
This study addresses the critical gap in evaluating large language models for non-English languages by creating MELAC, a comprehensive benchmark for Persian language and Iranian culture. The authors developed 19 new datasets covering linguistic, legal, and cultural knowledge specific to Iran, including both original creations and localized adaptations of English benchmarks. Testing 41 diverse models reveals that even Persian-specific fine-tuned models struggle with Iranian cultural knowledge, while proprietary models like GPT-4o outperform others but still show significant limitations. The work establishes that cultural alignment requires more than language translation, highlighting the need for region-specific evaluation frameworks.

## Method Summary
The MELAC benchmark evaluates 41 LLMs across 19 datasets: 13 original Persian-specific datasets (Iran-Law, Verb-Eval, Proverbs-Quiz, Expert-Eval) and 6 localized English benchmarks (MMLU-pro, GSM, ARC, PIQA, Winogrande). Multiple-choice tasks use log-probability scoring with zero-shot prompting, while generative tasks employ 3-shot prompting with regex-based answer extraction. Models are tested across six categories: Persian Linguistic, Persian Legals, Reading Comprehension, General Knowledge, Domain Specific Knowledge, and Common Sense Reasoning. Evaluation uses the lm-evaluation-harness framework, extended for local and API-based models, with temperature set to 0 for deterministic outputs.

## Key Results
- Only one model exceeded 50% accuracy on Iranian-specific tasks (Iran-Law, Religion-Rules)
- GPT-4o achieved highest overall performance (57.23%) while PersianMind (35.08%) and Maral (34.71%) underperformed
- Persian Legals category consistently lowest across all models (37-53% even for top performers)
- Domain Specific Knowledge showed steep performance degradation for all models

## Why This Works (Mechanism)

### Mechanism 1: Cultural Localization Beyond Translation
- Claim: Direct translation of benchmarks fails to capture cultural knowledge; localization via context-aware term substitution improves evaluation validity.
- Mechanism: A multi-step agentic workflow identifies culturally-bound terms in English benchmarks, maps them to Persian cultural equivalents via a curated dictionary, then performs translation with localized context.
- Core assumption: Concepts like "Thanksgiving" or "baseball" have Iranian cultural analogs that better test cultural reasoning than direct transliteration.
- Evidence anchors: [abstract], [section 3.2], [corpus]

### Mechanism 2: Private Test Sets Reduce Contamination Inflation
- Claim: Keeping benchmark datasets private prevents models from achieving artificially inflated scores via training-data memorization.
- Mechanism: Original datasets are created from non-digital or obscure sources (e.g., exam PDFs, expert-crafted questions) and never publicly released, reducing likelihood of inclusion in pretraining corpora.
- Core assumption: Models trained on web-scale data have likely seen publicly available benchmarks like MMLU or ARC.
- Evidence anchors: [section 3.1], [section 3.1], [corpus]

### Mechanism 3: Log-Probability Aggregation for Multiple-Choice Evaluation
- Claim: Summing token log-probabilities across candidate options provides a discriminative signal for answer selection in instruction-tuned models.
- Mechanism: Each option is appended to the prompt; cumulative log-probability is computed per option; the highest-scoring option is selected as the prediction.
- Core assumption: Models assign higher probability mass to correct completions when conditioned on the question context.
- Evidence anchors: [section 4.1], [section 4.1], [corpus]

## Foundational Learning

- Concept: **Cultural Alignment vs. Linguistic Competence**
  - Why needed here: The benchmark explicitly separates Persian Linguistic tasks (grammar, homographs) from Persian Legals (law, religion), showing models can handle syntax while failing cultural knowledge.
  - Quick check question: If a model scores 85% on verb conjugation but 35% on Iranian legal reasoning, which capability is actually being evaluated?

- Concept: **Log-Probability Scoring for Discrete Choices**
  - Why needed here: Evaluation relies on comparing probability distributions, not generated text; understanding this is essential for implementing the harness correctly.
  - Quick check question: Why might two models generate identical text outputs yet have different log-probability rankings for the same options?

- Concept: **Data Contamination in Benchmarking**
  - Why needed here: Interpreting results requires understanding that high scores on public benchmarks may not reflect genuine capability if models memorized test data.
  - Quick check question: If a model achieves 95% on a benchmark released in 2021, what additional tests would confirm this isn't memorization?

## Architecture Onboarding

- **Component map:**
  - Dataset Layer (19 datasets: 13 original + 6 localized) -> Evaluation Harness (lm-evaluation-harness + vLLM/API support) -> Scoring Layer (Log-probability for MC, EM/F1 for generative) -> Model Coverage (41 models spanning GPT, Gemini, Qwen, Persian-specific families)

- **Critical path:**
  1. Dataset curation (expert consultation for law/religion, transcription from PDFs for Expert-Eval)
  2. Localization workflow (term identification → dictionary mapping → translation → human validation with κ=0.92)
  3. Evaluation execution (temp=0, zero-shot for MC, 3-shot for generation)
  4. Score aggregation across 6 categories

- **Design tradeoffs:**
  - Private datasets (reduce contamination) vs. reproducibility (external researchers cannot verify)
  - Numerical (1-4) vs. alphabetical (A-D) options: tested, <2% difference across model families
  - Zero-shot MC vs. few-shot generation: generation tasks require 3-shot to constrain output format

- **Failure signatures:**
  - Persian Legals category consistently lowest (37-53% even for GPT-4 models)
  - Persian fine-tuned models (PersianMind: 35.08% avg, Maral: 34.71%) underperform multilingual baselines
  - Domain Specific Knowledge shows steep degradation (e.g., GPT-4.1 drops to 42.61%)

- **First 3 experiments:**
  1. **Baseline sweep:** Run your model on all 6 categories; expect CSR > GK > PLing > PLeg based on published rankings.
  2. **Localization ablation:** Compare performance on translated vs. localized versions of adapted datasets (MMLU-pro, GSM) to quantify cultural adaptation benefit.
  3. **Scale correlation check:** Test 2-3 parameter sizes within your model family; verify positive correlation between parameter count and average Persian task performance as reported.

## Open Questions the Paper Calls Out

- **How do LLM capabilities and cultural alignment differ when evaluated on the broader Persian-speaking world (e.g., Dari in Afghanistan, Tajik in Tajikistan) compared to the specific Iranian context focused on in this study?**
  - Basis in paper: [explicit] The authors state in the Limitations section that "future work would benefit significantly from expanding this scope to include broader Persian-speaking communities" as the current findings are shaped strictly by the Iranian context.
  - Why unresolved: The MELAC benchmark datasets were curated exclusively from Iranian sources (e.g., Iranian entrance exams, laws, and literature), leaving the performance on other Persian dialects and cultural variants untested.
  - What evidence would resolve it: Expanding the benchmark to include datasets derived from Tajik and Afghan educational, legal, and cultural sources, followed by a comparative evaluation of the same model sets.

- **To what extent can current LLMs maintain coherence and cultural nuance in long-form text generation and open-ended dialogue in Persian, given that current benchmarks only assess multiple-choice or short-answer accuracy?**
  - Basis in paper: [explicit] The authors acknowledge in the Limitations that "Important skills such as long-form text generation and dialogue coherence in Persian were not assessed" by their multiple-choice and specific generative tasks.
  - Why unresolved: The study relied on accuracy, F1-score, and exact match metrics, which do not capture the "complete range of an LLMs capabilities" regarding sustained conversational context or narrative generation.
  - What evidence would resolve it: The development of new Persian-specific benchmarks utilizing human or model-based evaluation metrics (like chatbot arena rankings) focused specifically on multi-turn dialogue coherence and long-form essay writing.

- **What specific characteristics of pre-training data or fine-tuning methods cause models specifically fine-tuned for Persian (like Dorna or PersianMind) to fail to outperform general multilingual models on Iranian-specific cultural tasks?**
  - Basis in paper: [inferred] The authors note in the Results section that "even models fine-tuned with Persian corpora demonstrate notable performance limitations" and often perform worse than proprietary models like GPT-4o on cultural benchmarks (Iran-Law, Religion-Rules), but they do not analyze the root cause of this deficiency.
  - Why unresolved: The study quantifies the performance gap but does not investigate whether the failure is due to insufficient data volume, a lack of cultural diversity in the fine-tuning data, or catastrophic forgetting during instruction tuning.
  - What evidence would resolve it: An ablation study analyzing the cultural density of the training corpora used for these models and a comparison of performance before and after specific cultural instruction tuning.

## Limitations

- Private dataset nature prevents independent verification and limits reproducibility
- Localization methodology quality depends on dictionary completeness and annotator judgment
- Results may not generalize to other Persian-speaking regions with different cultural norms

## Confidence

- **High confidence:** Cultural localization improves benchmark validity (κ=0.92 human validation)
- **Medium confidence:** Private datasets reduce contamination inflation (reasonable but untested)
- **Low confidence:** Log-probability method effectiveness across languages (weak corpus support)

## Next Checks

1. **Localization ablation study:** Run controlled experiment comparing model performance on translated vs. localized versions of same English benchmarks to quantify cultural adaptation benefit with statistical significance.

2. **Contamination detection:** Analyze training data overlap by checking whether source materials (Iranian exam PDFs, legal documents) appear in model training corpora through n-gram matching or other techniques.

3. **Cross-linguistic log-probability validation:** Test log-probability scoring method on multiple languages with known cultural distances from English to determine if tokenizer behavior systematically affects results.