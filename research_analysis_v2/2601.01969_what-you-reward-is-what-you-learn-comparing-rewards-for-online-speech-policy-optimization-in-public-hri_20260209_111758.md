---
ver: rpa2
title: 'What you reward is what you learn: Comparing rewards for online speech policy
  optimization in public HRI'
arxiv_id: '2601.01969'
source_url: https://arxiv.org/abs/2601.01969
tags:
- reward
- robot
- user
- success
- interaction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how different reward definitions steer
  online learning of speech policies in a public-facing service robot. Using Thompson
  sampling over six speech rate/verbosity combinations, the robot was deployed for
  12 days with over 1,400 encounters.
---

# What you reward is what you learn: Comparing rewards for online speech policy optimization in public HRI

## Quick Facts
- arXiv ID: 2601.01969
- Source URL: https://arxiv.org/abs/2601.01969
- Reference count: 40
- Primary result: Different binary reward definitions (user rating, conversation closure, engagement) lead to distinct learned speech policies in a public service robot

## Executive Summary
This study investigates how different reward definitions steer online learning of speech policies in a public-facing service robot. Using Thompson sampling over six speech rate/verbosity combinations, the robot was deployed for 12 days with over 1,400 encounters. Three binary rewards were tested: user rating, conversation closure, and at least two turns. Each reward led to a distinct learned arm distribution—user rating favored Normal-Concise and Normal-Detailed, conversation closure favored Normal-Concise, and engagement (two turns) favored Slow-Concise and Fast-Detailed. Generalized linear models showed that social context, such as group size and user motivation, further moderated these effects. The results demonstrate that reward design fundamentally shapes learned behavior, highlighting the need for context-aware online optimization in human-robot interaction.

## Method Summary
The study deployed a service robot in a shopping mall using Thompson Sampling to optimize six speech policies (combinations of rate × verbosity). Over 12 days, the robot conducted 1,400+ interactions, with a 30-interaction cold start followed by 450 interactions per condition across three binary reward definitions: user rating ≥6, conversation closure, and ≥2 user turns. Thompson Sampling with Beta-Bernoulli updates was used to learn optimal policies online, while GLM analysis examined how social context (group size, crowd level, motivation) moderated policy effectiveness.

## Key Results
- User rating reward favored Normal-Concise and Normal-Detailed speech policies
- Conversation closure reward favored Normal-Concise speech policy
- Engagement reward (two turns) favored Slow-Concise and Fast-Detailed speech policies
- Social context (group size, crowd level, motivation) significantly moderated policy effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The operational definition of the reward signal causally dictates the convergence of the robot's speech policy, causing distinct behavioral modes (satisfaction-oriented vs. efficiency-oriented vs. engagement-oriented).
- **Mechanism:** Thompson sampling estimates the posterior probability of success P(reward | arm). Because the three rewards measure orthogonal constructs—subjective satisfaction, task closure, and engagement—the algorithm identifies different arms as "optimal" depending solely on which signal it maximizes.
- **Core assumption:** The reward signal is a valid proxy for the intended operational goal.
- **Evidence anchors:** Abstract showing distinct learned arm distributions; section 4.2 citing systematic behavioral alterations from different feedback mixes.
- **Break condition:** If rewards were highly correlated, learned arm distributions would converge to a single policy.

### Mechanism 2
- **Claim:** Social context (group size, motivation, crowd level) moderates the probability of success for specific speech policies, implying that a global "best" arm is statistically suboptimal.
- **Mechanism:** GLMs revealed significant interaction terms (e.g., Arm × Group). The utility of an arm is conditional on state—verbose responses perform well for single users but fail in crowded or multi-party contexts.
- **Core assumption:** Interaction effects are stable regularities of HRI in public spaces, not artifacts of the specific sample.
- **Evidence anchors:** Abstract mentioning context moderation; Table 2 showing significant ND:grp_single_T and ND:crowd_T interactions.
- **Break condition:** If context detection fails or is noisy, the system cannot condition the policy, potentially performing worse than context-agnostic approaches.

### Mechanism 3
- **Claim:** Thompson Sampling enables online adaptation in non-stationary environments by stochastically exploring arms with high posterior uncertainty, preventing premature convergence to local optima.
- **Mechanism:** TS samples a success probability θ̃ₐ from the Beta posterior for each arm. Arms with high variance occasionally sample high values, forcing exploration. This allows recovery if the optimal arm changes due to shifting user demographics.
- **Core assumption:** The environment is stationary enough within short windows for estimates to be meaningful.
- **Evidence anchors:** Section 2.1 explaining principled exploration; section 4.3 noting essentiality of non-stationarity models for long-term deployments.
- **Break condition:** If cold start uniform allocation is insufficient or early data is noisy, posterior may form incorrect priors that take excessive samples to correct.

## Foundational Learning

- **Concept:** **Thompson Sampling (Bayesian Bandits)**
  - **Why needed here:** Core algorithm solving exploration-exploitation trade-off without complex state model. Understanding Beta-Bernoulli updates is required to interpret posterior arm distributions.
  - **Quick check question:** If an arm has been pulled 10 times with 0 successes, and another 100 times with 50 successes, which one might TS still select next and why?

- **Concept:** **Reward Operationalization (Proxy Design)**
  - **Why needed here:** The paper's central thesis is that "what you reward is what you learn." Mapping abstract goals into binary signals is critical for reproducing or critiquing the study.
  - **Quick check question:** Why did authors choose threshold of ≥6 for Rᵤ (User Rating) instead of ≥5?

- **Concept:** **Generalized Linear Models (GLM) with Interactions**
  - **Why needed here:** Paper moves beyond simple "which arm is best" to "which arm is best when." Understanding GLM interaction terms is required to extract design lessons for context-aware systems.
  - **Quick check question:** In Table 2, the main effect of grp_single_T is negative, but the interaction ND:grp_single_T is positive. What does this mean for a robot deploying the ND arm to a single user?

## Architecture Onboarding

- **Component map:** Sota Robot (Hardware) -> LLM/Dialog Manager (Software) -> TTS + Prompt Engineering (6 Arms) -> Thompson Sampling Module (Updates Beta posteriors) -> Reward Signal Source (Tablet Survey, Video Monitoring UI) -> PoseNet + STT Sensors

- **Critical path:** The latency of the Reward Signal. System relies on immediate feedback after interaction. Path 1: Interaction End → User Tablet Rating → API Call → Bandit Update. Path 2: Interaction End → Annotator Judgement → API Call → Bandit Update. Bottleneck: Human-in-the-loop annotation requires real-time monitoring staff; automated detection of "closure" or "turns" would be necessary for scaling.

- **Design tradeoffs:**
  - Binary vs. Continuous Rewards: Authors chose binary rewards for sample efficiency and simplicity, but this compresses nuance.
  - Cold Start: Uniform pre-allocation guarantees safety but wastes 30 interactions on potentially poor policies.
  - Context: Current system uses post-hoc GLM to find context. Production system would need online Contextual Bandits, increasing complexity and data requirements.

- **Failure signatures:**
  - Posterior Collapse: An arm's variance collapses to near-zero prematurely, causing system to stop exploring better alternatives.
  - Sparsity Errors: GLM coefficients becoming non-estimable because specific arm-context combinations are never sampled.
  - Reward Hacking: Robot learns policy that maximizes proxy but violates social norms.

- **First 3 experiments:**
  1. Offline Replay (Safety Check): Re-run collected dataset through TS algorithm with simulated delay or noise in reward signal to test robustness before live deployment.
  2. Cold Start Ablation: Compare current uniform initialization against informed prior (based on previous day's data) to measure convergence speed improvements.
  3. Contextual Feature Drop-in: Implement simple Contextual Bandit (e.g., LinUCB or Contextual TS) using just one significant feature (e.g., Group Size: Solo vs. Group) to validate if online context switching outperforms global average policy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How should multiple reward signals (e.g., satisfaction, closure, engagement) be combined into a single objective function for online learning?
- Basis in paper: Introduction lists "How should they be combined?" as central open question while study only evaluates rewards in isolation.
- Why unresolved: Paper compared three distinct binary rewards separately but did not investigate multi-objective optimization or weighted reward aggregation.
- What evidence would resolve it: Deployment comparing single-reward bandits against composite-reward bandit (e.g., weighted sum) to analyze trade-offs between satisfaction and efficiency.

### Open Question 2
- Question: Does incorporating non-stationarity handling (e.g., discounting past data) improve policy adaptation during long-term, in-situ deployments?
- Basis in paper: Section 4.3 states models that handle non-stationarity "are essential" as best policy might shift over time, yet study used static model.
- Why unresolved: Authors utilized standard Thompson sampling setup (λ=1) and did not analyze how optimal arms might have drifted over 12-day period.
- What evidence would resolve it: Time-series analysis of arm success rates showing significant temporal drift, or comparison of static vs. sliding-window bandit algorithms.

### Open Question 3
- Question: Do continuous or composite reward signals improve policy nuance compared to binary thresholds used in this study?
- Basis in paper: Section 4.4 notes binary rewards "may compress nuance" and suggests future work explore "alternative thresholds and... continuous or composite outcomes."
- Why unresolved: All learning relied on thresholded binary outcomes (r ∈ {0,1}), potentially discarding granular information from 7-point rating scale.
- What evidence would resolve it: Experiment comparing learning convergence and user satisfaction between robots trained on binary vs. continuous reward signals.

## Limitations

- Binary reward formulation may oversimplify complex human preferences, potentially leading to reward hacking where robot maximizes proxy at expense of genuine satisfaction.
- Context detection relies on human annotation for implicit rewards, creating scalability bottleneck and potential subjectivity in determining conversation closure and engagement.
- 12-day deployment period may not capture longer-term effects of non-stationary environments or changing user demographics.

## Confidence

- **High confidence**: The core finding that different reward definitions lead to distinct learned arm distributions is well-supported by experimental data.
- **Medium confidence**: The GLM analysis showing context moderation effects is methodologically sound, though specific numerical coefficients may be sensitive to video annotation process.
- **Medium confidence**: The claim that Thompson Sampling prevents premature convergence is theoretically justified, but actual exploration-exploitation balance depends on specific reward signal characteristics.

## Next Checks

1. Implement automated detection of conversation closure and engagement metrics to remove human annotation bottleneck and enable continuous online operation.
2. Conduct follow-up study with longer deployment duration (e.g., 4+ weeks) to assess how well learned policies generalize across different user demographics and seasonal variations.
3. Test learned policies in controlled lab setting with think-aloud protocols to understand qualitative user experience differences between three reward-driven behavioral modes.