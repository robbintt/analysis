---
ver: rpa2
title: Encoder-Decoder or Decoder-Only? Revisiting Encoder-Decoder Large Language
  Model
arxiv_id: '2510.26622'
source_url: https://arxiv.org/abs/2510.26622
tags:
- redllm
- decllm
- flops
- pretraining
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive comparative analysis of encoder-decoder
  (RedLLM) and decoder-only (DecLLM) large language models across model scales from
  150M to 8B parameters. The study addresses the lack of rigorous scaling perspective
  comparisons between these architectures, which have dominated LLM research in recent
  years.
---

# Encoder-Decoder or Decoder-Only? Revisiting Encoder-Decoder Large Language Model

## Quick Facts
- arXiv ID: 2510.26622
- Source URL: https://arxiv.org/abs/2510.26622
- Authors: Biao Zhang; Yong Cheng; Siamak Shakeri; Xinyi Wang; Min Ma; Orhan Firat
- Reference count: 40
- Primary result: Comprehensive comparative analysis of encoder-decoder (RedLLM) and decoder-only (DecLLM) large language models across scales from 150M to 8B parameters, showing distinct strengths during pretraining versus instruction tuning phases

## Executive Summary
This paper provides a comprehensive comparative analysis of encoder-decoder (RedLLM) and decoder-only (DecLLM) large language models across model scales from 150M to 8B parameters. The study addresses the lack of rigorous scaling perspective comparisons between these architectures, which have dominated LLM research in recent years. Through extensive pretraining on RedPajama V1 (1.6T tokens) and instruction tuning on FLAN, the research reveals that while decoder-only models dominate the compute-optimal frontier during pretraining, encoder-decoder models demonstrate comparable scaling properties and context length extrapolation capabilities, with substantially better inference efficiency after finetuning.

## Method Summary
The study pretrains both RedLLM and DecLLM architectures on RedPajama V1 (1.6T tokens) using prefix language modeling for RedLLM and causal language modeling for DecLLM, followed by instruction tuning on FLAN. Both architectures incorporate modern techniques including rotary positional embeddings with continuous positions and layer normalization improvements. Models range from 150M to 8B parameters, with 400K pretraining steps using Adafactor optimizer and 190K instruction tuning steps. Evaluation includes perplexity on in-domain and out-of-domain data, plus zero/few-shot accuracy on 13 downstream tasks.

## Key Results
- Decoder-only models achieve better compute-optimal scaling during pretraining due to higher parameter utilization per training FLOP
- Encoder-decoder models show comparable scaling properties and superior context length extrapolation capabilities
- After instruction tuning, encoder-decoder models achieve comparable and sometimes better downstream task performance with substantially better inference efficiency
- RedLLM scales poorly for zero-shot learning during pretraining but adapts well after finetuning, suggesting different architectures excel at different training phases

## Why This Works (Mechanism)

### Mechanism 1: Training Objective-Compute Alignment
- Claim: Decoder-only models with causal LM training achieve better compute-optimal scaling because the objective aligns more directly with token prediction across the full sequence, while encoder-decoder models trained with prefix LM waste compute on non-causal encoder representations during pretraining.
- Mechanism: Causal LM trains every token position to predict the next token, maximizing parameter utilization per training FLOP. Prefix LM splits the sequence, with the encoder processing only the prefix bidirectionally—effectively training fewer token positions per batch.
- Core assumption: Token-level prediction density correlates with downstream transfer quality during pretraining.
- Evidence anchors: Section 5 shows DecLLM is overall more compute-optimal during pretraining with higher efficiency in utilizing training tokens; Figure 3 shows DecLLM dominates the Pareto frontier at larger compute budgets.

### Mechanism 2: Bidirectional Encoder Representations Enable Finetuning Adaptation
- Claim: The bidirectional attention in RedLLM's encoder creates richer input representations that can be rapidly adapted during instruction tuning, explaining why RedLLM catches up despite weaker pretraining zero-shot performance.
- Mechanism: Bidirectional attention allows each input token to attend to all other input tokens, capturing dependencies that causal masking prohibits. During finetuning, this representation flexibility can be repurposed for task-specific patterns without relearning input understanding from scratch.
- Core assumption: Instruction tuning primarily optimizes output generation conditioned on fixed input representations rather than jointly learning input understanding and generation.
- Evidence anchors: Section 6 shows DecLLM + BiAttn largely outperforms RedLLM at zero-shot learning when bidirectional attention is added to DecLLM; RedLLM matches DecLLM after finetuning even under the same amount of model parameters.

### Mechanism 3: Attention Locality Preservation Enables Length Extrapolation
- Claim: RedLLM maintains more stable attention locality patterns at longer sequence lengths, which slows perplexity degradation beyond training context windows.
- Mechanism: In DecLLM, decoder self-attention shows "locality decay"—tokens increasingly attend uniformly as position increases. RedLLM's cross-attention distributes weights across a consistent subset of encoder tokens, potentially preserving useful signal at longer contexts.
- Core assumption: Local attention patterns are critical for maintaining token prediction quality as sequence length increases.
- Evidence anchors: Section 5 Figure 6 shows DecLLM suffers sharper "locality decay" in self-attention weights; RedLLM shows a surprisingly smoother increase in PPL beyond 4K tokens.

## Foundational Learning

- **Concept: Prefix Language Modeling**
  - Why needed here: This is RedLLM's pretraining objective—understanding how it differs from causal LM is essential for interpreting scaling comparisons.
  - Quick check question: Given a 2048-token sequence split at position 1024, which tokens receive gradient updates in prefix LM vs causal LM?

- **Concept: Cross-Attention Mechanics**
  - Why needed here: RedLLM's cross-attention (encoder-to-decoder) differs fundamentally from decoder-only self-attention; attention weight patterns explain extrapolation behavior.
  - Quick check question: In cross-attention, what determines the key and query sources, and how does this differ from decoder self-attention?

- **Concept: Compute-Optimal Scaling**
  - Why needed here: The paper's central comparison uses FLOP-based Pareto frontiers rather than simple parameter counts.
  - Quick check question: If Model A requires 2× FLOPs but achieves 10% lower perplexity than Model B at the same parameter count, which is compute-optimal?

## Architecture Onboarding

- **Component map:**
  RedLLM: Encoder (bidirectional self-attn) → Decoder (causal self-attn + cross-attn from encoder)
  DecLLM: Single Transformer (causal self-attn over full sequence)
  
  Shared components: RMSNorm, SwiGLU FFN, Rotary embeddings (continuous position), Tied embeddings
  RedLLM-specific: Extra LayerNorm on Q, K, V, and attention output for stability

- **Critical path:**
  1. Position encoding continuity: Decoder positions must continue from encoder's last position (not reset to 0)
  2. Attention normalization: Apply RMSNorm to Q, K, V vectors before computing attention scores
  3. Prefix split: Input sequence split at fixed point (1024 tokens); encoder processes prefix, decoder generates continuation

- **Design tradeoffs:**
  - Inference efficiency vs pretraining compute: RedLLM offers 2× better inference throughput (encoder can cache KV for input once) but requires more pretraining FLOPs for equivalent perplexity
  - Zero-shot prompting vs finetuning performance: DecLLM wins at zero-shot during pretraining; RedLLM catches up after instruction tuning
  - Stability vs simplicity: RedLLM requires extra normalization layers; DecLLM trains more stably with simpler architecture

- **Failure signatures:**
  - Training instability with attention logit explosion → Missing Q/K normalization (verify LN applied before softmax)
  - Poor zero-shot performance with RedLLM during pretraining → Expected behavior; do not debug as training failure
  - Sharp perplexity increase beyond 4K context in DecLLM → Locality decay; consider switching to RedLLM for long-context applications

- **First 3 experiments:**
  1. Baseline scaling replication: Train 150M and 1B variants of both architectures on a 10B-token subset; verify DecLLM achieves lower perplexity per FLOP and RedLLM catches up after 50K-step instruction tuning
  2. Bidirectional attention ablation: Enable bidirectional attention on input prefix for DecLLM during finetuning only; measure zero-shot vs few-shot gap reduction
  3. Context extrapolation stress test: Evaluate both 4B models on sequences from 2K to 16K tokens; plot perplexity curves and attention locality metrics to verify RedLLM's smoother degradation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the comparable scaling properties and finetuning advantages of RedLLM persist when scaling model parameters significantly beyond 8B?
- Basis in paper: Section 7 explicitly lists "exploring the scalability of RedLLM beyond 8B parameters" as a primary interest for future work.
- Why unresolved: The experimental scope was limited to 150M–8B parameters; scaling laws can shift unpredictably at much larger scales (e.g., 70B+).
- What evidence would resolve it: Pretraining and instruction-tuning benchmarks for RedLLM models scaled to 70B+ parameters compared against compute-equivalent decoder-only baselines.

### Open Question 2
- Question: Does adopting an imbalanced architecture, specifically a deep encoder and shallow decoder, improve the quality-efficiency trade-off?
- Basis in paper: Section 7 identifies investigating imbalanced architectures, as opposed to the balanced ones used in the study, as a "compelling avenue."
- Why unresolved: The study utilized equal layer counts for encoder and decoder, leaving the potential efficiency gains of asymmetric configurations untested.
- What evidence would resolve it: Comparative analysis of inference speed and downstream accuracy for RedLLM variants with varying encoder-to-decoder depth ratios.

### Open Question 3
- Question: What is the precise mechanism that enables RedLLM's superior context length extrapolation compared to DecLLM?
- Basis in paper: Section 7 notes that the "underlying reason behind RedLLM's long context behavior remains unclear," despite observing its smoother perplexity curves.
- Why unresolved: The paper visualizes attention patterns (e.g., locality decay) but does not isolate the specific architectural component responsible for the robustness.
- What evidence would resolve it: Ablation studies isolating the impact of continuous positional embeddings and cross-attention on extrapolation performance.

## Limitations

- The study incorporates multiple modern design choices (RMSNorm, rotary embeddings, continuous positions) that may interact with the encoder-decoder structure in ways not fully isolated from the core architectural comparison
- Pretraining evaluation focuses primarily on perplexity metrics without examining other downstream capabilities like structured reasoning or multi-modal adaptation
- The instruction tuning phase uses a single dataset (FLAN) without exploring whether architecture-specific advantages transfer to other fine-tuning approaches or task distributions

## Confidence

- **High confidence:** Compute-optimal scaling behavior during pretraining favoring DecLLM, and the observation that RedLLM catches up after instruction tuning
- **Medium confidence:** The mechanism linking bidirectional encoder representations to improved finetuning adaptation, as this requires assumptions about how instruction tuning utilizes input representations
- **Medium confidence:** Context length extrapolation advantages for RedLLM, as the attention locality analysis provides suggestive but not definitive evidence

## Next Checks

1. **Architecture ablation study:** Train RedLLM variants removing RMSNorm and rotary embeddings to isolate whether the encoder-decoder advantage persists with more standard architectural choices
2. **Alternative pretraining objectives:** Evaluate both architectures using UL2-style mixture of prefix and causal LM objectives to test whether compute advantages shift under different training paradigms
3. **Long-context capability validation:** Design tasks specifically requiring extended context integration (beyond simple perplexity measurement) to verify RedLLM's claimed extrapolation benefits translate to practical performance gains