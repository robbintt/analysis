---
ver: rpa2
title: Lessons From Red Teaming 100 Generative AI Products
arxiv_id: '2501.07238'
source_url: https://arxiv.org/abs/2501.07238
tags:
- teaming
- system
- these
- safety
- security
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Microsoft\u2019s AI Red Team red-teamed over 100 generative AI\
  \ products, developing a threat model ontology to guide testing for both security\
  \ and responsible AI harms. They found that simple, system-level attacks often work\
  \ better than complex, gradient-based methods, and that human judgment remains crucial\
  \ despite automation tools like PyRIT."
---

# Lessons From Red Teaming 100 Generative AI Products

## Quick Facts
- **arXiv ID:** 2501.07238
- **Source URL:** https://arxiv.org/abs/2501.07238
- **Reference count:** 40
- **Key outcome:** Microsoft's AI Red Team found simple, system-level attacks often outperform gradient-based methods, human judgment is crucial for RAI harms, and red teaming is distinct from safety benchmarking.

## Executive Summary
Microsoft's AI Red Team red-teamed over 100 generative AI products, developing a threat model ontology to guide testing for both security and responsible AI harms. They found that simple, system-level attacks often work better than complex, gradient-based methods, and that human judgment remains crucial despite automation tools like PyRIT. Their work shows AI red teaming is distinct from safety benchmarking, especially as novel harms emerge and context matters. They emphasize continuous testing, collaboration with subject matter experts, and the need to address both existing and AI-specific vulnerabilities.

## Method Summary
The team used the AIRT Threat Model Ontology (System → Actor → TTPs → Weakness → Impact) to structure operations, starting by defining downstream impacts based on system capabilities. They prioritized simple, system-level manual attacks (e.g., prompt engineering) over gradient-based optimization, augmented by PyRIT for scaling. Testing covered both security vulnerabilities (e.g., SSRF, RCE) and Responsible AI harms (e.g., bias, psychological impact), with human SMEs providing judgment for subjective assessments.

## Key Results
- Simple, system-level attacks often succeed where gradient-based optimization fails
- Human judgment provides irreplaceable signal in assessing subjective, context-dependent harms
- Red teaming surfaces novel harms that static benchmarks miss due to evolving capabilities and contexts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Simple, system-level attacks often succeed where gradient-based optimization fails
- Mechanism: Production AI systems expose multiple non-model surfaces (input filters, databases, tools, APIs) that lack the mathematical structure required for gradient computation. Attackers exploit this by chaining low-complexity techniques—prompt injections, encoding tricks, social engineering—across system boundaries rather than optimizing against model weights directly.
- Core assumption: Real adversaries prefer low-cost, transferable attacks over computationally expensive, model-specific optimization.
- Evidence anchors:
  - [abstract]: "simple, system-level attacks often work better than complex, gradient-based methods"
  - [Lesson 2]: "In our red teaming operations, we have also found that 'basic' techniques often work just as well as, and sometimes better than, gradient-based methods"
  - [corpus]: Related papers (BlackIce, Quality-Diversity Red-Teaming) emphasize automated attack generation frameworks, but do not contradict the finding that simple attacks remain effective
- Break condition: If future systems achieve unified differentiability across all components, gradient-based attacks may become more viable.

### Mechanism 2
- Claim: Human judgment provides irreplaceable signal in assessing subjective, context-dependent harms
- Mechanism: Responsible AI harms (bias, psychological impact, cultural offense) require interpretation grounded in social norms, domain expertise, and emotional intelligence. Automated scorers can detect explicit policy violations but fail to evaluate nuance, emerging harm categories, or cross-cultural context without human-defined rubrics.
- Core assumption: Harm is not purely syntactic or statistical; it depends on deployment context, user vulnerability, and societal norms.
- Evidence anchors:
  - [Lesson 5]: "Automation like PyRIT can support red teaming operations... [but] should not be used with the intention of taking the human out of the loop"
  - [Lesson 6]: "RAI harms are pervasive, but unlike most security vulnerabilities, they are subjective and difficult to measure"
  - [corpus]: Neighbor papers on automated red teaming (AutoRedTeamer, RedCodeAgent) focus on coverage, not harm interpretation
- Break condition: If formal harm taxonomies achieve complete, cross-cultural consensus, automation may reduce—but not eliminate—human oversight needs.

### Mechanism 3
- Claim: Red teaming surfaces novel harms that static benchmarks miss because capabilities and contexts co-evolve
- Mechanism: Benchmarks codify past harm definitions; red teaming probes for emergent risks arising from new model capabilities (e.g., persuasion, agentic tool use) and novel deployment contexts (e.g., healthcare, multilingual markets). This creates a discovery loop where red team findings inform new benchmarks.
- Core assumption: Novel capabilities and deployment contexts generate risk categories not yet formalized.
- Evidence anchors:
  - [Lesson 3]: "AI red teaming is not safety benchmarking... benchmarks measure preexisting notions of harm"
  - [Lesson 1]: "As models get bigger, they tend to acquire new capabilities... [which] can also introduce attack vectors"
  - [corpus]: OpenAI's red teaming approach paper similarly emphasizes discovering novel risks
- Break condition: If model capabilities plateau and deployment patterns stabilize, benchmark coverage may converge with red team findings.

## Foundational Learning

- Concept: **Threat model ontology (System, Actor, TTPs, Weakness, Impact)**
  - Why needed here: The paper uses this ontology to structure every operation; understanding it is prerequisite to reading their case studies or applying their methodology.
  - Quick check question: Given a chatbot that can email on behalf of users, can you map a phishing scenario to Actor → Tactic → Weakness → Impact?

- Concept: **Security vs. Responsible AI (RAI) harm categories**
  - Why needed here: AIRT explicitly separates these (see Figure 2) and assigns different probing/scoring methods; conflation leads to incomplete testing.
  - Quick check question: Is "model outputs private user data via prompt injection" a security or RAI issue? (Trick: it's both—security impact + potential RAI dimension)

- Concept: **Break-fix cycle**
  - Why needed here: The paper argues AI security is never "solved"; iterative red team–mitigate–retest loops are the operational norm.
  - Quick check question: Why might a mitigation introduce new vulnerabilities? (Hint: input filters can be bypassed; stricter refusals can degrade utility)

## Architecture Onboarding

- Component map:
  PyRIT framework -> AIRT ontology -> External taxonomies (MITRE ATT&CK, ATLAS, Microsoft RAI Standard)

- Critical path:
  1. Scope operation by downstream impact and deployment context (Lesson 1)
  2. Enumerate plausible Actors (adversarial and benign)
  3. Select TTPs from known catalogs and novel probes
  4. Execute attacks manually and via PyRIT automation
  5. Score results (automated for clear-cut harms, SME review for ambiguous)
  6. Report weaknesses and collaborate with product teams on mitigations

- Design tradeoffs:
  - Coverage vs. depth: Automation scales coverage; human probing finds deeper, novel issues
  - Gradient-based vs. prompt-based attacks: Gradient methods are powerful but often infeasible on production APIs
  - Standardized vs. context-specific probes: Benchmarks enable comparison; custom probes surface deployment-specific risks

- Failure signatures:
  - Only testing model, not end-to-end system → misses infrastructure, plugin, and data-flow vulnerabilities
  - Treating red teaming as one-time compliance check → misses regressions and novel attacks
  - Over-relying on LLM-as-judge for RAI scoring → misses cultural, psychological, and domain-specific nuance

- First 3 experiments:
  1. Run a known jailbreak (e.g., base64-encoded prompt) against a text-only endpoint, then a multimodal endpoint—compare refusal rates to surface modality-specific gaps.
  2. Probe a RAG-enabled copilot with cross-prompt injection (XPIA) payloads hidden in retrieved documents; measure data exfiltration success.
  3. Generate a set of occupation-related prompts for a text-to-image model without specifying gender; manually label output distributions to surface representation bias.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How should red teams effectively probe for dangerous capabilities in LLMs, such as persuasion, deception, and replication?
- Basis in paper: [explicit] The authors explicitly ask, "how should we probe for dangerous capabilities in LLMs such as persuasion, deception, and replication?"
- Why unresolved: These are complex, emergent behaviors that lack standardized testing methodologies compared to traditional security vulnerabilities.
- What evidence would resolve it: Development and validation of robust benchmarks or probing frameworks specifically designed to detect and quantify these nuanced capabilities.

### Open Question 2
- Question: What novel risks must be probed in video generation models and future state-of-the-art architectures?
- Basis in paper: [explicit] Section 3 asks, "what novel risks should we probe for in video generation models and what capabilities may emerge in models more advanced than the current state-of-the-art?"
- Why unresolved: The technology is evolving faster than safety taxonomies, and new modalities introduce failure modes not present in text-only models.
- What evidence would resolve it: A comprehensive taxonomy of video-specific harms and empirical studies identifying unique attack vectors in advanced generative systems.

### Open Question 3
- Question: How can red teaming practices be translated into diverse linguistic and cultural contexts?
- Basis in paper: [explicit] The authors ask, "how do we translate existing AI red teaming practices into different linguistic and cultural contexts?"
- Why unresolved: Current safety research is predominantly Western-centric, and harms often manifest differently across political and cultural boundaries.
- What evidence would resolve it: Successful open-source initiatives that leverage diverse cultural expertise to redefine harms and robustness for non-English languages.

### Open Question 4
- Question: In what ways should AI red teaming practices be standardized to facilitate clear communication of findings?
- Basis in paper: [explicit] The paper asks, "In what ways should AI red teaming practices be standardized so that organizations can clearly communicate their methods and findings?"
- Why unresolved: While ontologies exist (like the one proposed in the paper), they can be restrictive, and the field lacks universal reporting standards.
- What evidence would resolve it: Industry-wide adoption of modular tools that enable organizations to summarize, track, and communicate vulnerabilities consistently.

## Limitations

- Specific quantitative success rates or vulnerability counts from the 100+ products are not disclosed, limiting reproducibility assessment.
- Exact prompt datasets and probe configurations used for the 100+ products are not fully specified, which may affect replication fidelity.
- Guidelines for SME evaluation of psychosocial harms are noted as "still being developed," suggesting the methodology for subjective harm assessment is evolving.

## Confidence

- **High Confidence:** The distinction between AI red teaming and safety benchmarking is well-supported by the literature and aligns with industry practice.
- **Medium Confidence:** The assertion that human judgment is irreplaceable for RAI harm assessment is logically sound but depends on the evolving state of automated harm detection tools.
- **Medium Confidence:** The finding that novel harms emerge as models acquire new capabilities is plausible but requires longitudinal data to quantify the rate and nature of such discoveries.

## Next Checks

1. Replicate the core experiment by running a simple jailbreak (e.g., base64 encoding) on a production LLM endpoint, then test with a multimodal model, comparing refusal rates to validate modality-specific gaps.
2. Apply the AIRT ontology to a new generative AI system (e.g., a RAG-enabled copilot) by defining an Actor, selecting TTPs, and executing manual and automated attacks via PyRIT to assess coverage and effectiveness.
3. Conduct a cross-cultural harm assessment by generating occupation-related prompts for a text-to-image model, manually labeling output distributions, and comparing results to automated bias detection tools to evaluate the necessity of human judgment.