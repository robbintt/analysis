---
ver: rpa2
title: 'From Continual Learning to SGD and Back: Better Rates for Continual Linear
  Models'
arxiv_id: '2504.04579'
source_url: https://arxiv.org/abs/2504.04579
tags:
- forgetting
- continual
- evron
- then
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper tackles catastrophic forgetting in over\u2011parameterized\
  \ continual learning, where a linear model is trained sequentially on many jointly\
  \ realizable tasks. By proving that fitting each task is exactly one stochastic\u2011\
  gradient\u2011descent step on a suitably modified least\u2011squares objective,\
  \ the authors derive new last\u2011iterate SGD bounds and translate them into forgetting\
  \ guarantees."
---

# From Continual Learning to SGD and Back: Better Rates for Continual Linear Models  

## Quick Facts  
- **arXiv ID:** 2504.04579  
- **Source URL:** https://arxiv.org/abs/2504.04579  
- **Reference count:** 40  
- **Primary result:** Universal forgetting bounds for over‑parameterized continual linear models that improve prior rates and hold for random task orderings (with and without replacement).  

## Executive Summary  
The paper reframes continual learning of linear tasks as a single‑step stochastic‑gradient‑descent (SGD) update on a modified least‑squares objective. This reduction enables the authors to apply recent last‑iterate SGD analyses and obtain explicit forgetting guarantees that decay as \(O(k^{-1/4})\) (or faster depending on problem geometry). Random task orderings—both with and without replacement—are shown to be sufficient to prevent catastrophic forgetting, even in arbitrarily long sequences. The analysis also yields a universal bound for projection‑onto‑convex‑sets (POCS) methods, linking continual learning to classic Kaczmarz and block‑Kaczmarz algorithms.  

## Method Summary  
The authors consider a stream of \(T\) jointly realizable linear regression or separable‑classification tasks. For each incoming task they compute the minimum‑norm solution via the pseudoinverse and project the current weight vector onto the corresponding solution subspace. Algebraically this projection equals one SGD step on a specially crafted least‑squares loss, allowing the use of spectral analysis of the expected projection operator \(Q\). By bounding the eigenvalues of \(Q\) under random sampling (with/without replacement) they derive closed‑form forgetting rates that depend only on the ambient dimension \(d\), the average task rank \(\bar r\), and the number of updates \(k\). The same machinery yields a residual bound for POCS and related block‑Kaczmarz schemes.  

## Key Results  
- **Universal forgetting bound (with replacement):** \(O\!\big(\min\{k^{-1/4},\sqrt{d-\bar r}/k,\sqrt{T\bar r}/k\}\big)\), improving the previous \(O((d-\bar r)/k)\).  
- **First‑of‑its‑kind bound (without replacement):** \(O\!\big(\min\{T^{-1/4},(d-\bar r)/T\}\big)\), showing random ordering alone mitigates forgetting in long sequences.  
- **Separable‑classification rate:** Matching \(O(k^{-1/4})\) forgetting.  
- **POCS residual bound:** \(\le 7\,k^{-1/4}\min_{w\in C^\star}\|w_0-w\|^2\), extending to block‑Kaczmarz methods.  

## Why This Works (Mechanism)  
By proving that fitting each task to its minimum‑norm solution is exactly one SGD step on a modified least‑squares loss, the continual learning dynamics become amenable to modern SGD convergence theory. The random task ordering induces an expected projection operator \(Q\) whose spectral properties control the contraction of the error. When tasks are jointly realizable, each projection preserves the common solution, and the stochastic nature of the ordering ensures that the eigenvalues of \(Q\) are bounded away from 1, yielding the \(k^{-1/4}\) decay. The same reasoning applies to POCS and Kaczmarz updates, unifying these classic algorithms under the SGD lens.  

## Foundational Learning  
- **Stochastic Gradient Descent (SGD)**  
  - *Why needed:* The core reduction treats each task update as a single SGD step; understanding last‑iterate versus averaged convergence is essential for the derived forgetting rates.  
  - *Quick check:* Explain the difference between convergence guarantees for the *average* of SGD iterates and the *last* iterate.  

- **Projections onto Convex Sets (POCS) / Kaczmarz Method**  
  - *Why needed:* The continual update is mathematically identical to a Kaczmarz projection; familiarity with this classic algorithm clarifies the “single‑step SGD” claim.  
  - *Quick check:* In solving a linear system, how does the Kaczmarz method modify the current estimate to satisfy one equation?  

- **Catastrophic Forgetting & Joint Realizability**  
  - *Why needed:* The analysis assumes a single weight vector can perfectly solve all tasks, eliminating contradictory information and simplifying the forgetting analysis.  
  - *Quick check:* Why does joint realizability rule out directly conflicting tasks, and how does this simplify the theoretical treatment of forgetting?  

- **Spectral Analysis of Linear Operators**  
  - *Why needed:* Bounding the eigenvalues of the expected projection operator \(Q\) is the technical heart of the forgetting guarantees.  
  - *Quick check:* What does it mean for an operator’s spectral radius to be strictly less than 1 in the context of iterative projections?  

- **Over‑parameterization in Linear Models**  
  - *Why needed:* The results rely on the model having more parameters than the total rank of all tasks, ensuring a non‑empty intersection of solution subspaces.  
  - *Quick check:* How does over‑parameterization guarantee the existence of a common minimum‑norm solution across tasks?  

## Architecture Onboarding  
**Component map:**  
Data stream → Task index τ(t) → Projection matrix P_{τ(t)} → Model update w_t = P_{τ(t)} w_{t-1} → Spectral operator Q → Forgetting bound F(k)  

**Critical path:**  
Model & Algorithm → Reduction to single‑step SGD → Construction of Q → Spectral eigenvalue bound → Derivation of forgetting rates.  

**Design tradeoffs:**  
- *Universality vs. task‑specificity:* Bounds are independent of detailed task distributions, offering broad applicability but potentially looser constants compared to tailored analyses.  
- *Random vs. adversarial ordering:* Guarantees hold for random orderings; deterministic or adversarial sequences can break the spectral assumptions and lead to catastrophic forgetting.  

**Failure signatures:**  
- Applying the method to non‑linear models (e.g., deep nets) where linearity of updates no longer holds.  
- Violating joint realizability (conflicting labels) causing the projection to be ill‑defined.  
- Using inexact task fitting (finite gradient steps) which breaks the exact SGD equivalence.  

**First 3 experiments:**  
1. **Synthetic linear regression replication:** Generate jointly realizable tasks in an over‑parameterized regime, run the sequential projection algorithm, and plot average forgetting \(F(k)\) over random orderings to verify the predicted decay.  
2. **Ablation of random ordering:** Repeat the experiment with a fixed cyclic task order; observe deviations from the theoretical rate, highlighting the importance of randomness.  
3. **Single‑step SGD equivalence test:** Implement both the projection update and the corresponding SGD step on the modified loss for a simple case; confirm numerically that the iterates match at every step.  

## Open Questions the Paper Calls Out  
- **How do the forgetting guarantees degrade when the joint realizability assumption is violated by label noise or contradicting tasks?**  
- **Can the universal \(O(k^{-1/4})\) rate be extended to deterministic cyclic orderings, or is randomization strictly necessary?**  
- **Do these bounds hold when fitting each task with a finite number of gradient steps rather than to exact convergence?**  

## Limitations  
- Synthetic data generation details (distribution of \(X_m\), sample sizes, etc.) are not fully specified, hindering exact replication.  
- The theory assumes exact joint realizability and perfect convergence of each task fit; any regularization or numerical tolerance may break the equivalence.  
- Guarantees are limited to random task orderings; adversarial or deterministic sequences are not covered.  

## Confidence  
- **Equivalence of continual projection to one SGD step on a modified least‑squares loss → High**  
- **Universal forgetting bounds for with‑replacement orderings → Medium**  
- **First‑of‑its‑kind bound for without‑replacement orderings → Low**  

## Next Checks  
1. Replicate the synthetic experiment with a fully documented data‑generation pipeline (Gaussian \(X_m\), fixed \(n_m\), chosen \(d,T\)) and verify that the log‑log slope of forgetting matches the predicted \(k^{-1/4}\) regime.  
2. Introduce a small perturbation to the labels so joint realizability holds only approximately; measure whether the “single‑step SGD” iterates diverge from the projection updates.  
3. Compare forgetting under truly random orderings versus a deterministic cyclic ordering on the same tasks to quantify the degradation (or failure) of the bound when the randomness assumption is removed.