---
ver: rpa2
title: Elucidating the Design Space of FP4 training
arxiv_id: '2509.17791'
source_url: https://arxiv.org/abs/2509.17791
tags:
- loss
- best
- 'false'
- score
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a systematic study of 4-bit floating-point
  (FP4) training techniques. The authors introduce a gradient-based framework to analyze
  the computational costs of various stabilization methods and conduct extensive empirical
  evaluations across multiple machine learning tasks.
---

# Elucidating the Design Space of FP4 training

## Quick Facts
- **arXiv ID:** 2509.17791
- **Source URL:** https://arxiv.org/abs/2509.17791
- **Reference count:** 40
- **Primary result:** Identifies Hadamard transforms, tensor scaling, and stochastic rounding as the optimal FP4 training stabilization combination, with UE5M3 offering best range-precision trade-off.

## Executive Summary
This paper presents a systematic study of 4-bit floating-point (FP4) training techniques through a gradient-based framework that analyzes computational costs of various stabilization methods. The authors conduct extensive empirical evaluations across multiple machine learning tasks, including LLMs, diffusion models, and computer vision. Their findings identify that combining Hadamard transformations, tensor scaling, and stochastic rounding provides the best performance-to-overhead trade-off for FP4 training stability. The study also highlights that using UE5M3 as a scaling factor offers a good compromise between range and precision, while noting that FP4 training dynamics may vary across model scales.

## Method Summary
The study employs a simulation-based approach using BFLOAT16 to evaluate FP4 training techniques. The methodology involves block-wise quantization with shared scaling factors, tensor scaling for global normalization, and various stabilization techniques including Hadamard transforms for outlier mitigation and stochastic rounding for numerical stability. The authors implement a microscaling framework with block sizes of 16-32 elements and use Straight-Through Estimator (STE) for gradient approximation. Extensive experiments are conducted across regression, image classification (ImageNet, CIFAR), diffusion models, and LLMs (LLaMA from 9M to 1B parameters) to evaluate different configurations of scaling formats (E4M3, E8M0, UE5M3), gradient computation methods, and stabilization techniques.

## Key Results
- Hadamard transformations, tensor scaling, and stochastic rounding together provide optimal FP4 training stability with manageable overhead
- UE5M3 scaling format offers the best compromise between dynamic range and precision for FP4 training
- Straight-Through Estimator (STE) outperforms theoretically correct sparse gradient computations for FP4 training stability
- FP4 training dynamics show significant variation across model scales, with techniques effective for small models often failing at larger scales

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Scale factor dynamic range is the primary bottleneck for FP4 stability, often outweighing the precision of the mantissa.
- **Mechanism:** Limited exponent range in formats like E4M3 (NVFP4) causes underflow or overflow during normalization, even with tensor scaling. Formats with wider range (E8M0/MXFP4) or custom formats (UE5M3) offer better trade-off by covering tensor dynamic range more safely.
- **Core assumption:** Tensor value distributions require specific dynamic ranges that E4M3 cannot satisfy without significant tuning.
- **Evidence anchors:** Abstract mentions UE5M3 as compromise; Section 7 Principle 2 finds range profoundly affects stability; paper contrasts FP4 All the Way (NVFP4 focus) with E8M0/UE5M3 robustness.

### Mechanism 2
- **Claim:** Dense gradient updates via Straight-Through Estimator (STE) stabilize training better than theoretically correct sparse gradient approximations.
- **Mechanism:** Exact gradients for `absmax` normalization create sparse, high-variance updates that destabilize momentum-based optimizers. STE provides dense, biased updates that are empirically more stable.
- **Core assumption:** Optimizer stability relies more on consistent gradient density than exact gradient sparsity for scaling operations.
- **Evidence anchors:** Section 7 Principle 1 states gradient stability outweighs unbiasedness; Table 1 shows STE baseline vs Absmax/Softmax; related work proposes complex gradients found unnecessary.

### Mechanism 3
- **Claim:** Hadamard transformations efficiently mitigate outlier sensitivity with lower overhead than spectral decomposition.
- **Mechanism:** Random Hadamard transform rotates data distribution, spreading outlier energy across block dimensions. This concentrates distribution around median, making quantization less sensitive to extremes, and is fusable (O(n log l)).
- **Core assumption:** Outliers are primary source of quantization error in forward/backward passes.
- **Evidence anchors:** Abstract identifies combining Hadamard transformations as part of best trade-off; Table 2 lists Outlier concentration (Hadamard) as fusable with O(n log l) overhead; Quartet: Native FP4 Training also explores low-precision training with specific implementation for outliers.

## Foundational Learning

- **Microscaling (MX) Formats:** FP4 is applied in blocks with shared scaling factor. Understanding block size vs quantization granularity trade-off is essential. **Quick check:** How does reducing block size from 32 to 16 affect probability of outlier degrading entire block's precision?

- **Straight-Through Estimator (STE):** Recommended baseline for backward passes. Understand that STE ignores quantization function derivative, treating quantization as identity in backward pass. **Quick check:** Why would ignoring derivative of quantization step lead to more stable training than calculating it exactly?

- **Floating Point Range vs Precision:** Paper highlights E4M3 (high precision, low range) vs E8M0 (low precision, high range). Selecting wrong format for wrong tensor type causes divergence. **Quick check:** Which format (E4M3 or E8M0) would be safer for gradients, which typically have wider dynamic range but smaller magnitude?

## Architecture Onboarding

- **Component map:** Input (high-precision Tensor X) -> Pre-processing (Global Tensor Scaling -> Block-wise Scaling (E8M0/E4M3) -> Hadamard Transform) -> Quantization (Cast to FP4 using Stochastic Rounding) -> Compute (MatMul) -> Backward (STE for gradients; Stochastic Rounding for weight updates)

- **Critical path:** Scaling factor quantization (`q(s)`) and stochastic rounding implementation are most fragile components. Incorrect scale underflow handling (NaNs) kills training instantly.

- **Design tradeoffs:** NVFP4 (E4M3 scale) offers better precision but prone to overflow/underflow, requiring Tensor Scaling + NaN handling. MXFP4 (E8M0 scale) works "out of the box" for many models but might lack precision for specific tasks. UE5M3 (Custom scale) is proposed sweet spot but requires custom kernel support.

- **Failure signatures:** Loss spikes/divergence likely due to `absmax` gradient sparsity or optimizer instability (switch to STE). Stalling/zero grads likely scale underflow in E4M3 (implement "round to nearest subnormal" instead of "round to one"). Slow convergence indicates outliers dominating scaling factors (enable Hadamard transform).

- **First 3 experiments:** 1) Baseline Stability Check: Train Llama 60M on WikiText with pure MXFP4 (E8M0 scale) and STE, verify convergence close to BF16. 2) Format Ablation: Same model with NVFP4 (E4M3 scale) + Tensor Scaling, identify if divergence occurs and if "round to nearest subnormal" fixes it. 3) Technique Overhead Test: Add Stochastic Rounding and Hadamard transforms, measure accuracy gain vs simulated compute overhead.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do optimal FP4 training configurations and dynamics identified hold consistently as model scale increases beyond 1 billion parameters?
- **Basis in paper:** Explicit conclusion stating "FP4 training dynamics may not be consistent across model scales," explicitly leaving as critical direction for further research.
- **Why unresolved:** Empirical study limited to 1B parameters (LLaMA-1B), authors observed techniques beneficial for small models often failed or became detrimental at larger scales.
- **What evidence would resolve it:** Successful replication of proposed configurations (UE5M3 + stochastic rounding) on multi-billion parameter models without divergence or significant performance degradation.

### Open Question 2
- **Question:** Is UE5M3 scaling format truly optimal trade-off between range and precision for hardware implementation compared to existing standards?
- **Basis in paper:** Explicit conclusion recommending "exploring different scaling formats such as UE5M3," noting it yielded better results but required manageable computational overhead.
- **Why unresolved:** While UE5M3 outperformed MXFP4 (E8M0) and NVFP4 (E4M3) in study, authors frame it as "potential sweet spot" rather than definitive solution, acknowledging overhead it introduces.
- **What evidence would resolve it:** Hardware simulations or deployments demonstrating UE5M3 computational overhead is lower than performance penalty of E8M0/E4M3 across diverse workloads.

### Open Question 3
- **Question:** Why do theoretically sound gradient approximations (e.g., absmax) stabilize small models but destabilize large language and diffusion models?
- **Basis in paper:** Inferred from Section 7 Principle 1 noting differentiable gradients helped small classification tasks but were "detrimental" for larger models, speculating sparse updates cause high-variance momentum in optimizers like Adam.
- **Why unresolved:** Paper identifies phenomenon—dense stable updates (STE) outperform exact gradients at scale—but provides only hypothesis regarding optimizer variance rather than theoretical proof.
- **What evidence would resolve it:** Analysis of gradient variance and optimizer state dynamics in large models comparing STE vs exact gradient methods, showing quantitative link to training divergence.

### Open Question 4
- **Question:** Can existing stabilization techniques like SmoothSwiGLU effectively enable NVFP4 training for models larger than 1B parameters?
- **Basis in paper:** Inferred from Section 6.3 reporting inability to replicate SmoothSwiGLU success from other works for models up to LLaMA-1B, observing gap from BFLOAT16 performance grew with model size.
- **Why unresolved:** Suggests range limitations of NVFP4 (E4M3 scale) may be fundamental bottlenecks that current software-level adjustments cannot overcome for large-scale pre-training.
- **What evidence would resolve it:** Modified stabilization method allowing NVFP4 to converge at 1B+ scale with loss curves matching BF16 baselines.

## Limitations

- Study relies on simulation rather than real hardware implementation, which may not capture architectural optimizations and pipeline effects
- Custom UE5M3 format lacks standardized hardware support and requires significant engineering effort to implement
- Study focuses primarily on transformers and U-Nets with limited exploration of other architectures like CNNs or RNNs
- Model size range limited to 9M-1B parameters, leaving uncertainty about behavior at 10B+ scale

## Confidence

**High Confidence:** The gradient stability advantage of STE over exact gradient computations is well-supported by theoretical analysis and empirical results across multiple tasks.

**Medium Confidence:** Recommendation of UE5M3 as optimal format is based on simulated results and may not hold exactly on real hardware due to implementation differences.

**Low Confidence:** Assertion that FP4 training dynamics scale differently with model size is based on limited model size range (9M to 1B parameters) and would benefit from testing on larger models (10B+ parameters).

## Next Checks

1. **Hardware Implementation Validation:** Implement the best-performing configuration (UE5M3 with Hadamard transforms, tensor scaling, and stochastic rounding) on actual hardware supporting FP4 operations (e.g., NVIDIA Hopper with TF32/FP8 capabilities) and compare results to simulation predictions.

2. **Scale-Invariant Architecture Testing:** Test FP4 training techniques on architectures less sensitive to quantization errors, such as modern CNN architectures with batch normalization, to determine if benefits observed in transformers and U-Nets generalize across model families.

3. **Extreme Model Size Scaling:** Extend model size range to include 10B+ parameter models to validate the paper's hypothesis that FP4 training dynamics change with scale, and identify any new failure modes that emerge at larger scales.