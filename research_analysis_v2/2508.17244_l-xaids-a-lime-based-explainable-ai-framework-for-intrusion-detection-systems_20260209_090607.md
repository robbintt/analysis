---
ver: rpa2
title: 'L-XAIDS: A LIME-based eXplainable AI framework for Intrusion Detection Systems'
arxiv_id: '2508.17244'
source_url: https://arxiv.org/abs/2508.17244
tags:
- feature
- features
- explanations
- detection
- lime
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the lack of explainability in machine learning-based
  intrusion detection systems (IDS), which are often black-box models. The authors
  propose L-XAIDS, a framework combining LIME for local explanations and ELI5 for
  global explanations, along with Decision Tree, MLP, and XGBoost classifiers, to
  provide transparency in IDS decision-making.
---

# L-XAIDS: A LIME-based eXplainable AI framework for Intrusion Detection Systems

## Quick Facts
- **arXiv ID:** 2508.17244
- **Source URL:** https://arxiv.org/abs/2508.17244
- **Reference count:** 40
- **Key outcome:** Achieves 85% accuracy in classifying attack behavior using LIME for local explanations and ELI5 for global feature importance rankings on UNSW-NB15 dataset.

## Executive Summary
This paper addresses the lack of transparency in machine learning-based intrusion detection systems (IDS) by proposing L-XAIDS, a framework that combines LIME for local explanations and ELI5 for global feature importance. The framework trains multiple classifiers (Decision Tree, MLP, XGBoost) on the UNSW-NB15 dataset and provides both global rankings of feature significance and local explanations for specific predictions. By identifying discrepancies between local evidence and global importance, the system can detect potential false negatives where the model's prediction contradicts the explanatory evidence.

## Method Summary
The framework preprocesses UNSW-NB15 data through median imputation, label encoding, and standard scaling, then applies SMOTE for balancing. Three classifiers (Decision Tree, MLP, XGBoost) are trained in parallel. ELI5's permutation importance provides global feature rankings by measuring accuracy degradation when features are shuffled. LIME's tabular explainer generates local linear approximations by perturbing specific instances and training weighted surrogate models. The framework combines these to provide both overall transparency and instance-specific reasoning for IDS decisions.

## Key Results
- Decision Tree classifier achieves 85% accuracy on UNSW-NB15 dataset
- Global feature importance ranks `sttl` (source time-to-live) as most significant at 0.2755 weight
- Local explanations successfully identify false negatives where feature evidence contradicts model predictions
- Framework processes individual instances in approximately 2.3 seconds

## Why This Works (Mechanism)

### Mechanism 1: Local Linear Approximation via Perturbation
The framework generates perturbed versions of specific network packets, queries the black-box classifier for predictions, and trains a weighted linear surrogate model. Features with highest weights in this surrogate are presented as explanations for that instance. This works because complex global behavior can be approximated by linear models in small local neighborhoods.

### Mechanism 2: Global Feature Attribution via Permutation
ELI5 calculates permutation importance by shuffling single feature values across the dataset while holding others constant. If model accuracy drops significantly, the feature is deemed globally important. This measures feature impact through performance degradation rather than coefficients.

### Mechanism 3: Conflict Detection via Dual-Explanation Consensus
The framework compares local LIME weights against global ELI5 rankings. If local evidence strongly points to "Attack" but the classifier outputs "Normal," a discrepancy is flagged. This assumes global important features correctly represent attack indicators.

## Foundational Learning

- **Concept:** Surrogate Models (Local Interpretable Model-Agnostic Explanations)
  - **Why needed here:** L-XAIDS relies on understanding that "explanation" is a simpler approximation of the complex model, not its internal logic.
  - **Quick check question:** If LIME explanation says "Feature A caused this alert," does that mean the XGBoost model used Feature A in a decision tree split, or that Feature A was highly correlated with the prediction in the local perturbation space?

- **Concept:** Permutation Importance
  - **Why needed here:** To interpret the Global Explanation module (ELI5), one must understand importance derives from performance degradation, not coefficients.
  - **Quick check question:** If you shuffle the `sttl` column and model accuracy remains exactly the same, what is the implied importance of `sttl`?

- **Concept:** Gini Impurity vs. Permutation
  - **Why needed here:** The paper compares scikit-learn feature importance (often Gini-based for trees) with ELI5 (Permutation).
  - **Quick check question:** Does Gini importance measure impact on model accuracy or reduction in node impurity during training?

## Architecture Onboarding

- **Component map:** Input (UNSW-NB15 data) -> Preprocessing (Imputation, Encoding, Scaling) -> Classifier Layer (Decision Tree, MLP, XGBoost) -> Explanation Layer (ELI5 for Global, LIME for Local) -> Output (Binary label + Visual/Tabular explanation)

- **Critical path:** Data Cleaning (handling NaN values) → Train/Test Split (80/20) → Model Training (e.g., Decision Tree achieving 85% accuracy) → Generating LIME explanations for specific False Negatives/Positives to debug the model

- **Design tradeoffs:**
  - Accuracy vs. Runtime: LIME takes ~2.3s per instance, computationally expensive for real-time analysis but acceptable for forensic auditing
  - Stability: Local explanations (LIME) can vary for same instance if perturbation is randomized, whereas Global (ELI5) is more stable but less specific

- **Failure signatures:**
  - High LIME Variance: If explaining same packet twice yields different top features, kernel width or sample size may be insufficient
  - Misalignment: When Global importance says "Look at `sttl`", Local says "`sttl` looks like Attack", but Model says "Normal" - indicates model failure

- **First 3 experiments:**
  1. Baseline Replication: Train Decision Tree on UNSW-NB15 training set and reproduce 85% accuracy and `sttl` feature importance ranking
  2. False Negative Analysis: Isolate instances classified as "Normal" which are actually attacks, run LIME explainer to verify if feature weights should have indicated attack
  3. Sensitivity Test: Run LIME on MLP vs. XGBoost model for same malicious packet to compare how different black boxes justify same prediction

## Open Questions the Paper Calls Out

### Open Question 1
How can the L-XAIDS framework be adapted for decentralized environments using federated learning while maintaining explainability? The paper mentions future work on federated learning approaches for explainable IDS frameworks.

### Open Question 2
Does integrating adversarial robustness into the explainability techniques improve IDS resilience against emerging cyber threats? The paper suggests incorporating adversarial robustness could enhance resilience.

### Open Question 3
Can the computational overhead of the explainability module be optimized to support real-time analysis in high-speed networks? The paper notes explainability introduces overhead (e.g., 2.3s per instance) and suggests GPU acceleration or parallel computation.

## Limitations

- Accuracy metrics lack specification of hyperparameters, making exact replication difficult
- False negative detection mechanism assumes global feature importance correctly represents attack indicators, which may fail for zero-day exploits
- LIME's local linear approximations can be unstable for highly non-linear decision boundaries

## Confidence

- **High confidence:** Core methodology combining LIME (local) and ELI5 (global) for IDS explainability is well-established and clearly described
- **Medium confidence:** Specific feature importance rankings and accuracy metrics are reproducible given correct preprocessing steps
- **Low confidence:** Conflict detection mechanism's effectiveness in real-world scenarios is unproven

## Next Checks

1. Replicate the Decision Tree model achieving 85% accuracy on UNSW-NB15 test set after SMOTE balancing and proper preprocessing
2. Generate LIME explanations for specific false negative cases (like packet 100) to verify if feature weights correctly indicate attack behavior
3. Compare Gini importance vs. permutation importance rankings for the same model to understand discrepancies in feature attribution