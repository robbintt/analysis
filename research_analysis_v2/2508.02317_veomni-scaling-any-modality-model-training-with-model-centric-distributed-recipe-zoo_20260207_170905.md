---
ver: rpa2
title: 'VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed
  Recipe Zoo'
arxiv_id: '2508.02317'
source_url: https://arxiv.org/abs/2508.02317
tags:
- training
- omni-modal
- llms
- veomni
- frameworks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper tackles the difficulty of scaling end\u2011to\u2011\
  end omni\u2011modal LLM training, where heterogeneous encoders/decoders and ultra\u2011\
  long sequences cause load imbalance and high engineering overhead in existing frameworks.\
  \ VeOmni introduces a model\u2011centric design that separates computation from\
  \ communication, offering a plug\u2011and\u2011play encoder\u2011foundation\u2011\
  decoder stack and a \u201Cdistributed recipe zoo\u201D (FSDP, HSDP, expert, sequence\
  \ and pipeline parallelism) that can be freely composed without modifying model\
  \ code."
---

# VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo  

## Quick Facts  
- **arXiv ID:** 2508.02317  
- **Source URL:** https://arxiv.org/abs/2508.02317  
- **Reference count:** 40  
- **Primary result:** Trained a 30 B‑parameter omni‑modal MoE at >2,800 tokens / sec / GPU and achieved a 160 K‑token context length on 128 GPUs using 3‑D parallelism.  

## Executive Summary  
VeOmni addresses the long‑standing bottleneck of scaling end‑to‑end omni‑modal LLM training, where heterogeneous encoders/decoders and ultra‑long sequences cause severe load imbalance and engineering overhead. By decoupling computation from communication and exposing a “model‑centric” stack (encoder → foundation → decoder), VeOmni lets researchers compose a rich “distributed recipe zoo” (FSDP, HSDP, expert, sequence, and pipeline parallelism) without touching model code. The system demonstrates markedly higher throughput and scalability than prior multimodal pipelines, enabling a 30 B‑parameter MoE to run at >2,800 tokens / sec / GPU and to process 160 K‑token contexts on a 128‑GPU cluster.  

## Method Summary  
VeOmni’s core design separates the model definition from the communication layer. Users assemble a plug‑and‑play encoder‑foundation‑decoder stack, then select any combination of parallelism recipes from the zoo. Each recipe is implemented as a thin wrapper around PyTorch/FSDP primitives, allowing seamless composition (e.g., FSDP + pipeline + expert parallelism) without modifying the underlying model code. The authors validated this approach by training a 30 B‑parameter vision‑language MoE, measuring throughput, memory usage, and scaling behavior across 3‑D parallel configurations.  

## Key Results  
- **Throughput:** 30 B‑parameter MoE reaches >2,800 tokens / sec / GPU on 128 GPUs.  
- **Context length:** Scales to 160 K tokens via combined sequence, tensor, and pipeline parallelism.  
- **Scalability:** Demonstrates linear‑ish speed‑up compared with traditional multimodal pipelines, reducing engineering effort for new modalities.  

## Why This Works (Mechanism)  
- **Model‑centric compute/communication split** – By isolating communication primitives from the model graph, VeOmni eliminates the need for modality‑specific engineering hacks, allowing any encoder/decoder to be slotted in.  
- **Plug‑and‑play recipe zoo** – A library of composable parallelism strategies (FSDP, HSDP, expert, sequence, pipeline) can be stacked arbitrarily, letting users match the parallelism mix to the workload without rewriting code.  
- **3‑D parallelism synergy** – Combining tensor, pipeline, and sequence parallelism balances memory and compute across GPUs, enabling ultra‑long context training while keeping per‑GPU memory within limits.  

## Foundational Learning  
1. **Evidence Anchoring** – Needed to distinguish statements directly supported by the paper from inferred interpretations. *Quick check:* “Can you point to the exact sentence that reports the 2,800 tokens / sec / GPU figure?”  
2. **Causal Mechanism vs. Correlation** – Prevents attributing performance gains to the wrong factor (e.g., hardware vs. algorithm). *Quick check:* “Does the paper describe how the recipe zoo reduces load imbalance step‑by‑step?”  
3. **Modality‑agnostic Design** – Highlights why separating compute from communication enables new encoders without code changes. *Quick check:* “Is there an experiment where a non‑vision encoder is swapped in without modifying the training script?”  
4. **3‑D Parallelism Trade‑offs** – Explains the interaction between memory savings and communication overhead. *Quick check:* “What is the reported communication cost when adding sequence parallelism on top of tensor parallelism?”  
5. **Stability of MoE Load Balancing** – Important for long‑sequence training stability. *Quick check:* “Does the paper provide metrics on expert load imbalance across GPUs?”  

## Architecture Onboarding  
- **Component map:** Encoder → Foundation Model → Decoder → Distributed Recipe Zoo (FSDP, HSDP, Expert, Sequence, Pipeline) → GPU Cluster  
- **Critical path:** Input data → Encoder → Foundation → Decoder → Apply selected parallelism wrappers → Communication layer → Gradient aggregation → Parameter update.  
- **Design tradeoffs:**  
  - *Flexibility vs. overhead*: More recipes increase composability but add communication layers.  
  - *Memory vs. latency*: Tensor parallelism saves memory but may increase latency; pipeline parallelism improves throughput at the cost of pipeline bubbles.  
- **Failure signatures:**  
  - Imbalanced GPU utilization (e.g., some GPUs idle).  
  - Communication bottlenecks (high NCCL latency).  
  - MoE expert overload warnings or gradient spikes.  
- **First 3 experiments:**  
  1. Validate that a vanilla encoder‑foundation‑decoder stack runs correctly with a single recipe (e.g., FSDP) on a 4‑GPU testbed.  
  2. Add a second recipe (pipeline parallelism) and measure throughput change; confirm no code modifications are required.  
  3. Replace the vision encoder with a dummy audio encoder to test plug‑and‑play modality swapping, monitoring for runtime errors.  

## Open Questions the Paper Calls Out  
- **Scalability beyond 128 GPUs** – *Assumption:* The authors only report results on a 128‑GPU cluster; it remains unclear how performance and communication overhead behave on larger or heterogeneous clusters.  
- **Generalization to other modalities** – *Assumption:* Experiments focus on vision‑language; the paper does not present empirical evidence for audio, video, or graph encoders, leaving open whether the same recipe zoo yields comparable gains.  
- **Load‑balancing stability for extreme context lengths** – *Unknown:* While a 160 K‑token context is demonstrated, the paper provides limited quantitative analysis of MoE expert load imbalance or gradient variance at that scale.  
- **Overhead of recipe composition** – *Assumption:* Adding more parallelism recipes likely introduces extra synchronization steps; the paper does not quantify the latency penalty of stacking three or more recipes.  
- **Reproducibility of hyper‑parameters** – *Unknown:* Key training hyper‑parameters (learning‑rate schedule, optimizer settings, communication buffer sizes) are not fully disclosed, which may affect the ability to replicate the reported throughput.  

## Limitations  
- Implementation details (e.g., exact learning‑rate schedule, communication primitives) are not fully disclosed, hindering exact reproduction.  
- Reported throughput is limited to a single 128‑GPU configuration; scalability to larger or heterogeneous clusters is unclear.  
- Generality across modalities is demonstrated only for vision‑language; behavior on audio, video, or graph data remains unverified.  

## Confidence  
- **Model‑centric separation of compute/communication:** High – directly described in the architecture section.  
- **Reported 30 B MoE throughput ≈ 2,800 tokens / sec / GPU:** Medium – figure is stated but the paper does not provide a raw table or confidence interval.  
- **160 K‑token context length via 3‑D parallelism:** Medium – context length is reported, yet detailed memory‑usage breakdown is missing.  
- **Plug‑and‑play composability of the recipe zoo across any modality:** Low – only vision‑language experiments are shown; cross‑modality evidence is absent.  

## Next Checks  
1. **Reproduce throughput:** Run the 30 B MoE on a comparable 128‑GPU cluster using VeOmni’s default recipe zoo and verify tokens / sec / GPU ≈ 2,800.  
2. **Baseline comparison:** Train the same model with a conventional multimodal pipeline (e.g., DeepSpeed ZeRO + pipeline) on identical hardware and quantify the relative speedup.  
3. **Cross‑modality test:** Swap the vision encoder for an audio encoder while keeping the same recipe configuration; confirm training proceeds without code changes and measure any efficiency impact.