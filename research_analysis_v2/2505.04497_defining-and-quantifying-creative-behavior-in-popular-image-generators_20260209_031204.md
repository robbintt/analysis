---
ver: rpa2
title: Defining and Quantifying Creative Behavior in Popular Image Generators
arxiv_id: '2505.04497'
source_url: https://arxiv.org/abs/2505.04497
tags:
- image
- creativity
- chain
- chains
- strength
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces quantitative measures for evaluating creative
  behavior in image-to-image generative models. The authors define three criteria:
  satisfaction of prompt requirements, cohesion among generated artifacts, and diversity
  of artifacts.'
---

# Defining and Quantifying Creative Behavior in Popular Image Generators

## Quick Facts
- arXiv ID: 2505.04497
- Source URL: https://arxiv.org/abs/2505.04497
- Authors: Aditi Ramaswamy; Hana Chockler; Melane Navaratnarajah
- Reference count: 8
- Primary result: Higher temperature significantly increases creativity scores in image-to-image generative models, but only when textual captions anchor prompt requirements.

## Executive Summary
This paper introduces quantitative measures for evaluating creative behavior in image-to-image generative models by decomposing creativity into three components: prompt satisfaction, cohesion, and diversity. The authors construct iterative "chains" where generated images are repeatedly processed through models, measuring how well outputs maintain prompt fidelity while introducing novel elements. Experiments with three popular models (FLUX, Kandinsky 2.2, Stable Diffusion 3) at varying temperatures show that higher temperature increases creativity scores, but textual input is crucial—models perform poorly on image-only inputs. The measures successfully differentiate between models and temperature settings, demonstrating that creativity involves both maintaining prompt requirements and introducing diverse, cohesive new elements.

## Method Summary
The authors define creativity through three quantitative measures applied to iterative chains of image generation. For each chain, a seed image is processed through an image generator (FLUX, Kandinsky 2.2, or Stable Diffusion 3) with varying temperature/strength settings. Object detectors extract textual artifact labels from generated images, which are embedded in semantic space using SPACY. Requirements Satisfaction (RS) measures how long generated artifacts match seed artifacts, Cohesion (BR) measures relationships among generated artifacts, and Diversity (DR) measures novel elements introduced. The combined Creativity Ranking (CR) = RS × BR + DR/2 requires both prompt fidelity and novel elements to be non-zero. Chains are tested with three input types: image-only, caption-only, and image+caption.

## Key Results
- Higher temperature (strength 0.9 vs 0.3) significantly increases creativity scores for img_cap chains across all three models.
- Image-only chains produce near-zero creativity scores across all temperatures for Stable Diffusion 3 and Kandinsky 2.2, demonstrating text's crucial role in maintaining prompt requirements.
- FLUX shows higher creativity scores at lower temperatures (0.3-0.6) compared to Kandinsky 2.2 and Stable Diffusion 3, which require higher temperatures for comparable creativity.
- The three-component decomposition successfully differentiates between models and temperature settings, with statistical significance (p<0.05) for most comparisons.

## Why This Works (Mechanism)

### Mechanism 1: Artifact-Based Creativity Decomposition
Creative behavior can be decomposed into three quantifiable components—prompt satisfaction, cohesion, and diversity—that together distinguish meaningful creative output from both hallucination and rote copying. Object detectors extract textual artifact labels from images, which are embedded in semantic space and compared using cosine similarity. The creativity score CR = RS × BR + DR/2 requires both prompt fidelity (RS) and novel elements (DR) to be non-zero. This approach differs from prior metrics by focusing on individual detectable artifacts rather than overall image structure. The mechanism breaks when object detectors fail to extract relevant artifacts, producing false negatives for abstract or stylized outputs.

### Mechanism 2: Chain Construction as Temporal Creativity Probe
Iteratively feeding outputs back as inputs reveals creative capacity through the length of sustained prompt fidelity and diversity of accumulated artifacts. The process involves seed image → caption generation → img2img generation → repeat for max 10 steps. RS measures the longest unbroken sequence where seed artifacts remain detectable; DR and BR are computed at the last valid image. Models that maintain prompt requirements longer while adding novel artifacts are more creative than those that drift immediately or never diverge. The mechanism breaks when chain length truncation at step 1 for img_only chains results in DR=0 and BR=0 regardless of visual quality.

### Mechanism 3: Temperature Modulates Exploration-Exploitation Tradeoff
Higher input temperature increases creativity scores by expanding the artifact search space, but only when textual captions anchor prompt requirements. Temperature controls noise added during denoising; higher values allow lower-probability artifacts to appear in outputs. Statistical tests show significant differences between strength 0.3 and 0.9 for FLUX and Stable Diffusion 3. The mechanism breaks without textual grounding—img_only chains show near-zero CR across all temperatures for Stable Diffusion 3 and Kandinsky 2.2, indicating that temperature alone is insufficient for creative divergence.

## Foundational Learning

- **Semantic embedding spaces and cosine similarity**
  - Why needed here: All three measures (RS, BR, DR) rely on comparing artifact vectors in semantic space to determine if generated artifacts "match" or "relate to" seed artifacts.
  - Quick check: Given two artifact embeddings â and b̂ with dot product 0.72, are they more or less similar than the threshold t=0.65 used in the paper?

- **Object detection as artifact extraction**
  - Why needed here: The entire measurement pipeline depends on converting visual regions into textual labels that can be embedded. Understanding detector limitations is critical for interpreting scores.
  - Quick check: If GROUNDING DINO detects "cake" but the ground truth artifact is "apple pie," will the similarity threshold of 0.65 correctly classify this as prompt satisfaction?

- **Diffusion model denoising and strength/temperature**
  - Why needed here: The "strength" parameter controls how many denoising steps are applied, which directly affects how far outputs can diverge from inputs. This is the intervention variable in all experiments.
  - Quick check: At strength=0.3, are outputs more or less similar to the seed image than at strength=0.9, and why does this matter for the RS measure?

## Architecture Onboarding

- Component map: Seed Image → Caption Generator (KOSMOS) → Image Generator (FLUX/SD3/Kandinsky) ← [Temperature/Strength] → Generated Image → Object Detectors (GROUNDING DINO + DETR) → Artifact Labels → Word Embeddings (SPACY) → Cosine Similarity Calculations → RS, BR, DR Scores → CR (Creativity Ranking)

- Critical path: 1) Chain type selection determines input modalities; 2) Temperature/strength setting controls exploration range; 3) Object detection quality gates all downstream measurements—failed detection = zero scores; 4) Semantic similarity threshold (t=0.65) determines what counts as "satisfied"

- Design tradeoffs: Simple seed images (single artifact) vs. complex scenes—paper uses Food-101 for measurement clarity but may not generalize. Chain length (10 steps) vs. computational cost—longer chains could reveal more about drift patterns. Dual object detectors vs. single detector—using both increases artifact recall but introduces potential label conflicts.

- Failure signatures: Zero CR with visually creative outputs occurs when object detectors fail (e.g., abstract art returning no labels). High RS with zero CR occurs with mechanistic copying (RS=1 but DR=0 and BR=0). img_only chains with near-zero CR across all temperatures indicates text absence breaks prompt anchoring.

- First 3 experiments: 1) Reproduce img_cap chain for FLUX at strengths 0.3, 0.6, 0.9 using 10 food images to verify CR increases with strength; 2) Ablate object detector by substituting GROUNDING DINO with YOLO to measure detector dependence; 3) Test break condition by generating img_cap chains with abstract art seeds to confirm measurement gap for non-detectable creativity.

## Open Questions the Paper Calls Out

- How does increasing the complexity of the seed image artifact set influence the proposed creativity scores? The authors acknowledge that using simple Food-101 images could impact creativity scores and identify exploring complex datasets as a crucial next step.

- To what degree do the automated creativity rankings correlate with human subjective evaluations of creativity? The authors note that object detection tools occasionally make decisions that don't seem intuitive to humans and suggest incorporating human observation as a factor for future research.

- Can perceptual similarity metrics like LPIPS enhance the granularity of the Requirements Satisfaction measure? The authors state that existing metrics like LPIPS could be incorporated to improve the granularity of their measures.

## Limitations

- Object detector dependence creates measurement ceiling where abstract or stylized outputs may produce zero CR scores despite human-perceived creativity.
- Choice of Food-101 (single-subject images) limits generalizability to complex multi-artifact scenes and realistic creative tasks.
- Chain length truncation at 10 steps may miss longer-term drift patterns and the full evolution of creative divergence.

## Confidence

- **High confidence:** Temperature effects on creativity scores are robust and statistically significant, with the three-component decomposition successfully differentiating models and settings.
- **Medium confidence:** Chain construction methodology captures meaningful creativity patterns, though dependence on object detector quality introduces uncertainty.
- **Low confidence:** Generalizability to complex multi-artifact scenes and abstract art remains untested, with the 0.65 threshold potentially not optimal across different visual domains.

## Next Checks

1. **Detector Ablation Test:** Replace GROUNDING DINO with YOLO on identical chains to quantify how much measurement variance stems from detector choice rather than model creativity.

2. **Threshold Sensitivity Analysis:** Run chains with similarity thresholds of 0.5, 0.65, and 0.8 to determine if the 0.65 threshold systematically under/overestimates creativity.

3. **Complex Scene Validation:** Test chains using multi-object images from COCO instead of single-subject Food-101 to assess whether the measures scale to realistic creative tasks.