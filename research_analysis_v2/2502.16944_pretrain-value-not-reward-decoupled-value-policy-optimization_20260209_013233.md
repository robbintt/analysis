---
ver: rpa2
title: 'Pretrain Value, Not Reward: Decoupled Value Policy Optimization'
arxiv_id: '2502.16944'
source_url: https://arxiv.org/abs/2502.16944
tags:
- value
- policy
- reward
- training
- dvpo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method called DVPO for Reinforcement Learning
  from Human Feedback (RLHF). The key idea is to directly pretrain a value model,
  called a Global Value Model (GVM), on preference data, rather than first training
  a reward model and then deriving a value model online.
---

# Pretrain Value, Not Reward: Decoupled Value Policy Optimization

## Quick Facts
- arXiv ID: 2502.16944
- Source URL: https://arxiv.org/abs/2502.16944
- Reference count: 27
- Primary result: DVPO achieves RLHF performance on par with or surpassing state-of-the-art methods while reducing GPU memory by 30-40% and training time by 30-45%

## Executive Summary
DVPO proposes a fundamental shift in RLHF methodology by directly pretraining a value model (GVM) on preference data rather than the traditional reward-model-then-value derivation pipeline. The frozen GVM provides stable, token-level credit assignment during policy optimization, eliminating the need for online critic training. This decoupling reduces computational overhead while maintaining or improving performance on standard RLHF benchmarks.

## Method Summary
DVPO consists of two stages: (1) Train a Global Value Model (GVM) offline using temporal difference learning on preference trajectories from the UltraFeedback dataset, and (2) Freeze the GVM and perform PPO-style policy optimization where the frozen GVM provides advantage estimates. The GVM predicts return-to-go values for each token, enabling fine-grained credit assignment. Unlike standard actor-critic methods, only the policy model receives gradients during RL training, while the GVM remains fixed.

## Key Results
- MT-Bench scores competitive with or exceeding state-of-the-art RLHF methods across 3B, 7B, and 8B models
- GPU memory usage reduced by 23-34% compared to PPO
- Training time per step reduced by 32-44% across model sizes
- GVM accuracy of 68.1% on held-out test data vs 60.6% for PPO's online critic

## Why This Works (Mechanism)

### Mechanism 1: Informational Equivalence of Reward-Value Pipelines
Under fixed offline preference data, training a reward model then deriving values provides no additional information beyond direct value pretraining. The theoretical proof (Lemma 3.1) shows policy gradients from both approaches differ only by bounded approximation error κ(ε_R, ε_Q) that vanishes as model accuracy improves.

### Mechanism 2: Token-Level Credit Assignment via TD Learning
The GVM uses temporal difference learning to predict return-to-go at each position, enabling fine-grained differentiation between high-value and low-value tokens. Case studies show GVM assigning value 0.2099 to critical "not" token in correct responses vs -0.6177 in incorrect ones.

### Mechanism 3: Stability from Decoupled Training
Freezing the GVM eliminates critic drift inherent in standard actor-critic methods where policy and critic updates create moving targets. Only the policy model requires gradients during RL, providing stable advantage estimates throughout optimization.

## Foundational Learning

- **Value function vs. Reward function**: GVM predicts expected cumulative return rather than immediate reward. Quick check: For "The capital of France is", a value function predicts the expected quality of the complete response, while a reward function would score only the next word.
- **Temporal Difference (TD) Learning**: Enables training on incomplete trajectories without Monte Carlo rollouts by bootstrapping from future value predictions. Quick check: Why does TD learning allow training on partial sequences?
- **Actor-Critic Architecture**: Standard PPO alternates policy and critic updates creating instability. Quick check: In PPO, why does simultaneous policy and critic update create instability that DVPO avoids?

## Architecture Onboarding

- **Component map**: SFT checkpoint → GVM (frozen) + Policy Model (trainable) + Reference Model (frozen) → PPO optimization
- **Critical path**: Prepare offline preference data → Train GVM with TD loss → Freeze GVM → Initialize policy from SFT → PPO optimization with GVM advantages → Evaluate on benchmarks
- **Design tradeoffs**: 
  - GVM training data: reward model scores vs. direct pairwise preferences (+1/-1)
  - KL coefficient: 0.05 default balances constraint vs. exploitation
  - Trajectory conditioning: optional τ input to capture policy-specific behavior
- **Failure signatures**: 
  - GVM accuracy <60% on held-out pairs: insufficient training data
  - Policy reward plateaus early: GVM not generalizing to new policy distribution
  - Training loss diverges: check KL coefficient and learning rate stability
- **First 3 experiments**:
  1. Validate GVM quality on UltraFeedback-style data, target >65% accuracy
  2. Compare DVPO vs. PPO critic on convergence speed and MT-Bench score
  3. Test GVM generalization by training on dataset A, optimizing on held-out domain B

## Open Questions the Paper Calls Out
- How to extend DVPO to semi-online regimes with periodic GVM refreshing
- Whether performance advantages persist at scales significantly larger than 8B parameters
- How GVM guidance degrades under extreme policy distribution shifts

## Limitations
- Cannot incorporate new preference data during training without retraining the frozen GVM
- Performance depends on GVM's ability to generalize from offline training data to policy outputs
- Theoretical bounds on approximation error may not reflect practical performance gaps

## Confidence

**High Confidence**:
- Informational equivalence proof between reward-value pipelines
- 30-40% GPU memory reduction through decoupled training
- Performance parity/improvement on established benchmarks

**Medium Confidence**:
- Stability improvements from decoupling policy and critic
- Token-level credit assignment benefits from TD learning
- Computational efficiency gains in training time

**Low Confidence**:
- Long-term generalization across diverse domains
- Performance in interactive feedback scenarios
- Robustness to distribution shift in complex reasoning tasks

## Next Checks

1. **Distribution Shift Validation**: Train GVM on general instruction following data, optimize policy on mathematical reasoning prompts, measure GVM value accuracy degradation and policy performance impact.

2. **Theoretical Bound Verification**: Empirically measure policy gradient differences between reward-derived values and directly pretrained GVM values across multiple runs, compare against theoretical bound κ(ε_R, ε_Q).

3. **Interactive Feedback Scenario**: Implement periodic GVM retraining with newly collected preference data during training, compare performance against standard online critic methods to quantify trade-offs between efficiency and feedback incorporation.