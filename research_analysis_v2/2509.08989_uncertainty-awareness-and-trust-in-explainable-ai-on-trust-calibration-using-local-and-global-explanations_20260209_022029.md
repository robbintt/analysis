---
ver: rpa2
title: Uncertainty Awareness and Trust in Explainable AI- On Trust Calibration using
  Local and Global Explanations
arxiv_id: '2509.08989'
source_url: https://arxiv.org/abs/2509.08989
tags:
- trust
- explanation
- explanations
- were
- learner
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper examines trust calibration in explainable AI (XAI)\
  \ through the lens of uncertainty and global explanations. It investigates whether\
  \ the Unsupervised DeepView algorithm\u2014which combines uncertainty estimation\
  \ with robustness considerations\u2014can effectively calibrate user trust in AI\
  \ systems."
---

# Uncertainty Awareness and Trust in Explainable AI- On Trust Calibration using Local and Global Explanations

## Quick Facts
- arXiv ID: 2509.08989
- Source URL: https://arxiv.org/abs/2509.08989
- Reference count: 40
- Numeric explanations yielded higher satisfaction than visual ones, though both calibrated trust equally well

## Executive Summary
This study investigates trust calibration in explainable AI by comparing an uncertainty-aware global explanation method (Unsupervised DeepView) against simpler numeric explanations. In a between-subjects user study with 196 participants classifying handwritten digits, Unsupervised DeepView successfully calibrated user trust—participants could accurately distinguish between learners with different uncertainty profiles. However, the visual explanations were rated less satisfying than numeric ones, likely due to their complexity and perceived obscurity. The findings suggest that while uncertainty estimation combined with global explanations can enhance interpretability, XAI algorithms should prioritize simplicity and clarity in their visual representations to maximize user satisfaction and trust.

## Method Summary
The study employed a between-subjects design where participants evaluated two machine learning models for handwritten digit classification using either numeric accuracy metrics (control group) or Unsupervised DeepView visualizations (experimental group). Participants (n=196, recruited via Prolific) fluent in German completed a survey including demographic questions, two evaluation scenarios, and standardized scales measuring explanation satisfaction and trust in automation. The numeric group saw training/validation accuracies for both learners, while the visual group received Unsupervised DeepView global visualizations plus example instances. Analysis used paired t-tests for within-group comparisons, independent t-tests and Mann-Whitney U tests for between-group comparisons, and thematic coding of open responses.

## Key Results
- Unsupervised DeepView successfully calibrated trust: participants accurately distinguished between learners with different uncertainty levels (p < .001)
- Visual explanations yielded lower satisfaction than numeric ones: 72.2% vs 80.6% mean satisfaction scores (p = .042)
- Users reported both global (background color) and local (certain/uncertain instance examples) cues influenced their trust assessments

## Why This Works (Mechanism)
The mechanism underlying successful trust calibration involves presenting users with uncertainty information through Unsupervised DeepView's global visualizations. By showing how predictions vary across the feature space (via color-coded backgrounds) and providing concrete examples of certain versus uncertain classifications, users can form accurate mental models about model reliability. The algorithm's use of Local Outlier Factor (LOF) to estimate uncertainty at both global and local levels enables this nuanced representation. Users leverage both the overall pattern of uncertainty (global) and specific examples (local) to calibrate their trust appropriately, even when numeric accuracy metrics are similar between models.

## Foundational Learning
- **Uncertainty estimation methods**: Understanding how models quantify prediction uncertainty is crucial for interpreting reliability signals; quick check: verify the model outputs probability distributions or confidence scores alongside predictions
- **Local Outlier Factor (LOF)**: A density-based method for identifying anomalies that forms the basis for Unsupervised DeepView's uncertainty quantification; quick check: confirm LOF scores are being computed correctly on the feature space
- **Trust calibration in XAI**: The process of aligning user confidence with actual system reliability; quick check: ensure experimental design includes both high and low reliability scenarios for comparison
- **Between-subjects vs within-subjects design**: Between-subjects isolates explanation type effects but may reduce statistical power; quick check: verify randomization and sufficient sample size in each group
- **Explanation satisfaction measurement**: Standardized scales quantify user experience with explanations; quick check: confirm internal consistency (Cronbach's alpha) meets acceptable thresholds
- **Attention checks in online studies**: Quality control mechanisms to filter inattentive participants; quick check: verify attention check questions are clearly worded and appropriately challenging

## Architecture Onboarding
- **Component map**: MNIST dataset -> Two classifiers (regularized vs non-regularized) -> Unsupervised DeepView uncertainty estimation -> Global visualizations + local examples -> User survey interface -> Trust calibration measurement
- **Critical path**: Data preparation → Model training with different regularization → Uncertainty estimation via LOF → Visualization generation → Survey deployment → Statistical analysis of trust calibration
- **Design tradeoffs**: The study prioritized ecological validity through real-world digit classification over controlled lab conditions, accepting potential noise from online recruitment. The between-subjects design avoided carryover effects but required larger sample sizes.
- **Failure signatures**: Trust calibration fails if visualizations don't clearly show uncertainty differences between models, or if users can't interpret color-coded backgrounds. Low satisfaction indicates visualization complexity exceeds user comprehension.
- **First experiments**: 1) Verify regularized and non-regularized models produce meaningfully different uncertainty profiles on MNIST; 2) Test Unsupervised DeepView visualization clarity with a small pilot group; 3) Confirm attention check questions effectively filter inattentive participants

## Open Questions the Paper Calls Out
- Does the disparity in information quantity between experimental conditions confound the comparison of trust levels between numeric and visual explanations? The authors identify a "fundamental difficulty" regarding the "different amounts of information in the two conditions compared" which may have affected results.
- Which specific visual elements (global vs. local) do users focus on when assessing the trustworthiness of an AI system? The authors propose using "supplementary methods such as eye-tracking in order to evaluate in detail which parts of the explanations the participants focused on most."
- Can the visual complexity of global explanations be reduced to increase user satisfaction without compromising the ability to calibrate trust? The authors found that "background color led to considerable confusion" and explicitly suggest simplifying color schemes to "fewer colors" for better understandability.

## Limitations
- Lack of exact implementation details for both Unsupervised DeepView algorithm and MNIST learner architectures limits exact replication
- Small effect sizes and modest Cronbach's alpha values for scales suggest potential measurement reliability issues
- Between-subjects design limits causal inference about explanation type effects

## Confidence
- Trust calibration finding: Medium - statistical tests are clearly reported and design is sound, but exact replication of visualizations and uncertainty patterns is uncertain
- Satisfaction difference finding: Medium - potentially sensitive to translation, visualization design, and user comprehension
- Simplicity recommendation: Low - this is more of a design recommendation than a directly tested hypothesis

## Next Checks
1. Verify that the Unsupervised DeepView visualizations clearly show distinguishable uncertainty patterns between the regularized and non-regularized learners, matching the study's claims
2. Replicate the survey comprehension and satisfaction measures, ensuring German translations are accurate and visualizations are as described
3. Confirm that the two MNIST learners, with their specified accuracies and uncertainty profiles, are correctly implemented and produce the intended trust calibration effects