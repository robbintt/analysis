---
ver: rpa2
title: 'ES-C51: Expected Sarsa Based C51 Distributional Reinforcement Learning Algorithm'
arxiv_id: '2510.15006'
source_url: https://arxiv.org/abs/2510.15006
tags:
- es-c51
- ql-c51
- learning
- environments
- expected
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the instability in the C51 distributional
  reinforcement learning algorithm caused by greedy action selection when multiple
  actions have similar expected rewards but different variances. The proposed ES-C51
  algorithm replaces the greedy Q-learning update with an Expected Sarsa update using
  a softmax policy to combine information from all possible actions, reducing instability
  and improving learning performance.
---

# ES-C51: Expected Sarsa Based C51 Distributional Reinforcement Learning Algorithm

## Quick Facts
- **arXiv ID:** 2510.15006
- **Source URL:** https://arxiv.org/abs/2510.15006
- **Authors:** Rijul Tandon; Peter Vamplew; Cameron Foale
- **Reference count:** 23
- **Primary result:** ES-C51 improves stability and performance of C51 by replacing greedy Bellman backups with softmax-weighted Expected Sarsa backups, particularly in environments where multiple actions have similar expected rewards but different variances.

## Executive Summary
This study addresses instability in the C51 distributional reinforcement learning algorithm caused by greedy action selection when multiple actions have similar expected rewards but different variances. The proposed ES-C51 algorithm replaces the greedy Q-learning update with an Expected Sarsa update using a softmax policy to combine information from all possible actions, reducing instability and improving learning performance. Experiments on classic control environments from Gym and Atari-10 games show that ES-C51 outperforms the baseline QL-C51 algorithm across many environments. The softmax-based approach provides better exploration and stability, especially in early training stages. Performance improvements are achieved with comparable computational cost to the baseline algorithm.

## Method Summary
ES-C51 modifies the C51 algorithm by replacing the greedy Bellman backup with a softmax-weighted Expected Sarsa backup. Instead of using only the greedy action's distribution for the target, ES-C51 computes a mixture distribution by taking the weighted sum of all action distributions in the next state, where weights come from a softmax policy over Q-values. The temperature parameter controlling the softmax decays from 1.0 to 0.01 over 75% of training, allowing a smooth transition from exploration to exploitation. This modification aims to reduce instability when multiple actions have similar expected returns but different variances, which can cause repeated switches in greedy action selection in standard C51.

## Key Results
- ES-C51 outperforms QL-C51 baseline on classic control environments from Gym and Atari-10 games
- Softmax-based updates provide better exploration and stability, especially during early training stages
- Performance improvements are achieved with comparable computational cost to the baseline algorithm
- The method is particularly effective when multiple actions have similar expected rewards but different variances

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Replacing greedy Bellman backups with softmax-weighted Expected Sarsa backups reduces distributional instability when multiple actions have similar expected returns but different variances.
- **Mechanism:** In standard C51 (QL-C51), the target distribution comes from a single greedy action, which can switch repeatedly when actions have similar expectations but different variances, creating unstable updates. ES-C51 computes a mixture distribution using softmax weights over all actions, smoothing over action ambiguity by pooling distributional information.
- **Core assumption:** Instability stems primarily from greedy action switching rather than from distributional approximation error or network architecture.
- **Evidence anchors:** Abstract states the study addresses instability from greedy action selection with similar expected rewards but different variances; related work on Expected Sarsa shows stability benefits in tabular settings but not specifically in distributional RL.
- **Break condition:** If the environment has truly distinct optimal actions (no near-ties in expected value), the smoothing overhead provides no benefit and may slow convergence.

### Mechanism 2
- **Claim:** Softmax-weighted updates provide implicit variance regularization by down-weighting high-variance actions proportional to their softmax probability.
- **Mechanism:** Actions with high variance in their return distributions tend to produce more erratic Q-values during early training, reducing their softmax probability. These actions contribute less to the target mixture, effectively filtering out unstable distributional information while still allowing exploration.
- **Core assumption:** High-variance actions correlate with unreliable Q-estimates during early learning; this correlation diminishes as training progresses.
- **Evidence anchors:** Paper states high-variance actions might cause unstable performance; modification is designed to reduce variance and improve stability during early learning when Q-value estimates are noisy.
- **Break condition:** If high-variance actions are systematically undervalued (e.g., risky but optimal actions in exploration-sensitive tasks), softmax down-weighting could prevent learning optimal policies.

### Mechanism 3
- **Claim:** Decaying temperature schedule allows ES-C51 to smoothly transition from exploratory Expected Sarsa behavior to near-greedy exploitation without discrete policy switches.
- **Mechanism:** Temperature τ decays from 1.0 to 0.01 over 75% of training. High τ → uniform softmax → all actions contribute equally to target distribution (exploration). Low τ → peaked softmax → targets approach greedy Q-learning (exploitation). This avoids the hard transition in ε-greedy where exploration suddenly drops.
- **Core assumption:** Gradual exploration decay is beneficial; the specific 75% decay horizon is a reasonable default across diverse environments.
- **Evidence anchors:** Paper states ES-C51 naturally converges to a greedy policy as τ approaches zero; performance occasionally loses edge after τ decays to zero.
- **Break condition:** If optimal policy requires sustained exploration (e.g., highly stochastic environments with delayed rewards), aggressive temperature decay may premature exploitation.

## Foundational Learning

- **Concept: Categorical Distributional RL (C51)**
  - **Why needed here:** ES-C51 modifies C51's distributional Bellman backup. You must understand how C51 represents return distributions as discrete atoms {z_i} with probabilities p_θ(s,a,z_i) and how the projection operator φ handles shifted atoms.
  - **Quick check question:** Given a 3-atom distribution with support [0, 5, 10] and probabilities [0.2, 0.5, 0.3], what is the expected return? If the Bellman shift produces atoms [1.5, 6.5], how does projection redistribute mass?

- **Concept: Temporal Difference Backup Variants (Q-learning vs Expected Sarsa)**
  - **Why needed here:** The core modification is replacing max_a Q(s',a) with Σ_a π(a|s') Q(s',a). Understanding why this reduces maximization bias and variance is essential.
  - **Quick check question:** In a 2-action state where Q(s',a1) = Q(s',a2) = 10 but π(a1|s') = 0.9, π(a2|s') = 0.1, what target does Q-learning use vs Expected Sarsa?

- **Concept: Distributional Bellman Operator**
  - **Why needed here:** ES-C51 modifies how the distributional Bellman operator T^π combines next-state distributions. You need to understand that T^π Z(s,a) = R(s,a) + γ Z(s',a') operates on distributions, not scalars.
  - **Quick check question:** Why can't we simply average the expected values of next-action distributions? What information is preserved by mixing the full distributions?

## Architecture Onboarding

- **Component map:**
  State s_t → Neural Network f_θ → Logits for all (s_t, a, z_i) → Softmax over atoms → Distribution p_θ(s_t, a, z_i) for each action → Expected value Q(s_t,a) = Σ z_i · p_θ(s_t,a,z_i) → Softmax policy π_τ(a|s_t) → Sample action a_t

  Target computation (ES-C51):
  s' → Target Network f_θ' → Distributions Z(s', a') for all actions → Softmax weights π_τ(a'|s') → Mixture distribution Z̄(s') = Σ π_τ(a'|s') · Z(s', a') → Shift by r + γ → Projection φ → Z_target → Cross-entropy loss with predicted distribution

- **Critical path:** The key modification is computing Z̄(s') using softmax weights over all actions, requiring forward passes for ALL actions in the action space (not just the greedy action).

- **Design tradeoffs:**
  - QL-C51: Single forward pass for greedy action → faster per-step, but unstable under action ambiguity
  - ES-C51: |A| forward passes → higher compute, but stable targets; paper reports comparable runtime due to "streamlined gradient updates"
  - Temperature schedule: Paper uses τ = max(1.0 × (1 - t/(0.75×T)), 0.01). Alternative: fixed τ or slower decay for sustained exploration.

- **Failure signatures:**
  - ES-C51 underperforms in stochastic Atari environments after τ → 0.01 (e.g., Frostbite-v0, BattleZone-v0)
  - High-variance negative outliers in stochastic settings (Figure 3b shows -36% improvement in Amidar-v0)
  - Assumption: This occurs when early softmax exploration commits to suboptimal high-variance actions before value estimates stabilize

- **First 3 experiments:**
  1. **Ablation on temperature schedule:** Compare the paper's 75% decay against 50% and 100% (no decay) on CartPole-v1 and one Atari game. Measure stability (variance across seeds) and final performance.
  2. **Action ambiguity injection:** Create a modified CartPole where two actions have identical true expected returns but different variances (e.g., add noise to one action's reward). Verify ES-C51's advantage emerges specifically under this condition.
  3. **Target computation profiling:** Measure per-step runtime broken down by: (a) forward passes, (b) softmax computation, (c) projection operator. Validate the paper's claim that ES-C51 has "comparable computational cost" and identify where the claimed efficiency gains originate.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the optimal initial temperature (τ) and decay rate be systematically determined for a specific ES-C51 application?
- **Basis in paper:** The authors state there is a "broad opportunity for future work to determine the most optimal initial τ value and decay rate for each algorithm–environment pair."
- **Why unresolved:** The study uses a fixed decay formula but notes that performance is highly sensitive to these parameters, and suboptimal settings cause performance drops.
- **What evidence would resolve it:** An adaptive mechanism for tuning τ that maintains stability without requiring manual tuning for every environment.

### Open Question 2
- **Question:** Why does ES-C51 underperform compared to QL-C51 in specific stochastic environments like BattleZone and Amidar?
- **Basis in paper:** Table 3 shows that ES-C51 yields negative improvements in several stochastic settings, yet the paper does not analyze the cause of these regression cases.
- **Why unresolved:** The analysis focuses on the average success of the method, leaving the failure modes in "volatile" stochastic environments unexplained.
- **What evidence would resolve it:** A detailed analysis of the learned distributions in these specific environments to identify why the softmax mixture fails to converge.

### Open Question 3
- **Question:** Can the Expected Sarsa update strategy be effectively generalized to other distributional RL algorithms like QR-DQN or IQN?
- **Basis in paper:** The paper focuses solely on C51, while acknowledging other forms of DRL like quantile-based methods in the literature survey.
- **Why unresolved:** It is unknown if the benefits of averaging distributions persist when the distribution is represented by quantiles rather than categorical atoms.
- **What evidence would resolve it:** Experimental results applying the ES-C51 update logic to quantile-based DRL architectures.

## Limitations

- The claimed computational parity with QL-C51 is not fully explained; the mechanism for achieving comparable runtime with |A| target network forward passes is unclear.
- The variance regularization mechanism assumes high-variance actions correlate with unreliable Q-estimates during early training—this correlation is plausible but not empirically validated in the paper.
- The paper does not report statistical significance tests for the performance improvements, making it difficult to distinguish meaningful gains from noise.

## Confidence

- **Mechanism 1 (Greedy vs Expected Sarsa replacement):** Medium - The theoretical motivation is sound and the ablation is well-designed, but direct corpus evidence for this specific distributional RL context is limited.
- **Mechanism 2 (Variance regularization):** Low - The correlation between high-variance actions and unreliable Q-estimates is assumed but not validated in the paper.
- **Mechanism 3 (Temperature schedule):** Medium - The 75% decay to τ=0.01 is presented as a design choice without extensive sensitivity analysis, and performance degradation in some environments suggests it may not be universally optimal.

## Next Checks

1. **Temperature schedule ablation:** Compare 75%, 50%, and 100% decay schedules on CartPole-v1 and one Atari game, measuring both final performance and training stability across seeds.

2. **Action ambiguity test:** Create a controlled environment where two actions have identical expected returns but different variances, then verify ES-C51 specifically outperforms QL-C51 under this condition.

3. **Computational profiling:** Measure per-step runtime breakdown (forward passes, softmax, projection) to validate the claim of comparable computational cost and identify where efficiency gains originate.