---
ver: rpa2
title: Robust Exploratory Stopping under Ambiguity in Reinforcement Learning
arxiv_id: '2510.10260'
source_url: https://arxiv.org/abs/2510.10260
tags:
- given
- stopping
- ambiguity
- optimal
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a robust reinforcement learning framework for
  optimal stopping problems under ambiguity. The key idea is to combine the g-expectation
  framework for ambiguity aversion with entropy regularization to incorporate exploration
  into stopping rules.
---

# Robust Exploratory Stopping under Ambiguity in Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.10260
- Source URL: https://arxiv.org/abs/2510.10260
- Reference count: 40
- Key outcome: Combines g-expectation framework for ambiguity aversion with entropy regularization to derive a robust RL algorithm for optimal stopping that outperforms non-robust methods under distributional shifts

## Executive Summary
This paper addresses optimal stopping problems under ambiguity by integrating the g-expectation framework for robust decision-making with entropy regularization for exploration. The authors reformulate the stopping problem as an entropy-regularized optimal control problem, deriving the optimal Bernoulli-distributed control characterized by backward stochastic differential equations. A policy iteration theorem is established and implemented as a reinforcement learning algorithm. Numerical experiments demonstrate convergence and robustness across different ambiguity and exploration levels, showing the algorithm outperforms non-robust methods when facing distributional shifts in testing environments.

## Method Summary
The authors formulate optimal stopping under ambiguity as an entropy-regularized robust optimal control problem using g-expectation. They characterize the value function and optimal control via backward stochastic differential equations (BSDEs), then establish a policy iteration theorem for convergence. The algorithm implements policy iteration through a deep splitting method, where the value network is trained to solve the BSDE for the current policy, followed by policy updates using the logistic function. The implementation uses Euler-Maruyama discretization for simulating Geometric Brownian Motion trajectories, with volatility learned from realized quadratic covariation.

## Key Results
- Policy iteration algorithm converges to BSDE solution with monotonic improvement
- Robust algorithm (ε > 0) significantly outperforms non-robust (ε = 0) under drift misspecification
- Higher ambiguity levels (ε) provide greater robustness at the cost of potentially lower returns in- distribution

## Why This Works (Mechanism)
The g-expectation framework models ambiguity aversion by evaluating outcomes under worst-case perspectives across plausible probability measures, replacing standard linear expectation. Backward stochastic differential equations characterize the value function and optimal control in continuous time, providing the theoretical foundation. Entropy regularization provides principled randomization of stopping decisions, enabling exploration while maintaining convergence guarantees through the logistic-form policy controlled by temperature parameter λ.

## Foundational Learning

- Concept: **g-Expectation (Nonlinear Expectation)**
  - Why needed here: Mathematical operator modeling ambiguity aversion, replacing standard expectation to allow worst-case evaluation across plausible probability measures
  - Quick check question: If the driver function `g` is set to zero, what does the g-expectation reduce to?

- Concept: **Backward Stochastic Differential Equations (BSDEs)**
  - Why needed here: Characterize value function and optimal control for continuous-time problem under ambiguity, forming the theoretical backbone
  - Quick check question: A standard SDE moves forward from an initial condition. From what condition does a BSDE evolve, and in what direction?

- Concept: **Entropy-Regularized Exploration**
  - Why needed here: Provides principled randomization of stopping decisions rather than hard thresholds, leading to logistic-form policy controlled by temperature λ
  - Quick check question: How does increasing the temperature parameter λ affect the randomness of the stopping policy π?

## Architecture Onboarding

- Component map:
  1. **Parameter Store**: Stores hyperparameters (penalty N, temperature λ, ambiguity ε) and neural network parameters ϑ
  2. **Environment Simulator**: Generates state trajectories via Euler-Maruyama scheme, computes realized quadratic covariation Σ
  3. **Value Network**: Feed-forward network v_i(x; ϑ_i) approximating value function at each time step
  4. **Policy Module**: Computes stopping probability π_t using logistic function applied to R(X_t) - v(t, X_t; ϑ)
  5. **Loss & Optimizer**: Computes L2-loss from deep splitting method, updates value network via Adam optimizer

- Critical path:
  1. Initialize policy π_1
  2. **Outer Loop (Policy Iteration)**:
     a. For each iteration n:
     b. **Inner Loop (Policy Evaluation)**: Train value network to solve BSDE for current policy π_n
        i. Simulate batch of state trajectories
        ii. For each time step i (backward from T to 0): compute loss, update parameters
     c. **Policy Update**: Generate π_{n+1} using newly trained value network
     d. Check for convergence

- Design tradeoffs:
  - **Ambiguity Level (ε)**: Higher ε increases robustness but may cause overly conservative policies
  - **Exploration Temperature (λ)**: Higher λ encourages exploration but slows convergence and may keep policy suboptimal
  - **Penalty Factor (N)**: Must be large enough for hard stopping approximation but not cause numerical instability

- Failure signatures:
  - **Value Explosion**: If N too high or rewards unbounded, BSDE solution may explode causing NaN gradients
  - **Stagnation**: If λ too low, policy may converge prematurely to suboptimal deterministic stopping rule
  - **High Variance Loss**: Insufficient trajectories cause high variance in deep splitting loss, preventing learning

- First 3 experiments:
  1. **Sanity Check (No Ambiguity)**: Set ε = 0 and small λ, compare learned solution against analytic Black-Scholes put to validate implementation
  2. **Convergence Analysis**: Fix moderate ε and λ, run policy iteration recording value function at each iteration, plot to verify monotonic convergence
  3. **Robustness Test**: Train models with ε = 0 and ε > 0 on reference model, evaluate on test environments with drifted parameters, compare relative error decay as drift increases

## Open Questions the Paper Calls Out

- **Open Question 1**: Can full error analysis of policy iteration algorithm be developed incorporating neural approximation errors into convergence rate?
  - Basis: Remark 4.6 states "We defer this direction to a future work" regarding error analysis with relaxed regularity conditions
  - Evidence needed: Theorem with convergence rates explicitly depending on neural network approximation errors

- **Open Question 2**: Under what conditions does ambiguity persist versus diminish as agent accumulates observations?
  - Basis: Introduction cites Chen and Epstein [11] asking whether ambiguity disappears with learning
  - Evidence needed: Theoretical or empirical characterization relating optimal ambiguity level to sample size and observation noise

- **Open Question 3**: How does algorithm perform under volatility misspecification versus drift misspecification?
  - Basis: Numerical experiments only test drift misspecification, while g-expectation models volatility uncertainty
  - Evidence needed: Experiments with testing environments having different volatility specifications

- **Open Question 4**: Can framework extend to non-concave driver functions g?
  - Basis: Definition 2.1(iii) requires concavity of g, limiting ambiguity preferences that can be modeled
  - Evidence needed: Either counterexample showing non-concave drivers cause ill-posedness, or theoretical generalization

## Limitations
- Volatility learning from realized quadratic covariation requires sufficient data and may fail in high-noise regimes
- Euler-Maruyama discretization introduces bias for larger time steps
- Policy iteration convergence relies on smoothness assumptions about driver g and value functions that may not hold for all stopping problems

## Confidence

- **High Confidence**: Reformulation as entropy-regularized control problem, BSDE characterization, policy iteration scheme derivation, numerical convergence and robustness demonstration
- **Medium Confidence**: Practical implementation details for learning volatility from quadratic covariation (described but not fully specified)
- **Low Confidence**: Magnitude of improvement over non-robust methods (supported but not quantified beyond visual inspection)

## Next Checks
1. **Sensitivity Analysis**: Perform grid search over N, λ, ε to identify most influential factors on convergence speed and final policy performance
2. **Non-Markov State Extension**: Modify experiment to use non-Markov state (e.g., moving average of past prices), verify convergence and robustness properties maintained
3. **Computational Complexity**: Profile training time and memory usage per iteration to measure scalability to higher-dimensional stopping problems