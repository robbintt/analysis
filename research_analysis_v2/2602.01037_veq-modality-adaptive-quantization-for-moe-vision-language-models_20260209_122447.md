---
ver: rpa2
title: 'VEQ: Modality-Adaptive Quantization for MoE Vision-Language Models'
arxiv_id: '2602.01037'
source_url: https://arxiv.org/abs/2602.01037
tags:
- quantization
- tokens
- expert
- experts
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces VEQ, a novel post-training quantization
  framework designed to address the unique challenges of compressing Mixture-of-Experts
  (MoE) Vision-Language Models (VLMs). Existing quantization methods struggle with
  two critical forms of heterogeneity in MoE VLMs: the inherent discrepancy between
  vision and language tokens, and the non-uniform contribution of different experts.'
---

# VEQ: Modality-Adaptive Quantization for MoE Vision-Language Models

## Quick Facts
- arXiv ID: 2602.01037
- Source URL: https://arxiv.org/abs/2602.01037
- Authors: Guangshuo Qin; Zhiteng Li; Zheng Chen; Weihang Zhang; Linghe Kong; Yulun Zhang
- Reference count: 8
- Key outcome: VEQ achieves 2.04% and 3.09% average accuracy gains over state-of-the-art baselines for 3-bit weight quantization on Kimi-VL and Qwen3-VL respectively

## Executive Summary
This paper introduces VEQ, a novel post-training quantization framework designed to address the unique challenges of compressing Mixture-of-Experts (MoE) Vision-Language Models (VLMs). Existing quantization methods struggle with two critical forms of heterogeneity in MoE VLMs: the inherent discrepancy between vision and language tokens, and the non-uniform contribution of different experts. VEQ addresses these challenges through two key components: Modality-Expert-Aware Quantization, which assigns importance scores to experts based on activation frequency and modality sensitivity, and Modality-Affinity-Aware Quantization, which constructs an enhanced Hessian matrix by integrating token-expert affinity with modality information. Extensive experiments on Kimi-VL and Qwen3-VL demonstrate that VEQ consistently outperforms state-of-the-art baselines, achieving significant average accuracy gains of 2.04% and 3.09% respectively under 3-bit weight quantization across diverse multimodal benchmarks.

## Method Summary
VEQ is a post-training quantization (PTQ) framework specifically designed for MoE Vision-Language Models. It addresses two key challenges: the heterogeneity between vision and text tokens, and the non-uniform contribution of different experts. The framework consists of two components: VEQ-ME (Modality-Expert-Aware Quantization) uses importance scores based on expert activation frequency and modality sensitivity to weight reconstruction loss during grid search; VEQ-MA (Modality-Affinity-Aware Quantization) constructs an enhanced Hessian matrix that integrates token-expert affinity and modality information for GPTQ-style weight updates. The method requires calibration data to compute gradient ratios (approximately 22.4x for text vs vision tokens), expert activation counts, and router affinity scores.

## Key Results
- Achieves 2.04% average accuracy gain over state-of-the-art baselines for 3-bit weight quantization on Kimi-VL
- Achieves 3.09% average accuracy gain over state-of-the-art baselines for 3-bit weight quantization on Qwen3-VL
- Demonstrates consistent performance improvements across diverse multimodal benchmarks including MMMU, MMBench, AI2D, and various VQA datasets
- Maintains robustness in low-bit regimes where standard methods show catastrophic collapse (e.g., <40% on MMMU)

## Why This Works (Mechanism)

### Mechanism 1: Importance-Weighted Expert Reconstruction
- Claim: Assigning importance scores to experts based on activation frequency and modality sensitivity preserves overall model accuracy better than uniform error minimization.
- Mechanism: VEQ calculates an importance score $S_i = \gamma \cdot N^{text}_i + \beta \cdot N^{vis}_i$ for each expert. This score weights the reconstruction loss (Eq. 3), forcing the quantizer to prioritize minimizing error for "hot" experts that are frequently activated by high-sensitivity text tokens.
- Core assumption: Assumes that activation frequency and gradient magnitude are reliable proxies for an expert's contribution to the final inference result.
- Evidence anchors:
  - [abstract] "utilizes expert activation frequency to prioritize error minimization for pivotal experts"
  - [section 3.2] "importance score $S_i$... weighted summation... scaling down the frequent visual activations"
  - [corpus] MoEQuant and EAQuant similarly utilize expert affinity/activation to guide quantization, validating the heuristic.

### Mechanism 2: Affinity-Modulated Hessian Calibration
- Claim: Incorporating router confidence (affinity) and modality indicators into the Hessian matrix improves the precision of weight updates during calibration.
- Mechanism: The method constructs an enhanced Hessian $\tilde{H} = XCX^\top$ (Eq. 4). The diagonal matrix $C$ re-weights tokens using router affinity $p_j$ and a modality sensitivity factor $\alpha_j$. This ensures the second-order optimization prioritizes tokens the router is confident about.
- Core assumption: Assumes that the router's softmax probabilities (affinity) correlate with the semantic importance of the token-expert pair.
- Evidence anchors:
  - [abstract] "constructs an enhanced Hessian matrix by integrating token-expert affinity with modality information"
  - [section 3.3] "Tokens with high router affinity and high modality sensitivity exert a larger influence"
  - [corpus] Corpus papers (e.g., MoEQuant) support "affinity guidance" as a valid signal for MoE compression.

### Mechanism 3: Gradient-Based Modality Separation
- Claim: Treating vision and text tokens with distinct sensitivity weights is necessary because text tokens exhibit significantly higher gradient norms (information density).
- Mechanism: Analysis reveals text tokens have gradient norms ~22.4x higher than vision tokens (Figure 4). VEQ uses this ratio ($\gamma$) to scale the importance of text-processing experts and modulate the Hessian, preventing the "drowning out" of text signals by redundant visual tokens.
- Core assumption: Assumes that the gradient norm measured during a specific SFT phase generalizes to the inference-time sensitivity to quantization noise.
- Evidence anchors:
  - [section 3.1.1] "average gradient magnitude of text tokens exceeds that of visual tokens by a factor of 22.4"
  - [figure 4] Visualization of gradient ratios across 128 samples.
  - [corpus] Weak/missing direct confirmation of the "22.4x" factor in corpus, making this a paper-specific empirical finding.

## Foundational Learning

- Concept: **Mixture-of-Experts (MoE) Sparsity**
  - Why needed here: Unlike dense models, MoEs activate only a subset of parameters (experts) per token. Understanding this is critical because VEQ exploits the non-uniform "load imbalance" (hot vs. dormant experts) to guide quantization.
  - Quick check question: Why would a uniform quantization strategy fail on an MoE layer where 80% of tokens activate only 20% of the experts?

- Concept: **Post-Training Quantization (PTQ) Calibration**
  - Why needed here: VEQ is a PTQ method. You must understand that PTQ uses a small dataset (calibration set) to determine quantization parameters (scales/zeros) without retraining the model weights.
  - Quick check question: What specifically does VEQ modify during the calibration pass: the model weights, the router weights, or the quantization scale parameters?

- Concept: **Hessian-Guided Optimization**
  - Why needed here: VEQ-MA modifies the Hessian matrix (second-order derivative information) used by methods like GPTQ. You need to know that the Hessian approximates the "curvature" of the error landscape to guide better weight rounding.
  - Quick check question: In VEQ-MA, what does the diagonal matrix $C$ effectively do to the Hessian matrix $H$?

## Architecture Onboarding

- Component map:
  - Inputs: Vision & Text Tokens (distinct statistical distributions)
  - MoE Layer: Router + Sparse Experts (FFNs)
  - VEQ Wrapper:
    - *VEQ-ME:* Calculates $S_i$ (Importance Score) → Weights Reconstruction Loss (for AWQ-style grid search)
    - *VEQ-MA:* Calculates Affinity/Modality Weights → Modifies Hessian $\tilde{H}$ (for GPTQ-style weight update)
  - Quantizer: Outputs INT3/INT4 weights

- Critical path:
  1. Run calibration data through the model
  2. Intercept activations at MoE layers
  3. **VEQ-ME Path:** Count expert activations ($N^{text}_i, N^{vis}_i$) and compute gradient ratios ($\gamma$) to derive $S_i$
  4. **VEQ-MA Path:** Capture router logits ($p_j$) and modality tags to construct diagonal scaling matrix $C$
  5. Apply quantization (grid search or Hessian update) using these derived weights

- Design tradeoffs:
  - **VEQ-ME vs. VEQ-MA:** VEQ-ME (AWQ-based) focuses on identifying "which expert matters"; VEQ-MA (GPTQ-based) focuses on "how to optimally quantize specific channels" given modality affinity
  - **Calibration Cost:** Computing the gradient ratio $\gamma$ and affinity metrics adds overhead to the calibration pass compared to vanilla PTQ
  - **Generalization:** The fixed gradient ratio (22.4x) is derived from specific datasets (COCO); may need tuning for drastically different domains

- Failure signatures:
  - **Text Degradation:** If the modality scaling factor $\gamma$ is removed (set to 1), performance on reasoning tasks (MMMU) drops significantly (Table 2)
  - **Expert Collapse:** If $\beta$ (quantity normalization) is ignored, scores are dominated by massive vision token counts, degrading text-focused expert accuracy
  - **Low-Bit Crash:** Standard methods (RTN/AWQ) show catastrophic collapse at W3 (e.g., <40% on MMMU), while VEQ maintains robustness

- First 3 experiments:
  1. **Sanity Check (Ablation):** Implement VEQ-ME with $\gamma=1$ and $\beta=1$ on a single MoE layer. Verify that removal of modality/quantity normalization increases reconstruction error on text-heavy tokens
  2. **Hyperparameter Sweep:** Run a grid search on the gradient ratio $\gamma$ (e.g., 1x to 30x) for a validation set (Figure 7a style) to confirm the paper's "22.4x" finding holds for your specific model checkpoint
  3. **End-to-End Benchmark:** Quantize Kimi-VL or Qwen3-VL to W3 using VEQ-MA. Compare zero-shot accuracy on MMMU and AI2D against a GPTQ baseline to reproduce the ~2-3% gain claimed in Table 1

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the modality-expert-aware importance scoring (VEQ-ME) and the modality-affinity-aware Hessian refinement (VEQ-MA) be unified into a single optimization framework?
- Basis in paper: [inferred] Table 1 presents VEQ-ME and VEQ-MA as separate implementations built upon different baselines (AWQ and GPTQ), leaving their potential interaction unexplored.
- Why unresolved: VEQ-ME uses a weighted grid search, while VEQ-MA uses an enhanced Hessian matrix; the mathematical compatibility of these distinct optimization objectives is unknown.
- What evidence would resolve it: Experiments applying both the importance score $S_i$ and the enhanced Hessian $\tilde{H}$ simultaneously during the calibration of a single model.

### Open Question 2
- Question: Does VEQ effectively extend to weight-activation quantization (e.g., W4A4), or is its efficacy limited to weight-only scenarios?
- Basis in paper: [explicit] The paper focuses exclusively on weight-only quantization (W3A16, W4A16), as shown in Table 1 and stated in the abstract.
- Why unresolved: Activation quantization introduces outliers distinct from weight distribution issues. It is unclear if the token-affinity and expert-importance metrics sufficiently handle the dynamic range of activations.
- What evidence would resolve it: Evaluation results of VEQ on W4A4 or W3A3 configurations, comparing stability against baselines like SmoothQuant.

### Open Question 3
- Question: How does VEQ perform on MoE architectures that utilize routing mechanisms different from the standard Top-K gating (e.g., Expert Choice or Soft MoE)?
- Basis in paper: [inferred] The method is validated only on Kimi-VL and Qwen3-VL, and the theoretical analysis in Section 3.1.2 relies on the "Top-k selection" logic to define "decisive experts."
- Why unresolved: Alternative routing mechanisms may not produce the same "hot expert" phenomena or skewed affinity distributions that VEQ relies upon to assign importance scores.
- What evidence would resolve it: Benchmarking VEQ on architectures like DeepSeek-VL2 (mentioned in introduction but not evaluated) or models with auxiliary-load balancing losses that flatten expert utilization.

## Limitations

- The 22.4x gradient ratio between text and vision tokens is model-specific and may not generalize across different VLMs or domains
- VEQ relies on a small calibration set (128 samples) whose representativeness for target applications is not guaranteed
- The method is validated only on standard Top-K MoE architectures and may not transfer effectively to alternative routing mechanisms

## Confidence

- **High Confidence:** The core algorithmic framework (VEQ-ME and VEQ-MA) is technically sound and builds on established PTQ methods (AWQ, GPTQ) with well-motivated modifications. The observed improvements over baselines on standard benchmarks are likely reproducible.
- **Medium Confidence:** The specific quantitative results (e.g., 2.04% and 3.09% accuracy gains) depend on precise implementation details and hyperparameters that may not be fully specified. The ablation studies provide strong support but may not capture all edge cases.
- **Low Confidence:** The universal applicability of the 22.4x gradient ratio and the robustness of VEQ to diverse calibration data and model architectures remain unproven. Claims about performance in "low-bit regimes" are based on comparisons to existing methods that also exhibit degradation, rather than demonstrating absolute robustness.

## Next Checks

1. **Gradient Ratio Sensitivity Analysis:** Implement a hyperparameter sweep over the gradient ratio γ (e.g., 1x to 30x) on a validation set to confirm whether the 22.4x finding holds for your specific model checkpoint and dataset. Report performance as a function of γ to identify the optimal range.

2. **Calibration Set Size Study:** Systematically vary the size of the calibration dataset (e.g., 32, 128, 512, 1024 samples) and measure the impact on final quantized model accuracy. This will quantify the sensitivity to calibration data representativeness and help establish minimum requirements.

3. **Cross-Model Transferability Test:** Apply VEQ to a third, previously unseen MoE VLM (e.g., LLaVA-NeXT with MoE or Qwen2-VL) and evaluate performance on a held-out benchmark. Compare results to those obtained on Kimi-VL and Qwen3-VL to assess generalizability across architectures.