---
ver: rpa2
title: 'Merge before Forget: A Single LoRA Continual Learning via Continual Merging'
arxiv_id: '2512.23017'
source_url: https://arxiv.org/abs/2512.23017
tags:
- lora
- learning
- tasks
- continual
- merging
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a parameter-efficient continual learning method
  that merges LoRA updates sequentially into a single shared LoRA to prevent catastrophic
  forgetting and reduce memory usage. The key idea is to initialize new task learning
  using orthogonal basis extracted from previous LoRA and then merge updates via time-aware
  scaling that exploits the asymmetry between LoRA components.
---

# Merge before Forget: A Single LoRA Continual Learning via Continual Merging

## Quick Facts
- arXiv ID: 2512.23017
- Source URL: https://arxiv.org/abs/2512.23017
- Reference count: 40
- Primary result: SLAO achieves superior average accuracy on multiple benchmarks with constant memory complexity by merging LoRA updates sequentially into a single shared LoRA.

## Executive Summary
This paper proposes a parameter-efficient continual learning method that merges LoRA updates sequentially into a single shared LoRA to prevent catastrophic forgetting and reduce memory usage. The key idea is to initialize new task learning using orthogonal basis extracted from previous LoRA and then merge updates via time-aware scaling that exploits the asymmetry between LoRA components. Experiments on multiple benchmarks with Llama and Qwen models show that the method achieves superior average accuracy compared to data-free baselines while maintaining constant memory complexity regardless of task count. Theoretical analysis justifies the design choices and highlights how orthogonal initialization and asymmetric merging improve task retention and adaptation.

## Method Summary
The method operates by fine-tuning LoRA modules sequentially on each task, then merging them into a single shared LoRA using three key mechanisms: orthogonal basis initialization (extracting Q from previous A via QR decomposition), asymmetric merging (updating only B component while keeping A fixed), and time-aware scaling (using λ(i) = 1/√i). For each new task, the method extracts an orthogonal basis from the previous task's A matrix, initializes the new task's A with this basis and B with the previous B, fine-tunes on the new task, then merges by updating A to the new fine-tuned value and merging B using the time-aware scaling factor. This achieves constant memory complexity O(1) regardless of task count while preventing catastrophic forgetting through the geometric consistency maintained by orthogonal initialization.

## Key Results
- Superior average accuracy compared to data-free baselines across Standard CL, Large Task Count, and SuperNI benchmarks
- Constant memory complexity O(1) regardless of task count, reducing memory usage by 97.6% compared to keeping all LoRAs
- Theoretical analysis showing orthogonal initialization minimizes both forgetting and intransigence errors in the NTK regime
- Time-aware scaling with λ(i) = 1/√i maintains consistent parameter magnitude across merging steps

## Why This Works (Mechanism)

### Mechanism 1: Orthogonal Basis Initialization
- **Claim**: Initializing new task LoRA using orthogonal basis extracted from previous task's A matrix minimizes both forgetting and intransigence errors by maintaining geometric consistency across tasks.
- **Mechanism**: QR decomposition extracts Q_i from (A_{ft,i-1})^T, creating A^(0)_{ft,i} = Q_i^T where A^(0)_{ft,i}(A^(0)_{ft,i})^T = I_r. This ensures E[A_j A_i^T] ≈ I_r across tasks, reducing ||B_t A_t - B_i A_i||_F in the NTK regime bound.
- **Core assumption**: LoRA fine-tuning operates within the Neural Tangent Kernel (NTK) regime where empirical risk can be linearly approximated.
- **Evidence anchors**: Abstract states orthogonal basis extraction prevents forgetting; section 3.2 shows forgetting-intransigence decomposition; related work uses rank-1 expert pools but not orthogonal initialization.
- **Break condition**: If tasks are fundamentally incompatible or NTK approximation fails (large learning rates), orthogonal initialization may not provide geometric alignment benefits.

### Mechanism 2: Asymmetric Merging of B Only
- **Claim**: Merging only the B component while directly updating A exploits inherent LoRA asymmetry to reduce task interference.
- **Mechanism**: Analysis shows ||ΔB_i^T B_{i-1}||_F < ||A_{i-1} ΔA_i^T||_F when η ≪ 1/L. B updates are more orthogonal to initialization than A updates, making B merging better for task isolation.
- **Core assumption**: Learning rate remains small (η ≪ 1/L) throughout training.
- **Evidence anchors**: Abstract mentions time-aware scaling mechanism; section 3.3 shows cosine similarity analysis with A having higher cross-task similarity than B across 15 tasks; related work notes orthogonal LoRA tuning mitigates forgetting.
- **Break condition**: If learning rate is too large, orthogonality relationship breaks down; if tasks require similar B subspaces, merging may still cause interference.

### Mechanism 3: Time-Aware Scaling (λ(i) = 1/√i)
- **Claim**: Adaptive scaling that decreases with task count maintains consistent magnitude of merged parameters across sequential tasks.
- **Mechanism**: Since task vectors from different tasks are approximately orthogonal, scaling by 1/√i preserves parameter magnitude across T merging steps: variance scales as T·(1/√T)² = O(1).
- **Core assumption**: Task vectors remain approximately orthogonal across the task sequence.
- **Evidence anchors**: Abstract mentions time-aware scaling to balance knowledge; section 4.1 states λ(i) = 1/√i maintains magnitude across merging steps; corpus papers don't analyze time-aware scaling specifically.
- **Break condition**: If task vectors have high cosine similarity, fixed scaling may over/under-weight new tasks.

## Foundational Learning

- **Concept: Low-Rank Adaptation (LoRA)**
  - Why needed here: The method operates entirely on LoRA's A (r×n) and B (m×r) matrices; understanding their roles is essential for grasping asymmetric merging.
  - Quick check question: Can you explain why B is initialized to zero while A uses Gaussian initialization in standard LoRA?

- **Concept: Catastrophic Forgetting in Sequential Learning**
  - Why needed here: The core problem being solved—models forget previous tasks when fine-tuned on new ones. Forgetting error (F_t) and intransigence error (I_t) are the evaluation metrics.
  - Quick check question: What is the difference between forgetting (losing old knowledge) and intransigence (failing to learn new tasks)?

- **Concept: Model Merging / Task Arithmetic**
  - Why needed here: This method reframes continual learning as sequential model merging; understanding task vector addition is prerequisite.
  - Quick check question: Why does adding task vectors work for models fine-tuned from the same checkpoint but not from different initializations?

## Architecture Onboarding

- **Component map**: Previous LoRA (A_{ft,i-1}, B_{ft,i-1}) → [QR Decomposition] → Orthogonal basis Q_i → [Initialize] → A^(0)_{ft,i} = Q_i^T, B^(0)_{ft,i} = B_{ft,i-1} → [Fine-tune on task i] → A_{ft,i}, B_{ft,i} → [Asymmetric Merge] → A^i_merge = A_{ft,i}, B^i_merge = B^{i-1}_merge + λ(i)(B_{ft,i} - B^{i-1}_merge) → Merged LoRA (single pair)

- **Critical path**: QR decomposition correctness → orthogonal initialization preserves I_r → fine-tuning stays in NTK regime → B merge maintains task isolation. If any step fails, forgetting increases.

- **Design tradeoffs**:
  - Memory (O(1) vs O(T)): Achieved, but requires re-fine-tuning on new tasks rather than replay
  - Orthogonality vs expressiveness: Strong orthogonality may limit task-specific adaptation in A
  - Fixed λ(i) = 1/√i vs learned: Simple but may not adapt to task difficulty variation

- **Failure signatures**:
  - High backward transfer (BWT < -5%): Check if orthogonal initialization is actually producing A^(0)(A^(0))^T ≈ I_r
  - Poor new task accuracy: B initialization from previous task may bias toward old tasks; consider partial reset
  - Order sensitivity (high MOPD): Time-aware scaling may underweight early tasks in long sequences

- **First 3 experiments**:
  1. **Sanity check**: Single LoRA on 2-task sequence, compare random vs orthogonal initialization—should see 3-5% forgetting reduction
  2. **Asymmetry validation**: Merge A-only vs B-only on 5-task benchmark—expect B-only merge to outperform by 5-10% average accuracy
  3. **Scaling ablation**: Compare λ ∈ {0.1, 0.5, 1/√i, 0.9} on large number of tasks (15 tasks)—1/√i should show lowest variance across task orders

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Is the fixed time-aware scaling factor λ(i) = 1/√i optimal for continual merging, or could task-adaptive or learned scaling coefficients further improve the accuracy-forgetting trade-off?
- **Basis in paper**: Section 4.1 states λ(i) = 1/√i "follows the continual merging method proposed in Tang et al. [2025]" without ablations on alternative schedules; Figure 4 only compares fixed values.
- **Why unresolved**: The paper does not explore whether task-specific scaling or meta-learned coefficients could better balance old and new knowledge.
- **What evidence would resolve it**: Systematic ablations with adaptive λ(i) schedules (e.g., based on task similarity, gradient norms, or validation performance) across benchmarks.

### Open Question 2
- **Question**: How robust are SLAO's theoretical guarantees and empirical performance when the NTK regime assumptions (small learning rate, early training dynamics) are violated?
- **Basis in paper**: Theoretical analysis in Section 3.2 and Appendix A relies on NTK approximations and assumes η ≪ 1/L, but real-world fine-tuning may violate these conditions.
- **Why unresolved**: The paper does not evaluate SLAO under larger learning rates or extended training epochs where NTK assumptions break down.
- **What evidence would resolve it**: Experiments varying learning rate magnitude and training duration, measuring deviation from theoretical bounds.

### Open Question 3
- **Question**: Does the asymmetry-based decision to merge only B (and not A) generalize across diverse task types, model scales, and layer depths?
- **Basis in paper**: Section 3.3 motivates merging B from empirical cosine similarity (Figure 1) on the last layer; Table 3 validates this but only on limited benchmarks.
- **Why unresolved**: The asymmetry observation may be layer-specific or task-dependent; deeper analysis across layers and more diverse task distributions is lacking.
- **What evidence would resolve it**: Layer-wise analysis of A/B similarity across tasks and ablations merging A, B, or both at different layers.

### Open Question 4
- **Question**: Can SLAO be extended to support positive backward transfer (BWT > 0), where learning new tasks improves earlier task performance?
- **Basis in paper**: Table 9 shows negative BWT (−3.5) indicating forgetting; the current merging formulation does not explicitly enable forward/backward knowledge transfer.
- **Why unresolved**: The sequential merging focuses on retention; mechanisms to leverage task correlations for transfer are unexplored.
- **What evidence would resolve it**: Modifications incorporating task similarity or gradient alignment during merging, evaluated on BWT metrics.

## Limitations
- **Sensitivity to task ordering**: Despite MOPD improvements, SLAO shows order sensitivity particularly for early tasks in long sequences.
- **Fixed time-aware scaling**: The λ(i) = 1/√i may not adapt well to heterogeneous task difficulties across diverse benchmarks.
- **Orthogonality assumption**: The method assumes sufficient task orthogonality, which may not hold for related or overlapping tasks.

## Confidence
- **Orthogonal initialization mechanism**: Medium confidence - theoretically justified but real-world fine-tuning often operates outside strict NTK conditions
- **Asymmetric B-only merging**: High confidence - strong empirical evidence from cosine similarity analysis, though theoretical justification limited to small learning rate regime
- **Time-aware scaling λ(i) = 1/√i**: Medium confidence - empirically effective but lacks rigorous theoretical grounding beyond variance preservation arguments

## Next Checks
1. **Learning rate sensitivity**: Test SLAO across η ∈ {1e-5, 5e-5, 1e-4, 2e-4} to identify breaking points where orthogonal initialization and asymmetric merging lose effectiveness.

2. **Task similarity robustness**: Evaluate on semantically related task sequences (e.g., multiple sentiment analysis datasets) to test whether the orthogonality assumptions break down and measure forgetting in correlated task spaces.

3. **Orthogonal initialization ablation**: Compare against alternative initializations (zero, random, or learned meta-initialization) on diverse task sets to quantify the exact contribution of the QR-based orthogonal basis extraction mechanism.