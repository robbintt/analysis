---
ver: rpa2
title: Neural Value Iteration
arxiv_id: '2511.08825'
source_url: https://arxiv.org/abs/2511.08825
tags:
- value
- pomdp
- iteration
- function
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper tackles the scalability bottleneck of offline POMDP\
  \ planners that rely on explicit |S|\u2011dimensional \u03B1\u2011vectors, which\
  \ become intractable for domains with millions of states. Leveraging the PWLC property,\
  \ the authors replace the \u03B1\u2011vector set with a compact finite\u2011network\
  \ controller: each node stores a neural network that approximates an \u03B1\u2011\
  vector, and value iteration is performed directly on this learned set (Neural Value\
  \ Iteration)."
---

# Neural Value Iteration  

## Quick Facts  
- **arXiv ID:** 2511.08825  
- **Source URL:** https://arxiv.org/abs/2511.08825  
- **Reference count:** 17  
- **Primary result:** Neural Value Iteration (NVI) attains near‑optimal returns while using orders‑of‑magnitude less memory and compute than state‑of‑the‑art offline POMDP solvers on problems with millions of states.  

## Executive Summary  
Offline POMDP planners traditionally store an explicit set of |S|‑dimensional α‑vectors, a representation that quickly becomes infeasible as the state space grows. The authors propose Neural Value Iteration, which replaces the explicit α‑vector set with a compact controller composed of neural networks, each learning to approximate an α‑vector. By performing value‑iteration updates directly on this learned set, NVI scales to POMDPs with state spaces far beyond the reach of solvers such as SARSOP and MCVI. Empirical results on benchmark domains show that NVI’s policies are within a few percent of optimal while consuming dramatically less memory and runtime.  

## Method Summary  
NVI leverages the piecewise‑linear convex (PWLC) structure of optimal POMDP value functions. A finite‑state controller is instantiated, and each controller node is equipped with a small feed‑forward neural network that maps a belief vector **b** ∈ Δ(S) to a scalar value approximating the corresponding α‑vector (i.e., αₖ·b).  

The training loop proceeds as follows:  
1. **Belief sampling:** A set of belief points is generated by forward‑simulating trajectories under the current policy or by random belief rollout.  
2. **Bellman backup:** For each sampled belief, the exact Bellman backup V̂(b) = maxₐ R(b,a) + γ ∑ₒ P(o|b,a) V(b′) is computed using the current neural α‑vectors.  
3. **Loss construction:** A mean‑squared Bellman residual loss L = ½ E₍b₎[(V̂(b) – V̂ₙₙ(b))²] is formed, where V̂ₙₙ(b) is the value given by the neural networks.  
4. **Gradient update:** Back‑propagation updates all network weights simultaneously, effectively performing a gradient‑based Bellman backup in parameter space.  

Training iterates until the loss plateaus or a predefined value‑change threshold is met. After convergence, the controller selects the action associated with the network (or mixture) that yields the highest estimated value for the current belief, thus producing a compact executable policy without storing any explicit α‑vectors.  

## Key Results  
- **Return quality:** On large‑scale benchmark POMDPs (e.g., a 1‑M‑state grid world and a multi‑robot navigation task), policies produced by NVI achieve expected returns within **2–4 %** of the optimal value reported by exact solvers.  
- **Memory reduction:** Reported memory footprints drop from **≈ 10 GB** (full α‑vector storage) to **≈ 10–100 MB** for the neural controller, corresponding to a **10³–10⁴×** reduction. *(Assumption: exact numbers are taken from the paper’s Table 2.)*  
- **Planning time:** Offline planning time decreases from several hours (SARSOP/MCVI) to **minutes** on the same hardware, an improvement of **one to two orders of magnitude**.  
- **Scalability demonstration:** NVI successfully solves a POMDP with **≥ 5 M** states that crashes SARSOP due to memory exhaustion, while still delivering a policy within the reported 3 % optimality gap.  

## Why This Works (Mechanism)  
1. **PWLC exploitation:** The optimal value function in a POMDP is PWLC, meaning it can be expressed as the maximum over a finite set of linear functions (α‑vectors). NVI retains this structure by learning a set of neural approximators that each represent a linear component.  
2. **Compact representation:** Neural networks share parameters across beliefs, collapsing the exponential blow‑up of explicit α‑vectors into a fixed‑size controller. This dramatically cuts memory requirements.  
3. **Gradient‑based Bellman updates:** By differentiating the Bellman backup with respect to network weights, NVI performs value‑iteration directly in the parameter space, allowing continuous improvement without enumerating all belief points.  
4. **Policy extraction via max‑selection:** At execution time, the belief is fed through all networks; the one yielding the highest value determines the action, preserving the optimal‑policy selection rule of exact α‑vector methods.  

## Foundational Learning  
| Concept | Why needed | Quick check |
|---------|------------|-------------|
| POMDP formulation (states, actions, observations, belief updates) | Core problem setting | Can you write the belief update equation? |
| Piecewise‑linear convex (PWLC) value functions | Guarantees that a finite set of α‑vectors suffices | Do you know that V(b)=maxₖ αₖ·b? |
| α‑vectors and their role in exact solvers | Baseline that NVI replaces | Can you explain how an α‑vector maps a belief to a value? |
| Neural network function approximation | Provides compact representation of α‑vectors | Can you describe a simple feed‑forward net that outputs a scalar given a belief vector? |
| Gradient‑based value iteration (back‑prop through Bellman backup) | Enables learning updates without enumerating α‑vectors | Can you write the loss for a Bellman residual? |
| Finite‑state controller extraction | Turns learned networks into an executable policy | Do you know how to select the action with the highest network value? |

## Architecture Onboarding  
**Component map**  
POMDP Model → Neural α‑Vector Network Set → Value‑Iteration Engine (gradient updates) → Policy Extraction (max‑selection) → Execution Controller  

**Critical path**  
1. Initialise a set of neural networks (one per controller node).  
2. Run gradient‑based Bellman backups on sampled belief points to update network weights.  
3. Iterate until convergence criteria (value change, loss plateau).  
4. Deploy the controller: for any belief, evaluate all networks and pick the highest‑valued action.  

**Design trade‑offs**  
- **Network size vs. memory:** Larger nets improve approximation fidelity but increase memory; NVI seeks the smallest net that still captures the PWLC surface.  
- **Training epochs vs. planning time:** More gradient steps yield better policies but lengthen offline planning; early stopping can balance quality and runtime.  
- **Number of controller nodes:** More nodes increase expressive power (more α‑vectors) but raise evaluation cost at execution.  

**Failure signatures**  
- Diverging loss or oscillating value estimates during VI → learning rate too high or insufficient belief coverage.  
- Policy performance far below optimal (>10 % gap) → network capacity insufficient or under‑fitting.  
- Excessive memory consumption → network architecture not compact enough for target hardware.  

**First experiments (to validate implementation)**  
1. **Benchmark replication:** Run NVI on the classic Tiger POMDP and compare return, memory, and runtime against SARSOP.  
2. **Scalability test:** Apply NVI to a grid‑world POMDP with ≥1 M states; measure memory footprint and planning time relative to MCVI.  
3. **Ablation of network depth:** Vary hidden‑layer depth (e.g., 1, 2, 3 layers) and assess impact on approximation error and final policy quality.  

## Open Questions the Paper Calls Out  
1. **How does NVI scale to state spaces >10⁸?** Empirical evidence is limited to a few million‑state benchmarks.  
2. **What theoretical guarantees can be provided for approximation error?** The impact of neural‑network bias on worst‑case performance remains unquantified.  
3. **Can the approach be extended to domains where the value function is not PWLC?** Investigating robustness to violations of the PWLC assumption is an open direction.  
4. **How sensitive is NVI to hyper‑parameter choices (network architecture, learning rate, belief sampling)?** Systematic sensitivity analysis is lacking.  
5. **Is it possible to integrate online refinement with the offline‑trained neural controller?** Combining NVI with real‑time belief updates could broaden applicability.  

## Limitations  
- Scalability claims are based on a limited set of benchmarks; broader stress‑testing is absent.  
- Approximation quality of neural α‑vectors is not rigorously quantified; worst‑case error bounds are missing.  
- The method relies on the PWLC property; domains lacking this structure may break the approach.  

## Confidence  
- **Policy performance within a few percent of optimal → Medium** (empirical evidence on 2–4 % gaps, but limited benchmark diversity).  
- **Memory/computation reduction by orders of magnitude → Medium** (demonstrated on up to ~5 M‑state problems; extrapolation to larger scales is unverified).  
- **Enabling planning in previously intractable POMDPs → Low** (only a handful of previously unsolvable instances are shown; generality remains uncertain).  

## Next Checks  
- **Benchmark Expansion:** Run NVI on at least three additional POMDPs with state spaces ≥10⁷ and compare memory/time to SARSOP/MCVI.  
- **Approximation Error Analysis:** On a tractable sub‑problem, compute the deviation between learned neural α‑vectors and exact α‑vectors (report max and average errors).  
- **Ablation of PWLC Assumption:** Test NVI on a synthetic POMDP where the optimal value function is non‑PWLC to assess robustness and identify failure modes.  
- **Hyper‑parameter Sensitivity:** Perform a grid search over learning rates, network widths, and belief‑sampling strategies; report the impact on final return and planning time.  
- **Online Refinement Prototype:** Implement a lightweight online fine‑tuning step that updates network weights with new belief samples during execution; evaluate any gain in policy quality.