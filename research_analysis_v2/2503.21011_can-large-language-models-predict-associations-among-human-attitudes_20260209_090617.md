---
ver: rpa2
title: Can Large Language Models Predict Associations Among Human Attitudes?
arxiv_id: '2503.21011'
source_url: https://arxiv.org/abs/2503.21011
tags:
- attitudes
- arxiv
- human
- gpt-4o
- social
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tests whether large language models (LLMs) can infer
  human attitudes from one another beyond surface-level semantic similarity. Using
  a novel dataset of 376 U.S.
---

# Can Large Language Models Predict Associations Among Human Attitudes?

## Quick Facts
- arXiv ID: 2503.21011
- Source URL: https://arxiv.org/abs/2503.21011
- Authors: Ana Ma; Derek Powell
- Reference count: 27
- Large language models can predict human attitudes from one another beyond surface-level semantic similarity

## Executive Summary
This paper tests whether large language models (LLMs) can infer human attitudes from one another beyond surface-level semantic similarity. Using a novel dataset of 376 U.S. participants rating 64 diverse attitude statements, GPT-4o was prompted to predict attitudes from other responses. The model successfully reconstructed pairwise correlations among attitudes and predicted individual responses, even when using semantically-dissimilar prompts. While similarity improved accuracy, GPT-4o remained highly capable of meaningful social inference across dissimilar topics, indicating it captures latent structure in human belief systems.

## Method Summary
The study collected attitude ratings from 376 U.S. participants who evaluated 64 diverse statements (derived from Pew surveys and OPINION QA) on a 5-point scale. GPT-4o was prompted to predict individual responses and pairwise correlations among attitudes using persona-based prompts that included known attitudes as context. The methodology tested whether models could infer attitudes beyond semantic similarity by comparing performance on similar versus dissimilar predictor statements, using text-embedding-3-large to quantify semantic relatedness. Performance was evaluated against human correlation matrices and oracle random forest models.

## Key Results
- GPT-4o reconstructed pairwise attitude correlations with r = .77 (similar items) and r = .724 (dissimilar items, Sc < .20)
- Individual prediction accuracy reached 40.6% with similar items and 42.0% with dissimilar items (chance = 29.4%)
- Semantic similarity improved prediction accuracy but was not necessary for meaningful inference

## Why This Works (Mechanism)

### Mechanism 1: Latent Belief Structure Encoding
- Claim: LLMs encode implicit associations among human attitudes that extend beyond surface-level semantic similarity.
- Mechanism: Training on diverse human discourse exposes the model to co-occurring beliefs organized by ideology, social norms, and logical coherence—enabling inference of non-obvious attitude relationships (e.g., gun control and abortion views linked through political ideology).
- Core assumption: The training corpus contains sufficient examples of how human beliefs actually cluster across cultural and social contexts.
- Evidence anchors:
  - [abstract] "LLMs capture crucial aspects of the deeper, latent structure of human belief systems"
  - [section] Study 3: For items where dissimilar predictors outperformed similar ones, GPT-4o achieved 42.0% accuracy with dissimilar vs. 41.2% with similar (k=3), demonstrating inference without semantic shortcuts
  - [corpus] Limited direct support; related work on belief prediction exists but doesn't isolate latent structure
- Break condition: Novel attitude combinations absent from training data; culturally-specific beliefs with limited representation

### Mechanism 2: Semantic Similarity as Supplementary Channel
- Claim: Surface-level semantic similarity improves but does not fully explain predictive performance.
- Mechanism: Embedding similarity creates inference shortcuts—related topics share representational space—but filtering out similar items only modestly degrades accuracy.
- Core assumption: Cosine similarity of text-embedding-3-large embeddings meaningfully captures semantic relatedness.
- Evidence anchors:
  - [abstract] "while surface similarity improves prediction accuracy, the model was still highly-capable of generating meaningful social inferences between dissimilar attitudes"
  - [section] Filtering for dissimilar pairs (cosine similarity < .20) reduced human-LLM correlation from r=.77 to r=.724—significant but modest
  - [corpus] Related work on stereotype associations (arXiv:2504.07787) suggests spurious correlations can bias LLM outputs
- Break condition: Semantically similar but ideologically opposed statements (e.g., "freedom" rhetoric across political spectrum)

### Mechanism 3: Correlation Extremization Bias
- Claim: LLMs systematically overestimate attitude correlation magnitudes toward extremes.
- Mechanism: Model outputs cluster near ±1 rather than reflecting the approximately normal distribution of human correlations.
- Core assumption: This reflects a calibration issue in mapping continuous inference to discrete output categories.
- Evidence anchors:
  - [section] "correlations estimated from GPT-4o tended to be more extreme, following a bimodal distribution with peaks near -1 and 1"
  - [section] 64.9% of weakly-associated pairs were overestimated; 70.5% of strongly-associated pairs were underestimated
  - [corpus] No direct corpus evidence on this specific bias pattern
- Break condition: Probability distribution outputs or chain-of-thought prompting may reduce extremization (suggested in Discussion as future work)

## Foundational Learning

- **Correlation structure of belief systems**:
  - Why needed here: Human attitudes cluster through ideology, social influence, and coherence constraints—not random association
  - Quick check question: If someone supports environmental regulation, what else might they believe, and why?

- **Semantic embeddings and cosine similarity**:
  - Why needed here: The paper uses text-embedding-3-large to quantify surface-level similarity and isolate it from deeper inference
  - Quick check question: Why might two statements with high cosine similarity have weak real-world correlation?

- **Persona-based prompting**:
  - Why needed here: The methodology constructs prompts that condition model predictions on known attitudes to simulate individual respondents
  - Quick check question: What information must a prompt contain for an LLM to adopt a specific persona?

## Architecture Onboarding

- **Component map**: Human data collection -> Embedding generation -> Prompt construction -> GPT-4o inference -> Evaluation layer

- **Critical path**: Human correlation matrix -> Embedding similarity calculation -> Predictor selection (similar vs. dissimilar) -> Prompt generation -> Model inference -> Accuracy/correlation evaluation against ground truth

- **Design tradeoffs**:
  - k=3 vs. k=8 predictors: More context helps but dilutes signal if irrelevant items included
  - Similarity threshold (Sc < .20): Stricter filters reduce data but better isolate latent reasoning
  - Single-token output vs. probability distribution: Cleaner evaluation but loses granularity

- **Failure signatures**:
  - Bimodal correlation estimates indicate overconfident inference
  - Accuracy drops when semantic shortcuts removed but model still relies on them
  - Chance-level performance (29.4%) would indicate no social reasoning capacity

- **First 3 experiments**:
  1. Replicate Study 2 with your target LLM: Can it estimate pairwise correlations from single-example prompts?
  2. Vary similarity thresholds (Sc < .10, .20, .30) to quantify semantic dependency
  3. Compare k=3 vs. k=8 predictors on held-out items to identify optimal context window

## Open Questions the Paper Calls Out

- **Open Question 1**: Can chain-of-thought (CoT) prompting or reasoning-optimized models improve predictive accuracy and reduce semantic similarity bias?
  - Basis in paper: [explicit] The authors suggest that "future research could explore other prompting strategies" like CoT or "examine the capabilities of new reasoning models" to potentially improve performance and decrease biases.
  - Why unresolved: The study constrained GPT-4o to respond immediately without intermediate reasoning steps.
  - What evidence would resolve it: Comparative benchmarks showing whether CoT prompts or models like GPT-o1 achieve higher accuracy on dissimilar attitude pairs than the standard prompting used in the study.

- **Open Question 2**: Do models perform better when they explicitly infer latent variables (like ideology) during intermediate reasoning steps?
  - Basis in paper: [explicit] The authors propose examining "whether models perform better when they explicitly infer (and tokenize in their reasoning) the ideology of a target person."
  - Why unresolved: The current methodology relied on final output classification, lacking visibility into the model's internal inferential process.
  - What evidence would resolve it: An analysis of intermediate reasoning tokens (e.g., in a CoT output) correlating explicit mentions of ideological constructs with higher predictive accuracy.

- **Open Question 3**: Does the capability to predict attitudes from dissimilar items predict an LLM's ability to persuade or manipulate human users?
  - Basis in paper: [inferred] The authors note that "Persuasion often benefits from understanding of others' existing viewpoints" and link their findings to risks of "automated propaganda."
  - Why unresolved: The study isolated attitude prediction as a cognitive task without testing downstream behavioral outcomes like persuasion.
  - What evidence would resolve it: Experiments testing if models that successfully predict cross-domain attitudes are more effective in persuasive dialogue tasks than those that rely on semantic similarity.

## Limitations
- Correlation extremization bias shows systematic overconfidence in predictions
- Declarative statement transformations from survey questions may introduce interpretation uncertainties
- Sample size (376 participants) and demographic limitations may constrain generalizability

## Confidence
- Latent belief structure inference: High confidence (consistent results, substantial gap from chance)
- Semantic similarity supplementary role: High confidence (controlled experiments, strong correlation maintenance)
- Correlation extremization bias: Medium confidence (clear pattern but unclear mechanism, limited corpus evidence)

## Next Checks
1. Test the methodology with a different LLM (e.g., Claude 3.5 or Llama 3) to assess whether the latent reasoning capability generalizes beyond GPT-4o
2. Conduct a replication study with culturally diverse participant samples to evaluate cross-cultural applicability of the latent belief structure inference
3. Experiment with alternative prompting strategies (chain-of-thought, probability distributions) to determine if the correlation extremization bias can be mitigated