---
ver: rpa2
title: Class Adaptive Conformal Training
arxiv_id: '2601.09522'
source_url: https://arxiv.org/abs/2601.09522
tags:
- cact
- prediction
- training
- size
- coverage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Class Adaptive Conformal Training (CaCT) introduces a novel approach
  to conformal learning by formulating it as an augmented Lagrangian optimization
  problem. This method adaptively learns class-conditional penalties without requiring
  prior distributional assumptions, addressing the limitations of existing methods
  that use a single global penalty weight.
---

# Class Adaptive Conformal Training

## Quick Facts
- **arXiv ID:** 2601.09522
- **Source URL:** https://arxiv.org/abs/2601.09522
- **Reference count:** 40
- **Primary result:** Introduces Class Adaptive Conformal Training (CaCT) that adaptively learns class-conditional penalties without requiring prior distributional assumptions, outperforming prior conformal training methods across standard and long-tailed datasets.

## Executive Summary
Class Adaptive Conformal Training (CaCT) addresses a fundamental limitation in conformal prediction: the use of a single global penalty weight fails to account for class imbalance, leading to poor efficiency (large prediction sets) for minority classes. CaCT reformulates conformal training as an Augmented Lagrangian optimization problem, introducing class-conditional Lagrange multipliers that adapt penalty weights based on each class's constraint violations. This approach eliminates the need for manual hyperparameter tuning while maintaining coverage guarantees. Experiments on CIFAR-10/100, ImageNet, MNIST, and 20 Newsgroups demonstrate that CaCT consistently produces smaller prediction sets with reduced coverage gaps compared to state-of-the-art methods, particularly in long-tailed settings.

## Method Summary
CaCT formulates conformal learning as a constrained optimization problem where the objective is to minimize classification loss subject to per-class prediction set size constraints. The method introduces class-conditional Lagrange multipliers $\lambda_y$ that are updated via an Augmented Lagrangian Method based on constraint violations observed on a validation set. During training, smooth approximations replace non-differentiable conformal operations (hard indicators and quantile calculations) to enable gradient-based optimization. The model learns to minimize both the classification loss and a penalty term that depends on the class-conditional set sizes relative to their targets. At test time, the standard (non-smooth) conformal prediction procedure is restored to ensure formal coverage guarantees.

## Key Results
- CaCT consistently outperforms prior conformal training methods in both standard and long-tailed image recognition tasks
- The method produces significantly smaller average prediction sets while maintaining desired coverage guarantees
- Reductions in coverage gaps are observed across multiple datasets and imbalance levels compared to state-of-the-art approaches
- Class-conditional penalties eliminate the inefficiency bias toward majority classes seen in global penalty methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Replacing a global penalty weight with class-conditional Lagrange multipliers allows the model to enforce prediction set constraints uniformly across both frequent and rare classes.
- **Mechanism:** The authors reformulate the learning objective from a single marginal constraint on set size to K independent constraints (one per class). By introducing a specific multiplier λ_y for each class y, the optimization can penalize violations specifically for the class experiencing them, preventing majority classes from dominating the gradient updates.
- **Core assumption:** The validation set provides a sufficiently representative sample of each class to estimate the constraint violations and update multipliers reliably.
- **Evidence anchors:** [abstract]: "...adaptively learns class-conditional penalties without requiring prior distributional assumptions..."; [section 4.1]: "...treating all constraint violations equally with a single uniform penalty weight λ fails to accurately solve the associated constrained problem..."; [corpus]: [arXiv:2507.06867] confirms that standard CP methods struggle with long-tailed distributions.

### Mechanism 2
- **Claim:** Using an Augmented Lagrangian Method (ALM) automates the tuning of penalty weights, solving the sensitivity issues associated with manual hyperparameter search.
- **Mechanism:** Instead of treating λ as a fixed hyperparameter, CaCT treats it as a dynamic variable updated via dual ascent. If the average set size for a class exceeds the target η, the corresponding multiplier λ_k is increased, tightening the constraint pressure for the next epoch. This adapts the regularization strength based on the difficulty of the class.
- **Core assumption:** The optimization landscape, while non-convex, allows the alternating updates of model weights and multipliers to converge to a feasible solution.
- **Evidence anchors:** [section 4.2]: "This mechanism effectively provides an adaptive strategy for selecting the penalty weight in a penalty-based method."; [figure 5]: Shows the evolution of multipliers and set sizes, demonstrating dynamic adaptation over training epochs.

### Mechanism 3
- **Claim:** Smooth approximations of non-differentiable conformal operations enable gradient-based optimization of prediction sets while retaining formal coverage guarantees at test time.
- **Mechanism:** During training, the hard indicator function (1[a ≤ b]) is replaced by a sigmoid function, and the quantile calculation is replaced by a differentiable sorting operator. This creates a "soft" prediction set size that can be backpropagated. At test time, the original "hard" conformal prediction procedure is restored, ensuring the statistical guarantee holds.
- **Core assumption:** The smooth approximation is sufficiently accurate (controlled by temperature T) to guide the model toward a solution that minimizes the true (hard) set size.
- **Evidence anchors:** [section 4.2]: "...we use the sigmoid function to smooth the indicator... and compute the conformal threshold using a differentiable quantile operator..."; [table 4]: Shows that performance is maintained across different mis-coverage levels α at test time.

## Foundational Learning

### Concept: Split Conformal Prediction (Split CP)
- **Why needed here:** CaCT is designed to optimize the efficiency of Split CP sets. Without understanding that CP constructs sets by thresholding non-conformity scores on a calibration set to guarantee coverage, the training objective makes little sense.
- **Quick check question:** If a model outputs logits, how does Split CP turn them into a set of labels guaranteed to contain the truth with probability 1-α?

### Concept: Constrained Optimization & Lagrangians
- **Why needed here:** The core contribution is solving a constrained problem (minimize loss s.t. set size ≤ η) using an Augmented Lagrangian. One must distinguish between penalty methods (fixed weights) and Lagrangian methods (dynamic weights).
- **Quick check question:** In a standard penalty method, what happens to the loss if the constraint is violated but the weight λ is set too low?

### Concept: Long-Tailed Recognition
- **Why needed here:** A primary motivation for CaCT is the failure of global penalties in imbalanced settings. Understanding that models bias toward majority classes helps explain why a class-adaptive penalty is necessary to prevent large prediction sets for minority classes.
- **Quick check question:** In a dataset with 1,000 "dog" images and 10 "cat" images, how might a standard conformal training method (using a single global penalty) bias the prediction sets for "cats"?

## Architecture Onboarding

### Component map:
- **Backbone:** ResNet/BERT (outputs logits/log-probs)
- **Smooth CP Layer:** A temporary module active only during training that computes the differentiable quantile and the soft set size
- **ALM Controller:** An outer loop module that tracks per-class constraints on the validation set and updates the vector of multipliers λ ∈ ℝ^K

### Critical path:
1. **Forward Pass (Train):** Batch is split into B_cal and B_pred. Logits from B_cal determine the smooth threshold; B_pred computes the classification loss and the penalized set size loss using the current λ
2. **Outer Loop (Validation):** After an epoch, the model runs on D_val. For each class k, if average set size > η, update λ_k using Eq. 14
3. **Inference (Test):** Discard the Smooth CP Layer. Run standard Split CP using the trained model and a fresh calibration set

### Design tradeoffs:
- **Differentiability vs. Stability:** The paper uses log-probs during training rather than specific score functions (THR/APS) for stability (Appendix D). You trade exact simulation of the test score for training stability
- **Val Set Usage:** Using the validation set to update λ is crucial to avoid overfitting the training data, but it consumes data that could otherwise be used for calibration or validation of the final accuracy

### Failure signatures:
- **Multiplier Explosion:** If λ_k grows indefinitely without the constraint being satisfied, gradients may explode
- **High Coverage Gap:** If the validation set is too small or unrepresentative, the learned multipliers might over-penalize specific classes, leading to under-coverage or over-coverage in specific groups

### First 3 experiments:
1. **Baseline Reproduction:** Implement ConfTr (global λ) on CIFAR-10-LT to reproduce the "large set size for tail classes" failure mode shown in Figure 2
2. **Ablation on Constraints:** Compare CaCT with uniform λ vs. class-conditional λ to isolate the performance gain coming solely from the adaptive mechanism
3. **Temperature Sensitivity:** Sweep the smoothing temperature T (Appendix Figure 8) on a small dataset to observe the "Goldilocks zone" where gradients flow but the approximation remains valid

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can formal convergence guarantees be derived for Class Adaptive Conformal Training (CaCT) within the non-convex optimization landscapes typical of deep neural networks?
- **Basis in paper:** [explicit] The paper states on Page 5 that while theoretical convergence guarantees for Augmented Lagrangian Methods (ALM) are established in convex optimization, CaCT relies on empirical performance in non-convex scenarios.
- **Why unresolved:** The convergence properties of ALM in the stochastic, non-convex setting of deep learning (used here) remain theoretically unproven.
- **What evidence would resolve it:** A formal proof of convergence for the proposed algorithm on non-convex loss surfaces, or empirical density plots showing consistent convergence trajectories across diverse non-convex architectures.

### Open Question 2
- **Question:** Can the estimation of Lagrange multipliers be performed directly on the training set without leading to overfitting, thereby eliminating the need for a dedicated validation set?
- **Basis in paper:** [explicit] Page 5 notes that "Directly estimating Lagrange multipliers from the training set could easily lead to overfitting the training data," leading the authors to leverage a validation set (D_val).
- **Why unresolved:** The current method reduces the data available for training and calibration by requiring a hold-out validation set for the outer loop optimization.
- **What evidence would resolve it:** A modified training scheme that regularizes multiplier updates on the training set, demonstrating comparable performance to the validation-based approach without data partitioning.

### Open Question 3
- **Question:** Can the CaCT framework be adapted to continuous domains, such as regression or structured prediction, where the "set size" constraint is not discrete?
- **Basis in paper:** [inferred] The paper explicitly frames CaCT as a classification method (Page 4) with constraints on discrete set size |C_θ(X)|, despite acknowledging CP's broader application to regression in Related Work (Page 2).
- **Why unresolved:** The proposed penalty function P is applied to discrete class constraints, and it is unclear if this dynamic penalty mechanism scales to continuous intervals or high-dimensional structured outputs.
- **What evidence would resolve it:** An extension of the CaCT algorithm applied to a regression benchmark, defining constraints on interval length or volume rather than label count.

### Open Question 4
- **Question:** To what extent does the finite temperature T used for smoothing the objective function during training compromise the theoretical coverage guarantees compared to the non-smooth limit?
- **Basis in paper:** [explicit] Page 5 states that smooth approximations relax the calibration step, resulting in the loss of marginal coverage guarantees, although the formulation converges to the guarantee as T → 0.
- **Why unresolved:** In practice, T cannot be 0 during backpropagation; the specific impact of non-zero T on the tightness of the coverage gap during the training phase is not fully characterized.
- **What evidence would resolve it:** An ablation study analyzing the correlation between the temperature T and the "coverage gap" metric on the validation set throughout the training epochs.

## Limitations

- The method relies on validation set size being sufficient to reliably estimate constraint violations for each class, which may fail in extreme tail classes with very few validation samples
- Smooth approximations introduce approximation error that could accumulate, particularly with temperature settings that are too high or too low
- The Augmented Lagrangian method's convergence in non-convex deep learning settings is not guaranteed, with limited empirical evidence of convergence behavior across all experimental conditions

## Confidence

- **High Confidence:** The core mechanism of using class-conditional penalties to address long-tailed distribution issues is well-supported by the empirical results and mathematical formulation
- **Medium Confidence:** The ALM optimization approach is sound in theory, but the practical implementation details (particularly the differentiable quantile operator) are underspecified, limiting reproducibility
- **Low Confidence:** The claim that this approach "without requiring prior distributional assumptions" is somewhat overstated, as the method still requires sufficient validation data per class to estimate constraints reliably

## Next Checks

1. Test CaCT's performance on datasets with extreme class imbalance (1:1000 ratio) to identify the breaking point where validation set size becomes insufficient for reliable multiplier updates
2. Implement and compare multiple differentiable quantile operators (smooth sorting networks vs. Sinkhorn-based approaches) to assess sensitivity to this critical implementation choice
3. Conduct ablation studies varying the validation set size allocation (currently 20% of Val set) to quantify the tradeoff between reliable multiplier estimation and calibration set availability