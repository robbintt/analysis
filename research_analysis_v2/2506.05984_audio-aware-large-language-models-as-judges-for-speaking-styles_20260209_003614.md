---
ver: rpa2
title: Audio-Aware Large Language Models as Judges for Speaking Styles
arxiv_id: '2506.05984'
source_url: https://arxiv.org/abs/2506.05984
tags:
- style
- dialogue
- evaluation
- speaking
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces StyleSet, a benchmark to evaluate spoken
  language models'' (SLMs) ability to generate speech with appropriate speaking styles.
  The authors propose two tasks: voice style instruction following, which tests fine-grained
  control over emotion, pace, and prosody, and role-playing, which evaluates natural
  dialogue generation.'
---

# Audio-Aware Large Language Models as Judges for Speaking Styles

## Quick Facts
- arXiv ID: 2506.05984
- Source URL: https://arxiv.org/abs/2506.05984
- Reference count: 33
- Primary result: ALLMs can evaluate speaking styles with human-level agreement, with Gemini-2.5-pro achieving Pearson's r = 0.640

## Executive Summary
This paper introduces StyleSet, a benchmark for evaluating spoken language models' (SLMs) ability to generate speech with appropriate speaking styles. The authors propose two tasks—voice style instruction following and role-playing—to test fine-grained control over emotion, pace, and prosody, and natural dialogue generation. SLMs are evaluated by two audio-aware large language models (ALLMs) and human judges. Results show that while GPT-4o-audio outperforms other SLMs, all models still struggle with realistic dialogue and nuanced style control. Notably, the Gemini-2.5-pro judge achieves human-level agreement, demonstrating that ALLMs can serve as effective automatic judges for speaking styles.

## Method Summary
The paper constructs the StyleSet benchmark with 20 instances each for voice style instruction following and role-playing tasks. SLMs (GPT-4o-audio, GPT-4o-mini-audio, Step-Audio, Qwen-2.5-Omni) generate speech outputs using temperature=1.0, top_p=0.9, max_tokens=4096. Two ALLMs (GPT-4o-audio, Gemini-2.5-pro) evaluate outputs using chain-of-thought prompting and structured rubrics, sampling 5 responses per instance and ensembling verdicts. Evaluation metrics include 5-point Likert scale for style adherence and binary realism score for role-playing. Human evaluators validate ALLM judgments, with Gemini achieving Pearson's r = 0.640 against human scores.

## Key Results
- Gemini-2.5-pro achieves human-level agreement (Pearson's r = 0.640) on voice style instruction following, exceeding pairwise human agreement
- ALLM judges successfully evaluate nuanced speaking styles including emotion, pace, and prosody
- All SLMs struggle with realistic dialogue generation, with open-source models scoring near zero on realism
- Voice style instruction following proves easier than role-playing, with higher style scores across all models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ALLMs can evaluate speaking style with human-level agreement when provided structured rubrics and chain-of-thought reasoning.
- Mechanism: ALLMs process both textual content and paralinguistic features (emotion, prosody, pace, emphasis) from audio input. When given explicit scoring rubrics and prompted to reason step-by-step before outputting scores, the model produces evaluations that correlate with human judgments. The paper shows Gemini-2.5-pro achieves Pearson's r = 0.640 with human evaluators on voice style instruction following, compared to human-human agreement of 0.596.
- Core assumption: ALLMs have sufficiently rich internal representations of prosodic and emotional features to make fine-grained distinctions about speaking style.
- Evidence anchors:
  - [abstract] "the agreement between Gemini and human judges is comparable to the agreement between human evaluators"
  - [section 4.2] "Gemini achieves an average correlation of 0.640 with human evaluators, even higher than the pairwise correlation between humans"
  - [corpus] Related work (Jiang et al., 2025) found Qwen-2-Audio and 4o-audio did NOT align with human evaluation—suggesting this capability is model-specific and not universal across ALLMs.
- Break condition: If ALLMs lack audio pretraining data with style annotations, or if evaluation targets subtle non-verbal cues outside training distribution, agreement would degrade.

### Mechanism 2
- Claim: Ensembling multiple judge samples with self-consistency reduces evaluation variance and improves reliability.
- Mechanism: For each evaluation instance, the system samples 5 responses from the ALLM judge at temperature 1.0 and aggregates verdicts. This reduces noise from individual stochastic outputs while preserving the signal from the model's style understanding. Section 4.2 shows the Gemini judge's variance due to hyperparameters (temperature 0.5-1.5) is low (Pearson's r ranges 0.640-0.649), suggesting stable style representations.
- Core assumption: Style evaluation quality is consistent across sampling diversity—i.e., the model's "understanding" of style is stable even when word-level outputs vary.
- Evidence anchors:
  - [section 4.1] "For each instance to be evaluated, we sample five judge responses and ensemble the verdicts"
  - [section 4.2] Reports stable correlations across temperature settings
  - [corpus] Limited corpus evidence on ensembling specifically for audio evaluation; Wang et al. (2023) cited for self-consistency in text domain.
- Break condition: If judge model has inconsistent style representations or sampling amplifies biases rather than reducing noise.

### Mechanism 3
- Claim: Separating style quality from realism assessment reveals gaps in SLM dialogue generation that single-metric evaluation would miss.
- Mechanism: Role-playing evaluation uses two distinct scales—a 5-point style Likert score and a binary realism judgment. This decoupling exposes cases where an SLM produces stylistically appropriate speech within a dialogue that nonetheless fails to sound human-like. Section 4.3 shows human-recorded dialogue scores 4.03 on style but 0.97 on realism, while 4o-audio scores 3.39 on style but only 0.51 on realism—a much larger gap.
- Core assumption: Realism is a distinct construct from style adherence and requires separate evaluation criteria.
- Evidence anchors:
  - [section 3.2] "The reason to separately evaluate these two aspects is that even if the styles sound natural, the whole dialogue may still not sound realistic"
  - [section 4.3] "the realism rating shows that human-recorded dialogues are much realistic, with a realism score almost twice as much as 4o"
  - [corpus] No corpus evidence directly addresses this dual-metric approach for speech evaluation.
- Break condition: If style and realism are highly correlated, separate evaluation adds complexity without benefit.

## Foundational Learning

- Concept: **ALLM vs SLM distinction**
  - Why needed here: The paper uses ALLMs (audio-aware models that output text) to judge SLMs (spoken language models that output speech). Confusing these roles breaks understanding of the evaluation pipeline.
  - Quick check question: If you need to generate speech from text, which type do you use? If you need to evaluate the prosody of an audio clip, which type do you use?

- Concept: **LLM-as-a-judge paradigm**
  - Why needed here: The core contribution extends text-based LLM evaluation to audio. Prior work (Chiang and Lee 2023, Zheng et al. 2023) established that LLMs can substitute for human text evaluation under certain conditions.
  - Quick check question: What are the failure modes of LLM-as-a-judge that the paper explicitly tries to mitigate?

- Concept: **Paralinguistic features in speech**
  - Why needed here: The evaluation targets non-textual aspects—emotion, volume, pace, word emphasis, pitch control, non-verbal elements (laughter, sighs). Understanding what prosody encompasses is prerequisite to interpreting results.
  - Quick check question: Why does the paper evaluate both style and realism separately for role-playing?

## Architecture Onboarding

- Component map:
  - SLM layer: 4 models (4o-audio, 4o-mini-audio, Step-Audio, Qwen-2.5-Omni) generate speech outputs for two tasks
  - Task layer: Voice style IF (explicit style instructions) and Role-playing (implicit style from context)
  - Judge layer: 2 ALLMs (4o-audio, Gemini-2.5-pro) evaluate outputs with different prompts per task/metric
  - Aggregation layer: 5-sample ensembling per instance, regular expression extraction of numeric scores
  - Ground truth: 4 human evaluators per task for validation

- Critical path:
  1. Generate SLM responses with temperature=1.0, top_p=0.9, max_tokens=4096
  2. For role-playing: concatenate turns with 2-second silence, crop to 1 minute
  3. Run ALLM judge with temperature=1.0, top_p=0.9, max_new_tokens=256, CoT enabled
  4. Extract scores via regex, ensemble 5 samples
  5. Compare against human evaluation using Pearson's r

- Design tradeoffs:
  - **Single-wise vs pairwise evaluation**: Paper uses pointwise scoring; pairwise is harder when both options are equally bad (Qwen vs Step-Audio role-play)
  - **Turn-taking vs full-duplex**: Current evaluation uses turn-by-turn dialogue; full-duplex would increase realism but most SLMs don't support it
  - **Judge selection**: Gemini shows higher human agreement than 4o-audio (0.640 vs 0.355 on voice style IF), but 4o can judge its own outputs (self-enhancement bias concern)

- Failure signatures:
  - **SLM failures**: Models "read out loud" non-verbal cues (e.g., saying "sigh" instead of sighing); cannot vary pace within utterance
  - **Judge failures**: 4o-audio shows low correlation with humans; both judges struggle to distinguish among equally poor SLMs
  - **Task failures**: Role-playing realism scores near 0 for open-source models indicate generated dialogue sounds artificial

- First 3 experiments:
  1. Reproduce voice style IF evaluation with Gemini on 4o-audio outputs, verify Pearson's r > 0.6 against provided human scores
  2. Ablate ensembling: compare single-sample vs 5-sample judge outputs to quantify variance reduction
  3. Test judge robustness: evaluate same SLM outputs with temperature sweep (0.5, 1.0, 1.5) and confirm correlation stability within ±0.01

## Open Questions the Paper Calls Out
None

## Limitations
- ALLM judge agreement is highly model-dependent—GPT-4o-audio achieves only r = 0.355, suggesting the approach is not universally applicable across ALLMs
- The evaluation pipeline requires structured rubrics and chain-of-thought prompting, limiting generalizability to other style evaluation contexts
- StyleSet benchmark covers only 20 instances per task, and role-playing uses turn-by-turn dialogue rather than full-duplex conversation

## Confidence

- **High confidence**: Gemini-2.5-pro achieves human-level agreement on voice style instruction following (r = 0.640)
- **Medium confidence**: ALLM judges outperform human agreement in stability (r = 0.640 vs human-human r = 0.596), though this comparison is nuanced
- **Low confidence**: The claim that ALLMs can universally replace human judges for all speaking style evaluation tasks, given the model-specific performance differences

## Next Checks

1. **Model robustness test**: Evaluate the same SLM outputs across multiple ALLM judges (including newer models like Qwen-2-Audio) to determine if Gemini's performance is unique or if other ALLMs can achieve similar human agreement levels

2. **Prompt sensitivity analysis**: Systematically vary the chain-of-thought and rubric structures in the judge prompts to identify which components most contribute to human-level agreement, and whether simpler prompts can achieve comparable results

3. **Realism evaluation validation**: Conduct a controlled study comparing the ALLM realism judgments against a larger sample of human evaluators to verify that the binary realism scoring captures the same construct humans use to distinguish natural from artificial dialogue