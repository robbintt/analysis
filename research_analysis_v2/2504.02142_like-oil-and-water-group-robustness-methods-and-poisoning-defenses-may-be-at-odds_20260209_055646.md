---
ver: rpa2
title: 'Like Oil and Water: Group Robustness Methods and Poisoning Defenses May Be
  at Odds'
arxiv_id: '2504.02142'
source_url: https://arxiv.org/abs/2504.02142
tags:
- group
- samples
- robustness
- poisons
- poisoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a fundamental tension between group robustness
  methods and poisoning defenses in machine learning. Group robustness methods aim
  to improve model performance on minority groups but fail to distinguish legitimate
  minority samples from poison samples, inadvertently amplifying attacks and increasing
  success rates from 0% to over 97%.
---

# Like Oil and Water: Group Robustness Methods and Poisoning Defenses May Be at Odds

## Quick Facts
- arXiv ID: 2504.02142
- Source URL: https://arxiv.org/abs/2504.02142
- Reference count: 40
- Primary result: Group robustness methods and poisoning defenses are fundamentally at odds, with group robustness methods amplifying poisons and defenses eliminating minority samples.

## Executive Summary
This paper reveals a fundamental tension between group robustness methods and poisoning defenses in machine learning. Group robustness methods, designed to improve model performance on minority groups, inadvertently amplify poison samples because both types of samples exhibit similar "difficulty" signals (high loss, misclassification). Conversely, poisoning defenses that remove outliers in gradient space also eliminate legitimate minority samples, reducing group robustness by up to 15%. The paper demonstrates through theoretical analysis and empirical experiments that combining these methods does not resolve the tension, suggesting the need for new approaches that can balance both objectives.

## Method Summary
The study evaluates group robustness methods (JTT and GEORGE) against poisoning attacks (DLBD, SA, GM) on Waterbirds and CelebA datasets. Group robustness works by identifying "difficult" samples (high loss or misclassification) as minority groups and amplifying them through upsampling or Group DRO. Poisoning defenses (EPIc) identify and remove gradient-space outliers. The core finding is that poisons and minority samples exhibit similar difficulty signatures, causing group robustness methods to amplify poisons while defenses eliminate minorities. The paper provides theoretical bounds on identification confusion and conducts extensive empirical validation across different attack types and model architectures.

## Key Results
- Group robustness methods amplify poison samples, increasing attack success rates from 0% to over 97%
- Poisoning defenses reduce group robustness by up to 15% by eliminating legitimate minority samples
- Combining group robustness methods with poisoning defenses fails to resolve the fundamental tension
- Theoretical analysis proves that loss-based heuristics cannot distinguish between legitimate minority samples and poison samples under realistic conditions

## Why This Works (Mechanism)

### Mechanism 1
Group robustness heuristics inadvertently identify poison samples as minority group samples because both exhibit high training loss. Difficulty-based heuristics (loss values, misclassification counts) flag samples that are hard-to-learn. Since low-budget poisoning attacks inject few malicious samples, poisons exhibit high training loss—matching the signal used to detect legitimate under-represented groups. This works because poison samples are difficult to learn (high loss) due to limited injection, aligning with how minority samples behave under standard training.

### Mechanism 2
Poisoning defenses remove legitimate minority samples as collateral damage because defenses such as EPIc identify outliers in gradient space and eliminate them as potential poisons. Minority samples, due to their low representation and distinct learning signals, appear as outliers and are eliminated alongside poisons. This works because minority samples produce outlier-like behavior (isolated gradients, high loss) similar to poisons.

### Mechanism 3
Combining group robustness methods with poisoning defenses does not resolve the tension because both approaches pursue opposing goals using similar heuristics—amplify hard-to-learn samples vs. remove hard-to-learn samples. When combined, either the defense removes too many minority samples (rendering robustness methods ineffective) or too few poisons (allowing amplification of remaining attacks). This works because both methods rely on difficulty-based signals without orthogonal information to jointly succeed.

## Foundational Learning

- **Empirical Risk Minimization (ERM) and Minority Group Failure**: Why needed here—the paper's premise is that standard ERM performs well on average but poorly on minority groups, motivating group robustness methods. Quick check: Why does minimizing average training loss not guarantee good performance on under-represented groups?

- **Worst-Group Accuracy (WGA) vs. Standard Accuracy (ACC)**: Why needed here—the paper uses WGA as the primary metric for group robustness, distinct from overall accuracy, which can mask minority performance. Quick check: If a model achieves 95% ACC but 40% WGA, what does this indicate about its fairness?

- **Difficulty-Based Sample Heuristics**: Why needed here—both group robustness methods (JTT, GEORGE) and poisoning defenses (EPIc) rely on proxies like training loss or gradient isolation to identify target samples. Quick check: What signal do JTT and EPIc both use to classify samples, and why does this cause the identified tension?

## Architecture Onboarding

- **Component map**: Identification Phase (Group Robustness): Train initial model → Flag high-loss samples as minority candidates → Amplification Phase (Group Robustness): Upsample flagged samples (JTT) or apply Group DRO (GEORGE) → Defense Phase (EPIc): Iteratively identify gradient-space outliers → Remove them → Evaluation: Track WGA, ASR, ACC

- **Critical path**: 1. Identify minority samples using difficulty heuristic → 2. Amplify them (upsampling or Group DRO) → 3. Measure WGA improvement and ASR increase if poisons are present. 4. Alternatively, apply defense to remove outliers → 5. Measure WGA decrease and ASR reduction.

- **Design tradeoffs**: WGA vs. ASR: Higher minority amplification improves WGA but may increase ASR; stronger defense reduces ASR but harms WGA. Heuristic precision vs. recall: Stricter thresholds reduce false positives (fewer poisons amplified) but also reduce true positives (fewer minority samples correctly identified). Annotation availability: Explicit group labels avoid heuristic failures but are often unavailable due to privacy or cost.

- **Failure signatures**: Amplification Failure: WGA remains low despite upsampling—check if poisons dominate the amplified set (high IDNF on poisons). Defense Over-Pruning: ACC is high but WGA drops sharply—defense is eliminating minority samples (high ELMF on LRG). Combined Pipeline Stagnation: Neither WGA nor ASR improves—defense and robustness heuristics are canceling each other out.

- **First 3 experiments**: 1. Replicate Table 1: Train JTT/GEORGE on a poisoned dataset and compute IDNF for poisons vs. LRG-1 to verify identification confusion. 2. Replicate Table 3: Apply EPIc at varying stopping epochs and measure WGA and ASR to observe the trade-off curve. 3. Run Table 19 combined experiment: Test EPIc+JTT pipeline with different defense stopping points to confirm no configuration balances WGA and ASR simultaneously.

## Open Questions the Paper Calls Out

### Open Question 1
Can providing distribution-based defenses with carefully curated, balanced base sets effectively separate poisons from minority samples without sacrificing group robustness? The authors hypothesize that using a "poison-free" base set with a balanced distribution might prevent defenses from mistakenly eliminating minority samples, but creating such sets is practically difficult.

### Open Question 2
Do defenses that assume poisons are "easy to learn" (Category 2) avoid the disparate impact on minority samples observed in "difficult-to-learn" defenses? The paper notes these defenses "would not lead to these problems" but may be ineffective against realistic attacks with limited poison budgets.

### Open Question 3
Can heuristics that decouple "sample difficulty" from "minority status" successfully distinguish legitimate minority groups from poisons? Current methods (JTT, GEORGE) rely on difficulty heuristics (loss, clustering) which inherently fail to separate these two groups.

## Limitations
- The exact quantitative threshold EPIc uses to identify outliers in gradient space is not specified
- Some empirical results are based on synthetic minority groups created via loss/misclassification heuristics rather than naturally occurring imbalanced datasets
- The framework's generalizability to other types of attacks and model architectures requires further validation

## Confidence

- **High confidence**: Core empirical observation that group robustness methods amplify poisons (based on IDNF metrics showing 98-100% identification of poisons as minority samples)
- **Medium confidence**: Theoretical analysis linking poison difficulty to minority identification via loss heuristics
- **Medium confidence**: Conclusion that combining methods fails to resolve tension, given limited exploration of alternative defense strategies

## Next Checks

1. Verify EPIc outlier detection threshold and its impact on minority vs. poison elimination rates
2. Test the framework on naturally imbalanced datasets (e.g., medical imaging) to assess generalization
3. Explore alternative defense strategies that use group-balanced reference sets to distinguish legitimate minorities from poisons