---
ver: rpa2
title: 'VisualSimpleQA: A Benchmark for Decoupled Evaluation of Large Vision-Language
  Models in Fact-Seeking Question Answering'
arxiv_id: '2503.06492'
source_url: https://arxiv.org/abs/2503.06492
tags:
- question
- difficulty
- multimodal
- answer
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VisualSimpleQA is a benchmark designed for decoupled evaluation
  of large vision-language models (LVLMs) in fact-seeking question answering. It addresses
  the challenge of distinguishing between failures in visual recognition and linguistic
  knowledge by providing paired multimodal and text-only questions with rationales
  and evidence.
---

# VisualSimpleQA: A Benchmark for Decoupled Evaluation of Large Vision-Language Models in Fact-Seeking Question Answering

## Quick Facts
- arXiv ID: 2503.06492
- Source URL: https://arxiv.org/abs/2503.06492
- Reference count: 40
- A benchmark that isolates visual vs. linguistic module performance in LVLMs using paired multimodal and text-only questions

## Executive Summary
VisualSimpleQA addresses the challenge of evaluating large vision-language models (LVLMs) in fact-seeking question answering by decoupling visual recognition from linguistic knowledge. The benchmark provides paired multimodal and text-only questions with rationales and evidence, enabling fine-grained assessment of where model failures originate. With well-defined difficulty criteria including resolution, region of interest proportion, and knowledge popularity, VisualSimpleQA enables extraction of challenging subsets and comprehensive evaluation across diverse topics.

## Method Summary
The VisualSimpleQA benchmark consists of 500 samples, each containing an image, multimodal question, text-only reformulation, ground truth answer, rationale, ROI bounding box, and difficulty attributes. Evaluation involves running LVLMs on both question types, then using DeepSeek-V3 as an automatic grader to classify responses as correct, incorrect, or refusal. The primary metric is Relative Degradation (RD), calculated as (score_TQA - score_MQA) / score_TQA, which quantifies visual module performance by measuring degradation when switching from text-only to multimodal QA.

## Key Results
- GPT-4o achieves only 60%+ correctness on multimodal questions in VisualSimpleQA and 30%+ on VisualSimpleQA-hard
- Substantial performance gaps exist between visual and linguistic modules across all evaluated LVLMs
- Even state-of-the-art models show significant failures in fact-seeking visual question answering

## Why This Works (Mechanism)

### Mechanism 1: Decoupled Evaluation
- **Claim:** Decoupled evaluation isolates module-specific failures in LVLMs.
- **Mechanism:** Each sample contains multimodal and rationale-based text-only reformulations. Text-only performance proxies linguistic knowledge; performance degradation when switching to multimodal QA isolates visual recognition capability.
- **Core assumption:** Text-only questions fully capture visual rationale without requiring spatial reasoning.
- **Evidence anchors:** [abstract] "it enables streamlined and decoupled evaluation of LVLMs in visual and linguistic modalities"; [Section 3.1] Describes RD metric; HallusionBench (Guan et al., 2024) attempts similar decoupling.

### Mechanism 2: Multi-factor Difficulty Criteria
- **Claim:** Five quantified factors predict sample challenge level.
- **Mechanism:** Resolution, ROI proportion, rationale granularity, text-in-image presence, and knowledge popularity are averaged into a composite difficulty score. Higher scores correlate with higher failure rates.
- **Core assumption:** Each factor contributes independently and equally to difficulty.
- **Evidence anchors:** [Section 3.2.1] Defines each criterion; [Figure 7-8] Shows GPT-4o failure ratios increase with difficulty scores.

### Mechanism 3: Short-form Answers for Automatic Grading
- **Claim:** Short-form, indisputable answers enable objective automatic grading.
- **Mechanism:** Questions designed with single correct answers; DeepSeek-V3 classifies responses as correct/incorrect/refusal. Avoids ambiguity in long-form evaluation.
- **Core assumption:** Factuality can be adequately captured by short-form QA.
- **Evidence anchors:** [Section 4.1] "annotators are asked to create questions that have indisputable and short-form answers"; [Section 6.1] Reports 94% human verification accuracy.

## Foundational Learning

- **Concept: LVLM Architecture (Visual Encoder → Connector → LLM)**
  - Why needed here: Decoupled evaluation assumes distinct visual and linguistic modules; understanding this separation is prerequisite to interpreting RD metrics.
  - Quick check question: If a model has poor text-only QA but good multimodal QA, which module is likely the bottleneck?

- **Concept: Long-tailed Knowledge in Training Data**
  - Why needed here: Knowledge popularity scores assume training corpora underrepresent rare facts, affecting linguistic module performance.
  - Quick check question: Why might a model correctly identify a cartoon character but fail to answer a question about that character's creator?

- **Concept: Evaluation Bias From Data Contamination**
  - Why needed here: 40% of images are newly collected to avoid benchmark leakage into training data.
  - Quick check question: If an LVLM was trained on COCO images, why might its performance on OK-VQA be inflated?

## Architecture Onboarding

- **Component map:** Input: (Image, Multimodal Question) → LVLM → Answer; Evaluation pipeline: Extract ROI → Compute difficulty factors → Compare text-only vs multimodal scores → Calculate RD; Grading: LVLM response → DeepSeek-V3 classifier → {correct, incorrect, refusal}

- **Critical path:** 1. Annotate ROI and rationale for each sample; 2. Reformulate multimodal question to text-only using rationale; 3. Run model on both question types; 4. Grade responses and compute RD

- **Design tradeoffs:** Short-form answers simplify grading but exclude reasoning evaluation; single-ROI questions reduce complexity but limit coverage of multi-entity scenes; Knowledge popularity assessed by GPT-4o introduces circularity

- **Failure signatures:** High text-only accuracy + low multimodal accuracy + high RD → Visual module weakness; Low text-only accuracy → Linguistic module lacks required knowledge; High refusal rate → Model uncertainty calibration

- **First 3 experiments:** 1. Run your LVLM on VisualSimpleQA text-only questions to baseline linguistic knowledge; 2. Compute RD across difficulty levels; 3. Compare performance on newly collected vs. existing-dataset images

## Open Questions the Paper Calls Out
- Can the decoupled evaluation framework be effectively extended to multimodal questions requiring reasoning across multiple regions of interest (ROIs)?
- How can factuality evaluation be adapted for multimodal tasks involving visual description or reasoning where ground truth answers are long-form and non-unique?
- How can the diagnostic signals from relative performance degradation be utilized to create targeted training interventions for the visual modules of LVLMs?

## Limitations
- Decoupling mechanism assumes text-only questions fully capture visual information without requiring spatial reasoning
- Knowledge popularity metric relies on GPT-4o's judgment, creating potential circularity in difficulty assessment
- Short-form answer constraint limits evaluation of complex reasoning and long-form factuality

## Confidence
- **High Confidence:** Decoupling mechanism effectiveness for basic fact-seeking QA; difficulty criteria predictive power; automatic grading reliability (>94% human agreement)
- **Medium Confidence:** Knowledge popularity scores as difficulty predictors; equal weighting of difficulty factors; visual-linguistic module separation in integrated architectures
- **Low Confidence:** Generalizability to non-fact-seeking multimodal tasks; effectiveness for models with heavy visual-linguistic fusion; robustness against adversarial visual attacks

## Next Checks
1. Test decoupling assumption by running a subset through human evaluation to identify samples where text-only questions implicitly require visual reasoning
2. Validate difficulty metric by comparing predicted difficulty scores against empirical failure rates across multiple LVLM families and scales
3. Assess robustness by evaluating model performance on adversarial samples where visual information contradicts textual information in the image