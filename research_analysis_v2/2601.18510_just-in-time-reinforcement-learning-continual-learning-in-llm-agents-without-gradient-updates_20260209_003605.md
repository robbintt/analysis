---
ver: rpa2
title: 'Just-In-Time Reinforcement Learning: Continual Learning in LLM Agents Without
  Gradient Updates'
arxiv_id: '2601.18510'
source_url: https://arxiv.org/abs/2601.18510
tags:
- learning
- jitrl
- memory
- policy
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: JitRL enables frozen LLMs to continuously adapt at test time through
  memory-based policy optimization without gradient updates. The framework retrieves
  relevant trajectories to estimate action advantages and directly modulates LLM output
  logits, providing an exact closed-form solution to KL-constrained policy optimization.
---

# Just-In-Time Reinforcement Learning: Continual Learning in LLM Agents Without Gradient Updates

## Quick Facts
- arXiv ID: 2601.18510
- Source URL: https://arxiv.org/abs/2601.18510
- Reference count: 40
- Primary result: Enables frozen LLMs to continuously adapt at test time through memory-based policy optimization without gradient updates, achieving state-of-the-art performance among training-free methods.

## Executive Summary
JitRL introduces a training-free framework that enables frozen LLMs to continuously adapt at test time through memory-based policy optimization. The approach maintains a dynamic memory of past experiences and retrieves relevant trajectories to estimate action advantages on-the-fly, which are then used to directly modulate the LLM's output logits. This provides an exact closed-form solution to KL-constrained policy optimization, achieving state-of-the-art performance among training-free methods on WebArena and Jericho benchmarks while reducing computational costs by over 30x compared to fine-tuning approaches.

## Method Summary
JitRL operates by maintaining a non-parametric memory of state-action-return triplets and using k-nearest neighbor retrieval to estimate value functions at test time. The framework computes advantages by comparing Q-values for specific actions against baseline state values, then applies these advantages as additive updates to the LLM's output logits according to a closed-form KL-constrained optimization solution. A reflective evaluator assigns step-wise rewards to completed trajectories, enabling credit assignment in sparse-reward environments. The system operates entirely at inference time without any gradient updates to the frozen LLM backbone.

## Key Results
- Establishes new state-of-the-art among training-free methods on WebArena and Jericho benchmarks
- Outperforms computationally expensive fine-tuning methods like WebRL while reducing monetary costs by over 30x
- Demonstrates strong generalization across different LLM backbones and unseen tasks through effective cross-task memory utilization
- Achieves consistent performance gains through improved decision-making correction mechanisms

## Why This Works (Mechanism)

### Mechanism 1: Memory-based value estimation
Memory-based value estimation enables on-the-fly advantage computation without training a value network. The framework stores trajectory triplets (s, a, G) where G is the discounted return, retrieves top-k similar states via Jaccard similarity, and estimates V(s) as the mean return across neighbors. The advantage Â(s,a) = Q̂(s,a) - V̂(s) quantifies how much better an action is relative to the local baseline. This works under the assumption that state representations are sufficiently discriminative that retrieved neighbors share similar value profiles.

### Mechanism 2: Additive logit update
The additive logit update implements the exact closed-form solution to KL-constrained policy optimization. The optimal policy under a KL penalty is π*(a|s) ∝ πθ(a|s)·exp(β·A(s,a)), which translates to z'(s,a) = z(s,a) + β·Â(s,a) for the log-ratios. This directly modulates the frozen LLM's output distribution without backpropagation, working under the assumption that the LLM's softmax output faithfully represents πθ(a|s) and that advantage estimates are sufficiently accurate.

### Mechanism 3: Reflective step-wise rewards
Reflective step-wise rewards improve credit assignment in long-horizon tasks with sparse environment signals. After each episode, an LLM-based evaluator assigns scalar rewards rt ∈ [-3, +3] to each action based on usefulness and certainty, which are aggregated into discounted returns for storage. This assumes the evaluator's judgments correlate with true task progress and that the discount factor appropriately balances immediate versus future rewards.

## Foundational Learning

- **Advantage Function (RL)**: Understanding A(s,a) = Q(s,a) - V(s) is essential because JitRL's core mechanism computes advantages to rank actions relative to a baseline. Quick check: Given V(s) = 10, Q(s, a₁) = 12, Q(s, a₂) = 8, what are the advantages and which action should receive a positive logit boost?

- **KL-Divergence as a Trust Region**: Understanding why KL constraints prevent distribution collapse is essential for tuning β, as the logit update is derived as the solution to a KL-constrained optimization. Quick check: Why does maximizing expected advantage without any constraint risk incoherent outputs from an LLM?

- **k-Nearest Neighbors for Non-Parametric Estimation**: Understanding bias-variance tradeoffs in kNN is critical for selecting k, as V̂ and Q̂ are computed by averaging returns over retrieved neighbors. Quick check: What happens to value estimate variance when k is too small? What happens to bias when k is too large?

## Architecture Onboarding

- **Component map**: Memory Bank -> State Abstractor -> Retriever -> Value Estimator -> Logit Modulator -> LLM -> Evaluator (offline)
- **Critical path**: Observe state → abstract to representation → retrieve neighbors from memory → generate candidate actions from LLM + augment with retrieved actions → estimate V̂, Q̂, compute Â for each candidate → modulate logits, sample action → after episode: evaluator assigns rewards, compute G_t, store in memory
- **Design tradeoffs**:
  - k (retrieval neighbors): Optimal range 8-14 for tested domains. Lower k → high variance; higher k → noise.
  - β (temperature): Controls KL constraint strength. Higher β → faster adaptation but risk of incoherence.
  - λ (exploration rate): Controls optimism bonus for unseen actions. Too high → over-exploration; too low → premature convergence.
  - State abstraction granularity: Text-based summaries outperform embeddings for text games; regularized URLs for web navigation.
- **Failure signatures**:
  - Stagnant learning curve: Memory retrieval returning irrelevant neighbors → check state abstraction discriminability.
  - Erratic action selection: Advantage estimates too noisy → reduce β or increase k.
  - Repeating failed actions: Exploration bonus not triggering → check λ and α settings.
  - Cross-task transfer failure: If cross-task memory utilization <20%, retrieval may be overfitting to task-specific features.
- **First 3 experiments**:
  1. Baseline sanity check: Run Static agent on 5 WebArena tasks for 5 episodes each. Verify no learning occurs (Avg ≈ Final). Then run JitRL with default hyperparameters; expect 5-10% Final > Avg gap.
  2. Ablation on logit vs. prompt update: On a single domain (e.g., Admin), compare JitRL (Logit Update) vs. JitRL (Prompt Update). Expect logit update to outperform by 3-5% as per Table 8.
  3. Memory coverage analysis: After 10 episodes, compute |N(s)| distribution across states. Identify states with |N(s)| < 3 as under-explored; verify exploration bonus is activating for those states.

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability concerns for higher dimensional or continuous state spaces due to reliance on exact string-based or Jaccard similarity matching
- Assumption that neighborhood averaging yields consistent advantage estimates requires empirical validation across diverse task distributions
- Reflective evaluator reliability across task types depends on LLM's ability to assign meaningful scalar rewards without task-specific prompting
- Practical choice of β remains heuristic despite closed-form optimality under KL constraints

## Confidence
- **High** for the core mechanism of memory-based advantage estimation and its theoretical consistency guarantees
- **High** for the empirical performance improvements over training-free baselines on WebArena and Jericho
- **Medium** for the claimed generalization to unseen tasks, as cross-task transfer results are promising but not extensively validated
- **Medium** for the efficiency claims (30x cost reduction), as these are relative to fine-tuning methods and depend on specific deployment contexts
- **Low** for the robustness of the reflective evaluator mechanism, given limited ablation or sensitivity analysis in the paper

## Next Checks
1. Test JitRL on a benchmark with higher dimensional continuous states (e.g., Atari or MuJoCo) to assess scalability of the kNN retrieval approach.
2. Conduct an ablation study on the reflective evaluator: compare performance with and without evaluator-assigned rewards, and with alternative reward shaping methods.
3. Perform a hyperparameter sensitivity analysis for β across multiple task families to identify robust tuning guidelines.