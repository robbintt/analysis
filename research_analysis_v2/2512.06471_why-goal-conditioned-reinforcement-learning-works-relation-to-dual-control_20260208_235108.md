---
ver: rpa2
title: 'Why Goal-Conditioned Reinforcement Learning Works: Relation to Dual Control'
arxiv_id: '2512.06471'
source_url: https://arxiv.org/abs/2512.06471
tags:
- control
- state
- reward
- objective
- goal-oriented
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes goal-conditioned reinforcement learning (RL)
  from an optimal control perspective, showing that probabilistic rewards are theoretically
  justified and practically superior to classical dense rewards. The authors derive
  an optimality gap between goal-oriented and classical objectives, proving that the
  goal-conditioned reward provides better global performance, especially in nonlinear
  and uncertain environments.
---

# Why Goal-Conditioned Reinforcement Learning Works: Relation to Dual Control

## Quick Facts
- arXiv ID: 2512.06471
- Source URL: https://arxiv.org/abs/2512.06471
- Authors: Nathan P. Lawrence; Ali Mesbah
- Reference count: 4
- Primary result: Goal-conditioned RL is theoretically justified and practically superior to classical dense rewards, especially in nonlinear and uncertain environments

## Executive Summary
This paper analyzes goal-conditioned reinforcement learning (RL) from an optimal control perspective, showing that probabilistic rewards are theoretically justified and practically superior to classical dense rewards. The authors derive an optimality gap between goal-oriented and classical objectives, proving that the goal-conditioned reward provides better global performance, especially in nonlinear and uncertain environments. They extend the analysis to partially observed settings, connecting the reward to state estimation and dual control. Through case studies on a double inverted pendulum and a continuous stirred tank reactor, they demonstrate that goal-oriented policies outperform classical ones, particularly when combined with effective optimization methods.

## Method Summary
The paper compares goal-conditioned and classical reward formulations using theoretical analysis and empirical validation. For the theoretical analysis, they derive an optimality gap showing goal-conditioned rewards are strictly more expressive than classical quadratic/dense rewards. Empirically, they implement two case studies: (1) Double inverted pendulum stabilized using Differentiable Predictive Control (DPC) with SOAP optimizer, comparing Gaussian-shaped stage cost versus quadratic cost, and (2) Continuous Stirred Tank Reactor (CSTR) with Soft Actor-Critic (SAC) and particle filter belief states, comparing six agent variants (Full/Partial/Minimal × Goal-conditioned/Quadratic) under parameter uncertainty.

## Key Results
- Goal-conditioned rewards provide lower-bounded objectives strictly more expressive than classical quadratic/dense rewards
- The optimality gap between goal-oriented and classical objectives is proven via Jensen's inequality
- Goal-conditioned rewards naturally encode dual control behavior, balancing estimation and control
- Particle filter-based belief states enable effective partially observed Markov decision process (POMDP) solutions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Goal-conditioned rewards provide a lower-bounded objective that is strictly more expressive than classical quadratic/dense rewards for reaching target states.
- **Mechanism:** The paper proves (Theorem 1) via Jensen's inequality that the discounted sum of log-probabilities (classical) is always ≤ log of the discounted sum of probabilities (goal-oriented). This creates a mathematical separation: the objectives are equivalent only locally near the goal, but diverge globally.
- **Core assumption:** The state transition density is bounded and γ∈(0,1), ensuring the sums are convex combinations.
- **Evidence anchors:** [abstract] "We derive an optimality gap between more classical, often quadratic, objectives and the goal-conditioned reward."
- **Break condition:** If transition dynamics are nearly deterministic (low process noise ω), the probabilistic density p(x'=0|x,u) becomes degenerate (near-binary), and the advantage over classical rewards diminishes.

### Mechanism 2
- **Claim:** Goal-conditioned rewards naturally encode dual control behavior—balancing estimation and control—without explicit probing incentives.
- **Mechanism:** In partially observed settings (POMDP), the goal-conditioned reward r(b,u) = ∫p(x'=0|b,u,y')p(y'|b,u)dy' decomposes into two coupled terms: the first is goal-achievement probability conditioned on observation (control), and the second is measurement likelihood (probing).
- **Core assumption:** The agent maintains a belief state b_t via Bayesian filtering (Eq. 2) and has access to a measurement model p(y|x,ν).
- **Evidence anchors:** [abstract] "We then consider the partially observed Markov decision setting and connect state estimation to our probabilistic reward, further making the goal-conditioned reward well suited to dual control problems."
- **Break condition:** If particle filter collapse occurs (all particles concentrated at a single hypothesis), the measurement likelihood term provides no gradient for exploration.

### Mechanism 3
- **Claim:** The goal-conditioned objective is sparse in the time horizon (depends on when the goal is reached), not the initial state space, avoiding degeneracy from distant starting positions.
- **Mechanism:** For a trajectory starting far from the origin but reaching the goal in T steps: the classical objective exp(-½∥x₀∥²)exp(-½γ∥x₁∥²)... ≈ 0 (entirely dependent on the large initial distance), while the goal-oriented objective Σγᵗexp(-½∥x_t∥²) ≈ γᵀ/(1-γ) (depends only on arrival time T).
- **Core assumption:** The discount factor γ is sufficiently high (close to 1) that γᵀ/(1-γ) remains meaningful for achievable T values.
- **Evidence anchors:** [abstract] "elucidating the success of goal-conditioned RL and why classical 'dense' rewards can falter"
- **Break condition:** If γ is too low (e.g., γ<0.9 for long-horizon tasks), even the goal-oriented objective becomes dominated by early timesteps.

## Foundational Learning

- **Concept: Partially Observed Markov Decision Process (POMDP) and belief states**
  - Why needed here: The dual control analysis (Section 3.2) operates entirely in belief space, not state space. Understanding how b_t = p(x_t|I_t) evolves via recursive Bayesian estimation (Eq. 2) is essential for implementing the reward in Eq. 6.
  - Quick check question: Given a particle filter with particles {x⁽ⁱ⁾} and weights {w⁽ⁱ⁾}, can you compute the belief-state expectation E[f(x)|b_t] for an arbitrary function f?

- **Concept: Dynamic programming and the Bellman equation**
  - Why needed here: The entire analysis frames goal-conditioned RL through the Bellman optimality equation (Eq. 5), which the paper argues is more naturally expressed with probabilistic rewards. Corollary 4's proof requires understanding how the Bellman equation generates optimal policies.
  - Quick check question: In the Bellman equation V*(b) = max_u{r(b,u) + γE[V*(b')]}, explain why the optimal action depends on both the immediate reward and the future value function's dependence on the next belief state.

- **Concept: Exponential-of-quadratic vs. quadratic cost functions**
  - Why needed here: The core theoretical contribution (Theorem 1, Corollary 5) hinges on the relationship between J_classical = Σ(x'Mx + u'Ru) and J_goal = Σexp(-½x'Mx). Understanding how Jensen's inequality creates an optimality gap between these requires comfort with both formulations.
  - Quick check question: For a scalar state x=3 with M=1, compare the values of x² and exp(-½x²). Which one is more sensitive to large deviations from zero?

## Architecture Onboarding

- **Component map:**
  - Environment (Eq. 1): Transition f(x,u,ω), measurement g(x,ν) with process/measurement noise
  - Particle filter (Eq. 7): Approximates belief state b_t from observations {y₀,...,y_t} and actions
  - Reward estimator (Eq. 6): Computes r(b,u) using particle-based histogram approximation of p(x'=0|b,u,y')
  - Value network Q_φ(b,u): Belief-state critic using particle average (Eq. 8): Q_φ(b,u) ≈ (1/p)ΣQ̃_φ(x⁽ⁱ⁾)
  - Policy network μ_θ: Actor mapping particles to action via u = (1/p)Σμ_θ(x⁽ⁱ⁾)

- **Critical path:**
  1. Environment step: Execute u_t, observe y_{t+1}
  2. Belief update: Propagate particles through f, weight by p(y_{t+1}|x), resample
  3. Reward computation: For each particle, evaluate goal-proximity indicator; aggregate as histogram
  4. Value update: Train Q_φ using TD error with target network
  5. Policy update: Maximize E[Q_φ(b, μ_θ(b))] via gradient ascent

- **Design tradeoffs:**
  - Particle count: More particles → better belief approximation but higher compute. Paper shows 10 vs. 100 particles have similar control performance but different reward estimation accuracy (Fig. 2).
  - Reward shaping: Gaussian-shaped exp(-½∥x∥²/σ²) vs. true density p(x'=0|x,u). Paper advocates for Gaussian with tunable σ when true density is intractable (Corollary 5 discussion).
  - Optimizer: SOAP (Adam + higher-order preconditioning) significantly outperforms vanilla Adam for the double pendulum (Fig. 1), suggesting optimization landscape matters for goal-oriented objectives.

- **Failure signatures:**
  - Particle deprivation: If all particles converge to same hypothesis, belief-state value function loses gradient signal. Mitigation: Increase particle count, add regularization to particle diversity.
  - Reward sparsity in time: If goal is never reached during training, the exponential reward provides no learning signal. Mitigation: Curriculum learning starting closer to goal, or use herding/goal relabeling techniques.
  - Dense reward degeneracy: If using quadratic rewards on long-horizon tasks from distant initial states, the objective becomes ≈0 everywhere (Section 1.1). Detection: Check if episode returns are dominated by initial state distance rather than goal achievement.

- **First 3 experiments:**
  1. **Ablation on reward type (quadratic vs. goal-conditioned):** Implement both reward formulations on the double inverted pendulum (Section 5.1) with identical policy architecture and optimizer. Compare success rates (cos(θ) ≈ 1 for both links) and trajectory characteristics.
  2. **Particle count sensitivity:** Train the CSTR agent (Section 5.2) with p∈{5, 10, 25, 50, 100} particles. Measure evaluation performance (time near goal) and training stability.
  3. **Dual control validation:** Compare "Full" vs. "Minimal" agents (Fig. 3) on the CSTR with parameter uncertainty (α, β varying). Verify that the goal-conditioned agent improves when exposed to uncertainty during training (dual effect), while quadratic-reward agents show flat performance across settings.

## Open Questions the Paper Calls Out
- **Future work may extend the control horizon by combining the goal-oriented policy with local stabilizing policies or RL methods to guarantee indefinite stability.**
- **The authors note their DPC-trained policies were not trained beyond 75 time steps from the initial state, so they do not necessarily stabilize the system indefinitely.**

## Limitations
- The theoretical analysis assumes bounded transition densities and may not generalize to deterministic or highly constrained systems
- The empirical validation focuses on relatively low-dimensional systems (2-link pendulum, 4-state CSTR) without testing scalability
- The paper does not address how to guarantee indefinite stability beyond finite horizons

## Confidence
- **Mechanism 1 (Optimality Gap):** High confidence - The Jensen's inequality proof is mathematically rigorous and conditions are well-specified
- **Mechanism 2 (Dual Control):** Medium confidence - The derivation relies on accurate belief-state representation, but particle filter collapse could invalidate probing behavior
- **Mechanism 3 (Degeneracy Avoidance):** High confidence - Theoretical analysis is sound, but empirical validation in high-dimensional tasks is needed

## Next Checks
1. Test goal-conditioned vs. classical rewards on a long-horizon, high-dimensional task (e.g., quadruped locomotion) to verify Mechanism 3's degeneracy avoidance holds beyond 2-link pendulums
2. Implement belief-state RL with 10 vs. 100 particles on a partially observed maze navigation task to measure the impact of particle filter collapse on dual control behavior
3. Conduct a systematic ablation study on reward shaping bandwidth σ for the CSTR task to determine if the theoretical optimality gap persists across different reward parametrizations