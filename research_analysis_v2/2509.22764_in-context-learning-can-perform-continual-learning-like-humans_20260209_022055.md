---
ver: rpa2
title: In-Context Learning can Perform Continual Learning Like Humans
arxiv_id: '2509.22764'
source_url: https://arxiv.org/abs/2509.22764
tags:
- performance
- retention
- task
- length
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: In-context learning (ICL) in large language models (LLMs) can be
  extended to in-context continual learning (ICCL) by leveraging task scheduling and
  prompt rearrangement to achieve long-term retention across sequentially presented
  tasks. Experiments on Markov-chain benchmarks demonstrate that ICCL benefits from
  distributed practice (DP) in a manner analogous to human memory, consistently revealing
  a spacing "sweet spot" that enhances retention performance compared to single or
  massed practice.
---

# In-Context Learning can Perform Continual Learning Like Humans

## Quick Facts
- **arXiv ID:** 2509.22764
- **Source URL:** https://arxiv.org/abs/2509.22764
- **Reference count:** 40
- **Primary result:** ICCL with prompt rearrangement achieves spacing sweet spots for continual learning retention, with linear-attention models exhibiting human-like memory dynamics.

## Executive Summary
This paper extends in-context learning (ICL) to in-context continual learning (ICCL) by using task scheduling and prompt rearrangement to achieve long-term retention across sequentially presented tasks. Experiments on Markov-chain benchmarks show that ICCL benefits from distributed practice in a manner analogous to human memory, with a spacing "sweet spot" that enhances retention performance. A human-retention similarity metric reveals that linear-attention models such as MAMBA and RWKV exhibit particularly human-like retention dynamics, despite lower absolute performance than Transformer-based LLMs.

## Method Summary
The method extends ICL to continual learning by interleaving target task demonstrations with interference tasks using distributed practice scheduling. Task identifiers are prepended to experience segments to mitigate cross-task confusion. The framework tests retention performance across three scheduling strategies: Single Practice (one contiguous block), Massed Practice (extended block), and Distributed Practice (alternating target/interference blocks). Performance is measured via normalized retention using Bhattacharyya distance, and human-like retention dynamics are assessed via a Mahalanobis distance metric comparing fitted ACT-R parameters to human baselines.

## Key Results
- ICCL with distributed practice reveals a spacing "sweet spot" that enhances retention performance compared to single or massed practice.
- Linear-attention models (MAMBA, RWKV) exhibit retention dynamics that align more closely with human cognitive models than standard Transformers.
- Task identifiers act as effective contextual anchors that significantly improve retention performance, especially in complex tasks.

## Why This Works (Mechanism)

### Mechanism 1: Distributed Practice via Prompt Rearrangement
Retention is improved by spacing out exposures to a target task rather than massing them, provided the spacing interval is optimized. ICCL leverages the model's context window as a temporary memory buffer, forcing repeated retrieval and re-activation of latent representations through interleaving target and interference tasks.

### Mechanism 2: Task Identifiers as Contextual Anchors
Explicit labels prepended to experience sequences act as conditional signals that mitigate cross-task interference. The model learns to associate specific transition dynamics with arbitrary token IDs, reducing ambiguity during interleaved practice.

### Mechanism 3: Retention Dynamics Driven by Architecture Type
Linear-attention models exhibit retention dynamics that align more closely with human cognitive models than standard Transformers. This is attributed to inherent recurrence or decay mechanisms in state-space models that mimic human memory consolidation patterns.

## Foundational Learning

- **In-Context Learning (ICL):** Base capability being extended - LLMs perform tasks via demonstration in the prompt without weight updates. *Why needed:* This is the foundation ICCL builds upon. *Quick check:* Can you explain how a model performs a task using only examples in the input prompt?
- **Catastrophic Forgetting:** Primary problem ICCL solves - standard gradient-based learning overwrites previous knowledge when learning new tasks sequentially. *Why needed:* Understanding this problem motivates the ICCL approach. *Quick check:* Why does fine-tuning a model on Task B often ruin its performance on Task A?
- **Markov Chains (Discrete):** Evaluation benchmark used to isolate memory mechanisms from pre-existing knowledge. *Why needed:* Understanding the task structure is crucial for interpreting results. *Quick check:* If the current state is S1 and the transition probability to S2 is 0.8, what is the model's objective in this benchmark?

## Architecture Onboarding

- **Component map:** Backbones (LLM/SSM) -> Prompt Scheduler (SP/MP/DP) -> Identifier System (d_τ + D_τ) -> Model Inference -> Performance Measurement
- **Critical path:** 1) Select model backbone 2) Generate DMC data 3) Construct Historical Sequence using DP strategy 4) Inject Task Identifiers 5) Feed to model and measure Normalized Performance
- **Design tradeoffs:** GBCL (EWC) updates weights (stability/plasticity trade-off) but is stable; ICCL is inference-only (no forgetting of base weights) but bounded by Context Window. Transformers offer higher raw retention scores, but Linear-Attention models offer dynamics more similar to human memory.
- **Failure signatures:** Context Overflow (sequence exceeds context window), Missing Identifiers (performance drops 10-30%), Task Confusion without proper labeling
- **First 3 experiments:** 1) Replicate spacing sweet spot with 4-state Markov Chain varying φI 2) Ablate identifiers in DP experiment 3) Compare architecture retention vs. HRS-MD scores across Transformer and Recurrent models

## Open Questions the Paper Calls Out

### Open Question 1
How does ICCL retention performance and the spacing effect generalize from synthetic Markov-chain benchmarks to complex, real-world natural language tasks? The authors use DMCs to isolate memory mechanisms but don't validate spacing sweet spots on standard NLP benchmarks.

### Open Question 2
What architectural mechanisms allow linear-attention models to achieve higher Human Retention Similarity than Transformer-based models? The paper characterizes this as a surprising empirical finding without theoretical explanation.

### Open Question 3
Can the optimal inter-study interval (spacing sweet spot) be predicted or adapted dynamically by the model during inference? The paper establishes the sweet spot exists but uses manual grid search rather than adaptive scheduling.

## Limitations

- The ACT-R-based human-retention similarity metric relies on fitting three free parameters and assumes ACT-R adequately captures human memory dynamics across all task types
- Task identifiers may provide an artificial memory aid that humans don't have access to, raising questions about whether ICCL achieves truly human-like continual learning
- The spacing sweet spot and retention dynamics may not generalize from synthetic Markov-chain benchmarks to complex, real-world natural language tasks

## Confidence

- **High confidence:** Core experimental results showing spacing sweet spots in ICCL retention and the general framework of using prompt rearrangement for continual learning
- **Medium confidence:** Claim that ICCL achieves continual learning without catastrophic forgetting in a human-like manner, based on indirect comparison via ACT-R
- **Low confidence:** Specific claim that linear-attention models are "more human-like" than Transformers, based on a single similarity metric that may not capture all aspects of human-like cognition

## Next Checks

1. Cross-task identifier ablation with real-world tasks: Remove task identifiers and test ICCL on permuted MNIST or text classification sequence to determine if performance gains persist without artificial memory aids

2. Robustness of HRS-MD to parameter fitting: Systematically vary ACT-R parameter fitting initialization and optimization constraints across multiple runs, reporting variance in HRS-MD scores

3. Alternative similarity metrics: Compute additional retention curve similarity measures (area between curves, Wasserstein distance) between LLM and human data, comparing rankings to HRS-MD for consistency