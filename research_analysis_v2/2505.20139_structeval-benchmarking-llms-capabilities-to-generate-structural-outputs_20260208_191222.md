---
ver: rpa2
title: 'StructEval: Benchmarking LLMs'' Capabilities to Generate Structural Outputs'
arxiv_id: '2505.20139'
source_url: https://arxiv.org/abs/2505.20139
tags:
- tasks
- zhang
- wang
- structured
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces StructEval, a comprehensive benchmark for
  evaluating large language models' (LLMs) capabilities in generating and converting
  structured outputs across 18 formats and 44 task types. The benchmark addresses
  the growing need for precise structural control in software development and related
  domains, where LLMs must produce valid JSON, YAML, HTML, React, SVG, and other structured
  formats.
---

# StructEval: Benchmarking LLMs' Capabilities to Generate Structural Outputs

## Quick Facts
- arXiv ID: 2505.20139
- Source URL: https://arxiv.org/abs/2505.20139
- Reference count: 25
- Models show significant performance gaps, with best open-source models lagging ~10 points behind state-of-the-art

## Executive Summary
StructEval is a comprehensive benchmark evaluating large language models' capabilities in generating and converting structured outputs across 18 formats and 44 task types. The benchmark addresses the growing need for precise structural control in software development, where LLMs must produce valid JSON, YAML, HTML, React, SVG, and other structured formats. Experimental results reveal that even state-of-the-art models achieve only 75.58 average score, with generation tasks proving more challenging than conversion tasks, and visual content production more difficult than text-only structures.

## Method Summary
The benchmark employs two complementary paradigms: generation tasks (producing structured output from natural language prompts) and conversion tasks (translating between structured formats). It uses novel metrics including syntax validation, keyword matching via dot-path rules, and visual question answering for renderable formats. The evaluation pipeline involves zero-shot inference with specific prompt templates, syntax parsers for text formats, headless renderers for visual formats, and VQA evaluation using GPT-4.1-mini. The final scores weight syntax, keyword matching, and VQA differently for text-only versus visual tasks.

## Key Results
- Performance gaps are significant, with o1-mini achieving 75.58 average score and best open-source models lagging approximately 10 points behind
- Generation tasks are consistently more challenging than conversion tasks across all models
- Visual content production is more difficult than generating text-only structures, with StructEval-V scores trailing StructEval-T
- Several tasks remain particularly challenging, with all models scoring below 0.5, including Text→Mermaid and Matplotlib→TikZ conversions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Models perform better on structurally straightforward formats with high training prevalence than on niche formats requiring complex structural reasoning.
- Mechanism: Performance correlates with format frequency in training corpora. Common formats (JSON, HTML, Markdown) achieve >90% scores across models because syntax patterns are well-represented, while less common formats (Mermaid, TikZ) score <0.5 for all models due to insufficient exposure and more complex structural constraints.
- Core assumption: Training data distribution correlates with downstream benchmark performance on structured outputs.
- Evidence anchors:
  - [abstract] "Several tasks remain particularly challenging, with all models scoring below 0.5, including Text→Mermaid and Matplotlib→TikZ conversions."
  - [section 4.2] "Tasks... with most models achieving scores exceeding 90%... include generation tasks for common formats such as JSON, HTML, CSV, Markdown, and YAML."
  - [corpus] Related work (SO-Bench, SLOT) similarly finds that constrained decoding struggles with complex schemas, supporting training-prevalence hypothesis.
- Break condition: If model performance on niche formats improves dramatically without format-specific fine-tuning, the training-prevalence mechanism would be weakened.

### Mechanism 2
- Claim: Conversion tasks are easier than generation tasks because input structure provides explicit scaffolding for output organization.
- Mechanism: In conversion tasks, the model receives structured input (e.g., YAML with nested hierarchies) that constrains the output search space. The input's structure acts as an external template. In generation tasks, the model must simultaneously infer structure from natural language and produce valid syntax, increasing cognitive load.
- Core assumption: Task difficulty is partly determined by whether structural information is given versus inferred.
- Evidence anchors:
  - [abstract] "We find generation tasks more challenging than conversion tasks."
  - [section 4.2, Figure 6b] Shows consistent pattern across both StructEval-T and StructEval-V that conversion outperforms generation.
  - [corpus] SLOT and related structured output work emphasizes that decoding constraints help, but don't address the generation-from-scratch difficulty.
- Break condition: If models fine-tuned on generation tasks match conversion performance without architectural changes, the scaffolding mechanism would be insufficient.

### Mechanism 3
- Claim: Visual code generation requires an additional reasoning layer beyond syntax correctness—semantic-to-visual mapping.
- Mechanism: For renderable formats (HTML, SVG, React), models must (1) produce syntactically valid code AND (2) ensure the rendered output semantically matches requirements. The VQA evaluation (weighted 0.7 in StructEval-V) captures this second layer. Failure at either stage cascades, explaining why StructEval-V scores lag behind StructEval-T.
- Core assumption: Visual rendering correctness requires separate verification from syntax validity.
- Evidence anchors:
  - [abstract] "producing correct visual content is more difficult than generating text-only structures."
  - [section 3] StructEval-V final score weights: 0.2·syntax + 0.1·keyword + 0.7·VQA vs StructEval-T: 0.2·syntax + 0.8·keyword.
  - [corpus] Weak direct evidence—corpus papers focus on text structure, not visual rendering evaluation.
- Break condition: If models achieve high VQA scores but low keyword scores (or vice versa), the decomposition may need refinement.

## Foundational Learning

- **Structured Output Formats (JSON, YAML, XML, TOML)**
  - Why needed here: The benchmark evaluates 18 formats; understanding syntax rules, nesting patterns, and schema constraints is prerequisite to interpreting failure modes.
  - Quick check question: Can you trace why `planet.moons[0].name` is a valid keyword path in JSON but might fail in CSV?

- **Visual Rendering Pipelines (headless browser rendering, code-to-image)**
  - Why needed here: StructEval-V relies on rendering code (HTML, React, SVG) to generate images for VQA evaluation. Understanding this pipeline is essential for debugging evaluation failures.
  - Quick check question: If an HTML snippet passes syntax validation but the VQA score is 0, where would you debug first—CSS, DOM structure, or VLM judge?

- **Path-Based Evaluation (dot notation, wildcard matching, XML attributes)**
  - Why needed here: The keyword matching system uses path-based rules (Table 2) like `planet.moons.*.name` for nested structures. Understanding these patterns is critical for creating or interpreting evaluation metrics.
  - Quick check question: How would you express an XML attribute check for `<planet id="mars">` using the @-notation described?

## Architecture Onboarding

- **Component map:**
  Input Prompt → LLM → Structured Output → [Syntax Validator] → binary pass/fail → [Keyword Matcher] → path-based scoring → [Render Engine]* → Visual Output → [VLM Judge] → VQA score

- **Critical path:**
  1. Prompt template (Table 4) enforces `<|BEGIN_CODE|>` and `<|END_CODE|>` delimiters—parsing failures here cascade to all downstream metrics.
  2. For StructEval-V: rendering failure → VQA score = 0 regardless of code correctness.

- **Design tradeoffs:**
  - VQA weight (0.7) maximizes visual semantic checking but makes evaluation dependent on VLM (GPT-4.1-mini) reliability and latency.
  - Keyword matching uses exact/regex rules—fast but brittle to semantically equivalent variations (e.g., `class="btn"` vs `className="btn"` in React).
  - Two-pass expert review ensures quality but limits dataset scale to 2,035 examples.

- **Failure signatures:**
  - Malformed delimiter tags (e.g., phi-3-mini outputting `|<|END_CODE|>`) → syntax score = 0.
  - Visual code renders but elements mispositioned → high syntax score, low VQA score.
  - Conversion task loses nested hierarchy → keyword score fails for deep paths.

- **First 3 experiments:**
  1. Replicate evaluation on a subset (e.g., Text→JSON only) using vLLM with greedy decoding to validate scoring pipeline matches reported metrics.
  2. Ablate VQA weight: test StructEval-V with weights [0.2, 0.8, 0.0] for [syntax, keyword, VQA] to isolate visual reasoning contribution.
  3. Error analysis on phi-3-mini's TOML→YAML failures—inspect whether delimiter malformation or schema transfer is the bottleneck.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can extending StructEval to interactive, dynamic rendering tasks reveal additional capability gaps in LLMs for generating fully functional user interfaces?
- Basis in paper: [explicit] The authors state the benchmark is "currently limited to single-page, non-interactive formats" and explicitly call for "future work could extend the benchmark to include dynamic rendering tasks."
- Why unresolved: The current evaluation does not assess dynamic behaviors such as button interactions, animations, or scroll events.
- What evidence would resolve it: A modified benchmark suite with interactive tasks and corresponding evaluation metrics for functional UI behavior.

### Open Question 2
- Question: What is the impact of using an LLM (GPT-4.1-mini) as the VQA judge on evaluation reliability, given potential model-specific biases or circularity?
- Basis in paper: [inferred] The VQA Score relies entirely on GPT-4.1-mini to judge visual correctness, with 88.66% accuracy on fair questions; however, no analysis of judge model bias or comparison across different VLM judges is provided.
- Why unresolved: Using one model to evaluate others introduces unknown systematic biases.
- What evidence would resolve it: A comparative study using multiple VLM judges (e.g., Gemini, Claude) with inter-judge agreement analysis.

### Open Question 3
- Question: Do the fixed weightings (0.2/0.8 for text-only; 0.2/0.1/0.7 for visual) optimally reflect real-world task priorities, or should weights be task-adaptive?
- Basis in paper: [inferred] The paper applies uniform metric weights across all tasks without empirical justification or sensitivity analysis.
- Why unresolved: Different downstream applications may prioritize syntax validity vs. semantic correctness differently.
- What evidence would resolve it: Ablation studies varying weights and correlating with human preference or downstream task success rates.

## Limitations

- The benchmark relies on GPT-4.1-mini for VQA scoring, introducing potential judge bias and single-point-of-failure dependencies.
- The 2,035-example dataset, while larger than related work, may not fully represent the complexity of real-world structured output tasks.
- Evaluation thresholds (e.g., 0.5 scores considered "low") are arbitrary and may not reflect practical utility thresholds.

## Confidence

- **High confidence**: Performance gaps between state-of-the-art and open-source models (Qwen3-4B ~10 points behind o1-mini), and the general finding that generation tasks are harder than conversion tasks.
- **Medium confidence**: The specific mechanisms linking training data prevalence to format performance, and the relative difficulty of visual versus text-only structured outputs.
- **Low confidence**: The decomposition of visual code generation difficulty into syntax + keyword + VQA components, as this requires further ablation studies.

## Next Checks

1. **Ablation study**: Test StructEval-V with varying VQA weights (0.2, 0.8, 0.0) to isolate the contribution of visual reasoning versus syntax and keyword matching.
2. **Judge diversity**: Implement multi-judge VQA scoring using different VLMs (e.g., GPT-4o, Claude-3) to quantify judge-dependent variance.
3. **Real-world replication**: Apply StructEval metrics to a held-out dataset of actual structured output generation tasks from software engineering workflows to validate practical relevance.