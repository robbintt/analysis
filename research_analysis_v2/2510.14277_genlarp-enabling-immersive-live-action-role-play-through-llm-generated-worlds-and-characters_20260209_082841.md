---
ver: rpa2
title: 'GenLARP: Enabling Immersive Live Action Role-Play through LLM-Generated Worlds
  and Characters'
arxiv_id: '2510.14277'
source_url: https://arxiv.org/abs/2510.14277
tags:
- narrative
- system
- larp
- genlarp
- live
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The GENLARP system addresses the challenge of making live action
  role-playing more accessible by eliminating the need for multiple participants and
  physical infrastructure. It leverages large language models and virtual reality
  to transform user-provided stories into immersive, interactive LARP experiences
  where users can act as both creators and players.
---

# GenLARP: Enabling Immersive Live Action Role-Play through LLM-Generated Worlds and Characters

## Quick Facts
- arXiv ID: 2510.14277
- Source URL: https://arxiv.org/abs/2510.14277
- Authors: Yichen Yu; Yifan Jiang; Mandy Lui; Qiao Jin
- Reference count: 11
- Key outcome: The GENLARP system addresses the challenge of making live action role-playing more accessible by eliminating the need for multiple participants and physical infrastructure. It leverages large language models and virtual reality to transform user-provided stories into immersive, interactive LARP experiences where users can act as both creators and players.

## Executive Summary
GenLARP is a system designed to make live action role-playing (LARP) more accessible by using large language models and virtual reality to transform user-provided stories into immersive, interactive experiences. The system allows users to act as both creators and players, designing characters and environments based on their descriptions, and then living within the generated story world. By leveraging AI-driven agents and real-time VR rendering, GenLARP enables single users to experience rich, non-linear narratives without requiring multiple participants or physical infrastructure.

## Method Summary
GenLARP is implemented as a three-module pipeline: (1) Narrative Initialization uses GPT-4o to extract structured narrative elements (spatial context, temporal cues, conflict sources, role distribution, task motivation) from natural language story descriptions; (2) Interactive Role Design manages LLM-driven characters with persistent internal states (memories, motivations, beliefs, and evolving relationships) to produce coherent multi-turn interactions; (3) Live-Action Role Play renders the environment and interactions in real-time VR using SynCity for 3D scene generation and Unity for interaction and rendering. The system supports non-linear storytelling, perspective switching, and dynamic character behavior based on user interactions.

## Key Results
- GenLARP enables single users to create and inhabit immersive LARP worlds using natural language input.
- The system integrates GPT-4o for narrative processing, SynCity for real-time 3D scene generation, and Unity for VR interaction.
- Character agents maintain persistent internal states and evolving relationships, supporting dynamic, AI-driven interactions.
- The approach supports non-linear storytelling and perspective switching, allowing users to explore narratives from multiple viewpoints.

## Why This Works (Mechanism)

### Mechanism 1: Structured Semantic Extraction from Natural Language
- Claim: Converting free-form story descriptions into structured semantic representations enables consistent downstream generation across scenes and characters.
- Mechanism: GPT-4o processes natural language input to extract spatial context, temporal cues, conflict sources, role distribution, and task motivation, then formats these as unified structured prompts following a shared data protocol.
- Core assumption: LLMs can reliably infer and complete missing narrative elements (character structure, conflict design, spatial layout) when user input is vague or incomplete.
- Evidence anchors:
  - [abstract]: "transforms personalized stories into immersive live action role-playing (LARP) experiences"
  - [section 2.1]: "The module... is responsible for receiving the natural language text input by the user and transforming it into semantic information with narrative structure through language modeling... The structured prompt follows a unified data protocol"
  - [corpus]: Storycaster demonstrates related AI-driven room-based storytelling but corpus lacks direct validation of extraction quality for LARP contexts
- Break condition: User inputs that are deeply contradictory or lack sufficient semantic signal for reliable inference; extraction quality degrades without validation feedback loop.

### Mechanism 2: Character Agency Through Internal State Management
- Claim: AI-driven characters with persistent internal states (memory, affect, beliefs) produce coherent multi-turn interactions that reflect individualized logic rather than pure context-window responses.
- Mechanism: Each character maintains independent state variables including memories, affective states, motivations, and beliefs; behavioral responses draw on these states plus social relationships that evolve dynamically during runtime.
- Core assumption: Decoupling character behavior from immediate context and anchoring it in persistent state produces more consistent, believable agent behavior over extended interactions.
- Evidence anchors:
  - [abstract]: "Generative AI and agents powered by Large Language Models (LLMs) enrich these experiences"
  - [section 2.2]: "Characters' behavioral responses are not only dependent on the current context, but also on their individual perceptions, social locations, and goal tendencies... The module provides a complete state tracking, behavior path recording and belief dynamic updating mechanism"
  - [corpus]: BookWorld (cited in paper) provides precedent for agent societies but corpus lacks empirical validation of long-term coherence
- Break condition: Extended interaction sequences that exceed memory persistence mechanisms; relationship networks that evolve into incoherent configurations without narrative guardrails.

### Mechanism 3: Immersive Feedback Loop Through VR-Mediated Interaction
- Claim: Real-time VR feedback based on user behavioral signals (position, path, interaction history) maintains engagement and enables adaptive narrative pacing.
- Mechanism: System tracks user position, behavior paths, and interaction preferences in VR; adjusts scene state, character behavior, and narrative rhythm in response; supports perspective switching and localized plot regression for non-linear exploration.
- Core assumption: Dynamic adjustment based on behavioral signals improves immersion and narrative engagement compared to static, pre-scripted experiences.
- Evidence anchors:
  - [abstract]: "GenLARP enables users to act as both creators and players, allowing them to design characters based on their descriptions and live in the story world"
  - [section 2.3]: "Based on the user's position in the virtual environment, behavior path, interaction history, perspective preference and other information, the system adjusts the scene state and character behavior response in real time"
  - [corpus]: Quest Atlantis (cited) supports immersive learning value; corpus lacks specific validation of AI-driven narrative-VR integration effects
- Break condition: Latency in scene generation or character response breaks presence; VR motion discomfort limits session duration; perspective switching confuses narrative tracking.

## Foundational Learning

- Concept: **LLM Prompt Engineering for Structured Extraction**
  - Why needed here: The narrative initialization module's effectiveness depends on reliably extracting structured elements from unstructured text.
  - Quick check question: Given a paragraph-length story description, can you design a prompt that extracts character motivations, spatial settings, and conflict sources into a JSON-like schema?

- Concept: **Agent Memory and State Persistence**
  - Why needed here: Characters must maintain coherent internal states across many interaction turns; understanding state management patterns is essential.
  - Quick check question: How would you architect a data structure to track a character's evolving relationships, beliefs, and episodic memories over 100+ interaction turns?

- Concept: **VR System Integration and Latency Budgets**
  - Why needed here: Real-time VR requires tight latency constraints; generated assets must flow through GPT-4o → SynCity → Unity pipeline without breaking presence.
  - Quick check question: What is the approximate latency budget for maintaining VR presence (frame timing), and where in the GenLARP pipeline are the likely bottleneck points?

## Architecture Onboarding

- Component map:
  Narrative Initialization -> Interactive Role Design -> Live-Action Role Play

- Critical path:
  User story input -> GPT-4o extraction -> Structured prompt generation -> SynCity 3D scene construction -> Unity asset import -> VR rendering -> User interaction -> Character agent state update + response generation -> Scene state adjustment -> Loop

- Design tradeoffs:
  - Free-form input flexibility vs. narrative coherence: Auto-completion helps usability but may inject unintended elements.
  - Character autonomy vs. plot directionality: Independent agent behavior enriches emergent narrative but risks divergence from intended arcs.
  - Generation quality vs. latency: Higher-quality scene/character generation increases latency, potentially breaking VR immersion.

- Failure signatures:
  - Semantic drift: Characters behave inconsistently as session length grows (memory/state limits).
  - Scene-context mismatch: Generated 3D environments don't align with narrative semantics.
  - Interaction latency spikes: Delay between user action and system response breaks presence.
  - Relationship network collapse: Evolving character relationships become incoherent without guardrails.

- First 3 experiments:
  1. **Semantic extraction accuracy test**: Input varied story descriptions (complete, vague, contradictory) and measure extraction precision/recall against human-labeled ground truth for key narrative elements.
  2. **Character coherence stress test**: Run 50+ turn dialogues with AI characters; measure consistency of stated beliefs, recalled events, and relationship states across time.
  3. **End-to-end latency profiling**: Instrument the full pipeline (GPT-4o -> SynCity -> Unity -> VR) to identify bottlenecks; target sub-second response for interaction turns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can memory persistence and narrative planning frameworks be integrated to maintain long-term narrative coherence and character consistency during extended play sessions?
- Basis in paper: [explicit] Section 4 states that "maintaining long-term coherence over long periods of narrative remains a challenge" and suggests incorporating memory persistence and narrative planning.
- Why unresolved: Current LLM agents often suffer from context drift or "forgetting" earlier plot points over long interactions, and the paper does not detail specific architectures to solve this.
- What evidence would resolve it: Successful demonstration of a system that maintains consistent character beliefs and continuous plot arcs over significantly longer interaction periods (e.g., hours or days) without semantic breaks.

### Open Question 2
- Question: Does the integration of multimodal inputs (sketches, soundscapes, voice) improve the semantic alignment or user satisfaction of generated 3D scenes compared to the current text-only approach?
- Basis in paper: [explicit] Section 4 notes that "the current scene generation process is guided only by structured textual input" and proposes integrating "multimodal conditions such as sketches, soundscapes, or voice descriptions."
- Why unresolved: It is unclear if adding multimodal data improves the spatial context or immersion, or if it introduces conflicts in the semantic extraction process.
- What evidence would resolve it: A comparative user study evaluating scene quality and immersion between the baseline text-to-3D pipeline and a multimodal-input pipeline.

### Open Question 3
- Question: What is the specific impact of AI-driven role-playing in VR on a user's capacity for identity exploration and perspective-taking compared to traditional physical LARP?
- Basis in paper: [explicit] Section 4 calls for "user experience evaluations in real-world environments" to understand the impact on "imagination, identity exploration, and narrative engagement."
- Why unresolved: While the system architecture is described, there is currently no empirical data or user study validating the psychological or experiential efficacy of the system.
- What evidence would resolve it: Quantitative and qualitative data from user trials measuring presence, emotional engagement, and empathy using standardized scales (e.g., Immersive Experience Questionnaire).

## Limitations

- The paper lacks empirical validation of core claims, including semantic extraction accuracy, long-term character coherence, and VR immersion quality.
- Long-term narrative coherence and character consistency over extended interactions remain unresolved challenges.
- The system's practical utility and user experience have not been validated through user studies or quantitative evaluation.

## Confidence

- **High Confidence**: The architectural feasibility of integrating GPT-4o, SynCity, and Unity for LARP generation is plausible based on existing precedents in each component domain.
- **Medium Confidence**: The claim that structured semantic extraction from natural language can support consistent downstream generation is supported by LLM capabilities but not empirically validated for LARP contexts.
- **Low Confidence**: The assertion that characters with persistent internal states will produce coherent multi-turn interactions over extended sessions is theoretically grounded but lacks direct evidence.

## Next Checks

1. **Semantic Extraction Accuracy**: Input varied story descriptions (complete, vague, contradictory) and measure extraction precision/recall against human-labeled ground truth for key narrative elements (spatial context, character motivations, conflict sources).

2. **Character Coherence Stress Test**: Run 50+ turn dialogues with AI characters; measure consistency of stated beliefs, recalled events, and relationship states across time to identify memory/state persistence limits.

3. **End-to-End Latency Profiling**: Instrument the full pipeline (GPT-4o → SynCity → Unity → VR) to identify bottlenecks; target sub-second response for interaction turns to maintain VR presence and measure impact on user immersion.