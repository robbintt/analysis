---
ver: rpa2
title: Improving Controller Generalization with Dimensionless Markov Decision Processes
arxiv_id: '2504.10006'
source_url: https://arxiv.org/abs/2504.10006
tags:
- context
- which
- dimensionless
- controller
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper tackles the poor generalization of reinforcement\u2011\
  learning controllers when the underlying dynamics change (e.g., due to wear, varying\
  \ mass or length). It proposes the Dimensionless Markov Decision Process (\u03A0\
  \u2011MDP), which applies a context\u2011dependent, physics\u2011based non\u2011\
  dimensionalisation (via the Buckingham\u2011\u03A0 theorem) to both state and action\
  \ spaces, then trains a model\u2011based policy (PILCO with Gaussian\u2011Process\
  \ dynamics) entirely in this dimensionless space."
---

# Improving Controller Generalization with Dimensionless Markov Decision Processes  

## Quick Facts  
- **arXiv ID:** 2504.10006  
- **Source URL:** https://arxiv.org/abs/2504.10006  
- **Reference count:** 13  
- **Primary result:** A single policy trained in a dimension‑less space attains ≥90 % zero‑shot success when pole length, mass, or gravity are varied by ±30 % without any additional data.  

## Executive Summary  
Reinforcement‑learning controllers often crumble when the underlying plant dynamics shift (e.g., wear, payload changes). The authors introduce the **Dimensionless Markov Decision Process (Π‑MDP)**, which applies a physics‑based, context‑dependent non‑dimensionalisation (via the Buckingham‑Π theorem) to both states and actions. By training a model‑based policy (PILCO with Gaussian‑Process dynamics) entirely in this transformed space, the resulting controller becomes invariant to observable context variables. Experiments on under‑actuated pendulum and cart‑pole simulators demonstrate that a policy learned on a single nominal environment transfers zero‑shot to environments with up to ±30 % variations in length, mass, or gravity, achieving near‑optimal returns and success rates often ≈100 %.  

## Method Summary  
The approach first identifies a set of observable context variables (e.g., mass m, length ℓ, gravity g). Using the Buckingham‑Π theorem, it constructs dimensionless groups that map the original state‑action pair \((s,a)\) to a reduced, context‑free representation \((\tilde s,\tilde a)\). A Gaussian‑Process (GP) dynamics model is then learned in this space, and PILCO performs policy optimisation by propagating uncertainty through the GP rollouts. Because the transformation is equivariant, the learned policy \(\pi(\tilde s)\) can be applied to any environment sharing the same observable contexts, simply by re‑applying the inverse non‑dimensionalisation at execution time.  

## Key Results  
- Near‑optimal cumulative returns on nominal pendulum and cart‑pole tasks.  
- Zero‑shot transfer success ≥90 % (often ≈100 %) when pole length, mass, or gravity are shifted by ±30 % without extra data collection.  
- The dimensionless policy outperforms a baseline policy trained directly on raw states, which degrades sharply under the same perturbations.  

## Why This Works (Mechanism)  
1. **Equivariant non‑dimensionalisation** – By expressing dynamics in terms of dimensionless Π groups, the state‑action representation removes explicit dependence on observable physical parameters, making the MDP invariant to those contexts.  
2. **Accurate GP dynamics in reduced space** – The GP model operates on a lower‑dimensional, smoother manifold, improving sample efficiency and predictive fidelity across varied dynamics.  
3. **Model‑based policy optimisation (PILCO)** – Uncertainty‑aware rollouts allow the policy to anticipate the effect of dynamics changes, leading to robust control actions that generalise without retraining.  

## Foundational Learning  
| Concept | Why needed | Quick‑check question |
|---|---|---|
| Buckingham‑Π theorem | Provides the systematic way to construct dimensionless groups from physical variables. | Can you derive a dimensionless state for a simple pendulum using \(m, \ell, g\)? |
| Gaussian‑Process dynamics modeling | Captures smooth, uncertainty‑aware transition functions in low‑dimensional space. | Does the GP predict higher variance when queried outside the training region? |
| PILCO policy optimisation | Leverages GP rollouts to compute gradients of expected return under model uncertainty. | Does the policy improve return after each gradient step in a simulated rollout? |
| Equivariance / invariance | Guarantees that the same policy works across environments differing only in observable contexts. | If you scale all lengths by a factor, does the transformed state remain unchanged? |
| Dimensionless state‑action representation | Reduces the effective dimensionality, aiding both learning and generalisation. | How many dimensions does the transformed state have compared to the raw state? |

## Architecture Onboarding  
**Component map**  
Physical parameters (mass, length, gravity) → Non‑dimensionalisation (Π‑MDP) → GP dynamics model (PILCO) → Policy network (Gaussian‑process policy) → Action execution (inverse Π‑transform)  

**Critical path**  
1. Compute Π groups from observed contexts.  
2. Train GP dynamics on transformed trajectories.  
3. Optimise policy via PILCO using GP rollouts.  
4. Deploy policy by mapping real‑world states → Π space → policy → inverse map → actuator commands.  

**Design trade‑offs**  
- **GP scalability vs. model fidelity** – GPs excel in low‑dimensional regimes but scale poorly with data; limiting the state dimension via Π‑groups mitigates this but may restrict applicability to high‑DoF systems.  
- **Observability requirement** – The method assumes all context variables are measurable; unobservable changes break invariance.  
- **Simulation‑only validation** – No real‑world hardware tests, so sensor noise and latency are not accounted for.  

**Failure signatures**  
- Sudden drop in success rate when an unobserved parameter (e.g., friction) changes.  
- GP predictive variance spikes during rollout, leading to unstable policy gradients.  
- Policy divergence during optimisation (loss not decreasing) when the Π transformation is omitted.  

**First 3 experiments**  
1. Replicate the pendulum experiment: train a Π‑MDP policy on a nominal length and test zero‑shot success across ±30 % length variations.  
2. Replicate the cart‑pole experiment: train on nominal mass & gravity, evaluate on shifted mass and gravity values.  
3. Ablation study: train an identical PILCO policy on raw (dimensional) states and compare robustness to the Π‑MDP baseline.  

## Open Questions the Paper Calls Out  
The manuscript does not explicitly enumerate open research questions. Potential directions inferred from the discussion include:  

- How does the Π‑MDP approach scale to high‑DoF robots where the number of observable physical parameters grows?  
- Can the method be extended to handle unobservable or partially observable context variables?  
- What are the real‑world performance implications when sensor noise, latency, and unmodelled friction are present?  
- Are there alternative model‑based learners (e.g., neural‑network dynamics) that retain the benefits of dimensionless representations?  

## Limitations  
- The exact non‑dimensionalisation formulas and context variable sets are not fully disclosed, requiring independent derivation.  
- Gaussian‑Process dynamics and PILCO optimisation are computationally intensive beyond a few dozen dimensions, limiting scalability.  
- All validation is performed in simulation; no hardware experiments are provided to assess robustness to real‑world imperfections.  

## Confidence  
- **Equivariance of the Π‑MDP transformation → High**  
- **Zero‑shot transfer performance on pendulum and cart‑pole (≥90 % success) → Medium**  
- **Broad applicability to arbitrary under‑actuated systems and robustness to unseen context changes → Low**  

## Next Checks  
1. **Analytical verification** – Derive the dimensionless state‑action mapping for a simple pendulum and prove mathematically that the transformed dynamics are invariant to length, mass, and gravity.  
2. **Empirical replication** – Implement the Π‑MDP + PILCO pipeline (including GP kernel and rollout horizon) and reproduce the pendulum zero‑shot transfer results; compare success‑rate curves and return statistics to the reported ≥90 % figure.  
3. **Scalability test** – Apply the same pipeline to a 6‑DoF planar robot arm with observable link masses and lengths; measure GP training time, policy optimisation convergence, and zero‑shot success relative to the low‑dimensional benchmarks.