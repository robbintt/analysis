---
ver: rpa2
title: Independent Learning in Performative Markov Potential Games
arxiv_id: '2504.20593'
source_url: https://arxiv.org/abs/2504.20593
tags:
- performative
- policy
- lemma
- learning
- inpg
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies multi-agent reinforcement learning under performative
  effects, where agents' policies influence the environment's dynamics. The authors
  extend Markov Potential Games to incorporate performative effects and introduce
  the concept of performatively stable equilibrium (PSE).
---

# Independent Learning in Performative Markov Potential Games

## Quick Facts
- arXiv ID: 2504.20593
- Source URL: https://arxiv.org/abs/2504.20593
- Reference count: 40
- Primary result: Independent learning algorithms converge to performatively stable equilibria in Markov Potential Games with performative effects.

## Executive Summary
This paper extends Markov Potential Games (MPGs) to account for performative effects, where agents' policies influence the environment's dynamics. The authors introduce the concept of performatively stable equilibrium (PSE) and prove its existence under sensitivity assumptions. They provide convergence guarantees for independent policy gradient methods (IPGA and INPG) to approximate PSE, demonstrating through experiments that natural policy gradient methods are more robust against performative effects than standard policy gradient methods.

## Method Summary
The method involves independent agents learning policies in a performative MPG setting where the environment's transition probabilities and rewards depend on the current policy. Agents use either Independent Projected Gradient Ascent (IPGA) or Independent Natural Policy Gradient (INPG) to update their policies based on sampled trajectories. The algorithms aim to find a PSE where each agent's policy is optimal for the environment induced by the joint policy. Implementation requires defining the performative environment model, sampling trajectories, estimating Q-values or advantages, and applying the appropriate update rule with careful consideration of learning rates and regularization.

## Key Results
- PSE always exists in performative MPGs under reasonable sensitivity assumptions.
- IPGA converges to an approximate PSE in the best-iterate sense with O(1/ε²) iteration complexity.
- INPG achieves asymptotic last-iterate convergence and is more robust to performative effects.
- Experiments on safe-distancing and stochastic congestion games validate theoretical findings.

## Why This Works (Mechanism)

### Mechanism 1: Existence of Performatively Stable Equilibrium (PSE)
- **Claim:** A fixed-point equilibrium exists where agents' policies are optimal for the environment induced by those same policies, provided the environment's reaction is sufficiently smooth.
- **Mechanism:** The framework extends Markov Potential Games (MPGs) to include performative effects. By defining a potential function Φ that is continuous in both the "playing" policy and the "deployed" policy, the authors utilize the Kakutani fixed-point theorem. This maps the best-response correspondence back onto itself, proving that a PSE exists without requiring agents to know the global structure.
- **Core assumption:** Assumption 1 (Sensitivity): Rewards and transitions must change Lipschitz-continuously with policy changes (ωᵣ, ωₚ).
- **Evidence anchors:**
  - [Abstract] "...show that PSE always exists under a reasonable sensitivity assumption."
  - [Section 4] "Theorem 1. Under Assumption 1, every performative MPG admits a PSE."
  - [Corpus] Corpus neighbors confirm the broader context of performative stability but note that direct mathematical precedents for *existence in MPGs* are weak in this specific neighbor set.
- **Break condition:** If the environment is highly sensitive (large ω) or discontinuous, the fixed-point mapping fails, and no stable equilibrium may exist.

### Mechanism 2: Independent Policy Gradient Ascent (IPGA) Convergence
- **Claim:** Independent projected gradient ascent converges to an approximate PSE in the best-iterate sense, meaning the algorithm guarantees at least one policy during training was near-optimal.
- **Mechanism:** Agents independently update their policies using projected gradient ascent on Q-values. The proof leverages the potential function property: individual policy improvements increase the global potential. However, the "performative drift" (the environment changing every step) introduces an error term. The algorithm balances the gradient step size η to make progress while not destabilizing the shifting environment.
- **Core assumption:** Access to a gradient oracle or sufficient samples to estimate Q-values; bounded potential function difference CΦ.
- **Evidence anchors:**
  - [Abstract] "...independent policy gradient ascent (IPGA)... converge to an approximate PSE in the best-iterate sense..."
  - [Section 5.1.1] Theorem 3 establishes the iteration complexity O(1/ε²) plus a term dependent on performative sensitivity.
  - [Corpus] *Solving Zero-Sum Convex Markov Games* (Neighbor ID 39063) discusses independent policy gradient in related game structures, supporting the feasibility of independent learners.
- **Break condition:** If the learning rate is too high relative to the performative sensitivity, the policy changes may overshoot the stable region, causing divergence.

### Mechanism 3: Natural Policy Gradient (INPG) Robustness
- **Claim:** Independent Natural Policy Gradient (INPG) is more robust to performative effects and achieves asymptotic last-iterate convergence (the final policy is stable).
- **Mechanism:** INPG uses a multiplicative update rule (softmax parameterization) rather than Euclidean projection. This geometry is better suited for probability simplexes. The "natural" update direction implicitly accounts for policy curvature, allowing it to dampen the oscillations caused by the environment shifting underfoot. With log-barrier regularization, it avoids policy collapse (probability of optimal action → 0).
- **Core assumption:** Assumption 3 (Isolated stationary points) and Assumption 2 (State distribution coverage).
- **Evidence anchors:**
  - [Abstract] "...natural policy gradient methods are more robust against these effects."
  - [Section 5.1.2] Theorem 4/5 shows INPG convergence bounds and asymptotic last-iterate convergence.
  - [Corpus] *Achieving Logarithmic Regret in KL-Regularized Zero-Sum Markov Games* (Neighbor ID 65951) supports the hypothesis that entropy/KL regularization stabilizes multi-agent dynamics.
- **Break condition:** Poor initialization causing c → 0 (low probability of optimal actions) without sufficient regularization λ, stalling the multiplicative updates.

## Foundational Learning

- **Concept:** **Markov Potential Games (MPGs)**
  - **Why needed here:** This is the structural substrate. You must understand that unlike general-sum games where agents conflict, MPGs possess a global potential function Φ. This allows independent learners to converge because optimizing local value Vᵢ implicitly optimizes the global Φ.
  - **Quick check question:** Can you explain why independent learning fails in a general zero-sum game but works in an MPG?

- **Concept:** **Performative Feedback Loops**
  - **Why needed here:** This is the core distortion. In standard RL, the environment is stationary. Here, the transition matrix P and reward r are functions of the current policy π. You need to distinguish between the *sampling* noise and the *structural* shift of the environment.
  - **Quick check question:** If you deploy a policy πₜ and collect data, why is that data biased for evaluating the *next* policy πₜ₊₁?

- **Concept:** **Policy Gradient vs. Natural Policy Gradient**
  - **Why needed here:** The paper compares IPGA (Projected Gradient) vs. INPG (Natural Gradient). You need to distinguish between updating parameters in Euclidean space (IPGA) vs. updating the policy distribution in a Riemannian manifold defined by the Fisher Information Matrix (INPG).
  - **Quick check question:** Why does the "natural" update tend to prevent probabilities from collapsing to zero compared to a standard gradient clip?

## Architecture Onboarding

- **Component map:**
  Agent Workers -> Performative Environment Wrapper -> Sampler -> Optimizer

- **Critical path:**
  1. **Initialization:** Uniform random policies.
  2. **Deployment Step:** Joint policy πₜ is deployed. The Environment Wrapper applies the performative shift (e.g., modifies P based on πₜ).
  3. **Sampling:** Collect K trajectories from the *shifted* environment.
  4. **Update:** Agents calculate Q-values and apply the specific update rule (Eq. 4 for IPGA or Eq. 5 for INPG).
  5. **Repeat:** T rounds.

- **Design tradeoffs:**
  - **IPGA vs. INPG:** IPGA is easier to implement (simple projection) but theoretically weaker (best-iterate only). INPG requires careful normalization and regularization λ but offers last-iterate guarantees.
  - **Oracle vs. Sample:** Theoretical proofs assume exact gradients (Oracle). Practical implementation (Section 5.2) uses regression estimates, which introduces variance δₛₜₐₜ.

- **Failure signatures:**
  - **Rotational Drift:** Policy distance metric (Fig 1/2) oscillates rather than decaying. This implies learning rate η is too high for the performative sensitivity ω.
  - **Policy Collapse:** Probabilities for sub-optimal actions go to exactly zero. This breaks the exploration assumption (Assumption 2).
  - **Stale Gradients:** If the environment shifts faster than the sampling phase, gradients become uncorrelated with the true landscape.

- **First 3 experiments:**
  1. **Sanity Check (No Performative Effect):** Set ω = 0. Verify IPGA and INPG recover standard MPG convergence on the Safe-Distancing game.
  2. **Sensitivity Sweep:** Run IPGA on Stochastic Congestion Game. Sweep ω ∈ {0.01, 0.03, 0.1}. Observe if the policy distance (Eq. 10) settles or oscillates as shown in Fig 1.
  3. **Algorithm Comparison:** Fix ω = 0.1. Run IPGA-D, IPGA-L, and INPG (reg). Plot policy distance vs. steps to confirm INPG's superior robustness (Fig 2).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can finite-time last-iterate convergence guarantees be established for independent policy gradient methods in general performative Markov Potential Games?
- **Basis in paper:** [explicit] The conclusion states, "It remains an open question whether our finite-time last-iterate convergence can be extended to the general case."
- **Why unresolved:** The paper currently provides finite-time last-iterate convergence (Theorem 7) only for a special case with agent-independent transitions. For general MPGs, they only offer asymptotic last-iterate convergence (Theorem 4) or best-iterate convergence (Theorems 3 and 5).
- **What evidence would resolve it:** A theoretical proof showing that independent policy gradient algorithms converge to a Performatively Stable Equilibrium (PSE) with a specific iteration complexity bound in general MPGs without requiring agent-independent transitions.

### Open Question 2
- **Question:** Can the convergence of Independent Natural Policy Gradient (INPG) be guaranteed in the sample-based setting without exact gradient oracles?
- **Basis in paper:** [explicit] Section 5.2 notes, "Extending the results of the INPG algorithm is not straightforward, even in standard MPG; we leave that for future work."
- **Why unresolved:** The paper provides a sample-based guarantee for IPGA (Theorem 6) but restricts INPG analysis to the infinite-sample (oracle) setting.
- **What evidence would resolve it:** Deriving a sample complexity bound for INPG that accounts for the variance in estimating Q-values and advantage functions from finite trajectories.

### Open Question 3
- **Question:** Do weaker equilibrium concepts facilitate finite-time last-iterate convergence in the general performative setting?
- **Basis in paper:** [explicit] The conclusion suggests, "For that [finite-time convergence in the general case], an interesting avenue may be to study weaker equilibria concepts."
- **Why unresolved:** The authors suggest this as a potential method to overcome the difficulty of proving convergence to exact PSE or Nash Equilibria.
- **What evidence would resolve it:** Defining a relaxed equilibrium concept (e.g., a specific type of approximate equilibrium) and proving that independent learning dynamics converge to it in finite time in general performative MPGs.

## Limitations
- The performative effect models are hand-crafted and may not capture real-world dynamics, particularly the complete logic of the "influencer" agent in the safe-distancing game.
- Theoretical bounds depend heavily on sensitivity assumptions that may not hold in practical scenarios with discontinuous or chaotic environment responses.
- Results assume exact gradient oracles for theoretical analysis, though sample-based implementations are discussed.

## Confidence

- **High Confidence:** Existence of PSE under sensitivity assumptions (Theorem 1) - this follows directly from Kakutani's theorem applied to the well-defined potential function.
- **Medium Confidence:** Convergence of IPGA to approximate PSE in best-iterate sense (Theorem 3) - the proof is sound but assumes exact gradient oracles, which are rarely available in practice.
- **Medium Confidence:** INPG robustness and last-iterate convergence (Theorems 4-5) - the analysis is rigorous but the practical impact depends heavily on the regularization parameter λ and initialization.

## Next Checks

1. Implement the safe-distancing game with complete influencer agent logic and verify convergence patterns across the full range of α values.
2. Conduct ablation studies on the sensitivity parameter ω in the congestion game to test the theoretical bounds on convergence rates.
3. Test algorithm performance on a third, qualitatively different performative environment (e.g., traffic routing or supply chain) to assess generalizability.