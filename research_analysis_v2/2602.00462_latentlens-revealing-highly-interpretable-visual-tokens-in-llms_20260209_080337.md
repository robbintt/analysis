---
ver: rpa2
title: 'LatentLens: Revealing Highly Interpretable Visual Tokens in LLMs'
arxiv_id: '2602.00462'
source_url: https://arxiv.org/abs/2602.00462
tags:
- visual
- tokens
- token
- layer
- mean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LatentLens, a novel interpretability method
  for analyzing latent representations in vision-language models (VLMs). The core
  insight is that the most natural comparison for visual token representations are
  contextualized textual representations rather than the LLM embedding or unembedding
  matrix.
---

# LatentLens: Revealing Highly Interpretable Visual Tokens in LLMs

## Quick Facts
- **arXiv ID:** 2602.00462
- **Source URL:** https://arxiv.org/abs/2602.00462
- **Reference count:** 40
- **Primary result:** LatentLens achieves 72% interpretable visual tokens vs 23% for LogitLens

## Executive Summary
LatentLens is a novel interpretability method that reveals highly interpretable visual tokens in vision-language models by comparing visual token representations to contextualized textual embeddings rather than static vocabulary entries. The method pre-computes a database of contextualized text representations from a descriptive corpus and retrieves the top-k nearest neighbors for each visual token. This approach substantially outperforms existing methods like LogitLens and EmbeddingLens, with the majority of visual tokens being interpretable across all studied models and layers. The findings suggest that visual tokens are processed by LLMs in a semantically meaningful way, aligning more naturally with mid-layer text representations rather than input or output layers.

## Method Summary
LatentLens encodes a large text corpus and stores contextualized token representations for each token at multiple layers. Visual token representations are then compared to these contextualized representations using cosine similarity, with the top-k nearest neighbors providing interpretable descriptions. The method trains a 3-layer MLP projector to map vision encoder outputs to the frozen LLM embedding space, using PixMo-Cap image-caption pairs for training. Visual tokens are extracted from images processed through a vision encoder (ViT/DINO) and projector, then matched against the pre-computed contextual embedding database. The approach uses float8 compression to store approximately 2.5M embeddings per layer, balancing storage efficiency with retrieval accuracy.

## Key Results
- LatentLens achieves 72% interpretable visual tokens on average versus 23% for LogitLens and 30% for EmbeddingLens
- Visual tokens at input and early layers align most strongly with contextualized representations from mid-layers (layers 8-16)
- The method produces semantically meaningful, fine-grained sentence-level descriptions rather than individual subword tokens

## Why This Works (Mechanism)

### Mechanism 1: Contextual Alignment vs. Static Vocabulary Mapping
Visual tokens processed by LLMs are more semantically similar to contextualized text representations (intermediate hidden states) than to static vocabulary entries. Standard methods force comparison against input/output layers capturing lexical properties, while LatentLens compares against intermediate layer activations from a descriptive corpus, accounting for the projection layer mapping visual features into semantic LLM subspace.

### Mechanism 2: The "Mid-Layer Leap"
Visual tokens enter the LLM at a representational maturity equivalent to mid-layer text processing. The projector and vision encoder output highly abstracted tokens that bypass early LLM layers typically used for basic syntactic processing, immediately residing in a state comparable to text tokens contextualized by 8-16 layers of attention.

### Mechanism 3: Sentence-Level Semantic Resolution
Visual patches encode sufficient information to match full phrases, overcoming ambiguity of subword tokens. By indexing descriptions from Visual Genome, LatentLens retrieves specific context surrounding neighbor tokens, matching visual tokens to contextualized embeddings of complete phrases rather than individual words.

## Foundational Learning

- **Concept: Contextualized Embeddings**
  - **Why needed here:** LatentLens relies on the fact that a token's vector changes as it moves through transformer layers. Understanding that the vector for "apple" at layer 0 differs from layer 16 is crucial to grasp why LatentLens queries the latter.
  - **Quick check question:** Does the method query the static embedding matrix $E$ or the hidden states $h^{(l)}_t$ generated during a forward pass?

- **Concept: LogitLens vs. EmbeddingLens**
  - **Why needed here:** To situate the contribution, you must distinguish between projecting latent states to vocabulary via unembedding matrix (LogitLens) versus comparing against input embedding matrix (EmbeddingLens).
  - **Quick check question:** Which baseline method produces "next-token predictions" as interpretations, and which produces "nearest vocabulary neighbors"?

- **Concept: Vision-Language Projectors (Connectors)**
  - **Why needed here:** The interpretability hinges on how the MLP/Linear projector transforms visual features into the LLM's dimension.
  - **Quick check question:** If the projector is a simple linear layer, does the "Mid-Layer Leap" phenomenon disappear?

## Architecture Onboarding

- **Component map:** Image → Vision Encoder (ViT/DINO) → Projector (MLP) → Visual Tokens → LLM Layers → Retrieval → Database of Contextual Embeddings → Top-K Descriptions
- **Critical path:** The storage and indexing of contextual embeddings (approx 2.5M embeddings per layer). Without pre-computed "semantic memory," the method degrades to slow brute-force search.
- **Design tradeoffs:**
  - Storage vs. Granularity: Uses float8 compression to store millions of vectors (25% of fp32 size)
  - Corpus Domain: Visual Genome (dense captions) works for object-level interpretation; generic web corpus might yield lower relevance
- **Failure signatures:**
  - Low Norm Mismatch: Visual tokens often have much higher L2 norms than text tokens (can exceed 100,000 for LLaMA3/Qwen2)
  - Global vs. Concrete: Sometimes retrieves "Global" descriptions (objects elsewhere in image) rather than specific patch content
- **First 3 experiments:**
  1. Layer Alignment Ablation: Run LatentLens comparing visual layer 0 against text layers 0 through N to verify peak similarity at mid-layers
  2. Corpus Swap: Test retrieval using generic corpus (Wikipedia) vs. descriptive Visual Genome corpus
  3. Projector Complexity: Train Linear vs. MLP projector and measure change in interpretability scores

## Open Questions the Paper Calls Out

**Open Question 1:** Do interpretable visual tokens causally influence downstream task performance, such as reducing hallucinations? The authors suggest causal ablations investigating how interpretable vs. non-interpretable visual tokens affect task performance as a future direction.

**Open Question 2:** Can LatentLens effectively decode non-visual continuous inputs, such as soft prompts, speech embeddings, or latent reasoning chains? The authors state the method may extend beyond VLMs to other non-linguistic tokens.

**Open Question 3:** Why do visual tokens at the input layer align most strongly with text representations from mid-layers (the "Mid-Layer Leap") rather than the input layer? The specific training dynamics or geometric reasons forcing this cross-layer alignment are not fully isolated.

**Open Question 4:** Does a dynamic, on-the-fly corpus improve interpretability over the static Visual Genome corpus used in the study? The authors mention the method could be refined as a tool with a dynamic corpus and acknowledge limitations regarding the static corpus's noun bias.

## Limitations
- The method's effectiveness depends on training dynamics of the projector and LLM, with the "Mid-Layer Leap" diminishing when the LLM is fine-tuned
- Relies on a pre-computed corpus of contextualized embeddings that may lack relevant semantic context for divergent visual domains
- Primary interpretability metric depends on GPT-4o's VLM-judge, a black-box model whose understanding of "interpretability" may not align with human perception

## Confidence

**High Confidence:**
- LatentLens Outperforms Baselines: 72% interpretable visual tokens vs 23% for LogitLens is directly supported by quantitative results
- Semantic Resolution: Qualitative examples showing fine-grained descriptions versus unrelated subwords are compelling and consistent

**Medium Confidence:**
- Mid-Layer Leap Mechanism: Observation is well-supported by Figure 4, but explanation (vision encoder skips early contextualization) is inferred
- Projector Non-Linearity: Claim that MLP projector is necessary is plausible but not rigorously tested

**Low Confidence:**
- Universal Applicability: Claims demonstrated on narrow set of models (LLaMA3, Qwen2-VL) and single corpus (Visual Genome)
- VLM-Judge Objectivity: Judge verdicts not validated against human annotations

## Next Checks

1. **Corpus Domain Ablation:** Reproduce LatentLens using generic corpus (Wikipedia) versus Visual Genome. Measure drop in interpretability scores to quantify importance of domain-specific context.

2. **Projector Architecture Ablation:** Train Linear projector instead of MLP and measure change in interpretability and layer alignment to test whether "Mid-Layer Leap" requires non-linear transformation.

3. **VLM-Judge Human Validation:** Collect human annotations for subset of visual tokens (e.g., 100 random samples) and compare human interpretability rates to GPT-4o's verdicts. Report inter-annotator agreement and judge sensitivity to prompt variations.