---
ver: rpa2
title: 'CrowdAgent: Multi-Agent Managed Multi-Source Annotation System'
arxiv_id: '2509.14030'
source_url: https://arxiv.org/abs/2509.14030
tags:
- annotation
- human
- system
- annotator
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CrowdAgent introduces a multi-agent system for end-to-end process
  control in data annotation, integrating LLM, SLM, and human annotators with dynamic
  task assignment, quality assurance, and cost management. The system iteratively
  dispatches samples to the most suitable annotator based on real-time confidence,
  performance history, and budget analysis, leveraging Bayesian label aggregation
  and active learning.
---

# CrowdAgent: Multi-Agent Managed Multi-Source Annotation System

## Quick Facts
- **arXiv ID:** 2509.14030
- **Source URL:** https://arxiv.org/abs/2509.14030
- **Reference count:** 40
- **Primary result:** Multi-agent system integrating LLM, SLM, and human annotators achieves up to 5.47% higher accuracy than baselines while reducing costs through dynamic task assignment and Bayesian label aggregation.

## Executive Summary
CrowdAgent introduces a multi-agent system for end-to-end process control in data annotation, integrating LLM, SLM, and human annotators with dynamic task assignment, quality assurance, and cost management. The system iteratively dispatches samples to the most suitable annotator based on real-time confidence, performance history, and budget analysis, leveraging Bayesian label aggregation and active learning. Evaluated on six multimodal classification tasks, CrowdAgent consistently outperforms baselines like GPT-4o mini, FreeAL, and CoAnnotating, achieving up to 5.47% higher accuracy while significantly reducing costs through intelligent resource allocation. The modular, user-friendly system provides transparent workflow monitoring and supports scalable, high-quality dataset creation under budget constraints.

## Method Summary
CrowdAgent employs a multi-agent workflow for multimodal classification tasks, where LLM annotators provide initial labels and generate in-context learning demonstrations, SLM annotators denoise LLM labels using GMM-based clean sample selection, and human annotators are selectively invoked for low-confidence, diverse samples. The QA Agent performs Bayesian label aggregation using confusion matrices estimated from a golden set, computing confidence scores to determine sample convergence. The Scheduling Agent dynamically routes unconverged samples to the most suitable annotator based on cost-performance ratios and annotator profiles, while the Financing Agent tracks and optimizes resource usage. The system iterates until all samples converge (confidence ≥0.99) or the budget is exhausted, with human involvement limited to ~5-15% of samples.

## Key Results
- Outperforms GPT-4o mini, FreeAL, and CoAnnotating baselines by up to 5.47% accuracy on multimodal classification tasks.
- Achieves significant cost savings by intelligently routing samples to lower-cost annotators (LLM, SLM) for most rounds, reserving humans for low-confidence samples.
- Demonstrates consistent accuracy improvements across six datasets (CrisisMMD, MM-IMDb, COV-CTR, V-SNLI) with varying task types and modalities.

## Why This Works (Mechanism)

### Mechanism 1: Confidence-Gated Iterative Label Refinement
Iteratively assigning unconverged samples to progressively more capable annotators improves final accuracy while constraining cost. The QA Agent computes posterior probabilities via Bayesian label aggregation; samples below the confidence threshold (default 0.99) are re-dispatched. The Scheduling Agent selects the next annotator based on historical performance, cost-profile, and sample difficulty signals. Core assumption: Annotator errors are partially uncorrelated across sources (LLM, SLM, human), enabling aggregation to reduce noise. Evidence: Bayesian aggregation and round-by-round accuracy improvements shown in abstract and Table 2. Break condition: If annotator errors are highly correlated, aggregation provides limited gains and costs increase without quality improvement.

### Mechanism 2: SLM Denoising via Loss-Based Sample Partitioning
Training SLMs on LLM-generated labels with GMM-based clean/noisy partitioning distills usable signal from noisy annotations. SLMs are initially trained on all LLM labels; per-sample losses are fit to a two-component Gaussian Mixture Model; the component with lower mean loss is treated as clean. The SLM retrains on the clean subset. Core assumption: Deep networks memorize clean patterns before overfitting to label noise (the memorization effect). Evidence: GMM-based clean sample selection described in Section 2.1 and Section C.1. Break condition: If initial LLM labels are too noisy (>~40-50% error rate), GMM separation degrades and SLM may amplify errors rather than correct them.

### Mechanism 3: Human-in-the-Loop via Uncertainty + Diversity Sampling
Selecting human-annotated samples via low-confidence filtering followed by Core-Set diversity selection maximizes human impact per cost unit. From unconverged samples, select the lowest-confidence 10% as candidates, then apply Core-Set on SLM embeddings to choose a diverse 5% subset for human labeling. Core assumption: Low-confidence samples are informative for model improvement; diversity prevents redundant annotation of similar examples. Evidence: Combined uncertainty and diversity sampling strategy in Section 2.4 and Figure 4. Break condition: If confidence estimates are miscalibrated (overconfident on errors), low-confidence sampling may miss critical samples.

## Foundational Learning

- **Concept: Bayesian Truth Inference / Dawid-Skene Models**
  - **Why needed here:** The QA Agent uses confusion-matrix-based Bayesian updates to aggregate multi-source labels; understanding prior/posterior updates and EM estimation is essential.
  - **Quick check question:** Given three annotators with known confusion matrices who provide labels [A, B, A] for a sample, how would you compute the posterior probability that the true label is A?

- **Concept: Noisy-Label Learning (e.g., DivideMix, GMM loss separation)**
  - **Why needed here:** SLM Annotators rely on the memorization effect and loss-based sample partitioning to learn from LLM-generated noisy labels.
  - **Quick check question:** Why does training on the low-loss subset improve generalization when labels contain systematic noise?

- **Concept: Active Learning Query Strategies (Uncertainty + Core-Set)**
  - **Why needed here:** Human sample selection combines uncertainty sampling (lowest confidence) with diversity sampling (Core-Set) to optimize annotation budget.
  - **Quick check question:** What is the trade-off between uncertainty-based vs. diversity-based sample selection in early vs. late annotation rounds?

## Architecture Onboarding

- **Component map:**
  - Annotation Agents: LLM (multi-prompt ensemble), SLM (RoBERTa/ConvNeXt/MMBT), Human (Youling platform integration)
  - QA Agent: Label aggregation (Bayesian inference), confusion matrix estimation, annotator profiling, guideline generation
  - Financing Agent: Cost tracking (per-token/per-hour/per-sample), cost-performance ratio analysis
  - Scheduling Agent: Task dispatch, annotator selection, termination condition monitoring
  - Shared Message Pool: Structured file exchange for inter-agent communication

- **Critical path:**
  1. User submits unlabeled dataset + golden set + budget
  2. Round 1: LLM generates initial labels + synthetic ICL examples
  3. Round 2+: SLM trained on LLM labels via GMM-clean selection
  4. QA Agent aggregates via Bayesian inference, computes confidence
  5. Unconverged samples → Scheduling Agent selects next annotator
  6. Human invoked for lowest-confidence diverse subset (every ~5 rounds)
  7. Repeat until budget exhausted or all samples converged

- **Design tradeoffs:**
  - **Confidence threshold (0.99 default):** Higher = more rounds, higher accuracy, more cost; lower = early termination, risk of unlabeled errors
  - **Human batch size (5% default):** Larger = faster convergence, higher cost per round
  - **LLM prompt diversity:** More prompts reduce ensemble variance but increase API costs
  - **Golden set size:** Larger set improves confusion matrix estimation but reduces unlabeled pool

- **Failure signatures:**
  - Accuracy plateaus before convergence → annotator errors highly correlated; consider adding orthogonal annotators
  - Confidence scores high but accuracy low → miscalibrated confusion matrices; re-estimate on expanded golden set
  - Cost exceeds budget early → Scheduling Agent over-selecting expensive annotators; review financing feedback loop
  - SLM accuracy degrades across rounds → GMM partition failing; inspect loss distribution

- **First 3 experiments:**
  1. **Ablation on annotator sources:** Run with (LLM only), (LLM + SLM), (LLM + SLM + Human) to isolate contribution of each source on a held-out task.
  2. **Confidence threshold sweep:** Test thresholds [0.90, 0.95, 0.99, 0.999] on V-SNLI to plot accuracy vs. cost trade-off curve.
  3. **Golden set size sensitivity:** Vary golden set from 50 to 500 samples to measure impact on confusion matrix calibration and final accuracy.

## Open Questions the Paper Calls Out
- **Question:** How can residual errors in machine annotation be systematically detected and corrected without requiring excessive human intervention?
  - **Basis in paper:** The Limitations section states: "A key question is how to effectively handle the residual errors, which is common to any framework reliant on machine annotation."
  - **Why unresolved:** The current approach routes low-confidence samples to humans, but does not provide a principled method for distinguishing recoverable machine errors from systematically mislabeled data.
  - **What evidence would resolve it:** Experiments comparing automated error-correction mechanisms (e.g., self-consistency, debate-style verification) against human-only review for the residual unconverged subset.

## Limitations
- The scheduling policy for annotator selection is not fully specified, making it difficult to reproduce the exact decision logic.
- No ablation studies report performance with smaller or noisier golden sets, limiting robustness to calibration data scarcity.
- The system is validated only on classification tasks; generalization to structured prediction (NER, QA) requires prompt and aggregation re-engineering.

## Confidence
- **High confidence:** The mechanism of iterative label refinement via Bayesian aggregation and the overall workflow architecture are clearly described and supported by results.
- **Medium confidence:** The SLM denoising approach via GMM-based clean sample selection is theoretically sound but implementation details are not fully specified.
- **Medium confidence:** The human-in-the-loop strategy combining uncertainty and diversity sampling is plausible but Core-Set implementation specifics are unclear.

## Next Checks
1. **Ablation study:** Run CrowdAgent with only LLM annotators, then with LLM + SLM, and finally with all three sources (LLM + SLM + Human) on a held-out dataset to isolate the contribution of each annotator type.
2. **Confidence threshold sweep:** Test CrowdAgent with confidence thresholds of 0.90, 0.95, 0.99, and 0.999 on the V-SNLI dataset to plot the accuracy vs. cost trade-off curve and identify the optimal threshold for cost-constrained settings.
3. **Annotator error correlation analysis:** On the CrisisMMD dataset, compute the pairwise correlation of annotator errors (LLM, SLM, human) to empirically test the assumption that annotator mistakes are partially uncorrelated, which is critical for the effectiveness of label aggregation.