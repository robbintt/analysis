---
ver: rpa2
title: Scalable Complexity Control Facilitates Reasoning Ability of LLMs
arxiv_id: '2505.23013'
source_url: https://arxiv.org/abs/2505.23013
tags:
- complexity
- arxiv
- training
- initialization
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work investigates model complexity control as a fundamental\
  \ approach to enhance the reasoning ability of large language models (LLMs). The\
  \ authors demonstrate that adjusting initialization rate (\u03B3) and weight decay\
  \ coefficient (\u03BB) systematically improves both scaling laws and downstream\
  \ reasoning performance."
---

# Scalable Complexity Control Facilitates Reasoning Ability of LLMs

## Quick Facts
- arXiv ID: 2505.23013
- Source URL: https://arxiv.org/abs/2505.23013
- Authors: Liangkai Hang; Junjie Yao; Zhiwei Bai; Tianyi Chen; Yang Chen; Rongjie Diao; Hezhou Li; Pengxiao Lin; Zhiwei Wang; Cheng Xu; Zhongwang Zhang; Zhangchen Zhou; Zhiyu Li; Zehao Lin; Kai Chen; Feiyu Xiong; Yaoyu Zhang; Weinan E; Hongkang Yang; Zhi-Qin John Xu
- Reference count: 40
- This work investigates model complexity control as a fundamental approach to enhance the reasoning ability of large language models (LLMs). The authors demonstrate that adjusting initialization rate (γ) and weight decay coefficient (λ) systematically improves both scaling laws and downstream reasoning performance. Small-complexity configurations (γ=1, λ=1) yield steeper scaling slopes in test loss versus data and model size, and deliver consistent performance gains across 15 benchmarks. Specifically, 0.9B and 2.4B models trained with small-complexity settings outperform their large-complexity counterparts by +4.6% and +3.4% average score, respectively, with especially large gains on math reasoning tasks (e.g., +19.4% on GSM8K for 0.9B). Mechanistic analysis reveals that small-complexity models exhibit higher embedding cosine similarity and stronger attention matrix condensation, suggesting a shift from memorization to fundamental pattern learning. The authors provide theoretical justification via a complexity-based generalization framework and practical training guidelines, including the use of γ=0.58, λ=1 for 2.4B models to balance performance and stability. Overall, complexity control emerges as a scalable and effective direction for advancing LLM reasoning capabilities.

## Executive Summary
This paper presents a novel approach to enhance reasoning capabilities in LLMs by systematically controlling model complexity during pre-training. Rather than using fixed initialization scales, the authors introduce an initialization rate (γ) that scales with model width, combined with strategic weight decay (λ). Their small-complexity configurations (γ=1, λ=1) demonstrate steeper scaling laws and consistent performance gains across 15 reasoning benchmarks. Mechanistic analysis reveals these models exhibit higher embedding cosine similarity and stronger attention matrix condensation, suggesting a fundamental shift from memorization to pattern learning. The work provides both theoretical justification and practical guidelines for implementing complexity control, with implications for advancing LLM reasoning capabilities.

## Method Summary
The method centers on controlling model complexity through two key hyperparameters: initialization rate (γ) and weight decay coefficient (λ). Instead of using fixed initialization scales, weights are initialized as W ~ N(0, (d_in^{-γ})²) where d_in is the input dimension. The authors train Llama-style transformers with small-complexity configurations (γ=1, λ=1) and compare against large-complexity baselines (γ=0.5, λ=0.1). Training uses AdamW optimizer with cosine learning rate schedule, 5% warmup, and 40B-1T tokens depending on model scale. For stability at 2.4B parameters, they introduce embedding normalization and reduce γ to 0.58. The approach is evaluated across three model scales (180M, 0.9B, 2.4B) on reasoning benchmarks including GSM8K, MATH, and MMLU.

## Key Results
- Small-complexity models (γ=1, λ=1) achieve steeper scaling slopes in test loss versus data and model size
- 0.9B and 2.4B small-complexity models outperform large-complexity counterparts by +4.6% and +3.4% average score across 15 benchmarks
- GSM8K math reasoning performance improves by +19.4% for 0.9B model under small-complexity training
- Mechanistic analysis shows small-complexity models exhibit higher embedding cosine similarity and stronger attention matrix condensation
- The approach generalizes across three model scales (180M, 0.9B, 2.4B) with consistent performance gains

## Why This Works (Mechanism)

### Mechanism 1: Condensation-Driven Pattern Compression
Small initialization (γ > 0.5) induces neuron condensation that forces the model to compress data into fewer production rules rather than memorizing. With small initialization scale, neurons within each layer evolve into clustered groups (condensation), limiting effective neuron count. This constraint forces representations toward fundamental token relationships instead of storing individual patterns. Evidence shows correlation between condensation metrics and downstream performance, though causation remains inferred.

### Mechanism 2: Initialization Rate as Scaling Invariant
The initialization rate γ (exponent of std as a function of width) is the correct invariant for controlling scaling behavior across model sizes. Standard practice uses fixed σ (e.g., 0.02) regardless of model width. The paper argues that γ should remain constant while σ scales with width (σ ∝ d^{-γ}). This maintains consistent complexity characteristics as model size varies, enabling the scaling law to descend faster in both model and data sizes.

### Mechanism 3: Circuit Depth Preference via Norm Regularization
Small complexity shifts learned circuits from dense/shallow (memorization) to sparse/deep (reasoning) configurations. Theoretical analysis shows different γ values correspond to different functional norms (kernel regime vs. mean-field regime). Small γ prioritizes solutions minimizing circuit count with possible depth; large γ allows many shallow circuits. Deep circuits enable compositional reasoning, though this mechanism relies on assumptions about gradient descent behavior.

## Foundational Learning

- **Initialization rate (γ) vs. initialization scale (σ)**
  - Why needed here: The paper's core intervention depends on understanding γ as width-dependent rather than using fixed σ.
  - Quick check question: Given a network with width 1024 and γ=0.5, what σ should you use for weight initialization?

- **Condensation in neural networks**
  - Why needed here: The mechanistic explanation relies on condensation as the bridge between complexity control and reasoning improvement.
  - Quick check question: What happens to effective model capacity when neurons condense?

- **Barron norm vs. RKHS norm in function spaces**
  - Why needed here: Theoretical justification uses these norms to characterize memorization vs. reasoning solutions.
  - Quick check question: Which norm allows singular measures (condensation) and which requires absolute continuity?

## Architecture Onboarding

- **Component map**: Weight initialization (W ~ N(0, (d_in^{-γ})²)) -> AdamW training with cosine LR -> Weight decay with λ -> Complexity monitoring (Dc, Ds) -> Benchmark evaluation

- **Critical path**: 
  1. Set γ based on model scale (γ=1 for ≤0.9B, γ≈0.58 for 2.4B+)
  2. Set λ=1 for strong complexity control
  3. Implement embedding normalization and sandwich normalization for stability at scale
  4. Monitor parameter norm trajectory—should show monotonic increase for small-complexity regime

- **Design tradeoffs**: 
  - Smaller γ → better reasoning but higher instability risk (loss spikes)
  - Larger λ → stronger complexity control but slower training convergence
  - γ=1 optimal for performance; γ≈0.58 practical compromise for stability at 2.4B+

- **Failure signatures**: 
  - Loss spikes during training (especially with γ=1 at large scale)
  - Parameter norm plateauing early (insufficient complexity reduction)
  - Embedding vectors remaining near-orthogonal (no condensation occurring)

- **First 3 experiments**:
  1. Replicate 180M experiment: Train with γ∈{0.1, 0.5, 1.0}, λ∈{0, 0.1, 1.0} on 40B tokens; measure Dc/Ds and benchmark scores
  2. Ablate initialization vs. weight decay: Train with (γ=0.5, λ=1) vs. (γ=1, λ=0.1) to isolate contribution of each component
  3. Stability threshold test: At target model scale, sweep γ from 0.5 to 1.0 in 0.1 increments to find stability boundary before full training run

## Open Questions the Paper Calls Out

### Open Question 1
Can the complexity control framework be effectively scaled to models significantly larger than 2.4B parameters without encountering training instability? The authors were limited by computational resources; it remains unclear if the "loss spike" issues seen in 2.4B models will become unmanageable at scales of 7B parameters or more. Successful pre-training of a 7B+ parameter model using the small-complexity configuration that demonstrates stable loss curves and consistent reasoning improvements would resolve this.

### Open Question 2
Does applying complexity control during the post-training phase (e.g., SFT or RLHF) yield similar reasoning improvements to those observed in pre-training? The current work focuses on initialization and pre-training weight decay; the interaction between complexity constraints and fine-tuning dynamics remains unexplored. Ablation studies applying specific weight decay coefficients during instruction tuning, showing gains on benchmarks like GSM8K or MATH, would provide evidence.

### Open Question 3
Does the theoretical preference for "sparse and deep" circuits, derived for residual networks, rigorously explain the internal mechanics of Transformers trained with small initialization? The mathematical derivation uses simplified assumptions that do not perfectly map to the Transformer architecture. Mechanistic interpretability studies confirming that small-complexity Transformers utilize fewer, deeper circuits compared to large-complexity models would validate this mechanism.

## Limitations

- Mechanistic uncertainty: The causal relationship between condensation and reasoning improvement remains correlational rather than definitively established
- Architecture specificity: All experiments use Llama-style transformers; generalization to other architectures is asserted but untested
- Scale-dependent instability: Small initialization causes training instability at larger scales, requiring architectural modifications or reduced γ values
- Dataset and task bias: Performance gains are evaluated on specific reasoning benchmarks; effectiveness for other capabilities remains untested

## Confidence

**High confidence**: The empirical demonstration that complexity control affects scaling laws and downstream performance is well-supported by systematic experiments across three model scales.

**Medium confidence**: The theoretical justification connecting complexity control to memorization-versus-reasoning tradeoffs through norm minimization is plausible but relies on assumptions about gradient descent behavior and circuit selection.

**Low confidence**: The mechanistic claim that condensation directly causes improved reasoning represents the weakest link, with correlation established but causation not definitively proven.

## Next Checks

1. **Cross-architecture validation**: Train small-complexity models using identical γ and λ settings on non-transformer architectures (e.g., RNNs or CNNs) to test whether the complexity-control framework generalizes beyond Llama-style transformers.

2. **Ablation of condensation mechanism**: Use targeted interventions (e.g., explicit regularization to prevent condensation) while maintaining small initialization and weight decay to determine whether performance gains persist without condensation occurring.

3. **Capability transfer testing**: Evaluate small-complexity models on non-reasoning tasks (creative writing, instruction following, long-context understanding) to determine whether the performance improvements are specific to mathematical/logical reasoning or represent broader capability enhancements.