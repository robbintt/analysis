---
ver: rpa2
title: 'TyphoonMLA: A Mixed Naive-Absorb MLA Kernel For Shared Prefix'
arxiv_id: '2509.21081'
source_url: https://arxiv.org/abs/2509.21081
tags:
- typhoonmla
- absorb
- naive
- attention
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TyphoonMLA, a hybrid kernel that combines
  naive and absorb implementations of Multi-Head Latent Attention (MLA) to exploit
  data reuse in shared prefixes of KV-cache. While existing MLA kernels use absorb
  formulation for memory efficiency, they are compute-bound and miss opportunities
  from shared prefixes common in system prompts and parallel reasoning.
---

# TyphoonMLA: A Mixed Naive-Absorb MLA Kernel For Shared Prefix

## Quick Facts
- **arXiv ID:** 2509.21081
- **Source URL:** https://arxiv.org/abs/2509.21081
- **Reference count:** 40
- **Primary result:** Up to 3.2× speedup on both NPUs and GPUs for Multi-Head Latent Attention with only 3% HBM overhead

## Executive Summary
TyphoonMLA introduces a hybrid kernel approach that combines naive and absorb implementations of Multi-Head Latent Attention (MLA) to exploit data reuse in shared prefixes of KV-cache. While existing MLA kernels use absorb formulation for memory efficiency, they are compute-bound and miss opportunities from shared prefixes common in system prompts and parallel reasoning. TyphoonMLA applies naive formulation to compute-bound shared parts and absorb to memory-bound non-shared parts, achieving up to 3.2× speedup on both NPUs and GPUs with only 3% HBM overhead. The method is compatible with PagedAttention, RadixAttention, and supports tensor/sequence parallelism, making it easily integrable into existing frameworks.

## Method Summary
TyphoonMLA is a hybrid kernel strategy for MLA models that partitions attention computation into shared prefix and non-shared regions. It applies the naive MLA formulation (using uncompressed KV-cache) to the shared prefix to exploit compute-density through data reuse, and the absorb formulation (using compressed latent cache) to non-shared tokens to save bandwidth. The partial results are combined via Log-Sum-Exp (LSE) aggregation. The method includes automatic fallback to absorb-only for batch sizes below 64, where data reuse is insufficient to benefit from the naive approach. TyphoonMLA supports both NPU and GPU backends and integrates with existing attention frameworks like PagedAttention and RadixAttention.

## Key Results
- Up to 3.2× speedup on both NPUs and GPUs compared to absorb-only MLA kernels
- Only 3% HBM overhead from storing uncompressed shared KV-cache
- Consistent performance improvements across DeepSeek-v3 and Kimi K2 models
- Automatic fallback to absorb-only for batch sizes < 64 prevents performance regression

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Hybrid naive-absorb formulation achieves better performance than either formulation alone when shared prefixes exist.
- **Mechanism:** TyphoonMLA partitions attention into compute-bound (shared prefix) and memory-bound (non-shared) regions, applying naive formulation to the former (fewer MACs when data is reused) and absorb to the latter (smaller HBM reads). The partial results are combined using log-sum-exp (LSE) merge logic.
- **Core assumption:** The workload exhibits sufficient shared-prefix length and batch size to make the naive path compute-bound rather than memory-bound.
- **Evidence anchors:**
  - [abstract] "TyphoonMLA applies naive formulation to compute-bound shared parts and absorb to memory-bound non-shared parts, achieving up to 3.2× speedup."
  - [Section 3.2, Table 1] Shows TyphoonMLA requires BLs40k+BLn136k MACs vs absorb's BLs136k+BLn136k (3.4× fewer for shared), and Ls40k+BLn576 HBM reads vs naive's Ls40k+BLn40k (≈70× smaller for non-shared).
  - [corpus] FlashForge (arXiv 2505.17694) confirms prefix-sharing benefits for attention decoding but targets MHA/GQA, not MLA's compute-bound regime.
- **Break condition:** When shared prefix length approaches zero or batch size falls below ~64, naive gains vanish and fallback to absorb-only is required.

### Mechanism 2
- **Claim:** Data reuse in shared prefixes transforms memory-bound naive attention into compute-bound, making it faster than absorb.
- **Mechanism:** Reading shared KV-cache once and reusing across B queries increases operational intensity by factor B. At batch sizes >64, naive's throughput scales while absorb saturates (Figure 2 shows naive reaching 3.4× higher throughput at large batch sizes).
- **Core assumption:** Hardware has sufficient compute throughput (e.g., 400 TFLOPS cube units) relative to memory bandwidth (1.8TB/s) for the roofline crossover to occur within practical batch sizes.
- **Evidence anchors:**
  - [Section 2.3, Figure 2] Roofline analysis shows naive achieves up to 3.4× higher throughput than absorb at batch sizes >64 for DeepSeek-v3 and Kimi K2.
  - [Section 4, Figure 5] Latency breakdown confirms 3.3× speedup in shared part (5.37ms → 1.63ms) at batch size 1024, matching theoretical 3.4× MAC reduction.
  - [corpus] PAT (arXiv 2511.22333) demonstrates similar prefix-aware attention benefits for standard architectures, validating the shared-prefix optimization premise.
- **Break condition:** If memory bandwidth increases dramatically relative to compute (e.g., HBM4), the crossover batch size may shift beyond practical deployment ranges.

### Mechanism 3
- **Claim:** Adaptive fallback to absorb-only at small batch sizes prevents performance regression.
- **Mechanism:** Below a predefined batch-size threshold (empirically ~64 in Figure 8), insufficient data reuse makes naive slower than absorb. TyphoonMLA detects this condition and switches to absorb-only kernel automatically.
- **Core assumption:** The batch-size threshold can be determined statically or with low overhead at runtime.
- **Evidence anchors:**
  - [Section 3.1] "TyphoonMLA automatically switches to an absorb-only kernel whenever the batch size falls below a predefined threshold."
  - [Appendix A.2, Figure 8c] Shows TyphoonMLA matches absorb baseline below batch size 64, then exceeds it by up to 2× at batch size 512.
  - [corpus] No direct corpus evidence for MLA-specific adaptive thresholds; related work focuses on static kernel selection.
- **Break condition:** If workload characteristics change dynamically (e.g., variable prefix lengths within a batch), a single threshold may be suboptimal.

## Foundational Learning

- **Concept: Roofline Model**
  - **Why needed here:** Understanding when attention is memory-bound vs compute-bound is essential for selecting naive vs absorb formulations.
  - **Quick check question:** Given 1.8TB/s bandwidth and 400 TFLOPS compute, what's the operational intensity threshold separating memory-bound from compute-bound regimes?

- **Concept: MLA Absorption Trick**
  - **Why needed here:** Grasping how up-projection matrices can be reordered to keep KV-cache compressed is core to understanding why absorb exists and why it's compute-heavy.
  - **Quick check question:** Why does absorbing W_KVb into W_Qb increase query dimension and FLOPs but reduce KV-cache size?

- **Concept: Log-Sum-Exp (LSE) Combination**
  - **Why needed here:** Merging partial attention results from naive and absorb paths requires numerically stable softmax denominator handling.
  - **Quick check question:** How do you combine softmax outputs from two attention paths with different sequence lengths using LSE values?

## Architecture Onboarding

- **Component map:** Shared KV-cache detection → Naive kernel (shared prefix) + Absorb kernel (non-shared) → LSE combination → Output projection
- **Critical path:**
  1. Identify shared vs non-shared KV-cache portions (typically system prompt boundary)
  2. Route shared portion through naive attention with uncompressed K/V cache
  3. Route non-shared portion through absorb attention with compressed latent cache
  4. Merge partial softmax outputs using LSE values
  5. Apply output projection (W_O)
- **Design tradeoffs:**
  - HBM overhead: ~3% increase from storing uncompressed shared K/V (Figure 6)
  - Kernel complexity: Two attention kernels + combine logic vs single absorb kernel
  - Threshold tuning: Batch-size cutoff affects crossover point; wrong value causes regressions
  - Prefix detection: Requires upfront knowledge of shared vs per-query boundaries
- **Failure signatures:**
  - Performance regression at small batch sizes → fallback threshold too high
  - Memory OOM with long shared prefixes → HBM overhead exceeds capacity
  - Numerical divergence from baseline → LSE combine logic error
  - No speedup despite shared prefixes → prefix detection failing or batch sizes too small
- **First 3 experiments:**
  1. **Baseline comparison:** Measure attention-only latency for absorb-only vs TyphoonMLA across batch sizes 16-512 with fixed 4K shared prefix, 128-token queries. Confirm crossover at ~64.
  2. **Prefix-length sensitivity:** Vary shared prefix length (1K, 4K, 16K tokens) at batch size 256. Validate that speedup scales with shared/non-shared ratio.
  3. **HBM overhead measurement:** Profile total memory usage with and without uncompressed shared K/V for DeepSeek-v3 at batch 4K, sequence 128K. Verify ≈3% overhead claim.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal method for dynamically determining the batch size threshold at which TyphoonMLA should switch from absorb-only to hybrid naive-absorb implementation?
- Basis in paper: [explicit] The paper states TyphoonMLA "automatically switches to an absorb-only kernel whenever the batch size falls below a predefined threshold" and Figure 8 shows the crossover occurs around batch size 64, but the mechanism for setting this threshold is not discussed.
- Why unresolved: The threshold appears to be empirically determined for specific hardware configurations (Ascend NPU with 1.8TB/s bandwidth, 376 TOPS), but the optimal threshold likely depends on hardware specifications, model architecture parameters, and shared/non-shared ratio.
- What evidence would resolve it: A systematic study varying hardware specs (memory bandwidth, compute throughput), model configurations (head count, dimensions), and prefix ratios, measuring throughput across batch sizes to derive a predictive model or adaptive algorithm for threshold selection.

### Open Question 2
- Question: How does TyphoonMLA perform when integrated with speculative decoding techniques that generate multiple candidate token branches sharing common prefixes?
- Basis in paper: [explicit] The introduction mentions "certain speculative decoding techniques (Wang et al., 2025a) require validating multiple candidate tokens in parallel, which share a long sequence of past tokens" as a motivation, but no experiments or benchmarks include speculative decoding scenarios.
- Why unresolved: Speculative decoding creates a specific tree-structured KV-cache pattern with potentially different shared/non-shared ratios than the system prompt scenarios tested, and the branching structure may affect how the LSE combination logic performs.
- What evidence would resolve it: End-to-end benchmarks integrating TyphoonMLA with speculative decoding frameworks, measuring both acceptance rates and throughput improvements compared to baseline MLA kernels.

### Open Question 3
- Question: How does the HBM memory overhead of TyphoonMLA scale in edge cases where shared prefixes become extremely long relative to non-shared portions?
- Basis in paper: [inferred] The paper claims "only a 3% overhead in HBM size" and Figure 6 shows overhead remains minimal across tested configurations, but the analysis assumes shared prefix lengths of 4K-26K tokens. The uncompressed shared KV-cache storage could become problematic if system prompts grow to 100K+ tokens or if batch sizes are very small.
- Why unresolved: The memory analysis focuses on "typical deployment scenarios" with batch sizes 4K-32K and sequence lengths 32K-256K, but edge cases with extremely long shared prefixes and small batch sizes (where the uncompressed shared portion dominates) are not characterized.
- What evidence would resolve it: Memory profiling experiments with shared prefix lengths ranging from 1K to 200K tokens, varying batch sizes from 1 to 1024, measuring both HBM usage and the point at which memory overhead becomes a deployment constraint.

### Open Question 4
- Question: Can the hybrid naive-absorb approach provide benefits during the prefill stage when multiple queries share common prefixes?
- Basis in paper: [inferred] The paper states "naive kernels (e.g., FlashAttention) are typically preferred in training and prefill for their computational efficiency" and focuses exclusively on decode-stage optimization. However, the same shared prefix patterns exist during prefill for batched requests, suggesting potential unexplored optimization opportunities.
- Why unresolved: The prefill stage has different computational characteristics (longer sequences, different operational intensity patterns) than decode, and it's unclear whether the naive-absorb hybrid would provide benefits or if naive-only remains optimal even with shared prefixes.
- What evidence would resolve it: Prefill-stage benchmarks with shared system prompts across multiple concurrent requests, comparing throughput and latency of naive-only, absorb-only, and TyphoonMLA hybrid approaches across varying sequence lengths and batch configurations.

## Limitations
- Performance benefits require substantial shared prefixes and large batch sizes; small batches (<64) see no improvement
- Hybrid approach introduces implementation complexity with two separate kernels plus LSE combination logic
- 3% HBM overhead may become prohibitive for workloads with extremely long shared prefixes (>100K tokens)
- Method is specific to MLA architectures and requires different optimizations for standard multi-head attention

## Confidence
- **High confidence** in the core mechanism: Roofline analysis and latency measurements provide strong empirical evidence for shared-prefix optimization benefits
- **Medium confidence** in generalization: Consistent improvements on tested models, but performance may vary with different architectures and hardware
- **Medium confidence** in implementation simplicity claims: While described as "easily integrable," dual-kernel approach introduces moderate complexity

## Next Checks
1. **Dynamic prefix detection validation:** Test TyphoonMLA with workloads containing multiple different prefix lengths within the same batch (e.g., system prompts of varying lengths). Verify that the prefix detection mechanism correctly identifies and routes each request to the appropriate kernel path.

2. **Memory bandwidth sensitivity analysis:** Measure TyphoonMLA performance on hardware with varying memory bandwidth configurations (e.g., 1.2TB/s vs 2.4TB/s). Quantify how the compute-memory crossover point shifts and whether the 64-batch threshold remains optimal.

3. **Numerical stability stress test:** Generate attention outputs using TyphoonMLA with extreme input values (very large logits, near-zero denominators) and compare against reference implementations. Validate that LSE combination maintains numerical precision and that softmax outputs remain consistent within acceptable tolerances.