---
ver: rpa2
title: 'ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained
  Reward Modeling'
arxiv_id: '2402.06118'
source_url: https://arxiv.org/abs/2402.06118
tags:
- reward
- visual
- language
- image
- grounding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of inaccurate visual grounding
  in large vision language models (LVLMs), which leads to hallucinations, omissions,
  and incorrect inferences about images. The authors propose ViGoR, a framework that
  uses fine-grained reward modeling to improve visual grounding in LVLMs.
---

# ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling

## Quick Facts
- arXiv ID: 2402.06118
- Source URL: https://arxiv.org/abs/2402.06118
- Reference count: 40
- Primary result: ViGoR improves visual grounding in LVLMs by 15% (POPE F1), reduces hallucinations, and achieves state-of-the-art performance on multiple benchmarks

## Executive Summary
This paper addresses the problem of inaccurate visual grounding in large vision language models (LVLMs), which leads to hallucinations, omissions, and incorrect inferences about images. The authors propose ViGoR, a framework that uses fine-grained reward modeling to improve visual grounding in LVLMs. The key idea is to use both human feedback and automated methods to generate detailed reward scores for LVLM-generated text, and then use these scores to fine-tune the LVLM with rejection sampling. The human feedback component involves annotators providing sentence-level evaluations of generated descriptions, while the automated component uses open-set object detectors to verify the existence of mentioned objects in images. The authors demonstrate that their approach significantly improves the visual grounding capabilities of LVLMs across various benchmarks, including POPE, MME, and MMHal-Bench, while preserving the reasoning and creativity of the underlying models.

## Method Summary
ViGoR improves visual grounding in LVLMs by training a fine-grained reward model using both human feedback and automated object verification. The process involves generating multiple captions per image, scoring them with two parallel reward branches (human-trained LVLM for sentence-level semantic scores and automated Grounding DINO for object existence), combining normalized penalties, selecting the best sample via rejection sampling, refining by removing hallucinated sentences, and fine-tuning the base LVLM with supervised learning. The framework achieves this with 15K human annotations and open-set object detection, improving grounding while maintaining reasoning capabilities.

## Key Results
- ViGoR achieves 2.24 average preference rank vs. 3.24 for baseline LLaVA
- ViGoR improves POPE F1 score by 15% compared to baseline
- ViGoR achieves state-of-the-art performance on MME and MMHal-Bench benchmarks
- Fine-grained reward modeling outperforms holistic evaluation by 280 MME points
- Combining human and automated rewards yields better results than either alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sentence-level reward signals reduce annotation ambiguity and improve grounding compared to holistic evaluation.
- Mechanism: Instead of assigning one score to an entire caption (which may contain both correct and hallucinated sentences), annotators evaluate each sentence individually for specific error types (hallucinations, attribute errors, relationship errors, location errors, unreasonable conjectures). The reward model learns to predict these fine-grained scores given image and text.
- Core assumption: Dense, localized feedback creates stronger gradients linking visual features to specific linguistic outputs than sparse global signals.
- Evidence anchors:
  - [abstract] "The human feedback component involves annotators providing sentence-level evaluations of generated descriptions"
  - [Section 3.1] "In our design, annotators assign fine-grained, per-sentence evaluation scores... This fine-grained approach reduces ambiguity, and increases the detailed visual grounding of the reward model."
  - [Section 4.6] Ablation shows fine-grained reward model achieves 1309.3 MME score vs. 1027.4 for holistic-based approach
- Break condition: If generated text has strong sentence interdependencies (e.g., coreference chains), per-sentence scoring may misattribute blame.

### Mechanism 2
- Claim: Open-set object detectors can provide automated, scalable reward signals for noun-entity grounding.
- Mechanism: NLTK extracts nouns from generated captions → GroundingDINO verifies object existence in image → positive scores for detected objects, penalties for hallucinated ones. This distills grounding capabilities from perception models into the LVLM.
- Core assumption: The detector's open-set capability generalizes sufficiently to LVLM vocabulary, and detection confidence correlates with grounding correctness.
- Evidence anchors:
  - [abstract] "automated component uses open-set object detectors to verify the existence of mentioned objects in images"
  - [Section 3.2] "We use Grounding DINO [23] for its strong performance across a wide variety of image domains"
  - [Table 1] ViGoR-AT (automatic only) achieves 2.24 average preference rank vs. 3.24 for baseline LLaVA
  - [corpus] Limited direct corpus evidence; neighboring papers focus on decoding strategies rather than detector-based reward modeling
- Break condition: Detector failures (false negatives on unusual viewpoints, occluded objects) incorrectly penalize valid descriptions; cannot verify attributes, relationships, or non-object entities.

### Mechanism 3
- Claim: Combining human-preference and automated rewards with rejection sampling yields complementary improvements.
- Mechanism: Two parallel reward streams are normalized by their respective variances, linearly combined into a single penalty score. The lowest-penalty sample from 5 candidates is selected, refined (sentences with hallucinated nouns removed), then used for supervised fine-tuning.
- Core assumption: Error signals from both sources are partially independent; automated method catches object hallucinations while human feedback captures semantic and relational errors.
- Evidence anchors:
  - [abstract] "use these scores to fine-tune the LVLM with rejection sampling"
  - [Section 3.3] "we aggregate all negative signals from the two streams... by normalizing their values with their respective variances and linearly combine them"
  - [Table 1] ViGoR-All achieves 1.97 average rank vs. 2.24 (AT-only) and 2.55 (RM-only)
  - [Section 4.6] "two signal sources are complementary and provide better results than either one alone"
  - [corpus] Paper "Transferring Textual Preferences to Vision-Language Understanding through Model Merging" explores related theme of combining preference signals, though via model merging rather than rejection sampling
- Break condition: If variance estimates are poorly calibrated for one signal source, normalization will overweight that source.

## Foundational Learning

- Concept: **Reward Modeling in RLHF**
  - Why needed here: ViGoR trains a separate LVLM as a reward model to predict fine-grained scores; understanding the RLHF paradigm explains why this intermediate model is necessary rather than directly using human labels.
  - Quick check question: Can you explain why reward models are trained instead of using human preferences directly during policy optimization?

- Concept: **Rejection Sampling (Best-of-N)**
  - Why needed here: The core training loop samples 5 outputs, scores them, and uses only the best for fine-tuning. This is offline rejection sampling, distinct from PPO-based online RL.
  - Quick check question: Given a reward model R(x,y) and N samples, how would you select the training target? What are the computational tradeoffs vs. online RL?

- Concept: **Open-Set Object Detection**
  - Why needed here: GroundingDINO's ability to detect arbitrary noun phrases (not just fixed vocabulary) is what enables the automated reward branch. Standard COCO detectors would fail here.
  - Quick check question: What distinguishes open-set detection from closed-set detection, and why does grounding pre-training enable this?

## Architecture Onboarding

- Component map: Input Image + Prompt → Base LVLM (LLaVA) → Sample 5 caption candidates → Reward Branch 1 (Human-trained RM) and Reward Branch 2 (Automated) → Score aggregation (normalized linear combination) → Best sample selection + Refinement module (remove hallucinated sentences) → Supervised fine-tuning of base LVLM

- Critical path:
  1. Reward model quality (determines signal reliability for human branch)
  2. GroundingDINO thresholds (box threshold 0.25 per paper; controls precision/recall tradeoff)
  3. Refinement module aggressiveness (sentence removal vs. partial editing)

- Design tradeoffs:
  - **Offline vs. online rejection sampling**: Paper uses offline (fixed samples from initial model state) for efficiency; online (resampling from evolving model) could improve further but requires more compute
  - **Sentence-level vs. token-level granularity**: Sentence-level is more tractable for human annotation but coarser than token-level grounding
  - **Detection-only automated rewards**: Cannot catch attribute/relationship errors; limits automated branch coverage

- Failure signatures:
  - Reward model overfitting to annotator biases (trained on only 15.4K samples)
  - GroundingDINO false negatives causing removal of valid sentences
  - Domain shift: reward model trained on COCO images may not transfer to significantly different image distributions
  - Loss of creativity if negative penalties dominate

- First 3 experiments:
  1. **Reproduce fine-grained vs. holistic ablation** (Section 4.6): Train two reward models with identical architecture—one with sentence-level labels, one with holistic labels. Compare MME scores to verify ~280-point improvement claimed.
  2. **GroundingDINO threshold sensitivity**: Vary box threshold (default 0.25) and measure impact on POPE precision/recall. Determine if threshold should differ across noun categories.
  3. **Single-branch vs. combined analysis**: Train with only automatic rewards, only human rewards, and combined. On a held-out set, categorize errors caught by each branch (object hallucinations vs. attribute errors vs. relationship errors) to verify complementarity assumption.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can integrating Reinforcement Learning from Human Feedback (RLHF) training strategies, such as PPO, into the ViGoR framework provide measurable performance improvements over the current rejection sampling method?
- Basis in paper: [explicit] "We plan to apply RLHF training with our ViGoR framework, which may offer further improvements over our training based on rejection sampling."
- Why unresolved: The authors currently utilize an offline rejection sampling strategy for efficiency, leaving the potential gains from online RLHF optimization unexplored.
- What evidence would resolve it: A comparative study showing performance metrics (e.g., on MME or POPE) of a ViGoR model trained with PPO against the rejection sampling baseline.

### Open Question 2
- Question: How can the automated reward modeling component be evolved to verify complex semantic elements like object attributes, relationships, and "stuff" regions, which current object detectors cannot process?
- Basis in paper: [explicit] "Current version is limited to objects suitable for detectors while not being applicable to stuff regions, attributes, or layouts."
- Why unresolved: The automated pipeline relies solely on open-set object detectors (e.g., Grounding DINO) that identify noun entities but fail to assess descriptive adjectives or spatial relations.
- What evidence would resolve it: Development of an automated scorer that correlates with human judgment on attribute accuracy and relationship correctness without relying on human feedback for those specific error types.

### Open Question 3
- Question: Does explicitly linking visual entities (e.g., via bounding boxes) to their corresponding phrases in the generated text significantly enhance visual grounding capabilities compared to implicit feature alignment?
- Basis in paper: [explicit] "We anticipate that explicitly linking visual entities with associated phrases in the generated text can further improve visual grounding."
- Why unresolved: The current framework improves grounding implicitly through fine-grained reward scores but does not enforce or output explicit spatial alignments between text tokens and image regions.
- What evidence would resolve it: Experiments where the model is modified to output bounding boxes alongside text, demonstrating improved performance on grounding benchmarks (e.g., RefCOCO) compared to the implicit baseline.

## Limitations
- The automated reward branch can only verify noun entities and not attributes, relationships, or contextual inferences, limiting its coverage to approximately 40-50% of grounding errors
- The human feedback component relies on a relatively small annotation dataset (15.4K samples), raising concerns about potential overfitting and limited diversity in error patterns
- The sentence-level granularity, while more detailed than holistic evaluation, may miss token-level grounding errors and struggles with complex sentence structures involving coreference and coordination

## Confidence
- **High Confidence:** The mechanism of using fine-grained sentence-level rewards instead of holistic evaluation (Section 4.6 ablation showing 280-point MME improvement), and the complementary nature of human vs. automated reward signals (Table 1 preference ranking)
- **Medium Confidence:** The effectiveness of open-set object detectors for automated reward generation, as direct corpus evidence is limited and the paper provides minimal analysis of detector failure modes or threshold sensitivity
- **Medium Confidence:** The overall 15% improvement in visual grounding (POPE F1), though the specific contribution of each component is not fully decomposed in the results

## Next Checks
1. **Error Type Decomposition:** Analyze ViGoR's performance on POPE by categorizing errors (hallucinations, attribute errors, relationship errors, location errors) to verify that the combined approach indeed captures errors missed by individual branches
2. **Domain Generalization Test:** Evaluate ViGoR on non-ADE20K/COCO domains (e.g., medical, satellite, or artistic images) to assess whether the reward model and automated branch generalize beyond their training distribution
3. **Detector Threshold Sensitivity Analysis:** Systematically vary GroundingDINO confidence thresholds (0.1 to 0.5) and measure impact on both automated reward quality and final grounding performance to optimize the precision-recall tradeoff