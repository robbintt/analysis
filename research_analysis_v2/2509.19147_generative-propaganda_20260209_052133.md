---
ver: rpa2
title: Generative Propaganda
arxiv_id: '2509.19147'
source_url: https://arxiv.org/abs/2509.19147
tags:
- social
- deepfakes
- content
- generative
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a taxonomy to distinguish adversarial deepfakes
  from other uses of generative AI in political communications. Through 64 hours of
  interviews with 72 participants across Taiwan and India, the authors find that defenders
  overestimated the use of deceptive deepfakes while missing broader patterns of generative
  propaganda.
---

# Generative Propaganda

## Quick Facts
- arXiv ID: 2509.19147
- Source URL: https://arxiv.org/abs/2509.19147
- Reference count: 40
- Primary result: Develops taxonomy distinguishing adversarial deepfakes from other generative AI uses in political comms; finds defenders overestimated deceptive deepfakes while missing broader patterns of generative propaganda driven by efficiency gains.

## Executive Summary
This paper challenges the conventional focus on deceptive deepfakes by revealing that the primary risk of generative AI in political communications comes from efficiency gains—specifically the ability to scale multilingual, multimodal content and evade detection. Through 64 hours of interviews with 72 participants across Taiwan and India, the authors show that creators often make AI use obvious to manage legal and reputational risks, while defenders' overemphasis on high-fidelity deception causes them to miss broader "generative propaganda" tactics. The study introduces a four-quadrant taxonomy (Obvious/Hidden × Promotional/Derogatory) to help security researchers better understand and counter these threats.

## Method Summary
The study conducted 64 hours of semi-structured interviews with 72 participants across Taiwan and India using purposive and snowball sampling. Taiwan participants were defenders (fact-checkers, journalists, officials, civic technologists), while India included both defenders and creators (influencers, political consultants, advertisers). Interviews were recorded via Microsoft Teams, transcribed, and analyzed using Hey Marvin software through a two-phase abductive coding process. The analysis developed a taxonomy distinguishing obvious from hidden AI use and promotional from derogatory content, informed by weekly expert feedback and member checking.

## Key Results
- Creators intentionally make AI use obvious to reduce legal and reputational risks while still achieving persuasive goals
- Efficiency gains (multilingualism, multimodality, reduced detectability) represent the primary risk, not photorealism
- Defenders' overemphasis on deceptive deepfakes causes them to miss broader patterns of generative propaganda
- The "deepfakes" label creates discursive power that misaligns defender priorities with actual threat patterns

## Why This Works (Mechanism)

### Mechanism 1: Efficiency-Driven Scaling
- **Claim:** Generative AI increases propaganda threat primarily through efficiency gains rather than photorealism
- **Mechanism:** Actors use AI to generate "AIPasta" (perturbed content) and "Precision Propaganda" (localized content) to bypass detection and cross language barriers, flooding information channels with varied narratives
- **Core assumption:** The bottleneck for information manipulation is scaling diverse content, not creating single convincing fakes
- **Evidence anchors:** Abstract states primary risk is efficiency gains; section 5.2 documents AI's utility in cross-language communication and evasion
- **Break condition:** If detection becomes faster/cheaper than generation, or marginal utility of content variation drops to zero

### Mechanism 2: Risk-Constrained Obviousness
- **Claim:** Creators intentionally make AI use obvious to maintain persuasive framing while managing legal/reputational liabilities
- **Mechanism:** In regulated environments, creators use stylized "Soft Fakes" and "Deep Roasts" that are obviously AI-generated but still persuasive to reinforce narratives without triggering enforcement
- **Core assumption:** Audiences can be persuaded by "obviously fake" content that reinforces existing biases; creators act rationally to minimize personal risk
- **Evidence anchors:** Abstract notes creators make use obvious to reduce risks; section 4.1 provides example of cartoony imagery that still persuades target audience
- **Break condition:** If legal enforcement collapses or audiences universally reject obvious AI content as low-value

### Mechanism 3: Discursive Misalignment
- **Claim:** The "deepfakes" label misaligns defender priorities toward high-fidelity deception detection, causing them to miss broader generative propaganda tactics
- **Mechanism:** Defenders focus resources on spotting "Deep Fakes" (Hidden + Derogatory) while missing "Soft Fakes" (Promotional + Obvious) and "AIPasta" that actually constitute majority of observed activity
- **Core assumption:** Defender resources and threat models are rigid and influenced by popular terminology rather than observed field data
- **Evidence anchors:** Abstract notes "deepfakes" exerts outsized discursive power; section 6 documents overemphasis on deceptive use risks overlooking substantial additional uses
- **Break condition:** If security teams adopt the paper's taxonomy and broaden threat models to include obvious/promotional content

## Foundational Learning

- **Concept: Social Shaping of Technology (vs. Determinism)**
  - **Why needed here:** The paper rejects technological determinism, arguing social factors (laws, reputation, norms) determine how technology is deployed
  - **Quick check question:** Does a new AI capability automatically lead to specific attack, or do social costs (like jail time or losing a job) filter which capabilities are actually used?

- **Concept: Taxonomy of Representation (Obvious/Hidden x Promotional/Derogatory)**
  - **Why needed here:** To operationalize defense, one must distinguish between "Deep Fake" (Hidden/Derogatory) and "Soft Fake" (Obvious/Promotional)
  - **Quick check question:** When classifying content, ask: Is it trying to hide its AI origin, and is it attacking someone or promoting them?

- **Concept: Narrative Distortion vs. Factual Deception**
  - **Why needed here:** The paper highlights that "distortion" (flooding the zone, distracting) is often a more viable threat model than "deception" (convincing someone a lie is truth), especially via "AIPasta"
  - **Quick check question:** Is the goal of this content to make you believe a specific lie, or simply to overwhelm your ability to find the truth?

## Architecture Onboarding

- **Component map:** Multilingual/Multimodal streams → Filter 1 (Deepfake Detection) → Filter 2 (Narrative Cluster Analysis) → Context Engine (Legal/Reputational Risk) → Tiered alerts
- **Critical path:** Detection of AIPasta/Narrative Flooding is now higher priority than single-instance deepfake detection; systems must detect artificial variation at scale across languages
- **Design tradeoffs:** Precision vs. recall in taxonomy (strictly classifying "Deep Fakes" reduces false positives but requires high confidence in intent); global vs. local (models must balance generalizability with local context)
- **Failure signatures:** "Deepfake Deluge" Trap (system flags satirical obvious content as threat); "Silent Flood" Blindspot (system ignores low-quality AI Slop missing coordinated narrative attack)
- **First 3 experiments:**
  1. Quantify "Obvious" Usage: Audit current flagged content to measure ratio of Obvious/Promotional vs Hidden/Derogatory content
  2. AIPasta Detection: Build prototype to detect artificial variation in comments/posts (repetitive semantics with lexical variance)
  3. Reputational Constraint Mapping: Analyze if accounts with persistent identities are less likely to produce Hidden content compared to disposable accounts

## Open Questions the Paper Calls Out

- **Open Question 1:** How do non-expert audiences perceive and process "obvious" generative propaganda (e.g., soft fakes, deep roasts) compared to deceptive deepfakes? [explicit] The authors note need for further research on audience experiences.
- **Open Question 2:** Do the efficiency gains of AI (multilingualism, multimodality) and motivation to persuade rather than deceive generalize to Western democracies? [inferred] Study limited to Taiwan and India; authors note insights may over-index on Indian context.
- **Open Question 3:** How can platforms design reputation signals that effectively constrain disposable accounts used for AIPasta and troll operations? [inferred] Authors note reputational risk constrains influencers but fails to constrain faceless social media accounts.

## Limitations

- Qualitative study limited to Taiwan and India contexts, reducing generalizability to other political environments
- Sampling strategy may have missed certain actor types (troll groups, content farms) due to access barriers
- Taxonomy's applicability to non-political domains and different cultural contexts remains untested
- Analysis depends on self-reported behaviors and observed cases, potentially underrepresenting covert operations

## Confidence

- **High Confidence:** Taxonomy framework and observation that creators intentionally make AI use obvious to manage risk
- **Medium Confidence:** Claim that efficiency gains represent primary risk of generative AI in propaganda
- **Medium Confidence:** Assertion that defenders overemphasize deceptive deepfakes
- **Low Confidence:** Broader claim about AIPasta and narrative flooding as dominant tactics requires additional empirical validation

## Next Checks

1. **Quantitative Validation:** Analyze large corpus of flagged political content to empirically measure distribution of obvious vs hidden AI use and assess whether current detection systems over-prioritize photorealistic deepfakes
2. **Cross-Cultural Applicability:** Test taxonomy's utility in additional political contexts (e.g., US, Brazil, EU) to evaluate generalizability beyond Taiwan and India
3. **Detection Prototype:** Build and evaluate prototype system for detecting "AIPasta" (artificial content variation) to validate whether this represents tractable threat vector that current defenses miss