---
ver: rpa2
title: Large Language Models are Highly Aligned with Human Ratings of Emotional Stimuli
arxiv_id: '2508.14214'
source_url: https://arxiv.org/abs/2508.14214
tags:
- anew
- text
- human
- ratings
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated how well large language models (LLMs) align
  with human ratings of emotional stimuli across text and image datasets. Researchers
  elicited ratings from several LLMs (including GPT-4o) for emotion-laden words and
  images previously rated by humans, comparing responses across five discrete emotion
  categories (happiness, anger, sadness, fear, disgust) and two-dimensional arousal/valence
  scales.
---

# Large Language Models are Highly Aligned with Human Ratings of Emotional Stimuli

## Quick Facts
- arXiv ID: 2508.14214
- Source URL: https://arxiv.org/abs/2508.14214
- Reference count: 0
- LLMs show strong correlation (r = 0.89-0.93) with human ratings of emotional stimuli

## Executive Summary
This study evaluated alignment between large language models and human ratings of emotional stimuli across text and image datasets. Researchers elicited ratings from multiple LLMs for emotion-laden words and images previously rated by humans, comparing responses across five discrete emotion categories and two-dimensional arousal/valence scales. GPT-4o demonstrated strong alignment with human ratings across modalities, achieving correlations of 0.89-0.93 for discrete emotions and 0.79-0.90 for arousal/valence (all p < 0.001). LLM ratings were more homogeneous than human ratings, with consistently lower standard deviations. Overall, LLMs demonstrated better alignment with humans within the five-category emotion framework than the two-dimensional model, suggesting closer correspondence to human emotional processing patterns in key behavioral domains.

## Method Summary
The study elicited ratings from multiple LLMs (GPT-4o, GPT-4o-mini, Gemma2-9B, Llama3-8B, Solar 10.7B) for emotional words and images from standardized datasets (ANEW, OASIS, NAPS) that had been previously rated by humans. For each item, 20 LLM "participants" were run at temperature 1.0 using prompts asking them to rate stimuli on five discrete emotions (happiness, anger, sadness, fear, disgust) and two dimensions (arousal, valence). Item-level means and standard deviations were computed across the 20 runs and compared to human ratings using Pearson correlation and Wilcoxon rank sum tests.

## Key Results
- GPT-4o showed strong alignment with human ratings across modalities (r = 0.89-0.93 for discrete emotions, r = 0.79-0.90 for arousal/valence, all p < 0.001)
- LLM ratings were substantially more homogenous than human ratings, with lower standard deviations across all items (p < 0.001)
- LLMs aligned better within the five-category emotion framework than the two-dimensional (arousal/valence) model
- Arousal ratings showed the weakest alignment between humans and LLMs (r = 0.59-0.81), while happiness ratings were best aligned (r = 0.86-0.91)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs approximate human emotional ratings by reproducing statistical regularities from training corpora containing emotionally-annotated language.
- Mechanism: Word-image co-occurrence patterns and explicit emotion discourse in training data create latent associations that activate when models are prompted for ratings.
- Core assumption: Training corpora contain sufficient examples of explicit emotional language and ratings for the model to generalize.
- Evidence anchors:
  - [abstract] "GPT-4o responded very similarly to human participants across modalities, stimuli and most rating scales (r = 0.9 or higher in many cases)"
  - [section] Results show correlations of r = 0.89-0.93 for discrete emotions and r = 0.79-0.90 for arousal/valence (all p < 0.001)
  - [corpus] "Semantic Structure in Large Language Model Embeddings" finds LLM semantic associations exhibit similar low-dimensional structure to human ratings

### Mechanism 2
- Claim: LLMs align more strongly with categorical emotion frameworks than dimensional ones because language encodes emotion lexically through discrete categories.
- Mechanism: Emotion words (happiness, anger, sadness, fear, disgust) appear as distinct linguistic tokens with coherent co-occurrence patterns. Arousal and valence lack equivalent lexical anchors and require mapping through abstract associations.
- Core assumption: The superiority of categorical alignment reflects training data structure rather than inherent model limitations with continuous scales.
- Evidence anchors:
  - [abstract] "Overall LLMs aligned better within a five-category emotion framework than within a two-dimensional (arousal and valence) organization"
  - [section] "Overall arousal ratings were least aligned between humans and LLMs (r = 0.59 to 0.81), while happiness ratings were best aligned (r = 0.86 to 0.91)"
  - [corpus] "AI shares emotion with humans across languages and cultures" suggests cross-linguistic consistency in emotion representation

### Mechanism 3
- Claim: LLM response homogeneity emerges from probabilistic sampling that regresses toward learned central tendencies.
- Mechanism: At temperature 1.0, models sample from learned distributions that have compressed multiple perspectives into unified representations. Human populations contain genuine individual differences; models simulate a single "average" perspective per instance.
- Core assumption: Lower variance reflects statistical averaging during training rather than model inability to represent uncertainty.
- Evidence anchors:
  - [abstract] "LLM ratings were substantially more homogenous than human ratings"
  - [section] "in all cases, the standard deviation of ratings produced by the LLM participants for each item was lower than for human participants (all Wilcoxon rank sum tests, p < 0.001)"

## Foundational Learning

- Concept: Pearson correlation (r)
  - Why needed here: Interpreting the reported alignment strength (r = 0.89-0.93 vs. r = 0.59-0.81) requires understanding what correlation coefficients measure and their sensitivity to variance.
  - Quick check question: If human ratings for an item had SD = 0 (all identical), what would happen to the correlation calculation?

- Concept: Discrete vs. dimensional emotion models
  - Why needed here: The paper directly compares Ekman's five basic emotions against Russell's circumplex model (arousal/valence) as competing frameworks for LLM alignment.
  - Quick check question: Name one advantage and one limitation of representing "anxiety" in each framework.

- Concept: Temperature parameter in LLM sampling
  - Why needed here: The study used temperature = 1.0 to generate 20 simulated participants; understanding this setting is essential for interpreting whether homogeneity is an artifact or intrinsic property.
  - Quick check question: How would increasing temperature above 1.0 likely affect the standard deviation of LLM ratings?

## Architecture Onboarding

- Component map:
  - Stimuli inputs: ANEW (words), OASIS/NAPS (images) -> standardized emotional datasets
  - Prompt template: "Please rate [stimulus] for [emotion]: 1-5 scale. Respond only with a number."
  - LLM instances: 20 parallel runs per condition at temperature 1.0
  - Content filter: Items activating safety filters excluded per-model
  - Aggregation: Item-level means and SDs computed across 20 LLM instances
  - Comparison: Pearson correlation between LLM and human item-level means; Wilcoxon tests on SDs

- Critical path:
  1. Select standardized dataset with human norms
  2. Construct prompts matching original human study paradigm
  3. Run N=20 LLM instances per item (temperature 1.0)
  4. Filter items that trigger content safety mechanisms
  5. Aggregate to item-level statistics
  6. Compute correlation with human benchmark

- Design tradeoffs:
  - Temperature 1.0: Provides variation across runs but may not capture full human variance range
  - Item-level aggregation: Smooths instance-level noise but obscures individual-response patterns
  - Single prompt format: Ensures comparability but may not elicit optimal model performance
  - Content filter exclusion: Maintains safety but reduces comparability with humans who rated all items

- Failure signatures:
  - Very low correlations (< 0.3): Suggests prompt-dataset mismatch or model capability gap
  - Systematically biased means (e.g., all ratings clustering at 3): Central tendency bias from prompt or model
  - SD near zero across items: Temperature too low or model failing to discriminate
  - High filter exclusion rate: Dataset contains stimuli incompatible with model safety training

- First 3 experiments:
  1. Replicate with different temperature settings (0.5, 1.0, 1.5) to characterize whether homogeneity is parameter-dependent or intrinsic to model representations.
  2. Test prompt variations (e.g., "As a human would rate..." vs. neutral framing) to measure prompt sensitivity of emotional alignment.
  3. Extend to out-of-distribution stimuli (e.g., novel images, multilingual text) to test generalization bounds of learned emotional associations.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the stronger alignment of LLMs with the five-category emotion framework (vs. the two-dimensional model) reflect inherent properties of emotion representation or artifacts of language model training?
- **Basis in paper:** [explicit] The conclusion states that "further research is needed to determine whether this pattern reflects inherent properties of emotion representation or artifacts of language model training."
- **Why unresolved:** While the study establishes that LLMs align better with discrete categories, the causal mechanism—whether architectural or data-driven—remains unidentified.
- **What evidence would resolve it:** Ablation studies comparing models trained exclusively on categorical versus dimensional emotion data, or representational similarity analysis (RSA) of model layers.

### Open Question 2
- **Question:** What mechanisms drive the significantly higher homogeneity (lower standard deviation) in LLM ratings compared to human ratings?
- **Basis in paper:** [inferred] The results consistently show lower standard deviations for LLMs, but the paper does not investigate if this stems from training data averaging or the lack of simulated "personality" variance.
- **Why unresolved:** The phenomenon is documented, but it is unclear if this is an immutable feature of the model weights or a limitation of the prompting strategy (e.g., temperature settings).
- **What evidence would resolve it:** Experiments employing persona-based prompting to introduce demographic or temperamental variance, measuring if the standard deviations approach human baselines.

### Open Question 3
- **Question:** Why is the alignment between LLM and human ratings lower for "arousal" than for "valence" or discrete emotions?
- **Basis in paper:** [inferred] The text notes that "arousal ratings were less well aligned between human and LLM raters" (r = 0.59–0.81), but does not test the cause of this specific discrepancy.
- **Why unresolved:** It remains unclear if the concept of physiological arousal is less well-represented in the training data or if the term is semantically ambiguous to the model without visual/sensory context.
- **What evidence would resolve it:** Error analysis on misaligned items to see if they cluster around specific intensity levels, or testing if multimodal (audio/visual) inputs improve arousal alignment.

## Limitations
- The study relies on existing standardized datasets rather than novel stimuli, limiting generalizability to out-of-distribution contexts
- Content filter exclusions varied by model, potentially introducing systematic bias, particularly for disgust-related stimuli
- The use of temperature 1.0 with only 20 simulated participants may not capture the full range of human individual differences

## Confidence
- High confidence: GPT-4o's strong alignment with human ratings (r = 0.89-0.93 for discrete emotions, p < 0.001) is well-supported by the reported correlations and consistent across multiple datasets
- Medium confidence: The superiority of categorical over dimensional alignment is supported but could reflect prompt structure or training data encoding rather than inherent model limitations with continuous scales
- Medium confidence: LLM homogeneity is empirically demonstrated but the mechanism (statistical averaging vs. model limitation) remains unclear

## Next Checks
1. Test alignment at different temperature settings (0.5, 1.0, 1.5) to determine whether LLM homogeneity is parameter-dependent or intrinsic to model representations
2. Evaluate alignment with out-of-distribution stimuli including novel images, multilingual text, and culturally-specific emotional expressions to test generalization bounds
3. Compare prompt variations ("As a human would rate..." vs. neutral framing) to measure sensitivity of emotional alignment to framing effects