---
ver: rpa2
title: 'Bias vs Bias -- Dawn of Justice: A Fair Fight in Recommendation Systems'
arxiv_id: '2506.18327'
source_url: https://arxiv.org/abs/2506.18327
tags:
- bias
- fair
- fairness
- orig
- recommendations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses fairness in recommendation systems by proposing
  a re-ranking approach that mitigates social bias across multiple sensitive attributes
  (gender, age, occupation) while considering item categories. The method leverages
  counterfactual fairness by using category preferences of users with different sensitive
  attributes as a corrective mechanism against historical bias.
---

# Bias vs Bias -- Dawn of Justice: A Fair Fight in Recommendation Systems

## Quick Facts
- arXiv ID: 2506.18327
- Source URL: https://arxiv.org/abs/2506.18327
- Authors: Tahsin Alamgir Kheya; Mohamed Reda Bouadjenek; Sunil Aryal
- Reference count: 40
- Primary result: Multi-attribute re-ranking approach reduces bias across gender, age, and occupation groups while maintaining recommendation quality

## Executive Summary
This paper addresses fairness in recommendation systems by proposing a re-ranking approach that mitigates social bias across multiple sensitive attributes while considering item categories. The method leverages counterfactual fairness by using category preferences of users with different sensitive attributes as a corrective mechanism against historical bias. Experiments on three real-world datasets (MovieLens 100K, MovieLens 1M, Yelp) with multiple recommendation models show significant bias reduction with little to no degradation in performance metrics like NDCG and HitRatio.

## Method Summary
The approach computes Counterfactual Category Proportions (CCP) for each user by aggregating category preferences from users who do not share their sensitive attribute. A re-ranking objective combines relevance and KL divergence between the CCP and the actual recommended category distribution, optimized via greedy submodular selection. The method uses a parameter $\beta$ to control the trade-off between relevance and fairness, and $\gamma$ to control rank-weighting. Experiments demonstrate significant bias reduction across multiple sensitive attributes while maintaining recommendation quality.

## Key Results
- Significant bias reduction measured by pairwise differences in category distributions across gender, age, and occupation groups
- Little to no degradation in performance metrics like NDCG and HitRatio for most settings
- Gender-related bias most prominently reduced, with effectiveness varying across sensitive attributes
- Fairness improvements can sometimes enhance recommendation performance by improving coverage

## Why This Works (Mechanism)

### Mechanism 1: Counterfactual Category Proportion as Fairness Baseline
- Claim: Using the category preferences of users who do NOT share a user's sensitive attribute provides a reference distribution that counteracts historical bias amplification.
- Mechanism: For each user $u$ with sensitive attribute $s_u$, compute $o(c|s_u)$ — the time-weighted average proportion of category $c$ across all users whose sensitive attribute differs from $s_u$. This becomes the target distribution for re-ranking.
- Core assumption: Opposing biases exist across demographic groups (e.g., male users historically prefer action, female users prefer romance), and using one group's preferences can neutralize another's amplified bias.
- Evidence anchors:
  - [abstract] "leverages counterfactual fairness by using category preferences of users with different sensitive attributes as a corrective mechanism"
  - [Page 3] "male users are more biased towards action and thrillers, while less towards romance and comedies. And female users are more biased towards romance and comedies... We take advantage of these opposing biases"
- Break condition: If datasets become truly neutral (no historical bias patterns), the counterfactual baseline offers no corrective signal.

### Mechanism 2: KL Divergence Minimization via Greedy Submodular Optimization
- Claim: Minimizing KL divergence between the counterfactual target distribution and the actual recommended distribution, while preserving relevance, yields fair re-rankings with theoretical approximation guarantees.
- Mechanism: The objective $I^* = \text{argmax}_{I \subseteq TopN, |I|=k} (1-\beta) \cdot rel(I,u) + \beta \cdot \sum_c o(c|s_u) \log \sum_{j=1}^{|I|} \frac{1}{j^\gamma \tilde{r}(c|v_j)}$ is optimized greedily, guaranteeing $(1-1/e)$ approximation.
- Core assumption: The fairness term can be expressed as a submodular function amenable to greedy selection.
- Evidence anchors:
  - [Page 6] "we adopt a similar approach condensing our equation to... The simplified submodular greedy optimization has an optimal guarantee of $1 - 1/e$"
  - [Page 7] Algorithm 1 shows the greedy item-by-item selection process
- Break condition: If $\beta$ exceeds ~0.6–0.8, the fairness term dominates and can introduce reverse bias (observed for gender in Figure 5).

### Mechanism 3: Category-Aware Bias Measurement via Pairwise Distribution Differences
- Claim: Bias is quantified by summing pairwise differences in category distributions across demographic groups, providing granular visibility into disparities.
- Mechanism: Two metrics — CC (unweighted category proportion) and CDCG (rank-discounted) — compute per-group averages, then aggregate pairwise absolute differences across all groups and categories.
- Core assumption: Category-level granularity captures meaningful bias missed by aggregate item-level fairness metrics.
- Evidence anchors:
  - [Page 8-9] Equations 5-6 define CC and CDCG; "sum of pair-wise differences between all user groups... to quantify bias in multi-valued sensitive attribute groups"
  - [Page 11, Figure 3] Scatter plots show bias reduction across all categories for all models
- Break condition: If categories are too fine-grained or overlapping, the signal becomes noisy; if too coarse, bias is hidden within categories.

## Foundational Learning

- **KL Divergence**:
  - Why needed here: Core mathematical tool for measuring distributional mismatch between counterfactual target and actual recommendations.
  - Quick check question: Can you explain why KL divergence is asymmetric and requires smoothing ($\alpha$) to avoid division by zero?

- **Counterfactual Fairness (Kusner et al., 2017)**:
  - Why needed here: Provides the conceptual foundation — outcomes should not change if a user's sensitive attribute were different.
  - Quick check question: How does this paper's group-level category distribution approach differ from individual-level counterfactual fairness?

- **Submodular Functions & Greedy Optimization**:
  - Why needed here: Justifies why greedy item selection provides near-optimal solutions for the NP-hard re-ranking problem.
  - Quick check question: What is the approximation guarantee for maximizing a monotone submodular function under cardinality constraints?

## Architecture Onboarding

- **Component map**:
  1. Pre-trained Recommender (MF, WMF, NeuMF, VAE-CF) → generates candidate scores $score_{u,v}$ for top-N items
  2. CCP Computer → computes $o(c|s_u)$ for each sensitive attribute value from training data
  3. Re-ranking Engine → greedy selection using combined relevance + KL-fairness objective
  4. Evaluator → computes NDCG@k, HitRatio@k, CC, CDCG

- **Critical path**:
  - Pre-training baseline recommender (no fairness constraints) → computing CCP from historical interactions → per-user greedy re-ranking → evaluation

- **Design tradeoffs**:
  - $\beta \in [0,1]$: Higher values prioritize fairness over relevance; optimal range empirically 0.4–0.6
  - $\gamma \in [0,1]$: Controls rank-weighting for category contributions; 0.1 found optimal
  - $N$ vs $k$: Larger candidate pool (N) increases computation but improves fairness-quality trade-off

- **Failure signatures**:
  - NDCG drops sharply: $\beta$ too high; reduce to 0.3–0.5
  - Bias increases after re-ranking: $\beta$ exceeds threshold where counterfactual distribution over-corrects (especially for gender with strong historical bias)
  - Zero-division errors: $\alpha$ not applied to smooth $\tilde{r}(c|u,I)$

- **First 3 experiments**:
  1. **Baseline validation**: Run NeuMF on ML-100K, compute CC/CDCG for gender without re-ranking; confirm bias exists (Table 3 shows CC=0.2275 for NeuMF gender)
  2. **Ablation on $\beta$**: Sweep $\beta \in \{0.0, 0.2, 0.4, 0.6, 0.8\}$ with fixed $\gamma=0.1$; plot NDCG vs CC to identify Pareto frontier (replicate Figure 5 patterns)
  3. **Multi-attribute test**: Apply re-ranking for age (7 classes) and occupation (21 classes) on ML-1M; verify bias reduction is less pronounced than gender due to dilution across more classes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the re-ranking framework be extended to effectively mitigate bias for intersectional demographic groups (e.g., users belonging to both a specific gender and occupation) without suffering from data sparsity issues?
- Basis in paper: [explicit] The authors state in the conclusion: "we also intend to extend our work to address intersectional fairness for the consumers (like female and doctor)."
- Why unresolved: The current evaluation isolates sensitive attributes (gender, age, occupation) and evaluates them independently, which does not capture the compound disadvantages faced by users at the intersection of multiple groups.
- What evidence would resolve it: Experimental results on the same datasets showing bias reduction (lower CC/CDCG scores) specifically for intersectional sub-groups, alongside standard performance metrics.

### Open Question 2
- Question: How can the method be adapted to explicitly optimize for provider-side fairness (e.g., exposure of specific item brands) while maintaining the current consumer-side category fairness?
- Basis in paper: [explicit] The authors note: "In the future, we want to explicitly address the provider perspective (for instance, including item brands) to ensure a more holistic solution to social bias in recommendations."
- Why unresolved: The current approach implicitly affects providers through category coverage but lacks a mechanism to enforce fairness constraints on item providers or brands, potentially allowing disparities in supplier exposure to persist.
- What evidence would resolve it: A modification of the objective function to include provider constraints and results showing improved provider exposure metrics without significant degradation in consumer fairness (CC/CDCG) or relevance (NDCG).

### Open Question 3
- Question: How does the effectiveness of the Counterfactual Category Proportion (CCP) baseline degrade if the underlying dataset does not exhibit strong "opposing biases" or evolves to become historically neutral?
- Basis in paper: [inferred] The paper relies on the intuition of "opposing biases" to correct recommendations, but notes the method "may be less useful if future datasets evolve to be more neutral and unbiased."
- Why unresolved: The method assumes the presence of distinct historical disparities between groups to calculate a corrective counterfactual distribution; it is unclear if the KL divergence objective functions correctly or adds noise if the training data is already balanced.
- What evidence would resolve it: Experiments on synthetic or pre-processed "neutral" datasets analyzing the correlation between the magnitude of initial historical bias and the effectiveness of the re-ranking algorithm.

## Limitations
- Relies heavily on the assumption that opposing biases exist across demographic groups and that historical data contains exploitable patterns for counterfactual correction
- Requires pre-trained recommendation models, adding computational overhead and dependency on baseline performance
- Greedy re-ranking has $O(k \cdot N)$ complexity, which may be prohibitive for very large candidate pools

## Confidence
- **High confidence**: The mathematical formulation of the submodular objective and its $(1-1/e)$ approximation guarantee; experimental results showing NDCG/HitRatio stability across multiple models and datasets
- **Medium confidence**: The counterfactual fairness mechanism's effectiveness across all sensitive attributes (strong for gender, weaker for occupation with 21 classes); the claim that fairness improvements can enhance recommendation coverage
- **Low confidence**: The generalizability to other recommendation domains beyond movies and restaurants; the assumption that category-level bias correction translates to overall fairness in downstream user outcomes

## Next Checks
1. **Dataset neutrality test**: Apply the method to synthetic datasets with controlled bias levels to verify the claimed break condition when historical bias patterns disappear
2. **Multi-attribute interaction analysis**: Test scenarios where users belong to multiple sensitive groups (e.g., young female vs. old male) to evaluate how the method handles intersectional bias
3. **Temporal stability evaluation**: Measure how the counterfactual baseline and resulting fairness metrics change over time as user preferences evolve and recommendation systems adapt