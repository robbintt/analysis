---
ver: rpa2
title: Achieving $\varepsilon^{-2}$ Dependence for Average-Reward Q-Learning with
  a New Contraction Principle
arxiv_id: '2601.21301'
source_url: https://arxiv.org/abs/2601.21301
tags:
- lemma
- policy
- proof
- then
- q-learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes the first non-asymptotic convergence rates
  for both synchronous and asynchronous Q-learning for average-reward Markov decision
  processes without relying on contraction assumptions. The key idea is constructing
  an instance-dependent seminorm under which the lazy-transformed Bellman operator
  becomes contractive.
---

# Achieving $\varepsilon^{-2}$ Dependence for Average-Reward Q-Learning with a New Contraction Principle

## Quick Facts
- **arXiv ID:** 2601.21301
- **Source URL:** https://arxiv.org/abs/2601.21301
- **Reference count:** 40
- **Primary result:** First non-asymptotic O(ε⁻²) sample complexity bounds for average-reward Q-learning under reachability assumption

## Executive Summary
This paper establishes the first non-asymptotic convergence rates for both synchronous and asynchronous Q-learning for average-reward Markov decision processes without relying on contraction assumptions. The key innovation is constructing an instance-dependent seminorm under which the lazy-transformed Bellman operator becomes contractive. Under a reachability assumption, simple variants of synchronous and asynchronous Q-learning that use lazy sampling achieve optimal O(ε⁻²) sample complexity (up to logarithmic factors) to obtain ε-optimal policies and Q-function estimates.

## Method Summary
The method introduces "lazy" Q-learning variants that transform the transition kernel by adding self-loops with probability 1/2, creating aperiodicity. This enables a new contraction principle based on an instance-dependent seminorm (esp) that weights errors by reachability to a reference state. The paper presents both explicit and implicit implementations of lazy Q-learning, with the implicit version analytically averaging the "stay" and "move" outcomes without requiring explicit sampling. A correction step transforms the learned lazy Q-values back to standard Q-values. The approach achieves O(|S||A|ε⁻²) for synchronous updates and O(ε⁻²) for asynchronous updates under the reachability assumption.

## Key Results
- New contraction principle based on instance-dependent seminorm under which lazy Bellman operator is a strict one-step contraction
- Synchronous lazy Q-learning algorithms with O(|S||A|ε⁻²) sample complexity
- Asynchronous lazy Q-learning algorithms with O(ε⁻²) sample complexity
- First non-asymptotic O(ε⁻²) convergence rate for average-reward Q-learning without contraction assumptions

## Why This Works (Mechanism)

### Mechanism 1: Lazy Kernel Regularization
Introducing self-loops to the transition dynamics regularizes the Markov chain, enabling a contraction property that the original AMDP lacks. The method transforms the transition kernel $P$ into a "lazy" kernel $\bar{P}$ (Eq. 3.1), where the agent remains in the current state with fixed probability $1-\alpha$ (specifically $\alpha=1/2$). This ensures aperiodicity and provides the structural regularity required to bound hitting times, which is necessary for the subsequent contraction proof.

### Mechanism 2: Instance-Dependent Seminorm ($\text{esp}$)
Standard span seminorms fail to capture the contractive dynamics of average-reward operators. The authors construct a custom seminorm, $\text{esp}(\cdot)$, under which the Bellman operator *is* a strict one-step contraction. The seminorm $\text{esp}(\cdot)$ (Definition 3.2) weights the span of the Q-function error by the worst-case probability of reaching the reference state $s^\dagger$ over a horizon $K$. By penalizing directions in the Q-space that persist due to poor reachability, it transforms the non-expansive Bellman operator into a $\beta$-contraction (Theorem 3.4).

### Mechanism 3: Implicit vs. Explicit Sampling
One can implement the "lazy" update either by sampling the self-loop (Explicit) or by analytically averaging the value of staying vs. moving (Implicit), achieving similar theoretical guarantees. The Implicit method (Algorithm 2) updates using $\frac{1}{2} \max_{a'} Q(s,a') + \frac{1}{2} \max_{a'} Q(s',a')$ (Eq. 4.5), which is an unbiased estimator of the lazy Bellman operator without requiring a random number generator for the "stay" action.

## Foundational Learning

- **Concept: Span Seminorm vs. Norm**
  - **Why needed here:** Average-reward Q-functions are defined only up to an additive constant. Standard norms (like L2) penalize this constant shift, while the span seminorm $\text{sp}(x) = \max_i x_i - \min_i x_i$ ignores it. The paper builds a new seminorm $\text{esp}$ on this foundation.
  - **Quick check question:** If I add 100 to every entry of a Q-vector, does $\text{sp}(Q)$ change? (Answer: No).

- **Concept: The Reachability Assumption (Unichain vs. Multichain)**
  - **Why needed here:** The paper's convergence speed depends on $K$, the worst-case time to hit a reference state. You must distinguish between MDPs where all states communicate (Unichain/Reachability) vs. those with disconnected components.
  - **Quick check question:** In a grid world with two separate rooms with no doors, is Assumption 1 satisfied? (Answer: No).

- **Concept: Hitting Time ($K$)**
  - **Why needed here:** The sample complexity scales as $O(K \dots)$. This parameter measures how "connected" the MDP is. A large $K$ implies slow mixing and slower convergence.
  - **Quick check question:** If it takes 1000 steps on average to return to the start state, is the sample complexity higher or lower than if it takes 10 steps? (Answer: Higher).

## Architecture Onboarding

- **Component map:** Standard MDP $(S, A, P, r)$ -> Lazy Kernel Transformation $\bar{P}$ -> Q-Table storing $Q_t$ -> Implicit Lazy Bellman Operator (Eq. 4.5) -> Output Correction (Eq 4.7)

- **Critical path:** The implementation of the **Implicit Lazy Update** (Algorithm 2) is the most robust path. It avoids the variance injection of stochastic "stay" actions while maintaining the unbiased property. You must ensure the term $\frac{1}{2}\max_{a'} Q_t(s,a')$ is computed correctly at every step.

- **Design tradeoffs:**
  - **Synchronous vs. Asynchronous:** The synchronous version (Alg 2) converges faster ($O(\varepsilon^{-2})$) but requires a generative model (sampling arbitrary states). The asynchronous version (Alg 4) works on a single trajectory but has complex constants depending on mixing times.
  - **Hyperparameters:** The paper fixes $\alpha = 1/2$. Deviating from this requires re-proving the contraction factor $\beta$.

- **Failure signatures:**
  - **Divergence:** If the learning rate $\lambda$ is too high relative to the horizon $K$, the "geometric decay" of initial bias fails. (Paper suggests $\lambda \propto \ln T / T$).
  - **Optimality Gap:** If the MDP is Multichain (violating Assumption 1), the "reference state" logic fails, and the average reward may depend on the starting state, rendering the single scalar $g^*$ target invalid.

- **First 3 experiments:**
  1. **Toy Chain (Verification):** Implement the 4-state diamond MDP from Section 6. Verify that standard Q-learning oscillates or converges slowly, while Implicit Lazy Q-learning follows the $t^{-1/2}$ rate.
  2. **Ablation on $K$:** Construct a "River Swim" style chain of length $N$. Plot error vs. samples as $N$ increases. Check if sample complexity scales linearly with $N$ (proxy for $K$).
  3. **Implicit vs. Explicit:** Compare variance of the Q-estimates. The Implicit method should show lower variance in the estimates due to the deterministic averaging of the "stay" probability.

## Open Questions the Paper Calls Out

- **Can the dependence on the hitting-time parameter $K$ in the sample complexity be substantially improved?** The current construction results in a sample complexity that scales exponentially with $K$ (specifically involving terms like $2^{8K}$). A refined analysis framework or modified algorithm that achieves polynomial or logarithmic dependence on $K$ would resolve this.

- **Can the contraction principle and the $O(\varepsilon^{-2})$ convergence guarantees be extended to weakly communicating or multichain MDPs?** The current theoretical framework relies on Assumption 1 (Reachability), which assumes the existence of a reference state reachable from all other states under any policy. This structure is used to enforce contraction and may not hold in weakly communicating or multichain settings where multiple recurrent classes or transient states exist.

- **Is the lazy transformation (lazy sampling) strictly necessary to achieve the optimal $\tilde{O}(\varepsilon^{-2})$ sample complexity under the reachability assumption?** While the paper avoids strong a priori contraction assumptions, it introduces an algorithmic modification (lazy sampling) to construct a contraction. It remains unverified whether standard Q-learning can achieve the same rate under only the reachability assumption.

## Limitations
- **Reachability Assumption Dependency**: The entire theoretical framework hinges on Assumption 1 (Reachability), which may fail in real-world scenarios with disconnected or absorbing states.
- **Unknown Hitting Time K**: The convergence rate depends explicitly on the hitting time parameter K, which is instance-dependent and often unknown in practice, requiring manual tuning.
- **Synchronous vs. Asynchronous Gap**: While the synchronous algorithm achieves O(|S||A|ε⁻²) complexity, the asynchronous variant's guarantees depend on mixing times and visitation frequencies without full characterization.

## Confidence
- **High Confidence**: The core contraction principle using the instance-dependent seminorm (esp) is mathematically sound and well-proven in Theorems 3.4 and 3.5. The output correction mechanism (Eq 4.7) is explicitly defined and tested on the 4-state example.
- **Medium Confidence**: The O(ε⁻²) sample complexity bounds are theoretically established but rely heavily on the Reachability assumption and accurate estimation of K. The implicit vs. explicit sampling equivalence (Remark 4.1) is proven but requires careful implementation.
- **Low Confidence**: The practical performance in multichain MDPs where the Reachability assumption fails is not well-characterized. The paper doesn't provide extensive empirical validation beyond the simple 4-state example.

## Next Checks
1. **Multi-Chain MDP Test**: Implement a multichain MDP with disconnected components to verify whether the algorithm degrades gracefully or fails catastrophically when Assumption 1 is violated.

2. **Hitting Time Sensitivity**: Systematically vary the hitting time K in a controllable environment (e.g., chain MDP of varying lengths) and measure the actual convergence rate versus the theoretical O(ε⁻²) prediction.

3. **Implicit vs. Explicit Variance Comparison**: Implement both the implicit and explicit lazy Q-learning algorithms and measure the empirical variance of Q-value estimates across multiple runs to verify the claim of similar performance guarantees.