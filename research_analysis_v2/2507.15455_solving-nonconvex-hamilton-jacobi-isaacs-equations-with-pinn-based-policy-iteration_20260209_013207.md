---
ver: rpa2
title: Solving nonconvex Hamilton--Jacobi--Isaacs equations with PINN-based policy
  iteration
arxiv_id: '2507.15455'
source_url: https://arxiv.org/abs/2507.15455
tags:
- policy
- value
- iteration
- solution
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses high-dimensional nonconvex Hamilton-Jacobi-Isaacs
  equations arising in stochastic differential games by proposing a mesh-free policy
  iteration framework that combines classical dynamic programming with physics-informed
  neural networks (PINNs). The method alternates between solving linear second-order
  PDEs under fixed feedback policies and updating controls via pointwise minimax optimization
  using automatic differentiation.
---

# Solving nonconvex Hamilton--Jacobi--Isaacs equations with PINN-based policy iteration

## Quick Facts
- **arXiv ID:** 2507.15455
- **Source URL:** https://arxiv.org/abs/2507.15455
- **Reference count:** 34
- **Primary result:** Proposed PINN-based policy iteration achieves relative L2-errors below 10^-2 in a two-dimensional stochastic path-planning game and consistently outperforms direct PINN solvers in higher-dimensional publisher-subscriber differential games.

## Executive Summary
This paper addresses the challenge of solving high-dimensional nonconvex Hamilton-Jacobi-Isaacs (HJI) equations in stochastic differential games by introducing a mesh-free policy iteration framework that combines classical dynamic programming with physics-informed neural networks (PINNs). The method alternates between solving linear second-order PDEs under fixed feedback policies and updating controls via pointwise minimax optimization using automatic differentiation. Under Lipschitz and uniform ellipticity assumptions, the approach proves local uniform convergence to the unique viscosity solution. Numerical experiments demonstrate superior accuracy and stability compared to direct PINN solvers, particularly in higher dimensions.

## Method Summary
The proposed method implements policy iteration using PINNs by alternating between two steps: Policy Evaluation and Policy Improvement. In Policy Evaluation, a neural network with a special ansatz (v(t,x;θ) = g(x) + (T-t)N(t,x;θ)) solves a linear second-order PDE with fixed feedback policies, enforcing terminal conditions exactly. In Policy Improvement, the method updates controls via pointwise minimax optimization using automatic differentiation to compute gradients. The approach uses sinusoidal (SIREN) activations, ADAM optimization with learning rate 0.001, and resamples collocation points every 100 epochs. The algorithm warm-starts by initializing θⁿ⁺¹ ← θⁿ.

## Key Results
- Achieved relative L2-errors below 10^-2 in a two-dimensional stochastic path-planning game with moving obstacle
- Consistently outperformed direct PINN solvers in five- and ten-dimensional publisher-subscriber differential games
- Demonstrated smoother value functions and lower residuals compared to baseline methods
- Theoretical convergence proof established under Lipschitz and uniform ellipticity assumptions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Iterative linearization stabilizes the optimization of nonconvex Hamilton-Jacobi-Isaacs (HJI) equations by decoupling the minimax operation from the PDE solution.
- **Mechanism:** The method splits the problem into a Policy Evaluation step (solving a linear, second-order PDE with fixed controls) and a Policy Improvement step (updating controls via pointwise optimization). This avoids directly minimizing a nonconvex residual, which often traps standard PINNs in poor local minima.
- **Core assumption:** The Lagrangian is strongly convex in control $a$ and strongly concave in disturbance $b$.
- **Break condition:** Failure of strong convexity/concavity in the Lagrangian, leading to non-unique or unstable policy updates.

### Mechanism 2
- **Claim:** Enforcing hard terminal constraints via a specific neural ansatz prevents "temporal drift" and accelerates convergence relative to soft penalty methods.
- **Mechanism:** The network parameterizes the value function as $v(t, x; \theta) = g(x) + (T - t)N(t, x; \theta)$. This construction guarantees $v(T, x) = g(x)$ exactly for any network weights $\theta$, focusing the optimizer solely on the PDE residual dynamics.
- **Core assumption:** The terminal cost $g(x)$ is differentiable and known.
- **Break condition:** Singularities or discontinuities in the terminal cost $g(x)$ that the continuous network cannot represent.

### Mechanism 3
- **Claim:** Uniform ellipticity of the diffusion term ensures smoothness of value function iterates, allowing standard automatic differentiation to approximate gradients reliably.
- **Mechanism:** The diffusion term $\frac{1}{2}\text{Tr}(\sigma\sigma^\top D^2 v)$ acts as a regularizer. Under the assumption $\sigma\sigma^\top \succeq \lambda I$, the value function iterates are provably $C^{2+\beta}$ (smooth). This guarantees that $\nabla_x v$ exists and is Lipschitz, making the feedback policy updates well-posed.
- **Core assumption:** Uniform ellipticity (diffusion is always active and non-degenerate).
- **Break condition:** Degenerate diffusion ($\sigma \to 0$).

## Foundational Learning

- **Concept: Viscosity Solutions**
  - **Why needed here:** HJI equations generally do not have classical smooth solutions. The paper proves convergence to a "viscosity solution," which is the correct generalized solution concept for optimal control.
  - **Quick check question:** Can you explain why a non-smooth value function is acceptable in differential games, but requires a specific definition of "solution"?

- **Concept: Minimax (Saddle Point) Optimization**
  - **Why needed here:** HJI equations represent zero-sum games (Player I minimizes, Player II maximizes). The core loop relies on finding a saddle point in the Lagrangian.
  - **Quick check question:** In the Policy Improvement step, does one update the minimizing player first, or simultaneously?

- **Concept: Automatic Differentiation (AD)**
  - **Why needed here:** The "physics-informed" part relies on computing exact derivatives ($\nabla_x v, D^2_{xx} v$) of the network output with respect to inputs to compute PDE residuals.
  - **Quick check question:** How does AD differ from numerical differentiation (finite differences) in the context of computing the Hessian $D^2_{xx} v$ for training stability?

## Architecture Onboarding

- **Component map:**
  Value Network (SIREN MLP) -> Ansatz Layer (hard constraint) -> Residual Engine (AD) -> Policy Optimizer (minimax)

- **Critical path:**
  1. Sample collocation points $(t, x)$.
  2. Compute $\nabla_x v$ and $D^2_{xx} v$ via AD.
  3. Update feedback policies $(\alpha, \beta)$ locally (Policy Improvement).
  4. Train network to minimize linear PDE residual with fixed $(\alpha, \beta)$ (Policy Evaluation).
  5. Repeat until convergence.

- **Design tradeoffs:**
  - **Vs Direct PINN:** Higher implementation complexity (alternating optimization) vs. much lower residual error and better stability in high dimensions.
  - **Vs Grid-based:** Infinite resolution / curse-of-dimensionality avoidance vs. lack of strict guarantees on bounded domains and sensitivity to sampling.

- **Failure signatures:**
  - **Oscillatory Gradients:** If using ReLU/Tanh instead of SIREN (sine) activations, high-frequency derivatives in $D^2 v$ may vanish or explode.
  - **Boundary Drift:** If the domain is not sampled extensively near boundaries, the unbounded domain approximation fails.

- **First 3 experiments:**
  1. **2D Obstacle Avoidance:** Validate against Finite Difference Method (FDM) to establish baseline accuracy ($10^{-2}$ error).
  2. **5D Isotropic Game:** Test scalability; verify symmetry preservation where theoretical decomposition exists.
  3. **10D Anisotropic Game:** Stress test with non-separable dynamics; compare smoothness against direct PINN.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the framework be extended to first-order or degenerate HJI equations where diffusion is absent?
- **Basis in paper:** [explicit] The Conclusion states, "An important limitation... is the assumption of non-degenerate diffusion. We leave the development of PINN-based policy iteration schemes for such first-order HJI problems to future work."
- **Why unresolved:** The theoretical convergence (Theorem 1) relies on uniform ellipticity (Assumption 1) to ensure regularity via Schauder estimates, a property that fails in the deterministic limit ($\sigma \to 0$).
- **What evidence would resolve it:** A modified convergence proof or numerical validation for deterministic differential games ($\sigma = 0$) without artificial viscosity.

### Open Question 2
- **Question:** How can the framework be made robust to neural network initialization and gradient instability?
- **Basis in paper:** [explicit] The Conclusion notes the method inherits "limitations intrinsic to PINN-based approaches, such as sensitivity to network initialization and challenges in gradient stability."
- **Why unresolved:** The non-convex loss landscape of PINNs makes training dynamics highly dependent on weight initialization and random seeds, which the current work does not analytically address.
- **What evidence would resolve it:** An ablation study demonstrating consistent convergence across varied random seeds and initialization strategies without manual hyperparameter tuning.

### Open Question 3
- **Question:** Does integrating adaptive sampling or operator learning (e.g., DeepONet) improve performance over the standard MLP backbone?
- **Basis in paper:** [explicit] The Conclusion suggests future work should aim to address issues by "incorporating adaptive sampling strategies" and "exploring PINN variants based on operator learning."
- **Why unresolved:** Standard MLPs may suffer from spectral bias, and it is currently untested whether operator architectures preserve the Lipschitz regularity required for the policy update.
- **What evidence would resolve it:** Comparative benchmarks against the proposed MLP approach in dimensions $d > 10$ using DeepONet or Fourier Neural Operators.

## Limitations
- Theoretical convergence proof requires non-degenerate diffusion, excluding deterministic differential games
- Method inherits sensitivity to neural network initialization and gradient instability from PINN approaches
- Practical scalability and computational cost in very high dimensions remain untested beyond 10D

## Confidence

- **High confidence:** The mechanism of policy iteration (alternating policy evaluation and improvement) is well-established in classical dynamic programming. The theoretical convergence proof under stated assumptions is rigorous.
- **Medium confidence:** The numerical experiments convincingly show improved accuracy over direct PINN solvers in the tested 2D, 5D, and 10D cases. However, the sample size is limited, and the comparison is against only one baseline method.
- **Low confidence:** The practical impact of the hard terminal constraint ansatz on training stability is demonstrated empirically but not rigorously quantified against alternative methods. The paper does not analyze how sensitive the approach is to hyperparameters like network depth, learning rate, or collocation sampling density.

## Next Checks
1. **Robustness to initial policy:** Systematically test the method's convergence with different initial policy choices (e.g., uniform random, zero policy, or policy from a coarser discretization) on the 2D obstacle avoidance game to quantify sensitivity.
2. **Deterministic case analysis:** Investigate whether adding small artificial diffusion (σ→εI) and taking ε→0 recovers accurate solutions for the deterministic case, or if the policy iteration breaks down entirely.
3. **Scalability and memory profiling:** Measure training time and GPU memory usage as a function of dimension for the publisher-subscriber games to assess practical limits beyond the tested 10D case.