---
ver: rpa2
title: 'Online Curvature-Aware Replay: Leveraging $\mathbf{2^{nd}}$ Order Information
  for Online Continual Learning'
arxiv_id: '2502.01866'
source_url: https://arxiv.org/abs/2502.01866
tags:
- learning
- ocar
- online
- continual
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses catastrophic forgetting in online continual
  learning by proposing Online Curvature-Aware Replay (OCAR), a second-order optimization
  method that leverages Fisher Information Matrix (FIM) approximations via K-FAC to
  stabilize learning across nonstationary data streams. Unlike first-order methods,
  OCAR explicitly constrains KL divergence on replay data while accelerating updates
  in non-interfering directions.
---

# Online Curvature-Aware Replay: Leveraging $\mathbf{2^{nd}}$ Order Information for Online Continual Learning

## Quick Facts
- **arXiv ID:** 2502.01866
- **Source URL:** https://arxiv.org/abs/2502.01866
- **Reference count:** 40
- **Primary result:** OCAR uses second-order FIM preconditioning to stabilize online continual learning, achieving state-of-the-art AAA/WC-Acc on CIFAR-100, TinyImageNet, and Online CLEAR.

## Executive Summary
This paper addresses catastrophic forgetting in online continual learning by proposing Online Curvature-Aware Replay (OCAR), a second-order optimization method that leverages Fisher Information Matrix (FIM) approximations via K-FAC to stabilize learning across nonstationary data streams. Unlike first-order methods, OCAR explicitly constrains KL divergence on replay data while accelerating updates in non-interfering directions. Experiments on CIFAR-100, TinyImageNet, and Online CLEAR benchmarks show OCAR outperforms state-of-the-art methods in continual metrics (AAA, WC-Acc) and achieves higher average accuracy throughout training. The method also integrates well with other approaches, demonstrating robustness and improved feature learning compared to i.i.d. baselines.

## Method Summary
OCAR extends Experience Replay (ER) by incorporating second-order curvature information via Fisher Information Matrix (FIM) approximations using Kronecker-factored Approximate Curvature (K-FAC). The method jointly optimizes on current and replayed data while maintaining a KL-divergence constraint on buffer data through FIM preconditioning. Key components include: (1) K-FAC factor estimation with exponential moving averages, (2) λ-weighted FIM assembly prioritizing buffer data, (3) regularized inversion via τ damping, and (4) dynamic scheduling of τ and λ to control stability-plasticity tradeoff. The algorithm uses reservoir sampling for buffer management and works with standard Slim-ResNet18 architectures trained via SGD.

## Key Results
- OCAR achieves state-of-the-art AAA (Average Anytime Accuracy) and WC-Acc (Worst-Case Accuracy) on CIFAR-100, TinyImageNet, and Online CLEAR benchmarks
- Outperforms first-order methods (ER, MIR, DER++) and second-order methods (EWC, MAS) across all metrics
- Maintains stable optimization with bounded gradient norms and smoother parameter trajectories at task boundaries
- Integrates effectively with other methods (MER, CoPE, S-EWC), demonstrating broad applicability
- Achieves better feature learning than i.i.d. baselines while preserving continual learning performance

## Why This Works (Mechanism)

### Mechanism 1
Second-order preconditioning via FIM stabilizes learning on replay data while accelerating updates in directions that don't interfere with past knowledge. The Fisher Information Matrix approximates the curvature of the KL-divergence loss landscape. By inverting it (with regularization), OCAR transforms gradient steps to account for parameter space geometry—slowing updates in high-curvature directions (important for current/past tasks) and accelerating in low-curvature directions (non-interfering). Core assumption: The true Fisher (sampling from model predictions) better captures the loss geometry than the empirical Fisher (using ground-truth labels), and K-FAC's block-diagonal Kronecker factorization is a sufficient approximation for OCL.

### Mechanism 2
Explicit KL-divergence constraint on replay data, approximated via FIM, provides a hard stability bound that first-order methods lack. The optimization problem includes constraint KL(f_{t-1}(x_B) || f_t(x_B)) ≤ ρ. Taylor-expanding around w_{t-1} eliminates zeroth and first-order terms, leaving δ^T F_B δ/2 ≤ ρ. This forces updates to stay within a trust region defined by buffer data curvature. Core assumption: The buffer sufficiently represents past task distributions, and the FIM computed on buffer samples approximates the full past-data FIM.

### Mechanism 3
The α/τ ratio controls the stability-plasticity tradeoff by bounding effective step size in low-curvature directions. Eigenvalue analysis shows effective step size σ*_i = α/(σ_i + τ). When σ_i → 0, σ*_i → α/τ, capping maximum acceleration. Increasing τ over time stabilizes long-term learning as buffer information density grows. Core assumption: The eigenvalue spectrum spans orders of magnitude (verified empirically in Appendix), with small eigenvalues corresponding to directions needing regularization at task boundaries.

## Foundational Learning

- **Fisher Information Matrix (FIM)**: Approximates curvature of KL-divergence loss landscape; needed here as Riemannian metric for parameter space. Quick check: Can you explain why the true Fisher uses samples from p(y|x;w) while the empirical Fisher uses ground-truth labels, and why Kunstner et al. (2019) caution against the latter?

- **K-FAC (Kronecker-factored Approximate Curvature)**: Makes FIM inversion tractable by exploiting layer structure. OCAR computes A_l = E[a_l a_l^T] and G_l = E[g_l g_l^T] per layer, inverting smaller matrices via Kronecker properties. Quick check: Given K-FAC's block-diagonal assumption ignores inter-layer correlations, what might happen if layer couplings become critical during rapid task switching?

- **KL-divergence as optimization objective in OCL**: OCAR formalizes OCL as joint optimization with KL constraints. KL divergence connects to cross-entropy loss and has natural curvature properties via FIM. Quick check: Why does the KL constraint on buffer data, when Taylor-expanded around previous parameters, yield the FIM as the second-order term?

## Architecture Onboarding

- **Component map**: Input Stream → Batch Sampler (N_t new + B_t buffer) → Loss & Gradient Computation (joint on N_t ∪ B_t) → K-FAC Factor Estimation (EMA of A_l, G_l per layer) → FIM Assembly + λ-weighting (buffer F_B weighted by λ) → Preconditioner Inversion: (F_EMA + τI)^-1 → Gradient Transformation: ∇̃ = F_INV · ∇L → Parameter Update: w ← w - α∇̃ → Buffer Update: Reservoir sampling

- **Critical path**: 1. Initialize τ = α, λ = 1 2. Per batch: compute loss on (N_t + B_t), compute K-FAC factors weighting buffer by λ 3. Update EMA factors: A_EMA ← (1-γ)A_EMA + γA_batch; same for G_EMA 4. Reset G_EMA for last layer if classifier shape changed (new classes) 5. Invert preconditioner: F_INV = (F_EMA + τI)^-1 via Kronecker properties 6. Transform gradient, update weights, increment τ (and λ if class/time-based) 7. Reservoir-update buffer

- **Design tradeoffs**: τ scheduling speed vs. long-term plasticity (faster increase stabilizes earlier but may lock suboptimal representations); λ scheduling strategy (class-incremental vs domain-incremental); EMA decay γ (lower γ smooths FIM but lags during rapid shifts); buffer size vs computational cost (paper uses 2000-4000 samples)

- **Failure signatures**: Exploding gradients at task boundaries (check if τ too small relative to α); loss of plasticity (plateauing accuracy, τ or λ increased too aggressively); numerical instability in K-FAC inversion (check if τ damping insufficient or factor EMA degenerate); poor integration with non-KL losses (DER++ breaks assumptions, expect instability)

- **First 3 experiments**: 1. Sanity check on convex setting: Replicate Figure 3 with small linear model on 10 synthetic tasks, compare OCAR vs EWC vs NGD vs ER on cumulative loss (L_p, L_s) 2. Hyperparameter sensitivity sweep: On Split-CIFAR100 (first 5 tasks), sweep α ∈ {10^-3, 10^-2, 10^-1} and α/τ ∈ {0.1, 1, 10}, plot final accuracy, forgetting, Task 5 accuracy 3. Ablation of key components: Compare (a) full OCAR, (b) OCAR without λ-weighting, (c) OCAR without τ scheduling, (d) OCAR with empirical Fisher instead of true Fisher on Split-CIFAR100 AAA/WC-Acc

## Open Questions the Paper Calls Out

- **Can alternative curvature approximations (e.g. E-KFAC) improve OCAR's performance or efficiency in online continual learning?**: Future research directions include the comparison of different approximations of the curvature (e.g. George et al. (2018b)). Only K-FAC was tested; E-KFAC or other Kronecker-factored methods may provide better accuracy-computation tradeoffs.

- **Can α and τ be feasibly adapted dynamically during training to automatically balance stability and plasticity?**: Feasible dynamic adaptation of α and τ as a future research direction. Current scheduling (fixed τ increase, λ growth with classes) requires manual tuning; automatic adjustment could improve robustness across diverse streams.

- **Can OCAR be modified to work with losses not derived from KL divergence, such as entropy-regularized objectives?**: OCAR-DER found to be much less stable (failing optimization on TinyImageNet), conjectured to be caused by entropy regularization implicit in the DER loss, resulting in a loss "too different" from KL divergence.

## Limitations

- **Incomplete hyperparameter specification**: Exact α, Δτ, EMA parameter (α_EMA), and initial λ values only specified through grid search ranges rather than fixed schedules
- **K-FAC approximation limitations**: Block-diagonal assumption ignores inter-layer correlations, potentially problematic during rapid task switching
- **Buffer size constraints**: Reliance on relatively small buffer sizes (2000-4000 samples) may limit applicability to more challenging continual learning scenarios
- **Loss function restrictions**: FIM theoretical justification relies on KL-divergence properties; entropy terms break this assumption causing optimization failures

## Confidence

- **High confidence**: Core mechanism of using second-order curvature information via FIM to stabilize replay learning is well-supported by mathematical formulation and 2D trajectory visualizations
- **Medium confidence**: Empirical validation across three benchmarks is strong, though hyperparameter tuning details remain incomplete
- **Medium confidence**: Integration with other methods (MER, CoPE, S-EWC) is demonstrated, but generality across different loss functions needs more exploration

## Next Checks

1. **Sanity check on convex setting**: Replicate Figure 3 experiment with small linear model on synthetic tasks to verify OCAR achieves lower stability loss (L_s) without sacrificing plasticity loss (L_p)

2. **Hyperparameter sensitivity sweep**: On Split-CIFAR100 (first 5 tasks), systematically vary α and α/τ ratio to confirm the stability-plasticity tradeoff correlation shown in Figure 2

3. **Ablation of key components**: Compare full OCAR against variants without λ-weighting, τ scheduling, and true Fisher to quantify each component's contribution on Split-CIFAR100 metrics