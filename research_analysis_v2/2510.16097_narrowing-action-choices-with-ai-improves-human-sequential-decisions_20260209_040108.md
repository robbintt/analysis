---
ver: rpa2
title: Narrowing Action Choices with AI Improves Human Sequential Decisions
arxiv_id: '2510.16097'
source_url: https://arxiv.org/abs/2510.16097
tags:
- human
- decision
- support
- action
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a decision support system that improves human
  sequential decision-making by narrowing the action set using a pre-trained AI agent.
  The system uses a bandit algorithm to optimize the level of human agency, finding
  the ideal balance between human and AI input.
---

# Narrowing Action Choices with AI Improves Human Sequential Decisions

## Quick Facts
- arXiv ID: 2510.16097
- Source URL: https://arxiv.org/abs/2510.16097
- Authors: Eleni Straitouri; Stratis Tsirtsis; Ander Artola Velasco; Manuel Gomez-Rodriguez
- Reference count: 40
- Primary result: Decision support system improves human sequential decisions by ~30% over humans alone, using AI-narrowed action sets

## Executive Summary
This paper introduces a decision support system that enhances human sequential decision-making by using a pre-trained AI agent to narrow the human's action set. Rather than simply providing recommendations or ceding control entirely, the system uses a Lipschitz bandit algorithm to optimize the level of human agency (parameter ε) dynamically. In a large-scale human subject study (n=1,600) playing a wildfire mitigation game, participants using this system outperformed those playing alone by ~30% and the AI agent by >2%, despite the AI agent outperforming humans alone.

The key innovation is adaptively controlling human agency through action sets rather than forcing humans to cede control entirely. The system leverages the idea that humans possess complementary information to AI, and by constraining choices to a high-value subset, the human can make better decisions than either acting alone. The Lipschitz bandit algorithm efficiently identifies the optimal level of human agency, achieving sublinear regret guarantees.

## Method Summary
The system uses a pre-trained Deep Q-Network (DQN) agent to rank actions based on state valuations q(s,a). For each state, it constructs a restricted action set containing the top-ranked action and others within threshold ε of it (plus random noise). Humans select from this narrowed set while seeing the full state. A Lipschitz bandit algorithm optimizes ε over time by exploiting the smoothness of the reward function with respect to ε. The approach is validated through both simulations using heuristic human policies and a large-scale human subject study (n=1,600) in a wildfire mitigation game.

## Key Results
- Participants using the decision support system outperformed humans alone by ~30% in cumulative reward
- The system achieved >2% better performance than the AI agent alone, despite the AI outperforming humans individually
- The Lipschitz bandit algorithm achieved sublinear regret, efficiently identifying optimal agency levels
- Human-AI complementarity was demonstrated: combined performance exceeded either party acting independently

## Why This Works (Mechanism)

### Mechanism 1
Constraining the human's action space to a high-value subset can yield higher cumulative rewards than allowing either the human or the AI to act independently. The system uses a pre-trained AI agent to rank all possible actions and constructs a subset containing the top-ranked action and others within threshold ε of the top score (plus random noise). The human is forced to select from this narrowed set. This works because humans possess salient, hard-to-quantify information that the AI's lossy representation misses, allowing them to pick the best option among the good options provided by the AI.

### Mechanism 2
The optimal level of human agency (ε) can be identified efficiently by leveraging the Lipschitz continuity of the reward function. The authors prove that expected total reward is Lipschitz continuous with respect to ε, meaning similar ε values yield similar rewards. They exploit this smoothness using a "zooming" bandit algorithm that progressively narrows the search space for ε, discarding regions with low payoff. This works because the human decision-making policy and environment transitions satisfy mathematical smoothness properties required for the Lipschitz bounds.

### Mechanism 3
Randomizing the action set construction (via noise W) is necessary to enable efficient optimization of the agency level. The system adds half-normal noise to AI valuations before thresholding by ε, ensuring the probability distribution over possible action sets changes smoothly as ε changes. Without this, the optimization landscape might be discrete and jagged, preventing the Lipschitz bandit algorithm from working effectively. The randomization ensures that the probability distribution over action sets varies smoothly with ε.

## Foundational Learning

- **Concept: Lipschitz Continuity**
  - Why needed: This mathematical property is the theoretical engine of the paper, guaranteeing that the search for the optimal "agency level" (ε) is tractable
  - Quick check: If the reward function were not Lipschitz continuous, why would the "zooming" algorithm fail? (Answer: It couldn't guarantee that exploring a midpoint of an interval tells you anything about the edges, leading to inefficient search.)

- **Concept: Best Arm Identification (Bandits)**
  - Why needed: The system must dynamically find the best ε (the "arm") to use. This is distinct from standard RL as it optimizes a hyperparameter of the interaction mode, not the direct action in the game
  - Quick check: In this paper, what constitutes an "arm"? (Answer: A specific value of the parameter ε ∈ [0,1] representing the level of human agency.)

- **Concept: Structural Causal Models (SCM)**
  - Why needed: The paper models the sequential decision process (State → Action Set → Human Action → Reward) formally as an SCM to derive their theoretical guarantees
  - Quick check: Why is the distinction between the environment state Z_t (seen by the human) and the representation S_t (seen by the AI) crucial in the SCM? (Answer: It formalizes the "complementarity" assumption—the human sees something the AI doesn't.)

## Architecture Onboarding

- **Component map:** State (Z) -> Lossy Encoder -> AI Valuation (q) -> [Epsilon + Noise] -> Action Set (C) -> Human Choice (A) -> Reward
- **Critical path:** State (Z) → AI Valuation (q) → [Epsilon + Noise] → Action Set (C) → Human Choice (A) → Reward
- **Design tradeoffs:**
  - Discretization vs. Continuum: The algorithm treats ε as a continuous arm problem but relies on discrete intervals for zooming
  - Noise (σ): High noise smooths the optimization landscape (good for the bandit algo) but lowers the immediate quality of the action set (bad for the human)
  - Static vs. Dynamic ε: The current architecture optimizes for a global ε. A per-state ε(s) could perform better but is harder to optimize
- **Failure signatures:**
  - Regret Plateau: Algorithm 1 stops improving simple regret; likely the zooming dimension d is high (arms are not clustered)
  - Human Override: Users refuse to pick from C_t (if possible) or exhibit "gaming" behavior to bypass the filter
  - AI Collapse: The DQN q-values become flat (uninformative), causing the action set C_t to essentially become random
- **First 3 experiments:**
  1. Sanity Check (Synthetic Humans): Run the system using "simulated humans" (heuristic policies) to verify Algorithm 1 converges to the theoretical optimum before deploying to real users
  2. Ablation on Randomness: Vary σ (noise scale) to plot the curve between "smoothness for optimization" and "action set quality"
  3. Uniform vs. Zooming: Compare Algorithm 1 (Zooming) against Algorithm 2 (Uniform Discretization) to measure the efficiency gain (sample complexity) directly using the provided dataset

## Open Questions the Paper Calls Out

- **Open Question 1:** Would utilizing state-dependent parameters ε(s), rather than a single global ε, improve the performance of the decision support system in sequential tasks? (Basis: The paper suggests state-dependent ε could improve performance but doesn't explore this extension.)

- **Open Question 2:** Can the action-set-based decision support system achieve human-AI complementarity in real-world sequential decision-making tasks beyond the wildfire mitigation game? (Basis: The study only validates the approach on one synthetic task, raising questions about generalizability.)

- **Open Question 3:** What factors determine the degree of human-AI complementarity achieved by the system, and can they be used to predict when complementarity is most beneficial? (Basis: The paper demonstrates complementarity exists but doesn't analyze which conditions maximize the complementarity gap.)

- **Open Question 4:** How can the decision support system be extended to handle continuous action spaces? (Basis: The current methodology requires discrete actions, and extending to continuous spaces would require new mechanisms.)

## Limitations

- The key claim of >30% performance improvement hinges on the assumption that human participants possess private, complementary information not captured by the AI's state representation, which is assumed rather than directly measured
- The evaluation uses a single game domain (wildfire mitigation), limiting generalizability to other sequential decision-making contexts
- The Lipschitz continuity assumption for the reward function, while mathematically proven, may not hold in all real-world sequential decision-making contexts
- The paper doesn't analyze which conditions (AI quality, human expertise, task stochasticity) maximize the complementarity gap

## Confidence

- **High confidence:** The algorithmic framework (Lipschitz bandit optimization of ε) is sound and the theoretical guarantees are valid given the stated assumptions
- **Medium confidence:** The simulation results and performance claims are reproducible based on the code release, but the 30% improvement claim requires human subject validation which cannot be fully replicated computationally
- **Medium confidence:** The mechanism of narrowing action sets to leverage human complementary knowledge is plausible and supported by related literature, but direct causal evidence linking this specific implementation to the observed performance gains is limited

## Next Checks

1. **Direct complementarity measurement:** Design an experiment to empirically test whether humans consistently make better decisions when presented with AI-narrowed action sets versus the full action space, controlling for AI agent performance.

2. **Cross-domain validation:** Implement the framework in a different sequential decision-making domain (e.g., resource allocation or inventory management) to test the generalizability of the approach beyond wildfire mitigation.

3. **Ablation on noise parameter:** Systematically vary the noise parameter σ to quantify the tradeoff between optimization smoothness and immediate action set quality, validating the claim that randomization is necessary for efficient optimization.