---
ver: rpa2
title: 'LoaQ: Layer-wise Output Approximation Quantization'
arxiv_id: '2509.06297'
source_url: https://arxiv.org/abs/2509.06297
tags:
- quantization
- loaq
- gptq
- gptaq
- qronos
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes LoaQ, a layer-wise post-training quantization
  method that addresses the limitation of existing approaches by explicitly matching
  sub-block outputs, including residual connections and RMSNorm, rather than just
  linear-layer outputs. The key idea is to introduce correction terms that compensate
  for quantization errors accumulated through residual connections and to normalize
  these corrections for alignment with the model's normalization layers.
---

# LoaQ: Layer-wise Output Approximation Quantization

## Quick Facts
- arXiv ID: 2509.06297
- Source URL: https://arxiv.org/abs/2509.06297
- Reference count: 22
- Primary result: LoaQ achieves state-of-the-art 2-bit and 3-bit quantization performance on LLaMA and Qwen models, outperforming GPTAQ and Qronos across perplexity and accuracy benchmarks

## Executive Summary
This paper proposes LoaQ, a layer-wise post-training quantization method that addresses the limitation of existing approaches by explicitly matching sub-block outputs, including residual connections and RMSNorm, rather than just linear-layer outputs. The key idea is to introduce correction terms that compensate for quantization errors accumulated through residual connections and to normalize these corrections for alignment with the model's normalization layers. Experiments on LLaMA and Qwen model families demonstrate that LoaQ consistently outperforms strong baselines such as GPTAQ and Qronos across 2-bit and 3-bit channel-wise quantization, as well as weight-activation quantization settings. It also integrates effectively with techniques like the Hadamard transform and NeUQI, further improving performance. The method achieves better perplexity and accuracy on benchmarks like WikiText2, C4, ARC, HellaSwag, and WinoGrande, showing its potential to advance post-training quantization, especially under low-bit configurations.

## Method Summary
LoaQ builds on the GPTQ framework by introducing layer-wise correction terms that explicitly match sub-block outputs including residual connections and RMSNorm normalization. The method computes correction terms C = X^T(X' - X) for input mismatch and Δ = H^{-1}X^T(h' - h) for residual error accumulation, incorporating these into weight updates via (I + αH^{-1}C)W for input layers and (I + αH^{-1}C)W + βΔ for output layers. The Hessian matrix H = X^TX is computed from quantized inputs with diagonal damping for numerical stability. The approach decouples quantization error propagation through residual connections and normalizes corrections to align with the model's RMSNorm layers, preventing error growth across depth.

## Key Results
- LoaQ achieves 214 perplexity on WikiText2 for LLaMA-2 7B at 2-bit quantization, compared to 6875 for GPTQ baseline
- Outperforms GPTAQ and Qronos by 1.2-2.8 perplexity points on WikiText2 across multiple model sizes (7B, 13B, 70B)
- Maintains competitive performance on zero-shot tasks (ARC, HellaSwag, WinoGrande) while achieving significant perplexity improvements
- Successfully integrates with complementary techniques like Hadamard transform and NeUQI for additional performance gains

## Why This Works (Mechanism)

### Mechanism 1: Input Mismatch Correction for Linear Layers
The method computes correction term C = X^T(X' - X) to compensate for the discrepancy between pre-quantization and post-quantization inputs. This term is incorporated into weight updates via (I + αH^{-1}C)W, converting the problem back to standard GPTQ optimization. The Hessian matrix H = X^TX is invertible with diagonal damping for numerical stability.

### Mechanism 2: Residual Connection Error Accumulation Compensation
When quantizing W_out, the loss function incorporates hidden state differences: L(Q) = ||(h + XQ) - (h' + X'W)||_F^2. This adds term H^{-1}X^T(h' - h) to weight updates, explicitly compensating for accumulated errors from earlier layers through residual connections.

### Mechanism 3: Normalization-Aware Output Matching
The loss function L(Q) = ||ρ(h + XQ) - ρ(h' + X'W)||_F^2 operates on column-wise RMS-normalized outputs. By decoupling the scaling factor R(h + XW) from Q (replacing Q with W inside the scaling function), the problem reduces to the same form as sub-block approximation, preventing error growth and better matching what downstream layers receive.

## Foundational Learning

- **Hessian-based layer-wise PTQ (GPTQ framework)**: LoaQ builds directly on GPTQ's LDLQ algorithm; understanding the row-wise sequential quantization with error compensation is prerequisite. *Quick check*: Can you explain why GPTQ uses the Cholesky decomposition H = LL^T and how the compensation matrix T = D^{-1}(L - D) propagates quantization error to unquantized rows?

- **Transformer residual structure and RMSNorm**: The method explicitly targets sub-block outputs comprising RMSNorm → module → residual addition; misunderstanding this architecture leads to incorrect loss formulations. *Quick check*: In a standard LLaMA-style transformer block, trace the data flow through attention and MLP sub-blocks—where exactly does RMSNorm apply and how does the residual connection affect error propagation?

- **Pseudoinverse and least-squares reformulation**: The paper reformulates output matching losses into GPTQ-compatible forms using X† = (X^TX)^{-1}X^T; grasping this transformation is essential for implementation. *Quick check*: Given loss L(Q) = ||XQ - X'W||_F^2, show how introducing the pseudoinverse yields an equivalent objective matching GPTQ's form.

## Architecture Onboarding

- **Component map**: Calibration pass → Collect X, X', h, h' for each layer → For W_in: Compute H, C → Update fW = (I + αH^{-1}C)W → GPTQ quantize → For W_out: Compute rescaled H, C, Δ → Update fW = (I + αH^{-1}C)W + βΔ → GPTQ quantize → Forward pass to refresh h, h' → Repeat for next layer

- **Critical path**: 1) Calibration data sampling (128 sequences × 2048 tokens from C4), 2) Per-layer correction term computation (C for input mismatch, Δ for residual errors), 3) Tuning parameter selection (α ∈ {0.1×i}, β ∈ {0.05×j}), 4) GPTQ quantization with modified weight matrix

- **Design tradeoffs**: α = 1 (full correction) vs. α ≈ 0.4–0.6 (partial): Full correction degrades performance; intermediate values optimal; NOA vs. SOA only: NOA helps early layers but effect diminishes with depth; SOA provides consistent error reduction; Overhead vs. accuracy: LoaQ adds ~10-15% quantization time vs. GPTAQ/Qronos

- **Failure signatures**: Perplexity > 1000 in 2-bit: Likely incorrect calibration data or missing residual compensation; Early-layer MSE spikes: Check NOA rescaling implementation (R(·) operator); Numerical instability in H^{-1}: Ensure diagonal damping (1% of mean diagonal value)

- **First 3 experiments**: 1) Replicate Table 1 (2-bit, LLaMA-2 7B) with GPTQ baseline to verify ~6875 → 214 perplexity reduction, 2) Ablate SOA and NOA separately to confirm Table 5 patterns on a smaller model, 3) Integrate Hadamard transform and compare against Table 2 baselines to test orthogonality claim

## Open Questions the Paper Calls Out
The authors explicitly state that the evaluation is restricted to the layer-wise PTQ framework, and the resulting conclusions may not generalize to methods outside the layer-wise PTQ setting. The paper does not provide theoretical justification for why normalization-aware output matching loses effectiveness with depth, identifying this as an empirical observation rather than a mechanistic explanation.

## Limitations
- The method's effectiveness is demonstrated primarily on RMSNorm-based transformer architectures, limiting generalizability to models with different normalization schemes
- The correction mechanisms rely on specific architectural assumptions (residual connections, RMSNorm placement) that may not transfer to all transformer variants
- The paper provides empirical validation but lacks rigorous theoretical analysis of convergence properties or quantization error bounds

## Confidence
- **High confidence**: Empirical improvements over GPTAQ and Qronos on standard benchmarks are well-documented and reproducible with clear implementation details
- **Medium confidence**: Claims about residual error accumulation and mitigation through correction terms are supported by experiments but rely on specific architectural assumptions
- **Low confidence**: Theoretical justification for why normalization-aware matching outperforms standard output approximation only in early layers is observational rather than explanatory

## Next Checks
1. Apply LoaQ to transformer variants without RMSNorm (e.g., standard LayerNorm models) to evaluate whether correction mechanisms generalize or require architectural adaptation
2. Derive analytical bounds on quantization error accumulation through residual connections and compare with empirical correction terms proposed in the paper
3. Implement correction mechanisms in a non-GPTQ layer-wise PTQ framework (e.g., AdaRound or ZeroQuant) to test generalizability beyond the LDLQ algorithm