---
ver: rpa2
title: 'GLUE: Gradient-free Learning to Unify Experts'
arxiv_id: '2512.22467'
source_url: https://arxiv.org/abs/2512.22467
tags:
- learning
- glue
- target
- experts
- spsa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GLUE (Gradient-free Learning to Unify Experts) addresses the problem
  of efficiently initializing target models in domain expansion scenarios where multiple
  pretrained expert models must be combined. Traditional methods either use heuristic
  weighting (data size, proxy metrics) or expensive full-gradient optimization via
  backpropagation.
---

# GLUE: Gradient-free Learning to Unify Experts

## Quick Facts
- **arXiv ID**: 2512.22467
- **Source URL**: https://arxiv.org/abs/2512.22467
- **Reference count**: 0
- **Primary result**: GLUE achieves up to 8.5% accuracy improvement over data-size weighting and 9.1% over proxy-metric selection across three datasets and three network architectures while requiring only two forward passes per SPSA iteration.

## Executive Summary
GLUE (Gradient-free Learning to Unify Experts) addresses the problem of efficiently initializing target models in domain expansion scenarios where multiple pretrained expert models must be combined. Traditional methods either use heuristic weighting (data size, proxy metrics) or expensive full-gradient optimization via backpropagation. GLUE introduces a gradient-free approach that optimizes convex combination weights of fixed experts using two-point SPSA updates, requiring only two forward passes per iteration without backpropagation.

The method learns low-dimensional mixture coefficients that produce a strong parameter prior for target-domain fine-tuning. Theoretical analysis shows GLUE's per-iteration computational advantage over full-gradient mixing and demonstrates stability in low-dimensional mixture spaces. Empirically, GLUE improves test accuracy by up to 8.5% over data-size weighting and 9.1% over proxy-metric selection across three datasets (CIFAR-10, SVHN, Imagenette) and three network architectures (ResNet-20, MobileNetV2, ViT), while matching or slightly exceeding full-gradient mixing performance within 1.4%. The resulting priors enable effective fine-tuning on target domains.

## Method Summary
GLUE combines K pretrained expert models by learning convex mixture coefficients α via gradient-free SPSA optimization. The target model is initialized as θ(α) = Σᵢαᵢθᵢ, where experts' parameters remain fixed. SPSA estimates gradients by perturbing α along random directions using only two forward passes per iteration, avoiding backpropagation. The learned mixture coefficients are then used as an initialization prior for target-domain fine-tuning. GLUE assumes compatible expert architectures and operates within the convex hull of expert parameters, providing computational advantages over full-gradient mixing while maintaining strong empirical performance.

## Key Results
- **Accuracy improvements**: Up to 8.5% better than data-size weighting and 9.1% better than proxy-metric selection across CIFAR-10, SVHN, and Imagenette
- **Computational efficiency**: Requires only two forward passes per SPSA iteration versus backpropagation-based methods
- **Architecture agnostic**: Works across ResNet-20, MobileNetV2, and 8-layer ViT architectures
- **Fine-tuning effectiveness**: Learned priors enable successful target-domain adaptation matching or exceeding full-gradient mixing within 1.4%

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Low-dimensional mixture space enables stable gradient-free optimization via SPSA
- **Mechanism**: Two-point SPSA estimates gradients by perturbing α along random direction u, evaluating loss at α±μu via two forward passes, and computing finite-difference update. Variance scales as (K−1)/(mK) rather than with parameter dimension P.
- **Core assumption**: The number of experts K remains small enough that SPSA variance stays manageable.
- **Evidence anchors**:
  - [abstract] "learns the mixture coefficients of this combination via gradient-free two-point SPSA updates, requiring only two forward passes per step"
  - [Section 3.2] "Since our optimization variable is the K-dimensional mixture, and zeroth-order estimates are accurate in lower dimensions, we estimate the gradient... by using a two-point simultaneous perturbation stochastic approximator"
  - [Proposition 1] "The variance of the two-point (SPSA) gradient estimator is upper bounded as: E[∥ĝ−g∥²] ≤ (K−1)/(mK) σ_max(Θ)²∥∇_θ L∥² + O(μ²)"
- **Break condition**: When K (number of experts) grows large without increasing m, variance destabilizes updates.

### Mechanism 2
- **Claim**: Convex combination of fixed expert parameters produces a strong initialization prior
- **Mechanism**: θ(α) = Σᵢ αᵢθᵢ blends expert parameters while keeping experts frozen. Only α is optimized, constraining θ(α) to the convex hull.
- **Core assumption**: The target-domain optimum lies within or near the convex hull of expert parameters.
- **Evidence anchors**:
  - [abstract] "initializes the target model as a convex combination of fixed experts"
  - [Section 3.1] "Throughout, we only optimize the low-dimensional mixing coefficients α; the expert parameters {θᵢ} remain fixed"
  - [Section 6] Explicitly states limitation: "convex-hull restriction (the target optimum may lie outside the experts' span)"
- **Break condition**: When target domain requires capabilities not represented in any expert's training distribution.

### Mechanism 3
- **Claim**: Forward-only computation provides per-iteration cost advantage over backpropagation-based mixing
- **Mechanism**: Each SPSA step costs 2(F + C_mix) vs. (1 + γ)F + C_mix + D_α for full-gradient, where γ ≥ 2 is backward-to-forward cost ratio. Savings grow as network depth increases.
- **Core assumption**: Forward pass cost F dominates parameter blending cost C_mix.
- **Evidence anchors**:
  - [abstract] "requiring only two forward passes per step"
  - [Section 4.1] "T_full = (1+γ)F + C_mix + D_α, T_SPSA = 2(F + C_mix)" with condition "C_mix < (γ−1)F + D_α"
  - [corpus] Related test-time adaptation work (arxiv 2510.11068) similarly seeks backpropagation-free methods for edge deployment
- **Break condition**: When parameter dimension P and expert count K make C_mix = O(PK) competitive with forward pass cost.

## Foundational Learning

- **Concept: Zeroth-order optimization (SPSA)**
  - Why needed here: Core mechanism for gradient-free learning; understanding why it works in low dimensions but struggles in high dimensions.
  - Quick check question: If you double the number of experts K, how does SPSA variance change if you keep m=1?

- **Concept: Convex hull and parameter interpolation**
  - Why needed here: GLUE constrains solutions to convex combinations; understanding what this geometric restriction implies.
  - Quick check question: Can θ(α) ever produce a parameter value larger than max(θᵢ) across all experts?

- **Concept: Transfer learning initialization priors**
  - Why needed here: GLUE's output is an initialization, not a final model; understanding why a better prior improves fine-tuning.
  - Quick check question: Why might a learned mixture outperform data-size weighting even before fine-tuning?

## Architecture Onboarding

- **Component map**: Expert bank → Mixture coefficients α → SPSA optimizer → Blended model θ(α) → Forward passes → Loss computation → α update
- **Critical path**:
  1. Initialize α = (1/K, ..., 1/K) or β = 0
  2. Sample minibatch B from target domain
  3. Sample direction u, compute θ(α±μu)
  4. Forward pass both blends → L+, L−
  5. Update: β ← β − η · ((L+−L−)/(2μ)) · u
  6. Repeat until validation plateaus
  7. Output θ(α) for fine-tuning
- **Design tradeoffs**:
  - Perturbation radius μ: Large μ biases estimate; small μ amplifies noise
  - Direction samples m: More samples reduce variance but multiply forward passes
  - Expert count K: More experts increase coverage but scale variance as (K−1)/K
- **Failure signatures**:
  - No convergence / oscillation: μ too large or learning rate η too high
  - Slow progress: μ too small, or experts highly misaligned
  - Fine-tuning fails to improve: Target outside convex hull; check individual expert performance on target
  - High variance across runs: Increase m or reduce K
- **First 3 experiments**:
  1. **Sanity check**: K=3 experts on CIFAR-10 splits with ResNet-20; compare uniform vs. data-size vs. GLUE (m=1, μ=0.1, η=0.01). Verify forward-only works and matches or exceeds baselines.
  2. **Hyperparameter sweep**: Fix K=5, sweep μ ∈ {0.01, 0.05, 0.1, 0.5} and m ∈ {1, 2, 4}. Plot convergence speed and final accuracy to find stable region.
  3. **Scaling analysis**: Vary K ∈ {3, 5, 10, 20} with fixed total training budget. Measure when variance degrades performance vs. full-gradient baseline to identify practical expert count limits.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does GLUE scale to scenarios with many more experts (e.g., K ≫ 10), given that the two-point estimator variance grows with K?
- **Basis in paper**: [explicit] The Discussion section identifies "variance growing with K regardless of classical SPSA convergence-to-stationary-point guarantees" as a limitation. Proposition 1 bounds variance with a K-dependent term, yet experiments only evaluate K = 10.
- **Why unresolved**: The theoretical variance bound suggests potential degradation with more experts, but no empirical validation exists for larger expert pools common in production systems.
- **What evidence would resolve it**: Empirical evaluation of GLUE convergence speed and final accuracy with K ∈ {20, 50, 100} experts, comparing against full-gradient baselines.

### Open Question 2
- **Question**: Can GLUE be extended to handle heterogeneous expert architectures or misaligned preprocessing pipelines?
- **Basis in paper**: [explicit] The paper states "GLUE assumes that the architectures are compatible, meaning they share backbones and heads" and lists "possible misalignment across experts (objectives or preprocessing)" as a limitation.
- **Why unresolved**: Real-world deployments often involve diverse model families, but GLUE requires compatible parameter dimensions for convex combination.
- **What evidence would resolve it**: Experiments combining experts with different architectures (e.g., ResNet + ViT experts), or investigating parameter projection/alignment techniques before mixing.

### Open Question 3
- **Question**: Does allowing extrapolation beyond the convex hull improve target-domain performance when the optimal solution lies outside the experts' span?
- **Basis in paper**: [explicit] The paper identifies "the convex-hull restriction (the target optimum may lie outside the experts' span)" as a limitation.
- **Why unresolved**: The softmax parameterization constrains solutions to convex combinations; whether performance gains exist from relaxing this constraint remains untested.
- **What evidence would resolve it**: Modified GLUE with unbounded mixture coefficients (allowing negative weights or weights > 1), evaluated on domains significantly shifted from all expert training distributions.

### Open Question 4
- **Question**: How do SPSA hyperparameters (perturbation radius μ, number of directions m) interact with expert diversity and loss landscape curvature?
- **Basis in paper**: [inferred] The paper notes "hyperparameter sensitivity of SPSA (too large radius μ biases the estimate; too small increases noise)" but uses fixed m = 1 across all experiments without systematic ablation.
- **Why unresolved**: The variance bound in Proposition 1 depends on σ_max(Θ) (expert alignment), suggesting hyperparameter needs may vary with expert diversity.
- **What evidence would resolve it**: Ablation studies varying μ and m across different Dirichlet concentration parameters δ (expert data overlap levels) with analysis of convergence stability.

## Limitations

- **Convex-hull restriction**: The target optimum may lie outside the experts' span, limiting GLUE's ability to access parameter regions not representable as convex combinations of expert parameters.
- **SPSA variance scaling**: Theoretical variance bound grows with expert count K, potentially degrading performance in scenarios with many experts.
- **Architecture compatibility**: GLUE requires compatible expert architectures (shared backbones and heads), limiting applicability to heterogeneous model families.

## Confidence

- **High confidence**: The core methodology (convex parameter blending + SPSA optimization) is sound and the empirical improvements (8.5-9.1% over baselines) are substantial and well-documented across multiple datasets and architectures. The forward-only computation advantage is theoretically justified when γ > 1.
- **Medium confidence**: The SPSA variance scaling analysis and the claimed computational efficiency gains depend on unspecified hyperparameters (μ, m) that could affect practical performance. The paper demonstrates effectiveness but doesn't explore the full hyperparameter space or identify clear failure modes beyond the convex-hull limitation.
- **Low confidence**: The exact computational cost comparison between GLUE and full-gradient mixing would require precise measurements of F, C_mix, and D_α values, which aren't provided. The claim that GLUE "matches or slightly exceeds" full-gradient mixing within 1.4% needs verification across different expert count regimes.

## Next Checks

1. **SPSA hyperparameter sensitivity**: Systematically sweep μ ∈ {0.01, 0.05, 0.1, 0.5} and m ∈ {1, 2, 4} on CIFAR-10 with K=5 experts to identify stable operating regions and quantify variance reduction.

2. **Expert count scaling limits**: Test K ∈ {3, 5, 10, 20} experts with fixed total training budget to empirically measure when SPSA variance degrades performance relative to full-gradient mixing, validating the theoretical (K-1)/K scaling.

3. **Convex-hull boundary analysis**: On SVHN, train experts on extreme subsets (e.g., only 0-4 vs only 5-9 digits) and test whether GLUE can produce effective mixtures for digits requiring mixed capabilities, quantifying the practical impact of the convex-hull restriction.