---
ver: rpa2
title: 'Geometric Regularization in Mixture-of-Experts: The Disconnect Between Weights
  and Activations'
arxiv_id: '2601.00457'
source_url: https://arxiv.org/abs/2601.00457
tags:
- regularization
- weight
- expert
- arxiv
- activation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether geometric regularization of expert
  weights improves diversity and performance in MoE models. The core method applies
  orthogonality loss to expert weight matrices to enforce diversity.
---

# Geometric Regularization in Mixture-of-Experts: The Disconnect Between Weights and Activations

## Quick Facts
- arXiv ID: 2601.00457
- Source URL: https://arxiv.org/abs/2601.00457
- Reference count: 4
- Primary result: Weight-space orthogonality regularization fails to improve MoE diversity or performance, increasing weight overlap by 114% while activation overlap remains high (~0.6)

## Executive Summary
This paper investigates whether geometric regularization of expert weights can improve diversity in Mixture-of-Experts models. The core method applies orthogonality loss to expert weight matrices, but the primary finding is that this approach fails on multiple fronts: it increases weight-space overlap rather than reducing it, activation-space overlap remains consistently high regardless of regularization, and performance effects are inconsistent across datasets. The analysis reveals no significant correlation between weight and activation orthogonality, demonstrating that weight-space regularization neither achieves its geometric goal nor reliably improves performance.

## Method Summary
The paper evaluates orthogonality regularization in MoE language models using NanoGPT-MoE architecture (~130M parameters, 8 experts, 6 layers, top-2 routing). The orthogonality loss is applied to expert up-projection weights using Frobenius inner product with weight λ. Experiments span three datasets (TinyStories, WikiText-103, PTB) with varying seeds, measuring validation perplexity, weight MSO, activation MSO, and their correlation. The study systematically sweeps λ values to assess regularization effects on geometric diversity and model performance.

## Key Results
- Weight MSO increases by up to 114% under orthogonality regularization (from 5.43×10⁻⁴ to 1.16×10⁻³ at λ=0.01)
- Activation-space overlap remains high (~0.6) regardless of regularization strength
- No significant correlation between weight and activation orthogonality (r = -0.293, p = 0.523)
- Performance effects are inconsistent: marginal improvement on WikiText-103 (-0.9%), slight degradation on TinyStories (+0.9%), high variance on PTB (std > 1.0)

## Why This Works (Mechanism)

### Mechanism 1: Weight-Activation Geometry Disconnect
Orthogonality constraints on static parameters don't propagate to functional outputs through non-linear layers. Expert computation applies SiLU (element-wise gating dependent on magnitude) and LayerNorm (re-centering/re-scaling to unit variance), which compress angular differences between expert outputs regardless of weight geometry. Frobenius orthogonality ⟨W₁, W₂⟩_F = 0 only constrains the trace of W₁^T W₂, not its full structure—so activation inner products x^T W₁^T W₂ x remain generally non-zero.

### Mechanism 2: Regularization Interference with Training Dynamics
Explicit orthogonality loss can increase rather than decrease weight-space overlap. The regularization term competes with the language modeling objective. As λ increases, gradient updates prioritize orthogonality over task-relevant learning, destabilizing natural expert differentiation that gradient descent would otherwise achieve.

### Mechanism 3: Dataset-Scale Variability
Regularization effects are unstable on smaller datasets due to seed-dependent expert specialization patterns. Smaller corpora provide fewer gradient signals per expert, making routing and specialization more sensitive to initialization. This amplifies variance across seeds (std > 1.0 on PTB vs. ~0.05 on WikiText-103).

## Foundational Learning

- **Mean Squared Overlap (MSO)**: Core metric for measuring geometric diversity; paper's central finding is the 1000× gap between weight MSO (~10⁻⁴) and activation MSO (~0.6). Quick check: If two expert weight matrices have MSO = 0.001, what does that imply about their activation MSO? (Answer: Nothing predictable—paper shows no significant correlation.)

- **Top-k Routing in MoE**: Activation MSO is computed only over co-activated experts (top-2 selected per token); understanding routing is essential to interpreting overlap measurements. Quick check: Why does activation MSO measure only selected experts rather than all experts? (Answer: Only selected experts contribute to outputs; measuring all would include irrelevant pairs.)

- **Frobenius Inner Product**: Paper's orthogonality loss uses flattened weight vectors; understanding why trace(W₁^T W₂)=0 doesn't guarantee W₁^T W₂=0 explains the disconnect. Quick check: If W₁ and W₂ are Frobenius-orthogonal, is x^T W₁^T W₂ x necessarily zero for arbitrary inputs? (Answer: No—only the sum of eigenvalues is constrained.)

## Architecture Onboarding

- **Component map**: Input x → Router (top-2 selection) → Selected experts process in parallel → Each expert: W_up projection → SiLU activation → LayerNorm → output h_i → Orthogonality loss: L_orth = Σ_{i<j} |⟨W̃_i, W̃_j⟩|² added to LM loss with weight λ

- **Critical path**: Router selects experts → Expert weights transform input → SiLU+LayerNorm break geometric relationships → MSO_act computed on expert outputs

- **Design tradeoffs**: Weight-space regularization: Easy to compute, but paper shows it's ineffective; Activation-space regularization: May be more appropriate but requires forward passes during training; No auxiliary load balancing: Paper doesn't use load balancing loss—isolates geometric effects

- **Failure signatures**: Weight MSO increasing under orthogonality regularization (sign of training dynamics interference); Activation MSO ~0.6 regardless of λ (sign of non-linearity dominance); High perplexity variance across seeds (sign of instability, especially on small datasets)

- **First 3 experiments**: 
  1. Reproduce the MSO gap: Train baseline MoE, measure weight MSO vs. activation MSO per layer—expect 1000× ratio, larger gap in early layers
  2. Test regularization interference: Sweep λ ∈ {0, 0.001, 0.01, 0.1}, plot weight MSO trajectory—expect non-monotonic increase rather than decrease
  3. Validate activation-space regularization (follow-up): Implement MSO_act as a loss term directly penalizing activation overlap during training—compare to weight-space approach

## Open Questions the Paper Calls Out

### Open Question 1
Does the weight-activation gap persist at larger model scales (1B+ parameters) and across different MoE architectures? Basis: Experiments only used NanoGPT-MoE (~130M parameters, 8 experts, 6 layers). Scaling behavior may differ due to changes in capacity, routing dynamics, or implicit regularization effects. Evidence needed: Replicate weight-activation correlation analysis on larger MoE models (Mixtral-8x7B, DeepSeek-MoE) across regularization strength sweep.

### Open Question 2
Can activation-space regularization (directly penalizing MSOact during training) enforce functional diversity where weight-space regularization fails? Basis: Paper only tested weight-space orthogonality loss. Activation-space regularization requires backpropagating through expert outputs during training. Evidence needed: Compare MSOact and perplexity between weight-regularized and activation-regularized MoE models across multiple seeds and datasets.

### Open Question 3
Why do later layers develop higher weight MSO (less orthogonal weights) compared to early layers, and does this relate to functional specialization? Basis: Later layers (4–5) show smaller gaps (~300–450×) due to higher weight MSO (~10× higher than early layers). Evidence needed: Layer-wise analysis correlating weight MSO with expert selection patterns, gradient norms, and task-specific performance ablations.

### Open Question 4
What mathematical mechanisms cause SiLU and LayerNorm to compress angular differences between expert outputs, breaking the weight-activation geometry relationship? Basis: Paper provides intuition (element-wise gating, re-centering) but not formal analysis. Evidence needed: Theoretical analysis of angular distortion under SiLU+LayerNorm, validated against controlled experiments with synthetic inputs of known structure.

## Limitations
- The paper doesn't test activation-space regularization, leaving open whether the disconnect is fundamental or indicates the wrong regularization target
- No investigation of alternative geometric constraints (spectral regularization, manifold-based approaches) that might achieve desired diversity
- Limited architectural exploration—doesn't vary expert hidden dimension or intermediate projection size to understand how the gap scales with architecture

## Confidence
- **High Confidence**: Empirical findings that weight MSO increases under orthogonality regularization (up to +114%) and that activation MSO remains ~0.6 regardless of λ are directly measurable and reproducible
- **Medium Confidence**: Interpretation that the disconnect is "fundamental" due to SiLU+LayerNorm operations compressing angular differences is plausible but not rigorously proven
- **Low Confidence**: Claim that regularization "destabilizes natural expert differentiation" assumes baseline's near-orthogonality is optimal, but this isn't validated against alternative training objectives

## Next Checks
1. Implement activation-space regularization (MSO_act as direct loss term) and measure whether this achieves lower activation overlap while improving performance compared to weight-space regularization
2. Systematically vary expert hidden dimensions (256, 512, 1024) and intermediate projection sizes to quantify how the weight-activation gap scales with architectural choices
3. Measure weight and activation MSO separately for each MoE layer to determine whether the gap varies across depth, identifying whether early layers exhibit stronger disconnects