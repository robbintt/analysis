---
ver: rpa2
title: Leveraging Decoder Architectures for Learned Sparse Retrieval
arxiv_id: '2504.18151'
source_url: https://arxiv.org/abs/2504.18151
tags:
- sparse
- retrieval
- input
- decoder
- encoder-decoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effectiveness of learned sparse retrieval
  (LSR) across different transformer architectures (encoder-only, decoder-only, and
  encoder-decoder) and model scales. The study reveals that large language models
  struggle to generate effective sparse representations in zero-shot settings, often
  producing inappropriate term expansions that harm retrieval performance.
---

# Leveraging Decoder Architectures for Learned Sparse Retrieval

## Quick Facts
- arXiv ID: 2504.18151
- Source URL: https://arxiv.org/abs/2504.18151
- Reference count: 40
- Primary result: Large language models struggle with zero-shot learned sparse retrieval, requiring supervised fine-tuning to generate effective sparse representations.

## Executive Summary
This paper systematically investigates learned sparse retrieval (LSR) across transformer architectures (encoder-only, decoder-only, and encoder-decoder) and model scales. The study reveals that encoder-decoder architectures with multi-tokens decoding approach outperform other architectures by effectively aggregating lexical information across input sequences. While decoder-only models perform worse than encoder-only models in zero-shot settings, they show potential to outperform when scaled to larger parameter sizes. The research demonstrates that supervised fine-tuning is necessary for LLMs to create meaningful sparse representations for retrieval tasks, as zero-shot prompting often produces inappropriate term expansions that harm performance.

## Method Summary
The paper proposes a framework for learned sparse retrieval using different transformer architectures, with a focus on encoder-decoder models with multi-tokens decoding. The key innovation is the EncDec.MultiTokens architecture where the decoder receives both the encoder output and the full input text, avoiding single-token bottlenecks. The method uses knowledge distillation from cross-encoder teachers via MarginMSE loss with FLOPs regularization to train sparse representation heads (MLP, MLM-SingleToken, or MLM-MultiTokens). Models are trained on MS MARCO passage ranking data with 40M triplets augmented with hard negatives, and evaluated on MS MARCO Dev and TREC DL 2019/2020 benchmarks.

## Key Results
- Encoder-decoder architecture with multi-tokens decoding outperforms encoder-only and decoder-only backbones for LSR
- Large language models require supervised fine-tuning; zero-shot prompting fails due to inappropriate term expansions
- Decoder-only models show potential to outperform encoder-only models when scaled to larger parameter sizes
- MLM-MultiTokens head (max-pooling logits across all input tokens) outperforms MLM-SingleToken and MLP heads for encoder-decoder backbones

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Encoder-decoder architectures with multi-tokens decoding produce more effective sparse representations than encoder-only or decoder-only backbones for learned sparse retrieval.
- **Mechanism**: The multi-tokens approach copies the full input sequence to both encoder and decoder (prepended with start token), enabling the decoder to aggregate lexical information from all input positions rather than being bottlenecked by a single start-token representation. This hybrid attention—bidirectional in encoder, causal in decoder—captures different semantic dependency patterns.
- **Core assumption**: The decoder's hidden states across all input tokens contain complementary lexical signal that max-pooling can aggregate into a stronger sparse representation than single-vector bottlenecks.
- **Evidence anchors**:
  - [abstract]: "the encoder-decoder architecture with a multi-tokens decoding approach outperforms other architectures, effectively aggregating lexical information across the input sequence"
  - [section 2.1]: Equation 6 and description of EncDec.MultiTokens; Table 2 row 6 vs rows 1-2 shows EncDec.MultiTokens outperforming encoder-only baselines on NDCG@10 with lower FLOPs
  - [corpus]: Weak corpus signal—neighbor papers discuss LSR efficiency and encoder/decoder comparisons but do not directly validate multi-tokens decoding; no direct replication evidence available
- **Break condition**: If decoder hidden states become redundant with encoder hidden states (e.g., due to weight tying or insufficient decoder depth), multi-token aggregation may yield diminishing returns over encoder-only pooling.

### Mechanism 2
- **Claim**: MLM-MultiTokens heads (max-pooling logits across all input tokens) outperform MLM-SingleToken and MLP heads for encoder-decoder backbones in LSR.
- **Mechanism**: The MLM-MultiTokens head computes term importance by taking the maximum log(1+ReLU(logit)) across all token positions, enabling term expansion beyond input vocabulary while aggregating strongest lexical signals. This contrasts with MLP heads that weight only present terms and single-token heads that bottleneck on one position's representation.
- **Core assumption**: Max-pooling captures the most salient term importance signal per vocabulary entry without dilution from noise at other positions.
- **Evidence anchors**:
  - [abstract]: "identifying challenges such as inappropriate term expansions and reduced performance due to the lack of expansion"
  - [section 2.2]: Equation 8 defines MLM aggregation; Table 3 rows 10-11 shows MLM-MultiTokens significantly outperforming MLM-SingleToken (36.8 vs 30.4 MRR@10)
  - [corpus]: No direct corpus evidence for MLM-MT vs MLM-ST comparison; neighbor papers focus on efficiency and long-document adaptations
- **Break condition**: If input sequences are very long with many noisy tokens, max-pooling may select spurious high-logit terms, requiring length-aware regularization.

### Mechanism 3
- **Claim**: Large language models require supervised fine-tuning with knowledge distillation to generate effective sparse representations; zero-shot prompting fails due to inappropriate term expansion or failure to expand.
- **Mechanism**: Zero-shot LLMs assign high weights to noisy or irrelevant vocabulary terms (e.g., "ray," "s," "mil" in Figure 1) because their pretraining objective does not align with retrieval-specific term weighting. Supervised distillation from a cross-encoder teacher aligns student sparse representations with ranking signals via MarginMSE loss, while FLOPs regularization encourages sparsity.
- **Core assumption**: The teacher's margin distribution provides a learnable signal that can be compressed into sparse lexical weights without dense embeddings.
- **Evidence anchors**:
  - [abstract]: "large language models struggle to generate effective sparse representations in zero-shot settings, often producing inappropriate term expansions that harm retrieval performance"
  - [section 5, Table 1]: Zero-shot FlanT5-xl with expansion drops Recall@1k from 34.0 to 16.5; OPT-2.7B zero-shot achieves only 13.7 MRR@10 vs 36.8 for supervised models
  - [corpus]: No corpus papers directly address zero-shot LSR failure modes
- **Break condition**: If teacher score distributions are too sharp (e.g., RankLLaMA-13B in Figure 3), distillation may fail for decoder-only students, requiring score normalization or alternative losses.

## Foundational Learning

- **Concept**: Sparse representations and inverted indexes
  - **Why needed here**: LSR maps text to weighted bags of terms indexed via inverted lists; understanding why sparsity enables efficient retrieval is foundational.
  - **Quick check question**: Can you explain why an inverted index over sparse vectors is more storage-efficient than a dense vector index like HNSW?

- **Concept**: Transformer attention patterns (bidirectional vs causal)
  - **Why needed here**: The paper leverages encoder bidirectional attention and decoder causal attention differently; misunderstanding this leads to incorrect backbone selection.
  - **Quick check question**: For a decoder-only model, why can't position i attend to position i+1, and how does this affect multi-token sparse representation aggregation?

- **Concept**: Knowledge distillation for retrieval (MarginMSE)
  - **Why needed here**: All effective LSR models in the paper use distillation; the loss formulation directly impacts what the student learns.
  - **Quick check question**: What does margin MSE optimize, and why might a teacher with a narrow score distribution harm student learning?

## Architecture Onboarding

- **Component map**: Text input -> Transformer backbone (Encoder-only / Decoder-only / Encoder-Decoder) -> Hidden states h₁:n -> Sparse representation head (MLP / MLM-SingleToken / MLM-MultiTokens) -> |V|-dimensional sparse vector -> Inverted index

- **Critical path**:
  1. Select backbone (recommend EncDec.MultiTokens with Flan-T5-base for initial experiments)
  2. Attach MLM-MultiTokens head to decoder hidden states (Equation 8)
  3. Prepare distillation data: teacher scores + hard negatives
  4. Train with ADAM, warmup 6000 steps, batch size 16, λ quadratic scheduling (1e-1 to 1e-4 range)
  5. Index encoded documents with Anserini; retrieve with encoded queries

- **Design tradeoffs**:
  - **Encoder-only vs Encoder-decoder**: EncDec.MultiTokens yields higher NDCG@10 but requires more complex data loading (copying input to decoder); encoder-only is simpler but lower effectiveness
  - **MLP vs MLM-MT head**: MLP is more efficient (no expansion, lower FLOPs) but less effective; MLM-MT enables expansion at efficiency cost
  - **Teacher scale**: Larger teachers (RankLLaMA-13B) help encoder-only but may hurt decoder-only due to score distribution mismatch (Table 4, rows 4-6)

- **Failure signatures**:
  1. **Zero-shot expansion failure**: Recall@1k drops dramatically (e.g., 34.0 → 16.5) when enabling expansion without fine-tuning (Table 1)
  2. **Single-token bottleneck**: EncDec.SingleToken underperforms MultiTokens by 6+ MRR@10 points (Table 3)
  3. **Sharp teacher distribution**: Decoder-only models degrade with RankLLaMA teacher (Table 4 row 6: DL2020 nDCG drops to 61.0)
  4. **Pretraining mismatch**: Flan-T5 encoder-only underperforms DistilBERT encoder-only (Table 2 row 3 vs row 2) because Flan-T5's MLM head was trained on decoder outputs

- **First 3 experiments**:
  1. **Reproduce EncDec.MultiTokens with Flan-T5-base** on MS MARCO dev split using MiniLM-L6-v2 teacher; verify MRR@10 ≈ 36.8 to confirm setup correctness
  2. **Ablate sparse head**: Compare MLM-MultiTokens vs MLP vs MLM-SingleToken on same backbone; expect MLM-MT > MLP > MLM-ST for encoder-decoder
  3. **Scale student test**: Train OPT-350M and OPT-1.3B decoder-only models; verify that scaling improves DL2019/2020 nDCG@10 as reported in Table 4 rows 3 vs 8

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does scaling the teacher model to RankLLaMA-13B degrade the performance of decoder-only student models?
- Basis in paper: [explicit] The authors note a performance decline when using a larger teacher for decoder-only models and state, "Further investigation is needed to fully understand this behavior."
- Why unresolved: The paper identifies the "sharp distribution" of teacher scores as a likely cause but does not isolate the mechanism or confirm if this is the sole factor.
- What evidence would resolve it: Ablation studies using teachers with varying score distributions or experiments using different distillation loss functions on decoder-only architectures.

### Open Question 2
- Question: How does the pre-training architecture of a backbone (e.g., encoder-decoder vs. encoder-only) impact the effectiveness of the sparse representation head during fine-tuning?
- Basis in paper: [inferred] The authors observe that Flan-T5-base performs poorly when used as an encoder-only backbone compared to DistilBERT, hypothesizing an "incompatibility" arising from its original encoder-decoder pre-training.
- Why unresolved: The paper documents the performance gap but does not experimentally validate whether this is due to weight initialization, attention mask differences, or other architectural constraints.
- What evidence would resolve it: Experiments fine-tuning models like Flan-T5 with frozen encoder/decoder weights or analyzing gradient flows during the early stages of LSR training.

### Open Question 3
- Question: Can regularization techniques other than FLOPs-based sparsity enable decoder-only models to handle sharp teacher score distributions effectively?
- Basis in paper: [inferred] The authors suggest that relying "solely on FLOPs regulation in the loss function" makes it challenging for the decoder-only architecture to generate sparse vectors when learning from the sharp distributions of RankLLaMA.
- Why unresolved: The study limits its regularization analysis to FLOPs, leaving the potential of auxiliary losses or sparsity constraints (e.g., L1 regularization) unexplored for this specific failure case.
- What evidence would resolve it: Comparing the retrieval performance of decoder-only models trained with FLOPs regularization against those trained with alternative sparsity-inducing methods on sharp teacher distributions.

## Limitations

- Zero-shot LSR performance is poor across all transformer architectures, requiring supervised distillation for effective sparse representations
- The study only tests decoder-only models up to 1.3B parameters, leaving uncertainty about performance at larger scales
- The mechanism explaining why decoder-only models specifically struggle with term expansion in zero-shot settings is not fully explored

## Confidence

- **High confidence**: Encoder-decoder with multi-tokens decoding outperforms other architectures (supported by Table 2 with clear statistical improvements)
- **Medium confidence**: MLM-MultiTokens head outperforms alternatives for encoder-decoder (supported by Table 3, though comparison lacks broader ablation studies)
- **Medium confidence**: Supervised fine-tuning is necessary for LLMs to generate effective sparse representations (well-supported by zero-shot vs fine-tuned comparisons, though the exact teacher-student alignment mechanism could be more detailed)

## Next Checks

1. **Teacher Distribution Analysis**: Replicate the zero-shot failure experiments and analyze the teacher score distributions used for distillation, particularly comparing MiniLM-L6-v2 vs RankLLaMA-13B teachers with decoder-only students to understand why sharp distributions harm decoder performance.

2. **Term Expansion Mechanism Study**: Conduct controlled experiments varying expansion vocabulary size and analyzing which specific terms decoder-only models incorrectly weight in zero-shot settings, comparing against encoder-only behavior to identify architectural differences in term selection.

3. **Scaling Boundary Test**: Train decoder-only LSR models at 2.7B and 6.7B parameters (beyond the 1.3B tested) to determine if the scaling trend continues and whether decoder-only models eventually match or exceed encoder-only performance at sufficient scale.