---
ver: rpa2
title: Moments Matter:Stabilizing Policy Optimization using Return Distributions
arxiv_id: '2601.01803'
source_url: https://arxiv.org/abs/2601.01803
tags:
- policy
- distribution
- returns
- critic
- return
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work tackles the challenge of instability in deep RL policy\
  \ updates caused by environmental and algorithmic stochasticity, which leads to\
  \ highly variable post-update return distributions (R(\u03B8)). To improve stability\
  \ without costly Monte Carlo estimation of R(\u03B8), the authors propose replacing\
  \ the standard critic with a distributional critic and using higher-order statistics\
  \ (skewness and kurtosis) of the return distribution to regularize advantage estimates\
  \ in PPO."
---

# Moments Matter:Stabilizing Policy Optimization using Return Distributions

## Quick Facts
- arXiv ID: 2601.01803
- Source URL: https://arxiv.org/abs/2601.01803
- Reference count: 2
- Primary result: Moment-based regularization using distributional critic narrows post-update return distribution by up to 75% in Walker2D

## Executive Summary
This work addresses instability in deep RL policy updates caused by high variance in post-update return distributions. The authors propose replacing the standard critic with a distributional critic and using higher-order statistics (skewness and kurtosis) to regularize advantage estimates in PPO. By penalizing policies that lead to highly asymmetric or heavy-tailed return distributions, the method significantly narrows R(θ) without requiring costly Monte Carlo estimation. Empirical results on four Brax continuous control tasks show substantial stability improvements, particularly in environments where critic variance poorly aligns with true return variance.

## Method Summary
The method replaces PPO's scalar critic with a 51-quantile distributional critic that approximates the full return distribution Z_ϕ(s,a). From this distribution, the algorithm computes skewness and kurtosis, then regularizes the advantage estimate by subtracting penalty terms proportional to these moments. The modified advantage Âₜ ← Âₜ − w_skew·Skew(Z_ϕ) − w_kurt·Kurt(Z_ϕ) biases policy updates away from regions with extreme asymmetry or heavy tails in returns. This approach stabilizes training by avoiding parameter regions prone to catastrophic or erratic outcomes, with the regularization strength controlled by hyperparameters w_skew and w_kurt.

## Key Results
- DPPO-Kurt achieves 84% reduction in R(θ) standard deviation (σ=196.06 vs 1246.18) in Walker2D
- Method particularly effective when critic-return variance alignment is weak (Walker2D r=0.20 vs HalfCheetah r=0.74)
- Maintains comparable performance to vanilla PPO while substantially improving stability
- DPPO-Kurt outperforms DPPO-Skew in 3 out of 4 environments tested

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weak alignment between post-update critic variance and actual return variance causes unstable PPO updates.
- Mechanism: When the critic's variance σ²(V(θ)) fails to track the true return variance σ²(R(θ)), advantage estimates become unreliable—either overly aggressive in high-variance regions or inappropriately conservative. The paper measures this via Pearson correlation r between σ²(R(θ)) and σ²(V(θ)); low r indicates the critic is "blind" to actual return variability.
- Core assumption: The correlation between critic variance and return variance is a meaningful proxy for advantage estimate reliability.
- Evidence anchors:
  - [abstract] "We hypothesize that in environments where post-update critic values align poorly with post-update returns, standard PPO struggles to produce a narrow R(θ)."
  - [Section 3, Hypothesis 1] Formal statement of weak alignment causing instability; Table 1 shows Walker2D has r=0.20 vs HalfCheetah r=0.74.
  - [corpus] Related work on distributional RL supports that distributional approaches can capture variability that scalar critics miss, but does not directly validate the alignment hypothesis.
- Break condition: If critic-return correlation is already high (e.g., HalfCheetah), this mechanism provides little benefit—vanilla PPO already yields narrow R(θ).

### Mechanism 2
- Claim: Penalizing skewness and kurtosis in the advantage function biases policy updates away from unstable parameter regions.
- Mechanism: The modified advantage estimate Âₜ ← Âₜ − w_skew·Skew(Z_ϕ) − w_kurt·Kurt(Z_ϕ) reduces the advantage for actions leading to asymmetric or heavy-tailed return distributions. High absolute skewness indicates imbalanced outcomes; high kurtosis signals frequent extremes. By penalizing both, the policy avoids regions prone to catastrophic or erratic returns.
- Core assumption: Higher-order moments of the state-action return distribution Z^π(s,a) predict regions where policy updates will produce high variance in R(θ).
- Evidence anchors:
  - [Section 4, Eq. 1] Explicit regularization formulas for skewness and kurtosis penalties.
  - [Table 2] DPPO-Kurt achieves σ=196.06 in Walker2D vs DPPO σ=1246.18 (84% reduction); DPPO-Skew achieves σ=226.82.
  - [corpus] D2C-HRHR paper similarly uses distributional critics for high-variance tasks, suggesting moment-based approaches are broadly applicable, but does not directly validate skew/kurtosis penalties.
- Break condition: If the return distribution is already near-Gaussian (low skewness, low excess kurtosis), penalties provide negligible gradient signal.

### Mechanism 3
- Claim: A distributional critic captures environmental stochasticity that scalar critics cannot, enabling moment-based regularization.
- Mechanism: The 51-quantile distributional critic Z_ϕ(s,a) approximates the full return distribution under policy π_θ. This allows computation of E[Z], σ(Z), Skew(Z), and Kurt(Z) analytically from the quantile representation, rather than relying on Monte Carlo sampling which is computationally prohibitive.
- Core assumption: The quantile-based approximation sufficiently captures the true return distribution's higher-order properties.
- Evidence anchors:
  - [Section 4, Distributional Critic Setup] References DPPO implementation with 51-quantile support distribution per Schneider et al. [2024].
  - [Table 2] DPPO alone (without moment penalties) does not narrow R(θ) compared to PPO—suggesting distribution modeling alone is insufficient without explicit regularization.
  - [corpus] Unleashing Flow Policies with Distributional Critics corroborates that expressive actors benefit from distributional critics, but notes critics can become bottlenecks—consistent with finding that distributional critics require proper regularization.
- Break condition: If quantile resolution is too coarse or the distribution is multimodal beyond the representation capacity, higher-order moment estimates become unreliable.

## Foundational Learning

- **Distributional Reinforcement Learning (Z^π vs V)**
  - Why needed here: The method replaces a scalar critic V(s) with a distributional critic Z_ϕ(s,a) that outputs a full return distribution. Without understanding quantile representations and how distributions are learned (typically via quantile regression loss), the architecture is a black box.
  - Quick check question: Given 51 quantile values τ₁...τ₅₁ representing a return distribution, how would you compute its mean and variance?

- **Higher-Order Statistics (Skewness, Kurtosis)**
  - Why needed here: The regularization terms directly compute Skew(Z) = E[(Z-μ)³]/σ³ and Kurt(Z) = E[(Z-μ)⁴]/σ⁴. Interpreting these as asymmetry and tail-heaviness measures is essential for tuning w_skew and w_kurt.
  - Quick check question: A return distribution has skewness = -2.0 and kurtosis = 8.0. What does this tell you about the likelihood of catastrophic vs. unexpectedly good returns?

- **PPO Advantage Estimation and Clipping**
  - Why needed here: The moment penalties are applied directly to the advantage estimate Âₜ before PPO's clipped objective. Understanding how advantage enters the policy gradient clarifies why modifying Âₜ affects which actions are reinforced.
  - Quick check question: In PPO, if Âₜ is systematically underestimated for certain actions, what happens to the probability of selecting those actions over training?

## Architecture Onboarding

- Component map:
  Environment → Policy π_θ(a|s) → Actions
       ↓
  Distributional Critic Z_ϕ(s,a) → 51 quantiles {τᵢ, qᵢ}
       ↓
  Moment Computation: E[Z], σ(Z), Skew(Z), Kurt(Z)
       ↓
  Advantage Regularization: Âₜ ← Âₜ − w_skew·|Skew| − w_kurt·Kurt
       ↓
  PPO Clipped Objective → Policy Update

- Critical path: The distributional critic must be sufficiently trained before moment estimates are reliable. If Z_ϕ is inaccurate early in training, skew/kurtosis penalties may reinforce wrong behaviors.

- Design tradeoffs:
  - **Quantile count (51)**: Higher counts improve moment accuracy but increase memory/compute. Paper uses 51 as default from prior work.
  - **w_skew vs w_kurt**: Paper finds kurtosis regularization often more effective (DPPO-Kurt narrower than DPPO-Skew in 3/4 environments), but optimal balance is environment-dependent.
  - **When to apply**: In high-alignment environments (HalfCheetah), vanilla PPO is preferable—regularization adds complexity without stability gains.

- Failure signatures:
  - **R(θ) remains wide despite regularization**: Check critic-return alignment; if r > 0.6, the mechanism is not the right fit.
  - **Performance drops significantly**: Penalty weights may be too aggressive; try reducing w_skew and w_kurt by 10×.
  - **Moment estimates unstable early in training**: Consider warmup period where regularization is disabled or annealed.

- First 3 experiments:
  1. **Alignment diagnostic**: Sample 1000+ minibatches, compute σ²(R(θ)) and σ²(V(θ)), calculate Pearson r. If r > 0.5, expect limited gains from this method.
  2. **Ablation on penalty weights**: Grid search w_skew, w_kurt ∈ {0.0, 0.01, 0.1, 1.0} on target environment; measure both σ(R(θ)) and final return. Kurtosis-only often sufficient as starting point.
  3. **Compare to DPPO baseline**: Run DPPO without moment penalties to confirm that distributional critic alone does not narrow R(θ)—validating that explicit regularization is the active ingredient.

## Open Questions the Paper Calls Out
- **Open Question 1**: Can a formal theoretical framework explain why weak alignment between critic variance and return variance causes instability, and why higher-order moment regularization mitigates this?
  - Basis in paper: [explicit] Future work states: "developing a formal theoretical framework for our two hypotheses"
  - Why unresolved: The paper empirically validates Hypothesis 1 and Hypothesis 2 but provides no formal proof or theoretical grounding for the mechanisms connecting alignment, moments, and stability.
  - What evidence would resolve it: A theoretical analysis showing bounds on R(θ) variance as a function of critic-return alignment and moment regularization, with empirical validation across diverse environments.

- **Open Question 2**: Why does kurtosis regularization consistently outperform skewness regularization in narrowing R(θ), and when might skewness be more beneficial?
  - Basis in paper: [inferred] The paper notes "DPPO-Kurt often produces a narrower distribution than DPPO-Skew" but offers no explanation for this observed pattern.
  - Why unresolved: The relative effectiveness of kurtosis vs. skewness penalization is empirically observed but not analyzed mechanistically.
  - What evidence would resolve it: Systematic ablation studies across environments with varying return distribution shapes, coupled with analysis of how each moment affects advantage estimation dynamics.

- **Open Question 3**: How does the method perform when exploration noise is enabled alongside minibatch stochasticity?
  - Basis in paper: [inferred] The paper explicitly sets "exploration hyperparameter off" to isolate minibatch effects, leaving the combined stochasticity regime untested.
  - Why unresolved: Real-world training involves both sources simultaneously; their interaction with moment-based regularization remains unknown.
  - What evidence would resolve it: Experiments comparing stability and performance with exploration noise both enabled and disabled across the same environments.

- **Open Question 4**: Does combining skewness and kurtosis regularization yield additive or synergistic benefits over single-moment approaches?
  - Basis in paper: [inferred] The paper evaluates DPPO-Kurt and DPPO-Skew separately but never tests a combined DPPO-Kurt-Skew variant.
  - Why unresolved: The two moments capture different distributional properties; their joint effect on policy optimization is unexplored.
  - What evidence would resolve it: Ablation experiments with joint regularization, analyzing whether the penalties interact constructively or interfere with each other.

## Limitations
- The method's efficacy is highly environment-dependent, tied to critic-return variance alignment (weak in Walker2D, strong in HalfCheetah)
- Choice of 51 quantiles is not empirically justified—could be over-parameterized for moment estimation
- Results limited to continuous control in Brax; applicability to discrete action spaces or visual-input domains untested
- No theoretical framework explains why moment regularization mitigates instability

## Confidence
- **High**: The distributional critic architecture and moment computation methodology are well-specified and reproducible
- **Medium**: The correlation between critic variance and return variance is a meaningful proxy for advantage reliability, but the threshold for "poor alignment" (r < 0.5) is heuristic
- **Low**: The relative importance of skewness vs. kurtosis penalties across environments is not rigorously explained—observed differences could be noise or task-specific

## Next Checks
1. **Alignment Diagnostic**: Compute critic-return variance correlation r across all four environments. Confirm the method only improves stability when r < 0.5, validating the alignment hypothesis.
2. **Quantile Resolution Ablation**: Repeat Walker2D experiments with 11, 21, and 51 quantiles. Measure moment estimation accuracy and stability gains to determine if 51 is necessary.
3. **Penalty Weight Sensitivity**: Systematically vary w_skew and w_kurt across orders of magnitude (10⁻³ to 10⁰). Identify if the observed superiority of kurtosis penalties holds across all weight ranges or is an artifact of the chosen values.