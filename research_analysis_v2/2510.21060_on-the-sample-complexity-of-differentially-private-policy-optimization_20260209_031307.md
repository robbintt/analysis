---
ver: rpa2
title: On the Sample Complexity of Differentially Private Policy Optimization
arxiv_id: '2510.21060'
source_url: https://arxiv.org/abs/2510.21060
tags:
- policy
- private
- have
- privacy
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides the first systematic theoretical study of differentially
  private policy optimization (DP-PO) with explicit focus on sample complexity. It
  introduces a suitable DP definition for PO, addresses unique challenges in on-policy
  learning, and presents a unified meta-algorithm for private variants of policy gradient
  (DP-PG), natural policy gradient (DP-NPG), and REBEL (DP-REBEL).
---

# On the Sample Complexity of Differentially Private Policy Optimization

## Quick Facts
- arXiv ID: 2510.21060
- Source URL: https://arxiv.org/abs/2510.21060
- Reference count: 40
- Key outcome: First systematic theoretical study of DP-PO with explicit sample complexity focus, showing privacy costs often manifest as lower-order terms.

## Executive Summary
This paper provides the first systematic theoretical study of differentially private policy optimization (DP-PO) with explicit focus on sample complexity. It introduces a suitable DP definition for PO, addresses unique challenges in on-policy learning, and presents a unified meta-algorithm for private variants of policy gradient (DP-PG), natural policy gradient (DP-NPG), and REBEL (DP-REBEL). The analysis shows that privacy costs often manifest as lower-order terms in the overall sample complexity, and highlights subtle algorithmic considerations. Experiments on CartPole-v1 confirm that DP-NPG with moderate privacy budgets performs near-optimally, while performance degrades with stricter privacy constraints as predicted by theory.

## Method Summary
The paper proposes a unified meta-algorithm for DP-PO that operates in a one-pass, batched manner where each "user" (context) is encountered only once. For DP-PG, it adds Gaussian noise to the REINFORCE gradient estimate. For DP-NPG, it reduces the update to a private least-squares regression problem using either the exponential mechanism or Gaussian mechanism. For DP-REBEL, it applies similar regression to Bellman residual minimization. The privacy mechanism scales with batch size to ensure sensitivity decreases with larger batches, and the analysis shows that for sufficiently large sample budgets, the privacy noise becomes a lower-order term compared to statistical estimation error.

## Key Results
- Privacy costs in DP-PO often manifest as lower-order additive terms in sample complexity bounds
- DP-NPG with moderate privacy budgets (ε=5) achieves near-optimal performance on CartPole-v1
- Sample complexity for DP-PG is O((√(dT)/Nε)^(2/3)) while DP-NPG is O(d/√N) plus privacy terms
- One-pass constraint is necessary for the stated privacy guarantees

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Treating the "user" (or context) as the unit of privacy enables valid differential privacy in on-policy learning.
- **Mechanism:** Standard sample-level DP fails in PO because altering one trajectory changes the policy, which changes the distribution of all subsequent trajectories. By defining privacy at the user level and enforcing one-pass batched algorithms, changing one user affects only one record per iteration, enabling parallel composition.
- **Core assumption:** Learning agent interacts with fixed set of users in one-pass manner, with sensitive information associated with user's initial state/context.
- **Evidence anchors:** Section 3 defines user-level privacy; Section 4 explains one-pass enables parallel composition; corpus includes empirical DP-PG work contrasting with theoretical unit definition.

### Mechanism 2
- **Claim:** Sample complexity cost of privacy often manifests as lower-order term compared to statistical estimation cost.
- **Mechanism:** Privacy noise sensitivity scales as O(1/m) with batch size m. By calibrating noise to this sensitivity, noise variance decreases as m grows. Analysis balances iterations T against batch size m, showing noise decays faster than statistical error for large N.
- **Core assumption:** Sample budget N is large enough to allow batch sizes that sufficiently dampen privacy noise without prematurely halting optimization.
- **Evidence anchors:** Theorem 2 shows O(1/√N) statistical vs O((√(d)/Nε)^(2/3)) privacy terms; Remark 4 states privacy is lower-order additive term; corpus supports general principle of noise scaling.

### Mechanism 3
- **Claim:** NPG convergence preserved under privacy by reducing to private least-squares regression problems.
- **Mechanism:** NPG updates require computing w_t minimizing squared error between advantage function and linear approximation. Instead of perturbing policy parameters, algorithm instantiates PrivLS oracle to solve regression privately, bounding policy update accuracy by regression error.
- **Core assumption:** Advantage function can be approximated by linear combination of score function, and private regression oracle provides bounded error guarantee.
- **Evidence anchors:** Section 6.1 shows regression formulation; Algorithm 3 shows PrivUpdate calling PrivLS; corpus validates feasibility of private regression oracles.

## Foundational Learning

- **Concept:** Differential Privacy (DP) Composition Theorems
  - **Why needed here:** To understand why one-pass constraint is strictly necessary; multi-pass would accumulate privacy loss via sequential composition.
  - **Quick check question:** Does the algorithm process data from a specific user u more than once across all iterations T?

- **Concept:** The Policy Gradient Theorem & Advantage Function
  - **Why needed here:** Mechanisms rely on estimating gradient ∇J(θ) = E[Aπ(x,y)∇logπ(y|x)]; understanding Aπ is target of regression in NPG shows where noise is injected.
  - **Quick check question:** In DP-NPG, is privacy noise added directly to policy weights θ, or during intermediate calculation of weight vector w_t?

- **Concept:** Fisher Information Matrix & Coverage
  - **Why needed here:** Paper assumes "Fisher-non-degenerate" policy class; if Fisher matrix is singular, NPG update is ill-defined and sample complexity bounds would explode.
  - **Quick check question:** Why does sample complexity in Theorem 3 scale inversely with constant γ (lower bound on Fisher information eigenvalues)?

## Architecture Onboarding

- **Component map:**
  - Meta-Algorithm (Algorithm 1) -> Controls loop, samples batches of users, calls PrivUpdate
  - PrivUpdate Oracle -> Modular privacy component
    - Instance A (DP-PG) -> Computes REINFORCE gradient + Gaussian Noise
    - Instance B (DP-NPG) -> Calls PrivLS to solve regression for w_t
  - Policy Class -> Parameterized model πθ (e.g., Neural Network or Softmax tabular)

- **Critical path:**
  1. Batch Generation: Sample m fresh users and collect trajectories using current policy πθt
  2. Target Estimation: Compute advantage estimates Ât (or rewards for REBEL)
  3. Private Oracle: Call PrivUpdate (add noise to gradient OR run private regression)
  4. Update: θt+1 ← θt + η · result

- **Design tradeoffs:**
  - Batch Size (m) vs. Iterations (T): For fixed N, increasing m reduces privacy noise but reduces T (fewer updates)
  - Privacy Budget (ε, δ): Stricter privacy (smaller ε) requires higher noise variance σ² or larger batches m

- **Failure signatures:**
  - Diminishing Returns with Small ε: If ε is too strict for given N, noise term O(1/ε) dominates statistical error, causing random behavior
  - On-Policy Drift Violation: If one-pass constraint is violated, stated (ε, δ) guarantees are invalid
  - Gradient Explosion: In DP-PG, if m is small and d is large, noise σ ∝ √(d)/mε may overwhelm gradient

- **First 3 experiments:**
  1. Baseline Utility Check: Implement DP-NPG on CartPole-v1 with ε=5, verify performance ≈ 500 reward
  2. Sensitivity to Batch Size: Run DP-PG with fixed ε=3 and varying m (10, 50, 100), plot noise-to-signal ratio
  3. Algorithmic Comparison: Compare DP-PG vs. DP-NPG on linear bandit task to verify NPG's better sample efficiency

## Open Questions the Paper Calls Out

- **Question:** Can sample complexity of DP-PO be improved by utilizing multi-pass sampling strategies rather than one-pass setting?
  - **Basis in paper:** Appendix I states current results focus only on one-pass sampling and suggests multi-pass may admit further improvements
  - **Why unresolved:** Current analysis relies on one-pass structure enabling parallel composition; multi-pass introduces complex privacy accounting not covered
  - **What evidence would resolve it:** Theoretical bounds for DP-PG/NPG in multi-pass showing better scaling with N, or lower bound proving one-pass optimal

- **Question:** Can bounded norm assumption for weight vectors (wt) in DP-NPG/REBEL be relaxed using techniques like three-point lemma?
  - **Basis in paper:** Section 6 notes dimension dependence arises partly because W depends on √(d) due to Gaussian noise; authors conjecture three-point lemma could avoid bounded wt requirement
  - **Why unresolved:** Current analysis requires bounded weight vectors to control policy update divergence, necessitating artificial truncation
  - **What evidence would resolve it:** Modified proof of convergence that doesn't require bounded wt, potentially improving dimension dependence

- **Question:** Can computationally efficient algorithms be developed for private PO with general function classes retaining statistical utility of exponential mechanism?
  - **Basis in paper:** Remark 6 states Algorithm 4 (exponential mechanism) is not computationally efficient and results for general classes are mainly statistical
  - **Why unresolved:** Exponential mechanism provides optimal statistical rates but is often computationally intractable; existing efficient oracles are restricted to specific cases
  - **What evidence would resolve it:** Polynomial-time algorithm for DP-NPG using general function approximation achieving sample complexity comparable to exponential mechanism bounds

## Limitations

- **One-pass constraint:** Analysis and privacy guarantees strictly require one-pass sampling, limiting applicability to settings requiring data reuse
- **Bounded gradient/reward assumptions:** Theoretical bounds rely on bounded gradients and rewards, which may not hold in practice without clipping
- **High-dimensional scaling:** While privacy costs are shown to be lower-order, this depends on sufficiently large sample budgets relative to dimension and privacy budget

## Confidence

- **Sample complexity bounds:** High confidence - derived from rigorous theoretical analysis with supporting empirical results
- **Privacy guarantees:** High confidence - assuming one-pass constraint is strictly enforced
- **Empirical utility:** Medium confidence - limited to single environment (CartPole-v1) and small number of seeds (3)
- **Theoretical extensions:** Medium confidence - open questions suggest limitations in current analysis framework

## Next Checks

1. **Verify Privacy Accounting Under Data Reuse:** Implement DP-NPG variant with experience replay, rigorously compute sequential composition privacy loss, compare actual ε achieved to claimed one-pass value

2. **Test Scalability to High Dimensions:** Apply DP-NPG to high-dimensional RL task (LunarLander-v2 or MuJoCo), measure empirical sample complexity, verify privacy cost remains lower-order term

3. **Benchmark Against Alternative Privacy Units:** Implement DP-PO variant protecting at sample level instead of user level, compare performance and privacy-utility tradeoff to validate necessity of user-level definition