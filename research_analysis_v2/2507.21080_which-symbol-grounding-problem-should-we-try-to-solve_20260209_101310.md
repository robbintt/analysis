---
ver: rpa2
title: Which symbol grounding problem should we try to solve?
arxiv_id: '2507.21080'
source_url: https://arxiv.org/abs/2507.21080
tags:
- problem
- grounding
- symbol
- what
- floridi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The author critiques Floridi and Taddeo\u2019s \u201Czero semantic\
  \ commitment\u201D condition for symbol grounding, arguing it cannot be satisfied\
  \ even by their own solution. He compares this with Luc Steels\u2019 interactive\
  \ robotics approach and concludes that only the \u201Ceasy problem\u201D of symbol\
  \ grounding\u2014explaining and reproducing the behavioral function of meaning in\
  \ artificial computational agents\u2014is a viable research target."
---

# Which symbol grounding problem should we try to solve?

## Quick Facts
- arXiv ID: 2507.21080
- Source URL: https://arxiv.org/abs/2507.21080
- Authors: Vincent C. Müller
- Reference count: 0
- Key outcome: The author critiques Floridi and Taddeo's "zero semantic commitment" condition for symbol grounding, arguing it cannot be satisfied even by their own solution. He compares this with Luc Steels' interactive robotics approach and concludes that only the "easy problem" of symbol grounding—explaining and reproducing the behavioral function of meaning in artificial computational agents—is a viable research target. The "hard problem," linking physics to meaning or consciousness, remains untouched by current AI approaches.

## Executive Summary
This paper examines which symbol grounding problem should be pursued in AI research by critiquing Floridi and Taddeo's "zero semantic commitment" (Z) condition and comparing different approaches to grounding. The author argues that the Z condition cannot be satisfied, even by Floridi's own proposed solution, because any meaningful symbol use requires pre-existing goals or directedness. He concludes that only the "easy problem" of symbol grounding—reproducing the behavioral function of meaning in artificial computational agents—is tractable, while the "hard problem" of explaining how physics gives rise to meaning remains untouched and may be ill-formed.

## Method Summary
The paper uses conceptual analysis and philosophical argumentation to examine the symbol grounding problem. The author analyzes Floridi and Taddeo's Z condition, tests whether their own solution violates it, evaluates Luc Steels' interactive robotics approach, and maps both approaches onto Chalmers's hard/easy problem distinction. The method involves formal analysis of existing frameworks, case studies of proposed solutions, and analogical reasoning to determine which grounding problems are tractable for AI research.

## Key Results
- Floridi and Taddeo's "zero semantic commitment" condition cannot be satisfied, not even by their own proposed solution
- Meaningful symbol use requires pre-existing goals or directedness in an agent
- Only the "easy problem" of symbol grounding (behavioral function of meaning) is tractable for AI research
- The "hard problem" of explaining how physics gives rise to meaning may be ill-formed if the hard problem of consciousness is ill-formed

## Why This Works (Mechanism)

### Mechanism 1: Goal-Directedness as Prerequisite for Semantics
- Claim: Meaningful symbol use requires pre-existing goals or directedness in an agent; systems without goals cannot develop semantics
- Mechanism: Goals create normativity (success/failure conditions), which enables the "right or wrong" use of symbols. Without goals, symbol manipulation remains purely syntactic regardless of sensorimotor coupling
- Core assumption: Semantics implies normativity—the ability to use symbols correctly or incorrectly relative to some standard
- Evidence anchors:
  - [section 2.1-2.2] "Without goals, there is no 'trying', nothing is 'better', and there is no 'success'... Either the system really is an agent... which implies having goals, or it is just a system that interacts with its environment—without goals."
  - [section 2.2] "It appears that having a semantics implies normativity, since without normativity we do not have the 'right or wrong' use of symbols, the 'failure or success' or referring"
  - [corpus] Related work on symbol grounding limits (Paper 6581) distinguishes internal meaning from external grounding, supporting the normativity-requirement view
- Break condition: If normativity can emerge from purely syntactic operations without pre-existing goals, the mechanism fails

### Mechanism 2: Easy vs. Hard Problem Separation
- Claim: Symbol grounding decomposes into two distinct problems with different solvability profiles
- Mechanism: The "easy problem" (reproducing behavioral function of meaning) is tractable through interactive robotics and cognitive science. The "hard problem" (how physics gives rise to meaning/consciousness) remains untouched and may be ill-formed
- Core assumption: This analogy to Chalmers' consciousness distinction is valid for grounding
- Evidence anchors:
  - [section 5.2] Easy problem: "How can we explain and re-produce the behavioral ability and function of meaning [and other intentional phenomena] in artificial computational agents?"
  - [section 5.3] Hard problem: "How does physics give rise to meaning?"—requires solving the hard problem of consciousness
  - [corpus] Paper 65368 shows grounding may emerge in language models at scale, but this addresses only behavioral/functional grounding, not the hard problem
- Break condition: If behavioral reproduction necessarily requires solving the hard problem, the separation collapses

### Mechanism 3: Zero Semantic Commitment Impossibility
- Claim: Floridi's Z condition (no innatism, no externalism, only agent-internal resources) cannot be satisfied by any proposed solution
- Mechanism: Any working system must have some pre-existing structure (goals, success criteria, or semantic resources) either built-in or externally supplied. The Z condition excludes phylogenetic development and any normativity source
- Core assumption: Goals/directedness count as semantic resources under the Z condition's constraints
- Evidence anchors:
  - [abstract] "I argue briefly that their condition cannot be fulfilled, not even by their own solution."
  - [section 2.2] Floridi's own action-based semantics "has no goals that it is trying to achieve"—yet without goals, no meaning emerges. If goals are added, Z condition is violated
  - [corpus] Paper 22781 (by same author) formalizes the "paradox of intentions" in computational grounding, reinforcing this critique
- Break condition: If proto-meaning can bootstrap to full meaning without goals, or if goals don't constitute semantic commitment, the critique weakens

## Foundational Learning

- Concept: Chinese Room Argument (Searle, 1980)
  - Why needed here: The traditional symbol grounding problem originates from Searle's claim that syntactic manipulation alone cannot produce understanding. This frames the entire debate
  - Quick check question: Can you explain why Searle believes the whole system (not just the symbol manipulator) fails to understand Chinese?

- Concept: Syntax vs. Semantics
  - Why needed here: The paper's core argument hinges on whether computational operations over symbols can ever produce meaning, or whether meaning requires something beyond formal manipulation
  - Quick check question: What distinguishes a purely syntactic system from one with genuine semantic content?

- Concept: Normativity in Symbol Systems
  - Why needed here: The author argues semantics requires normativity (right/wrong use). Understanding this clarifies why goal-less systems cannot ground symbols
  - Quick check question: Can a system have correct or incorrect symbol use without goals? What would "correctness" mean?

## Architecture Onboarding

- Component map:
  - Sensorimotor layer: Body, sensors, actuators for environment interaction (Steels' prerequisite)
  - Signal/pattern processing: Converts raw input to structured representations
  - Symbol storage/retrieval: Semiotic networks or knowledge structures
  - Goal/value system: Determines what counts as success/failure (critical, often implicit)
  - Association mechanism: Links symbols to actions/states (Floridi's object-level/meta-level architecture)

- Critical path:
  1. Define what the system should achieve (goals)—this is the normativity source
  2. Enable sensorimotor interaction with environment
  3. Create mechanism for associating internal states/symbols with actions
  4. Validate that "success" criteria produce stable symbol-object mappings

- Design tradeoffs:
  - Built-in goals vs. emergent goals: Built-in violates "zero commitment" but may be necessary; emergent is philosophically cleaner but mechanism unclear
  - Behavioral adequacy vs. genuine understanding: Easy problem solutions may pass Turing-style tests without solving hard problem
  - Individual vs. social grounding: Steels' approach uses multi-agent interaction; Floridi's focuses on single agent

- Failure signatures:
  - System produces outputs but no stable symbol-world mapping (symbols remain ungrounded)
  - Changing goal structure invalidates all previously grounded symbols (symbols were goal-relative)
  - System passes behavioral tests but no account of normativity exists (sunflower-like heliotropism, not semantics)

- First 3 experiments:
  1. Goal-ablation test: Remove or alter the goal structure in a "grounded" system and observe whether symbol meanings remain stable. If they collapse, goals were the grounding source
  2. Normativity probe: Introduce situations where the system could use symbols "incorrectly" relative to its goals. Does the system show error-detection or correction behavior?
  3. Cross-agent symbol transfer test: Can symbols grounded in one agent's goal structure transfer to another agent with different goals? This reveals whether grounding is system-relative

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the "zero semantic commitment" condition be satisfied while still providing the goal-directedness necessary for semantics?
- Basis in paper: [explicit] The author argues Floridi's Z condition "cannot be fulfilled, not even by their own solution" because directedness/goals appear to be a necessary condition for semantics, yet the Z condition excludes innate or external sources of such directedness
- Why unresolved: Floridi claims action-based semantics satisfies the Z condition, but Müller contends this either requires hidden goals (violating Z) or produces no genuine meaning (like sunflowers)
- What evidence would resolve it: A formal demonstration that semantic normativity can emerge from a system with zero pre-existing semantic resources, goal-orientation, or success criteria

### Open Question 2
- Question: Does solving the "hard problem" of symbol grounding require first solving the hard problem of consciousness?
- Basis in paper: [explicit] Müller states the hard problem of grounding—"How does physics give rise to meaning?"—"directly involves conscious experience" and "might be an ill-formed problem if the respective problem of consciousness is one"
- Why unresolved: The relationship between meaning and phenomenal consciousness remains philosophically contested; current AI approaches do not address this link
- What evidence would resolve it: Either a theoretical framework showing meaning can exist without consciousness, or empirical evidence that meaning necessarily involves phenomenal experience

### Open Question 3
- Question: What functional architecture and causal connections are necessary for artificial agents to have genuine intentional states?
- Basis in paper: [inferred] Müller cites Di Paolo et al.'s critique that current robotic systems "don't have a life" and lack "the right functional architecture, the right causal connections" for genuine intentional states
- Why unresolved: The field lacks consensus on minimal necessary conditions for intentionality in artificial systems beyond behavioral success
- What evidence would resolve it: Systematic comparison of architectures showing which structural properties (e.g., precarious self-maintenance, autopoiesis) correlate with measurable intentional capabilities

## Limitations
- The paper relies heavily on philosophical distinctions rather than empirical validation
- The separation between "easy" and "hard" problems of symbol grounding remains conceptual rather than operational
- The critique of Floridi's Z condition assumes that goals constitute semantic resources, but this assumption could be contested
- The paper does not address whether modern large language models represent exceptions to the easy/hard problem framework

## Confidence
- High Confidence: The claim that symbol grounding research should focus on the "easy problem" of reproducing behavioral meaning functions rather than the "hard problem" of explaining how physics gives rise to meaning
- Medium Confidence: The critique of Floridi and Taddeo's zero semantic commitment condition as unsatisfiable
- Low Confidence: The assertion that without pre-existing goals, no semantics can emerge

## Next Checks
1. Goal-ablation experiment: Systematically remove goal structures from grounded systems and test whether symbol meanings persist or collapse
2. Normativity detection protocol: Develop tests to identify whether artificial agents can detect and correct "incorrect" symbol use relative to internal goals
3. Cross-agent transfer validation: Test whether symbols grounded in one agent's goal structure can be meaningfully transferred to agents with different goal architectures