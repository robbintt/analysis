---
ver: rpa2
title: 'Bias in the Ear of the Listener: Assessing Sensitivity in Audio Language Models
  Across Linguistic, Demographic, and Positional Variations'
arxiv_id: '2602.01030'
source_url: https://arxiv.org/abs/2602.01030
tags:
- across
- language
- option
- accent
- gemini
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates speech bias in multilingual multimodal\
  \ large language models (MLLMs) by constructing BIASINEAR, a 70.8-hour speech-augmented\
  \ benchmark spanning English, Chinese, and Korean, balanced by gender and accent.\
  \ Using four metrics\u2014accuracy, entropy, APES, and Fleiss\u2019 \u03BA\u2014\
  across nine models, the analysis reveals that option order induces the strongest\
  \ robustness degradation, while accent and gender have smaller but consistent effects."
---

# Bias in the Ear of the Listener: Assessing Sensitivity in Audio Language Models Across Linguistic, Demographic, and Positional Variations

## Quick Facts
- **arXiv ID**: 2602.01030
- **Source URL**: https://arxiv.org/abs/2602.01030
- **Reference count**: 40
- **Key outcome**: Speech-augmented benchmark shows option order induces strongest robustness degradation in MLLMs, with architecture and reasoning complexity modulating bias expression.

## Executive Summary
This study investigates speech bias in multilingual multimodal large language models (MLLMs) by constructing BIASINEAR, a 70.8-hour speech-augmented benchmark spanning English, Chinese, and Korean, balanced by gender and accent. Using four metrics—accuracy, entropy, APES, and Fleiss’ κ—across nine models, the analysis reveals that option order induces the strongest robustness degradation, while accent and gender have smaller but consistent effects. Models show higher uncertainty on culturally sensitive questions, and larger-scale models demonstrate improved stability. Speech amplifies pre-existing text-based biases, particularly in language and option order sensitivity. Architectural design and reasoning complexity significantly affect robustness, with pipeline models and chain-of-thought prompting reducing bias. The findings highlight underexplored vulnerabilities in speech-integrated AI and offer practical insights for designing fairer, more stable multimodal systems.

## Method Summary
The study constructs BIASINEAR by augmenting Global MMLU Lite with TTS-generated audio spanning three languages, two accents per language, male/female voices, and original/reversed option orders. Text questions are rewritten to remove math/symbols, then synthesized using Gemini 2.5 Flash TTS. Nine MLLMs (end-to-end and pipeline variants) are evaluated under 28 perturbation configurations per question using temperature=0 inference. Robustness is measured via accuracy, Shannon entropy, Average Pairwise Entropy Shift (APES), and Fleiss’ κ for consistency across conditions.

## Key Results
- Option order causes the strongest robustness degradation, with accuracy gaps of 0.5-6.75% and consistently negative κ values
- Speech amplifies pre-existing text-based biases, with APES values increasing from 0.081→0.178 (language) and 0.096→0.221 (option order) under audio vs. text
- Chain-of-thought prompting improves agreement by 19.01-27.20% for gender, accent, and language perturbations
- Pipeline architectures show higher κ and lower APES than end-to-end models across all bias dimensions

## Why This Works (Mechanism)

### Mechanism 1: Option Order Sensitivity Drives Robustness Degradation
- **Claim**: Structural positioning of answer options causes the strongest inconsistency in model predictions across perturbations.
- **Mechanism**: MLLMs appear to encode positional biases where earlier options receive higher implicit attention weights or probability mass, causing systematic accuracy drops when order is reversed. This is structural rather than semantic sensitivity.
- **Core assumption**: The mechanism assumes that speech processing preserves positional encoding patterns from text-based training.
- **Evidence anchors**:
  - [abstract]: "analysis reveals that option order induces the strongest robustness degradation"
  - [section 4.3]: "accuracy gap between option order configurations ranges from 0.5% to 6.75%, with the original order consistently outperforming the reversed order"
  - [section 4.4]: "option order consistently emerges as the weakest factor across all models... with negative κ and relatively high APES"
- **Break condition**: Mechanism weakens if models are explicitly trained with randomized option ordering during instruction tuning, which would disrupt learned positional priors.

### Mechanism 2: Speech Modality Amplifies Pre-existing Text Biases
- **Claim**: Audio input systematically magnifies robustness sensitivities already present in text-based processing, rather than introducing qualitatively new bias patterns.
- **Mechanism**: Speech introduces additional acoustic variability (timing, prosody, signal processing artifacts) that compounds with existing text-level sensitivities, creating a multiplicative rather than additive effect on prediction variance.
- **Core assumption**: Assumes the amplification is consistent across model architectures and not specific to certain audio encoders.
- **Evidence anchors**:
  - [abstract]: "Speech amplifies pre-existing text-based biases, particularly in language and option order sensitivity"
  - [section 5.4]: "all models exhibit consistently higher APES values under the audio condition than under text" (Table 8 shows APES increases from 0.081→0.178 for language, 0.096→0.221 for option order)
  - [corpus]: Limited direct corpus evidence for amplification mechanism; related work shows speech bias exists in ASR systems but doesn't directly address amplification hypothesis
- **Break condition**: Mechanism weakens if audio preprocessing normalizes prosodic features or if models are trained end-to-end with speech augmentation that explicitly equalizes acoustic variability.

### Mechanism 3: Reasoning Complexity and Architectural Design Modulate Bias Expression
- **Claim**: Increased reasoning steps and pipeline architectures reduce sensitivity to non-semantic perturbations.
- **Mechanism**: Chain-of-thought prompting distributes attention across reasoning tokens rather than concentrating on option positions; pipeline designs (ASR → text model) strip paralinguistic cues before the decision layer, reducing gender and accent sensitivity.
- **Core assumption**: Assumes CoT benefits transfer across languages and that pipeline ASR quality is sufficiently high to not introduce new errors.
- **Evidence anchors**:
  - [abstract]: "pipeline models and chain-of-thought prompting reducing bias"
  - [section 5.2]: "CoT prompting substantially improves agreement, with Fleiss' κ increasing by an average of 19.01%, 20.50%, and 27.20% for gender, accent, and language, respectively"
  - [section 5.3]: "under the pipeline setting, both models exhibit higher Fleiss' κ and lower APES across language, accent, and gender"
- **Break condition**: Mechanism may fail if reasoning steps introduce hallucinations that confound the original question, or if ASR errors cascade through the pipeline.

## Foundational Learning

- **Concept: Shannon Entropy for Prediction Uncertainty**
  - **Why needed here**: The paper uses normalized entropy (Eq. 1) as a primary robustness metric, measuring how concentrated vs. scattered model predictions are across conditions.
  - **Quick check question**: If a model assigns [0.7, 0.1, 0.1, 0.1] to four options, what's the normalized entropy? (Answer: ~0.28, indicating relatively concentrated predictions)

- **Concept: Fleiss' Kappa for Multi-rater Agreement**
  - **Why needed here**: Measures whether a model gives consistent answers across perturbation conditions, correcting for chance agreement (Eq. 4). κ ≈ 1 means consistent; κ < 0 means systematic disagreement.
  - **Quick check question**: A κ of -0.2 for option order means: (a) slight consistency, (b) chance-level agreement, or (c) systematic inconsistency worse than random? (Answer: c)

- **Concept: End-to-end vs. Pipeline Multimodal Architectures**
  - **Why needed here**: The paper shows these architectures have different robustness profiles—end-to-end models process audio directly and may preserve paralinguistic cues; pipeline models transcribe first, which can suppress speaker-dependent variability.
  - **Quick check question**: Which architecture would likely show higher gender bias if ASR quality is equal? (Answer: End-to-end, as it preserves voice characteristics through the decision layer)

## Architecture Onboarding

- **Component map**: Text questions → GPT OSS 120B rewriting → Gemini 2.5 Flash TTS synthesis → Audio concatenation (question + options) → End-to-end MLLM OR Pipeline (ASR → text-only LLM) → Standard/CoT prompting → Answer extraction

- **Critical path**: Question rewriting (math/symbols → spoken form) → TTS synthesis with controlled language/accent/gender → audio concatenation → model inference with temperature=0 → post-processing to extract answer letter → aggregate across 28 perturbation configurations per question

- **Design tradeoffs**:
  - Synthetic TTS vs. real speech: Synthetic enables controlled variation but may not fully capture natural accent continua
  - End-to-end vs. pipeline: End-to-end preserves richer signal but amplifies demographic biases; pipeline is more robust but depends on ASR quality
  - CoT vs. standard prompting: CoT improves robustness (κ ↑19-27%) but increases inference cost and may introduce reasoning errors

- **Failure signatures**:
  - High APES + low/negative κ for a variable indicates that variable systematically destabilizes predictions (e.g., option order in all tested models)
  - CS questions showing higher entropy than CA questions indicates cultural grounding increases uncertainty
  - Smaller model variants showing higher entropy than larger variants suggests parameter scale correlates with robustness

- **First 3 experiments**:
  1. **Replicate baseline**: Run Gemini 2.5 Flash on 100 BIASINEAR questions with original vs. reversed option order (English, American accent, male voice), compute accuracy drop and κ
  2. **Test architecture hypothesis**: Compare end-to-end vs. pipeline mode on same questions, measuring gender and accent APES differences
  3. **Probe reasoning effect**: Apply standard vs. CoT prompting to a culturally sensitive subset, reporting entropy reduction and κ improvement for language and option order variables

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do speech-based bias findings generalize robustly to fully natural human speech beyond TTS-generated and voice-cloned audio?
- Basis in paper: [inferred] The study relies primarily on TTS synthesis and validates with only three voice-cloned speakers per accent (Section 5.1); the authors acknowledge this limitation and conduct limited acoustic perturbation tests.
- Why unresolved: Real-world speech contains prosodic variation, disfluencies, background noise, and speaker idiosyncrasies absent from synthetic speech, which could alter bias patterns.
- What evidence would resolve it: Evaluation on large-scale corpora of human-spoken MMLU-style questions across diverse speakers and recording conditions, comparing bias metrics to TTS baselines.

### Open Question 2
- Question: Through what specific mechanisms does the speech modality amplify text-based biases in MLLMs?
- Basis in paper: [inferred] Section 5.4 shows audio inputs yield higher APES than text for language and option order, but concludes only that speech "amplifies existing biases" without identifying causal factors.
- Why unresolved: It remains unclear whether amplification stems from acoustic feature encoding, additional processing stages, cross-modal attention patterns, or model uncertainty from speech recognition noise.
- What evidence would resolve it: Ablation studies isolating ASR transcription quality, acoustic embedding contributions, and cross-modal attention weights; layer-wise probing to identify where bias amplification emerges.

### Open Question 3
- Question: How can "accent" be rigorously operationalized for bias evaluation when regional variation exists on a continuous spectrum?
- Basis in paper: [explicit] The limitations section states: "accent variation often exists on a continuous spectrum rather than as discrete categories. The boundaries between regional or social varieties are fuzzy and, in some cases, linguistically indeterminate."
- Why unresolved: Discrete accent labels may not capture gradient phonetic variation, limiting the validity of bias measurements and fairness interventions.
- What evidence would resolve it: Development of continuous accent representations (e.g., using phonetic feature vectors or dialectometry metrics) and evaluation of whether bias correlates with acoustic distance from "standard" accents rather than discrete labels.

### Open Question 4
- Question: Which specific components of pipeline architectures most effectively mitigate speech-induced bias compared to end-to-end designs?
- Basis in paper: [inferred] Section 5.3 shows pipeline models yield higher Fleiss' κ and lower APES than end-to-end, attributing this to "explicit transcription suppresses speaker-dependent variability," but the analysis is high-level and limited to two models.
- Why unresolved: It is unclear whether ASR choice, intermediate text representation, or the separation of perception and reasoning stages drives the improvement.
- What evidence would resolve it: Systematic ablation of pipeline components (ASR model, text normalization, prompt design) and comparison across multiple ASR backends to isolate the effective factor.

## Limitations
- Synthetic TTS approach may not capture natural speech variability and accent continua
- Analysis limited to multiple-choice QA tasks, limiting generalizability to open-ended reasoning
- Several evaluation metrics require stable baseline accuracy across conditions, which may not hold for all model-question pairs
- Paper acknowledges but doesn't fully address potential cascading effects from ASR errors in pipeline architectures

## Confidence
- **High confidence**: Option order sensitivity findings (strong κ degradation, high APES across all models, consistent accuracy drops 0.5-6.75%)
- **Medium confidence**: Speech amplification hypothesis (supported by APES increases but limited direct corpus evidence)
- **Medium confidence**: Reasoning complexity benefits (CoT improves κ by 19-27% but may introduce new failure modes)
- **Medium confidence**: Cross-linguistic bias patterns (observed but based on synthetic speech rather than natural recordings)

## Next Checks
1. **Cross-validate with natural speech**: Repeat core experiments using professionally recorded speech samples across the same demographic/linguistic variations to test whether TTS-specific artifacts drive the observed biases.

2. **Test architectural robustness**: Compare end-to-end vs. pipeline performance on a subset of questions with known ASR error patterns to quantify how transcription quality mediates demographic bias expression.

3. **Validate reasoning transfer**: Apply the same CoT prompting strategy to non-MCQ question types (e.g., short answer, generation) to assess whether reasoning complexity consistently reduces sensitivity across task formats.