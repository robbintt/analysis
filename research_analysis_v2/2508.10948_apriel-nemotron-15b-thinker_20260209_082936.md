---
ver: rpa2
title: Apriel-Nemotron-15B-Thinker
arxiv_id: '2508.10948'
source_url: https://arxiv.org/abs/2508.10948
tags:
- reasoning
- training
- benchmarks
- arxiv
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Apriel-Nemotron-15B-Thinker is a 15-billion parameter language\
  \ model that achieves state-of-the-art reasoning performance while maintaining only\
  \ half the memory footprint of comparable 30\u201332B models. The model is developed\
  \ through a four-stage pipeline: depth upscaling a 12B base model to 15B parameters,\
  \ continual pretraining on 68B tokens to enhance reasoning, supervised fine-tuning\
  \ with specialized domain datasets, and reinforcement learning using GRPO with rule-based\
  \ rewards."
---

# Apriel-Nemtron-15B-Thinker

## Quick Facts
- arXiv ID: 2508.10948
- Source URL: https://arxiv.org/abs/2508.10948
- Reference count: 33
- Achieves state-of-the-art reasoning performance while maintaining only half the memory footprint of comparable 30–32B models.

## Executive Summary
Apriel-Nemtron-15B-Thinker is a 15-billion parameter language model that achieves state-of-the-art reasoning performance while maintaining only half the memory footprint of comparable 30–32B models. The model is developed through a four-stage pipeline: depth upscaling a 12B base model to 15B parameters, continual pretraining on 68B tokens to enhance reasoning, supervised fine-tuning with specialized domain datasets, and reinforcement learning using GRPO with rule-based rewards. Evaluations show it matches or exceeds performance of larger models like o1-mini, QWQ-32B, and EXAONE-Deep-32B on enterprise benchmarks and academic reasoning tasks, while consuming significantly fewer inference tokens. The approach demonstrates that compute-efficient depth scaling combined with targeted post-training can produce highly capable reasoning models deployable on single high-end GPUs.

## Method Summary
The Apriel-Nemtron-15B-Thinker pipeline begins with depth upscaling Mistral-Nemo-Base-2407 from 12B to 15B parameters by duplicating intermediate transformer layers, then training on 100B tokens for stability. This is followed by continual pretraining on 68B tokens with a curriculum focused on reasoning traces (60%), chain-of-thought samples (25%), and replay data (15%). Supervised fine-tuning uses specialized datasets for different domains (math-focused with 200k samples trained 8 epochs vs. general function-calling/RAG with 1M samples trained 3 epochs), with the resulting models merged to avoid negative interference. Finally, GRPO with rule-based rewards (format verification, test-case pass rates, tool invocation success) refines the model, and the final checkpoint is produced by merging SFT and RL checkpoints with specific weight ratios.

## Key Results
- Matches or exceeds performance of larger models like o1-mini, QWQ-32B, and EXAONE-Deep-32B on enterprise benchmarks (MBPP, BFCL-live-V2, Enterprise RAG, MT-Bench, MixEval, IFEval, MultiChallenge)
- Achieves state-of-the-art performance on academic reasoning tasks (GPQA-Diamond, MATH-500, AIME-24/25, MMLU-Pro, AMC23)
- Consumes significantly fewer inference tokens while maintaining high performance
- Demonstrates that 15B parameters is a lower bound for advanced reasoning capabilities

## Why This Works (Mechanism)

### Mechanism 1: Depth Upscaling as Compute-Efficient Capacity Expansion
Duplicating transformer layers from a pre-trained 12B model to reach 15B parameters preserves learned representations while enabling further capability gains at ~20% of the compute cost of training from scratch. Layer duplication maintains functional continuity from the base model; subsequent training on 100B tokens adapts the expanded architecture without catastrophic forgetting. The paper empirically found that duplicating intermediate layers yielded the lowest initial training loss and stability compared to width expansion.

### Mechanism 2: Reasoning-Centric CPT Curriculum Induces Generalizable Problem-Solving Patterns
A 60/25/15 mixture of reasoning traces, chain-of-thought samples, and replay data on 68B tokens improves downstream reasoning by up to 20% on math/coding tasks—before task-specific SFT. Exposure to structured reasoning patterns during continued pretraining allows the model to internalize problem decomposition and multi-step inference as distributional priors, which SFT then activates and specializes.

### Mechanism 3: Specialize–Merge SFT Balances Negative Interference and Positive Transfer
Training domain-specialized SFT models (math-heavy vs. function-calling-heavy) and merging their weights yields simultaneous gains across domains without the interference observed in single-model multi-task training. Specialized models overfit to their domain in beneficial ways; linear weight interpolation averages these specialized adaptations while preserving shared foundational representations from the common core.

## Foundational Learning

- **Depth Upscaling vs. Width Upscaling**
  - Why needed here: The paper explicitly chooses depth duplication over width expansion for stability. Understanding why depth preserves functional continuity while width introduces instability is essential for reproducing or adapting this approach.
  - Quick check question: Given a 12-layer transformer, would adding 2 duplicated middle layers or widening the MLP dimension by 20% be more likely to preserve training stability? Why?

- **Negative Interference in Multi-Task SFT**
  - Why needed here: The specialize-merge strategy is a direct response to observed interference when training on heterogeneous domains (math, function calling, RAG) jointly.
  - Quick check question: If you observe that joint SFT on math and coding data improves coding but degrades math, what does the paper suggest as a mitigation strategy?

- **Rule-Based Rewards for RL Alignment**
  - Why needed here: GRPO with rule-based rewards replaces learned reward models, enabling targeted improvement on verifiable tasks.
  - Quick check question: For a coding task with 5 test cases, how would the paper's reward scheme assign credit to a solution that passes 4/5 tests?

## Architecture Onboarding

- **Component map:** Mistral-Nemo-Base-2407 (12B) -> Depth upscaling (12B→15B) -> 100B-token pretraining -> 68B-token CPT (60% reasoning/25% CoT/15% replay) -> Specialized SFT (Math: 200k samples/8 epochs; General: 1M samples/3 epochs) -> GRPO RL (8-sample rollouts, rule-based rewards) -> Final merge (30% E + 30% F + 40% G)

- **Critical path:** 1) Depth-upscale → 100B-token pretraining 2) CPT on reasoning curriculum → average 3 checkpoints 3) Parallel SFT specializations → merge specialized weights 4) GRPO RL on merged SFT → staged GRPO checkpoints → final merge

- **Design tradeoffs:** Depth scaling chosen over width scaling for stability (width "started out highly unstable"). Specialized SFT with more epochs (8 for math) vs. balanced SFT with fewer epochs (3 for function calling) trades compute for domain depth. GRPO with 8-sample rollouts at 32k max length is compute-intensive but enables long-context reasoning from the start.

- **Failure signatures:** Width upscaling: Training instability from initialization. Joint multi-task SFT: Negative interference causing suboptimal performance across tasks. Overfitting in math SFT: Mitigated by requiring 3–4 generations per prompt to maintain diversity. Checkpoint regression: GRPO improved BFCL/Enterprise RAG but caused slight AIME regression; addressed via weighted merging.

- **First 3 experiments:**
  1. Depth upscaling ablation: Take Mistral-Nemo-12B, duplicate layers 4–7 to create a 14–15B model, and train on a 10B-token subset. Compare training loss curves to a width-upscaled variant to validate stability claims.
  2. CPT curriculum impact: Train two CPT variants—one with the 60/25/15 reasoning/CoT/replay mix, one with uniform sampling—then run identical small SFT (15k samples). Evaluate on AIME'24 and AMC23 to quantify the 20% gain attribution.
  3. Specialize-merge vs. joint SFT: Train a joint SFT model on combined math + function-calling data, and compare to separately trained + merged models. Measure interference on held-out benchmarks (AIME, BFCL-Live).

## Open Questions the Paper Calls Out

### Open Question 1
Can width-upscaling methodologies be stabilized to provide a viable alternative to depth-upscaling for reasoning models? The authors state they "defer this experimentation to future work" because width-upscaling "started out highly unstable" (Section 2.1). This remains unresolved because the instability prevented any comparison of capability trade-offs between width and depth scaling. A stable training run of a width-upscaled model matching the depth-upscaled model's benchmark performance would resolve this.

### Open Question 2
Is the identified 15B parameter "lower bound" for advanced reasoning a universal scaling law or specific to the Mistral-Nemo architecture? The authors "empirically find" 15B to be the lower bound after ablating sizes (Section 2), but do not test other base model families. This remains unresolved because the finding relies on a single model lineage; generalizability to different architectures is unverified. Replication of the upscaling pipeline on diverse base architectures to see if the reasoning threshold shifts would resolve this.

### Open Question 3
Do specialized-merge strategies consistently outperform unified multi-task training with optimized data mixing? The paper notes "negative interference" in concurrent training, leading to a specialized-merge approach (Section 3.2), but does not compare against advanced unified mixing techniques. This remains unresolved because it is unclear if the interference is intrinsic to the tasks or a limitation of the specific mixing ratios used. A comparison against a single model trained with dynamic curriculum learning or gradient surgery to mitigate interference would resolve this.

## Limitations

- Exact layer indices for depth upscaling are omitted, making precise replication difficult
- Dataset compositions for 100B-token upscaling and 68B-token CPT stages are not specified
- Rule-based reward function implementation details are not provided
- Final checkpoint merging ratios (30%/30%/40%) lack ablation studies for justification
- Reliance on proprietary Mistral-Nemo-Base-2407 model may limit generalizability

## Confidence

- **High Confidence:** The overall 4-stage pipeline structure and demonstrate improvements from CPT and specialize-merge strategy
- **Medium Confidence:** Depth upscaling provides stable capacity expansion at ~20% of training cost from scratch
- **Low Confidence:** Exact attribution of performance gains to specific dataset compositions and checkpoint merging ratios

## Next Checks

1. **Depth Upscaling Ablation:** Reproduce the 12B→15B upscaling using different sets of intermediate layers (e.g., duplicating layers 4–7 vs. 6–9) and compare training stability and final task performance to validate that the original choice was optimal.

2. **CPT Curriculum Composition:** Train CPT variants with alternative curriculum mixtures (e.g., 40/40/20 or 50/30/20 for reasoning/CoT/replay) and measure downstream task performance (AIME-24, MATH-500) to isolate the contribution of the specific 60/25/15 split.

3. **Specialize-Merge vs. Joint Training:** Conduct a direct ablation comparing the paper's specialize-merge strategy to joint SFT on the same total dataset, measuring both task-specific gains (e.g., AIME vs. BFCL) and any cross-task interference to validate the merging approach.