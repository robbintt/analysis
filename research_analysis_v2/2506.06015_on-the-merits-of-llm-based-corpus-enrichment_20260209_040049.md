---
ver: rpa2
title: On the Merits of LLM-Based Corpus Enrichment
arxiv_id: '2506.06015'
source_url: https://arxiv.org/abs/2506.06015
tags:
- documents
- corpus
- document
- relevant
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes using large language models (LLMs) to enrich
  document corpora by generating new relevant documents or modifying existing ones,
  with the aim of improving query-based retrieval effectiveness. The core method involves
  prompting LLMs to create documents that are both relevant to specific queries and
  highly retrievable by standard ranking functions.
---

# On the Merits of LLM-Based Corpus Enrichment

## Quick Facts
- **arXiv ID**: 2506.06015
- **Source URL**: https://arxiv.org/abs/2506.06015
- **Reference count**: 40
- **Primary result**: LLM-generated documents achieve higher retrieval ranks than existing relevant documents, improving RAG accuracy and attribution quality.

## Executive Summary
This paper proposes enriching document corpora by generating new relevant documents using large language models (LLMs), with the goal of improving query-based retrieval effectiveness. The core method involves prompting LLMs to create documents that are both relevant to specific queries and highly retrievable by standard ranking functions. Experiments across three retrieval scenarios—ad hoc retrieval, retrieval-augmented generation (RAG), and answer attribution in QA—demonstrate that LLM-generated documents are ranked significantly higher than existing relevant documents, leading to improved retrieval performance. Additionally, corpus enrichment enhances the accuracy of RAG-based QA systems and increases the quality of attribution passages. Faithfulness measures confirm that generated documents maintain alignment with corpus content, particularly when conditioned on existing documents.

## Method Summary
The method enriches a corpus by generating new documents using LLM prompts that target query relevance and high retrievability. Five prompting methods are used: zero-shot generation (ZS), document modification (DM), and document synthesis with 2 or 3 source documents (2DS, 2DSR, 3DS). Generated documents are evaluated using standard retrieval metrics (NDCG, MAP) and faithfulness scores based on NLI entailment. The approach is tested on MS-MARCO V2 and Natural Questions datasets with retrievers including BM25, E5, and Contriever. Faithfulness is measured by checking entailment between generated sentences and existing corpus documents.

## Key Results
- Generated documents consistently achieve lower median ranks than existing relevant documents (e.g., DL21 Okapi: MG=9 vs ME=309)
- RAG accuracy improves from 35.2%→57.3% (Llama2) and 38.3%→49.3% (Llama3) with enrichment
- Attribution entailment accuracy reaches 74.5% with full enrichment vs ~46% baseline
- Document-conditioned methods (2DS, 3DS) achieve faithfulness scores >90% vs 68-84% for baseline relevant documents

## Why This Works (Mechanism)

### Mechanism 1: Vocabulary Alignment via Query-Biased Synthesis
LLM-generated documents achieve higher retrievability than existing relevant documents when prompted to incorporate query terms while preserving relevance. Prompts instruct LLMs to rewrite existing documents or synthesize new ones containing query vocabulary, explicitly targeting higher ranking positions while maintaining topical relevance. This reduces query-document vocabulary mismatch—a known retrieval failure mode.

### Mechanism 2: Faithfulness Preservation Through Document Conditioning
Grounding LLM generation in existing corpus documents reduces hallucination and maintains factual alignment with source material. Methods like 2DS/3DS provide 2-3 relevant documents as context; prompts explicitly forbid adding knowledge absent from source. NLI-based faithfulness scoring quantifies entailment between generated sentences and corpus subsets.

### Mechanism 3: Downstream Task Amplification via Retrieval Improvement
Corpus enrichment improves RAG QA accuracy and attribution quality by creating highly-retrievable documents containing correct answers. Enriched corpus increases probability that top-k retrieval contains answer-bearing passages. For attribution, generated documents serve as higher-quality evidence passages with better entailment scores.

## Foundational Learning

- **Concept**: Vocabulary mismatch problem in IR
  - Why needed: Core motivation for corpus enrichment; understanding why standard retrieval fails motivates the intervention
  - Quick check: Can you explain why BM25 might fail to retrieve a highly relevant document that uses different terminology than the query?

- **Concept**: Retrieval-Augmented Generation (RAG)
  - Why needed: Primary downstream application; enrichment targets RAG's retrieval bottleneck
  - Quick check: In a RAG pipeline, what happens if the retrieval stage returns documents that don't contain the answer?

- **Concept**: Hallucination and faithfulness in LLMs
  - Why needed: Critical risk in corpus enrichment; motivates faithfulness evaluation methodology
  - Quick check: Why does zero-shot generation (ZS) produce lower faithfulness scores than document-conditioned methods (2DS, 3DS)?

## Architecture Onboarding

- **Component map**: Query Set Q → Document Selector (retrieval-based or oracle) → LLM + Prompt (ZS/DM/2DS/3DS) → Generated Document d_g^q → Enriched Corpus C_e = C ∪ {d_g^q} → Retrieval System (BM25/E5/Contriever) → Downstream: Ad hoc ranking / RAG context / Attribution candidate

- **Critical path**: Document selection → Prompt construction → Generation → Faithfulness validation → Corpus insertion → Retrieval evaluation. The prompt design and source document quality most directly impact downstream gains.

- **Design tradeoffs**:
  - ZS vs conditioned generation: ZS requires no relevant documents (easier deployment) but lower faithfulness
  - Number of source documents (2DS vs 3DS): More context improves comprehensiveness but increases prompt length and hallucination surface
  - Generation per query vs per topic: Per-query optimizes for specific retrievals but scales poorly; per-topic is efficient but less targeted

- **Failure signatures**:
  - Low faithfulness scores (<70%): Indicates insufficient grounding; switch from ZS to conditioned methods or improve source document selection
  - Generated documents not retrieved in top-k: Prompt may over-optimize for query terms at expense of coherence; retrieval model may be semantic-heavy
  - RAG accuracy doesn't improve despite enrichment: Answer may not appear in generated documents; retrieval may not surface generated documents; generator LLM may ignore context

- **First 3 experiments**:
  1. Implement DM method on a small test set (50 queries); verify generated documents achieve higher ranks than existing relevant documents using BM25
  2. Compare ZS vs 2DS vs 3DS faithfulness scores on same query set; confirm conditioning improves entailment (target >85% for k=5)
  3. Run RAG pipeline with enriched corpus on held-out questions; measure accuracy gain vs baseline (target +10-15% absolute improvement)

## Open Questions the Paper Calls Out

- Can corpus enrichment methods maintain effectiveness when using pseudo-relevant documents for generation instead of ground-truth relevant documents?
- To what extent does corpus enrichment reduce performance variance across different queries representing the same information need?
- How effective are non-oracle document selection strategies for generating enrichment documents in RAG scenarios?

## Limitations

- Faithfulness evaluation depends on NLI models whose reliability is not independently validated
- All experiments use Wikipedia-based corpora, limiting generalization to other domains
- Method requires one LLM call per query, making it computationally expensive for large query sets

## Confidence

**High Confidence**:
- Generated documents achieve higher retrieval ranks than existing relevant documents across all tested retrievers and datasets
- Corpus enrichment improves RAG accuracy and answer coverage in top-k retrievals
- Document-conditioned methods produce significantly higher faithfulness scores than zero-shot generation

**Medium Confidence**:
- Vocabulary alignment mechanism explains improved retrieval performance
- Faithfulness scores correlate with improved attribution quality
- LLM generation can be reliably constrained through prompts

**Low Confidence**:
- NLI-based faithfulness metric accurately measures factual accuracy versus topical relevance
- Results generalize to non-Wikipedia domains without modification
- The enrichment method scales efficiently to production workloads

## Next Checks

1. Apply the enrichment method to a non-Wikipedia corpus (e.g., scientific papers) and measure whether the same performance gains hold.

2. Manually annotate a sample of generated documents to verify whether high NLI scores correspond to actual factual accuracy.

3. Measure the computational cost of enriching a corpus with 10K, 100K, and 1M queries, then evaluate whether the retrieval improvements justify the resource expenditure.