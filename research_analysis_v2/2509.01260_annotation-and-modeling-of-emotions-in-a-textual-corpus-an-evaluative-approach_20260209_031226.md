---
ver: rpa2
title: 'Annotation and modeling of emotions in a textual corpus: an evaluative approach'
arxiv_id: '2509.01260'
source_url: https://arxiv.org/abs/2509.01260
tags:
- pour
- nous
- dans
- corpus
- emotion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses emotion annotation in an industrial corpus
  using an evaluative framework, which is under-researched. The study finds high inter-annotator
  disagreement, leading to the hypothesis that annotations follow stable statistical
  trends driven by linguistic features.
---

# Annotation and modeling of emotions in a textual corpus: an evaluative approach

## Quick Facts
- arXiv ID: 2509.01260
- Source URL: https://arxiv.org/abs/2509.01260
- Reference count: 0
- Primary result: Fine-tuned GTE model recovers annotation proportions with 80-95% global precision across four evaluative dimensions

## Executive Summary
This study addresses emotion annotation in an industrial corpus using an evaluative framework, which is under-researched. The research finds high inter-annotator disagreement, leading to the hypothesis that annotations follow stable statistical trends driven by linguistic features. To test this, language models (Flaubert and GTE) are fine-tuned on averaged annotations. Results show that while the baseline Flaubert model performs poorly, the GTE model with its last layer trained can recover annotation proportions with high accuracy, supporting the hypothesis that underlying linguistic features drive annotation variability.

## Method Summary
The study uses a corpus of 4,873 verbatims from 2,446 posts about innovative concepts, annotated by 6 annotators with values {-1, 0, +1} per dimension. Averaged annotations are used as regression targets for three model configurations: frozen Flaubert-base-cased, frozen GTE-multilingual-base, and GTE with last layer trainable. Models are trained separately per dimension using project-level cross-validation (4 test projects) and evaluated with confusion matrices and threshold-based precision/recall at ±1/3.

## Key Results
- Flaubert baseline model fails to distinguish annotation values, collapsing to majority class predictions
- GTE frozen model shows diagonal trend in confusion matrix but severely under-predicts negative class
- Fine-tuned GTE achieves 95.2% global precision for Familiarity, 85.1% for Agréabilité, 80.0% for Utilité, 89.1% for Légitimité
- Model successfully distinguishes emotional situations based on evaluative criteria, supporting the hypothesis that underlying linguistic features drive annotation variability

## Why This Works (Mechanism)

### Mechanism 1: Annotation Disagreement Reflects Underlying Intensity Gradients
Inter-annotator disagreement is not random noise but reflects a probabilistic distribution governed by how intensely evaluative dimensions manifest in text. Texts with more explicit linguistic markers receive more unanimous annotations, while ambiguous texts produce proportional splits among annotators. The evaluative dimensions are continuous psychological constructs that manifest gradiently in language, not binary categories.

### Mechanism 2: Averaged Annotations Enable Continuous Target Learning
Averaging multi-annotator labels transforms discrete categorical disagreement into continuous proportion targets that transformer models can learn. Six annotators' ternary judgments are averaged into continuous values, creating regression-like targets that capture annotation consensus as magnitude. The averaged value represents meaningful ground truth about the text's position on each evaluative dimension.

### Mechanism 3: Model Architecture and Training Depth Determine Representational Adequacy
Pre-trained representations from GTE contain evaluative-dimension-relevant features that Flaubert lacks, and minimal fine-tuning (last layer only) is sufficient to recover annotation proportions. GTE's multilingual pre-training creates embeddings where evaluative dimensions are already partially separable; training the final transformer layer adjusts task-specific readout without destroying pre-trained representations.

## Foundational Learning

- Concept: Evaluative Theory of Emotion
  - Why needed here: The paper explicitly contrasts this with "basic emotion theory" and dimensional approaches. Understanding that emotions arise from cognitive appraisal of stimuli is essential to interpreting why the chosen dimensions are Familiarity/Agréabilité/Utilité/Légitimité.
  - Quick check question: If someone rates a product as "useful but unpleasant," which two evaluative dimensions are being distinguished?

- Concept: Inter-Annotator Agreement Metrics (Krippendorff's Alpha)
  - Why needed here: The paper reports α = 0.29-0.53 globally but α ≥ 0.80 for polarity-only agreement. Understanding that agreement varies by measurement modality is critical for interpreting whether "disagreement" reflects noise or genuine ambiguity.
  - Quick check question: Why might annotators agree on polarity (+1 vs. -1) but disagree on relevance (0 vs. ±1)?

- Concept: Transfer Learning with Frozen vs. Fine-Tuned Layers
  - Why needed here: The paper's central finding depends on comparing frozen Flaubert, frozen GTE, and GTE with last-layer training. Without understanding why freezing preserves representations but limits adaptation, the results are uninterpretable.
  - Quick check question: Why would a frozen model produce useful representations but fail to predict the target variable accurately?

## Architecture Onboarding

- Component map: Input -> Tokenizer -> Encoder -> Classification head -> Target
- Critical path: Data preparation -> Threshold decision -> Model selection -> Training protocol
- Design tradeoffs: Frozen vs. fine-tuned, Continuous vs. discretized targets, Ecological vs. filtered corpus
- Failure signatures: Flaubert frozen (collapse to majority class), GTE frozen (diagonal trend with negative class issues), Overfitting (high training accuracy, poor cross-project generalization)
- First 3 experiments: 1) Baseline establishment with frozen Flaubert, 2) Embedding quality probe with frozen GTE, 3) Minimal fine-tuning of GTE's last layer

## Open Questions the Paper Calls Out

1. What specific linguistic structures underlie the model's ability to distinguish emotional situations based on evaluative criteria?
2. Do the stable statistical trends observed in annotation disagreement persist when the emotional stimulus involves direct interpersonal interaction rather than distant video concepts?
3. How can the continuous outputs of the neural models be precisely quantified to validate the hypothesis of an underlying probabilistic annotation model?

## Limitations

- Corpus unavailability for independent verification of linguistic feature analysis
- No confidence intervals or statistical significance tests reported for precision metrics
- Arbitrary thresholds (±1/3) for binary classification without justification for their relationship to continuous annotation distribution

## Confidence

**High Confidence**: Averaged annotations produce continuous targets that can be learned by transformer models; inter-annotator disagreement reflects genuine ambiguity rather than noise.

**Medium Confidence**: GTE with last layer training is optimal for this task; superiority could be due to model size rather than architectural suitability.

**Low Confidence**: Generalizability of results to other evaluative frameworks or languages; specific mechanisms may not apply beyond French innovation contexts.

## Next Checks

1. Recompute reported precision metrics with 95% confidence intervals using bootstrapping across project-level folds to assess statistical significance.

2. Test whether model success depends on averaging annotations or if other aggregation methods (median, mode, weighted averages) would work equally well or better.

3. Conduct correlation analysis between linguistic markers and annotation patterns once corpus access is available to test hypothesized relationship between linguistic explicitness and annotation consensus.