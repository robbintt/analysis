---
ver: rpa2
title: 'Enhancing Systematic Reviews with Large Language Models: Using GPT-4 and Kimi'
arxiv_id: '2504.20276'
source_url: https://arxiv.org/abs/2504.20276
tags:
- kimi
- gpt-4
- performance
- systematic
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated GPT-4 and Kimi for systematic review coding
  using 32 articles and 11 yes-or-no questions from a peer-reviewed systematic review.
  Both models performed below unsupervised use thresholds, with GPT-4 showing better
  overall performance (accuracy 0.660 vs.
---

# Enhancing Systematic Reviews with Large Language Models: Using GPT-4 and Kimi

## Quick Facts
- arXiv ID: 2504.20276
- Source URL: https://arxiv.org/abs/2504.20276
- Reference count: 3
- Key outcome: Both GPT-4 and Kimi performed below unsupervised use thresholds, with GPT-4 showing better overall performance (accuracy 0.660 vs. 0.633) and stability across batch sizes, while Kimi excelled in precision (0.788 vs. 0.779) but only when processing single articles

## Executive Summary
This study evaluated GPT-4 and Kimi for systematic review coding using 32 articles and 11 yes-or-no questions from a peer-reviewed systematic review. Both models performed below unsupervised use thresholds, with GPT-4 showing better overall performance (accuracy 0.660 vs. 0.633) and stability across batch sizes, while Kimi excelled in precision (0.788 vs. 0.779) but only when processing single articles. Inter-coder reliability was strong for single-article batches (ICC 0.815-0.845) but declined for larger batches. Neither model achieved sufficient performance for independent use, suggesting need for prompt engineering and further research on question design and model optimization.

## Method Summary
The study used 32 PDF articles from a systematic review on technology-based assessments and 11 yes-or-no questions (5 inclusion criteria, 6 synthesis questions). GPT-4 and Kimi were tested across three batch sizes (1, 4, 8 articles) with 4 independent replications each, totaling 1,056 predictions. Human-generated codes served as ground truth. Performance was measured using accuracy, precision, recall, F1 score, and inter-coder reliability via ICC and Kappa.

## Key Results
- GPT-4 outperformed Kimi overall (accuracy 0.660 vs. 0.633) and maintained stable performance across all batch sizes
- Kimi achieved higher precision than GPT-4 (0.788 vs. 0.779) but only at batch size 1, with performance declining at larger batches
- Both models showed strong performance on inclusion criteria questions (Items 1-5) but struggled with synthesis questions (Items 6-11), particularly those requiring evaluation of "substantial content"
- Inter-coder reliability remained high for GPT-4 across all batch sizes (ICC ~0.815) but declined for Kimi at larger batches (ICC < 0.75 at batch size 8)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Batch size directly affects LLM coding consistency through context dilution
- Mechanism: When multiple articles are processed in a single prompt, attention is distributed across longer token sequences, reducing signal-to-noise ratio for any individual article's coding decisions. Kimi shows stronger degradation (ICC drops from 0.845 at batch=1 to 0.649 at batch=8) while GPT-4 maintains stability (ICC ~0.815 across all batch sizes).
- Core assumption: The performance degradation stems from context window crowding rather than model-specific prompt handling differences.
- Evidence anchors:
  - [abstract]: "performance of LLMs fluctuates by data volume and question complexity"
  - [section]: "Kimi's reliability declined below 0.75 when the batch size was eight" while "GPT-4's consistency remained high"
  - [corpus]: Limited direct evidence on batch-size mechanisms in systematic review literature; neighbor papers focus on single-article screening tasks.

### Mechanism 2
- Claim: Question type (existence vs. synthesis) creates differential performance patterns
- Mechanism: Inclusion criteria questions (Items 1-5) require detecting presence of topics, achieving high precision (1.000 for both models on several items). Synthesis questions (Items 6-11) require evaluating "substantial content," a more subjective threshold, resulting in accuracy drops to 0.492-0.690.
- Core assumption: The semantic ambiguity in "substantial content" is the primary driver of performance gaps, not question length or position.
- Evidence anchors:
  - [section]: "Item-specific performance... reveals that GPT-4 and Kimi performed well on inclusion criteria items (Items 1-5)... For synthesis items (Items 6-11), both models showed lower accuracy and recall"
  - [table]: Items 9-11 show near-zero recall for Kimi (0.017) and GPT-4 (0.067) on rare-positive questions
  - [corpus]: Neighbor paper "Large language models streamline automated systematic review" similarly notes performance variation across review tasks but does not isolate question-type mechanisms.

### Mechanism 3
- Claim: Model architecture differences create precision-recall tradeoffs at optimal batch sizes
- Mechanism: GPT-4's higher recall (0.791 vs. 0.712 overall) suggests broader pattern matching; Kimi's higher precision at batch=1 (0.811 vs. 0.780) suggests more conservative positive classification. This tradeoff inverts or disappears at larger batches.
- Core assumption: The precision-recall tradeoff reflects underlying tokenization or training differences rather than prompt-response formatting alone.
- Evidence anchors:
  - [section]: "Kimi outperformed GPT-4 in precision (0.811 vs. 0.780), but GPT-4 showed better recall (0.827 vs. 0.787)" at batch=1
  - [section]: "As the batch size increased... Kimi's precision advantage diminished, disappearing when coding eight articles"
  - [corpus]: Weak corpus evidence on Kimi-specific architecture; most literature focuses on GPT-family models.

## Foundational Learning

- Concept: Inter-class Correlation Coefficient (ICC)
  - Why needed here: The paper uses ICC to measure consistency across four LLM replications and human codes. Understanding ICC values (0.815-0.845 = strong agreement; <0.75 = declining reliability) is essential for interpreting whether LLM outputs are stable enough for supervised use.
  - Quick check question: If ICC drops from 0.84 to 0.65 when batch size increases from 1 to 8, what does this indicate about measurement consistency?

- Concept: Precision vs. Recall Tradeoff in Classification
  - Why needed here: The study explicitly reports precision (true positive accuracy) and recall (positive case detection rate) separately. GPT-4 favors recall; Kimi favors precision at batch=1. Understanding this tradeoff is critical for deciding which model fits specific review workflows.
  - Quick check question: A systematic review prioritizes not missing relevant studies. Which metric—precision or recall—should be optimized, and which model might be preferred?

- Concept: Yes/No Binary Coding with Ground Truth Comparison
  - Why needed here: The methodology treats human-generated codes as ground truth and LLM codes as predictions. Understanding false positives (LLM says Yes, human says No) vs. false negatives (LLM says No, human says Yes) is essential for interpreting Table 5's item-level results.
  - Quick check question: Item 9 shows 0.017 recall for Kimi. Does this indicate Kimi is missing almost all true positives, or correctly rejecting false positives?

## Architecture Onboarding

- Component map: Document Ingestion -> Prompt Construction -> LLM Processing -> Output Extraction -> Evaluation Layer
- Critical path: Ground truth establishment -> Prompt design -> Batch configuration -> Replication -> Metric computation
- Design tradeoffs:
  - Batch size vs. consistency: Larger batches reduce API calls but degrade Kimi's ICC below acceptable thresholds
  - Precision vs. recall optimization: GPT-4 for comprehensive screening (higher recall); Kimi for conservative filtering (higher precision at batch=1)
  - Question standardization vs. performance: Using original questions enables direct comparison but sacrifices potential accuracy gains from prompt optimization
- Failure signatures:
  - ICC < 0.75: Indicates unstable outputs; do not use for production coding
  - Recall < 0.10 on rare-positive items (e.g., Items 9-11): Model is defaulting to "No" responses, missing nearly all true cases
  - Precision advantage inversion at larger batches: Kimi's precision drops below GPT-4's at batch=8, eliminating its primary advantage
- First 3 experiments:
  1. Replicate single-article batch condition with your domain's articles; verify ICC > 0.80 before expanding batch size
  2. A/B test original questions vs. operationally-defined prompts (e.g., replace "substantial content" with "≥1 paragraph" or "≥100 words") on a 5-article subset
  3. Calibrate decision thresholds by adding explicit instructions like "When uncertain, respond 'No'" vs. "When uncertain, respond 'Yes'" to measure precision-recall sensitivity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent can prompt engineering or question modification improve the accuracy and reliability of LLM-based systematic review coding?
- Basis in paper: [explicit] The Discussion states, "Future studies should explore prompt engineering to improve the quality of questions we feed into LLMs and see how it can enhance coding accuracy."
- Why unresolved: This study deliberately used unmodified questions from the original human review to ensure comparability, leaving the potential of optimized prompts untested.
- What evidence would resolve it: A comparative study measuring performance differences between raw systematic review protocols and protocols refined specifically for LLM logic.

### Open Question 2
- Question: Which specific linguistic or structural features of coding questions (e.g., "substantial content" vs. simple presence) are responsible for the performance discrepancies observed between inclusion criteria and synthesis items?
- Basis in paper: [explicit] The authors note that "further research is needed to understand how item features affect coding outcomes" and observed that "both models showed lower accuracy and recall" for synthesis items compared to inclusion criteria.
- Why unresolved: While the study identifies performance fluctuations across the 11 questions, it does not isolate which specific semantic traits (e.g., ambiguity, subjectivity) cause LLMs to fail or succeed.
- What evidence would resolve it: A regression analysis or ablation study correlating specific question attributes (subjectivity, length, negation) with accuracy and reliability scores.

### Open Question 3
- Question: Do alternative high-capability models like Claude exhibit similar batch-size dependencies and trade-offs between precision and recall as observed in GPT-4 and Kimi?
- Basis in paper: [explicit] The authors suggest that future work "could involve other models like Claude, which has been gaining attention for its capabilities."
- Why unresolved: The current findings are limited to two specific models (GPT-4 and Kimi), leaving the generalizability of these batch-processing effects across the broader LLM landscape unknown.
- What evidence would resolve it: Replication of the specific batch-size protocol (1, 4, 8 articles) using Claude or other frontier models to compare stability and ICC metrics.

## Limitations

- Model architecture differences between GPT-4 and Kimi were not fully explored, limiting understanding of why batch processing affects them differently
- Using unmodified questions from the original systematic review may have artificially constrained LLM performance, as these questions were designed for human coders
- The specific prompt formatting for combining multiple articles in batch processing remains unclear, making it difficult to determine whether performance degradation stems from context window effects or prompt structure

## Confidence

- **High confidence**: The comparative performance findings between GPT-4 and Kimi are robust, as they're based on systematic evaluation across multiple metrics and batch sizes. The general conclusion that both models fall below unsupervised use thresholds is well-supported.
- **Medium confidence**: The mechanisms explaining batch-size effects and precision-recall tradeoffs are plausible but not definitively proven, as the study lacks direct evidence about attention distribution and tokenization differences.
- **Low confidence**: The recommendation against unsupervised LLM use may be overly conservative given that prompt engineering was not explored, which could potentially improve performance significantly.

## Next Checks

1. **Prompt optimization test**: Replicate the study using revised prompts that replace subjective terms like "substantial content" with objective operational definitions (e.g., word counts or section requirements) to determine if performance gaps are primarily semantic rather than capability-based.

2. **Architecture comparison**: Conduct a controlled experiment varying only the model architecture (e.g., GPT-4 vs. GPT-3.5) while keeping prompts and batch sizes constant to isolate the impact of model differences from other variables.

3. **Batch processing validation**: Test whether separating articles within a batch prompt (rather than combining them) restores Kimi's performance to single-article levels, helping determine if degradation is due to context crowding or other factors.