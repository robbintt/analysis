---
ver: rpa2
title: Applying Relation Extraction and Graph Matching to Answering Multiple Choice
  Questions
arxiv_id: '2511.14144'
source_url: https://arxiv.org/abs/2511.14144
tags:
- graph
- knowledge
- each
- question
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method that combines Transformer-based relation
  extraction with knowledge graph (KG) matching to answer multiple-choice questions
  (MCQs) in a "fill-in-the-blank" format while maintaining process traceability. The
  approach constructs a propositional graph for each answer option, extracts relations
  from the question text using relation extraction (RE) methods, and verifies these
  against factually correct KGs built from Wikipedia.
---

# Applying Relation Extraction and Graph Matching to Answering Multiple Choice Questions

## Quick Facts
- **arXiv ID:** 2511.14144
- **Source URL:** https://arxiv.org/abs/2511.14144
- **Reference count:** 6
- **Primary result:** Up to 70% accuracy on fill-in-the-blank MCQs using relation extraction and knowledge graph matching

## Executive Summary
This paper proposes a method that combines Transformer-based relation extraction with knowledge graph matching to answer multiple-choice questions in a "fill-in-the-blank" format while maintaining process traceability. The approach constructs propositional graphs for each answer option, extracts relations from question text using relation extraction methods, and verifies these against factually correct KGs built from Wikipedia. The method achieves up to 70% accuracy on two original MCQ datasets, with performance varying significantly across question categories. The approach provides clear explanations by visualizing verified edges, addressing the interpretability limitations of large language models in MCQ answering.

## Method Summary
The method follows a four-step pipeline: (1) Build propositional graph (PG) template by running RE on question with each option substituted; (2) Build knowledge graph (KG) by fetching Wikipedia summaries for each PG node label and applying RE; (3) Apply entity linking via Wikipedia title search to both graphs, then compute bipartite node mapping using semantic similarity (all-MiniLM-L6-v2); (4) Compute truthfulness T(o_i) as edge overlap ratio |φ(PG) ∩ KG| / |PG|, select answer with highest T(o_i), tie-break with average node similarity. The method uses REBEL and variants for relation extraction, Wikipedia API for knowledge acquisition, and maximum bipartite matching for node alignment.

## Key Results
- Achieved up to 70% accuracy on two custom MCQ datasets (KR-200m and KR-200s)
- Entity linking improved accuracy by approximately 11 percentage points (53.5% to 42.2% without EL on KR-200m)
- Performance varied dramatically across categories, with Mathematics/Science showing only 10-20% accuracy
- mREBEL400 showed near-equal performance to REBEL (220 relations), suggesting diminishing returns from more relation types

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Propositional Graph Generation via Relation Extraction
- Claim: Transformer-based RE methods can convert natural language question sentences into structured propositional graphs that represent factual claims
- Core assumption: The RE method can reliably extract meaningful relations from short natural language sentences
- Evidence anchors: REBEL's FMR=0.62 on AffilKG dataset; performance drops significantly on short sentences (KR-200s)

### Mechanism 2: Truthfulness Scoring via Closed-World Edge Verification
- Claim: The proportion of propositional graph edges that exist in the knowledge graph serves as a proxy for factual correctness
- Core assumption: Under closed-world assumption, edges absent from KG are false
- Evidence anchors: Higher edge overlap correlates with correct answers; method compares to Zero-Shot Fact-Checking paper using triplet verification

### Mechanism 3: Entity Linking for Cross-Graph Node Resolution
- Claim: Entity linking disambiguates node labels across PG and KG, enabling accurate edge verification
- Core assumption: Wikipedia article titles provide unambiguous entity identifiers
- Evidence anchors: Table 1 shows REBEL accuracy drops from 53.5% to 42.2% without entity linking

## Foundational Learning

- **Concept: Knowledge Graphs as Conjunctive Ground Atoms**
  - Why needed here: KGs are framed as "sets of conjunctions of binary ground atoms in first-order logic," making entailment equivalent to subgraph isomorphism
  - Quick check question: Given KG₁ = {(A, relates_to, B)} and KG₂ = {(A, relates_to, B), (B, connects_to, C)}, does KG₂ entail KG₁?

- **Concept: Closed-World Assumption (CWA)**
  - Why needed here: Verification treats absent edges in the KG as false, not unknown
  - Quick check question: If PG contains (Paris, capital_of, France) but KG has no such triplet, does CWA make this false or simply unverifiable?

- **Concept: Maximum Bipartite Matching for Node Alignment**
  - Why needed here: The method solves φ̂ = argmax Σ sim(label(v), label(φ'(v))) to align nodes across graphs
  - Quick check question: Why must φ' be a bijection rather than allowing many-to-one mappings?

## Architecture Onboarding

- **Component map:** Input Layer -> RE Module -> KG Builder -> Entity Linker -> Node Matcher -> Scorer -> Explainer
- **Critical path:** RE extraction quality → Entity linking accuracy → Node correspondence quality → Edge verification → Final score
- **Design tradeoffs:** REBEL (220 relations) vs. mREBEL400 (400 relations) shows near-equal performance; UniRel performs near-random (28.6%)
- **Failure signatures:** Low accuracy on Mathematics/Science (10-20%); UniRel near-random; KR-200s underperforms KR-200m
- **First 3 experiments:**
  1. Reproduce REBEL + EL baseline on KR-200m: Verify 53.5% accuracy; isolate per-category performance
  2. Ablate entity linking: Confirm ~11% drop; analyze which node mismatches cause verification failures
  3. Test on held-out category: Train/evaluate on 8 categories, test on 2 unseen (e.g., Pop Culture vs. Science)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can a flexible verification mechanism (e.g., approximate graph matching or logical rule integration) significantly improve accuracy in categories requiring abstract reasoning, such as Mathematics and Science?
- **Basis in paper:** The Conclusion states that future research should "introduce a more flexible verification mechanism to implement reasoning capability in the method," and the Results section notes poor performance on "Mathematics" and "Science" due to the need for "logical inferences."
- **Why unresolved:** The current method relies on strict subgraph isomorphism and the closed-world assumption, which fails to verify statements requiring multi-step derivations or abstract knowledge not explicitly stored as direct edges in the Knowledge Graph.
- **What evidence would resolve it:** Successful application of the method on the "Mathematics" and "Science" subsets of the KR-200 datasets using a revised matching algorithm that tolerates missing edges or infers implicit relationships.

### Open Question 2
- **Question:** How does the precision of the entity linking (article search) component affect the overall reliability of the graph verification process?
- **Basis in paper:** The Conclusion explicitly suggests the need to "improve the accuracy of article search in the KG creation process in order to prevent the mislinking of Wikipedia pages."
- **Why unresolved:** The current implementation uses a simple Wikipedia API title search for entity linking, which may incorrectly map node labels to irrelevant articles, thereby constructing factually incorrect "ground truth" Knowledge Graphs for verification.
- **What evidence would resolve it:** An ablation study comparing the current simple title search against state-of-the-art entity disambiguation tools (e.g., BLINK or REL) to measure the reduction in verification errors.

### Open Question 3
- **Question:** Does the performance of the proposed method generalize to human-authored datasets, given that the experimental data was generated by GPT-4o?
- **Basis in paper:** The paper notes that the datasets "KR-200m and KR-200s" were "created... using GPT-4o," while Section 6 mentions that existing datasets like MMLU are used for LLM evaluation.
- **Why unresolved:** GPT-4o may generate questions with linguistic patterns or relation structures that inherently favor Transformer-based RE methods, potentially limiting the validity of the results on natural, human-written exams.
- **What evidence would resolve it:** Benchmarking the REBEL-based graph matching method on established human-curated MCQ datasets (e.g., MMLU or ARC) to compare accuracy against the results obtained on the synthetic KR-200 datasets.

## Limitations

- Method shows poor performance (10-20%) on abstract domains like Mathematics and Science, where logical reasoning is required
- Evaluation limited to two custom datasets generated by GPT-4o, without comparison to established MCQ benchmarks
- Closed-world assumption may be too restrictive given Wikipedia's incompleteness for verification tasks

## Confidence

- **High Confidence:** The RE + KG matching pipeline works as described; ablation studies provide clear evidence of entity linking's contribution
- **Medium Confidence:** The 70% accuracy figure represents the method's potential, but category-specific performance reveals significant limitations requiring domain-specific adaptations
- **Low Confidence:** Generalizability to other MCQ formats, knowledge domains, and more comprehensive KG sources remains untested

## Next Checks

1. **Cross-Domain Generalization:** Test the complete pipeline on a held-out category (e.g., Science/Mathematics) to measure performance drop when knowledge graphs are less complete, validating the closed-world assumption's limitations.

2. **Relation Extraction Robustness:** Systematically evaluate REBEL's accuracy on sentences of varying lengths from KR-200s to quantify the impact of context scarcity on relation extraction quality.

3. **KG Completeness Analysis:** For questions where the method fails, manually inspect whether the correct relations exist in Wikipedia but were missed by RE, or whether they're genuinely absent from the KG.