---
ver: rpa2
title: 'SCS-SupCon: Sigmoid-based Common and Style Supervised Contrastive Learning
  with Adaptive Decision Boundaries'
arxiv_id: '2512.17954'
source_url: https://arxiv.org/abs/2512.17954
tags:
- contrastive
- scs-supcon
- learning
- loss
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses negative-sample dilution and the lack of adaptive
  decision boundaries in supervised contrastive learning, which limit performance
  in fine-grained classification tasks. The authors propose SCS-SupCon, a sigmoid-based
  contrastive loss with learnable temperature and bias parameters to create adaptive
  decision boundaries.
---

# SCS-SupCon: Sigmoid-based Common and Style Supervised Contrastive Learning with Adaptive Decision Boundaries

## Quick Facts
- arXiv ID: 2512.17954
- Source URL: https://arxiv.org/abs/2512.17954
- Authors: Bin Wang; Fadi Dornaika
- Reference count: 40
- Key outcome: Achieves state-of-the-art fine-grained classification performance, improving top-1 accuracy by ~3.9 percentage points over SupCon on CIFAR-100 and 0.4-3.0 points on fine-grained datasets.

## Executive Summary
This paper addresses negative-sample dilution and the lack of adaptive decision boundaries in supervised contrastive learning, which limit performance in fine-grained classification tasks. The authors propose SCS-SupCon, a sigmoid-based contrastive loss with learnable temperature and bias parameters to create adaptive decision boundaries. This formulation emphasizes hard negatives and mitigates negative-sample dilution. An explicit style-distance constraint further disentangles style and content representations. Experiments on six benchmark datasets show SCS-SupCon achieves state-of-the-art performance, improving top-1 accuracy by approximately 3.9 percentage points over SupCon on CIFAR-100 and 0.4-3.0 points on fine-grained datasets. Extensive ablation studies and statistical analyses confirm the method's robustness and generalization capability.

## Method Summary
SCS-SupCon extends supervised contrastive learning by replacing the InfoNCE softmax loss with a sigmoid-based pairwise logistic loss that operates independently on each labeled pair. The embedding is partitioned into common (192-dim) and style (64-dim) subspaces. The loss combines a sigmoid contrastive term on common features with a learnable temperature $t=\exp(t')$ and bias $b$, plus a style-distance penalty $\beta \cdot ||s_i - s_p||$ on positive-pair style vectors. Training occurs in two stages: Stage 1 trains encoder and projection head for 1000 epochs with the full SCS-SupCon loss; Stage 2 freezes the encoder and trains a linear classifier on common features only for 100 epochs.

## Key Results
- Achieves state-of-the-art performance on six fine-grained classification benchmarks
- Improves top-1 accuracy by approximately 3.9 percentage points over SupCon on CIFAR-100
- Shows 0.4-3.0 percentage point improvements on fine-grained datasets (CUB200, Stanford Dogs, VOC 2005)
- Demonstrates adaptive decision boundaries that converge to dataset-specific temperature and bias values

## Why This Works (Mechanism)

### Mechanism 1
The sigmoid-based pairwise contrastive loss mitigates negative-sample dilution by evaluating each labeled pair independently rather than normalizing over all negatives simultaneously. InfoNCE computes softmax over all negatives, diluting gradients from hard (informative) negatives. The sigmoid loss applies a binary logistic objective per pair: $-log(1/(1+exp(z_uv*(-t·c_u·c_v + b))))$, where $z_uv = +1$ for positive pairs and $-1$ for negatives. This allows hard negatives to receive stronger gradients proportional to their similarity. Core assumption: Hard negatives—visually similar samples from different classes—carry more discriminative signal than easy negatives and should not have their gradients diluted by the presence of many irrelevant comparisons.

### Mechanism 2
Learnable temperature $t = exp(t')$ and bias $b$ parameters enable data-adaptive decision boundaries that adjust to dataset granularity. The decision boundary in similarity space is located at $r* = b/t$. Temperature $t$ controls boundary sharpness (slope of the sigmoid transition); bias $b$ shifts the threshold for classifying a pair as positive vs. negative. These are learned jointly with the encoder via backpropagation. Core assumption: Fine-grained datasets require sharper decision boundaries (lower $t$) and higher positive-pair sensitivity (higher $b$) compared to coarse-grained datasets.

### Mechanism 3
The explicit style-distance constraint disentangles class-irrelevant style features from class-relevant common features, improving robustness to intra-class variation. The embedding is partitioned into common ($c$, 192-dim) and style ($s$, 64-dim) subspaces. A penalty $\beta \cdot ||s_i - s_p||$ on positive-pair style distances encourages style diversity within classes, preventing discriminative information from leaking into the style field. The Stage 2 classifier uses only common features. Core assumption: Intra-class style variations (pose, illumination, background) should be explicitly separated from semantic content to improve generalization.

## Foundational Learning

- **Supervised Contrastive Learning (SupCon)**: Why needed here: SCS-SupCon directly extends SupCon and CS-SupCon; understanding how SupCon incorporates class labels to define positive sets is prerequisite. Quick check question: In SupCon, how are positive samples defined differently than in self-supervised SimCLR?

- **InfoNCE Loss Structure and Negative-Sample Dilution**: Why needed here: The paper's core motivation is that InfoNCE's softmax normalization over all negatives dilutes gradients from hard negatives. Quick check question: In a batch with 100 easy negatives and 5 hard negatives, why does InfoNCE assign similar gradient weight to all of them?

- **Feature Disentanglement via Subspace Partitioning**: Why needed here: The 256-dim embedding is explicitly split into common (192) and style (64) subspaces with separate loss terms. Quick check question: What would happen if the Stage 2 classifier were trained on both common and style features concatenated?

## Architecture Onboarding

- **Component map**: Backbone (ResNet-50, ConvNeXt-T, TinyViT) -> Projection head (Linear -> ReLU -> Linear -> 256-dim) -> Embedding partition (c=first 192, s=last 64) -> Sigmoid contrastive loss with learnable t', b -> Style-distance penalty -> Stage 2 classifier (linear on c only)

- **Critical path**: 1) Stage 1 (1000 epochs): Train encoder + projection head with $L_{SCS-SupCon}$; $t'$, $b$ learned jointly; 2) Freeze encoder and projection head; 3) Stage 2 (100 epochs): Train linear classifier on common features $c$ with cross-entropy

- **Design tradeoffs**: Non-overlapping vs. overlapping partition: Paper uses non-overlapping for simplicity; overlapping variant from CS-SupCon is more flexible but less interpretable; Sigmoid vs. InfoNCE: Sigmoid adds two scalar parameters but enables adaptive boundaries; InfoNCE is parameter-free but fixed; Common-only classifier: Discarding style features avoids countering the disentanglement objective; fusion provided no consistent benefit in preliminary experiments

- **Failure signatures**: Temperature divergence: $t$ near 0 or very large → sigmoid saturation → gradient vanishing; Batch size too small: Insufficient hard negatives → boundary learning fails; $\beta$ too high: Style features become incoherent; validation accuracy drops; Early plateau in Stage 1: May indicate poor $t'$, $b$ initialization; try Bayesian search (Section 5.7)

- **First 3 experiments**: 1) CIFAR-100 baseline reproduction: Train SCS-SupCon with ResNet-50, verify ~3.9% improvement over SupCon (Table 5); confirm $t$ converges to ~0.11–0.12, $b$ to ~0.03–0.04; 2) $\beta$ sensitivity sweep: On CIFAR-100, sweep $\beta \in \{10^{-5}, 10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}\}$; verify peak at $\beta = 10^{-3}$ (Figure 8); 3) Sigmoid-only ablation: Train with $\beta = 0$ (no style penalty); confirm it still outperforms CS-SupCon, isolating the sigmoid loss contribution (Table 8)

## Open Questions the Paper Calls Out

### Open Question 1
Can explicit angular margin constraints be integrated with SCS-SupCon's adaptive sigmoid-based boundaries to further improve discrimination in extremely challenging fine-grained scenarios? Basis in paper: [explicit] The conclusion states: "it does not explicitly enforce angular margins between classes, so there is still room to further enhance the angular discriminative power... our future work will focus on developing more angular-discriminative variants of SCS-SupCon, for example by combining its adaptive decision boundaries with explicit angular margins." Why unresolved: The sigmoid-based loss operates on cosine similarities but lacks explicit angular margin enforcement; the authors identify this as a central challenge in metric learning. What evidence would resolve it: Experiments combining SCS-SupCon with angular margin objectives (e.g., ArcFace-style margins), measuring improvements on datasets with extremely subtle inter-class differences.

### Open Question 2
How does SCS-SupCon scale to ultra-large benchmarks (e.g., ImageNet, long-tailed datasets) and cross-domain scenarios? Basis in paper: [explicit] Section 6 states: "combining SCS-SupCon with large-scale pre-trained vision transformers and conducting experiments on ultra-large and cross-domain benchmarks (e.g., ImageNet and long-tailed or domain-shifted variants) would provide a more comprehensive assessment of its scalability." Why unresolved: Current experiments only cover six medium-scale benchmarks (largest: Tiny-ImageNet with 200 classes); scalability to thousands of classes or long-tailed distributions remains untested. What evidence would resolve it: Systematic evaluation on ImageNet-1K, iNaturalist, or domain-shifted variants showing whether adaptive boundaries remain beneficial under extreme class imbalance.

### Open Question 3
Can the style subspace be leveraged for semi-supervised or open-set learning while keeping the common subspace supervised? Basis in paper: [explicit] Section 6 proposes: "the explicit separation between common and style features makes SCS-SupCon a natural candidate for semi-supervised or open-set extensions, where pseudo-labelling or self-supervised objectives could be attached to the style field." Why unresolved: The current framework uses style only as a regularization signal; its potential for handling unlabeled data or unknown classes is unexplored. What evidence would resolve it: Experiments on semi-supervised benchmarks (e.g., CIFAR-100 with partial labels) or open-set recognition tasks, comparing style-based pseudo-labeling against baselines.

## Limitations
- The theoretical justification for why the sigmoid loss specifically addresses negative-sample dilution better than other pairwise losses is not fully explored
- The choice of fixed embedding dimensions (192 for common, 64 for style) and the partitioning scheme are somewhat arbitrary, though ablation studies support their effectiveness
- The method's sensitivity to hyperparameters like $\beta$ and the initial values of $t'$ and $b$ introduces potential for suboptimal performance if not carefully tuned

## Confidence
- **High Confidence**: The core claim that SCS-SupCon achieves state-of-the-art performance on fine-grained classification benchmarks is supported by extensive experiments across six datasets and comparison with multiple strong baselines. The ablation studies isolating the contribution of the sigmoid loss and style constraint are also robust.
- **Medium Confidence**: The claim that the sigmoid loss specifically mitigates negative-sample dilution is well-supported by the loss formulation and empirical results, but the theoretical mechanism could be more rigorously explained. The adaptive nature of the learned temperature and bias parameters is demonstrated but the precise relationship to dataset granularity is correlational.
- **Low Confidence**: The assertion that the style-distance constraint is the primary driver of disentanglement is less certain. The ablation study (Table 8) shows it improves performance, but the qualitative impact on the learned representations and its necessity for fine-grained tasks require further investigation.

## Next Checks
1. **Cross-Dataset Generalization**: Evaluate SCS-SupCon on a dataset outside the fine-grained domain (e.g., ImageNet) to test the robustness of the learned temperature and bias parameters to broader classification tasks
2. **Alternative Partition Schemes**: Experiment with overlapping or learned partitions of the embedding space (similar to CS-SupCon) to assess whether the fixed 192/64 split is optimal or a limitation
3. **Loss Function Ablation**: Replace the sigmoid loss with a standard binary cross-entropy loss on the same pairwise comparisons to isolate the benefit of the sigmoid formulation's gradient properties