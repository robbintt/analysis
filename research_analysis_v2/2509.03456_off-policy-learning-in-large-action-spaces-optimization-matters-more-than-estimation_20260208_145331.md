---
ver: rpa2
title: 'Off-Policy Learning in Large Action Spaces: Optimization Matters More Than
  Estimation'
arxiv_id: '2509.03456'
source_url: https://arxiv.org/abs/2509.03456
tags:
- policy
- learning
- optimization
- off-policy
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper argues that the prevalent estimator-centric approach
  in off-policy learning (OPL) is flawed due to difficult optimization landscapes,
  particularly in large action spaces. Current OPE-based objectives, while statistically
  motivated, create highly non-concave landscapes prone to local optima, and this
  problem scales with action space size.
---

# Off-Policy Learning in Large Action Spaces: Optimization Matters More Than Estimation

## Quick Facts
- arXiv ID: 2509.03456
- Source URL: https://arxiv.org/abs/2509.03456
- Authors: Imad Aouali; Otmane Sakhi
- Reference count: 40
- One-line primary result: PWLL-based methods outperform sophisticated OPE baselines in large-scale OPL, demonstrating optimization stability trumps estimation accuracy.

## Executive Summary
This paper challenges the conventional wisdom that accurate value estimation is paramount in off-policy learning (OPL). Through theoretical analysis and empirical validation, it demonstrates that OPE-based objectives create highly non-concave optimization landscapes in large action spaces, leading to poor convergence regardless of estimator quality. The authors propose Policy-Weighted Log-Likelihood (PWLL) objectives as an alternative, which prioritize optimization-friendly strongly concave landscapes over statistical accuracy. Across three large-scale datasets (MovieLens, Twitch, GoodReads), PWLL methods consistently outperform state-of-the-art OPE approaches despite having higher estimation error.

## Method Summary
The paper compares traditional OPE-based methods (IPS, DR, MIPS, OffCEM, POTEC) against PWLL objectives (LPI, cLPI, RegKL) for learning policies from logged bandit data. PWLL methods optimize objectives of the form `-mean(weight_function * log π(a|x))` where weights depend on rewards and logging policy probabilities. The theoretical contribution shows PWLL objectives with L2 regularization are strongly concave, guaranteeing unique global optima. Empirically, the authors evaluate on three large-scale datasets with up to 1M actions, comparing validation reward and convergence properties. Lightweight policy architectures are used to emphasize optimization over expressiveness.

## Key Results
- PWLL methods achieve higher validation reward than sophisticated OPE baselines (IPS, DR, MIPS, OffCEM, POTEC) across all datasets
- OPE methods with lower MSE do not correlate with better OPL performance
- Lightweight models trained with PWLL objectives converge faster and achieve higher final reward than heavyweight models
- OPE objectives become increasingly non-concave as action space size grows, while PWLL objectives maintain strong concavity

## Why This Works (Mechanism)

### Mechanism 1
OPE objectives create non-concave landscapes in large action spaces because they scale the policy linearly (π(a|x) · weight), which combined with softmax parametrization produces local maxima that grow exponentially with action space size K. PWLL objectives use log π(a|x), which with L2 regularization guarantees a unique global maximum for gradient descent. Core assumption: strong concavity holds for linear softmax models (Proposition 3.1). Break condition: highly non-linear policy parametrizations may degrade concavity guarantees.

### Mechanism 2
Effective OPL relies on optimization stability rather than estimator accuracy. OPE methods minimize variance and bias but create brittle surrogate objectives sensitive to hyperparameters. PWLL methods prioritize smooth gradient paths, ensuring consistent convergence to good policies even with poor value estimation. Core assumption: good policies can be recovered through reward-based re-weighting rather than explicit value function estimation. Break condition: extremely sparse rewards where logging policy has near-zero support on optimal actions.

### Mechanism 3
Lightweight policy architectures are sufficient for large-scale OPL with PWLL objectives because simpler models are easier to optimize and less prone to overfitting. Since PWLL objectives are already well-conditioned, heavy model capacity is unnecessary and may introduce noise. Core assumption: decision boundary complexity is low enough for lightweight models to approximate optimal policy improvement. Break condition: highly complex context-action relationships requiring deep interaction features.

## Foundational Learning

- **Concept: Inverse Propensity Scoring (IPS)**
  - Why needed here: IPS is the baseline OPE estimator that creates high variance and difficult optimization landscapes in large action spaces through its 1/π₀(a|x) weighting.
  - Quick check question: How does the variance of the IPS estimator change as the number of actions K increases while data remains finite?

- **Concept: Concavity vs. Non-Concavity in Optimization**
  - Why needed here: The core theoretical argument rests on PWLL being "strongly concave," meaning it has a single global maximum (easy to find) versus non-concave functions with many local maxima (hard to escape).
  - Quick check question: Why does the "linear in π" structure of IPS combined with softmax parametrization lead to a non-concave optimization landscape?

- **Concept: Support of the Logging Policy**
  - Why needed here: Both OPE and PWLL methods generally assume the logging policy π₀ has non-zero probability on actions we want to learn, with PWLL solutions explicitly depending on π₀.
  - Quick check question: If the logging policy never selects the optimal action for a given context, can PWLL (specifically LPI or cLPI) still learn to select it?

## Architecture Onboarding

- **Component map:** Context features x -> Policy Head (Softmax π(a|x)) -> Loss Function (PWLL vs IPS) -> Optimizer (SGD/Adam)

- **Critical path:**
  1. Fetch batch of logged data (x, a, r, π₀(a|x))
  2. Forward pass to get π_θ(a|x)
  3. Compute weight g(r, π₀) (e.g., r/max{π₀, τ} for cLPI)
  4. Compute loss = -mean(weight * log(π_θ(a|x)))
  5. Backpropagate and update θ

- **Design tradeoffs:**
  - Estimation vs. Optimization: Sacrificing unbiased value estimation for stable gradient descent
  - Model Capacity: Trading expressiveness for faster convergence and reduced overfitting
  - Bias introduction: cLPI introduces clipping bias but gains optimization stability

- **Failure signatures:**
  - OPE Failure: Training loss decreases but validation reward collapses or fluctuates wildly
  - PWLL Failure: Validation reward plateaus at suboptimal level significantly below logging policy

- **First 3 experiments:**
  1. Objective Comparison: Compare IPS vs cLPI learning curves while varying batch size (256 vs 4096) to replicate sensitivity analysis
  2. Action Space Scaling: Subsample action space (100, 1k, 10k, 60k items) and plot final policy value for PWLL vs OPE methods
  3. Architecture Ablation: Train lightweight vs heavyweight policies using cLPI to verify lightweight convergence speed

## Open Questions the Paper Calls Out
The paper states its analysis can be extended to other estimators [2, 23, 28] but omits this for conciseness, leaving the formal analysis of Exponential Smoothing (ES) and Policy Convolution landscapes open. Additionally, the paper assumes known logging policy π₀, but does not test performance when π₀ must be estimated from data, which is a common practical challenge. Finally, it remains unclear whether a hybrid objective can retain PWLL's strong concavity while incorporating reward models' generalization benefits.

## Limitations
- Theoretical strong concavity guarantees assume linear softmax parametrization; performance with non-linear architectures remains untested
- Empirical evaluation relies on logged data from a single source; results may not generalize to other logging policies
- Lightweight models' sufficiency depends on context-action complexity; may not hold for highly non-linear decision boundaries

## Confidence
- High: Optimization landscapes are harder in large action spaces for OPE methods; PWLL methods are more stable to train
- Medium: PWLL methods achieve better validation reward than OPE methods in tested settings
- Low: Lightweight models are always sufficient; theoretical strong concavity extends seamlessly to deep models

## Next Checks
1. **Architecture sensitivity test:** Train PWLL methods with varying model capacities (linear to deep) to verify lightweight sufficiency across complexity levels
2. **Logging policy ablation:** Repeat experiments with synthetic logging policies (high-entropy vs deterministic) to test support assumptions
3. **Convergence analysis:** Vary batch sizes and learning rates systematically for both OPE and PWLL methods to quantify optimization sensitivity