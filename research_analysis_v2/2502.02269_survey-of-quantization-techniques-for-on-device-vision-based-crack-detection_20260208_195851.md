---
ver: rpa2
title: Survey of Quantization Techniques for On-Device Vision-based Crack Detection
arxiv_id: '2502.02269'
source_url: https://arxiv.org/abs/2502.02269
tags:
- quantization
- accuracy
- crack
- techniques
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates quantization techniques for vision-based crack
  detection in structural health monitoring, focusing on resource-constrained UAV
  deployments. Two lightweight MobileNet models (MBNV1x0.25 and MBNV2x0.5) were assessed
  using dynamic quantization, post-training quantization (PTQ), and quantization-aware
  training (QAT) across TensorFlow, PyTorch, and ONNX platforms.
---

# Survey of Quantization Techniques for On-Device Vision-based Crack Detection

## Quick Facts
- arXiv ID: 2502.02269
- Source URL: https://arxiv.org/abs/2502.02269
- Reference count: 19
- Two lightweight MobileNet models (MBNV1x0.25 and MBNV2x0.5) evaluated using dynamic quantization, PTQ, and QAT across TensorFlow, PyTorch, and ONNX platforms.

## Executive Summary
This study evaluates quantization techniques for vision-based crack detection in structural health monitoring, focusing on resource-constrained UAV deployments. Two lightweight MobileNet models (MBNV1x0.25 and MBNV2x0.5) were assessed using dynamic quantization, post-training quantization (PTQ), and quantization-aware training (QAT) across TensorFlow, PyTorch, and ONNX platforms. Results show that QAT consistently achieves near-floating-point accuracy (e.g., F1-score of 0.8376 for MBNV2x0.5 with Torch-QAT) while maintaining efficient resource usage. PTQ significantly reduces memory and energy consumption but suffers from accuracy loss, particularly in TensorFlow. Dynamic quantization preserves accuracy but faces deployment challenges on PyTorch. The findings highlight QAT as the most robust technique for balancing accuracy and efficiency in autonomous crack detection systems on UAVs.

## Method Summary
The study fine-tuned pretrained MobileNetV1×0.25 and MobileNetV2×0.5 models on the SDNET2018 crack detection dataset (16,800 images, 224×224 RGB, split 8:1:1). Models were trained using TensorFlow 2.8.0 and PyTorch 2.5.1 with all layers unfrozen for 10 epochs at learning rate 0.001. Three quantization techniques were applied: dynamic quantization (runtime activation quantization), PTQ (calibration-based INT8 mapping), and QAT (training with fake quantization nodes). Models were converted to TensorFlow Lite and ONNX formats, then deployed via STM32Cube.AI on an STM32H7B3I-DK board. Performance was measured using micro-averaged F1-score, inference time, MACC, energy consumption, and memory usage.

## Key Results
- QAT consistently achieves near-floating-point accuracy, with F1-scores of 0.8285 (TF) and 0.8463 (PyTorch) for crack detection tasks.
- PTQ-TF achieves lowest inference time (14.776 ms) and energy consumption (16.549 mJ) but lowest accuracy (F1: 0.6843).
- PTQ-ONNX demonstrates best memory efficiency (274,379 bytes flash, 317,120 bytes RAM) with moderate accuracy (F1: 0.7682).
- Dynamic quantization preserves accuracy close to floating-point models but cannot be deployed via ONNX for PyTorch models.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Quantization-Aware Training (QAT) preserves near-floating-point accuracy by simulating quantization effects during training.
- Mechanism: During QAT, fake quantization nodes are inserted into the computational graph, allowing backpropagation to adjust weights while accounting for INT8 precision loss. The model learns compensatory weight distributions that remain stable after actual quantization.
- Core assumption: The training dataset sufficiently represents inference-time input distributions, enabling learned compensations to generalize.
- Evidence anchors:
  - [abstract] "QAT consistently achieves near-floating-point accuracy, such as an F1-score of 0.8376 for MBNV2x0.5 with Torch-QAT"
  - [Section IV-A] "QAT achieves performance close to FP models, with F1-scores of 0.8285 (TF) and 0.8463 (PyTorch) for the crack task"
  - [corpus] Related work on PTQAT (arXiv:2508.10557) confirms QAT avoids PTQ performance degradation but notes GPU memory/training costs
- Break condition: If training resources are unavailable or the calibration distribution diverges significantly from inference data, QAT benefits diminish. INT4 or lower quantization may exceed QAT's compensation capacity.

### Mechanism 2
- Claim: Post-Training Quantization (PTQ) achieves maximum resource reduction but introduces platform-dependent accuracy degradation.
- Mechanism: PTQ maps FP32 weights/activations to INT8 using calibration statistics without retraining. Platform-specific implementations differ: TensorFlow stores INT8 weights with separate quantize/dequantize layers (fully INT8 computation), while ONNX/PyTorch dequantize weights back to FP32 for convolution, introducing additional operations.
- Core assumption: The calibration dataset (subset of training data) captures activation range statistics representative of production inputs.
- Evidence anchors:
  - [Section IV-B] "PTQ-TF achieves the lowest inference time (14.776 ms), MACC (41,091,156), and energy consumption (16.549 mJ), but only reaches an F1 score of 0.6843"
  - [Section IV-B] "PTQ-ONNX demonstrates the best resource efficiency, requiring only 274,379 bytes of flash and 317,120 bytes of RAM"
  - [corpus] Limited direct corpus evidence on PTQ-specific degradation mechanisms; corpus focuses on LLM quantization safety (arXiv:2502.15799)
- Break condition: Highly non-uniform weight/activation distributions (common in crack detection with sparse positive samples) cause calibration outliers that compress dynamic range, degrading detection of minority class features.

### Mechanism 3
- Claim: Dynamic quantization preserves accuracy but faces deployment incompatibilities on ONNX conversion pathways.
- Mechanism: Dynamic quantization quantizes activations at runtime based on observed ranges, avoiding calibration dependency. However, ONNX (version 1.18.0) does not support conversion of dynamically quantized PyTorch models, blocking MCU deployment via this pathway.
- Core assumption: Runtime quantization overhead remains acceptable for target latency requirements.
- Evidence anchors:
  - [Section IV-A] "Dynamic quantization has minimal impact on the performance, with F1-scores close to the FP models in both tasks"
  - [Section IV-B] "DynamicQ-Torch-MBNV1x0.25" and "DynamicQ-Torch-MBNV2x0.5" show "-" for all deployment metrics, indicating failed deployment
  - [Section IV-B] "For PyTorch models, dynamic quantization could not be deployed, as ONNX does not support converting dynamically quantized models"
  - [corpus] No corpus evidence on dynamic quantization deployment constraints
- Break condition: If ONNX is required in the toolchain (e.g., for STM32Cube.AI compatibility), dynamic quantization is non-viable regardless of accuracy benefits.

## Foundational Learning

- Concept: **Depth-wise separable convolutions (MobileNet architecture)**
  - Why needed here: Understanding why MobileNet variants (V1 with depth-wise separable, V2 with inverted residuals) are chosen as lightweight baselines for MCU deployment.
  - Quick check question: How does a depth-wise separable convolution reduce MACC compared to standard convolution for a 3×3 kernel on 64 input/output channels?

- Concept: **Calibration dataset selection for PTQ**
  - Why needed here: PTQ accuracy depends on calibration data representing inference distribution; crack detection has class imbalance requiring careful sampling.
  - Quick check question: If your calibration set contains 90% non-crack images, what quantization artifact might appear on crack detection at inference?

- Concept: **Quantization granularity (per-tensor vs. per-channel)**
  - Why needed here: The paper reports differing PTQ accuracy across platforms, partly due to implementation choices in quantization parameter sharing.
  - Quick check question: Why would per-channel quantization improve accuracy for convolutional layers with output channels having different weight magnitude distributions?

## Architecture Onboarding

- Component map:
Training Framework (TF/PyTorch) -> Pretrained Model (ImageNet) -> Fine-tuning (SDNET2018) -> Quantization Technique (PTQ/QAT/Dynamic) -> Export Format (.tflite/.onnx) -> STM32Cube.AI Conversion -> C/C++ Deployment on STM32H7B3I-DK

- Critical path: QAT implementation requires: (1) modifying training loop with quantization-aware operations, (2) fine-tuning for sufficient epochs to converge, (3) converting to target format with quantization parameters embedded. Any break in this chain falls back to PTQ with accuracy penalty.

- Design tradeoffs:
  - **TF-PTQ**: Lowest inference time (14.776 ms) and energy (16.549 mJ), but severe accuracy drop (F1: 0.6843). Use only if latency constraints dominate.
  - **ONNX-PTQ**: Best memory efficiency (274KB flash), moderate accuracy (F1: 0.7682). Best for flash-constrained devices.
  - **Torch-QAT**: Best accuracy (F1: 0.8376), but highest training cost and 38 ms inference. Use when detection quality is critical.
  - **Dynamic quantization**: Not deployable via ONNX pathway; avoid for STM32 targets.

- Failure signatures:
  - F1-score drop >10% after quantization → Likely PTQ with insufficient/misrepresentative calibration data
  - Inference failure or "-" metrics after conversion → ONNX incompatibility with dynamic quantization
  - Flash overflow on MCU → Model too large; reduce width multiplier or apply PTQ
  - Inconsistent TF vs. PyTorch PTQ accuracy → Activation function mismatch (ReLU vs. Clip in MBNV2x0.5)

- First 3 experiments:
  1. **Baseline calibration sensitivity**: Train MBNV1x0.25 on SDNET2018, apply TF-PTQ with calibration sets of varying crack/non-crack ratios (50/50, 80/20, 95/5). Measure F1-score degradation to quantify calibration importance.
  2. **QAT convergence analysis**: Compare Torch-QAT accuracy at epochs 1, 5, 10, 20 on MBNV2x0.5. Plot F1-score vs. training cost to identify minimum viable training duration.
  3. **Memory budget mapping**: For each quantization technique/model combination, measure flash/RAM usage on STM32H7B3I-DK. Create decision matrix: if flash <300KB required → ONNX-PTQ; if accuracy >0.80 required → QAT; if latency <20ms required → TF-PTQ with accuracy tolerance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do QAT-based models perform in real-world UAV-supported Structural Health Monitoring scenarios when subjected to variable environmental and operational factors?
- Basis in paper: [explicit] The Conclusion states that validating models in real-world UAV-supported scenarios "considering environmental and operational factors" is a priority for future work.
- Why unresolved: The current study relies on the SDNET2018 benchmark dataset and laboratory deployment on an STM32H7B3I-DK board, which does not capture the dynamic variability of outdoor inspection environments.
- What evidence would resolve it: Field trials on UAVs comparing the F1-scores and energy consumption of QAT models against baselines in diverse lighting, weather, and surface conditions.

### Open Question 2
- Question: Can Post-Training Quantization (PTQ) methodologies be refined to minimize the significant accuracy degradation observed in TensorFlow implementations for crack detection?
- Basis in paper: [explicit] The Future Work section explicitly identifies "improving PTQ to enhance accuracy" as a research focus, noting that current PTQ techniques suffer from considerable accuracy loss (e.g., F1-score drops to 0.64).
- Why unresolved: The paper identifies the accuracy gap (up to 10-15% loss) as a consistent failure mode for PTQ but does not propose a solution to bridge the gap between PTQ efficiency and QAT accuracy.
- What evidence would resolve it: A modified PTQ pipeline (e.g., advanced calibration sets or hybrid rounding) that achieves F1-scores within 1-2% of the floating-point baseline on the TensorFlow backend.

### Open Question 3
- Question: What toolchain or framework modifications are required to successfully deploy dynamically quantized PyTorch models on microcontrollers using the ONNX workflow?
- Basis in paper: [explicit] The paper notes that "Dynamic quantization... faces deployment challenges on PyTorch" and Future Work aims to address these deployment challenges.
- Why unresolved: The authors found that while dynamic quantization preserves accuracy, the ONNX converter failed to support dynamically quantized models for the target MCU, preventing deployment entirely.
- What evidence would resolve it: A successful conversion pipeline that translates a dynamically quantized PyTorch model to a functional ONNX representation compatible with STM32Cube.AI or similar embedded runtimes.

## Limitations
- Evaluation limited to specific crack detection dataset (SDNET2018) and lightweight MobileNet architectures.
- Calibration dataset composition and size for PTQ not fully specified, introducing variability in reported accuracy differences.
- ONNX toolchain incompatibility with PyTorch dynamic quantization blocks a potentially valuable technique for certain deployment scenarios.

## Confidence
- **High confidence**: QAT consistently achieves near-floating-point accuracy across frameworks (supported by multiple F1-score measurements: 0.8376 for Torch-QAT, 0.8285 for TF-QAT).
- **Medium confidence**: PTQ achieves maximum resource efficiency but suffers platform-dependent accuracy degradation (evidence shows 14.776 ms inference for TF-PTQ vs. 38 ms for Torch-QAT, but accuracy drops from 0.82 to 0.68).
- **Low confidence**: Dynamic quantization accuracy preservation claims, as deployment failure prevents full validation (metrics marked "-" for ONNX-exported PyTorch models).

## Next Checks
1. **Calibration sensitivity analysis**: Systematically vary calibration dataset crack/non-crack ratios (50/50, 80/20, 95/5) for TF-PTQ on MBNV1x0.25 and measure per-class F1-score degradation to quantify calibration impact.
2. **ONNX toolchain exploration**: Investigate whether newer ONNX versions or alternative export methods support PyTorch dynamic quantization, or benchmark TF dynamic quantization as a replacement for STM32 deployment.
3. **Model architecture generalization**: Repeat QAT evaluation on a different lightweight architecture (e.g., EfficientNet-B0) using the same quantization-aware training protocol to assess technique robustness beyond MobileNet variants.