---
ver: rpa2
title: A Code Comprehension Benchmark for Large Language Models for Code
arxiv_id: '2507.10641'
source_url: https://arxiv.org/abs/2507.10641
tags:
- code
- task
- tasks
- comprehension
- grading
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a benchmark suite to evaluate code comprehension
  in large language models beyond surface-level syntactic pattern matching. The authors
  focus on semantics-oriented tasks such as subjectivity grading, question answering,
  code search, test-case prediction, bug fixing, and code comparison.
---

# A Code Comprehension Benchmark for Large Language Models for Code

## Quick Facts
- arXiv ID: 2507.10641
- Source URL: https://arxiv.org/abs/2507.10641
- Authors: Jayant Havare; Saurav Chaudhary; Ganesh Ramakrishnan; Kaushik Maharajan; Srikanth Tamilselvam
- Reference count: 21
- One-line primary result: Fine-tuning code models on semantically rich tasks significantly improves code comprehension beyond syntactic pattern matching.

## Executive Summary
This paper introduces a benchmark suite to evaluate code comprehension in large language models (LLMs) beyond surface-level syntactic pattern matching. The authors focus on semantics-oriented tasks such as subjectivity grading, question answering, code search, test-case prediction, bug fixing, and code comparison. They fine-tune three code models of varying sizes (8B, 22B, 32B parameters) on large-scale datasets to enhance their ability to understand code semantics. The most significant improvement is observed in the QWQ-32B model, where accuracy increases from 70% to 83.47% on the Subjectivity Grading Task. The DPO-fine-tuned Codestral-22B achieves the highest micro-accuracy of 87.66% on the same task. These results demonstrate that fine-tuning on semantically rich tasks significantly enhances code comprehension abilities in LLMs.

## Method Summary
The paper fine-tunes three base code models (QWQ-32B, Codestral-22B, Granite-8B) on downstream code comprehension tasks using supervised learning and Direct Preference Optimization (DPO). The primary dataset is CS101-Gold (27,699 datapoints) for Subjectivity Grading, supplemented with CodeQA, CodeSearchNet, Bug-Fix Pairs, and Code Comparison datasets. Models are fine-tuned independently on individual tasks and in combinations, with Codestral-22B specifically using DPO. Evaluation uses micro-accuracy and micro-F1 for classification tasks, ROUGE for retrieval/QA, and CodeBLEU for bug fixing. The approach aims to shift models from syntactic pattern matching to semantic understanding through task-specific fine-tuning.

## Key Results
- QWQ-32B accuracy improves from 70% to 83.47% on Subjectivity Grading Task after fine-tuning
- DPO-fine-tuned Codestral-22B achieves highest micro-accuracy of 87.66% on Subjectivity Grading
- Training on Bug Fix Task transfers effectively to Subjectivity Grading for Codestral-22B
- Larger models (Codestral-22B) show higher potential but also greater instability due to catastrophic forgetting

## Why This Works (Mechanism)

### Mechanism 1: Semantic Objective Shifting
If a model is fine-tuned on tasks requiring functional verification rather than next-token prediction, it may develop a more robust internal representation of code semantics. Standard pre-training optimizes for surface-level syntactic likelihood. By shifting the optimization objective to rubric adherence or functional correctness, the model is compelled to attend to control flow and data dependencies rather than just token co-occurrence. Core assumption: The model has sufficient capacity to map natural language intent to executable logic during fine-tuning.

### Mechanism 2: Repair-to-Assessment Transfer
Training on generative repair tasks (Bug Fix) appears to transfer more effectively to assessment tasks (Subjectivity Grading) than training on the assessment task itself for specific high-performance models. The generative pressure to "fix" code requires higher-fidelity error localization and correction than simply "grading" existing code. This rigorous reconstruction of correct logic may reinforce weights responsible for semantic evaluation, leading to superior grading performance. Core assumption: The semantic knowledge required to fix a bug is a superset of the knowledge required to identify the bug.

### Mechanism 3: Scale-Dependent Forgetting
Larger, high-performing models are more susceptible to performance degradation when fine-tuned on specific downstream tasks compared to models with more "headroom." Models already near saturation on code benchmarks may have fragile weight configurations that are easily disrupted by fine-tuning on narrower datasets, leading to regression on the target task. Core assumption: The degradation is due to weight interference rather than dataset misalignment.

## Foundational Learning

- **Concept: Surface-Level Syntax vs. Semantics**
  - Why needed here: The paper's central thesis is that "Next Token Prediction" teaches syntax but not semantics. Distinguishing these is vital for interpreting benchmark results.
  - Quick check question: Why does a model generate syntactically valid code that fails to execute (e.g., Fig 1a, Babylonian method)?

- **Concept: Catastrophic Forgetting**
  - Why needed here: Observed explicitly in the Codestral-22B results; a new engineer must understand why adding more data can sometimes reduce accuracy.
  - Quick check question: Why might a model lose general coding capability when intensely fine-tuned on a specific dataset like CS101-Gold?

- **Concept: Post-hoc vs. Pre-hoc Comprehension**
  - Why needed here: The paper defines evaluation dimensions based on whether the model analyzes existing code (Post-hoc) or generates new code (Pre-hoc).
  - Quick check question: Is "Code Refactoring" strictly Pre-hoc, or does it require Post-hoc analysis first? (The paper implies it requires both).

## Architecture Onboarding

- **Component map:** QWQ-32B, Codestral-22B, Granite-8B models → CS101-Gold dataset (27,699 datapoints) → Supervised Fine-Tuning/DPO → Subjectivity Grading Task evaluation
- **Critical path:** Select base model → Pre-process CS101-Gold into `<P, SP, C, RC>` tuples → Apply Supervised Fine-Tuning on Subjectivity Grading → Evaluate using Micro-Accuracy metric
- **Design tradeoffs:** Model Size vs. Stability (larger models offer higher peaks but are unstable); Task Combination (multi-task generally improves metrics but requires careful data balancing)
- **Failure signatures:** Syntactic Hallucination (model refactors code to look cleaner but breaks dependencies); False Confidence (model judges buggy code as correct because it "looks" like solution pattern); Output Instability (model generates responses in wrong format)
- **First 3 experiments:** 1) Baseline Assessment: Run zero-shot inference on CS101-Gold test set using base QWQ-32B (Expected: ~70%); 2) Semantic Fine-Tuning: Fine-tune QWQ-32B exclusively on Subjectivity Grading Task (Target: >83%); 3) Cross-Task Transfer: Fine-tune separate checkpoint on Bug Fix Task, then evaluate on Subjectivity Grading (Target: Match or exceed SFT performance)

## Open Questions the Paper Calls Out

- **Open Question 1:** Does integrating dynamic execution data, such as debugger output, into LLM inputs significantly improve performance on semantic code comprehension tasks?
  - Basis in paper: The authors state in the conclusion: "we are exploring ways where we can provide debugger output to models so that they can comprehend the code better."
  - Why unresolved: The current study focuses on static code analysis without utilizing runtime information.
  - What evidence would resolve it: A comparison of model performance on debugging tasks with and without debugger trace prompts.

- **Open Question 2:** Can incorporating inherent structural representations (e.g., ASTs, CFGs) into training enhance semantic understanding more effectively than task-based fine-tuning?
  - Basis in paper: The conclusion mentions "exploring other tasks where models can better capture the semantics, considering the inherent structure of the code."
  - Why unresolved: The experiments rely on text-based fine-tuning datasets rather than explicitly modeling code structure.
  - What evidence would resolve it: Ablation studies comparing structure-aware training against the current fine-tuning approach on the proposed benchmark.

- **Open Question 3:** Does multi-task fine-tuning on semantic tasks induce catastrophic forgetting in high-performing code models?
  - Basis in paper: The authors observe that Codestral-22B's performance degraded on subjectivity grading after training on downstream tasks, hypothesizing "catastrophic forgetting" as a potential cause.
  - Why unresolved: The paper identifies the performance drop but does not isolate the forgetting mechanism or propose mitigations.
  - What evidence would resolve it: Evaluation of pre-training knowledge retention before and after sequential fine-tuning.

## Limitations

- **Dataset Access Barrier:** The core CS101-Gold dataset is not publicly available, creating significant reproducibility challenges and uncertainty about whether the grading rubric captures true semantic understanding versus syntactic surface features.
- **Model-Specific Findings:** The most impressive results (87.66% accuracy with DPO-fine-tuned Codestral-22B) may be model-specific rather than generalizable, with observed catastrophic forgetting suggesting the approach may be brittle and dependent on model architecture.
- **Metric Limitations:** Micro-accuracy treats all grading decisions equally, but Subjectivity Grading may involve criteria of varying importance or difficulty that the metric doesn't appropriately weight.

## Confidence

**High Confidence:**
- Benchmark suite design and task categorization are methodologically sound
- General observation that fine-tuning improves code comprehension is supported
- Distinction between syntactic pattern matching and semantic comprehension is validated

**Medium Confidence:**
- Specific performance numbers are likely accurate for reported experiments but may not generalize
- Catastrophic forgetting observation in Codestral-22B is credible but mechanisms need deeper analysis

**Low Confidence:**
- Transferability claims between different task types lack sufficient empirical validation
- Assertion that improvements represent "robust understanding" rather than sophisticated pattern matching

## Next Checks

1. **Dataset Validation and Replication:** Attempt to obtain the CS101-Gold dataset or create a comparable benchmark using publicly available programming assignments with subjective grading criteria. Validate that the grading rubric captures true semantic understanding by testing whether models can distinguish between functionally equivalent but stylistically different implementations.

2. **Cross-Model Transferability Study:** Replicate the fine-tuning experiments on at least two additional code models (e.g., CodeLlama, StarCoder) to determine whether observed performance improvements and catastrophic forgetting patterns are model-agnostic or specific to tested architectures. Include both standard fine-tuning and DPO methods.

3. **Ablation on Task Combinations:** Systematically test the contribution of each auxiliary task (CodeQA, CodeSearch, Bug Fix, Code Comparison) to Subjectivity Grading performance. Include single-task fine-tuning, all possible pairwise combinations, and full multi-task setup to identify which tasks provide beneficial transfer learning signals versus potential interference.