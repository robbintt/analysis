---
ver: rpa2
title: Do Large Language Models Know What They Are Capable Of?
arxiv_id: '2512.24661'
source_url: https://arxiv.org/abs/2512.24661
tags:
- llms
- contract
- success
- confidence
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LLMs are systematically overconfident when predicting their own
  success on tasks, but most retain better-than-random discriminatory power. Newer
  and larger models do not consistently show improved calibration, with only Claude
  models displaying a clear trend of decreasing overconfidence.
---

# Do Large Language Models Know What They Are Capable Of?

## Quick Facts
- arXiv ID: 2512.24661
- Source URL: https://arxiv.org/abs/2512.24661
- Authors: Casey O. Barkan; Sid Black; Oliver Sourbut
- Reference count: 40
- LLMs are systematically overconfident when predicting their own success on tasks, but most retain better-than-random discriminatory power.

## Executive Summary
This paper evaluates whether large language models can accurately assess their own capabilities on coding tasks. The authors find that all tested LLMs exhibit systematic overconfidence - they consistently overestimate their probability of success - yet most maintain better-than-random ability to discriminate between tasks they can and cannot solve (AUROC > 0.5). Notably, newer and larger models do not show consistent improvements in calibration, with only Claude models demonstrating decreasing overconfidence as they increase in size. The study reveals that LLMs make decisions consistent with expected-utility maximization given their self-reported probabilities, but their overly optimistic estimates lead to suboptimal outcomes in work contract scenarios.

## Method Summary
The study employs three experimental paradigms to evaluate LLM confidence calibration. Experiment 1 uses single-step BigCodeBench tasks where LLMs provide in-advance confidence estimates before attempting each task. Experiment 2 presents sequential 9-contract work scenarios with $1 reward/penalty where models must decide whether to accept based on their confidence estimates, with full history maintained in context. Experiment 3 evaluates multi-step SWE-Bench Verified tasks with intermediate confidence estimates after each tool call. The authors measure overconfidence (predicted vs. true success rate), discriminatory power (AUROC), contract acceptance rates, and expected profit. Reasoning models had their hidden chain-of-thought disabled for single-step tasks to obtain genuine in-advance estimates.

## Key Results
- All 12 tested LLMs show systematic overconfidence in predicting task success
- Most models maintain AUROC > 0.5, indicating better-than-random discriminatory power
- Claude models show decreasing overconfidence with model size; other families do not
- Reasoning models perform no better than non-reasoning models in confidence estimation
- Some models (particularly Claude Sonnet and GPT 4.5) improve calibration after experiencing in-context failures
- LLMs make decisions approximately consistent with expected-utility maximization given their self-reported probabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs possess better-than-random discriminatory power in predicting task success, despite systematic overconfidence.
- Mechanism: LLMs assign higher predicted success probabilities to tasks they ultimately solve than to tasks they fail, creating separable distributions of confidence estimates. This separation is quantified via AUROC.
- Core assumption: The self-reported confidence estimates reflect an internal assessment process rather than random or pattern-matched outputs.
- Evidence anchors:
  - [abstract] "All LLMs we tested are overconfident, but most predict their success with better-than-random discriminatory power."
  - [section 3] "Most tested LLMs have a better-than-random ability to discriminate between tasks they can and cannot solve."
  - [corpus] Related work (Cash et al., Xu et al.) finds similar overconfidence patterns on trivia and interpretation tasks, but corpus lacks direct replication on coding tasks.
- Break condition: If LLM confidence estimates become decorrelated from task features (e.g., via prompt injection or adversarial task framing), discriminatory power should degrade toward AUROC ≈ 0.5.

### Mechanism 2
- Claim: In-context experiences of failure enable some LLMs to reduce overconfidence and improve decision-making, but this learning is model-dependent.
- Mechanism: Sequential presentation of past failures in context allows models to adjust their acceptance threshold without substantially changing their confidence estimates. Risk aversion amplifies the behavioral effect: small reductions in reported confidence produce larger reductions in contract acceptance.
- Core assumption: The in-context failures are attended to and integrated into subsequent predictions, rather than being ignored or discounted.
- Evidence anchors:
  - [abstract] "Some models (particularly Claude Sonnet and GPT 4.5) reduced overconfidence and improved decision-making after in-context experiences of failure, though no model fully eliminated overconfidence."
  - [section 4] "Sonnet 3.5 learns to accept much fewer contracts, roughly achieving the perfect baseline of 50% contract acceptance rate."
  - [corpus] Fang et al. (2025) report calibration improvements from in-context success/failure summaries, but corpus lacks resource acquisition decision-making evaluations.
- Break condition: If failures are presented with confounding variables (e.g., task difficulty labels), learning may misattribute causes. If context window is saturated, older failures may be forgotten.

### Mechanism 3
- Claim: LLM decisions are approximately rational given their self-reported probability estimates, but overconfidence causes suboptimal outcomes.
- Mechanism: LLMs apply consistent decision thresholds to their confidence estimates and make choices consistent with expected-utility maximization under those beliefs. Positive risk aversion further modulates acceptance rates.
- Core assumption: Self-reported confidence is the operative belief state guiding decisions, rather than a post-hoc rationalization.
- Evidence anchors:
  - [abstract] "All LLMs made decisions approximately consistent with expected-utility maximization given their self-reported probabilities, but their overly optimistic estimates led to suboptimal outcomes."
  - [appendix A] "The classification accuracy [for threshold adherence] is computed for each value of w... The values are close to 1, indicating that all LLMs quite consistently adhere to a decision threshold."
  - [corpus] Prior work (Chen et al. 2023) finds LLMs exhibit economic rationality; corpus confirms risk aversion is consistent across models.
- Break condition: If tasks have asymmetric or ambiguous reward structures, threshold consistency may break down. If confidence elicitation changes the decision process (obervation effect), rationality measures may not reflect default behavior.

## Foundational Learning

- Concept: **Calibration vs. Discriminatory Power**
  - Why needed here: These are independent dimensions. A model can be perfectly calibrated (predicted 70% success → actual 70% success) with poor discrimination, or miscalibrated with excellent discrimination. The paper finds overconfidence (miscalibration) with AUROC > 0.5 (discrimination).
  - Quick check question: If a model predicts 80% confidence on all tasks and succeeds 50% of the time, is it calibrated? Does it have discriminatory power?

- Concept: **Expected Utility and Risk Aversion**
  - Why needed here: Appendix A demonstrates LLMs have positive Arrow-Pratt risk aversion. This means a small drop in confidence produces a disproportionately large drop in contract acceptance, explaining why Sonnet 3.5's modest overconfidence reduction yields large behavioral changes.
  - Quick check question: Why would a risk-averse agent with 55% confidence decline a 50/50 contract with equal reward and penalty?

- Concept: **AUROC (Area Under Receiver Operating Characteristic Curve)**
  - Why needed here: AUROC quantifies how well confidence scores separate successful from failed tasks. AUROC = 0.5 is random guessing; AUROC = 1.0 is perfect discrimination. The paper uses DeLong et al.'s method for confidence intervals on correlated time-series data.
  - Quick check question: If an LLM assigns 90% confidence to solvable tasks and 60% confidence to unsolvable tasks, what would its AUROC be approximately?

## Architecture Onboarding

- Component map:
  - Task construction (BCB/SWE-Bench) -> Confidence elicitation -> Task execution -> Outcome recording -> Metrics computation (AUROC, overconfidence, acceptance rate)

- Critical path:
  1. Construct balanced task sets (S = solvable by all LLMs, F = unsolvable by all LLMs) to ensure 50% base rate.
  2. Force reasoning budget to 0 for single-step tasks to obtain true in-advance estimates.
  3. Elicit confidence in consistent format ("LIKELIHOOD OF SUCCESS: X%") before any task attempt.
  4. Compute AUROC using outcome labels and confidence estimates across all samples.

- Design tradeoffs:
  - **Hidden CoT exclusion**: Necessary for in-advance estimates on single-step tasks, but limits evaluation of reasoning models' native behavior.
  - **Self-reported confidence**: May not reflect "true" internal beliefs, though Appendix A validates decision-consistency. Alternative: token probability probes (white-box).
  - **Task domain**: Coding tasks chosen for agentic relevance; may not generalize to other domains.

- Failure signatures:
  - AUROC approaching 0.5 indicates no discriminatory signal.
  - Contract acceptance rate stuck at ~1.0 despite repeated failures indicates learning failure.
  - Confidence estimates not predicting decisions (low threshold adherence) suggests elicitation decoupling.

- First 3 experiments:
  1. Replicate Experiment 1 on a small BigCodeBench subset (20 tasks) with a single model to validate the confidence elicitation pipeline and AUROC calculation.
  2. Run a 3-contract mini version of Experiment 2 to verify in-context learning behavior and profit calculation before scaling to 9 contracts.
  3. Test a single SWE-Bench task with step-wise confidence elicitation to confirm tool call instrumentation works before running the full 500-task evaluation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does in-context learning from success and failure affect LLM decision-making when applied to multi-step tasks rather than single-step coding tasks?
- Basis in paper: [explicit] The authors note a significant limitation in Experiment 2 was the use of single-step tasks to allow for in-advance estimates and explicitly state that "future work could repeat Experiment 2 using such multi-step tasks" to better understand learning dynamics.
- Why unresolved: The paper's design for Experiment 2 necessitated single-step tasks to isolate in-advance confidence, leaving the interaction between multi-step reasoning, intermediate confidence updates, and learning from failure unexplored in a resource acquisition context.
- What evidence would resolve it: Results from a modified Experiment 2 protocol using multi-step agentic tasks (like SWE-Bench) where the model must decide whether to accept or reject a contract based on predicted success over a multi-step process.

### Open Question 2
- Question: Does LLM overconfidence and discriminatory power persist when evaluating dangerous capabilities relevant to AI misuse?
- Basis in paper: [explicit] The authors suggest that "expanding our experiments to tasks that evaluate dangerous capabilities could inform estimates of AI misuse and misalignment risks," specifically mentioning AI control evaluations.
- Why unresolved: The current study focuses on coding proficiency (BigCodeBench, SWE-Bench). It is unknown if the systematic overconfidence observed in standard coding tasks translates to niche, high-stakes capabilities like writing obfuscated code or exploiting vulnerabilities.
- What evidence would resolve it: Calibration metrics (AUROC and overconfidence) derived from offering LLMs "contracts" on tasks designed to test control evasion or cyber-offense capabilities.

### Open Question 3
- Question: How does the calibration of frontier LLMs compare to that of well-calibrated human experts on the same long-horizon coding tasks?
- Basis in paper: [explicit] The authors state that "experiments comparing LLMs to well-calibrated humans may be especially informative" but note that obtaining human baselines for long coding tasks was too expensive to include in the current work.
- Why unresolved: Without human baselines on the specific BigCodeBench and SWE-Bench tasks used, it is difficult to determine if the observed overconfidence and limited discriminatory power are unique failure modes of LLMs or common properties of intelligent agents on these difficult tasks.
- What evidence would resolve it: A study involving human software engineers estimating their probability of success on the same BCB tasks prior to attempting them, compared against the LLM performance profiles.

### Open Question 4
- Question: Can valid in-advance confidence estimates be elicited from reasoning models without disabling their hidden chain-of-thought capabilities?
- Basis in paper: [inferred] The paper notes that hidden chain-of-thought was excluded in Experiments 1 and 2 because reasoning models "can solve entire single-step tasks in hidden chain-of-thought, preventing us from obtaining in-advance confidence estimates."
- Why unresolved: It remains unclear if the "in-advance" concept is incompatible with current reasoning architectures or if prompting strategies could elicit a "true" prior confidence before the hidden reasoning process concludes the task internally.
- What evidence would resolve it: The development of elicitation techniques or "white-box" methods that can extract a confidence estimate from a reasoning model before the internal reasoning trace has converged on a solution.

## Limitations

- **Limited generalizability**: All experiments use coding tasks (Python programming), making it unclear whether findings extend to other domains like mathematical reasoning or commonsense QA.
- **Single-step task constraint**: Experiments 1 and 2 required single-step tasks to obtain in-advance confidence estimates, limiting evaluation of multi-step reasoning models' native behavior.
- **Self-report reliance**: The study depends on self-reported confidence estimates rather than objective measures of internal belief states, which may not fully capture model self-awareness.

## Confidence

- **High Confidence**: LLMs systematically exhibit overconfidence in predicting task success while maintaining better-than-random discriminatory power (supported by consistent AUROC results across all 12 models in Experiment 1)
- **Medium Confidence**: In-context experiences of failure reduce overconfidence and improve decision-making for some models (particularly Claude Sonnet 3.5), though the learning mechanism and generalizability remain unclear
- **Medium Confidence**: Newer and larger models do not consistently show improved calibration compared to smaller models (trend observed but not universally demonstrated across all model families)

## Next Checks

1. **Validation of learning mechanism**: Conduct controlled experiments varying the timing, salience, and presentation of failure feedback to isolate whether learning is driven by content of failures versus simple recency effects in the context window.

2. **Domain generalization testing**: Replicate core findings (overconfidence, discriminatory power, learning from failure) on non-coding domains (e.g., mathematical reasoning or commonsense QA) to assess whether results generalize beyond the programming task domain.

3. **White-box confidence verification**: Use token probability analysis or internal state probing to compare self-reported confidence with alternative confidence estimates derived from model internals, validating that reported confidence meaningfully reflects internal belief states.