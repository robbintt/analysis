---
ver: rpa2
title: Global Convergence of Continual Learning on Non-IID Data
arxiv_id: '2503.18511'
source_url: https://arxiv.org/abs/2503.18511
tags:
- continual
- learning
- data
- convergence
- theoretical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the theoretical understanding of continual
  learning for regression models under non-i.i.d. data conditions, where previous
  work relied on strict i.i.d.
---

# Global Convergence of Continual Learning on Non-IID Data

## Quick Facts
- arXiv ID: 2503.18511
- Source URL: https://arxiv.org/abs/2503.18511
- Authors: Fei Zhu; Yujing Liu; Wenzhuo Liu; Zhaoxiang Zhang
- Reference count: 0
- Key outcome: Establishes almost sure convergence for continual learning under general non-i.i.d. data conditions without persistent excitation requirements

## Executive Summary
This paper addresses the theoretical understanding of continual learning for regression models under non-i.i.d. data conditions, where previous work relied on strict i.i.d. assumptions and persistent excitation conditions. The authors develop a general theoretical framework using stochastic Lyapunov functions and martingale estimation techniques to analyze continual learning without requiring data to be i.i.d. or imposing excitation conditions. The key contributions include establishing almost sure convergence results for continual learning under general data conditions for the first time, and providing convergence rates for both forgetting and regret metrics without requiring any excitation condition on the data. The theoretical results demonstrate that the continual learning algorithm can achieve convergence guarantees under more realistic data conditions than previously established, making the theory more applicable to practical scenarios where strict i.i.d. assumptions often fail to hold.

## Method Summary
The paper develops a theoretical framework for continual learning of regression models that removes traditional assumptions about i.i.d. data and persistent excitation. The approach uses stochastic Lyapunov functions to track parameter evolution across sequential tasks, combined with martingale estimation techniques to establish convergence properties. The analysis covers general loss functions with bounded gradients and provides convergence guarantees without requiring the data to satisfy strict statistical independence or feature rank conditions. The framework establishes almost sure convergence and provides quantitative bounds on forgetting (performance loss on past tasks) and regret (cumulative suboptimality) metrics.

## Key Results
- Establishes almost sure convergence for continual learning under general non-i.i.d. data conditions for the first time
- Provides convergence rates for forgetting and regret metrics without requiring excitation conditions on the data
- Theoretical framework covers general loss functions with bounded gradients under weaker data assumptions than previous work

## Why This Works (Mechanism)

### Mechanism 1: Stochastic Lyapunov Function Framework
- Claim: Continual learning for regression models can achieve almost sure convergence under general (non-i.i.d.) data conditions without persistent excitation requirements.
- Mechanism: The authors construct a stochastic Lyapunov function that tracks parameter evolution across sequential tasks. By demonstrating that this function decreases in expectation and satisfies martingale difference properties, convergence is established without requiring statistical independence across samples or tasks.
- Core assumption: Loss functions have bounded gradients; the parameter space admits a suitable Lyapunov candidate.
- Evidence anchors:
  - [abstract] "By utilizing the stochastic Lyapunov function and martingale estimation techniques, we establish the almost sure convergence results of continual learning under a general data condition for the first time."
  - [corpus] Related work on Lyapunov-based learning without persistent excitation (arXiv:2505.10678) supports feasibility of this approach for neural networks.
- Break condition: If gradients become unbounded or the Lyapunov function cannot be constructed for the chosen model class, convergence guarantees may not hold.

### Mechanism 2: Martingale Estimation Under Relaxed Data Conditions
- Claim: Almost sure convergence can be proven without i.i.d. assumptions or persistent excitation by exploiting martingale concentration properties.
- Mechanism: The sequential learning process is modeled as a stochastic process where conditional expectations form a martingale structure. This allows concentration inequalities to bound cumulative deviation, enabling convergence analysis even when task distributions shift arbitrarily over time.
- Core assumption: Data-generating process satisfies general moment conditions (not full independence); gradients remain bounded.
- Evidence anchors:
  - [abstract] "...martingale estimation techniques, we establish the almost sure convergence results of continual learning under a general data condition for the first time."
  - [corpus] Weak direct corpus evidence; neighboring papers focus on mixed regression and federated settings rather than martingale-based CL theory.
- Break condition: If data exhibits heavy-tailed distributions violating moment conditions, or if gradient estimators have uncontrolled variance, martingale bounds degrade.

### Mechanism 3: Forgetting and Regret Rate Analysis Without Excitation
- Claim: Quantitative convergence rates for forgetting (performance loss on past tasks) and regret (cumulative suboptimality) can be derived without imposing excitation conditions on feature data.
- Mechanism: By decoupling convergence analysis from the rank/condition of the feature covariance matrix (traditional excitation requirement), the authors bound forgetting and regret using the Lyapunov decay rate and martingale tail bounds. This yields rates dependent on step-size schedules and gradient noise rather than data spectrum.
- Core assumption: Loss gradients bounded; tasks arrive sequentially without requiring uniform task distribution or feature rank conditions.
- Evidence anchors:
  - [abstract] "...without any excitation condition imposed on the data, the convergence rates for the forgetting and regret metrics are provided."
  - [corpus] Related work on deficient excitation (arXiv:2503.02235) addresses related rank-deficient settings, suggesting this relaxation is theoretically coherent.
- Break condition: If step-size is mis-specified (too large causing divergence, too small causing slow convergence), practical rates may not match theoretical bounds.

## Foundational Learning

- **Concept: Martingale Theory and Almost Sure Convergence**
  - Why needed here: The paper's core theoretical contribution relies on martingale concentration to prove convergence without independence assumptions. Understanding Doob's martingale convergence theorem and the difference between almost sure, in-probability, and $L^p$ convergence is essential.
  - Quick check question: Can you explain why a martingale can converge almost surely even when its increments are not independent?

- **Concept: Persistent Excitation and Its Relaxations**
  - Why needed here: Prior continual learning theory required persistent excitation (feature covariance matrix uniformly full-rank). This paper removes that requirement; understanding what PE is and why its relaxation matters clarifies the contribution.
  - Quick check question: What failure mode in parameter estimation does the persistent excitation condition prevent, and why is removing it significant?

- **Concept: Forgetting vs. Regret in Continual Learning**
  - Why needed here: The paper provides separate convergence rates for forgetting (backward transfer) and regret (cumulative loss). These are distinct evaluation axes with different practical implications.
  - Quick check question: In a 5-task sequence, if the model performs well on task 5 but poorly on task 1, which metric captures this failure?

## Architecture Onboarding

- **Component map:** Sequential task stream -> Gradient-based updates with bounded-gradient loss functions -> Stochastic Lyapunov function construction + martingale difference sequence identification -> Almost sure parameter convergence with forgetting and regret bounds

- **Critical path:**
  1. Verify loss function has bounded gradients for all tasks (required by theory)
  2. Confirm data satisfies general moment conditions (weaker than i.i.d.)
  3. Apply Lyapunov-based stability analysis to derive task-specific descent conditions
  4. Use martingale concentration to aggregate across task sequence
  5. Extract explicit forgetting/regret bounds from accumulated analysis

- **Design tradeoffs:**
  - **Excitation-free guarantees vs. convergence speed:** Removing excitation conditions makes theory more broadly applicable but may yield looser rates than rank-aware analysis
  - **Bounded gradients vs. model expressivity:** Bounded-gradient assumption may restrict architecture choices (e.g., unbounded ReLU outputs require careful normalization)
  - **Regression-specific theory vs. general applicability:** Current results target regression; extension to classification with cross-entropy loss is not directly covered

- **Failure signatures:**
  - Unbounded gradients during training signal theory assumptions violated
  - Forgetting increases without bound across tasks suggests martingale structure breakdown
  - Regret accumulates linearly rather than sublinearly indicates step-size mis-specification or distribution shift outside analyzed conditions

- **First 3 experiments:**
  1. **Verify bounded-gradient condition:** Train on synthetic regression tasks with varying label magnitudes; monitor gradient norms to confirm boundedness holds under chosen loss.
  2. **Non-IID stress test:** Construct a task sequence with explicit temporal correlation (e.g., drifting distribution); measure forgetting and regret against theoretical bounds.
  3. **Excitation ablation:** Compare convergence on feature sets with varying rank/condition number; confirm performance does not catastrophically degrade as excitation weakens, per the paper's claims.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the global convergence guarantees be extended to non-convex loss landscapes typical of deep neural networks?
- Basis: [inferred] The paper explicitly restricts its theoretical analysis to "continual learning of regression models" and "general loss functions," whereas modern continual learning primarily involves the non-convex optimization of deep networks.
- Why unresolved: The stochastic Lyapunov and martingale techniques used here rely on properties that may not hold in complex non-convex settings.
- What evidence would resolve it: A theoretical extension proving convergence for multi-layer networks or non-convex objectives without relying on strict convexity assumptions.

### Open Question 2
- Question: Is the assumption of bounded gradients strictly necessary for the established convergence rates?
- Basis: [inferred] The key contributions specify that the analysis "covers general loss functions with bounded gradients," suggesting this is a constraint of the current proof technique.
- Why unresolved: Many effective neural network architectures or loss functions may exhibit unbounded gradients, limiting the applicability of the current theory.
- What evidence would resolve it: Derivation of convergence bounds that depend on weaker assumptions, such as sub-Gaussian gradient distributions, rather than strict boundedness.

### Open Question 3
- Question: How does the removal of the persistent excitation condition affect the convergence speed compared to methods that assume full-rank data?
- Basis: [inferred] The paper highlights that it provides rates "without any excitation condition," but it does not explicitly compare the *tightness* of these rates against classical bounds that do assume excitation.
- Why unresolved: While existence is proven, the efficiency of the convergence in low-data or low-rank regimes relative to established baselines remains unclear.
- What evidence would resolve it: A comparative analysis showing the degradation (or lack thereof) in convergence rates when moving from excited data to the general conditions proposed.

### Open Question 4
- Question: Can the theoretical framework be adapted to handle classification tasks with discrete labels?
- Basis: [inferred] The summary and abstract specify the results are developed for "regression models," leaving the adaptation to classification tasks as a logical next step.
- Why unresolved: Classification often involves different loss metrics (e.g., cross-entropy) and discrete output spaces which may require modifications to the martingale estimation techniques.
- What evidence would resolve it: Extension of the theorems to classification problems or empirical verification that the regression bounds hold for classification tasks.

## Limitations
- The bounded-gradient assumption may significantly constrain applicable model architectures, particularly for modern deep networks with unbounded activations
- The absence of specific algorithm details (update rules, regularization methods, replay mechanisms) creates ambiguity about how theoretical guarantees translate to implementable systems
- The analysis focuses on regression tasks, leaving open questions about applicability to classification settings with different loss structures

## Confidence
- **Almost sure convergence claims:** High confidence based on well-established Lyapunov + martingale framework in stochastic optimization theory
- **Forgetting and regret rate claims:** Medium confidence due to absence of explicit algorithm details and metric definitions
- **Generalization to non-regression settings:** Low confidence given bounded-gradient restriction and focus on regression models

## Next Checks
1. **Algorithm specification validation:** Request explicit algorithm pseudocode and implementation details from authors to enable faithful reproduction of theoretical claims
2. **Gradient boundedness stress test:** Systematically evaluate convergence behavior across a spectrum of gradient norms, identifying the precise threshold where theoretical guarantees break down
3. **Cross-task distribution shift analysis:** Design experiments varying the degree of non-i.i.d. characteristics (covariate shift, label distribution shift, concept drift) to map the boundary of where martingale-based analysis remains effective