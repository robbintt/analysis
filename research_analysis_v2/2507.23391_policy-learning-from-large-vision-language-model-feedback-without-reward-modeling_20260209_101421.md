---
ver: rpa2
title: Policy Learning from Large Vision-Language Model Feedback without Reward Modeling
arxiv_id: '2507.23391'
source_url: https://arxiv.org/abs/2507.23391
tags:
- learning
- reward
- preference
- conference
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PLARE introduces a novel framework for training robotic agents
  using large vision-language models (VLMs) without explicit reward modeling. The
  approach addresses the challenge of designing effective reward functions for reinforcement
  learning by leveraging VLMs to generate preference feedback on pairs of visual trajectory
  segments.
---

# Policy Learning from Large Vision-Language Model Feedback without Reward Modeling

## Quick Facts
- arXiv ID: 2507.23391
- Source URL: https://arxiv.org/abs/2507.23391
- Reference count: 40
- Primary result: Achieves 70% average success rate on MetaWorld tasks, outperforming state-of-the-art VLM-based methods by 13.5%

## Executive Summary
PLARE introduces a novel framework for training robotic agents using large vision-language models (VLMs) without explicit reward modeling. The approach addresses the challenge of designing effective reward functions for reinforcement learning by leveraging VLMs to generate preference feedback on pairs of visual trajectory segments. Instead of learning an intermediate reward model, PLARE directly optimizes policies using contrastive preference learning from the VLM-generated preferences. Evaluated on manipulation tasks from MetaWorld and real-world robot manipulation, PLARE achieves an average success rate of 70%, outperforming state-of-the-art VLM-based reward generation methods by 13.5%. The method also demonstrates effectiveness in real-world settings, highlighting its potential for practical robotic applications.

## Method Summary
PLARE trains robotic policies directly from unlabeled trajectory datasets by querying a VLM (Gemini) for pairwise preferences on visual trajectory segments. The method extracts three frames (first, middle, last) from each segment and uses a two-stage prompt to generate ternary preference labels. These preferences are used to train the policy via a supervised contrastive loss, bypassing the need for an intermediate reward model. The framework includes dropout regularization to handle noisy VLM labels and excludes equal-preference cases to improve robustness. PLARE was evaluated on MetaWorld manipulation tasks and real-world robot experiments, demonstrating superior performance compared to VLM-based reward modeling approaches.

## Key Results
- Achieves 70% average success rate on MetaWorld manipulation tasks
- Outperforms state-of-the-art VLM-based reward generation methods by 13.5%
- Demonstrates effective transfer to real-world robot manipulation tasks
- Reduces human effort in reward function design while maintaining high performance

## Why This Works (Mechanism)

### Mechanism 1: Direct Preference Optimization Bypasses Reward Model Error Cascade
- Claim: Eliminating the intermediate reward model reduces compounding prediction errors that degrade policy performance.
- Mechanism: PLARE uses a supervised contrastive loss that directly shapes the policy from preference pairs, skipping the reward-model-to-critic-to-actor pipeline. The loss compares log-probability sums over preferred vs. dispreferred trajectory segments via a soft-max weighting.
- Core assumption: The contrastive objective provides a sufficient learning signal without an explicit scalar reward; preference feedback captures enough task-relevant information.
- Evidence anchors: [abstract] "PLARE directly optimizes policies using contrastive preference learning from the VLM-generated preferences, bypassing the need to learn explicit reward models." [section I, page 2] "First, relying on a learned reward model introduces cascading prediction errors that propagate from the reward function to the critic and ultimately to the actor, leading to high variance in policy performance."

### Mechanism 2: VLM Provides Task-Grounded Preference Judgments via Two-Stage Prompting
- Claim: Large VLMs (e.g., Gemini) can generate meaningful trajectory preferences from visual observations and language task descriptions, acting as an automated annotation oracle.
- Mechanism: For each segment pair, PLARE queries the VLM with three representative frames (first, middle, last) in a two-stage prompt: first analyzing what each sequence shows, then outputting a ternary preference label (0/1/-1). The VLM grounds its judgment in the language task description.
- Core assumption: The VLM's visual-language alignment transfers sufficiently to robotic manipulation observations; hallucination rates are tolerable with noise mitigation.
- Evidence anchors: [abstract] "PLARE queries a VLM for preference labels on pairs of visual trajectory segments based on a language task description." [section IV-A, page 3-4] Describes the two-stage querying process with 3-frame sampling.

### Mechanism 3: Dropout and Equal-Preference Filtering Mitigate Noisy Labels
- Claim: Regularization strategies (dropout tuning) and filtering ambiguous labels improve robustness to VLM-generated noise.
- Mechanism: Equal-preference labels (y=0.5) are excluded from training as they introduce uncertainty. Dropout is tuned per-task as a regularizer for noisy supervision, reducing overfitting to incorrect preferences.
- Core assumption: Noisy labels are random enough that dropout provides effective regularization; equal preferences indicate low-confidence judgments rather than meaningful task signals.
- Evidence anchors: [section IV-B, page 6] "We find that excluding equal preference labels often leads to improved performance... we tune the dropout rate during policy training, leveraging it as an effective regularizer for learning from noisy labels."

## Foundational Learning

- Concept: **Offline Reinforcement Learning**
  - Why needed here: PLARE operates on pre-collected, reward-free datasets without environment interaction; understanding offline RL constraints (distribution shift, extrapolation error) is essential.
  - Quick check question: Can you explain why offline RL methods like IQL use in-sample Q-learning rather than standard Bellman updates?

- Concept: **Preference-Based RL (PbRL)**
  - Why needed here: PLARE is a PbRL method; knowing how preference labels traditionally inform reward models helps contextualize why direct policy optimization is novel.
  - Quick check question: What is the standard pipeline in PbRL, and where does PLARE deviate from it?

- Concept: **Contrastive Learning Objectives**
  - Why needed here: The core loss function (Equation 1–2) is a contrastive objective; understanding how it pulls preferred trajectories toward higher probability while pushing dispreferred ones is critical.
  - Quick check question: How does the soft-max weighting in Equation 2 balance contributions from preferred vs. dispreferred segments?

## Architecture Onboarding

- Component map: Unlabeled Dataset D -> Segment Sampling -> VLM Query (10 hrs for 10k comparisons) -> Preference Buffer D_pref -> CPL Policy Training (0.4 hrs for 500k steps) -> Evaluation

- Critical path: Dataset → Segment Sampling → VLM Query → Preference Buffer → CPL Policy Training → Evaluation

- Design tradeoffs:
  - **Frame count per segment**: 3 frames chosen for balance; fewer lacks temporal context, more increases query time
  - **Equal-preference handling**: Excluding them improves performance but discards ~3–35% of queries depending on task
  - **Dropout rate**: Task-dependent (0.1–0.5); higher dropout helps noisy tasks but may underfit cleaner ones
  - **Segment length L**: Not specified in detail; likely tuned per dataset

- Failure signatures:
  - Policy fails to improve: Check preference label accuracy (target 70–80%); if lower, VLM prompts may need refinement
  - High variance across seeds: Dropout may be mismatched to noise level; tune regularization
  - Real-world transfer fails: Visual encoder (R3M) may not align with camera viewpoint; consider domain adaptation
  - Query time prohibitive: Reduce N or frame count; accept potential performance drop

- First 3 experiments:
  1. **Reproduce MetaWorld Drawer Open** with provided dataset (2500 episodes). Verify 10k VLM queries produce ~84% success rate; log preference accuracy and dropout impact
  2. **Ablation on equal-preference handling**: Train with vs. without y=0.5 labels; expect 2–5% degradation when included
  3. **Real-world Pickup Banana** with 1k queries: Confirm R3M + state input trains successfully; compare against IQL-CLIP baseline to validate 2× improvement suggested by Figure 4

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can PLARE effectively scale to long-horizon, multi-stage manipulation tasks where the temporal gap between action and outcome is significantly larger?
- Basis in paper: [explicit] The authors explicitly state in the Conclusion that the real-world tasks tested were "relatively simple" and suggest that "Future work can build on this foundation by extending PLARE to more complex, long-horizon, and multi-stage tasks."
- Why unresolved: The current experiments focus on short-horizon tasks (e.g., "Drawer Open," "Pickup Banana") where the correlation between visual frames and success is immediate, leaving the method's efficacy on complex sequences unstudied.
- What evidence would resolve it: Evaluation of PLARE on benchmarks requiring sequential reasoning (e.g., kitchen preparation or multi-object sorting) to see if the fixed frame sampling captures sufficient temporal context.

### Open Question 2
- Question: Does the replacement of uniform frame sampling with adaptive, saliency-based frame selection significantly improve the VLM's preference accuracy?
- Basis in paper: [explicit] In Section IV-A, the authors note that "more advanced methods could be used to select informative frames... we find that three uniformly spaced frames strike a practical and effective balance."
- Why unresolved: Uniform sampling may miss critical moments (e.g., contact events) in fast-moving trajectories, potentially providing ambiguous visual context to the VLM.
- What evidence would resolve it: An ablation study comparing the policy performance and VLM labeling accuracy when using uniformly sampled frames versus optical flow or motion-based keyframe selection.

### Open Question 3
- Question: To what extent does the method's reliance on task-specific dropout tuning limit its ability to function as a fully general "plug-and-play" learning framework?
- Basis in paper: [inferred] The ablation study in Section V-C shows optimal dropout probability varies significantly by task (e.g., 0.1 vs. 0.4), suggesting that the regularization required to handle VLM noise is not robustly automated across different environments.
- Why unresolved: While dropout mitigates noise, the need to tune this hyperparameter implies a hidden manual cost that contradicts the paper's goal of reducing human effort and domain expertise.
- What evidence would resolve it: Demonstration of an automated regularization mechanism (e.g., adaptive dropout or uncertainty-weighted loss) that maintains high performance across all tasks without manual per-task tuning.

## Limitations

- Exact CPL hyperparameters (α, λ) and policy network architecture details are not specified, requiring educated guesses
- VLM hallucination rates and systematic biases in preference judgments remain unclear without controlled experiments
- Real-world performance claims lack statistical significance testing across multiple seeds

## Confidence

- **High**: CPL loss function derivation, MetaWorld simulation results, exclusion of equal-preference labels improving performance
- **Medium**: VLM preference generation quality (~70-81% accuracy), dropout regularization effectiveness, real-world robot transfer claims
- **Low**: Exact VLM prompt templates beyond Figure 2, per-task dropout rate optimization without systematic search

## Next Checks

1. Replicate the VLM preference accuracy measurement across all MetaWorld tasks; verify 69-81% range holds with different prompt variations
2. Perform statistical significance testing on real-world robot experiments (PickupBanana, PushCan) with at least 5 random seeds each
3. Conduct systematic ablation studies on equal-preference handling: include, exclude, and relabel equal preferences to quantify information loss vs. noise reduction tradeoff