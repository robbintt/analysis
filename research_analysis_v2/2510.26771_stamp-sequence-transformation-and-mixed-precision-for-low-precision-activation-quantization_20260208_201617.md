---
ver: rpa2
title: 'STaMP: Sequence Transformation and Mixed Precision for Low-Precision Activation
  Quantization'
arxiv_id: '2510.26771'
source_url: https://arxiv.org/abs/2510.26771
tags:
- quantization
- stamp
- sequence
- activation
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of activation quantization for
  low-bitwidth deployment of large generative AI models. It proposes STaMP (Sequence
  Transformation and Mixed Precision), a method that applies linear transformations
  along the sequence dimension and uses mixed-precision quantization to exploit local
  correlations in tokens.
---

# STaMP: Sequence Transformation and Mixed Precision for Low-Precision Activation Quantization

## Quick Facts
- **arXiv ID:** 2510.26771
- **Source URL:** https://arxiv.org/abs/2510.26771
- **Reference count:** 40
- **Primary result:** Enables accurate 4-bit activation quantization for LLMs/LVMs by applying sequence dimension transforms and mixed-precision quantization

## Executive Summary
STaMP addresses the challenge of low-bitwidth activation quantization for large generative AI models by exploiting local correlations in token sequences. The method applies linear transformations along the sequence dimension to concentrate energy into a sparse subset of tokens, then uses mixed-precision quantization to allocate higher bit-widths to these high-energy tokens. This approach complements existing feature transformations and weight quantization methods, consistently improving performance across both LLMs and LVMs while adding minimal computational overhead.

## Method Summary
STaMP applies a Discrete Wavelet Transform (specifically Haar) along the sequence dimension of activation matrices, concentrating signal energy into the first few tokens. A mixed-precision quantizer then allocates 8-bit precision to the top-64 energy-concentrated tokens while quantizing the remaining tokens to 4-bit. The method processes activations as: apply DWT, quantize with mixed precision, perform matrix multiplication, apply inverse DWT, then add bias. This sequence transformation approach is orthogonal to feature transforms and weight quantization methods, allowing STaMP to be combined with existing techniques.

## Key Results
- Achieves state-of-the-art 4-bit activation quantization for LLMs and LVMs
- Consistently improves perplexity and SQNR metrics when combined with existing quantization methods
- Enables accurate quantization while adding minimal computational overhead
- Demonstrates effectiveness across multiple model architectures (LLaMA, Qwen, PixArt, SANA)

## Why This Works (Mechanism)

### Mechanism 1: Energy Compaction via Sequence Autocorrelation
If activations exhibit local correlation (neighboring tokens are dependent), an orthogonal transform applied along the sequence dimension can concentrate signal energy into a sparse subset of tokens. The method exploits the observation that the autocorrelation matrix of LLM/LVM activations is approximately Toeplitz. By applying transforms like the Discrete Cosine Transform (DCT) or Discrete Wavelet Transform (DWT)—which approximate the optimal Karhunen-Loève Transform (KLT) for such matrices—the method redistributes energy so that a few "low-frequency" tokens capture most of the variance, leaving other tokens with near-zero values. This works because intermediate activations must exhibit strong local dependencies (smoothness) along the sequence axis.

### Mechanism 2: Asymmetric Error Suppression via Mixed Precision
Allocating higher bit-widths specifically to the high-energy tokens (identified via Mechanism 1) reduces the theoretical upper bound of the total quantization error more efficiently than uniform bit allocation. The quantization error bound depends on the ratio of token energy to the dynamic range. Because the denominator grows exponentially with bits, assigning 8-bit precision to the few high-energy tokens (dominating the error sum) suppresses the largest error terms, while the majority of low-energy tokens can tolerate 4-bit quantization with minimal contribution to the total error. This requires the hardware/software stack to support dynamic mixed-precision quantization.

### Mechanism 3: Orthogonal Complementarity to Feature Transforms
Sequence transformations (left-multiplication) commute differently with linear layers than feature transformations (right-multiplication), allowing them to complement methods like QuaRot or SmoothQuant without interfering with weight quantization. Sequence transforms are applied to the input and inverted on the output, so the weights remain untouched. This allows STaMP to handle sequence redundancy while feature transforms handle channel outliers, addressing two distinct sources of quantization difficulty simultaneously.

## Foundational Learning

- **Concept:** **Karhunen-Loève Transform (KLT) and Toeplitz Matrices**
  - **Why needed here:** The paper justifies using DCT/DWT by arguing they approximate the KLT for Toeplitz autocorrelation matrices. Understanding this link explains why generic signal processing transforms work on neural activations.
  - **Quick check question:** Why does the "Toeplitz" structure of the autocorrelation matrix suggest that DCT is a good approximation for the optimal basis?

- **Concept:** **Quantization Error Bounds (L2 Norm vs. Clipping)**
  - **Why needed here:** The method relies on Theorem 1 to derive the mixed-precision strategy. You must understand how the bit-width relates to the error term to see why saving bits on low-energy tokens is mathematically safe.
  - **Quick check question:** According to Equation 8, does doubling the energy of a token require doubling the bits to maintain the same error contribution?

- **Concept:** **Group/Block Quantization Granularity**
  - **Why needed here:** The paper compares STaMP against per-token and per-block quantization. Context on how scales are shared across tokens/channels is necessary to evaluate the overhead claims.
  - **Quick check question:** STaMP keeps a per-token scale. Does the mixed-precision scheme require maintaining separate scale factors for the 4-bit vs 8-bit token groups?

## Architecture Onboarding

- **Component map:** Input -> DWT -> Mixed Precision Quantizer -> Matrix Multiplication -> Inverse DWT -> Bias Addition
- **Critical path:** The sequence transform and its inverse are the new critical bottlenecks. The paper selects Haar-DWT because it reduces complexity to O(ds), compared to the O(sd log d) of Hadamard/Feature transforms.
- **Design tradeoffs:** DCT vs. DWT: DCT concentrates energy most optimally but is computationally heavier. DWT is slightly sub-optimal for energy compaction but much faster and creates discrete energy levels suitable for simple "top-k" bit allocation. High-Precision Token Count: Increasing the number of 8-bit tokens improves accuracy but rapidly degrades the effective bit-width.
- **Failure signatures:** Non-Local Correlations: If activations do not have local correlation (e.g., shuffled tokens or specific attention outputs like `attn2.to_out` in cross-attention), STaMP provides no benefit. Memory Overhead: If the implementation requires materializing the transformed tensor as a new large tensor rather than fusing, memory bandwidth could bottleneck inference.
- **First 3 experiments:**
  1. **Energy Visualization:** Compute the autocorrelation matrix of inputs to a specific layer (e.g., LLaMA Layer 20). Plot to confirm Toeplitz/block structure before implementing the transform.
  2. **Ablation on k:** Run STaMP (DWT) on a PixArt block, varying the number of 8-bit tokens (e.g., 16, 32, 64, 128). Plot SQNR vs. Effective Bit-width to find the "knee" of the curve.
  3. **Orthogonality Test:** Apply QuaRot (feature transform) alone, then STaMP alone, then both. Verify that the combined SQNR gain is additive (log-scale), confirming they attack independent noise sources.

## Open Questions the Paper Calls Out
None explicitly identified in the provided text.

## Limitations
- The method's effectiveness depends on activations exhibiting strong local correlation along the sequence dimension, which may not hold for all architectures or attention mechanisms.
- The computational overhead of sequence transforms and mixed-precision bookkeeping is claimed to be minimal but not empirically validated with wall-clock measurements.
- The choice of 64 high-precision tokens appears arbitrary without shown sensitivity analysis across different model sizes and tasks.

## Confidence

- **High Confidence:** The orthogonality of sequence transforms to feature transforms is mathematically sound and well-demonstrated. The energy compaction visualization and ablation studies provide strong empirical support.
- **Medium Confidence:** The mixed-precision error suppression mechanism is theoretically justified by Theorem 1, but the practical benefits depend on the specific bit-width allocation strategy.
- **Low Confidence:** The claim that STaMP is "orthogonal" to weight quantization methods is stated but not empirically validated in the provided results.

## Next Checks

1. **Energy Correlation Validation:** Compute and visualize the autocorrelation matrices for multiple layers in LLaMA-3 8B on Wikitext-2. Confirm the Toeplitz/block structure claimed in Section 3.2. If the structure is absent or irregular, STaMP's core mechanism fails.

2. **Overhead Profiling:** Implement STaMP with CUDA kernels and measure: (a) end-to-end latency increase per Llama layer, (b) additional memory bandwidth required for intermediate tensor, (c) effective throughput (tokens/sec) compared to uniform 4-bit quantization. Compare against the theoretical O(ds) claim.

3. **Orthogonality Experiment:** Apply STaMP + GPTQ (or AWQ) on LLaMA-3 8B and measure if the combined perplexity is better than either method alone. This directly tests the claim that sequence transforms do not interfere with weight quantization.