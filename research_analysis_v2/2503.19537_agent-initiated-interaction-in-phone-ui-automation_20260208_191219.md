---
ver: rpa2
title: Agent-Initiated Interaction in Phone UI Automation
arxiv_id: '2503.19537'
source_url: https://arxiv.org/abs/2503.19537
tags:
- user
- interaction
- task
- agent
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AndroidInteraction, a dataset for detecting
  when a UI automation agent should interact with the user and generating appropriate
  messages. The dataset is built by annotating existing AndroidControl recordings
  to identify steps requiring user interaction.
---

# Agent-Initiated Interaction in Phone UI Automation

## Quick Facts
- arXiv ID: 2503.19537
- Source URL: https://arxiv.org/abs/2503.19537
- Reference count: 29
- This paper introduces AndroidInteraction, a dataset for detecting when a UI automation agent should interact with the user and generating appropriate messages.

## Executive Summary
This paper addresses the challenge of determining when a UI automation agent should initiate user interaction during phone task execution. The authors introduce AndroidInteraction, a dataset built by annotating existing AndroidControl recordings to identify steps requiring user interaction. The dataset emphasizes context-aware interaction timing, avoiding unnecessary questions while ensuring alignment with user intent. Baseline models using text-only, image-only, and combined inputs struggle with high false positives, particularly in confirming actions or seeking unnecessary information. Multimodal models perform better, especially with screenshots aiding screen understanding. The dataset highlights the subtlety of this task and suggests that fine-tuning on larger, more diverse data may be needed for improved performance.

## Method Summary
The paper proposes a two-stage architecture for agent-initiated interaction detection: first enumerating plausible next actions from the current screen state, then evaluating whether any action requires user input before execution. The AndroidInteraction dataset contains 3,605 steps across 772 episodes derived from AndroidControl recordings. Each step includes accessibility tree, screenshot, and action information, annotated with interaction need, message, and a 1-5 necessity score. Models are evaluated using Gemini 1.5 with text-only, screenshot-only, and multimodal inputs, comparing zero-shot and few-shot prompting approaches. Performance is measured via F1 score and manual message quality assessment.

## Key Results
- Multimodal models achieve 88-91% message adequacy versus 74% for text-only models
- Two-stage few-shot approach achieves F1=0.25 for text input tasks versus single-stage F1=0.20
- Baseline models show high false positive rates (51-52% of errors are unnecessary confirmations)
- Few-shot prompting provides modest precision improvements at the cost of recall

## Why This Works (Mechanism)

### Mechanism 1
Two-stage architectures improve interaction detection by separating action planning from interaction necessity assessment. Stage 1 lists plausible next actions; Stage 2 evaluates whether any action requires user input. This decomposition allows each component to focus on a distinct sub-problem.

### Mechanism 2
Multimodal inputs (accessibility tree + screenshot) improve interaction detection by compensating for modality-specific blind spots. Accessibility trees provide structured element hierarchy but miss visual context; screenshots disambiguate layout and reveal information not captured in metadata.

### Mechanism 3
Few-shot prompting modestly improves precision by providing interaction pattern signals that reduce over-eager confirmations, but the subtlety of when-to-ask vs when-to-act doesn't transfer well from limited examples.

## Foundational Learning

- **Concept: Accessibility Tree Representation**
  - Why needed here: The paper uses accessibility trees as the primary textual UI representation. Understanding their structure (hierarchical element list with type, properties, relationships) is essential for parsing screen state.
  - Quick check question: Can you identify which elements in an accessibility tree correspond to actionable UI components vs. static text containers?

- **Concept: Interaction Necessity Spectrum**
  - Why needed here: The dataset uses a 1-5 necessity score rather than binary labels, reflecting that interaction decisions are subjective and user-preference-dependent.
  - Quick check question: Given an instruction "set a timer," would asking "for how long?" rate higher or lower on necessity than asking "would you like to name this timer?"?

- **Concept: False Positive Asymmetry in User Interaction**
  - Why needed here: Baseline models show high false positive rates (unnecessary confirmations, navigation questions). Understanding why models over-ask is critical for improvement.
  - Quick check question: Why might an LLM default to asking for confirmation before executing actions that a user would expect to proceed autonomously?

## Architecture Onboarding

- **Component map**: Input Layer -> Stage 1 (Action Planner) -> Stage 2 (Interaction Detector) -> Output

- **Critical path**: Screen understanding → Action enumeration → Interaction necessity classification → Message generation. Errors propagate forward; poor Stage 1 outputs degrade Stage 2 regardless of Stage 2 quality.

- **Design tradeoffs**:
  - Text-only vs. Multimodal: Text-only is simpler but misses visual context; multimodal improves message quality (88-91% vs 74% adequacy) at inference cost
  - Single-stage vs. Two-stage: Two-stage improves F1 (0.25 vs 0.20) but adds complexity and latency
  - Zero-shot vs. Few-shot: Few-shot improves precision but reduces recall; neither achieves satisfactory performance

- **Failure signatures**:
  - High false positive rate (models ask unnecessary confirmations in 51-52% of false positives)
  - Navigation-related questions when agent should infer action (31% of multimodal false positives)
  - Missed interactions requiring scrolling to reveal relevant options (common false negative cause)
  - Text-only models miss personal detail requirements visible in screenshots

- **First 3 experiments**:
  1. Reproduce baseline F1 scores on AndroidInteraction validation split using Gemini 1.5 with text-only two-stage few-shot; confirm high false positive pattern matches paper (target: precision ~0.19, recall ~0.35).
  2. Ablate modality: run same prompts with (a) text-only, (b) screenshot-only, (c) combined; measure impact on message adequacy using manual review of true positives.
  3. Error analysis on 50 false positives: categorize into (unnecessary confirmation, navigation question, reasonable subjectivity) to identify highest-leverage improvement target for next iteration.

## Open Questions the Paper Calls Out

### Open Question 1
Would a larger-scale dataset for fine-tuning substantially improve model performance on interaction detection compared to few-shot prompting approaches? The authors conjecture that fine-tuning is necessary but haven't empirically tested this hypothesis.

### Open Question 2
Can a Wizard-of-Oz data collection paradigm effectively capture multi-turn agent-user interactions with direct message quality assessment? The current dataset is limited to single-turn interactions.

### Open Question 3
Can incorporating memory components (past interactions or user profiles) reduce unnecessary clarification requests while maintaining alignment with user preferences? The current work assumes no prior familiarity with the user.

### Open Question 4
What mechanisms can reduce the high false positive rate where models generate unnecessary confirmations and navigation-related questions? The error analysis reveals this as the most prevalent issue but the underlying cause isn't fully addressed.

## Limitations
- Dataset derived from single source (AndroidControl), raising generalizability concerns
- High false positive rates persist even with multimodal inputs
- Limited evaluation of long-term user experience impacts from interaction decisions

## Confidence

**High confidence**:
- Multimodal inputs significantly improve message adequacy (88-91% vs 74% for text-only)
- Baseline models exhibit high false positive rates (51-52% of errors are unnecessary confirmations)
- Two-stage architecture outperforms single-stage for text input tasks (F1 0.25 vs 0.20)

**Medium confidence**:
- Few-shot prompting provides modest improvements in precision at recall cost
- Screen understanding is the primary failure mode for text-only models
- Navigation-related questions constitute a significant false positive category

**Low confidence**:
- Long-term effectiveness of interaction detection across diverse user populations
- Optimal balance between false positive and false negative rates for user experience
- Generalization to non-phone UI automation contexts

## Next Checks

1. **Cross-dataset validation**: Evaluate the AndroidInteraction-trained models on a separate, independently collected phone UI automation dataset to assess generalization and measure performance degradation.

2. **User experience impact study**: Conduct a controlled user study comparing agent behavior with different interaction detection thresholds, measuring task completion time, user frustration levels, and overall satisfaction.

3. **Error type ablation analysis**: Systematically analyze the 50 most common false positive patterns to determine whether they cluster around specific UI components, app categories, or interaction types for targeted fine-tuning.