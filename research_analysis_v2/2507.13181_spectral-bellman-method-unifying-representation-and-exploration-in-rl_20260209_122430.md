---
ver: rpa2
title: 'Spectral Bellman Method: Unifying Representation and Exploration in RL'
arxiv_id: '2507.13181'
source_url: https://arxiv.org/abs/2507.13181
tags:
- learning
- representation
- bellman
- exploration
- spectral
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces the Spectral Bellman Method (SBM), a novel
  framework for representation learning in reinforcement learning (RL) that directly
  aligns feature learning with the structure of Bellman updates across a space of
  value functions. The method leverages a fundamental spectral relationship: under
  the zero-Inherent Bellman Error (IBE) condition, the transformation of a distribution
  of value functions by the Bellman operator is intrinsically linked to the feature
  covariance structure.'
---

# Spectral Bellman Method: Unifying Representation and Exploration in RL

## Quick Facts
- arXiv ID: 2507.13181
- Source URL: https://arxiv.org/abs/2507.13181
- Reference count: 27
- Primary result: SBM improves performance on hard-exploration and long-horizon Atari tasks through Bellman-aligned representation learning

## Executive Summary
The Spectral Bellman Method (SBM) introduces a theoretically-grounded framework for representation learning in reinforcement learning that aligns feature learning with the spectral structure of Bellman updates. Under the zero-Inherent Bellman Error (IBE) condition, SBM establishes that the transformation of value function distributions by the Bellman operator is intrinsically linked to feature covariance structure. This yields a new objective for learning state-action features that capture this Bellman-aligned covariance, enabling structured exploration through Thompson Sampling with feature covariance. The method demonstrates improved performance on the Atari benchmark, particularly for hard-exploration and long-horizon tasks.

## Method Summary
SBM learns state-action representations by optimizing a loss function that captures the spectral relationship between feature covariance and Bellman dynamics under zero-IBE. The method alternates between standard Q-learning updates and representation learning steps that minimize a quadratic loss involving moving-average covariance matrices. Thompson Sampling with feature covariance provides exploration by reducing maximum uncertainty in the learned representation space. The approach extends naturally to multi-step Bellman operators and requires only a simple modification to existing value-based RL algorithms.

## Key Results
- SBM outperforms DQN and R2D2 baselines on hard-exploration Atari games (Montezuma's Revenge, Pitfall!, Private Eye)
- Thompson Sampling with feature covariance provides substantial gains over ε-greedy exploration
- Performance improvements are most pronounced in long-horizon and sparse-reward environments
- The method naturally extends to multi-step Bellman operators for more powerful representations

## Why This Works (Mechanism)

### Mechanism 1
Under zero IBE, the feature and parameter matrices correspond to singular vectors of the distribution-weighted Bellman operator. When IBE = 0, the function space is closed under the Bellman operator, and the spectral decomposition theorem shows that augmented feature matrix Φ_P and parameter matrix Θ̃_P satisfy Φ_P = ŨΣ_ϕ and Θ̃_P = Σ_θṼ^⊤, where Σ_ϕΣ_θ = Σ (singular values of the Bellman operator). This structural alignment means learning features that capture this covariance inherently satisfies low-IBE.

### Mechanism 2
Alternating optimization via power iteration separates the joint representation-parameter problem into tractable subproblems. Proposition 1 establishes that the SBM loss decomposes into representation loss (updating ϕ given current θ̃) and parameter loss (updating θ̃ given current ϕ), with moving-average covariance matrices providing stable regularization versus noisy single-sample estimates in naive MSE.

### Mechanism 3
Thompson Sampling with feature covariance Σ provides structured exploration that reduces maximum uncertainty in the learned representation space. Given learned features ϕ, the least-squares parameter estimate θ̂_LS has covariance Σ⁻¹. Sampling exploration weights θ̂_TS ~ N(θ̂_LS, σ_exp Σ⁻¹) before each rollout generates policies that maximize uncertainty reduction, with spectral representation ensuring Σ reflects Bellman-relevant structure.

## Foundational Learning

- **Inherent Bellman Error (IBE)**: Measures closure of function space under Bellman operator. Why needed: Entire method derived from zero-IBE condition; essential to understand why spectral alignment matters. Quick check: Can you explain why IBE = 0 is weaker than Linear MDP assumption?

- **Singular Value Decomposition and Power Iteration**: Frame representation learning as finding singular vectors of Bellman operator via iterative methods. Why needed: Central to SBM's theoretical foundation. Quick check: What is the relationship between power iteration and dominant eigenvectors?

- **Thompson Sampling in Linear Bandits/RL**: Exploration mechanism relies on posterior sampling over linear parameters with covariance-based uncertainty. Why needed: Core exploration strategy in SBM. Quick check: Why does TS with covariance Σ⁻¹ provide better exploration than ε-greedy in linear settings?

## Architecture Onboarding

- **Component map**: Input frames → CNN backbone → Feature extractor (ϕ) → Q-learning head OR → SBM representation network → Covariance trackers → Thompson Sampling → Action selection

- **Critical path**: 1) Collect data with TS exploration (sample θ̂_TS ~ N(θ̂_t, σ_exp Σ⁻¹_t)), 2) Update Q-parameters θ̂ via standard Q-learning loss, 3) Sample parameters θ ~ N(θ̂_{t+1}, σ²_rep I) for representation learning, 4) Compute Bellman targets TQ_θ for sampled θ, 5) Update ϕ and θ̃ network via SBM loss with current Λ₁,ₜ, Λ₂,ₜ, 6) Update EMA covariances

- **Design tradeoffs**: Latent dimension d (higher captures more structure but increases variance), parameter sampling variance σ²_rep (too small → no exploration benefit; too large → unstable learning), alternation frequency (more representation steps emphasizes feature quality)

- **Failure signatures**: Performance degradation on precision-control games (Breakout) due to TS exploration noise, covariance collapse causing unstable TS sampling, representation-policy mismatch from poorly chosen parameter distribution

- **First 3 experiments**: 1) Sanity check on tabular MDP: Verify learned ϕ converges to true singular vectors and TS exploration outperforms ε-greedy, 2) Covariance estimation ablation: Compare EMA vs batch vs single-sample covariance on 3 Atari games, 3) Parameter distribution sensitivity: Sweep σ²_rep on Montezuma's Revenge

## Open Questions the Paper Calls Out

- **Theoretical convergence**: What are the theoretical convergence properties of SBM when zero-IBE condition is not strictly satisfied? The paper lists strengthening theoretical analysis (convergence, non-zero IBE) as future work, as convergence is only proven under strict zero-IBE assumption.

- **Adaptive parameter sampling**: Can an adaptive mechanism be developed for parameter sampling distribution ν(θ) to replace manual tuning of σ²_rep? The paper identifies this as crucial for successful learning and notes that grid search is currently required.

- **Extension to other RL settings**: Can SBM be effectively extended to actor-critic architectures or distributional reinforcement learning? The paper suggests extending to distributional or actor-critic methods as future work, as current derivation is specific to value-based RL.

## Limitations

- Zero-IBE assumption is central to theoretical derivation but rarely holds in practice; empirical validation of remaining IBE is missing
- Alternating optimization scheme's convergence properties under non-zero IBE conditions are not rigorously established
- Covariance estimation quality depends heavily on parameter distribution choice, which is currently manually tuned

## Confidence

- **High confidence**: Spectral decomposition theorem (Theorem 1) and its derivation under zero-IBE
- **Medium confidence**: Practical effectiveness of alternating optimization (Proposition 1, Proposition 2), limited corpus evidence
- **Medium confidence**: Thompson Sampling exploration benefits, supported by established literature but with unclear implementation details

## Next Checks

1. **IBE measurement**: Implement diagnostic to measure inherent Bellman error of learned representations on simple MDP with known ground truth features
2. **Covariance ablation**: Compare SBM with and without moving-average covariance regularization on 3 Atari games to isolate regularization effect
3. **Parameter distribution sensitivity**: Systematically vary parameter sampling distribution (Gaussian variance, different mean initialization) and measure impact on representation quality and downstream RL performance