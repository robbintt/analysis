---
ver: rpa2
title: Bounded PCTL Model Checking of Large Language Model Outputs
arxiv_id: '2509.18836'
source_url: https://arxiv.org/abs/2509.18836
tags:
- text
- generation
- tokens
- llms
- process
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces LLMCHECKER, a formal verification method\
  \ that applies PCTL model checking to verify properties of Large Language Model\
  \ (LLM) text generation. The key innovation is modeling the LLM text generation\
  \ process as a Discrete-Time Markov Chain (DTMC) using an \u03B1-k-bounded approach,\
  \ where only the top-k most probable tokens are considered at each step until their\
  \ cumulative probability reaches threshold \u03B1."
---

# Bounded PCTL Model Checking of Large Language Model Outputs

## Quick Facts
- arXiv ID: 2509.18836
- Source URL: https://arxiv.org/abs/2509.18836
- Authors: Dennis Gross; Helge Spieker; Arnaud Gotlieb
- Reference count: 40
- One-line primary result: LLMCHECKER enables formal PCTL verification of LLM text generation properties using α-k-bounded DTMC modeling to overcome state space explosion.

## Executive Summary
This paper introduces LLMCHECKER, a formal verification method that applies PCTL model checking to verify properties of Large Language Model (LLM) text generation. The key innovation is modeling the LLM text generation process as a Discrete-Time Markov Chain (DTMC) using an α-k-bounded approach, where only the top-k most probable tokens are considered at each step until their cumulative probability reaches threshold α. This significantly reduces the state space explosion problem in traditional model checking approaches.

The method allows verification of various properties expressed in PCTL (Probabilistic Computation Tree Logic) on LLM outputs, including gender bias, sentiment analysis, readability, and copyright violations. Experiments demonstrate successful application across multiple LLMs including Llama, Gemma, Mistral, Genstruct, and BERT. Results show that only a small subset of tokens are typically chosen during generation, enabling effective verification. The approach successfully identifies differences in model behavior across tasks and reveals sensitivity to minor input variations.

## Method Summary
LLMCHECKER encodes LLM text generation as a DTMC by recursively exploring token probabilities from the LLM, selecting only the top-k tokens at each step until their cumulative probability reaches threshold α. Each DTMC state represents a text string with associated quantification values, and transitions represent token additions with their probabilities. Unexplored branches terminate in aggregated terminal states with probability (1 − p_sum). The resulting DTMC is encoded in PRISM language and verified using the Storm model checker to compute satisfaction probabilities for PCTL properties about the generated text.

## Key Results
- The α-k-bounded approach successfully reduces state space while preserving verification validity, with experiments showing Gemma LLMs achieve 90% cumulative probability with only top-3 tokens.
- LLMCHECKER successfully verifies multiple properties across different LLMs including gender bias, sentiment, readability, and copyright similarity, demonstrating model-specific behavior patterns.
- The method reveals sensitivity to minor input variations, showing different models produce significantly different verification results for synonymous inputs like "athlete" vs "contestant".

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constraining token selection to top-k tokens until cumulative probability reaches α reduces state space explosion while preserving verification validity.
- Mechanism: LLMs naturally favor limited tokens per generation step. The approach exploits this by exploring only the most probable branches and aggregating remaining probabilities into terminal states rather than expanding them.
- Core assumption: Tokens not in top-k (or not reaching cumulative α) represent unlikely generation paths that can be safely aggregated without significantly affecting verification results.
- Evidence anchors:
  - [abstract]: "We empirically show that only a limited number of tokens are typically chosen during text generation"
  - [section V-A]: Figure 3 shows Gemma LLMs achieve 90% cumulative probability with top-3 tokens
  - [corpus]: Limited direct support; corpus papers focus on token-level control, not state space reduction
- Break condition: Uniform token distributions cause n to become large, triggering combinatorial explosion again (Section IV-B acknowledges this).

### Mechanism 2
- Claim: Modeling LLM text generation as a DTMC with terminal states for non-selected tokens enables exact PCTL verification of the bounded process.
- Mechanism: Each state represents a text string; transitions represent token additions with LLM-assigned probabilities. Unexplored branches terminate in aggregated terminal states with probability (1 − p_sum).
- Core assumption: The aggregated terminal state adequately represents all remaining possibilities not captured by the α-k-bounded process.
- Evidence anchors:
  - [section IV-C]: "At each step of the text generation, the process may terminate with a probability of 1−p_sum... modeled as a terminal state"
  - [Figure 2]: Visual comparison showing reduced DTMC structure vs full expansion
  - [corpus]: No direct corpus support for this encoding strategy
- Break condition: Verification requiring properties about specific low-probability tokens loses precision under aggregation.

### Mechanism 3
- Claim: Text quantification methods (bias counting, sentiment analysis, readability scores) enable PCTL formulas to reason about textual content.
- Mechanism: Text quantification M: Σ* → Z maps strings to integer values that become state features (atomic propositions), enabling PCTL formulas to express properties about text characteristics.
- Core assumption: Quantification methods reliably capture the properties being verified.
- Evidence anchors:
  - [section IV-A]: "We denote M: Σ* → Z as a text quantification method that maps a given string ω... This method quantifies, for instance, if a text is biased"
  - [Table II]: Shows successful verification across gender bias, sentiment, readability, and copyright quantification
  - [corpus]: Token-Guard paper relates to token-level control mechanisms but not quantification-based verification
- Break condition: Unreliable or inconsistent quantification measurements produce verification results that may not reflect actual text properties.

## Foundational Learning

- Concept: **Probabilistic Computation Tree Logic (PCTL)**
  - Why needed here: PCTL is the formal language used to express and verify properties about LLM text generation modeled as a DTMC.
  - Quick check question: Can you distinguish between "eventually" (F) and "always" (G) path operators, and explain what P(F gender > 0) computes?

- Concept: **Discrete-Time Markov Chains (DTMC)**
  - Why needed here: The core formal model representing LLM text generation as a probabilistic state transition system.
  - Quick check question: Given a DTMC state, what must be true about the sum of all outgoing transition probabilities?

- Concept: **State Space Explosion**
  - Why needed here: The fundamental challenge this paper addresses; naive DTMC encoding of LLM generation is impractical.
  - Quick check question: If vocabulary has 32,000 tokens and we model 5 token generations, how many states would naive full DTMC require in worst case?

## Architecture Onboarding

- Component map: LLM f(ω) -> TOP_α^k function -> DTMC Encoder (Algorithm 1) -> Text Quantification M(ω) -> Storm Model Checker
- Critical path: 1) User provides start string ω(0), PCTL query, text quantification M, parameters (α, k, L) 2) LLMCHECKER encodes DTMC via recursive LLM calls and state construction 3) Storm returns verification result (satisfaction probability or boolean)
- Design tradeoffs: Lower α → faster verification, less coverage; Higher k → more tokens explored, increased complexity; Assumption: Verification precision traded against computational tractability
- Failure signatures: State space explosion (|S| in millions): α/k too permissive; Zero probability results: PCTL property overly restrictive or quantification misconfigured; Encoding timeout: L too large or LLM inference slow; Model checker timeout: DTMC exceeds memory/complexity limits
- First 3 experiments: 1) Replicate RQ1 on Gemma-2B with varying k to confirm cumulative probability concentration 2) Run LLMCHECKER on "Player" start string with gender bias quantification (α=0.8, k=15, L=5), verify P(F gender > 0), compare to Table II 3) Test synonymous inputs ("athlete" vs "contestant") on same model to measure bias probability sensitivity per Figure 4

## Open Questions the Paper Calls Out
None

## Limitations
- The paper provides no implementation details for the text quantification methods (gender bias counting, sentiment analysis, readability scores, copyright similarity), making reproduction impossible without these critical components.
- Scalability claims remain theoretical as experiments only cover 2B-7B parameter models; no validation exists for frontier-scale models (175B+ parameters) where state space reduction benefits may diminish.
- The α-k-bounded approach fundamentally trades verification precision for computational tractability without systematic analysis of how different parameter configurations affect accuracy or quantification of aggregation error.

## Confidence
- High Confidence: The core theoretical framework (DTMC modeling of LLM generation, PCTL specification, α-k-bounded state space reduction) is well-established and the paper provides clear algorithmic descriptions with supporting experimental evidence for basic properties.
- Medium Confidence: The practical implementation details and scalability claims require validation. While the approach is theoretically sound, the specific implementation choices and their impact on real-world performance remain uncertain without access to the actual tool.
- Low Confidence: The quantification method implementations and their reliability for different types of properties (especially copyright similarity) are not verified or validated, making the results for these specific properties less trustworthy.

## Next Checks
1. **Quantification Method Validation**: Implement and validate the text quantification methods (gender bias counting, sentiment analysis, readability scores) independently, then verify that they produce consistent results on the same text samples used in the paper. Compare the quantification outputs with known benchmarks for each metric.

2. **Parameter Sensitivity Analysis**: Systematically vary (α,k) parameters across multiple orders of magnitude and measure the impact on: (a) state space size, (b) verification runtime, (c) probability results for the same PCTL queries. This would quantify the precision-computational trade-off and identify optimal parameter ranges.

3. **Larger Model Scalability Test**: Apply LLMCHECKER to a 70B parameter model (or the largest available) with the same verification tasks. Measure state space growth, verification time, and compare results to the 2B-7B experiments to empirically validate the scalability claims and identify practical limits.