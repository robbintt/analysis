---
ver: rpa2
title: Exploring Model Invariance with Discrete Search for Ultra-Low-Bit Quantization
arxiv_id: '2502.06844'
source_url: https://arxiv.org/abs/2502.06844
tags:
- quantization
- https
- arxiv
- language
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper tackles the challenge of ultra-low-bit (e.g., 2-bit)\
  \ quantization of large language models (LLMs), where severe rounding errors from\
  \ extreme quantization often degrade model performance. It proposes INVAREXPLORE,\
  \ a general framework that explores different types of model invariance\u2014permutation,\
  \ scaling, and rotation\u2014in a unified manner, and introduces an activation-guided\
  \ discrete search algorithm to optimize these transformations."
---

# Exploring Model Invariance with Discrete Search for Ultra-Low-Bit Quantization

## Quick Facts
- **arXiv ID:** 2502.06844
- **Source URL:** https://arxiv.org/abs/2502.06844
- **Reference count:** 19
- **Primary result:** INVAREXPLORE reduces 2-bit quantization perplexity by 30-50% and improves reasoning accuracy by up to 3 points over strong baselines.

## Executive Summary
This paper addresses the challenge of ultra-low-bit (e.g., 2-bit) quantization for large language models, where severe rounding errors typically degrade performance. The authors propose INVAREXPLORE, a unified framework that systematically explores permutation, scaling, and rotation invariances through an activation-guided discrete search algorithm. The key insight is that applying transformations and their inverses to neighboring linear blocks preserves mathematical output while potentially improving quantization performance by reducing rounding errors. Experiments on OPT models (1.3B to 13B parameters) demonstrate consistent improvements across language modeling and reasoning tasks when INVAREXPLORE is combined with existing quantization methods like GPTQ, AWQ, and OmniQuant.

## Method Summary
INVAREXPLORE applies transformations (permutation P, scaling S, rotation R) to feed-forward weights (W_up, W_down) such that W'_up = PSRW_up and W'_down = W_downR^⊤S^(-1)P^⊤, preserving mathematical equivalence while altering quantization error patterns. The method uses a hill-climbing discrete search algorithm that samples transformations, quantizes weights, and accepts changes based on a loss combining cross-entropy and activation matching. The search explores non-differentiable permutation invariance, which gradient-based methods struggle with, and leverages synergies between multiple invariance types. INVAREXPLORE is compatible with existing quantization methods and requires only a small calibration set (32 sequences of 512 tokens from Pile).

## Key Results
- INVAREXPLORE reduces 2-bit quantization perplexity by 30-50% compared to strong baselines across OPT models (1.3B-13B)
- Reasoning accuracy improves by up to 3 points on tasks like ARC, BoolQ, HellaSwag, PIQA, and WinoGrande
- The framework works synergistically with existing methods (GPTQ, AWQ, OmniQuant) and requires no additional training
- Permutation-only variants already provide substantial improvements, with combined transformations offering additional gains

## Why This Works (Mechanism)

### Mechanism 1
**Claim:** Exploring multiple invariance transformations jointly yields better quantization than optimizing them individually.
The INVAREXPLORE framework applies combined permutations, scaling, and rotations to feed-forward blocks, leveraging synergies between transformations that independent optimization misses. By searching the combined transformation space, the method finds configurations with lower quantization error unreachable by sequential optimization.

### Mechanism 2
**Claim:** Activation-guided discrete search can optimize non-differentiable permutation invariance, which gradient-based methods struggle with.
The algorithm uses hill-climbing with a loss function combining cross-entropy and activation matching to bypass the zero-gradient problem of quantization functions and the non-convex local optima of permutation symmetry. This approach works with as few as 512 tokens and avoids overfitting to the calibration set.

### Mechanism 3
**Claim:** Applying transformations and their inverses to neighboring linear blocks preserves the full-precision model's output but changes how quantization rounding errors affect the result.
The core invariance principle is that W_down f(W_up x) ≈ W'_down f(W'_up x) where W'_up = T W_up and W'_down = W_down T^(-1). Since the round() function in quantization is non-invertible, quant(W'_up) may have lower rounding error than quant(W_up), even though pre-quantization outputs are equivalent.

## Foundational Learning

- **Concept:** Post-Training Quantization (PTQ)
  - **Why needed here:** INVAREXPLORE is a PTQ method designed to work after a model is trained, without retraining. Understanding PTQ vs. QAT is fundamental to grasping why search-based methods are used instead of gradient-based learning.
  - **Quick check question:** Why does the paper use a calibration set of only 512 tokens instead of full training data?

- **Concept:** Model Invariance in Neural Networks
  - **Why needed here:** The paper's core premise is exploiting mathematical equivalences (permutation, scaling, rotation) that preserve model output. Understanding why P^⊤P = I or why ReLU allows scaling invariance (f(sx) = sf(x)) is critical for following the transformations.
  - **Quick check question:** Why does scaling invariance hold for ReLU but not for all activation functions?

- **Concept:** Integer Quantization with Group-wise Scaling
  - **Why needed here:** The paper targets ultra-low-bit integer quantization, which uses scale (s_g) and zero-point (z_g) parameters per group. Understanding how outliers affect these parameters explains why reducing rounding error via transformations helps.
  - **Quick check question:** How does a large outlier weight in a group affect the scale parameter and subsequent quantization error for other weights in that group?

## Architecture Onboarding

- **Component map:** Discrete Search -> Transformation Sampler -> Quantized Model -> Loss Evaluator -> Accept/Reject
- **Critical path:** 1) Initialize with FP16 model and compute baseline activations H_0 on calibration data. 2) For each target layer, iteratively sample and evaluate transformations. 3) Apply combined transformation to W_up, W_down via efficient indexing/scaling. 4) Quantize transformed weights using chosen base method. 5) Evaluate on calibration data with quantized model, compute combined loss (CE + MSE). 6) Commit transformation if loss improves. 7) Output final transformed, quantized model.
- **Design tradeoffs:** Search step size (10% neurons) balances exploration vs. acceptance rate; calibration set size affects generalization vs. memory; activation matching layers improve guidance but increase memory overhead; rotation standard deviation (σ_r=1e-5) keeps rotation close to identity to avoid instability.
- **Failure signatures:** Stagnant acceptance rate indicates step size too large or local optimum; divergent calibration loss suggests implementation errors in inverse transformations; high memory usage from activation matching; no improvement over base suggests incorrect base method implementation or insufficient search steps.
- **First 3 experiments:** 1) Baseline reproduction: Replicate AWQ/GPTQ 2-bit quantization on OPT 1.3B for perplexity baseline. 2) INVAREXPLORE integration: Apply permutation-only on single layer, monitor acceptance rate and loss. 3) Full ablation: Run full INVAREXPLORE (all transformations) on same model, compare against individual invariance variants.

## Open Questions the Paper Calls Out
- **Open Question 1:** Can exact rotational invariance be formalized and utilized in alternative architectures like LoRA where current approximate methods are necessary? The authors observe future opportunities to explore exact rotational invariance in architectures like LoRA, as the current framework relies on small-angle approximations that introduce minor errors.
- **Open Question 2:** Does exploiting the scaling invariance between the down-projection and the subsequent LayerNorm operation yield significant further reductions in quantization error? The paper notes this opportunity is "beyond the scope" of the current work, as the unified search space was restricted to the feed-forward block equations.
- **Open Question 3:** Is the performance of InvarExplore robust when applied to architectures with non-ReLU activations (e.g., SiLU or GELU) where strict scaling invariance is mathematically ungrounded? The method relies on OPT models (ReLU) and admits scaling invariance only holds for certain functions, stating it can only be "employed as an approximation" for others.

## Limitations
- Claims about synergistic benefits from joint optimization lack direct empirical validation through intermediate combination studies
- Performance gains appear highly dependent on base quantization method quality, assuming already strong baselines
- Search algorithm computational overhead (10K+ steps per layer) and memory requirements could limit practical deployment on extremely large models
- Paper does not address potential degradation in downstream task performance beyond perplexity and reasoning accuracy

## Confidence
- **High Confidence:** Mathematical foundation of model invariance is well-established and correctly applied; experimental results showing consistent improvements are reproducible and well-documented
- **Medium Confidence:** Claim that joint optimization yields superior results is plausible but lacks detailed ablation studies on combination effects; activation-guided discrete search effectiveness is demonstrated but could benefit from more rigorous comparison
- **Low Confidence:** Assertion that specific parameter choices (σ_s=1e-2, σ_r=1e-5, 10% neuron modification) are optimal is not substantiated; paper lacks sensitivity analysis for these hyperparameters

## Next Checks
1. **Ablation of Transformation Combinations:** Systematically test all pairwise and triplet combinations of invariance transformations to empirically validate the claimed synergistic benefits of the full INVAREXPLORE framework.
2. **Search Efficiency Analysis:** Measure computational overhead (wall-clock time, memory usage) and test whether the algorithm converges within stated 10K steps across different model sizes and task complexities.
3. **Generalization Beyond Accuracy Metrics:** Evaluate quantized models on robustness benchmarks and downstream task transfer learning to assess whether quantization improvements translate to broader performance benefits beyond perplexity and reasoning accuracy.