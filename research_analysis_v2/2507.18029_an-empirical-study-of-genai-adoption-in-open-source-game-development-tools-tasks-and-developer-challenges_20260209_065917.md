---
ver: rpa2
title: 'An Empirical Study of GenAI Adoption in Open-Source Game Development: Tools,
  Tasks, and Developer Challenges'
arxiv_id: '2507.18029'
source_url: https://arxiv.org/abs/2507.18029
tags:
- genai
- game
- issue
- development
- issues
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study plans to investigate how generative AI (GenAI) is adopted
  and discussed in open-source game development by analyzing GitHub issue discussions.
  The research categorizes issues into GenAI, traditional AI (TradAI), and NonAI groups,
  then applies open card sorting and thematic analysis to identify tools, tasks, and
  challenges.
---

# An Empirical Study of GenAI Adoption in Open-Source Game Development: Tools, Tasks, and Developer Challenges

## Quick Facts
- arXiv ID: 2507.18029
- Source URL: https://arxiv.org/abs/2507.18029
- Reference count: 29
- Authors: Xiang Echo Chen; Wenhan Zhu; Guoshuai Albert Shi; Michael W. Godfrey
- This study plans to investigate how generative AI (GenAI) is adopted and discussed in open-source game development by analyzing GitHub issue discussions.

## Executive Summary
This proposed empirical study aims to understand how generative AI (GenAI) is being adopted in open-source game development through analysis of GitHub issue discussions. The research employs stratified sampling to collect ~1,000 issues from game-related repositories, classifying them into GenAI, traditional AI (TradAI), and NonAI categories. Through open card sorting and thematic analysis, the study seeks to identify specific tools, tasks, usage patterns, and developer challenges associated with GenAI integration, while comparing these patterns against TradAI and NonAI approaches to understand whether GenAI represents a fundamental shift in development practices.

## Method Summary
The study employs GitHub issue mining to analyze GenAI adoption in open-source game development. Researchers will collect repositories with "game" topic labels, filter for active projects (≥10 stars, ≥10 commits in 2024, non-fork), and sample ~1,000 issues using stratified random sampling. Issues will be manually classified by two annotators into GenAI, TradAI, and NonAI categories, with Cohen's Kappa measuring inter-rater reliability. Open card sorting will generate type labels (bug, feature request, improvement, discussion, help wanted) and emergent content labels, followed by thematic analysis to build a hierarchical taxonomy. If GenAI issues are underrepresented, LLM-assisted expansion using DeepSeek will be applied before comparative analysis across all three groups.

## Key Results
- No empirical results yet - this is a proposed study design
- Planned methodology includes stratified sampling of ~1,000 GitHub issues
- Comparative analysis across GenAI, TradAI, and NonAI issue categories
- Open card sorting and thematic analysis to identify tools, tasks, and challenges

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GitHub issue discussions serve as a viable proxy for understanding real-world GenAI adoption practices in open-source game development.
- Mechanism: Issues capture coordination, brainstorming, and technical reasoning beyond task tracking, reflecting how developers engage with tools in practice. This enables retrospective analysis of adoption patterns without direct observation.
- Core assumption: Developers document their GenAI-related challenges and decisions in issues rather than only in private channels or unwritten knowledge.
- Evidence anchors:
  - [abstract] "issue threads because they capture real-time developer reflections on tool use, feature implementation, and integration challenges"
  - [Section I] "issues often include coordination, brainstorming, and technical reasoning, making them a valuable lens"
  - [corpus] Related paper "What do users ask in open-source AI repositories?" (Yang et al.) supports issue mining for understanding AI development challenges, though not specific to games.
- Break condition: If GenAI discussions predominantly occur in Discord/Slack or remain undocumented, issue analysis will underrepresent actual adoption patterns.

### Mechanism 2
- Claim: Stratified random sampling with manual classification can produce a representative dataset for comparing GenAI, TradAI, and NonAI issues.
- Mechanism: Repository filtering (≥10 stars, ≥10 commits in 2024, non-fork, non-educational) ensures active projects; proportional sampling per repository prevents large projects from dominating; dual-annotator classification with Cohen's Kappa ensures label reliability.
- Core assumption: The topic-label-based repository selection captures most relevant game+AI projects, and issue content accurately reflects the primary technology focus.
- Evidence anchors:
  - [Section IV.A] "repos must have at least 10 stars... at least 10 commits in 2024 or later"
  - [Section IV.B] "stratification is done per repository where issues are sampled proportionally"
  - [corpus] Limited corpus evidence directly validating this specific sampling strategy for GenAI in games.
- Break condition: If GenAI issues are sparse (<5% of sample) and LLM-assisted expansion fails to identify them reliably, comparative analysis may lack statistical power.

### Mechanism 3
- Claim: Open card sorting followed by thematic analysis can transform unstructured issue discussions into a structured taxonomy revealing tool usage, task patterns, and challenge themes.
- Mechanism: Annotators assign type labels (bug/feature/improvement/discussion/help-wanted) and content labels (both abstract concepts and specific tools/models); iterative grouping creates hierarchical relationships enabling cross-group comparison.
- Core assumption: The labeling scheme can adequately capture GenAI-specific concepts (e.g., prompt engineering, model integration) that differ meaningfully from TradAI concerns.
- Evidence anchors:
  - [Section V.B] "assign one or more labels to each issue based on... Type... Content"
  - [Section V.C] "group semantically similar labels into broader categories... hierarchical taxonomy"
  - [corpus] Related work by Yang et al. (2023) developed a 13-category issue taxonomy for AI repos using similar methods, but not game-specific.
- Break condition: If GenAI issues span too many disparate topics without clear patterns, thematic clustering may yield uninformative categories.

## Foundational Learning

- Concept: GitHub Issue Mining for Software Engineering Research
  - Why needed here: The entire methodology depends on understanding what issues contain and how to extract signal from them.
  - Quick check question: Can you explain why issue discussions might reveal different information than commit history or documentation?

- Concept: Open Card Sorting vs. Closed Card Sorting
  - Why needed here: The study uses open card sorting where labels emerge from data rather than being predefined—understanding this distinction is critical for interpreting the taxonomy.
  - Quick check question: What is the tradeoff between allowing labels to emerge organically versus starting with a predefined codebook?

- Concept: GenAI vs. TradAI in Game Development Context
  - Why needed here: Classification depends on distinguishing these categories; understanding that TradAI includes pathfinding, decision trees, PCG while GenAI includes LLMs, diffusion models is foundational.
  - Quick check question: Would procedural content generation using grammars be classified as TradAI or GenAI in this framework, and why?

## Architecture Onboarding

- Component map:
  - GitHub API queries → topic label extraction → repository filtering → issue collection
  - Stratified random sampling (95% confidence, ±3% margin) → ~1,000 issue sample
  - Dual-annotator manual labeling → GenAI/TradAI/NonAI → potential DeepSeek LLM expansion
  - Open card sorting (type + content labels) → thematic analysis → hierarchical taxonomy → comparative statistics

- Critical path:
  1. Define game-related and AI-related topic label sets through iterative expansion
  2. Filter repositories and collect issues
  3. Train annotators on classification rubric and achieve acceptable inter-rater reliability
  4. Execute card sorting with iterative label refinement
  5. Build taxonomy and perform cross-group comparisons

- Design tradeoffs:
  - Broad topic inclusion (max coverage) vs. manual review burden for label validation
  - Strict repository filters (quality) vs. potential exclusion of niche but relevant projects
  - LLM-assisted GenAI detection (scale) vs. risk of false positives requiring manual verification
  - Flat content labels (simplicity) vs. hierarchical taxonomy (expressiveness)—resolved by doing both sequentially

- Failure signatures:
  - Low Cohen's Kappa (<0.6) during pilot classification indicates ambiguous category definitions
  - GenAI issues <10% of sample after initial classification suggests need for LLM-assisted expansion
  - Content label explosion (>100 unique labels with low co-occurrence) indicates need for earlier thematic grouping

- First 3 experiments:
  1. Pilot annotation on 50 issues from 5 repositories to validate classification rubric and measure inter-rater agreement before full-scale labeling.
  2. Topic label expansion test: start with seed keywords, run 3 iterations of co-occurring topic discovery, measure coverage plateau to determine stopping point.
  3. DeepSeek validation test: classify 100 manually-labeled issues with the LLM, compute agreement rate, and analyze disagreement patterns to refine prompts before large-scale application.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific tools, tasks, and usage patterns characterize GenAI adoption in open-source game development, and how do they manifest in developer discussions?
- Basis in paper: [explicit] The authors state RQ1 as "What tools, tasks, and usage patterns associated with GenAI are evident in GenAI-related issue discussions?" with the goal of identifying GenAI technologies, supported tasks, and integration modes.
- Why unresolved: This is the primary research question of a proposed study; no empirical results have been collected or analyzed yet.
- What evidence would resolve it: Thematic analysis and open card sorting results from sampled GitHub issues, including aggregated label frequencies, co-occurrence patterns between content labels and specific tools/models, and descriptive statistics summarizing GenAI capabilities referenced in issue discussions.

### Open Question 2
- Question: What are the primary challenges and pain points developers encounter when integrating GenAI tools into open-source game development workflows?
- Basis in paper: [explicit] The authors state RQ2 as "What challenges do developers report in GenAI-related issue discussions?" and aim to identify common pain points through thematic analysis of GenAI-related issues.
- Why unresolved: This is a core research question of the planned study; the methodology describes how challenges will be identified but no analysis has been conducted.
- What evidence would resolve it: Thematic clustering of GenAI-related issues based on recurring expressions and developer concerns, supported by illustrative issue examples showcasing specific pain points, along with analysis of type labels indicating challenge-related intents.

### Open Question 3
- Question: Does GenAI integration introduce genuinely distinct development patterns and challenges compared to traditional AI and non-AI approaches in game development?
- Basis in paper: [explicit] RQ3 asks "How do the characteristics and challenges of GenAI-related issues differ from those observed in TradAI and NonAI issue discussions?" The motivation explicitly questions "whether GenAI truly represents a shift in developer behaviour or simply extends existing paradigms."
- Why unresolved: Comparative analysis across GenAI, TradAI, and NonAI issue groups is planned but not yet executed; the study design describes the methodology but results are pending.
- What evidence would resolve it: Statistical comparison of type label and content label distributions across all three groups, analysis of commonalities and differences in challenge themes, and assessment of whether GenAI issues involve more tool-specific discussions or higher conceptual complexity than other categories.

### Open Question 4
- Question: To what extent do findings from open-source game repositories on GitHub generalize to commercial game development and private or non-English-speaking developer communities?
- Basis in paper: [inferred] The external validity section acknowledges the study "focuses exclusively on public open-source repositories hosted on GitHub" and notes "results may not extend to private repositories, commercial game development environments, or other platforms" and "may underrepresent developer communities operating in other languages or regions."
- Why unresolved: The study design intentionally limits scope to accessible open-source data, creating a fundamental generalizability constraint that the methodology cannot address.
- What evidence would resolve it: Replication studies targeting commercial game studios (via surveys or interviews), analysis of private repositories (if accessible), cross-platform comparisons (GitLab, Bitbucket), and multi-lingual repository sampling to assess whether patterns hold across different development contexts.

## Limitations

- GitHub issues may underrepresent GenAI adoption if developers prefer Discord/Slack or private channels for experimentation
- Findings may not generalize to commercial game development, private repositories, or non-English-speaking developer communities
- Open card sorting and thematic analysis require extensive human judgment, introducing subjectivity and potential inconsistency

## Confidence

- **High:** Stratified sampling methodology (95% confidence, ±3% margin) and dual-annotator classification with Cohen's Kappa measurement are well-established, reliable practices.
- **Medium:** GitHub issue mining approach is reasonable for capturing adoption patterns, though representativeness assumption requires validation.
- **Low:** Hierarchical taxonomy construction through iterative card sorting is inherently subjective and may not yield consistent results across different annotator teams.

## Next Checks

1. Pilot phase validation: Execute the pilot annotation on 50 issues from 5 diverse repositories to measure actual Cohen's Kappa before full-scale classification. Target Kappa ≥0.7 for acceptable reliability.
2. Topic label coverage plateau: Run 3-4 iterations of co-occurring topic discovery starting from seed keywords. Stop when marginal coverage gain drops below 0.5% (elbow point). Document final topic label sets to ensure reproducibility.
3. LLM-assisted expansion validation: Before applying DeepSeek to the full dataset, test on 100 manually-classified issues. Compute precision/recall vs. human annotators and analyze false positive patterns to refine prompts and filtering criteria.