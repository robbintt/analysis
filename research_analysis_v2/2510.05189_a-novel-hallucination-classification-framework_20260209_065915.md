---
ver: rpa2
title: A novel hallucination classification framework
arxiv_id: '2510.05189'
source_url: https://arxiv.org/abs/2510.05189
tags:
- answers
- hallucinations
- correct
- responses
- umap
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a formal classification framework for LLM hallucinations
  using geometric clustering in embedding space. The method involves generating controlled
  hallucinations via prompt engineering, mapping responses to embeddings, reducing
  dimensionality with UMAP, and computing inter-centroid distances to quantify separation
  between correct and hallucinated outputs.
---

# A novel hallucination classification framework

## Quick Facts
- **arXiv ID:** 2510.05189
- **Source URL:** https://arxiv.org/abs/2510.05189
- **Reference count:** 12
- **Primary result:** Embedding-based classification of LLM hallucinations via geometric clustering achieves ~2x distance separation between correct and hallucinated responses

## Executive Summary
This paper proposes a formal classification framework for LLM hallucinations using geometric clustering in embedding space. The method generates controlled hallucinations via prompt engineering, maps responses to embeddings, reduces dimensionality with UMAP, and computes inter-centroid distances to quantify separation between correct and hallucinated outputs. Experiments with Llama3.1 on 500 samples show hallucinated responses form significantly more distant clusters from ground truth than correct LLM answers, supporting the feasibility of lightweight, embedding-based classification to distinguish and potentially type hallucinations.

## Method Summary
The framework involves generating controlled hallucinations through systematic prompt engineering, encoding responses with sentence transformers (all-MiniLM-L6-v2), reducing dimensionality using UMAP, and computing Euclidean distances between cluster centroids in the reduced space. The approach aims to classify different types of hallucinations based on their semantic characteristics by analyzing geometric separation patterns. Experiments used 500 question-answer pairs from the Natural Questions dataset, with responses generated by Llama3.1, Gemma2, and Phi3 models.

## Key Results
- Hallucinated responses form significantly more distant clusters from ground truth (distances ~5.6–6.4) than correct LLM answers (distances ~2.8–3.0)
- Geometric separation between correct and hallucinated clusters is consistently reproducible across varying sample sizes and random seeds
- The framework achieves reliable classification with a 2.0x distance ratio between hallucination and correct answer clusters

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Hallucinated responses occupy measurably distinct regions in embedding space compared to ground truth, allowing classification via geometric distance.
- **Mechanism:** The embedding model encodes semantic meaning such that factually grounded responses cluster near the ground truth centroid. Hallucinations—lacking this grounding—diverge, resulting in significantly larger inter-centroid distances (approx. 2x) in the reduced vector space.
- **Core assumption:** The embedding model captures "factual alignment" as a geometric property, and this separation is preserved during dimensionality reduction.
- **Evidence anchors:** [abstract]: "hallucinated responses form significantly more distant clusters from ground truth... (distances ~5.6–6.4 vs ~2.8–3.0)"; [Section 6]: Tables 1-4 consistently show the distance from "expected answers" to "hallucinations fabrication" is roughly double the distance to "correct answers"; [corpus]: The paper "Hallucination Detection: A Probabilistic Framework Using Embeddings Distance Analysis" supports the broader validity of using embedding distance as a signal for hallucination detection.

### Mechanism 2
- **Claim:** Systematic prompt engineering can effectively generate a labeled "hallucination manifold" for training and analysis.
- **Mechanism:** By constraining context or injecting misleading instructions via prompts, the model is forced to rely on parametric probability rather than retrieved truth. This generates "controlled hallucinations" that map the error space of the model.
- **Core assumption:** Prompt-induced hallucinations share structural characteristics (in embedding space) with naturally occurring hallucinations.
- **Evidence anchors:** [Section 4.1]: Authors used "corresponding prompt-engineering approaches" to generate hallucinated responses for Llama3.1, Gemma2, and Phi3; [Section 3]: The solution aims to analyze "potential... to classify... different types of hallucinations based on their semantic characteristics"; [corpus]: "HalluciNot" introduces a taxonomy of responses, implying structured error generation is a recognized approach, though specific prompt mechanisms vary.

### Mechanism 3
- **Claim:** Dimensionality reduction (UMAP) preserves the global topology necessary to maintain separation between truth and hallucination clusters.
- **Mechanism:** UMAP reduces high-dimensional vectors to a lower-dimensional manifold. If the "hallucination" signal is a strong global feature, the projection maintains the relative distances, allowing Euclidean distance in the reduced space to serve as a proxy for semantic divergence.
- **Core assumption:** The hyperparameters of UMAP (n_neighbors, min_dist) do not accidentally merge the distinct clusters or distort the distance ratios.
- **Evidence anchors:** [Section 4.3]: "UMAP... simplifies the visualization and helps to better understand how embeddings are distributed... enabling clear comparison"; [Section 7]: "Separability was consistently reproducible across varying sample sizes and pseudorandom number generator parameters"; [corpus]: No specific corpus evidence refutes or supports the use of UMAP over other methods (like PCA) specifically for this task; this is an internal methodological choice.

## Foundational Learning

- **Concept: Sentence Embeddings (Vector Space Models)**
  - **Why needed here:** The entire framework relies on mapping text to points in space. You must understand that semantic similarity is approximated by geometric closeness (cosine similarity or Euclidean distance).
  - **Quick check question:** If two sentences mean the opposite but share all the same words (e.g., "The cat sat on the mat" vs "The cat did not sit on the mat"), will an embedding model place them close or far apart?

- **Concept: Dimensionality Reduction (UMAP)**
  - **Why needed here:** Raw embeddings are high-dimensional (384+ dimensions). You need to understand how UMAP projects this into 2D/3D for clustering and visualization, and that it preserves "neighborhoods" rather than exact absolute distances.
  - **Quick check question:** Does UMAP guarantee that the Euclidean distance between two points in 2D space equals their distance in the original 384-dimensional space? (Answer: No, it preserves topology/relative structure, not absolute magnitude).

- **Concept: Centroid-based Classification**
  - **Why needed here:** The paper classifies by measuring the distance of a new point to the "hallucination centroid" vs the "truth centroid."
  - **Quick check question:** If a new response point lies exactly halfway between the Truth Cluster Centroid and the Hallucination Cluster Centroid, what does the framework suggest about the classification confidence?

## Architecture Onboarding

- **Component map:** Ingestion (Dataset + LLM Generator with Prompt Templates) -> Encoder (all-MiniLM-L6-v2) -> Projector (UMAP) -> Classifier (Centroid Distance Calculation)
- **Critical path:** The **Prompt Engineering -> Embedding** link. If the prompts do not reliably generate "pure" hallucinations that are semantically distinct, the clusters will overlap, and the centroid distance metric will become noisy.
- **Design tradeoffs:**
  - Using `all-MiniLM-L6-v2` is computationally efficient (lightweight) but may lack the nuance of larger models (e.g., `e5-large`), potentially missing subtle semantic drifts in complex hallucinations.
  - Computing distances in UMAP space (as done here) is intuitive for visualization but technically lossy. A production system might perform better computing distances in the raw high-dimensional space, trading off interpretability for precision.
- **Failure signatures:**
  - Cluster Overlap: High standard deviation in centroid distances (tables show some variance, e.g., 2.39 vs 2.93); if distributions overlap, binary classification fails.
  - Domain Shift: If the model encounters a domain not covered in the "controlled hallucination" generation phase, the "hallucination centroid" may shift, invalidating the threshold.
- **First 3 experiments:**
  1. Reproduce Baseline Separation: Run the provided framework on the Natural Questions dataset to verify you can achieve the ~2.0x distance ratio (Hallucination Distance / Correct Distance) reported in Table 1.
  2. Threshold Sensitivity Analysis: Determine the optimal classification threshold (cut-off distance) by calculating precision/recall. Do not assume the midpoint between centroids is the best separator.
  3. Out-of-Distribution Test: Feed the system data from a completely different domain (e.g., Legal or Medical QA) without retraining the centroids. Check if the "correct" answers from the new domain drift closer to the old "hallucination" cluster.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can this framework effectively distinguish between fine-grained hallucination subtypes (e.g., fabrication vs. logical inconsistency) rather than just binary separation?
- **Basis in paper:** [explicit] Section 6 states the results "open the door for further research where different forms of hallucinations... can be identified through clustering and distance patterns."
- **Why unresolved:** The current experiments aggregate all hallucinations into a single cluster for distance comparison against ground truth, without validating separation between the specific taxonomy categories defined in Section 2.
- **What evidence would resolve it:** Experiments demonstrating distinct, non-overlapping clusters for specific hallucination types (e.g., factual contradiction vs. context inconsistency) within the UMAP projection.

### Open Question 2
- **Question:** Does the geometric separation hold for naturally occurring hallucinations, or is it an artifact of the prompt-engineering method used to generate the dataset?
- **Basis in paper:** [inferred] Section 4.1 relies on prompt engineering to "generate... hallucinated responses," while Section 3 notes that naturally occurring hallucinations are "convincing" and distinct from "inventive" outputs.
- **Why unresolved:** Artificially induced hallucinations via specific prompts might cluster differently (e.g., more consistently) than spontaneous, subtle errors produced during standard inference.
- **What evidence would resolve it:** Replication of the centroid distance results using a dataset of organically occurring model errors verified by human annotators.

### Open Question 3
- **Question:** Are the centroid distance thresholds (e.g., ~2.8 vs ~5.9) transferable across different LLM architectures and embedding models?
- **Basis in paper:** [inferred] Section 4.1 and Section 6 focus heavily on Llama3.1, and the method relies on a specific embedding model (all-MiniLM-L6-v2).
- **Why unresolved:** The paper does not demonstrate that the specific distance metrics or cluster densities are stable when changing the underlying LLM or the embedding dimensionality.
- **What evidence would resolve it:** Cross-model validation showing consistent inter-centroid distance ratios across diverse architectures (e.g., Gemma, Phi, GPT).

## Limitations

- **Prompt engineering reproducibility:** The paper does not specify the exact prompt templates used to elicit controlled hallucinations, making independent reproduction challenging and raising questions about whether generated "hallucinations" structurally match naturally occurring ones.
- **Embedding model choice:** The use of `all-MiniLM-L6-v2` is computationally efficient but may lack the semantic precision of larger models, potentially missing subtle hallucinatory errors and reducing classification robustness.
- **Dimensionality reduction assumptions:** UMAP is used for visualization and clustering, but it does not preserve exact Euclidean distances from the high-dimensional space, introducing a small but non-zero approximation error in the reported distance metrics.

## Confidence

- **High confidence:** The geometric separation between correct and hallucinated clusters is real and measurable in the tested dataset.
- **Medium confidence:** The lightweight embedding-based approach is practical and scalable, but its precision may lag behind larger, more computationally expensive models.
- **Low confidence:** The assumption that prompt-induced hallucinations structurally mirror spontaneous hallucinations is plausible but unverified.

## Next Checks

1. **Threshold Optimization:** Calculate precision/recall across a range of distance thresholds to identify the optimal classifier cutoff, rather than assuming the midpoint between centroids is best.
2. **Out-of-Distribution Test:** Apply the classification framework to QA data from a new domain (e.g., legal or medical) without retraining centroids. Measure drift in "correct" answer clusters toward the hallucination centroid.
3. **High-Dimensional Distance Comparison:** Recompute centroid distances in the raw 384-dimensional embedding space (without UMAP) and compare to the reported 2D/3D UMAP distances to quantify approximation error.