---
ver: rpa2
title: Online Optimization for Offline Safe Reinforcement Learning
arxiv_id: '2510.22027'
source_url: https://arxiv.org/abs/2510.22027
tags:
- cost
- offline
- reward
- o3srl
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces O3SRL, a novel approach for offline safe reinforcement
  learning (OSRL) that addresses the challenge of learning reward-maximizing policies
  under cumulative cost constraints from static datasets. The method frames OSRL as
  a minimax optimization problem, iteratively combining offline RL with online optimization
  to adaptively update Lagrange multipliers.
---

# Online Optimization for Offline Safe Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.22027
- Source URL: https://arxiv.org/abs/2510.22027
- Reference count: 28
- One-line primary result: O3SRL achieves high rewards while reliably enforcing safety constraints in offline RL using online optimization of Lagrange multipliers

## Executive Summary
This paper introduces O3SRL, a novel approach for offline safe reinforcement learning that addresses the challenge of learning reward-maximizing policies under cumulative cost constraints from static datasets. The method frames OSRL as a minimax optimization problem, iteratively combining offline RL with online optimization to adaptively update Lagrange multipliers. A practical approximation avoids costly off-policy evaluation and extensive offline RL iterations by discretizing the Lagrange space and using a multi-armed bandit strategy. Experiments on the DSRL benchmark demonstrate that O3SRL reliably enforces safety constraints even under stringent cost budgets (κ=5), while achieving high rewards. With as few as two arms, it outperforms state-of-the-art methods, and performance improves with more arms (K=5 optimal).

## Method Summary
O3SRL addresses offline safe reinforcement learning by framing it as a minimax optimization problem over policies and Lagrange multipliers. The method iteratively updates both components: the offline RL oracle produces a policy distribution maximizing the λ-weighted Lagrangian reward, while a no-regret algorithm updates the Lagrange multiplier based on accumulated regret. To avoid costly off-policy evaluation, O3SRL discretizes the Lagrange multiplier space and treats it as a multi-armed bandit problem, where each discrete λ value corresponds to an arm. The EXP3 algorithm maintains a distribution over arms, sampling λ in each round and updating probabilities based on stochastic value estimates from the offline RL oracle. A practical approximation performs truncated gradient updates (M steps per round) initialized from the previous round's policy, avoiding the computational cost of running offline RL to convergence each iteration.

## Key Results
- O3SRL reliably enforces safety constraints (cost ≤ κ=5) while achieving high rewards on DSRL benchmark
- With as few as two arms, O3SRL outperforms state-of-the-art methods; performance improves with more arms (K=5 optimal)
- The method is compatible with different offline RL algorithms (TD3+BC, IQL), showcasing its generality and effectiveness in safety-critical applications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Discretizing the continuous Lagrange multiplier space and treating it as a multi-armed bandit problem avoids unstable off-policy evaluation (OPE) while still converging to approximate minimax equilibrium.
- Mechanism: Each discrete λ value defines an "arm" with associated modified reward r′ = r − λ(c − (1−γ)κ). The EXP3 algorithm maintains a distribution over arms, sampling λ in each round and updating probabilities based on stochastic value estimates from the offline RL oracle. This eliminates the need to evaluate V^π_c for arbitrary continuous λ values via OPE.
- Core assumption: The discretization error from using K arms remains bounded, and the stochastic value estimate from the offline RL oracle is unbiased (E[Ṽ] = E_π~D̃[V^π_f]).
- Evidence anchors:
  - [abstract] "We also present a practical approximation that can be combined with any offline RL algorithm, eliminating the need for offline policy evaluation."
  - [section 5] "This transforms the problem into a multi-armed bandit (MAB) setting, where each of the K arms corresponds to a particular λ value. Crucially, many MAB algorithms do not require estimates of the underlying value functions, thus avoiding the need for OPE."
  - [corpus] Weak direct evidence; neighbor papers focus on different OSRL approaches. Feasibility-Aware Pessimistic Estimation mentions long-horizon safety but doesn't address Lagrangian discretization.
- Break condition: If the optimal λ lies between discretization points and the cost function is highly sensitive to λ, discretization error may dominate; increasing K is required.

### Mechanism 2
- Claim: No-regret online optimization over the Lagrange multiplier, paired with an approximate offline RL oracle, converges to an ε-approximate minimax equilibrium where ε = ε_offline-RL(n) + R_T(Λ)/T.
- Mechanism: The dual problem min_λ max_D L(D,λ) is solved iteratively. Each round: (1) offline RL oracle produces policy distribution D_t maximizing the λ_t-weighted Lagrangian reward; (2) no-regret algorithm updates λ_{t+1} based on accumulated regret. The averaged outputs (D̄, λ̄) satisfy the ε-equilibrium guarantee by combining the offline RL approximation error with the diminishing per-round regret.
- Core assumption: The offline RL oracle has bounded estimation error ε_offline-RL(n) that depends on dataset coverage but not strongly on reward function; the no-regret algorithm's regret R_T(Λ) grows sublinearly.
- Evidence anchors:
  - [section 4.2] Theorem 1: "(D̄, λ̄) is ε-approximate equilibrium, where ε = ε_offline-RL(n) + R_T(Λ)/T."
  - [section 4.1] "The constraint in Equation (1) satisfies Slater's condition when Π is expressive enough that there exists at least one policy π∈Π that is strictly safe... thus strong duality holds."
  - [corpus] Neighbor papers don't directly analyze minimax convergence for OSRL; no comparative evidence.
- Break condition: If the offline dataset has poor coverage for the optimal policy under any λ, ε_offline-RL(n) may not decrease with more data; coverage assumptions must be verified empirically.

### Mechanism 3
- Claim: Truncated gradient updates (M steps per round) initialized from the previous round's policy provide sufficient optimization progress while avoiding the computational cost of running offline RL to convergence each iteration.
- Mechanism: Rather than calling the offline RL oracle to full convergence, O3SRL performs M stochastic gradient steps starting from π_{t-1}, exploiting the incremental nature of SGD-based offline RL algorithms. The policy warm-starts near the previous solution, and λ updates occur frequently enough to guide the policy toward constraint satisfaction before it over-commits to unsafe behavior.
- Core assumption: M gradient steps provide meaningful improvement toward the λ_t-weighted objective without requiring full convergence; the policy does not diverge between λ updates.
- Evidence anchors:
  - [section 5.1] "We perform a small number of gradient updates on the policy from the previous iteration for efficiency... We find this truncated update scheme to be effective even for small values of M."
  - [table 9] Ablation shows M=10 balances cost control and reward; M=1 increases costs (frequent aggressive λ updates); M=100 can drift unsafe.
  - [corpus] No direct evidence from neighbors on truncated updates in iterative OSRL.
- Break condition: If M is too small relative to task complexity, the policy may not make sufficient progress per round; if M is too large, the policy may converge to an unsafe solution before λ adapts.

## Foundational Learning

- **Constrained Markov Decision Processes (CMDPs)**:
  - Why needed here: The entire OSRL problem is formulated as a CMDP with cumulative cost constraint E[C(τ)] ≤ κ. Understanding Lagrangian relaxation and the primal-dual relationship is prerequisite to grasping why minimax optimization is the right framing.
  - Quick check question: Given a policy π with V^π_c = 10 and cost threshold κ = 5, what does the Lagrangian term −λ(V^π_c − κ) incentivize when λ is large?

- **No-Regret Online Learning**:
  - Why needed here: The λ update relies on no-regret algorithms (EXP3 for discrete arms, or FTRL for continuous). The convergence guarantee depends on R_T(Λ)/T → 0; understanding regret bounds is essential to interpret Theorem 1.
  - Quick check question: If an algorithm has regret R_T = O(√T), what is the per-round regret R_T/T as T→∞?

- **Offline RL and Distributional Shift**:
  - Why needed here: O3SRL wraps any offline RL algorithm (TD3+BC, IQL), so understanding why offline RL requires pessimism/regularization against out-of-distribution actions is critical for selecting and debugging the base oracle.
  - Quick check question: Why does the paper use TD3+BC (which adds a BC regularization term) rather than standard TD3 for the offline RL oracle?

## Architecture Onboarding

- **Component map**: Offline RL Oracle -> Reward Modifier -> EXP3 Bandit Controller -> Policy Accumulator -> Offline RL Oracle
- **Critical path**: 
  1. Initialize P_0(λ) = 1/K, policy network parameters.
  2. For each round t: sample λ_t ~ P_t → construct modified reward → run M gradient steps on offline RL oracle → obtain Ṽ_t → update P_{t+1} via EXP3.
  3. Return π_T (last iterate) or average.

- **Design tradeoffs**:
  - K (number of arms): Higher K reduces discretization error but increases EXP3 regret (∝√(KT log K)). Paper finds K=5 optimal; K=2 already works well.
  - M (gradient steps per round): Low M risks insufficient progress; high M risks policy converging to unsafe region before λ adapts. M=10 is robust.
  - C (max λ): Too small (C=2) under-penalizes violations; too large (C=10) may over-penalize and reduce reward. C=5 works across tasks.
  - λ set geometry: Uniform vs. adaptive (denser near low penalties). Adaptive better for looser constraints (κ=40).

- **Failure signatures**:
  - **Cost explodes (> κ)**: Likely C too small or M too large; increase C or reduce M.
  - **Reward near zero, cost safe**: Over-penalization; reduce C or use adaptive λ set with smaller intermediate values.
  - **High variance across seeds**: Check dataset quality; offline RL oracle may have coverage issues.
  - **Training instability (oscillating costs)**: M too small causing frequent aggressive λ updates; increase M.

- **First 3 experiments**:
  1. **Sanity check on a simple task (BallRun)**: Run O3SRL with K=5, M=10, C=5. Verify cost ≤ κ and reward is non-trivial. Compare against TD3+BC with fixed λ=0 (unsafe) and λ=5 (conservative).
  2. **Ablation on K**: Run K∈{2,5,10} on 2-3 tasks. Plot reward vs. cost to confirm K=5 is sufficient and K=10 yields diminishing returns.
  3. **Base offline RL swap**: Replace TD3+BC with IQL on Circle tasks. Verify similar performance to confirm modularity of the framework.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the O3SRL framework be effectively extended to the offline-to-online safe reinforcement learning setting?
- Basis in paper: [explicit] Section 7 states: "Future work includes... extending it to the offline-to-online safe RL setting."
- Why unresolved: The current framework is specifically designed for static datasets where interaction is prohibited, avoiding the complexities of online exploration and dynamic constraint satisfaction during training.
- What evidence would resolve it: A modification of the O3SRL algorithm that allows for safe online fine-tuning and empirical results showing it maintains safety while improving performance during online interactions.

### Open Question 2
- Question: How does O3SRL perform when applied to complex, real-world safety-critical applications outside of simulated benchmarks?
- Basis in paper: [explicit] Section 7 lists "applying O3SRL to real-world applications" as a primary direction for future work.
- Why unresolved: The paper evaluates the method exclusively on the DSRL benchmark (simulated environments like Bullet-Safety-Gym), which may not capture the noise, partial observability, or distributional shifts present in physical systems.
- What evidence would resolve it: Successful deployment of O3SRL on hardware (e.g., autonomous vehicles, robotics) demonstrating reliable constraint satisfaction and high reward in uncontrolled environments.

### Open Question 3
- Question: Does the practical decision to return the last-iterate policy ($\pi_T$) retain the convergence guarantees theoretically established for the average policy distribution ($\bar{D}$)?
- Basis in paper: [inferred] Theorem 1 guarantees convergence to an $\epsilon$-approximate equilibrium for the average distribution $\bar{D}$. However, Section 5.1 notes that returning $\bar{D}$ is "prohibitively memory intensive," so the practical algorithm returns the last-iterate $\pi_T$ instead.
- Why unresolved: The theoretical proof relies on averaging to smooth optimization trajectories; it is not explicitly proven if the final single policy satisfies the same optimality bounds.
- What evidence would resolve it: A theoretical analysis of the last-iterate convergence rate for O3SRL or empirical sensitivity analysis comparing the performance of the last iterate versus the average policy.

## Limitations

- The discretization-based approach assumes the optimal Lagrange multiplier can be well-approximated by a finite set of values, though this approximation error is not quantified for different problem geometries.
- The reliance on EXP3 (rather than continuous optimization methods) introduces a √K regret factor that may limit scalability for high-precision constraint satisfaction.
- The theoretical convergence guarantee depends on strong duality holding (Slater's condition), but the paper does not empirically verify when this assumption breaks.

## Confidence

- **High confidence**: The practical approximation mechanism (discretizing λ and using EXP3) is well-supported by experimental results showing K=2 already outperforms baselines and K=5 is optimal. The modular design allowing any offline RL oracle is clearly demonstrated.
- **Medium confidence**: The theoretical convergence guarantee (Theorem 1) follows standard no-regret analysis but assumes idealized conditions. The practical effectiveness across all DSRL tasks is demonstrated, but the sensitivity to hyperparameters (K, M, C) is not exhaustively studied.
- **Low confidence**: The exact implementation details of the stochastic oracle for EXP3 updates are not fully specified, making exact reproduction challenging. The comparison to purely offline methods is limited to specific baselines rather than a comprehensive survey of the OSRL literature.

## Next Checks

1. **Discretization error analysis**: Systematically vary K from 2 to 20 on 3 representative tasks and plot the trade-off between constraint satisfaction and reward to quantify the approximation error and identify the point of diminishing returns.

2. **Offline RL oracle ablation**: Replace the TD3+BC base with alternative offline RL methods (CQL, TD3+BC with different hyperparameters, or purely BC) to test the robustness of O3SRL's constraint enforcement across different pessimism mechanisms.

3. **Dataset coverage sensitivity**: Evaluate O3SRL on artificially degraded versions of the DSRL datasets (e.g., subsampling or removing critical state-action pairs) to identify when the offline RL oracle fails and how this impacts constraint satisfaction, validating the strong duality assumptions.