---
ver: rpa2
title: Mixture of Experts Made Intrinsically Interpretable
arxiv_id: '2503.07639'
source_url: https://arxiv.org/abs/2503.07639
tags:
- interpretability
- expert
- experts
- arxiv
- moe-x
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of improving interpretability
  in large language models, which often suffer from polysemantic neurons encoding
  multiple unrelated concepts. The authors propose MoE-X, a Mixture-of-Experts (MoE)
  architecture designed to be intrinsically interpretable by leveraging two key factors:
  wider hidden layers and sparse activations.'
---

# Mixture of Experts Made Intrinsically Interpretable

## Quick Facts
- arXiv ID: 2503.07639
- Source URL: https://arxiv.org/abs/2503.07639
- Authors: Xingyi Yang; Constantin Venhoff; Ashkan Khakzar; Christian Schroeder de Witt; Puneet K. Dokania; Adel Bibi; Philip Torr
- Reference count: 37
- Key outcome: MoE-X achieves GPT-2 level perplexity while significantly improving interpretability through wider hidden layers and sparse activations

## Executive Summary
This paper addresses the challenge of improving interpretability in large language models, which often suffer from polysemantic neurons encoding multiple unrelated concepts. The authors propose MoE-X, a Mixture-of-Experts (MoE) architecture designed to be intrinsically interpretable by leveraging two key factors: wider hidden layers and sparse activations. They demonstrate that MoE layers can be reformulated as sparse, wide MLPs, naturally aligning with interpretability objectives. To enhance interpretability, MoE-X incorporates ReLU experts for activation sparsity and a sparsity-aware routing mechanism that prioritizes experts producing the most sparse activations. Experiments on chess and natural language tasks show that MoE-X achieves performance comparable to dense models while significantly improving interpretability, with perplexity better than GPT-2 and interpretability surpassing sparse autoencoder (SAE)-based approaches. The method eliminates the need for expensive post-hoc interpretability methods while maintaining strong performance.

## Method Summary
MoE-X is a Mixture-of-Experts architecture designed for intrinsic interpretability through width and sparsity. The method uses ReLU activation functions in experts to induce high activation sparsity, combined with a sparsity-aware routing mechanism that estimates which experts will produce the sparsest activations using Gaussian approximations of weight distributions. The architecture reformulates MoE layers as sparse, wide MLPs where the effective hidden dimension is M×D but only k experts contribute non-zero activations. The model is upcycled from dense checkpoints and trained with AdamW optimizer, batch sizes of 100-320, and 60k-100k iterations. Key hyperparameters include 8 experts with 2 active per token, load balance loss λ=0.001, and cosine learning rate decay.

## Key Results
- MoE-X achieves validation loss of 0.211 vs 0.213 for GPT-2 on chess dataset
- BSP Coverage Score improves from 26% to 38% compared to Switch Transformer
- ReLU experts reduce average ℓ₀ from ~4091 (GELU) to ~166 with reconstruction score improving from 0.734 to 0.829
- Outperforms GPT-2+SAE in interpretability while maintaining comparable perplexity

## Why This Works (Mechanism)

### Mechanism 1: MoE Layer as Equivalent Wide-Sparse MLP
A Sparse Mixture-of-Experts layer can be mathematically reformulated as a single wide MLP with structured sparsity, where the effective hidden dimension is M×D but only k experts contribute non-zero activations. Concatenating expert decoder matrices into a "mega-decoder" W_dec and scaling each expert's hidden activations by gating weights ω_j yields the same output as standard MoE computation. Since top-k routing zeros out most ω_j, the concatenated hidden vector z has structured sparsity—entire expert blocks are zero when not selected.

### Mechanism 2: ReLU-Induced Intrinsic Sparsity Within Experts
Replacing GELU with ReLU activation in each expert MLP induces high activation sparsity, which correlates with improved interpretability metrics. ReLU zeros all negative pre-activations, creating hard sparsity (true zeros) rather than the soft near-zero values from GELU. This forces each expert to represent features with fewer active neurons, reducing polysemanticity within each expert.

### Mechanism 3: Sparsity-Aware Gating via Probabilistic Approximation
A gating function can estimate which experts will produce the sparsest activations without explicitly computing all expert outputs, enabling efficient routing toward interpretable representations. Under Gaussian assumptions about encoder weight distributions, pre-activation values h_i^(j) follow N(μ_h, σ_h²). The expected ℓ₀-norm after ReLU is approximately D × Φ(μ_h/σ_h), computed via the error function. The router selects experts minimizing this estimated sparsity.

## Foundational Learning

- **Concept: Polysemanticity and Superposition**
  - Why needed here: The paper's core motivation is that standard LLM neurons encode multiple unrelated concepts, obscuring interpretability. Understanding this problem is essential for grasping why width + sparsity helps.
  - Quick check question: If a neuron activates strongly for both "bank" (financial) and "bank" (river), what problem does this create for mechanistic interpretability?

- **Concept: Sparse Mixture-of-Experts (top-k routing)**
  - Why needed here: MoE-X builds on standard MoE architectures. You need to understand how token-to-expert routing works before understanding how sparsity-aware routing modifies it.
  - Quick check question: In an 8-expert MoE with top-2 routing, what fraction of expert parameters participate in computing any single token's output?

- **Concept: Board State Properties (BSP) as Interpretability Ground Truth**
  - Why needed here: The paper's primary evaluation uses chess BSP coverage/reconstruction scores. Understanding this metric is critical for interpreting experimental claims.
  - Quick check question: Why is chess useful for evaluating interpretability compared to general language tasks?

## Architecture Onboarding

- **Component map:**
  Input token x ∈ R^d → [Sparsity-Aware Router] → [ReLU Experts] → [Mega-Decoder View] → Output
  - Router computes μ_h, σ_h for each expert from W_enc statistics
  - Router computes -erf(μ_h / (√2 × σ_h)) for all experts
  - Top-k + Softmax → routing weights ω_j
  - Only top-k experts computed: z^(j) = ReLU(W_enc^(j) @ x)
  - Scaled activations: ω_j × z^(j)
  - Concatenate: z = [ω_1×z^(1), ..., ω_M×z^(M)]
  - Output: W_dec @ z

- **Critical path:** Router → Expert selection → ReLU activation → Weighted combination. The router's sparsity estimate must be accurate; the ReLU must produce genuine zeros; the weighted combination must preserve interpretability.

- **Design tradeoffs:**
  - ReLU vs. GELU: ReLU guarantees sparsity but may lose some representational nuance; GELU is smoother but produces fewer true zeros.
  - Sparsity-optimized routing vs. performance-optimized routing: May select different experts than standard top-k based on output magnitude.
  - Number of experts (M) vs. expert hidden size (D): More experts = wider effective MLP but more routing complexity.

- **Failure signatures:**
  - Low BSP coverage despite high sparsity → sparsity not aligned with meaningful features.
  - Router correlation with actual sparsity drops below ~0.9 → Gaussian assumption violated.
  - Performance gap vs. dense baseline widens → sparsity constraints too aggressive.
  - Load imbalance (some experts rarely selected) → may need auxiliary balancing loss.

- **First 3 experiments:**
  1. **Sanity check:** Train standard Switch Transformer vs. MoE-X on chess dataset with identical hyperparameters. Verify MoE-X achieves comparable validation loss but higher BSP coverage.
  2. **Ablation:** Disable sparsity-aware routing (use standard top-k), keeping ReLU experts. Measure drop in reconstruction score to isolate router contribution.
  3. **Scaling test:** Vary number of active experts k ∈ {1, 2, 4} while keeping total parameters fixed. Plot BSP coverage vs. k to find optimal sparsity-interpretability tradeoff point.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the optimal activation sparsity level be systematically determined to maximize interpretability without compromising model performance?
- Basis in paper: The authors state in Section 3.1 that while sparsity improves interpretability, "the most extreme sparsity does not always yield the best interpretability, and identifying the optimal sparsity level remains difficult."
- Why unresolved: The relationship between sparsity and interpretability is non-monotonic; simply maximizing sparsity can degrade utility, and no automated heuristic for finding this balance is provided.
- What evidence would resolve it: A systematic study correlating specific L0 norms with BSP Coverage scores across different model scales to derive a scaling law for sparsity.

### Open Question 2
- Question: Do alternative architectures claiming intrinsic interpretability, such as SoLU or Monet, actually improve interpretability compared to dense baselines when evaluated using rigorous ground-truth metrics?
- Basis in paper: Section 5.1 notes that SoLU and Monet underperformed on chess tasks, leading to the conclusion: "These findings call for a thorough re-evaluation of this field, as many claims of improved interpretability lack strong empirical support."
- Why unresolved: The paper's benchmarks contradict the claims of prior work, suggesting that improvements may be task-specific or artifacts of less rigorous evaluation metrics.
- What evidence would resolve it: Applying the chess Board State Properties (BSP) framework and the auto-interpretability pipeline to these other architectures to generate comparable, quantitative interpretability scores.

### Open Question 3
- Question: Does the Gaussian assumption required for efficient sparsity-aware routing hold for trained experts, and how does deviation from this assumption affect routing accuracy?
- Basis in paper: The sparsity-aware routing mechanism (Section 4.3.2) relies on assuming weights are i.i.d. Gaussian to estimate activation sparsity cheaply.
- Why unresolved: Neural network weights often diverge from Gaussian distributions during training; if this assumption fails, the router may select sub-optimal experts, degrading model performance.
- What evidence would resolve it: An analysis of the empirical weight distributions in trained MoE-X experts compared to Gaussian fits, and a measurement of the correlation between the estimated sparsity score and the actual L0 norm of activations.

## Limitations

- The Gaussian assumption underlying sparsity-aware routing may not hold for real-world weight distributions, potentially degrading routing effectiveness.
- The upcycling procedure from dense to sparse models is not fully detailed, creating uncertainty about whether performance gains come from the architecture itself or the initialization process.
- Interpretability evaluation relies on proxy measurements (BSP metrics, auto-interpretability pipelines) rather than ground-truth semantic understanding, limiting conclusiveness of interpretability claims.

## Confidence

**High Confidence:** The mathematical reformulation of MoE layers as wide-sparse MLPs is well-established and clearly demonstrated. The performance metrics (validation loss, perplexity) are standard and directly comparable to baseline models. The correlation between sparsity-aware routing and actual expert sparsity is empirically validated.

**Medium Confidence:** The ReLU-induced sparsity mechanism is well-supported by experiments showing reduced ℓ₀ norms and improved reconstruction scores, but the causal relationship between sparsity and interpretability could be more thoroughly established. The Gaussian approximation for routing decisions works well in practice but may not generalize to all model architectures or training regimes.

**Low Confidence:** The auto-interpretability pipeline using Llama 3.1 70B for detecting interpretable features introduces significant uncertainty, as the results depend heavily on the explainer/scorer model's capabilities and the quality of prompt engineering. The generalizability of chess-based BSP metrics to broader language tasks remains unclear.

## Next Checks

1. **Weight Distribution Validation:** Perform detailed analysis of expert encoder weight distributions across multiple training checkpoints to empirically validate the Gaussian assumption. Measure how deviations from normality affect the accuracy of sparsity estimates and downstream interpretability metrics.

2. **Cross-Architecture Generalization:** Test MoE-X on diverse model architectures (different embedding sizes, number of layers, vocabulary sizes) and tasks (beyond chess and language modeling) to assess robustness. Specifically examine whether the ReLU sparsity benefits transfer to convolutional or multimodal MoE architectures.

3. **Ground-Truth Interpretability Verification:** Conduct human evaluation studies comparing MoE-X features against manually annotated semantic units in controlled datasets. This would provide direct validation of the auto-interpretability pipeline's detection accuracy and establish whether increased sparsity truly correlates with improved semantic monosemanticity.