---
ver: rpa2
title: Can LLMs Simulate Human Behavioral Variability? A Case Study in the Phonemic
  Fluency Task
arxiv_id: '2505.16164'
source_url: https://arxiv.org/abs/2505.16164
tags:
- human
- word
- llms
- claude
- words
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models struggle to replicate human behavioral variability
  in the phonemic fluency task, despite matching average response counts. Thirty-four
  model configurations, including varying prompt specificity, sampling temperature,
  and model type, were evaluated against 106 human participants.
---

# Can LLMs Simulate Human Behavioral Variability? A Case Study in the Phonemic Fluency Task

## Quick Facts
- arXiv ID: 2505.16164
- Source URL: https://arxiv.org/abs/2505.16164
- Reference count: 24
- Primary result: LLMs fail to replicate human behavioral variability in phonemic fluency despite matching average response counts

## Executive Summary
This study evaluates whether large language models can simulate human behavioral variability in the phonemic fluency task, where participants generate words beginning with "F" within one minute. Thirty-four model configurations across six model families were tested against 106 human participants. While some models matched human averages and lexical preferences, none reproduced the scope of human variability. The research reveals fundamental differences in retrieval structure and word production patterns between LLMs and humans.

## Method Summary
The study used human data from 106 participants in an English F-fluency task. Thirty-four LLM configurations across OpenAI, Anthropic, Google, xAI, Meta, and Alibaba were prompted to simulate all 106 participants. Temperature settings ranged from 0.3-1.5, with prompt variations including full demographic/performance information, demographic-only, performance-only, and no-info conditions. Outputs were filtered for repetitions, out-of-category words, and same-lemma variants. Analysis included response counts, lexical diversity metrics (TTR, ITTR), production frequency distributions (Zipf's law), linguistic predictor correlations, and network analyses of word co-occurrences.

## Key Results
- No LLM configuration reproduced human-level behavioral variability despite matching average response counts
- LLMs showed significantly lower lexical diversity (TTR: 0.13-0.26 vs human 0.27) and flatter idiosyncratic diversity (ITTR: 0.25-0.29 vs human 0.42)
- Network analyses revealed fundamental structural differences: humans showed stronger local clustering but weaker global integration, opposite to Claude 3.7 Sonnet
- Models exhibited steeper frequency distributions (α: 1.19-1.28 vs human 0.89) and different linguistic predictors (word length vs orthographic neighborhood size)

## Why This Works (Mechanism)

### Mechanism 1
Training on written language constrains output variability by encoding a narrow slice of communicative behavior. LLMs learn probability distributions over written text corpora, producing consistent patterns that reduce idiosyncratic responses. This consistency reflects a design feature of LLM optimization rather than a bug.

### Mechanism 2
Frequency-biased retrieval produces steeper word production distributions than humans. LLMs optimize for high-probability tokens, amplifying common words and suppressing the "long tail" of rare responses. Retrieval dynamics follow surface-level frequency heuristics rather than form-based associative memory.

### Mechanism 3
Network-level structural differences indicate fundamentally different retrieval organizations. Humans organize lexical retrieval into tighter local clusters with weaker global integration; LLMs optimize for network-wide efficiency with more uniform connectivity. Local clustering in humans reflects form-based associative chains that LLMs cannot replicate through next-token prediction.

## Foundational Learning

- **Type-Token Ratio (TTR)**
  - Why needed here: Measures lexical diversity (unique words / total words). Critical for assessing whether LLMs match human vocabulary range.
  - Quick check question: If humans produce TTR=0.27 and Claude produces TTR=0.13, what does this tell you about output diversity?

- **Zipf's Law / Power-law distributions**
  - Why needed here: Word production frequency in verbal fluency follows power-law distributions. Comparing α coefficients reveals distributional shape differences.
  - Quick check question: A higher α coefficient indicates what kind of distribution compared to human baseline?

- **Network clustering coefficient and average shortest path length**
  - Why needed here: These metrics quantify local vs. global network organization. Essential for understanding structural differences in word co-occurrence patterns.
  - Quick check question: Higher clustering + longer path length suggests what kind of network structure?

## Architecture Onboarding

- Component map: Prompt engineering layer -> Sampling layer -> Evaluation layer -> Ensemble layer
- Critical path: 1. Define prompt specificity, 2. Set temperature and model configuration, 3. Generate N simulated participant responses, 4. Filter errors, 5. Compute TTR, ITTR, fit Zipf distribution, 6. Build co-occurrence networks, compare CC and ASPL
- Design tradeoffs: Higher temperature → more diversity but risk of incoherence; Full participant info → better response count matching but no variability improvement; Ensembles → expected diversity boost but empirically shows overlapping high-frequency vocabulary; Reasoning models → more variance in response counts but unstable constraint adherence
- Failure signatures: Extreme overproduction (O4-mini: 76 words vs human 17) indicates failed constraint following; Low ITTR (<0.30 vs human 0.42) indicates insufficient idiosyncratic responses; Steeper α (>1.19 vs 0.89) indicates high-frequency dominance; Lower clustering coefficient indicates missing local associative structure
- First 3 experiments: 1. Baseline comparison: Run Claude 3.7 Sonnet at temperatures [0.3, 0.7, 1.0] with full prompts; measure TTR and α coefficient against human baseline. 2. Prompt ablation: Test demographic-only vs. performance-only vs. no-info prompts; identify which constraints matter for response count vs. diversity. 3. Targeted noise injection: Implement explicit downweighting of top-K frequent words in sampling; measure whether long-tail diversity improves without breaking coherence.

## Open Questions the Paper Calls Out

### Open Question 1
Can incorporating granular cognitive profiles (e.g., reading history, specific task exposure) into prompts successfully induce human-like behavioral variability in LLMs? The authors note they "did not incorporate richer individual-level traits" and suggest future work explore "cognitively grounded persona modeling" to better simulate differences. This is unresolved because simple demographic prompting failed to increase variability; it is unknown if deeper cognitive context is sufficient to overcome models' inherent bias toward central tendency.

### Open Question 2
Do the findings regarding structural rigidity and low variability persist in languages with orthographic systems significantly different from English? The authors state results may "differ in languages with different orthographic or phonological systems" and list multilingual comparisons as necessary. This is unresolved because the study is restricted to English "F"; it is unclear if models' reliance on frequency-based heuristics is universal or an artifact of specific orthographic training data.

### Open Question 3
Can alternative sampling strategies or newer architectures correct the divergence in retrieval dynamics, specifically the mismatch in orthographic neighborhood influence? The discussion highlights humans rely on form-based associations while LLMs prioritize surface features, suggesting fundamental retrieval differences that current temperature settings cannot fix. This is unresolved because adjusting temperature and ensemble methods failed to bridge the gap in network structure, indicating the issue may be architectural rather than parameter-based.

## Limitations
- Single phonemic fluency task limits generalizability to other cognitive tasks
- Human sample from specific population constrains cross-cultural applicability
- Short response sequences may not capture asymptotic LLM behavior at larger scales
- Prompt engineering may not exhaustively explore space of possible instructions

## Confidence

*High Confidence:* LLMs fail to match human behavioral variability despite matching average response counts (supported by multiple independent metrics across 34 configurations).

*Medium Confidence:* Mechanistic explanations linking findings to training data constraints and frequency-biased retrieval are plausible but not definitively proven.

*Low Confidence:* Ensemble method failure based on single experimental design; more sophisticated strategies might yield different results.

## Next Checks

1. Cross-linguistic validation: Test methodology with phonemic fluency in languages with different orthographic transparency and non-Latin scripts to determine if training data language constraints systematically affect variability reproduction.

2. Asymptotic scaling analysis: Generate substantially longer word lists (100+ words per participant) to test whether LLM behavioral patterns converge toward or diverge further from human distributions at larger scales.

3. Targeted architectural intervention: Implement modified sampling that explicitly downweights high-frequency tokens during generation and measure whether this recovers flatter frequency distributions and increased long-tail diversity without sacrificing response coherence.