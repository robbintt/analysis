---
ver: rpa2
title: 'From Relative Entropy to Minimax: A Unified Framework for Coverage in MDPs'
arxiv_id: '2601.11890'
source_url: https://arxiv.org/abs/2601.11890
tags:
- coverage
- state
- action
- exploration
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces a unified framework for reward-free exploration\
  \ in Markov Decision Processes by proposing a parameterized family of concave coverage\
  \ objectives defined over state-action occupancy measures. The key idea is to design\
  \ exploration strategies that actively prioritize under-explored state-action pairs\
  \ using a weighted concave utility function parameterized by \u03C1."
---

# From Relative Entropy to Minimax: A Unified Framework for Coverage in MDPs

## Quick Facts
- arXiv ID: 2601.11890
- Source URL: https://arxiv.org/abs/2601.11890
- Authors: Xihe Gu; Urbashi Mitra; Tara Javidi
- Reference count: 40
- Primary result: Unified framework for reward-free exploration in MDPs that smoothly interpolates between average-case (KL divergence) and worst-case coverage via a parameter ρ

## Executive Summary
This paper introduces a unified framework for reward-free exploration in Markov Decision Processes by proposing a parameterized family of concave coverage objectives defined over state-action occupancy measures. The key innovation is a weighted concave utility function parameterized by ρ that enables smooth interpolation between average-case coverage (when ρ=1) and worst-case coverage (as ρ→∞). The authors develop a gradient-based algorithm that steers the induced occupancy toward desired coverage patterns, with theoretical guarantees showing convergence to the optimal coverage measure at rate O(t⁻¹/³).

## Method Summary
The method optimizes a parameterized concave utility function U_ρ over state-action occupancy measures using a Frank-Wolfe algorithm. The framework defines g_ρ(s,a,d_{s,a}) = μ_{s,a} log d_{s,a} for ρ=1 and g_ρ = μ_{s,a}^ρ/(1-ρ) × d_{s,a}^{1-ρ} for ρ>1. Algorithm 1 performs episodic updates where at each episode k it computes empirical occupancy, evaluates the gradient (μ_{s,a}/d_{s,a})^ρ with smoothing, solves a linear optimization over the occupancy polytope via Frank-Wolfe oracle, derives the policy, and executes it for τ_k = τ_1 × k² steps. The approach assumes known transition dynamics and ergodic, reversible chains.

## Key Results
- Smooth interpolation between average-case coverage (ρ=1, KL divergence) and worst-case coverage (ρ→∞) is theoretically proven
- Convergence to optimal coverage measure at rate O(t⁻¹/³) with high probability under Frank-Wolfe oracle setting
- Concave utility structure naturally captures diminishing returns in exploration, allocating effort toward under-covered regions
- Gradient-based approach provides explicit control to prioritize under-explored state-action pairs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The concave utility structure naturally captures diminishing returns in exploration, allocating effort toward under-covered regions.
- **Mechanism:** The gradient of U_ρ takes the form (∇U_ρ(d))_{s,a} = (μ_{s,a}/d_{s,a})^ρ. When d_{s,a} is large relative to μ_{s,a} (well-explored), the gradient approaches zero. When d_{s,a} is small (under-explored), the gradient is large. This creates an automatic re-balancing pressure without explicit thresholds.
- **Core assumption:** The occupancy measure d^π exists uniquely and the policy induces an ergodic Markov chain.
- **Evidence anchors:** [abstract] "concavity of U_ρ captures the diminishing return associated with over-exploration, the simple closed form of the gradient of U_ρ enables an explicit control to prioritize under-explored state-action pairs"; [section III] "state-action pairs that are already well covered (i.e., with large d_{s,a} relative to μ_{s,a}) induce a gradient close to zero, whereas poorly covered pairs with small d_{s,a} produce large gradients"
- **Break condition:** If the chain is not ergodic or has absorbing states, the stationary distribution assumption fails; gradient magnitudes may become uninterpretable for unreachable state-action pairs.

### Mechanism 2
- **Claim:** Increasing ρ smoothly interpolates the solution from average-case coverage (KL divergence at ρ=1) to worst-case minimax coverage (as ρ→∞).
- **Mechanism:** Lemma 3 shows that (1-ρ)U_ρ rescaled by 1/ρ converges to max_{s,a} μ_{s,a}/d_{s,a}. Furthermore, the gradient ratio between any two pairs with different coverage ratios tends to infinity as ρ→∞, causing optimization to focus exclusively on the worst-covered pair.
- **Core assumption:** The feasible occupancy set D is closed and the minimax optimizer exists.
- **Evidence anchors:** [abstract] "as ρ increases, the resulting exploration strategy increasingly emphasizes the least-explored state-action pairs, recovering worst-case coverage behavior in the limit"; [section IV.C] Lemma 4 formalizes that every limit point of {d_ρ^*} as ρ→∞ solves the minimax objective (5)
- **Break condition:** If ρ is set too large with noisy or sparse occupancy estimates, gradient variance explodes due to the (μ/d)^ρ amplification; numerical instability may occur.

### Mechanism 3
- **Claim:** The Frank-Wolfe oracle with episodic updates and growing episode lengths converges to the optimal coverage measure at rate O(t⁻¹/³).
- **Mechanism:** The algorithm treats (μ_{s,a}/d̂_{s,a})^ρ as a pseudo-reward, computes a policy maximizing expected cumulative pseudo-reward (FW direction), executes it for τ_k = τ_1 × k² steps, then updates the empirical occupancy. The smoothness constant C_η = ρ μ_max^ρ / (2η)^{ρ+1} (Lemma 5) bounds gradient variance, enabling the recursion analysis.
- **Core assumption:** Known transition dynamics (oracle setting); ergodic, reversible chains with spectral gap γ_π.
- **Evidence anchors:** [section V.C] Theorem 1 with explicit high-probability bound ξ_K = Õ(t_K^{-1/3}); [section V.D] "the FW oracle still accounts for the cumulative reward collected along trajectories, ensuring that its intermediate decisions move efficiently toward the worst-case state-action region rather than chasing it myopically"
- **Break condition:** If transitions are unknown or mis-specified, the FW oracle provides incorrect directions; convergence guarantees no longer hold.

## Foundational Learning

- **Concept: State-action occupancy measures**
  - Why needed here: The entire framework is defined over d^π_{s,a}, the stationary visitation frequencies. Without understanding that policies induce distributions over state-action pairs, the objective U_ρ(d^π) is opaque.
  - Quick check question: Given a policy π, can you compute the stationary distribution of the induced Markov chain for a small 3-state, 2-action MDP?

- **Concept: Concave optimization and diminishing returns**
  - Why needed here: The utility U_ρ is designed to be concave; understanding why concavity encodes "less marginal benefit from additional visits to already-covered pairs" is essential for interpreting the gradient behavior.
  - Quick check question: Explain why a concave utility over d_{s,a} means the first visit to a state-action pair provides more marginal benefit than the tenth visit.

- **Concept: Frank-Wolfe (conditional gradient) method**
  - Why needed here: Algorithm 1 uses a FW oracle to find ψ^{k+1} = argmax_{d∈D_η} ⟨∇U_ρ, d⟩. Understanding that this is a linear optimization over the occupancy polytope (solvable via LP or dynamic programming) is critical for implementation.
  - Quick check question: Why does Frank-Wolfe require only a linear oracle rather than a projection step, and what does the linear oracle compute in this context?

## Architecture Onboarding

- **Component map:** Weight manager -> Occupancy estimator -> Gradient computer -> Frank-Wolfe oracle -> Episode scheduler
- **Critical path:**
  1. Initialize visitation counts T_{s,a}(1) = 0
  2. At episode k: compute empirical occupancy d̂(t_k)
  3. Compute gradient (μ/d̂)^ρ with smoothing
  4. Call FW oracle to get ψ^{k+1}, extract policy π^{k+1}(a|s) = ψ^{k+1}(s,a) / Σ_b ψ^{k+1}(s,b)
  5. Execute π^{k+1} for τ_k steps, update counts
  6. Repeat until budget exhausted
- **Design tradeoffs:**
  - Low ρ: More uniform exploration, numerically stable, but may over-invest in already-covered regions
  - High ρ: Focused worst-case coverage, but gradient variance increases and may chase noise
  - Small η: Tighter boundary, higher smoothness constant C_η, slower convergence
  - Large τ_1: More samples per episode early, reduces variance but delays adaptation
- **Failure signatures:**
  - Gradient explosion: If d̂_{s,a} approaches zero for any (s,a), the gradient term diverges. Mitigation: smoothing with ε > 0 and the D_η constraint
  - Non-ergodic chain: If a policy induces disconnected components, stationary distribution is not unique; FW oracle may return infeasible directions
  - Slow mixing: If spectral gap γ_π is tiny, empirical occupancy d̂ deviates significantly from true ψ within episode; τ_k may be insufficient
- **First 3 experiments:**
  1. **Sanity check on a toy MDP**: Build a 4-state chain MDP with known transitions, set μ uniform, run Algorithm 1 with ρ ∈ {1, 2, 5, 10}, verify that final d̂ approaches uniform as ρ increases. Check that max_{s,a} μ_{s,a}/d̂_{s,a} decreases with larger ρ.
  2. **Gradient variance vs. ρ**: On a moderate-sized MDP (50 states, 5 actions), run multiple seeds and measure the variance of the gradient norm across episodes. Confirm that variance grows with ρ and identify a practical upper bound for stable learning.
  3. **Episode length ablation**: Fix ρ=2 and compare τ_k = τ_1 × k^α for α ∈ {1, 1.5, 2, 2.5}. Measure convergence rate of U_ρ(d̂(t_K)) to U_ρ(d*) and validate that α=2 provides the best tradeoff as predicted by Theorem 1.

## Open Questions the Paper Calls Out
- **Question:** How can the framework be extended to MDPs with unknown transition dynamics while maintaining convergence guarantees?
  - **Basis in paper:** [explicit] The authors state in Section V.C regarding the algorithm design: "Here, we consider the oracle setting in which the transition probabilities P(· | s, a) are known, and the algorithm can be extended to the unknown transition cases."
  - **Why unresolved:** The theoretical analysis (Theorem 1) relies on the Frank-Wolfe oracle solving for the stationary distribution exactly, which requires perfect knowledge of the transition kernel P.
  - **What evidence would resolve it:** Convergence rates or regret bounds for an algorithm that estimates P online while optimizing the U_ρ objective.

- **Question:** Can the importance weights μ be adapted online based on observed uncertainty without destabilizing the optimization?
  - **Basis in paper:** [explicit] Section VI explicitly lists "adaptive importance weights" as a direction for future extensions.
  - **Why unresolved:** The theoretical framework (Section III) assumes μ is finite, bounded, and fixed throughout the exploration process.
  - **What evidence would resolve it:** Analysis showing that updating μ_t (e.g., based on variance of transition estimates) retains the concavity and smoothness properties required for the gradient-based convergence.

- **Question:** What is the principled method for selecting the optimal parameter ρ for a specific downstream task?
  - **Basis in paper:** [inferred] The paper demonstrates that ρ smoothly interpolates between average and worst-case coverage, but it does not provide a theoretical criterion for selecting the "best" ρ value for a given estimation problem.
  - **Why unresolved:** While the limit behavior (ρ→∞) is known, the practical utility of intermediate values depends on the specific structure of the MDP and the downstream objective.
  - **What evidence would resolve it:** A theoretical mapping or heuristic connecting the spectral properties of the MDP or the signal-to-noise ratio of the downstream task to the optimal choice of ρ.

## Limitations
- Theoretical framework assumes known transition dynamics, limiting direct applicability to real-world exploration scenarios
- Convergence rate O(t⁻¹/³) is presented without comparison to empirical baselines or alternative exploration methods
- Sensitivity to choice of ρ, smoothing parameter ε, and episode length schedule τ₁ is not empirically characterized

## Confidence
- **High confidence**: The concave utility structure and its gradient mechanism are mathematically rigorous and well-supported by the formalism. The Frank-Wolfe algorithm framework for optimizing over occupancy measures is established and correctly applied.
- **Medium confidence**: The theoretical interpolation from average-case (ρ=1) to worst-case (ρ→∞) coverage is proven, but the practical implications and numerical stability at large ρ values require empirical validation. The convergence rate analysis assumes perfect FW oracle calls, which may not hold with approximate solvers.
- **Low confidence**: The claims about numerical stability and practical hyperparameter ranges (especially for large ρ) are not empirically supported. The extension to unknown transition dynamics is mentioned but not developed or validated.

## Next Checks
1. **Gradient variance and stability**: Systematically measure the variance of the gradient norm across episodes as ρ increases from 1 to 10, using multiple random seeds on a fixed MDP. Identify the threshold ρ where variance becomes problematic and suggest a practical upper bound.
2. **Empirical convergence comparison**: Implement a baseline exploration method (e.g., uniform random exploration or upper-confidence-bound-based exploration) and compare the convergence of U_ρ(d̂(t)) to U_ρ(d*) over the same budget. Measure both average-case and worst-case coverage metrics.
3. **Episode length schedule ablation**: Fix ρ=2 and compare τₖ = τ₁ × k^α for α ∈ {1, 1.5, 2, 2.5}. Measure the convergence rate of U_ρ(d̂(tₖ)) to U_ρ(d*) and validate that α=2 provides the optimal tradeoff as predicted by Theorem 1.