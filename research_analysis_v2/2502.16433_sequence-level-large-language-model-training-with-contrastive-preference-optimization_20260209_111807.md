---
ver: rpa2
title: Sequence-level Large Language Model Training with Contrastive Preference Optimization
arxiv_id: '2502.16433'
source_url: https://arxiv.org/abs/2502.16433
tags:
- arxiv
- training
- data
- language
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies a training-inference mismatch in large language
  models caused by next token prediction loss, which only provides token-level supervision
  during training but requires sequence-level generation during inference. To address
  this, the authors propose Contrastive Preference Optimization (CPO), a method that
  injects sequence-level information by contrasting ground truth continuations with
  synthetic negative samples.
---

# Sequence-level Large Language Model Training with Contrastive Preference Optimization

## Quick Facts
- arXiv ID: 2502.16433
- Source URL: https://arxiv.org/abs/2502.16433
- Reference count: 11
- One-line primary result: CPO consistently outperforms next token prediction with up to 13.8% improvement in win rate against ground truth

## Executive Summary
The paper addresses a fundamental training-inference mismatch in large language models caused by next token prediction loss, which only provides token-level supervision during training but requires sequence-level generation during inference. The authors propose Contrastive Preference Optimization (CPO), a method that injects sequence-level information by contrasting ground truth continuations with synthetic negative samples. Unlike RLHF or DPO, CPO does not require human preference data and can be applied at any training stage. Experiments on instruction-following and text generation tasks show consistent improvements across model scales.

## Method Summary
CPO optimizes language models by contrasting ground truth sequences with synthetic negatives generated through various strategies (autoregressive, batch, meanfield, truncation). The method computes a contrastive loss that maximizes the probability ground truth ranks highest among K candidates, implicitly learning sequence-level quality signals without human annotations. Training occurs in two phases: MLE pretraining to establish a reference model, followed by CPO fine-tuning using the contrastive objective. The approach also supports weight-space ensembles combining MLE and CPO parameters.

## Key Results
- CPO consistently outperforms next token prediction with up to 13.8% improvement in win rate against ground truth continuations
- MixN negative sampling strategy (combination of AN, BN, MN, TN) provides the best overall performance
- Weight-space ensembles (θ = α·θ_MLE + (1-α)·θ_CPO) further boost performance, with optimal α depending on model capability
- CPO achieves strong results on both instruction-following (Dolly dataset) and open-ended text generation (Wikidump)

## Why This Works (Mechanism)

### Mechanism 1: Exposure Bias Mitigation via Sequence-Level Contrast
CPO reduces training-inference mismatch by providing sequence-level supervision that next token prediction lacks. During standard training, the model predicts each token with full ground-truth context (teacher forcing). At inference, it must generate sequences autoregressively using only its own predictions. CPO bridges this by forcing the model to distinguish ground-truth completions from synthetic alternatives, teaching it to evaluate entire sequences rather than individual tokens.

### Mechanism 2: Implicit Reward Without Human Labels
CPO creates a preference signal using only ground-truth data and synthetic negatives, eliminating the need for human preference annotations required by RLHF/DPO. The CPO loss maximizes the probability that ground truth ranks highest among K candidates, implicitly defining a reward function via the EBM equivalence. By contrasting against synthetic negatives, the model learns what makes sequences "better" without explicit human rankings.

### Mechanism 3: Dense Ranking Signal via Embedding Similarity
Adding ranking information among negative samples (based on embedding similarity to ground truth) provides denser training signals than binary contrast. When using ranking, the model learns not just "ground truth > all negatives" but a partial ordering among negatives based on cosine similarity to ground truth. This creates graded supervision rather than binary preference.

## Foundational Learning

- **Exposure Bias in Sequence Modeling**: Why needed - The entire CPO motivation rests on understanding why next-token prediction creates train-test discrepancy. Quick check - Can you explain why a model trained with teacher forcing might fail when generating autoregressively at inference?

- **Noise Contrastive Estimation (NCE)**: Why needed - CPO is derived from NCE principles—learning by distinguishing data from noise. Quick check - How does NCE convert density estimation into a classification problem, and how does CPO adapt this for sequences?

- **Energy-Based Models and RLHF Equivalence**: Why needed - CPO's derivation relies on the equivalence between RLHF objectives and EBMs. Quick check - Given the EBM form π*(y|x) = (1/Z) · π_ref(y|x) · exp(r(x,y)/β), how can you recover the reward function r from the optimal policy π*?

## Architecture Onboarding

- **Component map**: Reference model (π_ref) -> Trainable policy (π_θ) -> Negative sampler -> Ranking module (optional) -> Weight-space ensemble
- **Critical path**: 1) Pre-train with MLE to get π_ref, 2) Sample ground-truth (x, y1) from dataset D, 3) Generate K-1 negatives using chosen sampling strategy, 4) (Optional) Compute rankings via embedding similarity, 5) Compute CPO loss using log-probability ratios, 6) Update π_θ and save checkpoints
- **Design tradeoffs**: AN vs BN/MN/TN negatives (quality vs speed), K (number of negatives) vs memory/compute, β (temperature) vs KL penalty strength, Ranking vs no ranking (signal density vs noise), α (ensemble weight) vs model capability
- **Failure signatures**: Win rate < 0.5 vs ground truth (check negative sample quality), DPO baseline outperforms CPO (negative sampling issue), Performance drops with ranking (embedding similarity misalignment), Training instability (β or learning rate issues)
- **First 3 experiments**: 1) Baseline comparison: Train with MLE for 2000 steps, then CPO (AN strategy, K=4, β=5) for 1000 steps, 2) Negative sampling ablation: Compare AN, BN, MN, TN, and MixN strategies, 3) Weight ensemble sweep: Interpolate weights with α ∈ {0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0}

## Open Questions the Paper Calls Out

- **Efficient negative sampling**: Can non-autoregressive methods generate synthetic negatives that match the quality of autoregressive sampling for CPO training? The authors explicitly call for research into generating high-quality negatives beyond the autoregressive fashion.

- **Optimal ensemble weight determination**: Is there a systematic method to determine the optimal weight-space ensemble coefficient (α) based on model architecture or capability? The paper notes that optimal α differs between model scales and states this requires further exploration.

- **Scaling of performance gains**: How does the relative performance gain of CPO over MLE scale with model size and capability? The authors hypothesize more capable models benefit more from CPO but only validate on GPT2-XL and OpenLlama-3B.

## Limitations
- Heavy dependence on negative sampling quality, with autoregressive negatives being computationally expensive
- Primary evaluation using GPT-3.5 win rates introduces potential biases and reliability concerns
- Theoretical claims about CPO's equivalence to NCE and RLHF are not fully proven
- Optimal weight-space ensemble coefficient varies significantly across model capabilities without systematic determination method

## Confidence

**High confidence**: The exposure bias problem exists in standard next-token prediction training; the CPO formulation is mathematically coherent and the general approach of using contrastive learning for sequence-level supervision is sound.

**Medium confidence**: The empirical results showing CPO outperforms MLE are robust across multiple datasets and model scales; the MixN negative sampling strategy consistently shows strong performance relative to other strategies.

**Low confidence**: The theoretical claims about CPO's equivalence to NCE and RLHF are not fully proven; the GPT-3.5 win rate evaluation methodology may not accurately capture true quality improvements; the robustness of results across different domains and model architectures is uncertain.

## Next Checks

1. **Ablation study on negative sampling strategies**: Systematically evaluate AN, BN, MN, TN, and MixN across multiple model scales (small: 125M, medium: 1.3B, large: 7B) and diverse domains (summarization, dialogue, reasoning) to identify which sampling strategies are most robust to domain shifts.

2. **Independent evaluation protocol**: Replicate the CPO experiments with human evaluation on a held-out test set, measuring inter-annotator agreement and comparing results to GPT-3.5 evaluations to assess the reliability of automated win rate metrics.

3. **Theoretical derivation completion**: Complete the mathematical proof of CPO's equivalence to NCE and RLHF, including rigorous derivation of the temperature parameter β's relationship to KL regularization strength and optimal sampling strategy selection.