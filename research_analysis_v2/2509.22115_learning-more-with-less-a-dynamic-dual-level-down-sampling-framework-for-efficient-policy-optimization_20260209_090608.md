---
ver: rpa2
title: 'Learning More with Less: A Dynamic Dual-Level Down-Sampling Framework for
  Efficient Policy Optimization'
arxiv_id: '2509.22115'
source_url: https://arxiv.org/abs/2509.22115
tags:
- arxiv
- policy
- grpo
- tokens
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the inefficiency of critic-free reinforcement
  learning methods like GRPO and GSPO, which suffer from slow convergence due to uninformative
  samples and tokens diluting learning signals. To improve training efficiency, the
  authors propose the Dynamic Dual-Level Down-Sampling (D3S) framework, which operates
  at two levels: (1) sample-level, selecting rollouts that maximize advantage variance
  to yield higher policy gradients, and (2) token-level, prioritizing tokens with
  high product of advantage magnitude and policy entropy to focus updates on uncertain
  and impactful tokens.'
---

# Learning More with Less: A Dynamic Dual-Level Down-Sampling Framework for Efficient Policy Optimization

## Quick Facts
- arXiv ID: 2509.22115
- Source URL: https://arxiv.org/abs/2509.22115
- Reference count: 25
- The paper introduces a dual-level down-sampling framework that improves GRPO training efficiency by selecting high-signal samples and tokens, achieving significant performance gains with fewer resources.

## Executive Summary
This paper addresses the inefficiency of critic-free reinforcement learning methods like GRPO and GSPO, which suffer from slow convergence due to uninformative samples and tokens diluting learning signals. To improve training efficiency, the authors propose the Dynamic Dual-Level Down-Sampling (D3S) framework, which operates at two levels: (1) sample-level, selecting rollouts that maximize advantage variance to yield higher policy gradients, and (2) token-level, prioritizing tokens with high product of advantage magnitude and policy entropy to focus updates on uncertain and impactful tokens. A dynamic down-sampling schedule inspired by curriculum learning is introduced to prevent overfitting and promote generalization. Experiments on Qwen2.5 and Llama3.1 models show that integrating D3S consistently improves performance and reduces sample/token requirements across diverse reasoning benchmarks.

## Method Summary
The Dynamic Dual-Level Down-Sampling (D3S) framework enhances critic-free RL methods like GRPO by selectively focusing computation on high-signal data. At the sample level, it selects rollouts maximizing advantage variance to increase policy gradient norms. At the token level, it prioritizes tokens with high advantage magnitude × entropy product to target uncertain yet impactful decisions. A dynamic schedule linearly interpolates from aggressive (N_init=8, K_init=5%) to mild (N_final=32, K_final=20%) down-sampling rates during training. The method is evaluated on mathematical reasoning tasks using Qwen2.5 and Llama3.1 models, showing improved performance and efficiency.

## Key Results
- D3S achieves average improvements of 4.5 in Pass@1 and 3.7 in Pass@8 on Qwen2.5-Math-7B compared to GRPO
- The framework uses fewer than 20% of tokens while maintaining or improving performance
- D3S demonstrates robustness across model scales and reasoning tasks, including AIME, AMC, MATH, and GSM8K benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Sample-Level Advantage Variance Maximization
The framework selects rollouts maximizing advantage variance to increase policy gradient norms. Standard GRPO normalizes advantages within groups (Var(A)=1), creating a fixed gradient norm upper bound. D³S selects subsets with Var(A')≥1, creating a variable upper bound that scales with advantage variance, accelerating convergence by focusing on high-signal samples.

### Mechanism 2: Token-Level Entropy-Advantage Weighted Selection
Tokens are ranked by the product of advantage magnitude (impact) and policy entropy (uncertainty). Only top K% contribute to gradients, concentrating learning on uncertain and impactful decision points. High-entropy tokens indicate model uncertainty where learning can be most productive, while high-advantage tokens indicate reward-critical positions.

### Mechanism 3: Dynamic Down-Sampling Schedule
A linear interpolation schedule progressively relaxes down-sampling from aggressive to mild configurations. Early aggressive sampling focuses on high-signal data for fast convergence, while later mild sampling expands the data pool for generalization. This prevents overfitting while maintaining early acceleration benefits.

## Foundational Learning

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: D³S is an add-on framework for critic-free methods like GRPO; understanding the base algorithm is essential.
  - Quick check question: Can you explain how GRPO estimates advantages without a value network?

- Concept: **Advantage normalization and its variance properties**
  - Why needed here: The theoretical contribution hinges on how standardization fixes variance at 1, and how subset selection can restore variance ≥1.
  - Quick check question: If you standardize rewards R to get advantages A with Var(A)=1, can you always find a subset with Var(A')≥1? (Answer: Yes, per Lemma 1)

- Concept: **Policy entropy as exploration signal**
  - Why needed here: Token-level selection uses entropy to identify uncertain tokens; understanding its role in RL is critical.
  - Quick check question: What does high entropy indicate about a token prediction, and why might it be valuable for learning?

## Architecture Onboarding

- Component map:
```
Input: Query x, rollouts {(y_i, R_i)} for i=1..G
  │
  ├─► [Advantage Computation] → A_i = normalize(R) within group
  │
  ├─► [Sample-Level Selection] → Select N_s samples maximizing Var(A)
  │         │                    (with optional cross-group pooling)
  │         ▼
  ├─► [Token-Level Selection] → For each selected sample, rank tokens by |A_i,t|×H_i,t
  │         │                    Keep top K%
  │         ▼
  ├─► [Dynamic Schedule] → Interpolate (N_s, K) based on training progress p
  │
  └─► [Policy Update] → Apply GRPO/GSPO objective on selected subset only
```

- Critical path:
  1. Implement sample-level variance maximization (greedy selection of top positive + bottom negative advantages)
  2. Add cross-group pooling for batch-level selection
  3. Implement token-level entropy-advantage ranking
  4. Add linear dynamic schedule for (N, K)
  5. Integrate with GRPO/GSPO loss function

- Design tradeoffs:
  - **Intra-group vs. cross-group selection**: Cross-group maximizes variance globally but can destabilize training for poorly aligned models (see Appendix D.1). Intra-group is more stable but may leave signal on the table.
  - **Aggressive vs. conservative initial K**: Lower K_init accelerates early convergence but risks missing useful tokens.
  - **Schedule slope**: Linear is simple; non-linear schedules might better match learning dynamics but add hyperparameters.

- Failure signatures:
  - **Gradient explosion early**: Check if N_s is too small relative to batch heterogeneity.
  - **KL divergence spikes**: Cross-group selection may be too aggressive for unaligned models; try intra-group variant (D³S-I).
  - **Entropy collapse**: Token-level selection may be too conservative; increase K_init.
  - **Late-stage performance degradation**: Dynamic schedule may be too aggressive; increase N_final, K_final.

- First 3 experiments:
  1. **Ablation on sample-level only (D¹S)**: Implement variance-based sample selection without token-level or dynamic schedule. Compare to GRPO baseline on a held-out reasoning benchmark (e.g., AIME24). Expect faster early convergence but potential late-stage overfitting.
  2. **Token-level contribution test**: Fix N_s=G, vary K ∈ {5%, 20%, 50%, 100%} and measure gradient norms, entropy dynamics, and Pass@1. Hypothesis: Low K yields higher per-token gradient impact but may miss useful tokens.
  3. **Schedule sensitivity**: Test linear vs. fixed down-sampling. Run (N_init=8, K_init=5%, N_final=32, K_final=20%) vs. static (N=16, K=10%). Measure both early convergence speed and final generalization gap (train vs. validation).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a mechanism be designed to automatically switch between intra-group and cross-group sampling based on real-time indicators of model alignment?
- Basis in paper: [explicit] Appendix D.1 observes that the cross-group D3S strategy induces unstable gradients for the unaligned Llama3.1-8B-Instruct, whereas the intra-group variant (D3S-I) maintains stability. The authors note that "the model's degree of alignment is a critical variable in determining the optimal sampling strategy."
- Why unresolved: The paper treats the choice between D3S and D3S-I as a hyperparameter selection based on the backbone model, rather than proposing a dynamic method to detect alignment states or gradient instability during training to toggle the strategy.
- What evidence would resolve it: A study introducing a dynamic threshold based on gradient variance or KL divergence rate that successfully triggers a switch from cross-group to intra-group sampling to prevent instability in unaligned models.

### Open Question 2
- Question: Is the linear interpolation schedule for down-sampling rates optimal for balancing convergence speed and generalization?
- Basis in paper: [inferred] Section 3.3 states, "we employ a linear schedule to interpolate between the initial aggressive configuration and the final milder configuration," suggesting this functional form was chosen as a heuristic rather than derived theoretically.
- Why unresolved: It remains unclear if a linear decay of the sampling size $N$ and token ratio $K$ is the most efficient path to convergence, or if non-linear (e.g., cosine) or performance-based adaptive schedules would yield better results.
- What evidence would resolve it: Comparative experiments evaluating the linear schedule against non-linear schedules (exponential decay) and adaptive schedules (adjusting based on validation Pass@1) in terms of final performance and convergence steps.

### Open Question 3
- Question: Does the D3S framework generalize to tasks with dense rewards or domains outside of mathematical reasoning?
- Basis in paper: [inferred] The experimental evaluation in Section 4 is strictly limited to mathematical reasoning benchmarks (AIME, AMC, MATH, etc.) where rewards are typically sparse and binary (correct/incorrect).
- Why unresolved: The token-level selection relies on the product of advantage magnitude and entropy. In tasks with dense rewards or different structural constraints (e.g., code generation), the distribution of "informative" tokens may differ, potentially affecting the method's efficacy.
- What evidence would resolve it: Experiments applying D3S to code generation benchmarks (e.g., MBPP, HumanEval) or open-ended instruction following tasks with dense, model-based reward signals.

## Limitations
- Theoretical analysis is limited to simplified single-token, deterministic-reward settings and may not fully capture real-world LLM training dynamics
- Cross-group selection can destabilize training for poorly aligned models due to distributional shift risks
- Dynamic schedule effectiveness depends heavily on correct timing and interpolation, with no guidance for non-linear schedules

## Confidence
- **High confidence**: The dual-level down-sampling concept is well-justified and empirical improvements (4.5 Pass@1, 3.7 Pass@8) are substantial and consistent across benchmarks
- **Medium confidence**: Theoretical bounds are valid for simplified settings but may not fully capture real-world dynamics where rewards are stochastic and tokenization is complex
- **Medium confidence**: Dynamic schedule's generalization benefits are demonstrated but the mechanism is heuristic and linear interpolation may not be optimal

## Next Checks
1. **Bias-variance tradeoff analysis**: Measure the effect of sample-level selection size N_s on both convergence speed and final performance. Systematically vary N_s ∈ {4, 8, 16, 32} while keeping token-level selection fixed to quantify the bias-variance tradeoff and identify optimal selection thresholds.

2. **Cross-group stability test**: Evaluate D3S with intra-group selection only (D3S-I variant) on a misaligned or poorly aligned model (e.g., base Llama3.1 without instruction tuning). Compare gradient stability, KL divergence, and final performance to assess the risk of distributional shift.

3. **Schedule ablation with curriculum learning**: Replace the linear dynamic schedule with a fixed selection (N=16, K=10%) and a two-phase curriculum (aggressive early, mild late). Measure both early convergence speed and final generalization gap to determine if the dynamic schedule provides benefits beyond simple curriculum learning.