---
ver: rpa2
title: Learning Contextually-Adaptive Rewards via Calibrated Features
arxiv_id: '2506.15012'
source_url: https://arxiv.org/abs/2506.15012
tags:
- feature
- learning
- features
- calibrated
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of learning robot rewards that
  adapt to changing contexts, such as avoiding a hot stove versus a cold one. Existing
  methods either treat contexts as separate tasks or implicitly learn contextual effects
  alongside task preferences, requiring substantial data to disentangle the two.
---

# Learning Contextually-Adaptive Rewards via Calibrated Features

## Quick Facts
- arXiv ID: 2506.15012
- Source URL: https://arxiv.org/abs/2506.15012
- Reference count: 40
- Primary result: Proposed method achieves 10x sample efficiency improvement in learning contextually-adaptive rewards through explicit modeling of feature saliency changes

## Executive Summary
This paper addresses the challenge of learning robot rewards that adapt to changing contexts, such as avoiding a hot stove versus a cold one. Traditional approaches either treat contexts as separate tasks or implicitly learn contextual effects alongside task preferences, requiring substantial data to disentangle the two. The authors propose explicitly modeling and learning how context affects feature saliency through "calibrated features" - modular representations that capture contextual effects on individual reward features.

The core method introduces contextual feature queries, where users compare states to indicate how context affects feature importance. This targeted feedback efficiently learns how each feature's saliency changes with context. The method composes these calibrated features with context-invariant preferences to create adaptive rewards, demonstrating 10x improvement in sample efficiency and up to 15% better performance in low-data regimes.

## Method Summary
The method learns contextually-adaptive rewards by explicitly separating the learning of contextual feature saliency from context-invariant preference weights. It uses calibrated features - neural networks that take state and base feature as input and output a context-modulated feature value. The learning process has two stages: first, contextual feature queries train the calibration networks to capture how context affects each feature's importance; second, preference queries train a linear reward head over the frozen calibrated features. This compositional approach achieves 10x sample efficiency improvement by isolating the contextual signal from preference learning.

## Key Results
- 10x improvement in sample efficiency compared to baselines
- Up to 15% better performance in low-data regimes (5-10 queries)
- User study with 12 participants validates effective teaching of contextual preferences
- Increasing query counts improves alignment between learned representations and user preferences

## Why This Works (Mechanism)

### Mechanism 1: Decoupling Saliency from Preference
The architecture splits reward learning into two stages, first learning contextual feature saliency through calibrated features, then learning context-invariant preferences. This prevents the model from needing to disentangle complex non-linear context effects from linear preference weights simultaneously, assuming high-level preferences are stable across contexts while feature relevance changes.

### Mechanism 2: Contextual Feature Queries for Isolated Signal
Instead of holistic preference queries, the system asks users to compare states specifically about feature saliency. This forces feedback to target the relationship between context and calibrated features, filtering out confounding factors from other features. This assumes users can access their mental model of "feature respect" independently of overall task performance.

### Mechanism 3: Compositional Modularity
Learned calibrated features are functions of only base features and state, not reward weights, making them independent modules. Once a contextual effect is learned (e.g., heat makes proximity dangerous), this module can be plugged into any reward function involving that feature. This assumes contextual effects are consistent across different high-level goals.

## Foundational Learning

- **Paired Comparison Models (Bradley-Terry)**: Needed to convert binary user choices into continuous probabilities for training neural networks via cross-entropy loss. Quick check: Can you explain why we need a probabilistic model rather than just a binary classification loss for preference learning?

- **Inverse Reinforcement Learning (IRL) vs. Feature Saliency**: Standard IRL learns a single reward function, while this paper learns a feature modifier. Quick check: In standard IRL, what does the weight vector θ represent, and how does this paper propose changing that representation?

- **Disentangled Representation Learning**: The core contribution separates contextual effects from task preferences. Quick check: What are the two factors of variation this paper tries to separate, and how does the query design force this separation?

## Architecture Onboarding

- **Component map**: Raw State s + Base Features φ(s) -> Calibration Networks ψ_i -> Calibrated Features Φ^c -> Linear Reward Head θ -> Reward R

- **Critical path**: 1) Identify contextually affected features via user indicator c_i 2) Stage 1: Collect Contextual Feature Queries, train Calibration Networks ψ_i using specialized loss, freeze weights 3) Stage 2: Collect Preference Queries, train Reward Head θ using standard preference loss over frozen calibrated features

- **Design tradeoffs**: Random sampling vs. active sampling (random misses critical state regions); freezing vs. fine-tuning (freezing critical for sample efficiency, fine-tuning degrades performance)

- **Failure signatures**: Uniform calibration (constant output indicates failed coverage of active context region); no convergence (check for contradictory equivalence labels)

- **First 3 experiments**: 1) Sanity Check: Train calibrated feature for "stove distance" using simulated optimal queries and visualize heatmap 2) Baseline Comparison: Reproduce Exp. 1 with M_REW=5, compare against SinglePref baseline 3) User Alignment: Run query interface with test subject, check alignment with their written description

## Open Questions the Paper Calls Out

1. Can active learning strategies improve the coverage and efficiency of calibrated feature learning compared to random sampling? The authors suggest active learning or dense data collection as future work since random sampling may miss important state regions.

2. Does the modularity of calibrated features facilitate better interpretability and iterative refinement for end-users? While the discussion notes potential for interpretability, it requires assessment for supporting iterative refinement and appropriate trust calibration.

3. Can learned contextual representations generalize across object types to avoid retraining for every new base feature? The authors identify this limitation and propose learning representations that generalize across object types, such as all "technology" or all "fragile" objects.

## Limitations
- Assumption of context-invariant preferences unverified - the paper assumes preferences remain stable while only feature saliency changes, but this may not hold for all contexts
- Synthetic human simulations vs. real human data - 10x efficiency gain demonstrated primarily through simulated humans, leaving deployment gap unclear
- Weak support for modular transfer claims - compositional performance shown in single experiment but not demonstrated for genuinely new task distributions

## Confidence

- **High Confidence**: Compositional architecture design clearly specified and reproducible; 10-15% performance advantage in low-data regimes consistently demonstrated
- **Medium Confidence**: User study validates engagement with contextual feature queries but small sample size and lack of alternative query design comparison limits generalizability
- **Low Confidence**: Claims about modular transfer across tasks weakly supported - only shows compositional performance in single experiment

## Next Checks

1. Test calibrated features' transfer capability by learning contextual effects in one task and evaluating performance when composing with completely new reward structures

2. Conduct between-subjects user study comparing contextual feature queries against standard preference queries, measuring both final reward accuracy and user effort/understanding

3. Evaluate failure modes by introducing contexts that reverse preference trade-offs to test limits of context-invariant weight assumption