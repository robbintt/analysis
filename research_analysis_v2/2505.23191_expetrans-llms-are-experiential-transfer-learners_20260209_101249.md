---
ver: rpa2
title: 'ExpeTrans: LLMs Are Experiential Transfer Learners'
arxiv_id: '2505.23191'
source_url: https://arxiv.org/abs/2505.23191
tags:
- task
- experience
- option
- source
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ExpeTrans is a framework that enables large language models (LLMs)
  to autonomously transfer task-solving experiences from existing source tasks to
  newly encountered target tasks. The framework addresses the challenge of requiring
  substantial human labor or time to gather task experiences for each new query by
  mimicking human cognitive intelligence for experience transfer.
---

# ExpeTrans: LLMs Are Experiential Transfer Learners

## Quick Facts
- arXiv ID: 2505.23191
- Source URL: https://arxiv.org/abs/2505.23191
- Authors: Jinglong Gao; Xiao Ding; Lingxiao Zou; Bibo Cai; Bing Qin; Ting Liu
- Reference count: 40
- Primary result: ExpeTrans achieves 63.8% average accuracy on 13 datasets by transferring task-solving experiences, outperforming Zero-shot-CoT (54.9%).

## Executive Summary
ExpeTrans is a framework that enables large language models (LLMs) to autonomously transfer task-solving experiences from existing source tasks to newly encountered target tasks. It addresses the challenge of requiring substantial human labor or time to gather task experiences for each new query by mimicking human cognitive intelligence for experience transfer. The framework uses task-wise experience memory to store functions, processes, and verified experiences from labeled datasets, then selects suitable source tasks based on function and process similarities to transfer their experiences to the target task. Experiments on 13 datasets demonstrate that ExpeTrans effectively improves LLM performance, achieving up to 63.8% average accuracy compared to baseline methods like Zero-shot-CoT at 54.9%. The framework dynamically skips unnecessary transfers for simple queries and maintains effectiveness even with limited task similarity, reducing human labor costs while offering a novel path for LLM generalization.

## Method Summary
ExpeTrans is a two-phase framework. Phase 1 (experience accumulation) processes labeled datasets to extract task functions and processes, identifies hard examples where the model initially fails, summarizes verified experiences, and stores them in task-wise memory. Phase 2 (transfer) handles new queries by extracting their task functions and processes, retrieving and selecting relevant source tasks using BM25 ranking on both function and process similarity, optionally transferring experiences, and reasoning with the selected insights. The framework uses a harmonic mean of BM25 rankings to favor tasks matching on both function and process, and includes dynamic skip mechanisms to avoid unnecessary or potentially harmful transfers.

## Key Results
- ExpeTrans achieves 63.8% average accuracy on 13 datasets, outperforming Zero-shot-CoT (54.9%) and other baselines.
- The dual similarity approach (function + process) improves accuracy by 3.2% compared to using either dimension alone.
- Experience transfer effectiveness remains significant even with limited task similarity, though performance drops to near-baseline levels under harsh conditions.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Selecting source tasks based on both function and process similarity improves experience transfer effectiveness compared to using either dimension alone.
- **Mechanism**: The framework extracts task function (goal) and task process (steps) for both source and target tasks, ranks candidates using BM25 on each dimension, computes the harmonic mean of rankings to favor tasks matching on *both* aspects, then prompts an LLM to make the final selection from top-K candidates. This dual-filter reduces irrelevant transfers.
- **Core assumption**: LLMs can reliably extract and compare task functions and processes; the Structure-Mapping Theory analogy holds for textual experience transfer.
- **Evidence anchors**:
  - [section 2.3.1]: "we also take these two aspects into account... all candidate tasks in memory are ranked according to their BM25-score with respect to the target task in terms of task function and task process, respectively; the harmonic mean of these two rankings is calculated"
  - [table 5]: Removing either function or process similarity (-w/o Func, -w/o Proc) reduces average accuracy from 63.8% to 60.6% and 61.2% respectively
- **Break condition**: When tasks are highly dissimilar or when extracted functions/processes are noisy/incorrect, BM25 rankings become unreliable, leading to poor source selection.

### Mechanism 2
- **Claim**: Experience summarized from "hard examples" (where the model initially fails) and verified through multiple attempts yields higher-quality transferable insights.
- **Mechanism**: During accumulation, each example is tested with Zero-shot-CoT; only incorrect responses trigger experience summarization. The summarized experience is then verified by attempting to solve the same example with the experience, retaining only experiences that pass all verification runs (3× in experiments).
- **Core assumption**: Experiences that help solve a specific hard example generalize to structurally similar tasks; multiple verification passes reduce randomness.
- **Evidence anchors**:
  - [section 2.2.2]: "according to previous studies... the experience summarized from hard examples is much more effective... we use Prompt 4 to verify their effectiveness, retaining only those that enable ChatGPT to correctly respond"
  - [figure 4]: Performance drops when verification count decreases from 3 to 1 (from ~63.5% to ~61%) and also drops slightly when increasing from 3 to 5
- **Break condition**: If verification passes are too few, low-quality experiences leak through; if too many, valid but task-specific experiences may be over-filtered, reducing coverage.

### Mechanism 3
- **Claim**: Dynamically skipping transfer for simple queries or when no suitable source exists prevents performance degradation from inappropriate transfers.
- **Mechanism**: Two skip triggers: (1) if the LLM selector returns an empty list, or (2) if the top-ranked source task is a non-hard example (which has empty stored experience), the framework falls back to Zero-shot-CoT without transfer. This filters out transfers that would add noise.
- **Core assumption**: The presence of non-hard examples in memory signals query simplicity; an empty selection output reliably indicates no suitable transfer exists.
- **Evidence anchors**:
  - [section 2.3.2]: "Not all queries require transfer, and inappropriate transfer may not only waste time but also potentially reduce the performance of LLMs"
  - [table 6]: Under harsh transfer conditions (Minimal setting), -w/o NoSrc (forced selection) drops accuracy from 55.5% to 54.5%; -w/o NonHard (forced hard-only) drops to 53.9%
- **Break condition**: If the skip logic is too aggressive (e.g., incorrectly marking complex queries as simple), beneficial transfers are missed, limiting potential gains.

## Foundational Learning

- **Concept: In-Context Learning (ICL) and Chain-of-Thought (CoT) Prompting**
  - **Why needed here**: ExpeTrans extends CoT by injecting textual experiences into prompts. Without understanding how demonstrations or reasoning steps influence LLM outputs, the mechanism of "experience-guided reasoning" is opaque.
  - **Quick check question**: Can you explain why adding "Let's think step by step" to a prompt often improves LLM accuracy on reasoning tasks?

- **Concept: Transfer Learning in NLP (Task Similarity and Adaptation)**
  - **Why needed here**: The core hypothesis is that experiences transfer across tasks with similar functions/processes. Understanding traditional transfer learning (e.g., fine-tuning, domain adaptation) provides context for how this *prompt-based* transfer differs.
  - **Quick check question**: What is the key difference between parameter-based transfer (e.g., fine-tuning) and this paper's experience-based transfer?

- **Concept: Sparse Retrieval (BM25) and Ranking Metrics**
  - **Why needed here**: Source task selection relies on BM25 ranking over textual task descriptions. Understanding TF-IDF and harmonic mean of rankings is necessary to replicate or modify the retrieval component.
  - **Quick check question**: Why might BM25 be chosen over dense embeddings for matching task functions/processes in this framework?

## Architecture Onboarding

- **Component map**:
  1. **Memory Store** (Task Function, Process, Experience per example)
  2. **Experience Accumulator** (parallel processing of labeled data: extract → test → reflect → verify → store)
  3. **Query Processor**:
     - Task Identifier (extract function/process)
     - Retriever (BM25 ranking on function & process)
     - Selector (LLM prompt to choose from top-K or skip)
     - Transfer Adapter (LLM prompt to adapt source experiences to target)
     - Reasoning Engine (LLM prompt with selected experiences)

- **Critical path**: Start with a small, diverse set of labeled datasets. Run the Experience Accumulator (offline). For online queries, ensure the Task Identifier and Retriever produce consistent task descriptions; tune K (candidate count) and M (insight count) based on latency/accuracy tradeoffs. The Selector and Adapter prompts are high-leverage—if they fail, the entire transfer chain produces noise.

- **Design tradeoffs**:
  - **Granularity**: One example per source task (Ours) vs. grouped tasks (-w/ Seg) vs. dataset-level (-w/ Overall). Finer granularity improves transfer precision but increases memory size and retrieval cost.
  - **Verification runs**: 3× verification balances quality and yield. Fewer runs increase noise; more reduce available experiences.
  - **K and M parameters**: K=10 candidates, M=5 insights trade off retrieval/compute cost against transfer coverage. Increasing K yields diminishing returns; increasing M beyond 5 shows no clear gain in experiments.

- **Failure signatures**:
  - **Transfer degradation**: When source tasks are from distant domains (Minimal setting), accuracy drops to near-baseline. Monitor average harmonic mean scores; if below a threshold, auto-skip.
  - **Experience noise**: If transferred experiences are too generic (e.g., "read carefully"), they can suppress detailed reasoning. Check for high-frequency, low-specificity insights in memory.
  - **Prompt mismatch across LLMs**: Performance varies significantly across base models (e.g., DeepSeek-V3, Gemma). Expect to adapt prompt phrasing and parsing for different LLM families.

- **First 3 experiments**:
  1. **Ablate dual similarity**: Run with only function similarity, only process similarity, and both. Compare accuracy and analyze failure cases where one dimension outperforms the other.
  2. **Vary verification intensity**: Test with 1, 3, and 5 verification passes. Measure not just accuracy but also the *yield* (number of experiences retained per hard example) to diagnose over-filtering.
  3. **Cross-category stress test**: Restrict source tasks to categories different from the target (e.g., target is Sentiment Analysis, sources only from Textual Reasoning). Quantify performance drop and identify which task pairs transfer best/worst to inform memory curation strategies.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the framework be adapted to provide significant performance gains for larger, more capable models (e.g., GPT-4) where the marginal utility of transferred experience currently diminishes?
- Basis in paper: [explicit] The authors state in the Limitations and Appendix E that their improvement on GPT-4 is less significant because such models have likely already "mastered" the experiences, reducing the framework's value for larger LLMs.
- Why unresolved: The current verification mechanism filters for experiences that aid "hard" examples, which become rarer as the base model's capability increases.
- What evidence would resolve it: Demonstrating significant performance improvements on frontier models when applying experiences derived from novel, highly complex tasks not found in standard pre-training corpora.

### Open Question 2
- Question: What dynamic control mechanisms can optimize the volume of data used during experience accumulation to minimize economic costs without sacrificing model performance?
- Basis in paper: [explicit] The Limitations section acknowledges that utilizing all available data "may not be economically optimal" and suggests that dynamic control of data usage is needed to balance costs and performance.
- Why unresolved: The current implementation processes all available source examples in parallel, lacking a mechanism to halt accumulation once a performance saturation point is reached.
- What evidence would resolve it: A study identifying the "break-even" point where the cost of processing additional source examples no longer yields statistically significant accuracy improvements.

### Open Question 3
- Question: How can the framework ensure robustness to prompt formatting differences across various LLM families without requiring manual prompt tuning?
- Basis in paper: [inferred] Appendix E notes that prompt effectiveness varies significantly across different base LLMs (e.g., DeepSeek, Llama) and the framework "may require adjusting the prompts" to align with specific output parsing formats.
- Why unresolved: The framework relies on structured outputs (e.g., JSON) for task selection and transfer, which smaller or differently instructed models often fail to generate consistently.
- What evidence would resolve it: Successful cross-family transfer results using a prompt-agnostic interface or an automated alignment module that standardizes outputs across different model architectures.

## Limitations

- The framework's effectiveness diminishes on larger, more capable models (e.g., GPT-4) where experiences are likely already "mastered."
- Current experiments are limited to 13 NLP datasets with 500 examples each, raising scalability concerns for larger datasets and non-NLP tasks.
- The memory and retrieval efficiency of BM25 over textual task descriptions have not been characterized for large-scale deployment.

## Confidence

- **High**: The core mechanism of task-wise experience storage and retrieval works as described, with clear empirical support from ablation studies (Table 5, 6).
- **Medium**: The claim that experiences from hard examples generalize effectively, though supported by verification results, may be overfit to the specific verification process used.
- **Low**: The assertion that the framework reduces human labor costs significantly, as this depends heavily on the scale of tasks and the cost of running the initial experience accumulation phase.

## Next Checks

1. Test the framework's performance on datasets 10x larger than the current 500-example limit to assess scalability and memory retrieval efficiency.
2. Evaluate cross-domain transfer by using source tasks from completely unrelated categories (e.g., code generation as sources for sentiment analysis) to quantify the true limits of task similarity requirements.
3. Measure the time and computational cost of the experience accumulation phase versus the accuracy gains to determine the labor cost tradeoff across different task volumes.