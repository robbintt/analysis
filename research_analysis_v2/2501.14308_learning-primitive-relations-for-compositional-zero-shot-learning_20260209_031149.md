---
ver: rpa2
title: Learning Primitive Relations for Compositional Zero-Shot Learning
arxiv_id: '2501.14308'
source_url: https://arxiv.org/abs/2501.14308
tags:
- learning
- compositions
- compositional
- states
- unseen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses compositional zero-shot learning (CZSL), where
  the goal is to recognize unseen state-object compositions using knowledge from seen
  compositions. The key limitation of existing methods is that they independently
  predict states and objects, ignoring their relationships.
---

# Learning Primitive Relations for Compositional Zero-Shot Learning

## Quick Facts
- **arXiv ID**: 2501.14308
- **Source URL**: https://arxiv.org/abs/2501.14308
- **Authors**: Insu Lee; Jiseob Kim; Kyuhong Shim; Byonghyo Shim
- **Reference count**: 25
- **Primary result**: Proposes Learning Primitive Relations (LPR) framework that outperforms state-of-the-art methods on all three CZSL benchmark datasets in both closed-world and open-world settings.

## Executive Summary
This paper addresses compositional zero-shot learning (CZSL), where the goal is to recognize unseen state-object compositions using knowledge from seen compositions. The key limitation of existing methods is that they independently predict states and objects, ignoring their relationships. The proposed Learning Primitive Relations (LPR) framework captures probabilistic relationships between states and objects using cross-attention mechanisms in two novel branches: state-object relation (sor) and object-state relation (osr). LPR outperforms state-of-the-art methods on all three CZSL benchmark datasets (MIT-States, UT-Zappos, and C-GQA) in both closed-world and open-world settings.

## Method Summary
LPR builds on a frozen CLIP ViT-L/14 backbone and introduces lightweight adapters for three branches: compositional (com), state-object relation (sor), and object-state relation (osr). The sor branch uses cross-attention to extract state information first, then uses the state-informed feature to extract object information. The osr branch reverses this order. During inference, predictions from all three branches are combined with scaling hyperparameters α and β. The model is trained with intermediate primitive supervision and combined loss functions, using specified hyperparameters λ1=2.0, λ2=1.5 (MIT-States, C-GQA) or λ1=3.0, λ2=1.0 (UT-Zappos).

## Key Results
- On C-GQA dataset, LPR achieves 32.9% harmonic mean accuracy in closed-world and 12.9% in open-world settings
- LPR outperforms previous best results by 3.5 percentage points (closed-world) and 2.0 percentage points (open-world) on C-GQA
- Ablation studies show that adding relation branches (sor/osr) improves performance over compositional branch alone
- LPR demonstrates consistent improvements across all three benchmark datasets (MIT-States, UT-Zappos, C-GQA)

## Why This Works (Mechanism)

### Mechanism 1: Conditional Feature Refinement via Cross-Attention
The framework uses cross-attention where image features act as queries and text features of one primitive (e.g., states) act as keys/values. This generates a "state-informed" image feature that is then used to query the second primitive (objects), effectively filtering object predictions based on the visual evidence of the state.

### Mechanism 2: Bidirectional Ensemble Bias Correction
The model aggregates three scores: a direct compositional score and two decomposed scores from sor and osr branches. This ensemble mitigates the bias towards seen classes inherent in standard classifiers by enforcing primitive-level consistency that generalizes to unseen combinations.

### Mechanism 3: Intermediate Primitive Supervision
The loss function includes terms that supervise intermediate cross-attention outputs with primitive labels. This forces intermediate feature vectors to be predictive of their respective primitives, ensuring the attention mechanism extracts meaningful semantic content.

## Foundational Learning

- **Concept: Compositional Zero-Shot Learning (CZSL)**
  - Why needed: Core problem definition requiring recognition of state-object pairs never seen together during training
  - Quick check: If a model sees "red apple" and "green pear" during training, should it classify "green apple" as seen or unseen class?

- **Concept: CLIP (Contrastive Language-Image Pre-training)**
  - Why needed: LPR relies on frozen CLIP backbone for feature extraction and cosine similarity classification
  - Quick check: Why does LPR use cosine similarity between image feature and text features to predict the class?

- **Concept: Cross-Attention**
  - Why needed: "Relation" in LPR is implemented via cross-attention Query/Key/Value paradigm
  - Quick check: In sor branch, which modality provides Query and which provides Key/Value for first attention block?

## Architecture Onboarding

- **Component map**: Input Image + Text Prompts → CLIP Backbone → Adapters → (com, sor, osr) branches → Weighted Ensemble

- **Critical path**: Input: Image I and Text Prompts → Extraction: CLIP encodes I→xi and Text→(Ts, To, Tc) → Branching: xi passes through 3 Adapters → (xc, xs, xo) → Relation Logic: sor uses xs queries Ts→xs, then xs queries To→xso; osr reverses → Fusion: Weighted sum using α, β

- **Design tradeoffs**: Hyperparameter α controls trade-off between seen and unseen accuracy; higher α biases toward com branch (better for seen classes), lower α favors relation branches (better for unseen)

- **Failure signatures**: Object-driven shortcuts may occur if object classifier is too strong; infeasible compositions may be predicted in open-world settings if relation branches fail to penalize low-probability primitive pairings

- **First 3 experiments**: 1) Path ablation: run inference using only com, then com+sor, then all three to verify harmonic mean lift; 2) Hyperparameter sensitivity: sweep α from 0.2 to 0.8 to confirm optimal sweet spot; 3) Qualitative check: visualize prediction rows to see if com predicts seen classes and sor/osr corrects to correct unseen class

## Open Questions the Paper Calls Out

### Open Question 1
How does the computational cost of LPR's dual cross-attention branches scale with the number of primitives, particularly for large-scale datasets beyond C-GQA's ~1,000 primitives? The paper evaluates on datasets with up to 413 states and 674 objects but does not report inference time, memory usage, or scalability analysis.

### Open Question 2
To what extent does LPR's performance depend on the quality and biases of the pre-trained CLIP embeddings versus the proposed relation learning mechanism? The contribution of CLIP's inherent compositional understanding versus learned relations is not disentangled.

### Open Question 3
Can the learned probabilistic state-object relations transfer to new domains or novel primitive types not present during training? The paper does not test whether learned relations generalize when entirely new states or objects are introduced.

## Limitations
- Architecture relies on specific hyperparameter settings (λ1, λ2, α) that may not generalize across datasets with different compositional distributions
- Bidirectional cross-attention mechanism may introduce computational overhead without proportional accuracy gains on smaller datasets
- Open-world evaluation effectiveness depends heavily on the chosen set of "feasible" compositions, which is not explicitly defined for all datasets

## Confidence
- **High Confidence**: Core architectural contribution and empirical improvements on benchmark datasets are well-supported
- **Medium Confidence**: Ablation studies show importance of each component, though exact contribution of intermediate supervision versus bidirectional ensemble effect is difficult to disentangle
- **Medium Confidence**: Claim of outperforming state-of-the-art is supported, but magnitude of improvement varies significantly across datasets

## Next Checks
1. **Hyperparameter Sensitivity Analysis**: Systematically vary α from 0.1 to 0.9 on validation sets to confirm reported sweet spot and determine if optimal value generalizes across datasets
2. **Computational Complexity Evaluation**: Measure inference time and memory usage of LPR versus baseline methods to quantify practical cost of bidirectional cross-attention mechanism
3. **Qualitative Failure Analysis**: Collect and analyze cases where LPR fails on unseen compositions, distinguishing between failures due to poor primitive relation learning versus failures inherited from CLIP backbone limitations