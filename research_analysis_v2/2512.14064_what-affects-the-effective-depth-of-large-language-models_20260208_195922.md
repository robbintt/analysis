---
ver: rpa2
title: What Affects the Effective Depth of Large Language Models?
arxiv_id: '2512.14064'
source_url: https://arxiv.org/abs/2512.14064
tags:
- qwen2
- layer
- gsm8k
- b-instruct
- hellaswag
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates how the effective depth of large language\
  \ models varies with model scale, training type, and task difficulty. Using methods\
  \ like residual cosine similarity, logit lens, and layer-skipping interventions,\
  \ the authors analyze Qwen-2.5 models (1.5B\u201332B parameters) and their long-CoT\
  \ counterparts across datasets of varying difficulty."
---

# What Affects the Effective Depth of Large Language Models?

## Quick Facts
- arXiv ID: 2512.14064
- Source URL: https://arxiv.org/abs/2512.14064
- Authors: Yi Hu; Cai Zhou; Muhan Zhang
- Reference count: 40
- Primary result: Current LLMs underutilize their depth across scales, training paradigms, and task difficulties

## Executive Summary
This study investigates how the effective depth of large language models varies with model scale, training type, and task difficulty. Using methods like residual cosine similarity, logit lens, and layer-skipping interventions, the authors analyze Qwen-2.5 models (1.5B–32B parameters) and their long-CoT counterparts across datasets of varying difficulty. The key findings are: (1) While the number of effective layers increases with model size, the effective depth ratio remains stable, indicating inefficient depth utilization; (2) Long-CoT models show no increase in effective depth despite improved reasoning performance, suggesting gains come from better context handling rather than deeper computation; (3) Models do not dynamically allocate more depth for harder tasks, with effective depth remaining consistent across difficulty levels. These results demonstrate that current LLMs underuse their available depth across scales, training paradigms, and task difficulties, highlighting opportunities for improving layer utilization, model pruning, and early exiting strategies.

## Method Summary
The paper analyzes effective depth utilization in LLMs using five probing methods: (1) residual cosine similarity to detect phase transitions from feature composition to refinement; (2) logit lens measuring KL divergence and top-5 token overlap; (3) layer-skipping interventions to assess causal influence; (4) residual erasure; and (5) integrated gradients. The analysis covers Qwen-2.5 models (1.5B-32B parameters) and DeepSeek-R1-Distill variants across three tasks of varying difficulty. Effective depth ratio is computed as (ED+1)/L, where ED is the identified effective depth and L is total layers.

## Key Results
- Effective depth increases with model size but effective depth ratio remains stable (~0.7-0.8), indicating consistent underutilization
- Long-CoT models show no increase in effective depth despite improved reasoning performance
- Models do not dynamically allocate more depth for harder tasks, maintaining consistent effective depth across difficulty levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformer layers exhibit a consistent phase transition from feature composition to refinement, with effective depth determined by a measurable shift in residual-residual cosine similarity.
- Mechanism: Early layers write orthogonal features into the residual stream (cosine similarity near zero), middle layers engage in feature erasure (negative similarity), and later layers perform minor refinements (positive similarity indicating amplification of existing features). The transition point—where similarity crosses from negative to positive—defines effective depth.
- Core assumption: The cosine similarity between layer contributions and residual state meaningfully captures whether a layer is composing novel features versus refining existing ones.
- Evidence anchors:
  - [abstract] "layers in the second half are simply refining existing representations rather than contributing to novel feature composition"
  - [Page 4] "an initial positive phase, followed by a decline into negative values, and a final return to positive"
  - [corpus] Csordás et al. [6] introduced this methodology; this paper replicates and extends it across scales
- Break condition: If later layers in some model architectures showed negative cosine similarity while measurably improving task performance, the metric would not capture useful computation.

### Mechanism 2
- Claim: Long-CoT reasoning gains stem from extended sequential computation (more tokens) rather than deeper per-token computation.
- Mechanism: Chain-of-thought training optimizes the model's ability to distribute reasoning across context positions, but does not alter how layers process each individual token. The effective depth ratio remains stable between base models and their DeepSeek-R1-Distill counterparts despite substantial reasoning improvements.
- Core assumption: Improved reasoning in long-CoT models should manifest as increased per-token computational depth if deeper processing were the mechanism.
- Evidence anchors:
  - [abstract] "improved reasoning stems from longer context rather than deeper per-token computation"
  - [Page 4] "we find no significant difference in effective depth ratio between long-CoT and base models"
  - [corpus] No corpus papers directly test this specific claim; it is a novel contribution
- Break condition: If future work found that long-CoT models allocate depth differently per token position (e.g., deeper at reasoning-critical tokens), the claim would need refinement.

### Mechanism 3
- Claim: Current LLMs do not dynamically allocate more layers for harder tasks; effective depth is task-agnostic.
- Mechanism: Regardless of whether the task is simple language understanding (HellaSwag), grade-school math (GSM8K), or competition math (AIME24), the phase transition point and effective depth ratio remain consistent. This suggests depth allocation is baked into the pretrained architecture rather than adaptively deployed.
- Core assumption: Harder tasks should require—and models should be capable of—activating deeper computation if such capacity existed and were useful.
- Evidence anchors:
  - [abstract] "Models do not dynamically allocate more depth for harder tasks"
  - [Page 4] "effective depth remains largely consistent across all tasks"
  - [corpus] Related work "The Curse of Depth in LLMs" confirms layer underutilization but does not test task difficulty as a factor
- Break condition: If adaptive depth emerged with different training objectives (e.g., reinforcement learning on difficulty-stratified data), the finding would be contingent on current training paradigms.

## Foundational Learning

- Concept: Residual stream architecture
  - Why needed here: All five probing methods (cosine similarity, logit lens, layer skipping, residual erasure, integrated gradients) assume understanding of how Transformer layers read from and write to a persistent residual state.
  - Quick check question: Can you explain why `h_{l+1} = h_l + a_l + m_l` implies that cosine similarity near zero indicates orthogonal feature insertion?

- Concept: Logit lens interpretation
  - Why needed here: The method decodes intermediate hidden states directly to vocabulary distributions, assuming the unembedding matrix can be applied at any layer.
  - Quick check question: If KL divergence from final output drops sharply at layer 20 in a 28-layer model, what does this suggest about layers 21-28?

- Concept: Chain-of-thought as test-time compute
  - Why needed here: The paper's central counterintuitive finding is that long-CoT models do not use "deeper" computation per token, requiring clear distinction between sequential (across tokens) and vertical (across layers) compute.
  - Quick check question: If a model solves problems by generating 50 reasoning tokens instead of 5, does this paper's evidence suggest it is using more layers per token?

## Architecture Onboarding

- Component map:
  Input: Token embeddings → `h_0` → Layer block (repeated L times) → Output: RMSNorm → Unembedding → Softmax
  Layer block: RMSNorm → Self-Attention → Add to residual → RMSNorm → MLP → Add to residual

- Critical path:
  1. Forward pass records residual states at all layers
  2. Cosine similarity computed between `(a_l + m_l)` and `h_l`
  3. Phase transition identified where averaged similarity crosses zero
  4. Logit lens confirms: decode `h_l @ W_out`, track KL divergence and top-5 overlap
  5. Intervention validation: skip layer `s`, measure downstream effect on layers `l > s` and final output

- Design tradeoffs:
  - Cosine similarity metric: Intuitive but threshold-dependent; requires smoothing across layers
  - Logit lens KL threshold (half-max) vs. overlap threshold (0.3): Different sensitivity; Table 1 shows ratios differ by up to 0.3 between methods
  - Layer skipping vs. residual erasure: Skipping tests causal influence; erasure tests token-specific information persistence

- Failure signatures:
  - Effective depth ratio varies erratically across model scales → probing methods may be capturing noise
  - Long-CoT models show different effective depth but same performance → suggests metric not capturing relevant computation
  - Harder tasks show deeper effective depth → would contradict paper's claim of static allocation

- First 3 experiments:
  1. Replicate Figure 1 for a single model: plot cosine similarity of attention, MLP, and combined contributions across all layers on GSM8K; identify the phase transition point
  2. Implement logit lens: extract hidden states at each layer, apply unembedding, compute KL divergence and top-5 token overlap vs. final output; compare transition layer to cosine similarity result
  3. Layer-skipping intervention: for a 28-layer model, skip each layer in turn and measure (a) effect on all later layer contributions, (b) change in final output distribution; confirm later layers have smaller effects

## Open Questions the Paper Calls Out

- How can more robust and well-validated quantitative measures of effective depth be developed to overcome the instability found in current logit lens and residual cosine similarity metrics? The authors call for developing more robust and well-validated measures as their metrics remain relatively straightforward and exhibit some instability.

- What specific architectural modifications or training objectives can successfully force LLMs to utilize deeper layers for novel feature composition rather than mere refinement? The study highlights the need for future work to explore architectural or training approaches that enable models to leverage their full depth more effectively.

- Can the identification of consistent "ineffective" layers in the second half of the network be leveraged to create lossless model pruning or early-exiting strategies that significantly reduce inference costs? The authors identify model pruning and early exiting as specific research opportunities pointed out by the results regarding depth under-utilization.

## Limitations

- The study focuses on decoder-only Transformers with RMSNorm and may not generalize to encoder-decoder architectures or models using alternative normalizations.
- The analysis is restricted to English-centric tasks and may not reflect depth utilization patterns in multilingual or code-generation contexts.
- The paper does not investigate whether effective depth varies with inference-time techniques such as temperature scaling, top-k sampling, or beam search.

## Confidence

- High confidence: The finding that effective depth increases with model size while the effective depth ratio remains stable is well-supported by the data across multiple probing methods and model scales.
- Medium confidence: The claim that long-CoT models show no increase in effective depth despite improved reasoning performance is compelling but relies on a single architectural family.
- Medium confidence: The assertion that models do not dynamically allocate more depth for harder tasks is based on three datasets of varying difficulty.

## Next Checks

1. Replicate the effective depth analysis on models with different architectures (e.g., LLaMA, Mistral, or models using LayerNorm) to test whether the stable effective depth ratio finding generalizes beyond Qwen-2.5.

2. Design a dataset with continuous difficulty scaling (e.g., math problems with incremental complexity) to test whether effective depth shows any task-dependent variation that the current three-dataset analysis might miss.

3. Evaluate effective depth under different sampling strategies (temperature 0 vs. 1.0, top-k vs. nucleus sampling) to determine whether depth utilization is fixed or adapts to generation conditions.