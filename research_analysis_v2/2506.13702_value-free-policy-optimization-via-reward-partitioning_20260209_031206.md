---
ver: rpa2
title: Value-Free Policy Optimization via Reward Partitioning
arxiv_id: '2506.13702'
source_url: https://arxiv.org/abs/2506.13702
tags:
- policy
- learning
- optimization
- reward
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RPO replaces value function learning in direct reward optimization
  with empirical reward normalization. It estimates the partition function from data,
  enabling direct supervised training on the policy without auxiliary models or joint
  optimization.
---

# Value-Free Policy Optimization via Reward Partitioning

## Quick Facts
- **arXiv ID:** 2506.13702
- **Source URL:** https://arxiv.org/abs/2506.13702
- **Authors:** Bilal Faye; Hanane Azzag; Mustapha Lebbah
- **Reference count:** 36
- **Primary result:** RPO replaces value function learning in direct reward optimization with empirical reward normalization, outperforming baselines on instruction-tuning tasks

## Executive Summary
RPO eliminates the need to learn neural value functions in direct reward optimization by computing the partition function directly from data samples. It estimates the partition function per prompt using reference policy probabilities and observed rewards, enabling direct supervised training on the policy without auxiliary models or joint optimization. Experiments on Flan-T5 models show RPO outperforms baselines like DRO and KTO on instruction-tuning tasks with LLM-based preference evaluation showing win rates over 83%.

## Method Summary
RPO optimizes a policy Ï€_Î¸ to maximize expected reward while regularizing against a frozen reference policy Ï€_ref. Instead of learning a value function, RPO estimates the partition function ZÌ‚(x) = Î£_{jâˆˆI_x} Ï€_ref(y_j|x) exp(r(x,y_j)/Ï„) for each prompt x from available samples. The value is then VÌ‚(x) = Ï„ log ZÌ‚(x). The RPO loss directly regresses the log-policy ratio toward the normalized reward signal using squared error: L_RPO = Â½ð”¼[(log Ï€_Î¸(y|x)/Ï€_ref(y|x) - (r - VÌ‚(x))/Ï„)Â²]. This transforms policy optimization into a straightforward supervised learning problem without auxiliary models.

## Key Results
- RPO outperforms SFT, KTO, and DRO on instruction-tuning tasks with LLM-based preference evaluation showing win rates over 83%
- RPO demonstrates faster training convergence across all model sizes (77M to 2.85B parameters)
- RPO shows greater stability across temperature settings compared to baselines
- The method eliminates the need for value function learning while maintaining or improving performance

## Why This Works (Mechanism)

### Mechanism 1: Empirical Partition Function Substitutes for Learned Value Function
RPO eliminates the need to learn a neural value function by computing the partition function directly from available data samples sharing the same prompt. Instead of approximating V*(x) through a separately trained network (as DRO does), RPO aggregates all responses for a given prompt x and computes ZÌ‚(x) = Î£_{jâˆˆI_x} Ï€_ref(y_j|x) exp(r(x,y_j)/Ï„), then sets VÌ‚(x) = Ï„ log ZÌ‚(x). This transforms an unstable learning problem into a deterministic computation at each training step.

### Mechanism 2: Squared-Error Regression on Log-Policy Ratios Provides Direct Policy Supervision
The RPO loss directly regresses the log-policy ratio toward the normalized reward signal, providing absolute supervision rather than relative ranking. The loss L_RPO = Â½ð”¼[(log Ï€_Î¸(y|x)/Ï€_ref(y|x) - (r - VÌ‚(x))/Ï„)Â²] penalizes deviation between the policy's implicit advantage and the centered reward. The gradient âˆ‡_z L = (1/N)Î£_i Î´_i(1_{y=y_i} - Ï€_Î¸(y|x)) pushes the policy toward targets weighted by residual errorâ€”interpretable as weighted cross-entropy.

### Mechanism 3: Eliminating Policy-Value Coupling Reduces Optimization Instability
By removing the need to jointly optimize policy and value parameters, RPO avoids the instability that arises from coupled gradient updates and off-policy value estimation errors. DRO minimizes Bellman residuals involving both V_Ï•(x) and Ï€_Î¸(y|x), creating interdependent gradients. RPO computes VÌ‚(x) as a fixed computation per batch (no backpropagation through value estimates), decoupling the optimization into simple supervised learning on Î¸ alone.

## Foundational Learning

- **KL-Regularized Policy Optimization**:
  - Why needed here: The entire RPO framework derives from the KL-regularized objective max_Ï€ E[r(x,y)] - Ï„Â·KL(Ï€||Ï€_ref), yielding the exponential form Ï€* âˆ Ï€_ref Â· exp(r/Ï„) that RPO operationalizes.
  - Quick check question: Can you explain why adding KL regularization produces a closed-form optimal policy, unlike unconstrained reward maximization?

- **Soft Value Functions and Partition Functions**:
  - Why needed here: Understanding why DRO needs V* and how RPO bypasses this via the equivalence V*(x) = Ï„ log Z*(x), where Z*(x) is the partition function summing over all responses.
  - Quick check question: What is the computational challenge in computing Z*(x) exactly, and why does empirical estimation help?

- **Log-Likelihood Ratio Gradient**:
  - Why needed here: The RPO gradient involves âˆ‡_z log Ï€_Î¸(y|x), producing (1_{y=y_i} - Ï€_Î¸(y|x))â€”the standard policy gradient form that enables interpretation as weighted cross-entropy.
  - Quick check question: Why does the gradient of log softmax with respect to logits produce the difference between the indicator and the predicted probability?

## Architecture Onboarding

- **Component map**:
  Reference Policy Ï€_ref -> Empirical Partition Estimator -> Value Estimator VÌ‚(x) -> Loss Module -> Trainable Policy Ï€_Î¸

- **Critical path**:
  1. Precompute or cache log Ï€_ref(y|x) for all samples during data loading
  2. Group samples by prompt x (can be done once at dataset initialization)
  3. For each batch, compute ZÌ‚(x_i) using all samples sharing prompt x_i (may require cross-batch lookups)
  4. Forward pass computes log Ï€_Î¸(y_i|x_i)
  5. Compute residual Î´_i = log Ï€_Î¸/Ï€_ref - (r_i - VÌ‚(x_i))/Ï„
  6. Backpropagate mean squared loss

- **Design tradeoffs**:
  - **Memory vs Computation**: Precomputing ZÌ‚(x) for all prompts requires one full dataset pass but enables faster per-batch computation; on-the-fly computation increases per-step cost
  - **Batching Strategy**: Samples sharing the same prompt should ideally be in the same batch for efficient partition estimation; random batching may require cross-batch communication
  - **Dataset Requirement**: RPO requires multiple completions per prompt; single-completion datasets need special handling or may be unsuitable

- **Failure signatures**:
  - **Single-completion prompts**: ZÌ‚(x) has only one term, making VÌ‚(x) â‰ˆ r(x,y) + Ï„ log Ï€_ref(y|x); loses normalization benefit
  - **Reward scale explosion**: If rewards have large magnitude, exp(r/Ï„) overflows; standardization is critical
  - **Reference policy collapse**: If Ï€_ref assigns near-zero probability to responses in data, log Ï€_ref â†’ -âˆž
  - **Temperature too low**: Ï„ = 0.1 caused instability at larger scales in experiments

- **First 3 experiments**:
  1. **Partition estimation sanity check**: On a small subset, manually compute ZÌ‚(x) for a prompt with 5+ completions; verify VÌ‚(x) values are finite and reasonably scaled. Compare against treating VÌ‚(x) = 0 (no normalization) to confirm the normalization effect.
  2. **Ablation on normalization**: Implement RPO-noNorm (skip VÌ‚ computation, use raw rewards) and compare against full RPO on a held-out validation set. Replicate the >90% preference gap shown in Table VI.
  3. **Temperature sweep with convergence monitoring**: Train with Ï„ âˆˆ {0.5, 1.0, 2.0, 5.0} on identical data splits; log training loss curves and final win rates against SFT. Identify the stable operating range for your reward distribution.

## Open Questions the Paper Calls Out

- **Question**: Does RPO retain its convergence speed and stability advantages over DRO when applied to larger decoder-only LLMs (e.g., 7B+ parameters) typically used in production?
  - **Basis**: The authors state in the Conclusion, "We plan to scale RPO to larger GPT-style models," as the current experiments are limited to Flan-T5 encoder-decoder models up to 2.85B parameters.
  - **Why unresolved**: Encoder-decoder architectures (T5) and smaller parameter counts often exhibit different optimization dynamics and memory constraints than large decoder-only models.
  - **What evidence**: Benchmarking RPO against DRO and KTO on a standard decoder-only base model (e.g., Llama 3 or Mistral) at scales exceeding 7B parameters.

- **Question**: How can the empirical partition function estimation in RPO be adapted for multi-turn dialogue where rewards are sparse or dependent on the full conversation history?
  - **Basis**: The authors identify the evaluation of RPO on "multi-turn feedback" as a specific direction for future work to address the limitation of single-trajectory triplets.
  - **Why unresolved**: The current RPO formulation estimates the partition function ZÌ‚(x) based on immediate prompt-response pairs, lacking a mechanism for credit assignment across sequential turns.
  - **What evidence**: Extending the RPO loss function to handle sequential decision making and testing performance on a multi-turn conversational benchmark.

- **Question**: To what extent does the performance of RPO degrade in low-data regimes where specific prompts have very few completions, affecting the reliability of the empirical partition function?
  - **Basis**: RPO estimates the partition function ZÌ‚(x) by summing over samples sharing the same prompt I_x. The Conclusion notes the method "relies on the quality and coverage of observed data," implying that sparse coverage per prompt could destabilize the normalization.
  - **Why unresolved**: If the dataset lacks multiple diverse responses per prompt, the empirical estimate of the partition function becomes a poor approximation of the true expectation, potentially biasing the policy gradient.
  - **What evidence**: An ablation study analyzing policy performance and gradient variance when artificially restricting the number of responses per prompt in the training set.

## Limitations
- Performance claims rely entirely on LLM-as-judge evaluation without human verification or automatic metrics reporting
- The method requires multiple completions per prompt, limiting applicability to sparse datasets
- Temperature hyperparameter is sensitive, with Ï„=0.1 causing instability for larger models
- Limited to single-turn optimization; multi-turn dialogue remains an open challenge

## Confidence
- **High confidence**: The mathematical derivation of RPO loss and its equivalence to weighted cross-entropy is sound and well-supported by equations in Section III.
- **Medium confidence**: The empirical results showing faster convergence and better win rates against baselines are compelling but rely heavily on automated preference evaluation without human verification.
- **Low confidence**: Claims about "value-free" optimization are somewhat overstated - the method still requires accurate empirical partition estimation, which can be as challenging as value function learning in sparse-data regimes.

## Next Checks
1. **Dataset sparsity analysis**: Compute the distribution of completions per prompt in your dataset. If >20% of prompts have <3 completions, implement a fallback smoothing mechanism or batch-level partition estimation.
2. **Cross-validation of preference evaluation**: Run a small human evaluation (10-20 examples) comparing RPO vs baseline outputs to verify LLM judge consistency. If human preferences diverge from LLM scores, adjust evaluation methodology.
3. **Temperature stability test**: Systematically vary Ï„ from 0.1 to 5.0 on a validation set and measure both convergence speed and final win rates. Identify the minimum Ï„ that maintains stability for your model scale.