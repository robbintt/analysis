---
ver: rpa2
title: Structural features of the fly olfactory circuit mitigate the stability-plasticity
  dilemma in continual learning
arxiv_id: '2502.01427'
source_url: https://arxiv.org/abs/2502.01427
tags:
- learning
- plasticity
- synaptic
- loss
- olfactory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study addresses the stability-plasticity dilemma in continual
  learning by introducing a minimal model of the fly olfactory circuit as a plug-and-play
  component (Fly Model). The model captures three key features: large expansion ratio
  (~40) from projection neurons to Kenyon cells, sparse synaptic connections (~6 per
  Kenyon cell), and sparse coding via winner-take-all inhibition.'
---

# Structural features of the fly olfactory circuit mitigate the stability-plasticity dilemma in continual learning

## Quick Facts
- arXiv ID: 2502.01427
- Source URL: https://arxiv.org/abs/2502.01427
- Authors: Heming Zou; Yunliang Zang; Xiangyang Ji
- Reference count: 40
- Key outcome: A fly olfactory circuit-inspired model improves continual learning by achieving 22-25% accuracy gains over standard methods

## Executive Summary
This paper addresses the stability-plasticity dilemma in continual learning by introducing a minimal model of the fly olfactory circuit as a plug-and-play component. The model captures three key biological features: large expansion ratio (~40) from projection neurons to Kenyon cells, sparse synaptic connections (~6 per Kenyon cell), and sparse coding via winner-take-all inhibition. When integrated with modern machine learning methods, the Fly Model enhances both memory stability and learning plasticity across challenging continual learning scenarios.

## Method Summary
The Fly Model implements a three-layer architecture: a pre-trained Vision Transformer (ViT-B/16) extracts 768-dimensional features; these are projected via a fixed, random sparse binary matrix to ~30,000 Kenyon cells (40x expansion); a top-k winner-take-all operation creates sparse activations; and a trainable linear layer maps these to output classes. The projection matrix remains frozen during training, while only the final classification layer is updated. This design leverages biological principles to improve continual learning performance without requiring extensive retraining.

## Key Results
- Accuracy improvements of 22-25% when combined with SGD on CIFAR-100, CUB-200-2011, and VTAB datasets
- 8-15% accuracy gains when combined with synaptic intelligence methods
- Effective mitigation of plasticity loss in streaming learning tasks
- Maintains performance under class-imbalanced conditions

## Why This Works (Mechanism)

### Mechanism 1: Dimensionality Expansion for Gradient Orthogonality
The 40x expansion from Projection Neurons to Kenyon Cells orthogonalizes gradients across tasks. In high-dimensional spaces, random vectors are more likely to be orthogonal, causing weight updates for new tasks to interfere less with previous tasks' optima.

### Mechanism 2: Weight Magnitude Reduction for Plasticity Maintenance
Larger numbers of Kenyon cells distribute representations with smaller average synaptic weights, theoretically improving the Hessian condition number and enabling faster convergence for new tasks.

### Mechanism 3: Top-k Inhibition for Sub-network Isolation
Winner-take-all inhibition enforces sparse coding, creating isolated sub-networks for different tasks. By minimizing activation overlap, synaptic updates for new tasks modify different weight subsets than those storing old memories.

## Foundational Learning

### Concept: Stability-Plasticity Dilemma
**Why needed here:** This is the core problem the Fly Model attempts to solve - balancing retaining old knowledge while learning new knowledge.
**Quick check:** Can you explain why regularization methods (like SI/EWC) typically solve stability at the expense of plasticity?

### Concept: Random Projection (Johnson-Lindenstrauss Lemma)
**Why needed here:** The PN-to-KC layer uses a fixed, random, sparse binary matrix. Understanding that random projections can preserve distances and enable linear separability is key to grasping why the expansion works without training that layer.
**Quick check:** Why does the PN->KC projection matrix remain frozen during the learning process?

### Concept: Sparse Coding / Winner-Take-All (WTA)
**Why needed here:** The biological function of the Anterior Paired Lateral (APL) neuron is modeled as a Top-k operation, implementing active forgetting/selective retention.
**Quick check:** How does reducing the number of active neurons paradoxically increase memory capacity for distinct patterns?

## Architecture Onboarding

### Component map:
Backbone (ViT-B/16) -> Fly Layer (PN→KC random sparse matrix, frozen) -> Top-k activation -> KC→MBON linear layer (trainable)

### Critical path:
The Sparse Projection Matrix must be binary (0 or 1) and sparse (approx 6-40 connections per KC). A dense matrix defeats the computational efficiency and biological "distinct input subset" mechanism.

### Design tradeoffs:
- **Expansion Ratio:** Higher is better for performance but increases memory (30,000 KCs)
- **Coding Level (k):** The critical hyperparameter
  - High k → High Plasticity (more neurons active) but Low Stability (interference)
  - Low k → High Stability (isolation) but Low Plasticity (dormant units/capacity loss)

### Failure signatures:
- **High Plasticity Loss:** Check "Dormant Units" metric. If >50% of KCs never activate across tasks, k is too low or expansion is insufficient
- **High Forgetting:** Check gradient angles. If forgetting is high despite the Fly Model, the expansion ratio might be too low

### First 3 experiments:
1. Baseline Integration: Replace standard linear classifier head of ViT on CIFAR-100 with Fly Model and compare accuracy against standard linear head using SGD
2. Ablation on Sparsity: Run continual learning task with varying coding levels (k ∈ {0.5%, 1%, 5%}) to find the "Pareto front" for specific dataset
3. Compatibility Check: Combine Fly Model with Synaptic Intelligence (SI) and verify recovery of plasticity typically lost when using SI alone

## Open Questions the Paper Calls Out

### Open Question 1
Can the Fly Model maintain its effectiveness when applied to diverse cognitive tasks outside of image classification? The authors note that more general CL paradigms involving diverse cognitive tasks need to be explored beyond the visual datasets tested.

### Open Question 2
Can an adaptive algorithm be developed to automatically select the optimal coding level (k) for varying network architectures and tasks? The authors note that while an optimal coding level exists, it can vary under different setups and developing an automatic parameter selection algorithm would greatly enhance real-world deployment.

### Open Question 3
Does incorporating more complex biological factors, such as state-dependent learning or branch-dependent computing, significantly improve continual learning performance over the minimal model? The authors list factors like "branch-dependent computing" and "state-dependent learning" that were excluded from the minimal model, suggesting these may be missing pieces for continual learning.

## Limitations
- Theoretical analysis of plasticity enhancement through expansion is described as heuristic without direct mathematical tools
- Focus on class-incremental learning with fixed task boundaries, leaving open questions about task-agnostic scenarios
- Computational cost scales linearly with Kenyon cell count (30,000), potentially prohibitive for larger applications

## Confidence
- **High confidence**: Empirical performance improvements across multiple datasets and continual learning scenarios
- **Medium confidence**: Theoretical mechanism explaining gradient orthogonality through high-dimensional random projections
- **Medium confidence**: Weight magnitude reduction hypothesis for plasticity maintenance based on optimization theory correlations

## Next Checks
1. Test Fly Model performance on non-stationary streaming scenarios where task boundaries are unknown or absent
2. Systematically vary the coding level k to map the full stability-plasticity trade-off curve and identify optimal operating points
3. Evaluate memory and computational scaling when increasing KC count beyond 30,000 to determine practical limits for larger problems