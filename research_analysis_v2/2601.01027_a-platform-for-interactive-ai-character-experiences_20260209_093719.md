---
ver: rpa2
title: A Platform for Interactive AI Character Experiences
arxiv_id: '2601.01027'
source_url: https://arxiv.org/abs/2601.01027
tags:
- user
- einstein
- character
- system
- personality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work presents a system for creating interactive, story-driven\
  \ AI characters capable of engaging in meaningful conversations. The authors address\
  \ the challenge of integrating multiple AI components\u2014conversational intelligence,\
  \ character integrity, personality modeling, emotion recognition, knowledge and\
  \ memory, speech synthesis, animation generation, and real-world interaction\u2014\
  into a unified platform."
---

# A Platform for Interactive AI Character Experiences

## Quick Facts
- arXiv ID: 2601.01027
- Source URL: https://arxiv.org/abs/2601.01027
- Authors: Rafael Wampfler; Chen Yang; Dillon Elste; Nikola Kovacevic; Philine Witzig; Markus Gross
- Reference count: 38
- This work presents a system for creating interactive, story-driven AI characters capable of engaging in meaningful conversations.

## Executive Summary
This work presents a system for creating interactive, story-driven AI characters capable of engaging in meaningful conversations. The authors address the challenge of integrating multiple AI components—conversational intelligence, character integrity, personality modeling, emotion recognition, knowledge and memory, speech synthesis, animation generation, and real-world interaction—into a unified platform. Their approach combines large language models (GPT-4o and Llama 3), multimodal sensing, speech-driven facial animation, motion-captured body movements, and image generation to create believable digital characters. As a proof-of-concept, they developed "Digital Einstein," allowing users to converse with a digital representation of Albert Einstein. The system achieves real-time performance with an average latency of 4.7-5.1 seconds per interaction and demonstrates adaptability across different contexts, as shown by its deployment at both tech and scientific events. The modular design enables customization for various characters and narratives, advancing the state of interactive digital storytelling.

## Method Summary
The platform integrates conversational AI, multimodal perception, and real-time animation through a modular architecture. The system uses GPT-4o for primary dialogue generation with embedding-based topic steering, Llama 3 for fallback responses, and a personality rewriting layer to maintain character consistency. It processes user speech through Azure STT, retrieves context from a vector database, and generates responses with Azure TTS. Facial animation uses NVIDIA Audio2Face for speech-driven lip-sync, while body animations come from motion capture libraries. The platform includes multimodal sensing for user detection, emotion recognition, and image generation for visual responses. Training involved fine-tuning Llama 3 8B on synthetic conversations, generating 4,402 topic-specific dialogues, and creating motion capture libraries for avatar animations.

## Key Results
- Achieves real-time performance with average latency of 4.7-5.1 seconds per interaction
- Successfully deployed Digital Einstein at tech and scientific events, demonstrating system adaptability
- Modular design enables customization for various characters and narratives while maintaining coherence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The system maintains narrative coherence by mapping conversation turns to a pre-defined embedding space of topics.
- Mechanism: Synthetic conversations are generated for specific topics and embedded (text-embedding-3-large). Live user input is embedded and compared via cosine similarity to these topic clusters. If the conversation drifts, the system selects a "next topic" from nearest neighbors to steer the LLM prompt.
- Core assumption: The semantic meaning of user inputs aligns linearly with the pre-generated topic clusters, allowing reliable mathematical proximity to dictate narrative context.
- Evidence anchors:
  - [abstract] "...enhanced by synthetic conversation generation, embedding-based prompt steering..."
  - [section] Page 4: "To facilitate smooth topic transitions, the system performs a neighbor search... One of the three nearest neighbors is randomly selected as the next topic."
  - [corpus] *Living the Novel* addresses similar "persona drift" in generic LLMs, suggesting this is a common failure mode in interactive agents.
- Break condition: If a user's query is ambiguous or polysemous, it may land equidistant between unrelated clusters, causing erratic topic transitions.

### Mechanism 2
- Claim: The platform mitigates the latency inherent in distributed cloud AI services by masking processing time with a "thinking" state and asynchronous asset generation.
- Mechanism: While the LLM and TTS modules process input (taking 4.7–5.1s total), the avatar enters a "thinking" state with motion-captured idle animations. Concurrently, images are generated asynchronously (taking ~32s) and cached for future turns rather than immediate display.
- Core assumption: Users perceive a "thinking" animation as natural cognitive delay rather than system lag, preserving immersion despite the >4s latency.
- Evidence anchors:
  - [abstract] "...achieves real-time performance with an average latency of 4.7-5.1 seconds..."
  - [section] Page 7: "The avatar’s thinking state, accompanied by corresponding animations, effectively masks this latency."
  - [corpus] *OceanChat* discusses interactive conversational agents where latency and flow impact behavioral change, implying latency management is critical for engagement.
- Break condition: If the LLM response time exceeds the duration of the "thinking" animation loop, the illusion of cognitive processing breaks, revealing system lag.

### Mechanism 3
- Claim: Character believability is achieved through a separation of concerns: a fine-tuned LLM handles logic/knowledge, while a secondary "personality infusion" model rewrites the output for tone.
- Mechanism: The system does not rely solely on the base LLM to maintain personality. Instead, it uses a two-step process: generate the content (Llama 3 or GPT-4o), then rewrite it (GPT-4o) based on 5 personality dimensions (vibrancy, conscientiousness, etc.) controlled by physical sliders.
- Core assumption: Decoupling "what is said" (factual accuracy) from "how it is said" (personality style) produces more consistent character integrity than a single prompt attempting to do both.
- Evidence anchors:
  - [abstract] "...maintaining character integrity, managing personality and emotions..."
  - [section] Page 5: "...rewrites the LLM responses to align them with predefined personality profiles..."
  - [corpus] Weak corpus evidence for specific decoupling mechanism; related works like *What's it like to be a chat?* focus on the philosophy of mind rather than this specific architectural decoupling.
- Break condition: If the rewriting model is too aggressive, it may strip necessary context or factual nuance from the base response to fit the personality constraints.

## Foundational Learning

- **Concept: Vector Embeddings & Cosine Similarity**
  - Why needed here: The system relies on vectorizing conversation history and topics (RAG) to steer the dialogue. You cannot debug the "topic transition" logic without understanding how the model measures semantic distance.
  - Quick check question: Can you explain why two sentences with different words might have a high cosine similarity score?

- **Concept: Asynchronous Audio-Driven Facial Animation**
  - Why needed here: The system uses NVIDIA Audio2Face, which requires streaming audio buffers (0.5s windows) to generate blendshapes. Understanding this streaming buffer is key to debugging lip-sync latency.
  - Quick check question: Why does the system process audio in 0.5-second windows rather than waiting for the full sentence to finish?

- **Concept: Finite State Machines (FSM) for Avatar Control**
  - Why needed here: The character transitions strictly between `idle`, `listening`, `thinking`, and `speaking` states based on sensor triggers. Debugging interaction flow requires tracing these state transitions.
  - Quick check question: What specific sensor input triggers the transition from `listening` to `thinking`?

## Architecture Onboarding

- **Component map:**
  Unity App (Core) -> Cognitive Layer -> Perception Layer -> Synthesis Layer
  (Avatar FSM + Sliders/Mic) -> (GPT-4o/Llama 3 + RAG + Personality) -> (OpenVINO Models) -> (Azure TTS + Audio2Face + SALSA)

- **Critical path:**
  User Speech -> [Mic] -> Azure STT -> [Text] -> LLM (Context Retrieval + Generation) -> [Text] -> Personality Rewriter -> [Text] -> Azure TTS -> [Audio] -> Audio2Face -> [Blendshapes] -> Unity Render.
  *Note: This serial chain is the primary source of the 4.7s latency.*

- **Design tradeoffs:**
  - **Latency vs. Robustness:** The system allows switching between high-latency cloud models (GPT-4o) and faster, lower-quality local models (Llama 3) or fallback animation (SALSA vs Audio2Face).
  - **General vs. Specific:** RAG is used instead of full fine-tuning for the primary model (GPT-4o), trading deep domain expertise for easier adaptability to new characters.

- **Failure signatures:**
  - **Stuck in "Thinking":** Occurs if the LLM service times out or the message queue between Unity and Python services stalls.
  - **Uncanny Lip-Sync:** Happens if the Audio2Face streaming buffer desynchronizes from the TTS audio output.
  - **Amnesiac Character:** Vector database (Memory Context) failing to retrieve relevant turns, resulting in repetitive questions.

- **First 3 experiments:**
  1. **Latency Profiling:** Measure the round-trip time for the Chatbot Service specifically to determine if GPT-4o or the Personality Rewriter is the bottleneck.
  2. **Topic Steering Validation:** Input a conversation strictly about "Nobel Prize" and trace the embedding vectors to verify the system correctly identifies the topic cluster without drifting.
  3. **Fallback Testing:** Manually disable the internet connection to verify the system successfully switches to the local Llama 3 model and SALSA lip-sync without crashing.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can real-time interruption handling be integrated into the conversational pipeline without degrading response quality or significantly increasing latency?
- Basis in paper: [explicit] "Additionally, interrupting the interlocutor is not yet implemented, but is planned as future work."
- Why unresolved: Current architecture processes user input sequentially through speech recognition, LLM generation, and synthesis; supporting mid-response interruption requires architectural changes to cancel/redirect ongoing generation.
- What evidence would resolve it: A modified system implementing barge-in capability with quantitative comparison of latency, user satisfaction, and conversation coherence against the baseline.

### Open Question 2
- Question: What is the relative contribution of each system component (personality modeling, emotion-aware animation, image generation, memory retrieval) to user engagement and perceived immersion?
- Basis in paper: [explicit] "Finally, we will conduct structured user studies to assess the contribution of individual system components to user engagement and perceived immersion."
- Why unresolved: Current evaluation reports aggregate metrics and deployment statistics but no ablation study isolating individual components.
- What evidence would resolve it: Controlled ablation experiments systematically disabling components and measuring engagement metrics (turn count, session duration, user ratings).

### Open Question 3
- Question: Can generative gesture models replace motion-capture libraries while maintaining animation quality on stylized characters with non-human body proportions?
- Basis in paper: [inferred] Authors note that "retargeting [Audio2Gesture outputs] to our stylized character leads to unnatural motion due to mismatched body proportions," motivating their current motion-capture library approach.
- Why unresolved: Generative models trained on human motion data may not transfer well to stylized avatars without extensive manual adjustment.
- What evidence would resolve it: Comparative study of motion quality (naturalness, synchronization) between generative models fine-tuned for stylized characters versus the curated motion-capture approach.

## Limitations
- High latency (4.7-5.1s) per interaction remains perceptible despite animation masking
- System relies heavily on cloud services that may introduce variability in real-world deployments
- Synthetic conversation generation requires manual topic definition and synthetic data generation for new characters

## Confidence
- **High**: The technical implementation details (latency measurements, component architecture, API usage) are well-documented and reproducible.
- **Medium**: The claims about user experience improvements through animation masking and personality rewriting are supported by the authors' deployment experiences but lack formal user studies.
- **Low**: The generalizability of the topic embedding approach to domains outside scientific discourse remains untested.

## Next Checks
1. Conduct A/B testing comparing user engagement with and without the "thinking" animation to empirically validate the latency masking claim.
2. Test the system with a non-scientific character (e.g., fictional persona) to assess whether the topic embedding approach generalizes beyond the Einstein use case.
3. Measure the personality rewriting model's impact by comparing responses with and without the secondary personality infusion step to quantify consistency improvements.