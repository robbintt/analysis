---
ver: rpa2
title: 'XLM: A Python package for non-autoregressive language models'
arxiv_id: '2512.17065'
source_url: https://arxiv.org/abs/2512.17065
tags:
- dataset
- datamodule
- language
- collator
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces XLM, a Python package designed to streamline
  the development and comparison of non-autoregressive language models (LMs), which
  differ from traditional autoregressive LMs by generating text in parallel rather
  than sequentially. The core method involves a modular framework built on PyTorch,
  PyTorch Lightning, and Hydra, allowing researchers to implement and swap model components
  like neural networks, loss functions, and predictors independently.
---

# XLM: A Python package for non-autoregressive language models

## Quick Facts
- arXiv ID: 2512.17065
- Source URL: https://arxiv.org/abs/2512.17065
- Reference count: 5
- Pre-trained small models for non-autoregressive LMs, achieving 100% sequence accuracy on synthetic planning tasks and competitive NLL scores on LM1B

## Executive Summary
XLM is a Python package that streamlines the development and comparison of non-autoregressive language models (LMs) through a modular framework built on PyTorch, PyTorch Lightning, and Hydra. Unlike traditional autoregressive LMs that generate text sequentially, XLM enables parallel or iterative token generation, potentially offering speed benefits while maintaining competitive quality. The library provides pre-trained small models and supports workflows for training, evaluation, and inference, successfully reproducing known results for non-autoregressive approaches like Insertion Language Models (ILM) and Masked Diffusion LMs (MDLM) on both synthetic planning tasks and LM1B language modeling.

## Method Summary
The framework implements non-autoregressive LMs through a component delegation pattern where a Harness class delegates model-specific logic (forward pass, loss computation, generation) to separate instances of Model, LossFunction, and Predictor. Data preparation is handled by a DataModule that delegates to DatasetManager instances, each containing a Dataset, Collator for batch preparation, and Preprocessor. Hydra configuration files define these components through `_target_` fields specifying fully qualified class paths, enabling runtime component swapping without code modification. The library includes pre-configured experiments for synthetic planning tasks (Star Graphs) and LM1B language modeling, with small pre-trained models (5M-90M parameters) available for immediate use.

## Key Results
- Achieved 100% sequence accuracy on StarEasy and StarMedium synthetic planning tasks using ILM
- Replicated published ILM and MDLM results within 2% of reported scores on both synthetic and LM1B benchmarks
- Demonstrated successful reproduction of ILM achieving 97.5% sequence accuracy on StarHard compared to ARLM's 25.2%

## Why This Works (Mechanism)

### Mechanism 1: Component Delegation via Composition
The modular architecture enables independent swapping of model components through delegation rather than inheritance. The Harness class delegates model-specific logic (forward pass, loss computation, generation) to separate component instances (Model, LossFunction, Predictor). DataModule similarly delegates to DatasetManager instances. Components communicate through typed interfaces, allowing any compatible implementation to be substituted.

### Mechanism 2: Configuration-Driven Instantiation
Hydra configuration enables runtime component swapping without code modification. Each component specifies a `_target_` field with fully qualified class path plus constructor arguments. At runtime, `hydra.utils.instantiate` uses Python's dynamic import system to load and construct the specified class with provided parameters. Configuration files can compose and override each other hierarchically.

### Mechanism 3: Non-Autoregressive Generation via Iterative Refinement
Non-autoregressive models can achieve competitive quality with potential speed benefits through parallel token generation. Unlike autoregressive models that generate left-to-right sequentially, non-autoregressive approaches (ILM, MDLM, MLM) generate tokens in parallel or through iterative refinement steps. ILM inserts tokens at arbitrary positions; MDLM denoises masked sequences.

## Foundational Learning

- **Non-autoregressive vs. autoregressive generation**: Why needed here: XLM specifically targets non-AR methods where tokens are generated in parallel or iteratively rather than strictly left-to-right. Understanding this distinction is essential for implementing appropriate LossFunction and Predictor logic. Quick check question: Why does parallel token generation potentially sacrifice coherence compared to sequential generation, and how might iterative refinement mitigate this?

- **Dependency injection pattern**: Why needed here: XLM's entire architecture relies on injecting model-specific implementations into generic Harness/DataModule containers through configuration. Quick check question: What would break if Harness directly instantiated `ILMModel()` instead of receiving it via configuration injection?

- **PyTorch Lightning training loop abstraction**: Why needed here: Harness inherits from LightningModule, leveraging Lightning's training utilities (logging, checkpointing, distributed training) without reimplementing boilerplate. Quick check question: What methods must a class inheriting from LightningModule implement to participate in training/evaluation loops?

## Architecture Onboarding

- **Component map**:
Harness (LightningModule) -> Model (torch.nn.Module), LossFunction, Predictor, Metrics
DataModule (LightningDataModule) -> DatasetManager -> Dataset, Collator, Preprocessor
Configuration (Hydra) -> experiment/*.yaml -> model/*.yaml -> model_type/*.yaml -> datamodule/*.yaml

- **Critical path**:
1. Run `xlm-scaffold <model_name>` to generate directory structure
2. Implement 4 core files: `model_*.py`, `loss_*.py`, `predictor_*.py`, `datamodule_*.py`
3. Create corresponding config files under `configs/` with `_target_` pointing to implementations
4. Create experiment config composing all components
5. Run: `xlm job_type=train experiment=<name>`

- **Design tradeoffs**:
- **Independence vs. code reuse**: "Copy over branching" principle prioritizes self-contained implementations over DRY principle, accepting duplication for faster prototyping
- **Flexibility vs. type safety**: Arbitrary code injection via Hydra enables rapid experimentation but sacrifices compile-time validation
- **Research focus vs. production security**: Design explicitly favors "flexibility [that] is a boon for research" over security concerns

- **Failure signatures**:
- `Hydra: Unable to find a package...` → Verify `_target_` path with manual `python -c "from path import Class"`
- `Hydra composition errors` → Check defaults list ordering; try single experiment config without nested composition
- `Shape mismatch in loss computation` → Collator output keys must match LossFunction input expectations
- `Training loss not decreasing` → Check that Collator properly prepares targets for your specific generation paradigm

- **First 3 experiments**:
1. **Validate setup with pre-configured ILM**: `xlm job_type=train experiment=star_easy_ilm debug=overfit` — should overfit single batch quickly, verifying data pipeline and model connectivity
2. **Train ILM on LM1B**: Follow Appendix A to implement unconditional language modeling, comparing NLL scores against Table 2 baselines
3. **Implement minimal custom model**: Use scaffold to create a 2-layer transformer variant; swap into existing experiment config to test component independence claim

## Open Questions the Paper Calls Out

- **Open Question 1**: How will FlexAttention integration affect memory efficiency and inference speed for non-autoregressive models with sequence packing?
- **Open Question 2**: Can XLM effectively scale to large non-autoregressive models (>1B parameters) despite its "copy over branching" design?
- **Open Question 3**: How do non-text sequence generation tasks perform within XLM's text-centric architecture?
- **Open Question 4**: What causes the ~2% benchmark deviation from original papers, and can exact reproducibility be achieved?

## Limitations

- Runtime flexibility through dependency injection creates debugging complexity and potential silent failures when component interfaces are incompatible
- Focus on small pre-trained models (5M-90M parameters) and synthetic benchmarks limits immediate applicability to production-scale language modeling tasks
- Evaluation methodology relies heavily on external reference implementations without providing specific evaluation protocols or inference templates

## Confidence

- **High Confidence**: Core architectural claims regarding modular composition and Hydra-based configuration management are directly supported by code inspection and successful reproduction of published ILM and MDLM results
- **Medium Confidence**: Claims about research acceleration benefits and flexibility advantages are substantiated by demonstrated ability to implement new models through scaffolded workflow, but lack empirical measurements
- **Low Confidence**: Comparative performance claims against autoregressive baselines are well-documented on synthetic tasks, but library's effectiveness on more complex, real-world language modeling tasks remains unproven beyond LM1B benchmark

## Next Checks

1. **Component Interface Compatibility Testing**: Systematically test framework's ability to swap components across different generation paradigms by attempting to use ILM's predictor with MDLM's loss function and vice versa, documenting which combinations succeed and where interface mismatches occur

2. **Configuration Debugging Pipeline**: Create comprehensive debugging guide for common Hydra configuration errors, including automated validation of `_target_` paths, schema checking for component interfaces, and regression tests for configuration composition

3. **Real-World Task Performance Scaling**: Evaluate framework's small pre-trained models (5M-90M parameters) on established benchmarks like GLUE or SuperGLUE to assess whether quality degradation compared to larger autoregressive models remains acceptable as task complexity increases beyond synthetic planning and unconditional language modeling