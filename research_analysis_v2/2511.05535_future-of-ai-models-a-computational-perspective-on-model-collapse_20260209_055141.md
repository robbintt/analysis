---
ver: rpa2
title: 'Future of AI Models: A Computational perspective on Model collapse'
arxiv_id: '2511.05535'
source_url: https://arxiv.org/abs/2511.05535
tags:
- data
- https
- similarity
- arxiv
- diversity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study quantifies the evolution of linguistic similarity in
  web datasets to estimate the onset of AI model collapse. It filters English-language
  Wikipedia articles from Common Crawl (2013-2025), embeds them using BAAI/bge-large-en-v1.5
  transformers, and computes average cosine similarity across years.
---

# Future of AI Models: A Computational perspective on Model collapse

## Quick Facts
- arXiv ID: 2511.05535
- Source URL: https://arxiv.org/abs/2511.05535
- Authors: Trivikram Satharasi; S Sitharama Iyengar
- Reference count: 21
- Primary result: Predicts AI model collapse by 2035 based on exponential rise in linguistic similarity of web datasets

## Executive Summary
This study quantifies the evolution of linguistic similarity in web datasets to estimate the onset of AI model collapse. By filtering English-language Wikipedia articles from Common Crawl (2013-2025), embedding them using BAAI/bge-large-en-v1.5 transformers, and computing average cosine similarity across years, the research reveals a steady rise in similarity starting before public LLM adoption (2013-2017), with a marked increase after 2017-2022 following the transformer breakthrough. An exponential fit predicts 90%, 95%, and 99% similarity saturation around 2035, 2042, and 2057, respectively, indicating potential collapse by 2035 if trends continue. These projections are preliminary due to limited data and unaccounted future model advancements.

## Method Summary
The methodology filters English-language Wikipedia articles from Common Crawl snapshots (2013-2025) using pycld2 for language detection and domain filtering. Articles are embedded using BAAI/bge-large-en-v1.5 (1024-dimensional output) via GPU-accelerated transformers. Pairwise cosine similarities are computed within each year's corpus and averaged to produce annual homogeneity metrics. An exponential saturation model h(y) = 0.35 + 0.0935(1-e^(-0.1029(y-2013))) is fitted using SciPy gradient descent, projecting saturation thresholds of 90%, 95%, and 99% around 2035, 2042, and 2057 respectively.

## Key Results
- Steady rise in linguistic similarity detected starting before public LLM adoption (2013-2017)
- Marked similarity increase after 2017-2022 following transformer breakthrough and widespread LLM deployment
- Exponential projection predicts 90% similarity saturation around 2035, indicating potential model collapse

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Recursive training on synthetic data progressively erases low-probability features ("the tails") from the learned distribution
- Mechanism: Synthetic samples oversample high-probability regions while undersampling rare patterns; when these samples re-enter training corpora, the model's estimated distribution contracts around common patterns
- Core assumption: Rare features in human-authored data are both valuable for generalization and systematically under-represented in synthetic outputs
- Evidence anchors: Abstract's statement on recursive training risks eroding linguistic diversity; "Forgetting the Tails of the Distribution" references; corpus links collapse to generalization-to-memorization shift

### Mechanism 2
- Claim: Cosine similarity of transformer embeddings provides a measurable proxy for corpus-level semantic homogenization
- Mechanism: Transformer encoders map semantically similar text to proximate regions in high-dimensional space; as synthetic content proliferates, average pairwise cosine similarity increases
- Core assumption: Embedding geometry correlates with meaningful linguistic diversity loss
- Evidence anchors: Abstract's observation of similarity rise pre- and post-LLM adoption; fitted exponential showing acceleration post-2017; related work on variance shrinkage

### Mechanism 3
- Claim: Web-scale data contamination creates a feedback loop where synthetic content displaces human-authored material in training corpora
- Mechanism: AI-generated text is published online → scraped into datasets → trains next-generation models → those models generate more homogeneous content → cycle repeats with compounding information loss
- Core assumption: Web scraping remains the dominant data acquisition method and synthetic content detection/filtering is insufficient
- Evidence anchors: Abstract's statistic on 74.2% of newly published webpages containing AI-generated material; Ahrefs 2025 study; detection prevention as potential intervention

## Foundational Learning

- Concept: **Transformer Self-Attention and Embedding Spaces**
  - Why needed here: The paper's methodology relies on transformer encoders to project text into 1024-dimensional space; understanding attention helps interpret why semantic convergence manifests as cosine similarity increase
  - Quick check question: Can you explain why semantically similar sentences produce vectors with smaller angular separation in a trained embedding space?

- Concept: **Exponential Growth and Saturation Modeling**
  - Why needed here: The projection to 90/95/99% saturation (2035/2042/2057) assumes exponential convergence; understanding model fitting helps assess confidence intervals
  - Quick check question: Given h(y) = 0.35 + 0.0935(1−e^(−0.1029(y−2013))), what does the parameter 0.1029 control, and how would higher uncertainty in this value affect prediction confidence?

- Concept: **Distribution Tails and Generalization**
  - Why needed here: "Forgetting the tails" is the core theoretical mechanism; rare patterns enable models to handle edge cases and novel inputs
  - Quick check question: If a language model's training distribution loses 50% of rare n-grams, what types of queries would most likely show degraded performance?

## Architecture Onboarding

- Component map: Common Crawl → Wikipedia filtering → BAAI/bge-large-en-v1.5 embedding → Cosine similarity computation → Annual averaging → Exponential fitting
- Critical path: 1) Extract and filter Common Crawl snapshots by year (2013-2025) 2) Generate embeddings for each document using GPU-accelerated transformer encoder 3) Compute pairwise cosine similarities within each year's corpus 4) Average similarities to produce annual homogeneity metric 5) Fit exponential model; extrapolate to saturation thresholds
- Design tradeoffs:
  - Wikipedia-only filtering reduces noise but may underestimate collapse rate in broader web corpora
  - Cosine captures semantic angle but ignores magnitude; alternatives might surface different degradation patterns
  - Single embedding model assumes generalization across 2013-2025 text styles; model-specific biases may contaminate similarity estimates
- Failure signatures:
  - Year-over-year fluctuations exceeding trend signal due to corpus size variation
  - Embedding model drift if encoder was trained on synthetic-heavy data
  - Saturation estimates shifting dramatically with additional years of data
- First 3 experiments:
  1. Robustness check: Re-run similarity analysis using OpenAI embeddings and E5 to assess encoder-agnostic trends
  2. Controlled corpus: Sample pre-2010 human-authored text to establish baseline similarity floor
  3. Cross-domain validation: Apply methodology to Reddit, arXiv, or news corpora to test Wikipedia representativeness

## Open Questions the Paper Calls Out
1. How do adaptive feedback mechanisms between human and synthetic content generation affect the preservation of data diversity?
2. Does the observed trend of semantic homogenization persist in multi-modal datasets?
3. Is high semantic similarity a reliable proxy for the functional "collapse" of model utility?

## Limitations
- Limited data: Exponential projection relies on only 13 years of data with minimal post-LLM samples
- Embedding model bias: Similarity measures may be compromised if BAAI/bge-large-en-v1.5 was trained on increasingly synthetic data
- Conservative estimate: Wikipedia filtering may underestimate collapse velocity in broader web corpus

## Confidence
- **High Confidence**: Pre-2017 similarity increase and post-2017 acceleration are empirically observable
- **Medium Confidence**: 90% saturation projection around 2035 follows from fitted parameters but assumes trend continuation
- **Low Confidence**: 95% (2042) and 99% (2057) saturation dates are extrapolations with minimal supporting data

## Next Checks
1. Cross-Encoder Validation: Replicate analysis using OpenAI embeddings and E5 models to verify encoder-agnostic trends
2. Synthetic Content Quantification: Measure actual proportion of synthetic content in Wikipedia vs. broader web domains
3. Human Baseline Establishment: Create pre-2010 human-authored control dataset to establish natural similarity floor