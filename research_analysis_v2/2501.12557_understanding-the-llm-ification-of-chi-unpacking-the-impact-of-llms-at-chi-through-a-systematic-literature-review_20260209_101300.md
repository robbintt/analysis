---
ver: rpa2
title: 'Understanding the LLM-ification of CHI: Unpacking the Impact of LLMs at CHI
  through a Systematic Literature Review'
arxiv_id: '2501.12557'
source_url: https://arxiv.org/abs/2501.12557
tags:
- llms
- computing
- systems
- human
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents the first systematic literature review examining
  how large language models (LLMs) have transformed human-computer interaction (HCI)
  research at CHI conferences from 2020-2024. Through analysis of 153 papers, the
  authors identify 10 application domains, 5 roles for LLMs in research, and 29 limitations
  and risks.
---

# Understanding the LLM-ification of CHI: Unpacking the Impact of LLMs at CHI through a Systematic Literature Review

## Quick Facts
- arXiv ID: 2501.12557
- Source URL: https://arxiv.org/abs/2501.12557
- Reference count: 40
- This study presents the first systematic literature review examining how large language models (LLMs) have transformed human-computer interaction (HCI) research at CHI conferences from 2020-2024.

## Executive Summary
This systematic literature review analyzes 153 CHI papers to understand how LLMs have transformed HCI research. The authors identify 10 application domains, 5 distinct roles for LLMs in research, and 29 limitations and risks. Key findings show that LLMs are primarily used for empirical and artifact contributions across diverse domains, with the most common roles being system engines and objects of study. The paper provides actionable guidance for researchers through guiding questions and identifies opportunities to expand theoretical contributions and standardize prototyping practices in the field.

## Method Summary
The study conducted a systematic literature review of CHI papers from 2020-2024, filtering 4,077 proceedings via title/abstract keywords. Four rounds of 10-paper samples developed a 51-code codebook, which was then applied to 153 papers by three researchers with consensus resolution. Inter-rater reliability ranged from 0.77-0.89. The dataset is publicly available on GitHub for reproduction.

## Key Results
- LLMs primarily function as system engines (62.74%) and objects of study (20.26%) in CHI research
- 61.44% of papers contribute artifacts, compared to 24.50% baseline in CHI literature
- 84.98% of studies use closed-source models (primarily GPT variants), raising reproducibility concerns
- 29 distinct limitations and risks identified, with validity and reproducibility being most common

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs lower the barrier to producing high-fidelity research artifacts, potentially shifting field norms around prototyping.
- **Mechanism:** By functioning as "system engines," LLMs allow researchers to generate complex interactive systems without deep traditional software engineering.
- **Core assumption:** Artifact frequency correlates with reduced development friction rather than purely increased theoretical interest.
- **Evidence anchors:** Section 4.2 shows high artifact contribution frequency; Section 5.1.3 discusses the shift from "Wizard of Oz" to automated prototypes.

### Mechanism 2
- **Claim:** Researchers use "modular" system design as a defense mechanism against rapid obsolescence and non-determinism of closed-source models.
- **Mechanism:** Because closed models change without notice and limit reproducibility, researchers design systems where the LLM is a swappable component.
- **Core assumption:** The core research insight is separable from the specific model version used.
- **Evidence anchors:** Section 4.4.3 describes modular design practices; Section 5.2.1 notes proprietary models may inject unspecified edits.

### Mechanism 3
- **Claim:** Applying the "5 Roles" taxonomy reveals methodological gaps in how LLMs are integrated into the research lifecycle.
- **Mechanism:** Classifying LLMs by role (Engine, Tool, Participant, Object, Perception) exposes specific validity risks unique to each.
- **Core assumption:** Risks associated with LLMs are dependent on the functional role the technology plays.
- **Evidence anchors:** Section 4.3 defines the 5 roles and their prevalence; Section 4.4 details how validity risks differ by context.

## Foundational Learning

- **Concept:** **Wobbrock and Kientz's Research Contributions Taxonomy**
  - **Why needed here:** The paper relies on this framework to prove that LLM research is skewing heavily toward "Artifact" and "Empirical" contributions while neglecting "Theoretical" and "Methodological" ones.
  - **Quick check question:** If a paper introduces a new dataset generated by an LLM, is it an *Artifact* or a *Dataset* contribution under this specific taxonomy?

- **Concept:** **Ecological Validity vs. Internal Validity**
  - **Why needed here:** The review identifies a tension where LLMs allow for highly realistic prototypes but suffer from hallucination and non-determinism.
  - **Quick check question:** Does a "Wizard of Oz" prototype provide higher internal validity or higher ecological validity compared to a real-time LLM system?

- **Concept:** **The "Automation Trap" (Bainbridge)**
  - **Why needed here:** Section 5.3 explicitly references this regarding *LLMs as Research Tools*.
  - **Quick check question:** If using an LLM to label 1,000 data points requires 2,000 manual checks to ensure quality, has the "automation" succeeded?

## Architecture Onboarding

- **Component map:** Prompts (zG3) & User Intent -> The LLM Role (Engine, Tool, Participant, Object, Perception) -> Validity Checks (zS questions) & Safety Guardrails -> The Contribution (Artifact, Empirical Data, Theory)

- **Critical path:**
  1. **Role Definition:** Select 1 of 5 roles (zG1)
  2. **Model Justification:** Choose Open vs. Closed source based on reproducibility requirements (zG2)
  3. **Modular Implementation:** Decouple the system logic from the specific model API
  4. **Prompt Disclosure:** Log and release exact prompts (zG3) to satisfy external validity

- **Design tradeoffs:**
  - **Closed vs. Open Models:** Closed models offer state-of-the-art performance but destroy reproducibility; open models require significant compute resources
  - **Simulation vs. Real Users:** Using LLMs as "Simulated Users" is faster/cheaper but risks "Representational Harms" and stereotyping

- **Failure signatures:**
  - **The "Black Box" Limitation:** Authors attribute errors to "unspecified biases" without analysis
  - **The "Moving Target" Error:** Results are non-reproducible because the underlying closed model was updated
  - **The "Automation Trap":** Spending more resources verifying LLM output than generating it manually

- **First 3 experiments:**
  1. **Role Audit:** Classify 10 recent LLM papers into the 5 roles and check if methodology matches validity checks required for that role
  2. **Prompt Sensitivity Test:** Swap underlying model (GPT-4 to Llama 3) while holding prompts constant to test modularity assumption
  3. **Reproducibility Check:** Attempt to replicate exact results of a paper that did not release prompts

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the availability of LLMs change the required fidelity and evaluation standards for prototyping in HCI research?
- **Basis in paper:** [explicit] The authors ask about prototype fidelity needed to demonstrate new interactions (Section 5.1.3)
- **Why unresolved:** LLMs have lowered the barrier to creating functional prototypes, potentially making traditional low-fidelity methods less ecologically valid, yet standards for evaluating these new "high-fidelity" artifacts are undefined
- **What evidence would resolve it:** Comparative studies assessing validity and generalizability of insights from LLM-backed prototypes versus traditional low-fidelity prototyping methods

### Open Question 2
- **Question:** How can researchers validate the use of LLMs as proxies for human participants without misrepresenting specific populations?
- **Basis in paper:** [explicit] The authors ask how an LLM-powered tool can adequately stand in for the target population (Section 5.3)
- **Why unresolved:** LLMs often fail to capture complexity of lived experiences or specific demographics, raising ethical concerns regarding consent and representation
- **What evidence would resolve it:** Empirical benchmarks comparing LLM-simulated user data against real human data across diverse intersectional groups

### Open Question 3
- **Question:** How can the HCI community establish norms for reproducibility when research relies on closed-source, proprietary models?
- **Basis in paper:** [explicit] Proprietary LLMs raise reproducibility concerns, with 84.98% studying closed GPT-family models (Section 5.2.1)
- **Why unresolved:** Closed models change via API updates without notice, and many authors fail to disclose prompts, making exact replication impossible
- **What evidence would resolve it:** Development and adoption of standardized reporting artifacts that correlate with higher replication success rates

## Limitations
- The 51-code codebook definitions were not fully specified, limiting reproducibility of specific qualitative classifications
- The sample may underrepresent early LLM applications due to keyword-based filtering
- Inter-rater reliability (0.77-0.89) suggests moderate consistency but indicates some ambiguity in code application

## Confidence
- **High Confidence:** Quantitative statistics on paper distribution across domains, roles, and contribution types
- **Medium Confidence:** Qualitative findings about limitations and risks, given moderate IRR and potential codebook ambiguity
- **Low Confidence:** Specific guidance for researchers, as this extrapolates from descriptive findings rather than prescriptive evidence

## Next Checks
1. **Codebook Validation:** Obtain and test the full 51-code definitions with independent coders to verify classification reliability
2. **Temporal Analysis:** Compare pre- and post-ChatGPT periods (2020-2022 vs. 2023-2024) to assess if adoption patterns reflect broader technological shifts
3. **External Validity Test:** Apply the 5-role taxonomy and guiding questions to a sample of non-CHI HCI papers to evaluate generalizability of the framework