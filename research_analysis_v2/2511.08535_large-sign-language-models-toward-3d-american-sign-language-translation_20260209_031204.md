---
ver: rpa2
title: 'Large Sign Language Models: Toward 3D American Sign Language Translation'
arxiv_id: '2511.08535'
source_url: https://arxiv.org/abs/2511.08535
tags:
- language
- sign
- motion
- recognition
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Large Sign Language Models (LSLM), a framework
  for translating 3D American Sign Language using Large Language Models (LLMs). Unlike
  prior methods relying on 2D video, LSLM uses 3D skeletal data from SMPL-X models
  to capture spatial and gestural depth information, enabling more robust and accurate
  translation.
---

# Large Sign Language Models: Toward 3D American Sign Language Translation

## Quick Facts
- arXiv ID: 2511.08535
- Source URL: https://arxiv.org/abs/2511.08535
- Reference count: 40
- One-line result: 3D skeletal data + LLM achieves higher BLEU, ROUGE, CIDEr than video-based methods on ASL translation

## Executive Summary
This paper introduces Large Sign Language Models (LSLM), a framework for translating 3D American Sign Language (ASL) to text using Large Language Models (LLMs). Unlike prior video-based methods, LSLM processes 3D skeletal data from SMPL-X models, capturing spatial and gestural depth information. The approach includes a vector-quantized variational autoencoder (VQ-VAE) to encode sign motion into discrete tokens, followed by alignment with LLM embeddings. LSLM is pretrained and instruction-tuned to perform direct gesture-to-text translation and follow external prompts for flexible outputs. Results show LSLM outperforms existing methods on ASL translation tasks, with improved BLEU, ROUGE, and CIDEr scores, and demonstrates strong performance across different LLM backbones.

## Method Summary
LSLM is a three-stage pipeline: (1) VQ-VAE tokenizer training converts 3D skeletal motion sequences into discrete tokens using a 1024-entry codebook; (2) Modality alignment and pretraining learn to project gesture tokens to LLM embedding space, then jointly fine-tune LLM on gesture-translation pairs; (3) Instruction fine-tuning adapts the model to follow diverse translation prompts. The framework uses 3D SMPL-X skeletal data (52 joints) instead of 2D video, enabling environment-agnostic processing. Training uses SignAvatars (ASL subset from How2Sign) with ~30K samples, evaluated with BLEU, ROUGE, CIDEr, and WER metrics.

## Key Results
- Direct Alignment + LLM-only instruction tuning achieved best semantic scores: BLEU@1 14.2, CIDEr 28.0
- Outperformed MotionGPT baseline: BLEU@1 12.1 vs 7.9, CIDEr 28.0 vs 20.8
- Llama-3.2-3B-Instruct backbone outperformed Qwen2.5-3B-Instruct on CIDEr (28.0 vs 22.3)
- Qwen showed higher insertion rates (10.1 vs 4.0) than Llama

## Why This Works (Mechanism)

### Mechanism 1: 3D Skeletal Representation Enables Environment-Agnostic Processing
- Using 3D SMPL-X skeletal data instead of 2D video provides more robust translation by decoupling motion from visual artifacts. SMPL-X represents sign language using 52 joints (body, hands, face—eyeballs excluded) parameterized by pose, shape, and expression, creating a normalized representation that strips away lighting, background, and camera angle variations while preserving kinematic information essential for sign comprehension.
- Core assumption: Accurate 3D skeleton extraction from arbitrary video input is available.
- Evidence anchors: [abstract], [section 3.1], [corpus].
- Break condition: If SMPL-X estimation fails on real-world videos (occlusion, extreme poses, poor lighting), downstream tokenization degrades irrecoverably.

### Mechanism 2: VQ-VAE Discretization Creates LLM-Compatible Sign Tokens
- Vector-quantized variational autoencoder transforms continuous motion into discrete tokens that interface with LLM embedding spaces. Encoder maps frames to latent vectors, quantized against learnable codebook via nearest-neighbor matching, producing discrete token sequences that compress spatio-temporal dynamics while remaining reconstructable by decoder.
- Core assumption: A 1024-entry codebook captures sign language distinctions without excessive sparsity.
- Evidence anchors: [section 3.1], [section 4.1, Figure 2], [corpus].
- Break condition: Codebook too small → sign distinctions collapse; too large → sparse usage, overfitting, poor generalization.

### Mechanism 3: Explicit Modality Alignment Enables LLM Comprehension
- MLP-based projection aligns gesture embeddings to LLM text embedding space, enabling pretrained text-only LLMs to process sign language. Two MLP layers map quantized tokens to aligned embeddings, which fuse with language embeddings before LLM processing, bridging the representation gap between motion tokens and pretrained word embeddings.
- Core assumption: Linear projection suffices for modality alignment; no complex cross-modal attention required.
- Evidence anchors: [section 3.2], [table 1], [corpus].
- Break condition: Insufficient alignment causes LLM to treat gesture tokens as noise, producing hallucinated or irrelevant text.

## Foundational Learning

- **Concept: SMPL-X Parametric Body Model**
  - Why needed here: Entire pipeline depends on understanding 3D pose representation (54→52 joints, 623-dimensional feature vectors encoding root velocity, joint positions, rotations).
  - Quick check question: Why does SMPL-X separate pose parameters (joint rotations) from shape parameters (body proportions), and which is more critical for distinguishing ASL signs?

- **Concept: Vector Quantization**
  - Why needed here: Discretization step converts continuous motion to tokens processable by LLMs.
  - Quick check question: Given a latent vector and a 1024-entry codebook, how do you compute its quantized representation, and what does the commitment loss (β term in Equation 2) enforce?

- **Concept: Instruction Tuning for LLMs**
  - Why needed here: Stage 3 uses instruction templates to enable flexible translation behavior beyond direct input-output mapping.
  - Quick check question: Given gesture-translation pairs, how would you construct an instruction-following dataset that teaches the model to respond differently to "Translate..." vs "Explain the meaning of..." prompts?

## Architecture Onboarding

- **Component map:** Raw video → SMPL-X estimator → 52-joint 3D skeleton (623-dim per frame) → Encoder → Latent vectors → Quantizer (1024 codebook) → Discrete tokens z_1:L → 2-layer MLP φ(·) → Aligned gesture embeddings → LLM Backbone → Text output

- **Critical path:** VQ-VAE codebook quality → Alignment MLP training → LLM pretraining → Instruction tuning. Poor codebook reconstruction (high FID/MPJPE) propagates irrecoverably.

- **Design tradeoffs:**
  - Training scheme: Direct Alignment + LLM-only instruction tuning achieved best semantic scores (BLEU@1 14.2, CIDEr 28.0), but Joint Pretraining had lower WER
  - LLM backbone: Llama outperformed Qwen on CIDEr (28.0 vs 22.3); Qwen showed higher insertion rates (10.1 vs 4.0)
  - Codebook size: 1024 optimal for FID; larger sizes increased MPJPE without semantic gains

- **Failure signatures:**
  - High insertion rate: model generates irrelevant words → weak gesture-text grounding
  - High deletion rate: model omits content → alignment or tokenization failure
  - Degradation after instruction tuning (MotionGPT baseline showed this): indicates poor instruction-following adaptability

- **First 3 experiments:**
  1. Validate VQ-VAE tokenizer: Train on How2Sign 3D data, sweep codebook sizes {512, 1024, 1528}, measure FID/MPJPE/PAMPJPE. Confirm 1024 achieves lowest FID.
  2. Ablate alignment strategies: Compare (a) MLP-only alignment, (b) Joint MLP+LLM pretraining, (c) Staged pretraining. Report BLEU, ROUGE, CIDEr, WER.
  3. Test instruction generalization: After instruction tuning, evaluate on held-out prompt templates (e.g., "Summarize this sign in one sentence"). Verify model follows prompt intent rather than ignoring it.

## Open Questions the Paper Calls Out
- How can linguistically-aware data augmentation strategies specifically address the structural differences between ASL grammar and written English in LSLM training? [explicit]
- Does the explicit exclusion of eyeball joints and the reliance solely on skeletal pose limit the model's ability to capture non-manual markers essential for semantic accuracy? [inferred]
- How does translation performance degrade when the input 3D skeletal data contains the noise typical of real-world SMPL-X estimations rather than the "accurate" extraction assumed? [inferred]

## Limitations
- Architectural transparency: Critical implementation details for VQ-VAE and MLP alignment layers remain unspecified
- Generalization assumptions: Framework assumes accurate 3D skeleton extraction but does not validate robustness to real-world noise
- Dataset constraints: Results demonstrated only on single ASL subset with ~30K samples, limiting generalization claims

## Confidence
- High confidence: Core claim that 3D skeletal representation enables more robust translation than 2D video approaches
- Medium confidence: VQ-VAE discretization approach effectiveness supported by reconstruction metrics but lacks semantic validation
- Low confidence: Instruction-tuning generalization effectiveness not systematically evaluated across diverse prompt types

## Next Checks
1. **Semantic codebook analysis**: Evaluate whether the 1024-entry codebook preserves semantic distinctions by testing nearest-neighbor consistency for similar signs and ensuring dissimilar signs have distinct representations
2. **Cross-dataset generalization**: Train and evaluate LSLM on a different ASL dataset (e.g., RWTH-PHOENIX-Weather 2014T) to verify environment-agnostic processing claims
3. **Instruction-following stress test**: Systematically vary prompt templates to include ambiguous, contradictory, or multi-step instructions to measure true instruction comprehension versus pattern matching