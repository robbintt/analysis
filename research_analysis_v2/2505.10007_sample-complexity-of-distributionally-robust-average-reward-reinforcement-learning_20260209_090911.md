---
ver: rpa2
title: Sample Complexity of Distributionally Robust Average-Reward Reinforcement Learning
arxiv_id: '2505.10007'
source_url: https://arxiv.org/abs/2505.10007
tags:
- then
- tminorize
- sample
- have
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces the first finite-sample complexity guarantees
  for distributionally robust average-reward reinforcement learning (DR-AMDP). The
  authors propose two algorithms: one that reduces the problem to a distributionally
  robust discounted MDP and another that introduces an anchoring state to stabilize
  the controlled transition kernels within the uncertainty set.'
---

# Sample Complexity of Distributionally Robust Average-Reward Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2505.10007
- **Source URL:** https://arxiv.org/abs/2505.10007
- **Reference count:** 40
- **Key outcome:** First finite-sample complexity guarantees for distributionally robust average-reward MDPs with near-optimal $\widetilde{O}(|S||A|t_{\mathrm{mix}}^2\varepsilon^{-2})$ rates.

## Executive Summary
This paper provides the first finite-sample complexity analysis for distributionally robust average-reward reinforcement learning (DR-AMDP). The authors develop two algorithms: one that reduces the problem to a distributionally robust discounted MDP through careful discount factor calibration, and another that introduces an anchoring state to ensure all transition kernels within the uncertainty set remain ergodic. Both algorithms achieve near-optimal sample complexity bounds of $\widetilde{O}(|S||A|t_{\mathrm{mix}}^2\varepsilon^{-2})$ for estimating the optimal policy and robust average reward under KL and $f_k$-divergence-based uncertainty sets, assuming the nominal MDP is uniformly ergodic. The theoretical analysis is validated through numerical experiments on a 2-state MDP that demonstrate the expected convergence rates.

## Method Summary
The paper proposes two algorithms for DR-AMDP: a reduction approach that solves a robust discounted MDP with calibrated discount factor $\gamma = 1 - 1/\sqrt{n}$, and an anchored approach that modifies the uncertainty set to include a restart probability to a designated state. Both methods leverage strong duality to solve the inner minimization problem over transition kernels efficiently. The reduction method introduces approximation bias that diminishes as the sample size increases, while the anchored method ensures all kernels within the uncertainty set satisfy the Doeblin condition required for the Bellman equation to hold. The theoretical analysis shows that under uniform ergodicity assumptions, both algorithms achieve near-optimal sample complexity.

## Key Results
- First finite-sample complexity guarantees for DR-AMDP with $\widetilde{O}(|S||A|t_{\mathrm{mix}}^2\varepsilon^{-2})$ rates
- Near-optimal dependence on state-action space size and accuracy
- Both reduction and anchored algorithms achieve the same asymptotic sample complexity
- Numerical experiments validate theoretical convergence rates on a 2-state MDP

## Why This Works (Mechanism)

### Mechanism 1: Discounting Reduction with Bias-Variance Calibration
The reduction algorithm sets $\gamma = 1 - n^{-1/2}$ to balance approximation bias (from artificial discounting) and statistical estimation error. This specific rate ensures the total error shrinks at the optimal rate, coupling the discount factor to the sample complexity $n$.

### Mechanism 2: Anchoring for Kernel Stability
The anchored algorithm introduces a small restart probability $\xi = 1/\sqrt{n}$ to a designated state, forcing all transition kernels within the uncertainty set to remain uniformly ergodic. This prevents the policy from getting stuck in transient states or disconnected chains.

### Mechanism 3: Bounded Adversarial Perturbation (Assumption 2)
The theoretical analysis requires the uncertainty radius $\delta$ to satisfy $\delta \leq \frac{1}{8m^2_{\vee}p_{\wedge}}$ to ensure the Radon-Nikodym derivative between perturbed and nominal kernels remains lower-bounded, preserving mixing properties.

## Foundational Learning

- **Concept: Uniform Ergodicity & Mixing Time**
  - **Why needed here:** The analysis relies on average reward being constant across states, which uniform ergodicity guarantees by ensuring rapid mixing
  - **Quick check:** Can you explain why an MDP that is "weakly communicating" but not "uniformly ergodic" might violate the sample complexity bounds?

- **Concept: Span Semi-norm vs. $\ell_{\infty}$ Norm**
  - **Why needed here:** The span of value functions (max - min) rather than $\ell_{\infty}$ norm is used to bound errors in average-reward settings
  - **Quick check:** If you add constant $C$ to every entry of discounted value function $V^*$, does optimal policy change?

- **Concept: Robust Bellman Equation & Duality**
  - **Why needed here:** To implement algorithms, one must solve the worst-case transition minimization using duality to convert it to tractable optimization
  - **Quick check:** Why does the reduction algorithm require solving a dual optimization problem at every Bellman backup step?

## Architecture Onboarding

- **Component map:** Empirical Generator -> Uncertainty Set Constructor -> Robust Solver -> Policy Extractor
- **Critical path:** Calculation of the robust operator $\mathcal{T}^*_{\gamma}$ or $\mathcal{T}^*$ involving solving the convex dual problem to find the worst-case expected value
- **Design tradeoffs:** Reduction (easier to implement but introduces bias) vs. Anchored (exact for modified MDP but requires structural changes)
- **Failure signatures:** Non-convergence suggests mixing time violation; overly pessimistic policies suggest uncertainty radius too large
- **First 3 experiments:** Scaling validation to verify convergence rate; radius stress test to identify breaking point; algorithm comparison on larger state spaces

## Open Questions the Paper Calls Out

- Can finite-sample complexity guarantees be extended to weakly communicating or multichain MDPs?
- What is the sample complexity under general $l_p$-norm or Wasserstein metric uncertainty sets?
- Can the constraint on uncertainty radius $\delta$ be relaxed or removed while retaining theoretical guarantees?

## Limitations

- Theoretical analysis assumes uniformly ergodic MDPs, excluding many practical scenarios
- Sample complexity bound scales with $t_{\text{mix}}^2$, which can be prohibitive for slowly mixing systems
- Anchoring mechanism fundamentally alters MDP dynamics and may introduce significant bias

## Confidence

- **High Confidence:** Core algorithmic framework and sample complexity bounds under stated assumptions appear mathematically sound
- **Medium Confidence:** Numerical experiments demonstrate convergence rates matching predictions but limited to very small MDPs
- **Low Confidence:** Discussion of computational complexity doesn't adequately address runtime overhead of solving dual optimization at each Bellman backup

## Next Checks

1. **Stress Test Assumption 2:** Systematically vary uncertainty radius $\delta$ beyond theoretical bounds to empirically identify threshold where guarantees break down
2. **Scalability Experiment:** Implement algorithms on MDPs with $|S|>2$ and $|A|>2$ to verify $\widetilde{O}(|S||A|t_{\text{mix}}^2\varepsilon^{-2})$ scaling holds in practice
3. **Alternative Uncertainty Sets:** Test algorithms with uncertainty sets based on different divergences (total variation, Wasserstein) to assess robustness to divergence choice