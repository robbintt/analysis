---
ver: rpa2
title: Music Generation using Human-In-The-Loop Reinforcement Learning
arxiv_id: '2501.15304'
source_url: https://arxiv.org/abs/2501.15304
tags:
- music
- user
- generation
- hitl
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to algorithmic music generation
  using Human-In-The-Loop Reinforcement Learning (HITL RL) combined with music theory
  principles. The system generates musical compositions without pre-existing data
  dependencies, relying on user ratings as rewards to iteratively improve quality.
---

# Music Generation using Human-In-The-Loop Reinforcement Learning

## Quick Facts
- arXiv ID: 2501.15304
- Source URL: https://arxiv.org/abs/2501.15304
- Reference count: 0
- This paper introduces a novel approach to algorithmic music generation using Human-In-The-Loop Reinforcement Learning (HITL RL) combined with music theory principles.

## Executive Summary
This paper presents a novel approach to algorithmic music generation using Human-In-The-Loop Reinforcement Learning (HITL RL) combined with music theory principles. The system generates musical compositions without pre-existing data dependencies, relying on user ratings as rewards to iteratively improve quality. An episodic tabular Q-learning algorithm with epsilon-greedy exploration policy was employed, where actions modify melody pitches, durations, percussion elements, and note removal. Testing involved 13 users (3 music theory experts), with generated compositions evaluated on musicality, novelty, and coherence. Results showed steady improvement in training episode quality over time, with expert users identifying potential for enhanced chord progressions and dynamic expression. The approach successfully demonstrated autonomous music generation while maintaining user customization and avoiding data dependencies.

## Method Summary
The system uses episodic tabular Q-learning with epsilon-greedy exploration (ε=0.5) to generate music through iterative user feedback. Initial tracks are generated from user-specified parameters (base note, scale type, track length) with pitches selected from the generated scale. The agent applies discrete actions to modify specific musical elements: pitch ±1, duration ±0.25, percussion change, or note removal. User ratings (1-10) serve as immediate rewards, updating Q-values via the Bellman equation with α=0.1 and γ=0.9. The process repeats for minimum 10 episodes, with Q-tables saved/loaded for persistence.

## Key Results
- Steady improvement in training episode quality demonstrated across user evaluations
- Expert users identified potential for enhanced chord progressions and dynamic expression
- Successfully generated musical compositions without pre-existing data dependencies
- User ratings served as effective rewards for iterative quality improvement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: User ratings serve as a personalized reward function that guides the agent toward compositions matching individual aesthetic preferences.
- Mechanism: Each generated composition receives a user rating (1-10), which becomes the immediate reward R(s,a) in the Q-learning update. The Bellman equation propagates this signal backward through state-action pairs, building a value function specific to each user's subjective taste.
- Core assumption: Users provide consistent ratings that reflect genuine preferences, and the rating scale maps meaningfully to musical quality.
- Evidence anchors:
  - [abstract] "The reward function for this process is the subjective musical taste of the user."
  - [section III-C] "User ratings serve as immediate rewards, influencing the agent's decisions. The Bellman's Equation... is applied in each episode to update Q-values."
  - [corpus] Related work "Aligning Generative Music AI with Human Preferences" discusses systematic preference alignment for music generation, suggesting this is an active research direction but not yet standardized.
- Break condition: If user ratings are noisy, inconsistent, or don't reflect true preferences, the learned Q-function encodes noise rather than meaningful aesthetic guidance. User fatigue during long training sessions may degrade rating quality.

### Mechanism 2
- Claim: The discrete action space enables targeted modifications to specific musical elements, allowing iterative refinement rather than wholesale regeneration.
- Mechanism: Six discrete actions (pitch ±1, duration ±0.25, percussion change, note removal) create a bounded modification space. The epsilon-greedy policy (ε=0.5) balances exploring new modifications against exploiting learned high-value actions.
- Core assumption: The action space is sufficient to reach musically satisfying states from any initialization.
- Evidence anchors:
  - [section III-A] "The action space A includes discrete alterations to the track array: A={0,1,2,3,4,5}..."
  - [section IV-A-1b] "Fig. 3 displays the exploration-exploitation balance throughout the training process, affirming a reasonable distribution."
  - [corpus] Corpus lacks direct comparison of discrete vs. continuous action spaces for music generation; this remains underexplored.
- Break condition: The action space may be insufficient for complex musical goals (expert users noted limited chord progressions and dynamic expression). Granularity of pitch/duration changes may not capture nuanced musical variations.

### Mechanism 3
- Claim: Music theory constraints on initialization reduce the effective search space, enabling convergence despite the astronomical state space.
- Mechanism: Initial tracks are generated within user-specified constraints (base note, scale type from {major, minor, diminished}, track length). Pitches are randomly selected only from the generated scale rather than all possible notes.
- Core assumption: Valid musical compositions are more likely within theory-constrained regions of the state space.
- Evidence anchors:
  - [section III-A] "A scale is then generated based on these parameters... Melodic note pitches are randomly selected from this scale."
  - [section V-A-2] "For an 8-note track... P = 7^8 × 4 × 2^4 × 4 = 1.511208e12... These statistics underscore the magnitude of the challenge."
  - [corpus] "Representing Classical Compositions through Implication-Realization Temporal-Gestalt Graphs" discusses alternative structural representations that could further constrain search.
- Break condition: Constraints that are too loose leave search intractable; constraints too tight may limit creative novelty. The paper notes convergence remains an open question for this state space scale.

## Foundational Learning

- Concept: **Tabular Q-Learning and Bellman Equation**
  - Why needed here: The core algorithm updates Q(s,a) values using Q(s,a) ← Q(s,a) + α[R(s,a) + γ·max(Q(s',a')) - Q(s,a)]. Understanding temporal difference learning and value propagation is essential.
  - Quick check question: Given α=0.1, γ=0.9, current Q(s,a)=5, reward R=7, and max(Q(s',a'))=6, what is the updated Q(s,a)? (Answer: 5 + 0.1×[7 + 0.9×6 - 5] = 5 + 0.1×[7 + 5.4 - 5] = 5.74)

- Concept: **Epsilon-Greedy Exploration**
  - Why needed here: With ε=0.5, the agent randomly explores half the time. Understanding exploration-exploitation tradeoffs is critical for diagnosing learning behavior.
  - Quick check question: After 100 steps with ε=0.5, approximately how many actions were random exploration vs. greedy exploitation? (Answer: ~50 each, assuming stationary epsilon)

- Concept: **Markov Decision Process (MDP) Formulation**
  - Why needed here: The problem is framed as MDP(S, A, P, R). Understanding what constitutes states, actions, and rewards in this context is prerequisite to modifying the architecture.
  - Quick check question: In this system, what represents the state s? (Answer: The track array = [melody array, percussion array])

## Architecture Onboarding

- Component map:
  User Input (GUI) → MusicGenerator → Track Array (state) → HITL_RL_Agent
         ↑                                              ↓
         └──────── User Rating (reward) ←──────── Playback

- Critical path:
  1. User specifies: base note, scale type, track length, tempo
  2. MusicGenerator initializes track array from scale-constrained randomization
  3. Agent applies epsilon-greedy action selection → modifies track array
  4. Modified track plays back → user rates (1-10)
  5. Q-value update via Bellman equation with α=0.1, γ=0.9
  6. Repeat for minimum 10 episodes (user can extend)

- Design tradeoffs:
  - **Tabular Q-learning vs. Deep RL**: Tabular is interpretable and computationally light but faces state space explosion. Paper notes ~1.5 trillion possible states for 8-note tracks.
  - **Discrete action space**: 6 actions enable targeted edits but experts noted insufficient expressiveness for chords/dynamics.
  - **Epsilon=0.5**: Fixed exploration rate simplifies implementation but may over-explore in later training. Paper suggests epsilon tuning as future work.
  - **No data dependency**: Eliminates copyright concerns but requires more user interaction to reach quality.

- Failure signatures:
  - **Non-convergence**: Q-values continue fluctuating after many episodes (monitor Q-value stability over time)
  - **User fatigue**: Declining rating quality or engagement after extended sessions
  - **Coherence collapse**: Generated tracks sound disjointed; coherence metric ratings trend low
  - **Mode collapse**: Agent repeatedly selects same actions regardless of state (check exploration ratio in logs)
  - **State space explosion**: New states rarely revisit existing Q-table entries, preventing learning accumulation

- First 3 experiments:
  1. **Baseline validation**: Run 10 episodes with ε=0.5, track rating trajectory. Confirm upward trend matches Figure 2. Log Q-value variance to assess convergence behavior.
  2. **Epsilon sensitivity**: Compare ε ∈ {0.3, 0.5, 0.7} over fixed episode count. Measure final rating distribution and action diversity. Hypothesis: Lower ε converges faster but may miss better optima.
  3. **State representation ablation**: Test alternative state encodings (e.g., interval-from-base-note instead of absolute pitch). Track Q-table size growth and convergence speed. Hypothesis: Relative representation reduces state space and accelerates learning (per Section V-C-3 suggestion).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would a compound reward function incorporating musicality, coherence, and novelty ratings produce compositions that better align with expert standards than the current single 1–10 rating?
- Basis in paper: [explicit] Section III-D states: "exploring a reward function incorporating musicality, coherence, and novelty remains a potential avenue for future experimentation"; Section V-C-4 elaborates on this direction.
- Why unresolved: The current reward is a single subjective rating; the authors designed evaluation metrics (musicality, novelty, coherence) but used them only for post-hoc assessment, not as training rewards.
- What evidence would resolve it: A comparative experiment where one agent uses the single-rating reward and another uses a weighted compound reward; evaluate outputs on the three metrics with statistical testing.

### Open Question 2
- Question: Can alternative state space representations (e.g., encoding pitches relative to the base note) reduce the state space sufficiently to accelerate Q-value convergence?
- Basis in paper: [explicit] Section V-C-3: "experiment with a novel representation of the track array, one that is relative to the base note or employs another form of representation. Such an approach could potentially reduce the size of the state space."
- Why unresolved: The current track-array-as-state yields ~1.5×10^12 permutations (Section V-A-2), making convergence slow and hyperparameter selection difficult.
- What evidence would resolve it: Measure convergence speed (episodes to stable Q-values) and final policy quality under different state encodings on identical generation tasks.

### Open Question 3
- Question: How do alternative exploration strategies (e.g., softmax, Upper Confidence Bound) compare to epsilon-greedy in balancing composition diversity and quality?
- Basis in paper: [explicit] Section V-A-3: "evaluating the impact of using other exploration strategies is another area for potential study."
- Why unresolved: Only epsilon-greedy (ε=0.5) was tested; the authors note fine-tuning epsilon is an open problem (Section II-B, V-A-3).
- What evidence would resolve it: Run controlled comparisons of exploration strategies, reporting both exploration-exploitation balance and user-rated musicality/novelty/coherence across episodes.

## Limitations

- State space complexity (~1.5 trillion possible 8-note tracks) creates severe scalability concerns
- Evaluation methodology relies entirely on subjective user ratings from only 13 participants (3 experts)
- Discrete action space insufficient for expressing complex musical elements like chord progressions and dynamic variations
- Requires continuous user interaction and feedback for every generation, limiting true autonomy

## Confidence

- **High Confidence**: The fundamental RL mechanism (tabular Q-learning with epsilon-greedy exploration) is well-established and correctly implemented. The theoretical framework for using user ratings as rewards is sound.
- **Medium Confidence**: The claim of steady improvement in training episode quality is supported by the limited user study, but the small sample size (13 users) and subjective rating scale (1-5) limit generalizability. The scalability concerns regarding state space explosion are acknowledged but not empirically tested beyond 8-note tracks.
- **Low Confidence**: The assertion that the approach "successfully demonstrates autonomous music generation" is overstated given that it requires continuous user interaction and feedback for every generation. The novelty claim is weakened by related work in preference-aligned music generation that the authors do not adequately address.

## Next Checks

1. **State Space Scalability Test**: Systematically measure Q-table growth and learning efficiency for increasing track lengths (8, 12, 16, 20 notes). Plot state coverage percentage and convergence time to empirically validate scalability claims.

2. **Expert User Study Replication**: Conduct a larger-scale study with ≥50 participants including ≥10 music theory experts. Implement blinded evaluation where experts rate compositions generated by HITL RL versus baseline methods (random generation, rule-based composition) to establish relative performance.

3. **Action Space Expressiveness Benchmark**: Extend the action space to include chord progression modification and dynamic expression controls. Compare user ratings and learning efficiency between the original 6-action system and the expanded action set across identical training episodes.