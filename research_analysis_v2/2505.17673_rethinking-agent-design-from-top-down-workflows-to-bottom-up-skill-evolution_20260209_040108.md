---
ver: rpa2
title: 'Rethinking Agent Design: From Top-Down Workflows to Bottom-Up Skill Evolution'
arxiv_id: '2505.17673'
source_url: https://arxiv.org/abs/2505.17673
tags:
- skill
- agents
- agent
- bottom-up
- skills
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a bottom-up agent paradigm that contrasts
  with the prevalent top-down approach, where humans define workflows and assign agents
  to execute tasks. The bottom-up method allows agents to acquire skills autonomously
  through a trial-and-reasoning mechanism, exploring, reflecting on outcomes, and
  abstracting skills over time.
---

# Rethinking Agent Design: From Top-Down Workflows to Bottom-Up Skill Evolution

## Quick Facts
- **arXiv ID:** 2505.17673
- **Source URL:** https://arxiv.org/abs/2505.17673
- **Reference count:** 40
- **Primary result:** Agents learn to play open-ended games from raw pixels and mouse actions without prior APIs, achieving high skill library growth and >98% execution response rate.

## Executive Summary
This paper proposes a bottom-up agent paradigm that contrasts with the prevalent top-down approach of defining workflows and assigning agents to execute tasks. The bottom-up method allows agents to acquire skills autonomously through a trial-and-reasoning mechanism, exploring, reflecting on outcomes, and abstracting skills over time. The approach leverages LLM reasoning to enable agents to learn from experience rather than from curated human data. The paper evaluates this paradigm in two open-ended games, Slay the Spire and Civilization V, where agents perceive through raw visual inputs and act via mouse outputs, the same as human players. Using a unified, game-agnostic codebase without any game-specific prompts or privileged APIs, the bottom-up agents acquire skills entirely through autonomous interaction.

## Method Summary
The paper introduces a bottom-up skill evolution approach where agents start with no prior knowledge and learn through autonomous interaction with the environment. The method uses GPT-4o for reasoning and vision, Segment Anything Model (SAM) for UI element identification, and a skill library that evolves through trial-and-reasoning. The algorithm consists of three main phases: augmentation (random exploration to build initial skills), invocation (LLM-guided skill selection using MCTS), and refinement (VLM-based evaluation of skills through semantic comparison of pre/post screenshots). The approach is evaluated on two open-ended games, Slay the Spire and Civilization V, using raw pixel inputs and mouse/keyboard outputs without game-specific APIs or priors.

## Key Results
- In Civilization V, the agent learns to build armies, attack City-States, and establish new cities, completing 50 turns and unlocking 8 technologies under a zero-prior setting.
- In Slay the Spire, the agent clears 13 floors and achieves a 98.6% execution response rate.
- The bottom-up agents demonstrate superior learning capabilities compared to top-down approaches in complex, open-ended environments.

## Why This Works (Mechanism)
The bottom-up approach works by allowing agents to learn through direct interaction with the environment rather than following pre-defined workflows. Agents explore the environment randomly when no skills exist, then refine and build upon successful actions through reasoning about visual outcomes. The LLM provides reasoning capabilities to evaluate and select appropriate skills, while MCTS helps optimize skill selection. The VLM-based semantic comparison enables objective evaluation of skill effectiveness through visual change detection.

## Foundational Learning
1. **Trial-and-Reasoning Mechanism** - Agents explore randomly, then use LLM reasoning to evaluate and refine skills based on visual outcomes. Needed because traditional supervised learning requires curated datasets that don't exist for open-ended environments. Quick check: Can the agent distinguish between skills that produce meaningful visual changes versus those that don't?

2. **Zero-Prior Skill Acquisition** - Agents start with empty skill libraries and build competence through interaction. Needed to demonstrate true autonomous learning without relying on pre-trained knowledge or human demonstrations. Quick check: Does the skill library grow from empty to containing useful skills within the first 100 steps?

3. **Visual Grounding for Skill Evaluation** - Uses VLM semantic comparison of pre/post screenshots to evaluate skill effectiveness. Needed because traditional reward functions may not capture meaningful progress in open-ended environments. Quick check: Can the agent identify which skills lead to progressive game states versus random visual changes?

## Architecture Onboarding

**Component Map:** Game Environment -> SAM Segmentation -> LLM Reasoning -> MCTS Selection -> Skill Execution -> VLM Evaluation -> Skill Library

**Critical Path:** Raw pixel input → SAM segmentation → LLM skill selection → MCTS optimization → Skill execution → VLM evaluation → Skill library update

**Design Tradeoffs:** The approach trades computational cost (high token usage for GPT-4o and VLM evaluations) for autonomy and flexibility. Uses lightweight MCTS rather than deep learning policies to maintain interpretability and avoid training requirements.

**Failure Signatures:** Visual grounding errors (clicking empty screen space), exploration stagnation (skill library remains small), semantic evaluation failures (cannot distinguish meaningful visual changes).

**First Experiments:**
1. Run the agent in a simple environment with clear visual feedback to verify skill library growth.
2. Test the SAM-LLM integration by providing known UI elements and checking if the agent can correctly identify and interact with them.
3. Evaluate the MCTS effectiveness by comparing skill selection success rates with and without MCTS guidance.

## Open Questions the Paper Calls Out
1. **Skill Abstraction and Parameterization:** How can bottom-up agents abstract flat action sequences into callable, parameterized functions without predefined APIs? The authors note that current skills lack functional abstraction and parameterization, functioning via record-and-replay. Under a zero-prior setting, it's unclear which parts of a skill should be parameterized or how to infer valid values from raw interaction data.

2. **Credit Assignment for Latent Strategies:** How can agents perceive and attribute credit to latent strategies that lack immediate visual signals? The paper notes that meaningful behaviors like defensive preparations produce only subtle or delayed visual signals, suggesting the need to combine reinforcement learning techniques with implicit reward shaping.

3. **Standardized Evaluation Protocols:** What protocols are needed to standardize episode resetting and skill evaluation in open-ended environments? The authors identify the lack of reliable resetting mechanisms as a fundamental challenge and state that developing standardized protocols for episode resetting and controlled skill re-evaluation will be critical.

## Limitations
- High computational cost due to reliance on GPT-4o and VLM evaluations (~$7-$12 per run)
- Results reported without statistical variation, making repeatability assessment difficult
- Performance metrics are task-dependent and don't guarantee generalization to other domains
- Claim of "zero-prior" learning is questionable as the agent still relies on GPT-4o's pre-trained world knowledge

## Confidence
- **High confidence** in the viability of the bottom-up paradigm for skill acquisition in the tested games
- **Medium confidence** in the scalability claim, as only two games are evaluated and no ablation on MCTS depth or VLM thresholds is provided
- **Low confidence** in the claim of "zero-prior" learning because the agent still relies on GPT-4o's pre-trained world knowledge and SAM for UI segmentation

## Next Checks
1. **Statistical Replication:** Run the agent for 10 independent seeds in each game and report mean ± std for floors cleared and turns completed.
2. **Ablation Study:** Remove MCTS and compare skill invocation success rates; test different $R_{semantics}$ thresholds for pruning.
3. **Cross-Domain Transfer:** Apply the same codebase to a third, structurally different game (e.g., Minecraft) to test true game-agnostic behavior.