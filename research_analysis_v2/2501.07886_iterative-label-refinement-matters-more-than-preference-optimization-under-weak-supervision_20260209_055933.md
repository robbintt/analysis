---
ver: rpa2
title: Iterative Label Refinement Matters More than Preference Optimization under
  Weak Supervision
arxiv_id: '2501.07886'
source_url: https://arxiv.org/abs/2501.07886
tags:
- unreliable
- feedback
- human
- demonstrations
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how well current language model post-training
  pipelines perform under unreliable supervision, which is becoming increasingly important
  as language models tackle complex tasks where human supervision is unreliable. The
  authors simulate unreliable demonstrations and comparison feedback using small language
  models and time-constrained human annotators.
---

# Iterative Label Refinement Matters More than Preference Optimization under Weak Supervision

## Quick Facts
- **arXiv ID:** 2501.07886
- **Source URL:** https://arxiv.org/abs/2501.07886
- **Reference count:** 40
- **Primary result:** Iterative label refinement (ILR) outperforms direct preference optimization (DPO) under unreliable supervision for post-training language models on math, coding, and safe instruction-following tasks.

## Executive Summary
This paper investigates how well current language model post-training pipelines perform under unreliable supervision, which is becoming increasingly important as language models tackle complex tasks where human supervision is unreliable. The authors simulate unreliable demonstrations and comparison feedback using small language models and time-constrained human annotators. They find that while supervised fine-tuning (SFT) with unreliable demonstrations retains some effectiveness, direct preference optimization (DPO), a common reinforcement learning from human feedback (RLHF) algorithm, fails to improve the model beyond SFT. To address this, the authors propose iterative label refinement (ILR), which improves the SFT data by using comparison feedback to decide whether human demonstrations should be replaced by model-generated alternatives, then retrains the model via SFT on the updated data. SFT+ILR outperforms SFT+DPO on several tasks with unreliable supervision (math, coding, and safe instruction-following), suggesting that directing comparison feedback towards improving the training data rather than continually training the model may be more effective as we train language models on complex tasks where human supervision is unreliable.

## Method Summary
The paper proposes iterative label refinement (ILR) as an alternative to direct preference optimization (DPO) under unreliable supervision. ILR splits the SFT dataset into two halves, trains separate SFT models on each half, and uses these models to generate proposals for the held-out prompts. Comparison feedback (either from a classifier trained on ground truth vs. intermediate checkpoint outputs, or from humans) determines whether proposals should replace original labels. At most α fraction of labels are updated per iteration (α=0.15), and the model is retrained from scratch on the refined dataset. This process repeats for K rounds. The key insight is that ILR redirects comparison feedback toward data refinement rather than direct model optimization, avoiding the overoptimization problem that plagues DPO under unreliable supervision.

## Key Results
- SFT with unreliable demonstrations shows weak-to-strong generalization, outperforming the unreliable supervisor across all tasks
- DPO fails to improve over SFT under unreliable comparison feedback due to a regularization dilemma between preventing overoptimization and enabling sufficient model updates
- SFT+ILR consistently outperforms SFT+DPO across math (GSM8K), coding (BIRD), and safe instruction-following (SaferPaca) tasks under unreliable supervision
- ILR achieves higher KL divergence from the initial SFT model than DPO while avoiding overoptimization

## Why This Works (Mechanism)

### Mechanism 1: Weak-to-Strong Generalization in SFT
Models trained via SFT on unreliable demonstrations can outperform their unreliable supervisors. A larger pretrained language model, when finetuned on imperfect demonstrations, filters out some noise and generalizes beyond the errors in its training data, producing outputs that are more reliable than the demonstrations themselves. This assumes the pretrained model has sufficient capacity and prior knowledge to distinguish signal from noise in the unreliable labels. Evidence shows SFT models consistently outperform unreliable supervisors across tasks, and humans moderately prefer model-generated outputs over original human demonstrations for >20% of questions during ILR.

### Mechanism 2: DPO's Regularization Dilemma Under Unreliable Feedback
DPO fails to improve over SFT when both demonstrations and comparison feedback are unreliable, due to a fundamental tension between preventing overoptimization and enabling sufficient model updates. DPO implicitly regularizes via KL divergence from the SFT model. With unreliable feedback, strong regularization (large β) is needed to prevent overoptimization, but this constrains the model from making the large updates necessary to correct errors learned from unreliable demonstrations. Weak regularization allows larger updates but leads to overoptimization on noisy preferences. Evidence shows DPO improvement under oracle feedback with weak regularization, but unreliable feedback requires strong regularization that limits gains.

### Mechanism 3: ILR Enables Large Model Updates Without Overoptimization
ILR achieves larger KL divergence from the initial SFT model than DPO while avoiding overoptimization, by redirecting comparison feedback toward data refinement rather than direct model optimization. ILR uses comparison feedback to selectively replace unreliable demonstrations with model-generated proposals. Each iteration retrains the model from scratch on refined data, allowing large changes without accumulating optimization errors. The cross-labeling split ensures proposals are generated on held-out prompts, reducing memorization of training errors. Evidence shows SFT label accuracy increases across rounds during ILR, and ILR models exhibit significantly higher KL divergence from initial SFT compared to DPO.

## Foundational Learning

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed here: The paper positions ILR as an alternative to DPO under unreliable supervision. Understanding DPO's objective—optimizing an implicit reward model using preference pairs without training an explicit reward model—is essential to grasp why it struggles with noisy feedback.
  - Quick check question: Can you explain why DPO uses KL regularization and what the β hyperparameter controls?

- **Concept: Weak-to-Strong Generalization (W2SG)**
  - Why needed here: W2SG explains why SFT on unreliable data still works and enables ILR's cross-labeling to generate proposals that may exceed the quality of original labels. This phenomenon is central to the paper's empirical observations.
  - Quick check question: Why might a larger model finetuned on noisy labels outperform the smaller model that generated those labels?

- **Concept: Model Collapse**
  - Why needed here: The paper explicitly warns that naive replacement of labels with model outputs (without curation via comparison feedback) leads to performance degradation, consistent with model collapse observed when training on synthetic data.
  - Quick check question: What happens if you repeatedly train a generative model on its own outputs without external supervision or curation?

## Architecture Onboarding

- **Component map:** Unreliable demonstration generator (p̃) -> SFT initial model -> Cross-labeling split -> Two half-data SFT models -> Proposal generation -> Comparison feedback provider (q̃) -> Label refinement -> Retrained SFT model

- **Critical path:** Generate unreliable demonstrations → SFT initial model → split data → train two half-data models → cross-generate proposals → collect comparison feedback → accept proposals with >50% preference → update at most α fraction of labels → retrain SFT from scratch on refined data → repeat for K rounds

- **Design tradeoffs:**
  - α (refinement rate): Too small slows improvement; too large risks instability. Paper finds 0.15 robust across tasks.
  - Cross-labeling vs. single model: Cross-labeling avoids memorization of training errors on seen prompts, but doubles training cost per round.
  - DPO vs. ILR: DPO is computationally cheaper but fails under unreliable supervision; ILR requires full SFT retraining each round but handles noise better.
  - Proposal filtering: Only collect feedback for proposals sufficiently different from originals (different answer/execution result/embedding distance).

- **Failure signatures:**
  - Naive ILR (no feedback): Direct replacement without comparison feedback causes steady degradation—diagnose by checking if label accuracy decreases over rounds.
  - DPO plateau or decline: With unreliable feedback and insufficient β, overoptimization causes accuracy drop; with excessive β, updates are too small to help.
  - W2SG failure: If SFT model does not outperform unreliable supervisor, check model size gap, data quality, or task complexity.

- **First 3 experiments:**
  1. Replicate GSM8K 2B→7B setting: Train p̃ (2B) on half of GSM8K train set; generate unreliable labels for other half. SFT 7B on unreliable labels. Run 4 rounds of ILR vs. DPO. Compare accuracy curves.
  2. Ablate refinement rate α: Run ILR with α ∈ {0.05, 0.15, 0.30} on GSM8K. Plot model accuracy and label accuracy over rounds. Verify 0.15 is near-optimal.
  3. Test naive replacement baseline: Replace α fraction of labels with model proposals without comparison feedback. Confirm performance degrades, establishing necessity of curation.

## Open Questions the Paper Calls Out
None

## Limitations
- Empirical scope limited to three tasks (math, coding, safe instruction-following) with synthetic unreliability that may not fully capture real-world human annotation errors
- Cross-labeling split requires training two separate models per round, doubling computational costs
- Reliance on comparison feedback assumes annotators can reliably identify better responses >50% of the time, which may not hold for highly subjective tasks
- Does not explore what happens when comparison feedback quality degrades over multiple ILR rounds or with aggressive refinement rates

## Confidence

- **High confidence:** SFT's robustness to unreliable demonstrations (W2SG phenomenon), DPO's failure under unreliable supervision, and ILR's effectiveness with moderate α (0.15) are well-supported by controlled experiments across multiple tasks.
- **Medium confidence:** The theoretical explanation for DPO's regularization dilemma (KL-regularized RLHF overoptimization with noisy feedback) is plausible but relies on extrapolation from classification settings and requires more direct analysis of reward model behavior.
- **Medium confidence:** The necessity of cross-labeling (two half-data models) versus single-model proposals is demonstrated empirically but lacks ablation studies showing the exact contribution of this design choice versus other ILR components.

## Next Checks

1. Test ILR with human comparison feedback: Replace the synthetic classifier q̃ with time-constrained human annotators on a subset of prompts. Measure whether the >50% preference threshold for better responses holds in practice and whether label accuracy continues to improve across ILR rounds.

2. Ablate cross-labeling design: Run ILR with single-model proposals (no cross-labeling split) on GSM8K. Compare model accuracy and label accuracy trajectories to the standard cross-labeling approach to quantify the benefit of holding out prompts during proposal generation.

3. Test aggressive refinement rates: Run ILR with α ∈ {0.30, 0.50, 0.70} on BIRD. Plot model accuracy and label accuracy over rounds to identify the point where aggressive updates cause instability or model collapse, establishing practical bounds on refinement rate.