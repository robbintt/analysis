---
ver: rpa2
title: 'Synthetic Data Blueprint (SDB): A modular framework for the statistical, structural,
  and graph-based evaluation of synthetic tabular data'
arxiv_id: '2512.19718'
source_url: https://arxiv.org/abs/2512.19718
tags:
- data
- synthetic
- metrics
- fidelity
- statistical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper tackles the fragmented, ad\u2011hoc evaluation of synthetic\
  \ tabular data, which hampers reproducibility and cross\u2011domain comparison.\
  \ It introduces Synthetic Data Blueprint (SDB), a modular Python library that (i)\
  \ automatically detects feature types, (ii) computes distribution\u2011level fidelity\
  \ metrics (KS, KLD, JSD, Wasserstein) and inter\u2011feature dependency scores,\
  \ (iii) adds graph\u2011 and embedding\u2011based structural preservation measures,\
  \ and (iv) generates standardized visual reports."
---

# Synthetic Data Blueprint (SDB): A modular framework for the statistical, structural, and graph-based evaluation of synthetic tabular data

## Quick Facts
- **arXiv ID:** 2512.19718  
- **Source URL:** https://arxiv.org/abs/2512.19718  
- **Reference count:** 40  
- **Primary result:** SDB provides a fully automated, modular pipeline that generates comprehensive statistical, dependency, and graph‑based quality scores for mixed‑type synthetic tabular data, eliminating ad‑hoc scripting and enabling reproducible cross‑domain benchmarking.

## Executive Summary
Synthetic tabular data are increasingly used to share sensitive information while preserving privacy, yet the community lacks a standardized, reproducible evaluation framework. The authors introduce Synthetic Data Blueprint (SDB), a Python library that automatically detects column types, computes a suite of distribution‑level fidelity metrics (KS, KLD, JSD, Wasserstein), assesses inter‑feature dependencies, and adds graph‑ and embedding‑based structural preservation scores. Applied to three heterogeneous real‑world datasets—clinical diagnostics, socioeconomic‑financial records, and network‑traffic logs—SDB produced complete, comparable quality profiles without manual intervention, demonstrating its suitability for high‑cardinality, high‑dimensional, mixed‑type data.

## Method Summary
SDB follows a four‑stage pipeline. First, a type‑inference engine scans each column and assigns it to categorical, ordinal, continuous, or datetime categories using heuristics on value distribution and metadata. Second, the library computes per‑feature statistical divergences (Kolmogorov‑Smirnov, Kullback‑Leibler, Jensen‑Shannon, Wasserstein) between real and synthetic marginals. Third, it evaluates joint relationships through pairwise mutual information and correlation matrices, and constructs a k‑NN graph on the combined real‑synthetic space to derive embedding‑based structural similarity scores (e.g., graph Laplacian distance). Finally, SDB assembles all metrics into a standardized visual report (tables, heatmaps, and summary plots) that can be exported for downstream comparison. The modular design allows users to plug in alternative metrics or graph constructions with minimal code changes.

## Key Results
- Produced per‑feature divergence tables and global similarity indices for three distinct domains, showing consistent metric behavior across clinical, financial, and network‑traffic data.  
- Demonstrated that SDB’s automatic type detection and metric computation run end‑to‑end without any hand‑crafted preprocessing scripts.  
- Generated reproducible visual reports that enable direct comparison of synthetic data generators across mixed‑type, high‑cardinality datasets.

## Why This Works (Mechanism)
1. **Automatic type detection** ensures that each column is evaluated with the most appropriate statistical distance (e.g., KS for continuous, chi‑square for categorical), preventing metric mismatch that would otherwise bias fidelity scores.  
2. **Distribution‑level divergences** (KS, KLD, JSD, Wasserstein) capture both location and shape differences, providing a multi‑faceted view of how well synthetic marginals replicate real data.  
3. **Graph‑based structural scores** preserve higher‑order relationships by embedding the joint data manifold; similarity of the real‑synthetic graphs reflects preservation of latent dependencies that marginal metrics miss.  
4. **Modular architecture** isolates each evaluation component, allowing independent validation, easy extension, and transparent benchmarking across datasets.

## Foundational Learning
| Concept | Why needed | Quick check |
|--------|------------|-------------|
| Feature‑type inference | Correct metric selection hinges on knowing whether a column is categorical, ordinal, continuous, or datetime. | Can you list the heuristics SDB uses to distinguish a numeric column stored as a string from a true categorical column? |
| Divergence metrics (KS, KLD, JSD, Wasserstein) | Different metrics are sensitive to distinct aspects of distributional change (e.g., tail behavior vs. overall shape). | Given two univariate distributions, which metric would best detect a shift in variance while keeping the mean unchanged? |
| Graph construction (k‑NN) | Captures multivariate relational structure that marginal statistics ignore. | If you increase the k‑value in the k‑NN graph, how does that affect the sensitivity of the structural similarity score? |
| Mutual information for dependency | Quantifies non‑linear inter‑feature relationships that Pearson correlation may miss. | Compute mutual information for a pair of binary variables with a known joint table; does it exceed the absolute Pearson correlation? |
| Visual report generation | Enables rapid human inspection and cross‑study comparison without re‑running code. | Does the generated heatmap correctly align rows/columns with the original feature order? |

## Architecture Onboarding
**Component map**  
type‑detection → statistical‑divergence → dependency‑analysis → graph‑construction → structural‑score → report‑generation  

**Critical path**  
The fastest end‑to‑end run is dominated by the graph‑construction step (k‑NN on the combined dataset), which scales roughly O(N log N) with the number of rows; all preceding steps are linear in the number of columns.

**Design trade‑offs**  
- *Flexibility vs. speed*: Allowing custom metric plug‑ins adds extensibility but can increase runtime if users select computationally heavy distances.  
- *Graph density*: A larger k yields smoother structural scores but incurs higher memory usage; a smaller k is faster but may miss subtle dependencies.  
- *Automatic type detection*: Heuristic‑based inference works out‑of‑the‑box but may misclassify ambiguous columns (e.g., encoded dates).

**Failure signatures**  
- Missing or NaN values in the type‑detection output → downstream metric functions raise “Unsupported column type” errors.  
- Graph‑construction memory error → indicates k is too large for the dataset size.  
- Report‑generation empty tables → suggests that no divergence metrics were computed, often due to all columns being flagged as unsupported.

**First three experiments**  
1. **Baseline sanity check** – Run SDB on the OpenML “adult” dataset (known mixed types) and verify that each column receives the correct inferred type.  
2. **Metric consistency test** – Generate synthetic data with CTGAN, compute KS and Wasserstein scores via SDB, and compare against published CTGAN benchmark values.  
3. **Structural robustness trial** – Vary the k‑NN parameter (k = 5, 10, 20) on a high‑dimensional network‑traffic log and observe the stability of the graph‑based similarity score.

## Open Questions the Paper Calls Out
The manuscript does not explicitly enumerate open research questions. Based on the presented work, the following areas emerge as natural next steps:

- How does SDB’s automatic type detection perform on columns with mixed or ambiguous encodings (e.g., timestamps stored as strings, categorical IDs with numeric values)?  
- What is the sensitivity of the graph‑based structural scores to the choice of distance metric, graph sparsity, and embedding dimensionality?  
- Can the SDB framework be extended to evaluate privacy guarantees (e.g., differential privacy) alongside data fidelity?

## Limitations
- The robustness of automatic type detection to ambiguous or mixed‑type columns is not empirically validated.  
- Structural‑preservation scores depend on graph‑construction hyper‑parameters (k, distance metric) that are not fully disclosed, potentially affecting reproducibility.  
- Cross‑domain consistency claims are based on three datasets; broader validation on additional domains (e.g., genomics, image metadata) is absent.

## Confidence
| Claim | Confidence |
|-------|------------|
| SDB delivers a complete, modular evaluation pipeline | Medium |
| Performance is consistent across clinical, socioeconomic, and network‑traffic data | Low |
| Automatic type detection and visual report generation work out‑of‑the‑box | Low‑Medium |

## Next Checks
1. **Feature‑type detection audit** – Apply SDB to benchmark tables with ground‑truth column types (e.g., OpenML “credit‑g”); compute precision/recall of the inferred types.  
2. **Metric reproducibility test** – Generate synthetic samples with a baseline model (CTGAN), evaluate using SDB’s KS, KLD, JSD, and Wasserstein scores, and compare to the authors’ reported numbers using paired statistical tests (e.g., Wilcoxon signed‑rank).  
3. **Cross‑domain robustness trial** – Run SDB on a completely new high‑dimensional dataset (e.g., a single‑cell RNA‑seq expression matrix) and verify that all modules execute without manual tweaks, producing a full visual report.