---
ver: rpa2
title: Consistency Evaluation of News Article Summaries Generated by Large (and Small)
  Language Models
arxiv_id: '2502.20647'
source_url: https://arxiv.org/abs/2502.20647
tags:
- summaries
- evaluation
- summary
- consistency
- summarization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the factual consistency of summaries generated
  by a variety of models, including small fine-tuned transformers and large language
  models, using both traditional metrics and LLM-powered evaluations. The authors
  introduce a novel meta evaluation approach that directly measures the accuracy of
  the LLM evaluation system itself.
---

# Consistency Evaluation of News Article Summaries Generated by Large (and Small) Language Models

## Quick Facts
- arXiv ID: 2502.20647
- Source URL: https://arxiv.org/abs/2502.20647
- Authors: Colleen Gilhuly; Haleh Shahzad
- Reference count: 40
- Key outcome: This paper investigates the factual consistency of summaries generated by a variety of models, including small fine-tuned transformers and large language models, using both traditional metrics and LLM-powered evaluations. The authors introduce a novel meta evaluation approach that directly measures the accuracy of the LLM evaluation system itself. Across a dataset of news articles, all summarization models produce summaries with high factual consistency, outperforming the human-written reference summaries from the dataset. The results highlight that standard reference-based metrics may not be suitable for comparing modern summarization models, and emphasize the need for reliable evaluation systems that directly assess factual consistency.

## Executive Summary
This paper evaluates the factual consistency of summaries generated by various models (TextRank, T5-small, BART-large, Mistral-7B-Instruct, Llama3-8B-Instruct, Falcon-40B-Instruct, GPT-3.5-Turbo) on the XL-Sum dataset using both traditional metrics and LLM-powered evaluations. A novel meta evaluation framework is introduced to directly assess the accuracy of the LLM evaluation system itself by comparing a text against itself. The key finding is that all summarization models produce summaries with higher factual consistency than the human-written reference summaries, challenging the validity of reference-based metrics for comparing modern summarization models. Fact-checking evaluation, which extracts atomic facts and verifies each against the source, achieves higher accuracy than QA-based evaluation.

## Method Summary
The paper uses the XL-Sum dataset (English test split, filtered to 100-400 word articles, yielding 6,370 pairs) to evaluate summaries generated by various models. Summaries are generated using specified prompts with temperature=0 for reproducibility. Two evaluation approaches are employed: QA evaluation (generating 4 yes/no questions from the summary, answering via summary and source) and fact-checking evaluation (extracting facts from summary, judging TRUE/FALSE vs source). A novel meta evaluation score measures evaluator accuracy by comparing a text against itself. Traditional metrics (ROUGE-1, ROUGE-L, BERTScore) are also computed against both references and source articles.

## Key Results
- All summarization models produce summaries with higher factual consistency than the human-written reference summaries from XL-Sum.
- Fact-checking evaluation achieves higher meta evaluation accuracy (0.87-0.94) than QA-based evaluation (0.83-0.87) across all models.
- Reference-based metrics (ROUGE, BERTScore) fail to distinguish modern summarization models when model outputs exceed reference quality.

## Why This Works (Mechanism)

### Mechanism 1
Meta evaluation scores quantify the accuracy gap in LLM-powered evaluation systems by comparing a text against itself. Since a text is definitionally consistent with itself, any deviation from perfect scores reveals evaluator error. The meta evaluation score = fraction of facts/questions where the evaluator correctly affirms self-consistency.

### Mechanism 2
Fact-checking evaluations achieve higher meta evaluation accuracy than QA-based evaluations because fact extraction requires less transformation than question generation, reducing prompt complexity and failure modes. Questions may miss information coverage; facts enumerated from the summary naturally cover all claims.

### Mechanism 3
Reference-based metrics fail to distinguish modern summarization models when model outputs exceed reference quality because these metrics measure similarity to a fixed reference, not factual accuracy. When model summaries are more factually consistent than human references (which may contain external knowledge), the metric rewards similarity to inconsistent references.

## Foundational Learning

- **Extractive vs. Abstractive Summarization**: Why needed - The paper compares TextRank (extractive) with LLMs (abstractive); understanding the distinction clarifies why hallucination risk differs. Quick check - If a summary copies sentences verbatim from the source, is it extractive or abstractive?
- **Hallucination in LLMs**: Why needed - The entire evaluation framework is designed to detect hallucinations (facts in the summary not grounded in source). Quick check - If a summary correctly states "Paris is in France" but the source article never mentions France, is this a hallucination for a closed summarization task?
- **Reference-free Evaluation**: Why needed - The paper argues for evaluation that does not require human-written summaries, enabling scalable assessment of factual consistency. Quick check - Does ROUGE require a reference summary to compute a score?

## Architecture Onboarding

- **Component map**: Summarization module (TextRank, T5-small, BART-large, or LLM) -> Evaluation module (QA or fact-checking) -> Meta evaluation layer (re-runs evaluator with summary as both source and target) -> Scoring aggregation (consistency/hallucination scores)
- **Critical path**: 1) Generate summaries for corpus (temperature=0), 2) Run fact-checking evaluation (higher meta accuracy), 3) Compute meta evaluation score to quantify evaluator error bounds, 4) If meta score < 0.9, refine prompts before trusting model rankings
- **Design tradeoffs**: QA evaluation provides interpretable failure cases but lower accuracy; fact-checking offers higher accuracy with fewer API calls but coarser granularity; larger evaluator models improve meta scores but increase cost
- **Failure signatures**: Meta score << 1.0 indicates prompt errors; content filter rejections block evaluation on sensitive topics (388 articles rejected); answer key format violations break QA parsing
- **First 3 experiments**: 1) Run fact-checking evaluation on 100 samples; compute meta score and iterate if < 0.85, 2) Compare consistency scores for TextRank vs your model; investigate if your model scores lower, 3) Evaluate your model's summaries against a high-quality human benchmark to establish real-world consistency level

## Open Questions the Paper Calls Out

### Open Question 1
Does fine-tuning summarization models on human-written reference summaries that contain extrinsic information inadvertently encourage model hallucinations? The paper identifies that human summaries in XL-Sum frequently contain information not found in the source text but does not empirically measure whether using such data for fine-tuning propagates this behavior.

### Open Question 2
Does incorporating chain-of-thought prompting improve the accuracy of LLM-based factual consistency evaluations? The authors hypothesize that reflection strategies could bridge the performance gap in their imperfect meta-evaluation scores but did not test this approach.

### Open Question 3
Does restricting the LLM evaluator to answer one question at a time improve the reliability of QA-based consistency metrics? The authors identified this as a potential improvement but did not implement it due to increased time and cost.

### Open Question 4
How does the inclusion of domain-specific context in prompts affect the performance of LLM evaluation systems in specialized fields? The study focused on general news articles, leaving the impact of domain-informed prompts untested.

## Limitations
- The meta evaluation framework assumes that evaluator errors on self-comparison directly translate to errors in cross-document evaluation, which may not fully capture complex failure modes.
- Content filter rejections (388 articles for GPT-3.5-Turbo) suggest potential systematic biases in evaluation based on topic sensitivity.
- The use of XL-Sum references as a baseline is problematic since these references contain external knowledge not present in source articles, making them inherently inconsistent.

## Confidence

- **High confidence**: All summarization models outperform human references in factual consistency
- **Medium confidence**: Fact-checking evaluations achieve higher accuracy than QA-based evaluations
- **Medium confidence**: Reference-based metrics fail to distinguish modern models

## Next Checks

1. Conduct a small-scale human evaluation comparing model summaries to source articles to validate the LLM evaluation framework's accuracy in real-world scenarios.
2. Test the meta evaluation framework with a deliberately flawed evaluator prompt to confirm that low meta scores reliably indicate evaluator problems.
3. Replicate the consistency analysis using a different dataset with verified high-quality references to assess whether the observed patterns hold when references are genuinely consistent with sources.