---
ver: rpa2
title: 'Evaluating Generative AI in the Lab: Methodological Challenges and Guidelines'
arxiv_id: '2601.16740'
source_url: https://arxiv.org/abs/2601.16740
tags:
- genai
- system
- user
- systems
- studies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Generative AI systems produce varied outputs for identical inputs,
  challenging traditional lab-based HCI evaluation methods that assume consistent
  behavior. This study reflects on four lab-based user studies with GenAI-integrated
  prototypes across conversational in-car assistants and image generation tools.
---

# Evaluating Generative AI in the Lab: Methodological Challenges and Guidelines

## Quick Facts
- arXiv ID: 2601.16740
- Source URL: https://arxiv.org/abs/2601.16740
- Reference count: 40
- Primary result: Generative AI's non-deterministic outputs require adapted HCI evaluation methods, with 18 guidelines proposed across 5 challenges.

## Executive Summary
This paper reflects on four lab-based user studies with GenAI-integrated prototypes to identify methodological challenges unique to evaluating non-deterministic AI systems. Through thematic analysis, five challenges were identified: amplified reliance on familiar patterns, fidelity-control trade-offs, redefined feedback and trust loops, gaps in usability evaluation, and interpretive ambiguity. The authors propose 18 recommendations organized into five guidelines to address these challenges, focusing on participant preparation, prototype design, feedback mechanisms, evaluation strategies, and study flexibility. These guidelines aim to support more transparent, robust, and comparable GenAI evaluations in controlled research settings.

## Method Summary
The study uses a multi-case reflective analysis approach, examining four lab-based user studies with GenAI-integrated prototypes across conversational in-car assistants and image generation tools. The authors conducted inductive thematic analysis using affinity diagramming to identify methodological challenges, then developed practice-oriented guidelines. The cases span different GenAI applications: an LLM in-car assistant with 30 drivers, a multimodal LLM navigation assistant with 21 drivers/passengers, a paper-based GenAI image tool prototype with 7 designers, and a functional GenAI image tool with designers/students. Models used include GPT-4o, DALL·E 2/3, GPT-image-1, and Flux.1 Kontext Pro.

## Key Results
- Non-deterministic outputs amplify user reliance on familiar interaction patterns, limiting exploration of novel affordances
- Stochastic outputs redefine feedback loops, undermining user trust in whether inputs were recognized
- Output variability creates interpretive ambiguity between interface design flaws and model behavior, confounding post-hoc analysis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Non-deterministic outputs amplify user reliance on familiar interaction patterns, limiting exploration of novel affordances.
- Mechanism: When GenAI produces variable outputs for identical inputs, users cannot form stable input–output mappings. Without predictable feedback, users default to previously learned strategies rather than exploring new modalities.
- Core assumption: Familiarity-seeking behavior intensifies when feedback is unpredictable rather than merely novel.
- Evidence anchors:
  - [abstract] "GenAI systems are inherently non-deterministic, producing varied outputs even for identical inputs. While this variability is central to their appeal, it challenges established HCI evaluation practices that typically assume consistent and predictable system behavior."
  - [Section 4.1] "GenAI's unpredictability amplifies users' reliance on familiar input modes, reducing their willingness to explore new affordances and constraining what lab studies can reveal about interaction designs." Cases C and D showed designers defaulting to text prompts despite scribble/annotation availability.
  - [corpus] Related work on adaptive systems notes similar familiarity effects, but corpus lacks direct replication of this amplification mechanism in GenAI contexts.
- Break condition: If users have extensive prior experience with the specific GenAI modality, or if onboarding explicitly frames variability as a feature rather than a defect, reliance on familiar patterns may decrease.

### Mechanism 2
- Claim: Stochastic outputs redefine feedback loops, undermining user trust in whether inputs were recognized.
- Mechanism: GenAI's opaque processing and variable responses make it unclear how inputs map to outputs. Users hesitate to explore unfamiliar modalities because they cannot determine whether unexpected results stem from their input, interface design, or model behavior.
- Core assumption: Trust in interactive systems depends on interpretable feedback loops, not just output quality.
- Evidence anchors:
  - [Section 4.3] "In GenAI systems, feedback is not simply less reliable but redefined. Users may hesitate to explore unfamiliar modalities not because of interface flaws, but because feedback can be unstable or ambiguous."
  - [Section 4.3] Case B: conversational hallucinations led participants to question whether miscommunication stemmed from their phrasing or model behavior. Case D: scribble-based prompts yielding mismatched images caused uncertainty about input recognition.
  - [corpus] Weak direct evidence—corpus neighbors focus on data integrity and education, not feedback-trust mechanisms in evaluation.
- Break condition: Real-time feedback cues (e.g., speech-to-text transcription, visual acknowledgment of sketches) may restore trust even under output variability.

### Mechanism 3
- Claim: Output variability creates interpretive ambiguity between interface design flaws and model behavior, confounding post-hoc analysis.
- Mechanism: When usability issues arise, overlapping factors—interface affordances, user strategies, stochastic model outputs—make attribution difficult. Traditional evaluation methods assume consistent system behavior, so observed problems cannot be cleanly traced to interface vs. system causes.
- Core assumption: Accurate causal attribution is necessary for valid evaluation conclusions.
- Evidence anchors:
  - [Section 4.5] "GenAI's unpredictability amplifies existing interpretive challenges in usability evaluation. Overlapping effects from user behavior, interface design, and model responses blur causal boundaries."
  - [Section 5.5, R5.3] The paper recommends "systematically labeling known limitations, such as 'latency > 3s,' 'output failures,' or 'hallucination detected,' within session logs" to create an "audit trail" for analysis.
  - [corpus] No direct corpus support for this specific attribution mechanism.
- Break condition: Fine-grained logging of system events (hallucinations, latency, model switches) combined with qualitative debriefs can partially disentangle interface from system effects.

## Foundational Learning

- Concept: **Non-determinism vs. stochasticity**
  - Why needed here: The paper distinguishes GenAI outputs as stochastic (probabilistic but patterned) rather than arbitrary. Understanding this helps researchers avoid treating variability as random noise to be eliminated.
  - Quick check question: Can you explain why GenAI output variability is described as "stochastic, but not arbitrary" rather than purely random?

- Concept: **Fidelity–control trade-offs in prototyping**
  - Why needed here: Central to Challenge C2. High-fidelity prototypes capture authentic GenAI behavior but introduce uncontrollable variability; low-fidelity prototypes increase experimental control but sacrifice generative qualities.
  - Quick check question: What study characteristics would lead you to choose a paper-based prototype over a fully functional GenAI system?

- Concept: **Triangulation in mixed-methods evaluation**
  - Why needed here: Challenge C4 shows standard usability metrics (SUS, UEQ) fail to separate interface issues from model variability. The paper recommends pairing quantitative measures with qualitative reflections for valid interpretation.
  - Quick check question: If SUS scores are low in a GenAI study, what additional data would help you determine whether the problem is interface design or model behavior?

## Architecture Onboarding

- Component map: The paper structures its contribution as **5 challenges (C1–C5) → 5 guidelines (G1–G5) → 18 recommendations (R1.1–R5.3)**. Challenges represent methodological constructs "amplified, redefined, or newly introduced" by GenAI. Guidelines provide strategic responses; recommendations are actionable tactics. Figure 2 maps recommendations across study phases: planning → prototyping → data collection → analysis.

- Critical path: For a new GenAI evaluation study, apply in order: (1) **G1**—screen for prior experience, design contextual onboarding, offer low-pressure trial phase; (2) **G2**—align fidelity to goals, prepare fallback strategies, log system behavior; (3) **G3**—make feedback loops interpretable, provide real-time input acknowledgment; (4) **G4**—extend metrics with trust/intent-alignment constructs, pair with qualitative reflections; (5) **G5**—pilot test, monitor live, label system limitations in logs.

- Design tradeoffs: **Control vs. realism** (R2.1, R2.2): constraining backend prompts reduces variability but may misrepresent real behavior. **Transparency vs. priming** (R1.1): disclosing "AI-powered" framing may narrow exploration vs. task-focused framing. **Flexibility vs. consistency** (R5.2): allowing retries/skips preserves engagement but complicates cross-participant comparison.

- Failure signatures: (1) Participants default to text-only input despite multimodal options → likely C1/G1 failure, insufficient onboarding or familiarity screening. (2) SUS scores correlate with hallucination frequency rather than interface design → likely C4/G4 failure, need GenAI-specific constructs. (3) Cannot determine if low scores stem from latency, hallucinations, or UI → likely C5/G5 failure, insufficient logging.

- First 3 experiments:
  1. **Pilot with logging**: Before main study, run 3–5 participants with detailed event logging (latency, hallucinations, model switches) to identify which system behaviors most disrupt tasks.
  2. **Onboarding framing A/B test**: Compare two conditions—"AI-powered" framing vs. task-focused framing—to measure impact on modality exploration rates.
  3. **Post-task debrief protocol test**: Add structured questions ("What did you expect?", "What did the system understand?") after each task to surface intent–output mismatches invisible in logs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different fallback strategies (retrying vs. pre-generated outputs) influence participant trust and interaction strategies during GenAI system failures?
- Basis in paper: [explicit] Section 6.4 suggests conducting comparative studies on variables like "fallback strategies (retrying versus pre-generated outputs)" to understand their impact on trust and confidence.
- Why unresolved: The current work reflects on specific case studies where fallbacks were used inconsistently (e.g., Case B lacked them) but does not provide comparative data on their efficacy.
- What evidence would resolve it: A controlled experiment comparing user trust ratings and behavioral data across groups exposed to different fallback mechanisms during system errors.

### Open Question 2
- Question: Does framing onboarding as task-focused rather than AI-focused measurably increase the exploration of novel input modalities?
- Basis in paper: [explicit] Section 6.4 identifies "onboarding framings (AI-highlighted versus task-highlighted instructions)" as a key variable for future comparative studies to refine best practices.
- Why unresolved: While Guideline 1 recommends framing around interaction possibilities, the authors call for direct comparisons to verify when specific designs lead to distinct user behaviors.
- What evidence would resolve it: A study comparing groups receiving different onboarding scripts, measuring the diversity of input modalities used (e.g., text vs. scribble) and exploration breadth.

### Open Question 3
- Question: Do the identified methodological challenges (e.g., reliance on familiar patterns, feedback ambiguity) persist or diminish in longitudinal and in-the-wild deployments?
- Basis in paper: [explicit] The authors acknowledge in Section 6.4 that they "did not study long-term adaptation, in-the-wild use," identifying these as important directions for future work.
- Why unresolved: The lab-based nature of the four case studies limits the generalizability of findings to long-term usage where users might adapt to stochastic behavior.
- What evidence would resolve it: Longitudinal field studies tracking how user strategies and trust evolve over time compared to the baseline lab behaviors reported in Challenges C1 and C3.

## Limitations

- The study's guidelines are based on interpretive thematic analysis rather than empirical validation, limiting confidence in mechanism strength
- Key materials (prompts, onboarding scripts, interview protocols, prototype code) are not publicly available, restricting direct replication
- The four case studies focus on specific GenAI domains (in-car assistants, image generation) without testing generalizability to other applications

## Confidence

- **High** for descriptive claims about methodological challenges (C1–C5) based on observed patterns across four studies
- **Medium** for proposed guidelines (G1–G5) and recommendations (R1.1–R5.3) as they are practice-oriented and logically derived from challenges, though empirical validation is pending
- **Low** for mechanistic explanations (e.g., "amplified reliance on familiar patterns") due to limited direct evidence and reliance on interpretive analysis

## Next Checks

1. Conduct a replication study in a new GenAI domain (e.g., conversational agents or image generation) using the proposed guidelines, comparing evaluation outcomes with and without guideline adherence
2. A/B test onboarding framing (AI-powered vs. task-focused) to measure impact on modality exploration rates and user trust, as suggested in R1.1
3. Implement fine-grained system logging (latency, hallucinations, model switches) in a pilot study to assess whether labeled limitations improve post-hoc attribution of usability issues to interface vs. model causes, as recommended in R5.3