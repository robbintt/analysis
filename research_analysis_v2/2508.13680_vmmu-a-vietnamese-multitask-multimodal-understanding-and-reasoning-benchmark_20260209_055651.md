---
ver: rpa2
title: 'VMMU: A Vietnamese Multitask Multimodal Understanding and Reasoning Benchmark'
arxiv_id: '2508.13680'
source_url: https://arxiv.org/abs/2508.13680
tags:
- question
- vlms
- visual
- reasoning
- vietnamese
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VMMU is a Vietnamese multimodal reasoning benchmark of 2,548 questions
  across 7 domains, designed to test how vision-language models jointly process Vietnamese
  text and visual content. All questions require genuine multimodal integration, not
  just OCR of text-in-image.
---

# VMMU: A Vietnamese Multitask Multimodal Understanding and Reasoning Benchmark

## Quick Facts
- arXiv ID: 2508.13680
- Source URL: https://arxiv.org/abs/2508.13680
- Reference count: 40
- Key outcome: SOTA VLMs achieve 66.76% accuracy on Vietnamese multimodal reasoning, with thinking models outperforming non-thinking models by 15.02%.

## Executive Summary
VMMU is a Vietnamese multimodal reasoning benchmark of 2,548 questions across 7 domains, designed to test how vision-language models jointly process Vietnamese text and visual content. All questions require genuine multimodal integration, not just OCR of text-in-image. VLMs show strong Vietnamese OCR performance (mean BLEU 89.01%, F1 94.30%), but overall accuracy is low (66.76%) because errors stem from multimodal grounding and reasoning. Separating question text from visual evidence improves accuracy for every model (+5.98% on average), with larger gains for non-thinking VLMs. Thinking VLMs perform significantly better, indicating that multimodal reasoning is the main bottleneck. Even without visual evidence, VLMs achieve above-random accuracy (51.47%), suggesting reliance on text-only priors.

## Method Summary
VMMU evaluates zero-shot vision-language model performance on Vietnamese multimodal reasoning and OCR tasks. The benchmark contains 2,548 multiple-choice questions across 7 domains requiring integration of text and visual evidence (diagrams, charts). Two experimental settings are used: Integrated-MM (single images with interleaved Vietnamese text and visuals) and Split-MM (text prompts provided separately with cropped visual evidence). Proprietary models (GPT-4.1, o3, Gemini) and open-source models (Qwen, Gemma) are evaluated using temperature 0 and prompts asking models to return answers in curly brackets (e.g., {A}). OCR quality is measured via BLEU, F1, CER, and WER, while QA accuracy is the primary metric.

## Key Results
- VLMs achieve strong Vietnamese OCR performance (BLEU 89.01%, F1 94.30%) but only 66.76% overall accuracy due to multimodal reasoning challenges
- Separating question text from visual evidence improves accuracy by +5.98% on average across all models
- Thinking VLMs outperform non-thinking VLMs by 15.02%, showing multimodal reasoning is the primary bottleneck
- Even without visual evidence, VLMs achieve 51.47% accuracy, suggesting reliance on text-only priors

## Why This Works (Mechanism)
None

## Foundational Learning
- **Multimodal reasoning**: Understanding how VLMs integrate visual and textual information for complex reasoning tasks. Needed because VMMU explicitly tests genuine multimodal integration beyond simple OCR. Quick check: Can the model correctly answer questions requiring visual evidence interpretation combined with text understanding?
- **Vietnamese language processing**: Handling Vietnamese text with its diacritical marks and specific linguistic structures. Needed because VMMU uses native Vietnamese text requiring proper language understanding. Quick check: Does the model achieve high OCR accuracy (F1 > 90%) on Vietnamese text?
- **Chain-of-thought reasoning**: Internal reasoning processes that thinking VLMs use to solve complex problems. Needed because thinking VLMs show significantly better performance on VMMU. Quick check: Does the model show step-by-step reasoning traces when processing multimodal questions?
- **OCR quality metrics**: BLEU, F1, CER, and WER scores for evaluating text recognition accuracy. Needed because VMMU reports strong OCR performance as a baseline. Quick check: Does the model achieve BLEU > 85% and F1 > 90% on Vietnamese OCR tasks?
- **Multimodal grounding**: The ability to correctly associate textual information with relevant visual elements. Needed because VMMU requires genuine integration of text and visual evidence. Quick check: Can the model correctly identify which visual elements correspond to specific textual references?

## Architecture Onboarding

**Component Map**: VMMU Dataset -> Vision Encoder -> Text Encoder -> Multimodal Fusion -> Reasoning Head -> Answer Output

**Critical Path**: Image Preprocessing -> Vision Encoding -> Text Encoding -> Cross-Attention Fusion -> Multimodal Reasoning -> Answer Selection

**Design Tradeoffs**: 
- Single-image vs. separated-text approaches (Integrated-MM vs Split-MM)
- Thinking vs non-thinking model configurations
- Proprietary vs open-source model choices
- Temperature settings for inference stability

**Failure Signatures**: 
- Low OCR accuracy (<80% F1) breaks reasoning capabilities
- Format extraction failures when models don't output answers in {Z} format
- Catastrophic performance drops when visual evidence is separated from text
- Significant gaps between thinking and non-thinking models (>15%)

**3 First Experiments**:
1. Run OCR-only baseline on 210-question subset to verify reported F1=94.30%
2. Test format extraction robustness on 100 samples to ensure >99% success rate
3. Replicate Split-MM experiment with proxy crops to verify +5.98% accuracy gain

## Open Questions the Paper Calls Out

**Open Question 1**: Does Vietnamese-centric instruction tuning improve multimodal reasoning capabilities more effectively than relying on translated English-centric datasets?
- Basis: Future Directions section states robust reasoning "likely requires Vietnamese-centric instruction tuning and data that target grounding and reasoning skills beyond what English-centric resources provide."
- Why unresolved: Paper shows translating to English reduces accuracy but doesn't test if native Vietnamese training is superior to translated English data.
- Evidence needed: Comparative study fine-tuning models on native VMMU-style data versus translated English multimodal corpora.

**Open Question 2**: What specific mechanisms allow "thinking" VLMs to handle interleaved text and visual evidence better than non-thinking models?
- Basis: Future directions highlight "large gap between thinking and non-thinking VLMs" and suggest "headroom for methods that improve multimodal reasoning" in dense layouts.
- Why unresolved: Paper identifies performance gap but doesn't isolate whether advantage stems from chain-of-thought, attention patterns, or robustness to text-in-image interference.
- Evidence needed: Ablation studies analyzing attention heatmaps and intermediate reasoning traces.

**Open Question 3**: How well does high performance on VMMU's standardized "white-canvas" format correlate with performance on realistic, cluttered, or handwritten Vietnamese documents?
- Basis: Limitations section notes benchmark uses "standardized white-canvas style, which may underrepresent more realistic inputs such as natural photographs, mobile captures, cluttered documents, or handwritten content."
- Why unresolved: Unclear if models succeeding on clean typeset exams possess robustness for real-world visual noise and layout variability.
- Evidence needed: Evaluation on supplementary dataset of "in-the-wild" Vietnamese document photos.

## Limitations
- Benchmark uses standardized white-canvas style, potentially underrepresenting realistic inputs like natural photographs, mobile captures, cluttered documents, or handwritten content
- Unknown reasoning configurations for thinking VLMs (o3, GPT-5) with "reasoning_effort: medium" may cause variance in reproduced scores
- Manual cropping coordinates for Split-MM experiment are unclear, preventing full replication of results showing +5.98% accuracy gains

## Confidence
- **High Confidence**: Claims about VMMU's unique design (genuine multimodal integration), 66.76% baseline accuracy, and finding that separating text from visual evidence improves accuracy across all models
- **Medium Confidence**: Conclusion that "multimodal reasoning is the main bottleneck" based on thinking VLMs' superior performance
- **Low Confidence**: Model-specific performance differences beyond main trends due to sensitivity to unknown reasoning configurations

## Next Checks
1. Validate OCR baseline first by running the OCR-only experiment on the 210-question subset with ground truth transcripts to confirm reported F1=94.30%
2. Test format extraction robustness by logging raw model outputs for at least 100 samples to verify {Z} format extraction rate exceeds 99%
3. Replicate Split-MM with proxy crops by creating your own visual evidence crops based on question text to approximate the experiment and verify +5.98% accuracy gain