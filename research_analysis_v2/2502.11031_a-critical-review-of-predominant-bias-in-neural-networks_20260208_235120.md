---
ver: rpa2
title: A Critical Review of Predominant Bias in Neural Networks
arxiv_id: '2502.11031'
source_url: https://arxiv.org/abs/2502.11031
tags:
- bias
- fairness
- learning
- conference
- type
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper investigates two types of biases in neural networks:
  Type I Bias (uneven performance across demographic groups) and Type II Bias (dependence
  between model predictions and protected attributes). Through a comprehensive review
  of 415 papers, the authors find significant confusion in the literature regarding
  these biases.'
---

# A Critical Review of Predominant Bias in Neural Networks

## Quick Facts
- **arXiv ID:** 2502.11031
- **Source URL:** https://arxiv.org/abs/2502.11031
- **Reference count:** 40
- **Primary result:** Disambiguates Type I Bias (uneven performance across groups) from Type II Bias (dependence between predictions and protected attributes) and shows they are distinct phenomena with different causes.

## Executive Summary
This paper addresses the widespread confusion in machine learning literature regarding different types of algorithmic bias. Through a comprehensive review of 415 papers, the authors identify two fundamentally distinct forms of bias: Type I (performance disparities across demographic groups) and Type II (statistical dependence between model predictions and protected attributes). The paper provides formal mathematical definitions for both types and demonstrates through extensive experiments on synthetic, census, and image datasets that these biases are orthogonal phenomena with different real-world manifestations. The study reveals that many existing papers misattribute motivations or implement inappropriate mitigation techniques, and offers recommendations to clarify terminology and improve future research in this field.

## Method Summary
The authors conducted a systematic review of 415 papers on algorithmic bias, then conducted extensive experiments on three types of datasets. For synthetic data, they generated controlled datasets with specific bias properties using 2D data with defined generative distributions. For real-world data, they used the Adult Income Dataset (tabular) and CelebA (images). They trained standard classifiers (fully connected layers for synthetic, 3-layer MLP for Adult, ResNet18 for CelebA) using binary cross-entropy loss. To create different bias scenarios, they manipulated datasets by sub-sampling minority groups for Type I bias and creating "Extreme Bias" splits with controlled conditional entropy for Type II bias. They evaluated both bias types simultaneously on test sets to verify their orthogonality.

## Key Results
- Type I and Type II biases are empirically shown to be orthogonal phenomena that can exist independently of each other
- Many existing papers cite motivations related to one type of bias while implementing mitigation techniques for the other type
- Data imbalance is identified as the primary cause of Type I bias, while spurious correlation between targets and attributes drives Type II bias
- The paper provides a comprehensive mapping of existing fairness criteria to these two bias definitions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Disambiguating bias types via their dependency on ground truth labels ($Y$) may resolve taxonomic confusion in literature.
- **Mechanism:** The paper proposes a formal partition where Type I Bias is defined by uneven performance metrics $d(\hat{Y}, Y | A)$ (requiring ground truth), while Type II Bias is defined by statistical dependence $P(\hat{Y} | A)$ (independent of ground truth). By forcing a check on whether $Y$ is part of the evaluation equation, researchers can categorize problems that previously used identical terminology but referred to fundamentally different phenomena.
- **Core assumption:** The confusion in the field stems primarily from semantic ambiguity where terms like "algorithmic bias" are applied indiscriminately to both performance disparities and spurious correlations.
- **Evidence anchors:**
  - [abstract] "The paper investigates two types of biases... Type I Bias (uneven performance... Type II Bias (dependence between model predictions and protected attributes)."
  - [Page 1, Table I] Explicitly lists "Use of ground truth Y" as the differentiator (Type I: ✓, Type II: ✗).
  - [corpus] Related work *AI Biases as Asymmetries: A Review to Guide Practice* supports the need for evolving bias definitions to guide practice, though it frames bias more broadly as integral to systems rather than strictly error-based.
- **Break condition:** If a scenario exists where "fairness" requires both equalized performance and strict independence simultaneously, this binary classification may fail to capture the multi-objective tradeoff.

### Mechanism 2
- **Claim:** Unifying disparate fairness criteria under two mathematical definitions allows for the identification of "Inaccurate Motivation" in existing research.
- **Mechanism:** By mapping "Equalized Odds" to Type I and "Demographic Parity" to Type II, the review reveals that many papers cite motivations related to one type (e.g., uneven accuracy in face recognition) while implementing mitigation techniques for the other (e.g., removing attribute dependence). The mechanism works by cross-referencing the problem statement against the optimization objective.
- **Core assumption:** A misalignment between the cited social harm (motivation) and the mathematical constraint (method) indicates a flaw in research validity or clarity.
- **Evidence anchors:**
  - [Page 5, Section V.B] "Existing work addressing these two types of bias inaccurately cites each other for their own motivation."
  - [Page 4, Table III] Maps fairness criteria (Equalized Odds vs. Demographic Parity) to the proposed bias definitions.
  - [corpus] *Trustworthy AI: Safety, Bias, and Privacy* highlights the struggle with failure modes, implicitly supporting the need for clearer categorization to address specific vulnerabilities.
- **Break condition:** If a mitigation technique (like adversarial training) effectively reduces both bias types simultaneously, the distinction in motivation might be practically irrelevant for that specific method.

### Mechanism 3
- **Claim:** Separating the underlying causes (data imbalance vs. spurious correlation) enables better diagnosis of model failure modes.
- **Mechanism:** The framework associates Type I primarily with "insufficient training in underrepresented groups" (long-tail distribution) and Type II with "correlation between target $Y$ and attribute $A$." This suggests different interventions: data re-sampling/re-weighting for Type I vs. invariant representation learning/information minimization for Type II.
- **Core assumption:** These causes are distinct and separable; that is, fixing data imbalance will not necessarily fix spurious correlation, and vice versa.
- **Evidence anchors:**
  - [Page 14, Section VIII.A] "Data imbalance... is commonly accepted as the possible cause for Type I Bias."
  - [Page 15, Section VIII.B] "The association between prediction targets and attributes... is widely considered the possible cause of Type II Bias."
  - [corpus] *Ensuring Medical AI Safety* explicitly discusses "shortcut learning" (aligning with Type II/spurious correlation) as distinct from simple performance drops, reinforcing the need for this separation.
- **Break condition:** In datasets where the minority group is perfectly correlated with a specific outcome (intersection of imbalance and spurious correlation), the distinct causal pathways may merge, making it difficult to attribute the bias source.

## Foundational Learning

- **Concept: Conditional Independence vs. Performance Parity**
  - **Why needed here:** The core of the paper rests on distinguishing $P(\hat{Y}|A)$ (Dependence/Type II) from $d(\hat{Y}, Y|A)$ (Performance/Type I). You must understand that a model can have equal accuracy for males and females (Type I fairness) yet still overwhelmingly predict "doctor" for males and "nurse" for females (Type II bias).
  - **Quick check question:** If a model predicts "High Income" for 90% of Group A and 90% of Group B, but is correct 100% of the time for Group A and 0% for Group B, which type(s) of bias are present?

- **Concept: Spurious Correlations (Shortcut Learning)**
  - **Why needed here:** The paper identifies this as the primary driver of Type II bias. Understanding that neural networks optimize for the easiest predictive path (e.g., using background color instead of object shape) explains why removing the attribute $A$ from the input (Fairness Through Unawareness) often fails if proxies exist.
  - **Quick check question:** Why might balancing a dataset (fixing Type I issues) fail to fix a model that relies on a background artifact correlated with the class label?

- **Concept: Fairness Criteria Hierarchy (Group vs. Individual)**
  - **Why needed here:** The paper unifies various "Fairness Criteria" (Equalized Odds, Demographic Parity) under its bias definitions. You need to know that these are not just arbitrary metrics but mathematical constraints that often conflict with one another.
  - **Quick check question:** Does "Demographic Parity" (Type II fairness) require the ground truth label $Y$ to be known during the fairness evaluation?

## Architecture Onboarding

- **Component map:** Input Data ($X, Y, A$) -> Model Core (latent representation $Z$) -> Output Head ($\hat{Y}$) -> Evaluation Module (Performance Disparity vs. Prediction Disparity)
- **Critical path:**
  1. **Define the Constraint:** Determine if the application demands even performance (Type I, e.g., biometrics) or decision independence (Type II, e.g., loan approval).
  2. **Audit Dataset:** Calculate $P(A)$ for imbalance and $P(Y|A)$ for correlation.
  3. **Select Metric:** Use Acc/TPR/FPR differences for Type I; use Calders-Verwer or Demographic Parity Distance for Type II.
- **Design tradeoffs:**
  - **Accuracy vs. Independence:** Enforcing Type II fairness (independence) often degrades overall accuracy if $A$ is genuinely predictive of $Y$ in the training data.
  - **Incompatible Metrics:** The paper notes that metrics like Accuracy Difference are "not suitable" for Type II bias because they conflate the model's dependence on $A$ with the inherent difficulty of the task.
- **Failure signatures:**
  - **Metric Mismatch:** Reporting "Equalized Odds" to claim mitigation of a Type II bias problem (e.g., "removing gender dependence").
  - **Synthetic Artifacting:** Generating synthetic data to balance groups (Type I) but inadvertently amplifying the correlation between $A$ and $Y$ (worsening Type II).
  - **Zero-Sum Mitigation:** Aggressively debiasing for Type II resulting in a significant performance drop for a minority group (inducing Type I).
- **First 3 experiments:**
  1. **Synthetic Isolation:** Train a classifier on synthetic data where $A$ is imbalanced but uncorrelated with $Y$ (Type I only). Verify if standard debiasing improves accuracy parity but leaves prediction rates unchanged.
  2. **Correlation Injection:** Train on a dataset where $A$ is balanced but highly correlated with $Y$ (Type II only). Verify if the model uses $A$ as a shortcut, and check if "fairness through unawareness" (dropping $A$) actually works (it often shouldn't if proxies exist).
  3. **Metric Audit:** Apply both Accuracy Difference (Type I) and Demographic Parity Distance (Type II) to a standard model on the CelebA dataset. Confirm empirically that minimizing one does not necessarily minimize the other, validating the paper's distinction.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What are the fundamental causes of Type I Bias (uneven performance across groups) beyond data imbalance, given that it manifests even in balanced datasets?
- **Basis in paper:** [Explicit] Section VIII.A states that the "formerly widely accepted cause (data imbalance) has been challenged by the experiment that Type I Bias exists even for a balanced dataset," and identifies investigating root causes as a future direction.
- **Why unresolved:** Current literature predominantly attributes this bias to long-tail distributions (underrepresentation). However, the persistence of performance disparities in perfectly balanced settings implies other factors are at play that are currently undefined or misunderstood.
- **What evidence would resolve it:** Empirical or theoretical results identifying specific non-distributional factors (e.g., feature complexity, intrinsic subgroup difficulty) that cause performance gaps even when demographic groups have identical sample sizes.

### Open Question 2
- **Question:** How can debiasing methods be improved to effectively handle the "strong bias region" of Type II Bias where training data contains extreme correlations between targets and protected attributes?
- **Basis in paper:** [Explicit] Section VIII.B suggests exploring the "strong bias region... where the target and the attribute are strongly associated in the training set," noting this scenario is currently "overlooked by many existing work."
- **Why unresolved:** Standard mitigation techniques may fail or become unstable when the spurious correlation in the training data approaches determinism, a common edge case in real-world data collection.
- **What evidence would resolve it:** Development of algorithms that successfully achieve independence between predictions and attributes on benchmark datasets specifically constructed with extreme (near 100%) target-attribute correlation.

### Open Question 3
- **Question:** What methodologies can effectively mitigate Type II Bias in scenarios where protected attributes are absent, unlabeled, or completely unknown?
- **Basis in paper:** [Explicit] Section VIII.B emphasizes the importance of exploring "more challenging scenarios where attribute labels are absent... or unknown biases emerge," listing it as a key future direction.
- **Why unresolved:** Most Type II debiasing relies on explicit knowledge of the protected attribute for supervision or adversarial training. Removing this requirement necessitates unsupervised discovery of bias structures, which remains a difficult open problem.
- **What evidence would resolve it:** Techniques that can identify and mitigate latent biases without ground-truth attribute labels, validated by improved fairness metrics on datasets where attributes are hidden during training but available for auditing.

## Limitations

- The paper's distinction between bias types relies on the assumption that data imbalance and spurious correlation are fundamentally separable causes, which may not hold in complex real-world scenarios where these mechanisms are intertwined.
- The claim that accuracy-based metrics are "not suitable" for Type II bias evaluation could be overly restrictive in practical applications where the relationship between accuracy and decision fairness is context-dependent.
- The analysis is primarily based on synthetic datasets and controlled scenarios, which may not fully capture the complexity and nuance of real-world bias manifestations.

## Confidence

- **High Confidence:** The mathematical formalization of Type I and Type II biases as distinct phenomena based on their dependence on ground truth labels is well-supported and internally consistent.
- **Medium Confidence:** The empirical demonstration that these bias types are orthogonal in practice, while compelling, relies on synthetic scenarios that may not fully capture the complexity of real-world datasets.
- **Medium Confidence:** The claim that existing literature suffers from "inaccurate motivation" in citing bias types is supported by the review but may overstate the prevalence of deliberate misalignment versus genuine confusion.

## Next Checks

1. **Real-World Intersection Analysis:** Test whether the proposed bias types remain orthogonal in real-world datasets where data imbalance and spurious correlations may coexist (e.g., medical imaging where certain conditions are both rare and culturally associated with specific demographics).

2. **Intervention Transferability:** Evaluate whether mitigation techniques designed for one bias type (e.g., re-weighting for Type I) inadvertently introduce or exacerbate the other type (Type II), validating the claim of distinct causal mechanisms.

3. **Metric Applicability Spectrum:** Conduct a systematic evaluation of accuracy-based metrics across diverse Type II bias scenarios to determine if the blanket statement about their unsuitability holds universally or is context-dependent.