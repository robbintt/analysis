---
ver: rpa2
title: 'GABFusion: Rethinking Feature Fusion for Low-Bit Quantization of Multi-Task
  Networks'
arxiv_id: '2511.05898'
source_url: https://arxiv.org/abs/2511.05898
tags:
- quantization
- feature
- quantized
- gradient
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses performance degradation in low-bit quantization
  of multi-task networks like YOLO due to task-specific feature discrepancies and
  gradient conflicts. The authors propose GABFusion, which introduces learnable scaling
  factors before feature fusion and a layer normalization post-fusion to balance gradients
  across branches.
---

# GABFusion: Rethinking Feature Fusion for Low-Bit Quantization of Multi-Task Networks

## Quick Facts
- arXiv ID: 2511.05898
- Source URL: https://arxiv.org/abs/2511.05898
- Reference count: 10
- Primary result: Average mAP improvements of 3.3% on PASCAL VOC and 1.6% on COCO for low-bit quantized multi-task detectors

## Executive Summary
This paper addresses performance degradation in low-bit quantization of multi-task networks like YOLO due to task-specific feature discrepancies and gradient conflicts. The authors propose GABFusion, which introduces learnable scaling factors before feature fusion and a layer normalization post-fusion to balance gradients across branches. They also introduce Attention Distribution Alignment (ADA), a feature-level distillation strategy that aligns attention distributions between quantized and full-precision models. Experiments on PASCAL VOC and COCO datasets show average mAP improvements of 3.3% and 1.6% respectively. On YOLOv5 under 4-bit quantization, the accuracy gap with full-precision models is reduced to only 1.7% on VOC. The method is modular, architecture-agnostic, and compatible with any existing QAT technique.

## Method Summary
GABFusion addresses gradient imbalance in low-bit quantized multi-task networks by inserting learnable scaling factors (α) before feature fusion and LayerNorm after fusion. The scaling factors adjust the contribution of shallow and deep branches, while LayerNorm redistributes gradients across concatenated channels. Additionally, Attention Distribution Alignment (ADA) uses SimAM to compute attention maps and minimizes JS divergence between quantized and full-precision models. The method is compatible with any QAT technique and can be inserted at any concatenation or fusion node in multi-branch architectures.

## Key Results
- Average mAP improvements of 3.3% on PASCAL VOC and 1.6% on COCO
- Reduces YOLOv5 4-bit quantization accuracy gap to 1.7% on VOC
- Effective across multiple QAT methods (PACT, LSQ, N2UQ, DSQ)
- ADA provides additional 0.3-0.6% AP50 gain when combined with GABFusion

## Why This Works (Mechanism)

### Mechanism 1
Low-bit quantization causes asymmetric gradient flow at feature fusion points, where deep branches (classification) dominate shallow branches (regression). Quantization error accumulates across layers via Jacobian transformations, causing deep features to carry larger perturbations. The network prioritizes semantic optimization over spatial precision, degrading regression tasks.

### Mechanism 2
Learnable scaling factors (α) combined with post-fusion LayerNorm rebalance gradients across branches without altering inference architecture. Pre-fusion scaling lets the optimizer adjust feature contributions, while LayerNorm redistributes gradients by subtracting the mean and dividing by shared σ, effectively "whitening" gradients across concatenated channels.

### Mechanism 3
Attention Distribution Alignment (ADA) provides feature-level supervision that recovers spatial attention patterns lost during quantization. SimAM computes parameter-free attention weights based on activation distinctiveness, and minimizing JS divergence between full-precision and quantized attention guides the model to preserve task-relevant feature saliency.

## Foundational Learning

- **Quantization-Aware Training (QAT) and Straight-Through Estimator (STE)**: Essential for understanding how STE approximates gradients through discrete quantization and why gradient imbalances emerge. Quick check: Can you explain why STE causes gradient oscillation and how this differs from gradient imbalance?

- **Multi-Scale Feature Fusion in Object Detectors (FPN, PANet)**: Critical for understanding why shallow and deep features are fused in YOLO's PANet neck. Quick check: In a Feature Pyramid Network, which scale is typically responsible for small object detection, and why does that matter for quantization?

- **Knowledge Distillation Losses (KL vs JS Divergence)**: Important for interpreting why KL divergence may outperform JS for some QAT methods. Quick check: Why might KL divergence be better suited to bounded activation ranges (PACT) while JS suits methods with nonlinear transformations (N2UQ)?

## Architecture Onboarding

- **Component map**: Backbone (CSPDarknet) -> Neck (PANet with Concat layers) -> Head (Detection heads). GABFusion is inserted at each Concat node.

- **Critical path**: 1) Identify all Concat or fusion nodes in target architecture. 2) Insert α scaling on each incoming branch. 3) Insert LayerNorm immediately after concatenation. 4) Attach ADA loss at selected feature maps. 5) Train with standard QAT.

- **Design tradeoffs**: α initialization (0.5 neutral), LayerNorm position (post-fusion for gradient redistribution), ADA divergence choice (KL for bounded activations, JS for nonlinear), inference overhead (LayerNorm is training-only).

- **Failure signatures**: No improvement or degradation (incorrect fusion nodes, wrong normalization axis, α collapsing to 0/1, ADA loss weight too high), regression task degrading (α favoring deep features), training instability (LayerNorm statistics unstable).

- **First 3 experiments**: 1) Baseline gradient profiling to confirm imbalance exists. 2) GABFusion-only ablation to isolate gradient balancing contribution. 3) Divergence comparison for ADA across multiple QAT methods.

## Open Questions the Paper Calls Out

### Open Question 1
Can GABFusion be effectively adapted for low-bit quantization of transformer-based unified multi-task architectures? The conclusion states future work could explore applying GABFusion to transformer-based unified multi-task architectures, which remain underexplored in low-bit quantization research.

### Open Question 2
Does the identified gradient conflict between shallow (regression) and deep (classification) branches persist in dense prediction tasks like semantic segmentation? While the authors argue regression tasks suffer most from gradient bias, it is unclear if pixel-wise segmentation tasks exhibit the same magnitude of gradient discrepancy at fusion nodes.

### Open Question 3
Is there a unified theoretical framework for selecting the optimal distillation divergence metric (KL vs. JS) for Attention Distribution Alignment? The ablation study notes that "quantization methods exhibits varying responses to KL and JS divergences," suggesting the choice relies on the specific quantizer's distribution characteristics rather than a general rule.

## Limitations
- Key training hyperparameters (learning rate schedule, distillation weight λ) are deferred to an appendix, creating potential variability in reproduction
- Gradient imbalance diagnostics are shown for YOLO architectures; applicability to non-FPN or non-detector networks is assumed but not validated
- SimAM-based attention as a proxy for task-relevant feature importance is intuitive but lacks external validation beyond the reported ablation

## Confidence
- **High confidence**: Core architectural contributions (GABFusion module with α scaling and LayerNorm) are well-specified and modular
- **Medium confidence**: Gradient imbalance diagnosis is supported by internal measurements but not independently verified
- **Medium confidence**: ADA improvements are measurable in ablation but rely on assumptions about SimAM attention fidelity

## Next Checks
1. **Gradient profiling validation**: Replicate Figure 3 analysis on your target architecture to confirm gradient imbalance exists before applying GABFusion
2. **GABFusion isolation test**: Apply GABFusion without ADA to isolate gradient balancing contribution from attention alignment
3. **Divergence method comparison**: Compare KL vs. JS divergence for ADA across multiple QAT methods to verify the paper's observation about bounded vs. nonlinear activations