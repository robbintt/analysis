---
ver: rpa2
title: Training Dynamics Impact Post-Training Quantization Robustness
arxiv_id: '2510.06213'
source_url: https://arxiv.org/abs/2510.06213
tags:
- quantization
- training
- learning
- error
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the relationship between training dynamics
  and post-training quantization robustness in large language models. The authors
  analyze quantization degradation across open-source training trajectories up to
  32B parameters and 15T tokens, finding that quantization errors are driven by complex
  interactions between learning rate and other hyperparameters rather than training
  data scale alone.
---

# Training Dynamics Impact Post-Training Quantization Robustization

## Quick Facts
- arXiv ID: 2510.06213
- Source URL: https://arxiv.org/abs/2510.06213
- Reference count: 40
- Primary result: Training dynamics (learning rate decay and weight averaging) significantly impact post-training quantization robustness in LLMs

## Executive Summary
This paper investigates how training dynamics affect post-training quantization robustness in large language models. The authors analyze quantization degradation across open-source training trajectories up to 32B parameters and 15T tokens, finding that quantization errors are driven by complex interactions between learning rate and other hyperparameters rather than training data scale alone. Through controlled experiments on their own models, they demonstrate that strategic training hyperparameter interventions—particularly maintaining larger learning rates and weight averaging—can significantly improve quantization quality at scale. Their analysis of loss landscape geometry reveals that these interventions promote convergence to flatter minima, which correlates with better quantization robustness.

## Method Summary
The authors conduct controlled experiments using Pythia-160M transformers trained on FineWebEdu with AdamW optimizer (β1=0.9, β2=0.95, weight decay=0.1, grad clip=1) and WSD schedules with peak LR 3e-3. They analyze quantization error trajectories through stable vs. decay phases using GPTQ 3/4-bit quantization with group size 128. The study includes analysis of existing open-source training trajectories (OLMo, SmolLM3, Apertus, OpenSci, Amber) up to 32B parameters and 15T tokens. Key interventions tested include varying peak learning rates, weight averaging (LAWA with window of 5 checkpoints), and comparing cosine vs WSD schedules.

## Key Results
- Quantization error spikes when learning rate decays, independent of training data scale
- Higher peak learning rates yield lower quantization error at equivalent validation loss
- Weight averaging can match or exceed LR decay performance for 3-bit quantized models
- Flatter minima correlate with better quantization robustness (measured via Hessian trace and sharpness)

## Why This Works (Mechanism)

### Mechanism 1: Learning Rate Decay Triggers Quantization Degradation
- Claim: Quantization error spikes when learning rate decays, independent of training data scale.
- Mechanism: As LR decays, models traverse sharper regions of the loss landscape, increasing sensitivity to weight perturbations from quantization. The Hessian trace and sharpness (max eigenvalue) surge during decay phases, mirroring quantization error trajectories.
- Core assumption: Sharpness causally increases quantization sensitivity (correlational evidence shown; causation not proven).
- Evidence anchors:
  - "Specifically, once learning rates decay, validation loss and quantization error diverge, largely independent of training data scale."
  - "The maximum eigenvalue shows a consistent rapid surge whenever the learning rate decays... The trace presents a similar pattern... remarkably mirroring the evolution of quantization error."
- Break condition: If quantization error rises during constant-LR phases without decay, this mechanism is insufficient.

### Mechanism 2: Larger Peak Learning Rates Promote Flatter Minima with Better Quantization Robustness
- Claim: Higher peak learning rates yield lower quantization error at equivalent validation loss.
- Mechanism: Larger LR introduces more gradient noise, implicitly regularizing toward flatter minima. Flatter basins mean quantized weights remain in lower-loss regions despite perturbation.
- Core assumption: Flatter minima are more robust to weight perturbations (theoretical link from Hochreiter & Schmidhuber 1997; empirical support shown but causation not isolated).
- Evidence anchors:
  - "Figure 6a shows that higher learning rates consistently lead to smaller errors... at similar validation loss, the larger rate achieves better low-bit quantization, at no apparent cost."
  - "The learning rate magnitude is proportional to the flatness of the basin of the loss... the sharpness of the basin is such that ŵₖ falls in a higher loss level."
- Break condition: If increasing LR beyond stability limits degrades both FP and quantized performance equally, benefits vanish.

### Mechanism 3: Weight Averaging Achieves Flat Minima Without LR Decay's Sharpness Penalty
- Claim: Weight averaging (LAWA, model souping) improves quantization robustness, potentially matching or exceeding LR decay.
- Mechanism: Averaging checkpoints along training trajectories smooths parameter noise and produces wider minima with lower Hessian sharpness than decay endpoints. This decouples quantization robustness from the sharpness increase caused by decay.
- Core assumption: Averaged weights have fundamentally different curvature properties than decayed endpoints.
- Evidence anchors:
  - "For 3-bit quantized models... checkpoints obtained through weight averaging can match or even surpass the performance of those trained with learning rate decay."
  - "Our analysis also indicates that averaging weights during training leads to wider minima... we show that the two methods produce solutions with substantially different curvature properties."
- Break condition: If averaged models from very different training stages show inconsistent benefits, the mechanism depends on trajectory coherence.

## Foundational Learning

- **Post-Training Quantization (PTQ)**:
  - Why needed here: Understanding that PTQ maps high-precision weights to low-bit representations via reconstruction error minimization is essential for grasping why loss landscape geometry matters.
  - Quick check question: Can you explain why minimizing weight error ||W - Ŵ|| differs from minimizing reconstruction error ||XW^T - XŴ^T||?

- **Learning Rate Schedules (WSD vs. Cosine)**:
  - Why needed here: The paper's central finding hinges on distinguishing constant-LR (stable) phases from decay phases, and how different schedules control end-of-training LR.
  - Quick check question: At what point in a cosine schedule does the LR become uncontrollably small, and why might this matter for quantization?

- **Loss Landscape Geometry (Flat vs. Sharp Minima)**:
  - Why needed here: The proposed unifying mechanism links all interventions to flatness, measured via Hessian trace and sharpness.
  - Quick check question: If a model sits in a sharp minimum, would a small weight perturbation (like quantization) cause a larger or smaller loss increase than in a flat minimum?

## Architecture Onboarding

- **Component map**: Training pipeline (LR scheduler → optimizer → checkpoints → weight averaging) → Quantization pipeline (trained model → calibration dataset → GPTQ/AWQ → quantized weights) → Analysis pipeline (checkpoints → Hessian estimation → sharpness/trace → correlation with PTQ error)

- **Critical path**:
  1. Run controlled ablations varying peak LR while keeping other hyperparameters fixed
  2. Measure quantization error at multiple checkpoints during stable vs. decay phases
  3. Estimate Hessian sharpness/trace at key checkpoints to verify flatness correlation

- **Design tradeoffs**:
  - Higher peak LR: Better quantization robustness, but risks training instability
  - WSD vs. Cosine: WSD offers control over decay timing; cosine may have sharper end-of-training curvature
  - Weight averaging: Adds complexity but can avoid decay-phase sharpness penalty

- **Failure signatures**:
  - Quantization error spikes during decay despite good FP performance → consider larger peak LR or weight averaging
  - Inconsistent PTQ robustness across checkpoints → check LR schedule; decay phase likely culprit
  - Averaged models underperforming → verify averaging window aligns with stable-phase checkpoints

- **First 3 experiments**:
  1. Replicate Figure 4: Train a small model (160M params) with WSD, decaying at different token budgets. Confirm quantization error spikes at decay regardless of data scale.
  2. Replicate Figure 6: Vary peak LR (e.g., 3e-4, 1e-3, 3e-3, 1e-2) on same training run. Plot FP vs. quantized validation loss curves to verify larger LR improves PTQ at equivalent FP loss.
  3. Test weight averaging: Apply LAWA to constant-LR checkpoints and compare 3-bit PTQ performance against decayed endpoints. Verify if averaging matches or exceeds decay without sharpness penalty.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does flatness of the loss minimum causally determine quantization robustness, or is it merely correlated?
- Basis in paper: [explicit] The authors state "we hypothesize that, as the learning rate decays, the model traverses a sharper region of the loss landscape, making it more sensitive to perturbations such as quantization." They observe correlations but do not establish causation.
- Why unresolved: The paper shows correlations between flatness metrics (Hessian trace, sharpness) and quantization error, but cannot distinguish whether flatness causes robustness or both share a common cause.
- What evidence would resolve it: Intervention experiments that directly manipulate loss landscape geometry without changing other training properties, or ablation studies controlling for flatness while varying other factors.

### Open Question 2
- Question: Do the findings on training dynamics and quantization robustness generalize to sparse mixture-of-expert (MoE) and sub-quadratic architectures (e.g., Mamba)?
- Basis in paper: [explicit] In the limitations section: "we expect similar conclusions for sparse (Shazeer et al., 2017) and sub-quadratic architectures (Gu & Dao, 2024)" but this expectation is untested.
- Why unresolved: The analysis is limited to dense transformer architectures; MoE models have different parameter distributions and sub-quadratic models have fundamentally different computational structures.
- What evidence would resolve it: Replication of the learning rate and weight averaging experiments on MoE and state-space model architectures.

### Open Question 3
- Question: How do schedule-free optimizers and alternative optimization strategies affect quantization robustness?
- Basis in paper: [explicit] "Factors such as optimizer choice may also affect quantization performance, and we leave the exploration of schedule-free methods (Defazio et al., 2024) to follow-up work."
- Why unresolved: The paper focuses on AdamW with standard learning rate schedules; modern schedule-free methods may exhibit different quantization dynamics.
- What evidence would resolve it: Controlled experiments comparing quantization degradation under schedule-free optimizers versus standard scheduled AdamW.

### Open Question 4
- Question: Through what mechanism does weight averaging improve quantization robustness—is it solely through flatness promotion?
- Basis in paper: [inferred] The paper shows LAWA improves PTQ performance beyond standard LR decay and links this to curvature properties, but notes weight averaging and LR decay "produce solutions with substantially different curvature properties" despite both improving quantization.
- Why unresolved: The relationship between the averaging mechanism, curvature changes, and quantization robustness remains under-characterized.
- What evidence would resolve it: Systematic analysis of Hessian properties under different averaging schemes with matched final loss values to isolate the mechanism.

## Limitations

- Causal mechanism between sharpness and quantization sensitivity remains unproven (correlational evidence only)
- Findings limited to dense transformer architectures; generalization to MoE and sub-quadratic models untested
- Analysis relies heavily on existing open-source training trajectories with uncontrolled hyperparameter variations

## Confidence

**High Confidence**: The empirical observation that quantization error increases during learning rate decay phases, regardless of training scale. The controlled experiments on peak learning rate effects are well-designed and reproducible. The Hessian analysis showing correlation between sharpness and quantization error is methodologically sound.

**Medium Confidence**: The proposed mechanism that larger learning rates promote flatter minima through implicit regularization. While the correlation is strong, the causal pathway from gradient noise to curvature properties needs more direct experimental validation. The weight averaging benefits are demonstrated but could vary significantly with averaging strategy choices.

**Low Confidence**: The generalizability of findings to architectures beyond standard transformers, and to quantization methods other than GPTQ/AWQ. The analysis relies heavily on existing open-source training trajectories, which may have uncontrolled hyperparameter variations.

## Next Checks

1. **Causal Sharpness Experiment**: Design a controlled study where sharpness is artificially manipulated (e.g., through regularization strength) while keeping other factors constant, then measure the direct impact on quantization error to establish causation.

2. **Weight Averaging Strategy Sweep**: Systematically test different weight averaging strategies (window size, checkpoint frequency, averaging method) across multiple training runs to identify optimal configurations and verify the robustness of the mechanism.

3. **Cross-Architecture Validation**: Apply the training dynamics interventions to a different model architecture (e.g., state-space models or vision transformers) to test whether the quantization robustness principles transfer beyond standard language models.