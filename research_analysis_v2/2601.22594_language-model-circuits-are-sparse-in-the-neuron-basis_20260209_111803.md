---
ver: rpa2
title: Language Model Circuits Are Sparse in the Neuron Basis
arxiv_id: '2601.22594'
source_url: https://arxiv.org/abs/2601.22594
tags:
- tokens
- neuron
- neurons
- circuits
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the assumption that neurons are an uninterpretable
  basis for circuit tracing by showing that MLP neuron activations form a sparse and
  effective feature basis. The authors demonstrate that using MLP activations instead
  of MLP outputs reduces circuit size by ~100x and achieves performance comparable
  to sparse autoencoders (SAEs).
---

# Language Model Circuits Are Sparse in the Neuron Basis

## Quick Facts
- arXiv ID: 2601.22594
- Source URL: https://arxiv.org/abs/2601.22594
- Reference count: 40
- Primary result: MLP neuron activations form a sparse feature basis, reducing circuit size by ~100x compared to MLP outputs while achieving performance comparable to sparse autoencoders

## Executive Summary
This paper challenges the assumption that neurons are an uninterpretable basis for circuit tracing by demonstrating that MLP neuron activations form a sparse and effective feature basis. The authors show that using MLP activations instead of MLP outputs reduces circuit size by approximately 100× while maintaining comparable performance to sparse autoencoders. They introduce RelP, a gradient-based attribution method that outperforms Integrated Gradients, achieving near-perfect faithfulness and completeness with only ~200 neurons. The method successfully uncovers interpretable circuits for tasks like subject-verb agreement and multi-hop reasoning, and is validated across multiple model families.

## Method Summary
The method uses RelP (a gradient-based attribution technique) on MLP activations to identify sparse circuits. RelP constructs a linearized replacement model by treating nonlinearities as constants during backpropagation, then computes attribution as activation × gradient. The approach filters neurons by attribution threshold and computes edge attributions with stop-gradients on intermediate MLPs to isolate direct effects. Circuits are evaluated via mean ablation of the complement circuit, measuring faithfulness (how well the circuit reproduces the full model's behavior) and completeness (how much of the output the circuit explains).

## Key Results
- MLP activations yield circuits ~100× smaller than MLP outputs due to privileged basis properties
- RelP achieves near-perfect faithfulness and completeness with only ~200 neurons, outperforming Integrated Gradients
- The method successfully identifies interpretable circuits for subject-verb agreement, multi-hop reasoning, addition problems, and multilingual antonyms
- Results are validated across Llama 3.1 and Gemma 2 model families without requiring additional training

## Why This Works (Mechanism)

### Mechanism 1
MLP activations form a sparse, interpretable basis while MLP outputs do not. The element-wise nonlinearity (e.g., SiLU) in MLPs creates a privileged basis where semantically meaningful features naturally align with individual neuron dimensions. After the down-projection (MLP output), this alignment is destroyed by rotation.

### Mechanism 2
RelP yields more faithful circuits than Integrated Gradients by constructing a locally linear replacement model. RelP freezes nonlinearities during backpropagation and computes attribution as activation × gradient, preserving completeness while requiring only one backward pass.

### Mechanism 3
Stop-gradients on intermediate MLP layers isolate direct causal edges between neurons. Computing edge attribution without stop-grades captures total effects (including paths through other neurons); with stop-grades on intermediate MLPs, only direct effects are captured, producing more faithful edge-pruned circuits.

## Foundational Learning

- **Privileged basis**: A basis where features naturally align with individual dimensions. Essential for understanding why MLP activations are sparse. Quick check: Explain why a basis is "privileged" and name one operation that creates one in Transformers.

- **Attribution completeness and faithfulness**: Completeness means attribution sums to output; faithfulness means ablated circuit matches full model. Core evaluation metrics. Quick check: If a circuit has faithfulness 1.0 and completeness 0.0, what does that imply?

- **Linearization / replacement models**: RelP's core technique of freezing nonlinearities to enable single-pass gradient attribution. Critical for understanding RelP's implementation. Quick check: How does the half rule preserve completeness for gated MLPs?

## Architecture Onboarding

- **Component map**: Input -> MLP activations (h_i, d_ffn dims) -> MLP outputs (m_i, d_model dims) -> RelP attribution (activation × gradient) -> Edge attribution (with stop-grades) -> Pruned circuit

- **Critical path**:
  1. Define metric (logit difference or top-k logits)
  2. Build replacement model (linearize RMSNorm, SiLU, attention)
  3. Compute RelP attributions for all neurons
  4. Filter neurons by attribution threshold (τ = 0.005 relative to total logit)
  5. Compute edge attributions with stop-grades on intermediate MLPs
  6. Prune edges; remove neurons with no remaining connections
  7. Evaluate via mean ablation of complement

- **Design tradeoffs**:
  - Neurons vs SAEs: Neurons avoid training cost but may be less interpretable per-unit
  - Direct vs total effects (stop-grades): Direct edges are sparser; total effects capture full causal paths
  - Mean vs zero ablation: Mean ablation preserves distribution; zero ablation is simpler

- **Failure signatures**:
  - High completeness (near 1.0) with low faithfulness: Circuit too small; attribution threshold too aggressive
  - Many "always-on" neurons appearing in every circuit: Need manual filtering
  - Attribution scores clustered near zero: Replacement model may be misconfigured

- **First 3 experiments**:
  1. Replicate SVA benchmark comparing MLP activations vs outputs vs residual stream using IG
  2. Swap IG for RelP; verify near-perfect faithfulness with ~200 neurons
  3. Run multi-hop capitals case study; identify 23 key neurons; validate steering effects

## Open Questions the Paper Calls Out

- How can neurons in a circuit be automatically clustered into interpretable groups without human intervention?
- Do these findings generalize to attention heads and other model components beyond MLP activations?
- How robust is RelP attribution across model scale, especially for models larger than 8B parameters?

## Limitations

- The privileged basis assumption may not hold for architectures with different activation functions
- Edge attribution with stop-grades may miss important indirect effects in multi-hop reasoning
- The method requires careful hyperparameter tuning (attribution threshold, stop-grade placement)

## Confidence

- **High**: MLP activations are more sparse than outputs for SVA tasks; RelP attribution works as described
- **Medium**: RelP's completeness guarantees hold across architectures; stop-grades correctly isolate direct effects
- **Low**: Generalization to novel tasks beyond SVA and multi-hop reasoning; scalability to larger models

## Next Checks

1. Test MLP activation sparsity on non-SVA tasks (e.g., arithmetic reasoning, code generation) to verify generalization
2. Implement ablation studies comparing direct vs total effects for multi-hop tasks to quantify information loss
3. Validate RelP faithfulness on attention-heavy architectures (e.g., BERT variants) where linearization assumptions may break down