---
ver: rpa2
title: Online Episodic Convex Reinforcement Learning
arxiv_id: '2505.07303'
source_url: https://arxiv.org/abs/2505.07303
tags:
- learning
- convex
- online
- where
- probability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses online episodic convex reinforcement learning
  (CURL), where the goal is to minimize a convex objective function over the state-action
  distributions induced by a policy in an unknown Markov decision process (MDP). Unlike
  standard RL with linear losses, the non-linearity of CURL invalidates Bellman equations,
  requiring new algorithmic approaches.
---

# Online Episodic Convex Reinforcement Learning

## Quick Facts
- arXiv ID: 2505.07303
- Source URL: https://arxiv.org/abs/2505.07303
- Reference count: 40
- Primary result: Near-optimal regret bounds for online episodic convex reinforcement learning using online mirror descent with exploration bonuses

## Executive Summary
This paper addresses online episodic convex reinforcement learning (CURL), where the goal is to minimize a convex objective function over the state-action distributions induced by a policy in an unknown Markov decision process. The non-linearity of CURL invalidates classical Bellman equations, requiring new algorithmic approaches. The authors propose algorithms using online mirror descent (OMD) with exploration bonuses that achieve near-optimal regret bounds without model assumptions.

## Method Summary
The method treats convex RL as a convex optimization problem over occupancy measures using online mirror descent. For unknown MDPs, it adds exploration bonuses to the gradient to compensate for transition kernel uncertainty. In bandit settings, it uses one-point gradient estimators with entropic or log-barrier regularization to navigate the convex constraint set despite only observing scalar objective values.

## Key Results
- Achieves regret bounds of Õ(N^(3/2) S^(3/2) A^(1/2) T^(1/2)) for full-information feedback
- Extends to bandit feedback settings with Õ(N^(3/2) S^(5/4) A^(5/4) T^(1/2)) bounds
- Demonstrates effectiveness on multi-objective and constrained MDP tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Optimizing over state-action distributions via OMD allows handling convex objectives where standard Bellman-based dynamic programming fails.
- **Mechanism:** Treats policy optimization as convex optimization over valid occupancy measures using specific Bregman divergence to ensure closed-form policy updates.
- **Core assumption:** Objective function is convex and Lipschitz with respect to occupancy measure.
- **Break condition:** Non-Markovian objective functions that violate occupancy measure assumptions.

### Mechanism 2
- **Claim:** Adding exploration bonuses to the gradient compensates for unknown transition kernel errors.
- **Mechanism:** Calculates uncertainty bonus based on state-action visit counts and subtracts from objective gradient during OMD step.
- **Core assumption:** Confidence intervals on transition kernel shrink at rate Õ(1/√N).
- **Break condition:** Massive state spaces with low visit counts causing high bonus terms.

### Mechanism 3
- **Claim:** One-point gradient estimator with regularization enables bandit feedback navigation.
- **Mechanism:** Perturbs occupancy measure and uses scalar value to estimate gradient, using lower-dimensional representation and barrier functions.
- **Core assumption:** Strictly positive transitions or non-empty feasible set interior.
- **Break condition:** MDPs with unreachable states causing empty feasible set interior.

## Foundational Learning

- **Occupancy Measures (State-Action Distributions)**
  - **Why needed here:** Shifts optimization from value functions to distributions, turning convex RL into standard convex optimization.
  - **Quick check:** Can you map a policy π to its induced occupancy measure μ^π for known transition kernel P?

- **Online Mirror Descent (OMD)**
  - **Why needed here:** Workhorse for online convex optimization; choice of mirror map affects update rule and regret bound.
  - **Quick check:** How does Bregman divergence differ from Euclidean distance in policy updates?

- **Concentration Inequalities (Hoeffding/Bernstein)**
  - **Why needed here:** Exploration bonus relies on bounding estimated transition kernel error.
  - **Quick check:** How does confidence radius scale with visit count N_t in bonus term b_t?

## Architecture Onboarding

- **Component map:** Counters -> Estimator -> Bonus Module -> OMD Solver -> Agent
- **Critical path:** Bonus Module -> OMD Solver interaction; incorrect bonus scaling fails to cancel model bias or causes excessive exploration.
- **Design tradeoffs:**
  - Entropic vs. Log-Barrier: Entropic simpler but requires positive transitions; Log-Barrier more general but computationally complex.
  - Model-Oblivious vs. Bonus: Trades dimension dependence degradation for closed-form policy update.
- **Failure signatures:**
  - Policy Collapse: Excessive exploration causing uniformization over states
  - Constraint Violation: Gradient estimation sphere exceeding feasible bounds
  - Non-sublinear Regret: High variance in estimated transitions preventing cancellation
- **First 3 experiments:**
  1. Multi-objective Grid World: Validate distribution concentrates on multiple targets, compare convergence to Greedy MD-CURL
  2. Constrained MDP Navigation: Test avoidance of constraint states while reaching target
  3. Ablation on Bonus Scaling: Vary C_δ to confirm Greedy failure without bonuses

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can self-concordant regularization be extended to unknown MDPs while maintaining sub-linear regret?
- **Basis:** Authors leave extending to unknown MDPs for future work.
- **Evidence needed:** Regret analysis for bandit CURL with log-barrier regularization using estimated transition kernels.

### Open Question 2
- **Question:** Can optimal Õ(√T) regret rate be achieved for general CURL with bandit feedback?
- **Basis:** Current methods yield T^(3/4) rates or depend on restrictive assumptions.
- **Evidence needed:** Novel bandit algorithm achieving high-probability Õ(√T) bound.

### Open Question 3
- **Question:** Can dependence on minimum transition probability ε be eliminated in bandit CURL regret bounds?
- **Basis:** Main shortcoming of entropic method is reliance on restrictive Assumption 4.2.
- **Evidence needed:** Sampling sphere based on reachability rather than strict positivity.

### Open Question 4
- **Question:** Are additional N and √|X| factors in full-information bound tight?
- **Basis:** Bound has additional factors compared to linear RL SoTA.
- **Evidence needed:** Lower bound proof matching these dependencies or improved upper bound.

## Limitations

- Exploration bonus mechanism sensitivity to state-space dimensionality when visit counts become sparse
- Bandit feedback algorithms require strong assumptions about MDP structure (positive transitions or non-empty feasible set)
- Experiments limited to small grid-world environments, raising scalability questions

## Confidence

- **High Confidence:** Regret bounds for full-information feedback (Õ(N^(3/2) S^(3/2) A^(1/2) T^(1/2))) - follow from OMD analysis with standard exploration bonus techniques
- **Medium Confidence:** Bandit feedback results - theoretical framework sound but assumptions very restrictive
- **Medium Confidence:** Exploration bonus cancellation mechanism - mathematical derivation correct but empirical validation limited

## Next Checks

1. Test Algorithm 1 in larger grid-world (50×50) to verify bonus effectiveness with sparse visit counts
2. Implement bandit algorithm on known MDP with artificially constrained empty feasible set interior
3. Run ablation study removing exploration bonus entirely to confirm Greedy baseline failure in multi-objective setting