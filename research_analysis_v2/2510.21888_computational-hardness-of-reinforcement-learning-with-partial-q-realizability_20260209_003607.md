---
ver: rpa2
title: "Computational Hardness of Reinforcement Learning with Partial $q^\u03C0$-Realizability"
arxiv_id: '2510.21888'
source_url: https://arxiv.org/abs/2510.21888
tags:
- policy
- realizability
- state
- max-3sat
- instance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper establishes computational hardness for reinforcement\
  \ learning under partial q\u03C0-realizability, a linear function approximation\
  \ setting weaker than full q\u03C0-realizability but stronger than q-realizability.\
  \ The authors show that learning an \u03B5-optimal policy is NP-hard under greedy\
  \ policies and requires exponential time under softmax policies (assuming rETH),\
  \ even when the policy class includes the optimal policy."
---

# Computational Hardness of Reinforcement Learning with Partial $q^π$-Realizability

## Quick Facts
- arXiv ID: 2510.21888
- Source URL: https://arxiv.org/abs/2510.21888
- Authors: Shayan Karimi; Xiaoqi Tan
- Reference count: 40
- This paper establishes computational hardness for reinforcement learning under partial qπ-realizability, a linear function approximation setting weaker than full qπ-realizability but stronger than q*-realizability.

## Executive Summary
This paper establishes computational hardness for reinforcement learning under partial qπ-realizability, a linear function approximation setting weaker than full qπ-realizability but stronger than q*-realizability. The authors show that learning an ε-optimal policy is NP-hard under greedy policies and requires exponential time under softmax policies (assuming rETH), even when the policy class includes the optimal policy. The hardness is proved by reducing from δ-MAX-3SAT problems to constructed MDPs with linear realizability. This reveals that computational challenges persist even with expanded policy classes, suggesting fundamental limits in RL with function approximation.

## Method Summary
The paper proves computational hardness through polynomial-time reductions from δ-MAX-3SAT to RL problems under partial qπ-realizability. The construction creates MDPs where states represent partial variable assignments, actions assign truth values, and terminal rewards encode clause satisfaction ratios. The authors design feature vectors that enable linear realizability of qπ values for all policies in the class, then show that finding ε-optimal policies corresponds to solving the original SAT instance. Two variants are analyzed: greedy policies (argmax-based) yielding NP-hardness, and softmax policies (temperature-free exponential) yielding exponential lower bounds under rETH.

## Key Results
- Learning ε-optimal policies under greedy policy sets is NP-hard for ε ≤ 0.05
- Under rETH, no randomized algorithm can solve SLINEAR-κ-RL in sub-exponential time exp(o(d^(1/3)/polylog(d^(1/3))))
- Partial q^π-realizability can be achieved via feature-weight decomposition for all π ∈ Π
- Hardness results hold even when the policy class includes the optimal policy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learning ε-optimal policies under greedy policy sets is NP-hard for ε ≤ 0.05
- Mechanism: Polynomial-time reduction from δ-MAX-3SAT maps variable assignments to MDP trajectories, with terminal rewards proportional to satisfied clause ratios. Finding ε-optimal policy corresponds to finding satisfying assignment.
- Core assumption: NP ≠ P (standard complexity assumption)
- Evidence anchors:
  - [abstract]: "we establish NP-hardness under a parameterized greedy policy set (i.e., argmax)"
  - [Theorem 3.1]: Formal statement of NP-hardness for GLINEAR-κ-RL
  - [corpus]: Corpus lacks direct evidence on this specific reduction technique
- Break condition: If P = NP, polynomial-time algorithms become possible; if ε > δ/2, reduction fails

### Mechanism 2
- Claim: Under rETH, no randomized algorithm can solve SLINEAR-κ-RL in sub-exponential time exp(o(d^(1/3)/polylog(d^(1/3))))
- Mechanism: McDiarmid's concentration inequality bounds reward deviation from expectation under softmax policies; connects satisfiability probability to algorithm success via bounded differences property
- Core assumption: Randomized Exponential Time Hypothesis (3-SAT requires 2^(Ω(v)) time with randomized algorithms)
- Evidence anchors:
  - [abstract]: "exponential lower bound (exponential in feature vector dimension) holds when the policy set contains softmax policies, under the Randomized Exponential Time Hypothesis"
  - [Section E.3, Eq. 32-36]: Detailed concentration analysis with b/|C|√(H·ln(1/p₀)/2) bound
  - [corpus]: Weak corpus support for rETH-based RL hardness results
- Break condition: If rETH is false, or if randomized algorithms can solve 3-SAT in sub-exponential time

### Mechanism 3
- Claim: Partial q^π-realizability can be achieved via feature-weight decomposition for all π ∈ Π
- Mechanism: Feature vector ϕ(s,a) tracks satisfied clauses (b_h) and undecided clause counts (Y_h); weight vector θ_h encodes future expected satisfactions under policy π, enabling q^π(s,a) = ⟨ϕ(s,a), θ_h⟩
- Core assumption: Policy set Π is properly parameterized via PSP features ϕ' and weights θ'
- Evidence anchors:
  - [Proposition 4.1]: Proves ∃ϕ, θ_h such that q^π(s,a) = ⟨ϕ(s,a), θ_h⟩ for all π ∈ Π_g
  - [Section 4.2.3, Eq. 11]: ϕ(s_h,a) = (1/|C|)·[b_h, Y_h] construction
  - [corpus]: Related work (paper 63273) discusses q^π-realizability but not partial variants
- Break condition: If feature dimension d must grow super-polynomially; if ϕ' ≠ ϕ constraint is relaxed (noted as future work limitation)

## Foundational Learning

- Concept: Realizability spectrum (q* vs q^π vs partial q^π)
  - Why needed here: This paper's core contribution is defining an intermediate setting between weak (q*) and strong (q^π) realizability assumptions
  - Quick check question: Why is partial q^π-realizability strictly stronger than q*-realizability but strictly weaker than q^π-realizability?

- Concept: Complexity-theoretic reductions
  - Why needed here: All hardness results derive from reducing δ-MAX-3SAT to RL instances
  - Quick check question: What property of the MDP construction ensures that solving the RL problem would also solve the SAT instance?

- Concept: Randomized Exponential Time Hypothesis
  - Why needed here: Standard NP-hardness is insufficient for randomized algorithms; rETH provides stronger lower bounds
  - Quick check question: Why does analyzing softmax policies require rETH while greedy policies only need NP ≠ P?

## Architecture Onboarding

- Component map:
  - MDP layer: States = partial variable assignments; Actions = assign True/False; Rewards = satisfied clause ratios
  - Feature layer: ϕ(s,a) ∈ R^d tracks clause satisfaction state; ϕ'(s,a) ∈ R^(d') parameterizes policy class
  - Policy layer: Π_g (greedy via argmax) or Π_sm (softmax via temperature-free exponential)
  - Reduction layer: Maps SAT variables → MDP states; SAT clauses → terminal rewards

- Critical path:
  1. Parse δ-MAX-3SAT instance → extract variables X and clauses C
  2. Construct MDP with 2^(n+1)-1 states, deterministic transitions, terminal rewards R(s_H) = |C_true|/|C|
  3. Build ϕ ∈ R^d (d = O(n³)) tracking satisfied/undecided clause counts
  4. Verify linear realizability: q^π(s,a) = ⟨ϕ(s,a), θ_h⟩ for all π ∈ Π
  5. Prove reduction correctness: ε-optimal policy → (1-δ)-satisfying assignment

- Design tradeoffs:
  - **Greedy vs Softmax**: Greedy gives simpler NP-hardness proof; Softmax requires rETH but covers stochastic policies
  - **Feature dimensions**: Larger d enables linear representation but increases computational cost (O(n³) for ϕ construction)
  - **Horizon scaling**: H = Θ(d^(1/3)) balances state space size against concentration bounds

- Failure signatures:
  - **Reduction failure**: If ε > δ/2, optimal RL policy may not map to satisfying SAT assignment
  - **Realizability collapse**: If ϕ' = ϕ required (unified features), current proof technique breaks (noted in conclusion)
  - **Concentration violation**: If H = o(1/(v* - 0.9)²), McDiarmid bounds fail for softmax analysis
  - **Hardness reversal**: If NP = RP or rETH false, all lower bounds become vacuous

- First 3 experiments:
  1. **Sanity check**: Implement MDP construction for n=3 variables (d=27), verify Proposition 4.1 holds numerically for all 8 greedy policies
  2. **Empirical hardness**: Compare runtime of LSVI/value iteration on constructed GLINEAR-2-RL instances vs random linear MDPs as d increases (expect exponential gap)
  3. **Softmax validation**: Sample softmax policy trajectories on constructed MDPs, measure empirical R(s_H) concentration vs theoretical McDiarmid bounds (Eq. 33)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the hardness results for partial qπ-realizability be extended to the unified feature setting where ϕ′ = ϕ?
- Basis in paper: [explicit] "Extending our proof techniques to accommodate partial qπ-realizability with a unified feature vector (i.e., ϕ′ = ϕ) remains a significant challenge and is left for future work."
- Why unresolved: The reduction construction requires separate parameterization for policy set construction (ϕ′, θ′) versus value function realizability (ϕ, θ). A unified representation may impose additional structure that could enable efficient algorithms.
- What evidence would resolve it: A reduction proof showing NP-hardness (or exponential lower bound under rETH) when ϕ′ = ϕ, or conversely, a polynomial-time algorithm for this unified setting.

### Open Question 2
- Question: What structural constraints on the feature mapping ϕ or policy class Π would enable computationally efficient learning under partial qπ-realizability?
- Basis in paper: [inferred] The conclusion notes that hardness results "highlight the need for efficient algorithms under extra assumptions, such as structural constraints on policies or features."
- Why unresolved: The paper establishes worst-case hardness; identifying sufficient conditions for tractability remains open.
- What evidence would resolve it: Identification of specific structural conditions (e.g., low-rank features, policy monotonicity constraints) under which polynomial-time algorithms exist.

### Open Question 3
- Question: Do efficient algorithms exist for partial qπ-realizability under alternative access models (e.g., online access instead of generative model)?
- Basis in paper: [inferred] The paper assumes generative model access, while noting that qπ-realizability achieves computational efficiency under this model. The interaction model's effect on partial realizability remains unexplored.
- Why unresolved: Different access models may impose constraints that change computational complexity.
- What evidence would resolve it: Hardness results or efficient algorithm guarantees under online access or other interaction protocols.

## Limitations

- The hardness bounds critically depend on NP ≠ P and rETH assumptions, which remain unproven
- The ε-optimal threshold of 0.05 for greedy policies appears arbitrary and may not be tight
- The reduction technique's scalability with larger clause widths (b > 3) remains unclear
- The analysis assumes ϕ' ≠ ϕ for partial realizability, leaving unified-feature cases as future work

## Confidence

- **High confidence**: NP-hardness under greedy policies (mechanism 1) - direct polynomial reduction with clear mapping
- **Medium confidence**: Exponential lower bound under softmax (mechanism 2) - relies on rETH which has less empirical validation
- **Medium confidence**: Partial realizability construction (mechanism 3) - proof is sound but the general feature construction may not extend to all MDP classes

## Next Checks

1. Verify the MDP-to-SAT mapping preserves ε-optimality relationships by implementing the full reduction pipeline and testing with known δ-MAX-3SAT instances
2. Characterize the exact relationship between ε₀ and MDP parameters (d, H, v*) for softmax hardness bounds through systematic parameter sweeps
3. Test the partial realizability construction on non-SAT-derived MDPs to assess generalizability beyond the reduction framework