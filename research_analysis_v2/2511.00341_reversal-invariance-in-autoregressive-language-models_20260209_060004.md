---
ver: rpa2
title: Reversal Invariance in Autoregressive Language Models
arxiv_id: '2511.00341'
source_url: https://arxiv.org/abs/2511.00341
tags:
- language
- causal
- reversal
- objective
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces reversal invariance, a formal property of
  the autoregressive language modeling (CLM) objective showing that next-token negative
  log-likelihood is symmetric under sequence reversal. This explains why models trained
  on reversed text can achieve comparable perplexity to forward-trained models, despite
  the inherent directionality of human language and reasoning.
---

# Reversal Invariance in Autoregressive Language Models

## Quick Facts
- **arXiv ID**: 2511.00341
- **Source URL**: https://arxiv.org/abs/2511.00341
- **Reference count**: 31
- **Primary result**: Formal proof that autoregressive language modeling (CLM) is direction-blind—next-token NLL is symmetric under sequence reversal up to vocabulary permutation.

## Executive Summary
This paper identifies reversal invariance as a fundamental property of the autoregressive language modeling objective, showing that next-token negative log-likelihood assigns identical loss to a corpus and its reversal up to vocabulary permutation. This formalizes why models trained on reversed text can achieve comparable perplexity to forward-trained models, despite human language's inherent directionality. Using information-theoretic analysis, the authors demonstrate that while entropy rates are reversal-invariant, natural language exhibits nonzero time-reversal divergence. They argue this symmetry represents a limitation of current pretraining objectives, as it ignores time-asymmetric linguistic regularities like phonology, morphology, and causal flow. The paper concludes that explicitly modeling the arrow of language—rather than relying on bidirectional statistical correlation—may be essential for capturing causal and temporal reasoning in LLMs.

## Method Summary
The paper presents a theoretical analysis of autoregressive language modeling (CLM) objectives, proving that the next-token prediction loss is formally invariant under sequence reversal up to vocabulary permutation. This involves analyzing the conditional probability factorization in AR models, demonstrating that permuting embedding matrix rows and output projection columns preserves likelihood under consistent token relabeling. The authors use information-theoretic tools to distinguish between entropy rate (reversal-invariant) and time-reversal divergence (direction-sensitive), arguing that while both converge to the same perplexity floor, natural language exhibits measurable asymmetry. No empirical experiments are conducted—this is a theoretical position paper that formalizes existing observations about model behavior on reversed text.

## Key Results
- AR language modeling objective is formally reversal-invariant under tokenization stability and positional encoding constraints
- Natural language exhibits nonzero time-reversal divergence (A > 0) due to directional linguistic structures
- The success of chain-of-thought prompting and preference alignment can be viewed as attempts to reintroduce directionality after pretraining
- Current CLM pretraining is "direction-blind," learning bidirectional correlations without distinguishing temporal order

## Why This Works (Mechanism)

### Mechanism 1: Vocabulary Permutation Equivariance
- Claim: The autoregressive NLL objective assigns identical loss to a corpus and its reversal, up to token relabeling.
- Mechanism: When sequence order is reversed, adjacent token statistics are preserved (just reordered). A vocabulary permutation π maps forward tokens to their reversed equivalents, and permuting embedding matrix rows and output projection columns produces equivalent likelihood under the reversed tokenization. The loss landscape is identical under this reparameterization.
- Core assumption: Tokenizers trained on forward and reversed corpora are approximately stable under reversal (frequency-based tokenizers like BPE satisfy this in large-data regimes).
- Evidence anchors:
  - [abstract] "Formally, the next-token prediction loss assigns identical likelihood to a corpus and its reversal, implying that standard CLM pretraining is direction-blind."
  - [Section 4.3] Lemma 4.3 proves that conditional probabilities are preserved under consistent token relabeling: p_θ(z|c) = p_{Φπ(θ)}(π(z)|π(c))
  - [corpus] Related work confirms models trained on reversed corpora achieve comparable perplexity (Yu et al., 2025), though direct empirical tests of the formal theorem are limited.
- Break condition: If tokenization differs substantially between forward and reversed text (e.g., due to tie-breaking in BPE merge rules on small corpora), the permutation mapping becomes approximate rather than exact.

### Mechanism 2: Entropy Rate Reversal Symmetry
- Claim: The perplexity floor—the statistical ceiling for any language model—is identical for forward and reversed text.
- Mechanism: Shannon entropy is invariant under symbol reordering within the probability distribution. For a stationary stochastic process, h(X) = h(X^R). Since perplexity is defined as PPL = exp(h), both directions converge to the same theoretical lower bound regardless of linguistic structure.
- Core assumption: The token sequence distribution is approximately stationary over long contexts.
- Evidence anchors:
  - [Section 5.2] "Because Shannon entropy is invariant under symbol reordering inside the probability distribution, one obtains h(X) = h(X^R)."
  - [Section 5.4] "Entropy rate h and perplexity are reversal-invariant. But the KL-based asymmetry A is strictly positive for human language."
  - [corpus] Weak direct evidence; information-theoretic analysis in this paper is theoretical, not empirical.
- Break condition: For non-stationary processes or very short sequences where boundary effects dominate, the entropy rate equivalence may not hold.

### Mechanism 3: Time-Reversal Divergence Captures Directional Structure
- Claim: Natural language exhibits nonzero time-reversal divergence (A > 0), meaning the forward and backward conditional distributions differ, but NLL optimization is blind to this asymmetry.
- Mechanism: The KL divergence between forward and reversed path measures—D_KL(P(X_{1:n}) || P^R(X_{1:n}))—grows with sequence length when the process has directional constraints (phonotactics, morphology, causality). However, since NLL targets entropy rate h rather than asymmetry A, models learn bidirectional correlations without learning which direction is "correct."
- Core assumption: Human language encoding of phonological, morphological, and causal dependencies creates measurable statistical asymmetry.
- Evidence anchors:
  - [Section 5.3] "For natural languages, prior linguistic evidence suggests that A is nonzero: e.g., phonotactic rules, syntactic dependencies, and discourse structures are direction-sensitive."
  - [Section 6.2] "The success of chain-of-thought prompting, fine-tuning, and reinforcement alignment can be viewed as successive attempts to reintroduce directionality after pretraining."
  - [corpus] The "reversal curse" literature (Berglund et al., 2024; Golovneva et al., 2024) documents brittle directional generalization, consistent with—but not direct proof of—this mechanism.
- Break condition: If natural language asymmetry is primarily encoded at semantic/pragmatic levels not captured by token-level KL divergence, the quantitative measure A may underestimate true directional structure.

## Foundational Learning

- **Concept: Autoregressive Factorization**
  - Why needed here: The paper's core argument depends on understanding that AR models decompose joint probability as a product of conditionals: P(z) = ∏ p(z_k | z_{<k}). This factorization is what creates the permutation equivariance.
  - Quick check question: Can you explain why permuting the vocabulary in an AR model preserves the loss if both input embeddings and output projections are permuted consistently?

- **Concept: Shannon Entropy Rate**
  - Why needed here: The distinction between entropy rate h (reversal-invariant) and time-reversal divergence A (direction-sensitive) is the core information-theoretic insight.
  - Quick check question: Why does H(X_1, ..., X_n) = H(X_n, ..., X_1) hold for joint entropy, even when the conditional distributions p(x_t | x_{<t}) and p(x_t | x_{>t}) differ?

- **Concept: Causal Masking vs. Learned Directionality**
  - Why needed here: The paper emphasizes that architectural causal masks impose a decoding convention at inference time, but do not create directional bias in the learning signal itself.
  - Quick check question: If you train a model on reversed text but apply standard left-to-right causal masking at inference, what happens to generation quality?

## Architecture Onboarding

- **Component map:** Tokenizer (τ) -> Embedding matrix (E) -> Output projection (W) -> Causal mask
- **Critical path:** For a new engineer, the key insight is that the loss function (NLL) is symmetric even though the architecture (causal attention) appears directional. Understanding this separation is prerequisite to evaluating proposals for "asymmetry-aware" objectives.
- **Design tradeoffs:**
  - Adding directional signals (e.g., forward-backward consistency losses, causal graph supervision) may improve temporal reasoning but adds complexity and requires additional supervision.
  - Bidirectional objectives (BERT-style) capture more context but sacrifice generative autoregressive capability.
  - The paper argues current scaling approaches mask this limitation—larger models memorize more bidirectional statistics without recovering true directionality.
- **Failure signatures:**
  - Models trained on reversed corpora achieving perplexity comparable to forward-trained models (this is the predicted behavior under reversal invariance).
  - Brittle generalization on direction-sensitive tasks (e.g., reversal curse: "A is B" does not imply "B is A").
  - Reliance on chain-of-thought or post-hoc alignment to achieve coherent temporal/causal reasoning.
- **First 3 experiments:**
  1. **Tokenizer stability audit:** Take a representative corpus, reverse it, retrain BPE from scratch, and measure how often τ^T(T(s)) ≠ π(rev(τ(s))) for some permutation π. Quantify approximation error.
  2. **Controlled reversal training:** Train identical architectures on forward vs. reversed text, compare learning curves on held-out validation sets (both forward and reversed). Confirm the formal prediction of indistinguishable loss dynamics.
  3. **Time-reversal divergence estimation:** Estimate A on real text corpora using finite-context approximations (e.g., n-gram or small Transformer models trained bidirectionally). Test whether A > 0 correlates with downstream performance gaps on direction-sensitive reasoning tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can pretraining objectives that explicitly break reversal invariance improve causal and temporal reasoning in language models, and what formulation would preserve standard language modeling capacity while encoding directional asymmetry?
- Basis in paper: [explicit] The authors "motivat[e] future work on loss functions and architectures that explicitly model the arrow of language while retaining standard language modeling capacity."
- Why unresolved: The paper is a theoretical position paper; no new objective is proposed or tested.
- What evidence would resolve it: Design an objective incorporating the time-reversal divergence A and demonstrate improved performance on causal reasoning benchmarks without degrading perplexity.

### Open Question 2
- Question: To what extent does reversal invariance contribute to the Reversal Curse, where models trained on "A is B" fail to infer "B is A"?
- Basis in paper: [explicit] The authors "trace part of this brittleness to a symmetry of the AR objective itself and hypothesize adding explicit directional signals at pretraining time."
- Why unresolved: The paper identifies a plausible theoretical connection but does not empirically isolate the objective's contribution from data or architectural factors.
- What evidence would resolve it: Compare directional generalization in models trained with asymmetry-aware objectives versus standard CLM, controlling for data and architecture.

### Open Question 3
- Question: Can explicit directional signals during pretraining reduce reliance on post-hoc preference alignment (e.g., RLHF) for direction-sensitive behaviors?
- Basis in paper: [explicit] The authors "hypothesize that providing explicit directional signals during pretraining could, if validated, reduce reliance on post-hoc preference data."
- Why unresolved: No experiments test whether pretraining asymmetry affects downstream alignment efficiency.
- What evidence would resolve it: Measure the data efficiency and final performance of RLHF on models pretrained with and without directional invariance breaking.

### Open Question 4
- Question: What is the empirical magnitude and linguistic structure of time-reversal divergence (A) in natural language corpora?
- Basis in paper: [inferred] The paper introduces A theoretically and claims it is "strictly positive for human language," but does not measure it on real data.
- Why unresolved: Estimating KL divergence between forward and reversed text distributions requires tractable approximations not provided.
- What evidence would resolve it: Compute estimated A across multiple languages and domains, and correlate with linguistic properties (phonotactics, morphology, discourse).

## Limitations
- The paper's core claims are theoretical rather than empirical, with no direct experimental validation of the reversal invariance theorem on actual language model training runs.
- The practical impact of reversal invariance on downstream reasoning tasks remains correlative rather than causally established.
- The key information-theoretic claims about time-reversal divergence A depend on unproven assumptions about how well finite-context or neural models can approximate true path measures.

## Confidence
- **High confidence**: The formal proof of reversal invariance under tokenization stability (Theorem 4.5) - the mathematical derivation is rigorous and the mechanism (vocabulary permutation equivariance) is well-specified.
- **Medium confidence**: The claim that natural language exhibits nonzero time-reversal divergence A - supported by linguistic reasoning but lacking direct empirical measurement, and the practical significance depends on unknown scales.
- **Low confidence**: That reversal invariance is a "limitation" requiring new pretraining objectives - this is speculative; the paper doesn't demonstrate that models trained with directional bias would actually perform better on temporal/causal reasoning tasks.

## Next Checks
1. **Empirical test of reversal invariance**: Train identical autoregressive Transformer architectures (same size, same hyperparameters) on forward and reversed versions of the same corpus, using identical training protocols. Compare learning curves and final perplexities on both forward and reversed validation sets. This would directly test whether the formal theorem predicts actual model behavior.

2. **Quantify tokenizer reversal stability**: For a standard BPE tokenizer, measure the permutation error rate when reversing text - how often does τ^T(T(s)) ≠ π(rev(τ(s)))? Compute this on held-out data across different corpus sizes to establish when the approximation becomes accurate enough for the formal theorem to apply.

3. **Estimate time-reversal divergence**: Using finite-context models (n-grams or small neural models trained bidirectionally), estimate A = lim DKL(P||P^R)/n on multiple natural language corpora. Measure whether A > 0 correlates with performance gaps on direction-sensitive reasoning benchmarks, providing empirical grounding for the claim that natural language has measurable directional structure.