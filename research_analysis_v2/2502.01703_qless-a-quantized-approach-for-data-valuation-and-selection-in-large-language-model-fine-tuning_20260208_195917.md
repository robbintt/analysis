---
ver: rpa2
title: 'QLESS: A Quantized Approach for Data Valuation and Selection in Large Language
  Model Fine-Tuning'
arxiv_id: '2502.01703'
source_url: https://arxiv.org/abs/2502.01703
tags:
- qless
- data
- gradient
- quantization
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: QLESS is a quantized data valuation method that extends LESS by
  applying absmax-based gradient quantization to dramatically reduce memory usage
  while maintaining data selection effectiveness. The approach compresses low-rank
  projected gradients to 1-8 bit precision and reconstructs them during influence
  computation through cosine similarity.
---

# QLESS: A Quantized Approach for Data Valuation and Selection in Large Language Model Fine-Tuning

## Quick Facts
- arXiv ID: 2502.01703
- Source URL: https://arxiv.org/abs/2502.01703
- Reference count: 40
- Primary result: QLESS reduces memory usage by up to 16x through gradient quantization while maintaining data selection effectiveness comparable to LESS.

## Executive Summary
QLESS is a quantized data valuation method that extends LESS by applying absmax-based gradient quantization to dramatically reduce memory usage while maintaining data selection effectiveness. The approach compresses low-rank projected gradients to 1-8 bit precision and reconstructs them during influence computation through cosine similarity. Experiments across LLaMA, Mistral, and Qwen models on MMLU, BBH, and TyDiQA benchmarks show that QLESS achieves comparable data selection performance to LESS while reducing memory usage by up to 16x. Notably, even 1-bit gradient quantization preserves data valuation quality, often matching or exceeding random selection baselines.

## Method Summary
QLESS implements a three-stage pipeline: (1) Warmup training on 5% random subset to obtain initial LoRA weights, (2) Feature extraction where LoRA gradients are computed for all training samples, projected to 8192 dimensions using a fixed random matrix, and quantized using absmax-based scalar quantization to 1-8 bit precision, and (3) Selection where influence scores are computed via cosine similarity of normalized quantized gradients, and the top 5% of samples are selected for retraining. The method focuses on LoRA parameters rather than full-model gradients, enabling aggressive compression while preserving task-relevant adaptation information.

## Key Results
- QLESS achieves comparable data selection performance to LESS while reducing memory usage by up to 16x.
- Even 1-bit gradient quantization preserves data valuation quality, often matching or exceeding random selection baselines.
- The method demonstrates robustness across different model architectures (LLaMA, Mistral, Qwen) and quantization levels, with only minor performance degradation at extreme compression settings.

## Why This Works (Mechanism)

### Mechanism 1: Directional Preservation via Random Projection and Cosine Similarity
QLESS maintains data valuation effectiveness because random projection followed by normalization and cosine similarity comparison preserves relative gradient directions essential for influence estimation, even under quantization. The method projects high-dimensional LoRA gradients to a lower-dimensional space (k=8192) using a random matrix R. Since influence is computed via cosine similarity, the absolute magnitude is discarded, and quantization primarily affects magnitude while preserving sign and relative proportions of gradient components, which dominate cosine similarity calculations.

### Mechanism 2: Information Sufficiency of Binary/Coarse Gradient Features
Even 1-bit quantization (sign of gradient components) provides sufficient signal for differentiating informative from non-informative training samples. Influence is determined by overlap in sign patterns of training and validation gradients. A high positive influence score implies broad alignment of gradient signs, suggesting that coarse-grained directional agreement is a robust proxy for fine-grained influence scores in data selection tasks.

### Mechanism 3: Compression via LoRA-based Gradient Extraction
Focusing on LoRA gradients captures task-relevant adaptation information in a lower-dimensional, more compressible representation. LoRA adapters represent the model's adaptation direction, and by extracting only these gradients, QLESS operates in a lower-dimensional space than full-gradient methods. The subsequent projection and quantization further compress this, leveraging potential redundancy in LoRA gradients that can be aggressively quantized without losing core "adaptation signal."

## Foundational Learning

- **Gradient-based Influence Functions (TracIn):** Why needed - QLESS extends gradient-based valuation. Influence is estimated by alignment (dot product) of training and validation gradients. Quick check - If a training sample has a negative dot product with a validation sample's gradient, does it have positive or negative influence on that validation sample's loss?
- **Dimensionality Reduction (Random Projection & JL Lemma):** Why needed - The first compression step is random projection. Understanding that it approximately preserves distances is key to seeing why quantization doesn't destroy all information. Quick check - What mathematical guarantee does the Johnson-Lindenstrauss lemma provide when projecting vectors from a high to low-dimensional space?
- **Scalar Quantization (Absmax-based):** Why needed - The core innovation is this quantization step. You must understand how floats are mapped to integers to diagnose issues like the "sparsity problem." Quick check - In absmax quantization, what single value is used to scale all components of a vector before rounding to integers?

## Architecture Onboarding

- **Component map:** Gradient Extractor -> Projection Layer -> Quantizer -> Datastore -> Influence Scorer -> Selector
- **Critical path:** 1) Warmup Training: Fine-tune on small subset to get initial LoRA weights. 2) Gradient Extraction & Storage (ALL training data): For each sample, compute LoRA gradient, project, quantize, store. Most expensive step. 3) Selection: Compute validation gradients, compare against datastore, select top 5%.
- **Design tradeoffs:** Bit-width (b) vs. Fidelity - 1-bit is most efficient but noisy; 8-bit is closest to LESS. Absmax vs. Absmean - Absmax is simpler but causes a "zero-bin" problem at low bits (2, 4). LoRA Rank (d) vs. Projection Dimension (k) - Higher rank captures more info but larger initial gradient.
- **Failure signatures:** Performance Collapse to Random - Check: (1) Warmup stable? (2) Projection matrix R correctly initialized and fixed? (3) Gradients non-zero? 2-bit Specific Degradation - If 2-bit performs worse than 1-bit, suspect "zero-bin sparsity." Try absmean quantization. Memory Not Reduced - Ensure datastore stores low-bit integers, not inadvertently upcast to float.
- **First 3 experiments:** 1) Baseline Reproduction: Replicate LESS 16-bit pipeline on a small model (Llama 3.2 3B) and single benchmark (MMLU). Validate gradient extraction and projection. 2) Quantization Sweep: Add QLESS quantization. Run with b âˆˆ {1, 2, 4, 8}. Plot performance vs. storage. Check for 2-bit "dip." 3) Absmax vs. Absmean Ablation: At problematic bit-width (2 or 4), replace absmax with absmean. Compare performance to diagnose zero-bin sparsity.

## Open Questions the Paper Calls Out

- **Joint Optimization of Compression Steps:** Can joint optimization of dimensionality reduction and precision reduction retain more information for influence estimation than the sequential approach used in QLESS? The sequential compression approach may not optimally preserve influence relationships.
- **Theoretical Connections Between Extreme Quantization and Influence:** What are the theoretical connections between extreme gradient quantization (1-bit) and influence estimation that explain the preservation of data valuation quality? The surprising effectiveness of 1-bit representations suggests investigating theoretical connections between gradient quantization and influence estimation.
- **Dynamic Quantization Schemes:** Can dynamic quantization schemes that adapt to gradient distributions during training improve upon the static absmax-based method? QLESS could be extended to support dynamic quantization schemes that adapt to gradient distributions during training.

## Limitations
- The sequential compression approach (projection then quantization) may not optimally preserve influence relationships compared to joint optimization.
- The paper lacks rigorous theoretical analysis of why extreme quantization preserves data valuation quality.
- 2-bit quantization suffers from a sparsity problem where gradients cluster in the zero bin, degrading performance.

## Confidence

- **High:** Claims about absolute memory reduction (16x) and basic functionality of the quantization pipeline.
- **Medium:** Claims about maintaining data selection quality across bit-widths, except for 2-bit.
- **Low:** Claims about 1-bit quantization being "effective" without establishing superiority over random selection.

## Next Checks
1. **Random vs. 1-bit Ablation:** Run QLESS with 1-bit quantization against a true random selection baseline (same 5% subset) on all three benchmarks. Measure if 1-bit consistently outperforms random.
2. **Full-gradient vs. LoRA ablation:** Implement a version of QLESS using full-model gradients (not just LoRA) with the same quantization pipeline. Compare performance and memory usage to validate the LoRA focus claim.
3. **Direction preservation analysis:** For a subset of samples, compute the angle between original projected gradients and quantized gradients at each bit-width. Plot the distribution to quantify how much directional information is lost during quantization.