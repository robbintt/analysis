---
ver: rpa2
title: 'Optimal Control of Fluid Restless Multi-armed Bandits: A Machine Learning
  Approach'
arxiv_id: '2502.03725'
source_url: https://arxiv.org/abs/2502.03725
tags:
- control
- state
- optimal
- problem
- problems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a machine learning approach for solving fluid
  restless multi-armed bandit (FRMAB) problems with affine or quadratic state equations.
  The authors derive fundamental properties of these systems and use them to efficiently
  implement a numerical solution algorithm called the shooting method.
---

# Optimal Control of Fluid Restless Multi-armed Bandits: A Machine Learning Approach

## Quick Facts
- arXiv ID: 2502.03725
- Source URL: https://arxiv.org/abs/2502.03725
- Reference count: 12
- Machine learning approach achieves up to 100% accuracy and under 1.8% suboptimality on fluid restless multi-armed bandit problems

## Executive Summary
This paper introduces a machine learning framework for solving fluid restless multi-armed bandit (FRMAB) problems with affine or quadratic state equations. The approach combines a novel shooting method that exploits closed-form trajectory expressions with Optimal Classification Trees with hyperplane splits (OCT-H) to learn interpretable state feedback policies. The learned policies achieve near-optimal performance while providing significant computational speed-ups (up to 26 million times faster than solving problems from scratch). The method is demonstrated on machine maintenance, epidemic control, and fisheries control problems.

## Method Summary
The approach consists of two main phases: solving and learning. First, instances of FRMAB problems are solved using a shooting method that exploits closed-form expressions for state and costate trajectories under affine or quadratic dynamics. The shooting method iteratively adjusts the initial costate guess until the terminal condition is satisfied, using Broyden's method for root-finding. Second, the solved trajectories are used to extract training data consisting of state-time pairs and corresponding optimal controls. OCT-H is then trained on these data with data-driven nonlinear feature augmentation to capture switching curves. The resulting policy maps state-time pairs to binary control decisions.

## Key Results
- Achieved up to 100% classification accuracy on out-of-sample instances across tested problems
- Maximum suboptimality remained under 1.8% compared to optimal solutions
- Speed-ups of up to 26 million times compared to solving problems from scratch
- Training times ranged from minutes to hours on a personal laptop

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Piecewise constant optimal controls emerge from the index-based structure, enabling decision tree learning.
- Mechanism: Pontryagin's Maximum Principle reduces optimal control to solving a linear optimization problem over index functions γᵢ(t). The dual of this LO has binary optimal solutions, creating piecewise constant trajectories with binary (0 or 1) values.
- Core assumption: Concave state equations (Assumption 1), making necessary optimality conditions also sufficient.
- Evidence anchors: Proposition 1 and its proof showing binary optimal controls via dual LO analysis; Equation (5) showing the index policy structure.
- Break condition: Non-concave state equations may break the sufficient optimality guarantee.

### Mechanism 2
- Claim: Closed-form trajectory expressions eliminate numerical instability in the shooting method.
- Mechanism: For affine dynamics (Eq. 7) and quadratic dynamics (Eq. 9), the state and costate ODEs admit closed-form solutions within intervals of constant control, avoiding numerical integration.
- Core assumption: State equations are either affine or quadratic.
- Evidence anchors: Remark 1 explicitly states this advantage; Equations (7) and (9) provide closed-form expressions.
- Break condition: More general state equations would require numerical integration, reintroducing instability.

### Mechanism 3
- Claim: Nonlinear feature augmentation allows linear decision boundaries to approximate switching curves.
- Mechanism: Optimal control switches occur when γᵢ(t) = γⱼ(t) or γᵢ(t) = 0. By augmenting feature vectors with nonlinear transformations like t, xᵢ, 1/(xᵢ + α/β), and xᵢ², OCT-H's hyperplane splits can effectively partition the state space.
- Core assumption: Coefficients in the affine combinations of nonlinear terms can be treated as approximately constant across the state space.
- Evidence anchors: Propositions 2 and 3 derive the nonlinear terms; Section 4.2 describes the heuristic approximation approach.
- Break condition: Complex switching curve geometries not captured by the proposed transformations.

## Foundational Learning

- Concept: Pontryagin's Maximum Principle
  - Why needed here: Provides necessary and sufficient optimality conditions for the fluid control problem, introducing the costate variable y(t) and the Hamiltonian structure that underlies the index policy.
  - Quick check question: Can you explain why the transversality condition y(T) = 0 matters for the shooting method?

- Concept: Restless Multi-Armed Bandits
  - Why needed here: The paper addresses the fluid approximation of RMABs; understanding the stochastic original problem clarifies why fluid approximations are valuable and what's being sacrificed.
  - Quick check question: What makes RMABs "restless" compared to classic bandits, and why does this make them PSPACE-hard?

- Concept: Boundary Value Problems and Shooting Methods
  - Why needed here: The optimal control problem becomes a two-point boundary value problem (initial state x(0) = x₀, terminal costate y(T) = 0). The shooting method iteratively guesses y(0) until y(T) ≈ 0.
  - Quick check question: Why does having closed-form trajectory expressions help with shooting method stability?

## Architecture Onboarding

- Component map: Problem Instance Generator -> Shooting Method Solver -> Training Data Extractor -> Feature Augmenter -> OCT-H Trainer -> Policy Evaluator
- Critical path: 1) Verify problem fits affine or quadratic dynamics with concave ϕᵤ,ᵢ(·); 2) Configure shooting method tolerances (ε = 10⁻⁵, δ = 10⁻⁴); 3) Generate M = 3000 training instances, extracting N = 10 points per trajectory; 4) Apply data-driven feature augmentation (Algorithm 2); 5) Train OCT-H with depth tuning over [5, 10, 15]; 6) Validate on held-out instances measuring accuracy and suboptimality
- Design tradeoffs: Interpretability vs. expressiveness (OCT-H maintains interpretability but requires manual feature engineering); Training time vs. coverage (more training instances improve policy quality but increase upfront cost); Tree depth vs. overfitting (deeper trees capture more complex switching curves but risk overfitting)
- Failure signatures: Shooting method non-convergence (check if dynamics satisfy concavity assumption); Low classification accuracy (>2% error) (likely indicates switching curves not captured by feature augmentation); High suboptimality (>5%) (may indicate insufficient training coverage or tree depth too shallow); State constraint violations (verify Propositions 6-8 style analysis for new problems)
- First 3 experiments: 1) Replicate infinite-server queue example (Section 4.3) to verify OCT-H recovers the time-threshold policy with ≈100% accuracy; 2) Ablate feature augmentation on machine maintenance problem to quantify accuracy drop; 3) Scale stress test by varying n ∈ {5, 10, 20, 50} to measure training time, tree depth, and suboptimality scaling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the machine learning approach with OCT-H be extended to FRMAB problems with general (non-affine, non-quadratic) state equations that lack closed-form trajectory expressions?
- Basis in paper: "However, the principles of our approach can be extended to more general state equations."
- Why unresolved: The current method relies on closed-form expressions of state and costate trajectories within subintervals. Without these, the shooting method would require numerical ODE solvers, introducing approximation errors.
- What evidence would resolve it: A demonstration of the approach on problems with cubic, exponential, or other nonlinear state equations, with analysis of accuracy degradation when numerical approximations replace closed-form solutions.

### Open Question 2
- Question: How well do the learned fluid feedback policies perform when applied to their stochastic restless bandit counterparts?
- Basis in paper: The paper mentions that fluid policies "often demonstrate strong empirical performance for their stochastic counterparts" in queueing literature, but this paper does not test stochastic versions.
- Why unresolved: The authors focus exclusively on deterministic fluid problems. The gap between fluid approximations and stochastic realities remains unexplored for the proposed method.
- What evidence would resolve it: Computational experiments applying learned OCT-H policies to stochastic restless bandit simulations, measuring regret or suboptimality relative to optimal or heuristic stochastic policies.

### Open Question 3
- Question: Can the heuristic feature augmentation technique be replaced with a more principled or automated approach for selecting nonlinear transformations?
- Basis in paper: "We propose a heuristic approach to choosing nonlinear transformations on the state variables."
- Why unresolved: The current method relies on problem-specific derivations of switching curve approximations, requiring analytical work and potentially not generalizing well to novel problem structures.
- What evidence would resolve it: Development of an automated feature selection method (e.g., based on symbolic regression or learning switch points directly) that achieves comparable or better accuracy without manual derivation.

## Limitations
- Approach is currently restricted to affine and quadratic state dynamics with concave control cost functions
- Feature augmentation heuristic lacks theoretical guarantees for arbitrary switching curve geometries
- Convergence of the shooting method depends on initial costate guesses and Broyden's method behavior

## Confidence
- **High Confidence**: Shooting method convergence proof for affine/quadratic dynamics; OCT-H classification accuracy on tested problems; fundamental index-based structure of optimal controls
- **Medium Confidence**: Data-driven feature augmentation approach; scalability claims for larger problem dimensions; practical convergence behavior across diverse problem classes
- **Low Confidence**: Theoretical guarantees for feature augmentation in complex switching curve scenarios; performance guarantees for general nonlinear dynamics beyond affine/quadratic

## Next Checks
1. **Feature augmentation ablation study**: Systematically test OCT-H performance with and without nonlinear features across all three problem classes to quantify the augmentation's contribution
2. **Cross-problem generalization**: Train on one problem class (e.g., machine maintenance) and test on another (e.g., epidemic control) to assess policy transferability
3. **Scalability boundary**: Systematically vary n and T parameters to identify where training time becomes prohibitive or accuracy degrades significantly, establishing practical limits of the approach