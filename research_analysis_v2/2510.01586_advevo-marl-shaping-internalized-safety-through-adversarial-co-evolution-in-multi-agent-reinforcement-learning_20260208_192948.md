---
ver: rpa2
title: 'AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution
  in Multi-Agent Reinforcement Learning'
arxiv_id: '2510.01586'
source_url: https://arxiv.org/abs/2510.01586
tags:
- agents
- safety
- task
- arxiv
- multi-agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AdvEvo-MARL addresses safety vulnerabilities in multi-agent systems
  by internalizing defense capabilities within task agents through co-evolutionary
  reinforcement learning. The framework jointly optimizes attackers and defenders
  in adversarial environments, where attackers generate evolving jailbreak prompts
  and defenders learn to resist these threats while maintaining task performance.
---

# AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.01586
- Source URL: https://arxiv.org/abs/2510.01586
- Reference count: 9
- Primary result: Keeps attack success rates below 20% while improving task accuracy by up to +3.67% on reasoning tasks

## Executive Summary
AdvEvo-MARL addresses safety vulnerabilities in multi-agent systems by internalizing defense capabilities within task agents through co-evolutionary reinforcement learning. The framework jointly optimizes attackers and defenders in adversarial environments, where attackers generate evolving jailbreak prompts and defenders learn to resist these threats while maintaining task performance. A key innovation is the public baseline mechanism, which enables agents within the same functional group to share a group-level mean return for advantage estimation, fostering intra-group coordination and reducing training variance. Experiments across three attack scenarios demonstrate consistent safety improvements while preserving or enhancing task performance.

## Method Summary
AdvEvo-MARL trains attacker and defender agents in a co-evolutionary framework using MARL with REINFORCE++. Attackers are first warm-up via supervised fine-tuning on curated jailbreak datasets, then jointly optimized with defenders through adversarial play. The public baseline mechanism computes advantage estimates using group-level mean returns rather than individual returns, reducing variance and promoting coordination. Training follows a curriculum where safety is prioritized initially, then task performance, using reward weighting schedules. The approach is evaluated on three attack scenarios (NetSafe, AutoInject, UserHijack) with 3-agent MAS topologies.

## Key Results
- Attack Success Rate (ASR) consistently kept below 20% across all three attack scenarios
- Task accuracy improvements up to +3.67% on reasoning tasks compared to baselines
- Baseline methods achieved ASR up to 38.33%, demonstrating significant safety improvements
- Public baseline mechanism reduces training variance and improves coordination compared to individual baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial co-evolution between attackers and defenders produces more robust safety behaviors than static defense training.
- Mechanism: Attackers learn to generate diverse jailbreak prompts through RL, while defenders simultaneously learn to resist these evolving attacks. The key insight is that defenders trained against adaptive adversaries develop generalizable safety behaviors rather than overfitting to fixed attack distributions.
- Core assumption: The co-evolutionary process reaches a stable equilibrium where defenders generalize to unseen threats.
- Evidence anchors:
  - [abstract] "jointly optimizes attackers and defenders in adversarial environments, where attackers generate evolving jailbreak prompts and defenders learn to resist these threats"
  - [Section 4.3] "attackers rewrite and refine these prompts to create more potent adversarial inputs, while defenders are simultaneously optimized"
  - [corpus] Related work "Chasing Moving Targets with Online Self-Play" supports dynamic adversary training for safety
- Break condition: If attackers converge to a narrow set of attacks or defenders overfit to training-specific attack patterns, generalization fails.

### Mechanism 2
- Claim: The public baseline mechanism reduces training variance and promotes intra-group coordination.
- Mechanism: Instead of each agent using only its own return for advantage estimation, agents within the same functional group (attackers or defenders) share the group's mean return as baseline. This enables agents to learn from peer behaviors and reduces variance in policy updates.
- Core assumption: Group-level baseline provides meaningful signal for individual agent improvement.
- Evidence anchors:
  - [abstract] "public baseline mechanism, which enables agents within the same functional group to share a group-level mean return for advantage estimation"
  - [Section 4.3] Equation 3-4 formalize the baseline computation; Figure 5 shows training stability benefits
  - [corpus] Limited direct corpus evidence for this specific mechanism; primarily paper-internal validation
- Break condition: If group composition is highly heterogeneous, the shared baseline may obscure individual agent contributions.

### Mechanism 3
- Claim: Attacker warm-up via supervised fine-tuning accelerates early exploration and prevents trivial attacks.
- Mechanism: Before MARL training, attackers are fine-tuned on a curated dataset of (harmful behavior, jailbreak prompt) pairs with reasoning traces. This injects prior knowledge of effective jailbreak strategies, preventing unproductive random exploration.
- Core assumption: The seed prompt pool covers representative attack strategies that transfer to the target MAS.
- Evidence anchors:
  - [Section 4.2] "We construct dataset D_adv consisting of paired samples... attackers rewrite and refine these prompts"
  - [Section 4.2] Describes 4,000 training samples derived from existing jailbreak datasets
  - [corpus] No direct corpus evidence for this warm-up approach
- Break condition: If the seed pool lacks diversity or doesn't match deployment attack patterns, warm-up provides limited benefit.

## Foundational Learning

- Concept: Multi-agent reinforcement learning (MARL) with policy gradients
  - Why needed here: The entire framework is built on joint optimization of multiple agents with different objectives. Understanding how credit assignment works across agents is essential.
  - Quick check question: Can you explain why standard single-agent RL would fail in a co-evolutionary setting with competing objectives?

- Concept: Advantage estimation and baseline subtraction
  - Why needed here: The public baseline mechanism modifies standard advantage calculation. Understanding why baselines reduce variance without introducing bias is critical.
  - Quick check question: What happens to policy gradient variance if you use no baseline? What if you use a baseline from a different distribution than the current policy?

- Concept: Partially observable Markov games
  - Why needed here: The formal framework (Section 3) defines the MAS as a POMG with disjoint attacker/defender sets. Understanding local observations vs. global state matters for reward design.
  - Quick check question: Why can't defenders directly observe attacker intentions in this formulation?

## Architecture Onboarding

- Component map:
  - **Attacker agents**: Initialized with warm-up SFT, then optimized via REINFORCE++ to maximize attack success. Generate jailbreak prompts by rewriting seed attacks.
  - **Defender agents**: Task agents optimized to resist attacks while maintaining task performance. Receive combined safety + task + format rewards.
  - **Training orchestrator**: Manages rollout episodes, computes group baselines, and coordinates synchronous policy updates.
  - **Reward evaluator**: Provides local (per-response) and global (system-output) rewards using LLM-as-judge.

- Critical path:
  1. Construct seed attack pool from existing jailbreak datasets
  2. Warm-up attackers via supervised fine-tuning on (behavior, attack) pairs
  3. Initialize defenders from base instruction-tuned models
  4. Run MARL episodes with adversarial play between attacker and defender groups
  5. Compute group-level baselines and update policies using REINFORCE++
  6. Monitor attacker diversity and defender robustness metrics

- Design tradeoffs:
  - Safety vs. task performance: Reward weights shift during training (safety prioritized early, task later)
  - Attacker diversity vs. attack strength: Diverse attacks improve generalization but may slow convergence
  - Group baseline vs. individual baseline: Group baseline improves coordination but may slow individual specialization
  - Warm-up quality vs. exploration: Better warm-up accelerates early training but may bias toward known attack types

- Failure signatures:
  - **ASR plateau early**: Attackers not evolving; check warm-up quality and reward scaling
  - **Task performance collapse**: Defenders overcompensating for safety; check reward weight schedule
  - **High contagion rate**: Defenders not coordinating; verify public baseline implementation
  - **Training instability (Figure 5c-d)**: Without public baseline, attacker/defender rewards oscillate

- First 3 experiments:
  1. **Sanity check**: Run vanilla agents (no safety training) against each attack scenario to establish baseline ASR and contagion rates.
  2. **Ablation on warm-up**: Train with vs. without attacker warm-up to measure early exploration efficiency (expect slower convergence without warm-up).
  3. **Public baseline validation**: Compare training dynamics with group baseline vs. individual baseline (Figure 5 reproduction).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the public baseline mechanism maintain training stability and variance reduction in systems with significantly more agents (e.g., >10 agents) or dynamic team compositions?
- Basis in paper: [inferred] Section 5.1 states, "All experiments are conducted with three agents."
- Why unresolved: The public baseline relies on a group-level mean return; as the number of agents increases, individual contributions to the group success may become obscured, potentially leading to noisy gradient estimates and the "lazy agent" problem common in MARL.
- What evidence would resolve it: Scaling experiments showing Attack Success Rate (ASR) and task accuracy trends when increasing the agent count to 10, 20, or 50 agents in the "Complete" topology.

### Open Question 2
- Question: Can defenders successfully generalize to fundamentally new attack vectors that are structurally absent from the seed prompt pool used in the warm-up phase?
- Basis in paper: [inferred] Section 4.2 notes attackers are warmed up using a "curated pool of adversarial prompts derived from representative attack strategies."
- Why unresolved: While co-evolution encourages diversity (Section 5.3), evolutionary search might remain confined to the manifold of the initial seed strategies, leaving the system vulnerable to "out-of-distribution" attacks that function differently from the training examples.
- What evidence would resolve it: Evaluation against zero-day jailbreak strategies or multi-modal attacks not represented in the initial dataset, comparing performance against static defenses.

### Open Question 3
- Question: Is the specific dynamic weight reversal schedule (prioritizing safety then task utility) strictly necessary, or does it introduce risks of catastrophic forgetting?
- Basis in paper: [inferred] Section 4.3 describes a heuristic schedule where "we prioritize safety in the first half of training... and reverse the weights afterward."
- Why unresolved: Abruptly shifting reward priorities during reinforcement learning can cause the model to discard previously learned behaviors (forgetting safety constraints) to maximize the new objective (task utility).
- What evidence would resolve it: Ablation studies comparing the proposed curriculum against static reward weightings to determine if the complex scheduling is a strict requirement for the observed safety-utility balance.

## Limitations
- Evaluation scope limited to three specific attack scenarios and relatively small model sizes (3B/7B parameters)
- Public baseline mechanism lacks extensive external validation beyond paper's internal experiments
- Warm-up approach heavily depends on quality and diversity of seed attack pool, which may not generalize to novel attack patterns

## Confidence
- **High confidence**: Core experimental results showing ASR reduction below 20% across all three attack scenarios
- **Medium confidence**: Public baseline mechanism's effectiveness in reducing training variance and improving coordination
- **Medium confidence**: Attacker warm-up's role in accelerating early exploration and preventing trivial attacks

## Next Checks
1. Test AdvEvo-MARL with larger model architectures (14B/32B) to assess scalability limits and verify whether safety improvements persist with increased model capacity
2. Evaluate against a broader attack surface including zero-shot jailbreaks and adversarial examples not represented in the seed pool to test generalization claims
3. Conduct ablation studies isolating the public baseline contribution by comparing training dynamics with individual vs. group baselines across different MAS topologies