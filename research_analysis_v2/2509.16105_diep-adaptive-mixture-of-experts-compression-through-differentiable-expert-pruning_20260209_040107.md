---
ver: rpa2
title: 'DiEP: Adaptive Mixture-of-Experts Compression through Differentiable Expert
  Pruning'
arxiv_id: '2509.16105'
source_url: https://arxiv.org/abs/2509.16105
tags:
- expert
- layer
- pruning
- experts
- diep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DiEP addresses the challenge of compressing large Mixture-of-Experts
  (MoE) models by introducing a differentiable pruning framework. Unlike existing
  methods that apply uniform pruning across layers, DiEP adaptively adjusts pruning
  rates at the layer level by jointly learning intra-layer expert importance and inter-layer
  importance scores.
---

# DiEP: Adaptive Mixture-of-Experts Compression through Differentiable Expert Pruning

## Quick Facts
- **arXiv ID**: 2509.16105
- **Source URL**: https://arxiv.org/abs/2509.16105
- **Reference count**: 40
- **Primary result**: Adaptive non-uniform pruning retains ~92% performance on Mixtral 8×7B with only half the experts

## Executive Summary
DiEP introduces a differentiable pruning framework that adaptively compresses large Mixture-of-Experts (MoE) models by learning layer-specific expert importance scores. Unlike uniform pruning methods, DiEP performs global ranking of all experts across layers based on combined intra-layer and inter-layer importance, enabling more effective removal of redundant experts. The method transforms discrete pruning into a continuous optimization problem, allowing gradient-based pruning without exhaustive search. Experiments on five advanced MoE models demonstrate significant performance retention (up to 92% on Mixtral 8×7B) while achieving 50% sparsity and improved inference efficiency through online expert skipping.

## Method Summary
DiEP learns two sets of scores: $\alpha$ (intra-layer expert importance) and $\beta$ (inter-layer importance) to compute a global importance score $s^{(l)}_i = \alpha_i^{(l)} \cdot \beta_l$ for each expert. The framework uses alternating gradient updates with a 3:1 ratio between $\alpha$ and $\beta$ to avoid interference. Experts are ranked globally and the bottom $K$ are pruned to achieve target sparsity. A reconstruction loss term encourages the pruned model to mimic the original. For inference efficiency, DiEP optionally skips experts whose outputs are highly similar to already-activated experts, based on routing weights and CKA similarity.

## Key Results
- Retains ~92% of original Mixtral 8×7B performance on MMLU at 50% sparsity
- Outperforms other pruning methods by up to 7.1% on MMLU
- Achieves 1.28× inference speedup through adaptive expert skipping
- Validated across five advanced MoE models including Mixtral, DeepSeek, and Qwen

## Why This Works (Mechanism)

### Mechanism 1: Global Non-Uniform Expert Allocation
DiEP calculates global importance scores by multiplying learnable intra-layer scores ($\alpha$) and inter-layer scores ($\beta$), then ranks all experts globally for pruning. This captures depth-dependent redundancy, as shallow layers require more experts than deeper layers. Evidence shows shallow layers retain higher average importance scores than deeper layers.

### Mechanism 2: Decoupled Gradient Optimization
The framework alternates between updating $\alpha$ (3 times) and $\beta$ (1 time) to avoid gradient interference between local expert competition and global layer balancing. This "Alternating Update Strategy" isolates expert-level contributions from layer-level contributions, with theoretical convergence analysis supporting the approach.

### Mechanism 3: Similarity-Guided Inference Skipping
DiEP dynamically skips redundant experts during inference when the routing weight of a candidate expert falls below a threshold $\gamma$ based on the primary expert's weight and CKA similarity. This improves latency with negligible performance loss, achieving up to 1.28× speedup.

## Foundational Learning

- **Sparsely-Gated Mixture of Experts (SMoE)**: Understanding how routers select "Top-k" experts per token is crucial for grasping what is being pruned.
- **Continuous Relaxation (Gumbel-Softmax/Concrete)**: DiEP transforms discrete "keep/drop" decisions into continuous softmax scores to enable gradient descent.
- **Centered Kernel Alignment (CKA)**: CKA measures similarity between expert outputs to determine functional redundancy for skipping or merging decisions.

## Architecture Onboarding

- **Component map**: Token embeddings $x^{(l)}$ -> Router -> Expert outputs $F_i^{(l)}(x^{(l)})$ -> Weighted sum $F'(x)$ -> Loss computation -> Alternating updates to $\alpha$ and $\beta$
- **Critical path**: The alternating optimization loop (Algorithm 1) with the 3:1 update ratio between $\alpha$ and $\beta$
- **Design tradeoffs**: Global ranking finds optimal expert sets but may create unbalanced layers; high reconstruction loss $\lambda$ forces close mimicry but slows pruning convergence
- **Failure signatures**: Mode collapse (dead layers with zero experts), skipping collapse (over-aggressive skipping degrading accuracy)
- **First 3 experiments**: 1) Ablation on 3:1 vs 1:1 update ratios, 2) Calibration size sensitivity (32 vs 128 vs 1024 samples), 3) Inference latency profiling with/without skipping

## Open Questions the Paper Calls Out
- The authors explicitly state their study primarily focuses on language models, leaving the effectiveness of DiEP in multimodal MoE architectures unexplored.
- They could not conduct experiments on models like Deepseek V3 (256 experts per layer) due to computational constraints.

## Limitations
- Computational cost scales with model size and expert count, making frontier-scale MoE models with thousands of experts challenging
- Optimal reconstruction loss weight $\lambda$ depends on task difficulty and model architecture, but sensitivity analysis is limited
- Risk of creating "dead layers" or severely imbalanced layers is acknowledged but not thoroughly quantified

## Confidence
- **High**: Core differentiable pruning mechanism through continuous relaxation is sound with theoretical convergence analysis
- **Medium**: Empirical performance claims are convincing on tested models but limited model diversity prevents generalization
- **Low**: Adaptive skipping mechanism effectiveness primarily demonstrated through speedup metrics rather than accuracy retention under aggressive thresholds

## Next Checks
1. After pruning Mixtral 8×7B to 50% sparsity, analyze the distribution of retained experts per layer and identify any layers with fewer than 2 retained experts
2. Apply DiEP sequentially at 25%, 50%, and 75% sparsity levels to measure whether performance degradation is linear or cumulative
3. Evaluate pruned Mixtral 8×7B on medical/biological QA datasets and coding benchmarks to test domain-specific performance retention