---
ver: rpa2
title: 'ATLAS: Automated Toolkit for Large-Scale Verified Code Synthesis'
arxiv_id: '2512.10173'
source_url: https://arxiv.org/abs/2512.10173
tags:
- test
- verified
- code
- atlas
- synthesis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ATLAS is an automated pipeline that synthesizes verified Dafny
  programs from algorithmic problems with reference implementations and test cases.
  By applying iterative refinement and soundness/completeness checks, ATLAS generates
  2.7K verified programs with machine-checked proofs.
---

# ATLAS: Automated Toolkit for Large-Scale Verified Code Synthesis

## Quick Facts
- **arXiv ID:** 2512.10173
- **Source URL:** https://arxiv.org/abs/2512.10173
- **Reference count:** 40
- **Primary result:** ATLAS generates 2.7K verified Dafny programs and trains Qwen 2.5 7B Coder to achieve 56.9% Pass@1 on DafnyBench and 65.8% Pass@5 on DafnySynthesis.

## Executive Summary
ATLAS is an automated pipeline that synthesizes verified Dafny programs from algorithmic problems with reference implementations and test cases. By applying iterative refinement and soundness/completeness checks, ATLAS generates 2.7K verified programs with machine-checked proofs. These are decomposed into 19K training examples across multiple tasks. Fine-tuning Qwen 2.5 7B Coder on this synthetic data improves performance from 32.4% to 56.9% on DafnyBench and from 15.8% to 65.8% on DafnySynthesis, demonstrating that synthetic data generation is an effective approach to scaling LLM capabilities for formal verification.

## Method Summary
The method employs a two-phase pipeline: Phase 1 generates and validates formal contracts (method signatures, specifications, and test cases) using iterative refinement guided by Dafny compiler feedback and soundness/completeness lemma verification. Phase 2 synthesizes implementations against frozen contracts with validation checks for contract adherence, Dafny verification, and test case passing. The pipeline produces verified programs that are decomposed into six task types (NL-to-Code, NL-to-Spec, Spec-to-Code, Specification Repair, Implementation Repair, and Proof Infilling), yielding 19K training examples from 2.7K verified programs. Fine-tuning uses LoRA on Qwen 2.5 7B Coder with Hugging Face TRL on 8× H100 GPUs for up to 10 epochs.

## Key Results
- ATLAS generates 2,751 verified Dafny programs from 12,800 TACO-verified problems (21% success rate)
- Fine-tuned Qwen 2.5 7B Coder achieves 56.9% Pass@1 on DafnyBench (up from 32.4%)
- ATLAS achieves 65.8% Pass@5 on DafnySynthesis (up from 15.8%)
- Ablation shows soundness/completeness checks improve DafnySynthesis Pass@5 by 7.4 percentage points (from 58.4% to 65.8%)

## Why This Works (Mechanism)

### Mechanism 1: Soundness and Completeness Checks
Test cases provide coverage to detect specification weaknesses through verification lemmas. Soundness lemmas verify contracts hold for known correct examples; completeness lemmas (via perturbation) check modified outputs are rejected. Failed lemmas generate structured feedback for LLM refinement. Assumes test cases provide sufficient coverage and Dafny's SMT solver can discharge verification lemmas.

### Mechanism 2: Multi-Task Decomposition
Each pipeline run produces intermediate failures alongside final verified programs, curated into six task types. Repair tasks comprise >35% of the dataset, teaching models error correction patterns. Assumes models benefit from learning verification-aware programming through failure analysis.

### Mechanism 3: Problem Freezing
Two-phase operation with immutable contracts in Phase 2 prevents specification weakening during iterative repair. Enforces that implementations preserve original requires clauses and provide superset of ensures clauses. Assumes decoupling prevents models from trivializing problems by weakening postconditions.

## Foundational Learning

- **Floyd-Hoare logic and weakest precondition calculus**: Dafny translates code and specifications into verification conditions via weakest precondition calculus. Essential for debugging verification failures. Quick check: Given postcondition `ensures m >= a`, what is weakest precondition for `m := a + b`?

- **Dafny's auto-active verification model**: Uses SMT solver (Z3) for automated reasoning with ghost code (assertions, loop invariants, lemmas) to guide solver. Determines what annotations models must learn to generate. Quick check: When Z3 cannot verify a loop, what annotation type is typically required?

- **SMT solver capabilities and limitations**: Soundness/completeness lemmas rely on Z3's ability to discharge verification conditions. Some proofs exceed automated theorem provers' capabilities without guidance, motivating perturbation-based completeness over contradiction-based proofs. Quick check: Why might a contradiction lemma fail to verify even when specification is complete?

## Architecture Onboarding

**Component map:**
TACO-verified dataset → CONTRACT GENERATION (signature, specifications, test cases, soundness/completeness checks) → IMPLEMENTATION GEN (code, proofs, contract adherence, verification, test passing) → TASK DECOMPOSITION (6 task types, 7× expansion) → SFT (LoRA, Qwen 2.5 7B)

**Critical path:** Soundness/completeness check in Phase 1 is the gating factor for data quality. Without it, specifications progressively weaken during iterative correction, confirmed by 7.4 percentage point drop on DafnySynthesis when removing s&c checks.

**Design tradeoffs:**
- Perturbation-based completeness vs. contradiction-based: Trades formal guarantees for practical automation
- Allowing specification regeneration during implementation: Permits regenerating specs in Phase 2 with adherence constraints because models struggle to output only implementations
- Problem difficulty vs. success rate: EASY problems achieve 47% success; VERY HARD drops to ~20%

**Failure signatures:**
- Soundness lemma failure: Contract rejects known-correct input-output pair → specification overly restrictive
- Perturbation lemma success (bad): Contract accepts modified output → postconditions insufficiently constrained
- Contract adherence violation: Implementation changes requires clauses or removes ensures clauses → reject and retry
- Verification timeout: Complex proofs may exceed resource limits

**First 3 experiments:**
1. Reproduce pipeline on 100 TACO-verified problems, measuring success rate by difficulty tier and comparing to reported ~21% overall
2. Ablate soundness vs. completeness checks separately to isolate individual contributions to DafnySynthesis improvement
3. Evaluate on Verus or F* to test language portability claims and identify Dafny-specific components

## Open Questions the Paper Calls Out

1. What reward structures would enable effective reinforcement learning for verification-aware code generation? (Basis: Authors state application to RL remains underexplored due to lack of well-defined reward structures.)

2. Why does ATLAS-trained model plateau at Pass@5 while larger frontier models continue improving with additional attempts? (Basis: ATLAS achieves Pass@5=56.9% equals Pass@10, while Claude 3 Opus improves from 53.8% to 67.8%.)

3. How can completeness verification be strengthened when automated provers cannot discharge contradiction lemmas? (Basis: Perturbation approach trades formal completeness guarantees for practical automated verification.)

4. Does formally verifying operations on groups of objects present fundamentally greater challenges than discrete element manipulations? (Basis: Authors observe range query problems involving intervals present greater verification challenges.)

## Limitations
- Soundness/completeness checks rely on test case coverage that may be insufficient for complex specifications
- Perturbation-based completeness approach trades formal guarantees for practical automation without quantifying how often it succeeds versus contradiction-based approaches
- Pipeline success heavily depends on TACO-verified dataset quality and assumption that Python reference implementations provide adequate specifications
- Ablation study only examines combined soundness/completeness removal, not individual contributions

## Confidence

**High confidence:** Core pipeline architecture and two-phase separation is clearly specified and validated through 2.7K verified programs and downstream benchmark improvements.

**Medium confidence:** Multi-task decomposition into 19K training examples is well-documented, but relative contribution of each task type to performance gains is not explicitly measured.

**Medium confidence:** Problem freezing mechanism preventing specification weakening is theoretically sound but empirical evidence is limited to single ablation showing 7.4 percentage point drop.

## Next Checks

1. Isolate soundness vs. completeness contributions by removing only soundness checks, then only completeness checks, measuring impact on DafnySynthesis Pass@5.

2. Evaluate perturbation lemma effectiveness by generating both perturbation-based and contradiction-based completeness lemmas for 100 problems, measuring success rates.

3. Cross-language portability test by porting Phase 1 to Verus or F* while keeping Phase 2 unchanged, measuring success rate and identifying Dafny-specific components.