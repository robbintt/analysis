---
ver: rpa2
title: 'From Sight to Insight: Improving Visual Reasoning Capabilities of Multimodal
  Models via Reinforcement Learning'
arxiv_id: '2601.00215'
source_url: https://arxiv.org/abs/2601.00215
tags:
- reasoning
- reward
- visual
- answer
- think
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies that multimodal large language models struggle
  with visual puzzles due to inadequate visual perception rather than algorithmic
  reasoning. The authors propose using reinforcement learning with custom reward functions
  to incentivize longer, more grounded visual reasoning without relying on dense chain-of-thought
  annotations.
---

# From Sight to Insight: Improving Visual Reasoning Capabilities of Multimodal Models via Reinforcement Learning

## Quick Facts
- arXiv ID: 2601.00215
- Source URL: https://arxiv.org/abs/2601.00215
- Reference count: 20
- Qwen-2.5-VL-7B trained with GRPO and custom rewards achieves up to 5.56% improvement over baseline on visual puzzle tasks

## Executive Summary
This paper identifies that multimodal large language models (MLLMs) struggle with visual puzzles primarily due to visual perception gaps rather than reasoning deficits. The authors demonstrate that converting images to structured text descriptions yields substantial performance gains (26.7% for Claude-3.5, 23.6% for Claude-3.7), isolating perception as the bottleneck. They propose using reinforcement learning with custom reward functions to incentivize longer, more grounded visual reasoning without requiring dense chain-of-thought annotations. Six reward functions are evaluated using GRPO on Qwen-2.5-VL-7B, achieving up to 5.56% improvement over baseline and reducing drift toward algorithmic-only reasoning.

## Method Summary
The method uses Qwen-2.5-VL-7B as the base policy model, trained via Group Relative Policy Optimization (GRPO) with custom reward functions. The training data consists of 9,000 synthetically generated samples across four puzzle categories (Clock, Maze, Move Box, N-Queens). Six reward functions are implemented: Vanilla, Only-Accuracy, Mixture, Continuous, Visual-Fusion, and No-Accuracy. These rewards incentivize structured output with tags like `<image description>`, `思索`, `<rethink>`, and `<answer>`, scoring unique content within tags using tanh smoothing and repetition penalties. The models are evaluated on the AlgoPuzzleVQA benchmark in open-ended settings.

## Key Results
- Qwen-2.5-VL-7B with GRPO and custom rewards achieves up to 5.56% improvement over baseline on visual puzzle tasks
- Text-only substitution yields 26.7% improvement for Claude-3.5 and 23.6% for Claude-3.7, confirming perception as the primary bottleneck
- Diverse training (8k samples across 4 categories) achieves 15.56% average accuracy on out-of-domain tasks vs. 10.5% baseline
- Single-category training (Clock-3k) shows limited out-of-domain generalization (4.25% accuracy)

## Why This Works (Mechanism)

### Mechanism 1: Perception-Bottleneck Diagnosis via Textual Substitution
Converting images to structured textual descriptions significantly improves performance (26.7% for Claude 3.5, 23.6% for Claude 3.7), isolating visual perception as the primary bottleneck rather than reasoning deficits.

### Mechanism 2: Reward-Shaped Reasoning Length via Tag-Based Scoring
Reward functions distribute credit across structured reasoning tags, incentivizing longer, more grounded reasoning trajectories without supervised chain-of-thought data through unique content scoring and repetition penalties.

### Mechanism 3: Generalization via Diverse Training Distribution
Training on diverse puzzle categories (4 categories, 8k samples) improves out-of-domain generalization more than single-category hard-sample training by exposing models to varied visual reasoning patterns.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**: The RL algorithm used to update policy models based on reward signals; understanding its exploration/exploitation balance is critical for debugging reward hacking
  - Quick check: How does GRPO differ from standard PPO in handling group-wise reward comparisons?

- **Reward Hacking and Repetition Penalization**: The paper explicitly penalizes duplicate content within tags using `tanh(unique_sentences) - tanh(duplicate_sentences)` to prevent models from generating repetitive filler
  - Quick check: What happens to the reward if a model generates the same sentence 10 times within a `<think/>` block?

- **Partial Credit in Continuous Rewards**: Continuous rewards assign fractional credit for near-correct answers (e.g., clock times within 2 hours), providing denser learning signals for numeric/temporal outputs
  - Quick check: For a ground-truth answer of 5, what reward does a prediction of 6 receive under the continuous numeric formula?

## Architecture Onboarding

- **Component map**: Policy Model (Qwen-2.5-VL-7B) -> Reward Calculator (tag parser + unique-content counter + tanh smoothing) -> GRPO Trainer (group-relative rewards) -> Model weights update
- **Critical path**: 1. Prompt template selection -> 2. Model generates tagged completion -> 3. Reward calculator extracts tags, counts unique content -> 4. GRPO computes policy gradient -> 5. Model weights update
- **Design tradeoffs**: Vanilla vs. Mixture (simpler vs. structure encouragement), Accuracy-only vs. Multi-component (robustness vs. interpretability), KL coefficient = 0.0 (exploration vs. drift risk)
- **Failure signatures**: Visual grounding failure (misidentifies image elements), Algorithmic error (correct vision but wrong arithmetic), Contradiction (reasoning vs. final answer mismatch), No-answer generation (fails to close `<answer/>` tag)
- **First 3 experiments**: 1. Baseline comparison (Qwen-2.5-VL-7B on AlgoPuzzleVQA) to establish accuracy floor, 2. Reward ablation on single category (Clock-3k) to compare in-domain accuracy and reasoning length, 3. Diversity scaling test (Diverse-2k vs. Diverse-8k) to validate generalization hypothesis

## Open Questions the Paper Calls Out

- **LLM-based judges for visual grounding**: The paper suggests that incorporating LLM-based judges to evaluate intermediate reasoning steps could improve visual grounding accuracy, representing a promising direction for future research
- **Structured rewards vs. accuracy-only performance**: Only-Accuracy reward performs on par with or better than structured rewards in out-of-domain settings despite producing less interpretable reasoning, leaving the relationship between reasoning structure and generalization unclear
- **ID/OOD trade-off mechanisms**: The paper observes a trade-off between in-domain accuracy and out-of-domain generalization, hypothesizing overfitting during RL training, but doesn't empirically validate this through regularization experiments or representation analysis

## Limitations

- The perception-bottleneck diagnosis relies heavily on textual substitution experiments that may not generalize across diverse MLLM architectures
- The reward function design assumes structured tag generation correlates with improved visual reasoning without conclusively demonstrating that longer reasoning equals better visual perception
- The claim that diverse training improves out-of-domain generalization lacks direct evidence from the corpus for this pattern in visual reasoning tasks

## Confidence

- **High confidence**: MLLMs struggle with visual puzzles more than algorithmic reasoning, supported by the substantial text-only performance gap (26.7% improvement for Claude-3.5)
- **Medium confidence**: GRPO with custom rewards improves visual reasoning, as evidenced by the 5.56% baseline improvement, though this may depend heavily on implementation details
- **Low confidence**: Diverse training improves out-of-domain generalization, as the corpus provides no direct evidence for this generalization pattern in visual reasoning tasks

## Next Checks

1. **Reward Function Ablation Study**: Systematically test each of the six reward functions on a held-out validation set to determine which components contribute most to performance gains, isolating the impact of mixture reward design

2. **Generalization Stress Test**: Evaluate RL-trained models on puzzle categories not present in training data (e.g., Sudoku, Tangrams) to verify whether claimed generalization benefits from diverse training hold across truly unseen visual reasoning patterns

3. **Perceptual Grounding Analysis**: Conduct human evaluation of model outputs to distinguish between cases where models generate correct final answers through accurate visual perception versus cases where they arrive at correct answers through algorithmic reasoning despite visual misinterpretations