---
ver: rpa2
title: 'Adjusting Model Size in Continual Gaussian Processes: How Big is Big Enough?'
arxiv_id: '2408.07588'
source_url: https://arxiv.org/abs/2408.07588
tags:
- inducing
- size
- points
- learning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of automatically determining the
  appropriate size of a model in continual learning scenarios, where data arrives
  incrementally and the final dataset size is unknown. The authors focus on Gaussian
  processes (GPs) and develop a method to adaptively adjust the number of inducing
  points (analogous to neurons) as new data arrives, maintaining near-optimal performance
  while minimizing computational waste.
---

# Adjusting Model Size in Continual Gaussian Processes: How Big is Big Enough?

## Quick Facts
- arXiv ID: 2408.07588
- Source URL: https://arxiv.org/abs/2408.07588
- Reference count: 40
- The paper develops VIPS (Vegas Inducing Point Selection) to adaptively determine the number of inducing points needed in continual Gaussian process learning, maintaining near-optimal performance with minimal computational waste.

## Executive Summary
This paper addresses the challenge of automatically determining the appropriate model size for Gaussian processes in continual learning scenarios where data arrives incrementally and the final dataset size is unknown. The authors develop a method that adaptively adjusts the number of inducing points (analogogs to neurons) as new data arrives, maintaining near-optimal performance while minimizing computational waste. Their approach, called Vegas Inducing Point Selection (VIPS), uses a principled stopping criterion based on the KL divergence between the approximate and true posterior, selecting new inducing points from incoming data using a greedy variance criterion.

## Method Summary
VIPS operates by iteratively adding inducing points until a stopping criterion based on the KL divergence between the approximate and true posterior is met. The algorithm computes the gap between a computable online Evidence Lower Bound (ELBO, $\hat{L}$) and a derived upper bound ($\hat{U}$). While this gap exceeds a threshold $\alpha$, the model is considered to have insufficient capacity, and new inducing points are added. New inducing points are selected from incoming data using a greedy variance criterion that maximizes the reduction in marginal variance. The method requires only a single hyperparameter $\delta$ that works consistently across diverse datasets without dataset-specific tuning.

## Key Results
- VIPS achieves minimal model sizes while maintaining accuracy thresholds, outperforming other adaptive methods on UCI datasets and a robotics application.
- The method requires only a single hyperparameter ($\delta \approx 0.035$) that works consistently across diverse datasets without dataset-specific tuning.
- Experimental results demonstrate that VIPS maintains performance within 5-10% of full-batch GP while using significantly fewer inducing points.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Model capacity is adaptively adjusted to bound the approximation error relative to the true posterior.
- **Mechanism:** The algorithm computes the gap between a computable online ELBO ($\hat{L}$) and a derived upper bound ($\hat{U}$). While the gap $\hat{U} - \hat{L} > \alpha$, the model is considered to have insufficient capacity, and new inducing points are added.
- **Core assumption:** The online upper bound $\hat{U}$ is a valid surrogate for the optimal marginal likelihood $L^*$.
- **Evidence anchors:**
  - Uses a "stopping criterion based on the KL divergence between the approximate and true posterior."
  - Provides the guarantee: $KL[q_n(f)||p(f|y_o, y_n, \theta_o)] \le \alpha + \Psi$.
- **Break condition:** If kernel hyperparameters $\theta$ shift drastically between batches, the guarantee degrades.

### Mechanism 2
- **Claim:** A single hyperparameter ($\delta$) can control the accuracy-computation trade-off across diverse datasets without retuning.
- **Mechanism:** VIPS normalizes the stopping threshold $\alpha$ relative to a "noise code" baseline ($L_{noise}$), scaling the tolerance to the intrinsic information content of the specific data stream.
- **Core assumption:** The noise model baseline $L_{noise}$ provides a consistent reference frame for "information captured" across different data distributions.
- **Evidence anchors:**
  - States the method "requires only a single hyperparameter that works consistently across diverse datasets."
  - Defines $\alpha = \delta(\hat{U} - L_{noise})$, taking inspiration from compression/MDL.
- **Break condition:** Datasets with extreme dynamic range or non-stationary noise profiles might violate the assumption.

### Mechanism 3
- **Claim:** Inducing points are selected from new data based on the reduction of marginal variance in the function space.
- **Mechanism:** The "greedy variance" criterion iteratively selects the input location $x$ that maximizes the posterior variance $k(x,x) - k_b(x)^\top K_{bb}^{-1}k_b(x)$.
- **Core assumption:** The marginal variance is a sufficient statistic for determining the "information gain" of a candidate inducing point.
- **Evidence anchors:**
  - Mentions selecting points "using a greedy variance criterion."
  - Algorithm details the selection: $x = \arg\max_{x \in X_n} k(x,x) - \dots$.
- **Break condition:** If the input space expands into regions completely disconnected from previous inducing points, the greedy selection might initially miss optimal placement.

## Foundational Learning

- **Concept:** **Sparse Variational Gaussian Processes (SVGPs)**
  - **Why needed here:** VIPS is fundamentally an extension of SVGPs to the continual learning domain. You must understand that inducing points approximate the full GP posterior and that the ELBO measures the quality of this approximation.
  - **Quick check question:** How does the computational cost of an SVGP scale with the number of inducing points $M$?

- **Concept:** **Online / Continual ELBO**
  - **Why needed here:** The mechanism relies on optimizing a specific objective function ($\hat{L}$) that approximates the full-batch ELBO using only the current batch and summary statistics from old batches.
  - **Quick check question:** In the online ELBO derivation, what role does the "approximate likelihood" term $l(a)$ play regarding past data?

- **Concept:** **KL Divergence & Information Theory**
  - **Why needed here:** The stopping criterion is motivated by bounding the KL divergence and uses concepts from Minimum Description Length (MDL) to set thresholds relative to a noise baseline.
  - **Quick check question:** Why is a raw constant threshold for KL divergence insufficient when comparing models trained on datasets of varying sizes?

## Architecture Onboarding

- **Component map:**
  1. **Data Stream:** Input batches $X_n, y_n$.
  2. **Summary Statistics:** Stored parameters from the previous batch (old inducing locations $Z_o$, variational params $\tilde{m}_a, D_a$).
  3. **Selector (VIPS Loop):** Greedy variance module proposes points; KL-Gap module evaluates the stopping criterion ($\hat{U} - \hat{L} \le \alpha$).
  4. **Optimizer:** L-BFGS updates variational parameters and hyperparameters using the online ELBO ($\hat{L}$).

- **Critical path:** The calculation of the Upper Bound $\hat{U}$ and Lower Bound $\hat{L}$ inside the loop.

- **Design tradeoffs:**
  - **Strictness vs. Speed:** Setting $\delta$ too low increases accuracy but causes linear/uncontrolled growth of $M$.
  - **Candidate Pool:** Restricting new inducing points to the current batch $X_n$ vs. the union of old and new ($Z_o \cup X_n$).

- **Failure signatures:**
  - **Catastrophic Oversizing:** If $\delta$ is set too low or the kernel lengthscale is initialized too small, the model may add inducing points indefinitely.
  - **Stagnation:** If $\delta$ is too high, the model stops adding points early, and performance lags behind the "Exact GP" baseline.
  - **Hyperparameter Drift:** If kernel hyperparameters $\theta$ are updated too aggressively without sufficient capacity, the bound guarantees may weaken.

- **First 3 experiments:**
  1. **Synthetic Validation:** Generate data from $y = \sin(2x) + \cos(5x)$. Stream batches and plot the number of inducing points $M$ over time against the RMSE.
  2. **Hyperparameter Sensitivity:** Run VIPS on "Kin8nm" and "Naval" datasets using a fixed $\delta=0.05$. Check if the final model sizes and RMSEs are comparable to the results in Table 1 without tuning $\delta$ per dataset.
  3. **Ablation on Threshold:** Compare the "Noise-Relative" threshold ($\alpha = \delta(\hat{U} - L_{noise})$) against a "Fixed" threshold.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the variational stopping criterion developed for sparse Gaussian Processes be effectively adapted to dynamically adjust the width and structure of deep neural networks?
- **Basis in paper:** The authors state that while their method finds the number of neurons for a single-layer network, they "hope the ideas presented in this work can inspire similar mechanisms for adaptive neural networks."
- **Why unresolved:** The theoretical equivalence is currently limited to single-layer neural networks (via the arc-cosine kernel), and the implementation has not been extended to multi-layer deep architectures.
- **What evidence would resolve it:** A derivation of a growth criterion for deep networks based on similar KL-divergence principles, accompanied by experimental results showing reduced parameter waste without performance loss.

### Open Question 2
- **Question:** Does dynamically optimizing the locations of previously selected inducing points during continual learning yield a significant improvement in approximation quality over the fixed-point method used in this study?
- **Basis in paper:** Section 4 notes that while the authors fix old inducing points "for the sake of simplicity," optimizing them "does lead to a strictly better approximation."
- **Why unresolved:** The paper does not quantify the trade-off between the computational cost of re-optimizing old points versus the marginal gain in model accuracy or compression.
- **What evidence would resolve it:** Ablation studies comparing the final model sizes and predictive accuracy of the current fixed approach against a version that re-optimizes all inducing points at each batch.

### Open Question 3
- **Question:** How can the stopping criterion and threshold calculation be efficiently modified for continual learning scenarios with very large batch sizes where the exact calculation of the upper bound becomes intractable?
- **Basis in paper:** Section 4.3 acknowledges that calculating the specific threshold $L^*$ is "intractable for large batch sizes $N_n$," and while an upper bound is mentioned, it is noted as being computationally heavier due to constant factors.
- **Why unresolved:** The proposed method relies on a calculation that is only feasible for the small batch sizes typical of standard continual learning benchmarks; scaling to high-throughput streams remains unaddressed.
- **What evidence would resolve it:** A modified algorithm utilizing a scalable approximation of the upper bound that maintains the theoretical guarantees of the VIPS method on datasets with significantly larger batch sizes.

## Limitations

- The method's guarantees weaken if kernel hyperparameters shift significantly between batches, potentially leading to insufficient model capacity.
- The computational complexity of calculating the upper bound becomes prohibitive for very large batch sizes, potentially requiring approximations not fully explored in the paper.
- While the single hyperparameter approach works across tested datasets, broader validation is needed for highly heterogeneous data with extreme non-stationarity.

## Confidence

- **High:** The core mechanism of using KL divergence bounds for adaptive model sizing is well-founded and mathematically rigorous.
- **Medium:** The single hyperparameter approach ($\delta=0.035$) works across tested datasets, but broader validation is needed for highly heterogeneous data.
- **Medium:** The greedy variance selection criterion is effective, though its optimality in highly non-stationary domains remains unproven.

## Next Checks

1. Test VIPS on a dataset with known non-stationary kernel hyperparameters to quantify the degradation of KL bounds when $\theta_o \neq \theta_n$.
2. Implement a variant that uses the looser bound ($\bar{\alpha}$) for large batches and compare performance/accuracy trade-offs.
3. Evaluate VIPS on a dataset with rapidly expanding input space (e.g., time-series with structural breaks) to test the greedy variance criterion's ability to discover optimal inducing point locations.