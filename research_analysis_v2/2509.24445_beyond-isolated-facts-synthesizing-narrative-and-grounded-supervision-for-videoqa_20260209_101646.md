---
ver: rpa2
title: 'Beyond Isolated Facts: Synthesizing Narrative and Grounded Supervision for
  VideoQA'
arxiv_id: '2509.24445'
source_url: https://arxiv.org/abs/2509.24445
tags:
- video
- data
- pairs
- supervision
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework for synthesizing richer supervision
  signals for Video Question Answering (VideoQA) beyond isolated question-answer pairs.
  The proposed approach, Question-Based Paraphrasing (QBP) and Question-Based Captioning
  (QBC), transforms fragmented QA pairs into narrative-level descriptions and fine-grained
  visual rationales respectively.
---

# Beyond Isolated Facts: Synthesizing Narrative and Grounded Supervision for VideoQA

## Quick Facts
- arXiv ID: 2509.24445
- Source URL: https://arxiv.org/abs/2509.24445
- Reference count: 22
- Primary result: 3B model achieves 72.5% on STAR (+4.9%) and 7B model reaches 80.8% on NExT-QA (+4.6%)

## Executive Summary
This paper introduces a framework for synthesizing richer supervision signals for Video Question Answering (VideoQA) beyond isolated question-answer pairs. The proposed approach, Question-Based Paraphrasing (QBP) and Question-Based Captioning (QBC), transforms fragmented QA pairs into narrative-level descriptions and fine-grained visual rationales respectively. Experiments on STAR and NExT-QA benchmarks demonstrate significant accuracy improvements, with a 3B model achieving 72.5% on STAR (+4.9%) and a 7B model reaching 80.8% on NExT-QA (+4.6%). The framework also shows substantial benefits in cross-dataset generalization and accelerates model convergence by over 2.5x. Human evaluation confirms high factual consistency and logical coherence of the synthesized data, validating its effectiveness as training supervision.

## Method Summary
The framework synthesizes two types of supervisory data from existing QA pairs: QBP generates narrative paragraphs by consolidating all QA pairs for a video into a single coherent description, while QBC generates visual rationales by conditioning caption generation on the ground truth answer to ground responses in specific visual evidence. These are produced offline using a text LLM (e.g., GPT-4o) for QBP and an MLLM (e.g., Gemini) for QBC, then used to fine-tune VideoQA models (Qwen2.5-VL, MiMo-VL) with standard next-token prediction. The combined dataset $D_{QBP} \cup D_{QBC}$ is used for training, which demonstrates faster convergence and better cross-dataset generalization compared to training on raw QA pairs alone.

## Key Results
- 3B model achieves 72.5% accuracy on STAR benchmark (+4.9% over baseline)
- 7B model reaches 80.8% accuracy on NExT-QA benchmark (+4.6% over baseline)
- Training on QBP data accelerates convergence by more than 2.5x (220 steps vs 600 steps to plateau)
- QBP narratives improve cross-dataset generalization compared to raw QA fine-tuning
- Human evaluation shows high factual consistency and logical coherence of synthesized data

## Why This Works (Mechanism)

### Mechanism 1: Semantic Density & Dependency Integration (QBP)
Consolidating fragmented QA pairs into a single narrative paragraph reduces training redundancy and exposes latent causal/temporal dependencies, accelerating convergence. The Question-Based Paraphrasing (QBP) process acts as a "reasoning integrator." Instead of sampling isolated facts (low semantic density), the model ingests a holistic narrative that explicitly links entities and actions (e.g., linking "sitting" to "resting" via "snowmobile"). This forces the model to learn the *connective tissue* of events rather than just atomic correlations.

### Mechanism 2: Constrained Rationale Generation (QBC)
Conditioning the synthesis of visual captions on the *ground truth answer* creates a "visual rationale" that grounds reasoning in evidence, mitigating hallucination. Question-Based Captioning (QBC) inverts the standard generation process. By providing the answer $A$ to the MLLM alongside the question $Q$ and video $V$, the task shifts from "predict the answer" to "locate the evidence for the known answer." This creates a training signal where the model learns to associate specific visual features with the *correctness* of an answer.

### Mechanism 3: Overfitting Mitigation via Structure Regularization
Training on synthesized narratives generalizes better to unseen datasets than training on raw QA pairs because it abstracts away dataset-specific linguistic biases in favor of event structure. Raw QA pairs often contain specific phrasing patterns (biases) unique to a dataset (e.g., NExT-QA). QBP rephrases this content into a standardized narrative format. When the model trains on this synthesized distribution, it learns the underlying *events* rather than the *style* of the source dataset, reducing overfitting.

## Foundational Learning

- **Concept: Next-Token Prediction (Unified Objective)**
  - Why needed: The framework treats QA, Narratives, and Captions as text generation tasks. Understanding that the model uses the exact same loss function (cross-entropy) for a 2-word answer and a 100-word narrative is crucial.
  - Quick check: Does the model require a separate classification head for the QBP/QBC data? (No, it uses the standard LM head).

- **Concept: Visual Grounding**
  - Why needed: QBC relies on the MLLM's ability to "ground" text to pixels. Without this, QBC cannot produce "visual rationales."
  - Quick check: How does the model distinguish between describing a generic action and identifying the specific evidence that proves a specific answer?

- **Concept: Semantic Density**
  - Why needed: The paper argues QBP works because it compresses multiple low-density samples (QA pairs) into one high-density sample (Narrative).
  - Quick check: Why would training on fewer samples (3k narratives vs 30k QA pairs) lead to faster convergence? (Because each sample carries more information/gradient signal per step).

## Architecture Onboarding

- **Component map:** Source Dataset -> [LLM (e.g., GPT-4)] -> QBP Narratives; Source Dataset + Video -> [MLLM (e.g., Gemini)] -> QBC Rationales -> VideoQA Model (e.g., Qwen2.5-VL) -> Input: Video + Synthesized Text -> Output: Text Generation.

- **Critical path:** The quality of the **QBC Rationales**. If the MLLM generates "justified fabrications" (hallucinating evidence to match the answer), the student model will learn to hallucinate.

- **Design tradeoffs:**
  - *Cost vs. Convergence:* QBP/QBC requires expensive API calls to advanced LLMs/MLLMs for synthesis. The tradeoff is a 2.5x reduction in GPU hours for the target VideoQA model.
  - *Diversity vs. Coherence:* QBP forces coherence, potentially losing the diverse, messy nature of real human questions.

- **Failure signatures:**
  - **Justified Fabrication:** The model learns to output fluent, confident explanations for wrong answers or non-existent visual details (identified in Section 4.1).
  - **Temporal Confusion:** QBP narratives may incorrectly order events if the seed QA pairs lack explicit temporal markers (identified in Section 4.1).
  - **Entity Drift:** The narrative refers to "the person" or "he" when the video actually shows multiple distinct people, confusing the student model.

- **First 3 experiments:**
  1. **Validation of Synthesis Quality:** Randomly sample 100 QBP/QBC outputs and manually verify "Factual Consistency" against the raw video/QA to ensure the synthesizer isn't hallucinating (replicating the human eval in Section 4.1).
  2. **Convergence Benchmarks:** Train two identical 3B models, one on Raw NExT-QA and one on QBP-only data. Plot validation loss over steps to verify the claimed 2.5x convergence speedup.
  3. **Ablation on Grounding:** Train a model with QBC where the Ground Truth answer is *not* provided to the synthesizer. Compare accuracy to the standard QBC setup to measure the value of "answer-conditioned" rationales.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the "justified fabrication" failure mode in Question-Based Captioning (QBC) be automatically detected and filtered without relying on human evaluation?
- Basis in paper: [explicit] The authors explicitly identify "justified fabrication," where the MLLM "invents plausible-sounding visual details to rationalize the answer," as the most notable failure mode of QBC (Page 7).
- Why unresolved: The current framework relies on the MLLM's integrity and standard prompting, which does not guarantee visual groundedness when evidence is ambiguous or subtle.
- What evidence would resolve it: The development of an automated verification metric or module that correlates with human judgments of "justified fabrication" and improves model accuracy when applied as a filter during data synthesis.

### Open Question 2
- Question: Does incorporating explicit temporal constraints or timestamps into the Question-Based Paraphrasing (QBP) prompt eliminate the observed temporal ordering errors?
- Basis in paper: [explicit] The paper notes that QBP exhibits "occasional errors in temporal ordering," which is "sometimes exacerbated by imprecise temporal boundary annotations in the source QA pairs" (Page 7).
- Why unresolved: The current QBP synthesis relies solely on text-based QA pairs which often lack explicit time markers, forcing the LLM to infer sequence from potentially ambiguous text.
- What evidence would resolve it: An ablation study showing that augmenting the QBP input with ground-truth timestamp data significantly reduces logical coherence errors in the generated narratives.

### Open Question 3
- Question: Is the 2.5x acceleration in convergence attributed to the semantic density of narratives or the reduction in training sample count?
- Basis in paper: [explicit] The authors state they "hypothesize" the speedup is due to semantic density reducing redundancy, but acknowledge the shift from ~30k QA pairs to ~3k paragraphs (Page 8).
- Why unresolved: The experiment compares two distinct variables simultaneously: the *form* of the data (narrative vs. QA) and the *volume* of the data (3k vs 30k instances).
- What evidence would resolve it: A controlled comparison where a model is trained on a random subset of raw QA pairs equal in number to the QBP paragraphs (3k), to isolate the effect of the narrative format from the effect of dataset size.

## Limitations
- The approach fundamentally depends on the synthesis quality of large language and multimodal models, creating a fragile dependency chain.
- While narrative consolidation (QBP) shows benefits, it may obscure the diversity of human question patterns that real-world VideoQA systems must handle.
- The cross-dataset generalization improvements may not generalize to domains with fundamentally different visual distributions beyond the evaluated online video benchmarks.

## Confidence
- **High confidence:** The core technical contribution (QBP and QBC synthesis framework) and its implementation details are well-specified and reproducible. The claimed accuracy improvements on STAR (72.5%, +4.9%) and NExT-QA (80.8%, +4.6%) benchmarks are directly measurable.
- **Medium confidence:** The convergence speedup claim (2.5x) relies on precise synthesis model behavior that may vary across implementations. The cross-dataset generalization benefits depend heavily on the assumption that narrative consolidation abstracts away dataset-specific biases effectively.
- **Low confidence:** The human evaluation results for factual consistency and logical coherence, while promising, are based on a small sample (100 samples) and may not capture edge cases where justified fabrication or temporal confusion occurs systematically.

## Next Checks
1. **Controlled hallucination audit:** Generate 200 QBC rationales and have three independent annotators verify whether each visual rationale contains at least one hallucinated detail that is not present in the corresponding video clip. Calculate the percentage of "justified fabrications" to quantify the risk.
2. **Temporal coherence stress test:** Create a synthetic test set where the ground truth QA pairs contain contradictory temporal information (e.g., "What happens before the person sits?" vs. "What happens after the person sits?"). Measure whether QBP narratives maintain temporal coherence or introduce contradictions.
3. **Distribution shift robustness:** Fine-tune the same model architecture on three different supervision types (raw QA pairs, QBP-only, QBC-only) and evaluate performance on both in-distribution and out-of-distribution VideoQA datasets (e.g., NExT-QA â†’ STAR, and additionally test on a surveillance or instructional video dataset not seen in the original paper).