---
ver: rpa2
title: Why Cannot Large Language Models Ever Make True Correct Reasoning?
arxiv_id: '2508.10265'
source_url: https://arxiv.org/abs/2508.10265
tags:
- reasoning
- correct
- llms
- premises
- conclusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper argues that large language models (LLMs) can never
  achieve true correct reasoning due to fundamental limitations in their working principle.
  The author defines correct reasoning as a process where premises provide conclusive
  relevant evidence for the conclusion, and establishes that the fundamental logic
  underlying correct reasoning must satisfy three essential requirements: relevance,
  ampliative capability, and paracompleteness/paraconsistency.'
---

# Why Cannot Large Language Models Ever Make True Correct Reasoning?

## Quick Facts
- arXiv ID: 2508.10265
- Source URL: https://arxiv.org/abs/2508.10265
- Authors: Jingde Cheng
- Reference count: 0
- Key outcome: LLMs cannot achieve true correct reasoning due to fundamental architectural limitations preventing embedding of logical validity criteria

## Executive Summary
This paper argues that large language models cannot achieve true correct reasoning due to fundamental architectural limitations. The author defines correct reasoning as a process where premises provide conclusive relevant evidence for conclusions, requiring a logical system that satisfies three essential requirements: relevance, ampliative capability, and paracompleteness/paraconsistency. Strong relevant logics (SRLs) are claimed as the only logic family meeting these criteria. The paper contends that LLMs, being probabilistic generative models, cannot embed formal correctness evaluation criteria or dynamic evaluation mechanisms, making true correct reasoning impossible regardless of scale or training data.

## Method Summary
This is a position paper presenting a theoretical argument rather than an empirical study. The author establishes a strict definitional framework for "correct reasoning" and argues that only strong relevant logics satisfy the necessary requirements. The paper analyzes why LLM architecture (probability theory, statistics, deep learning, token-by-token generation) fundamentally prevents embedding of logical validity criteria. The argument is built on logical analysis and philosophical reasoning rather than experimental evidence, referencing the author's prior work on strong relevant logics.

## Key Results
- LLMs can only simulate reasoning patterns from training data but cannot generate truly correct reasoning
- The architecture of LLMs (probabilistic + statistical + incremental generation) prevents embedding of formal logical validity criteria
- Users' perception of LLM reasoning ability is an illusion caused by pattern copying, the ELIZA effect, and fluent communication

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Correct reasoning requires premises that provide conclusive relevant evidence for conclusions, and the underlying logic must satisfy three requirements: relevance, ampliative capability, and paracompleteness/paraconsistency.
- Mechanism: The author establishes a strict definitional framework where reasoning correctness depends on the logical relation between premises and conclusion—not merely on output quality or pattern matching. The framework excludes classical mathematical logic because it uses the Classical Account of Validity (CAV), which ignores relevance and represents conditionals as material implication.
- Core assumption: Strong relevant logics (SRLs: Rc, Ec, Tc) are claimed as the only logic family satisfying all three requirements.
- Evidence anchors:
  - [section] "The present author considers that the fundamental logic system underlying correct reasoning must satisfy the following three essential requirements: First, as a general logical criterion for the validity of reasoning, the fundamental logic must be able to underlie correct reasoning as well as truth-preserving reasoning in the sense of conditional... Second, the fundamental logic must be able to underlie ampliative reasoning... Third, the fundamental logic must be able to underlie paracomplete reasoning and paraconsistent reasoning."
  - [corpus] Weak corpus support—neighbor papers investigate LLM reasoning empirically (Chain-of-Thought analysis, geometric understanding) but do not engage with strong relevant logic frameworks.
- Break condition: If one rejects the premise that SRLs are necessary for correct reasoning, or accepts probabilistic validity as sufficient for practical reasoning, the argument dissolves.

### Mechanism 2
- Claim: LLMs cannot embed a formal correctness evaluation criterion because their architecture combines probability theory, statistics, and deep learning—none of which support embedding a logical validity criterion.
- Mechanism: LLMs are "generative mathematical models of the statistical distribution of tokens" trained to predict likely continuations. The author argues that statistical plausibility ≠ logical correctness, and no probabilistic system can guarantee 100% correct outputs as required by the definition.
- Core assumption: A formal logic system must be built-in as a validity criterion for true correct reasoning to be possible.
- Evidence anchors:
  - [section] "As long as the LLMs are working based on probability theory, statistics, and deep learning, a formal logic system cannot be embedded as the built-in logical validity evaluation criterion."
  - [corpus] Neighbor paper "The Outputs of Large Language Models are Meaningless" (arXiv 2509.22206) argues LLM outputs lack meaning due to missing intentionality—philosophically compatible but independently argued.
- Break condition: If one demonstrates that neural networks can learn to approximate logical validity criteria sufficiently, or that external verifiers can provide the missing evaluation mechanism, this limitation becomes practical rather than fundamental.

### Mechanism 3
- Claim: LLMs' perceived "reasoning ability" is an illusion arising from four factors: vague user understanding of reasoning, pattern copying from training data, the ELIZA effect, and fluent simulation of reasoning forms without correctness guarantees.
- Mechanism: LLMs sometimes reproduce correct reasoning examples from training data, but this is "copying" not "making" reasoning. The token-by-token incremental generation prevents global evaluation, so LLMs can simulate the form (premises → process → conclusion) without verifying the evidential relation.
- Core assumption: There is a categorical distinction between "simulating reasoning forms" and "making true correct reasoning."
- Evidence anchors:
  - [section] "LLMs are trained on vast amounts of text where humans are reasoning and of course include many good reasoning examples. Therefore, LLMs sometimes (but NOT 100%) may output (copy!) some good reasoning example, and this is mistaken by people for having the reasoning ability by LLMs themselves."
  - [corpus] "Unveiling and Causalizing CoT" (arXiv 2502.18239) notes that CoT reasoning remains a "black box" and correct answers don't guarantee understandable or valid reasoning chains—empirically aligned with the concern.
- Break condition: If emergent reasoning capabilities can be demonstrated that generalize beyond training distribution with verified validity, the "copying" characterization becomes insufficient.

## Foundational Learning

- Concept: **Relevant logic vs. Classical logic**
  - Why needed here: The paper's entire argument hinges on rejecting classical mathematical logic's treatment of conditionals (material implication) and validity (truth-preservation without relevance). Understanding why "if A then B" in classical logic allows paradoxes (e.g., from a false premise, anything follows) is essential.
  - Quick check question: Can you explain why "If the moon is made of cheese, then 2+2=5" is true under material implication but fails the relevance requirement?

- Concept: **Paracomplete and Paraconsistent reasoning**
  - Why needed here: The third requirement demands that reasoning systems handle incomplete knowledge (paracomplete: not every proposition is either true or false) and inconsistent knowledge (paraconsistent: contradictions don't entail everything). Classical logic explodes from contradictions.
  - Quick check question: In classical logic, from "A and not-A," can you derive "B" for arbitrary B? Why does this matter for real-world reasoning?

- Concept: **Ampliative reasoning**
  - Why needed here: The second requirement specifies that valid reasoning must be "ampliative"—the conclusion's truth is recognized after reasoning completes, not invoked in establishing premise truth. This excludes circular/tautological reasoning.
  - Quick check question: Why is "A, therefore A" logically valid but not ampliative? What does this mean for knowledge expansion?

## Architecture Onboarding

- Component map:
  - Definition layer: Author's strict definition of "correct reasoning" (premises provide conclusive relevant evidence)
  - Logic layer: Three requirements (relevance, ampliative, paracomplete/paraconsistent) → only SRLs satisfy
  - LLM architecture layer: Probability + statistics + deep learning + token-by-token generation
  - Illusion analysis layer: Four explanatory factors for why reasoning seems present but isn't
  - Conclusion layer: Architecture prevents embedding validity criteria → fundamental impossibility

- Critical path:
  1. Accept/reject the definition of correct reasoning
  2. Accept/reject that SRLs uniquely satisfy the three requirements
  3. Accept/reject that LLM architecture cannot embed logical validity criteria
  4. Accept/reject the simulation vs. making distinction

- Design tradeoffs:
  - The argument trades empirical flexibility for definitional rigor—if you accept the framework, LLMs fail; if you want to evaluate LLMs as potentially reasoning, you must reject the framework
  - "100% correctness" requirement vs. "sufficient for practical use" tolerance
  - Built-in validity criteria vs. external verification systems

- Failure signatures:
  - If the paper addressed empirical benchmarks or architectural modifications, it would shift from philosophical argument to empirical claim
  - The paper does not engage with neuro-symbolic approaches that might embed logical constraints
  - No acknowledgment that "reasoning" might be a spectrum rather than a binary property

- First 3 experiments:
  1. Formal analysis: Map the three requirements (relevance, ampliative, paracomplete/paraconsistent) against specific neuro-symbolic architectures to test the "cannot embed" claim
  2. Empirical test: Evaluate LLM outputs on problems requiring paraconsistent reasoning (reasoning from inconsistent premises) to characterize actual vs. theoretical limitations
  3. Comparative analysis: Apply the author's validity criteria to human reasoning under uncertainty—do humans meet the 100% correctness standard?

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the notion of "new" conclusions in reasoning be defined formally and satisfactorily?
- Basis in paper: [explicit] The author states: "Note that how to define the notion of 'new' formally and satisfactorily is still a difficult open problem until now. Its discussion is beyond this paper."
- Why unresolved: Formalizing novelty in logical inference remains contentious across philosophy of science and logic.
- What evidence would resolve it: A rigorous formal definition that accounts for both syntactic and semantic novelty in reasoning processes.

### Open Question 2
- Question: Are strong relevant logics (SRLs) truly the only family of logics satisfying the three requirements (relevance, ampliative capability, and paracompleteness/paraconsistency)?
- Basis in paper: [inferred] The paper asserts SRLs are "the only family" meeting all three requirements without exhaustive proof that no alternatives exist.
- Why unresolved: The claim of exclusivity requires surveying all possible logical systems.
- What evidence would resolve it: A formal proof of uniqueness or discovery of alternative systems meeting all criteria.

### Open Question 3
- Question: Could modified LLM architectures embed an explicit correctness evaluation criterion and dynamic evaluation mechanism?
- Basis in paper: [inferred] The author claims LLMs cannot embed these mechanisms, but this assertion rests on current architectures rather than a theoretical impossibility proof.
- Why unresolved: No systematic exploration of hybrid architectures combining statistical learning with formal logic has been conducted.
- What evidence would resolve it: Empirical demonstration of an LLM variant successfully integrating a logic-based evaluation layer.

## Limitations
- The argument is purely theoretical and philosophical rather than empirical
- The "100% correctness" requirement may be overly restrictive for practical applications
- The claim that SRLs are the only valid foundation for correct reasoning is asserted but not comprehensively proven

## Confidence
- **High confidence**: The logical argument that LLMs, as probabilistic generative models, cannot guarantee 100% correctness for any definition of correctness that requires certainty
- **Medium confidence**: The characterization of LLMs' reasoning ability as simulation rather than genuine reasoning, given the architecture's token-by-token generation without global evaluation
- **Low confidence**: The exclusive claim that strong relevant logics are the only valid foundation for correct reasoning, as this requires acceptance of specific philosophical positions about logic and reasoning

## Next Checks
1. Implement and test strong relevant logic validation against LLM outputs to empirically verify the architectural limitation claims
2. Survey AI ethics and philosophy of AI literature to assess whether the "100% correctness" requirement is reasonable or overly restrictive
3. Investigate neuro-symbolic approaches that might embed logical validity criteria to test whether the architectural impossibility claim holds under modified architectures