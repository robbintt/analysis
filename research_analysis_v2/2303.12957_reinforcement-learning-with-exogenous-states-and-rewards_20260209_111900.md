---
ver: rpa2
title: Reinforcement Learning with Exogenous States and Rewards
arxiv_id: '2303.12957'
source_url: https://arxiv.org/abs/2303.12957
tags:
- uni00000003
- uni00000013
- uni00000048
- uni0000004c
- uni00000056
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of slow reinforcement learning
  caused by exogenous state variables and rewards that introduce uncontrolled variation
  into the reward signal. The authors formalize exogenous state variables and rewards,
  showing that if the reward function decomposes additively into endogenous and exogenous
  components, the MDP can be decomposed into an exogenous Markov Reward Process and
  an endogenous Markov Decision Process.
---

# Reinforcement Learning with Exogenous States and Rewards

## Quick Facts
- **arXiv ID:** 2303.12957
- **Source URL:** https://arxiv.org/abs/2303.12957
- **Reference count:** 40
- **Primary result:** Additive reward decomposition allows RL speedups by removing exogenous reward variance

## Executive Summary
This paper addresses the challenge of slow reinforcement learning caused by exogenous state variables and rewards that introduce uncontrolled variation into the reward signal. The authors formalize exogenous state variables and rewards, showing that if the reward function decomposes additively into endogenous and exogenous components, the MDP can be decomposed into an exogenous Markov Reward Process and an endogenous Markov Decision Process. Any optimal policy for the endogenous MDP is also optimal for the original MDP, but the endogenous MDP is easier to solve due to reduced variance.

The paper introduces algorithms for discovering the exogenous and endogenous subspaces of the state space when they are mixed through linear combination. These algorithms can be applied during reinforcement learning to discover the exogenous subspace, remove the exogenous reward, and focus learning on the endogenous MDP. Experiments on challenging synthetic MDPs show that these methods, applied online, discover large exogenous state spaces and produce substantial speedups in reinforcement learning.

## Method Summary
The method decomposes an MDP into endogenous and exogenous components when rewards are additive and exogenous states are causally disconnected from actions. It discovers the exogenous subspace using Conditional Correlation Coefficient (CCC) minimization on a Stiefel manifold, then fits a reward regressor to predict and remove the exogenous reward component. The RL agent then trains on the residual endogenous reward using standard algorithms like PPO. The approach alternates between collecting experience, discovering the exogenous subspace, fitting the reward model, and continuing RL training.

## Key Results
- Decomposition algorithms can discover large exogenous subspaces that yield substantial speedups in reinforcement learning
- Methods are effective even when MDPs have nonlinear rewards, nonlinear dynamics, combinatorial action spaces, or discrete states and actions
- Theoretical guarantees show that any optimal policy for the endogenous MDP is also optimal for the original MDP

## Why This Works (Mechanism)

### Mechanism 1: Reward Variance Reduction via Additive Decomposition
The paper posits that the observed reward is the sum of a controllable endogenous reward and an uncontrollable exogenous reward. By regressing to learn the exogenous component and subtracting it, the agent trains on the residual endogenous reward, removing noise variance contributed by exogenous factors and allowing faster convergence. The core assumption is that the reward function must decompose additively into components that depend only on exogenous variables and those that depend on endogenous variables/actions.

### Mechanism 2: Policy Invariance through Action-Disconnection
The paper defines variables as "causally exogenous" if there is no directed path from action to the exogenous variable in the causal graph. Consequently, the exogenous value function does not depend on the policy. Since the total value equals the sum of exogenous and endogenous components, maximizing the total value is equivalent to maximizing the endogenous component. The core assumption is that exogenous variables are "action-disconnected" in the causal graph.

### Mechanism 3: Subspace Discovery via Conditional Correlation Coefficient (CCC)
The exogenous subspace can be discovered from mixed state data using a correlation-based proxy for conditional independence. To find the linear projection that separates endogenous and exogenous states, the algorithm minimizes the Conditional Correlation Coefficient (CCC), defined as the trace of a normalized cross-covariance matrix. This is a tractable optimization problem on a Stiefel manifold that approximates the constraint that exogenous variables are conditionally independent of actions given endogenous states.

## Foundational Learning

- **Conditional Mutual Information (CMI)**: The paper relies on the constraint that exogenous variables are conditionally independent of actions given endogenous states. Without understanding CMI, the objective of the discovery algorithms is opaque. *Quick check:* If I(Y; Z|X) = 0, what does that say about the relationship between Y and Z once X is known?

- **Stiefel Manifold Optimization**: The algorithms optimize the projection matrix under the constraint that it must be orthonormal. Standard gradient descent is insufficient; optimization must occur on the manifold of orthonormal matrices. *Quick check:* Why does a standard gradient update step typically violate the constraint W^T W = I, and how does a retraction step fix this?

- **Markov Reward Process (MRP) vs. Markov Decision Process (MDP)**: The paper decomposes the full problem into an MRP (the exogenous part, no actions/decisions involved) and an MDP (the endogenous part). *Quick check:* In an MRP, does the value function depend on a policy Ï€? Why is this crucial for the paper's decomposition proof?

## Architecture Onboarding

- **Component map:** Experience Buffer -> Manifold Optimizer -> Regressor -> Learner -> Policy
- **Critical path:** The manifold optimizer must find a stable exogenous subspace before the regressor can be trained. If the projection is incorrect, the regressor learns a biased function, and subsequent RL training fails.
- **Design tradeoffs:**
  - GRDS vs. SRAS: GRDS finds the maximal subspace (theoretically sound) but requires solving high-dimensional optimization problems (computationally expensive). SRAS solves one-dimensional problems (fast) but may not find the maximal subspace (suboptimal decomposition).
  - Simplified vs. Full CCC: Simplified objective is faster but ignores indirect causal paths. Full objective is safer but harder to optimize.
  - Linear vs. Neural Regression: Linear regression is fast but fails on non-linear exogenous rewards. Neural regression works for non-linear rewards but risks overfitting with small data buffers.
- **Failure signatures:**
  - Oscillating CCC: If CCC minimization never drops below epsilon, the assumption of linearity or additive decomposition is likely violated.
  - Policy Collapse: If the regressor accidentally learns the endogenous reward instead of the exogenous reward, subtracting it removes the learning signal entirely.
  - Slow Convergence: If the discovered subspace includes endogenous variables, the regression problem becomes harder due to irrelevant features.
- **First 3 experiments:**
  1. Sanity Check (Linear MDP): Construct a simple linear MDP with known additive reward decomposition. Verify that the discovery algorithms recover the ground-truth exogenous subspace.
  2. Ablation on CCC Objective: Compare simplified vs. full objective on a problem with indirect causal links to confirm the simplified method fails while the full method succeeds.
  3. Non-linear Reward Stress Test: Use a synthetic environment with non-linear exogenous rewards. Compare training curves for linear vs. neural regression to quantify the benefit of non-linear modeling.

## Open Questions the Paper Calls Out

- Can the decomposition algorithms be extended to handle non-linear exogenous subspaces using neural networks? The current algorithms rely on linear manifold optimization and linear projections.
- Does enforcing exogeneity constraints over multiple future time steps improve the discovery of valid subspaces? The current method relies on a 2-time-step structure which may not capture long-range dependencies.
- How prevalent is the required additive reward decomposition in real-world applications? The theoretical guarantees and speedups rely on additive reward splitting, but this was only validated on synthetic benchmarks.
- Are the proposed algorithms robust to non-additive reward functions? The paper proves optimality for additive rewards but does not analyze error bounds when rewards are multiplicative or non-decomposable.

## Limitations

- The core decomposition assumption (additive reward function) may not hold in many real-world RL problems where rewards involve complex interactions between endogenous and exogenous factors.
- The CCC proxy for conditional independence may be unreliable for high-dimensional, non-Gaussian systems.
- The policy invariance proof depends critically on the "action-disconnectedness" assumption, which may be violated in practical scenarios where actions indirectly influence exogenous states.

## Confidence

- **High confidence:** The mathematical framework for reward decomposition and the proof of policy invariance under stated assumptions are rigorous and internally consistent.
- **Medium confidence:** The algorithmic approach for discovering exogenous subspaces using CCC optimization is sound, but its empirical performance across diverse problem classes needs validation.
- **Low confidence:** The practical applicability of these methods to real-world RL problems with complex, non-linear dynamics and reward structures remains largely untested.

## Next Checks

1. **Stress Test with Non-Additive Rewards:** Construct environments where the reward function is multiplicative or includes interaction terms. Measure whether the decomposition algorithm still improves learning speed or if it degrades performance by removing necessary reward components.

2. **Policy Invariance Under Causal Violation:** Design a synthetic MDP where actions indirectly influence exogenous states (violating the "action-disconnectedness" assumption). Compare the optimal policies found by the endogenous MDP solver versus the full MDP solver to quantify performance degradation.

3. **CCC Approximation Accuracy:** Generate high-dimensional state spaces with known causal structure. Compare the CCC-based exogenous subspace discovery against ground-truth subspaces to measure false positive/negative rates. Test sensitivity to non-Gaussian noise distributions and varying levels of regularization.