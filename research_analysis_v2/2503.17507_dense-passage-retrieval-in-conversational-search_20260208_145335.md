---
ver: rpa2
title: Dense Passage Retrieval in Conversational Search
arxiv_id: '2503.17507'
source_url: https://arxiv.org/abs/2503.17507
tags:
- retrieval
- dense
- conversational
- query
- cast
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper applies dense passage retrieval (DPR) to conversational
  search, addressing the challenge of retrieving relevant passages from large corpora
  in multi-turn, context-dependent conversations. The authors reformulate queries
  using GPT-2 and fine-tune DPR on the CAsT dataset, comparing it to BM25 and other
  baseline methods.
---

# Dense Passage Retrieval in Conversational Search

## Quick Facts
- arXiv ID: 2503.17507
- Source URL: https://arxiv.org/abs/2503.17507
- Reference count: 18
- Primary result: DPR outperforms BM25 in conversational search with best system achieving NDCG@3 of 0.229

## Executive Summary
This paper applies dense passage retrieval (DPR) to conversational search, addressing the challenge of retrieving relevant passages from large corpora in multi-turn, context-dependent conversations. The authors reformulate queries using GPT-2 and fine-tune DPR on the CAsT dataset, comparing it to BM25 and other baseline methods. Results show that DPR outperforms BM25 even without extensive fine-tuning, with the best GPT2QR+DPR system achieving an NDCG@3 of 0.229 on the CAsT evaluation set, ranking 6th among automatic runs.

## Method Summary
The approach combines query reformulation with dense passage retrieval for conversational search. GPT-2 is fine-tuned on QReCC to generate self-contained queries by attending to dialog history, resolving co-reference and omission issues. The reformulated queries are then processed by DPR, which uses dual-encoder BERT models to create dense embeddings for efficient semantic matching. The system is evaluated on TREC CAsT 2019, using MSMARCO passages and TREC CAR paragraphs as the corpus.

## Key Results
- DPR outperforms BM25 baseline even without fine-tuning (NDCG@3 of 0.145 vs 0.101)
- GPT2QR+DPR+ui-1 achieves best NDCG@3 of 0.229, ranking 6th among automatic runs
- Fine-tuned DPR with AllenNLP reformulation achieves NDCG@3 of 0.226
- Including immediately preceding utterance (ui-1) improves retrieval over using only first utterance (u0)

## Why This Works (Mechanism)

### Mechanism 1
Dense retrieval captures semantic relationships that term-frequency methods miss, enabling better passage matching in conversational contexts. Dual-encoder DPR independently maps queries and passages to shared dense vector space where similarity is computed via dot product. The semantic space learned from QA datasets transfers to conversational search without requiring full retraining.

### Mechanism 2
Query reformulation resolves conversational dependencies (co-reference and omission) that make raw utterances uninterpretable by standard retrievers. GPT-2, fine-tuned on QReCC with teacher forcing, generates self-contained queries by attending to dialog history. Special tokens structure the input sequence to separate previous questions from the current utterance requiring reformulation.

### Mechanism 3
Including the immediately preceding utterance (ui-1) with reformulated queries improves retrieval more than prepending only the first utterance (u0), as it captures subtopic transitions. Concatenating ui-1 provides local context that helps resolve subtopic shifts within a conversation, converting ad-hoc search sessions to incorporate context-dependent information without requiring the full dialog history.

## Foundational Learning

- **Dual-encoder architecture and contrastive learning**: DPR uses two separate BERT encoders trained to bring relevant query-passage pairs closer in embedding space while pushing irrelevant pairs apart. Understanding this is essential for debugging retrieval failures.
  - Quick check question: Given a query embedding and 3 passage embeddings, can you compute which passage DPR would retrieve using dot product similarity?

- **In-batch negative sampling vs. hard negatives**: The paper couldn't use hard negatives from MSMARCO because 70% of BM25 top-1000 passages were actually relevant (false negatives). Understanding this distinction is critical for training data preparation.
  - Quick check question: Why would using false negatives as training examples hurt model convergence?

- **Maximum Inner Product Search (MIPS)**: Pre-computed passage embeddings must be efficiently searchable at inference time. MIPS enables approximate nearest neighbor retrieval over millions of dense vectors.
  - Quick check question: If you have 8.8M passage embeddings of dimension 768, why can't you compute exact dot products with every passage at query time?

## Architecture Onboarding

- Component map: Input Query (turn i) + Dialog History -> GPT2QR (Query Reformulation) -> Reformulated Query -> DPR Query Encoder (BERT) -> Query Embedding (d=768) -> MIPS over Pre-indexed Passage Embeddings -> Top-k Retrieved Passages

- Critical path:
  1. Query reformulation quality (GPT2QR) directly determines whether DPR receives interpretable input
  2. Passage embeddings must be pre-computed offline (170 hours for 8.8M passages on 1 GPU)
  3. Fine-tuning DPR requires regenerating passage embeddings if encoder weights change

- Design tradeoffs:
  - **In-batch negatives only** vs. hard negatives: Simpler training but potentially less informative gradients (Section I explains MSMARCO annotation limitations forced this choice)
  - **ui-1 concatenation** vs. u0 concatenation: Better subtopic handling vs. simpler implementation (Table II shows 0.029 NDCG improvement)
  - **Pre-trained DPR** vs. training from scratch: Resource constraints prevented full training (9+ hours for 1 epoch vs. original 25 epochs on 8×32GB GPUs)

- Failure signatures:
  - NDCG@3 < 0.15: Likely using raw queries without reformulation (Table II shows Raw+DPR = 0.145)
  - Training loss not decreasing: Check for false negatives in training data
  - Nonsensical query reformulations: Bi-GRU with GloVe produced this (Section III-A)—use transformer decoder instead
  - Out-of-memory during passage indexing: Shard into ~70 chunks as authors did

- First 3 experiments:
  1. **Baseline replication**: Load pre-trained DPR (multiset weights), encode CAsT passages, run retrieval with raw queries. Expect NDCG@3 ~0.145. Validates infrastructure.
  2. **Query reformulation ablation**: Compare Raw+DPR vs. AllenNLP+DPR vs. GPT2QR+DPR. Expect progressive improvement validating reformulation mechanism.
  3. **Context concatenation test**: Test GPT2QR alone vs. GPT2QR+u0 vs. GPT2QR+ui-1. Expect ui-1 variant to perform best (0.229 NDCG@3).

## Open Questions the Paper Calls Out

### Open Question 1
Can pointer-generator networks with copy mechanisms improve query reformulation over pure GPT-2 generation? The authors propose future work on adding copy distribution from tokens in previous questions to improve performance, motivated by pointer-generator networks and their success with text summarization. Only standard GPT-2 generation was implemented; the copy mechanism combining as w·pgen + (1−w)·pcopy was not tested.

### Open Question 2
Would fine-tuning DPR on GPT2QR+ui-1 configuration outperform the current best AllenNLP configuration? The authors were unable to fine tune on their best trial of GPT2QR+DPR+ui-1 due to resources and time limitation but expect it will outrank the AllenNLP+u0 configuration.

### Open Question 3
How much would ANCE-based hard negative sampling improve DPR performance in conversational search? The authors could not use hard negatives due to 70% false negatives in MSMARCO's BM25 pool. ANCE was identified as solution but was "too resource extensive."

## Limitations

- Limited training data: Only 108 training utterances available for fine-tuning, raising concerns about overfitting and generalization
- Single dataset evaluation: Results only validated on TREC CAsT 2019, limiting confidence in cross-dataset performance
- Computational overhead: Generating and storing dense passage embeddings requires significant resources (170+ GPU hours for indexing)

## Confidence

- **Medium confidence**: DPR outperforms BM25 in conversational contexts (evaluation uses single dataset with limited training examples)
- **Low confidence**: ui-1 concatenation superiority (lacks strong ablation evidence and statistical significance testing)
- **High confidence**: Query reformulation resolving conversational dependencies (well-established in literature and empirically validated)

## Next Checks

1. **Statistical significance testing**: Conduct t-tests or bootstrap confidence intervals on NDCG@3 scores across multiple runs to verify that GPT2QR+DPR+ui-1's 0.229 score is statistically better than GPT2QR+DPR+u0 (0.200) and other baseline methods.

2. **Cross-dataset generalization**: Evaluate the same DPR fine-tuning approach on TREC CAsT 2020 or other conversational search benchmarks to test whether the method generalizes beyond the original training data.

3. **Hard negative sampling validation**: Create a synthetic test set where BM25's false negatives can be verified as truly irrelevant, then measure whether incorporating these as hard negatives during fine-tuning improves retrieval performance beyond in-batch negatives.