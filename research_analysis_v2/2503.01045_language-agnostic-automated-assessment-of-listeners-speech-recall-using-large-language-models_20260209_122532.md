---
ver: rpa2
title: Language-agnostic, automated assessment of listeners' speech recall using large
  language models
arxiv_id: '2503.01045'
source_url: https://arxiv.org/abs/2503.01045
tags:
- recall
- speech
- story
- speakers
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that automated speech recall scoring using
  large language models (LLMs) can accurately assess comprehension of naturalistic
  stories across 11 languages. The method employs text embeddings and prompt engineering
  to segment stories and recall data, calculate semantic similarity, and derive recall
  metrics including temporal order, primacy/recency effects, and background noise
  impact.
---

# Language-agnostic, automated assessment of listeners' speech recall using large language models

## Quick Facts
- arXiv ID: 2503.01045
- Source URL: https://arxiv.org/abs/2503.01045
- Reference count: 16
- Automated speech recall scoring using LLMs accurately assesses comprehension across 11 languages, detecting temporal order, primacy/recency effects, and background noise impact

## Executive Summary
This study demonstrates that automated speech recall scoring using large language models (LLMs) can accurately assess comprehension of naturalistic stories across 11 languages. The method employs text embeddings and prompt engineering to segment stories and recall data, calculate semantic similarity, and derive recall metrics including temporal order, primacy/recency effects, and background noise impact. Results show sensitivity to these known comprehension effects and high similarity of recall scores across languages. The approach overcomes limitations of traditional speech tests by enabling assessment of naturalistic, story-like speech in multiple languages without requiring manual scoring, offering a scalable solution for clinical and research applications.

## Method Summary
The study used GPT-4o to generate seven story texts (~300 words each), translate them into 10 languages, and synthesize speech audio. Participants listened to stories in their native language and provided free recall, which was transcribed using Whisper-1 and punctuation-corrected with GPT-4o. Stories and recall transcriptions were segmented into 10 equal parts with 20% overlap, then embedded using LaBSE (768-dimensional) or rated with GPT-4o-mini prompts. Semantic similarity matrices were computed, and recall metrics (maximum recall, original/reverse order scores, primacy/recency effects, temporal-order divergence) were extracted. The method was validated on 55 participants (29 English, 26 non-English) with sensitivity to temporal order, primacy/recency effects, and speech clarity (+2 dB SNR) tested.

## Key Results
- Automated scoring detected known memory effects: original-order recall exceeded reverse-order recall (F1,53 > 300, p < 0.001)
- Primacy and recency effects observed: beginning and end segments recalled better than middle segments
- Recall scores were similar across 11 languages despite participant ratings of non-English speech pronunciation quality
- Method showed sensitivity to speech clarity, with clearer speech yielding higher recall scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Text embeddings capture semantic similarity between story content and recall output across languages
- Mechanism: The LaBSE model maps text segments to 768-dimensional vectors where semantically related content yields higher correlation values. By segmenting both stimulus and recall into 10 chunks, the approach creates a 10×10 correlation matrix enabling fine-grained temporal analysis rather than a single aggregate score
- Core assumption: Embedding similarity corresponds to comprehension quality; that language-agnostic embeddings preserve cross-linguistic semantic equivalence without systematic bias
- Evidence anchors: [abstract] "LLM text-embeddings and LLM prompt engineering with semantic similarity analyses to score speech recall revealed sensitivity to known effects of temporal order, primacy/recency, and background noise"; [Methods] "Each segment of the story texts and recall transcriptions was mapped onto one 768-dimensional embedding vector using the Language-agnostic BERT Sentence Embedding (LaBSE) model"
- Break condition: If embedding correlations remain high for semantically unrelated story-recall pairings (poor discriminant validity), the approach fails

### Mechanism 2
- Claim: Temporal structure of recall reflects comprehension fidelity
- Mechanism: The diagonal of the story×recall matrix captures whether participants recalled segments in original order. Off-diagonal peaks indicate temporal reordering. Primacy/recency effects emerge from comparing diagonal subsections (first two vs. middle vs. last two segments)
- Core assumption: Faithful temporal recall indicates better comprehension; that serial position effects generalize from word-list paradigms to narrative speech
- Evidence anchors: [Results] "original-order score was greater than the reverse-order score (effect of Score Type; for both analyses: F1,53 > 300, p < 0.001)"; [Results] "Recall scores were also greater for the beginning than the middle of a story (primacy... and the end than the middle of a story (recency"
- Break condition: If narrative comprehension fundamentally differs from list memory such that gist-preserving but temporally-reordered recall is penalized unfairly

### Mechanism 3
- Claim: Prompt engineering provides superior dynamic range versus embedding correlations
- Mechanism: Zero-shot prompts instruct GPT-4o-mini to rate semantic capture on 0–100 scale. Unlike embeddings where even unrelated passages correlate ~0.2, LLM ratings approach zero for unrelated content, improving interpretability for clinical thresholds
- Core assumption: LLM ratings approximate human semantic judgments; that temperature=0 ensures sufficient consistency across repeated queries
- Evidence anchors: [Methods] "GPT prompt engineering showed somewhat better performance than the embedding approach, such that scores had a greater dynamic range and chance-level scores were near zero"; [Results] "The rmANOVAs showed that the maximum-recall score was greater than chance recall score... and that the original-order score was greater than the reverse-order score"
- Break condition: If prompt-based ratings exhibit systematic language bias or inconsistency across translations

## Foundational Learning

- Concept: **Text embeddings as semantic representations**
  - Why needed here: The entire embedding approach assumes understanding that words/phrases map to vector space where distance reflects meaning similarity
  - Quick check question: Given three embedding vectors A, B, C—if cosine(A,B) > cosine(A,C), what does that imply about semantic relationships?

- Concept: **Serial position effects (primacy/recency)**
  - Why needed here: The paper uses primacy and recency as validation signals—if the scoring method doesn't detect these well-established memory phenomena, it's not capturing real comprehension
  - Quick check question: In a 10-item list, which positions typically show highest recall probability, and why might this differ for narratives?

- Concept: **Signal-to-noise ratio (SNR) in speech perception**
  - Why needed here: The +2 dB SNR manipulation tests whether scoring is sensitive to degraded input—critical for clinical validity where patients often report difficulty in noise
  - Quick check question: What does +2 dB SNR mean quantitatively, and why might 12-talker babble be chosen over white noise?

## Architecture Onboarding

- Component map: GPT-4o generates story text -> GPT-4o translates to target language -> Whisper TTS synthesizes audio -> Participant listens -> Free recall recorded -> Whisper speech-to-text transcribes -> GPT-4o repairs punctuation -> Segment text (10 segments, 20% overlap) -> LaBSE embedding (768-dim vectors) -> Spearman correlation matrix -> Extract metrics

- Critical path: Transcription quality -> Segmentation parameters -> Embedding model choice (or prompt design) -> Metric extraction. The paper notes Whisper can produce unpunctuated text for non-English languages, requiring the punctuation repair step

- Design tradeoffs:
  - **Fewer segments (6)**: Higher sensitivity to speech-clarity effects for recall scores but lower temporal resolution
  - **More segments (18)**: Better temporal-order divergence detection but reduced clarity-sensitivity
  - **Embedding vs. prompt**: Embedding is fast and free; prompt has better dynamic range but costs ~$1.50/participant with GPT-4o-mini (would be >$1200 with GPT-4o)
  - **Voice consistency**: Single voice (Alloy) across languages controls acoustics but introduces English accent for non-English languages (rated poorly by non-native speakers)

- Failure signatures:
  - High correlation values for unrelated story-recall pairings (embedding floor effect)
  - Systematically lower scores for specific languages due to translation/synthesis quality
  - Temporal-order divergence scores that don't differentiate clear vs. noisy conditions
  - Non-English speakers showing uniformly lower scores (would indicate language bias)

- First 3 experiments:
  1. **Validation on held-out languages**: Test on 2–3 languages not in training set (e.g., Swedish, Greek) to verify true language-agnosticity rather than overfitting to the 11 tested languages
  2. **Human scorer correlation**: Have bilingual human raters score a subset of recall transcripts; correlate automated scores with human judgments to establish convergent validity (paper cites prior work showing ~0.8 correlations but doesn't conduct this directly)
  3. **Clinical population test**: Deploy with older adults with diagnosed hearing loss to verify sensitivity to clinically-relevant comprehension deficits beyond the young normal-hearing sample tested (mean age ~23 years)

## Open Questions the Paper Calls Out

- **Question**: How well does the automated recall scoring generalize to older adults and individuals with hearing loss, the primary clinical target population?
  - **Basis in paper**: [explicit] The paper states that "Speech-comprehension difficulties are common among older people" and aims for "clinical applicability," but all participants were young (18–37 years) with normal hearing
  - **Why unresolved**: The study only tested young, normal-hearing participants, yet the intended clinical application is for older adults with hearing loss who may have different recall patterns, cognitive profiles, and speech comprehension challenges
  - **What evidence would resolve it**: Administering the same automated recall assessment to older adults with and without hearing loss, comparing recall metrics and scoring sensitivity to known comprehension effects in these populations

- **Question**: Does using native-language prompts (rather than English prompts) for LLM-based recall scoring improve scoring accuracy for non-English languages?
  - **Basis in paper**: [explicit] The authors state: "Whether scoring performance could be improved somewhat by using non-English prompts for non-English recall scoring may be worth exploring in the future"
  - **Why unresolved**: The current study used English instructions with text in other languages; it remains unknown whether language-matched prompts would yield better semantic similarity ratings or scoring accuracy
  - **What evidence would resolve it**: Systematic comparison of recall scores obtained using English prompts versus native-language prompts across the same set of non-English recall data

- **Question**: How do LLM-automated recall scores correlate with human expert ratings across different languages?
  - **Basis in paper**: [inferred] The paper notes previous work showed text-embedding scores correlate with human ratings, but the current multi-language approach was not validated against human expert scoring of the recall data
  - **Why unresolved**: While sensitivity to known effects (primacy, recency, noise) supports validity, direct comparison with human raters across languages would strengthen confidence in the automated scoring for clinical use
  - **What evidence would resolve it**: Having human scorers fluent in each language rate recall transcripts, then correlating these ratings with LLM-generated scores across all tested languages

## Limitations

- Limited dynamic range of embedding approach (chance-level correlations ~0.2) may compromise clinical sensitivity thresholds
- All participants were healthy young adults, leaving unclear whether method detects clinically-relevant comprehension deficits in target populations
- Claims of language-agnosticity rest on 11 languages, with untested generalization to truly low-resource languages
- Single "alloy" voice across all languages introduces accent consistency but may mask language-specific acoustic variations

## Confidence

- **High confidence**: The method successfully detects temporal order effects and primacy/recency patterns in recall data (supported by strong statistical results showing clear score differences)
- **Medium confidence**: The approach shows similar recall scores across languages, but this rests on a limited language sample and controlled stimuli generation
- **Medium confidence**: The method's sensitivity to speech clarity (+2 dB SNR) is demonstrated, but effects are modest and may not generalize to more severe hearing impairments

## Next Checks

1. **Cross-linguistic generalization test**: Deploy the system with 2-3 languages not in the original 11 (e.g., Swedish, Greek, Swahili) to verify true language-agnosticity beyond the tested set
2. **Clinical population validation**: Test with older adults with diagnosed hearing loss to verify sensitivity to clinically-relevant comprehension deficits beyond the young normal-hearing sample
3. **Human scorer comparison**: Have bilingual human raters score a subset of recall transcripts and correlate automated scores with human judgments to establish convergent validity for the clinical context