---
ver: rpa2
title: An Empirical Analysis of Discrete Unit Representations in Speech Language Modeling
  Pre-training
arxiv_id: '2509.05359'
source_url: https://arxiv.org/abs/2509.05359
tags:
- speech
- discrete
- language
- units
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically investigates discrete unit representations
  in speech language models (SLMs) during continual pre-training. The research examines
  how model architecture, data representation, and training robustness influence the
  adaptation of pre-trained language models to speech modality.
---

# An Empirical Analysis of Discrete Unit Representations in Speech Language Modeling Pre-training

## Quick Facts
- arXiv ID: 2509.05359
- Source URL: https://arxiv.org/abs/2509.05359
- Reference count: 31
- This study systematically investigates discrete unit representations in speech language models (SLMs) during continual pre-training, identifying optimal discretization strategies for different model capacities.

## Executive Summary
This study systematically investigates discrete unit representations in speech language models (SLMs) during continual pre-training. The research examines how model architecture, data representation, and training robustness influence the adaptation of pre-trained language models to speech modality. Through experiments across model scales (135M to 1.7B parameters), four speech encoders (WavLM, HuBERT, XLS-R, Wav2Vec), and varying discretization granularities (125 to 5000 clusters), the study identifies optimal discretization strategies for different model capacities. Key findings include a direct correlation between model speech modeling capabilities and discrete unit granularity, with smaller models struggling at higher discretization levels. The analysis reveals that discrete units effectively align with phonemes while capturing paralinguistic information, and that domain matching between clustering data and target applications significantly impacts model robustness.

## Method Summary
The study employs continual pre-training of text LLMs (SmolLM-135M/360M/1.7B) to process discrete speech units. Four self-supervised speech encoders (WavLM, HuBERT, XLS-R, Wav2Vec 2.0) extract 50 Hz frame-level features from their final hidden layer. K-means clustering creates discrete vocabularies (k=125-5,000) from 2,000 hours of unlabeled speech. The LLM tokenizers are expanded with these units, and LoRA adapters (rank=64, alpha=16) enable parameter-efficient adaptation. Models are trained on LibriSpeech (960h) for 300 steps using NLL loss, with evaluations on clean and noisy conditions to assess robustness.

## Key Results
- WavLM with moderate cluster sizes (k ≤ 500) consistently achieves the best performance across model scales
- Smaller models (135M) struggle with high discretization levels (k > 500) due to token sparsity
- Domain matching between clustering data and target applications significantly improves model robustness
- WavLM and HuBERT encoders demonstrate superior cluster utilization (77-92%) compared to XLS-R and Wav2Vec (52-68%)

## Why This Works (Mechanism)

### Mechanism 1: Discretization Granularity Matches Model Capacity
- **Claim:** Smaller discrete vocabularies (k ≤ 1,000) yield superior speech modeling performance, but optimal granularity scales with model capacity.
- **Mechanism:** K-means clustering creates a discrete token vocabulary from speech encoder outputs. Smaller cluster counts produce denser token distributions, enabling models to learn stable representations with limited parameters. Larger models (1.7B) can handle higher granularity (k ≤ 1,000) without degradation, while smaller models (135M) struggle beyond k = 500 due to token sparsity overwhelming their capacity.
- **Core assumption:** Token distribution density directly affects learnability; excessively sparse distributions create noisy learning signals.
- **Evidence anchors:** [abstract]: "smaller models struggling at higher discretization levels"; [Section 3.1]: "smaller cluster sizes (k ≤ 1,000) consistently yield better performance"; [Section 3.2]: "1.7B model demonstrating remarkable stability (NLL 1.83-2.28) within its operational range (k ≤ 1,000)"

### Mechanism 2: Encoder-Specific Cluster Utilization Efficiency
- **Claim:** WavLM and HuBERT encoders produce more efficiently utilized discrete vocabularies than XLS-R and Wav2Vec 2.0.
- **Mechanism:** Self-supervised speech encoders extract frame-level representations at 50 Hz from their final hidden layer. The quality of these representations determines how well k-means can partition the acoustic space. WavLM/HuBERT produce representations that distribute more evenly across clusters (77-92% utilization), while XLS-R/Wav2Vec produce representations that cluster unevenly (52-68% utilization), effectively wasting vocabulary capacity.
- **Core assumption:** Higher cluster utilization percentage indicates better representation quality and more discriminative power per token.
- **Evidence anchors:** [Section 3.1]: "WavLM achieves the best performance (NLL=2.05, k=500) at Step 300"; [Section 3.4]: "HuBERT and WavLM demonstrate superior cluster utilization (77-92% and 74-91% respectively)"

### Mechanism 3: Domain Matching Enables Robust Generalization
- **Claim:** Domain alignment between k-means clustering training data and target application improves robustness to acoustic perturbations.
- **Mechanism:** K-means clustering trained on speech data from a specific domain learns to discretize acoustic patterns relevant to that domain. When clustering data matches the target domain (e.g., LibriHeavy → LibriSpeech), the discrete units capture relevant acoustic variations efficiently. Mismatched domains (e.g., GigaSpeech → LibriSpeech) produce units that either over-generalize or capture irrelevant acoustic features, degrading performance under perturbations.
- **Core assumption:** Discrete units encode domain-specific acoustic patterns; transfer requires domain overlap.
- **Evidence anchors:** [abstract]: "domain matching between clustering data and target applications significantly impacts model robustness"; [Section 3.3]: "LibriHeavy-trained models show superior performance and stability, with NLL increasing only marginally from 2.62 (clean) to 2.70 (perturbed)"

## Foundational Learning

- **Concept: Speech Encoders (WavLM, HuBERT, Wav2Vec 2.0, XLS-R)**
  - **Why needed here:** These self-supervised models convert raw audio into dense representations that k-means discretizes. Understanding their properties (frame rate, layer selection, training data) determines discrete unit quality.
  - **Quick check question:** Can you explain why the paper uses the final hidden layer for feature extraction rather than intermediate layers?

- **Concept: K-Means Clustering for Discretization**
  - **Why needed here:** This converts continuous speech representations into discrete tokens (units) that LLMs can process. The choice of k directly impacts vocabulary size and token distribution.
  - **Quick check question:** What happens to token sparsity when k increases from 500 to 5,000, and how does this affect smaller models differently than larger models?

- **Concept: LoRA (Low-Rank Adaptation)**
  - **Why needed here:** The paper uses LoRA adapters for parameter-efficient fine-tuning of pre-trained LLMs to process speech tokens. Understanding this is essential for reproducing the training pipeline.
  - **Quick check question:** Why would LoRA be preferred over full fine-tuning when adapting text LLMs to speech modality?

## Architecture Onboarding

- **Component map:** Raw Audio → Speech Encoder (WavLM/HuBERT/etc.) → Dense Representations (50 Hz, final hidden layer) → K-Means Clustering (k=125-5000 clusters) → Discrete Units <3><76><99>... → LLM with Expanded Vocabulary (SmolLM-135M/360M/1.7B) → LoRA Adapters (rank=64, alpha=16) → Autoregressive LM Head

- **Critical path:**
  1. Select encoder and extract features from 2,000 hours of unlabeled speech
  2. Train k-means on extracted features to create discrete vocabulary
  3. Expand LLM tokenizer with new discrete unit tokens
  4. Apply LoRA adapters to LLM (rank=64, alpha=16)
  5. Train on LibriSpeech (960h) with NLL loss for 300 steps (~157M tokens)

- **Design tradeoffs:**
  - **Small k (125-500):** Better NLL, denser tokens, faster convergence → May under-represent phonetic detail
  - **Large k (2500-5000):** More phonetic precision → Sparse tokens, higher NLL, OOM on larger models
  - **WavLM encoder:** Best overall performance → May be less multilingual than XLS-R
  - **Domain-matched k-means:** Superior robustness → Requires curated clustering data

- **Failure signatures:**
  - NLL > 4.0 at step 300: Likely k too large for model capacity
  - Large clean-to-noisy NLL gap (>0.2): K-means trained on mismatched domain
  - Cluster utilization <70%: Encoder-representation mismatch or suboptimal k
  - OOM with k > 2,500 on larger models: Vocabulary expansion exceeds memory

- **First 3 experiments:**
  1. **Baseline validation:** Train SmolLM-135M with WavLM, k=500, LibriHeavy k-means. Target: NLL ≈2.05 at step 300 (matches Table 1).
  2. **Granularity sweep:** Test k ∈ {125, 500, 1000} on SmolLM-360M to verify capacity-granularity tradeoff. Expect k=500 optimal, degradation at k=1000.
  3. **Domain mismatch test:** Train identical models with LibriHeavy vs. GigaSpeech k-means, evaluate on clean and Noise-H conditions. Expect LibriHeavy NLL gap <0.1, GigaSpeech gap >0.15.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the balance between semantic and paralinguistic information in discrete units affect performance on downstream tasks like Spoken Question Answering and ASR?
- Basis in paper: [explicit] The conclusion states that understanding this balance "remains crucial, necessitating evaluation across diverse tasks including Spoken Question Answering, Spoken Language Understanding, and ASR."
- Why unresolved: The study focused on optimization during the pre-training stage using Negative Log-Likelihood (NLL), rather than measuring the transfer learning performance on specific downstream applications.
- What evidence would resolve it: Benchmarking the trained SLM configurations on downstream tasks (e.g., sWUGGY, Spoken QA) to correlate pre-training NLL with task-specific accuracy.

### Open Question 2
- Question: Why does training k-means clustering on noisy or diverse acoustic data fail to improve model robustness to perturbations?
- Basis in paper: [inferred] Section 3.3 notes that models built on noisy datasets (GigaSpeech, CommonVoice) showed performance degradation, "challenging the assumption that exposure to bad acoustic conditions during training necessarily benefits model resilience."
- Why unresolved: The authors identify the counter-intuitive result but do not isolate the mechanism causing domain mismatch or the failure to generalize robustness.
- What evidence would resolve it: An ablation study analyzing the entropy and purity of clusters derived from noisy vs. clean data, and how they map to perturbed audio features.

### Open Question 3
- Question: Can increasing model scale beyond 1.7B parameters resolve the performance degradation and memory issues observed with high-discretization granularities (k ≥ 2500)?
- Basis in paper: [inferred] Section 3.2 shows that larger models handle higher cluster counts better (NLL 1.83-2.28) than smaller ones, but eventually fail (OOM), suggesting a correlation between capacity and optimal granularity.
- Why unresolved: The study was limited by the OOM (Out of Memory) errors on the 1.7B model at high k-values, leaving the upper limits of this scaling behavior untested.
- What evidence would resolve it: Training larger parameter models (e.g., 7B+) with cluster sizes k ≥ 2500 to determine if the semantic modeling capabilities continue to improve or if the vocabulary sparsity remains a bottleneck.

## Limitations
- K-means clustering hyperparameters (iterations, convergence criteria) were not explicitly reported, potentially affecting discrete unit quality
- The analysis focuses exclusively on autoregressive modeling with NLL loss, leaving open questions about discrete units' performance in masked or bidirectional settings
- The cluster utilization metric, while showing correlation with performance, lacks direct validation of whether high utilization truly indicates better phonetic or linguistic representation

## Confidence

**High Confidence (Likelihood >80%):**
- WavLM with moderate cluster sizes (k ≤ 500) consistently achieves superior performance across model scales
- Discrete units align with phonemes while capturing paralinguistic information
- Domain matching between clustering data and target applications improves model robustness

**Medium Confidence (Likelihood 50-80%):**
- The direct correlation between model capacity and optimal discretization granularity
- Encoder-specific cluster utilization efficiency differences reflect representation quality
- WavLM's superiority stems from its ability to handle noisy token distributions

**Low Confidence (Likelihood <50%):**
- The exact mechanism explaining why smaller models struggle with higher discretization levels
- Whether cluster utilization percentage is the primary driver of encoder performance differences
- The assumption that domain matching provides robustness benefits beyond data quality effects

## Next Checks

1. **Cross-validation of Cluster Utilization Metric:** Compute cluster utilization on held-out speech data from the same domain and verify correlation with NLL scores. A strong correlation (R² > 0.7) would validate the metric's reliability.

2. **Domain Transfer Robustness Test:** Train k-means on mismatched domains (e.g., GigaSpeech clustering → LibriSpeech evaluation) and measure clean-to-noisy NLL gaps. This would test whether domain matching provides genuine robustness benefits beyond data quality effects.

3. **Alternative Representation Analysis:** Apply PCA to discrete unit sequences and measure phonetic alignment scores (e.g., phone error rate when mapping units to ground-truth phones). This would validate whether discrete units truly capture phonetic structure as claimed.