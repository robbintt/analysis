---
ver: rpa2
title: Black-Box Combinatorial Optimization with Order-Invariant Reinforcement Learning
arxiv_id: '2510.01824'
source_url: https://arxiv.org/abs/2510.01824
tags:
- rl-eda
- qubo
- optimization
- each
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an order-invariant reinforcement learning
  framework for black-box combinatorial optimization. The method uses a neural autoregressive
  generative model that samples random generation orders during training, acting as
  a form of information-preserving dropout.
---

# Black-Box Combinatorial Optimization with Order-Invariant Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2510.01824
- **Source URL**: https://arxiv.org/abs/2510.01824
- **Reference count**: 40
- **Key outcome**: This paper introduces an order-invariant reinforcement learning framework for black-box combinatorial optimization, achieving best performance across a wide range of benchmark problems while consistently avoiding catastrophic failures.

## Executive Summary
This paper proposes an order-invariant reinforcement learning framework for black-box combinatorial optimization that uses a neural autoregressive generative model. The method samples random generation orders during training, acting as a form of information-preserving dropout that promotes search-space diversity and focuses the model on the most relevant variable dependencies. By adapting Group Relative Policy Optimization (GRPO) to this setting, the approach provides stable policy-gradient updates from scale-invariant advantages. The method demonstrates strong performance across QUBO, pseudo-boolean NK landscapes, categorical NK3 problems, and the real-world neural architecture search benchmark NAS-Bench-101.

## Method Summary
The approach uses a neural autoregressive generative model where $n$ independent MLPs generate solutions sequentially, with each MLP producing the probability distribution for one variable conditioned on previously assigned variables. During generation, a random permutation $\sigma_G$ determines the variable assignment order, and a binary mask hides "future" variables. The algorithm samples $\lambda=10$ solutions, evaluates them using the black-box objective, and computes advantages based on rank-based utility. Training uses GRPO with a KL-penalty to maintain diversity, and crucially, a different random permutation $\sigma_T$ is sampled for each training context to ensure order invariance. The method can use either independent MLPs (for $n \leq 256$) or shared-parameter MLPs (for $n=1024$) to manage computational complexity.

## Key Results
- The method achieves best performance across a wide range of benchmark problems including QUBO, NK landscapes, and NAS-Bench-101
- Consistently avoids catastrophic failures that affect other methods, particularly on larger instance sizes
- Demonstrates robustness to different problem types and instance sizes up to $n=1024$
- Shows strong results on real-world neural architecture search benchmark NAS-Bench-101

## Why This Works (Mechanism)

### Mechanism 1: Random Permutation as Information-Preserving Dropout
Sampling random generation orders during training acts as a regularizer that maintains population diversity and forces the model to identify the most relevant variable dependencies. By sampling different permutations for generation ($\sigma$) and training ($\sigma'$), the model receives a masked view of the solution space while retaining the joint distribution's full information. This hierarchical learning process enables the network to efficiently identify parent variables and recognize unrelated variables.

### Mechanism 2: Scale-Invariant GRPO (Group Relative Policy Optimizer)
The algorithm calculates advantages based on the rank of a solution within the current population rather than absolute fitness values. This maps the problem to the Information-Geometric Optimization framework, ensuring the policy gradient is invariant under monotonic transformations of the objective function. Scale-invariance is particularly desirable in black-box optimization settings as it enhances robustness to the scaling of objective values.

### Mechanism 3: Neural Estimators for Implicit Dependency Capture
Neural networks can implicitly capture complex variable dependencies without explicitly learning a dependency graph. Unlike classical EDAs that must solve an NP-hard problem to learn graph structure, this approach parameterizes conditional distributions using MLPs that learn the structure implicitly through gradient descent on the log-likelihood or PPO objective, sidestepping expensive graph inference.

## Foundational Learning

- **Estimation of Distribution Algorithms (EDAs)**: These maintain a probabilistic model $P_t(x)$ that generates candidates instead of evolving a population via crossover/mutation. *Quick check*: How does updating a probability distribution differ from selecting survivors in a standard Genetic Algorithm?

- **Policy Gradient (PPO) & Importance Sampling**: The training loop uses a variant of Proximal Policy Optimization with KL-penalty and importance sampling ratios $\frac{\pi_\theta}{\pi_{\theta_{old}}}$ to reuse generated samples. *Quick check*: Why does the paper prefer the KL-penalty version of PPO over the clipped version for handling dropout/permutations? (Hint: See Appendix F).

- **Autoregressive Factorization**: The model builds solutions sequentially: $P(x) = \prod P(x_i | x_{<i})$. While the order changes, the fundamental concept is that solving $x_i$ depends on previously solved variables. *Quick check*: In a standard autoregressive model, the order is fixed. How does randomizing this order affect the "causal mask" seen by the network?

## Architecture Onboarding

- **Component map**: MLPs (one per variable or shared) -> Input Mask (binary mask from permutation) -> Advantage Calculator (rank-based utility) -> Optimizer (Adam on GRPO objective)

- **Critical path**: Initialize random weights $\theta$ -> Sample $\lambda$ trajectories using random generation orders $\sigma_G$ -> Evaluate black-box fitness $f(x)$ and rank -> Sample training permutations $\sigma_T$ -> Compute GRPO loss with KL penalty $\beta$ -> Update $\theta$ for $E$ epochs -> Repeat until budget exhausted

- **Design tradeoffs**: Use independent MLPs for stability on small/medium problems ($n \le 256$), shared-parameter MLPs for large problems ($n=1024$) to save memory/time. Small $\lambda$ (e.g., 10) converges faster but risks local optima; larger $\lambda$ improves exploration. High $\beta$ maintains diversity but slows convergence; low $\beta$ risks premature collapse.

- **Failure signatures**: Stagnation/local optima if distance plot drops to 0 too quickly without score improving (increase $\beta$ or $\lambda$); instability if scores oscillate wildly (KL constraint too loose or learning rate too high); slow convergence if score curve is flat for thousands of evaluations (insufficient network capacity).

- **First 3 experiments**: 1) Implement $(\delta, \delta)$ vs. $(\sigma, \sigma)$ on NK landscape ($N=64, K=4$) to verify random orders delay convergence but achieve higher final score. 2) Run $(\sigma, \sigma)$ with $\beta \in \{0.1, 1.0, 10.0\}$ on QUBO to observe trade-off between convergence speed and diversity. 3) Generate NK instance with $N=500$ and compare standard vs. shared-parameter versions for wall-clock time and final score.

## Open Questions the Paper Calls Out
- How can the order-invariant RL framework be effectively extended to multi-modal optimization landscapes? The current architecture relies on a single probabilistic policy which tends to converge to a single basin of attraction, while integrating mixture models introduces significant training stability challenges.

## Limitations
- Core empirical claims rely on synthetic benchmarks with 10 instances per setting, limiting diversity of problem types
- Ablation studies for random permutation and GRPO advantages are not extensively detailed in the main paper
- Parameter settings (KL penalty $\beta=1$, population size $\lambda=10$) are fixed across experiments without sensitivity analysis

## Confidence
- **High confidence**: Order-invariance framework design, neural autoregressive architecture, use of GRPO for scale-invariant updates
- **Medium confidence**: Performance improvements over baselines, robustness to different problem types, effectiveness of random permutation as regularizer
- **Low confidence**: Claims about hierarchical dependency learning, sensitivity to hyperparameter choices, performance on very large-scale instances ($n>1024$)

## Next Checks
1. Conduct ablation studies varying KL penalty $\beta$ and population size $\lambda$ to assess sensitivity
2. Test the method on a broader set of combinatorial problems, including real-world instances beyond NAS-Bench-101
3. Implement and evaluate the curriculum adaptation variant for short evaluation budgets to verify its claimed performance boost