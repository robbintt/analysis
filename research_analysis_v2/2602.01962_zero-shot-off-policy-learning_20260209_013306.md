---
ver: rpa2
title: Zero-Shot Off-Policy Learning
arxiv_id: '2602.01962'
source_url: https://arxiv.org/abs/2602.01962
tags:
- learning
- policy
- zero-shot
- latent
- off-policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Zero-Shot Off-Policy Learning (ZOL) tackles the challenge of adapting
  pretrained agents to new tasks without additional training or environment interaction,
  a key issue in zero-shot reinforcement learning. ZOL leverages a theoretical connection
  between successor measures and stationary density ratios, enabling it to infer optimal
  importance sampling ratios directly from pretrained Forward-Backward (FB) representations.
---

# Zero-Shot Off-Policy Learning

## Quick Facts
- arXiv ID: 2602.01962
- Source URL: https://arxiv.org/abs/2602.01962
- Reference count: 40
- Primary result: Achieves double-digit percentage gains in unnormalized returns across motion tracking, continuous control, and long-horizon manipulation tasks without any online interaction or finetuning.

## Executive Summary
Zero-Shot Off-Policy Learning (ZOL) addresses the challenge of adapting pretrained agents to new tasks without additional training or environment interaction. The method leverages a theoretical connection between successor measures and stationary density ratios, enabling direct inference of importance sampling ratios from pretrained Forward-Backward (FB) representations. This allows ZOL to perform stationary distribution correction for any task on the fly, without per-task optimization. By reweighting task supervision and steering latent policy selection toward task-aligned, dataset-consistent behavior, ZOL mitigates extrapolation and overestimation errors common in offline RL.

## Method Summary
ZOL operates on pretrained FB embeddings to enable zero-shot task adaptation. The method first computes an initial task latent through reward-weighted regression, then optimizes this latent to maximize a centered return objective with self-normalized importance weights. The key innovation is estimating stationary density ratios directly from FB representations using the identity w_π/β(s,a) ≈ (1-γ)E_ρ₀[F(s₀,a₀,z)]ᵀB(s,a), which avoids per-task optimization while correcting for distribution shift. The optimization balances return maximization with χ² divergence regularization and trust region constraints to maintain dataset consistency.

## Key Results
- Outperforms state-of-the-art baselines including FB, ReLA, LoLA, HILP, and TD-JEPA across motion tracking (SMPL Humanoid), continuous control (ExoRL), and long-horizon manipulation (OGBench)
- Achieves double-digit percentage gains in unnormalized returns while requiring no online interaction or finetuning
- Ablation studies show ZOL is robust to hyperparameter choices, with performance degradation occurring only under aggressive optimization

## Why This Works (Mechanism)

### Mechanism 1
Stationary density ratios can be derived directly from pretrained FB representations without per-task optimization. The successor density m_π in FB decomposes as F(s₀,a₀,z)ᵀB(s,a). Taking expectation over initial states yields w_πz/β(s,a) ≈ (1-γ)W_πzᵀB(s,a), where W_πz = E_ρ₀[F(s₀,a₀,z)]. This provides a low-rank, dataset-only estimator of occupancy correction. Core assumption: FB factorization accurately approximates true successor measure; rewards are approximately linear in backward features.

### Mechanism 2
Standard FB latent inference (z = E[B(s,a)r(s,a)]) is suboptimal under biased dataset support because it weights by d_β rather than policy-induced occupancy. ZOL replaces support-conditioned inference with an objective that explicitly accounts for visitation induced by candidate z. The density ratio upweights states likely under πz while downweighting unsupported regions, steering latent search toward dataset-consistent behaviors. Core assumption: pretrained latent space contains good behaviors; the issue is inference, not representation capacity.

### Mechanism 3
Reward centering plus self-normalized importance weights recovers the on-policy return objective up to a constant baseline. J_c(π) = E_dβ[w(r - r̄)] = E_dπ[r] - r̄. This converts optimization to advantage-weighted form, preventing collapse to do-nothing behaviors when rewards contain large offsets. Core assumption: rewards are centered meaningfully; χ² penalty coefficient is appropriately tuned.

## Foundational Learning

- **Concept: Successor Measures / Successor Features**
  - Why needed here: ZOL's core insight is that successor measures encode visitation distributions. Understanding SMs is prerequisite to grasping why density ratios emerge from FB representations.
  - Quick check question: Can you explain why Q^π_r(s,a) = ∫ M^π(s,a,s',a') r(s',a') ds'da' separates dynamics from rewards?

- **Concept: Stationary Occupancy and Density Ratios**
  - Why needed here: The paper's entire mechanism hinges on w_π/β = dπ/dβ and its FB approximation. Without understanding occupancy measures, the DICE connection is opaque.
  - Quick check question: What does w_π/β(s,a) > 1 indicate about policy π relative to the behavior policy?

- **Concept: Forward-Backward Representations**
  - Why needed here: ZOL is a plug-in for pretrained FB models. Understanding the bilinear structure F(s,a,z)ᵀB(s',a') is essential for implementing the ratio estimator.
  - Quick check question: In FB, what do the forward embedding F and backward embedding B each encode?

## Architecture Onboarding

- **Component map:** Pretrained FB model (F_θ, B_ψ) -> Task inference (c_T = E[B(s,a)r_T]) -> Ratio estimator (w_i(z) = softmax(softplus((1-γ)B(s_i)ᵀμ̂(z)))) -> Latent optimizer (Adam on z maximizing J_total)

- **Critical path:** 1. Cache N reset states {s₀^(j)} for μ̂(z) estimation 2. Compute μ̂(z) = (1/N)Σ F(s₀^(j), π_z(s₀^(j)), z) 3. Compute weights w_i(z) for batch samples 4. Evaluate objective with centered rewards and regularizers 5. Gradient step on z; repeat for T steps

- **Design tradeoffs:** Fewer optimization steps (100 vs 200) often better - aggressive optimization causes degradation; moderate trust penalty (0.02-0.05) improves stability; large χ² penalty overly conservative; weight clipping has minimal impact primarily as safety constraint

- **Failure signatures:** Performance identical to FB baseline → likely dβ ≈ dπ, no correction needed; negative improvement on some tasks → check if task requires sustained out-of-support behavior; numerical instability → verify weight clipping and gradient norm clipping are active

- **First 3 experiments:** 1. Toy validation: Replicate 2D donut experiment with your FB implementation to verify ratio estimator recovers task structure before scaling 2. Hyperparameter sweep: On one DMC domain, sweep {steps: [100,200], trust: [0, 0.02, 0.05], χ²: [0, 0.001, 0.005]} following Appendix F protocol to establish robustness 3. Ablation components: Disable each regularizer (centering, χ², trust region) individually to confirm each contributes to stability on Walker/Quadruped tasks

## Open Questions the Paper Calls Out
- Can the ZOL framework be effectively extended to handle stochastic dynamics or partial observability (POMDPs)? The current theoretical derivation relies on FB representations defined for standard MDPs, making the low-rank stationary density ratio approximation unclear when the Markov property is violated.

- How can ZOL overcome the limitation of trading off performance for conservatism when a task requires sustained out-of-support behavior? The method operates strictly within dataset support, inherently penalizing necessary exploration for optimal behavior in unsupported regions.

- Does incorporating uncertainty quantification into the density ratio estimation improve the reliability of zero-shot adaptation? The method currently assumes FB factorization accurately captures successor features, potentially confidently steering optimization toward suboptimal regions when representations are flawed.

## Limitations
- Method's reliance on accurate FB factorization creates a single point of failure - if backward features don't span reward space adequately, density ratio estimates become unreliable
- Tasks requiring sustained exploration beyond dataset support remain fundamentally challenging, as reweighting cannot create information absent from offline buffer
- Computational overhead from caching reset states and computing full batch weights may become prohibitive for high-dimensional state spaces

## Confidence
- **High confidence** in theoretical derivation connecting successor measures to density ratios and practical effectiveness on standard benchmarks
- **Medium confidence** in robustness claims - relative to other off-policy methods rather than absolute guarantees
- **Low confidence** in scalability claims beyond tested domains due to computational overhead concerns

## Next Checks
1. **Stress-test failure modes**: Systematically evaluate ZOL on tasks requiring significant out-of-support behavior to quantify breakdown point of density ratio estimation
2. **Computational scaling analysis**: Measure runtime and memory scaling as function of state/action dimensionality and dataset size to establish practical limits
3. **Generalization robustness**: Test ZOL on tasks where pretrained FB embeddings were trained on different data distributions than evaluation tasks, assessing sensitivity to distribution shift between pretraining and adaptation phases