---
ver: rpa2
title: Learning Real-World Acrobatic Flight from Human Preferences
arxiv_id: '2508.18817'
source_url: https://arxiv.org/abs/2508.18817
tags:
- reward
- preference
- learning
- human
- preferences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of learning complex acrobatic
  drone maneuvers through human preferences rather than hand-designed reward functions.
  The proposed method, REC Preference PPO, extends Preference PPO by introducing a
  probabilistic reward model that explicitly accounts for uncertainty in human feedback
  using an ensemble of reward predictors with uncertainty quantification.
---

# Learning Real-World Acrobatic Flight from Human Preferences

## Quick Facts
- arXiv ID: 2508.18817
- Source URL: https://arxiv.org/abs/2508.18817
- Reference count: 38
- Key outcome: Preference-based learning achieves 88.4% of shaped-reward baseline in acrobatic drone flight, outperforming standard methods despite only 60.7% agreement with engineered rewards

## Executive Summary
This paper addresses the challenge of learning complex acrobatic drone maneuvers through human preferences rather than hand-designed reward functions. The proposed method, REC Preference PPO, extends Preference PPO by introducing a probabilistic reward model that explicitly accounts for uncertainty in human feedback using an ensemble of reward predictors with uncertainty quantification. This approach significantly improves learning stability and performance, achieving 88.4% of the shaped reward baseline in simulation compared to 55.2% with standard Preference PPO. The method successfully transfers to real-world drones, enabling learning of both standard powerloops and novel vertical Figure-8 maneuvers purely from human preference labels.

## Method Summary
The method uses an ensemble of 5 MLP reward predictors that output both mean and standard deviation per timestep, enabling probabilistic preference modeling via Gaussian CDF. A noisy reward aggregation scheme adds exploration in high-uncertainty regions, and a reset strategy maintains ensemble plasticity. The approach combines unsupervised pretraining with particle-based entropy intrinsic reward, followed by preference-based RL using PPO. Human preferences are collected by comparing trajectory pairs, with query selection combining random, disagreement, and epoch-based sampling. The method achieves superior performance in both simulation (Flightmare) and real-world drone experiments, learning complex maneuvers from only 1000 preference labels.

## Key Results
- REC Preference PPO achieves 88.4% of shaped-reward baseline performance in simulation vs 55.2% for standard Preference PPO
- Successfully learns real-world powerloop and novel vertical Figure-8 maneuvers from human preferences alone
- Learns effective policies despite only 60.7% agreement between human preferences and engineered rewards
- Ensemble resetting strategy provides 2.5× improvement (382.4 vs 153.9 reward)

## Why This Works (Mechanism)

### Mechanism 1
Probabilistic reward modeling with explicit uncertainty quantification improves preference learning stability and final policy performance compared to deterministic reward predictions. Each ensemble member predicts both mean reward and standard deviation per timestep, with trajectory-level uncertainty computed via sum of Gaussians and used in a probabilistic preference model via the CDF of the difference distribution. This prevents overconfident predictions on ambiguous preferences. If human preferences are highly consistent and unambiguous, uncertainty modeling provides diminishing returns and may slow convergence.

### Mechanism 2
Noisy reward aggregation scaled by ensemble disagreement encourages exploration in high-uncertainty regions of the state-action space. The aggregated reward adds Gaussian noise with variance proportional to ensemble disagreement, promoting exploration where ensemble members disagree more. If disagreement reflects irreducible noise rather than exploration opportunity, noise injection may destabilize training.

### Mechanism 3
Ensemble resetting prevents reward model overfitting by periodically reinitializing poorly-performing ensemble members. Before each reward retraining, evaluate each ensemble member on new preferences and reset the worst n_reset members. This maintains model plasticity and prevents ensemble collapse. If the reset rate is too aggressive, ensemble never converges; if too conservative, overfitting persists.

## Foundational Learning

- **Bradley-Terry Model for Preferences**: Core mathematical framework converting trajectory rewards to preference probabilities; must understand before modifying the preference loss. Given two trajectories with rewards r₁=5 and r₂=3, what preference probability does the BT model predict? (Answer: softmax → ~88%)

- **Proximal Policy Optimization (PPO)**: Base RL algorithm; preference learning modifies the reward but keeps PPO policy optimization unchanged. What role does the clipping parameter ε play in PPO? (Answer: Constrains policy update magnitude to prevent collapse)

- **Ensemble Uncertainty via Disagreement**: REC uses variance across ensemble predictions as uncertainty signal; understanding epistemic vs. aleatoric uncertainty helps diagnose failure modes. If all ensemble members predict identical rewards but high variance, what type of uncertainty is this? (Answer: Aleatoric/data uncertainty, not epistemic/model uncertainty)

## Architecture Onboarding

- Component map: Environment (Flightmare) → Policy π_θ → Trajectories τ → Query Selection (Random/Disagreement/Epoch) → Human/Synthetic Labels → Reward Ensemble (5 MLPs) → Mean reward + Uncertainty → Probabilistic Preference Loss → Reward Model Update → Noisy Aggregation → PPO Policy Update → Sim-to-Real Transfer

- Critical path: **Query selection → Reward ensemble training → Policy optimization**. If reward ensemble fails to capture preferences (low agreement, high variance), downstream policy will not converge regardless of PPO tuning.

- Design tradeoffs:
  - More ensemble members → better uncertainty estimates but slower training
  - Higher query frequency → more labels but more human effort
  - Ensemble resetting stabilizes training but may discard useful hypotheses

- Failure signatures:
  - Policy stuck at initial behavior (e.g., half-flip only): Insufficient exploration → check unsupervised pretraining, increase reward noise
  - High variance across seeds: Exploration failure → ensemble disagreement may not correlate with useful exploration
  - Low human-reward agreement (<65%): Reward function may not capture task → validate preference labels, check query selection diversity

- First 3 experiments:
  1. **Sanity check**: Train PPO with ground-truth shaped reward in Flightmare powerloop environment; verify baseline convergence before preference methods
  2. **Ablation validation**: Run Preference PPO with synthetic labels; confirm ~55% performance vs. baseline matches paper before adding REC components
  3. **Ensemble diagnostics**: Log per-member reward predictions, disagreement statistics, and reset frequency; verify ensemble does not collapse to single mode during training

## Open Questions the Paper Calls Out

### Open Question 1
Can more sophisticated exploration strategies or curriculum learning approaches reduce the high variance in acrobatic task learning and enable agents to reliably discover complete maneuvers beyond initial partial movements? The authors state that "agents frequently failed to explore beyond the initial half-flip maneuver, resulting in low evaluation rewards and preventing us from demonstrating statistical significance across all conditions. This exploration challenge represents a common limitation in complex continuous control tasks and suggests that future work could benefit from incorporating more sophisticated exploration strategies or curriculum learning approaches."

### Open Question 2
How can the viewpoint dependency of human preference judgments be mitigated to improve learning consistency and reduce bias from camera placement during trajectory evaluation? The authors note that "human judgments are inherently viewpoint-dependent, as the perceived quality of acrobatic maneuvers can vary significantly based on the camera angle and perspective from which the flight is observed, potentially affecting the overall learning performance."

### Open Question 3
What specific trajectory features do humans prefer in acrobatic flight that are not captured by standard engineered reward functions, given the observed 60.7% agreement rate? The paper reports only 60.7% agreement between human preferences and engineered rewards, concluding that "effective policies were learned despite only 60.7% agreement between human preferences and engineered rewards." This gap suggests humans value qualities not formalized in the shaped reward.

## Limitations
- High variance across seeds (±80–190 reward) suggests exploration challenges remain unresolved
- Several key implementation details unspecified (σ_target value, exact reset criteria)
- Human preference collection interface and viewpoint bias not fully characterized
- Assumption that ensemble disagreement indicates exploration opportunity not rigorously validated

## Confidence

- **High confidence**: REC Preference PPO outperforms standard Preference PPO in simulation (88.4% vs 55.2% of baseline)
- **Medium confidence**: Probabilistic reward modeling improves stability and handles preference ambiguity
- **Medium confidence**: Sim-to-real transfer achieves novel acrobatic maneuvers from preferences alone
- **Low confidence**: Ensemble disagreement reliably indicates exploration opportunities

## Next Checks

1. **Ablation on uncertainty modeling**: Compare REC with deterministic ensemble (no std predictions, no probabilistic loss) to isolate contribution of uncertainty handling

2. **Human preference reproducibility**: Re-run preference collection with different annotators and measure inter-rater reliability and agreement with shaped reward

3. **Reset frequency sensitivity**: Systematically vary n_reset and reset schedule to find optimal balance between plasticity and convergence