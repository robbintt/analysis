---
ver: rpa2
title: Growable and Interpretable Neural Control with Online Continual Learning for
  Autonomous Lifelong Locomotion Learning Machines
arxiv_id: '2505.12029'
source_url: https://arxiv.org/abs/2505.12029
tags:
- learning
- locomotion
- robot
- figure
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Growable Online Locomotion Learning Under
  Multicondition (GOLLUM), a novel approach for autonomous lifelong locomotion learning
  in robots. GOLLUM addresses four key challenges: incomprehensibility, sample inefficiency,
  lack of knowledge exploitation, and catastrophic forgetting.'
---

# Growable and Interpretable Neural Control with Online Continual Learning for Autonomous Lifelong Locomotion Learning Machines

## Quick Facts
- arXiv ID: 2505.12029
- Source URL: https://arxiv.org/abs/2505.12029
- Reference count: 11
- Primary result: Hexapod robot learns multiple locomotion skills (walking, slope climbing, bouncing) autonomously within an hour using prediction-based neurogenesis

## Executive Summary
This paper introduces GOLLUM, a novel approach for autonomous lifelong locomotion learning in robots that addresses four key challenges: incomprehensibility, sample inefficiency, lack of knowledge exploitation, and catastrophic forgetting. The core method utilizes an interpretable neural control structure with two dimensions: layer-wise interpretability for neural control function encoding and column-wise interpretability for robot skill encoding. GOLLUM employs neurogenesis to unsupervisely increment columns, each trained separately to encode and maintain a specific primary robot skill. It also transfers parameters to new skills and supplements learned combinations through another neural mapping layer added with online supplementary learning.

## Method Summary
GOLLUM implements an interpretable neural control architecture with 7 layers (FB, I', I, C, B, PM, M/V/O) that learns locomotion skills through dual-layer learning: primary learning updates skill-specific motor connections while supplementary learning updates inter-skill connections for composition. The system detects novel conditions through simultaneous deviation in both value predictions and observations, triggering unsupervised neurogenesis that creates new skill-specific subnetworks initialized from the most similar existing skill. Experience replay with 8 samples per episode enables efficient learning across multiple terrains and conditions without human intervention.

## Key Results
- Successfully acquired multiple locomotion skills (walking, slope climbing, bouncing) on physical hexapod robot within one hour
- Achieved final walking speed of almost 10 cm/s after 10 repetitions on flat terrain
- Only approach addressing all four challenges (incomprehensibility, sample inefficiency, knowledge exploitation, catastrophic forgetting) without human intervention
- Required only minutes to achieve comparable performance to previous methods that took hours

## Why This Works (Mechanism)

### Mechanism 1: Column-Wise Skill Separation Prevents Catastrophic Forgetting
Structurally separating skills into distinct neural columns prevents interference between learned behaviors. Each skill is encoded in a dedicated subnetwork (ring-like CPG structure). Primary learning updates only the active subnetwork's connections (PM→M), masking updates for inactive subnetworks. This isolation preserves previously learned parameters.

### Mechanism 2: Prediction Mismatch Triggers Autonomous Neurogenesis
Detecting novel conditions through value/observation prediction deviation enables unsupervised skill expansion. The network maintains value predictions (V[t]) and observation predictions (O[t]) with adaptive uncertainty boundaries (Vδ, Oδ). When both (1) actual reward falls below V - Vδ AND (2) observation exceeds O ± Oδ, a new subnetwork is created and initialized with weights from the most similar active skill.

### Mechanism 3: Supplementary Learning Enables Cross-Skill Composition
Inter-subnetwork connections from basis (B) to premotor (PM) layers allow learned skills to supplement new skill acquisition. While primary learning refines skill-specific motor outputs, supplementary learning updates weights between internal states of different subnetworks. This allows the robot to access and combine action patterns from inactive skills during new skill learning.

## Foundational Learning

- Concept: Central Pattern Generators (CPGs) as rhythmic priors
  - Why needed here: The sequential CPG layer (C) generates rhythmic internal states without sensory input, providing locomotion-timing structure before learning begins.
  - Quick check question: Can you explain how a ring network of mutually inhibiting neurons produces stable oscillations?

- Concept: Sparse triangular basis functions
  - Why needed here: The basis layer (B) converts discrete CPG states to smooth, minimally-overlapping signals that decouple learning updates across time steps.
  - Quick check question: Why does having sparse, near-orthogonal basis functions accelerate gradient-based policy learning compared to dense activations?

- Concept: Prediction-based uncertainty estimation
  - Why needed here: Value/observation prediction boundaries (Vδ, Oδ) must be learned alongside predictions to detect statistically meaningful novelty vs. normal variance.
  - Quick check question: How should the learning rates for predictions vs. boundaries differ to avoid collapse (boundaries → 0) or explosion (boundaries → ∞)?

## Architecture Onboarding

- Component map: FB (21 sensory inputs) -> I' (classification) -> I (gated selection) -> C (CPG rings) -> B (triangular bases) -> PM (action patterns) -> M (motor commands)
- Critical path: FB → I' → I → C → B → PM → M (primary skill); B → PM (inter-skill supplementary)
- Design tradeoffs:
  - More C neurons per subnetwork → finer action granularity but more parameters
  - Larger prediction boundaries (Vδ, Oδ) → fewer neurogenesis events but slower adaptation
  - Higher supplementary learning rate → faster skill transfer but risk of interference
- Failure signatures:
  - Neurogenesis not triggering: Check that both reward AND observation deviation conditions are met; boundaries may be too large
  - Catastrophic forgetting: Verify primary learning mask (Mi) is correctly zeroing updates for inactive basis neurons
  - Skill not combining: Supplementary weights stuck at zero; may need higher learning rate or different initialization
  - Gait instability: C layer parameters (wCi,Ck, biases) may need tuning for desired frequency/phase relationships
- First 3 experiments:
  1. **Flat terrain locomotion from scratch**: Validate sample efficiency; expect ~10 cm/s within 200 episodes (10 mins). Monitor when robot first achieves forward motion.
  2. **Slope transition with neurogenesis**: Start flat, introduce 10° slope. Verify new subnetwork creation and weight copying from previous skill. Confirm no forgetting when returned to flat.
  3. **Ablation: Disable supplementary learning**: Compare reward curves on multi-terrain task with/without supplementary connections. Expect ~40% degradation on complex terrain combinations.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several implications emerge from the methodology and results.

## Limitations
- Limited validation of prediction-based neurogenesis mechanism with only one directly comparable study in the corpus
- Architectural complexity with 7 layers and dual learning rates creates substantial implementation variance risk
- Most significant uncertainty is the unproven assumption that value and observation prediction deviations always coincide with genuinely novel conditions

## Confidence

| Claim | Confidence |
|-------|------------|
| Sample efficiency (10 cm/s within 200 episodes) | High |
| Catastrophic forgetting prevention mechanism | Medium |
| Autonomous skill discovery through prediction-based neurogenesis | Low |

## Next Checks
1. Test neurogenesis sensitivity by varying prediction boundary sizes (Vδ, Oδ) to determine optimal trade-off between adaptation speed and stability.
2. Conduct ablation studies comparing GOLLUM against a simplified version without supplementary learning to quantify the claimed 40% performance improvement.
3. Validate skill composition by attempting to learn a terrain that requires blending multiple previously learned skills, measuring whether supplementary weights actually facilitate cross-skill transfer.