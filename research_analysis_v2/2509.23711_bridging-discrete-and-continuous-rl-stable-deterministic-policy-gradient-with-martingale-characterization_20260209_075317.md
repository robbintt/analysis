---
ver: rpa2
title: 'Bridging Discrete and Continuous RL: Stable Deterministic Policy Gradient
  with Martingale Characterization'
arxiv_id: '2509.23711'
source_url: https://arxiv.org/abs/2509.23711
tags:
- policy
- continuous-time
- learning
- time
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of extending discrete-time reinforcement
  learning to continuous-time environments, where existing methods suffer from instability
  and slow convergence due to time discretization sensitivity. The authors propose
  a novel continuous-time deterministic policy gradient method (CT-DDPG) based on
  a martingale characterization of the advantage rate function, enabling stable learning
  in continuous-time stochastic systems.
---

# Bridging Discrete and Continuous RL: Stable Deterministic Policy Gradient with Martingale Characterization

## Quick Facts
- **arXiv ID:** 2509.23711
- **Source URL:** https://arxiv.org/abs/2509.23711
- **Reference count:** 32
- **Primary result:** CT-DDPG achieves superior stability and faster convergence compared to both discrete-time and continuous-time baselines, particularly under small time steps and high noise levels.

## Executive Summary
This paper addresses the challenge of extending discrete-time reinforcement learning to continuous-time environments, where existing methods suffer from instability and slow convergence due to time discretization sensitivity. The authors propose a novel continuous-time deterministic policy gradient method (CT-DDPG) based on a martingale characterization of the advantage rate function, enabling stable learning in continuous-time stochastic systems. The method uses a multi-step temporal difference update that provably avoids gradient variance blow-up as discretization step size approaches zero. Extensive experiments across various control tasks demonstrate that CT-DDPG achieves superior stability and faster convergence compared to both discrete-time and continuous-time baselines, particularly under small time steps and high noise levels. The theoretical analysis also provides insights into why standard one-step TD methods fail in continuous-time settings.

## Method Summary
The method extends DDPG to continuous-time stochastic control systems governed by SDEs, using a martingale characterization of the advantage rate function. The algorithm employs a multi-step temporal difference update where the total horizon Lh is fixed to prevent gradient variance blow-up. The advantage rate function q_ψ is re-parameterized relative to the current policy to enforce the Bellman constraint efficiently without sampling actions. The critic is trained using a martingale loss that enforces orthogonality conditions, while the policy is updated using deterministic policy gradients. The method requires smooth dynamics and value functions (C^{1,2} regularity) and uses sinusoidal time embeddings to handle the finite horizon structure.

## Key Results
- CT-DDPG achieves bounded gradient variance as discretization step h→0 by using multi-step TD with fixed total horizon δ=Lh
- The martingale characterization enables stable deterministic policy gradients without requiring stochastic policy entropy terms
- Extensive experiments show CT-DDPG outperforms discrete-time and continuous-time baselines on Pendulum-v1, HalfCheetah-v5, Hopper-v5, and Walker2d-v5, especially under small time steps and high noise levels

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Replacing standard Bellman constraints with a martingale orthogonality condition enables stable gradient estimation for deterministic policies in continuous time.
- **Mechanism**: The paper characterizes the "advantage rate function" A_φ not via a recursive backup, but by enforcing that a specific stochastic process is a martingale (Theorem 3.2). This allows the algorithm to learn the gradient ∇_a A directly without requiring the density estimation or entropy terms necessary for stochastic policies in continuous time.
- **Core assumption**: The value function V_φ is C^{1,2} (once differentiable in time, twice in space) and the dynamics satisfy regularity conditions (Assumption 1).
- **Evidence anchors**:
  - [Section 3.2]: Establishes the martingale criterion for the advantage rate function.
  - [Section 3.3]: Discusses the inefficiency of stochastic policy constraints (Eq 3.9) compared to the deterministic re-parameterization.
  - [Corpus]: Related work on continuous-time RL often relies on stochastic policies or model-based assumptions; this mechanism avoids both.
- **Break condition**: If the function approximator for V or q lacks the smoothness required by Assumption 1, the theoretical guarantees regarding the gradient formula may fail.

### Mechanism 2
- **Claim**: Utilizing a multi-step Temporal Difference (TD) objective is necessary to prevent the variance of the stochastic gradient from diverging as the time step h→0.
- **Mechanism**: The paper proves that for standard one-step TD, the variance of the stochastic gradient scales as O(1/h) (Proposition 4.1), causing instability at high frequencies. By using a multi-step objective where the total horizon Lh is fixed (e.g., δ), the variance remains bounded (Proposition 4.2).
- **Core assumption**: The diffusion term σσ^T is uniformly elliptic (bounded below), ensuring noise does not vanish.
- **Evidence anchors**:
  - [Section 4.2]: "lim_{h→0} h * Var(g) = Θ(1)" proves the variance blow-up for single-step methods.
  - [Abstract]: "uses a multi-step temporal difference update that provably avoids gradient variance blow-up".
  - [Corpus]: Related papers like "Making deep q-learning methods robust to time discretization" identify discretization sensitivity but rely on different architectural fixes (e.g., action repetition).
- **Break condition**: If the integral interval δ=Lh is set too small, the variance reduction benefit is lost.

### Mechanism 3
- **Claim**: Re-parameterizing the advantage rate function q_ψ relative to the current policy enforces the Bellman constraint (Eq 3.6) efficiently without sampling actions.
- **Mechanism**: The algorithm defines q_ψ(x,a) = q̄_ψ(x,a) - q̄_ψ(x, μ_φ(x)) (Eq 4.2). This structural choice forces the advantage rate to be zero at the current policy action a=μ_φ(x), satisfying the constraint q(t,x,μ)=0 analytically. This removes the need to compute expectations over action distributions required by stochastic policy methods.
- **Core assumption**: The exploration policy covers the action neighborhood O_{μ_φ} sufficiently to learn q̄_ψ accurately.
- **Evidence anchors**:
  - [Section 4.1]: "Bellman constraints... we re-parameterize the advantage rate function..."
  - [Section 3.3]: Highlights that stochastic policies require costly integration over action space (Eq 3.9).
  - [Corpus]: "Accuracy of Discretely Sampled Stochastic Policies" suggests inherent difficulties in sampling stochastic policies continuously, which this mechanism avoids.
- **Break condition**: If exploration is insufficient, the critic learns incorrect values for off-policy actions, leading to a biased policy gradient.

## Foundational Learning

- **Concept: Martingale Orthogonality / Stochastic Calculus**
  - **Why needed here**: The core loss function for the critic is derived from the property that a specific process is a martingale. You must understand that E[M_t] = M_0 and how orthogonality conditions constrain function classes to grasp why the "Martingale Loss" works.
  - **Quick check question**: Can you explain why checking martingale orthogonality against a set of test functions is equivalent to verifying the pricing kernel in a Black-Scholes economy?

- **Concept: Itô's Lemma**
  - **Why needed here**: The derivation of the gradient formula (Thm 3.1) and the variance analysis (Prop 4.1) rely on Itô's expansion of the value function V(t, X_t). The "drift" and "diffusion" terms in the loss are direct results of this lemma.
  - **Quick check question**: If dX_t = μ dt + σ dW_t, what is the second-order term in the expansion of dV(X_t) that distinguishes it from standard calculus?

- **Concept: Deterministic Policy Gradient (DPG)**
  - **Why needed here**: This is an extension of DDPG to continuous time. You need to know that DPG updates the policy by moving the action in the direction of the gradient of the Q-function (or advantage rate), rather than using the log-probability trick used in stochastic policies.
  - **Quick check question**: Why does DPG typically require off-policy exploration noise (e.g., Ornstein-Uhlenbeck or Gaussian) added to the deterministic action?

## Architecture Onboarding

- **Component map**: State/time embedding -> Actor μ_φ -> Action; State/time embedding -> Critic V_θ -> Value; State/time embedding, Action -> Critic q̄_ψ -> Advantage rate; Value target V_θ^tgt -> Terminal constraint
- **Critical path**: The **Martingale Loss (L_M)**. The implementation depends on computing the integral of rewards minus the advantage rate over a trajectory segment of length L. If L is implemented incorrectly (e.g., L=1 while h is small), variance blows up and learning fails.
- **Design tradeoffs**:
  - **Step size h vs. Horizon L**: As h decreases (higher frequency), L must increase proportionally (L≈δ/h) to maintain a fixed integral window δ. A larger δ reduces variance but introduces bias if the function approximation is poor over long horizons.
  - **Re-parameterization**: Enforcing q(x, μ)=0 via subtraction is elegant but creates a dependency where the critic's output magnitude must scale correctly with the policy changes.
- **Failure signatures**:
  - **Gradient Blow-up**: If using 1-step TD (L=1) with small h, check for exploding variance in the critic gradients (Fig 3 behavior).
  - **Time Inconsistency**: If the time embedding is omitted from the input, the agent cannot distinguish between early and late states in the finite horizon, leading to suboptimal terminal behavior.
  - **Stiff Dynamics**: In MuJoCo environments, if the "generalized force" noise injection is too high relative to h, the Euler-Maruyama discretization used for data collection may become unstable.
- **First 3 experiments**:
  1. **Variance Validation**: Reproduce Fig 3. Plot the "Noise-to-Signal Ratio" of the critic gradient for DAU (L=1) vs CT-DDPG (L>1) as h→0. This validates the core Proposition 4.1.
  2. **Ablation on Re-parameterization**: Compare the proposed q_ψ(x,a) = q̄ - q̄(x, μ_φ) against a naive unconstrained q-network. This tests the efficiency of the Bellman constraint enforcement described in Section 4.1.
  3. **Robustness Sweep**: Train on HalfCheetah while sweeping discretization step h (e.g., h∈[0.01, 0.001]). Plot performance degradation of standard DDPG/SAC vs. CT-DDPG to verify the discretization-invariance claim.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the limitations section raises several important unresolved issues regarding the applicability of the method to infinite-horizon problems, non-smooth dynamics, and the optimal selection of the multi-step horizon L.

## Limitations
- The theoretical analysis assumes smooth dynamics (C^{1,2} regularity) that may not hold for many real-world environments with contact dynamics or discontinuities
- The multi-step TD approach requires setting the total horizon δ=Lh appropriately, with no principled guidance on the optimal value
- The re-parameterization trick creates dependencies between critic and policy that could lead to instability if exploration is insufficient

## Confidence

- **High Confidence**: The martingale characterization of the advantage rate function and the variance analysis showing O(1/h) blow-up for one-step methods (Proposition 4.1) are mathematically rigorous and well-supported by the proofs in the appendices.
- **Medium Confidence**: The experimental results showing superior stability and performance across different environments are compelling, though the paper could provide more ablation studies on the impact of different hyperparameters like L, δ, and exploration noise levels.
- **Medium Confidence**: The claim about avoiding the need for stochastic policy gradients by using the re-parameterization trick is theoretically sound, but practical implementation details about ensuring sufficient exploration coverage are not fully addressed.

## Next Checks

1. **Variance Scaling Validation**: Reproduce the numerical experiment from Proposition 4.1 by computing the ratio h · Var(g_{θ,h}) for different step sizes h and TD horizons L, verifying that it remains bounded when L>1 but scales as Θ(1/h) when L=1.

2. **Exploration Sufficiency Test**: Design an experiment where the exploration noise level is gradually reduced to zero, monitoring the critic's ability to accurately estimate q_ψ(x,a) for actions a far from the current policy μ_φ(x). This validates the assumption that the exploration policy covers the action neighborhood O_{μ_φ}.

3. **Time Embedding Ablation**: Train CT-DDPG with and without the sinusoidal time embeddings in the state representation on a finite-horizon task, measuring the difference in performance particularly near the terminal time T. This tests the importance of time-awareness for the martingale-based learning objective.