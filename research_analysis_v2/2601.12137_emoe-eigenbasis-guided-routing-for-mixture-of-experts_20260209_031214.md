---
ver: rpa2
title: 'EMoE: Eigenbasis-Guided Routing for Mixture-of-Experts'
arxiv_id: '2601.12137'
source_url: https://arxiv.org/abs/2601.12137
tags:
- expert
- emoe
- experts
- routing
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the fundamental challenges in Mixture-of-Experts
  (MoE) models, specifically the load imbalance problem ("rich get richer" phenomenon)
  and expert homogeneity problem, where experts learn redundant representations. The
  authors propose EMoE (Eigen-Mixture-of-Experts), a novel architecture that uses
  a learned orthonormal eigenbasis for routing decisions.
---

# EMoE: Eigenbasis-Guided Routing for Mixture-of-Experts

## Quick Facts
- arXiv ID: 2601.12137
- Source URL: https://arxiv.org/abs/2601.12137
- Authors: Anzhe Cheng; Shukai Duan; Shixuan Li; Chenzhong Yin; Mingxi Cheng; Shahin Nazarian; Paul Thompson; Paul Bogdan
- Reference count: 0
- One-line primary result: EMoE achieves 88.14% Top-1 accuracy on ImageNet with ViT-H while maintaining better load balance across experts than standard MoE.

## Executive Summary
This paper addresses fundamental challenges in Mixture-of-Experts (MoE) models: load imbalance ("rich get richer") and expert homogeneity (redundant representations). The authors propose EMoE, which uses a learned orthonormal eigenbasis for routing decisions instead of conventional gating networks. By projecting input tokens onto this eigenbasis and routing based on alignment with principal components, EMoE achieves balanced expert utilization and specialized expert representations without auxiliary loss functions. The method demonstrates competitive performance on ImageNet classification and generalizes well to biomedical applications like brain age prediction from MRI data.

## Method Summary
EMoE replaces standard MoE gating with an Eigen Router that maintains a learnable orthonormal eigenbasis U ∈ ℝ^(D×r). Input tokens are projected onto this basis, and routing decisions are based on the "energy" along each principal direction. Expert assignment uses learned weights π_{j,k} over basis dimensions, transforming eigen-alignments into expert logits. The method includes an orthonormality loss to prevent basis collapse and uses top-1 sparse gating for efficiency. Only patch tokens are routed through the MoE branch; the class token bypasses it. The architecture consists of ViT blocks with parallel MoE branches containing 8 expert MLPs, followed by residual addition.

## Key Results
- Achieves 88.14% Top-1 accuracy on ImageNet with ViT-H backbone
- Maintains significantly better load balance across experts than standard MoE
- Reduces brain age prediction error by 10.4% compared to standard 3D CNNs (2.16 years MAE vs 2.41)
- Demonstrates strong few-shot learning capabilities on CIFAR-10/100 and Tiny ImageNet

## Why This Works (Mechanism)

### Mechanism 1: Orthonormal Eigenbasis Projection for Routing
Projecting input tokens onto a learned orthonormal eigenbasis creates natural geometric partitions in the feature space, enabling balanced routing without auxiliary losses. The router maintains a learnable eigenbasis matrix U that is constrained to remain orthonormal via regularization. Tokens are projected into this subspace, and their "energy" along each basis direction determines routing affinity. Expert assignment uses learned weights over basis dimensions, transforming eigen-alignments into expert logits.

### Mechanism 2: Orthonormality Loss Prevents Basis Collapse
An explicit orthonormality regularizer prevents the eigenbasis from collapsing or becoming degenerate, maintaining routing diversity. By penalizing deviations from orthonormality, the basis vectors remain mutually orthogonal and unit-normalized. This ensures each basis direction captures independent variance in the data, which in turn ensures tokens have distinguishable routing profiles across experts.

### Mechanism 3: Top-1 Sparse Gating with Learned Expert-Basis Affinity
Associating each expert with a learned weight vector over basis dimensions allows experts to specialize on coherent feature directions while maintaining load balance. Expert scores are computed as weighted sums of token energy across basis dimensions. The learned weight matrix encodes which basis directions each expert "owns," creating specialization by construction. Since tokens are distributed across basis directions by their natural variance, load is balanced geometrically rather than via auxiliary loss.

## Foundational Learning

- **Concept: Principal Component Analysis (PCA) / Eigendecomposition**
  - Why needed here: EMoE's routing is fundamentally based on projecting tokens onto principal components. Understanding eigenvalues, eigenvectors, and covariance matrices is essential to debug routing behavior.
  - Quick check question: Given a batch of token features H ∈ ℝ^(N×D), how would you compute the top-r principal components?

- **Concept: Mixture-of-Experts (MoE) Sparse Gating**
  - Why needed here: EMoE modifies standard MoE gating. Understanding baseline Top-k gating, load-balancing losses, and the "rich-get-richer" problem provides context for why eigenbasis routing is novel.
  - Quick check question: In a standard MoE with Top-2 gating and auxiliary load-balancing loss, what happens if the loss weight is too high?

- **Concept: Orthonormality Constraints in Optimization**
  - Why needed here: The L_ortho loss enforces orthonormality on U. Understanding regularization effects and how to tune λ is critical for stable training.
  - Quick check question: What is the Frobenius norm of (U^T U - I) when U is perfectly orthonormal? When U has two identical columns?

## Architecture Onboarding

- **Component map**: Input Tokens -> ViT Block (Attention → LayerNorm) -> Eigen Router (project → compute energy → route to top-1 expert) -> Expert MLP (8 experts, 2-layer FFN with GELU) -> Residual addition

- **Critical path**: 
  1. Initialize U (random orthonormal matrix)
  2. Initialize expert-basis weights Π, scales γ, biases b
  3. Forward pass: project tokens → compute energy → route to top-1 expert → apply expert MLP → residual add
  4. Backward pass: task loss + L_ortho regularization
  5. Continuously re-orthogonalize U during training

- **Design tradeoffs**:
  - Number of basis dimensions (r): Higher r captures more variance but increases routing computation
  - Number of experts (8): Fixed at 8 in experiments
  - Top-1 vs Top-k gating: Paper uses Top-1 for efficiency
  - With vs without auxiliary LBL: EMoE removes LBL entirely

- **Failure signatures**:
  - Expert collapse: Heatmaps show near-zero rows (unused experts)
  - Single expert monopoly: One expert receives >50% of tokens
  - Training instability: Loss spikes may indicate eigenbasis drift
  - Degraded accuracy vs baseline: May indicate routing is not aligning with semantic structure

- **First 3 experiments**:
  1. Baseline replication: Implement standard MoE with Top-2 gating + auxiliary LBL on CIFAR-10 with ViT-B
  2. Ablation on L_ortho weight: Train EMoE with λ_ortho ∈ {0, 0.001, 0.01, 0.1}
  3. Expert specialization visualization: For trained EMoE, compute expert-class routing heatmap and measure pairwise expert similarity

## Open Questions the Paper Calls Out
- Can EMoE effectively scale to heterogeneous, multi-modal biomedical data beyond the evaluated brain age prediction task?
- Does the eigenbasis routing mechanism generalize to sequential domains like Natural Language Processing (NLP)?
- How sensitive is the balance between expert utilization and specialization to the choice of the reduced dimensionality r?

## Limitations
- Critical hyperparameters (eigenbasis dimension r, orthonormality regularization weight λ_ortho) are not specified
- Implementation details for maintaining orthonormal eigenbasis during training are sparse
- Biomedical application results are based on a single dataset without extensive medical baseline comparisons
- Lack of comprehensive ablation studies on how architecture scale affects performance

## Confidence
- High confidence: Load balancing improvement over standard MoE
- Medium confidence: Competitive ImageNet classification accuracy
- Medium confidence: Few-shot learning generalization
- Low confidence: 10.4% reduction in brain age prediction error

## Next Checks
1. Ablation on orthonormality weight: Systematically vary λ_ortho and measure both basis orthonormality and load balance
2. Direct comparison of expert specialization: Compute pairwise expert representation similarity for EMoE vs baseline MoE
3. Distribution drift analysis: Monitor eigenbasis orthonormality and routing entropy throughout training under domain shift