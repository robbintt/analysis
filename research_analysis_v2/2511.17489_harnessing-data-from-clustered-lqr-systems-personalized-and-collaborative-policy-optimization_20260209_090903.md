---
ver: rpa2
title: 'Harnessing Data from Clustered LQR Systems: Personalized and Collaborative
  Policy Optimization'
arxiv_id: '2511.17489'
source_url: https://arxiv.org/abs/2511.17489
tags:
- policy
- agents
- epoch
- learning
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles personalized policy learning in clustered Linear
  Quadratic Regulator (LQR) systems with unknown dynamics. It proposes PCPO, a novel
  algorithm combining sequential elimination for clustering with zeroth-order policy
  optimization, to learn personalized policies per cluster.
---

# Harnessing Data from Clustered LQR Systems: Personalized and Collaborative Policy Optimization

## Quick Facts
- **arXiv ID:** 2511.17489
- **Source URL:** https://arxiv.org/abs/2511.17489
- **Reference count:** 40
- **Primary result:** PCPO guarantees correct clustering with high probability and achieves a sub-optimality gap scaling as O(1/√(T|M|)), where T is the number of rollouts per agent and M is the cluster size, with no additional bias from heterogeneity.

## Executive Summary
This paper introduces PCPO, a novel algorithm for personalized policy learning in clustered Linear Quadratic Regulator (LQR) systems with unknown dynamics. PCPO combines sequential elimination for clustering with zeroth-order policy optimization to learn personalized policies per cluster without requiring prior knowledge of system dynamics or cluster identities. The algorithm achieves significant sample-complexity gains proportional to cluster size while maintaining theoretical guarantees against negative transfer, incurring only logarithmic communication overhead.

## Method Summary
PCPO is a model-free algorithm that learns personalized policies for agents in clustered LQR systems where dynamics and costs are identical within clusters but heterogeneous across clusters. The algorithm maintains two separate policy sequences per agent: a local sequence for safe clustering exploration and a global sequence for collaborative optimization. It operates in epochs using sequential elimination to identify cluster memberships, then performs collaborative zeroth-order gradient descent within correctly identified clusters. The method requires only an initial stabilizing controller and achieves a sub-optimality gap of O(1/√(T|M|)) where M is the cluster size.

## Key Results
- Guarantees correct clustering with high probability through sequential elimination strategy
- Achieves sub-optimality gap scaling as O(1/√(T|M|)) proportional to cluster size
- Maintains zero bias from heterogeneity while achieving sample-complexity gains
- Incurs only logarithmic communication overhead
- Prevents negative transfer through two-policy-sequence design

## Why This Works (Mechanism)

### Mechanism 1
The algorithm guarantees correct clustering of agents into their true groups with high probability through sequential elimination. It maintains local policy sequences used exclusively for identification, performing policy optimization until policies are Δ_l/8-suboptimal and estimating resulting costs. Agents prune neighbors whose estimated costs differ by more than Δ_l/2. Because Δ_l eventually drops below the true separation Δ/2, agents from different clusters are mathematically guaranteed to exclude each other, while true cluster members (sharing identical optimal costs) are retained. This works when cluster separation gap Δ > 0 and noise processes are independent across agents.

### Mechanism 2
The algorithm learns personalized near-optimal policies with sample complexity speedup proportional to cluster size |M| through collaborative gradient aggregation within correctly identified clusters. Once clustering stabilizes, agents compute zeroth-order gradient estimates and send them to a server that averages gradients only over confirmed cluster members. Averaging |M| independent estimates reduces gradient noise variance by a factor of |M|, enabling faster convergence (∝ 1/√|M|) than single-agent learning while maintaining zero bias toward the specific cluster optimum. This requires correct clustering and noise independence.

### Mechanism 3
The system maintains stability and avoids negative transfer by isolating learning and clustering processes into separate policy sequences. PCPO maintains local sequences for clustering and global sequences for collaboration, reinitializing the global sequence whenever neighborhood sets change. This prevents contamination of the global policy with gradients from misclustered agents that could cause instability or negative transfer in control. The local policy optimization subroutine keeps iterates within the stabilizing set with high probability. This design requires an initial stabilizing controller and proper hyperparameter selection.

## Foundational Learning

- **Concept: Linear Quadratic Regulator (LQR) & Stability**
  - **Why needed here:** The framework assumes agents control linear dynamical systems and optimize quadratic costs; a stabilizing controller is required to keep costs finite.
  - **Quick check question:** If a control policy K is not stabilizing for system (A, B), what happens to the infinite-horizon cost C(K)? (Answer: It diverges to infinity.)

- **Concept: Zeroth-Order Optimization (Gradient Estimation)**
  - **Why needed here:** The algorithm is model-free and estimates gradients ∇C(K) using only cost samples from perturbed policies rather than analytical derivatives.
  - **Quick check question:** How does the smoothing radius r in the gradient estimator affect the bias-variance trade-off?

- **Concept: Sequential Elimination (Bandits)**
  - **Why needed here:** The clustering mechanism progressively eliminates candidate neighbors as confidence increases, mirroring multi-armed bandit strategies.
  - **Quick check question:** Why does the algorithm halve the estimated separation gap Δ_l in each epoch instead of setting it to the true value immediately?

## Architecture Onboarding

- **Component map:**
  1. Agent Node: Maintains Local Policy (X) and Global Policy (K̂). Runs LocalPO for clustering and GlobalPO for learning. Computes noisy cost rollouts.
  2. Server Node: Maintains Neighborhood Sets (N_i) for all agents. Aggregates gradients from agents in same neighborhood. Enforces Reinitialization rule if clusters change.

- **Critical path:**
  1. Epoch Start: Agent sets parameters (Δ_l, M_l, R_l).
  2. Local Phase: Agent runs LocalPO → gets X_i. Estimates cost Ĉ(X_i). Sends result to Server.
  3. Clustering Update: Server updates N_i based on cost differences (Sequential Elimination). Triggers reinitialization of K̂ if sets changed.
  4. Global Phase: Agent runs GlobalPO using N_i^(l-1). Server averages gradients. Agent updates K̂.
  5. Repeat.

- **Design tradeoffs:**
  - Sample Efficiency vs. Communication: Communication-efficient (logarithmic) but requires sufficient samples M_l per epoch for clustering accuracy.
  - Safety vs. Speed: Two separate policy sequences add overhead but prevent negative transfer and instability during early uncertain epochs.

- **Failure signatures:**
  - Continuous Reinitialization: Global policy keeps resetting, indicating clustering not converging (likely Δ too small or noise too high).
  - Cost Divergence: Rollouts return NaN or Inf, indicating policy left stabilizing set; check step size η and smoothing radius r.

- **First 3 experiments:**
  1. Homogeneous Baseline: Run PCPO with all agents in single cluster. Verify global policy converges to optimal LQR gain K* with √N speedup.
  2. Two-Cluster Separation: Simulate two distinct clusters with known gap Δ. Log neighborhood sets N_i per epoch. Verify correct pruning into two disjoint sets once Δ_l ≤ Δ/2.
  3. Negative Transfer Check: Force "naive" collaboration (averaging all agents immediately) on two-cluster data. Compare final sub-optimality and stability against PCPO.

## Open Questions the Paper Calls Out
- **Open Question 1:** Can PCPO framework and its sample complexity guarantees be extended to non-linear dynamical systems or settings where standard LQR assumptions do not hold?
- **Open Question 2:** How can PCPO algorithm be adapted to online settings where system dynamics or cluster structure may drift over time?
- **Open Question 3:** How would alternative measures of dissimilarity between systems impact sample complexity and separation requirements?
- **Open Question 4:** How does the algorithm perform when cluster separation gap Δ approaches zero, potentially leading to overlapping or "soft" clusters?

## Limitations
- Requires unknown problem constants (Lipschitz smoothness parameters, initial gap Δ₀) for hyperparameter selection
- Assumes availability of an initial stabilizing controller without providing a procedure to obtain one
- Sequential elimination may be sensitive to hyperparameter choice when cluster separation gap is small relative to noise variance
- Analysis assumes perfect knowledge of problem-dependent constants for practical implementation

## Confidence
- **High confidence:** Theoretical convergence rate O(1/√(T|M|)) for correctly clustered agents given correct hyperparameter settings; variance reduction mechanism is mathematically sound
- **Medium confidence:** Practical effectiveness of sequential elimination clustering strategy in real-world scenarios with finite samples and unknown Δ
- **Medium confidence:** Stability guarantees when clusters are correctly identified; practical threshold for safe collaboration under real noise conditions requires empirical validation

## Next Checks
1. Implement a practical scheme to estimate or adaptively select Δ₀ without prior knowledge of true cluster separation, testing convergence across varying Δ/σ ratios
2. Systematically vary smoothing radius r, step size η, and epoch parameters M_l, R_l across different problem regimes to identify practical selection rules or adaptive schemes
3. Design experiments that deliberately violate stability conditions to empirically identify when and how negative transfer manifests in global policy sequence, validating two-sequence design's protective effect