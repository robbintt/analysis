---
ver: rpa2
title: Sobolev Training of End-to-End Optimization Proxies
arxiv_id: '2505.11342'
source_url: https://arxiv.org/abs/2505.11342
tags:
- training
- optimization
- sobolev
- sensitivities
- power
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel approach to training machine learning
  models as fast, differentiable proxies for optimization problems. The core idea
  is Sobolev training, which incorporates first-order sensitivities from the underlying
  optimization solver into the loss function.
---

# Sobolev Training of End-to-End Optimization Proxies

## Quick Facts
- arXiv ID: 2505.11342
- Source URL: https://arxiv.org/abs/2505.11342
- Authors: Andrew W. Rosemberg; Joaquim Dias Garcia; Russell Bent; Pascal Van Hentenryck
- Reference count: 40
- One-line primary result: Sobolev training reduces AC-OPF proxy error by up to 56% and constraint violations by 400% compared to MSE-only approaches.

## Executive Summary
This paper introduces Sobolev training for machine learning proxies of optimization problems, incorporating first-order sensitivities into the loss function alongside function values. The method improves proxy accuracy and constraint satisfaction while providing high-fidelity gradients for downstream decision-making. Evaluated on AC optimal power flow and mean-variance portfolio optimization, Sobolev training demonstrates significant performance gains over standard MSE-only approaches, with the sparse Jacobian masking technique enabling tractable training on large-scale problems.

## Method Summary
Sobolev training extends supervised learning by matching both function values and first-order sensitivities (Jacobians) between the proxy model and the true optimization solution. Sensitivities are extracted via KKT system differentiation using DiffOpt.jl. A sparse masking strategy retains only 5-25% of Jacobian entries per batch to reduce gradient interference and memory requirements. The loss combines standard MSE with a Jacobian-matching term weighted by λ. The approach is implemented in Julia using Lux.jl for the neural proxy and tested on AC-OPF benchmarks and portfolio optimization problems.

## Key Results
- For AC-OPF (ieee300), Sobolev training reduces MSE by up to 56% and median worst-case constraint violation by up to 400% compared to MSE-only training.
- Optimality gaps remain below 0.22% for all tested AC-OPF benchmarks.
- In portfolio optimization, Sobolev training halves the average optimality gap in the medium-risk region while maintaining similar performance to MSE in high-risk regions.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Matching first-order sensitivities alongside function values reduces uniform approximation error from O(δ) to O(δ²) where δ is the training set covering radius.
- **Mechanism:** The Sobolev loss combines value regression with derivative matching: L(θ) = ℓ(ĝθ(pi), g(pi)) + λ·ℓd(M ⊙ Dĝθ(pi), M ⊙ Dg(pi)). When both values and Jacobians match at training points, the fundamental theorem of calculus shows the interpolation error between points depends on the second-order term (remainder), yielding quadratic rather than linear error scaling with training set density.
- **Core assumption:** The true solution mapping g and proxy ĝθ are both C¹ with Lipschitz-continuous derivatives (Lipschitz constants Mg, Mĝg), and regularity conditions (LICQ, SOSC, strict complementarity) hold for the underlying optimization problem.
- **Evidence anchors:**
  - [abstract] "Under Lipschitz-continuity assumptions on the true solution mapping, matching first-order sensitivities is shown to yield uniform approximation error proportional to the training-set covering radius."
  - [section 4, Theorem 3] "sup_{p∈P} ||ĝθ(p) - g(p)|| ≤ (1/2)(Mg + Mĝg)δ²" with proof showing remainder cancellation.
  - [corpus] Related work "Optimization Learning" (FMR=0.67) discusses optimization proxies but without this specific theoretical bound.
- **Break condition:** If the solution mapping has discontinuous derivatives (e.g., active set changes, degenerate duals), or if training points don't cover the parameter space densely enough, the O(δ²) bound degrades. Sparse or zero derivatives from stepwise solution mappings provide uninformative gradient signals.

### Mechanism 2
- **Claim:** Sparse masking (retaining only 5-25% of Jacobian entries) improves training stability and reduces constraint violation compared to dense Jacobian supervision.
- **Mechanism:** Full Jacobian matching overconstrains the model—each batch enforces thousands of derivative relationships simultaneously, creating gradient interference between competing constraints. Random masking of 75-95% of entries per mini-batch reduces this contention, allowing the model to learn smoother approximations without sacrificing the theoretical benefits of derivative information.
- **Core assumption:** The masked subset of sensitivities is sufficiently representative of the local geometry; the unmasked entries don't systematically exclude critical constraint sensitivities.
- **Evidence anchors:**
  - [abstract] "sparsity-masked, first-order Sobolev loss enforces agreement... on only a carefully chosen subset of partial derivatives. The mask (ii) eliminates the gradient interference that appears under dense supervision."
  - [section 7, Limitations] "enforcing every entry of the solver Jacobian in each batch overconstrained the model, degrading performance relative to the MSE-only benchmark."
  - [Appendix D, Table 5] Mask sparsity 5% yields MSE 0.0065; 100% dense yields MSE 0.0148 on ieee300.
  - [corpus] No directly comparable sparsity mechanism found in neighbor papers; this appears novel to this work.
- **Break condition:** If the mask systematically excludes sensitivities for binding constraints (e.g., always masks thermal limit derivatives), the proxy may learn to violate those constraints. The paper uses random masking per-instance to mitigate this risk.

### Mechanism 3
- **Claim:** Solver sensitivities extracted via KKT system differentiation provide ground-truth derivative signals that align the proxy's learned geometry with the optimization landscape.
- **Mechanism:** At a KKT point s* = (x(p), λ(p)), the implicit function theorem gives ∇p s* = -(∇s F)*⁻¹ ∇p F where F encapsulates the KKT conditions. This Jacobian describes how primal/dual variables shift with parameter changes. Computing this requires: (1) solving the optimization problem, (2) extracting the KKT Jacobian structure, (3) solving the linear system. These sensitivities are then used as supervision targets.
- **Core assumption:** The KKT Jacobian ∇s F(s*, p) is non-singular (satisfied under LICQ + SOSC), ensuring unique, well-defined sensitivities.
- **Evidence anchors:**
  - [section 2.1, Eq. 1] Explicit formula for ∇p s* via implicit function theorem.
  - [section 3, Dataset Creation] "Solver sensitivities Dg(pi) are extracted through sensitivity analysis using software such as DiffOpt.jl."
  - [corpus] "A General and Streamlined Differentiable Optimization Framework" (FMR=0.55) describes updated DiffOpt.jl capabilities, confirming this infrastructure exists.
- **Break condition:** If regularity conditions fail (degenerate active sets, non-unique solutions), the KKT Jacobian becomes singular and sensitivities are undefined or discontinuous. The paper notes "fix-and-relax" methods can restore validity in edge cases.

## Foundational Learning

- **Concept:** Karush-Kuhn-Tucker (KKT) Conditions and Constraint Qualifications
  - **Why needed here:** The entire Sobolev training framework depends on extracting valid sensitivities from the KKT system. Without understanding LICQ (Linear Independence Constraint Qualification) and SOSC (Second-Order Sufficient Conditions), you cannot diagnose when sensitivity extraction will fail or produce garbage.
  - **Quick check question:** Given a constrained optimization problem, can you identify whether the active constraints at the optimum are linearly independent (LICQ holds) and whether strict complementarity is satisfied? If the dual variable for a binding inequality is zero, what does that imply for sensitivity validity?

- **Concept:** Automatic Differentiation Through Optimization Layers
  - **Why needed here:** The proxy training requires computing ∂ĝθ/∂p (the network's Jacobian w.r.t. inputs) to compare against solver sensitivities. This involves nested AD—differentiating through the network to get its Jacobian, then using that in a loss function that itself requires gradients for backpropagation.
  - **Quick check question:** If your deep learning framework doesn't support efficient Jacobian-vector products or higher-order differentiation, what workarounds exist? Can you explain why standard reverse-mode AD alone is insufficient for computing network input-output Jacobians efficiently?

- **Concept:** Implicit Function Theorem for Parametric Optimization
  - **Why needed here:** The theoretical guarantee (Theorem 3) and the sensitivity extraction mechanism both rely on the implicit function theorem applied to KKT systems. Understanding this connection explains why the method works for "well-behaved" problems and fails for degenerate ones.
  - **Quick check question:** In the equation ∇p s* = -(∇s F)⁻¹ ∇p F, which matrix must be invertible for sensitivities to exist? If an active constraint becomes weakly active (on the boundary but with zero dual), how does this affect invertibility?

## Architecture Onboarding

- **Component map:**
  ```
  [Parameter Sampler] → [JuMP/PowerModels Model] → [IPOPT Solver] → Primal Solution x*
         ↓
  [DiffOpt.jl] ← KKT Structure ← [Solver Internals]
         ↓
  KKT Jacobian & Sensitivities Dg(p)
         ↓
  [Sparse Mask M] → Masked Sensitivities
         ↓
  [Training Data: (p_i, x*_i, Dg(p_i)_masked)]
         ↓
  [Lux.jl Neural Proxy] → Predictions x̃, Jacobian Dĝθ(p)
         ↓
  [Sobolev Loss: MSE + λ·||M ⊙ Dĝθ - M ⊙ Dg||²]
         ↓
  [Adam Optimizer] → Updated θ
  ```

- **Critical path:**
  1. **Dataset generation** (offline, most time-consuming): Sample parameters → solve optimization → extract sensitivities via DiffOpt → apply sparse mask. This requires running IPOPT + sensitivity extraction for ~10K instances.
  2. **Training loop** (online): Forward pass through proxy → compute Jacobian via Lux.jl nested AD → evaluate Sobolev loss → backpropagate. The Jacobian computation is the memory bottleneck; sparse masking reduces this from O(n×d) to O(0.05×n×d).
  3. **Inference** (deployment): Single forward pass, millisecond-scale.

- **Design tradeoffs:**
  - **Mask sparsity vs. gradient coverage:** 5% sparsity yields lowest MSE but risks missing critical constraint sensitivities. 25% provides more coverage but degrades performance. Paper recommends 5-25% based on problem scale.
  - **Supervised vs. self-supervised:** Supervised requires pre-solving all training instances (expensive) but provides exact sensitivities. Self-supervised uses Lagrangian loss without labels but may struggle in tight-constraint regimes (portfolio optimization showed 2x worse performance when σ_max ≤ 10% of budget).
  - **Architecture depth vs. regularity:** Deeper networks with C² activations (tanh, softplus) satisfy theoretical assumptions; ReLU networks have discontinuous second derivatives but worked well empirically in AC-OPF experiments.
  - **λ weighting:** Too high → overfits to derivatives at expense of value accuracy; too low → no benefit over MSE. Paper uses λ ∈ [0.1, 4.35] depending on problem.

- **Failure signatures:**
  - **Training divergence with dense Jacobian loss:** If loss oscillates or increases after initial epochs, mask sparsity is likely too high (>50%). Reduce to 5-10%.
  - **High constraint violation despite low MSE:** Proxy fits values but geometry is wrong. Check that sensitivities for binding constraints aren't systematically masked. Increase mask density or add feasibility projection layer.
  - **NaN sensitivities during data generation:** KKT Jacobian is singular. Check for degenerate active sets (multiple constraints active at same boundary) or violated regularity conditions. Use fix-and-relax methods or filter out invalid instances.
  - **Self-supervised training produces high optimality gap (>20%):** Problem is in tight-constraint regime where first-order information is uninformative. Switch to supervised training or use mixture-of-experts (benchmark in tight regions, Sobolev elsewhere).

- **First 3 experiments:**
  1. **Sanity check on small problem:** Implement the pipeline on a trivial convex problem (e.g., 5-variable quadratic program with 2 parameters). Generate 100 samples, train with 0%, 5%, and 100% mask density. Verify that 5% achieves lower interpolation error than 0% (MSE-only) and lower training instability than 100%. This validates the core mechanism before scaling.
  2. **Mask sparsity ablation on target problem:** Using a single AC-OPF benchmark (e.g., ieee300), sweep mask sparsity [5%, 10%, 25%, 50%, 100%] while holding λ fixed. Plot test MSE, worst-case constraint violation, and training stability (loss variance). Identify optimal sparsity before running expensive large-scale experiments.
  3. **Supervised vs. self-supervised comparison:** On the portfolio optimization task, train two proxies: (a) fully supervised with exact solutions and sensitivities, (b) self-supervised with Lagrangian loss only. Stratify test results by constraint tightness (σ_max/B ratio). Confirm paper's finding that self-supervised Sobolev excels in medium-risk but underperforms in tight-risk regimes. This determines whether your application can avoid expensive pre-solving.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can incorporating second-order derivative information (Hessian sensitivities) further improve approximation fidelity of optimization proxies beyond first-order Sobolev training?
- Basis in paper: [explicit] "Future research may explore the incorporation of second-order derivative information to further improve approximation fidelity"
- Why unresolved: The current work only leverages first-order sensitivities; second-order information could capture curvature but introduces computational and memory challenges not addressed.
- What evidence would resolve it: Empirical comparison of proxies trained with Hessian-augmented losses versus first-order Sobolev on benchmarks like AC-OPF, measuring MSE, constraint violation, and computational cost.

### Open Question 2
- Question: Can Sobolev training be effectively adapted to mixed-integer optimization problems where the solution mapping is inherently discontinuous?
- Basis in paper: [explicit] "adapt the Sobolev approach to mixed-integer and stochastic optimization problems"
- Why unresolved: Sobolev training relies on differentiability of the solution mapping; integer variables introduce combinatorial structure that breaks the smoothness assumptions underpinning the theoretical guarantees.
- What evidence would resolve it: A modified Sobolev framework for mixed-integer problems (possibly using continuous relaxations or surrogate gradients) demonstrating improved proxy accuracy over MSE-only baselines on unit commitment or facility location benchmarks.

### Open Question 3
- Question: Would a mixture-of-experts architecture that selectively invokes the benchmark proxy in tight-constraint regions and the Sobolev proxy elsewhere consistently outperform either approach alone?
- Basis in paper: [explicit] "The pronounced split in both training and test suggests a mixture-of-experts strategy that calls the benchmark inside the high-constraint region and the Sobolev proxy elsewhere."
- Why unresolved: The paper observes regime-dependent performance splits but does not implement or evaluate the proposed mixture approach.
- What evidence would resolve it: Construction and evaluation of a gating-based mixture model on the portfolio optimization task, reporting optimality gaps across risk regimes compared to standalone baselines.

### Open Question 4
- Question: How can feasibility-restoration or projection techniques be integrated to handle instances where solver regularity conditions (LICQ, SOSC, strict complementarity) are marginally violated?
- Basis in paper: [explicit] "develop feasibility-restoration or projection techniques to handle instances where solver regularity conditions are marginally violated"
- Why unresolved: Degeneracies produce invalid or discontinuous sensitivities; current experiments excluded such cases without providing a systematic remedy.
- What evidence would resolve it: A robust training pipeline that detects irregular cases and applies corrective projections, evaluated on stress-test datasets with known degeneracies, showing stable proxy performance.

## Limitations

- The theoretical O(δ²) error bound assumes C¹ smoothness and regularity conditions that may fail in real-world optimization problems with degenerate active sets or non-unique solutions.
- The evaluation focuses on relatively standard test cases and may not generalize to highly nonlinear or discrete optimization problems.
- Self-supervised Sobolev training performs poorly in tight-constraint regimes where first-order information is uninformative.

## Confidence

- **High**: Core value regression component (matching function values)
- **Medium**: Sobolev extension (matching derivatives) - theoretically sound but relies on assumptions that may not hold
- **High**: Sparse masking mechanism - strong empirical support
- **Medium**: Self-supervised training - effective in medium-risk regions but underperforms in tight-constraint regimes

## Next Checks

1. Test Sobolev training on optimization problems with known non-smooth solution mappings (e.g., mixed-integer programs or problems with many equality constraints) to assess the O(δ²) error bound's validity.
2. Systematically vary mask sparsity beyond the 5-25% range to identify whether there's a fundamental limit to the sparsity level before theoretical guarantees break down.
3. Compare Sobolev-trained proxies against specialized architectures (e.g., graph neural networks for power systems) to determine if the benefits are architecture-specific or general to the Sobolev loss formulation.