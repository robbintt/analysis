---
ver: rpa2
title: 'The Quasi-Creature and the Uncanny Valley of Agency: A Synthesis of Theory
  and Evidence on User Interaction with Inconsistent Generative AI'
arxiv_id: '2508.18563'
source_url: https://arxiv.org/abs/2508.18563
tags:
- user
- frustration
- cognitive
- agency
- valley
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper develops a theoretical framework to explain the intense
  frustration users experience when interacting with large-scale generative AI systems,
  which exhibit both superhuman fluency and baffling, inconsistent failures. The authors
  introduce two key constructs: the "Quasi-Creature," an entity that simulates intelligence
  without embodiment or genuine understanding, and the "Uncanny Valley of Agency,"
  where user comfort plummets when a highly agentic AI proves erratically unreliable.'
---

# The Quasi-Creature and the Uncanny Valley of Agency: A Synthesis of Theory and Evidence on User Interaction with Inconsistent Generative AI

## Quick Facts
- arXiv ID: 2508.18563
- Source URL: https://arxiv.org/abs/2508.18563
- Reference count: 0
- One-line primary result: Perceived inefficiency (β=-0.45) is the strongest predictor of user frustration with generative AI, with a strong negative correlation (r=-0.85) between perceived AI efficiency and frustration.

## Executive Summary
This paper develops a theoretical framework to explain intense user frustration when interacting with large-scale generative AI systems that exhibit both superhuman fluency and baffling, inconsistent failures. The authors introduce two key constructs: the "Quasi-Creature," an entity that simulates intelligence without embodiment or genuine understanding, and the "Uncanny Valley of Agency," where user comfort plummets when a highly agentic AI proves erratically unreliable. An illustrative mixed-methods study ("Move 78," N=37) reveals a powerful negative correlation between perceived AI efficiency and user frustration (r=-0.85), with multiple regression showing perceived inefficiency (β=-0.45) as the strongest predictor of frustration.

## Method Summary
The study employed a between-subjects experiment with 37 participants from design and business fields, randomly assigned to solo, dyad, or small group conditions. Participants engaged in a 3-hour collaborative creative task (service concept generation) using a text-only conversational application based on a Retrieval Augmented Generation (RAG) architecture, deliberately modified to be inconsistent. The "Move 78 Playbook" provided five worksheets for the task, and participants completed pre/post surveys measuring AI familiarity and efficiency, while interaction logs tracked questions asked and reaction button usage. Analysis focused on NASA-TLX frustration scores, negative reaction ratios, and correlations with perceived efficiency.

## Key Results
- Perceived inefficiency (β=-0.45) was the strongest predictor of frustration in multiple regression analysis
- Strong negative correlation between perceived AI efficiency and frustration (r=-0.85)
- Total number of questions asked showed strong positive correlation with frustration level (r=+0.74)
- Dyads exhibited net negative sentiment but the lowest flag rate (0.01), suggesting frustration absorption

## Why This Works (Mechanism)

### Mechanism 1: The Efficiency-Frustration Nexus
- **Claim:** User frustration with generative AI is primarily predicted by perceived inefficiency rather than general fallibility.
- **Mechanism:** When an AI exhibits high fluency (creating an expectation of competence) but fails at basic consistency, it forces the user into a high-effort "repair" mode. The interaction shifts from a flow state to a troubleshooting state, drastically increasing cognitive load.
- **Core assumption:** Users approach generative AI with an expectation of coherence based on its linguistic fluency.
- **Evidence anchors:**
  - [abstract] "perceived inefficiency ($\beta=-0.45$) as the strongest predictor of frustration" and "powerful negative correlation between perceived AI efficiency and user frustration ($r=-0.85$)."
  - [section 3.6] "Engagement as a Symptom of Struggle... total number of questions asked... has a very strong positive correlation with frustration level ($r=+0.74$)."
  - [corpus] "Noosemia" (arXiv:2508.02622) supports the cognitive-phenomenological account of intentionality attribution which breaks down here.
- **Break condition:** This mechanism likely fails if users treat the AI purely as a stochastic search engine (low agency expectation) rather than a collaborative partner, thereby reducing the "inefficiency" surprise.

### Mechanism 2: The Ontological "Uncanny Valley" of Agency
- **Claim:** The "Uncanny Valley" effect applies to cognitive agency; discomfort spikes when an entity mimics high-level intentionality but reveals a lack of genuine understanding.
- **Mechanism:** Users unconsciously apply the "intentional stance" (attributing beliefs/desires) to the AI. When the AI hallucinates or loses context, it violates this social heuristic. The user experiences "cognitive whiplash" because the failure is perceived as a cognitive breach (a "stupid mistake") rather than a software bug.
- **Core assumption:** Humans automatically anthropomorphize agentic-seeming interfaces (CASA paradigm).
- **Evidence anchors:**
  - [abstract] "Intense frustration... is an ontological problem... rooted in users' failed attempts to attribute coherent mental states."
  - [section 2.3] References the tension between Dennett's "intentional stance" and Searle's "Chinese Room."
  - [corpus] "Mitigating the Uncanny Valley Effect" (arXiv:2503.16449) links LLM conversational capability to uncanny effects in robotics.
- **Break condition:** If the system design explicitly signals "tool" status (e.g., visible confidence scores, non-conversational UI), the attribution of agency may be suppressed, bypassing the valley.

### Mechanism 3: Social Buffering in Dyadic Interactions
- **Claim:** The social structure of the user (Individual vs. Dyad) mediates the behavioral expression of frustration.
- **Mechanism:** Dyads (pairs) exhibit a "frustration-absorption" effect. When the AI fails, partners validate each other's frustration internally ("commiseration"), reducing the need to escalate or formally flag errors. The "Thumbs Down" becomes a communicative act between humans, not just feedback for the machine.
- **Core assumption:** Users seek social validation for negative experiences with technology.
- **Evidence anchors:**
  - [section 3.5] Dyads showed "net negative sentiment" (ratio 0.94) but the "lowest Flag Rate (0.01)," contradicting the assumption that dissatisfaction leads to escalation.
  - [section 3.5] "The 'Thumbs Down' reaction becomes a tool for intra-dyad communication."
  - [corpus] Weak/missing explicit support for this specific "dyadic buffering" mechanism in the provided corpus.
- **Break condition:** This may not hold in high-stakes professional environments where dyads might actually be *more* critical due to shared reputational risk.

## Foundational Learning

- **Concept:** Theory of Mind (ToM)
  - **Why needed here:** To understand why users get angry at "stupid" AI answers. Users aren't just seeing wrong data; they are failing to model the "mind" of the agent, leading to the "Ontological Problem."
  - **Quick check question:** When the AI hallucinates, does the user try to "correct" the data (tool mindset) or ask "why did you say that?" (social mindset)?

- **Concept:** Phenomenology (Ready-to-Hand vs. Present-at-Hand)
  - **Why needed here:** Explains the *feeling* of the "Rupture." A working AI is "ready-to-hand" (invisible tool); a broken AI becomes "present-at-hand" (an object of scrutiny and frustration).
  - **Quick check question:** At what point in the error cycle does the user stop looking at the task output and start staring at the chat interface/ Prompts?

- **Concept:** The CASA Paradigm (Computers Are Social Actors)
  - **Why needed here:** Provides the baseline rule that people treat computers like people. The "Quasi-Creature" theory is an extension of CASA into the realm of inconsistent agency.
  - **Quick check question:** Are your users being polite to the bot? If yes, they are in the CASA zone and vulnerable to the Uncanny Valley of Agency.

## Architecture Onboarding

- **Component map:**
  - User (with Mental Model: Tool vs. Social Actor) -> Interaction Layer (RAG system + Reaction Buttons) -> AI Core (LLM generating fluent but potentially inconsistent text) -> Measurement (NASA-TLX + Sentiment Ratios)

- **Critical path:**
  1.  User inputs prompt → AI generates high-fluency response (sets high expectation)
  2.  AI fails context/hallucinates (Rupture event)
  3.  User attempts "repair" (re-prompting)
  4.  AI fails again → "Quasi-Creature" perception emerges → Frustration spikes (r=-0.85 with efficiency)

- **Design tradeoffs:**
  - **High Agency Persona:** Increases engagement initially but risks a deeper fall into the "Uncanny Valley of Agency" upon failure.
  - **Opaque vs. Transparent:** Hiding reasoning reduces clutter but increases the "inscrutability" factor, fueling the "Expert User Expectation Gap."

- **Failure signatures:**
  - **High Volume Looping:** A high count of `behav_total_questions` relative to task completion (correlated with frustration at r=+0.74)
  - **Negative Reaction Saturation:** A `behav_neg_react_ratio` approaching 1.0 without corresponding "Flag" events (indicating learned helplessness or dyadic absorption)
  - **Expert Gap:** Highly familiar users showing *higher* frustration than novices (violating the "easy-to-use" metric)

- **First 3 experiments:**
  1.  **Correlate Efficiency vs. Frustration:** Implement NASA-TLX and efficiency ratings in your current logs. Verify if the r=-0.85 correlation holds for your specific user base.
  2.  **The "Dyad" A/B Test:** Run a study comparing Individual vs. Dyad usage. Measure if Dyads report lower "Flag" rates despite high negative sentiment (testing the social buffering mechanism).
  3.  **Intentional Stance Probe:** Introduce a "reason-giving" feature for AI errors. Test if explaining *why* the AI failed (making it less of a "black box") reduces the "cognitive whiplash" described in Section 2.3.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is there a specific, identifiable threshold of cognitive inconsistency that reliably causes users to fall into the "Uncanny Valley of Agency"?
- Basis in paper: [explicit] The authors explicitly call for "Experimental Manipulation of the Uncanny Valley of Agency" in Section 5.2 to determine this threshold.
- Why unresolved: The current study observed a correlation between inefficiency and frustration but did not experimentally manipulate the ratio of coherent to incoherent responses to establish causality.
- What evidence would resolve it: Controlled experiments that vary the frequency of AI failures to pinpoint the exact moment user comfort drops precipitously.

### Open Question 2
- Question: How do rupture-and-repair cycles evolve over time as users develop more sophisticated mental models of the AI?
- Basis in paper: [explicit] Proposed in Section 5.2 as "Longitudinal Studies of the Rupture-and-Repair Cycle."
- Why unresolved: The "Move 78" study was limited to a single 3-hour workshop, preventing the observation of long-term user adaptation or habituation.
- What evidence would resolve it: A longitudinal study tracking the co-adaptation of users and AI systems over several months to see if frustration diminishes or persists.

### Open Question 3
- Question: What specific cognitive repair strategies (e.g., simplification, re-framing) do users deploy when an AI violates their developing Theory of Mind?
- Basis in paper: [explicit] Listed in Section 5.2 under "Investigating the Failed ToM Attempt."
- Why unresolved: The current data relied on interaction logs and surveys, which capture behavioral outputs but not the internal cognitive strategies driving them.
- What evidence would resolve it: Qualitative data from think-aloud protocols or stimulated recall interviews conducted during moments of AI failure.

## Limitations

- The core mechanism relies on an illustrative case study (N=37) rather than full-scale validation
- The specific failure patterns of the "Move 78" system remain underspecified, making replication difficult
- The dyadic buffering effect lacks direct empirical support in the corpus and may be confounded by unmeasured variables

## Confidence

- **High Confidence**: The correlation between perceived efficiency and frustration (r=-0.85) is well-supported by the provided regression analysis and multiple evidence anchors. The basic observation that inconsistent AI triggers repair behavior is robust.
- **Medium Confidence**: The "Uncanny Valley of Agency" framework provides a useful theoretical lens, but its empirical validation is limited to the illustrative case. The mechanism assumes users apply intentional stance by default, which may vary by context.
- **Low Confidence**: The dyadic buffering mechanism lacks direct evidence from the corpus and may be an artifact of the specific task design or participant pool. The assumption that dyads reduce formal feedback while maintaining high negative sentiment needs independent validation.

## Next Checks

1. **Validate the Efficiency-Frustration Correlation in Your Population**: Implement NASA-TLX and efficiency ratings in your current logs. Test whether your user base shows the same r=-0.85 correlation, or if the relationship differs by user expertise level.

2. **Test the Intentional Stance Intervention**: Add a "reason-giving" feature that explains AI errors (e.g., "I lost context due to limited memory window"). Measure whether this reduces cognitive whiplash compared to unexplained failures.

3. **Replicate the Dyadic Effect with Controlled Variables**: Run a between-subjects experiment comparing Individual vs. Dyad conditions, controlling for task type and participant personality. Specifically test whether dyads show lower Flag rates despite equivalent or higher negative sentiment.