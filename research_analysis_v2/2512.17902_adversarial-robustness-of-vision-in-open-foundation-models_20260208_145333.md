---
ver: rpa2
title: Adversarial Robustness of Vision in Open Foundation Models
arxiv_id: '2512.17902'
source_url: https://arxiv.org/abs/2512.17902
tags:
- adversarial
- vision
- arxiv
- llama
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the adversarial robustness of vision components
  in open foundation models, specifically LLaVA-1.5-13B and Llama 3.2 Vision-8B-2.
  The authors employ untargeted Projected Gradient Descent (PGD) attacks against the
  visual input modality and evaluate performance degradation on the VQA v2 dataset
  subset using VQA accuracy.
---

# Adversarial Robustness of Vision in Open Foundation Models

## Quick Facts
- arXiv ID: 2512.17902
- Source URL: https://arxiv.org/abs/2512.17902
- Reference count: 28
- Primary result: LLaVA-1.5-13B achieved 87.4% baseline VQA accuracy but dropped by up to 36.0% under PGD attack, while Llama 3.2 Vision-8B-2 showed 42.8% baseline but only 10.4% maximum drop.

## Executive Summary
This paper investigates adversarial robustness of vision components in open foundation models using untargeted PGD attacks on visual input. The study compares LLaVA-1.5-13B and Llama 3.2 Vision-8B-2 on VQA v2 dataset subset. While LLaVA showed higher baseline performance, it was significantly more vulnerable to adversarial perturbations. Llama 3.2 Vision exhibited greater robustness despite lower baseline accuracy, suggesting that architectural design and training scale influence robustness independently of benchmark performance.

## Method Summary
The study employs untargeted Projected Gradient Descent (PGD) attacks with L∞ constraint on visual inputs of two VLMs. The attack iteratively modifies image pixels to maximize model loss through gradient ascent. Six perturbation budgets (ε) are tested ranging from subtle (2/255) to complete noise (255/255), with step sizes and iterations scaled proportionally. Models are evaluated using VQA accuracy on a 500-sample subset of VQA v2 validation data. Implementation uses float16 precision and gradient-based optimization targeting the model's internal loss function.

## Key Results
- LLaVA-1.5-13B: 87.4% baseline VQA accuracy, dropped by up to 36.0% under maximum perturbation
- Llama 3.2 Vision-8B-2: 42.8% baseline VQA accuracy, dropped by only up to 10.4% under maximum perturbation
- Vision modality proved to be a viable attack vector for both models
- Robustness did not correlate with standard benchmark performance
- Architectural differences (projection vs cross-attention) and training scale influenced robustness

## Why This Works (Mechanism)

### Mechanism 1: Gradient-Based Optimization of Perturbations
An untargeted PGD attack degrades VQA accuracy by iteratively modifying input pixel values to maximize the model's internal loss function. The attack computes gradients of loss with respect to input images, takes steps in loss-increasing directions, and projects back into ε-ball. This pushes inputs into regions where visual representations lead to incorrect outputs. Effectiveness depends on gradient availability and model sensitivity to pixel-wise changes.

### Mechanism 2: Architectural Influence on Robustness
The choice of multimodal connector affects sensitivity to adversarial perturbations. LLaVA uses simple linear projection layer, allowing direct perturbation impact on language model processing. Llama 3.2 Vision employs cross-attention adapter with additional trainable layers that can mediate and dampen adversarial noise propagation from vision encoder to language model, creating more robust integration.

### Mechanism 3: Impact of Training Scale and Data
Scale and nature of training data influence stability under attack. Llama 3.2 Vision trained on ~6B image-text pairs for pre-training plus 500M higher-quality images, while LLaVA used only 595K pairs. Larger, more diverse datasets may force learning of more robust visual representations less sensitive to small perturbations, compared to models that might overfit to specific visual cues.

## Foundational Learning

**Concept: Projected Gradient Descent (PGD) Attack**
Why needed: Core experimental method for evaluating robustness as a "universal first-order adversary"
Quick check: How does iterative PGD differ from single-step FGSM, and why is it stronger?

**Concept: Vision-Language Model (VLM) Architecture**
Why needed: Key comparative point between projection and cross-attention adapter approaches
Quick check: What is the primary functional difference between linear projection layer and cross-attention adapter?

**Concept: L∞ Perturbation Budget (ε)**
Why needed: Constraint defining attack strength and perceptibility across all results
Quick check: What does L∞ norm constrain in image perturbation, and what does higher ε signify?

## Architecture Onboarding

**Component map**: Image -> Vision Encoder (ViT) -> [Connector] -> LLM -> Text Output

**Critical path**: Image -> Vision Encoder -> [Connector] -> LLM -> Text Output. Adversarial perturbation applied to initial Image.

**Design tradeoffs**: Simpler connector (LLaVA) achieved higher baseline accuracy but less robustness. More complex connector (Llama 3.2 Vision) with extensive training showed lower baseline accuracy but greater relative robustness, suggesting architectural complexity and training scale improve stability but may not directly translate to higher benchmark scores.

**Failure signatures**: Under attack, models generate "incorrect, irrelevant, or nonsensical answers" compared to clean outputs. Key signature is significant VQA accuracy drop even with subtle (low ε) perturbations.

**First 3 experiments**:
1. Baseline Accuracy Measurement: Evaluate VQA accuracy on clean subset to establish reference point
2. PGD Attack at Fixed Epsilon: Implement untargeted PGD with ε=4/255, measure VQA accuracy drop for both models
3. PGD Attack Across Epsilon Range: Run PGD across ε ∈ {2/255, 8/255, 16/255, 255/255} to observe accuracy degradation patterns

## Open Questions the Paper Calls Out

**Open Question 1**: What specific implementation factors (prompt formatting, image preprocessing, generation parameters) cause Llama 3.2 Vision's baseline VQA accuracy to drop from reported 75.2% to 42.8%? The authors acknowledge discrepancy but could not isolate cause, attributing it to "implementation-specific factors" without identification.

**Open Question 2**: Do native multimodal architectures (early fusion, joint pre-training) provide greater adversarial robustness than adapter-based approaches like Llama 3.2 Vision? The paper only compares one adapter-based model against one projection-based model; native multimodal architectures remain untested under same threat model.

**Open Question 3**: Which architectural component or training phase most strongly determines adversarial robustness in VLMs: adapter design, pre-training data scale, or alignment techniques? The two models differ in architecture, training data scale (billions vs. millions of pairs), and alignment methods simultaneously, confounding causal attribution.

**Open Question 4**: What defense mechanisms can effectively mitigate visual adversarial attacks in open-weight VLMs without degrading clean performance? The paper demonstrates vulnerability but does not evaluate any defenses; Meta's own recommendation of adversarial training remains untested on these models.

## Limitations
- Unknown implementation details (prompt templates, generation parameters, preprocessing) may affect baseline measurements
- Findings based on only two specific VLM architectures limit generalization
- Single attack vector (untargeted PGD on visual input) leaves other vulnerability scenarios unexplored
- Dataset and task specificity (VQA v2 subset) may not extend to other multimodal tasks

## Confidence

**High Confidence**: LLaVA-1.5-13B achieved higher baseline VQA accuracy (87.4%) but showed greater vulnerability to PGD attacks (up to 36.0% accuracy drop) - well-supported empirical finding.

**Medium Confidence**: Attribution of robustness differences primarily to architectural choices and training scale is reasonable but involves speculation without controlled ablation studies.

**Low Confidence**: Broader claim that "adversarial robustness does not necessarily correlate directly with standard benchmark performance" requires more extensive testing across diverse architectures and tasks.

## Next Checks

**Check 1: Controlled Architectural Ablation**: Implement hybrid model using LLaVA's vision encoder and LLM with Llama 3.2 Vision's cross-attention adapter to isolate whether robustness differences stem from connector design or other architectural variations.

**Check 2: Multi-Task Robustness Evaluation**: Replicate PGD attack experiments across multiple multimodal datasets and tasks (image captioning, visual entailment, referring expressions) to determine whether robustness patterns are task-specific or general VLM characteristics.

**Check 3: Alternative Threat Model Analysis**: Conduct experiments with targeted PGD attacks, L2-norm bounded perturbations, and black-box transfer attacks to assess whether relative robustness rankings remain consistent across different adversarial scenarios.