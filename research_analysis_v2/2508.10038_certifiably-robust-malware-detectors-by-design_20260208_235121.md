---
ver: rpa2
title: Certifiably robust malware detectors by design
arxiv_id: '2508.10038'
source_url: https://arxiv.org/abs/2508.10038
tags:
- malware
- adversarial
- robust
- features
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of building malware detectors that
  are robust to adversarial examples, where attackers modify malware to evade detection
  while preserving functionality. The authors propose a certifiably robust-by-design
  detector using a monotonic classifier with carefully selected features that are
  hard for attackers to modify.
---

# Certifiably robust malware detectors by design

## Quick Facts
- arXiv ID: 2508.10038
- Source URL: https://arxiv.org/abs/2508.10038
- Authors: Pierre-François Gimenez; Sarath Sivaprasad; Mario Fritz
- Reference count: 20
- Primary result: Proposes ERDALT, a framework that learns empirically robust malware detectors by combining linear feature transformation with monotonic classifiers, achieving 96% robustness on Windows PE malware.

## Executive Summary
This paper addresses the challenge of building malware detectors that maintain performance against adversarial examples—malware modified to evade detection while preserving functionality. The authors propose a certifiably robust-by-design approach using monotonic classifiers with carefully selected features that are difficult for attackers to modify. They prove that any robust detector can be decomposed into a monotonic classifier on transformed features, then develop ERDALT, a framework that jointly trains a linear feature transformation and monotonic classifier using adversarial examples. Experiments show their approach significantly outperforms standard defenses like adversarial training and PV feature selection, achieving 96% robustness while maintaining competitive detection accuracy.

## Method Summary
The approach leverages the theoretical insight that robust malware detectors can be decomposed into monotonic classifiers operating on monotonic feature transformations. ERDALT implements this by training a linear layer constrained to map observed perturbation vectors to non-negative outputs, combined with a monotonic classifier. The training process jointly optimizes classification accuracy while enforcing that the linear transformation produces non-negative outputs for all perturbation vectors in the threat model. This ensures that when combined with a monotonic classifier, the detector maintains its decision threshold even under adversarial transformations.

## Key Results
- Manual monotonic features yield 100% robustness with 76-94% ROC AUC
- ERDALT achieves 96% robustness and 93% ROC AUC on EMBER features
- ERDALT outperforms standard adversarial training and PV feature selection
- Combined ERDALT + adversarial training reaches 100% robustness at 85.5% AUC

## Why This Works (Mechanism)

### Mechanism 1: Monotonic Feature Mapping with Monotonic Classifier
- Claim: If features can only increase under attacker transformations, and the classifier is monotonic, then attackers cannot reduce the detector's output below the detection threshold.
- Mechanism: Define a preorder ⪯M over programs where P ⪯M P′ means P can be transformed into P′ using attacker transformations. If ϕ(P) ≤ ϕ(P′) and f is monotonically increasing, then f(ϕ(P′)) ≥ f(ϕ(P)) ≥ τ, preventing evasion.
- Core assumption: The selected features are truly monotonic with respect to the threat model—attackers can only increase them, not decrease.
- Break condition: If attackers discover transformations that decrease any monotonic feature (e.g., finding ways to reduce section count or imported functions), robustness guarantees fail.

### Mechanism 2: Decomposition Theorem (All Robust Classifiers ≈ Monotonic on Transformed Features)
- Claim: Any robust classifier can be mathematically decomposed into a feature transformation g followed by a monotonic classifier f∘h.
- Mechanism: Define a preorder on feature space based on classifier outputs. Map features through g to a latent space where the induced ordering aligns with standard vector ordering, enabling a monotonic classifier.
- Core assumption: The threat model M and feature mapping ϕ are fixed and known; the decomposition is constructive but may not be unique or easily learnable.
- Break condition: The proof is existential, not constructive—finding g and h in practice requires learning from data.

### Mechanism 3: ERDALT Learned Linear Transformation + Monotonic Classifier
- Claim: A linear layer constrained to map observed perturbation vectors to non-negative outputs, combined with a monotonic classifier, yields empirically robust detection.
- Mechanism: Train a linear layer L (no bias/activation) such that L(δ) ≥ 0 for all perturbation vectors δ in ∆M. Stack a monotonic classifier on top. Proposition 3 proves that if ∆M covers all transformations, robustness is guaranteed.
- Core assumption: Perturbation vectors are software-independent (finite ∆M) and the training adversarial examples adequately cover ∆M.
- Break condition: If ∆M is incomplete (new transformations emerge) or perturbations are software-dependent, learned L may not generalize.

## Foundational Learning

- Concept: **Partial Orders and Preorders**
  - Why needed here: The paper formalizes attacker capabilities as a preorder ⪯M over programs. Understanding reflexive, transitive, non-anti-symmetric relations is essential to follow the proofs.
  - Quick check question: Given transformations {add_section, rename_function}, if P can become P′ via add_section, and P′ can become P″ via rename_function, does P ⪯M P″ hold?

- Concept: **Monotonic Functions**
  - Why needed here: Both feature mappings and classifiers must be monotonic for robustness guarantees. Monotonicity means higher inputs produce higher outputs.
  - Quick check question: If f(x₁, x₂) = x₁ - x₂, is f monotonic in both arguments? Which argument breaks monotonicity?

- Concept: **Functionality-Preserving Adversarial Examples in Malware**
  - Why needed here: Unlike image perturbations, malware modifications must preserve executable semantics. This constrains the attacker's transformation set M.
  - Quick check question: Why can't an attacker simply flip arbitrary bytes in a PE file to evade detection?

## Architecture Onboarding

- Component map:
  Input Layer (PE features) -> Linear Transformation Layer (g) -> Monotonic Classifier (f∘h) -> Decision Threshold

- Critical path:
  1. Extract features ϕ(P) from PE binary.
  2. Apply learned linear transformation L(ϕ(P)).
  3. Pass through monotonic classifier.
  4. Threshold output for benign/malicious decision.

- Design tradeoffs:
  - **Manual features vs. EMBER**: Manual features yield higher robustness (100%) but lower AUC (76–94%); EMBER yields higher AUC but lower robustness without protection.
  - **ERDALT vs. Adversarial Training**: ERDALT provides interpretable feature selection; adversarial training is model-agnostic but less explainable. Both can be combined (Table 2: 100% robustness, 85.5% AUC).
  - **Linear layer expressiveness**: Diagonal matrix = feature selection; full matrix = feature combinations. Trade-off between simplicity and capacity.

- Failure signatures:
  - **Robustness drops below 90%**: Likely new transformations not in ∆M, or feature monotonicity assumption violated.
  - **AUC drops below 80%**: Linear layer may be dropping too many informative features; check l₃ weight.
  - **l₂ loss remains high**: Perturbation vectors not fully covered; need more diverse adversarial examples.

- First 3 experiments:
  1. **Feature Set Ablation**: Train monotonic classifier on manual features only. Expect ~100% robustness, ~70–76% AUC. Confirms Proposition 1.
  2. **ERDALT on EMBER**: Train ERDALT (linear + monotonic) with 1033 adversarial examples. Expect ~96% robustness, ~93% AUC. Compare to Table 2.
  3. **Perturbation Coverage Test**: Withhold 20% of transformation types from training, evaluate robustness on held-out attacks. Expect degraded robustness, validating ∆M coverage assumption.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the ERDALT framework be effectively adapted for robust malware dynamic analysis?
- Basis in paper: [explicit] The conclusion states, "In future work, we will adapt ERDALT to... robust malware dynamic analysis."
- Why unresolved: The current implementation and theoretical proofs are tailored to static analysis features (e.g., PE headers) and specific perturbation constraints that may differ significantly in dynamic execution environments.
- What evidence would resolve it: Successful application of the framework to dynamic analysis datasets, demonstrating high robustness against evasion attacks that modify runtime behavior.

### Open Question 2
- Question: How does the assumption that perturbation vectors are independent of the modified software impact performance against advanced attacks?
- Basis in paper: [inferred] The paper assumes "perturbation vectors related to one transformation does not depend on the modified software itself" to ensure the set ∆M is finite and linear layer learning is feasible.
- Why unresolved: Real-world attacks may use transformations whose effect on features depends on the specific context of the binary, violating the independence assumption and potentially bypassing the linear layer's monotonicity constraints.
- What evidence would resolve it: Evaluation of ERDALT against context-aware attacks where the feature perturbation is a function of the binary's existing structure.

### Open Question 3
- Question: Can ERDALT be applied to robust network packet classification without loss of detection capability?
- Basis in paper: [explicit] The conclusion lists "robust classifiers of network packets" as a target for future adaptation.
- Why unresolved: Network traffic features have different spatial and temporal dependencies compared to static binary features, and it is unclear if the linear transformation plus monotonic classifier model is expressive enough for this domain.
- What evidence would resolve it: Experiments showing ERDALT maintaining a high ROC AUC on standard network intrusion datasets while resisting adversarial packet perturbations.

## Limitations

- The exact implementation details of the 40 "manual robust" features are not specified, creating reproducibility challenges.
- The assumption that perturbation vectors are finite and software-independent may not hold against evolving attack strategies.
- The decomposition theorem is existential rather than constructive, requiring additional empirical validation for practical implementation.
- Evaluation is limited to Windows PE files, restricting generalizability to other malware types or platforms.

## Confidence

**High Confidence** (supported by formal proofs and experimental results):
- The theoretical framework proving that monotonic classifiers on monotonic features yield certifiable robustness
- ERDALT's ability to achieve 96% robustness while maintaining competitive detection performance
- Manual feature selection yielding 100% robustness on their dataset

**Medium Confidence** (theoretically sound but practically uncertain):
- The assumption that ∆M is finite and software-independent
- The completeness of adversarial example generation covering all relevant transformations
- The generalizability of results beyond the specific Windows PE malware dataset used

**Low Confidence** (speculative or under-specified):
- The exact implementation details of the 40 manual robust features
- Long-term robustness against evolving attack strategies not present in the training perturbation set
- Performance on malware families or platforms not represented in the dataset

## Next Checks

1. **Feature Implementation Verification**: Implement the 40 manual robust features based on the textual descriptions and validate their monotonicity properties with respect to the threat model transformations. Test whether these features maintain their monotonic behavior when applied to previously unseen malware samples.

2. **Perturbation Coverage Analysis**: Systematically withhold subsets of transformation types from the adversarial example generation process and measure the impact on ERDALT's robustness. This will quantify how well the learned linear transformation generalizes to unseen attack strategies.

3. **Cross-Platform Generalization Test**: Apply the ERDALT framework to a different malware detection task (e.g., Android malware) with platform-specific features and transformations. Measure whether the monotonicity-based approach transfers or requires significant adaptation for different malware ecosystems.