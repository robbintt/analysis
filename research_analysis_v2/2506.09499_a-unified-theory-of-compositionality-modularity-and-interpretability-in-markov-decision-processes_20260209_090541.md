---
ver: rpa2
title: A Unified Theory of Compositionality, Modularity, and Interpretability in Markov
  Decision Processes
arxiv_id: '2506.09499'
source_url: https://arxiv.org/abs/2506.09499
tags:
- function
- goal
- will
- where
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Option Kernel Bellman Equations (OKBEs) as
  a reward-free framework for Markov Decision Processes that directly optimizes a
  state-time option kernel (STOK) to maximize goal completion probability while avoiding
  constraint violations. The STOK is compositional, modular, and interpretable, allowing
  for high-dimensional planning through decomposition and forward search.
---

# A Unified Theory of Compositionality, Modularity, and Interpretability in Markov Decision Processes

## Quick Facts
- **arXiv ID:** 2506.09499
- **Source URL:** https://arxiv.org/abs/2506.09499
- **Reference count:** 0
- **Primary result:** Introduces Option Kernel Bellman Equations (OKBEs) as a reward-free framework for Markov Decision Processes that optimizes a state-time option kernel (STOK) to maximize goal completion probability while avoiding constraint violations.

## Executive Summary
This paper introduces Option Kernel Bellman Equations (OKBEs) as a reward-free framework for Markov Decision Processes that directly optimizes a state-time option kernel (STOK) to maximize goal completion probability while avoiding constraint violations. The STOK is compositional, modular, and interpretable, allowing for high-dimensional planning through decomposition and forward search. The authors demonstrate that OKBEs facilitate compositionality, modularity, and interpretability by constructing predictive maps that record goal success and constraint violation events. The framework supports verifiable planning and intrinsic motivation via empowerment, an information-theoretic measure of controllability.

## Method Summary
The method introduces OKBEs as a dynamic programming approach that replaces reward maximization with feasibility optimization. It defines a Task Markov Decision Process (TMDP) where goals and constraints are separable functions over state-action space. The core algorithm, Feasibility Iteration, iteratively updates a Cumulative Feasibility Function (κ), policy (π), and STOKs (η) using achievement and continuation functions. The framework scales to high-dimensional spaces through STOK factorization, which decomposes joint transition probabilities into independent base-level and high-level prediction kernels under specific homogeneity conditions.

## Key Results
- OKBEs provide a principled approach to planning and goal-directed behavior that scales to dynamic, high-dimensional environments while maintaining interpretability and verifiability
- The STOK factorization theorem enables efficient planning in high-dimensional spaces by decomposing the problem into local subproblems
- Empirically demonstrated to solve complex tasks involving multiple coupled state spaces, such as hydration regulation and task sequencing, without relying on reward maximization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Optimizing for feasibility rather than cumulative reward allows for interpretable, verifiable planning in high-dimensional spaces.
- **Mechanism:** The framework replaces standard reward-based Bellman equations with OKBEs that optimize Cumulative Feasibility Function (κ) and State-Time Option Kernel (STOK, η). The optimization maximizes probability of entering goal-success absorbing states while avoiding constraint-violation absorbing states.
- **Core assumption:** Problems can be modeled as TMDPs where goals and constraints are separable functions (f_g, f_c) over state-action space, and transition dynamics are known or learned.
- **Evidence anchors:** Abstract states OKBEs "directly construct and optimize a predictive map called a state-time option kernel (STOK) to maximize the probability of completing a goal while avoiding constraint violations."

### Mechanism 2
- **Claim:** High-dimensional planning scalability is achieved by factorizing full product-space STOK into independent base-level and high-level prediction kernels.
- **Mechanism:** STOK Factorization Theorem (Thm 2.1) decomposes joint transition probability into product of Base-Level STOK (η_π), High-Level State Prediction Kernels (ρ_z), and Temporal Event Function (ξ).
- **Core assumption:** System dynamics are modular and coupled via affordance function F, with homogeneous high-level dynamics within specific regions that are conditionally independent of base-level dynamics given time-to-event.
- **Evidence anchors:** Abstract notes "STOK factorization theorem enables efficient planning in high-dimensional spaces by decomposing the problem into local subproblems."

### Mechanism 3
- **Claim:** Compositionality and verifiability are preserved by using STOKs as modular building blocks that sum to 1 and record event probabilities.
- **Mechanism:** Unlike value functions, STOKs (η) are proper transition kernels that record probability distribution over terminal states and times. They are composed using Chapman-Kolmogorov equations (convolutions), allowing complex plans to be built by "stitching" options.
- **Core assumption:** Policies can be represented as options (policy + termination condition) where termination events are strictly defined by goal/constraint functions.
- **Evidence anchors:** Abstract states "STOKs can be composed using Chapman-Kolmogorov equations... [and] record the probabilities of semantically interpretable goal-success and constraint-violation events."

## Foundational Learning

- **Concept:** Options Framework (Hierarchical RL)
  - **Why needed here:** Entire OKBE framework is built upon "options" (temporally extended actions defined by policy π and termination condition β). Understanding underlying Options framework is necessary to grasp STOKs.
  - **Quick check question:** Can you explain the difference between a primitive action and an "option" in terms of time extension?

- **Concept:** Markov Decision Processes (MDPs) & Bellman Equations
  - **Why needed here:** Authors explicitly reformulate standard Bellman equation (which sums rewards) into feasibility equation. Understanding standard Value Iteration is necessary to grasp how "Feasibility Iteration" modifies this process.
  - **Quick check question:** In a standard MDP, what quantity does the Bellman equation recursively update?

- **Concept:** Chapman-Kolmogorov Equations
  - **Why needed here:** Mathematical rule allowing composition of transition probabilities over time steps. Paper relies on this to allow STOKs to "stitch" together.
  - **Quick check question:** How do you calculate probability of a two-step transition X→Z if you know probabilities for X→Y and Y→Z?

## Architecture Onboarding

- **Component map:** TMDP Definition -> OKBE Solver (Feasibility Iteration) -> STOK Composition -> Forward Planner
- **Critical path:** Definition of Affords Function (F) and Goal/Constraint Functions (f_g, f_c). If these are not correctly separable and defined, STOK factorization cannot be applied.
- **Design tradeoffs:**
  - Interpretability vs. Optimality: Method sacrifices potential for scalar optimization for verifiable goal satisfaction
  - Memory vs. Compute: Requires storing STOKs for all options, trading memory against computational expense of full product space planning
  - Completeness: Tree search is complete for deterministic cases but relies on pruning for stochastic high-dimensional spaces
- **Failure signatures:**
  - Infeasibility Loops: If feasibility iteration converges to κ(x)=0 for all states, goal is unreachable
  - Factorization Breakdown: If high-level dynamics depend on specific path through base-level space, joint prediction diverges
  - Tree Search Explosion: In stochastic environments with many options, branching factor may explode
- **First 3 experiments:**
  1. Gridworld Feasibility: Implement 2D gridworld TMDP with fire constraint and goal state. Run Feasibility Iteration and visualize κ map to verify it captures probability of reaching goal without hitting fire.
  2. STOK Composition: Define two simple options ("Go North", "Go East"). Compute individual STOKs. Use Chapman-Kolmogorov equation to compose them into "Go North-East" and verify resulting state distribution.
  3. High-Dimensional Decomposition: Create coupled system (Navigation + Hydration). Implement STOK Factorization to separate navigation planning from hydration prediction. Verify joint prediction matches brute-force simulation but runs faster.

## Open Questions the Paper Calls Out

- **Question:** How can OKBEs be modified to produce risk-calibrated policies for multi-goal problems under uncertainty?
  - **Basis in paper:** Authors state "Creating a risk-calibrated set of STOKs and options for multi-goal problems under uncertainty is an important theoretical problem we are pursuing."
  - **Why unresolved:** Current formulation is risk-sensitive; probability mass of constraint-violating events directly reduces cumulative feasibility function, incentivizing overly cautious policies.
  - **What evidence would resolve it:** Theoretical extension introducing risk parameter to trade off caution for goal-achievement probability, validated in stochastic environments.

- **Question:** What pruning criteria or option-set restrictions are necessary to make tree search efficient for dynamic, high-dimensional state-spaces?
  - **Basis in paper:** Conclusion notes "We believe improvements can be found which makes tree-search more efficient, whether it be by restricting the option/STOK set, or by using improved pruning criteria."
  - **Why unresolved:** While Affordance Option Set theorem reduces complexity for static high-level kernels, full state-action option set leads to prohibitive branching factor in dynamic environments.
  - **What evidence would resolve it:** Derivation of specific pruning heuristic or restricted option generation method maintaining optimality guarantees while reducing tree depth.

- **Question:** How can OKBE framework be extended to compute closed-loop meta-policies for stochastic problems using tree search?
  - **Basis in paper:** Authors acknowledge "We did not provide ways of computing closed-loop meta-policies over options to find optimal solutions to stochastic problems with tree-search, this remains an important problem for future work."
  - **Why unresolved:** Current method optimizes open-loop policies via breadth-first search. In stochastic settings, open-loop sequence is often suboptimal because it cannot react to unexpected state transitions.
  - **What evidence would resolve it:** Algorithm constructing contingency plan or policy tree during search process, allowing agent to select different options based on observed outcomes.

## Limitations

- The paper introduces novel theoretical framework with strong claims about compositionality and scalability, but practical implementation details are underspecified
- STOK Factorization Theorem relies on abstract "homogeneity" and "region" conditions that may not hold in complex real-world environments
- Framework is presented as "reward-free" but secondary objective in Policy Optimization OKBE introduces time-minimization component that could be interpreted as implicit reward shaping

## Confidence

- **High Confidence:** Core mathematical framework (TMDP definition, OKBE equations, STOK composition via Chapman-Kolmogorov) is internally consistent and builds logically on established MDP theory
- **Medium Confidence:** Feasibility iteration algorithm will converge to valid solution for well-defined TMDPs with absorbing goals/constraints, but practical convergence rates and numerical stability require further empirical validation
- **Low Confidence:** Scalability claims for high-dimensional planning via STOK factorization are theoretically justified but rely on strong assumptions about structure of state space that may not hold in complex environments

## Next Checks

1. **Convergence Diagnostics:** Implement Algorithm 1 on simple TMDP and systematically test different convergence criteria (fixed iterations vs. tolerance thresholds) to identify stable and efficient stopping conditions

2. **Factorization Stress Test:** Construct TMDP where high-level dynamics are only approximately homogeneous within regions. Quantify error in joint prediction η as approximation conditions are violated

3. **Memory-Compute Tradeoff Analysis:** Benchmark memory usage and planning speed of full OKBE approach against standard RL methods (e.g., PPO with constraints) on fixed set of gridworld tasks, measuring both solution quality and resource consumption