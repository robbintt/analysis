---
ver: rpa2
title: What Do AI-Generated Images Want?
arxiv_id: '2510.20350'
source_url: https://arxiv.org/abs/2510.20350
tags:
- mitchell
- images
- image
- pictures
- want
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper reframes W.J.T. Mitchell\u2019s question \u201CWhat\
  \ do pictures want?\u201D for AI-generated images, arguing that these images fundamentally\
  \ seek specificity and concreteness despite being inherently abstract."
---

# What Do AI-Generated Images Want?

## Quick Facts
- arXiv ID: 2510.20350
- Source URL: https://arxiv.org/abs/2510.20350
- Reference count: 0
- One-line primary result: AI-generated images fundamentally seek specificity and concreteness despite being inherently abstract, with their hidden "desire" being for the water resources consumed in their production.

## Executive Summary
This paper reframes W.J.T. Mitchell's question "What do pictures want?" for AI-generated images, arguing that these images fundamentally seek specificity and concreteness despite being inherently abstract. Unlike Mitchell's pictures, AI-generated images lack objecthood—the material form that gives pictures agency—yet they appear concrete through singular outputs. Drawing from debates on abstraction in art history and Peter Galison's analysis of scientific imagery, the author shows that AI images are mathematical abstractions longing for materiality, but their collective statistical origins prevent true objecthood. Examples from Midjourney illustrate how hybrid imagery emerges from competing data associations. Ultimately, the paper concludes that AI images "want our water," highlighting the environmental cost of their production, which remains hidden behind abstraction. Their desires are not anthropomorphic but reflect the systemic resource consumption that underpins their existence.

## Method Summary
The paper employs a critical/theoretical methodology drawing on Mitchell's "holy trinity" framework (image, object, discourse) and Galison's analysis of scientific imagery to examine what AI-generated images "want." The analysis is based on qualitative interpretation of two Midjourney-generated images with specific prompts: (1) "a school classroom with a hotel bed in the middle --ar 16:9 --style raw" (v.6.1) and (2) "Trick-or-treaters wearing Christmas costumes --ar 16:9 --raw" (v.7.0). The author demonstrates hybrid imagery emerging from competing data associations through these examples, arguing that AI images are mathematical abstractions that appear concrete through projection from high-dimensional space into singular pixel outputs.

## Key Results
- AI-generated images achieve apparent concreteness through projection from high-dimensional abstract space into singular pixel output
- Singular outputs remain contaminated by collective statistical associations from training data, producing hybrid artifacts
- The text-to-image pipeline obscures representational regress, creating an illusion of direct translation
- AI images fundamentally lack objecthood—the material form that gives pictures agency—despite appearing concrete
- Their hidden "desire" is for the water resources consumed in their production, which remains hidden behind abstraction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AI-generated images achieve apparent concreteness through projection from high-dimensional abstract space into singular pixel output
- Mechanism: Multimodal models like CLIP encode text and image into a shared latent space (512-dimensional for CLIP). User prompts traverse this space, sampling a coordinate that gets decoded into a discrete image. The abstraction is mathematically real; the concreteness is a projection artifact
- Core assumption: The model's latent space meaningfully encodes semantic relationships between text and visual concepts
- Evidence anchors: CLIP has a 512-dimensional embedding space and requires projection to visualize; multimodal text-to-image models are based on the premise that text and image are interchangeable tokens with commensurability
- Break condition: If latent space encoding fails to capture semantic structure, outputs may appear incoherent rather than concrete

### Mechanism 2
- Claim: Singular outputs remain contaminated by collective statistical associations from training data, producing hybrid artifacts
- Mechanism: Diffusion/transformer models sample from learned distributions where prompts activate overlapping concept clusters. Statistically dominant associations "bleed" across intended boundaries, producing hybrids (e.g., a "hotel bed" in a "school classroom" rendered with childish colors because school→childhood associations dominate hotel→neutral-adult associations)
- Core assumption: Training data contains asymmetrically weighted concept associations that persist through inference
- Evidence anchors: The bed depicted in the school classroom appears as a child's bed rather than neutrally decorated beds found in most hotel rooms; the abstraction of child-like associations in a school classroom effectively bleeds into the image
- Break condition: If concept clusters are well-separated in latent space or prompts use disambiguating context, bleed may reduce

### Mechanism 3
- Claim: The text-to-image pipeline obscures representational regress, creating an illusion of direct translation
- Mechanism: Text is tokenized → embedded → correlated with visual tokens → sampled → decoded → pixels. The user sees prompt→image but not the layered mathematical transformations. This creates a "magical" appearance that encourages anthropomorphic attribution
- Core assumption: Users lack visibility into intermediate representations and process steps
- Evidence anchors: The user pipeline that sees textual input become visual output obscures this representational regress and makes it seem like one form transforms into the other—as if by magic; the power of AI-generated images is mixed up in both the magical power of images and its concrete relationship to text mathematically represented by digital tools
- Break condition: If pipeline intermediates are exposed (e.g., attention maps, latent visualizations), the illusion of magic may weaken

## Foundational Learning

- Concept: **Latent Space and Embeddings**
  - Why needed here: AI images are defined as "mathematical abstractions" existing in high-dimensional space. Understanding what embeddings are and how distance in that space corresponds to semantic similarity is prerequisite to grasping why generated images behave as they do
  - Quick check question: Can you explain why two different prompts might map to nearby points in a 512-dimensional embedding space, and what "nearby" means mathematically?

- Concept: **CLIP (Contrastive Language-Image Pre-training)**
  - Why needed here: The paper identifies CLIP as the underlying architecture enabling text-to-image generation by placing text and image in a single model space. Without understanding multimodal contrastive learning, the "commensurability" claim is opaque
  - Quick check question: What objective function does CLIP optimize, and how does it create a shared representation for text and images?

- Concept: **Mitchell's "Picture" Trinity (Image, Object, Discourse)**
  - Why needed here: The paper's central theoretical move is to distinguish AI "images" from Mitchell's "pictures" based on the absence of objecthood. This conceptual framework underpins the entire argument
  - Quick check question: What are the three components of Mitchell's picture, and which one does the paper argue AI-generated images lack?

## Architecture Onboarding

- Component map: Tokenizer → Text encoder (e.g., CLIP text tower) → Shared embedding space → Generation backbone (Diffusion/Transformer) → Output decoder → Pixel synthesis
- Critical path: 1. Prompt tokenization and text encoding; 2. Latent space traversal guided by prompt embedding; 3. Sampling from learned distribution (with statistical bleed risk); 4. Decoding to pixel output; 5. Physical resource consumption at inference time (water, electricity)
- Design tradeoffs:
  - Dimensionality vs. interpretability: Higher-dimensional latent spaces capture more nuance but are harder to visualize or audit for bias/artifacts
  - Prompt specificity vs. statistical dominance: Detailed prompts may reduce bleed but cannot fully escape training data weighting
  - Output quality vs. resource cost: Higher-fidelity generation (more steps, larger models) increases water/energy consumption
- Failure signatures:
  - Hybrid artifacts: Concepts bleeding across prompt boundaries (e.g., holiday-themed Halloween costumes)
  - Mode dominance: Over-represented concepts in training data overwhelming under-specified prompts
  - Incoherence: When latent space sampling lands in poorly populated regions
  - Resource opacity: No traceable link between a specific output and its physical resource cost
- First 3 experiments:
  1. **Bleed quantification**: Generate image sets with deliberately incongruous prompt pairings (e.g., "bank vault with a birthday cake"). Score outputs for unintended concept contamination. Map contamination rates to concept frequency in training data
  2. **Latent space probing**: Visualize CLIP embeddings for controlled prompt variations. Test whether semantically similar prompts cluster as expected, and whether problematic associations (stereotypes, biases) form tight clusters
  3. **Resource tracing pilot**: Instrument an inference endpoint to log energy/water consumption per request. Correlate consumption with generation parameters (steps, resolution, model size) to establish baseline resource-per-image estimates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the mathematical commensurability of text and image tokens in multimodal models fundamentally alter the semiotic relationship between representation and meaning compared to traditional mixed media?
- Basis in paper: The author asks, "What does it mean that AI-generated images are produced by a model that does not semantically separate the data forms representing text and image?"
- Why unresolved: The paper identifies the "representational regress" and mathematical enmeshment but leaves the specific semiotic consequences of this lack of separation for meaning-making largely unexplored
- What evidence would resolve it: A comparative semiotic analysis of "imagetexts" generated by multimodal models versus human-created mixed media to map differences in semantic density and reference

### Open Question 2
- Question: Can AI-generated images ever achieve the "specificity" and "concreteness" the author claims they desire, or is the "statistical bleed" of collective associations an insurmountable barrier?
- Basis in paper: The paper argues images "want specificity" but are prevented from obtaining true objecthood by the "pollution" of collective associations (illustrated by the hybrid Midjourney examples)
- Why unresolved: The paper demonstrates the failure to achieve singularity in specific examples but does not determine if this failure is absolute or conditional based on prompt complexity or model architecture
- What evidence would resolve it: Technical experiments measuring the "bleed" of unwanted associations in controlled generation tasks to see if "pure" object representation is mathematically possible within current latent spaces

### Open Question 3
- Question: How can the specific environmental resource consumption ("desire") of an individual AI image be accurately traced given the abstraction of cloud infrastructure?
- Basis in paper: The author notes that even the water usage is an abstraction: "One cannot point to the actual water used in the making of a single picture... The complexity... make[s] it so that even the materiality... is abstract"
- Why unresolved: The systemic nature of data centers hides the granular cost of specific outputs, making the "desire" of the image for resources empirically vague
- What evidence would resolve it: Development of granular auditing tools that link specific generation events to real-time resource usage logs, creating a "material provenance" for digital files

## Limitations
- The analysis is fundamentally theoretical and speculative, relying on qualitative interpretation rather than empirical measurement
- The "wants" attributed to AI images are metaphorical projections rather than verifiable properties
- The paper's argument depends heavily on subjective interpretation of generated images and their associations, which may vary across viewers and model versions
- No quantitative evidence is provided for the claimed statistical bleed effects or resource consumption patterns

## Confidence
- **Medium confidence**: The core theoretical framework (Mitchell's picture trinity, Galison's abstraction analysis) is well-established in art history and philosophy. The mathematical basis for latent space embeddings and CLIP architecture is well-documented
- **Low confidence**: Claims about specific AI image "desires" (specificity, concreteness, water) are interpretive and lack empirical validation. The hybrid bleed examples are anecdotal rather than systematically measured
- **Medium confidence**: The resource consumption argument is supported by broader research on AI infrastructure costs, though no specific water/energy measurements are provided for the images discussed

## Next Checks
1. **Hybrid bleed quantification**: Systematically generate and analyze 100+ image pairs with deliberately incongruous prompt combinations (e.g., "corporate boardroom with a children's birthday party"). Use automated image classification to measure concept contamination rates and correlate with training data frequency statistics
2. **Latent space association mapping**: Extract CLIP embeddings for controlled prompt variations across concept pairs (e.g., "school," "hotel," "childhood," "adult"). Visualize embedding neighborhoods and measure cosine similarity to test whether claimed bleed patterns reflect actual statistical clustering in the model
3. **Resource consumption tracing**: Instrument an inference endpoint to log energy and water usage per generation request. Generate standardized image sets across different model sizes and quality settings, then correlate resource consumption with output parameters to establish baseline environmental impact metrics