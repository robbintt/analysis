---
ver: rpa2
title: Predicting Language Models' Success at Zero-Shot Probabilistic Prediction
arxiv_id: '2509.15356'
source_url: https://arxiv.org/abs/2509.15356
tags:
- llms
- tasks
- scores
- performance
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors conducted a large-scale empirical study of large language
  models' zero-shot performance on 316 tabular prediction tasks across 31 datasets.
  They investigated whether task-level metrics derived from unlabeled data could predict
  LLM predictive performance.
---

# Predicting Language Models' Success at Zero-Shot Probabilistic Prediction

## Quick Facts
- **arXiv ID**: 2509.15356
- **Source URL**: https://arxiv.org/abs/2509.15356
- **Reference count**: 40
- **Primary result**: Risk score variance on unlabeled data strongly predicts LLM zero-shot performance (R² up to 0.605 for GPT-4o)

## Executive Summary
This paper presents a large-scale empirical study examining whether task-level metrics derived from unlabeled data can predict large language models' zero-shot performance on 316 tabular prediction tasks across 31 datasets. The study finds that individual-level predictions are poorly calibrated but informative for abstention, with only 19% of performance variance explained at the dataset level. The strongest predictor of task-level performance is the standard deviation of predicted risk scores, with higher variance correlating with better AUC. Additional information from the full distribution of risk scores provides marginal gains. The findings suggest using risk score distribution analysis as a screening tool to prioritize promising tasks for further evaluation.

## Method Summary
The authors conducted a large-scale empirical study using 31 tabular datasets, creating 316 total tasks through auxiliary masking experiments. They serialized tabular rows into text prompts using "Feature: Value" pairs followed by binary multiple-choice questions, then extracted risk scores from LLM token probabilities. Task-level performance was evaluated using AUC, while calibration was assessed through ECE and accuracy-confidence correlations. The study computed 8 task-level proxies including standard deviation and verbalized confidence of risk scores, and tested XGBoost models using 201 percentile features from the full risk score distribution to predict AUC.

## Key Results
- Standard deviation of predicted risk scores is the strongest predictor of task-level performance (R² up to 0.605 for GPT-4o)
- Only 19% of performance variance is explained at the dataset level
- Individual-level predictions are poorly calibrated but useful for abstention when base task performance is strong
- Full distribution of risk scores provides marginal gains over simple variance metrics (R² increases of 0.01-0.15)
- Masking experiments showed very weak correlations (R²≈0.03-0.10) between auxiliary and target task performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Standard deviation of predicted risk scores on unlabeled data strongly predicts downstream zero-shot AUC
- Mechanism: Heterogeneous predictions across instances signal meaningful class separation capability; low variance indicates model inability to discriminate
- Core assumption: Variance reflects genuine discriminative signal rather than noise
- Evidence anchors: R² = 0.605 for GPT-4o; R² = 0.270 for Llama-3.1-8b-Instruct; variance-AUC correlation is highest among evaluated metrics

### Mechanism 2
- Claim: LLM calibration errors manifest as systematic over/under-prediction of entire distributions
- Mechanism: LLMs correctly identify feature-label correlations but misjudge marginal probability scale, producing calibration curves entirely above or below identity line
- Core assumption: Miscalibration is consistent within a task
- Evidence anchors: Calibration curves often remain entirely above/below identity line; predictions consistently too high or too low

### Mechanism 3
- Claim: When LLMs perform well on base task, confidence scores become more informative for abstention
- Mechanism: Accuracy-confidence correlation strengthens at higher base AUCs; extreme predictions more likely correct when model has genuine task competence
- Core assumption: Internal confidence is grounded in task understanding
- Evidence anchors: AUC of original prediction task highly correlated with AUC of failure prediction (R² = 0.720 for GPT-4o-mini)

## Foundational Learning

- Concept: **AUC (Area Under ROC Curve)**
  - Why needed here: Primary metric for evaluating binary classification quality independent of threshold; central to all task-level predictions
  - Quick check question: If a model outputs risk scores [0.2, 0.4, 0.6, 0.8] for positives and [0.1, 0.3, 0.3, 0.5] for negatives, is AUC = 1.0?

- Concept: **Expected Calibration Error (ECE)**
  - Why needed here: Quantifies how well predicted probabilities match empirical frequencies; reveals systematic over/under-prediction
  - Quick check question: A model predicts 0.7 probability for 100 examples, and 80 are positive. What's the calibration error for this bin?

- Concept: **Intraclass Correlation Coefficient (ICC)**
  - Why needed here: Quantifies how much variance in performance is explained at dataset vs. task level (19% dataset-level in this study)
  - Quick check question: If ICC = 0.19, does knowing which dataset a task comes from tell you most of what you need to predict LLM performance?

## Architecture Onboarding

- Component map: Serialize tabular data -> prompt LLM -> extract risk scores -> compute std(risk_scores) -> threshold screening -> (optional) XGBoost on distribution percentiles -> predict AUC
- Critical path: 1) Serialize tabular data to prompts, 2) Query LLM and extract risk scores, 3) Compute std(risk_scores) on unlabeled data, 4) Screen tasks by minimum variance threshold, 5) (Optional) Train XGBoost on distribution percentiles
- Design tradeoffs: Larger models show stronger variance-AUC correlation but cost more; threshold-based screening is simpler but discards information; full distribution analysis adds marginal predictive power
- Failure signatures: Low variance + high claimed confidence indicates model collapse; elicited confidence disagreeing with variance metrics suggests poor calibration; same dataset with wildly different AUCs indicates don't generalize within dataset
- First 3 experiments: 1) Plot std(risk_scores) vs. actual AUC to establish model-specific thresholds, 2) Measure mean AUC of tasks above varying std thresholds to find optimal screening cutoff, 3) Test whether filtering low-confidence predictions improves accuracy, stratified by base task AUC

## Open Questions the Paper Calls Out

- Question: Do task-level performance predictors generalize to domains completely unseen during LLM pre-training, or are they artifacts of memorization in public datasets?
- Basis in paper: The authors state they "can't rule out that predictors of task-level performance might be different in domains that are completely unseen during LLM training" due to potential data contamination
- Why unresolved: Study relied on publicly available datasets which likely appear in training corpora; unclear if correlation holds for proprietary or genuinely novel data
- What evidence would resolve it: Replication using private, non-public datasets or data generated after LLMs' training cutoff dates

- Question: Does the utility of risk score distributions for predicting performance persist when using chain-of-thought or few-shot prompting strategies?
- Basis in paper: The Limitations section notes that the prompting approach "do[es] not explore alternative prompts, few-shot settings, or chain-of-thought reasoning"
- Why unresolved: Complex prompting strategies might alter risk score distribution or model's calibration, potentially strengthening or weakening correlation
- What evidence would resolve it: Experiments comparing zero-shot template performance against few-shot and CoT performance using proposed metrics

- Question: What specific distributional features beyond standard deviation explain remaining variance in LLM performance across tasks?
- Basis in paper: While XGBoost models using full distribution percentiles outperform simple variance metrics, different models favor different distribution shapes
- Why unresolved: Paper establishes "full distribution captures additional information" but doesn't isolate which higher-order features drive gains for specific architectures
- What evidence would resolve it: Ablation study analyzing feature importances of specific distribution percentiles or shapes in regression models

## Limitations

- The study's core findings rely on publicly available datasets that may appear in LLM training corpora, making it unclear if results generalize to truly novel domains
- The masking experiments showed very weak correlations (R²≈0.03-0.10) between auxiliary and target task performance, suggesting limited multitask transfer
- The practical utility of the full distribution-based XGBoost model is questionable given only marginal gains over simple variance metrics and added implementation complexity

## Confidence

- **High confidence**: std(risk_scores) correlates with AUC across all tested models (R²=0.605 for GPT-4o, 0.270 for Llama-3.1-8b-Instruct)
- **Medium confidence**: Variance analysis is useful for screening tasks, though practical utility depends on threshold selection
- **Medium confidence**: LLM calibration is systematic rather than inverted-sigmoid pattern, but underlying mechanism remains unexplained
- **Low confidence**: Practical utility of full distribution-based XGBoost model due to marginal gains and complexity

## Next Checks

1. Test whether risk score variance remains a strong predictor for non-tabular tasks (text classification, image captioning, code generation) to verify mechanism isn't dataset-specific

2. Design experiments to distinguish variance driven by genuine discriminative ability versus prompt artifacts by varying prompt templates systematically

3. Conduct systematic study of tradeoff between screening threshold and downstream AUC, including cost analysis of LLM queries versus performance gains