---
ver: rpa2
title: Mitigating Biases in Language Models via Bias Unlearning
arxiv_id: '2509.25673'
source_url: https://arxiv.org/abs/2509.25673
tags:
- language
- bias
- data
- training
- debiasing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces BiasUnlearn, a framework for mitigating
  social bias in language models through targeted unlearning. The approach combines
  dual-pathway mechanisms: stereotype forgetting via negative preference optimization
  and anti-stereotype retention through gradient descent on counterfactual data.'
---

# Mitigating Biases in Language Models via Bias Unlearning

## Quick Facts
- arXiv ID: 2509.25673
- Source URL: https://arxiv.org/abs/2509.25673
- Reference count: 40
- Primary result: Dual-pathway unlearning framework that reduces stereotyping scores to near-optimal levels while preserving language modeling capabilities through targeted stereotype forgetting and anti-stereotype retention

## Executive Summary
This paper introduces BiasUnlearn, a framework for mitigating social bias in language models through targeted unlearning. The approach combines dual-pathway mechanisms: stereotype forgetting via negative preference optimization and anti-stereotype retention through gradient descent on counterfactual data. It also employs adversarial forget sets and dynamic dataset swapping to prevent bias polarity reversal. Experimental results show BiasUnlearn significantly reduces stereotyping scores across multiple model sizes while maintaining language modeling capabilities. Notably, debiasing weights learned on base models effectively transfer to instruction-fine-tuned variants, indicating bias representations become entrenched during pre-training.

## Method Summary
BiasUnlearn employs a dual-pathway unlearning mechanism that simultaneously reduces stereotypical associations while reinforcing anti-stereotypical representations. The framework uses Negative Preference Optimization (NPO) on stereotypical data to forget biased patterns and cross-entropy loss on counterfactual data to retain balanced associations. An adversarial forget set containing sampled anti-stereotypical data introduces gradient interference that prevents over-debiasing. Dynamic dataset swapping is triggered when any bias type's stereotype score drops below 50, swapping forget and retain sets for that bias. The method uses LoRA fine-tuning with combined loss (0.4×L_forget + 0.4×L_retain + 0.2×L_KL) and early stopping when all bias types' scores are within 2 points of 50.

## Key Results
- Achieves stereotype scores near 50 (optimal fairness) across gender, profession, race, and religion bias types
- Maintains language modeling capabilities with minimal performance degradation (LMS changes <2 points)
- Demonstrates effective weight transfer from base models to instruction-fine-tuned variants
- Outperforms baseline debiasing methods in both bias reduction and capability preservation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining stereotype forgetting with anti-stereotype retention preserves language modeling capabilities while reducing bias.
- Mechanism: Dual-pathway training applies Negative Preference Optimization (NPO) on stereotypical data to reduce biased associations while simultaneously training with cross-entropy loss on anti-stereotypical counterfactual data to reinforce balanced representations.
- Core assumption: Stereotype and anti-stereotype patterns can be independently manipulated without catastrophic interference to general knowledge.
- Evidence anchors:
  - [abstract] "dual-pathway unlearning mechanisms coordinating stereotype forgetting with anti-stereotype retention"
  - [Section 4.3] "BiasUnlearn achieved the minimal change on LMS (The minimum change of BiasUnlearn is only 0.2 and the maximum is only 1.42)"
  - [corpus] Weak direct validation; related work BiasEdit also reports capability preservation but uses different approach
- Break condition: If retention loss is removed, LMS drops by 35.23 points, causing model collapse (Table 5).

### Mechanism 2
- Claim: Adversarial forget set construction prevents over-debiasing by introducing gradient interference.
- Mechanism: Including a subset of anti-stereotypical data in the forget set forces the model to optimize contradictory objectives simultaneously, regularizing parameter updates and preventing overfitting to a single objective.
- Core assumption: Gradient interference from contradictory data provides beneficial regularization rather than destabilizing training.
- Evidence anchors:
  - [Section 3] "forces the model to optimize the same set of parameters to accommodate contradictory objectives"
  - [Section 4.4] "Without KL divergence and Adversarial Forget Set, the LMS exhibits respective reductions of 4.73 and 6.1 points"
  - [corpus] No direct corpus evidence for this specific mechanism
- Break condition: Without adversarial forget set, SS score drops to 44.72 indicating over-debiasing toward anti-stereotypes (Table 5).

### Mechanism 3
- Claim: Bias representations become entrenched during pre-training and persist through fine-tuning, enabling weight transfer across model variants.
- Mechanism: Debiasing weights learned on base models can be transferred to instruction-fine-tuned variants because bias-related parameters are learned during pre-training and not substantially modified during instruction tuning.
- Core assumption: Bias representations are localized in specific parameters that remain stable across fine-tuning stages.
- Evidence anchors:
  - [abstract] "debiasing weights learned on base models effectively transfer to instruction-fine-tuned variants"
  - [Section 4.3] "both instruction-fine-tuned models of Llama and Mistral achieve strong debiasing effects after loading the base model's debiasing weights"
  - [corpus] KnowBias paper (arxiv 2601.21864) suggests bias mitigation at neuron level, providing indirect support for localized bias representations
- Break condition: Transfer effectiveness may degrade if fine-tuning substantially restructures underlying representations rather than adding task-specific adapters.

## Foundational Learning

- Concept: Negative Preference Optimization (NPO)
  - Why needed here: Core mechanism for stable unlearning; more stable than gradient ascent with slower progression toward catastrophic collapse.
  - Quick check question: Can you explain why NPO's sigmoid-based formulation prevents the runaway loss escalation seen in gradient ascent?

- Concept: Counterfactual Data Augmentation
  - Why needed here: Generates anti-stereotypical training pairs by substituting sensitive words, providing the retention signal.
  - Quick check question: How would you construct a counterfactual for "The male nurse..." that maintains semantic coherence while reversing the stereotype?

- Concept: Stereotype Score (SS) Calibration
  - Why needed here: Target metric where SS=50 indicates optimal fairness; guides dynamic dataset swapping decisions.
  - Quick check question: Why is SS approaching but not falling below 50 the target, rather than minimizing SS entirely?

## Architecture Onboarding

- Component map: StereoSet data → forget set (stereotypical + adversarial subset) + retain set (anti-stereotypical) → Loss Computation (L_Forget + L_Retention + L_KL) → Training Loop with LoRA → Monitoring (SS scores) → Dynamic swapping trigger

- Critical path:
  1. Prepare counterfactual pairs by substituting sensitive words in stereotypical contexts
  2. Construct adversarial forget set by sampling subset of anti-stereotypical data into forget set
  3. Train with combined loss, monitoring SS scores per bias type
  4. Swap forget/retain sets dynamically for bias types showing SS<50

- Design tradeoffs:
  - Higher forget:retain ratio → faster debiasing but higher collapse risk
  - Larger adversarial subset → more stable but slower convergence
  - KL weight → stronger capability preservation but potentially weaker debiasing
  - LoRA rank → parameter efficiency vs. representation capacity

- Failure signatures:
  - LMS dropping >5 points: retention loss insufficient or learning rate too high
  - SS oscillating around 50 without convergence: swapping threshold too sensitive
  - Forgetting loss growing rapidly (Figure 3 pattern): retention loss not being applied correctly
  - Bias reversal (SS<45): adversarial set too small or training too long without swapping

- First 3 experiments:
  1. Reproduce GPT2-medium baseline from Table 1 to validate pipeline: target SS~62.74 → ~50, ΔLMS <1.0
  2. Ablate adversarial forget set: expect SS drop to ~44-47 and LMS drop ~6 points per Table 5
  3. Test weight transfer from Mistral-7B base to Mistral-7B-Instruct: compare direct training vs. transferred weights on BBQ benchmark

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of pre-training objective (e.g., causal language modeling vs. masked language modeling) affect the entrenchment of bias representations and the efficacy of unlearning mechanisms?
- Basis in paper: [explicit] The Conclusion states the authors will "investigate the relationship between bias representations and different pre-training objectives."
- Why unresolved: The current study evaluates models with established architectures (GPT-2, Llama, Mistral) but does not isolate the pre-training objective as an independent variable to determine if certain objectives facilitate deeper bias entrenchment than others.
- What evidence would resolve it: Comparative experiments applying BiasUnlearn to models of similar size trained with different objectives, analyzing the difficulty of unlearning and the geometry of bias representations.

### Open Question 2
- Question: Can the BiasUnlearn framework be effectively adapted to mitigate non-social forms of bias, such as cognitive biases or logical fallacies, without compromising model utility?
- Basis in paper: [explicit] The Conclusion proposes to "extend BiasUnlearn to other forms of bias beyond social biases."
- Why unresolved: The current methodology relies on a dual-pathway mechanism (stereotype forgetting vs. anti-stereotype retention) specifically designed for social group associations; it is untested whether this structure maps effectively to individual cognitive biases which may not have clear "anti-stereotype" counterfactuals.
- What evidence would resolve it: Successful application of the framework to cognitive bias benchmarks, demonstrating reduction in logical errors while maintaining performance on standard reasoning tasks.

### Open Question 3
- Question: To what extent does optimizing the BiasUnlearn objective for a specific bias type (e.g., religion) inadvertently alter parameters associated with statistically correlated biases (e.g., race) due to shared underlying features?
- Basis in paper: [explicit] The Limitations section notes the method "does not fully disentangle the representation space of different biases" and that "parameters associated with other related biases are adjusted jointly."
- Why unresolved: While the paper observes that joint adjustment occurs, it does not quantify the trade-offs or determine if mitigating one bias destabilizes the fairness of another in the representation space.
- What evidence would resolve it: An analysis using probing classifiers to measure the independence of bias subspaces in the representation geometry before and after targeted unlearning.

## Limitations
- The method does not fully disentangle the representation space of different biases, causing parameters associated with other related biases to be adjusted jointly.
- Critical hyperparameters (β parameter in NPO loss, LoRA rank, adversarial forget set ratio) are not specified, making exact reproduction challenging.
- Effectiveness is primarily demonstrated on StereoSet with limited evaluation on other bias detection benchmarks, raising questions about generalization across different bias types and evaluation methodologies.

## Confidence
- High confidence: The dual-pathway architecture combining stereotype forgetting with anti-stereotype retention is well-grounded and the empirical results showing capability preservation (LMS changes <2 points) are convincing.
- Medium confidence: The weight transfer effectiveness from base to instruction-tuned models is demonstrated but the underlying assumption that bias representations remain localized and stable across fine-tuning stages requires further validation.
- Low confidence: The specific hyperparameter configurations that lead to optimal debiasing without model collapse are not fully specified, making it difficult to assess whether reported results are reproducible or depend on carefully tuned settings not disclosed in the paper.

## Next Checks
1. **Hyperparameter sensitivity analysis**: Systematically vary the β parameter in NPO loss, LoRA rank, and adversarial forget set ratio across multiple runs to determine the stability and robustness of debiasing performance. This would clarify whether the method's success depends on specific hyperparameter configurations or exhibits consistent behavior across a range of settings.

2. **Cross-dataset generalization**: Evaluate BiasUnlearn on multiple bias detection datasets (e.g., CrowS-Pairs, WinoBias, RealToxicityPrompts) beyond StereoSet to assess whether the approach generalizes across different bias types and evaluation methodologies. This would address concerns about overfitting to a single benchmark.

3. **Transferability stress test**: Conduct extensive experiments transferring debiasing weights across different model families, sizes, and fine-tuning approaches (including adapter-based tuning, prefix tuning, and full fine-tuning) to validate the claim that bias representations are consistently localized and transferable. This would include testing on models with substantially different architectures from the training base.