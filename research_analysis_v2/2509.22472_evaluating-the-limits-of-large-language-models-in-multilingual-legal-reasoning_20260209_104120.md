---
ver: rpa2
title: Evaluating the Limits of Large Language Models in Multilingual Legal Reasoning
arxiv_id: '2509.22472'
source_url: https://arxiv.org/abs/2509.22472
tags:
- uni00000013
- uni00000048
- uni00000011
- uni0000004c
- uni00000044
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates the performance of Large Language Models\
  \ (LLMs) like Meta\u2019s LLaMA and Google\u2019s Gemini on multilingual legal reasoning\
  \ tasks, highlighting their strengths and weaknesses in complex, high-stakes contexts.\
  \ The authors use an LLM-as-a-Judge framework to assess outputs across tasks such\
  \ as classification, summarization, and fairness prediction, employing both traditional\
  \ and semantic evaluation metrics."
---

# Evaluating the Limits of Large Language Models in Multilingual Legal Reasoning

## Quick Facts
- arXiv ID: 2509.22472
- Source URL: https://arxiv.org/abs/2509.22472
- Reference count: 40
- Primary result: Legal reasoning tasks show accuracies often below 50% for LLMs, significantly worse than general NLP tasks, with performance correlating to syntactic similarity to English.

## Executive Summary
This study evaluates the performance of Large Language Models (LLMs) like Meta's LLaMA and Google's Gemini on multilingual legal reasoning tasks, highlighting their strengths and weaknesses in complex, high-stakes contexts. The authors use an LLM-as-a-Judge framework to assess outputs across tasks such as classification, summarization, and fairness prediction, employing both traditional and semantic evaluation metrics. Their findings show that legal tasks are significantly harder for LLMs, with accuracies often below 50% on legal reasoning benchmarks, compared to over 70% on general tasks like XNLI. While English generally provides more stable results, it does not always yield higher accuracy. Prompt sensitivity and adversarial vulnerability persist across languages, and performance correlates with syntactic similarity to English. Overall, the study underscores the limitations of current LLMs in multilingual legal applications, even as newer models like Gemini 2.5 show notable improvements.

## Method Summary
The study evaluates multiple LLMs (LLaMA 3.1-8B, LLaMA 3.2-3B, Gemini 1.5 Flash, Gemini 2.5 Flash) on multilingual legal reasoning tasks using zero-shot inference. The evaluation pipeline employs Hugging Face datasets (MultiEURLEX, LEXam, XNLI, etc.) across 15 languages, with adversarial robustness tested using contextual word substitution and character insertion perturbations. Performance is measured through traditional metrics (accuracy, F1, ROUGE-L) and LLM-as-a-Judge scores using Gemini 2.0 Flash. The modular pipeline includes data loading, prompt application, optional perturbation, inference, post-processing, and evaluation phases.

## Key Results
- Legal reasoning tasks show accuracies often below 50% for LLMs, significantly worse than general NLP tasks (over 70% on XNLI).
- English provides more stable results but doesn't always yield higher accuracy compared to other languages.
- Prompt sensitivity and adversarial vulnerability persist across languages, with models defaulting to neutral predictions under attack.
- Performance correlates with syntactic similarity to English, though outliers like Maltese show unexpected high performance.
- Gemini 2.5 shows notable improvements but suffers from format drift issues.

## Why This Works (Mechanism)

### Mechanism 1: Syntactic Similarity Anchor
LLMs rely on an "English-centric" reasoning substrate. When input syntax aligns with this dominant training distribution, the model activates more robust internal representations, reducing prediction entropy. This correlation breaks down for specific low-resource languages (e.g., Maltese) where unexpected performance gains occur, suggesting tokenization or latent linguistic factors may override syntax.

### Mechanism 2: Prompt-Induced Debiasing
General-purpose LLMs appear optimized for non-confrontational outputs. Assertive instructions likely shift the sampling probability mass away from neutral labels, forcing the model to commit to definitive classifications. Over-assertive prompts may lead to "hallucinated" unfairness, where the model classifies fair clauses as unfair to satisfy the instruction.

### Mechanism 3: Semantic Adversarial Fragility
Models are more susceptible to semantic (word-level) perturbations than character-level noise. Context-aware word substitutions alter the semantic landscape while maintaining fluency, confusing the model's reasoning path and triggering a fallback to high-uncertainty/neutral outputs.

## Foundational Learning

- **Concept: LLM-as-a-Judge Evaluation**
  - Why needed here: Traditional metrics (BLEU/ROUGE) fail to capture the nuance of legal reasoning (e.g., "correct logic vs. correct wording"). The paper relies on a stronger model (Gemini 2.0) to grade weaker models.
  - Quick check question: Why does the paper use an LLM Judge for the LEXam open-ended questions instead of just ROUGE scores?

- **Concept: Syntactic vs. Semantic Transfer**
  - Why needed here: Understanding why English performance doesn't linearly transfer to other languages is central to the paper's diagnosis of multilingual failure modes.
  - Quick check question: According to the paper, why might a high-resource language like Spanish perform worse than expected if the evaluation setup is not controlled?

- **Concept: Adversarial Entropy**
  - Why needed here: The paper uses entropy and consistency metrics to measure robustness. High entropy under attack indicates the model is "guessing" rather than reasoning.
  - Quick check question: Does a character-level attack (typo) or a word-substitution attack (synonym swap) result in higher prediction entropy in this study?

## Architecture Onboarding

- **Component map:** Data Loader (Hugging Face datasets) -> Inference Engine (Gemini API / Ollama) -> Perturbation Module (nlpaug) -> Evaluator (Traditional metrics + LLM-as-a-Judge)

- **Critical path:** 1. Load dataset & select language. 2. Apply prompt (Assertive vs. Basic). 3. *Optional:* Apply adversarial perturbation. 4. Inference via target model. 5. Post-processing (extract labels from free text). 6. Evaluation (Traditional metrics + LLM Judge call).

- **Design tradeoffs:** Gemini 1.5 vs. 2.5: 2.5 has higher reasoning accuracy but suffers from "formatting drift" (ignoring output constraints). 1.5 is more stable but less accurate. Normalization: Aggregating scores across datasets requires min-max scaling, which exaggerates differences in small datasets.

- **Failure signatures:** Language Drift: Model answers in German when prompted in English (observed in Gemini 2.5 on LEXam). Safety Bias: Model defaults to "clearly fair" or "neutral" in ToS/XNLI tasks unless aggressively prompted. Format Loss: Model returns a full sentence explanation instead of the requested label index (0, 1, 2).

- **First 3 experiments:**
  1. **Prompt Sensitivity Stress Test:** Run the ToS "fairness" task with the Basic vs. Highly Assertive prompt to reproduce the 50% accuracy jump described in Section 7.1.4.
  2. **Cross-Lingual Robustness Check:** Apply the BERT-based word substitution attack on XNLI for English vs. Thai to verify the disparity in entropy/consistency described in Section 7.2.2.
  3. **Format Compliance Check:** Compare Gemini 1.5 and 2.5 on the LEXam multiple-choice task to detect "formatting drift" where 2.5 fails to output the strict format.

## Open Questions the Paper Calls Out

### Open Question 1
What specific linguistic or resource-based factors beyond syntactic similarity to English explain the high performance of outliers like Maltese? The authors observe that Maltese achieves unexpectedly strong performance despite having the lowest syntactic similarity to English, noting that "other latent factors... can play an important role." The study focused primarily on correlating performance with English-centric syntactic similarity and did not isolate other variables.

### Open Question 2
How does measuring linguistic similarity using pairwise comparisons or closest typological neighbors differ from an English-centric baseline in predicting multilingual legal reasoning performance? The authors acknowledge that using English as the sole reference point is "not the only possible baseline" and explicitly state they "leave such analyses for future work." The current analysis relies exclusively on similarity to English to predict performance drops and stability.

### Open Question 3
Do performance gains in newer model iterations (e.g., Gemini 2.5 vs 1.5) inherently trade off against instruction adherence and format consistency in high-stakes domains? The authors highlight that Gemini 2.5 Flash shows clear performance improvements but "occasional inconsistencies in output formatting and language use," suggesting progress does not always equal reliability. The paper identifies this trade-off anecdotally through formatting errors but does not quantify it.

## Limitations

- The study's findings may not generalize across different legal domains and jurisdictions since the models were not specifically trained on legal corpora.
- The LLM-as-a-Judge framework introduces potential circularity and evaluator bias that is not fully addressed.
- The correlation between syntactic similarity to English and performance may not account for deeper semantic or cultural factors that influence legal reasoning across languages.

## Confidence

- **High Confidence:** Legal reasoning tasks are significantly more difficult for LLMs than general NLP tasks, with accuracies below 50% compared to over 70% on XNLI.
- **Medium Confidence:** Prompt sensitivity and adversarial vulnerability persist across languages, though specific mechanisms could benefit from more direct causal evidence.
- **Medium Confidence:** The correlation between syntactic similarity to English and model performance is demonstrated empirically but may not fully explain observed patterns.
- **Low Confidence:** The assertion that newer models like Gemini 2.5 show "notable improvements" lacks sufficient comparative data to establish domain-specificity.

## Next Checks

1. **Domain Transfer Experiment:** Test whether the observed performance patterns hold when models are fine-tuned on legal corpora versus using zero-shot inference, to determine if limitations are architectural or training-data dependent.

2. **Evaluator Consistency Validation:** Conduct inter-annotator agreement studies using multiple human legal experts to validate the LLM-as-a-Judge framework's reliability, particularly for subjective fairness assessments.

3. **Adversarial Robustness Granularity:** Perform a finer-grained analysis of which specific legal concepts (contract terms, rights definitions, etc.) are most vulnerable to semantic perturbations, to identify whether the fragility is uniform across legal reasoning or concept-specific.