---
ver: rpa2
title: Understanding World or Predicting Future? A Comprehensive Survey of World Models
arxiv_id: '2411.14499'
source_url: https://arxiv.org/abs/2411.14499
tags:
- world
- arxiv
- preprint
- video
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey systematically categorizes world models into two primary
  functions: understanding the external world through internal representations and
  predicting future states for simulation and decision-making. It reviews recent progress
  across video generation, embodied environments, and applications in gaming, robotics,
  autonomous driving, and social simulation.'
---

# Understanding World or Predicting Future? A Comprehensive Survey of World Models

## Quick Facts
- arXiv ID: 2411.14499
- Source URL: https://arxiv.org/abs/2411.14499
- Reference count: 40
- This survey systematically categorizes world models into understanding the external world through internal representations and predicting future states for simulation and decision-making.

## Executive Summary
This comprehensive survey provides a unified framework for understanding world models by categorizing them into two primary functions: understanding the external world through internal representations and predicting future states for simulation and decision-making. The work reviews recent progress across video generation, embodied environments, and applications in gaming, robotics, autonomous driving, and social simulation. It highlights key challenges including physical rule adherence, social dimension enrichment, benchmark standardization, simulation efficiency, and ethical concerns while providing a roadmap of world model development from 2018-2025.

## Method Summary
The survey employs systematic literature review methodology to construct a unified framework for World Models by categorizing them into "Implicit Representations" (understanding the world) versus "Future Predictions" (simulating the world), and mapping these to applications. It curates a list of 40 representative papers/references and maintains a GitHub repository for code resources. The method involves qualitative categorization based on model functionality, comprehensive coverage of literature from 2018-2025, and summarization of code resources through systematic classification and verification of taxonomy.

## Key Results
- Establishes a binary taxonomy of world models into "Implicit Representations" for understanding versus "Future Predictions" for simulation
- Reviews representative papers and code repositories from 2018-2025 across multiple application domains
- Identifies critical challenges including physical rule adherence, social dimension enrichment, and simulation-to-reality gaps
- Provides evaluation frameworks and benchmarks for assessing world model capabilities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Agents learn to minimize prediction error regarding state transitions, constructing a latent model of the environment sufficient for decision-making without real-world interaction.
- **Mechanism:** Systems encode high-dimensional observations into compact latent states and learn dynamics models to predict next states given current states and actions, minimizing divergence between predicted and actual rollouts.
- **Core assumption:** Environment dynamics are Markovian and can be compressed into latent space without losing critical causal information.
- **Evidence anchors:** Abstract mentions "constructing internal representations to understand the mechanisms of the world" and Section 3.1.1 details MBRL's focus on learning transition dynamics.
- **Break condition:** Fails when distributional shift causes models to hallucinate rewards or dynamics not present in real environments.

### Mechanism 2
- **Claim:** Video generation models function as world simulators by predicting future visual states conditional on actions or text, provided they capture temporal consistency and physical rules.
- **Mechanism:** Systems like Sora use diffusion or transformer architectures to autoregressively predict next video frames or latent representations, conditioning generation on input actions.
- **Core assumption:** High-fidelity visual synthesis is a proxy for understanding physical dynamics through scaling data and compute.
- **Evidence anchors:** Abstract mentions "predicting its future dynamics" and Section 4.1.1 discusses video world models simulating future evolution while adhering to physical laws.
- **Break condition:** Breaks when visual realism diverges from physical realism, causing impossible object collisions.

### Mechanism 3
- **Claim:** Large Language Models function as world models by retrieving internalized spatial, temporal, and social knowledge encoded during pre-training to simulate reasoning or social interactions.
- **Mechanism:** Models predict next tokens in sequences, capturing world knowledge that can be elicited via prompting to simulate agent behavior or verify plan feasibility.
- **Core assumption:** Statistical correlations learned from text data correspond to structured, logical relationships in the real world.
- **Evidence anchors:** Abstract mentions "applications in social simulation" and Section 3.2 discusses spatial and temporal neurons organizing world knowledge.
- **Break condition:** Fails when text correlations don't reflect causal reality, leading to hallucinations or logical inconsistencies.

## Foundational Learning

- **Concept:** **Markov Decision Processes (MDPs)**
  - **Why needed here:** The survey frames world models fundamentally as learning transition dynamics $T(s'|s,a)$ and reward functions $R(s,a)$ within an MDP framework. Understanding state, action, and transition probabilities is a prerequisite for grasping model-based reinforcement learning.
  - **Quick check question:** Can you define the difference between a model-free agent (which learns a policy directly from rewards) and a model-based agent (which learns the environment's transition rules)?

- **Concept:** **Latent Space Representations**
  - **Why needed here:** Raw sensory data is too high-dimensional for efficient planning. Section 3.1.1 emphasizes representation learning as the first step in compressing the world into a manageable state for the dynamics model.
  - **Quick check question:** Why is training a dynamics model in a compressed latent space generally more efficient than training on raw pixel space?

- **Concept:** **Diffusion Models**
  - **Why needed here:** The survey identifies diffusion models as the dominant architecture for the "Future Prediction" category, used in systems like Sora and DriveDreamer to generate video simulations.
  - **Quick check question:** How does a diffusion model generate an image (starting from noise vs. conditioning on a previous frame)?

## Architecture Onboarding

- **Component map:** Perception Encoder -> Dynamics Model (Core) -> Planner/Policy -> Renderer/Generator (Optional)
- **Critical path:**
  1. Select Paradigm: Choose between "Implicit Representation" (latent dynamics for control) or "Future Prediction" (video generation)
  2. Data Collection: Gather interaction trajectories or video data
  3. Train Dynamics: Minimize prediction error or denoising loss
- **Design tradeoffs:**
  - Simulation Efficiency vs. Visual Fidelity: Transformer-based models offer high fidelity but high inference costs, while latent models are faster but less visually interpretable
  - Generalization vs. Precision: LLM-based models generalize well to social tasks but fail at precise physical manipulation
- **Failure signatures:**
  - Physical Hallucination: Generated video displays physically impossible events
  - Error Accumulation: Small errors compound in long-horizon predictions, causing divergence from reality
- **First 3 experiments:**
  1. Visual Consistency Check: Generate video of ball dropping to verify gravity and collision preservation
  2. MBRL Benchmarking: Train DreamerV3 agent on control suite to validate sample efficiency gains
  3. Counterfactual Simulation: Test if text-video models follow counterfactual prompts or adhere to learned physical constraints

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can purely data-driven generative models acquire robust physical laws from raw visual data alone?
- **Basis in paper:** Section 6.1 notes that while models like Sora produce realistic sequences, studies reveal "persistent physical-law failures" in areas like gravity and fluid dynamics.
- **Why unresolved:** Evidence suggests data-driven scaling yields "case-based" generalization rather than rule-based understanding, breaking down on combinatorial tests.
- **What evidence would resolve it:** Models successfully passing first-principles benchmarks (e.g., T2VPhysBench) and counterfactual simulations without explicit physics engine integration.

### Open Question 2
- **Question:** How can autonomous agents be designed to simulate realistic, comprehensive human behavior and social interactions?
- **Basis in paper:** Section 6.2 states that designing agents to simulate "realistic and comprehensive human behavior" remains an open problem.
- **Why unresolved:** Evaluation currently relies on subjective human assessment, which cannot scale, and LLMs struggle with complex social nuances.
- **What evidence would resolve it:** Development of a scalable, reliable evaluation scheme that correlates strongly with human judgment of social realism.

### Open Question 3
- **Question:** How can the simulation-to-reality gap be closed to transfer embodied intelligence to the physical world?
- **Basis in paper:** Section 6.4 identifies the "simulation-to-reality gap" as a long-standing research problem.
- **Why unresolved:** Policies trained in simulation often fail on physical robots due to discrepancies in environment dynamics, despite improvements in generative realism.
- **What evidence would resolve it:** Demonstration of agents successfully transferring skills learned in generative world models to physical hardware with minimal fine-tuning.

## Limitations

- The survey's qualitative nature relies on systematic literature review rather than empirical validation of claims
- Binary taxonomy (Implicit Representation vs. Future Prediction) may oversimplify hybrid models combining both approaches
- Selection bias exists in the choice of 40 representative papers from broader literature
- Boundary between "world understanding" and "future prediction" remains somewhat subjective

## Confidence

- **High Confidence:** Categorization framework for world models is well-supported by literature evidence and provides useful organizing principles
- **Medium Confidence:** Claims about LLM-based world models functioning as simulators are supported by neural evidence but remain debated regarding causal understanding
- **Low Confidence:** Assertion that video generation models truly "understand" physical dynamics versus simply producing plausible pixels is explicitly questioned in the survey

## Next Checks

1. **Cross-validation of taxonomy:** Select 10 additional world model papers not in the survey and independently classify them using the framework to test generalizability and identify edge cases

2. **Benchmarking study:** Conduct controlled experiments comparing model-based RL agents (using learned dynamics) against model-free baselines across multiple environments to quantify claimed sample efficiency gains

3. **Physical consistency testing:** Systematically evaluate video generation models (e.g., Sora, Genie) on physical reasoning benchmarks (rigid body dynamics, collision handling) to measure the gap between visual plausibility and physical accuracy