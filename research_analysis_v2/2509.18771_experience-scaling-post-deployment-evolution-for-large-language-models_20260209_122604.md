---
ver: rpa2
title: 'Experience Scaling: Post-Deployment Evolution For Large Language Models'
arxiv_id: '2509.18771'
source_url: https://arxiv.org/abs/2509.18771
tags:
- experience
- llms
- scaling
- data
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Experience scaling introduces a framework for continuous post-deployment
  learning of large language models through autonomous interaction with the environment.
  Unlike traditional scaling methods that rely on static human-generated data, this
  approach captures raw interactions, distills them into compact experiences, and
  refines stored knowledge to maintain efficiency.
---

# Experience Scaling: Post-Deployment Evolution For Large Language Models

## Quick Facts
- arXiv ID: 2509.18771
- Source URL: https://arxiv.org/abs/2509.18771
- Reference count: 40
- Primary result: Experience-augmented LLMs achieve 4.6% higher accuracy than plain inference and 3% better than memory-based methods

## Executive Summary
Experience scaling introduces a framework for continuous post-deployment learning of large language models through autonomous interaction with the environment. Unlike traditional scaling methods that rely on static human-generated data, this approach captures raw interactions, distills them into compact experiences, and refines stored knowledge to maintain efficiency. The framework enables LLMs to evolve by continuously incorporating new information gathered during deployment.

The system demonstrates significant performance improvements across multiple benchmarks, with experience-augmented models achieving 4.6% higher accuracy than plain inference and 3% better than memory-based methods. Under repetitive queries, accuracy improves steadily across datasets like MMLU and SciQ, with self-reported confidence scores consistently higher for correct predictions. The approach addresses data exhaustion challenges and opens new pathways for LLM development beyond the limits of human-generated datasets.

## Method Summary
The experience scaling framework implements a client-server architecture where environmental interactions are captured, distilled into compact experiences, and stored in a distributed backend. Raw inputs are buffered into episodes sharing semantic similarity, then synthesized by an LLM into compact experience items. At inference time, BM25-based retrieval fetches top-k similar experiences to augment LLM generation. Periodic refinement condenses redundant experiences through pairwise similarity grouping and LLM-based condensation, maintaining database efficiency under storage saturation.

## Key Results
- Experience-augmented LLMs achieve 4.6% higher accuracy than plain inference
- Models show 3% improvement over memory-based retrieval methods
- Under repetitive queries, accuracy improves steadily across epochs with better confidence calibration
- Refinement processes can recover up to 25% accuracy improvement under storage saturation

## Why This Works (Mechanism)

### Mechanism 1: Experience Distillation from Raw Interactions
- Claim: Distilling raw environmental data into compact experiences enables knowledge transfer to unseen but related queries
- Mechanism: Raw inputs are buffered into episodes sharing semantic similarity, then synthesized into compact experience items
- Core assumption: Distilled experiences capture reusable reasoning patterns, not just memorized question-answer pairs
- Evidence anchors: 4.6% improvement over plain inference on held-out test questions from same topic domain

### Mechanism 2: Retrieval-Augmented Inference via BM25 Similarity
- Claim: Retrieving top-k semantically similar experiences improves factual accuracy and self-calibration
- Mechanism: Backend servers use BM25 lexical similarity scores to return top-k experiences for query augmentation
- Core assumption: Lexically similar stored experiences contain relevant information for current queries
- Evidence anchors: Steady accuracy increases across epochs under repetitive queries with better confidence calibration

### Mechanism 3: Periodic Refinement via Similarity-Based Condensation
- Claim: Condensing redundant experiences preserves accuracy under storage saturation
- Mechanism: Server computes pairwise similarity matrix, groups high-similarity experiences, condenses via LLM
- Core assumption: High-similarity experiences contain redundant information that can be merged without losing critical details
- Evidence anchors: Up to 25% accuracy improvement with BM25 threshold of 100; degradation at overly aggressive thresholds

## Foundational Learning

- **Retrieval-Augmented Generation (RAG)**: Why needed - extends RAG by storing dynamically generated experiences rather than static documents. Quick check - Can you explain how retrieved context is incorporated into LLM generation?

- **BM25 and Lexical Similarity**: Why needed - both retrieval and refinement depend on BM25 similarity scoring. Quick check - How does BM25 differ from dense embedding-based similarity in handling rare terms?

- **Knowledge Distillation and Compression**: Why needed - the condensation operator uses an LLM to merge experiences, analogous to knowledge distillation. Quick check - What information risks being lost when condensing multiple documents into one summary?

- **Continual Learning and Catastrophic Forgetting**: Why needed - experience scaling avoids weight updates, preventing catastrophic forgetting. Quick check - Why does updating model weights risk forgetting previously learned tasks?

## Architecture Onboarding

- **Component map**: Frontend (client) captures raw data → episode buffer → LLM distillation → classify type → Network routes to backend → Backend stores experiences → periodic refinement → handles retrieval via BM25 → returns top-k → Frontend augmented inference → output

- **Critical path**: Environment/user → Frontend (capture raw data → episode buffer) → Frontend (distill episode → classify type) → Network (route to backend) → Backend (store experience → periodic refinement when threshold reached) → Query → Frontend (classify query type) → Network (route to backend) → Backend (BM25 retrieval → return top-k) → Frontend (augmented inference → output)

- **Design tradeoffs**: Client-server vs. single-machine deployment (distributed enables collaborative learning but adds latency; single-machine prioritizes personalization); Refinement frequency vs. compute cost (more frequent maintains quality but consumes idle compute); Condensation threshold (high preserves detail but risks bloat; low risks over-condensation)

- **Failure signatures**: Accuracy degradation over epochs without refinement; Performance drop at low BM25 thresholds indicating over-condensation; High variance across domains suggesting domain-specific tuning needed

- **First 3 experiments**:
  1. **Generalized Experience**: Train on 80% of topic dataset, evaluate on held-out 20% comparing plain inference, memory-based retrieval, and experience-augmented retrieval
  2. **Repetitive Experience**: Deploy multiple clients querying same topic over multiple epochs, track accuracy and confidence per epoch
  3. **Refined Experience**: Saturate database (100 iterations per question), compare full vs. refined databases at different BM25 thresholds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a serverless peer-to-peer framework effectively replace the client-server architecture to reduce communication latency and improve resource efficiency?
- Basis in paper: Current client-server model introduces communication latency and results in under-utilized backend compute resources during idle periods
- Why unresolved: Current implementation relies on centralized backend creating bottleneck not yet tested against decentralized alternatives
- What evidence would resolve it: Comparative analysis of time-to-first-token and compute utilization rates between current and proposed peer-to-peer implementations

### Open Question 2
- Question: How can the experience lifecycle be adapted to effectively distill and refine multi-modal environmental data (e.g., video, sensor streams)?
- Basis in paper: Framework designed for diverse inputs but current implementation centers on textual experiences requiring future extension
- Why unresolved: Existing algorithms optimized for text may not capture spatial or temporal features inherent in sensor data
- What evidence would resolve it: Successful demonstration of accuracy gains in tasks requiring visual or auditory understanding using multi-modal experience database

### Open Question 3
- Question: Does storing experiences in an LLM-native representation yield better performance and efficiency than current human-readable format?
- Basis in paper: Authors hypothesize preserving experience in LLM-native representations might retain deeper semantic structures lost in human-readable formats
- Why unresolved: Current system stores data in original input form for ease of analysis, leaving potential performance benefits of native encoding untested
- What evidence would resolve it: Benchmarks comparing retrieval relevance and inference accuracy between LLM-native storage vectors and current raw text storage

## Limitations
- Domain-specific performance variations suggest framework may not generalize equally well across all knowledge domains
- Computational overhead of pairwise similarity matrix computation and LLM-based condensation not fully characterized
- Critical hyperparameters (BM25 settings, similarity thresholds, LLM prompts) are unspecified, requiring extensive tuning for reproduction

## Confidence

**High Confidence (Evidence Strong)**:
- Mechanism 1 (Experience Distillation): 4.6% accuracy improvement over plain inference on held-out test questions
- Mechanism 2 (BM25 Retrieval): Steady accuracy increases across epochs under repetitive queries with better confidence calibration
- Mechanism 3 (Refinement): Up to 25% accuracy improvement with appropriate BM25 threshold settings

**Medium Confidence (Evidence Moderate)**:
- Framework Generalization: Results across multiple datasets provide reasonable evidence but domain-specific variations exist
- Computational Efficiency: Framework addresses data exhaustion but computational overhead not fully characterized

**Low Confidence (Evidence Weak or Missing)**:
- Hyperparameter Robustness: Critical parameters unspecified, making sensitivity assessment difficult
- Long-term Evolution Dynamics: Limited epochs and dataset sizes prevent characterization of long-term behavior

## Next Checks

1. **Prompt Ablation Study**: Systematically vary experience distillation and condensation prompts across multiple datasets to assess impact on accuracy and identify robust prompt designs

2. **BM25 Parameter Sensitivity Analysis**: Conduct experiments varying BM25 hyperparameters (k1, b) and top-k retrieval counts across different domain datasets to identify optimal parameter ranges and assess sensitivity

3. **Longitudinal Scaling Experiment**: Deploy framework across extended time periods (1000+ queries) on diverse datasets to evaluate accuracy trends, storage growth, refinement frequency requirements, computational overhead, and performance stability across similarity thresholds