---
ver: rpa2
title: 'Hard Work Does Not Always Pay Off: Poisoning Attacks on Neural Architecture
  Search'
arxiv_id: '2405.06073'
source_url: https://arxiv.org/abs/2405.06073
tags:
- data
- poisoning
- architectures
- attacks
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how data poisoning impacts neural architecture
  search (NAS), finding that while NAS appears robust in terms of raw accuracy, the
  expected performance gains over random search can drop by up to 156%. The authors
  introduce a systematic poisoning framework, evaluating four attacks (random label
  flipping, confidence-based label flipping, Gaussian noise, and gradient canceling)
  across four NAS algorithms (training-based P-DARTS and NSGANetV2, training-free
  TE-NAS, and hybrid RoBoT).
---

# Hard Work Does Not Always Pay Off: Poisoning Attacks on Neural Architecture Search

## Quick Facts
- **arXiv ID**: 2405.06073
- **Source URL**: https://arxiv.org/abs/2405.06073
- **Reference count**: 25
- **Primary result**: Data poisoning can reduce NAS improvement over random search by up to 156%, even when raw accuracy appears minimally affected

## Executive Summary
This paper systematically investigates how data poisoning attacks impact Neural Architecture Search (NAS) algorithms. The authors introduce a poisoning framework and evaluate four attack strategies (random label flipping, confidence-based label flipping, Gaussian noise, and gradient canceling) against four NAS algorithms (P-DARTS, NSGANetV2, TE-NAS, and RoBoT). They find that while NAS appears robust in terms of raw accuracy, the expected performance gains over random search can drop by up to 156%. Training-based methods are most vulnerable to label-flipping attacks, while training-free methods are most resilient but produce architectures comparable to random samples. The results question the true value of NAS and suggest that standard defenses like data sanitization show limited effectiveness.

## Method Summary
The authors develop a poisoning framework that evaluates four attack strategies across four NAS algorithms on CIFAR-10 and CIFAR-100 benchmarks. Poisoning budgets are set at {1%, 10%, 50%} of training data. The evaluation uses two metrics: classification accuracy of final architectures trained on clean data, and ∆Improvement (percentage-point accuracy change versus random sampling baseline). The method isolates the search phase by retraining discovered architectures from scratch on clean data. Statistical significance is assessed via one-sided Welch's t-test with Benjamini-Hochberg FDR control at α=0.05, with 10 trials per configuration.

## Key Results
- Data poisoning can reduce NAS performance gains over random search by up to 156%, even when raw accuracy appears minimally affected
- Training-based NAS is uniquely vulnerable to label-flipping attacks, with ∆Imp. dropping to near-zero or negative values under 50% poisoning
- Training-free NAS methods show inherent insensitivity to distribution shifts, making them robust but potentially uninformative as they produce architectures comparable to random samples
- Out-of-distribution datasets like MNIST can yield effective architectures for CIFAR-10, questioning the value of dataset-specific NAS

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data poisoning can reduce NAS performance gains over random search by up to 156%, even when raw accuracy appears minimally affected.
- Mechanism: Poisoning introduces adversarial distributional shifts that mislead the architecture selection process. Since NAS typically provides marginal improvements (e.g., 0.58% on CIFAR-10), even small perturbations to architecture rankings can eliminate or reverse these gains. The ∆Imp. metric captures this by measuring percentage-point change relative to random sampling.
- Core assumption: NAS algorithms optimize for marginal gains; architectures exist in a high-performing subspace where random samples already achieve ~96.7% on CIFAR-10.
- Evidence anchors:
  - [abstract] "expected improvement can be substantially diminished under data poisoning"
  - [section 4.2] "reduction in ∆Imp. ... correspond to a 34–156% reduction in the benefit provided by NAS"
  - [corpus] No direct corpus evidence for poisoning-NAS interaction; related work focuses on NAS efficiency, not robustness.
- Break condition: If poisoning budget p < 1%, effects may not reach statistical significance; if training-free NAS is used, label-flipping has no effect.

### Mechanism 2
- Claim: Training-based NAS is uniquely vulnerable to label-flipping because architectural parameters are optimized via gradient descent on corrupted supervision signals.
- Mechanism: Differentiable NAS (e.g., P-DARTS) jointly optimizes architectural weights α and supernet weights using the training data's labels. Label corruption introduces contradictory gradients that shift α away from optimal operations—for example, over-selecting skip connections that minimize error propagation under noisy labels.
- Core assumption: Architectural parameters are learned from data-dependent gradients; no orthogonal regularization prevents degenerate architectures.
- Evidence anchors:
  - [section 5.1] "heavily label-flipped data... caused P-DARTS to select more skip-connections as they have less propensity to introduce error"
  - [section 4.2] "RLF with p = 50%... ∆Imp. drops to 0.04 on CIFAR-10 and -1.05 on CIFAR-100"
  - [corpus] Assumption: related NAS methods (OptiProxy-NAS, RAM-NAS) use similar gradient-based optimization but do not discuss robustness.
- Break condition: Training-free NAS bypasses this entirely; hybrid methods with limited training show intermediate robustness.

### Mechanism 3
- Claim: Training-free metrics exhibit inherent insensitivity to distribution shifts, making training-free and hybrid NAS robust but potentially uninformative.
- Mechanism: Metrics like NTK condition number (κ_NTK), gradient norm, and linear region count measure architectural properties (trainability, expressivity) computed from forward passes on data batches. These metrics change <10% even under 50% poisoning (Table 2), meaning architecture rankings remain stable—but this stability reflects insensitivity to dataset-specific information, not principled robustness.
- Core assumption: Training-free metrics capture general architectural quality independent of target distribution.
- Evidence anchors:
  - [section 5.2] "training-free metrics are largely insensitive to data poisoning: differences range from 0–65.42% and remain below 10% in all but two cases"
  - [abstract] "training-free NAS approaches... produce architectures that perform similarly to random selections"
  - [corpus] RBFleX-NAS and Zero-Shot NAS similarly use training-free proxies; corpus does not address robustness implications.
- Break condition: If poisoning specifically targets metric computation (e.g., adversarial examples crafted to maximize κ_NTK variance), robustness may degrade.

## Foundational Learning

- **Differentiable Architecture Search (DARTS)**: Why needed here: P-DARTS extends DARTS; understanding the supernet formulation and architectural parameter optimization is essential for interpreting why gradient-based attacks transfer.
  - Quick check question: Explain how Eq. (1) fuses operations via architectural parameters α and how discretization derives a final architecture.

- **Data Poisoning Threat Model**: Why needed here: The paper assumes white-box knowledge for some attacks; distinguishing dirty-label (label manipulation) vs. clean-label (perturbation-only) attacks determines which defenses apply.
  - Quick check question: For a given poisoning budget p=10%, which attack requires model access and which operates black-box?

- **Training-Free Proxies (Zero-Cost Metrics)**: Why needed here: Understanding what κ_NTK, grad_norm, snip, grasp, and fisher measure explains why they resist label corruption but fail to capture dataset-specific optimization.
  - Quick check question: If κ_NTK increases by 65% under GC attack (Table 2), why might this not meaningfully affect architecture selection?

## Architecture Onboarding

- **Component map**: Poisoning module -> NAS runners -> Evaluation pipeline -> Statistical testing
- **Critical path**:
  1. Select NAS algorithm and poisoning attack/budget
  2. Inject poisons into training data before search phase
  3. Run NAS search, extract final discretized architecture
  4. Retrain architecture on clean data for 600 epochs
  5. Compare Acc. to random sampling baseline (96.73% for CIFAR-10, 80.70% for CIFAR-100) to compute ∆Imp.
  6. Assess significance against clean-data control
- **Design tradeoffs**:
  - **∆Imp. vs. Acc.**: Use ∆Imp. for detecting relative degradation; use Acc. for absolute robustness claims
  - **White-box vs. black-box attacks**: GC requires algorithm knowledge; RLF/CLF do not
  - **Search-only vs. joint poisoning**: Paper isolates search phase; poisoning both search and training would amplify degradation but conflates mechanisms
- **Failure signatures**:
  - ∆Imp. approaches 0 or goes negative (architecture worse than random)
  - Architectural visualization shows excessive skip connections or missing pooling in reduction cells
  - Training-free metrics show <5% change but ∆Imp. drops significantly (training-based NAS only)
- **First 3 experiments**:
  1. Replicate P-DARTS + RLF with p=50% on CIFAR-10 to verify ∆Imp. degradation to near-zero or negative
  2. Compare TE-NAS on clean CIFAR-10 vs. MNIST-sourced search to confirm OOD generalization (Table 3)
  3. Apply loss-based sanitization defense to P-DARTS + CLF p=50% to test if removing high-loss samples recovers ∆Imp.

## Open Questions the Paper Calls Out

- **Cross-dataset robustness analysis**: Can NAS-specific clean-label poisoning attacks be designed to overcome data augmentation defenses and the dynamic nature of evolving supernets during search?
- **Multi-modal NAS evaluation**: What is the combined impact of poisoning both the architecture search phase and subsequent model training simultaneously?
- **Adaptive poisoning defense study**: Do training-free NAS methods discover architectures that are meaningfully better than random sampling, or are they simply returning near-random candidates with high robustness?

## Limitations

- The analysis is limited to image classification benchmarks (CIFAR-10/100), leaving questions about NAS robustness for other modalities like language or graphs
- Poisoning budgets tested (1%, 10%, 50%) represent extreme cases—real-world attacks may operate at much lower budgets with different distributional properties
- The paper's conclusion that NAS provides limited benefit over random search is based on CIFAR-10's high random baseline (~96.7%), which may not generalize to more challenging datasets

## Confidence

**High Confidence**: Claims about training-based NAS vulnerability to label-flipping attacks are well-supported by consistent experimental evidence across multiple algorithms (P-DARTS, NSGANetV2) and budgets. The mechanistic explanation linking corrupted gradients to architectural parameter optimization is sound.

**Medium Confidence**: The assertion that training-free NAS produces architectures similar to random samples relies on CIFAR-10's unusually high random baseline. While the experimental evidence is strong, the generalizability to other datasets and the implications for training-free method design require further validation.

**Low Confidence**: The claim that MNIST-sourced search can produce effective CIFAR architectures, while intriguing, is based on a single data point. The mechanism explaining why OOD datasets might still yield useful architectures is speculative and requires more systematic investigation across multiple dataset pairs.

## Next Checks

1. **Cross-dataset robustness analysis**: Repeat the poisoning experiments on a more challenging image dataset (e.g., TinyImageNet or STL-10) where random architecture performance is significantly lower than optimized architectures, to test whether NAS still provides meaningful benefits under poisoning.

2. **Multi-modal NAS evaluation**: Extend the poisoning framework to NAS methods for non-image tasks (e.g., Transformer architecture search for language modeling) to determine if the observed vulnerabilities generalize across different NAS paradigms and data types.

3. **Adaptive poisoning defense study**: Implement and evaluate active defense mechanisms that adapt to poisoning attacks during search, such as robust optimization techniques or anomaly detection for architectural parameters, to assess whether NAS robustness can be improved beyond simple data sanitization.