---
ver: rpa2
title: Spike-and-Slab Posterior Sampling in High Dimensions
arxiv_id: '2503.02798'
source_url: https://arxiv.org/abs/2503.02798
tags:
- posterior
- lemma
- algorithm
- sampling
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces the first provable algorithms for spike-and-slab\
  \ posterior sampling that work with measurement counts sublinear in problem dimension\
  \ and apply for any signal-to-noise ratio. The authors provide two polynomial-time\
  \ samplers for Gaussian priors that achieve total variation error \u03B4 using n\
  \ = \u03A9(k\xB3 polylog(d/\u03B4)) measurements, with runtime O(n\xB2d\xB9\xB7\u2075\
  \ polylog(\xB7)) or O(nd log(\xB7)) for different variants."
---

# Spike-and-Slab Posterior Sampling in High Dimensions

## Quick Facts
- arXiv ID: 2503.02798
- Source URL: https://arxiv.org/abs/2503.02798
- Reference count: 40
- This paper introduces the first provable algorithms for spike-and-slab posterior sampling that work with measurement counts sublinear in problem dimension and apply for any signal-to-noise ratio.

## Executive Summary
This paper resolves a fundamental challenge in Bayesian sparse inference by providing the first polynomial-time algorithms for spike-and-slab posterior sampling that work with sublinear measurements. The authors develop a two-stage approach that combines sparse recovery with centered rejection sampling, achieving total variation error δ using only n = Ω(k³ polylog(d/δ)) measurements. This represents a significant breakthrough as prior work either required linear measurements or only provided point estimation. The algorithms work for Gaussian priors at any signal-to-noise ratio and extend to Laplace priors under bounded noise conditions.

## Method Summary
The paper introduces a two-stage algorithmic framework for spike-and-slab posterior sampling. First, sparse recovery (using Lasso) produces a denoised estimate θ̂ that identifies large signal coordinates with ℓ∞ guarantees. Second, centered rejection sampling uses conditional Poisson sampling over support sets containing the denoised support, leveraging dynamic programming for efficient proposal generation. The key innovation is approximating the true posterior over a high-probability region where the signal is sparse, then performing rejection sampling with carefully designed proposal distributions. For Gaussian priors, the method achieves polylogarithmic scaling in 1/δ, while for Laplace priors, it requires polynomial dependence on 1/δ due to the need to estimate normalizing constants via annealing.

## Key Results
- First provable polynomial-time spike-and-slab posterior samplers with sublinear measurements (n = Ω(k³ polylog(d/δ)))
- Achieves D_TV(π', π(·|X,y)) ≤ δ with runtime O(n²d¹·⁵ polylog(·)) or O(nd log(·)) for different variants
- Extends to Laplace priors with n = Ω(k⁵ polylog(d/δ)) measurements when noise level is bounded by O(1/k)
- Demonstrates that polynomial-time sampling is possible beyond the linear regime of compressed sensing

## Why This Works (Mechanism)
The algorithm works by first identifying the "signal support" through sparse recovery, then performing rejection sampling conditioned on including these coordinates. The centered rejection sampling framework approximates the true posterior by restricting attention to a high-probability region where the signal is sparse, making the problem tractable. The use of conditional Poisson sampling allows efficient exploration of the support space while maintaining the required sparsity constraints.

## Foundational Learning
- **Total Variation Distance**: Measures the difference between two probability distributions; needed to quantify the accuracy of the approximate posterior sampler; quick check: verify D_TV bounds in Theorems 1-4
- **Restricted Isometry Property (RIP)**: Ensures stable sparse recovery; required for the denoising stage to work properly; quick check: validate RIP constants for measurement matrices used
- **Centered Rejection Sampling**: A variant of rejection sampling that conditions on including certain coordinates; essential for maintaining sparsity structure; quick check: verify acceptance rates match theoretical predictions
- **Dynamic Programming for Conditional Sampling**: Enables efficient sampling from constrained support sets; critical for polynomial runtime; quick check: test DP table computation for various sparsity levels
- **Annealing for Normalizing Constants**: Used to estimate intractable constants for Laplace priors; necessary when closed-form solutions don't exist; quick check: verify convergence of annealing schedule

## Architecture Onboarding

**Component Map:** Sparse Recovery -> Denoising Hint -> Conditional Poisson Sampling -> Centered Rejection Sampling -> Approximate Posterior

**Critical Path:** The algorithm's success depends on the sparse recovery stage accurately identifying large signal coordinates. If θ̂ fails to capture the true support, the rejection sampling stage will have very low acceptance rates, making the algorithm impractical.

**Design Tradeoffs:** The choice between O(n²d¹·⁵ polylog(·)) and O(nd log(·)) runtimes represents a fundamental tradeoff between computational efficiency and implementation complexity. The Gaussian prior approach handles arbitrary SNRs but requires more sophisticated rejection sampling, while the Laplace approach is simpler but limited to bounded noise.

**Failure Signatures:** Low acceptance rates in rejection sampling indicate either poor denoising performance or that the true signal contains more large coordinates than expected. Runtime degradation suggests the DP table computation is becoming a bottleneck for the given sparsity level.

**First Experiments:**
1. Test sparse recovery accuracy on synthetic data with varying sparsity and noise levels
2. Measure rejection sampling acceptance rates as a function of SNR and sparsity
3. Validate total variation error empirically for different measurement counts

## Open Questions the Paper Calls Out
- Can spike-and-slab posterior sampling be achieved with only n ≈ k measurements, matching the information-theoretic limit? The current algorithms require n = Ω(k³ polylog(d)) or n = Ω(k⁵ polylog(d)) measurements, leaving a substantial gap from the theoretical optimum of n ≈ k.
- For which diffuse densities μ beyond Gaussian can spike-and-slab posterior sampling work for arbitrary SNRs and sublinear measurements? The authors think it is an interesting open direction to characterize the types of μ for which polynomial-time sampling is tractable.
- Can the polynomial dependence on 1/δ in the Laplace prior sampler be improved to polylogarithmic? The lack of closed-form normalizing constants for Laplace-distributed posterior components necessitates approximation, introducing polynomial δ-dependence.
- Can diffusion-based methods be adapted to provide provable guarantees for spike-and-slab posterior sampling? Existing diffusion methods for posterior sampling are not analyzed in discrete time, and their χt(π) parameter is unbounded for spike-and-slab priors.

## Limitations
- Requires n = Ω(k³ polylog(d/δ)) measurements, leaving a substantial gap from the theoretical optimum of n ≈ k
- Polynomial dependence on 1/δ (O(1/δ⁴)) for Laplace priors limits precision of uncertainty quantification
- Performance heavily relies on successful sparse recovery in the first stage - failure to identify true large-coordinate support severely impacts rejection sampling

## Confidence

**High confidence:** The correctness of the two-stage algorithmic framework (denoising + centered rejection sampling) and the general approach of using sparse recovery to guide posterior sampling

**Medium confidence:** The specific polynomial bounds on measurement complexity and runtime, as these depend on tight control of constants in RIP and concentration inequalities

**Medium confidence:** The extension to Laplace priors, given the additional technical machinery for normalizing constant estimation via annealing

## Next Checks
1. **RIP Validation**: Implement a concrete test for (ϵ, k*)-RIP on Gaussian measurement matrices with varying n, d, k to empirically verify when the theoretical bounds become practically achievable
2. **Annealing Schedule Implementation**: Fully specify and implement the annealing schedule {σᵢ} for Laplace normalizing constant estimation, including convergence criteria and computational overhead
3. **Rejection Rate Analysis**: Characterize the empirical acceptance rate of the centered rejection sampling as a function of signal-to-noise ratio and sparsity level, comparing against theoretical predictions from Proposition 5