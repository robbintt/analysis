---
ver: rpa2
title: 'Efficient Inference Using Large Language Models with Limited Human Data: Fine-Tuning
  then Rectification'
arxiv_id: '2511.19486'
source_url: https://arxiv.org/abs/2511.19486
tags:
- fine-tuning
- variance
- labeled
- sample
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a fine-tuning then rectification framework
  that optimally allocates limited labeled data between improving LLM surrogates and
  correcting their residual biases. Unlike standard approaches that minimize mean
  squared prediction error, the authors introduce a variance-based fine-tuning objective
  specifically tailored for downstream prediction-powered inference (PPI).
---

# Efficient Inference Using Large Language Models with Limited Human Data: Fine-Tuning then Rectification

## Quick Facts
- arXiv ID: 2511.19486
- Source URL: https://arxiv.org/abs/2511.19486
- Authors: Lei Wang; Zikun Ye; Jinglong Zhao
- Reference count: 20
- This paper proposes a fine-tuning then rectification framework that optimally allocates limited labeled data between improving LLM surrogates and correcting their residual biases. Unlike standard approaches that minimize mean squared prediction error, the authors introduce a variance-based fine-tuning objective specifically tailored for downstream prediction-powered inference (PPI).

## Executive Summary
This paper addresses the challenge of mean estimation from text data when labeled samples are limited. The authors propose a "Fine-Tuning then Rectification" framework that optimally allocates scarce labeled data between training an LLM surrogate and correcting its residual biases using Prediction-Powered Inference (PPI). Unlike standard approaches that minimize mean squared error, they introduce a variance-based fine-tuning objective specifically designed for PPI, where bias correction is handled separately. By leveraging empirical scaling laws, they derive an optimal allocation rule that minimizes the final estimator's variance. The method achieves 45%-66% sample savings compared to the sample mean estimator and 18%-54% variance reduction compared to MSE-based fine-tuning.

## Method Summary
The method consists of two stages: first, fine-tuning an LLM surrogate to predict ratings from text reviews using a variance-based loss function that minimizes the dispersion of prediction residuals; second, applying Prediction-Powered Inference (PPI) to correct systematic biases in the model's predictions using a separate set of labeled data. The key innovation is optimizing for residual variance rather than mean squared error during fine-tuning, which is optimal when bias correction will be handled by PPI. The framework uses a data-driven ramp-up procedure to estimate scaling law parameters, then solves for the optimal allocation of labeled samples between fine-tuning and PPI stages. The final estimator combines predictions from the fine-tuned model with corrections from PPI to produce an unbiased estimate of the population mean.

## Key Results
- Empirical validation on Wine Reviews dataset confirms scaling law's high goodness-of-fit (R²=0.998)
- Theoretically optimal allocation ratio s*/n ≈ 10.3% closely matches empirical optimum (~10%)
- Proposed method consistently outperforms all benchmarks, achieving 45%-66% sample savings vs. sample mean estimator
- Achieves 18%-54% variance reduction compared to MSE-based fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
Minimizing residual variance during fine-tuning, rather than mean squared error (MSE), yields lower-variance downstream estimators when combined with prediction-powered inference (PPI). The PPI estimator variance depends on Var(Y−f(X))/(n−s), where n−s is the PPI sample size. Bias in f(X) is corrected by PPI's rectification step, so residual dispersion—not prediction error—determines efficiency. Example 1 (p. 9) shows a predictor with constant bias (high MSE, zero variance) is optimal for PPI. Core assumption: The labeled data used for PPI rectification is held out from fine-tuning and is representative of the target distribution.

### Mechanism 2
Residual variance after fine-tuning follows a power-law scaling relationship with training data size. The scaling law Var(Y−f(X)) = as^(−α) + b implies predictable, diminishing returns from additional labeled data. This enables forward planning and optimal allocation before full training. Core assumption: The scaling law parameters (a, α, b) are stable across the data regime of interest and can be reliably estimated from a ramp-up procedure.

### Mechanism 3
The theoretically optimal allocation rule derived from scaling laws closely matches the empirically optimal allocation. The optimal s* solves αan·s^(−α−1) − (α+1)as^(−α) − b = 0. This balances improved model quality from more fine-tuning data against reduced PPI sample size. The solution can be found via binary search. Core assumption: The scaling law parameters are known or can be estimated accurately before allocation decisions are made.

## Foundational Learning

- **Concept: Prediction-Powered Inference (PPI)**
  - **Why needed here:** This is the rectification method that corrects LLM biases using labeled data, forming the second stage of the framework.
  - **Quick check question:** If an LLM systematically over-predicts ratings by 5 points, does PPI produce an unbiased estimate of the population mean? (Answer: Yes, as long as labeled data is representative.)

- **Concept: Residual Variance vs. Mean Squared Error**
  - **Why needed here:** The key insight is that these objectives diverge: MSE penalizes both bias and variance, while residual variance penalizes dispersion only. For PPI, bias is irrelevant.
  - **Quick check question:** A model predicts Ŷ = Y + 10 for all samples. What is its MSE? Its residual variance? (Answer: MSE = 100, residual variance = 0.)

- **Concept: Scaling Laws for Neural Networks**
  - **Why needed here:** The framework relies on the assumption that model quality follows a predictable power-law relationship with data size.
  - **Quick check question:** If α = 0.5, how much does residual variance decrease when you double the fine-tuning data? (Answer: By a factor of 2^0.5 ≈ 1.41, i.e., ~29% reduction.)

## Architecture Onboarding

- **Component map:**
  1. Fine-tuning module: Trains LLM surrogate using variance-based loss L_var = (1/k)Σ(r_i − r̄)² on mini-batches
  2. Scaling law estimator: Fits a, α, b via ramp-up procedure using validation residuals
  3. Allocation solver: Binary search for s* satisfying Eq. (4)
  4. PPI estimator: Combines fine-tuned predictions with labeled residuals per Eq. (1)
  5. Inference module: Constructs confidence intervals using sample variance estimates

- **Critical path:**
  1. Reserve a validation set (e.g., 5,000 samples) that is never used for fine-tuning or PPI
  2. Run ramp-up procedure with increasing fine-tuning subsets to estimate scaling law
  3. Solve for optimal allocation s*/n
  4. Fine-tune on s* samples, run PPI on remaining n−s* samples
  5. Report estimate and confidence interval

- **Design tradeoffs:**
  - Validation set size vs. estimation precision: Larger validation sets improve scaling law estimates but reduce available data for training/inference
  - Ramp-up stages vs. compute cost: More stages improve allocation accuracy but require additional training runs
  - Model capacity vs. scaling law extrapolation: The paper uses Qwen3-Embedding-0.6B; larger models may have different scaling exponents

- **Failure signatures:**
  - R² of scaling law fit < 0.95 suggests unstable scaling; do not rely on allocation rule
  - Condition (6) not satisfied (b/σ² too high): FT+PPI will not beat sample mean regardless of allocation
  - Residual variance does not decrease with more fine-tuning data: scaling law assumption violated

- **First 3 experiments:**
  1. Sanity check: On a held-out validation set, confirm that variance-based fine-tuning yields lower Var(Y−f(X)) than MSE-based fine-tuning across multiple subset sizes
  2. Scaling law validation: Plot residual variance vs. training data size on log-log axes; verify linearity and R² > 0.99
  3. End-to-end comparison: For fixed n, compare FT+PPI (variance loss) against FT+PPI (MSE loss), PPI-only, and sample mean in terms of estimator variance and confidence interval width across 20 replications

## Open Questions the Paper Calls Out

- **Question 1:** Is the data-driven "ramp-up" procedure for estimating scaling law parameters statistically optimal compared to adaptive or fixed designs?
  - **Basis in paper:** [explicit] The conclusion states that "the optimality of the data-driven ramp-up strategy could be further investigated."
  - **Why unresolved:** The paper proposes the procedure as a practical heuristic but does not derive theoretical guarantees for its efficiency or sample complexity.
  - **What evidence would resolve it:** Theoretical analysis comparing the regret or variance of the ramp-up estimator against an oracle or minimax optimal baseline.

- **Question 2:** How do transfer learning dynamics affect the scaling law parameters and optimal allocation when fine-tuning on independent but non-identical data?
  - **Basis in paper:** [explicit] The conclusion notes that "more theoretical and empirical analysis of such transfer-learning dynamics remains to be explored" regarding the extension in Appendix EC.4.
  - **Why unresolved:** While the framework accommodates external data by shifting parameters, it does not model how distribution shifts specifically influence the exponent α or noise floor b.
  - **What evidence would resolve it:** Empirical studies showing the relationship between domain shift magnitude and the resulting scaling law parameters in the target task.

- **Question 3:** How should the sample allocation strategy change if the cost of LLM inference for the unlabeled dataset is non-trivial?
  - **Basis in paper:** [inferred] The variance derivation (Eq. 2) assumes the unlabeled dataset size m → ∞, justified by the assumption that unlabeled data and inference costs are "negligible."
  - **Why unresolved:** If LLM inference costs are significant (e.g., using proprietary large models), the optimal trade-off between fine-tuning samples s and unlabeled inference count m is undefined.
  - **What evidence would resolve it:** A modified optimization framework that minimizes total cost (labeling + inference) subject to a variance constraint, validated in a scenario with expensive inference.

## Limitations

- The framework relies critically on the assumption that residual variance follows a predictable power-law scaling relationship, which may not generalize across tasks with different data distributions or noise characteristics.
- The method assumes a representative validation set for scaling law estimation, which may be challenging to obtain in practice when labeled data is scarce.
- The optimal allocation depends on the noise floor parameter b, which could be high for certain tasks, potentially limiting the benefits of the approach.

## Confidence

**High Confidence:**
- The variance-based fine-tuning objective is optimal for downstream PPI rectification
- The scaling law Var(Y-f(X)) ≈ as^(-α) + b provides a good fit for the Wine Reviews dataset
- The theoretically optimal allocation rule s*/n ≈ 10.3% closely matches empirical performance

**Medium Confidence:**
- The scaling law parameters are stable and generalizable across different text regression tasks
- The 45%-66% sample savings and 18%-54% variance reduction will translate to other domains
- The method outperforms MSE-based fine-tuning consistently across different LLM architectures

## Next Checks

1. **Cross-Domain Scaling Law Validation**: Test the scaling law estimation procedure on 2-3 additional text regression datasets (e.g., Amazon product reviews, IMDb ratings) to verify that R² > 0.95 holds consistently across domains.

2. **Allocation Sensitivity Analysis**: Systematically vary the validation set size used for scaling law estimation (5% to 20% of total data) and measure the impact on final estimator variance to determine sensitivity to validation set representativeness.

3. **Noise Floor Impact Study**: Create synthetic datasets with controlled noise levels and explicitly verify Proposition 3's condition b/σ² to identify when FT+PPI outperforms the sample mean estimator versus when it does not.