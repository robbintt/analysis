---
ver: rpa2
title: "R\xE9sum\xE9 abstractif \xE0 partir d'une transcription audio"
arxiv_id: '2504.11803'
source_url: https://arxiv.org/abs/2504.11803
tags:
- pour
- dans
- plus
- donn
- apprentissage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes an end-to-end (E2E) audio summarization model
  using large language models and parameter-efficient fine-tuning techniques. The
  author studies Low-Rank Adaptation (LoRA) and Adaptive LoRA (AdaLoRA) for text summarization,
  demonstrating that AdaLoRA outperforms LoRA and approaches the performance of fully
  fine-tuned models.
---

# Résumé abstractif à partir d'une transcription audio

## Quick Facts
- arXiv ID: 2504.11803
- Source URL: https://arxiv.org/abs/2504.11803
- Authors: Ilia Derkach
- Reference count: 35
- This work proposes an end-to-end audio summarization model using large language models and parameter-efficient fine-tuning techniques, demonstrating that AdaLoRA outperforms LoRA and approaches full fine-tuning performance.

## Executive Summary
This work proposes an end-to-end (E2E) audio summarization model using large language models and parameter-efficient fine-tuning techniques. The author studies Low-Rank Adaptation (LoRA) and Adaptive LoRA (AdaLoRA) for text summarization, demonstrating that AdaLoRA outperforms LoRA and approaches the performance of fully fine-tuned models. For automatic speech recognition, the author fine-tunes Whisper models with quantization (int4 and int8) and AdaLoRA, showing significant memory compression and improved performance compared to full fine-tuning. The study identifies int8+AdaLoRA as optimal for large models. The resulting E2E model is expected to offer lower memory footprint, faster training, and reduced computational requirements compared to existing cascade approaches.

## Method Summary
The study fine-tunes MBart/T5 with LoRA and AdaLoRA on attention weights for text summarization, and fine-tunes Whisper models with quantization (int4/int8) and AdaLoRA for automatic speech recognition. For summarization, optimal ranks are r≈16-32, trained for 5-7 epochs. For ASR, learning rates are 40× smaller than original Whisper paper (e.g., large: 4.375×10⁻⁶, batch 1), with 6 epochs of training. The E2E model is proposed but not yet implemented.

## Key Results
- AdaLoRA achieves ROUGE-1 scores of 33.4 vs 24.1 for LoRA on Gazeta dataset
- int8+AdaLoRA yields WER of 9.8 vs 11.3 with int8 alone for Whisper Large (1550M params)
- The study identifies int8+AdaLoRA as optimal for large models

## Why This Works (Mechanism)

### Mechanism 1
AdaLoRA outperforms standard LoRA for text summarization by adaptively allocating parameter budgets across attention layers based on importance scores. AdaLoRA parameterizes adapters using SVD-like decomposition (ΔW = PΛQ) and dynamically prunes less important singular values during training via importance scoring, while adding orthogonality regularization to maintain stable representations. Core assumption: Not all attention layers contribute equally to task performance; some require higher-rank updates than others. Evidence: Tables 3-4 show AdaLoRA achieving ROUGE-1 scores of 33.4 vs 24.1 for LoRA on Gazeta dataset. Break condition: If orthogonality regularization term (γ) is too high, optimization may become unstable.

### Mechanism 2
int8 quantization combined with AdaLoRA provides optimal memory-performance tradeoff for large Whisper ASR models. 8-bit quantization reduces model weights from 32-bit floats while preserving sufficient precision; AdaLoRA adapters trained in higher precision (BF16) compensate for quantization-induced errors through low-rank corrections. Core assumption: Quantization noise can be corrected by low-rank adapters without requiring full-precision gradient computation through base weights. Evidence: Table 8 shows Whisper Large (1550M params) achieves WER of 9.8 with int8+AdaLoRA vs 11.3 with int8 alone. Break condition: If quantization granularity is too coarse, adapter corrections may be insufficient for accurate recovery.

### Mechanism 3
End-to-end audio summarization should theoretically capture prosodic and acoustic information that cascade approaches lose. Unified encoder-decoder framework jointly optimizes acoustic and linguistic representations, allowing the model to leverage intonation, pauses, and speaker emotion as contextual signals. Core assumption: Prosodic features provide meaningful signal for summarization quality beyond lexical content alone. Evidence: The paper states E2E systems can potentially capture "des informations plus riches" and cascade systems suffer from "incapacité à utiliser des informations non verbales et acoustiques". Break condition: If paired audio-summary training data is insufficient, E2E models may underperform cascade systems despite theoretical advantages.

## Foundational Learning

- Concept: **Low-Rank Matrix Decomposition**
  - Why needed here: Core to understanding how LoRA/AdaLoRA reduce trainable parameters by factor ~100× (n·k vs (n+k)·r)
  - Quick check question: Given a weight matrix W of size 512×768, how many parameters does a rank-16 LoRA adapter add?

- Concept: **Quantization-Aware Fine-Tuning**
  - Why needed here: Explains why QLoRA dequantizes to BF16 for computation while storing in NF4/int8; gradient computation requires higher precision
  - Quick check question: Why must LoRA adapters remain in full precision while base weights are quantized?

- Concept: **Transformer Self-Attention Mechanics**
  - Why needed here: LoRA/AdaLoRA specifically target Q, K, V projection matrices (W^Q, W^K, W^V)
  - Quick check question: Which attention weight matrices does LoRA modify, and why not FFN layers?

## Architecture Onboarding

- Component map: Audio Input → [Whisper Encoder-Decoder + int8+AdaLoRA] → Transcript → [T5/MBart + AdaLoRA] → Summary
- Critical path:
  1. Select Whisper model size based on memory constraints (Medium recommended: 769M params, WER 11.3 with int8+AdaLoRA)
  2. Apply int8 quantization + AdaLoRA with learning rate 6.25×10⁻⁶ (40× lower than paper defaults)
  3. Fine-tune T5/MBart with AdaLoRA r=16–32 for 5–7 epochs
- Design tradeoffs:
  - int4 vs int8: int4 gives ~65% compression but lower quality; int8+AdaLoRA optimal for large models
  - LoRA vs AdaLoRA: AdaLoRA consistently superior but requires SVD-based parameterization and importance scoring
  - Model size: Small→Medium transition shows qualitative jump; Medium offers best quality/memory balance
- Failure signatures:
  - Large models (Medium, Large) show initial WER degradation in early epochs without AdaLoRA (information forgetting)
  - Learning rates from original papers are 40× too high for fine-tuning; use Table 5 values
  - ROUGE metrics plateau at r≈32; higher ranks show diminishing or negative returns
- First 3 experiments:
  1. Validate AdaLoRA superiority: Fine-tune MBart on Gazeta with LoRA (r=16) vs AdaLoRA; expect ROUGE-1 gap of ~9 points
  2. Quantization ablation: Train Whisper-Medium with int4+AdaLoRA vs int8+AdaLoRA; expect WER gap of ~4 points
  3. Rank sensitivity: Sweep r∈{4,8,16,32,64} for T5 on WikiHow; identify optimal r≈16–32 with plateau thereafter

## Open Questions the Paper Calls Out

### Open Question 1
Can the independently fine-tuned ASR (Whisper) and text summarization (T5/MBart) components be effectively integrated into a unified End-to-End (E2E) model that retains the memory and efficiency benefits of the PEFT techniques? The conclusion states that the main objective for the immediate future is to train the final proposed E2E model. Unresolved because the current work evaluates components in isolation; evidence would require successful training and benchmarking of the combined architecture on the How2 dataset.

### Open Question 2
Does the inclusion of visual features (video frames) significantly improve the quality of audio summarization compared to the audio-only approach? In the "Future Works" section, the author identifies adding "video images of the event" as a key axis for improving model quality. Unresolved because the current study is restricted to audio signals and text transcripts; evidence would require a comparative ablation study showing improved ROUGE scores for a multimodal model over the audio-only baseline.

### Open Question 3
Do simultaneous quantization-aware fine-tuning methods (such as LoftQ) offer superior performance or compression compared to the sequential application of quantization and AdaLoRA utilized in this study? The author suggests exploring new approaches, specifically citing literature on simultaneous quantization and adapter use, to further improve compression. Unresolved because while the paper establishes int8+AdaLoRA as optimal for sequential application, it does not test joint optimization strategies; evidence would require benchmarks demonstrating that LoftQ achieves lower WER or higher ROUGE scores at equivalent or lower bit-widths.

## Limitations
- Critical implementation details for AdaLoRA parameter budgeting, importance scoring, and regularization coefficients are not provided
- Theoretical advantages of E2E audio summarization remain untested against cascade approaches
- Weak corpus support for PEFT comparisons; neighboring papers focus on ASR/AMT rather than parameter-efficient fine-tuning validation

## Confidence

- **High confidence**: int8+AdaLoRA provides optimal memory-performance tradeoff for large Whisper models (WER 9.8 vs 11.3 with int8 alone on 1550M params)
- **Medium confidence**: AdaLoRA consistently outperforms LoRA for text summarization (ROUGE-1 33.4 vs 24.1 on Gazeta), though exact mechanisms are underspecified
- **Low confidence**: Theoretical benefits of E2E audio summarization capturing prosodic information beyond cascade approaches, as no direct comparison is provided

## Next Checks

1. Implement AdaLoRA with SVD-based parameterization and importance scoring on Gazeta dataset; verify ROUGE-1 gap of ~9 points over LoRA
2. Conduct systematic ablation study comparing int4 vs int8 quantization on Whisper-Medium; confirm ~4-point WER difference
3. Perform rank sensitivity analysis (r∈{4,8,16,32,64}) on T5 summarization to identify optimal r≈16-32 with diminishing returns beyond