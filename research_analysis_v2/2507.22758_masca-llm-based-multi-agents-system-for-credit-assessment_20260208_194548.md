---
ver: rpa2
title: 'MASCA: LLM based-Multi Agents System for Credit Assessment'
arxiv_id: '2507.22758'
source_url: https://arxiv.org/abs/2507.22758
tags:
- credit
- financial
- data
- risk
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MASCA, a hierarchical multi-agent system for
  credit assessment that outperforms baseline methods with 60% accuracy, 83.33% recall,
  and 73.33% F1 score. It employs specialized LLM-based agents to mirror real-world
  credit evaluation processes, with each agent focusing on distinct sub-tasks like
  data preprocessing, risk modeling, and reward analysis.
---

# MASCA: LLM based-Multi Agents System for Credit Assessment

## Quick Facts
- arXiv ID: 2507.22758
- Source URL: https://arxiv.org/abs/2507.22758
- Authors: Gautam Jajoo; Pranjal A Chitale; Saksham Agarwal
- Reference count: 32
- Primary result: Hierarchical multi-agent credit assessment system achieving 60% accuracy, 83.33% recall, 73.33% F1 score

## Executive Summary
MASCA introduces a hierarchical multi-agent system that decomposes credit assessment into specialized LLM-based agents, achieving superior performance over single-agent approaches. The system employs a 4-layer architecture where each layer handles distinct sub-tasks: data preprocessing, risk modeling, reward analysis, and final decision orchestration. By integrating signaling game theory and contrastive learning principles, MASCA creates a balanced evaluation framework that outperforms baseline methods while providing insights into gender and ethnicity biases in loan approvals.

## Method Summary
MASCA is a hierarchical multi-agent system designed for binary credit assessment using the German Credit Dataset. The 4-layer architecture consists of specialized agents: Data Analyst, Contextualizer, and Feature Engineer in Layer 1; Risk Modeler, Income & Stability Analyst, Debt Analyst, and Reward Modeler in Layer 2; Risk-Reward Optimizer in Layer 3; and Decision Orchestrator in Layer 4. The system uses GPT-4o for specialized analysis tasks and o3-mini for final decision-making, employing contrastive learning principles between risk and reward assessment teams. The method achieves 60% accuracy, 83.33% recall, and 73.33% F1 score on 200 test samples.

## Key Results
- 60% accuracy on German Credit Dataset binary classification task
- 83.33% recall with 73.33% F1 score, outperforming single-agent and zero-shot baselines
- Hierarchical structure improves performance by 7.77% accuracy over single-level systems
- Identifies gender bias: 65.22% accuracy for male-labeled data vs 58.26% for female-labeled data

## Why This Works (Mechanism)

### Mechanism 1
Task decomposition into specialized agents improves credit assessment performance over single-agent or zero-shot approaches. The hierarchical structure separates concerns—data preprocessing, risk assessment, reward evaluation, and final orchestration—allowing each agent to specialize rather than context-switch. Later layers receive structured "signals" from earlier layers, reducing error propagation compared to end-to-end reasoning chains.

### Mechanism 2
Contrasting risk vs. reward assessments creates a balanced decision signal. The Multidimensional Assessment Layer runs parallel Risk Assessment Team (3 agents) and Reward Assessment Team (1 agent) with opposing objectives. The Strategic Optimization Layer synthesizes these into a risk-reward ratio, preventing single-perspective bias.

### Mechanism 3
Heterogeneous model assignment (GPT-4o for agents, o3-mini for orchestration) improves over homogeneous setups. GPT-4o handles specialized analysis tasks while o3-mini, described as better at reasoning, serves as Decision Orchestrator. This exploits model-specific strengths.

## Foundational Learning

- **Signaling Game Theory**: Why needed here: The paper frames hierarchical agent communication as a signaling game where higher-level agents (senders) with private information signal to lower-level agents (receivers) who update beliefs and act. Quick check: Can you explain why information asymmetry between layers could lead to strategic (non-truthful) signaling, and how the paper's framework might prevent this?

- **Contrastive Learning in Multi-Agent Systems**: Why needed here: The risk-reward teams are explicitly inspired by contrastive learning—learning by comparing opposing perspectives. Quick check: How would you verify that the risk and reward scores are genuinely contrastive rather than correlated?

- **LLM Agent Specialization vs. Generalization**: Why needed here: The paper's core claim rests on specialized agents outperforming generalist single agents. Quick check: What metrics would you track to determine if an agent is genuinely specializing versus just receiving narrower prompts?

## Architecture Onboarding

- **Component map**: Data Analyst → Contextualizer → Feature Engineer → (Risk Modeler, Income & Stability Analyst, Debt Analyst || Reward Modeler) → Risk-Reward Optimizer → Decision Orchestrator

- **Critical path**: Raw application → Data Analyst (formatting) → Feature Engineer (ratios) → Risk/Reward Teams (parallel scoring) → Risk-Reward Optimizer (ratio synthesis) → Decision Orchestrator (final call)

- **Design tradeoffs**: Depth vs. latency: 4 layers add processing time but claim higher F1; ablation shows 2-level achieves 53.77% accuracy vs. single-level 46%. Homogeneous vs. heterogeneous models: Heterogeneous improves accuracy by ~7-9% but increases API complexity and cost. Risk-recall vs. precision: System tuned for high recall (83.33%) at precision cost (65.48%), appropriate for identifying creditworthy applicants but may over-approve.

- **Failure signatures**: Gender bias: 65.22% accuracy (male) vs. 58.26% (female-labeled same data)—7% disparity from gender attribute alone. Ethnicity bias: Asian applicants at 52.5% accuracy vs. 60% ground truth, near disparate impact threshold. CoT baseline failure: 36% accuracy suggests complex reasoning chains without structure propagate errors.

- **First 3 experiments**: 1. Reproduce ablation: Run single-level, two-level, and full 4-level hierarchy on held-out samples to verify hierarchical gains are not dataset-specific. 2. Bias audit: Test approval rates with gender/ethnicity attributes masked vs. present across all 4 layers to isolate which layer introduces bias. 3. Model swap test: Replace GPT-4o with an open-source model (e.g., LLaMA) in agent roles while keeping o3-mini as orchestrator to test robustness of heterogeneous routing claim.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the MASCA framework perform when implemented with open-source models compared to proprietary ones? Basis: Authors state experiments were conducted only with GPT models and extending to open-source models like LLaMA is needed.

- **Open Question 2**: Which specific proxy features cause performance disparities when protected attributes (gender/ethnicity) are removed? Basis: Bias analysis notes accuracy drops even when gender information is removed, suggesting other features contribute to disparity, but these are not isolated.

- **Open Question 3**: Can data synthesis techniques effectively replicate complex credit scenarios to validate the framework further? Basis: Authors note limited availability of credit assessment data and explicitly aim to explore data synthesis techniques to enhance robustness.

- **Open Question 4**: Does the hierarchical signaling mechanism empirically converge to a Perfect Bayesian Equilibrium? Basis: Paper claims system guides agents toward Perfect Bayesian Equilibrium via signaling games, but experiments only report classification metrics rather than convergence metrics.

## Limitations

- Evaluation limited to German Credit Dataset with 200 test samples, constraining generalizability to other credit scenarios and geographies
- Critical orchestration and aggregation mechanisms underspecified, preventing full replication without implementation details
- Performance gains rely on proprietary models (GPT-4o, o3-mini) without validation using open-source alternatives

## Confidence

**High Confidence**: Hierarchical decomposition concept is logically sound and supported by ablation results (53.77% vs. 46% accuracy for 2-level vs. single-level). Bias analysis methodology is transparent and reproducible.

**Medium Confidence**: Heterogeneous model routing claim is supported by direct comparison but lacks corpus validation. Contrastive learning integration is theoretically justified but not empirically benchmarked against other multi-objective frameworks.

**Low Confidence**: Generalization to other credit datasets or domains remains untested. Long-term robustness to model API changes or concept drift is unaddressed.

## Next Checks

1. **Cross-Dataset Validation**: Evaluate MASCA on at least two additional credit datasets (e.g., Home Credit Default Risk, Lending Club) to test generalizability beyond the German Credit Dataset.

2. **Open-Source Model Swap**: Replace GPT-4o with LLaMA or Mistral in agent roles while keeping o3-mini as orchestrator to verify heterogeneous routing benefits are not model-specific.

3. **Bias Attribution Audit**: Mask gender/ethnicity attributes at each layer and measure accuracy changes to isolate which layer(s) introduce bias, then test mitigation strategies like debiased fine-tuning or adversarial training.