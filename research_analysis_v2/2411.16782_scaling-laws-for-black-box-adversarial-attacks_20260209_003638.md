---
ver: rpa2
title: Scaling Laws for Black box Adversarial Attacks
arxiv_id: '2411.16782'
source_url: https://arxiv.org/abs/2411.16782
tags:
- scaling
- attack
- adversarial
- laws
- default
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the scaling behavior of transfer-based
  black-box adversarial attacks by systematically increasing the number of surrogate
  models in an ensemble. The authors propose that when gradient conflicts are properly
  resolved using advanced optimizers like CW A, the attack success rate (ASR) exhibits
  a robust and universal log-linear scaling law: ASR scales linearly with the logarithm
  of the ensemble size T.'
---

# Scaling Laws for Black box Adversarial Attacks

## Quick Facts
- arXiv ID: 2411.16782
- Source URL: https://arxiv.org/abs/2411.16782
- Authors: Chuan Liu; Huanran Chen; Yichi Zhang; Jun Zhu; Yinpeng Dong
- Reference count: 40
- Key outcome: Transfer-based black-box attacks exhibit robust log-linear scaling of attack success rate with ensemble size when gradient conflicts are resolved using advanced optimizers

## Executive Summary
This paper investigates the scaling behavior of transfer-based black-box adversarial attacks by systematically increasing the number of surrogate models in an ensemble. The authors propose that when gradient conflicts are properly resolved using advanced optimizers like CW A, the attack success rate (ASR) exhibits a robust and universal log-linear scaling law: ASR scales linearly with the logarithm of the ensemble size T. This is empirically validated across diverse target families including standard classifiers, SOTA defenses, and multimodal large language models (MLLMs), with a dataset of 56,000 generated adversarial examples. The attack distills robust, semantic features of the target class as T increases. Applying this insight to attack vision encoders of SOTA MLLMs, the authors achieve devastatingly high ASR (e.g., 90% on Qwen3-VL-235B, 85% on GPT-4o) while revealing a clear robustness hierarchy, with Claude-3.5-Sonnet showing exceptional resilience (31% ASR). The findings suggest a paradigm shift from designing intricate algorithms on small ensembles to understanding the principled threat of scaling for robustness evaluation.

## Method Summary
The authors propose SSA-CWA (Spectrum Simulation Attack + Common Weakness Attack) as a scalable black-box attack framework. The method uses an ensemble of T surrogate models, applies spectrum transformation to simulate frequency variations, and employs CWA optimizer to resolve gradient conflicts and find flat minima. The optimization minimizes a loss function that encourages transferability across the ensemble while maintaining perturbation constraints. The key insight is that naive logit averaging causes gradient norm collapse as T increases, while CWA preserves the gradient signal by aligning gradients and seeking common vulnerabilities across models.

## Key Results
- Attack success rate (ASR) exhibits robust log-linear scaling with ensemble size T: ASR ∝ log(T)
- CWA optimizer resolves gradient conflicts that cause logit-averaging (MI-FGSM) to fail at scale
- Scaling forces perturbations to evolve from unstructured noise into semantic features of the target class
- Applied to MLLMs, achieved 90% ASR on Qwen3-VL-235B and 85% on GPT-4o, revealing robustness hierarchy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Naïve model ensembling (logit averaging) causes optimization stagnation at scale; resolving gradient conflict is a prerequisite for scaling.
- **Mechanism:** As the ensemble size (T) increases, gradients from diverse models tend to cancel each other out (random directions), causing the ensemble gradient norm to collapse. Advanced optimizers like Common Weakness Attack (CWA) align gradients or seek flat minima, preserving the gradient signal.
- **Core assumption:** Surrogate models, despite different architectures, share a "common weakness" or vulnerability manifold that can be isolated from model-specific noise.
- **Evidence anchors:**
  - [Abstract]: "by resolving gradient conflict with advanced optimizers, we discover a robust and universal log-linear scaling law..."
  - [Section 2.2.3]: "As T increases... gradients... tend to cancel each other out... causing the ensemble gradient norm to collapse."
  - [Corpus]: Limited direct support in provided corpus for the specific gradient conflict mechanism, though "GreedyPixel" and "Q-FAKER" discuss fine-grained black-box generation strategies.
- **Break condition:** Using standard logit-averaging (MI-FGSM) with large T (e.g., T > 16) will result in near-zero gradient updates and fail to improve ASR.

### Mechanism 2
- **Claim:** Scaling ensemble size forces perturbations to evolve from unstructured noise into semantic features of the target class.
- **Mechanism:** Optimizing against many models simultaneously suppresses "non-robust" features (specific to one model) and amplifies "robust" semantic features (shared across the dataset/manifold). The perturbation effectively learns the visual concept of the target label.
- **Core assumption:** Adversarial transferability relies on shared robust features rather than shared non-robust artifacts.
- **Evidence anchors:**
  - [Abstract]: "scaling distills robust, semantic features of the target class."
  - [Section 5.2]: "As T increases to 64, the perturbation evolves to contain clear, interpretable, and semantic features of the target class."
  - [Corpus]: "Enabling Heterogeneous Adversarial Transferability" discusses feature permutation, aligning with the idea of isolating transferable features.
- **Break condition:** If the surrogate pool lacks diversity (e.g., only ResNets), the perturbation may overfit to architecture-specific artifacts rather than semantic features, reducing transferability to distant architectures (e.g., ViTs).

### Mechanism 3
- **Claim:** Attack Success Rate (ASR) exhibits a log-linear relationship with ensemble size (T) rather than a saturating exponential or polynomial curve.
- **Mechanism:** The paper empirically observes that ASR ∝ log(T). While Theorem 1 suggests a theoretical convergence rate of O(1/T) for loss under i.i.d. assumptions, the authors note these assumptions are unrealistic. The log-linear trend is an empirical discovery of how ASR behaves when optimizing over correlated, non-i.i.d. model manifolds.
- **Core assumption:** The relationship holds until ASR approaches saturation (near 1.0) or T is too small (high variance).
- **Evidence anchors:**
  - [Section 3.3]: "We identify a robust empirical phenomenon: the attack success rate (ASR) exhibits strong log-linear scaling laws with the ensemble cardinality T."
  - [Section 3.1]: The authors explicitly distinguish the theoretical O(1/T) motivation from the empirical log-linear reality ("The central question... remains open").
  - [Corpus]: Neighbors focus on attack success but do not verify the log-linear scaling law, suggesting this is a novel empirical finding in the target paper.
- **Break condition:** The linear trend on a log-scale flattens as ASR approaches 100% (saturation) or when attacking models with exceptional, non-standard robustness (e.g., Claude-3.5-Sonnet in the paper).

## Foundational Learning

- **Concept: Transfer-based Black-Box Attacks**
  - **Why needed here:** The entire paper operates on the premise that one can attack a "black-box" target (unknown weights) by generating attacks on "white-box" surrogates.
  - **Quick check question:** If I generate an attack on ResNet-50, why would it fool a ViT-B/16? (Answer: Shared robust features/decision boundaries).

- **Concept: Gradient Conflict / Cosine Similarity**
  - **Why needed here:** Understanding why simply adding more models fails is central to the paper's motivation for using CWA.
  - **Quick check question:** If Model A wants to move the pixel left and Model B wants to move it right, what happens to the average gradient? (Answer: Cancellation/Stagnation).

- **Concept: Log-Linear Scaling**
  - **Why needed here:** Interpreting the results requires understanding that doubling the models yields a *linear* additive gain in success rate, implying diminishing returns in terms of compute vs. gain.
  - **Quick check question:** If doubling T from 2 to 4 gives +10% ASR, what gain do you expect from 32 to 64? (Answer: Roughly +10% ASR, assuming the law holds).

## Architecture Onboarding

- **Component map:**
  Surrogate Pool -> Spectrum Transform (SSA) -> CWA Loss Computation -> Momentum Gradient Descent -> Adversarial Perturbation

- **Critical path:**
  1. Sample subset of T surrogate models from the pool
  2. Apply Spectrum Transform (SSA) to input to simulate frequency variations
  3. Compute CWA loss (minimizing second-order Taylor series to find flat minima) instead of simple logit averaging
  4. Update perturbation via Momentum gradient descent
  5. Evaluate on black-box target (e.g., GPT-4o, Gemini)

- **Design tradeoffs:**
  - **Ensemble Size (T):** Higher T increases ASR log-linearly but increases GPU memory and compute linearly
  - **Optimizer:** CWA is computationally heavier than MI-FGSM but is strictly required for the scaling law to emerge (naive averaging collapses)
  - **Surrogate Diversity:** High diversity aids semantic distillation but may increase gradient conflict, necessitating the robust optimizer (CWA)

- **Failure signatures:**
  - **Stagnation:** ASR flatlines as T increases (indicates failure to resolve gradient conflict—likely using naive averaging)
  - **Overfitting:** High ASR on surrogates but low ASR on target (indicates model-specific non-robust features are being learned—need more diverse surrogates or stronger input transformations)
  - **Saturation:** The curve bends downward at high T (ASR nearing 100%)

- **First 3 experiments:**
  1. **Gradient Norm Validation:** Replicate Figure 2. Plot gradient norm vs. T for MI-FGSM vs. CWA. Confirm that CWA maintains norm while MI-FGSM collapses.
  2. **Small-Scale Scaling Law:** Use a small surrogate pool (N=10). Measure ASR on a standard classifier (e.g., ConvNeXt) while varying T from 1 to 10. Fit a line to ASR vs log(T).
  3. **Cross-Modal Transfer:** Use the 12 CLIP surrogates to attack an open-source VLM (e.g., LLaVA). Verify that perturbations generated with high T contain visible semantic patterns of the target class (sanity check visualization).

## Open Questions the Paper Calls Out
None

## Limitations
- The log-linear scaling law is empirically observed but lacks rigorous theoretical foundation
- The semantic feature distillation mechanism is primarily supported by qualitative visualization rather than quantitative metrics
- The computational overhead of CWA optimizer may limit practical applicability in resource-constrained scenarios

## Confidence
- **Log-linear scaling law (ASR ∝ log(T))**: Medium confidence - Strong empirical support across diverse targets, but theoretical foundation remains open
- **CWA optimizer necessity**: High confidence - Clear gradient norm collapse with MI-FGSM is demonstrated and explained mechanistically
- **Semantic feature distillation**: Low-Medium confidence - Qualitative evidence present but lacks quantitative validation of semantic robustness
- **Cross-modal transfer to MLLMs**: Medium confidence - Results impressive but MLLM robustness evaluation remains an emerging field

## Next Checks
1. **Theoretical Gap Analysis**: Rigorously test whether the log-linear relationship holds when relaxing the i.i.d. assumption. Systematically vary surrogate model correlation (using only similar architectures vs. highly diverse ones) to map the boundary conditions of the scaling law.

2. **Semantic Feature Quantification**: Develop quantitative metrics to measure the semantic content of adversarial perturbations (e.g., CLIP-based semantic similarity between perturbations and target class). Validate whether perturbations with higher semantic content achieve better cross-modal transferability.

3. **Computational Cost-Benefit Analysis**: Benchmark CWA against alternative gradient conflict resolution strategies (e.g., gradient clipping, adaptive learning rates) to determine whether the performance gains justify the computational overhead, particularly for resource-constrained applications.