---
ver: rpa2
title: The Effect of Document Summarization on LLM-Based Relevance Judgments
arxiv_id: '2512.05334'
source_url: https://arxiv.org/abs/2512.05334
tags:
- https
- judgments
- summarization
- relevance
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically compares full-document and summary-based
  LLM relevance judgments across three TREC datasets. Using GPT-4o and Llama-3.1-8B,
  the research evaluates agreement with human labels, system effectiveness, and ranking
  stability under different compression levels (80/120 tokens).
---

# The Effect of Document Summarization on LLM-Based Relevance Judgments

## Quick Facts
- **arXiv ID**: 2512.05334
- **Source URL**: https://arxiv.org/abs/2512.05334
- **Reference count**: 0
- **Primary result**: Summary-based judgments achieve comparable ranking stability to full-document judgments while reducing costs by up to 85%

## Executive Summary
This study systematically compares full-document and summary-based LLM relevance judgments across three TREC datasets. Using GPT-4o and Llama-3.1-8B, the research evaluates agreement with human labels, system effectiveness, and ranking stability under different compression levels (80/120 tokens). Results show that summary-based judgments achieve comparable stability in systems' ranking to full-document judgments, with 80-token summaries often performing best. GPT-4o maintains strong alignment with human judgments across modalities, while Llama-3.1-8B shows greater sensitivity to dataset complexity and compression. Summarization reduces token and financial overhead substantially, particularly in large collections like RAG-24, offering a scalable, cost-efficient alternative to full-document judging while preserving evaluation reliability.

## Method Summary
The study employs GPT-4o to generate document summaries at 80 and 120 token limits, then uses both GPT-4o and Llama-3.1-8B-Instruct to judge relevance using the UMBRELA zero-shot prompt. The methodology compares these judgments against human-labeled TREC qrels across three datasets (DL-19, DL-20, RAG-24) using agreement metrics (Cohen's κ, Krippendorff's α), effectiveness measures (NDCG@10, MAP), and ranking stability (Kendall's τ, Spearman's ρ, RBO). The approach systematically varies compression levels while holding the judging prompt constant to isolate the effect of semantic compression on LLM-based relevance assessment.

## Key Results
- Summary-based judgments preserve system ranking stability comparable to full-document judgments
- 80-token summaries frequently outperform 120-token summaries in ranking stability metrics
- GPT-4o maintains strong alignment with human judgments across both full-document and summary modalities
- Llama-3.1-8B shows greater sensitivity to dataset complexity and compression, with reduced stability
- Summarization reduces token and financial overhead by up to 85% in large collections like RAG-24

## Why This Works (Mechanism)

### Mechanism 1: Semantic Compression and Noise Filtering
Summarization improves LLM-human agreement by removing distracting context while preserving key relevance signals. The summarizer (GPT-4o) acts as a semantic filter, retaining essential content and discarding distractors, allowing the judge model to focus on the core query-document relationship. This "sharpening" of the signal works when the summary preserves critical evidence (who/what/where) necessary for relevance determination.

### Mechanism 2: Model Capacity and Stability Threshold
High-capacity models (GPT-4o) maintain stable label distributions under compression because they have greater capacity to maintain reasoning state even when context is compressed. Lower-capacity models (Llama-3.1-8B) lose the semantic buffer required to resolve ambiguity, leading to erratic label distributions and over-production of certain labels (e.g., "Highly Relevant").

### Mechanism 3: System-Level Ranking Preservation
Summary-based judgments preserve the relative ordering of IR systems because evaluation metrics like NDCG are comparative. If summarization bias is consistent across systems (e.g., slight score inflation for all), the rank of the systems remains stable, enabling reliable evaluation even with imperfect absolute scores.

## Foundational Learning

**Concept: Graded Relevance vs. Binary Relevance**
- Why needed: The paper evaluates on a 4-point scale (0-3) and binarizes (0 vs 1-3), making it critical to understand how compression collapses fine-grained distinctions
- Quick check: If a summary causes a model to rate a document "2" instead of "3", does this affect Cohen's Kappa more in the graded setting or the binary setting (where both map to 1)?

**Concept: Rank-Biased Overlap (RBO)**
- Why needed: The paper uses RBO to measure ranking stability, which gives more weight to the top of the ranking where search evaluation cares most
- Quick check: Why would RBO be low while Kendall's τ is high in a search evaluation context?

**Concept: Semantic Compression Fidelity**
- Why needed: The entire methodology relies on the summary being a faithful representation of the document
- Quick check: If a summary has high "Fluency" (G-Eval) but low "Consistency" (hallucinations), how would that specifically impact relevance judgments?

## Architecture Onboarding

**Component map:**
- TREC Qrels + Raw Documents -> GPT-4o Summarizer (80/120 tokens) -> GPT-4o/Llama-3.1-8B Judge -> Agreement/Ranking Metrics

**Critical path:**
1. Preprocessing: Generate Summ-80 and Summ-120 for all documents in the qrels
2. Inference: Run the Judge model on three modalities: Summ-80, Summ-120, Full-Doc
3. Analysis: Compare Judge labels against Human Qrels to compute Agreement and System Ranking stability

**Design tradeoffs:**
- 80 vs. 120 Tokens: 80 tokens offer ~42% cost savings and often better stability (filtering noise), but higher risk of dropping context
- Model Selection: GPT-4o is robust but expensive; Llama-3.1-8B is cheaper but unstable (less reliable for evaluation)

**Failure signatures:**
- Label Collapse: Llama-3.1-8B on RAG-24 showed strange distributions (e.g., missing label 1 entirely)
- Score Inflation: Summary-based judgments consistently scored systems higher than humans

**First 3 experiments:**
1. Sanity Check: Reproduce the "Label Distribution" (Table 1) for a small sample (50 docs) to ensure your Summarizer prompt is not hallucinating wildly
2. Cost/Latency Benchmark: Measure the end-to-end latency and token cost for Summ-80 + Judge vs Full-Doc + Judge on the RAG-24 collection size
3. Sensitivity Test: Take 10 documents where Full-Doc and Summ-80 judgments disagree significantly; manually inspect to see if the summary lost a key fact or if the full doc contained distracting noise

## Open Questions the Paper Calls Out

**Open Question 1:** Does using the same LLM for both summary generation and relevance judgment (self-summary) improve performance compared to cross-model summary usage? The study held the summary source constant, so it cannot determine if Llama's instability was due to model capacity or misalignment with the summary style.

**Open Question 2:** Is the reliability of summary-based judgments robust across different prompting strategies? The study used a constant UMBRELA prompt to ensure comparability, leaving open whether the observed stability is specific to this prompt or generalizes to other designs.

**Open Question 3:** Does the effectiveness of summary-based judging degrade for documents significantly longer than the MS MARCO passages used in this study? While results were positive for RAG-24, the specific interaction between extreme document length and semantic compression reliability was not explicitly isolated.

## Limitations
- All datasets derive from the MS MARCO corpus, limiting generalization to domains with different document structures
- The study uses a single summarizer (GPT-4o) and judging model (GPT-4o), limiting understanding of how results transfer to other LLM architectures
- Economic analysis gaps exist regarding the trade-off between reduced judgment costs and potential accuracy degradation

## Confidence

**High Confidence Claims:**
- Summary-based judgments preserve system ranking stability across different compression levels
- GPT-4o maintains consistent performance across full-document and summary modalities
- 80-token summaries often outperform 120-token summaries in ranking stability

**Medium Confidence Claims:**
- Summary-based judgments achieve comparable agreement with human labels to full-document judgments
- The 80/120 token compression level is optimal for balancing cost and accuracy
- Llama-3.1-8B's instability is primarily due to model capacity limitations rather than prompt engineering

**Low Confidence Claims:**
- The generalizability of findings to non-MARCO domains
- The absence of significant bias differences between summary lengths across all datasets
- The complete characterization of failure modes for different document types

## Next Checks

1. **Cross-Domain Replication Test:** Apply the summary-based judgment methodology to a non-MARCO collection (e.g., TREC-COVID or biomedical datasets) to validate whether the observed stability and cost-effectiveness transfer to domains with different document structures and relevance criteria.

2. **Model Architecture Comparison:** Systematically compare summary-based judgments across different LLM architectures (Claude, Gemini, open-source alternatives) to determine whether the observed patterns are specific to GPT-4o or represent broader LLM behavior under semantic compression.

3. **Bias Characterization Study:** Conduct a detailed analysis of systematic biases introduced by different compression levels, examining whether certain document types (tables, code, mathematical content) are disproportionately affected and whether these biases correlate with specific IR system characteristics.