---
ver: rpa2
title: Sample-Efficient Optimization over Generative Priors via Coarse Learnability
arxiv_id: '2503.06917'
source_url: https://arxiv.org/abs/2503.06917
tags:
- assumption
- algorithm
- samples
- target
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses optimizing a global objective under local constraints
  using a generative prior, such as an LLM. It proposes ALDRIFT, an iterative algorithm
  combining Metropolis-Hastings sampling and model fine-tuning, guided by a temperature
  schedule.
---

# Sample-Efficient Optimization over Generative Priors via Coarse Learnability

## Quick Facts
- **arXiv ID:** 2503.06917
- **Source URL:** https://arxiv.org/abs/2503.06917
- **Reference count:** 26
- **Primary result:** ALDRIFT achieves polynomial sample complexity for optimizing under local constraints using generative priors by iteratively combining Metropolis-Hastings sampling with model fine-tuning guided by a temperature schedule.

## Executive Summary
This paper tackles the challenge of optimizing a global objective under local constraints when the constraint space is implicitly defined by a generative prior such as an LLM. The proposed ALDRIFT algorithm iteratively refines a proposal distribution by sampling via Metropolis-Hastings and fine-tuning the model on accepted samples, guided by a temperature schedule. Under a novel coarse learnability assumption—where a polynomial number of samples enables polynomial-factor density approximation—the method provably converges to the target distribution. Theoretical support covers both misspecified (e.g., Gaussian) and realizable (exponential family) settings, and empirical results with GPT-2 demonstrate that LLMs can iteratively learn combinatorial constraints from few samples, validating the coarse learnability assumption.

## Method Summary
The method optimizes a global objective under local constraints by iteratively refining a generative model. Starting with a base LLM L₀, the algorithm constructs intermediate distributions p_τ(s) ∝ L(s)·e^{-τ·d(s)} with τ increasing from 0 to T. At each iteration τ, it generates m samples via a Metropolis-Hastings chain (M steps each) using the previous model L_{τ-} as proposal, then fine-tunes L_{τ-} on these samples to obtain L_τ. The M-H correction ensures samples converge to the true target despite model approximation errors. A simpler heuristic TOPIFT is also proposed, which directly selects top-m samples without M-H correction. The key insight is that maximum likelihood estimation naturally produces "coverage envelopes" satisfying coarse learnability, enabling polynomial sample complexity even under model misspecification.

## Key Results
- ALDRIFT provably approximates the target distribution using polynomially many samples under coarse learnability
- Maximum likelihood estimation naturally induces coverage properties for both exponential families and misspecified models
- GPT-2 can iteratively learn spanning tree and scheduling constraints from few samples, validating coarse learnability empirically
- The algorithm handles both the sampling and optimization challenges in a unified framework

## Why This Works (Mechanism)

### Mechanism 1: Temperature Annealing Prevents Coverage Gaps
Gradual temperature annealing prevents exponential coverage gaps from compounding across iterations. The algorithm constructs intermediate distributions p_τ(s) ∝ L(s)·e^{-τ·d(s)} with τ increasing from 0 to T in steps of 1/D (where D = max_s d(s)). Since d(s) ∈ [0,D], each successive target differs from the previous by at most a factor of e, ensuring polynomially bounded density ratios between consecutive distributions. Coarse learnability guarantees the learned model maintains polynomial coverage of each intermediate target. If temperature steps are too large (> 1/D), density ratios between consecutive targets become exponentially large, breaking the polynomial coverage guarantee.

### Mechanism 2: M-H Correction Prevents Error Accumulation
Metropolis-Hastings correction prevents learning errors from accumulating across fine-tuning iterations. The learned model L_{τ-} serves as a proposal distribution. The M-H acceptance probability β_i = min(1, w_τ(ŝ_i)·L_{τ-}(s_{i-1}) / (L_{τ-}(ŝ_i)·w_τ(s_{i-1}))) ensures samples converge to the true target p_τ rather than the approximate model distribution. Crucially, the normalizing constant cancels, avoiding the need to estimate partition functions. If one were to draw samples S_τ directly from L_{τ-} and re-weight them by e^{-d(s)/D}, the resulting distribution would differ from p_τ by a multiplicative factor in KL divergence, causing this error to compound. If the proposal distribution L_{τ-} has zero probability on regions where p_τ has significant mass, the M-H chain cannot reach those regions.

### Mechanism 3: MLE Produces Coverage Envelopes
Maximum likelihood estimation naturally produces "coverage envelopes" that satisfy coarse learnability even under model misspecification. MLE minimizes forward KL divergence D_KL(p||q), which is "mass-covering" and "zero-avoiding"—it penalizes assigning zero probability to regions where the target has mass. This forces the learned model to spread probability mass across the target's support rather than collapsing to a single mode. For a Gaussian mixture target p(x) and single Gaussian learner, with probability at least 1-δ, the learned model satisfies the global coverage condition: sup_x p(x)/L_MLE(x) = O(Δ·e^k). If the model family is too restrictive to cover the target's support (e.g., unimodal learner on a widely-separated multimodal target), coverage ratios may be exponential.

## Foundational Learning

- **Model-Based Optimization / Cross-Entropy Method**: Why needed: ALDRIFT extends classical MBO theory from exponential families to deep generative models; understanding the CE method's iterative elite-selection and parameter updates is prerequisite. Quick check: Can you explain why the Cross-Entropy method updates parameters to maximize likelihood on elite samples?
- **Metropolis-Hastings Sampling**: Why needed: The M-H step is the algorithmic core that corrects for model approximation error; understanding acceptance probabilities and mixing time is essential. Quick check: Why does M-H only require relative likelihood ratios rather than normalized probabilities?
- **Total Variation and KL Divergence**: Why needed: The paper bounds convergence using d_tv(·,·) and relates coverage to KL divergence properties; these metrics quantify distributional approximation quality. Quick check: Why is forward KL D_KL(p||q) "mass-covering" while reverse KL D_KL(q||p) is "mode-seeking"?

## Architecture Onboarding

- **Component map:** Generative Prior L₀ -> Temperature Scheduler -> M-H Sampler -> Fine-tuning Module -> L_τ
- **Critical path:**
  1. Initialize L₀ ← L (base model)
  2. For each τ ∈ {1, ..., T·D}:
     a. Generate m samples via M-H chain (M steps each) targeting p_τ
     b. Fine-tune L_{τ-} on these m samples to obtain L_τ
  3. Return L_T (samples from L_T approximate p_T)
- **Design tradeoffs:**
  - Sample budget m: Larger m improves coarse learnability probability but increases compute; paper uses m = poly(T, D)
  - M-H chain length M: M = m² ensures mixing; shorter chains may not converge to target
  - Temperature granularity: Smaller steps (1/D) guarantee polynomial coverage ratios; larger steps risk exponential gaps
  - TOPIFT vs ALDRIFT: TOPIFT (Algorithm 1) is simpler but lacks formal guarantees; ALDRIFT provides convergence bounds but requires probability access
- **Failure signatures:**
  - Mode collapse: If fine-tuning collapses to a subset of modes, coverage breaks; check if L_τ assigns near-zero probability to samples with high p_τ weight
  - Non-mixing M-H: If acceptance rates are very low (< 1%), the proposal distribution poorly covers the target; increase m or check coarse learnability
  - Exponential coverage gaps: If d_tv(p_τ, L_τ) grows with τ rather than staying bounded, temperature steps may be too large
- **First 3 experiments:**
  1. Validate coarse learnability on synthetic data: Train a single Gaussian on samples from a 3-component Gaussian mixture; verify that sup_x p(x)/L(x) remains polynomial in sample size m
  2. Implement TOPIFT on line scheduling: Replicate the K=10 station experiment with visit bounds [1,20] and travel time t_i=10; confirm wait time decreases to 0 over ~8 iterations with m=4, M=12
  3. Test M-H necessity: Compare ALDRIFT against a variant that samples directly from L_{τ-} and reweights; measure whether approximation error compounds across iterations

## Open Questions the Paper Calls Out

### Open Question 1
Under what specific architectural or training conditions do modern Large Language Models (LLMs) formally satisfy the coarse learnability assumption? The paper concludes by asking to "formally characterize the class of coarse learners and determine under what conditions modern LLMs fall into this category." This remains unresolved because while the paper provides evidence for Gaussian mixtures and exponential families, and empirical evidence for GPT-2, establishing this for transformer-based models remains an open theoretical challenge. A theoretical proof linking transformer architecture properties or SGD dynamics to the polynomial coverage guarantees defined in Assumption 2 would resolve this.

### Open Question 2
Can the strict pointwise coverage requirement be relaxed to intermediate notions, such as coverage localized to specific sublevel sets, without sacrificing sample efficiency? The conclusion suggests "exploring relaxations of the strict pointwise coverage requirement... localized to specific sublevel sets" as a promising direction. This is unresolved because the current analysis relies on strict pointwise density ratios since Total Variation Distance is insufficient for tail-event sampling; a middle ground has not been formulated. Deriving sample complexity bounds for ALDRIFT under a weaker coverage metric (e.g., restricted to high-probability regions of the target) would resolve this.

### Open Question 3
Can more powerful optimization oracles, such as those providing gradients of the log-probability, be utilized to implement Langevin MCMC instead of Metropolis–Hastings? The conclusion identifies "exploring more powerful optimization oracles... could enable the use of Langevin MCMC in place of Metropolis–Hastings." This is unresolved because the current ALDRIFT algorithm is designed specifically for the zeroth-order setting (function evaluations only) and relies on M-H for correction. Convergence analysis and empirical benchmarks of a gradient-based variant of the algorithm comparing its efficiency to the zeroth-order approach would resolve this.

## Limitations
- Coarse learnability assumption remains largely empirical for deep generative models, with theoretical support limited to exponential families and Gaussian mixtures
- Temperature step size (1/D) is theoretically sound but may require careful tuning in practice to avoid exponential coverage gaps
- The M-H correction's effectiveness depends on maintaining polynomial coverage across iterations, which may fail if the model family cannot adequately represent the target's support
- Current analysis focuses on zeroth-order optimization; extension to gradient-based methods remains open

## Confidence
- **High confidence**: The algorithmic framework combining M-H sampling with iterative fine-tuning is sound and implementable
- **Medium confidence**: Theoretical guarantees hold for the specified model classes (exponential families, Gaussian mixtures) under coarse learnability
- **Medium confidence**: Empirical results demonstrate the approach works on small-scale combinatorial problems, though scaling to larger instances remains untested

## Next Checks
1. **Test coarse learnability empirically on structured data**: Fine-tune a small transformer on samples from a multi-modal distribution over combinatorial structures (e.g., trees with varying degree constraints) and measure whether coverage ratios remain polynomial in sample size
2. **Validate M-H necessity through ablation**: Implement a variant of ALDRIFT that samples directly from L_{τ-} without M-H correction and compare approximation error accumulation across temperature iterations
3. **Stress-test temperature scheduling**: Run ALDRIFT with varying temperature step sizes (1/D vs 2/D vs D/D) on the spanning tree problem to quantify the impact on convergence quality and identify breaking points where polynomial coverage guarantees fail