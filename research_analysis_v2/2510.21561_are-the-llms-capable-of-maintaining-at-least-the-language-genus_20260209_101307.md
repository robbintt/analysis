---
ver: rpa2
title: Are the LLMs Capable of Maintaining at Least the Language Genus?
arxiv_id: '2510.21561'
source_url: https://arxiv.org/abs/2510.21561
tags:
- language
- genus
- fidelity
- switchscores
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether Large Language Models (LLMs) exhibit
  sensitivity to genealogical language structure by analyzing their multilingual behavior
  on the MultiQ dataset. The study examines genus fidelity (whether models switch
  to genealogically related languages when prompt fidelity is not maintained) and
  knowledge consistency (whether correct answers in one language translate to correct
  answers in related languages).
---

# Are the LLMs Capable of Maintaining at Least the Language Genus?

## Quick Facts
- **arXiv ID**: 2510.21561
- **Source URL**: https://arxiv.org/abs/2510.21561
- **Reference count**: 0
- **Key result**: LLMs encode genealogical language structure, showing higher knowledge consistency within genera than across them, though training data imbalances remain the primary driver of multilingual performance.

## Executive Summary
This paper investigates whether Large Language Models exhibit sensitivity to genealogical language structure by analyzing their multilingual behavior on the MultiQ dataset. The study examines genus fidelity (whether models switch to genealogically related languages when prompt fidelity is not maintained) and knowledge consistency (whether correct answers in one language translate to correct answers in related languages). Results show that genus-level effects exist but are strongly conditioned by training resource availability, with models displaying distinct multilingual strategies across families. While LLMs encode aspects of genus-level structure, training data imbalances remain the primary factor shaping their multilingual performance.

## Method Summary
The study analyzes five open-weight language models (Llama-2, Mistral, Mixtral, Qwen, and Apertus) using the MultiQ dataset of 27,400 parallel questions across 137 languages. Model outputs are evaluated for language identification using GlotLID and answer correctness using GPT-4. Languages are mapped to 47 genera via the WALS database. Two metrics are computed: FidelityScore (genus match between prompt and output) and SwitchScore (knowledge transfer within vs. across genera). The analysis filters questions by minimum correct answers per genus (Nc=20, 50, 100) to ensure statistical reliability.

## Key Results
- All models show substantially higher knowledge consistency within genera (80-90%) compared to cross-genus transfers (40-50%)
- Training data imbalances are the primary factor shaping multilingual performance, with well-resourced genera serving as robust attractors
- Models display distinct fallback hierarchies when prompt fidelity fails (e.g., Llama defaults to English/Germanic; Qwen varies between Chinese and other languages)
- Apertus, with balanced multilingual pretraining, achieves the highest genus fidelity (92.3%) among all tested models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs encode genealogical language structure, enabling higher knowledge consistency within linguistic genera than across them.
- Mechanism: When an LLM answers correctly in one language, the same latent knowledge representation is more accessible in genealogically related languages due to shared structural, lexical, or tokenization patterns learned during pretraining.
- Core assumption: Genus-level clustering in representation space exists and is detectable via consistency metrics.
- Evidence anchors:
  - [abstract]: "We show that genus-level effects are present but strongly conditioned by training resource availability."
  - [section 4.2]: "All models show substantially higher knowledge consistency within genera (80-90%) compared to cross-genus transfers (40-50%). This 35-40 percentage point advantage demonstrates that genealogical relatedness significantly facilitates knowledge preservation."
  - [corpus]: Related work on cross-lingual collapse (arxiv:2506.05850) documents models reverting to dominant pretraining languages, suggesting language-family clustering in representations.
- Break condition: If training data is extremely imbalanced, target-language resource availability may dominate and suppress genus-level effects (observed for low-resource genera like Javanese, Kuki-Chin).

### Mechanism 2
- Claim: Training data distribution is the primary driver of multilingual performance, often overriding genealogical proximity.
- Mechanism: High-resource genera (e.g., Germanic, Romance) form robust attractor basins in representation space; models default to these when fidelity fails or when target-language resources are scarce.
- Core assumption: Resource availability correlates with representation quality and accessibility during generation.
- Evidence anchors:
  - [abstract]: "training data imbalances remain the primary factor shaping their multilingual performance."
  - [section 4.2]: "Well-resourced genera (Germanic, Romance) serve as robust targets regardless of source, while poorly resourced genera (e.g., Kuki-Chin) yield degraded performance even from high-resource sources."
  - [corpus]: Tokenization bias work (arxiv:2509.20045) shows dialect performance gaps tied to data and tokenization, consistent with resource-driven effects.
- Break condition: For genera with minimal or no training data, genus-level structure may not be encoded at all; models will fallback to any high-resource language regardless of genealogy.

### Mechanism 3
- Claim: Models develop family-specific fallback hierarchies when prompt language fidelity is not maintained.
- Mechanism: Language-switching behavior follows model-specific attractors shaped by pretraining composition (e.g., Llama → English/Germanic; Qwen → Chinese), not purely random or uniformly distributed.
- Core assumption: Fallback patterns reflect the interaction between prompt language characteristics and dominant training languages.
- Evidence anchors:
  - [section 3.2]: "We also observed a strong tendency of Llama models to default to English (a Germanic language) when confronted with non-English prompts."
  - [section 3.2]: "Mixtral almost always uses English while Qwen is more variable in fallbacks, sometimes producing outputs in unrelated languages (oftentimes in Chinese)."
  - [corpus]: Cross-lingual collapse paper (arxiv:2506.05850) describes chain-of-thought reverting to dominant pretraining language even with non-English prompts.
- Break condition: If a model has balanced multilingual pretraining (e.g., Apertus with ~40% non-English, 1,800 languages), fallback hierarchies flatten and fidelity increases (Apertus reaches 92.3% genus fidelity).

## Foundational Learning

- Concept: **Linguistic genus vs. family taxonomy**
  - Why needed here: The paper argues family-level grouping (e.g., Indo-European) is too coarse because it mixes high- and low-resource languages with divergent scripts and syntax. Genus provides the right granularity for detecting systematic multilingual patterns.
  - Quick check question: Can you explain why English and Hindi—both Indo-European—might show very different fidelity patterns in LLMs?

- Concept: **Language fidelity and generation language**
  - Why needed here: The paper measures whether models respond in the same language (or genus) as the prompt; fidelity is the core dependent variable.
  - Quick check question: If a model is prompted in Turkish but responds in English, what is the generation language and is fidelity maintained?

- Concept: **Knowledge consistency across languages**
  - Why needed here: The paper isolates whether correct answers in one language transfer to related languages within the same genus, independent of whether the model stays in the prompt language.
  - Quick check question: If a model answers correctly in French but incorrectly in Italian (same genus), what does this suggest about its cross-lingual knowledge representation?

## Architecture Onboarding

- Component map:
  - MultiQ dataset -> Model inference -> GlotLID language identification -> GPT-4 correctness assessment -> WALS genus mapping -> FidelityScore/SwitchScore computation

- Critical path:
  1. Prompt model in language L₁ (source) with question Q.
  2. Collect output, identify generation language L₂ via GlotLID.
  3. Map L₁ and L₂ to genera G₁, G₂ via WALS.
  4. Score fidelity (G₁ = G₂?) and correctness (answer match?).
  5. For switch analysis, filter to questions answered correctly in source, then compute SwitchScore across target genera.

- Design tradeoffs:
  - **Coarse vs. fine taxonomy**: Family-level is simpler but conflates diverse languages; genus-level is more informative but reduces per-category sample sizes.
  - **Filtering thresholds (Nc = 20, 50, 100)**: Higher thresholds improve statistical reliability but shrink dataset coverage (Table 2 shows genus count drops from 44 to 27 as Nc increases from 20 to 100 for Apertus).
  - **Model selection**: Open-weight models (Llama, Mistral, Qwen) have opaque training data; Apertus is fully open but may differ in architecture/size.

- Failure signatures:
  - Very low fidelity (<20%) with strong English bias indicates underrepresentation of prompt language in training (e.g., Llama-7B on most non-Germanic genera).
  - High fidelity but low accuracy suggests language is recognized but knowledge is not stored or accessible in that language.
  - Asymmetric switch scores (high A→B, low B→A) indicate target-resource dominance, not symmetric genus effects.

- First 3 experiments:
  1. **Baseline fidelity audit**: Run all models on a 50-language subset of MultiQ; compute per-genus FidelityScore to identify which genera each model handles reliably.
  2. **Controlled switch experiment**: For 5 genera (Germanic, Romance, Slavic, Turkic, Chinese), select 100 questions answered correctly in at least one language per genus; compute SwitchScore-In vs. SwitchScore-Out to quantify genus advantage.
  3. **Fallback hierarchy mapping**: For all non-faithful outputs, aggregate the distribution of generation genera by source genus; visualize model-specific fallback patterns (similar to Figure 3) to diagnose training-data attractors.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the independent effects of genealogical structure be disentangled from training resource availability and script similarity?
- Basis in paper: [explicit] The authors note that "genealogical classification does not perfectly predict transfer success" and lists "training data distribution, script similarity, and other linguistic factors" as competing influences (Section 4.2).
- Why unresolved: The study observes correlations but relies on existing datasets where these variables are confounded (e.g., high-resource languages often share specific scripts).
- What evidence would resolve it: Controlled experiments using low-resource languages with Latin scripts or artificially balanced training corpora to isolate structural effects from resource bias.

### Open Question 2
- Question: What mechanisms drive the asymmetry in cross-genus knowledge transfer, where performance depends more on the target genus than the source?
- Basis in paper: [explicit] The paper observes that "results are asymmetric," noting that transferring from Javanese to Germanic maintains high accuracy, while the reverse direction degrades (Section 4.2).
- Why unresolved: While the authors identify that target language representation dominates, they do not determine the internal model mechanisms causing this directional bias.
- What evidence would resolve it: Probing studies analyzing internal activation patterns during directional transfer tasks between asymmetric language pairs.

### Open Question 3
- Question: Can models be trained to prioritize genealogical coherence over resource-driven "English bias"?
- Basis in paper: [inferred] The conclusion states that "training data imbalances remain the primary factor" and observes that models like Llama systematically default to Germanic (English) outputs.
- Why unresolved: The study evaluates pre-trained models but does not explore training interventions to correct the observed lack of fidelity in low-resource genera.
- What evidence would resolve it: Evaluating genus fidelity in models specifically fine-tuned on phylogenetically balanced data.

## Limitations

- Data availability bias: The analysis excludes genera with fewer than the filtering threshold (Nc=20/50/100) correct answers, potentially missing systematic patterns in very low-resource languages.
- Model training data opacity: For open-weight models, training data composition remains proprietary, making it impossible to definitively attribute fallback hierarchies to specific training data characteristics.
- Language identification accuracy: GlotLID accuracy for low-resource languages or code-switched outputs is not reported, which could impact both fidelity and knowledge consistency metrics.

## Confidence

**High confidence**: The core finding that knowledge consistency is substantially higher within genera (80-90%) than across genera (40-50%) is supported by clear statistical separation and consistent patterns across multiple models and filtering thresholds.

**Medium confidence**: The claim that training data imbalances are the "primary factor" shaping multilingual performance is well-supported by the data showing well-resourced genera serving as attractors, but the counterfactual (what would happen with perfectly balanced training) cannot be tested.

**Low confidence**: The mechanism explanations for why specific fallback hierarchies emerge (e.g., why Llama strongly prefers English over other Germanic languages) remain speculative without knowledge of exact training data composition.

## Next Checks

1. **Edge-case language analysis**: Manually audit a stratified sample of 100 non-faithful outputs across 5 models, focusing on edge cases where language identification confidence is low or where genus mappings are ambiguous.

2. **Resource correlation validation**: Replicate the main analysis but stratify results by both genus and training data resource tier (high/medium/low based on external estimates) to test whether the genus advantage persists within each resource tier.

3. **Cross-dataset generalization**: Apply the same fidelity and knowledge consistency analysis to a different multilingual dataset (e.g., XSTEST or Flores-200) to verify that the genus-level patterns are not artifacts of MultiQ's specific question distribution or linguistic coverage.