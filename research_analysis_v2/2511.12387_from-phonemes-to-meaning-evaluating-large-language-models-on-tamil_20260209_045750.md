---
ver: rpa2
title: 'From Phonemes to Meaning: Evaluating Large Language Models on Tamil'
arxiv_id: '2511.12387'
source_url: https://arxiv.org/abs/2511.12387
tags:
- linguistic
- tamil
- language
- evaluation
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study introduces ILAKKANAM, the first Tamil-specific linguistic\
  \ benchmark built from 820 manually curated school-level examination questions.\
  \ The benchmark is annotated by linguists across five linguistic categories and\
  \ a factual knowledge category, covering Grades 1\u201313."
---

# From Phonemes to Meaning: Evaluating Large Language Models on Tamil

## Quick Facts
- **arXiv ID:** 2511.12387
- **Source URL:** https://arxiv.org/abs/2511.12387
- **Reference count:** 0
- **Primary result:** ILAKKANAM benchmark reveals performance declines with linguistic complexity; Gemini 2.5 leads, but disconnect between accuracy and classification suggests memorization over understanding.

## Executive Summary
This study introduces ILAKKANAM, the first Tamil-specific linguistic benchmark built from 820 manually curated school-level examination questions across Grades 1–13. The benchmark is annotated across five linguistic categories and a factual knowledge category, and evaluated using a standardized framework with both closed-source and open-source LLMs. Results show Gemini 2.5 achieves the highest overall performance, while open-source models lag behind. Performance declines with increasing linguistic complexity, and no strong correlation is found between overall accuracy and linguistic category classification, suggesting that performance may stem from exposure rather than genuine understanding.

## Method Summary
The study constructs ILAKKANAM by extracting 820 questions from Grade 1–13 Tamil examination papers, converting essay questions to multiple-choice format, and annotating them by linguists across five linguistic categories and a factual knowledge category. Models are evaluated using Abacus.AI's Unified API with temperature set to 0 for deterministic outputs. A secondary task asks models to classify each question's linguistic category before answering. Scores are calculated as percentage of correct responses, and manual review validates "valid alternative" answers.

## Key Results
- Gemini 2.5 achieves the highest overall performance (~80%) on the benchmark.
- Performance declines significantly between Grades 1–5 and Grades 10–13.
- No strong correlation exists between overall accuracy and linguistic category classification ability.
- Open-source models (Qwen 2.5 72B) lag significantly behind closed-source models despite similar parameter counts.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** High benchmark scores may reflect training data exposure rather than abstract linguistic competence.
- **Mechanism:** LLMs likely rely on pattern matching familiar question formats from web-scale corpora rather than executing meta-linguistic analysis.
- **Core assumption:** School-level examination questions appear in model training data with sufficient frequency to enable memorization or shallow pattern heuristics.
- **Evidence anchors:** No strong correlation between performance and linguistic category classification; Gemini 2.5's strong performance may stem from training data coverage rather than understanding.
- **Break condition:** If models were fine-tuned on entirely novel linguistic puzzles unseen in pre-training, performance would likely degrade to chance while classification ability might remain stable if true understanding existed.

### Mechanism 2
- **Claim:** Performance degrades non-linearly as morphological complexity and syntactic freedom increase.
- **Mechanism:** Tamil's agglutinative nature means meaning is encoded through complex suffix chains. Models handle simpler morphology well but fail as token-density of grammatical information increases.
- **Core assumption:** Current tokenizers and attention mechanisms struggle to bind discontinuous morphological dependencies in low-resource scripts.
- **Evidence anchors:** Performance declines with increasing linguistic complexity; Tamil described as agglutinative with rich morphological constructions.
- **Break condition:** If sub-word tokenization is replaced by morpheme-aware tokenization, performance on higher-grade/complexity tasks should theoretically stabilize.

### Mechanism 3
- **Claim:** Closed-source models outperform open-source models primarily due to proprietary multilingual data curation, not just parameter count.
- **Mechanism:** The performance gap suggests raw scale is insufficient without specific, high-quality exposure to Tamil linguistic structures.
- **Core assumption:** Large open-source models have lower effective Tamil token coverage compared to proprietary equivalents.
- **Evidence anchors:** Qwen 2.5 72B recorded the lowest score despite large parameter count; closed-source models dominate across categories.
- **Break condition:** If an open-source model is trained on a curated, high-quality Tamil corpus equal in density to closed-source training sets, the performance gap should close independent of parameter count.

## Foundational Learning

- **Concept: Agglutinative Morphology**
  - **Why needed here:** Tamil builds words by stacking suffixes, complicating tokenization and error analysis.
  - **Quick check question:** Does the model error involve a wrong word choice, or a wrong suffix on an otherwise correct root?

- **Concept: Surface Form vs. Linguistic Competence**
  - **Why needed here:** Getting the right answer doesn't mean the model "knows" the rule; evaluators must distinguish between valid reasoning and lucky retrieval.
  - **Quick check question:** If you change proper nouns to nonsense words, does the model still apply the correct grammatical structure?

- **Concept: Resource Scarcity (Low-Resource NLP)**
  - **Why needed here:** Despite 90M speakers, Tamil is "low-resource" in digital NLP terms, explaining why models fail on nuanced semantics despite general fluency.
  - **Quick check question:** Is the model failing because the task is hard, or because the specific linguistic phenomenon is under-represented in the training corpus?

## Architecture Onboarding

- **Component map:** JSON question set -> Abacus.AI Unified API -> Model output -> Score calculation -> Manual validation
- **Critical path:** 1. Load JSON question set 2. Prompt Model with standardized instruction (Zero-shot) 3. Extract response (JSON) 4. Calculate Score Percentage (SP) 5. Run Secondary Task: Prompt model to classify question category
- **Design tradeoffs:** MCQ vs. Generation: Loss of generative capability assessment; gain in automated scoring reliability. Manual vs. Automated Cleaning: Higher data fidelity; slower dataset expansion.
- **Failure signatures:** Semantic Hallucination: High scores on structure but low on Facts or Semantics. Classification Disconnect: High accuracy but low classification accuracy indicates "memorization without understanding." Grade Collapse: Performance dropping >15% between Grade 5 and Grade 13 indicates failure to handle complex syntax/agglutination.
- **First 3 experiments:** 1. Baseline Replication: Run ILAKKANAM on target model using Temperature=0. Report SP for Grades 1-5 vs. Grades 10-13. 2. Ablation on Categories: Isolate L3 (Morphology) vs. L1 (Phonetics). Determine if failures correlate with input token length or category type. 3. Classification Probe: Ask model to classify questions by linguistic category before answering. Check if forcing explicit categorization improves answer accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do high-performing LLMs on Tamil benchmarks exhibit genuine linguistic understanding or primarily rely on pattern memorization from training data exposure?
- **Basis in paper:** The authors state "performance may be driven by exposure rather than genuine understanding" after finding no strong correlation between overall accuracy and linguistic category classification ability.
- **Why unresolved:** The dissociation between task performance and metalinguistic awareness raises fundamental questions about what these models have actually learned.
- **What evidence would resolve it:** Probing experiments on held-out novel linguistic constructions not present in training data would help distinguish memorization from understanding.

### Open Question 2
- **Question:** Would expanding ILAKKANAM beyond two papers per grade adequately capture the full diversity of Tamil linguistic phenomena, including regional variants?
- **Basis in paper:** The authors acknowledge that "only two recent papers were selected from each grade level, which may not fully capture the breadth and diversity of Tamil linguistic phenomena."
- **Why unresolved:** Tamil exhibits regional variations across countries where it holds official status, which may not be captured by Sri Lankan examination papers alone.
- **What evidence would resolve it:** Comparative evaluation using expanded benchmarks incorporating materials from Tamil Nadu, Singapore, and diaspora communities would reveal cross-regional performance gaps.

### Open Question 3
- **Question:** How would separating higher-order linguistic categories (pragmatics, discourse, stylistics) from semantics reveal distinct model capabilities and failure modes?
- **Basis in paper:** The authors note that "Pragmatics, Discourse, Stylistics and other higher order were incorporated into the Semantics category (L5)... This grouping may reduce the granularity of analysis."
- **Why unresolved:** Subsuming these categories under semantics conflates distinct linguistic competencies, masking important failure patterns.
- **What evidence would resolve it:** Re-annotating the dataset with finer-grained category distinctions and analyzing category-specific performance patterns would reveal selective strengths and weaknesses.

## Limitations

- The paper does not provide evidence that benchmark questions are absent from model training data, leaving open the possibility that high scores reflect memorization rather than true linguistic understanding.
- No error analysis is provided to distinguish whether failures are due to tokenization issues, morphological complexity, or lack of semantic knowledge.
- The open-source models evaluated are not fine-tuned on Tamil, making the comparison potentially unfair and the performance gap attributable to both data scarcity and lack of adaptation.

## Confidence

- **High Confidence:** Performance declines with increasing linguistic complexity (Grades 10–13 vs. 1–5) is well-supported by the data and aligns with Tamil's agglutinative nature.
- **Medium Confidence:** The claim that closed-source models outperform open-source models due to proprietary data curation is plausible but not directly proven.
- **Low Confidence:** The conclusion that the disconnect between accuracy and classification ability definitively proves "memorization without understanding" is speculative without evidence that benchmark questions are novel to training data.

## Next Checks

1. **Novelty Verification:** Conduct a reverse search to confirm that a statistically significant subset of ILAKKANAM questions do not appear in public training corpora (e.g., Common Crawl, C4). This would strengthen the "exposure vs. understanding" claim.

2. **Morphology-Aware Tokenization:** Retest a subset of high-complexity Tamil questions using a morpheme-aware tokenizer (e.g., UniMorph or Tamil-specific segmentation) and compare performance to standard subword tokenization. This would isolate whether tokenization is a bottleneck.

3. **Fine-Tuning Ablation:** Fine-tune an open-source model (e.g., Qwen 2.5 72B) on a high-quality Tamil linguistic corpus (e.g., manually curated grammar exercises) and re-evaluate on ILAKKANAM. If performance closes the gap with closed-source models, it would support the data curation hypothesis.