---
ver: rpa2
title: 'QG-CoC: Question-Guided Chain-of-Captions for Large Multimodal Models'
arxiv_id: '2511.03206'
source_url: https://arxiv.org/abs/2511.03206
tags:
- prompting
- methods
- qg-coc
- multi-image
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: QG-CoC is a zero-shot prompting method for multimodal large language
  models that addresses the challenge of reasoning across multiple images. It decomposes
  a question into sub-questions, generates targeted captions for each sub-question
  to extract relevant visual evidence, and then integrates these sub-answers to form
  the final response.
---

# QG-CoC: Question-Guided Chain-of-Captions for Large Multimodal Models

## Quick Facts
- arXiv ID: 2511.03206
- Source URL: https://arxiv.org/abs/2511.03206
- Authors: Kuei-Chun Kao; Hsu Tzu-Yin; Yunqi Hong; Ruochen Wang; Cho-Jui Hsieh
- Reference count: 5
- Primary result: Up to 12% improvement on MMIU and 6% on MUIR multi-image reasoning benchmarks

## Executive Summary
QG-CoC is a zero-shot prompting method for multimodal large language models that addresses the challenge of reasoning across multiple images. It decomposes a question into sub-questions, generates targeted captions for each sub-question to extract relevant visual evidence, and then integrates these sub-answers to form the final response. The method improves fine-grained perception and synthesis across disparate images. Evaluated on both open-source and closed-source models, QG-CoC consistently outperforms existing prompting methods on multi-image benchmarks, achieving up to 12% improvement on MMIU and 6% on MUIR datasets. It also shows strong generalization on single-image tasks.

## Method Summary
QG-CoC is a three-stage prompting pipeline that decomposes complex multi-image questions into manageable sub-questions, generates targeted captions for each sub-question per image, and synthesizes the results into final answers. The method works by first using an LLM to break down the original question into focused sub-questions targeting specific visual aspects. Then, for each sub-question and image, the MLLM generates a targeted caption conditioned on that sub-question. Finally, the model answers each sub-question using its corresponding caption as evidence, and integrates all sub-answers to produce the final response. This approach improves relevance by focusing attention on task-specific visual evidence rather than generating comprehensive scene descriptions.

## Key Results
- QG-CoC achieves up to 12% improvement on MMIU multi-image reasoning benchmark
- Shows 6% improvement on MUIR dataset compared to existing prompting methods
- Demonstrates strong generalization to single-image tasks, outperforming baselines on MMMU, MMBench, and ScienceQA
- Consistent improvements across both open-source and closed-source models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Question decomposition improves multi-image reasoning by isolating task-relevant visual evidence from irrelevant detail.
- Mechanism: The original question is segmented into sub-questions targeting specific aspects (e.g., subject, action, outcome). Each sub-question narrows the visual search space, reducing attentional dilution across disparate images. This modularity likely mitigates reasoning failures by providing explicit intermediate goals rather than requiring end-to-end inference.
- Core assumption: The MLLM can perform reliable sub-question generation aligned with task intent; decomposition quality determines caption relevance.
- Evidence anchors:
  - [abstract]: "It decomposes a question into sub-questions, generates targeted captions for each sub-question to extract relevant visual evidence, and then integrates these sub-answers."
  - [section 4]: "Each sub-question targets a specific aspect of the image(s), such as the subject's action, outcome, or reaction. This decomposition ensures that the reasoning is detailed and aligned with the intent of the question."
  - [corpus]: AVAM (2508.17860) addresses visual redundancy in multi-image VQA through adaptive anchoring, suggesting attention management is a shared concern across methods.
- Break condition: If sub-questions are misaligned with the original question intent (Error type E1: 33.3% of errors), captions will target wrong evidence, propagating failure downstream.

### Mechanism 2
- Claim: Question-guided captioning yields higher relevance than generic captioning by conditioning extraction on task-specific prompts.
- Mechanism: Instead of generating comprehensive scene descriptions, captions are produced in response to each sub-question. This conditions the visual encoder-LLM pathway on task-relevant features, filtering out unrelated object descriptions that burden reasoning.
- Core assumption: The MLLM's captioning module can dynamically attend to question-relevant regions; this capacity scales with model capability.
- Evidence anchors:
  - [section 3.1, Table 1]: Question-Guided (N/Y) shows consistent improvement across models (e.g., LLaVA-OV: 47.4→47.8 on MMIU; Mantis: 45.5→46.0).
  - [section 3.1]: "Our findings reveal that question-guided captioning each image in detail benefits more than captioning multiple images as a whole or concisely."
  - [corpus]: Migician (2501.05767) emphasizes free-form multi-image grounding, indicating that flexible visual-text alignment is an active research direction but not yet solved.
- Break condition: If the MLLM lacks fine-grained perception (Error type E2: 31.7% of errors), question-guided captions will omit or misrepresent critical details, especially for counting, attribute, and matching tasks.

### Mechanism 3
- Claim: Integrating sub-question and sub-answer pairs as prior knowledge scaffolds final reasoning by providing explicit intermediate rationales.
- Mechanism: Sub-answers derived from targeted captions form a reasoning chain that decomposes the inference burden. The final integration step uses these pairs as contextual anchors, improving both accuracy and explainability.
- Core assumption: The model can correctly synthesize sub-answers into coherent final responses; reasoning errors at this stage are irrecoverable.
- Evidence anchors:
  - [abstract]: "integrates these sub-answers to form the final response. The method improves fine-grained perception and synthesis across disparate images."
  - [section 4, Step 3]: "These individual answers are then combined to produce the final answer to the original question, supported by visual evidence from the images."
  - [corpus]: No direct corpus evidence for this specific integration mechanism; related work focuses on grounding or benchmarking rather than synthesis.
- Break condition: If reasoning over synthesized information fails (Error type E3: 35.0% of errors), even accurate sub-answers will not salvage the final response. This is most prevalent in holistic tasks like difference detection, scene understanding, and action recognition.

## Foundational Learning

- **Concept: Multi-image attention management**
  - Why needed here: MLLMs struggle with fine-grained perception across disparate images because attention is diluted across multiple visual inputs (Section 1).
  - Quick check question: Given 3 images and a question about temporal ordering, can your model identify which visual details are decision-critical vs. redundant?

- **Concept: Zero-shot prompting pipelines**
  - Why needed here: QG-CoC is a multi-stage prompting method requiring sequential prompt engineering without fine-tuning (Section 4).
  - Quick check question: Can you design a two-stage prompt where Stage 1 output becomes conditional input for Stage 2, without modifying model weights?

- **Concept: Decomposition-then-synthesis reasoning**
  - Why needed here: The method relies on breaking complex queries into sub-problems, solving each, then recombining—this pattern is critical for interpretable multi-step inference (Section 4).
  - Quick check question: For a multi-image comparison task, what sub-questions would isolate perceptual differences from semantic relationships?

## Architecture Onboarding

- **Component map:** Question Decomposition Module → Question-Guided Captioning Module → Sub-Answer Module → Integration Module
- **Critical path:** Decomposition quality (Mechanism 1) → Caption relevance (Mechanism 2) → Sub-answer accuracy → Final synthesis (Mechanism 3). Failure at any stage propagates; E1 and E3 errors are most impactful.
- **Design tradeoffs:**
  - Token overhead vs. accuracy: QG-CoC adds ~127 tokens/sample and ~2.6s runtime over baseline (Table 6), but yields up to +12% accuracy gains on open-source models.
  - Model dependency: Performance relies on strong captioning ability; less advanced models may show deterioration (Limitations section).
  - Generality vs. specialization: QG-CoC generalizes to arbitrary image counts and task types, but spatial reasoning (2D/3D) remains weak across all prompting methods (Table 8, Discussion).
- **Failure signatures:**
  - E1 pattern: Sub-questions misinterpret original intent (e.g., "tortoise" confused with "duck" in Figure 9a).
  - E2 pattern: Captions contain factual errors (e.g., "L shape has 4 squares" when correct is 3; Figure 9b).
  - E3 pattern: Accurate sub-components but wrong final inference (e.g., correctly identifies "person walking" but misanswers difference question; Figure 9c).
- **First 3 experiments:**
  1. **Ablation by component:** Run QG-CoC with only decomposition (no guided captioning), only guided captioning (no decomposition), and full pipeline. Measure delta on MMIU/MUIR to isolate each mechanism's contribution (replicate Table 3).
  2. **Caption strategy sweep:** On a held-out multi-image subset, compare concise vs. detailed, summarized vs. individual, and question-guided vs. unguided captioning (replicate Table 1 settings). Verify that individual + detailed + question-guided yields highest accuracy.
  3. **Error mode profiling:** Sample 10 errors per MUIR task category (as in Table 5) and classify into E1/E2/E3. Identify which task types are E3-dominated (e.g., Difference, Scene, Action) vs. E2-dominated (e.g., Matching, Counting) to prioritize targeted improvements.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can QG-CoC be adapted to effectively handle complex geometric shapes and 2D/3D spatial information?
- Basis in paper: [explicit] The authors state in the Limitations section that "a more diverse and complicated scenario should be explored in the future, such as complex geometric shapes and even 2D, 3D-spatial information."
- Why unresolved: The current method relies on textual captioning which may struggle to represent precise geometric properties or 3D depth cues compared to semantic content.
- What evidence would resolve it: Demonstrated performance gains on spatial reasoning benchmarks (e.g., MMIU 3D tasks) using modified prompting strategies that capture spatial coordinates or geometry.

### Open Question 2
- Question: Can QG-CoC be modified to remain robust for less advanced language models that lack strong captioning capabilities?
- Basis in paper: [explicit] The paper notes, "Our proposed method relies on the captioning ability of advanced MLLMs. Therefore, it might cause performance deterioration in less advanced language models."
- Why unresolved: The pipeline assumes high-quality intermediate captioning; weaker models may hallucinate or fail to extract relevant visual evidence in Step 2, breaking the reasoning chain.
- What evidence would resolve it: Successful application of QG-CoC on smaller, open-source models (without external oracle captioners) showing consistent improvements over baselines.

### Open Question 3
- Question: How can the "Wrong Reasoning" error rate be reduced given that accurate context does not guarantee correct inference?
- Basis in paper: [inferred] The error analysis (Table 4) identifies "Wrong reasoning" (E3) as the most common error (35.0%), occurring even when question decomposition and perception are accurate.
- Why unresolved: This suggests a gap in the model's ability to synthesize correct answers from verified visual evidence, a limitation the current prompting structure does not fully address.
- What evidence would resolve it: A modification to the integration step (Step 3) that significantly lowers the E3 error percentage in the error analysis statistics.

## Limitations

- **Spatial reasoning weakness**: The method shows no advantage over baselines on 2D/3D spatial reasoning tasks (Table 8), suggesting fundamental limitations in geometric perception that prompting alone cannot address.
- **Model dependency**: Performance relies heavily on the captioning ability of advanced MLLMs, with potential deterioration for less capable models.
- **Token and runtime overhead**: QG-CoC adds approximately 127 tokens per sample and 2.6 seconds of inference time, which may be prohibitive for latency-sensitive applications.

## Confidence

- **High confidence**: The reported accuracy improvements (up to 12% on MMIU, 6% on MUIR) are well-supported by quantitative results across multiple models and benchmarks. The mechanism of question-guided captioning improving relevance over generic captioning is consistently validated.
- **Medium confidence**: The decomposition-then-synthesis reasoning mechanism is theoretically sound and supported by ablation results, but the integration step's reliability depends heavily on the underlying MLLM's reasoning capabilities, which vary significantly across models.
- **Low confidence**: The error analysis patterns (E1-E3) are based on qualitative sampling rather than systematic annotation, making it difficult to assess their prevalence or severity without replication.

## Next Checks

1. **Component ablation validation**: Systematically disable each QG-CoC component (decomposition, guided captioning, integration) on MMIU and measure individual contribution to performance gains, confirming the mechanism dependencies described.
2. **Cross-model robustness test**: Apply QG-CoC to a diverse set of MLLMs with varying captioning abilities (from strong closed-source to weaker open-source) to quantify the performance ceiling and identify models where the method may degrade accuracy.
3. **Error mode quantification**: Perform systematic annotation of 100+ errors across all MUIR task categories, classifying each into E1/E2/E3 types to validate the reported error distribution and identify the most problematic task categories for targeted improvement.