---
ver: rpa2
title: 'BBox DocVQA: A Large Scale Bounding Box Grounded Dataset for Enhancing Reasoning
  in Document Visual Question Answer'
arxiv_id: '2511.15090'
source_url: https://arxiv.org/abs/2511.15090
tags:
- answer
- page
- reasoning
- document
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BBox-DocVQA, a large-scale dataset for document
  visual question answering with explicit bounding-box grounding. Unlike existing
  page-level DocVQA datasets, BBox-DocVQA provides fine-grained spatial annotations
  for each QA pair, enabling evaluation of both spatial grounding and reasoning accuracy.
---

# BBox DocVQA: A Large Scale Bounding Box Grounded Dataset for Enhancing Reasoning in Document Visual Question Answer

## Quick Facts
- arXiv ID: 2511.15090
- Source URL: https://arxiv.org/abs/2511.15090
- Reference count: 40
- Primary result: Large-scale dataset with bounding-box-grounded QA pairs for document understanding, showing VLMs struggle with spatial grounding but improve with fine-tuning.

## Executive Summary
BBox-DocVQA is a new dataset for document visual question answering that provides explicit bounding-box grounding for each QA pair, unlike existing page-level DocVQA datasets. The dataset was constructed using an automated Segment-Judge-and-Generate pipeline that segments document regions, filters and classifies them with a vision-language model, and generates QA pairs using advanced models followed by human verification. It contains 32K QA pairs from 3.6K documents, covering single- and multi-region as well as single- and multi-page scenarios. Benchmarking multiple state-of-the-art VLMs revealed that these models struggle to accurately localize evidence regions, with grounding accuracy often much lower than answer correctness. Fine-tuning on BBox-DocVQA significantly improves both localization and answer generation, demonstrating its effectiveness in enhancing spatial reasoning and interpretability in vision-language models.

## Method Summary
The dataset was constructed using an automated Segment-Judge-and-Generate pipeline. SAM (ViT-H) segments page images into candidate regions, which are filtered to 5-70% page area with 10px padding. Qwen2.5-VL-72B judges quality (keep=true/false) and classifies type (text/table/image), with overlap-based deduplication (≥90% overlap: keep smaller for text, larger for table/image). GPT-5 generates QA pairs from cropped regions, with human verification applied to a benchmark subset. The dataset contains 30,780 training QA pairs from 3,671 arXiv papers (42,380 pages) and 1,623 manually annotated benchmark QA pairs from 80 papers. Three task types are defined: Single-Page Single-BBox (SPSBB), Single-Page Multi-BBox (SPMBB), and Multi-Page Multi-BBox (MPMBB). PDFs are converted to 300 DPI PNG images.

## Key Results
- State-of-the-art VLMs show persistent challenges in spatial grounding and reasoning accuracy on BBox-DocVQA
- Fine-tuning on BBox-DocVQA substantially improves both bounding box localization and answer generation
- Grounding accuracy (IoU) is often much lower than answer correctness, revealing a significant gap in spatial reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicit bounding-box grounding may reduce visual-semantic noise, thereby strengthening answer generation.
- Mechanism: By constraining the model's attention to semantically complete evidence regions (e.g., a full paragraph or table), spatial grounding limits interference from irrelevant page content and focuses reasoning on coherent units.
- Core assumption: The evidence region contains all information necessary for the answer; page-level noise is a primary source of error.
- Evidence anchors:
  - [abstract] "Benchmarking multiple state-of-the-art VLMs... on BBox-DocVQA reveals persistent challenges in spatial grounding and reasoning accuracy."
  - [Section 4.2, Table 3] Accuracy for `gt subpage answer` is consistently higher than `gt page answer` and `pages bbox answer` across models, with Delta 1 gaps of 1.73–12.38 pp.
  - [corpus] Related work on "Spatially-Grounded Document Retrieval" (arXiv:2512.02660) similarly argues that patch/region-level grounding improves retrieval utility for downstream reasoning.
- Break condition: If evidence regions are fragmented, mislabeled, or do not span the full reasoning context, grounding may introduce new errors or omissions.

### Mechanism 2
- Claim: The Segment-Judge-and-Generate pipeline enables scalable creation of semantically coherent, spatially grounded QA pairs.
- Mechanism: SAM provides candidate regions; a VLM (Qwen2.5-VL-72B) judges completeness and classifies type (text/table/image); GPT-5 generates QA conditioned on these crops; overlap-based deduplication reduces redundancy. Human verification is applied to a benchmark subset.
- Core assumption: VLM-based judgment and generation can approximate human-quality semantic segmentation and QA creation.
- Evidence anchors:
  - [abstract] "We further present an automated construction pipeline, Segment–Judge–and–Generate, which integrates a segment model for region segmentation, a VLM for semantic judgment, and another advanced VLM for question–answer generation, followed by human verification."
  - [Section 3.1] Details on SAM-based segmentation, Qwen2.5-VL-72B judgment with defined positive/negative criteria, and GPT-5 QA generation.
  - [corpus] No direct corpus evidence evaluates the pipeline's fidelity; related work on document chunking (arXiv:2501.05485) notes that integrating spatial and semantic analysis improves segmentation, which aligns with this pipeline's design.
- Break condition: If the judgment VLM fails to identify truncated or noisy regions, or the QA generator relies on external knowledge, the resulting supervision may be noisy or ungrounded.

### Mechanism 3
- Claim: Fine-tuning on BBox-DocVQA likely improves spatial-semantic alignment, leading to gains in both localization and answer accuracy.
- Mechanism: Joint supervision with bounding-box and QA targets provides an explicit training signal for models to learn which visual regions support specific questions, refining attention and coordinate prediction.
- Core assumption: Current VLMs lack sufficient spatial grounding training data; additional grounded examples can transfer to improved reasoning.
- Evidence anchors:
  - [abstract] "Furthermore, fine-tuning on BBox-DocVQA substantially improves both bounding box localization and answer generation."
  - [Section 4.2, Discussion] "BBox-DocVQA fills this gap by offering bounding-box–grounded supervision that explicitly links each question to its reasoning substrate."
  - [corpus] Weak direct evidence; one related framework (ARIAL, arXiv:2511.18192) targets precise answer localization in DocVQA but does not provide training data.
- Break condition: If the model architecture cannot represent fine-grained spatial coordinates or if the dataset scale is insufficient, fine-tuning gains may be limited.

## Foundational Learning

- Concept: **Bounding Box Representation (Pixel Coordinates)**
  - Why needed here: BBox-DocVQA uses absolute pixel coordinates [left x, top y, right x, bottom y] for evidence localization; understanding this format is essential for implementing evaluation metrics and model outputs.
  - Quick check question: How would you compute IoU between two boxes in [x1, y1, x2, y2] format?

- Concept: **Intersection over Union (IoU)**
  - Why needed here: IoU is the primary metric for evaluating bounding box grounding accuracy across SPSBB, SPMBB, and MPMBB tasks.
  - Quick check question: If ground truth is [10,10,50,50] and prediction is [20,20,60,60], what is the IoU?

- Concept: **Vision-Language Model (VLM) Spatial Grounding**
  - Why needed here: The paper's core hypothesis is that VLMs struggle with spatial grounding; understanding how VLMs typically attend to images helps interpret the results and design interventions.
  - Quick check question: Why might a VLM trained on image-caption pairs fail to predict precise document region coordinates?

## Architecture Onboarding

- Component map: Document PDF -> SAM segmentation -> Qwen judgment -> deduplication -> GPT-5 QA generation -> (optional human verification) -> training/benchmark data
- Critical path: Document PDF → SAM segmentation → Qwen judgment → deduplication → GPT-5 QA generation → (optional human verification) → training/benchmark data. Errors in early stages propagate; judgment quality directly affects QA grounding.
- Design tradeoffs:
  - Automation vs. quality: The pipeline is fully automated for training data, with human verification only for the benchmark. This trades annotation cost for potential noise in training labels.
  - Region size thresholds (5–70% area ratio): Filters noise but may exclude valid small (e.g., footnotes) or large (e.g., full-page tables) regions.
  - Overlap handling: Retaining smaller text vs. larger table/image regions during deduplication optimizes for granularity vs. structural integrity.
- Failure signatures:
  - Low IoU despite high answer accuracy: Model relies on global textual cues, not grounded evidence (see Table 4 vs. Table 5, e.g., GPT-5 with 0.9% mean IoU but 81.45% answer accuracy).
  - High accuracy on `gt subpage answer` but low on `pages bbox answer`: Model fails to localize, indicating weak spatial grounding (e.g., Qwen3VL-32B: 56.38% vs. 30.87%).
  - Type confusion in judgment: If the Judge VLM misclassifies a table as text, subsequent QA may lack structural context.
- First 3 experiments:
  1. **Baseline grounding evaluation**: Run existing VLMs (e.g., Qwen2.5-VL-72B, InternVL) on the BBox-DocVQA benchmark in simultaneous perception-and-answering mode; measure IoU and answer accuracy to establish current weaknesses.
  2. **Ablation on pipeline stages**: Train models on data from (a) full pipeline, (b) without judgment filtering, (c) without deduplication; compare grounding performance to assess each component's contribution.
  3. **Fine-tuning impact analysis**: Fine-tune a mid-sized VLM (e.g., Qwen2.5-VL-7B) on BBox-DocVQA training data; measure pre/post gains in IoU and answer accuracy, focusing on SPSBB vs. MPMBB to test multi-page reasoning transfer.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What architectural modifications are necessary to prevent standard image preprocessing (e.g., resizing) from degrading precise coordinate prediction in Vision-Language Models?
- Basis in paper: [explicit] The paper notes that models like InternVL and GPT-5 exhibit very low IoU, "likely because their internal image preprocessing (e.g., resizing) disrupts precise coordinate prediction," and calls for "grounding-aware model architectures."
- Why unresolved: The authors identify the bottleneck where visual encoders destroy the spatial fidelity needed for bounding box regression, but they evaluate existing models rather than proposing a structural solution to preserve spatial resolution.
- What evidence would resolve it: A model architecture that maintains high-resolution spatial features or employs variable resolution processing, demonstrating significantly higher IoU on the BBox-DocVQA benchmark without losing semantic reasoning capability.

### Open Question 2
- Question: To what extent does fine-tuning on academic arXiv documents transfer to industrial document understanding tasks involving distinct layouts like invoices or handwritten forms?
- Basis in paper: [inferred] The dataset is constructed exclusively from "4,000 PDF documents from the arXiv platform," covering only academic domains like Computer Science and Physics.
- Why unresolved: While the dataset is large, the paper does not address domain adaptation. It is unclear if the "spatial reasoning" learned from double-column academic text and scientific figures generalizes to the sparse, tabular, or handwritten structures found in financial or legal documents.
- What evidence would resolve it: Cross-domain evaluation results showing models trained on BBox-DocVQA performing on external datasets like DocVQA (industry documents) or InfographicsVQA to measure the robustness of the learned grounding signals.

### Open Question 3
- Question: How can multi-page reasoning systems be optimized to close the performance gap between single-region (SPSBB) and complex multi-region (MPMBB) tasks?
- Basis in paper: [explicit] The authors conclude that "multi-page reasoning remains challenging" and suggest the dataset will catalyze "multi-step reasoning paradigms," citing a significant accuracy drop in MPMBB compared to simpler tasks.
- Why unresolved: The paper demonstrates that models struggle to aggregate evidence across pages (accuracy drops from ~84% to ~55% for Qwen3VL-32B) but does not propose a specific mechanism to manage the expanded search space and cross-page dependencies effectively.
- What evidence would resolve it: A novel retrieval or attention mechanism specifically designed for cross-page context aggregation that significantly improves MPMBB scores relative to SPSBB scores.

## Limitations

- Domain specificity: Dataset constructed exclusively from arXiv papers, limiting generalization to other document types like invoices or legal documents
- Pipeline quality uncertainty: Human verification limited to benchmark set, leaving training data quality potentially noisy
- Multi-page complexity: MPMBB task shows significant performance gaps, indicating need for improved cross-page reasoning mechanisms

## Confidence

- **High**: The dataset construction pipeline is well-specified and reproducible; the benchmark evaluation setup is detailed and clear.
- **Medium**: Claims about fine-tuning benefits are supported by results but lack full hyperparameter details; MPMBB task complexity may require additional architectural support not addressed.
- **Low**: No explicit validation of the pipeline's QA quality beyond the benchmark subset; generalizability across document domains is untested.

## Next Checks

1. **Pipeline fidelity audit**: Manually inspect 100 training QA pairs to measure the rate of truncation, misclassification, or hallucination in the judgment and generation stages.
2. **Cross-domain transfer test**: Fine-tune a VLM on BBox-DocVQA and evaluate on a held-out, non-ArXiv document set (e.g., clinical reports or financial statements) to measure domain generalization.
3. **Multi-page reasoning probe**: Design a synthetic MPMBB evaluation set with controlled cross-page dependencies to isolate grounding vs. retrieval failures.