---
ver: rpa2
title: Do we Really Need Visual Instructions? Towards Visual Instruction-Free Fine-tuning
  for Large Vision-Language Models
arxiv_id: '2502.11427'
source_url: https://arxiv.org/abs/2502.11427
tags:
- visual
- data
- vift
- arxiv
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ViFT is the first instruction-free fine-tuning framework for large
  vision-language models (LVLMs) that achieves state-of-the-art performance without
  requiring visual instruction data during training. The method separates the learning
  of task-solving ability (from text-only instructions) and visual perception ability
  (from image captions), then combines these abilities during inference using steering
  vectors extracted from the model's hidden states.
---

# Do we Really Need Visual Instructions? Towards Visual Instruction-Free Fine-tuning for Large Vision-Language Models

## Quick Facts
- arXiv ID: 2502.11427
- Source URL: https://arxiv.org/abs/2502.11427
- Reference count: 40
- Primary result: ViFT achieves SOTA visual reasoning performance without visual instruction data, using <30% of training data vs prior work

## Executive Summary
ViFT introduces the first instruction-free fine-tuning framework for large vision-language models that achieves state-of-the-art performance without requiring visual instruction data during training. The method separates the learning of task-solving ability (from text-only instructions) and visual perception ability (from image captions), then combines these abilities during inference using steering vectors extracted from the model's hidden states. ViFT outperforms existing LVLMs on visual reasoning benchmarks MathVerse (34.8 vs 31.0) and MathVision (24.0 vs 18.1) while using less than 30% of the training data compared to state-of-the-art approaches.

## Method Summary
ViFT is a two-stage fine-tuning framework that learns task-solving and visual perception abilities separately using text-only instructions and image captions respectively, then combines them at inference via steering vector arithmetic. The method uses modality-specific batching during training to prevent token-length disparity, and fuses steering vectors in the top 50% of transformer layers during inference. This approach achieves state-of-the-art visual reasoning performance while requiring only text instructions and captions during training, eliminating the need for expensive visual instruction data collection.

## Key Results
- Outperforms state-of-the-art visual instruction tuning on MathVerse (34.8 vs 31.0) and MathVision (24.0 vs 18.1)
- Uses less than 30% of the training data compared to visual instruction tuning approaches
- Maintains strong performance on visual instruction following tasks while excelling at visual reasoning
- Two-stage training is ~30% faster than one-stage with negligible performance difference

## Why This Works (Mechanism)

### Mechanism 1: Ability Disentanglement via Modality-Specific Fine-tuning
Training task-solving and visual perception separately on different data modalities reduces interference compared to joint visual instruction tuning. Text-only instructions optimize text-to-text relations for reasoning/instruction-following; image captions optimize image-to-text alignment for visual perception. Modality-specific batching prevents token-length disparity from degrading training efficiency.

### Mechanism 2: Steering Vector Arithmetic for Ability Fusion at Inference
Hidden states extracted from different input modalities encode separable "ability vectors" that can be combined via weighted addition to induce multimodal behavior. Extract h(q) from text-only forward pass (task-solving vector); extract h(v,q) from multimodal forward pass (visual perception vector); compute h'(v,q) = αh(v,q) + βh(q); replace hidden states of input tokens at selected layers before autoregressive generation.

### Mechanism 3: Layer-Selective Fusion Targeting Behavioral Control Layers
Fusing steering vectors in the top 50% of transformer layers is optimal; early-layer fusion disrupts visual-text aggregation. Visual information aggregates with text tokens in early layers; behavioral control is concentrated in later layers. Fusion at layers 14-28 (for a 28-layer LLM) preserves aggregation while enabling ability combination.

## Foundational Learning

- **Steering Vectors / Representation Engineering**: Essential for understanding how ViFT manipulates hidden states to combine abilities. Quick check: Given a model that produces hidden states h_layer_20 for input "Translate to French:", how would you use this vector to induce French translation behavior on an unrelated prompt?

- **Vision-Language Model Architecture (Encoder-Connector-LLM Pipeline)**: Critical for understanding where visual tokens enter the computation graph and how to correctly extract vectors. Quick check: In a SigLIP + 2-layer MLP + 28-layer LLM architecture, at which layer do visual tokens first participate in self-attention with text tokens?

- **Autoregressive Language Modeling Objective**: Necessary for understanding how empty visual inputs are handled during training. Quick check: In the loss L(θ) = -Σ log P(r_j | v, q, r_{<j}; θ), how should v be treated when training on text-only instructions?

## Architecture Onboarding

- **Component map**: Visual Encoder (SigLIP, lr=1e-5) -> Connector (2-layer MLP, lr=2e-6) -> LLM Backbone (Qwen2.5-7B-Instruct, 28 layers, lr=1e-5)
- **Critical path**: Stage 1: Train on 1M web captions (LAION); Stage 2: Train on 1.7M high-quality captions + 200K text instructions (modality-specific batching); Inference: Text-only forward pass → extract h(q); Multimodal forward pass → extract h(v,q); Compute h'(v,q) = αh(v,q) + βh(q); Replace hidden states at layers 14-28; Generate autoregressively
- **Design tradeoffs**: β=0.1 for visual reasoning vs β=0.15 for instruction following; Two-stage vs one-stage training (two-stage is ~30% faster); Optional VQA data (ViFT-A) improves MathVista (+7.5) but hurts MathVision (-3.5)
- **Failure signatures**: β > 0.4 produces random strings; Fusion at layers 0-7 drops LLaVABench to 64.8 (vs 82.2); No ability-fused inference produces caption-only outputs; α ≤ 0.8 causes refusal to answer
- **First 3 experiments**: 1) Ablate ability-fused inference: expect LLaVABench ~59 (vs 82.2); 2) Layer sweep: confirm layers 14-28 is optimal; 3) Scaling comparison: verify ViFT continues improving with scale while VIT plateaus

## Open Questions the Paper Calls Out

- Are coarse-grained image captions truly optimal for visual perception learning across all vision domains? The paper uses captions as the primary multimodal data but only evaluates on a limited set of domains. Different visual domains (e.g., medical imaging, satellite imagery) may require more fine-grained or structured visual representations.

- Can ViFT efficiently transfer advanced LLM capabilities like long-thought reasoning to visual tasks? Current evaluation focuses on mathematical reasoning and instruction following, but long-form chain-of-thought reasoning with extended deliberation remains untested.

- How can optimal fusion weights (α, β) be determined automatically without task-specific hyperparameter tuning? The paper manually selects fusion weights per task type, which limits practical deployment and generalization to novel tasks.

## Limitations

- The paper does not specify exact mixing ratios between caption and text instruction data during Stage 2 training, nor does it detail the specific caption query pool used to convert caption data into instruction format.
- Steering vector implementation details are sparse, making reproducibility of the inference-time fusion mechanism questionable.
- Layer-wise specialization claims lack direct empirical validation within this work and may be architecture-specific.

## Confidence

**High Confidence** (validated through multiple experiments):
- ViFT achieves state-of-the-art performance on visual reasoning benchmarks
- ViFT uses less than 30% of the training data compared to state-of-the-art approaches
- Two-stage training is ~30% faster than one-stage with negligible performance difference
- Modality-specific batching improves training efficiency

**Medium Confidence** (supported by experiments but with implementation uncertainties):
- Ability disentanglement through separate training modalities improves performance
- Steering vector arithmetic successfully combines task-solving and visual perception
- Layer-selective fusion at 14-28 layers is optimal for the Qwen2.5-7B architecture
- β=0.1 for visual reasoning and β=0.15 for instruction following are optimal fusion ratios

**Low Confidence** (theoretical claims with limited empirical support):
- The general claim that task-solving and visual perception can be completely separated and recombined for any LVLM architecture
- The assertion that visual information aggregates in early layers across all LVLM architectures
- The claim that steering vector arithmetic works universally across different backbone configurations

## Next Checks

1. **Implement and validate ability-fused inference**: Reproduce the steering vector fusion mechanism with detailed implementation of hidden state extraction and replacement. Test on a simple visual question-answering task to confirm that the fusion produces coherent multimodal responses.

2. **Layer-wise ablation study across architectures**: Perform systematic ablation studies on different LVLM architectures (e.g., LLaVA, Qwen2-VL) to validate whether layer 14-28 is universally optimal or architecture-specific.

3. **Data scaling comparison with controlled datasets**: Conduct a controlled experiment comparing ViFT vs visual instruction tuning at multiple data ratios (1/64, 1/16, 1/4, 1/1) using identical datasets for both methods.