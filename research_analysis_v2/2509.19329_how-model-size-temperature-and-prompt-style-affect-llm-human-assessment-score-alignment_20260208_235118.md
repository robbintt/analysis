---
ver: rpa2
title: How Model Size, Temperature, and Prompt Style Affect LLM-Human Assessment Score
  Alignment
arxiv_id: '2509.19329'
source_url: https://arxiv.org/abs/2509.19329
tags:
- human
- prompt
- raters
- clinical
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluated how Large Language Model (LLM) parameters\u2014\
  model size, temperature, and prompt style\u2014affect their alignment with human\
  \ raters in assessing clinical reasoning skills in medical students. Using eight\
  \ distinct LLM configurations, we analyzed consistency within and between models,\
  \ and their alignment with human ratings."
---

# How Model Size, Temperature, and Prompt Style Affect LLM-Human Assessment Score Alignment

## Quick Facts
- arXiv ID: 2509.19329
- Source URL: https://arxiv.org/abs/2509.19329
- Reference count: 27
- Primary result: Larger models (GPT-4o) align better with human raters than smaller models (GPT-4o-mini); temperature and expert persona prompts increase scores but don't consistently improve human alignment.

## Executive Summary
This study systematically evaluates how Large Language Model (LLM) parameters—model size, temperature, and prompt style—affect alignment with human raters in clinical reasoning assessment. Using eight distinct LLM configurations, researchers analyzed consistency within and between models and their alignment with human ratings of German medical students' patient dialogues. The study reveals that model size is the most critical factor, with larger models demonstrating significantly better human alignment and internal consistency compared to smaller models. Temperature and expert persona prompts increase scores but reduce human alignment, suggesting that internal consistency alone is insufficient for validation.

## Method Summary
The study used 21 German medical students' dialogues with four standardized patients each, scored by two human expert raters using the 8-item Clinical Reasoning Inventory for Health Profession Trainees (CRI-HTS). Eight LLM configurations were tested: GPT-4o vs GPT-4o-mini, temperature 0.2 vs 0.7, and regular vs expert persona prompts. Each configuration rerated all transcripts twice via API. Intraclass Correlation Coefficients (ICCs) measured internal consistency, inter-LLM agreement, and LLM-human alignment. Linear regression with robust standard errors analyzed score differences across configurations.

## Key Results
- Model size was the primary determinant of human alignment, with GPT-4o showing ICC levels comparable to human-human agreement while GPT-4o-mini showed very poor agreement
- Higher temperature and expert persona prompts both significantly increased scores (β = 0.26 and β = 0.27) but did not improve human alignment
- Expert persona prompting unexpectedly reduced alignment with human raters, suggesting it triggered lenient scoring rather than expert reasoning
- The optimal configuration for human alignment was GPT-4o with regular prompt and low temperature (mean score = 3.29 vs human mean = 3.19)

## Why This Works (Mechanism)

### Mechanism 1: Model Capacity and Human Judgment
Larger models (GPT-4o) possess greater representational capacity to capture nuanced, domain-specific reasoning patterns that humans apply when evaluating clinical skills. This allows them to better approximate the latent decision criteria human experts use, rather than relying on surface-level pattern matching.

### Mechanism 2: Temperature and Score Inflation
Higher temperature (0.7 vs 0.2) flattens probability distributions, increasing the chance of selecting higher-probability "lenient" scoring tokens when the model is uncertain about evaluation criteria. This creates score inflation through randomized response variability.

### Mechanism 3: Expert Persona Priming Effects
Persona prompts ("act as a rater with medical expertise") prime the model to generate outputs associated with expert identity markers in training data. These associations may include generosity toward learners or supportive feedback patterns, rather than the critical evaluation criteria experts actually apply.

## Foundational Learning

- **Concept: Intraclass Correlation Coefficient (ICC) for inter-rater reliability**
  - Why needed here: The entire methodology rests on ICC comparisons to measure alignment. Without understanding that ICC captures absolute agreement (not just correlation), readers may misinterpret "high ICC among LLMs" as evidence of validity.
  - Quick check question: If two raters always give scores exactly 2 points apart, would their ICC be high or low?

- **Concept: Score inflation vs. alignment**
  - Why needed here: The paper distinguishes between internal consistency (LLMs agree with each other) and human alignment (LLMs agree with humans). This distinction is critical for validity claims—high consistency without alignment produces systematically biased scores.
  - Quick check question: An LLM achieves ICC = 0.85 with itself across reruns but ICC = 0.15 with human raters. Is this model reliable? Is it valid for replacing human scoring?

- **Concept: Interaction effects in factorial design**
  - Why needed here: Temperature effects differ by model size (significant negative interaction: β = -0.26). Understanding that the effect of one parameter depends on another is essential for practical configuration decisions.
  - Quick check question: If high temperature adds +0.26 to scores in small models but the interaction term is -0.26, what's the net temperature effect for large models?

## Architecture Onboarding

- **Component map**: German student-patient dialogue transcripts → Preprocessing → Model selector (GPT-4o vs GPT-4o-mini) × Temperature (0.2/0.7) × Prompt template (regular/expert persona) → 8-item CRI-HTS rubric → Item-level scores (1-5 Likert) → Items averaged → Person-level mean score → ICC computation → Alignment metrics

- **Critical path**: Model size → Human alignment. Temperature and prompt style are secondary modifiers; the large model with regular prompt and low temperature produced scores closest to human mean (M = 3.29 vs human M = 3.19).

- **Design tradeoffs**:
  - Larger models: Better alignment but higher cost and latency
  - Lower temperature: More consistent scores but may miss edge cases requiring nuanced judgment
  - Regular prompts: Better human alignment but may underperform on tasks requiring domain expertise signaling
  - Internal consistency checks: Fast but insufficient for validation without human benchmark

- **Failure signatures**:
  - High LLM-LLM ICC (>0.75) combined with low LLM-human ICC (<0.30) → Systematic score drift from human standards
  - Expert persona + high temperature + small model → Unstable scoring (lowest within-model ICC)
  - Item-level discrepancies (e.g., Item 4: high LLM consistency, low human alignment) → Criterion interpretation mismatch between model and rubric intent

- **First 3 experiments**:
  1. **Baseline alignment check**: Run large model (GPT-4o), temperature 0.2, regular prompt on your assessment data. Compute ICC against available human ratings. If ICC < 0.40, stop—task may require prompt engineering or different model.
  2. **Score inflation calibration**: Compare mean scores across temperature settings (0.2 vs 0.7) holding model and prompt constant. If inflation > 0.3 points, document expected score shift before deployment.
  3. **Persona prompt ablation**: Test regular vs expert persona prompts on subset with human ratings. If persona increases scores by > 0.2 points without improving ICC, discard persona approach for this task type.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can few-shot prompting strategies improve LLM-human alignment for specific criteria where expert persona prompts failed?
- Basis in paper: The authors suggest future research explore "alternative prompting strategies, such as few-shot prompting," particularly for items like Item 4 where LLMs and humans interpreted criteria differently.
- Why unresolved: This study only tested zero-shot regular and expert persona prompts, finding that persona prompts unexpectedly reduced alignment.
- What evidence would resolve it: An experiment comparing few-shot prompting against the persona configurations using the same clinical reasoning dataset.

### Open Question 2
- Question: How do specific facets of error (raters, tasks, criteria) contribute to variance in LLM-generated scores?
- Basis in paper: The authors state that with only 21 students, they were "unable to comprehensively explore or decompose sources of measurement error" and recommend using Generalizability Theory in future studies.
- Why unresolved: The small sample size restricted the analysis to simple ICC and regression models, preventing a multi-faceted error analysis.
- What evidence would resolve it: A large-scale study applying Generalizability Theory to quantify variance components across different LLM configurations and human raters.

### Open Question 3
- Question: Do the observed effects of temperature and persona prompts on score inflation generalize to non-OpenAI models?
- Basis in paper: The study analyzed only GPT-4o and GPT-4o-mini; the findings regarding how temperature affects "creativity" and scoring consistency may be specific to OpenAI's model architectures.
- Why unresolved: Different model families may handle temperature randomness and persona adoption differently, leading to different alignment outcomes.
- What evidence would resolve it: Replication of the experimental design using open-source or alternative proprietary models (e.g., Llama, Claude).

## Limitations
- Limited generalizability due to small sample size (20 transcripts) and single assessment type (German medical dialogues)
- Missing human-human reliability data prevents direct comparison of LLM-human alignment quality against human consistency standards
- Score inflation mechanism remains speculative—we cannot determine whether higher scores reflect leniency, shifted interpretation, or response bias

## Confidence
- Model size effect on human alignment: **High** (clear ICC differences between GPT-4o and GPT-4o-mini with consistent pattern)
- Temperature-induced score inflation: **Medium** (statistically significant but mechanism not fully characterized)
- Expert persona prompt degradation of alignment: **Medium** (unexpected finding, limited corpus support, potential domain-specific artifact)

## Next Checks
1. Replicate findings on larger, diverse assessment corpus with multiple rubric types to test domain transfer
2. Collect human-human reliability data on same transcripts to benchmark LLM-human alignment against human consistency
3. Test constrained persona prompts (explicit behavioral rules) to isolate whether inflation stems from identity priming versus lenient interpretation