---
ver: rpa2
title: The Limits of Preference Data for Post-Training
arxiv_id: '2505.19964'
source_url: https://arxiv.org/abs/2505.19964
tags:
- preference
- reasoning
- data
- rlhf
- backtracking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Preference data is a scalable way to collect human feedback but\
  \ fundamentally limits the ability of reinforcement learning to optimize outcomes.\
  \ Even with ideal conditions\u2014infinite, noiseless, online preference data\u2014\
  ordinal feedback can prevent finding even approximately optimal solutions."
---

# The Limits of Preference Data for Post-Training

## Quick Facts
- arXiv ID: 2505.19964
- Source URL: https://arxiv.org/abs/2505.19964
- Reference count: 40
- One-line primary result: Preference data fundamentally limits reinforcement learning's ability to optimize outcomes, particularly for reasoning tasks requiring robust strategies.

## Executive Summary
Preference data is a scalable way to collect human feedback but fundamentally limits the ability of reinforcement learning to optimize outcomes. Even with ideal conditions—infinite, noiseless, online preference data—ordinal feedback can prevent finding even approximately optimal solutions. This limitation arises from the inherent inability of preference rankings to capture the full utility of outcomes. The problem is particularly acute for reasoning tasks, where robust strategies like backtracking are essential but often dispreferred by labelers. These robust behaviors are "compromise candidates" that do well across many queries but lose in head-to-head comparisons. While adding even a small amount of cardinal feedback can mitigate this issue, task-specific heuristics may be necessary to effectively elicit desired behaviors.

## Method Summary
The paper combines theoretical analysis with empirical validation to demonstrate the limits of preference data. Theoretically, it formalizes post-training as query-to-circuit routing and maps this to voting theory, proving distortion lower bounds that show preference data cannot recover optimal utilities. Empirically, it validates the suppression of robust reasoning behaviors through three experiments: measuring robustness to perturbations in different model families, classifying backtracking in DeepSeek R1 responses, and simulating preference data collection to compare labeling instructions. The experiments use a mix of established benchmarks (AIME, MATH, LiveBench) and LM-based scoring and classification.

## Key Results
- Preference data creates Ω(√|S₀|) distortion lower bound, preventing approximate optimality even with infinite samples
- Robust reasoning strategies (backtracking) are systematically suppressed by preference data, with 65-85% preference for non-backtracking responses when both are correct
- Adding even small amounts of cardinal feedback can reduce distortion to O(1), but practical implementation remains challenging

## Why This Works (Mechanism)

### Mechanism 1: Ordinal Information Loss via Distortion
Preference rankings (A > B > C) cannot recover utility magnitudes, causing systematic suboptimality in outcome optimization. The paper formalizes this via distortion from social choice theory—the ratio between optimal achievable utility and utility from preference-based learning. Theorem 3.3 proves distortion lower bound of Ω(√|S₀|) where |S₀| is the number of pretrained circuits. Core assumption: Utilities exist but are unobservable; labelers provide only pairwise preferences.

### Mechanism 2: Post-training as Query-to-Circuit Routing
Post-training primarily re-routes existing pretrained circuits rather than creating new capabilities, limiting what preference data can optimize. Model M = (φ, g, S) routes queries Q → internal representations Z → distributions over circuits S. Preference data must learn this routing without observing utility directly. Core assumption: |Q| ≫ |S|, |Z|—models cannot have dedicated circuits per query and must generalize routing decisions.

### Mechanism 3: Robustness Behaviors as Compromise Candidates
Reasoning behaviors (backtracking, verification, explicitness) are "compromise candidates"—good across queries but dispreferred in pairwise comparisons. A robust strategy s_robust achieves decent utility across queries Q but loses head-to-head vs. a direct strategy s_direct on any single query where direct suffices. This mirrors the Borda count failure in Example 3.2. Core assumption: Labelers penalize unnecessary robustness overhead (e.g., backtracking when not needed).

## Foundational Learning

- **Distortion in Social Choice Theory**
  - Why needed here: The paper's core theoretical contribution frames post-training suboptimality as a distortion problem.
  - Quick check question: Can you explain why Borda count (sum of rankings) can select a lower-utility candidate than the true utility-maximizer?

- **Bradley-Terry Preference Models**
  - Why needed here: Theorem 3.4 shows impossibility persists even under common noise models; understanding this clarifies what "noise" cannot fix.
  - Quick check question: Does Bradley-Terry noise with linear score functions allow recovery of cardinal utilities from preference probabilities?

- **RLHF vs. RLVR Distinction**
  - Why needed here: The paper contrasts RLHF (preference-based, human feedback) with RLVR (verifiable rewards) to motivate why reasoning succeeds in one but not the other.
  - Quick check question: Why does backtracking suppression matter less for instruction-tuning than for mathematical reasoning?

## Architecture Onboarding

- Component map: Pretrained Model M₀ = (φ₀, g₀, S₀) → Post-training learns improved (φ, g) → M = (φ, g, S₀) routes queries Q → internal representations Z → distributions over circuits S

- Critical path:
  1. Formalize query-to-circuit routing (Section 2)
  2. Map to voting: circuits = candidates, queries = voters
  3. Apply discrepancy bounds to construct adversarial utilities
  4. Prove distortion lower bounds (Theorems 3.3, 3.4)
  5. Empirically validate robustness suppression (Section 4)

- Design tradeoffs:
  - Ordinal data: scalable but Ω(√|S₀|) distortion guaranteed
  - Cardinal data: O(1) distortion possible but requires calibrated scoring
  - Task-specific heuristics: instruct labelers to reward robustness; works but requires a priori knowledge of desired behaviors

- Failure signatures:
  - Reasoning models trained via RLHF show suppressed backtracking
  - Preference data collection yields systematically shorter, less robust outputs
  - Over-regularization toward "clean" responses even when correct
  - Distortion increases with model complexity |S₀|

- First 3 experiments:
  1. Replicate Figure 4.3: Compare labeler preferences for backtracking vs. non-backtracking correct responses on MATH/LiveBench. Expect 60-80% preference for succinct.
  2. Cardinal feedback ablation: Add small amount of magnitude feedback (e.g., "rate 1-5 how much better") and measure reduction in reasoning behavior suppression.
  3. Robustness-to-perturbation test: Following Figure 4.1a, measure accuracy degradation under chain-of-thought corruptions for RLHF vs. RLVR post-trained models.

## Open Questions the Paper Calls Out

### Open Question 1
Can we design practical cardinal feedback mechanisms that effectively proxy true utility and mitigate distortion, and what task-specific heuristics are necessary to make them scalable? The paper proves that logarithmically few cardinal queries suffice theoretically (Theorem 3.5), but does not specify how to collect such data in practice or validate that it proxies true utility.

### Open Question 2
What formal characterizations distinguish tasks where RLHF succeeds (e.g., instruction-tuning, safety) from those where it fails (e.g., reasoning requiring robust strategies)? The paper heuristically categorizes tasks based on frontier capabilities but lacks crisp theoretical criteria for predicting success or failure.

### Open Question 3
Can task-aware annotation instructions (e.g., explicitly rewarding robust behaviors like backtracking) reliably overcome the suppression of reasoning strategies in preference data? This is proposed as a potential workaround but not empirically evaluated; it remains unclear whether such instructions generalize or introduce new biases.

## Limitations
- Theoretical framework assumes perfect preference data and infinite samples—conditions that may not hold in practice
- Circuit-routing model is a simplification that may not capture all aspects of modern large language models
- Empirical validation relies on proxy metrics (LM-based scoring, keyword detection for backtracking) that may not perfectly align with human judgment

## Confidence

- **High confidence**: The distortion lower bound (Theorem 3.3) and its implications for preference-based optimization—this follows from well-established social choice theory.
- **Medium confidence**: The claim that robustness behaviors are systematically suppressed by preference data—supported by empirical evidence but could be influenced by prompt engineering choices.
- **Medium confidence**: The circuit-routing model of post-training—plausible given empirical observations but not definitively proven.

## Next Checks

1. Replicate the preference experiment with human annotators on a small sample (n=20) to validate the LM-based preference judgments and check for consistency across labelers.
2. Test alternative robustness definitions by applying perturbations that specifically target backtracking (e.g., removing "Wait" statements) to isolate whether the robustness advantage comes from the backtracking itself or other correlated factors.
3. Measure distortion empirically by comparing RLHF-optimized models against cardinal-feedback models on reasoning tasks, quantifying the gap between achieved and optimal utilities under controlled conditions.