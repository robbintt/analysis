---
ver: rpa2
title: Improving the Effectiveness of Potential-Based Reward Shaping in Reinforcement
  Learning
arxiv_id: '2502.01307'
source_url: https://arxiv.org/abs/2502.01307
tags:
- reward
- potential
- shaping
- function
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of effective potential-based reward
  shaping (PBRS) in reinforcement learning, particularly how the effectiveness depends
  on initial Q-values and external rewards. The core method introduces a generalized
  framework for PBRS requirements and proposes a constant bias term in the potential
  function to improve shaping effectiveness without altering encoded preferences.
---

# Improving the Effectiveness of Potential-Based Reward Shaping in Reinforcement Learning

## Quick Facts
- arXiv ID: 2502.01307
- Source URL: https://arxiv.org/abs/2502.01307
- Reference count: 19
- Primary result: Introduces a constant bias term in potential-based reward shaping to improve effectiveness by decoupling shaped rewards from initial Q-values and external rewards, validated across tabular and deep RL settings.

## Executive Summary
This paper addresses a fundamental limitation in potential-based reward shaping (PBRS) where the effectiveness depends on the alignment between initial Q-values, external rewards, and the potential function. The authors introduce a generalized framework for PBRS requirements and propose a constant bias term that can be added to the potential function to improve shaping effectiveness without altering encoded preferences. Through theoretical analysis and experiments on Gridworld, Cart Pole, and Mountain Car environments, they demonstrate that correctly choosing this bias significantly improves sample efficiency compared to incorrect choices or no shaping. The paper also explores exponential potential functions as an alternative to linear potentials for reducing incorrect shaping signals.

## Method Summary
The method introduces a constant bias term b = (1−γ)Q_init − r_∞ added to the potential function as Φ_b(s) = Φ(s) + b/(γ−1). This shifts all shaped rewards (except terminal transitions) by b, ensuring that the TD-update target R' + γQ_init exceeds Q_init precisely when the transition leads to higher potential states. The approach works for both goal-directed MDPs (sparse rewards) and on-step MDPs (constant per-step rewards). The paper also proposes exponential potential functions e^Φ(s) with a tunable base e to reduce incorrect shaping signals for small potential differences compared to linear potentials.

## Key Results
- Correctly chosen bias (b = (1−γ)Q_init − r_∞) significantly improves sample efficiency in Gridworld, Cart Pole, and Mountain Car environments
- Incorrect bias choices lead to no improvement or worse performance compared to unshaped baselines
- Exponential potential functions show theoretical advantages for small potential differences but require manual tuning of base e
- The bias mechanism decouples shaped rewards from initial Q-values and external rewards while maintaining policy invariance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding a constant bias to the potential function improves PBRS effectiveness by decoupling shaped rewards from initial Q-values and external rewards.
- Mechanism: The bias term b = (1−γ)Q_init − r_∞ is added to the potential function as Φ_b(s) = Φ(s) + b/(γ−1). This shifts all shaped rewards (except terminal transitions) by b, ensuring that the TD-update target R' + γQ_init exceeds Q_init precisely when the transition leads to higher potential states. The agent can then immediately exploit shaped rewards via advantage-based action selection.
- Core assumption: The initial Q-values and per-step reward r_∞ are known or can be estimated; the potential function correctly encodes state preferences.
- Evidence anchors:
  - [abstract]: "We formally derive how a simple linear shift of the potential function can be used to improve the effectiveness of reward shaping without changing the encoded preferences in the potential function, and without having to adjust the initial Q-values"
  - [section 5.2]: "If we set b = (1−γ)Q_init − r_∞, we are able to remove the dependence on both the external reward and the initial Q-values"
  - [corpus]: Weak corpus support—related papers focus on alternative shaping methods (Action-Dependent, Bootstrapped) but do not validate this specific bias formulation.
- Break condition: In short-episode settings or tasks where distinguishing between terminal states is critical, the bias can incorrectly incentivize reaching any terminal state (including truncation) rather than the goal.

### Mechanism 2
- Claim: Exponential potential functions reduce incorrect shaping signals for small potential differences compared to linear potential functions.
- Mechanism: Linear potentials produce incorrect signed shaping rewards when δ < ((1−γ)/γ)Φ(s) (positive potentials) or δ > ((γ−1)/γ)Φ(s) (negative potentials). Exponential potentials e^Φ(s) make the threshold for correct shaping depend on log(Φ(s)) rather than Φ(s) directly, allowing smaller potential differences to still yield correct positive/negative shaping rewards.
- Core assumption: The base e of the exponential can be chosen appropriately for the environment's potential granularity.
- Evidence anchors:
  - [abstract]: "We show the theoretical limitations of continuous potential functions for correctly assigning positive and negative reward shaping values"
  - [section 5.3]: Equations 13–18 derive the limitations; "the smallest supported difference δ would scale with the logarithm of the potential, which would still be beneficial over the linear scaling"
  - [corpus]: No direct validation in corpus; related work does not address exponential vs linear potentials.
- Break condition: If the potential function changes dynamically during training, the Q-values may become misaligned with the new potential, reducing effectiveness.

### Mechanism 3
- Claim: PBRS effectiveness is constrained by bounds on potential scale in goal-directed MDPs, preventing simple scaling as a remedy for Q-value mismatch.
- Mechanism: For goal-directed rewards, inequalities r_∞ − (1−γ)Q_init < Φ(s) < r_g − (1−γ)Q_init bound the potential function. Scaling cannot simultaneously satisfy incentive requirements for both goal-reaching and non-terminal transitions when Q_init and r_∞ are misaligned.
- Core assumption: The task is episodic with distinguishable goal terminal states (r_g > r_∞).
- Evidence anchors:
  - [section 5.1]: "it is not possible to utilize the scale of the potential function to compensate the mismatch between the original rewards and the initial Q-values"
  - [section 6.2.1]: Gridworld experiments confirm only the correctly biased potential converges within training budget.
  - [corpus]: No corpus papers address this scale-limitation result.
- Break condition: On-step reward functions (r_g = r_∞) cannot create correct goal incentives via PBRS alone.

## Foundational Learning

- Concept: **Potential-Based Reward Shaping (PBRS)**
  - Why needed here: The entire paper modifies PBRS; you must understand F(s,a,s') = γΦ(s') − Φ(s) and why policy invariance holds.
  - Quick check question: Given Φ(s)=0.5, Φ(s')=0.8, γ=0.9, what is the shaping reward for transition s→s'?

- Concept: **TD-Update and Q-Value Dynamics**
  - Why needed here: The bias mechanism is derived from whether R' + γQ_init exceeds Q_init in early updates.
  - Quick check question: If Q_init = 0 and R' = −0.05, will the Q-value increase or decrease after the first update?

- Concept: **Goal-Directed vs On-Step Reward Structures**
  - Why needed here: The bias formula and scale bounds differ fundamentally between sparse goal rewards and dense per-step rewards.
  - Quick check question: In an on-step reward setting with r = −1, what problem arises for terminal state incentives?

## Architecture Onboarding

- Component map:
  - Potential function Φ(s): encodes prior knowledge (e.g., distance to goal, inverse angle)
  - Bias module: computes b = (1−γ)Q_init − r_∞ and applies Φ_b(s) = Φ(s) + b/(γ−1) for non-terminal states
  - Exponential transform (optional): converts Φ(s) to e^Φ(s) with base e as hyperparameter
  - Standard PBRS wrapper: computes F(s,a,s') = γΦ_b(s') − Φ_b(s) and adds to environment reward

- Critical path:
  1. Identify r_∞ (per-step reward) and estimate Q_init from network initialization
  2. Compute bias b; apply to potential for all non-terminal states
  3. Ensure Φ(terminal) = 0 (required for policy invariance)
  4. (Optional) Apply exponential transform with e ∈ [8, 64] per paper experiments

- Design tradeoffs:
  - Larger bias magnitudes improve non-terminal guidance but risk incorrect terminal-state incentives
  - Higher exponential base e supports smaller potential differences but may reduce interpretability
  - Exact Q_init estimation is difficult in deep RL; approximate ranges [−1, 1] were used in experiments

- Failure signatures:
  - Agent converges to non-goal terminal states → bias too large, terminal incentives dominate
  - No improvement over unshaped baseline → bias sign incorrect or potential function poorly aligned with task
  - High variance between seeds → potential function gives conflicting signals near truncation boundaries

- First 3 experiments:
  1. Tabular Gridworld with goal-directed reward (r_g=1, r_∞=0), Q_init ∈ {−1, 0, 1}, bias b ∈ {−0.05, 0, 0.05}; verify only matching b converges quickly.
  2. Mountain Car with r=−1 per step, test b=0 vs b=1; confirm b=1 (offsetting negative reward) yields reliable solving.
  3. Cart Pole with r=+1 per step, test b ≤ 0 vs b > 0; verify negative/zero bias accelerates convergence while positive bias fails.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can reinforcement learning agents maintain effective convergence and high sample efficiency when utilizing dynamically changing potential functions without resetting previously learned Q-values?
- Basis in paper: [explicit] The authors state, "we leave the details of how to still guarantee effective convergence and obtain high sample efficiency for dynamically changing potential functions for future work."
- Why unresolved: While the paper suggests dynamic adjustment of the exponential base $e$ could optimize shaping for minimal potential differences, changing the potential function mid-training invalidates the relationship between the current Q-values and the new shaping rewards.
- What evidence would resolve it: An algorithm capable of online adaptation of the potential function (e.g., the base $e$) that preserves the Q-value policy structure or demonstrates faster convergence without requiring a Q-value reset.

### Open Question 2
- Question: How can the constant bias shift technique be modified to correctly handle transitions into terminal states, particularly in environments with short episode lengths?
- Basis in paper: [inferred] Section 5.2 explicitly identifies a limitation where the bias term removes dependence for non-terminal transitions but creates "incorrect (dis-)incentivizing transitions into terminal states," limiting application in short episodes or those with distinct terminal states.
- Why unresolved: The theoretical requirement that terminal potential must be zero ($\Phi(s_{terminal})=0$) prevents the bias term from adjusting the shaped reward for the final transition, causing the agent to potentially prefer truncation over reaching the actual goal.
- What evidence would resolve it: A modification of the framework or a theoretical proof showing how to align terminal incentives with the bias shift without violating the policy invariance guarantee required for terminal states.

### Open Question 3
- Question: To what extent does the non-stationary nature of Q-values in deep reinforcement learning affect the robustness of the fixed bias term $b = (1-\gamma)Q_{init} - r_\infty$?
- Basis in paper: [inferred] In Section 6.2.2, the authors note that in Deep Q-Networks (DQN), "Q-values for not visited states can change at any time during the training," making a precise match with the initial Q-values difficult.
- Why unresolved: The derivation of the bias relies on a fixed $Q_{init}$, but function approximation causes Q-values to fluctuate globally during training, potentially rendering a static bias term $b$ suboptimal or incorrect as training progresses.
- What evidence would resolve it: An empirical analysis tracking the divergence between the static bias $b$ and the dynamic distribution of Q-values over training steps, or an adaptive bias method that tracks these changes.

### Open Question 4
- Question: Is there a theoretical method for automatically determining the optimal base parameter $e$ for exponential potential functions based on the granularity of the state space?
- Basis in paper: [inferred] Section 5.3 introduces exponential potentials to solve issues with small gradients, and Section 6.1.3 mentions the base $e=32$ was chosen experimentally ("Larger values for $e$ did not show any different results").
- Why unresolved: The paper demonstrates that exponential potentials help but does not provide a theoretical grounding for selecting $e$, relying instead on manual tuning.
- What evidence would resolve it: A formula or heuristic that correlates the minimum potential difference $\delta$ and discount factor $\gamma$ with the necessary base $e$ to ensure correct shaping sign (positive/negative) for all transitions.

## Limitations

- The exponential potential function's effectiveness remains theoretical without rigorous experimental validation beyond the basic scaling advantage.
- The bias mechanism assumes accurate knowledge of initial Q-values and per-step reward r_∞, which is difficult to estimate in deep RL settings and may lead to suboptimal bias selection.
- The paper's conclusions about scale limitations for goal-directed MDPs rely on idealized assumptions about reward structures and may not generalize to more complex, multi-goal environments.

## Confidence

- **High Confidence**: The core bias mechanism and its mathematical derivation for improving PBRS effectiveness are sound and well-supported by the theoretical analysis.
- **Medium Confidence**: Experimental results on simple environments validate the bias approach, but the generalization to complex deep RL tasks needs further testing.
- **Low Confidence**: The proposed exponential potential functions lack sufficient experimental validation to confirm their practical advantages over linear potentials.

## Next Checks

1. Test the bias mechanism with estimated vs. exact Q_init values in deep RL to assess robustness to initialization uncertainty.
2. Conduct experiments comparing exponential and linear potentials on a challenging continuous control task to validate the theoretical scaling advantages.
3. Evaluate the bias approach on a multi-goal environment to test its effectiveness beyond the simple goal-directed and on-step reward structures studied in the paper.