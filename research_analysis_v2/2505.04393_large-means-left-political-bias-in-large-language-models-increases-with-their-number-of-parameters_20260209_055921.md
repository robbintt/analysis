---
ver: rpa2
title: 'Large Means Left: Political Bias in Large Language Models Increases with Their
  Number of Parameters'
arxiv_id: '2505.04393'
source_url: https://arxiv.org/abs/2505.04393
tags:
- political
- german
- alignment
- parties
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper quantifies political bias in large language models (LLMs)
  using the Wahl-O-Mat alignment metric from German politics. LLMs are prompted in
  both German and English with 38 political statements and scored on alignment with
  major German parties.
---

# Large Means Left: Political Bias in Large Language Models Increases with Their Number of Parameters

## Quick Facts
- arXiv ID: 2505.04393
- Source URL: https://arxiv.org/abs/2505.04393
- Reference count: 40
- LLMs show increasing left-leaning bias with model size (9% for 7-8B, 14% for 32B, 22% for 70B)

## Executive Summary
This paper quantifies political bias in large language models (LLMs) using the Wahl-O-Mat alignment metric from German politics. LLMs are prompted in both German and English with 38 political statements and scored on alignment with major German parties. Results show all models exhibit a consistent left-leaning bias, with severity increasing alongside model size (9% for 7-8B models, 14% for 32B, 22% for 70B). The bias is stronger in English than German prompts, with language changes shifting bias scores by 3.5 percentage points on average. Release date also impacts bias, with newer models showing greater left-leaning tendencies. The Chinese-developed DeepSeek R1 exhibits less bias than American models, suggesting origin has minimal effect.

## Method Summary
The study evaluates 7 open-source LLMs (7B-70B parameters) using 38 German political statements from the Wahl-O-Mat instrument, translated into English. Models are prompted with forced-answer formatting (`<ANSWER>Yes</ANSWER>`) in both languages. Responses are parsed and compared against official party positions to calculate alignment scores. A theta (θ) positioning score is derived using Bundestag seating arrangement weights to quantify left-right positioning. The methodology tests four factors: model size, language, release date, and model origin.

## Key Results
- All models exhibit consistent left-leaning bias (θ < 0), with severity increasing by parameter count: 9% for 7-8B models, 14% for 32B, 22% for 70B
- English prompts amplify left-leaning bias by 3.5 percentage points compared to German, with fewer neutral answers in English
- Newer models show greater bias (3 percentage point increase from Llama 2 to Llama 3 equivalents)
- DeepSeek R1 (Chinese) shows less bias than American models, indicating origin has minimal effect

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Political bias magnitude scales positively with model parameter count
- Mechanism: Larger models more completely internalize distributional patterns in training corpora, including political skews. As capacity increases, models capture higher-order correlations between linguistic patterns and ideological content that smaller models cannot represent.
- Core assumption: Training data contains systematic political biases that scale with model capacity rather than averaging out
- Evidence anchors: [abstract] "Results show all models exhibit a consistent left-leaning bias, with severity increasing alongside model size"; [section III] "larger models are consistently positioned further left than smaller ones"
- Break condition: If smaller models exhibited equal or greater bias than larger models within the same model family

### Mechanism 2
- Claim: Prompt language influences expressed political positioning, with English amplifying left-leaning bias relative to German
- Mechanism: Tokenization differences, language-specific training data distributions, and cultural-linguistic framing may cause models to retrieve different political associations. English-language training data may contain stronger progressive/left-leaning discourse patterns.
- Core assumption: Political concepts do not transfer identically across languages; the semantic space for political terms differs by language
- Evidence anchors: [abstract] "The bias is stronger in English than German prompts, with language changes shifting bias scores by 3.5 percentage points on average"
- Break condition: If language had no systematic directional effect

### Mechanism 3
- Claim: Model release date correlates with bias intensity, with newer models showing increased left-leaning tendencies
- Mechanism: Temporal drift in training data composition (more recent web content reflecting progressive discourse trends) and/or evolution in alignment/RLHF procedures may introduce stronger political conditioning over time.
- Core assumption: Neither training data curation nor alignment procedures are politically neutral; both may shift over development cycles
- Evidence anchors: [section III] "Direct comparisons of Llama 2 8B and Llama 2 70B to the equivalent Llama 3 8B and Llama 3 70B show that the newer models, on average, have a 3 percentage point higher θ score"
- Break condition: If newer models showed reduced bias, or if the temporal effect reversed within model families

## Foundational Learning

- Concept: Wahl-O-Mat alignment scoring
  - Why needed here: The entire methodology hinges on this metric, which computes agreement between a respondent's positions and party platforms across 38 political statements
  - Quick check question: If an LLM agreed with 30 of 38 statements exactly as the Green party does, what would its alignment score be with that party?

- Concept: θ (theta) score as left-right positioning
  - Why needed here: The paper reduces multi-party alignment to a single-axis positioning metric using Bundestag seating arrangement weights
  - Quick check question: Why does θ use seat-weighted positioning rather than simple alignment averages?

- Concept: Scaling laws and emergent capabilities
  - Why needed here: The central finding—that bias intensifies with scale—connects to broader research on what properties emerge or amplify as model size increases
  - Quick check question: Name one capability that improves with scale and one behavior or risk that may also increase

## Architecture Onboarding

- Component map: 38 Wahl-O-Mat statements → System message + human prompt → 7 LLMs (7B-70B) → Regex parsing → Alignment formula (Eq 1) → θ score (Eq 2) → Cross-factor comparison

- Critical path: 1. Prompt each model with all 38 statements in both languages 2. Extract Yes/Neutral/No responses 3. Compute alignment matrix per party for each language 4. Convert alignments to θ positioning scores 5. Compare across factors (size, language, release date, origin)

- Design tradeoffs: German political context with English prompts tests cross-linguistic transfer but may not generalize to other political systems; forced three-choice responses improve comparability but lose nuance; Bundestag seat-alignment for θ grounds scores but ties metric to specific electoral outcome; open-source models only enable reproducibility but exclude proprietary systems

- Failure signatures: Models outputting explanations instead of tokens require manual extraction; DeepSeek R1's reasoning preamble needs special token-wrapping; Simplescaling S1 producing no neutral answers in English indicates language-specific calibration issues; inconsistent Neutral usage across models complicates cross-model comparison

- First 3 experiments: 1. Replicate with US-focused political instruments to test generalizability beyond German politics 2. Control for prompt framing by testing neutral vs opinion-eliciting system prompts 3. Run ablation across temperature and sampling parameters to determine bias consistency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does the political bias inherent in LLMs quantifiably influence real-world voter decision-making processes?
- Basis in paper: [explicit] The authors state they "cannot, however, comment on the extent to which LLMs influenced political opinions"
- Why unresolved: The study quantifies bias within the models but lacks the necessary data on user interaction and trust to bridge the gap between model output and human behavioral change
- What evidence would resolve it: A study correlating specific LLM usage patterns with shifts in voter sentiment or longitudinal surveys tracking political changes in users who rely on LLMs for information

### Open Question 2
- Question: What is the primary technical source of the observed left-leaning bias: training data composition, tokenizer skewing, or representation gaps in model memory?
- Basis in paper: [explicit] The authors note, "It is not certain where the bias originates," but suggest "inherent bias in the training data," "representation gaps," or "data skewing by the tokenizer" as reasonable estimates
- Why unresolved: The paper measures the output (the "what") but does not perform ablation studies or component-level analysis to determine the internal mechanics (the "why") of the bias
- What evidence would resolve it: Mechanistic interpretability studies or controlled training experiments where datasets are modified to balance political representation

### Open Question 3
- Question: Is the correlation between model size and political bias causal, or is it confounded by the tendency for newer, larger models to utilize more recent, potentially more biased training corpora?
- Basis in paper: [inferred] The results show that "Newer models are more prone to expressing political bias" and that "release date also impacts bias"
- Why unresolved: The study observes a correlation with size and recency simultaneously but does not isolate variables to determine if scaling architecture alone increases bias independent of data freshness
- What evidence would resolve it: An evaluation of models with identical parameter counts trained on data from different time periods

## Limitations

- Language-specific political framing: German political context with English translation may not generalize to other political systems
- Unknown training data provenance: Cannot definitively isolate whether bias originates from web crawl distributions, alignment procedures, or model architecture
- Model coverage constraints: Only 7 open-source models examined, excluding major proprietary systems like GPT-4, Claude, or Gemini

## Confidence

- High confidence: Directional finding that all tested models exhibit left-leaning bias is robust across languages, model families, and parameter scales
- Medium confidence: Cross-linguistic effect (English amplifying bias by 3.5 percentage points) may reflect translation artifacts rather than inherent language properties
- Low confidence: Claim about minimal impact from model origin based on single non-Western model comparison is underdetermined

## Next Checks

1. Cross-political system replication: Run the same methodology using US political instruments to determine whether left-leaning bias is universal or specific to German political framing

2. Training data audit: Analyze the actual training corpora used by these models for political content distribution, focusing on whether progressive/left-leaning discourse is systematically overrepresented in web-scale datasets

3. Alignment procedure isolation: Test whether bias magnitude varies with different alignment approaches (base models vs instruction-tuned vs RLHF variants) within the same model family to isolate the contribution of alignment techniques versus pretraining data