---
ver: rpa2
title: Correlation-Aware Feature Attribution Based Explainable AI
arxiv_id: '2511.16482'
source_url: https://arxiv.org/abs/2511.16482
tags:
- excir
- features
- table
- agreement
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes ExCIR, a correlation-aware feature attribution
  method designed to provide computationally efficient, consistent, and scalable explanations
  for complex black-box models. ExCIR quantifies sign-aligned co-movement between
  features and model outputs after robust centering (e.g., median or mid-mean), producing
  scores that are translation/scale invariant and monotonic in aligned evidence.
---

# Correlation-Aware Feature Attribution Based Explainable AI

## Quick Facts
- arXiv ID: 2511.16482
- Source URL: https://arxiv.org/abs/2511.16482
- Reference count: 37
- Proposes ExCIR: correlation-aware feature attribution method providing computationally efficient, consistent, and scalable explanations for complex black-box models

## Executive Summary
This paper introduces ExCIR, a correlation-aware feature attribution method designed to provide computationally efficient, consistent, and scalable explanations for complex black-box models. ExCIR quantifies sign-aligned co-movement between features and model outputs after robust centering (e.g., median or mid-mean), producing scores that are translation/scale invariant and monotonic in aligned evidence. A lightweight transfer protocol enables reproducing full-model rankings using only 20-40% of the data, reducing runtime by 3-9×. Experiments on 29 heterogeneous datasets (text, tabular, image, signal, networks) show strong agreement with established baselines: high top-k Jaccard overlap (>0.85), small Procrustes residuals, and stable rank correlations (Spearman/Kendall) when using the lightweight protocol. ExCIR also introduces BLOCKCIR for groupwise scoring of correlated features, mitigating double-counting. Overall, ExCIR delivers global feature rankings efficiently while maintaining accuracy and robustness across modalities.

## Method Summary
ExCIR computes correlation-aware feature attributions through a single-pass accumulation algorithm. For each feature j, it first centers values using a robust location estimate (mid-mean or median) to obtain x̃_ij and ỹ_i. Then it accumulates per-feature evidence N_j = Σ(x̃_ij × ỹ_i) and co-movement mass D_j = Σ|x̃_ij × ỹ_i|. The final Correlation Impact Ratio is CIR_j = 0.5 × (1 + N_j/D_j), producing bounded [0,1] scores that capture sign-aligned co-movement. BLOCKCIR extends this to groups G by aggregating evidence before scoring. The lightweight transfer protocol runs ExCIR on subsampled data (f=20-40%) to reproduce full-data rankings with 3-9× speedup while maintaining high top-k agreement.

## Key Results
- ExCIR achieves high top-k Jaccard overlap (>0.85) with full-data rankings using only 20-40% of data
- Runtime reduction of 3-9× through lightweight transfer protocol
- Strong agreement with baselines: Spearman/Kendall rank correlations remain stable at subsampled fractions
- BLOCKCIR effectively mitigates double-counting for correlated feature groups while preserving top-ranked features

## Why This Works (Mechanism)

### Mechanism 1: Robust Centering with Sign-Aligned Co-Movement Scoring
ExCIR uses mid-mean centering (average of 25th and 75th percentiles) to compute Correlation Impact Ratio (CIR) = 0.5 × (1 + N/D), where N is total signed evidence and D is total co-movement mass. This produces translation- and positive-scale invariant scores that are monotonic in aligned evidence. The linear co-movement assumption captures most feature-output relationships, though nonlinear patterns (e.g., sinusoidal) produce near-zero scores.

### Mechanism 2: Lightweight Transfer via Controlled Subsampling
The lightweight transfer protocol achieves 3-9× speedup by running ExCIR on 20-40% of rows under identical training/validation splits. Global correlation structure stabilizes with modest sample sizes, preserving top-k rankings. The protocol assumes the subsample preserves the same joint distribution as full data; covariate shift invalidates this approach.

### Mechanism 3: Groupwise Aggregation (BLOCKCIR) for Correlated Feature Clusters
BLOCKCIR aggregates co-movement terms over predefined or data-driven feature groups before scoring, treating correlated features as single units. This reduces double-counting while preserving top-ranked features. The approach assumes group definitions capture meaningful feature clusters; poorly defined groups may obscure signal or over-aggregate unrelated features.

## Foundational Learning

- **Concept**: Robust location estimation (mid-mean, median, trimmed mean)
  - Why needed here: ExCIR's translation invariance depends on centering with outlier-resistant statistics; mean-based centering drifts under heavy tails
  - Quick check question: Given samples [1, 2, 3, 100], what is the mid-mean vs. arithmetic mean, and which better represents central tendency?

- **Concept**: L1-normalized covariance and correlation measures
  - Why needed here: CIR is formulated as signed L1-normalized covariance; L1 normalization provides bounded [0,1] scores
  - Quick check question: Why does normalizing by Σ|p_ij| (L1) produce a bounded ratio while normalizing by sqrt(Σp²_ij) (L2) would not?

- **Concept**: Streaming quantile algorithms (Greenwald-Khanna, t-digest)
  - Why needed here: ExCIR claims streaming-friendliness via compact quantile sketches; practical deployment requires understanding memory-accuracy tradeoffs
  - Quick check question: What is the memory complexity of maintaining mid-mean estimates for d features in a stream using t-digest?

## Architecture Onboarding

- **Component map**: Preprocessing layer (compute mid-means) -> Accumulation layer (single-pass scan maintaining N_j, D_j) -> Scoring layer (compute CIR_j) -> Lightweight transfer layer (subsample at fraction f)
- **Critical path**: Preprocessing → Accumulation → Scoring. The accumulation step is Θ(nd) time and Θ(d + |G|) memory; this dominates runtime.
- **Design tradeoffs**: 
  - Mid-mean vs. median centering: Mid-mean (default) and median yield near-identical rankings; median is simpler but mid-mean is more stable under symmetric heavy tails
  - f=0.2 vs. f=0.4: Lower f gives faster runtime but may miss rare-feature signals; Pareto analysis shows knees typically at f=0.2-0.4
  - Per-feature vs. BLOCKCIR: BLOCKCIR adds Θ(|G|) memory; skip if features are approximately independent
- **Failure signatures**:
  - Nonlinear relationships: ExCIR scores approach 0 (sinusoidal score = 0.004). Diagnostic: check residual scatterplots of x̃ vs. ỹ
  - Causal confounding: ExCIR detects association, not causation (scores 0.997 vs 0.999 under confounding)
  - Multicollinearity: Both correlated features receive identical scores (0.80). Use BLOCKCIR to diagnose
  - Heavy-tailed noise: Scores drop to 0.007. Mid-mean centering helps but extreme outliers may still dominate
- **First 3 experiments**:
  1. Sanity check on synthetic data: Generate X with known ground-truth feature importance, verify ExCIR rankings correlate with true importance (expect Spearman ρ ≈ 0.34)
  2. Lightweight transfer validation: Run ExCIR at f ∈ {0.2, 0.4, 0.6, 1.0} on a held-out dataset; plot Jaccard@8 vs. runtime to identify Pareto knee
  3. Correlation robustness test: Create dataset with two highly correlated features (ρ > 0.9); compare per-feature CIR vs. BLOCKCIR with group {j₁, j₂}

## Open Questions the Paper Calls Out

### Open Question 1
How can the proposed Mutual-ExCIR (mCIR) extension capture non-linear dependencies without sacrificing the single-pass, low-computational-cost architecture of the base ExCIR algorithm? Standard mutual information estimation is computationally expensive and requires density estimation, which conflicts with ExCIR's Θ(nd) efficiency goal. Evidence would be a formulation running in linear time that demonstrates high attribution scores on synthetic non-linear benchmarks where current ExCIR fails.

### Open Question 2
What is the precise mathematical formulation of Conditional ExCIR (cCIR) for isolating unique contributions from correlated neighbors, and does it provide advantages over existing partial correlation techniques? While BLOCKCIR groups features, the paper doesn't define a mechanism for statistically conditioning out neighbor influence to find unique contribution of a single feature within a correlated block. Evidence would be empirical evaluation on datasets with controlled multicollinearity showing cCIR effectively distinguishing unique predictors better than standard partial correlation.

### Open Question 3
To what extent does the lower top-k sufficiency of ExCIR (approx. 0.62 accuracy) compared to model-specific baselines like TreeGain (approx. 0.77) limit its utility for error analysis in high-stakes debugging? If features ranked high by ExCIR don't preserve model accuracy as effectively as TreeGain, practitioners may miss critical features required for debugging model failures despite efficiency gains. Evidence would be a user study or quantitative analysis measuring "debugging success rate" when using ExCIR vs. TreeGain on corrupted datasets.

## Limitations
- Lightweight transfer protocol's generalizability across heterogeneous data distributions remains untested; extreme heterogeneity or rare subpopulations could invalidate subsampling approach
- BLOCKCIR aggregation mechanism assumes well-defined feature groups, but paper doesn't specify how to construct groups when domain knowledge is unavailable
- ExCIR's linear co-movement assumption fails for strongly nonlinear relationships, producing near-zero scores and limiting applicability to purely linear attribution problems

## Confidence

- **High confidence**: Computational efficiency claims (3-9× speedup), top-k Jaccard overlap results (>0.85 at f=0.2-0.4), and translation/scale invariance properties are well-supported by experimental design and multiple datasets
- **Medium confidence**: Lightweight transfer protocol's effectiveness across unseen data distributions and BLOCKCIR aggregation approach's robustness to poorly defined groups need further validation
- **Low confidence**: Claims about handling causal confounding and nonlinear relationships are explicitly limited, with clear failure modes documented

## Next Checks

1. **Heterogeneity stress test**: Apply lightweight transfer (f=0.2, 0.4, 1.0) to a dataset with known subpopulation structure (e.g., imbalanced classification with rare classes) and measure whether top-k Jaccard overlap degrades below 0.85 at f=0.4

2. **BLOCKCIR group construction**: Implement data-driven group creation using correlation thresholds (e.g., |ρ| > 0.8) on the gene_expression dataset and compare results against the paper's unspecified "data-driven" approach

3. **Nonlinear relationship benchmark**: Create a synthetic dataset with known nonlinear feature-output relationships (e.g., sinusoidal, polynomial interactions) and verify ExCIR produces near-zero scores while baseline methods (SHAP, PFI) capture the nonlinear patterns