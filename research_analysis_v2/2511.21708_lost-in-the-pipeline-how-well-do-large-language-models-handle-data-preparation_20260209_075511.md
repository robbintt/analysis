---
ver: rpa2
title: 'Lost in the Pipeline: How Well Do Large Language Models Handle Data Preparation?'
arxiv_id: '2511.21708'
source_url: https://arxiv.org/abs/2511.21708
tags:
- data
- llms
- preparation
- tasks
- users
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) can
  effectively support users in data preparation tasks such as profiling, cleaning,
  and wrangling tabular data. The authors evaluate both general-purpose and tabular-specific
  LLMs using a custom-designed quality model that incorporates traditional data quality
  metrics and newly introduced metrics tailored to LLM outputs.
---

# Lost in the Pipeline: How Well Do Large Language Models Handle Data Preparation?

## Quick Facts
- arXiv ID: 2511.21708
- Source URL: https://arxiv.org/abs/2511.21708
- Reference count: 21
- Primary result: General-purpose LLMs outperform tabular-specific models on data preparation tasks, with performance degrading as dataset dirtiness increases

## Executive Summary
This paper investigates whether large language models can effectively support data preparation tasks including profiling, cleaning, and wrangling tabular data. The authors evaluate both general-purpose and tabular-specific LLMs using a custom quality model that incorporates traditional data quality metrics and newly introduced metrics tailored to LLM outputs. A user study with 61 participants validates the quality model and refines it based on practitioner expectations. Experiments with polluted datasets show that general-purpose LLMs outperform tabular-specific models across most tasks, with varying effectiveness depending on task complexity and dataset dirtiness. The study concludes that while LLMs cannot yet replace traditional tools, they can complement them by offering contextual insights.

## Method Summary
The authors created a 100-row sample from a Kaggle USA real estate dataset and injected errors at 10%, 30%, and 50% contamination levels using six pollution functions (outliers, de-standardization, missing values, duplicates, structural issues, dependencies). They tested eight LLMs including GPT-4o, Claude-3-5-sonnet, Gemini-1.5-pro, Llama-3.3-70b-versatile, DeepSeek-V3 (API), and TableGPT2-7B, TableLLM-13b (local) using a unified prompt template with temperature=0. A custom quality model with five metrics (Completeness, Accuracy, Prescriptivity, Readiness, Specificity) was used to evaluate outputs through manual checklist scoring, with F1 scores for detection tasks.

## Key Results
- General-purpose LLMs consistently outperform tabular-specific models across all evaluated tasks
- LLM performance degrades predictably as dataset dirtiness increases
- The five-dimensional quality model captures practitioner-relevant aspects of LLM output utility
- TableLLM frequently fails to produce any relevant content, while other models show task-specific strengths

## Why This Works (Mechanism)

### Mechanism 1
General-purpose LLMs outperform tabular-specific fine-tuned models because their broader training corpus enables more flexible semantic reasoning about data context. GLLMs leverage diverse pre-training on code, documentation, and data science literature to recognize patterns like format inconsistencies and missing value representations, then map these to appropriate remediation strategies. Tabular-specific models appear to overfit to narrow table structures, losing transferability.

### Mechanism 2
LLM effectiveness degrades predictably with dataset dirtiness, particularly for tasks requiring precise counting or multi-step reasoning. As error density increases, LLMs face compounding ambiguity—each additional pollution type increases the solution search space. Fixed context windows limit exposure to full dataset patterns, unlike traditional tools with unlimited input.

### Mechanism 3
User-validated quality metrics (Completeness, Accuracy, Prescriptivity, Readiness, Specificity) capture distinct dimensions of practical utility that traditional accuracy-only metrics miss. Practitioners value not just whether an answer is correct, but whether it covers all issues, provides executable guidance, requires no post-processing, and targets specific columns.

## Foundational Learning

- **Data Quality Dimensions**: Traditional metrics like Completeness, Accuracy, Consistency, and Timeliness extend to the quality model; understanding their definitions is prerequisite to interpreting results. Quick check: If an LLM correctly identifies 8 of 10 missing values but proposes invalid imputation methods for 3, what are its Completeness and Accuracy scores?

- **Precision-Recall-F1 for Detection Tasks**: Outlier detection, deduplication, and dependency discovery use F1 scoring; results tables report these directly. Quick check: A model detects 6 true duplicates, misses 4, and falsely flags 2 non-duplicates. What are its precision, recall, and F1?

- **Temperature Parameter in LLM Sampling**: All models were tested at temperature=0 (deterministic); understanding why this matters for reproducibility is critical. Quick check: Why would temperature=0 be essential for a benchmark comparing model capabilities on data preparation tasks?

## Architecture Onboarding

- **Component map**: Polluted Datasets (100-row sample) -> Prompt Construction (CSV text + task name) -> LLM Inference (temperature=0) -> Manual Checklist Scoring -> Metric Aggregation -> Cross-model Comparison

- **Critical path**: Dataset pollution → Prompt construction → LLM inference → Manual checklist scoring → Metric aggregation → Cross-model comparison

- **Design tradeoffs**: Single-shot prompting vs. multi-turn (authors chose single-shot for fairness but acknowledge this limits complex tasks); Manual vs. automated evaluation (manual ensures validity but doesn't scale; automation is listed as future work); 100-row sample vs. larger datasets (small sample enables exhaustive manual review but may not reflect real-world complexity)

- **Failure signatures**: TLLM silent failures (TableLLM often "fails entirely, not including any relevant content"); Task confusion (GPT and TableGPT2 conflate standardization with normalization); Non-exact duplicate blindness (all models fail on near-duplicates)

- **First 3 experiments**: 1) Baseline replication: Run GPT-4o and Claude on 30% dirtiness dataset for data cleaning; verify Completeness > 0.5 and Readiness variance across models matches reported results; 2) Metric sensitivity analysis: For data imputation task, ablate each metric (set to N/A) and observe ranking changes—if rankings are stable, metrics may be redundant; 3) Pollution-type isolation: Test outlier detection on only-outlier-polluted data vs. mixed-pollution data; quantify degradation attributable to interference vs. pure error density

## Open Questions the Paper Calls Out

1. Can iterative prompt engineering or conversational interaction significantly improve LLM performance in data preparation tasks compared to single-shot prompting? The current study relied on a single-turn prompt approach (zero-shot), whereas complex data cleaning often requires multi-turn reasoning to correct specific errors.

2. To what extent can the manual evaluation framework be automated using LLM-based evaluators without inheriting model biases? The authors identify "automating the evaluation framework instead of manually assessing the answers" as a necessary next step.

3. How robust are the data preparation capabilities of general-purpose LLMs when applied to datasets with heterogeneous schemas and domains beyond the single real estate dataset tested? The experiments were restricted to a 100-row sample from a single Kaggle real estate dataset, limiting the generalizability of the findings to other data structures or industries.

## Limitations
- Manual evaluation bottleneck: Human checklist scoring is inherently subjective and non-scalable
- Single-shot prompting constraint: Artificially caps performance on multi-step tasks
- Dataset size and representativeness: 100-row samples from one domain may not capture performance on larger, more complex tables

## Confidence

- **High confidence**: General-purpose LLMs outperform tabular-specific models on data preparation tasks, and this performance degrades with dataset dirtiness. Supported by systematic experiments across multiple tasks and pollution levels with clear statistical patterns.

- **Medium confidence**: The five-dimensional quality model captures practitioner-relevant aspects of LLM output utility. While user study validation exists, the manual evaluation methodology limits generalizability and reproducibility.

- **Low confidence**: Tabular-specific models underperform due to overfitting that narrows reasoning vocabulary. The paper demonstrates the performance gap but doesn't test alternative fine-tuning strategies or provide mechanistic evidence for why this occurs.

## Next Checks

1. Run ablation studies removing each quality metric individually to quantify redundancy and identify which dimensions drive ranking changes between models.

2. Test outlier detection performance on datasets containing only outliers versus mixed-pollution datasets to quantify interference effects versus pure error-density degradation.

3. Compare single-shot versus iterative prompting for complex tasks like dependency discovery to quantify the performance ceiling imposed by the single-turn constraint.