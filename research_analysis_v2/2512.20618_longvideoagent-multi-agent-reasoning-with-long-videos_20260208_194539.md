---
ver: rpa2
title: 'LongVideoAgent: Multi-Agent Reasoning with Long Videos'
arxiv_id: '2512.20618'
source_url: https://arxiv.org/abs/2512.20618
tags:
- agent
- visual
- arxiv
- grounding
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses long-video question answering (QA), where
  information is sparse across hours of content and multiple modalities. Most existing
  methods compress the video into summaries or rely on limited toolsets, which leads
  to loss of fine-grained cues and weakens temporal grounding.
---

# LongVideoAgent: Multi-Agent Reasoning with Long Videos

## Quick Facts
- arXiv ID: 2512.20618
- Source URL: https://arxiv.org/abs/2512.20618
- Reference count: 16
- Primary result: A multi-agent framework achieves state-of-the-art long-video QA performance by iteratively coordinating grounding and vision agents with RL fine-tuning.

## Executive Summary
This paper tackles long-video question answering, where relevant information is sparse across hours of content and multiple modalities. The authors propose a multi-agent framework in which a master LLM coordinates two specialist agents: one for temporal grounding and one for visual extraction. Through iterative planning and tool-calling, the system accumulates evidence across multiple turns until confident to answer. The framework is evaluated on newly constructed episode-level datasets (LongTVQA, LongTVQA+) derived from TVQA/TVQA+, showing significant improvements over non-agent baselines, with further gains from RL fine-tuning.

## Method Summary
The method employs a master LLM that iteratively plans and calls two specialist agents: a grounding agent that temporally localizes question-relevant segments from subtitles, and a vision agent that extracts targeted visual observations from those segments. The master agent maintains a running context and decides at each step whether to request grounding, visual inspection, or answer, continuing up to K steps or until confident. For open-source LLM backbones, the framework is trained with reinforcement learning (GRPO) using structural validity and answer correctness rewards. The authors construct new episode-level datasets by extending TVQA/TVQA+ clips into hour-scale episodes.

## Key Results
- Multi-agent approach significantly outperforms non-agent baselines on LongTVQA benchmarks
- RL fine-tuning provides substantial gains for smaller open-source models (Qwen2.5-3B: 23.5→47.4 accuracy)
- Both grounding and vision components are independently essential for performance
- Larger temporal windows and stronger vision models yield further improvements

## Why This Works (Mechanism)

### Mechanism 1: Iterative Evidence Accumulation via Multi-Agent Coordination
A master LLM coordinating specialist agents can accumulate evidence across multiple turns more effectively than single-pass video processing. The master agent maintains a running context and decides at each step whether to request grounding, visual inspection, or answer, allowing it to refine focus based on partial evidence rather than committing to a fixed encoding.

### Mechanism 2: Specialized Division of Labor Between Grounding and Vision Agents
Separating temporal localization (grounding) from fine-grained visual extraction (vision) improves evidence quality compared to monolithic toolsets. The grounding agent localizes relevant segments from subtitles alone, while the vision agent extracts detailed observations from the localized frames, allowing each agent to focus on its specialized modality.

### Mechanism 3: RL Policy Optimization for Concise, Correct Multi-Agent Cooperation
Reinforcement learning with GRPO can shape the master agent's policy toward efficient tool use and correct answers, particularly for smaller open-source models. The master agent is trained with two rewards: structural validity (binary, checks proper action-tag formatting) and answer correctness (exact match at termination), encouraging both well-formed intermediate actions and correct final answers.

## Foundational Learning

- **LLM Agent Tool-Calling (ReAct-style reasoning)**: Understanding how LLMs emit structured action tokens and how external tools return observations is foundational for grasping the master agent's iterative decision-making process.
  - Quick check: Can you trace a 3-turn ReAct trajectory where an LLM calls a search tool, receives output, and decides whether to search again or answer?

- **Reinforcement Learning from Outcome Rewards (RL/GRPO basics)**: Understanding policy gradients, advantage estimation, and reward shaping helps debug training failures and comprehend how the master agent learns efficient multi-step coordination.
  - Quick check: Given a trajectory that emits correct action tags but produces a wrong final answer, would the gradient signal differ from one with wrong tags and correct answer?

- **Temporal Grounding in Videos**: Understanding how temporal boundaries are represented and evaluated (e.g., IoU-based metrics) clarifies what "successful grounding" means and how the grounding agent contributes to evidence accumulation.
  - Quick check: If a grounding agent returns a 60-second window but the relevant content is a 5-second span within it, is this a grounding failure, a recall win, or both?

## Architecture Onboarding

- **Component map**: Question + subtitles → Master Agent → (if grounding) Grounding Agent → (if vision) Vision Agent → Master Agent → Repeat until answer
- **Critical path**: Question + subtitles → Master decides action → (if grounding) GroundingAgent returns clip tag → (if vision) VisionAgent returns description → Master updates context → Repeat until `<answer>` or K=5 steps reached
- **Design tradeoffs**: Step limit K (diminishing returns beyond 5), window context (2-3 clips improve accuracy but increase latency), vision model choice (stronger models yield higher accuracy but cost/latency tradeoffs), frozen vs. joint training (current design freezes specialists for stability)
- **Failure signatures**: Infinite loop/no answer (master never emits `<answer>`), malformed action tags (structural reward = 0), repeated grounding with no progress (master keeps requesting re-grounding), vision returns generic descriptions (uninformative outputs)
- **First 3 experiments**:
  1. Baseline sanity check: Run non-agent LLM (subtitles-only) vs. multi-agent on held-out LongTVQA subset to verify positive delta
  2. Ablate each agent: Disable grounding (random clip selection) and separately disable vision (subtitle-only) to quantify each component's contribution
  3. RL training curve: Train master agent with GRPO, logging structural reward and answer accuracy per 100 steps to confirm convergence and check for reward hacking

## Open Questions the Paper Calls Out

- **Joint fine-tuning of all agents**: Would jointly fine-tuning all three agents (master, grounding, vision) with reinforcement learning improve coordination and accuracy compared to freezing the grounding and vision agents? The paper notes this could further improve robustness and accuracy but leaves it unexplored.

- **Raw audio integration**: Can integrating raw audio processing (ASR, acoustic features) improve long-video QA performance beyond the current subtitle-only textual channel? The paper plans to integrate an audio-to-subtitles module in future work.

- **Intermediate dense rewards**: Would incorporating intermediate dense rewards (e.g., grounding quality, visual query relevance) improve learning efficiency and final performance over the current simple format+answer reward? The paper notes the reward is intentionally simple and may have room for improvements.

## Limitations

- Heavy reliance on closed-source vision and grounding agents (GPT-4o, Grok-4-fast-reasoning) whose performance cannot be fully verified without access to the same models
- Episode-level datasets are derived from TV shows with structured scenes, limiting generalization to arbitrary video domains
- RL training assumes sparse terminal rewards are sufficient for learning efficient multi-agent coordination, potentially leading to suboptimal policies
- Exact construction details of LongTVQA(+) datasets and re-indexing logic are not publicly available, limiting reproducibility

## Confidence

- **High confidence**: Core multi-agent architecture is clearly specified and ablation studies demonstrate component contributions
- **Medium confidence**: RL training improves smaller models significantly, but exact hyperparameters and training dynamics are not fully detailed
- **Low confidence**: Claims about scalability to truly open-ended, long-form video domains are supported only by curated TV episode datasets

## Next Checks

1. **Closed-model dependency test**: Replace the default GPT-4o vision agent with an open-source MLLM (e.g., Qwen2-VL) and measure accuracy drop to quantify reliance on proprietary models

2. **Dataset generalization probe**: Evaluate the trained multi-agent system on a different long-video domain (e.g., user-generated content or lectures) to test robustness beyond TV episodes

3. **Reward shaping audit**: Train with an intermediate grounding reward (e.g., IoU-based overlap with ground-truth segments) and compare convergence speed and final accuracy to the sparse terminal reward setup