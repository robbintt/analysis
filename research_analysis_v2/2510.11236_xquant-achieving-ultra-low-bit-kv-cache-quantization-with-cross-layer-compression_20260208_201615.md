---
ver: rpa2
title: 'XQuant: Achieving Ultra-Low Bit KV Cache Quantization with Cross-Layer Compression'
arxiv_id: '2510.11236'
source_url: https://arxiv.org/abs/2510.11236
tags:
- cache
- quantization
- xquant
- compression
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'XQuant addresses the challenge of reducing memory consumption
  in large language model inference, particularly due to the growth of KV cache during
  long-text generation, which limits deployment in resource-constrained environments.
  The paper proposes a training-free, plug-and-play framework that achieves ultra-low
  equivalent bit-width KV cache quantization through two key innovations: a data-free
  calibration method that improves quantization accuracy without additional data,
  and cross-layer KV cache compression that exploits similarity between quantized
  caches of adjacent layers to reduce storage and computation.'
---

# XQuant: Achieving Ultra-Low Bit KV Cache Quantization with Cross-Layer Compression

## Quick Facts
- arXiv ID: 2510.11236
- Source URL: https://arxiv.org/abs/2510.11236
- Authors: Haoqi Yang; Yao Yao; Zuchao Li; Baoyuan Qi; Guoming Liu; Hai Zhao
- Reference count: 40
- Primary result: Achieves <1.4 equivalent bit-width KV cache quantization with minimal accuracy loss

## Executive Summary
XQuant addresses the challenge of reducing memory consumption in large language model inference, particularly due to the growth of KV cache during long-text generation, which limits deployment in resource-constrained environments. The paper proposes a training-free, plug-and-play framework that achieves ultra-low equivalent bit-width KV cache quantization through two key innovations: a data-free calibration method that improves quantization accuracy without additional data, and cross-layer KV cache compression that exploits similarity between quantized caches of adjacent layers to reduce storage and computation. Experimental results demonstrate that XQuant achieves an equivalent bit-width of less than 1.4 bits while maintaining superior performance compared to state-of-the-art methods like KIVI-2bit and AsymKV-1.5bit, establishing an improved trade-off between memory efficiency and model accuracy.

## Method Summary
XQuant implements a two-pronged approach to ultra-low bit KV cache quantization. First, it introduces a data-free calibration method that adjusts quantization representative values away from fixed endpoints (0 and 1 for 1-bit) using parameters η₁ and η₂, which reduces reconstruction error by mapping values to intermediate positions (η, 1-η) rather than extremes. Second, it exploits cross-layer similarity by sharing quantized KV caches between adjacent layers while maintaining separate scaling factors and zero-points per layer, with group size G=2 being optimal. The framework applies asymmetric quantization configurations, using more aggressive compression for value caches (1-bit) while maintaining 2-bit precision for key caches due to their higher sensitivity to quantization errors.

## Key Results
- Achieves equivalent bit-width of less than 1.4 bits across multiple models and tasks
- Outperforms state-of-the-art methods (KIVI-2bit, AsymKV-1.5bit) on TruthfulQA and LongBench benchmarks
- Maintains competitive performance to full-precision baselines while significantly reducing memory requirements
- Validated across Mistral-7B-v0.3/Instruct-v0.2 and Llama-2-7b/chat models

## Why This Works (Mechanism)

### Mechanism 1: Data-Free Calibration
Standard quantization maps values to endpoints (0, 1 for 1-bit), but the paper introduces a calibration parameter η that shifts the effective range. The calibrated zero-point and scaling factor become: ẑ = z + ηs(2^B−1), ŝ = (1−2η)s. This is mathematically equivalent to mapping to intermediate values η and 1−η instead of 0 and 1, reducing MSE when η ∈ (0, 0.5). The transformed values X_T follow approximately uniform distribution U(0,1) within quantization groups, which the paper states is "particularly for 1-bit quantization within small group sizes."

### Mechanism 2: Cross-Layer Compression via Quantization-Enhanced Similarity
Quantized KV caches between adjacent layers exhibit sufficient similarity that one layer's cache can substitute for another's with minimal accuracy loss. Under 2-bit quantization, values are restricted to {0,1,2,3}. The paper finds >80% of positions between adjacent layers differ by at most 1. For 1-bit mapping, >80% are identical. They exploit this by having layers share a single quantized cache X^Q while maintaining separate scaling factors and zero-points per layer.

### Mechanism 3: Asymmetric Key-Value Bit-Width Configuration
Key and value caches have different quantization tolerances; aggressive value compression is viable while key compression is more fragile. The paper empirically finds that 1-bit key cache quantization is "practically infeasible," causing severe degradation. They therefore apply asymmetric configurations: more 2-bit layers for keys, more 1-bit for values, plus cross-layer compression only on value cache in optimal settings.

## Foundational Learning

- **KV Cache dynamics during autoregressive generation**: Understanding that K,V tensors accumulate per-token during decoding, scaling as O(sequence_length × hidden_dim × num_layers), is prerequisite to the paper's motivation.
  - Why needed here: The paper's entire motivation is KV cache growth
  - Quick check question: Why does memory increase during decoding but not during prefill?

- **Uniform quantization with zero-point and scale**: Equations 1-3 define the baseline quantization; calibration modifies these parameters.
  - Why needed here: You must understand how z and s define the quantization grid before grasping how η shifts it
  - Quick check question: If min(X)=−2 and max(X)=6, what are z and s for 2-bit quantization?

- **Group Query Attention (GQA) and multi-head structures**: The paper uses notations like b, h, s, d (batch, heads, sequence, dim) and references GQA.
  - Why needed here: Memory calculations depend on head count
  - Quick check question: How does reducing KV heads affect memory vs. quality tradeoffs?

## Architecture Onboarding

- **Component map**: Input Token → Attention Layer L → Compute K_L, V_L (FP16) → [Quantization Phase] → [Cross-Layer Compression] → [Dequantization Phase] → Attention computation with X̂_L

- **Critical path**: The calibration parameters (η₁, η₂) and cross-layer grouping (G, k) are the only configuration decisions. Start with defaults: η₁=1/6, η₂=0.045, G=2, k=0 (first layer dominant).

- **Design tradeoffs**: Higher η → lower reconstruction error but may shift distribution away from original; Table 3 shows η=0.2 works better than η=0 for some tasks. Larger group size G → more compression but similarity degrades beyond adjacent layers; Table 5 shows G=2 outperforms G=3,4. Earlier cross-layer start (lower k_m, v_m) → more compression but riskier for early layers that may encode more distinct information.

- **Failure signatures**: Sudden score collapse (e.g., 48→7 on MFQA-Zh) indicates over-aggressive key quantization. Gradual degradation with sequence length suggests cumulative dequantization errors. Inconsistent results across tasks suggests η values need tuning or static hyperparameters are insufficient.

- **First 3 experiments**:
  1. Reproduce calibration ablation (Table 3 variant): Run with η=0 vs. η=0.2 on a single LongBench subset to validate MSE reduction translates to task performance
  2. Cross-layer similarity audit: For your target model, compute layer-wise KV cache differences under 2-bit quantization to verify >70% positions differ by ≤1 before enabling compression
  3. Breakpoint search for key quantization: Starting from full 2-bit, incrementally move key layers to 1-bit (mimicking Table 8) to find the degradation threshold for your model family

## Open Questions the Paper Calls Out

### Open Question 1
Can XQuant maintain its performance-compression trade-off when applied to significantly larger models (e.g., 70B+ parameters) or newer architectures beyond Llama-2 and Mistral? The authors state that "its robustness and generalizability could be further validated by extending evaluations to a wider range of newer-generation or larger-scale models." The study primarily validates the method on 7B parameter models (with a single 14B test in the appendix), leaving the scaling behavior for ultra-large models unconfirmed.

### Open Question 2
Is it possible to develop an automated mechanism to determine optimal configuration parameters (like $\eta$ and layer groupings) to replace task-specific manual tuning? The paper notes that "our current work relies on task-specific configurations" and identifies "the development of an automated method to search for optimal configurations" as a valuable future direction. While a static setting is shown to be robust, the current best performance relies on tuned hyperparameters, limiting "plug-and-play" optimality across diverse downstream tasks.

### Open Question 3
Can XQuant's cross-layer compression be mathematically integrated with token eviction methods to achieve additive memory savings? The authors suggest it would be "fruitful... to investigate their compatibility and potential synergies" with other compression paradigms like eviction methods. Eviction methods drop tokens based on attention scores, which may disrupt the inter-layer similarity patterns XQuant relies on, potentially causing error propagation.

## Limitations

- **Cross-layer similarity assumption**: The core cross-layer compression mechanism relies on empirical observation that adjacent layer KV caches differ by at most 1 value position under 2-bit quantization, which may not generalize across diverse model architectures.

- **Calibration effectiveness bounds**: The data-free calibration shows theoretical MSE reduction for uniformly distributed values, but practical effectiveness depends on actual distribution of KV cache values, which may not be uniformly distributed across all attention patterns.

- **Key-value asymmetry generality**: The claim that 1-bit key quantization is "practically infeasible" is based on limited experimental evidence and may vary across architectures with different attention mechanisms or key-value separation strategies.

## Confidence

**High Confidence**:
- Calibration improves quantization accuracy under ultra-low bit-widths through MSE reduction (Eq. 9-12 are mathematically sound)
- Cross-layer similarity exists in the tested Mistral-7B architecture under 2-bit quantization
- Memory reduction claims are verifiable through implementation

**Medium Confidence**:
- Generalization of cross-layer similarity across model architectures
- Universal optimality of η=1/6 (1-bit) and η=0.045 (2-bit) parameters
- Asymmetric key-value bit-width configuration strategy

**Low Confidence**:
- Claims about superiority over all state-of-the-art methods without extensive ablation studies
- Assertion that the method is universally "training-free" given potential task-specific tuning requirements

## Next Checks

1. **Cross-layer similarity validation**: Implement a systematic measurement of layer-wise KV cache differences under 2-bit quantization across multiple model architectures (Llama-2, Qwen, Gemma) to verify the >80% similarity claim holds beyond Mistral-7B before applying cross-layer compression.

2. **Calibration distribution analysis**: Profile the actual distribution of KV cache values within quantization groups during inference on representative tasks. Compare empirical MSE reduction against theoretical predictions to validate the uniform distribution assumption underlying the calibration mechanism.

3. **Key sensitivity boundary exploration**: Systematically test key quantization bit-widths across a broader range (1-bit, 1.5-bit, 2-bit) on multiple model families to map the degradation threshold and verify the claimed sensitivity differential is architecture-dependent rather than universal.