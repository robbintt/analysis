---
ver: rpa2
title: 'Evaluating the AI-Lab Intervention: Impact on Student Perception and Use of
  Generative AI in Early Undergraduate Computer Science Courses'
arxiv_id: '2505.00100'
source_url: https://arxiv.org/abs/2505.00100
tags:
- genai
- students
- intervention
- more
- focus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines the impact of a structured "AI-Lab" intervention
  on undergraduate students' perceptions and usage of generative AI in early computer
  science courses. The intervention, implemented across four courses over three semesters,
  included scaffolding and guided exploration of AI tools.
---

# Evaluating the AI-Lab Intervention: Impact on Student Perception and Use of Generative AI in Early Undergraduate Computer Science Courses

## Quick Facts
- arXiv ID: 2505.00100
- Source URL: https://arxiv.org/abs/2505.00100
- Reference count: 25
- Primary result: Structured AI-Lab scaffolding significantly increased student comfort and openness to using GenAI for conceptual, debugging, and homework tasks in early CS courses, with large effect sizes, without increasing usage frequency or overreliance.

## Executive Summary
This study evaluates a structured "AI-Lab" intervention designed to shape undergraduate students' perceptions and usage of generative AI in early computer science courses. Implemented across four courses over three semesters, the intervention used scaffolding and guided exploration of AI tools. Pre- and post-intervention surveys (n=831) and focus groups revealed large effect sizes in students' comfort and openness to using AI for conceptual, debugging, and homework tasks, though overall usage frequency remained stable. Focus group data highlighted a shift from naive to more mindful, strategic use of AI, with students gaining better understanding of AI's limitations. The findings suggest structured scaffolding can promote responsible and effective AI integration without increasing overreliance or academic dishonesty, offering evidence-based guidance for educators.

## Method Summary
The study used a mixed-methods approach combining pre- and post-intervention surveys with focus group discussions. Surveys were administered to 831 students across four courses (DSA-CS, DSA-DSAI, Competitive Programming, ENGR) over two semesters. The AI-Lab intervention consisted of four stages: instructor preparation of topics exposing AI limitations, prelab asynchronous materials and pre-survey, in-class demonstration and peer critique of AI failures, and post-lab homework requiring documented AI use and post-survey. Quantitative analysis used Wilcoxon signed-rank tests with Rank-Biserial Correlation effect sizes, while qualitative data from focus groups underwent thematic analysis. The study measured openness, comfort, and frequency of AI use across conceptual, debugging, homework, and programming tasks.

## Key Results
- Large effect sizes (p < 0.001, Rank-Biserial > 0.50) in student comfort and openness to using AI for conceptual, debugging, and homework tasks
- Stable overall usage frequency despite increased comfort and openness
- Qualitative shift from naive to mindful, strategic use of AI with better understanding of limitations
- No evidence of increased overreliance or academic dishonesty

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Guided exposure to AI failures reduces over-reliance and improves critical evaluation skills.
- **Mechanism:** The intervention forces students to encounter "hallucinations" or incorrect outputs during in-class labs on topics specifically selected to expose current model limits, creating a "safe failure" environment that calibrates student trust levels.
- **Core assumption:** Students maintain naive trust in AI outputs until they directly experience authoritative-looking failures in a low-stakes setting.
- **Evidence anchors:** Abstract notes shift from naive to mindful use; section 1.1 specifies choosing topics that expose AI limitations; section 3.3.1 describes students' difficulty distinguishing accurate from inaccurate AI output.

### Mechanism 2
- **Claim:** Scaffolding prompts acts as a cognitive apprenticeship, shifting usage from "copy-paste" to "iterative refinement."
- **Mechanism:** By explicitly teaching prompting strategies and requiring documentation of attempts to guide the AI, the intervention moves students from passive receivers to active directors of the tool.
- **Core assumption:** Students view GenAI as a search engine rather than a stochastic reasoning partner; explicit instruction changes this mental model.
- **Evidence anchors:** Abstract emphasizes structured scaffolding promotes responsible integration; section 3.3.1 shows students refining prompts with context; corpus evidence supports need for pedagogical design.

### Mechanism 3
- **Claim:** Social negotiation of AI outputs accelerates error detection.
- **Mechanism:** The in-class lab utilizes peer discussion to critique AI-generated code, where students collectively identify errors, reinforcing the "More Knowledgeable Other" dynamic.
- **Core assumption:** Peer pressure and collective scrutiny are more effective at motivating critical analysis than individual grading.
- **Evidence anchors:** Section 1.1 highlights peer discussion and collaborative correction; section 3.3.1 reports students learning to refine prompts through interactive conversations; corpus evidence is weak for this specific mechanism.

## Foundational Learning

- **Concept:** Debugging & Code Comprehension
  - **Why needed here:** Students cannot detect AI "hallucinations" or logical errors without the ability to mentally execute code. The intervention relies on students spotting what the AI got wrong.
  - **Quick check question:** Can you trace a recursive function by hand and identify where the AI's proposed base case fails?

- **Concept:** Prompt Engineering (Context Setting)
  - **Why needed here:** The study shows a shift from "giving up" to "refining prompts." Students need to know how to constrain the AI (e.g., "do not use libraries not in the standard std").
  - **Quick check question:** How do you modify a prompt to force the AI to explain its logic step-by-step before providing code?

- **Concept:** Academic Integrity Boundaries
  - **Why needed here:** The paper highlights student "Internal Tension" and confusion about policies. A clear personal framework is required to navigate when AI use crosses into "cheating."
  - **Quick check question:** Distinguish between using AI to explain a syntax error vs. using it to generate the solution logic for a graded assignment.

## Architecture Onboarding

- **Component map:** Instructor Prep -> Prelab (Offline) -> In-Class Lab -> Post-Lab
- **Critical path:** The Instructor Preparation phase. If the instructor selects a topic where the AI performs perfectly, the "Critical Analysis" learning outcome fails immediately. You must find the AI's breaking point.
- **Design tradeoffs:**
  - *Effort vs. Insight:* Analyzing prompt logs provides high insight into student thought processes but creates significant grading overhead compared to auto-graded code.
  - *Guidance vs. Exploration:* Too much instructor demo reduces student exploration; too little leads to frustration if the AI fails opaquely.
- **Failure signatures:**
  - Usage frequency spikes without comfort increase: Indicates "cheating" or over-reliance rather than mastery.
  - Stagnant "Openness" scores: Suggests the intervention failed to demystify the tool or addressed fears inadequately.
  - "Blind Acceptance": Focus group feedback showing students treat AI output as "compilable truth" without verification.
- **First 3 experiments:**
  1. **The "Vulnerability Scan":** Before class, run the planned lab problem through 3 different GenAI models. If all solve it perfectly without logic errors, discard or complicate the problem until the AI fails.
  2. **The "Prompt-Debug" Race:** In-class, give students a buggy AI-generated solution and ask them to fix the prompt that generated it, rather than the code itself.
  3. **The "Failure Log":** For homework, require students to submit one instance where they tried to use AI but it failed, documenting why they think it failed (limitation vs. bad prompt).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do student perceptions and usage patterns of GenAI evolve longitudinally over a multi-year degree program?
- **Basis in paper:** Section 4.5 states future work should "track and examine semester-to-semester shifts as student perspectives and usage patterns continue to change."
- **Why unresolved:** The study was limited to three distinct semesters and observed significant behavioral differences between the Spring 2024 and Fall 2024 cohorts.
- **Evidence:** A longitudinal study tracking a single cohort from freshman to senior year, measuring perception changes relative to their maturation.

### Open Question 2
- **Question:** Does the AI-Lab intervention improve objective learning outcomes, such as code quality or problem-solving retention, compared to unguided usage?
- **Basis in paper:** The paper claims the intervention enables students to harness GenAI "without undermining essential competencies" and "core problem-solving abilities," but relies entirely on self-reported surveys rather than objective performance metrics.
- **Why unresolved:** "Comfort" and "openness" are attitudinal proxies and do not verify that actual technical competence or skill retention was preserved.
- **Evidence:** A comparative analysis of exam scores, code efficiency, or debugging success rates between students who received the intervention and a control group.

### Open Question 3
- **Question:** Can the AI-Lab framework be effectively adapted for non-STEM disciplines where GenAI usage differs significantly from code generation?
- **Basis in paper:** Section 4.5 suggests investigating "how different instructional contexts or disciplines beyond computer science and engineering might be able to adapt and leverage the AI-Lab approach."
- **Why unresolved:** The study validated the framework only in technical courses where students are already inclined to experiment with technology.
- **Evidence:** Implementing the scaffolded AI-Lab modules in Humanities or Social Sciences courses and analyzing shifts in critical evaluation of AI-generated text.

## Limitations

- Study findings rest on self-reported survey data and voluntary focus group participation, potentially introducing selection bias toward students with strong opinions about AI tools.
- Intervention was implemented at one institution across specific courses, limiting generalizability to different institutional contexts, course structures, or student populations.
- Study measured immediate post-intervention perceptions but did not track long-term retention of critical evaluation skills developed.

## Confidence

- **High Confidence**: The quantitative findings showing increased comfort and openness to AI use across all measured tasks (p < 0.001, large effect sizes). The statistical methodology is appropriate and the sample size is adequate for detecting effects.
- **Medium Confidence**: The qualitative findings about shifts from "naive to mindful use" and improved understanding of AI limitations. While focus group data supports this, the interpretation relies on researcher coding and thematic analysis.
- **Low Confidence**: The claim that structured scaffolding prevents academic dishonesty. The study did not directly measure cheating behavior, only perceptions of AI use and student intentions.

## Next Checks

1. **Longitudinal Follow-up**: Re-survey the same cohort 6-12 months later to assess whether increased comfort and openness to AI translate into sustained, responsible usage patterns or if effects decay over time.

2. **Academic Integrity Assessment**: Design a quasi-experimental study comparing assignment submissions from courses with and without the AI-Lab intervention, using similarity detection tools to quantify actual AI-generated content and analyze patterns of potential misuse.

3. **Cross-Institutional Replication**: Partner with 2-3 institutions of varying selectivity and demographics to implement the AI-Lab intervention, comparing whether observed effects on comfort and openness are consistent across different student populations and institutional contexts.