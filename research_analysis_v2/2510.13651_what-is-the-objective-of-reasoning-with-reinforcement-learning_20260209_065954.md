---
ver: rpa2
title: What is the objective of reasoning with reinforcement learning?
arxiv_id: '2510.13651'
source_url: https://arxiv.org/abs/2510.13651
tags:
- learning
- algorithm
- gradient
- correct
- reinforcement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper shows that several popular reinforcement learning algorithms
  for fine-tuning large language models with binary rewards can be viewed as stochastic
  gradient ascent on monotone transformations of the probability of correct answers.
  The authors demonstrate that REINFORCE, rejection sampling, and GRPO algorithms
  all optimize closely related objective functions that rescale the probability of
  correctness in different ways.
---

# What is the objective of reasoning with reinforcement learning?

## Quick Facts
- arXiv ID: 2510.13651
- Source URL: https://arxiv.org/abs/2510.13651
- Reference count: 2
- Primary result: Popular RL algorithms (REINFORCE, Rejection Sampling, GRPO) optimize different monotone transformations of the probability of correct answers

## Executive Summary
This paper provides a unifying theoretical framework showing that popular reinforcement learning algorithms for fine-tuning large language models with binary rewards can be understood as stochastic gradient ascent on different monotone transformations of the probability of correct answers. The authors demonstrate that REINFORCE, rejection sampling, and GRPO algorithms all optimize closely related objective functions that rescale the probability of correctness in distinct ways. Specifically, REINFORCE maximizes the probability directly, rejection sampling targets a function close to the logarithm, and GRPO optimizes a function close to the arcsine of the square root. The paper concludes that the choice of rescaling function is context-dependent and argues that these algorithms are statistically similar to choosing different loss functions in classification problems.

## Method Summary
The paper analyzes a meta-algorithm that samples M responses from a model, assigns binary rewards, computes per-sample weights Z_i based on specific rules, and updates parameters using weighted gradient ascent. The key insight is that different choices of Z_i correspond to optimizing different monotone transformations of the probability of correct answers. The authors use Bernstein polynomial theory to prove that specific weighting schemes approximate specific continuous functions (identity, log, arcsine), providing a mathematical bridge between the algorithm and the objective. The analysis focuses on binary reward settings and shows that while all methods share the same global optimum, they differ in optimization dynamics and convergence behavior.

## Key Results
- REINFORCE, rejection sampling, and GRPO are mathematically equivalent to optimizing identity, log, and arcsine transformations of the probability of correct answers, respectively
- All algorithms share the same global optimum but differ in optimization dynamics, analogous to different loss functions in classification
- The choice of rescaling function affects learning velocity across different probability regions, with potential implications for model architecture and initial competence
- The specific shape of the objective function is determined by the Bernstein polynomial expansion of the chosen weighting scheme

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Popular RL algorithms (REINFORCE, Rejection Sampling, GRPO) are mathematically equivalent to stochastic gradient ascent on distinct monotone transformations of the probability of a correct answer.
- **Mechanism:** Rather than optimizing the raw probability $p(\text{correct})$ directly, the choice of "advantage" weights ($Z_i$) in the update rule rescales the objective function. This reweighting effectively changes the gradient step magnitude based on the current probability mass, shaping the optimization landscape without changing the optimal solution (the mode).
- **Core assumption:** The reward signal is binary (correct/incorrect), and the goal is to maximize the probability of the correct set $C(x)$.
- **Evidence anchors:** [abstract] "viewed as stochastic gradient ascent on a monotone transform of the probability of a correct answer." [section 3] Defines the objective $J_h(\theta)$ and the general update rule (Algorithm 1).

### Mechanism 2
- **Claim:** The specific shape of the objective function is determined by the Bernstein polynomial expansion of the chosen weighting scheme.
- **Mechanism:** The paper demonstrates that the expected gradient update $\mathbb{E}[Z_i \nabla \log \pi]$ maps to the derivative of a function $h_M(t)$. The weights $a_s, b_s$ in the algorithm act as coefficients in a Bernstein polynomial approximation of the target function $h'(t)$.
- **Core assumption:** The number of samples $M$ is finite, making the approximation $h_M$ distinct from the limit function $h$.
- **Evidence anchors:** [section 4] "The weights we consider... induce a transform of the form... related to the incomplete beta function." [section 7] "The derivative of $h_M$... is a Bernstein polynomial, which provides a basis in which to approximate any continuous function."

### Mechanism 3
- **Claim:** All derived algorithms (Linear, Log, Arcsin) share the same global optimum but differ in optimization dynamics, analogous to different loss functions in classification.
- **Mechanism:** Because the transformation functions $h(t)$ (e.g., identity, log, arcsin) are strictly monotonic, maximizing $h(p)$ is equivalent to maximizing $p$ itself in terms of the final solution. The difference lies in the "velocity" of learning across different probability regions.
- **Core assumption:** The model family $\pi_\theta$ is expressive enough to reach the global optimum (placing all mass on correct answers).
- **Evidence anchors:** [section 7] "Arguing whether GRPO or REINFORCE is best is like arguing whether log loss is better than hinge loss... Statistically, both return comparable classifiers."

## Foundational Learning

- **Concept:** Williams' REINFORCE Algorithm (Policy Gradient)
  - **Why needed here:** The paper frames all modern LLM fine-tuning methods as descendants or special cases of this general stochastic gradient rule. Understanding $\nabla \log \pi$ is the baseline.
  - **Quick check question:** Can you explain why multiplying a reward by $\nabla \log \pi$ increases the probability of actions with positive rewards?

- **Concept:** Bernstein Polynomials & Function Approximation
  - **Why needed here:** The paper uses Bernstein polynomials to prove that specific discrete weighting schemes (like GRPO) approximate specific continuous functions (like arcsin). This is the mathematical bridge between the algorithm and the objective.
  - **Quick check question:** How does increasing the degree $M$ (number of samples) affect the accuracy of a Bernstein polynomial approximation of a continuous function?

- **Concept:** Binary vs. Dense Rewards
  - **Why needed here:** The paper's "Mechanism 1" relies entirely on the reward being binary ($R_i \in \{0,1\}$). This simplifies the expectation analysis to Binomial distributions and Beta functions.
  - **Quick check question:** In the paper's derivation, what distribution does the "leave-one-out total reward" $S_i$ follow? (Answer: Binomial).

## Architecture Onboarding

- **Component map:** Prompt Sampler -> Generator -> Verifier -> Weighting Module -> Optimizer
- **Critical path:** The **Weighting Module** is the only architectural difference between REINFORCE, Rejection Sampling, and GRPO in this framework. The paper reduces "algorithm design" to "choosing coefficients $(a_s, b_s)$."
- **Design tradeoffs:**
  - **Identity (REINFORCE):** Direct optimization, but may suffer from high variance or slow convergence if rewards are sparse.
  - **Log (Rejection Sampling):** Strongly penalizes moving probability mass away from correct answers; effective for "gold" data but requires correct answers to exist.
  - **Arcsin (GRPO):** Normalizes by variance; theoretically accelerates learning when the model is uncertain (probability $\approx 0.5$), but behavior is complex near boundaries ($0$ or $1$).
- **Failure signatures:**
  - **Base Model Incompetence:** If the generator cannot produce any correct answers ($R_i=0$ for all $i$), the gradient is undefined (log) or zero (linear).
  - **Small Sample Artifacts:** With small $M$, the induced function $h_M$ deviates significantly from the target $h$.
- **First 3 experiments:**
  1. **Gradient Alignment Check:** Implement the "Weighting Module" for GRPO and REINFORCE. Verify that for a fixed prompt, the cosine similarity of the gradients aligns with the theoretical derivatives of $h(t)=t$ vs $h(t)=\arcsin(\sqrt{t})$.
  2. **Sample Scaling ($M$):** Run the Rejection Sampling variant with $M \in \{2, 16, 256\}$ on a synthetic task. Plot the empirical loss curve against the theoretical $h_M(t)$ approximations to confirm the Bernstein convergence claims.
  3. **Base Model Threshold:** Fine-tune models with varying initial competence (e.g., 1%, 10%, 50% accuracy) using GRPO. Confirm that below a certain threshold of correct answer generation, the "arcsin" normalization fails to stabilize or improve the model.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Is there a rigorous theoretical justification for the logarithmic scaling targeted by rejection sampling fine-tuning in reinforcement learning contexts?
- **Basis in paper:** [explicit] The authors state in Section 5 that "it’s not clear whether there is a rigorous justification for using the logarithmic scaling in reinforcement learning contexts," noting it lacks a natural maximum likelihood interpretation unless there is a single correct answer.
- **Why unresolved:** While the log-objective is standard in supervised learning, its theoretical benefits over linear probability maximization are ambiguous when multiple correct responses exist for a prompt.
- **What evidence would resolve it:** A theoretical framework establishing statistical efficiency or generalization bounds for log-scaling in multi-label RL settings, or empirical evidence demonstrating consistent superiority over linear scaling.

### Open Question 2
- **Question:** Why is the arcsine of the square root the specific objective function targeted by the GRPO algorithm, and is this scaling theoretically advantageous?
- **Basis in paper:** [explicit] Section 6 analyzes the GRPO objective $h(t) \approx 2\arcsin(\sqrt{t})$ and explicitly asks, "Why is this a good scaling function? As we discuss in the conclusion, we’re not sure."
- **Why unresolved:** The derivation shows GRPO optimizes this specific transform, but the authors note there is no apparent "magical property" that makes this rescaling inherently better than others.
- **What evidence would resolve it:** A comparative analysis of optimization dynamics showing that variance normalization (leading to arcsine) specifically improves convergence speed or stability compared to simpler transforms.

### Open Question 3
- **Question:** How does the choice of monotone rescaling function ($h$) practically affect optimization dynamics and final performance when training on a corpus with diverse questions?
- **Basis in paper:** [explicit] The conclusion notes that "it is hard to guess the practical effects of monotone rescaling of the objective function in this manner when there are multiple questions in the corpus," suggesting the choice impacts optimization dynamics even if global optima are similar.
- **Why unresolved:** The paper establishes that different algorithms optimize different $h$ functions, but it remains unclear which functions act as better loss landscapes for stochastic gradient descent in large-scale LLM training.
- **What evidence would resolve it:** Large-scale ablation studies comparing REINFORCE, GRPO, and rejection sampling on identical base models to isolate the impact of the rescaling function on learning curves and task accuracy.

## Limitations

- **Theoretical Scope:** The unification applies specifically to binary reward settings; extension to continuous rewards would require new derivations beyond the current framework.
- **Practical Validation Gap:** The paper provides theoretical proofs and visualizations but lacks empirical demonstrations on real LLM fine-tuning tasks to validate optimization dynamics claims.
- **Finite Expressiveness:** While the paper proves convergence to global optimum in infinite capacity limit, real-world model architectures have finite expressiveness that may prevent reaching this optimum.

## Confidence

- **High Confidence:** The mathematical derivations connecting REINFORCE, Rejection Sampling, and GRPO to monotone transformations of probability are rigorous and well-established through Bernstein polynomial theory.
- **Medium Confidence:** The claim that these methods are "statistically similar to choosing different loss functions in classification" is reasonable but lacks empirical backing.
- **Low Confidence:** The assertion about "statistical similarity" between these algorithms doesn't account for implementation details, hyperparameter sensitivity, or interaction with specific model architectures that could create meaningful practical differences.

## Next Checks

1. **Empirical Convergence Comparison:** Implement all three algorithms (REINFORCE, Rejection Sampling, GRPO) on a standard LLM fine-tuning task with binary rewards. Measure and compare convergence rates, final accuracy, and stability across different initial model competence levels (e.g., 10%, 50%, 90% initial accuracy).

2. **Architecture Interaction Study:** Test the algorithms on models with different architectures (e.g., standard transformer vs. MoE) to validate the paper's implicit claim that the choice of transformation function $h(t)$ interacts with model architecture. Focus on scenarios where the model is initially incompetent versus highly competent.

3. **Sample Size Sensitivity Analysis:** Systematically vary the number of samples $M$ from 2 to 256 and measure how closely the empirical objective function matches the theoretical $h_M(t)$ approximation. This would validate the Bernstein polynomial convergence claims and identify practical sample size thresholds where the approximation becomes accurate.