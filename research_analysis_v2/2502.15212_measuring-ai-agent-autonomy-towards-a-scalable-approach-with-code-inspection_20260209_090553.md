---
ver: rpa2
title: 'Measuring AI agent autonomy: Towards a scalable approach with code inspection'
arxiv_id: '2502.15212'
source_url: https://arxiv.org/abs/2502.15212
tags:
- agent
- autonomy
- human
- code
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a code-based approach to measuring AI agent
  autonomy by analyzing the orchestration code rather than running the agent. The
  method uses a taxonomy focused on two attributes: impact (what actions an agent
  can take and in what environment) and oversight (how agent interactions are orchestrated,
  human-in-the-loop requirements, and observability).'
---

# Measuring AI agent autonomy: Towards a scalable approach with code inspection

## Quick Facts
- arXiv ID: 2502.15212
- Source URL: https://arxiv.org/abs/2502.15212
- Reference count: 21
- Primary result: Introduced a code-based method to measure AI agent autonomy by analyzing orchestration code, achieving substantial inter-rater agreement (κ = 0.64) without runtime execution.

## Executive Summary
This paper introduces a code-based method for measuring AI agent autonomy that analyzes orchestration code rather than running the agent. The approach uses a taxonomy focused on two key attributes: impact (what actions an agent can take and in what environment) and oversight (how agent interactions are orchestrated, human-in-the-loop requirements, and observability). Applied to AutoGen and ten of its applications, the method successfully scored autonomy levels without executing agents, reducing risk and cost. The study found that most applications used framework defaults for oversight, highlighting the importance of default settings in responsible AI development.

## Method Summary
The method involves three independent raters scoring agent applications across five attributes (Actions, Environment, Orchestration, Human-in-the-loop, Observability) at three levels (Lower/Middle/Higher) using explicit code flags. Raters examine orchestration code for specific configurations like `human_input_mode`, `code_execution_config`, and `use_docker` to map to the taxonomy. The approach eliminates runtime execution risks while enabling scalable assessment across multiple repositories.

## Key Results
- Achieved substantial inter-rater agreement (Fleiss' kappa = 0.64) across all evaluation categories
- Most applications used framework defaults for oversight configurations
- The method successfully identified autonomy levels without running agents, reducing safety risks
- Lower agreement for specific attributes (Actions: κ = 0.30, Observability: κ = 0.47) due to ambiguity in custom tools and developer vs. user visibility

## Why This Works (Mechanism)

### Mechanism 1: Static Mapping of Orchestration Bounds
- Claim: Inspecting orchestration code allows for autonomy assessment without runtime execution risks
- Mechanism: Maps code configurations (e.g., `human_input_mode`, `use_docker`) to a taxonomy of Impact and Oversight to predict theoretical behavior limits
- Core assumption: Orchestration code accurately reflects operational boundaries without significant runtime divergence
- Evidence anchors: [abstract] eliminates need to run agents; [Table 1] contrasts reduced risk vs. limited emergent behavior insight
- Break condition: Fails if agents exhibit significant emergent behaviors or dynamically modify orchestration rules at inference time

### Mechanism 2: Attribute-Based Decomposition for Inter-Rater Reliability
- Claim: Decomposing "autonomy" into discrete attributes enables consistent scoring
- Mechanism: Uses three-level taxonomy for five specific attributes with code flag indicators instead of holistic judgment
- Core assumption: Selected attributes are sufficient proxies for overall system autonomy and risk
- Evidence anchors: [section 4] substantial inter-rater agreement (κ = 0.64); [Table 2] detailed taxonomy mapping
- Break condition: Consistency drops if attributes are ambiguous, particularly for custom tools and observability distinctions

### Mechanism 3: Defaults as Governance Levers
- Claim: Framework defaults drive safety outcomes because developers retain standard configurations
- Mechanism: Secure defaults (e.g., Docker execution, human-in-the-loop) create safer agents through "path of least resistance"
- Core assumption: Developers lack incentive or expertise to override default safety configurations
- Evidence anchors: [section 5] relevance of defaults; most default setups used consistently
- Break condition: Weakens if dominant use cases require overriding defaults for functionality

## Foundational Learning

- Concept: **Static Analysis vs. Dynamic Evaluation**
  - Why needed here: Understanding the safety/scalability vs. emergent behavior trade-off is critical
  - Quick check question: Can static inspection detect if an agent loops indefinitely based on user input? (Answer: Only if loop logic is hard-coded; dynamic conditions may be missed)

- Concept: **Orchestration vs. Model Agency**
  - Why needed here: The paper measures system autonomy, not LLM internal reasoning capability
  - Quick check question: Does high "Impact" score imply model intelligence for malicious use? (Answer: No, only implies capacity for impact exists)

- Concept: **Inter-Rater Reliability (Fleiss' Kappa)**
  - Why needed here: Understanding that 0.64 is "substantial" but not "perfect" contextualizes method maturity
  - Quick check question: Why might "Actions" have lower Kappa (0.30) than "Orchestration"? (Answer: Custom tools harder to categorize than standard parameters)

## Architecture Onboarding

- Component map: Source Code -> Human/AI Inspector -> Code Flags -> Taxonomy -> Autonomy Score
- Critical path:
  1. Acquisition: Gain access to agent's source code (setup/config files)
  2. Flagging: Scan for specific parameters (e.g., `human_input_mode`, `code_execution_config`, `use_docker`)
  3. Scoring: Map flags to Taxonomy levels (Lower/Middle/Higher)
  4. Validation: Verify flags against actual behavior in sandbox (optional, for calibration)
- Design tradeoffs:
  - Safety vs. Insight: Gain safety by not running agent but lose insight into emergent behaviors
  - Framework Specificity vs. Generalization: Current taxonomy is AutoGen-centric; requires mapping new flags for other frameworks
- Failure signatures:
  - Hidden Logic: External API or opaque binary handles "real" logic (Code shows "Lower" autonomy, reality is "Higher")
  - Dynamic Override: Configuration loaded from file/database at runtime makes static inspection inaccurate
  - Observability Confusion: Log files exist (high developer observability) but no dashboard (low user observability)
- First 3 experiments:
  1. Repo Scan: Score 3 distinct AutoGen repositories using Code Flags in Table 5 without reading full logic
  2. Calibration: Run one scored repository in sandbox; compare actual behavior against predicted "Human-in-the-loop" score
  3. Framework Translation: Apply taxonomy to non-AutoGen agent (e.g., LangChain) to identify flag changes needed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do static code-inspection autonomy scores correlate with dynamic, inference-time assessments using evaluation harnesses?
- Basis in paper: [explicit] Authors state in "Conclusion and Future Work" that code inspection results can be compared to inference-time assessments
- Why unresolved: Current study only piloted static inspection method; unknown if static scores predict actual runtime autonomy
- What evidence would resolve it: Comparative study measuring correlation coefficient between code-based scores and runtime autonomy levels

### Open Question 2
- Question: Can the code-inspection taxonomy be reliably automated using AI-assisted grading methods?
- Basis in paper: [explicit] Authors propose scaling for all open source AutoGen applications using AI-assisted grading
- Why unresolved: Pilot relied on human raters with substantial but imperfect agreement (κ = 0.64), particularly struggling with "Action" and "Observability" categories
- What evidence would resolve it: Development and validation of AI classifier matching or exceeding human rater accuracy

### Open Question 3
- Question: How can "Human-in-the-loop" attribute be refined to weight significance of specific actions requiring approval?
- Basis in paper: [inferred] Notes limitation in differentiating oversight scenarios based on when human intervention occurs
- Why unresolved: Current taxonomy treats "Human-in-the-loop" categorically, failing to capture nuance of *when* intervention occurs relative to action impact
- What evidence would resolve it: Revised scoring methodology differentiating high-criticality interventions from low-criticality interventions

## Limitations
- Cannot capture emergent behaviors that arise during runtime through dynamic interactions or user inputs
- Current taxonomy is AutoGen-specific and requires significant adaptation for other frameworks
- Assumes orchestration code fully constrains autonomy, which may not hold for agents that modify behavior based on runtime conditions

## Confidence
- **High Confidence**: Inter-rater reliability results (κ = 0.64) demonstrate method consistency in structured environments; code flag mapping is technically sound and reproducible
- **Medium Confidence**: Scalability claim holds for framework-level assessment but becomes less reliable for complex applications with multiple interacting components
- **Low Confidence**: Method's ability to predict actual risk outcomes without runtime validation remains unproven; correlation between autonomy scores and real-world safety incidents not established

## Next Checks
1. **Runtime Validation Study**: Apply code-inspection method to 10 additional AutoGen applications, then run them in controlled sandboxes to compare predicted vs. actual autonomy behaviors, particularly for edge cases and error conditions
2. **Framework Generalization Test**: Adapt taxonomy to assess LangChain agents and compare resulting autonomy scores with AutoGen applications to identify framework-specific biases or gaps
3. **Risk Correlation Analysis**: Track sample of code-inspected applications over 6 months to measure if higher autonomy scores correlate with documented safety incidents, user complaints, or beneficial outcomes in production environments