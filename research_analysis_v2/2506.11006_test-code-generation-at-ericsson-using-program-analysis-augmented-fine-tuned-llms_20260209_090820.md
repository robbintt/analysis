---
ver: rpa2
title: Test code generation at Ericsson using Program Analysis Augmented Fine Tuned
  LLMs
arxiv_id: '2506.11006'
source_url: https://arxiv.org/abs/2506.11006
tags:
- code
- test
- methods
- generation
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper describes a test code generation pipeline using Large
  Language Models (LLMs) in Ericsson. The input is a natural language test step description,
  and the output is Java code implementing that step.
---

# Test code generation at Ericsson using Program Analysis Augmented Fine Tuned LLMs

## Quick Facts
- arXiv ID: 2506.11006
- Source URL: https://arxiv.org/abs/2506.11006
- Reference count: 13
- Test code generation pipeline using RAG and fine-tuned MoE models improves F1-score by 8% over base model

## Executive Summary
This paper presents a test code generation pipeline for Ericsson that addresses the challenge of generating compilable Java test code from natural language descriptions. The key innovation combines Retrieval Augmented Generation (RAG) with static program analysis to provide contextual information about available APIs, along with fine-tuning a Mixture of Experts (MoE) model using a custom prompt template. The approach successfully reduces API hallucinations and improves method usage accuracy, with an 8% average improvement in F1-score over the base model, matching the performance of a significantly larger 8x22b MoE model.

## Method Summary
The method involves three core components: (1) Static program analysis extracts available classes and public method signatures from the target Java repository to constrain the model's output space, (2) RAG retrieves two similar test code blocks based on semantic similarity of natural language descriptions to provide exemplars of repository-specific patterns, and (3) Instruction Fine-Tuning (IFT) on a Mixtral 8x7b MoE model using LoRA adapters with rank 256 to optimize parameter efficiency. The prompt template combines these elements: instruction, available methods, retrieved exemplars, and the target description, all within a 10,000 token context window.

## Key Results
- Fine-tuning 8x7b MoE model achieves 8% average improvement in F1-score over base model
- Fine-tuned 8x7b model performs comparably to 8x22b MoE model (0.63 vs 0.66 F1-score)
- F1-score measures correctness of method usage in generated code, not logical correctness

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Constrained decoding via static analysis context reduces API hallucinations.
- **Mechanism:** By injecting a pre-computed list of available classes and public method signatures into the prompt, the model's output space is restricted to valid internal APIs. This moves the task from pure generation (recall) to selection and assembly (guided synthesis), mitigating the tendency of LLMs to invent plausible but non-existent standard library functions.
- **Core assumption:** The static analysis graph accurately captures all dependencies and visible methods at the time of query.
- **Evidence anchors:** Abstract states RAG and static analysis alleviate the problem of LLMs assuming non-existent functions; methodology describes extracting imports and methods for the prompt's `<methods>` tag.
- **Break condition:** If the repository undergoes refactoring where method signatures change but the static analysis index is not updated, the prompt will provide stale context, leading to compilation errors.

### Mechanism 2
- **Claim:** Repository-specific idiosyncrasies are learned via In-Context Learning (ICL) with retrieved exemplars.
- **Mechanism:** Providing two retrieved code blocks similar to the target description allows the model to mimic the specific structural patterns of the codebase (e.g., the `TestBegin`/`TestEnd` boilerplate). This acts as a "soft" specification, guiding the model on style and mandatory utility calls that are not explicitly defined in the method signatures.
- **Core assumption:** The semantic embeddings of the test descriptions accurately correlate with the structural implementation of the tests.
- **Evidence anchors:** Page 3 describes retrieving similar code blocks via RAG for ICL; the paper notes this was necessary for learning code idiosyncrasies like logging requirements.
- **Break condition:** If the retrieved examples have low semantic similarity to the query but high lexical overlap, the model may be misled into using incorrect logic structures.

### Mechanism 3
- **Claim:** Instruction Fine-Tuning (IFT) on a Mixture of Experts (MoE) model optimizes parameter efficiency, matching larger dense models.
- **Mechanism:** Fine-tuning (specifically PEFT/LoRA) on a custom prompt template forces the 8x7b MoE model to specialize in the mapping between Ericsson-specific terminology and the Java code structure. This suggests that domain-specific code generation relies heavily on specific expert routing within the MoE, which can be optimized with less compute than scaling parameter count.
- **Core assumption:** The F1-score based on method usage correlates strongly with the practical utility of the generated code.
- **Evidence anchors:** Abstract reports 8% average improvement from fine-tuning 8x7b model, comparable to 8x22b model; Table 2 shows Mixtral 8x7b IFT (0.63 F1) closing gap with Mixtral 8x22B (0.66 F1).
- **Break condition:** This mechanism relies on the stability of the "Instruction - Methods - Exemplars" template; changing the prompt structure significantly post-training would likely degrade performance.

## Foundational Learning

- **Concept: Retrieval Augmented Generation (RAG)**
  - **Why needed here:** The paper explicitly uses RAG to solve the "spurious objects" problem. Without it, the LLM relies on generic training data rather than the specific proprietary codebase.
  - **Quick check question:** If the vector database were deleted, would the model still know the specific naming conventions for Ericsson's test logging?

- **Concept: Parameter-Efficient Fine-Tuning (PEFT/LoRA)**
  - **Why needed here:** The paper utilizes PEFT (Rank 256) to fine-tune a large MoE model. Understanding this is critical to replicating the "smaller model matching larger model" result without enterprise-scale training hardware.
  - **Quick check question:** Why does the paper use LoRA (Low-Rank Adaptation) instead of full parameter fine-tuning for the Mixtral 8x7b model?

- **Concept: Static Program Analysis (Dependency Graphs)**
  - **Why needed here:** The "context" provided to the LLM is not just text but structured knowledge (class ownership, imports, public methods) extracted via analysis. One must understand how to traverse the Abstract Syntax Tree (AST) or call graph to populate the `<methods>` tag in the prompt.
  - **Quick check question:** The prompt includes "fully qualified class names." Why is the fully qualified name necessary compared to just the class name?

## Architecture Onboarding

- **Component map:** Input (TCBD) -> Analysis Engine (static analysis) -> Retriever (RAG) -> Prompt Constructor -> LLM Core (Mixtral 8x7b with LoRA) -> Evaluator (F1 calculation)
- **Critical path:** The **Prompt Constructor** is the most fragile component. It requires precise alignment between the static analysis graph (identifying valid imports) and the current test file. If the import list is truncated (exceeding 10k tokens) or missing dependencies, the model reverts to hallucination.
- **Design tradeoffs:**
  - **Metric vs. Reality:** The system optimizes for *F1-score of method usage* (presence of correct calls), not *logical correctness* (if/else flow). The paper explicitly notes this limitation.
  - **Context Window vs. Detail:** The system caps context at 10,000 tokens. This forces a tradeoff between the number of exemplars (RAG) and the number of available helper methods (Static Analysis) included in the prompt.
- **Failure signatures:**
  - **High False Positives:** The model generates valid Java syntax but uses methods that do not exist in the repository (Context window failure or missing static analysis).
  - **Low F1 on Conditionals:** The paper notes poor scores on "custom exception calls" and complex conditionals, likely because the exemplar format struggles to capture distinct branching logic.
  - **Syntactic Drift:** The model ignores the `<methods>` constraints if the `TestBegin` description is ambiguous or too short.
- **First 3 experiments:**
  1. **Context Ablation:** Run the generator with RAG disabled (zero-shot with static analysis) and then with Static Analysis disabled (RAG only) to isolate the contribution of each component to the F1 score.
  2. **Token Limit Stress Test:** Systematically increase the number of helper classes included in the `<methods>` tag to find the threshold where the 10k context window degrades retrieval quality or truncates instructions.
  3. **Hallucination Audit:** Manually inspect 20 generated code blocks where F1 < 0.2 to classify failure modes (e.g., "Wrong Method Signature," "Missing Import," "Total Fabrication").

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the semantic correctness of the generated logic compare to the ground truth beyond just method invocation matching?
- **Basis in paper:** [explicit] The authors state future work should consider "correctness of the logic implemented" rather than relying solely on F1-scores based on method names.
- **Why unresolved:** The current evaluation metric only captures if the necessary method invocations are present; it fails to validate if conditionals (if-then-else) or loops function as intended.
- **Evidence would resolve it:** A manual code review or automated symbolic execution comparing the runtime behavior of generated blocks against the original developer code.

### Open Question 2
- **Question:** Does continual pre-training on domain-specific repositories improve test generation performance compared to the current instruction fine-tuning approach?
- **Basis in paper:** [explicit] The conclusion lists "Inclusion of continual pre-training" as a scope for future research to help models learn unique company coding styles.
- **Why unresolved:** The current approach uses base models and fine-tunes them (IFT), but does not evaluate the intermediate step of domain-specific pre-training to ground the model in the proprietary codebase style.
- **Evidence would resolve it:** Comparative benchmarks of F1-scores between a continually pre-trained model and the current fine-tuned Mixtral 8x7b model on the same test set.

### Open Question 3
- **Question:** What are the quantitative productivity benefits for developers relative to the economic and environmental costs of training and hosting these large models?
- **Basis in paper:** [explicit] The authors note that "productivity gains... need to be quantified and compared against the economic and environmental costs" of the system.
- **Why unresolved:** While initial user feedback was positive, a formal study quantifying the actual time saved versus the computational expense has not been conducted.
- **Evidence would resolve it:** A controlled user study measuring task completion times with and without the tool, correlated with an analysis of GPU hours and energy consumption required for the pipeline.

## Limitations
- The evaluation metric focuses only on method invocation correctness, not logical correctness of generated code
- The proprietary dataset and custom BERT-based embedder prevent direct reproduction of results
- The static analysis component's exact implementation for filtering protected methods across repositories is underspecified

## Confidence
- **High confidence:** The retrieval-augmented prompting mechanism effectively reduces API hallucinations by constraining the model's output space to valid internal APIs
- **Medium confidence:** The LoRA fine-tuning results showing an 8% F1 improvement and comparable performance to a larger model
- **Low confidence:** The claim that this approach is directly transferable to other codebases without significant adaptation

## Next Checks
1. **Context Ablation Study:** Run the generator with RAG disabled (static analysis only) and then with Static Analysis disabled (RAG only) to isolate the contribution of each component to the F1 score and test the robustness of the combined approach.
2. **Token Limit Stress Test:** Systematically increase the number of helper classes included in the `<methods>` tag to identify the threshold where the 10k context window degrades retrieval quality or truncates instructions, potentially revealing hidden performance bottlenecks.
3. **Hallucination Audit:** Manually inspect 20 generated code blocks where F1 < 0.2 to classify failure modes (e.g., "Wrong Method Signature," "Missing Import," "Total Fabrication") and determine if these failures stem from prompt construction, static analysis incompleteness, or model limitations.