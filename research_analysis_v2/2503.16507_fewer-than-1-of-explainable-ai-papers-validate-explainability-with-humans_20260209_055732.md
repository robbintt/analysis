---
ver: rpa2
title: Fewer Than 1% of Explainable AI Papers Validate Explainability with Humans
arxiv_id: '2503.16507'
source_url: https://arxiv.org/abs/2503.16507
tags:
- papers
- human
- explainability
- explainable
- literature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This large-scale literature analysis found that only 0.7% of explainable
  AI papers provide empirical evidence of human explainability, revealing a significant
  gap between claims and validation. The authors, in collaboration with a professional
  librarian, analyzed 18,254 XAI papers and identified just 128 that conducted human
  studies to validate explainability methods.
---

# Fewer Than 1% of Explainable AI Papers Validate Explainability with Humans

## Quick Facts
- arXiv ID: 2503.16507
- Source URL: https://arxiv.org/abs/2503.16507
- Reference count: 26
- Primary result: Only 0.7% of XAI papers provide empirical evidence of human explainability

## Executive Summary
This large-scale literature analysis found that only 0.7% of explainable AI papers provide empirical evidence of human explainability, revealing a significant gap between claims and validation. The authors, in collaboration with a professional librarian, analyzed 18,254 XAI papers and identified just 128 that conducted human studies to validate explainability methods. Their findings highlight that nearly half of papers claiming human explainability never test these claims with actual users, raising concerns about the rigor and trustworthiness of XAI research. The authors call for increased emphasis on human evaluations in XAI studies and provide their methodology to enable reproducibility and further investigation into this widespread issue.

## Method Summary
The authors conducted a bibliometric analysis using Scopus database searches to identify explainable AI papers, then applied keyword filters to find papers claiming human explainability. Three reviewers with XAI backgrounds manually scored each paper using binary validation criteria based on whether it conducted human experiments to evaluate the XAI method. They excluded review articles, workshop papers, non-English papers, and 16 off-topic papers (meta-reviews, philosophical arguments).

## Key Results
- Only 0.7% (128 out of 18,254) of XAI papers provide empirical evidence of human explainability
- Nearly half of papers making human-related claims never test these claims with actual users
- The authors identified a widespread validation gap in XAI research methodology

## Why This Works (Mechanism)

### Mechanism 1: The Signal-Transmission Verification Loop
- **Claim:** If an AI system produces an explanation artifact without human validation, it cannot validly claim to be explainable
- **Mechanism:** Treats explainability as successful information transmission that requires confirmation of receipt and understanding by a human user
- **Core assumption:** Explanation is a communicative act between AI and human, not a static data attribute
- **Evidence anchors:** Abstract finding of 0.7% validation rate; Section 4.2 argument about sending signals into a void
- **Break condition:** If explainability is defined solely by internal model properties without human consumption intent

### Mechanism 2: Bibliometric Filtering for "Inmate" Behavior
- **Claim:** Analyzing keyword co-occurrence can isolate the prevalence of researchers validating methods against internal metrics rather than external human reality
- **Mechanism:** Down-selects corpus using lexical markers to contrast claims of explainability against validation actions
- **Core assumption:** Specific search terms reliably correlate with human-subject experimentation
- **Evidence anchors:** 128 validated papers out of 18,254; use of Miller et al. validation criteria
- **Break condition:** If search terms fail to capture non-standard terminology or if explainability is used metaphorically

### Mechanism 3: The Proxy-Failure Hypothesis
- **Claim:** Relying on algorithmic metrics or "intuitively obvious" properties acts as a failed proxy for actual human understanding
- **Mechanism:** Identifies substitution of hard-to-measure human cognitive outcomes with easy-to-measure algorithmic artifacts
- **Core assumption:** High variance in human interpretation requires empirical verification
- **Evidence anchors:** Section 4.2 critique of intuitively obvious claims; Section 5.2 comparison to dataset limitations
- **Break condition:** If target user is the original developer (self-explanation), proxy metrics may be sufficient

## Foundational Learning

- **Concept: The Definition of "Interpretable" vs. "Explainable"**
  - **Why needed here:** To understand what is being claimed and critiqued
  - **Quick check question:** Does the paper claim the model is transparent by design (interpretable) or are they applying a technique to a black box (explainable)?

- **Concept: Empirical Rigor in HCI (Human-Computer Interaction)**
  - **Why needed here:** To understand why the 0.7% figure represents a failure of rigor
  - **Quick check question:** Does the paper measure "explainability" by asking humans to perform a task (empirical) or by measuring output sparsity (theoretical)?

- **Concept: Literature Review / Meta-Analysis Methodology**
  - **Why needed here:** To evaluate the validity of the "0.7%" result
  - **Quick check question:** Are the search criteria broad enough to capture the field, or specific enough to exclude false positives?

## Architecture Onboarding

- **Component map:** Scopus Database -> XAI Keywords filter -> Human-related Keywords filter -> Manual Scoring -> Validation Ratio
- **Critical path:** 1) Define scope (Scopus/English/Non-review), 2) Apply broad XAI filter, 3) Apply "human claim" keyword filter, 4) Manual Verification
- **Design tradeoffs:** Precision vs. Recall (strict keyword filtering may miss papers), Automation vs. Accuracy (keywords over-report validation, manual review required)
- **Failure signatures:** False Positive (human feedback for training not testing), False Negative (human validation without standard keywords), Off-Topic (philosophical essays without methods)
- **First 3 experiments:**
  1. Reproduce exact Scopus search to verify 18,254 count and 253 "claim" subset
  2. Audit 50 excluded papers to test if keyword filter was too strict
  3. Analyze 128 validated papers to score quality of human studies

## Open Questions the Paper Calls Out

- **Open Question 1:** Do papers that validate with human studies tend to be published at human-centered conferences (e.g., CHI) rather than AI-centered conferences (e.g., NeurIPS)?
  - **Basis in paper:** Section 5.1 explicitly asks this question
  - **Why unresolved:** The current work quantified validation rate but did not correlate papers with specific publication venues
  - **What evidence would resolve it:** Bibliometric analysis categorizing validated papers by venue type

- **Open Question 2:** What is the distribution of participant types, specifically domain experts versus students, used in XAI human evaluations?
  - **Basis in paper:** Section 5.1 asks about distribution of subject types
  - **Why unresolved:** Paper notes frequent failures to report expertise but did not systematically code available data
  - **What evidence would resolve it:** Coding analysis of validated papers to classify participant pools by expertise level

- **Open Question 3:** What distinguishing characteristics exist between papers that validate with humans versus those that claim interpretability without validation?
  - **Basis in paper:** Section 5.1 asks what can be learned from comparing validated and non-validated papers
  - **Why unresolved:** Paper identifies validation gap but has not conducted qualitative comparative analysis
  - **What evidence would resolve it:** Comparative content analysis identifying methodological trends across both groups

## Limitations

- Bibliometric methodology may undercount papers using non-standard terminology for human validation
- Binary validation scoring (0/1) does not capture quality or rigor of human studies
- Search scope excludes non-English literature and conference abstracts
- Does not address whether validated methods are more effective or trusted in practice

## Confidence

- **High Confidence**: The core finding that <1% of XAI papers conduct human validation (0.7% = 128/18,254) is reproducible given transparent methodology
- **Medium Confidence**: The claim that "nearly half" of papers making human-related claims never test them is accurate but may include false positives from keyword matching
- **Low Confidence**: The assertion that proxy metrics are ineffective is conceptually sound but lacks direct empirical evidence from this study

## Next Checks

1. **Quality Assessment Audit**: Analyze the 128 validated papers to score study quality (sample size, participant expertise, experimental design) and determine if a small subset is driving the "validated" count
2. **False Negative Investigation**: Take a random sample of 100 papers from the excluded "All XAI" set and manually check for human validation to quantify the recall rate
3. **Temporal Trend Analysis**: Examine whether the proportion of human-validated papers has increased over time by analyzing publication dates of both full corpus and validated subset