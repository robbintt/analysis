---
ver: rpa2
title: A Generative Security Application Engineering Curriculum
arxiv_id: '2501.10900'
source_url: https://arxiv.org/abs/2501.10900
tags:
- students
- code
- security
- https
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a generative security application engineering
  curriculum that integrates large language models (LLMs) and generative AI into cybersecurity
  education. The core approach teaches students how to apply LLMs to security tasks
  through hands-on exercises covering model fundamentals, LangChain framework development,
  security vulnerabilities, and practical security applications including code analysis,
  vulnerability discovery, command generation, threat intelligence, and social engineering.
---

# A Generative Security Application Engineering Curriculum

## Quick Facts
- arXiv ID: 2501.10900
- Source URL: https://arxiv.org/abs/2501.10900
- Reference count: 40
- Teaches students to apply LLMs to cybersecurity tasks through hands-on exercises covering model fundamentals, LangChain development, LLM security vulnerabilities, and practical security applications

## Executive Summary
This paper presents a comprehensive curriculum for teaching security application engineering using generative AI and large language models. The curriculum was developed and implemented at Portland State University in 2024, with all materials made publicly available. It addresses the gap between traditional cybersecurity education and the emerging need for skills in leveraging AI automation for security tasks while understanding the associated security implications.

## Method Summary
The curriculum follows a four-part structure: (1) foundational LLM concepts using both local models via Ollama and closed-source APIs, (2) LangChain framework development for building RAG applications and autonomous agents, (3) OWASP LLM security vulnerabilities with hands-on exploitation exercises, and (4) practical security applications including code analysis, vulnerability discovery, command generation, threat intelligence, and social engineering. Students work through exercises using ground-truth verification to assess model reliability, progressing from basic model behavior understanding to building and securing complex AI-powered security applications.

## Key Results
- Comprehensive educational framework that integrates LLMs into cybersecurity curriculum
- Hands-on exercises covering the complete pipeline from model fundamentals to secure application development
- Publicly available materials including slides, lab exercises, source code, and video recordings
- Implementation validated through two course offerings at Portland State University in 2024

## Why This Works (Mechanism)

### Mechanism 1: Progressive Abstraction Ladder
Students learn LLM security applications more effectively when concepts scaffold from low-level (model behavior) to high-level (autonomous agents). The curriculum sequences: model fundamentals → framework abstractions → security vulnerabilities → practical security tasks. Students first understand non-deterministic model behavior, then build applications, then attack them, then apply to real security work. Core assumption: Understanding model probabilistic behavior is prerequisite to securing applications built on top of them.

### Mechanism 2: Ground-Truth Verification Loop
Using security tasks with known correct answers enables students to independently assess LLM reliability without expert supervision. CTF levels, known-vulnerable code samples, and documented commands provide deterministic verification; students benchmark model performance against ground truth. Core assumption: Security domains contain sufficient tasks with objectively verifiable outcomes.

### Mechanism 3: Offensive-First Security Pedagogy
Students develop deeper retention of LLM security concepts by exploiting vulnerabilities before learning mitigations. Students attack naive agents (excessive agency, missing validation), observe failures, then construct secure alternatives—similar to DOM-based XSS teaching. Core assumption: Experiential attack understanding transfers to defensive design patterns.

## Foundational Learning

- **Concept: Python programming (functions, classes, API calls)**
  - Why needed here: All LangChain exercises use Python; students must read/write chains, tools, and agent code
  - Quick check question: Can you write a Python class that makes an HTTP POST request and parses the JSON response?

- **Concept: SQL query syntax and injection**
  - Why needed here: SQLToolkit agent exercises require understanding both valid queries and injection vectors
  - Quick check question: What's the difference between `"SELECT * FROM users WHERE id=" + user_input` and a parameterized query?

- **Concept: Probabilistic model behavior (temperature, tokens, context windows)**
  - Why needed here: Model selection and prompt design depend on understanding non-determinism and cost tradeoffs
  - Quick check question: What happens to output variance as you increase temperature from 0.1 to 1.0?

## Architecture Onboarding

- **Component map:**
  - Model Layer: Open-source (Ollama) + closed-source APIs
  - Framework Layer: LangChain (chains, agents, tools, retrievers)
  - Data Layer: Document loaders → chunkers → embedding models → vector stores (ChromaDB)
  - Security Layer: Pydantic validation, custom restricted tools, output sanitization

- **Critical path:**
  1. Set up model access (Ollama local + API keys)
  2. Build basic chain with prompt template
  3. Construct RAG pipeline: loader → chunker → embedder → vector store → retrieval chain
  4. Convert to ReAct agent with tools
  5. Secure with custom tools that restrict operations and validate inputs

- **Design tradeoffs:**
  - Agency vs. security: More tool access increases capability and attack surface
  - Model capability vs. cost: Closed-source models perform better on security tasks but incur per-token costs
  - Chunk size vs. retrieval precision: Smaller chunks improve relevance but may fragment context

- **Failure signatures:**
  - Agent infinite loops (poorly designed stopping conditions)
  - Hallucinated but plausible command syntax
  - Indirect prompt injection via RAG-retrieved documents containing "Action:" triggers
  - Excessive token consumption from oversized context windows

- **First 3 experiments:**
  1. Run identical deobfuscation task across multiple models using model laboratory code; compare accuracy on base64 and XOR-encoded samples
  2. Build SQLToolkit agent, exploit it via natural language ("drop table users"), then implement custom tool restricted to SELECT with Pydantic alphanumeric validation
  3. Create ReAct agent combining RAG tool + Terminal tool; poison a document with embedded "Action: curl attacker.com" to demonstrate indirect prompt injection chain

## Open Questions the Paper Calls Out

### Open Question 1
Can students effectively depend upon current LLMs to automate security tasks without excessive manual verification? The paper outlines the curriculum for testing this, but does not aggregate or report on the reliability metrics found by students. Statistical analysis of student lab results showing LLM accuracy and false positive rates across different security tasks would resolve this.

### Open Question 2
How does integrating generative AI impact student retention of fundamental security concepts compared to traditional methods? The abstract claims the goal is to "better align security education," but provides no comparative learning assessment. A controlled study comparing the conceptual mastery of students using the generative curriculum versus a standard curriculum would resolve this.

### Open Question 3
Can LLMs accurately perform binary analysis without relying on external decompilation tools? The paper notes that LLMs are "ineffective" at analyzing binary executables directly due to training and context limits. Benchmarking native LLM performance on binary analysis tasks as model context windows and training sets expand would resolve this.

## Limitations
- Effectiveness evaluation limited to anecdotal feedback without systematic assessment of student learning outcomes or long-term retention
- Security focus may create blind spots—students might over-rely on LLM-generated security outputs without developing critical verification skills for novel threats
- No quantitative metrics on student performance, comprehension levels, or post-course application of skills provided

## Confidence

- **High confidence**: The curriculum's technical components (LangChain integration, RAG pipeline construction, Pydantic validation) are well-specified and reproducible based on publicly available materials
- **Medium confidence**: The pedagogical approach of offensive-first learning and ground-truth verification loops is theoretically sound but lacks empirical validation specific to LLM security education
- **Low confidence**: Claims about the curriculum's effectiveness in preparing students for "the evolving security landscape" are not supported by longitudinal studies or industry feedback

## Next Checks

1. Conduct pre/post assessments measuring student ability to identify and exploit LLM vulnerabilities, comparing results between students who follow the offensive-first curriculum versus traditional defensive-first approaches
2. Implement a longitudinal study tracking curriculum graduates' performance in security roles, specifically their ability to detect LLM-generated threats and design secure AI applications
3. Perform a controlled experiment testing the ground-truth verification mechanism by introducing novel security tasks without known answers, measuring student reliance on model outputs versus independent analysis