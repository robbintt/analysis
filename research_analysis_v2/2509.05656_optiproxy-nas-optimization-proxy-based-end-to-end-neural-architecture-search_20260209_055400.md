---
ver: rpa2
title: 'OptiProxy-NAS: Optimization Proxy based End-to-End Neural Architecture Search'
arxiv_id: '2509.05656'
source_url: https://arxiv.org/abs/2509.05656
tags:
- search
- architecture
- proxy
- neural
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OptiProxy-NAS, a novel framework for neural
  architecture search (NAS) that reformulates the problem as a differentiable optimization
  task. Unlike traditional predictor-guided or differentiable NAS methods, OptiProxy-NAS
  employs an optimization proxy to create a continuous, differentiable, and smooth
  search space, enabling gradient-based optimization.
---

# OptiProxy-NAS: Optimization Proxy based End-to-End Neural Architecture Search

## Quick Facts
- arXiv ID: 2509.05656
- Source URL: https://arxiv.org/abs/2509.05656
- Authors: Bo Lyu; Yu Cui; Tuo Shi; Ke Li
- Reference count: 40
- Key outcome: Achieves optimal architectures with up to 50% fewer queries than state-of-the-art methods across 12 NAS tasks.

## Executive Summary
OptiProxy-NAS introduces a novel framework that reformulates neural architecture search (NAS) as a differentiable optimization problem. By employing an optimization proxy with re-parameterization techniques, the method creates a continuous, differentiable search space that enables gradient-based optimization of architecture parameters. The framework excels in both standard accuracy maximization and hardware-aware multi-objective optimization scenarios, demonstrating superior sample efficiency and performance across diverse NAS benchmarks.

## Method Summary
OptiProxy-NAS operates through an iterative SMBO loop where it trains a proxy model (typically a GCN) to predict architecture performance, then uses gradient descent on continuous architecture parameters (via Gumbel-Softmax re-parameterization) to find high-performing regions in the proxy space. The method samples new architectures from these regions, evaluates them, and updates the dataset. This approach enables end-to-end differentiable optimization while incorporating real-world metrics like latency constraints without requiring differentiable hardware estimators in the main training loop.

## Key Results
- Achieves optimal architectures with up to 50% fewer queries than state-of-the-art methods
- Excels in hardware-aware and low-fidelity scenarios on HW-NAS-Bench
- Adds minimal computational overhead (0.037% runtime, 0.25% memory)
- Demonstrates compatibility with various proxy models and encoding schemes across 12 NAS tasks

## Why This Works (Mechanism)

### Mechanism 1: Differentiable Search via Proxy Re-parameterization
The framework enables gradient-based optimization over discrete architecture variables by re-parameterizing them into a continuous, differentiable proxy space using the Gumbel-Softmax trick. This allows the proxy model's predicted performance to directly guide the search distribution via gradient descent, avoiding the need for discrete samplers like EA or RL.

### Mechanism 2: Sequential Model-Based Optimization with Dynamic Proxy
OptiProxy-NAS iteratively refines both the proxy model and search distribution, focusing evaluation budget on promising regions. The system loops through training the proxy on evaluated architectures, optimizing architecture parameters via gradient descent, sampling new architectures, and updating the dataset. This ensures the proxy progressively learns high-performance regions.

### Mechanism 3: Hardware-Aware Multi-Objective Optimization
The framework optimizes for non-differentiable real-world metrics by training the proxy model to predict hardware latency, then using gradient-based search to maximize weighted aggregations of accuracy and predicted latency constraints. The optimization happens in the proxy's prediction space, decoupling hardware metric non-differentiability from the search process.

## Foundational Learning

- **Gumbel-Softmax / Concrete Distribution**: Enables discrete architecture choices to be treated as continuous variables for gradient descent. Quick check: Can you explain how the temperature parameter τ controls the trade-off between sampling randomness and deterministic selection?
- **Graph Neural Networks (GNNs) as Surrogates**: The proxy model uses GNNs to predict performance from graph representations of architectures. Quick check: How does a GCN aggregate information from neighbor nodes, and why is this suitable for DAG-structured networks?
- **Bi-level Optimization in NAS**: While simplifying standard DARTS, OptiProxy introduces implicit bi-level optimization between proxy model parameters and architecture distribution parameters. Quick check: Why must the proxy model be retrained before searching for new architectures?

## Architecture Onboarding

- **Component map**: Encoding Scheme -> Proxy Model (GCN) -> Proxy Sampler (Gumbel-Softmax) -> Search Optimizer (AdamW)
- **Critical path**: Initialize (α, β) → Random Sample initial set D → Loop: Fit Proxy f on D → Freeze f → Gradient Descent on (α, β) to maximize f(s(α, β)) → Sample discrete architectures → Query Oracle → Update D
- **Design tradeoffs**: Proxy complexity vs. speed (larger proxies increase overhead), temperature schedule (fast decay risks local optima, slow decay wastes queries), encoding depth (simple adjacency matrices vs. path-based encodings)
- **Failure signatures**: Mode collapse (α distributions converge too early), proxy-reality gap (proxy predictions don't match real performance), latency violation (found architectures violate constraints upon deployment)
- **First 3 experiments**: 1) Sanity check on NB-201 with 20 queries to verify Random Search outperformance, 2) Visualize α entropy over search epochs to verify convergence pattern, 3) Hardware constraint test on HW-NAS-Bench comparing proxy latency predictions to real latency

## Open Questions the Paper Calls Out

The paper explicitly mentions the potential for parallelizing OptiProxy-NAS and states this will be explored in future work. The conclusion indicates that different parameter groups share metrics with "almost zero additional cost," suggesting parallelization could further reduce wall-clock search time, though no empirical analysis is provided.

## Limitations

- Performance heavily depends on proxy model quality, which may struggle with complex search spaces or subtle performance differences
- The assumption that proxy landscape optimization leads to optimal true-space architectures may fail if proxy-reality alignment is poor
- Computational overhead of retraining proxy models accumulates for very large search spaces or complex proxy architectures
- Claims about generalizability to any proxy model or encoding scheme lack extensive validation beyond specific configurations

## Confidence

- **High Confidence**: Experimental results showing superior sample efficiency and performance across multiple NAS benchmarks are well-supported by presented data
- **Medium Confidence**: Theoretical mechanism claims about differentiable optimization are sound but practically depend on proxy model quality not extensively analyzed
- **Low Confidence**: Generalizability claims to any proxy model or encoding scheme are theoretical, validated only on specific configurations

## Next Checks

1. **Proxy-Reality Gap Analysis**: Compute correlation between proxy predictions and actual benchmark evaluations across all search steps to quantify landscape alignment
2. **Ablation on Proxy Model Complexity**: Run searches on NB-201 using different proxy architectures (MLP vs. GCN vs. Transformer) while keeping hyperparameters constant
3. **Sensitivity to Initial Sample Size**: Repeat NB-201 experiment with varying initial sample sizes (5, 15, 25) to test robustness to initial conditions