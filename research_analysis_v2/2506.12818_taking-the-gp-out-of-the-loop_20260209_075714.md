---
ver: rpa2
title: Taking the GP Out of the Loop
arxiv_id: '2506.12818'
source_url: https://arxiv.org/abs/2506.12818
tags:
- optimization
- observations
- time
- turbo-enn
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of scaling Bayesian optimization
  (BO) to settings with many observations (BOMO), where traditional Gaussian process
  (GP) surrogates become computationally prohibitive. The authors propose Epistemic
  Nearest Neighbors (ENN), a lightweight surrogate that estimates function values
  and uncertainty using K-nearest-neighbor observations.
---

# Taking the GP Out of the Loop

## Quick Facts
- arXiv ID: 2506.12818
- Source URL: https://arxiv.org/abs/2506.12818
- Reference count: 38
- Primary result: TuRBO-ENN achieves 10-100× speedup over TuRBO while maintaining competitive solution quality on BOMO benchmarks with up to 50,000 observations

## Executive Summary
This paper addresses the computational bottleneck of Gaussian process surrogates in Bayesian optimization when scaling to thousands of observations. The authors propose Epistemic Nearest Neighbors (ENN), a lightweight surrogate that estimates function values and uncertainty using K-nearest-neighbor observations with O(N) query time. By treating nearby observations as independent estimates and using a Pareto-optimal acquisition method, ENN enables Bayesian optimization without the expensive GP machinery while maintaining solution quality comparable to state-of-the-art methods.

## Method Summary
The paper introduces TuRBO-ENN, which replaces the Gaussian process surrogate in TuRBO with the ENN surrogate. ENN computes function value estimates as precision-weighted averages of K nearest neighbors and epistemic uncertainty based on distance from the nearest neighbor. For noise-free problems, hyperparameters are fixed (s₀=0, cₑ=1) and a non-dominated sorting acquisition method selects candidates from the Pareto front of (μ,σ) pairs. For noisy problems, hyperparameters are fit via subsampled LOOCV pseudolikelihood optimization and UCB acquisition is used. The method integrates with TuRBO's trust region framework and RAASP candidate generation.

## Key Results
- TuRBO-ENN achieves 10-100× speedup in proposal time compared to TuRBO across various benchmarks
- Solution quality remains competitive with TuRBO on physics simulations and hyperparameter optimization tasks with up to 50,000 observations
- The method satisfies Pseudo-Bayesian Optimization axioms, providing convergence guarantees for continuous objectives on compact domains
- ENN maintains O(N) query time complexity while GPs scale as O(N²) or O(N³)

## Why This Works (Mechanism)

### Mechanism 1: Linear-Time Surrogate via Independence Approximation
ENN achieves O(N) query time by treating K-nearest-neighbor observations as independent estimates of the function value at a query point. Each neighbor provides an independent estimate with variance σ² = (s₀² + sₘ²) + cₑd(x, xₘ)², and precision-weighted averaging combines these estimates without the O(N²) kernel matrix construction required by GPs. The core assumption is that nearby observations provide independent information about a query point's value, with epistemic uncertainty growing with distance from observed points.

### Mechanism 2: Pareto-Based Acquisition for Uncalibrated Uncertainty
Non-dominated sorting over (μ, σ) pairs enables acquisition without requiring calibrated uncertainty scales. NDS compares μ-values among themselves and σ-values among themselves separately—never directly comparing μ to σ. This eliminates the need to fit cₑ for calibration when noise level is known (e.g., s₀=0 for noise-free problems), allowing the method to work with uncalibrated uncertainty estimates.

### Mechanism 3: Convergence Guarantee via Pseudo-BO Axioms
TuRBO-ENN satisfies Pseudo-Bayesian Optimization axioms, providing algorithmic consistency guarantees. ENN's mean predictor satisfies local consistency (LC); the distance-based uncertainty satisfies the sequential no-empty-ball property (SNEB); Pareto-compatible scalarized acquisition satisfies improvement property (IP). This framework provides convergence guarantees under the assumptions that the objective function is continuous and the design space is compact.

## Foundational Learning

- **Gaussian Process Surrogates**: Understanding the O(N²)–O(N³) bottleneck ENN replaces; GP provides the baseline for comparison. Quick check: Can you explain why computing the GP posterior requires O(N³) operations naively and O(N²) with modern solvers?

- **Trust Region Bayesian Optimization (TuRBO)**: TuRBO-ENN inherits TuRBO's trust region framework; understanding RAASP sampling and trust region updates is essential. Quick check: How does restricting Thompson sampling to a trust region improve efficiency in high-dimensional BO?

- **Pareto Optimality and Non-Dominated Sorting**: The noise-free acquisition uses NDS over (μ, σ) pairs; understanding Pareto fronts is critical for this variant. Quick check: Given a set of candidates with (μ, σ) pairs, which points lie on the first Pareto front when maximizing μ and σ?

## Architecture Onboarding

- **Component map**: Data → ENN Surrogate (K-NN mean/uncertainty) → Hyperparameter Fitting (subsampled LOOCV) → Trust Region (TuRBO) → Acquisition (NDS/UCB) → Evaluate → Append to Data

- **Critical path**: Initialize with LHD sample, evaluate initial observations → For each round: optionally fit (s₀, cₑ) via subsampled LOOCV, select incumbent, generate candidates via RAASP, compute ENN (μ, σ) for candidates, select arm via NDS or UCB, evaluate and append to dataset → Return best observation by value (noise-free) or by μ (noisy)

- **Design tradeoffs**: Speed vs. calibration (ENN provides uncalibrated uncertainty; faster than GP but may be less reliable for UCB-style acquisition); K selection (paper fixes K=10; smaller K is faster but may increase variance; larger K smooths but dilutes local information); Fitting vs. fitting-free (noise-free variant skips hyperparameter fitting entirely; noisy variant requires O(PN ln K) fitting cost)

- **Failure signatures**: Exploration collapse (if σ estimates shrink too fast, acquisition may over-exploit; monitor Pareto front diversity); Curse of dimensionality (Euclidean distance discriminates poorly in high D; may need dimension weighting); Hyperparameter instability (if LOOCV pseudolikelihood fails to converge, fall back to default)

- **First 3 experiments**: 1) Scaling benchmark: Run TuRBO-ENN and TuRBO on synthetic functions (Ackley, Rastrigin) with N ∈ {1000, 10000, 50000}, measure proposal time ratio; 2) Noise-free vs. noisy comparison: Test both acquisition variants on simulation with frozen noise vs. natural noise; 3) Ablation on K: Vary K ∈ {5, 10, 20, 50} on LunarLander-v3 to assess sensitivity

## Open Questions the Paper Calls Out

### Open Question 1
Can ENN be adapted to use approximate nearest neighbor (ANN) algorithms to achieve O(log N) scaling? The current implementation relies on exact K-nearest-neighbor calculations (O(N) query time), which creates a bottleneck even if smaller than the O(N²) of GPs. An implementation of ENN using ANN (e.g., HNSW graphs) that demonstrates sub-linear scaling while maintaining competitive optimization performance would resolve this.

### Open Question 2
Can ENN be extended to support joint sampling for Thompson Sampling acquisition? ENN currently treats observations as independent to maintain O(N) complexity, which simplifies variance estimation but precludes the generation of the correlated samples required for Thompson Sampling. A modified ENN model that can generate consistent, correlated function samples across a batch of candidate points without degrading the linear time complexity would address this.

### Open Question 3
Can a fast, scalable method for weighting dimensions (ARD) be integrated into ENN? Current ENN uses Euclidean distance, which weights all dimensions equally. This is inefficient for high-dimensional problems where irrelevant dimensions can distort distance metrics, yet standard ARD is computationally expensive. A method for learning per-dimension weights within the ENN framework that preserves the surrogate's O(N) scalability would resolve this.

### Open Question 4
Can theoretical regret bounds be established for TuRBO-ENN? While there is a convergence guarantee, the paper provides no regret bounds. The paper proves the method satisfies Pseudo-BO axioms (convergence), but standard BO metrics like cumulative regret bounds are mathematically distinct and currently missing. A formal theoretical derivation providing an upper bound on the cumulative regret for the TuRBO-ENN algorithm would address this.

## Limitations

- The independence assumption in ENN may fail for correlated nearby observations, potentially leading to overconfident uncertainty estimates requiring empirical validation
- Hyperparameter fitting via subsampled LOOCV pseudolikelihood introduces computational overhead that may erode the claimed speedup for noisy problems
- The Pareto-based acquisition method's performance in high-dimensional spaces with poorly discriminative Euclidean distances remains untested

## Confidence

- **High confidence**: O(N) query time achievement and one-to-two order of magnitude speedup relative to TuRBO (supported by Figure 1 and Table 1)
- **Medium confidence**: Solution quality comparable to TuRBO across benchmarks (results show competitive performance but with variance)
- **Medium confidence**: Convergence guarantee via Pseudo-BO axioms (formally proven but relies on noiseless evaluation assumption)

## Next Checks

1. Run ablation study on K parameter across multiple benchmarks to determine optimal K value and assess sensitivity to this critical hyperparameter
2. Implement dimension weighting for high-dimensional problems to test whether the proposed future work addressing Euclidean distance limitations improves performance
3. Compare ENN uncertainty calibration against GP uncertainty on a synthetic problem where ground truth uncertainty is known to quantify potential overconfidence issues