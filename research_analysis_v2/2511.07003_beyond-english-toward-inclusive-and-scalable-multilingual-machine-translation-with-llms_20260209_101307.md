---
ver: rpa2
title: 'Beyond English: Toward Inclusive and Scalable Multilingual Machine Translation
  with LLMs'
arxiv_id: '2511.07003'
source_url: https://arxiv.org/abs/2511.07003
tags:
- languages
- translation
- language
- data
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper identifies and addresses a previously overlooked issue\
  \ in large-scale multilingual machine translation: directional degeneration. This\
  \ phenomenon occurs during symmetric multi-way fine-tuning when models overfit to\
  \ high-frequency English/Chinese target mappings, degrading translation quality\
  \ in reverse directions (X\u2192En/Zh)."
---

# Beyond English: Toward Inclusive and Scalable Multilingual Machine Translation with LLMs

## Quick Facts
- **arXiv ID**: 2511.07003
- **Source URL**: https://arxiv.org/abs/2511.07003
- **Reference count**: 40
- **Primary result**: Identifies and solves "directional degeneration" in multilingual fine-tuning with Strategic Downsampling and PMP, achieving state-of-the-art performance among models of comparable language coverage.

## Executive Summary
This paper addresses a critical challenge in large-scale multilingual machine translation: directional degeneration during symmetric multi-way fine-tuning, where models overfit to high-frequency English/Chinese target mappings at the expense of translation quality in reverse directions. The authors propose Strategic Downsampling—retaining only 5% of reverse-direction samples—to mitigate this degeneration, and introduce Parallel Multilingual Prompting (PMP) to enhance cross-lingual transfer using typologically related auxiliary languages. Their LMT model suite, covering 60 languages in 234 directions, achieves state-of-the-art performance among comparably-sized models, with the 4B-parameter LMT-60-4B surpassing significantly larger models like Aya-101-13B and NLLB-54B by substantial margins.

## Method Summary
The method employs a two-stage adaptation approach: (1) CPT (continued pre-training) on 90B tokens using a 1:1:1 mixture of monolingual, Chinese-centric bilingual, and English-centric bilingual data to establish foundational multilingual competence, followed by (2) SFT (supervised fine-tuning) with Strategic Downsampling (5% retention for X→En/Zh directions) and Parallel Multilingual Prompting (PMP) using typologically related auxiliary languages. The models are built on Qwen3 base models (0.6B to 8B parameters) and trained on parallel data from Flores-200, NTREX, SMol, and WMT/IWSLT test sets, achieving broad coverage across 60 languages.

## Key Results
- LMT-60-4B achieves 40.9 COMET on FLORES-200, surpassing Aya-101-13B by 2.5 points and NLLB-54B by 2.8 points despite having significantly fewer parameters.
- Strategic Downsampling at 5% ratio prevents directional degeneration while maintaining translation quality across all directions.
- PMP improves translation quality by +1.8 COMET for In-Group HRL→LRL pairs and +0.7 for Out-of-Group pairs, demonstrating effective cross-lingual transfer.

## Why This Works (Mechanism)

### Mechanism 1: Strategic Downsampling to Prevent Directional Degeneration
- **Claim**: Retaining only 5% of X→En/Zh samples during SFT prevents quality collapse in many-to-one translation directions.
- **Mechanism**: Symmetric multi-way parallel data creates excessive many-to-one mappings where each English/Chinese target sentence repeats up to 59 times. The model learns a "Shallow Mapping Trap" shortcut—mapping diverse sources to a limited set of high-frequency target patterns instead of learning faithful source-conditioned generation. Downsampling reduces target-side repetition, preserving alignment signal without triggering the shortcut.
- **Evidence**: Figure 4 shows performance degradation at p=100% and recovery with as little as p=0.5%, peaking around p=5%; oracle experiment confirms the issue is symmetric reuse, not direction difficulty.

### Mechanism 2: Parallel Multilingual Prompting (PMP) for Cross-Lingual Transfer
- **Claim**: Augmenting translation instructions with parallel sentences from typologically related auxiliary languages improves translation quality, especially for low-resource directions.
- **Mechanism**: PMP provides explicit semantic and lexical anchors during training. For En↔X directions, the auxiliary is a typologically similar high-resource language; for Zh↔X directions, English serves as a universal pivot. The auxiliary context guides the model toward higher-fidelity translations without discarding source information.
- **Evidence**: Figure 6 shows +1.8 COMET gain for In-Group HRL→LRL pairs and +0.7 for Out-of-Group pairs; PMP-S outperforms PMP-O for X→En/Zh directions.

### Mechanism 3: Two-Stage CPT→SFT Adaptation for Broad Multilingual Competence
- **Claim**: CPT on large-scale monolingual + bilingual data provides foundational multilingual competence; SFT refines translation-specific behavior.
- **Mechanism**: CPT injects translation knowledge across 60 languages using 90B tokens (1:1:1 monolingual:Chinese-centric:English-centric). SFT then aligns with high-quality human translations using Strategic Downsampling and PMP. The stages are complementary: CPT strengthens foundational linguistic knowledge where base models lack exposure; SFT prevents degeneration and enables cross-lingual transfer.
- **Evidence**: Figure 5 ablation shows CPT contributes +3.80 to +8.23 COMET points across directions, with larger gains for medium/low-resource languages.

## Foundational Learning

- **Concept**: Many-to-One Mapping in Multilingual SFT
  - **Why needed here**: Understanding why symmetric bidirectional data causes directional degeneration requires recognizing that repeating the same target (English/Chinese) across 59 source languages creates a pathological learning signal.
  - **Quick check question**: Can you explain why bidirectional reuse of parallel data harms X→En/Zh but not En/Zh→X directions?

- **Concept**: Pivot Translation vs. In-Context Auxiliary
  - **Why needed here**: PMP resembles pivot translation but differs critically—it preserves source information while using auxiliary translations as semantic hints. Understanding this distinction clarifies why PMP-S can outperform PMP-O.
  - **Quick check question**: How does PMP differ from traditional two-step pivot translation (X→En→Zh), and why does this matter for source fidelity?

- **Concept**: Cross-Lingual Transfer from Typologically Related Languages
  - **Why needed here**: PMP's auxiliary language selection relies on linguistic typology (shared family, shared script). Understanding this helps diagnose when PMP will succeed or fail.
  - **Quick check question**: For a low-resource language with no typologically close high-resource relative, what fallback strategy should PMP use?

## Architecture Onboarding

- **Component map**: Base Model (Qwen3) → CPT (90B tokens, monolingual + bilingual) → SFT (596K pairs, Strategic Downsampling + PMP) → LMT
- **Critical path**: CPT quality filtering determines foundational multilingual competence; Strategic Downsampling proportion (p=5%) prevents directional degeneration; PMP auxiliary language selection determines cross-lingual transfer effectiveness
- **Design tradeoffs**: Language coverage vs. per-language data (60 languages with 90B tokens vs. depth-focused alternatives); PMP inference cost vs. quality (self-generated vs. oracle auxiliary); Downsampling aggressiveness (p<1% risks under-alignment; p>20% risks degeneration)
- **Failure signatures**: Fluent but unfaithful X→En/Zh outputs indicate directional degeneration from insufficient downsampling; asymmetric performance gaps suggest many-to-one mapping trap; left-skewed quality distributions for low-resource pairs indicate data scarcity or QE model bias
- **First 3 experiments**: (1) Validate directional degeneration on your base model by comparing 100% symmetric data vs. 5% Strategic Downsampling; (2) Ablate PMP auxiliary language selection by comparing typological vs. random vs. no PMP; (3) Calibrate CPT data mixture by testing 1:1:1 vs. bilingual-heavy vs. monolingual-heavy formulations

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the bi-centric adaptation strategy be successfully extended to tri- or multi-centric configurations (e.g., adding Arabic or Spanish hubs) without inducing catastrophic interference or exacerbating directional degeneration? The paper only validates Chinese-English pivot; adding additional hubs significantly increases the complexity of balancing the "many-to-one" mapping problem.

- **Open Question 2**: Why does inference with self-generated auxiliary hints (PMP-S) outperform oracle hints (PMP-O) specifically for the X→En/Zh directions? The paper notes it is "surprising" that PMP-S achieves the best performance for these directions, suggesting the model benefits from hints "aligned with its internal biases," but the underlying mechanism remains unproven.

- **Open Question 3**: Does the LMT model's high performance on academic benchmarks translate to cultural appropriateness and robustness in real-world translation scenarios? The paper acknowledges evaluation is "primarily conducted on academic benchmarks" and suggests future work should assess cultural appropriateness and domain adaptation beyond FLORES-200.

## Limitations

- The directional degeneration root cause is primarily established through empirical observation rather than theoretical grounding, relying heavily on the assumption that many-to-one target mappings create pathological learning signals.
- PMP typological selection lacks empirical validation—the paper reports improved performance but doesn't establish that typological similarity is the optimal selection criterion versus alternatives like embedding-based distance metrics.
- Several critical implementation details remain unspecified, including exact CometKiwi quality thresholds, the algorithm for typologically related auxiliary language selection, and specific models used for pseudo-parallel data synthesis.

## Confidence

- **High Confidence**: The existence of directional degeneration and the effectiveness of Strategic Downsampling (5% ratio) in mitigating it. Well-supported by multiple experiments including the oracle study and controlled ablations.
- **Medium Confidence**: The PMP mechanism and its typological selection heuristic. Performance improvements are demonstrated, but the theoretical justification for why typological similarity drives transfer benefit remains heuristic rather than rigorously established.
- **Medium Confidence**: The two-stage CPT→SFT architecture effectiveness. The ablation study shows clear benefits, but the optimal data mixture ratios and comparative advantage over alternative architectures remain underexplored.

## Next Checks

1. **Cross-Model Validation**: Replicate the directional degeneration phenomenon and Strategic Downsampling effectiveness on at least two additional base models (e.g., Llama3.1, Gemma2) with different multilingual pre-training strategies to establish whether findings generalize beyond Qwen3.

2. **PMP Selection Criteria Comparison**: Systematically compare PMP performance using different auxiliary language selection methods: (a) typological similarity, (b) embedding-based language distance metrics, (c) transfer learning similarity scores, and (d) random selection to empirically validate whether linguistic typology provides optimal transfer benefits.

3. **Downsampling Threshold Analysis**: Conduct a more granular analysis of the Strategic Downsampling parameter space, testing multiple proportions (0.1%, 0.5%, 1%, 2%, 5%, 10%, 20%) across different language subsets (high-resource, medium-resource, low-resource) to establish precise boundaries where degeneration begins and optimal trade-offs between quality and coverage emerge.