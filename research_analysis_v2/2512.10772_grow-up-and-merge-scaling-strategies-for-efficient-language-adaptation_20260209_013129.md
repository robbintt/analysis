---
ver: rpa2
title: 'Grow Up and Merge: Scaling Strategies for Efficient Language Adaptation'
arxiv_id: '2512.10772'
source_url: https://arxiv.org/abs/2512.10772
tags:
- language
- merging
- languages
- english
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates upscaling as a strategy for adapting pretrained
  language models to new target languages. The authors compare models scaled via HyperCloning
  with multilingual baselines and find that upscaled models achieve higher performance
  on linguistic acceptability and information parity tasks, while also preserving
  English capabilities better than smaller models.
---

# Grow Up and Merge: Scaling Strategies for Efficient Language Adaptation

## Quick Facts
- arXiv ID: 2512.10772
- Source URL: https://arxiv.org/abs/2512.10772
- Reference count: 33
- Models scaled via HyperCloning achieve higher performance on linguistic acceptability and information parity tasks than smaller models

## Executive Summary
This paper investigates upscaling as a strategy for adapting pretrained language models to new target languages. The authors compare models scaled via HyperCloning with multilingual baselines and find that upscaled models achieve higher performance on linguistic acceptability and information parity tasks, while also preserving English capabilities better than smaller models. Upscaling directly on target language data is more compute-efficient than first scaling on English, especially for medium-to-high-resource languages. Model merging, while less effective than joint multilingual training, benefits from upscaling, with upscaled merges outperforming those of smaller models.

## Method Summary
The study uses SmolLM2-based architecture with Llama 3.3 tokenizer and investigates three upscaling strategies: 1× (direct target language adaptation), 1× cloned (upscale first, then adapt), and 2× (upscale a base model pretrained on English). HyperCloning duplicates and scales linear layer weights symmetrically, preserving output distribution for a warm start. Target languages include Swedish, Icelandic, Faroese, Estonian, and Persian, adapted from a 180M parameter seed model trained on code and English. The study evaluates using Linguistic Acceptability (BLiMP-style), Information Parity (NLL ratio vs English), and Factual Knowledge (mParaRel probes) metrics.

## Key Results
- Upscaled models consistently outperform smaller models on linguistic acceptability and information parity tasks
- Direct upscaling on target language data is more compute-efficient than pre-upscaling on English for medium-to-high-resource languages
- Upscaled models preserve English capabilities better than smaller models, reducing catastrophic forgetting
- Merges of upscaled models outperform merges of smaller models, though still lag behind joint multilingual training

## Why This Works (Mechanism)

### Mechanism 1
HyperCloning provides a compute-efficient warm start by preserving functional equivalence while expanding capacity. The upscaled model begins training with retained accuracy plus additional capacity for new language acquisition, reducing catastrophic forgetting of English capabilities.

### Mechanism 2
Direct upscaling on target-language data is more compute-efficient than pre-upscaling on English for medium-to-high-resource languages. When sufficient target-language data exists, the 1× cloned setup achieves comparable performance to 2× with lower total FLOPs because it avoids redundant computation on English data the model already processes well.

### Mechanism 3
Upscaled models merge more effectively because larger capacity reduces interference between language-specific representations. Larger models have more parameters to distribute language-specific information, reducing destructive interference when averaging weights.

## Foundational Learning

- Concept: HyperCloning / Function-Preserving Model Expansion
  - Why needed here: Understanding how to upscale models without losing learned behavior is prerequisite to interpreting the paper's central methodology.
  - Quick check question: Can you explain why duplicating weights symmetrically preserves output distribution?

- Concept: Catastrophic Forgetting in Language Adaptation
  - Why needed here: The paper positions upscaling as a forgetting-mitigation strategy; understanding this tradeoff is essential.
  - Quick check question: Why might increasing model capacity reduce interference between English and target-language representations?

- Concept: Task Arithmetic and Model Merging
  - Why needed here: The merging experiments build directly on these methods; understanding task vectors is necessary to interpret why simple averaging outperforms trimming approaches.
  - Quick check question: Why might trimming methods (TIES, DARE-TIES) that work for task merging fail for language merging?

## Architecture Onboarding

- Component map:
  Seed model (180M): Code-pretrained → Code+English (80%) → Base models
  1× path: 180M → target language adaptation
  1× cloned path: 180M → HyperClone (scale 2× to 572M) → target language adaptation
  2× path: 180M → HyperClone → English (20%) → target language adaptation
  Merge layer: Linear merging / Task Arithmetic / TIES / DARE-TIES / MultiSlerp via mergekit

- Critical path: Data preparation (target language + replay) → Base model selection → Upscaling decision (1× vs 1× cloned vs 2×) → Continued pretraining → Evaluation (LA, IP, mParaRel)

- Design tradeoffs:
  1×: Lowest compute, highest English forgetting, best when English preservation not critical
  1× cloned: Compute-efficient for medium/high-resource languages, balanced forgetting
  2×: Best for low-resource languages or when English preservation is critical, but higher compute cost
  Merging: Enables modular systems but underperforms joint multilingual training; works best for ≤2 closely related languages

- Failure signatures:
  DARE-TIES merges collapse to near-random performance (0.50–0.51 on LA) for language adaptation
  Merging Estonian-Persian (different families, different scripts) shows largest degradation
  Icelandic 2× shows anomalous low scores in compute-matched comparison—suggests data-resource threshold effects

- First 3 experiments:
  1. Replicate the 1× vs 1× cloned comparison on a held-out language to validate compute-efficiency claims.
  2. Test whether increasing the density hyperparameter in TIES/DARE-TIES improves language-merging performance.
  3. Probe whether language vectors exhibit orthogonality similar to task vectors (open question noted in Section 2.3).

## Open Questions the Paper Calls Out

### Open Question 1
Does instruction-tuning facilitate the merging of upscaled language-adapted models more effectively than pretraining-only baselines? The authors note they have not run experiments with instruction-tuned models, which may be easier to merge.

### Open Question 2
Can merging methods specialized for language-level integration close the performance gap with joint multilingual training? The authors suggest potential for improvement through merging approaches specialized for language-level integration.

### Open Question 3
Does increasing the density hyperparameter in trimming-based merging methods (TIES, DARE-TIES) prevent the catastrophic loss of linguistic capabilities? The authors propose exploring tuning the density hyperparameter to higher values.

## Limitations
- Limited direct empirical support for core mechanisms; relies heavily on theoretical reasoning
- Scaling thresholds and resource dependencies not fully characterized; complex interactions between language data size and upscaling benefits
- Merging method limitations; comparison primarily against smaller models rather than sophisticated techniques or joint training
- Limited generalization beyond tested languages; 5 target languages may not represent all adaptation scenarios

## Confidence

**High confidence**: The empirical finding that upscaled models achieve higher linguistic acceptability and information parity scores than smaller models.

**Medium confidence**: The claim that direct upscaling on target language data is more compute-efficient than pre-upscaling on English for medium-to-high-resource languages.

**Low confidence**: The mechanism explanation that larger models merge more effectively due to reduced interference between language-specific representations.

## Next Checks

1. **Controlled ablation of HyperCloning parameters**: Systematically vary the scaling factor (1.5×, 2×, 3×) and measure both preservation of English capabilities and adaptation performance.

2. **Probe representational orthogonality**: Conduct singular value decomposition or centered kernel alignment analysis on language-specific weight modifications to test whether language vectors exhibit orthogonality.

3. **Cross-linguistic transfer efficiency**: Design experiments that measure how much English pre-training transfers to target languages under different upscaling strategies, quantifying the exact contribution of capacity expansion versus transfer learning.