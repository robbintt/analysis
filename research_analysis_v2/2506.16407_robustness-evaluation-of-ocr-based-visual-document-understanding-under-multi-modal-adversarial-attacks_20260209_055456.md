---
ver: rpa2
title: Robustness Evaluation of OCR-based Visual Document Understanding under Multi-Modal
  Adversarial Attacks
arxiv_id: '2506.16407'
source_url: https://arxiv.org/abs/2506.16407
tags:
- text
- layout
- attacks
- adversarial
- funsd
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "We introduce the first framework for multi-modal adversarial attacks\
  \ on OCR-based Visual Document Understanding (VDU) models. The framework enables\
  \ simultaneous perturbations of layout (bounding boxes), text, and image content\
  \ under unified budget constraints (e.g., IoU \u2265 0.6 for layout shifts)."
---

# Robustness Evaluation of OCR-based Visual Document Understanding under Multi-Modal Adversarial Attacks

## Quick Facts
- arXiv ID: 2506.16407
- Source URL: https://arxiv.org/abs/2506.16407
- Authors: Dong Nguyen Tien; Dung D. Le
- Reference count: 6
- Key outcome: Introduces first framework for multi-modal adversarial attacks on OCR-based VDU models, showing up to 29.18% F1 drop under compound perturbations

## Executive Summary
This paper presents the first comprehensive framework for multi-modal adversarial attacks on OCR-based Visual Document Understanding (VDU) models. The authors propose a unified attack framework that simultaneously perturbs layout (bounding boxes), text, and image content under spatial budget constraints. Using a trainable differentiable BBox predictor and Projected Gradient Descent (PGD) optimization, the framework achieves significantly higher attack success rates compared to random shifts, with line-level attacks and compound perturbations causing the most severe performance degradation. The work establishes new baselines for evaluating VDU robustness and reveals critical vulnerabilities in current state-of-the-art models.

## Method Summary
The framework attacks VDU models through three modalities: bounding box perturbations, text modifications, and pixel-level noise, all under unified IoU budget constraints. A 4-layer Transformer-based BBox Predictor is trained to map token embeddings to bounding box coordinates, enabling gradient-based optimization via PGD. The attack objective combines task loss minimization with IoU constraint satisfaction, using a projection step to maintain spatial plausibility. Six attack scenarios are evaluated across four datasets (FUNSD, CORD, SROIE, DocVQA) at both word and line granularity, targeting six VDU model families including LayoutLMv2/v3, ERNIE-Layout, and LayTextLLM. Line-level attacks are implemented by merging vertically aligned word boxes.

## Key Results
- PGD-based BBox attacks consistently outperform random-shift baselines across all budgets
- Line-level attacks cause greater performance degradation than word-level attacks (up to 29.18% F1 drop)
- Compound perturbations (BBox + Pixel + Text) yield the most severe performance degradation
- Layout budget constraints significantly impact attack effectiveness, with tighter budgets (Ï„=0.9) limiting success
- Adversarial attacks transfer to text-only models without visual input

## Why This Works (Mechanism)

### Mechanism 1
If layout perturbations are optimized via Projected Gradient Descent (PGD) rather than random shifting, they cause significantly higher performance degradation, even under strict spatial budgets. The framework employs a trainable, differentiable BBox Predictor to map discrete token embeddings to continuous bounding box coordinates, allowing gradients to backpropagate from task loss to input embeddings while respecting IoU constraints.

### Mechanism 2
Attacking document structure at the line-level causes greater performance collapse than word-level attacks due to disruption of semantic density. Line-level bounding boxes enclose multiple tokens, so shifting a single line-box simultaneously perturbs relative spatial relations of all contained words, creating broader misalignment signals than individual word shifts.

### Mechanism 3
Compound perturbations (Layout + Pixel + Text) exploit cross-modal alignment vulnerabilities more effectively than single-modality attacks. By perturbing all three modalities simultaneously, the attack breaks cross-modal attention alignment between visual features and text embeddings, preventing the model from correcting errors via other modalities.

## Foundational Learning

- **Projected Gradient Descent (PGD)**: Core attack algorithm that iteratively adjusts input data to maximize loss and then "projects" back into valid range (IoU budget). Quick check: If IoU budget is 0.6 and gradient step produces box with 0.4 overlap, projection fixes it by adjusting box to meet minimum 0.6 threshold.

- **Intersection over Union (IoU) as Budget**: Frames adversarial robustness under constraints rather than simply "breaking" the model. Quick check: Higher IoU threshold (0.9 vs 0.6) represents smaller allowable perturbation, making attacks more difficult.

- **Multimodal Alignment in Transformers**: VDU models use spatial coordinates to align text embeddings with visual image patches. Quick check: If text token "Date:" is embedded with coordinates (0,0) but image crop is at (100,100), cross-attention fails because spatial alignment is broken.

## Architecture Onboarding

- **Component map**: Document Image + OCR Output (BBoxes + Text) -> BBox Predictor (4-layer Transformer + 2-layer MLP) -> PGD Attack Engine -> Target VDU Model

- **Critical path**: 1) Train BBox Predictor on clean target embeddings, 2) Generate attack using PGD via predictor to create adversarial boxes, 3) Apply adversarial boxes + pixel/text perturbations to input, feed to frozen target VDU model

- **Design tradeoffs**: White-box dependency requires model embedding access; line-level attacks are more potent but may lose fine-grained spatial precision in non-aggregate metrics

- **Failure signatures**: Low predictor mIoU (<80%) causes PGD to fail generating valid adversarial examples; constraint saturation occurs if IoU budget is too tight combined with weak predictor

- **First 3 experiments**: 1) Train BBox predictor on LayoutLMv3 embeddings for FUNSD, verify mIoU exceeds ~85%, 2) Run S1 attack on FUNSD using LayoutLMv3, compare F1 drop between Random Shift and PGD, 3) Generate adversarial samples on LayoutLMv3 and test on LLaMA-3 baseline to confirm transferability

## Open Questions the Paper Calls Out

- How can the unified budget-constrained framework be adapted for OCR-free Visual Document Understanding models that lack explicit bounding box representations?
- Can the proposed multi-modal adversarial attacks remain effective in strict black-box settings where only model outputs are accessible?
- Does adversarial training using the proposed compound perturbations effectively improve VDU model robustness without degrading clean accuracy?

## Limitations

- Framework relies on trainable surrogate (BBox Predictor) whose accuracy directly determines attack effectiveness
- Compound attack scenarios assume independent and cumulative perturbation application
- Evaluation limited to four benchmark datasets that may not capture full real-world document diversity
- White-box access requirement limits practical deployment scenarios

## Confidence

**High Confidence (8-10/10):**
- PGD-based BBox attacks consistently outperform random shifts under layout budgets
- Line-level attacks cause greater performance degradation than word-level attacks
- Compound perturbations yield most severe degradation

**Medium Confidence (6-7/10):**
- Transferability to text-only models
- Specific numerical F1 drop values across all combinations
- Exact contribution of each perturbation type in compound scenarios

**Low Confidence (3-5/10):**
- Attack effectiveness on document types outside evaluated datasets
- Performance under different OCR systems or preprocessing
- Real-world deployment scenarios with mixed document types

## Next Checks

1. Systematically evaluate BBox Predictor mIoU across all dataset/model combinations and correlate with attack success rates to establish minimum predictor accuracy threshold

2. Repeat attack framework using different OCR systems (Tesseract vs. commercial APIs) to assess dependence on underlying OCR quality

3. Evaluate attacks on out-of-domain documents (medical records, scientific papers, receipts) not represented in benchmark datasets to test generalization