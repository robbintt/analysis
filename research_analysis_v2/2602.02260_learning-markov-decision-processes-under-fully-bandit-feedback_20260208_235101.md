---
ver: rpa2
title: Learning Markov Decision Processes under Fully Bandit Feedback
arxiv_id: '2602.02260'
source_url: https://arxiv.org/abs/2602.02260
tags:
- state
- policy
- algorithm
- regret
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a new bandit learning framework for episodic\
  \ Markov Decision Processes (MDPs) where the agent only receives aggregate reward\
  \ feedback at the end of each episode, without observing the trajectory or per-step\
  \ rewards. The authors develop an efficient online learning algorithm that achieves\
  \ O((Ak)^H \u221AT log T) regret, where A is the number of actions, k is the width\
  \ (number of states per stage), H is the horizon, and T is the number of episodes."
---

# Learning Markov Decision Processes under Fully Bandit Feedback

## Quick Facts
- arXiv ID: 2602.02260
- Source URL: https://arxiv.org/abs/2602.02260
- Reference count: 15
- Primary result: Achieves O((Ak)^H √T log T) regret under fully bandit feedback, with exponential dependence on horizon shown to be necessary

## Executive Summary
This paper tackles the challenging problem of learning in episodic Markov Decision Processes (MDPs) where the agent only receives aggregate reward feedback at the end of each episode, without observing the trajectory or per-step rewards. The authors develop an efficient online learning algorithm that achieves O((Ak)^H √T log T) regret, where A is the number of actions, k is the width (number of states per stage), H is the horizon, and T is the number of episodes. They prove this exponential dependence on H is necessary through a matching lower bound. For a special class of "ordered" MDPs that arise in stochastic optimization problems like k-item prophet inequality and sequential posted pricing, they provide an improved algorithm achieving O((2eAH^2/k)^k √k^3H/A T log T) regret, which is near-optimal.

## Method Summary
The paper introduces two algorithms: ExpRef for general MDPs and OrderedExpRef for ordered MDPs. Both use a successive-elimination framework that maintains active sets of actions at each state and refines them based on empirical maximizers. The key technical insight is using backward induction with value normalization to isolate the contribution of single actions to aggregate rewards, combined with exponential threshold elimination to manage compounding errors across stages. The algorithms use a doubling trick with ε-halving to progressively refine the active sets while ensuring sufficient statistical coverage through randomized exploration.

## Key Results
- General MDPs: O((Ak)^H √T log T) regret upper bound, with exponential dependence on horizon H shown to be necessary
- Ordered MDPs: Improved O((2eAH^2/k)^k √k^3H/A T log T) regret bound for stochastic optimization problems
- Empirical validation on k-item prophet inequality shows performance comparable to UCB-VI despite significantly less feedback
- Theoretical analysis proves the exponential regret dependency is fundamental to fully bandit feedback

## Why This Works (Mechanism)

### Mechanism 1: Statistical Coverage via Uniform Exploration
The algorithm ensures sufficient statistical "coverage" of all state-action pairs through randomized exploration. In each learning phase, the agent samples actions uniformly from the current "active set" for all stages preceding the one being optimized. This randomized policy guarantees that the probability of visiting any specific state is non-zero and bounded below by 1/A, allowing the algorithm to statistically attribute a portion of the aggregate reward to the specific action being tested.

### Mechanism 2: Backward Induction with Value Normalization
The algorithm proceeds backwards from the horizon to stage 1, fixing the policy for future stages to the currently known "empirical best" tail policy. It then plays different actions at the current stage and relates the observed aggregate reward to the local value function using the visitation probability. This decouples the optimization problem into H local estimation problems, isolating the contribution of single actions.

### Mechanism 3: Exponential Threshold Elimination
The algorithm manages compounding error across stages by using elimination thresholds that grow exponentially as it moves backward from the horizon. When eliminating suboptimal actions, the algorithm uses a threshold C_{l,i} · ε, where C_{l,i} ≈ (Ak)^{H-i}. This specific exponential growth is necessary because an error in an early stage affects all subsequent transitions, and the paper proves this exponential dependence is a necessary condition.

## Foundational Learning

- **Concept: Markov Decision Processes (MDPs) & Bellman Equations**
  - Why needed: The framework relies on the episodic MDP structure and recursive value function decomposition
  - Quick check: Why does the value of a state at stage i depend on the policy used at stage i+1?

- **Concept: Multi-Armed Bandits (MAB) & Exploration-Exploitation**
  - Why needed: This is a bandit problem over a massive space of policies, requiring balancing exploration and exploitation
  - Quick check: Why does "fully bandit" feedback make the exploration problem exponentially harder than "semi-bandit" feedback?

- **Concept: Concentration Inequalities (Hoeffding's Inequality)**
  - Why needed: The algorithm relies on empirical averages being close to true means for confidence intervals
  - Quick check: As the number of samples N increases, how does the confidence interval width scale?

## Architecture Onboarding

- **Component map:** ActiveSetManager -> ExplorationPolicy -> ExpRef -> DoublingDriver
- **Critical path:**
  1. Initialize: Set active sets to all actions
  2. Loop (Doubling): Set target accuracy ε
  3. Explore: Run ExpRef to sample trajectories and collect aggregate rewards
  4. Eliminate: Update active sets by discarding actions outside exponential thresholds
  5. Commit: Use refined policy for next doubling phase

- **Design tradeoffs:**
  - Feedback Granularity vs. Regret: Trades rich feedback for exponential regret dependency on horizon H
  - Computation vs. Sample Efficiency: Computationally cheap but sample inefficient due to uniform exploration

- **Failure signatures:**
  - Active Set Collapse: Empty set returned for reachable state (thresholds too strict)
  - Regret Stagnation: Linear regret (optimal action eliminated early)
  - Memory Overflow: Storing values for A^(Hk) policies instead of structured decomposition

- **First 3 experiments:**
  1. Validate on prophet inequality - replicate Figure 2 results comparing ExpRef to UCB-VI
  2. Horizon Sensitivity - run on synthetic MDPs with increasing H to confirm exponential regret scaling
  3. Ordered vs. General MDP - compare ExpRef and OrderedExpRef on stochastic knapsack problems

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the gap between upper bound O((Ak)^H √T) and lower bound Ω(A^(H/2) √T) for general MDPs be closed?
- Basis: Theorem 3.1 gives upper bound and Theorem 4.1 gives lower bound, leaving an exponential factor gap
- Why unresolved: The successive-elimination approach may be overly conservative, requiring new analysis techniques
- What evidence would resolve it: Improved algorithm achieving O(A^(cH) √T) with c < 1, or stronger lower bound with c' > 1/2

### Open Question 2
- Question: Are there natural intermediate feedback models between trajectory and fully bandit feedback with interpolating regret bounds?
- Basis: Paper positions fully bandit as "far more restrictive" but doesn't explore intermediate granularities
- Why unresolved: Feedback spectrum is discrete; partial trajectory observability remains unexplored
- What evidence would resolve it: Analysis of feedback models where agents observe subsets of state-action pairs

### Open Question 3
- Question: Can ordered MDP structural assumptions be relaxed while retaining improved regret bounds?
- Basis: Algorithm exploits specific monotonicity assumptions for O((2eAH^2/k)^k √T) regret
- Why unresolved: Real-world problems may only approximately satisfy assumptions
- What evidence would resolve it: Algorithms with provable guarantees for approximately monotone MDPs

## Limitations
- Exponential regret dependency on horizon length H is fundamental and may make the algorithm impractical for long horizons
- Performance relies heavily on "one-step regret" decomposition which may not extend to non-Markovian rewards
- Ordered MDP assumptions are specific to stochastic optimization and may not generalize broadly

## Confidence
- **High confidence** in core algorithmic framework and backward induction mechanism (rigorous theoretical analysis with matching lower bounds)
- **Medium confidence** in practical performance claims (limited empirical validation on synthetic problems only)
- **Low confidence** in generality of ordered MDP assumptions (applicability to real-world problems unclear)

## Next Checks
1. Test robustness to noise and variance by evaluating on MDPs with varying reward noise levels and transition stochasticity
2. Evaluate scalability beyond small horizons by testing performance on MDPs with H > 20 to empirically confirm exponential regret scaling
3. Implement and test on stochastic shortest path problems and other structured MDPs to assess whether ordered MDP assumptions can be relaxed