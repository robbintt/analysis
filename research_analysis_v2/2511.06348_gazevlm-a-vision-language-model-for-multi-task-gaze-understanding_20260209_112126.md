---
ver: rpa2
title: 'GazeVLM: A Vision-Language Model for Multi-Task Gaze Understanding'
arxiv_id: '2511.06348'
source_url: https://arxiv.org/abs/2511.06348
tags:
- gaze
- gazevlm
- vision
- object
- depth
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GazeVLM introduces the first Vision-Language Model for unified
  multi-task gaze understanding, integrating person detection, gaze target detection,
  and gaze object identification. By leveraging both RGB and depth modalities (encoded
  as HHA), and employing a cross-attention fusion strategy, GazeVLM achieves state-of-the-art
  performance on standard benchmarks.
---

# GazeVLM: A Vision-Language Model for Multi-Task Gaze Understanding

## Quick Facts
- **arXiv ID:** 2511.06348
- **Source URL:** https://arxiv.org/abs/2511.06348
- **Reference count:** 40
- **Primary result:** First unified Vision-Language Model for multi-task gaze understanding achieving SOTA performance on standard benchmarks.

## Executive Summary
GazeVLM introduces the first Vision-Language Model for unified multi-task gaze understanding, integrating person detection, gaze target detection, and gaze object identification. By leveraging both RGB and depth modalities (encoded as HHA), and employing a cross-attention fusion strategy, GazeVLM achieves state-of-the-art performance on standard benchmarks. On GazeFollow and VideoAttentionTarget datasets, it demonstrates significant improvements in accuracy metrics such as AUC, Dist., Angle, and AP, while also introducing a novel object-level gaze detection metric (APob). The model's ability to generalize across tasks within a single architecture offers a robust and efficient solution for real-world gaze analytics.

## Method Summary
GazeVLM fine-tunes Qwen2-VL-2B with RGB and HHA-encoded depth as dual visual inputs. The model processes each modality independently through the frozen vision encoder, concatenates their embeddings, and applies cross-attention with text embeddings for task-specific fusion. HHA encoding transforms monocular depth into three channels (inverse depth, height, angle) normalized to [0, 255] for compatibility with pretrained vision encoders. The architecture uses structured text prompts with special tokens to format outputs for person detection, gaze target localization, and object identification tasks. Training employs AdamW optimizer (learning rate 1e-5) for 20 epochs, minimizing NLL loss on tokenized output sequences.

## Key Results
- **SOTA Performance:** GazeVLM achieves 0.929 AUC on GazeFollow, outperforming previous methods by significant margins.
- **Multi-Modal Advantage:** RGB+HHA encoding surpasses RGB-only and RGB+raw depth baselines, with HHA providing geometric cues complementary to appearance features.
- **Unified Architecture Success:** Single VLM handles all three gaze understanding tasks without auxiliary networks, preventing error cascades while maintaining competitive accuracy.

## Why This Works (Mechanism)

### Mechanism 1: HHA Encoding for Frozen Vision Encoder Compatibility
Converting raw depth maps to HHA encodings enables effective depth integration without vision encoder pretraining on depth data. HHA transforms single-channel depth into a three-channel RGB-like representation (inverse depth, vertical position, surface angle), exploiting structural similarities with natural images to leverage pretrained visual features.

### Mechanism 2: Cross-Attention Fusion with Text-Guided Query Selection
Cross-attention with text-derived queries and concatenated visual keys/values enables task-specific modality weighting. Text embeddings generate queries (Q) while concatenated RGB and HHA features provide keys (K) and values (V), allowing the model to dynamically attend to appearance cues (RGB) or geometric cues (HHA) based on task context from the prompt.

### Mechanism 3: Unified VLM Architecture for Error Cascade Prevention
Single unified VLM with task-specific prompts prevents cascading errors across person detection, gaze localization, and object identification. Shared representations across tasks enable end-to-end learning; text prompts specify task mode, and outputs are generated directly without sequential dependencies on separate models.

## Foundational Learning

- **Concept: HHA (Horizontal Disparity, Height, Angle) Encoding**
  - Why needed here: Critical for integrating depth into RGB-pretrained vision encoders without additional pretraining; raw depth degrades performance.
  - Quick check question: Why does inverse depth (disparity) provide better encoder compatibility than linear depth values?

- **Concept: Cross-Attention vs. Self-Attention**
  - Why needed here: Core fusion mechanism; understanding query/key/value source separation is essential for debugging modality integration.
  - Quick check question: In cross-attention, what happens if text queries (Q) fail to align with visual keys (K)?

- **Concept: Bounding Box Normalization to [0, 1000)**
  - Why needed here: GazeVLM represents spatial coordinates as normalized strings for VLM token compatibility; misunderstanding this causes coordinate decoding failures.
  - Quick check question: How does the λ margin in Equation 4 affect gaze point to bounding box conversion for object matching?

## Architecture Onboarding

- **Component map:**
  - **Vision Tower** (Qwen2-VL-2B encoder): Processes RGB and HHA images independently, outputs high-dimensional embeddings
  - **Cross-Attention Fusion**: Concatenates RGB+HHA features as K,V; text embeddings provide Q
  - **Text Decoder**: Generates task-specific outputs (bounding boxes, gaze points, class labels) in string format
  - **Preprocessing Pipeline**: Depth → HHA encoding (Eq. 8-15); annotations → ChatML format with special tokens

- **Critical path:**
  Input (RGB + depth) → HHA encoding → Dual vision tower pass → Feature concatenation → Cross-attention with text → Decoder → Parse output string → Extract coordinates/labels

- **Design tradeoffs:**
  1. **Frozen vision encoder + HHA** vs. **fine-tuned encoder + raw depth**: Paper chooses frozen for efficiency; HHA compensates for lack of depth pretraining
  2. **Unified multi-task model** vs. **specialized single-task models**: Unified reduces deployment complexity but risks negative transfer
  3. **Detic object detector for labels** (1200 classes) vs. **closed-vocabulary detector**: Open vocabulary improves coverage but introduces detection noise

- **Failure signatures:**
  1. **Depth integration failure**: Using raw depth instead of HHA causes AUC drop (0.902 → 0.897); verify HHA preprocessing pipeline
  2. **Coordinate parsing errors**: Bounding box tokens (`<box start>`, `<box end>`) must match training format exactly
  3. **Out-of-frame misclassification**: Model outputs "looking out of the image" text tag; verify AP metric handles this correctly
  4. **Object identification failure**: Gaze-to-object IoU matching fails if λ margin is too small or object detector misses targets

- **First 3 experiments:**
  1. **Modality ablation**: Train RGB-only, RGB+depth, RGB+HHA variants; confirm HHA advantage on validation split
  2. **Task isolation test**: Train single-task models separately vs. unified; measure mAP difference to detect negative transfer
  3. **Cross-dataset generalization**: Train on GazeFollow only, test on VideoAttentionTarget; assess domain gap and out-of-frame handling

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can GazeVLM be extended to video-based analysis to effectively capture temporal dynamics in gaze behavior?
- **Basis in paper:** [explicit] The authors state, "Currently, GazeVLM supports only static image-based analysis... Further work will extend GazeVLM to videos by incorporating visual prompts to focus on regions of interest across frames."
- **Why unresolved:** The current architecture processes frames individually and lacks mechanisms (like temporal attention or memory banks) to track gaze continuity or shifts across time.
- **What evidence would resolve it:** A modified GazeVLM architecture evaluated on video benchmarks, demonstrating improved temporal consistency and gaze prediction accuracy compared to frame-by-frame baselines.

### Open Question 2
- **Question:** What specific optimization mechanisms can successfully adapt GazeVLM for real-time applications without significantly degrading accuracy?
- **Basis in paper:** [explicit] The authors identify a goal to "improve the computational efficiency of GazeVLM for real-time applications" and plan to "explore optimization mechanisms."
- **Why unresolved:** Vision-Language Models are generally computationally heavy; the paper does not explore quantization, pruning, or distillation techniques required for low-latency deployment.
- **What evidence would resolve it:** Benchmarks showing inference speed (FPS) and latency improvements on standard hardware (e.g., consumer GPUs) while maintaining SOTA AUC and Distance metrics.

### Open Question 3
- **Question:** Would pretraining the vision encoder on raw depth maps provide superior performance gains compared to the current HHA encoding strategy?
- **Basis in paper:** [inferred] The paper notes that "Pretraining the vision encoder with depth maps could allow the direct use of raw depth maps for better accuracy," but relies on HHA encoding to avoid this costly pretraining step.
- **Why unresolved:** It is unclear if the HHA representation—which transforms depth to align with RGB-pretrained weights—is an optimal geometric proxy or merely a convenient approximation.
- **What evidence would resolve it:** An ablation study comparing the current frozen RGB-encoder + HHA setup against a model where the vision encoder is explicitly pretrained on raw depth data.

## Limitations

- **Reproducibility Gaps:** Critical implementation details missing, including specific depth estimation model weights, cross-attention integration code, and λ margin value for gaze bounding box creation.
- **Evaluation Scope:** APob metric for object-level gaze detection only validated on VideoAttentionTarget; cross-dataset generalization untested.
- **Architectural Constraints:** Frozen vision encoder assumption limits adaptability to datasets with different depth characteristics.

## Confidence

**High Confidence Claims:**
- The HHA encoding mechanism provides measurable benefits over raw depth when using frozen RGB-pretrained encoders. The ablation study results (AUC 0.929 vs 0.897) are directly reported and internally consistent.

**Medium Confidence Claims:**
- The cross-attention fusion strategy with text-guided query selection effectively integrates RGB and HHA modalities. While the mechanism is theoretically sound and the formulation is explicit, the corpus lacks direct evidence for this specific implementation pattern.
- The unified VLM architecture prevents error cascades across gaze understanding tasks. This claim is supported by comparative analysis with Gaze-LLE but lacks ablation studies isolating the cascade prevention effect.

**Low Confidence Claims:**
- The model's generalization across all three gaze understanding tasks within a single architecture without negative transfer. The paper reports SOTA performance but doesn't provide single-task baselines for comparison.
- The choice of 1200-class Detic object detector optimally balances coverage and noise. No ablation study explores different object detection vocabularies or architectures.

## Next Checks

1. **Modality Ablation Study:** Systematically compare RGB-only, RGB+raw depth, and RGB+HHA variants on the validation split to confirm HHA encoding benefits are consistent across different depth estimation models and input resolutions.

2. **Task Isolation Analysis:** Train and evaluate single-task variants of GazeVLM (person detection only, gaze target detection only, object identification only) to quantify negative transfer effects and identify tasks that benefit most from joint optimization.

3. **Cross-Dataset Generalization Test:** Train on GazeFollow only and evaluate on VideoAttentionTarget to measure domain adaptation performance and identify failure modes specific to out-of-frame gaze detection and object identification tasks.