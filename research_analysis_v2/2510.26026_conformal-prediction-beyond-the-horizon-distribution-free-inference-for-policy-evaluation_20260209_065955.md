---
ver: rpa2
title: 'Conformal Prediction Beyond the Horizon: Distribution-Free Inference for Policy
  Evaluation'
arxiv_id: '2510.26026'
source_url: https://arxiv.org/abs/2510.26026
tags:
- stest
- distribution
- edcal
- policy
- coverage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a distribution-free conformal prediction framework
  for infinite-horizon policy evaluation in reinforcement learning. The key innovation
  is constructing pseudo-returns by combining truncated rollouts with tail sampling
  from learned return distributions, which addresses the challenge of unobserved returns
  in infinite-horizon settings.
---

# Conformal Prediction Beyond the Horizon: Distribution-Free Inference for Policy Evaluation

## Quick Facts
- arXiv ID: 2510.26026
- Source URL: https://arxiv.org/abs/2510.26026
- Reference count: 40
- Primary result: Distribution-free conformal prediction framework for infinite-horizon policy evaluation using pseudo-returns with experience replay and weighted subsampling

## Executive Summary
This paper addresses the challenge of constructing valid prediction intervals for infinite-horizon returns in reinforcement learning, where the true return is never observed. The proposed framework constructs pseudo-returns by combining truncated rollouts with tail sampling from learned return distributions, enabling conformal prediction despite the unobserved target. The method employs experience replay and weighted subsampling to restore exchangeability and handle distribution shifts between behavior and target policies. Theoretical analysis establishes asymptotic coverage guarantees, and empirical results demonstrate significant improvements over distributional RL baselines in synthetic environments and Mountain Car.

## Method Summary
The method trains a distributional RL agent on a training dataset, then constructs k-step pseudo-returns by summing observed rewards over k steps and bootstrapping the remaining tail via a single sample from the learned return distribution. These pseudo-returns serve as proxies for the true unobserved returns during calibration. The framework uses an experience replay buffer to store transitions and decorrelate them via random subsampling, addressing the temporal dependence inherent in RL trajectories. For off-policy evaluation, weighted subsampling based on estimated importance weights corrects for distribution shifts between behavior and target policies. Nonconformity scores are computed as absolute deviations between pseudo-returns and predicted values, and prediction intervals are aggregated across multiple resampling iterations.

## Key Results
- Achieves 90% coverage in Example 1 (two-state MDP) with k=3 and weighted subsampling, outperforming DRL-QR baseline
- Maintains valid coverage in Example 2 (continuous state space) across varying truncation widths k=1-5
- Demonstrates improved coverage and reliability in Mountain Car benchmark under policy shifts compared to standard distributional RL approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Enables CP for infinite-horizon returns by constructing observable pseudo-returns
- **Mechanism:** Combines k-step observed rewards with tail sampling from learned return distribution to create observable proxies for unobserved targets
- **Core assumption:** Learned DRL model provides reasonable tail approximation with bounded Wasserstein distance
- **Break condition:** Severe model misspecification causing bootstrapped tails to diverge significantly from reality

### Mechanism 2
- **Claim:** Restores exchangeability required for valid CP despite temporal dependence
- **Mechanism:** Uses experience replay buffer with random subsampling to decorrelate sequential transitions
- **Core assumption:** Random subsampling sufficiently breaks temporal links between calibration data points
- **Break condition:** Rapidly shifting non-stationary environments or non-ergodic state distributions

### Mechanism 3
- **Claim:** Handles distribution shifts without high-variance weighted quantile calculation
- **Mechanism:** Employs weighted subsampling with importance weights instead of full trajectory weighting
- **Core assumption:** Accurate enough importance weight estimation to guide subsampling
- **Break condition:** High variance in weight estimation causing effective sample size collapse

## Foundational Learning

- **Concept: Distributional RL (DRL) / Quantile Regression**
  - **Why needed here:** Standard RL predicts mean return, but this method needs full return distribution to sample tails for pseudo-returns
  - **Quick check question:** Can you explain why predicting the 0.9-quantile of a return differs from predicting the mean return?

- **Concept: Exchangeability in Conformal Prediction**
  - **Why needed here:** CP coverage guarantees mathematically break if calibration and test data aren't exchangeable
  - **Quick check question:** If we sorted calibration data by time before splitting, would CP coverage guarantees still hold theoretically?

- **Concept: Importance Sampling (Off-Policy Correction)**
  - **Why needed here:** Corrects for behavior policy πb ≠ target policy π using density ratios to reweight data
  - **Quick check question:** Why does importance weight variance typically explode as horizon length increases?

## Architecture Onboarding

- **Component map:** Training Phase (DRL Agent) -> Pseudo-Return Engine (observed rewards + tail sampling) -> Replay Buffer (decorrelation) -> Calibrator (importance weights + weighted subsampling) -> Aggregator (B resampling iterations)

- **Critical path:** Accuracy of DRL model determines pseudo-return bias, which directly impacts final interval validity. Model bias term in Theorem 1 dominates if DRL is poorly trained.

- **Design tradeoffs:** 
  - Step width k: Larger k uses more observed data (reduces model bias) but makes importance weight estimation harder (increases variance)
  - Subsampling size l: Smaller samples increase interval width variance; larger samples may dilute weighted correction

- **Failure signatures:**
  - Consistent undercoverage: DRL model bias too large or importance weights mis-estimated
  - Explosive interval widths: High variance in importance weights (effective sample size collapse)

- **First 3 experiments:**
  1. Run method without replay buffer subsampling on simple MDP (Example 1) to verify coverage drops without exchangeability fix
  2. Sweep k=1…5 in Example 2 to plot coverage vs. interval width visualizing bias-variance trade-off
  3. Introduce large distribution shift in Mountain Car and compare weighted subsampling vs. unweighted calibration

## Open Questions the Paper Calls Out

- **Open Question 1:** How can truncation horizon k be optimally selected to balance model bias and importance weight variance?
  - **Basis:** Conclusion states choice of k "remains underexplored"
  - **Evidence:** An adaptive algorithm selecting k based on estimated density ratios or Wasserstein distances

- **Open Question 2:** Can unified prediction regions be constructed by aggregating intervals from multiple k values?
  - **Basis:** Conclusion suggests using Cauchy Combination Test for correlated p-values
  - **Evidence:** Empirical results showing combined regions maintain nominal coverage and efficiency

- **Open Question 3:** How can this framework extend to policy optimization rather than just evaluation?
  - **Basis:** Conclusion identifies "extending framework to policy optimization" as future work
  - **Evidence:** Policy gradient algorithm integrating conformal intervals for risk-aware policies

## Limitations

- Fundamental tension between distribution-free CP guarantees and model-dependent pseudo-return construction
- Effectiveness of exchangeability restoration and weighted subsampling relies on assumptions about ergodicity and stable importance weight estimation
- Limited empirical validation to two synthetic examples and one benchmark, raising questions about scalability

## Confidence

- **High:** Theoretical framework and core mechanisms (pseudo-return construction, exchangeability restoration, weighted subsampling)
- **Medium:** Empirical results given limited environment diversity and critical dependence on DRL model quality
- **Low:** Scalability claims without validation on higher-dimensional state spaces

## Next Checks

1. **Sensitivity analysis of truncation parameter k**: Systematically vary k across range 1-10 in Example 2, measuring trade-off between coverage stability and interval width to validate Theorem 1's bias-variance prediction

2. **Importance weight variance stress test**: Create extreme policy shifts to measure how weight variance affects effective sample size and coverage, comparing weighted subsampling against direct weighted conformal prediction baselines

3. **Cross-environment robustness evaluation**: Implement framework on additional continuous control benchmarks (Pendulum, Acrobot) with varying state dimensions to assess scalability and robustness beyond current proof-of-concept environments