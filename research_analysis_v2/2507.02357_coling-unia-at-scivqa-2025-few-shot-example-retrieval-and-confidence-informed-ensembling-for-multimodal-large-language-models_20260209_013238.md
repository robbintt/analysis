---
ver: rpa2
title: 'Coling-UniA at SciVQA 2025: Few-Shot Example Retrieval and Confidence-Informed
  Ensembling for Multimodal Large Language Models'
arxiv_id: '2507.02357'
source_url: https://arxiv.org/abs/2507.02357
tags:
- internvl
- question
- pixtral
- figure
- type
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper describes a system for the SciVQA 2025 Shared Task on
  Scientific Visual Question Answering. The approach uses an ensemble of two Multimodal
  Large Language Models (MLLMs) with few-shot example retrieval strategies.
---

# Coling-UniA at SciVQA 2025: Few-Shot Example Retrieval and Confidence-Informed Ensembling for Multimodal Large Language Models

## Quick Facts
- **arXiv ID:** 2507.02357
- **Source URL:** https://arxiv.org/abs/2507.02357
- **Reference count:** 10
- **Key result:** Third place in SciVQA 2025 Shared Task with 85.12 average F1 score across ROUGE-1, ROUGE-L, and BERTScore

## Executive Summary
This paper describes a system for the SciVQA 2025 Shared Task on Scientific Visual Question Answering. The approach uses an ensemble of two Multimodal Large Language Models (MLLMs) with few-shot example retrieval strategies. Few-shot examples are selected based on question or question-and-image similarity, and model configuration is chosen based on question type. The system also uses confidence scores to select high-confidence answers. On the blind test data, the system ranks third out of seven submissions with an average F1 score of 85.12 across ROUGE-1, ROUGE-L, and BERTScore. The approach demonstrates that MLLMs can effectively answer questions about scientific figures, though performance varies by question type.

## Method Summary
The system employs an ensemble of InternVL3-78B and Pixtral-Large-Instruct-2411 models with confidence-informed answer selection and few-shot example retrieval. Stage 1 uses InternVL3-78B with BLIP-2 few-shot examples, accepting answers with confidence ≥0.9. Low-confidence instances are routed to Stage 2, where model choice depends on question type: binary questions use Pixtral with SBERT-retrieved examples, infinite-answer questions use Pixtral with CLIP-retrieved examples, and other questions use InternVL with SBERT-retrieved examples. Few-shot examples are filtered by matching figure type and exclude instances from the same image.

## Key Results
- Third place out of seven submissions in SciVQA 2025 Shared Task
- Average F1 score of 85.12 across ROUGE-1, ROUGE-L, and BERTScore on blind test data
- Confidence-informed routing accepts ~50% of answers from InternVL3-78B directly
- Performance varies significantly by question type, with finite answer sets performing better than infinite ones

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Confidence-informed answer selection enables reliable routing between a high-precision first-pass model and specialized fallback configurations.
- **Mechanism:** The system first accepts all predictions from InternVL3-78B (1s_q_img_f, BLIP-2) with confidence ≥0.9, which covers ~50% of instances. For remaining low-confidence cases, it routes to model configurations optimized per question type. Confidence is approximated by exponentiating the mean log-probability of generated tokens.
- **Core assumption:** Model confidence scores are calibrated such that high confidence correlates with empirical accuracy.
- **Evidence anchors:**
  - [abstract] "We also select answers based on the models' confidence levels."
  - [Section 3.4] "InternVL3-78B (1s_q_img_f, BLIP2) with examples derived from BLIP-2... is meaningfully calibrated. This means that high confidence scores indicate highly likely correct instances."
  - [corpus] Weak direct support; corpus papers focus on ensembling and retrieval but not confidence calibration specifically.
- **Break condition:** If the calibration curve degrades (high-confidence predictions become unreliable), the routing advantage collapses and performance may drop below a single-model baseline.

### Mechanism 2
- **Claim:** Few-shot example retrieval improves performance by providing in-context demonstrations, with 2-shot (one answerable, one unanswerable) being particularly effective for distinguishing question types.
- **Mechanism:** Examples are retrieved from training data using either (1) question similarity via SBERT embeddings or (2) question-image similarity via CLIP or BLIP-2 embeddings. Retrieval can be filtered to same figure type or searched across the entire training set.
- **Core assumption:** Retrieved examples share sufficient task-relevant structure with the input to guide model behavior.
- **Evidence anchors:**
  - [abstract] "Our system employs... various few-shot example retrieval strategies."
  - [Section 3.1] "Searching for examples using only question similarity leads to matching the input instance's question type far more often than searching using image and question similarity."
  - [Section 3.2] "Adding few-shot examples generally improves performance... using one answerable and one unanswerable example helps the model to distinguish between these two types of instances."
  - [corpus] "Leveraging Information Retrieval to Enhance Spoken Language Understanding Prompts in Few-Shot Learning" supports retrieval-enhanced prompting.
- **Break condition:** If retrieved examples are semantically dissimilar or introduce conflicting signals, performance may degrade versus zero-shot.

### Mechanism 3
- **Claim:** Question-type and figure-type specific model selection captures systematic performance variations across the problem space.
- **Mechanism:** Cross-validation on the development set identifies optimal (model, few-shot strategy) pairs for each question type / figure type combination. Binary questions use Pixtral with 2-shot question similarity; infinite-answer questions use Pixtral with 2-shot question-image similarity; others use InternVL with 1-shot question similarity.
- **Core assumption:** Performance patterns observed on development data generalize to test distribution.
- **Evidence anchors:**
  - [Section 3.3] "Performance varies greatly by question type... Results on finite answer sets are considerably better than on infinite ones."
  - [Figure 3/4] Show substantial performance variance across question types and figure types.
  - [corpus] "Which Prompting Technique Should I Use?" suggests task-specific prompting selection, indirectly supporting adaptive configuration.
- **Break condition:** If test distribution shifts significantly from development data, type-specific configurations may become suboptimal.

## Foundational Learning

- **Concept: MLLM confidence calibration**
  - **Why needed here:** The core routing mechanism assumes confidence scores reflect correctness probability. Without understanding calibration (reliability diagrams, expected calibration error), you cannot diagnose routing failures.
  - **Quick check question:** If a model assigns 0.9 confidence to 100 predictions but only 60 are correct, is it well-calibrated?

- **Concept: In-context learning with retrieved examples**
  - **Why needed here:** The system relies on few-shot examples to steer model behavior without fine-tuning. Understanding how similarity metrics (cosine on embeddings) relate to example usefulness is critical.
  - **Quick check question:** Why might SBERT question similarity outperform CLIP image-question similarity for retrieving examples of the same question type?

- **Concept: Ensemble diversity and specialization**
  - **Why needed here:** The system combines models (InternVL, Pixtral) with different strengths. Effective ensembling requires understanding when models disagree and why.
  - **Quick check question:** Two models achieve 75% accuracy individually. What conditions must hold for their ensemble to exceed 75%?

## Architecture Onboarding

- **Component map:** Input (image + question + metadata) → [Stage 1] InternVL3-78B (1s_q_img_f, BLIP-2) → Confidence ≥ 0.9? → Accept answer → Confidence < 0.9? → Route to Stage 2 → [Stage 2] Question-type router → Binary → Pixtral (2s_q_f, SBERT) → Infinite answer → Pixtral (2s_q_img_f, CLIP) → Other → InternVL (1s_q_f, SBERT) → Output (answer)

- **Critical path:** Confidence estimation → calibration validation → routing logic → fallback model inference. Errors in confidence estimation propagate to all downstream decisions.

- **Design tradeoffs:**
  - Filtering by figure type reduces candidate pool but may miss useful cross-type examples.
  - 2-shot improves unanswerable detection but requires context window capacity (InternVL cannot fit 2-shot).
  - BLIP-2 embeddings emphasize image content, causing ties; SBERT embeddings emphasize question semantics.

- **Failure signatures:**
  - Low acceptance rate in Stage 1 suggests miscalibrated confidence threshold.
  - Poor performance on infinite-answer visual questions indicates fundamental limitation (acknowledged in paper).
  - High variance across figure types suggests insufficient specialization or data imbalance.

- **First 3 experiments:**
  1. **Calibration audit:** Plot confidence vs. accuracy for InternVL3-78B on a held-out split; verify the 0.9 threshold is justified.
  2. **Ablate retrieval method:** Compare SBERT-only vs. CLIP vs. BLIP-2 retrieval on per-question-type performance to confirm routing choices.
  3. **Stress test unanswerable detection:** Evaluate precision/recall on unanswerable questions specifically, as the paper notes this is where few-shot helps most but also notes dataset artifacts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can MLLM architectures be refined to better handle open-ended questions requiring visual feature extraction from scientific figures?
- Basis in paper: [explicit] The authors state that "answering infinite answer set questions about visual features of images remains challenging, highlighting the need for a more sophisticated approach."
- Why unresolved: Current few-shot and ensemble strategies successfully handle binary and multiple-choice questions but fail to close the performance gap for infinite visual queries.
- What evidence would resolve it: A model or prompt strategy that significantly raises the ROUGE-1 F1 score for the "inf (v)" question type to match the levels of finite answer set questions.

### Open Question 2
- Question: Does the high accuracy on "unanswerable" questions generalize to real-world scenarios where unanswerability arises from factors other than missing external references?
- Basis in paper: [explicit] The authors note the dataset's unanswerable questions "follow a different pattern," often referring to unavailable material rather than visual features, making real-world performance "difficult to determine."
- Why unresolved: The models may be learning to identify the specific artifact of the dataset's unanswerable style (off-topic queries) rather than genuinely reasoning about visual insufficiency.
- What evidence would resolve it: Evaluation on a counterfactual benchmark where unanswerable questions are derived from visual contradictions within the image rather than references to external text.

### Open Question 3
- Question: Why does the strict alignment of question types between the input and retrieved few-shot examples fail to yield significant performance improvements?
- Basis in paper: [inferred] The authors observe that while SBERT-based retrieval matches question types more often than CLIP-based retrieval, this higher alignment "does not seem to make a marked difference in overall performance."
- Why unresolved: This suggests the in-context learning mechanism may rely more on general visual or semantic similarity than on the structural logic of the question type.
- What evidence would resolve it: An ablation study decoupling "question type matching" from "semantic similarity" to quantify their independent contributions to model accuracy.

## Limitations
- The paper does not report confidence calibration metrics to verify that the 0.9 threshold is statistically justified rather than empirically convenient.
- The few-shot retrieval process lacks detail on prompt formatting, making exact reproduction challenging despite provided code.
- Performance on infinite-answer visual questions is acknowledged as poor, yet the paper does not explore whether this reflects fundamental limitations of current MLLMs or specific architectural choices.

## Confidence
- **High confidence:** The ensemble approach works better than single models (supported by test ranking and stated F1 scores).
- **Medium confidence:** The confidence-based routing mechanism is effective (supported by description but lacks calibration validation).
- **Medium confidence:** Few-shot retrieval improves performance (supported by stated improvements but lacks ablation studies on retrieval method impact).

## Next Checks
1. **Calibration audit:** Plot confidence vs. accuracy for InternVL3-78B on a held-out split; verify the 0.9 threshold is justified rather than arbitrarily chosen.
2. **Retrieval method ablation:** Compare SBERT-only vs. CLIP vs. BLIP-2 retrieval performance on each question type to validate the routing choices made in the system.
3. **Unanswerable detection stress test:** Evaluate precision/recall specifically on unanswerable questions, as this is where few-shot is claimed to help most but dataset artifacts may confound results.