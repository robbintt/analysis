---
ver: rpa2
title: Leveraging Large Language Models for Rare Disease Named Entity Recognition
arxiv_id: '2508.09323'
source_url: https://arxiv.org/abs/2508.09323
tags:
- disease
- entity
- rare
- shot
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates GPT-4o for rare disease named entity recognition
  (NER) in low-resource settings. The authors compare zero-shot, few-shot, fine-tuning,
  and retrieval-augmented generation (RAG) approaches against a BioClinicalBERT baseline
  on the RareDis Corpus.
---

# Leveraging Large Language Models for Rare Disease Named Entity Recognition

## Quick Facts
- arXiv ID: 2508.09323
- Source URL: https://arxiv.org/abs/2508.09323
- Authors: Nan Miles Xi; Yu Deng; Lin Wang
- Reference count: 0
- Key outcome: Prompt-optimized LLMs achieve F1 >0.83 on rare disease NER in low-resource settings

## Executive Summary
This paper evaluates GPT-4o for rare disease named entity recognition in low-resource settings. The authors compare zero-shot, few-shot, fine-tuning, and retrieval-augmented generation approaches against a BioClinicalBERT baseline on the RareDis Corpus. Prompt engineering with semantically guided example selection outperforms random selection, and task-level fine-tuning achieves the best overall performance, with F1 scores exceeding 0.83 for rare disease entities. Few-shot learning with semantically similar examples achieves near-state-of-the-art results, while RAG provides limited gains except for improving recall on challenging sign and symptom entities. The study demonstrates that prompt-optimized LLMs can serve as effective alternatives to traditional supervised models in biomedical NER when annotated data is scarce.

## Method Summary
The study employs GPT-4o and GPT-4o-mini for rare disease named entity recognition using a structured prompt framework with five components: task description, output format, task guidance, disambiguation rules, and input text. The authors evaluate four approaches: zero-shot prompting with basic prompts only, few-shot in-context learning with various example selection strategies (random, KNN-based semantic similarity, and cluster-based), task-level fine-tuning on the RareDis corpus, and retrieval-augmented generation using Orphanet knowledge snippets. Performance is measured using exact-match F1 scores at the document level with 10,000 bootstrap iterations for confidence intervals.

## Key Results
- Fine-tuning GPT-4o achieves F1=0.837 for rare disease entities, exceeding BioClinicalBERT baseline (F1=0.704)
- Semantic similarity-based example selection (Inquiry-KNN) consistently outperforms random selection across all entity types
- Few-shot learning with semantically similar examples achieves near-state-of-the-art results without parameter updates
- RAG provides limited gains except for improving recall on sign and symptom entities, where it increases recall by 5-7 percentage points

## Why This Works (Mechanism)

### Mechanism 1: Semantic Similarity-Based Few-Shot Example Selection
Selecting in-context examples via embedding similarity improves NER accuracy over random selection, particularly for semantically ambiguous entity types. The Inquiry-KNN strategy computes Euclidean distance between test inquiry embeddings and training examples, selecting the k nearest neighbors. This surfaces demonstrations that share lexical and semantic patterns with the target input, enabling the model to generalize from structurally analogous cases rather than relying on potentially irrelevant random samples. The core assumption is that embedding similarity correlates with task-relevance for NER demonstrations; the pretrained embedding space captures biomedical semantic relationships sufficiently for this purpose. Evidence shows Inquiry-KNN consistently outperforms random selection even with few examples (k=2), leading to substantial improvement over zero-shot. This mechanism may break when test inputs contain ultra-rare entities absent from the training set's semantic neighborhood.

### Mechanism 2: Structured Prompt Encoding of Domain Knowledge
Multi-component prompts that explicitly define entity boundaries and disambiguation rules improve LLM performance on semantically overlapping entity types without labeled examples. The five-component prompt injects explicit domain knowledge through task guidance (formal definitions) and disambiguation rules (meta-instructions for frequent errors). This constrains the generation space and reduces type confusion. The core assumption is that LLMs can reliably follow complex multi-part instructions and apply abstract definitions to specific span-level decisions. Evidence shows adding disambiguation rules improves rare disease F1 from 0.614 to 0.702. This mechanism may fail when entity boundaries are inherently ambiguous in the source text.

### Mechanism 3: Task-Level Fine-Tuning for Domain Internalization
Fine-tuning LLMs on domain-specific prompts achieves superior performance by internalizing annotation conventions that in-context learning cannot fully capture. Unlike in-context learning, fine-tuning updates model parameters to minimize cross-entropy loss over the training corpus, allowing the model to learn implicit patterns such as typical span boundaries, annotation granularity, and entity co-occurrence statistics. The core assumption is that the training corpus size (N=729 documents, 5,221 rare disease mentions) is sufficient for fine-tuning without catastrophic forgetting or overfitting. Evidence shows fine-tuning achieves F1=0.837 for rare disease entities, exceeding BioClinicalBERT. This mechanism may break when labeled data is extremely scarce (<100 examples).

## Foundational Learning

- Concept: **In-Context Learning (ICL)**
  - Why needed here: The paper's core contribution is demonstrating that ICL with well-selected examples can approach supervised performance. Understanding how demonstrations influence generation without gradient updates is essential for interpreting the results.
  - Quick check question: Given a prompt with 4 labeled examples, can you explain why the model's predictions improve without any parameter changes?

- Concept: **Named Entity Recognition as Sequence Generation**
  - Why needed here: The paper formulates NER as conditional text generation (outputting comma-separated entity lists) rather than token classification. This design choice affects evaluation and failure modes.
  - Quick check question: How does treating NER as text generation change the error profile compared to BIO-tagging approaches?

- Concept: **Embedding-Based Retrieval**
  - Why needed here: Both the Inquiry-KNN example selection and the RAG component rely on computing embedding distances. Understanding embedding spaces and similarity metrics is prerequisite to reproducing and extending the method.
  - Quick check question: Why might Euclidean distance in embedding space fail to capture task-relevant similarity for rare disease entities?

## Architecture Onboarding

- Component map: Embedding Module -> Example Selector -> Prompt Builder -> LLM Inference -> Evaluator
- Critical path: 1. Embed training and test documents → 2. For each test input, select k examples via chosen strategy → 3. Assemble prompt with examples → 4. Call LLM API → 5. Parse comma-separated output → 6. Compute exact-match metrics
- Design tradeoffs:
  - Inquiry-KNN vs. Cluster-KNN: Inquiry-KNN optimizes per-query relevance but requires embedding every test-train pair; Cluster-KNN amortizes selection across cluster members, reducing compute but sacrificing query-specific optimization
  - More examples vs. token cost: Performance saturates around k=4-8 (0.62-1.05¢ per query); beyond this, marginal F1 gains <0.002 per additional cent
  - RAG vs. few-shot: RAG adds ~50 tokens per snippet with limited F1 gains (+0.013 average); few-shot provides higher returns per token budget
- Failure signatures:
  - Spurious overgeneration: Symptom entity shows 46% spurious outputs, often due to sign/symptom boundary confusion (55.5% of spurious symptoms match sign annotations)
  - Boundary drift: 20-31% of errors involve incorrect span boundaries, particularly for sign entities
  - Missed entities: Disease recognition loses 29% of ground-truth mentions; indicates recall bottleneck
- First 3 experiments:
  1. Baseline replication: Implement zero-shot with basic prompt only on a held-out subset; verify F1 matches reported values within CI bounds (rare disease F1≈0.614)
  2. Ablate example selection: Compare Inquiry-Random vs. Inquiry-KNN at k=2, 4, 8; confirm semantic selection yields statistically significant improvement (use paired bootstrap test)
  3. Error taxonomy audit: Run Inquiry-KNN (k=8) on 50 test documents; manually classify errors into the six categories; verify spurious and boundary errors dominate as reported

## Open Questions the Paper Calls Out

### Open Question 1
Can an entity-centric, sentence-level retrieval strategy significantly outperform the definition-style RAG method used in this study for rare disease NER? The authors propose a future direction to "make retrieval more targeted than the definition style snippets" by indexing "short sentences rather than whole documents" to provide usage examples. This remains unresolved because the current RAG implementation provided limited gains, likely because the retrieved snippets (definitions) overlapped with the model's pre-existing knowledge rather than demonstrating entity boundaries in context.

### Open Question 2
To what extent can post-processing heuristics reduce the high rates of spurious outputs and boundary drift observed in prompt-based rare disease NER? The discussion suggests that "improvements in model performance may be more effectively achieved through post-processing heuristics, such as dictionary-based filtering... and head-noun alignment." This remains unresolved because the error analysis revealed that "Spurious" and "Boundary" errors account for the majority of failures (e.g., 46% spurious rate for symptoms), which pure prompt engineering failed to eliminate.

### Open Question 3
Can a compact model fine-tuned on synthetic annotations generated by GPT-4o achieve state-of-the-art performance while ensuring privacy? The authors suggest that "A compact model fine-tuned on [synthetic annotations generated by the LLM] might then serve as a cost-effective and privacy-preserving solution that approaches SOTA." This remains unresolved because while the study demonstrates GPT-4o's extraction capabilities, it did not validate whether the synthetic data it generates is of sufficient quality to train a smaller, local model to the same standard.

## Limitations
- Performance claims based on single corpus (729 documents) with limited size, raising statistical robustness concerns
- Results may not transfer to different annotation schemas or medical domains beyond rare diseases
- Sign and symptom recognition remains challenging with boundary errors and entity type confusion accounting for over 50% of total errors

## Confidence

| Claim | Confidence Level | Evidence |
|-------|------------------|----------|
| Semantic example selection outperforms random selection | High | Direct comparisons show consistent improvement across entity types |
| Fine-tuning exceeds in-context learning performance | High | Robust across multiple evaluation conditions |
| RAG provides limited utility | Medium | Marginal F1 gains (0.013 average) relative to additional complexity |

## Next Checks

1. Conduct cross-corpus validation using a different rare disease NER dataset to assess generalizability beyond the RareDis corpus
2. Perform ablation studies on prompt components to quantify the individual contributions of task guidance versus disambiguation rules
3. Implement an error analysis pipeline that automatically classifies model outputs into the six error types identified in Figure 2, enabling systematic tracking of performance bottlenecks across different entity types and evaluation conditions