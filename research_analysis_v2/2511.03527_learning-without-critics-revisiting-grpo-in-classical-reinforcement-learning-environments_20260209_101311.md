---
ver: rpa2
title: Learning Without Critics? Revisiting GRPO in Classical Reinforcement Learning
  Environments
arxiv_id: '2511.03527'
source_url: https://arxiv.org/abs/2511.03527
tags:
- grpo
- learning
- environments
- group
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides the first systematic evaluation of Group Relative
  Policy Optimization (GRPO) in classical reinforcement learning environments. Through
  controlled ablations across discrete and continuous control tasks, the authors compare
  GRPO against PPO variants under varying baselines, discount factors, and group sizes.
---

# Learning Without Critics? Revisiting GRPO in Classical Reinforcement Learning Environments

## Quick Facts
- arXiv ID: 2511.03527
- Source URL: https://arxiv.org/abs/2511.03527
- Reference count: 35
- First systematic evaluation of GRPO in classical RL environments, comparing against PPO variants

## Executive Summary
This paper systematically evaluates Group Relative Policy Optimization (GRPO) in classical reinforcement learning environments through controlled ablations across discrete and continuous control tasks. The authors compare GRPO against PPO variants under varying baselines, discount factors, and group sizes to determine conditions where critic-free methods remain viable alternatives. Key findings reveal that learned critics remain essential for long-horizon tasks—GRPO performs comparably to simple batch baselines but substantially underperforms PPO except in short-horizon environments like CartPole.

## Method Summary
The study compares GRPO against PPO variants across standard Gymnasium environments (CartPole-v1, Acrobot-v1, MountainCarContinuous-v0, HalfCheetah-v4, Humanoid-v4) using CleanRL defaults. GRPO computes advantages through group-relative normalization over G parallel trajectories without a learned critic, while PPO uses GAE with a value function. The experimental design includes baseline ablations (no baseline, batch mean, EMA, GRPO-batch), discount factor sweeps (γ ∈ {0, 0.1, 0.5, 0.95, 0.99, 1}), and group size sensitivity tests (G ∈ {8, 16, 32, 64}). Training runs for 1M steps with 10 seeds per configuration, reporting mean episodic returns with 95% confidence intervals.

## Key Results
- GRPO underperforms PPO in long-horizon tasks without early termination but remains competitive in short-horizon or termination-rich environments
- High discount factors (γ=0.99) benefit GRPO in most environments, with HalfCheetah as an exception favoring moderate discounting (γ=0.9)
- Smaller group sizes (G=8) generally outperform larger ones, suggesting limitations in batch-based grouping strategies

## Why This Works (Mechanism)

### Mechanism 1
Group-relative normalization provides variance-reducing baselines without learned critics by computing advantages as (R(τᵢ) - μG) / (σG + ε) across G parallel trajectories. This assumes trajectories from the same environment share sufficient statistical structure for meaningful comparisons.

### Mechanism 2
Early termination creates natural episode boundaries that enable critic-free baselines to extract learning signals by separating successful from unsuccessful trajectories into distinct return clusters.

### Mechanism 3
Discount factor effectiveness depends on environment termination structure—high γ preserves long-term credit in termination-rich tasks, while moderate γ helps dense-reward tasks without termination by preventing signal dilution.

## Foundational Learning

- **Policy gradient with baselines (REINFORCE framework)**: Understanding why baselines reduce variance without introducing bias is prerequisite to evaluating critic-free alternatives. Quick check: Can you explain why subtracting a baseline b(s) from returns preserves unbiased gradient estimates while reducing variance?

- **Generalized Advantage Estimation (GAE) and bias-variance tradeoffs**: PPO's learned critic with GAE (λ=0.95, γ=0.99) serves as the comparison baseline. Quick check: How does the λ parameter in GAE control the bias-variance tradeoff, and why might episode-level advantages sacrifice temporal credit assignment?

- **PPO's clipped surrogate objective**: Both PPO and GRPO use clipping (ε=0.2) to prevent destructive updates. Quick check: Why does clipping the importance sampling ratio stabilize policy updates, and what happens if advantage estimates have high variance?

## Architecture Onboarding

- **Component map**: Policy network (actor) -> Group sampler (collects G trajectories) -> Advantage computer (GAE or group normalization) -> Optimizer (Adam) -> Update policy (and critic if present)

- **Critical path**: 1) Collect N_steps timesteps from N_envs parallel environments 2) Form groups (GRPO: batch episodes together; PPO: process individually) 3) Compute advantages (GRPO: normalize across group returns; PPO: GAE with bootstrapping) 4) Apply clipped objective over N_epochs=4 with N_mb=1 minibatch 5) Update policy (and critic if present)

- **Design tradeoffs**: Critic-free vs. learned critic removes ~50% memory overhead but loses state-dependent credit assignment; larger groups reduce baseline variance but decrease update frequency and may mix unrelated episodes; discount γ=0.99 suits termination-rich environments while γ=0.9 helps dense-reward, no-termination tasks

- **Failure signatures**: Long-horizon tasks without early termination show flat learning curves; large group sizes with heterogeneous episodes degrade performance despite more samples; γ=1 in dense-reward/no-termination settings causes HalfCheetah-style failure with no learning progress

- **First 3 experiments**: 1) Baseline ablation comparing PPO vs. GRPO vs. batch-mean baseline on target environment 2) Discount factor sweep γ ∈ {0.9, 0.95, 0.99, 1.0} on your environment 3) Group size sensitivity varying G ∈ {8, 16, 32, 64} while controlling for update frequency

## Open Questions the Paper Calls Out

- Can similarity-based or state-distribution clustering grouping strategies unlock the theoretical benefits of larger group sizes in GRPO for classical RL? The authors state that "the design of effective grouping mechanisms for classical RL environments remains an open problem."

- How do exploration mechanisms such as entropy regularization interact with GRPO's critic-free advantage estimation? The authors explicitly note that "future work should systematically investigate exploration mechanisms such as entropy regularization and other implementation details."

- Does GRPO's effectiveness generalize to partially observable environments or sparse reward domains beyond the fully observable benchmarks tested? The authors note that "our evaluation focused on fully observable standard RL benchmarks; extending to partially observable environments or sparse reward domains may reveal different performance characteristics and trade-offs."

## Limitations

- Architectural assumptions rely on CleanRL defaults without explicit network specifications, potentially affecting reproducibility
- Group sampling methodology uses batch-based grouping which may not represent the most effective strategy
- Environmental scope doesn't examine high-dimensional observations or sparse rewards, limiting conclusions about broader applicability

## Confidence

- **High confidence**: GRPO underperforms PPO in long-horizon tasks without early termination; group normalization provides effective baselines only when episode returns are meaningfully separated
- **Medium confidence**: Optimal discount factor depends on termination structure rather than reward density alone; smaller group sizes generally outperform larger ones due to heterogeneous episode mixing
- **Low confidence**: GRPO's viability as critic-free alternative is limited to specific environmental conditions; findings may not extend to high-dimensional or sparse-reward domains

## Next Checks

1. **Architectural ablation**: Reproduce key results while varying MLP widths (64→128→256 units) and activation functions (ReLU vs. tanh) to determine sensitivity to network design choices

2. **Alternative grouping strategies**: Implement state-similarity or trajectory-similarity based grouping rather than batch-based grouping, and compare performance across group sizes

3. **Sparse-reward environments**: Test GRPO on sparse-reward variants of existing tasks and high-dimensional observations to assess scalability limitations beyond the current testbed