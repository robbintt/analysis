---
ver: rpa2
title: 'RefSR-Adv: Adversarial Attack on Reference-based Image Super-Resolution Models'
arxiv_id: '2601.01202'
source_url: https://arxiv.org/abs/2601.01202
tags:
- adversarial
- image
- reference
- refsr
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RefSR-Adv, the first adversarial attack specifically
  targeting reference-based image super-resolution (RefSR) models. Unlike prior attacks
  on single-image super-resolution, RefSR-Adv manipulates only the reference image
  to degrade SR outputs while leaving the low-resolution input untouched.
---

# RefSR-Adv: Adversarial Attack on Reference-based Image Super-Resolution Models

## Quick Facts
- **arXiv ID**: 2601.01202
- **Source URL**: https://arxiv.org/abs/2601.01202
- **Reference count**: 22
- **Primary result**: First adversarial attack targeting reference-based image super-resolution (RefSR) models, achieving up to 10.64 dB PSNR degradation while maintaining stealthiness

## Executive Summary
This paper introduces RefSR-Adv, the first adversarial attack specifically designed for reference-based image super-resolution (RefSR) models. Unlike traditional attacks on single-image super-resolution, RefSR-Adv manipulates only the reference image while keeping the low-resolution input unchanged. The attack exploits the fact that RefSR models transfer textures from reference images through feature matching and fusion. By injecting optimized perturbations into the reference image using projected gradient descent, the attack degrades SR outputs while maintaining visual stealthiness with PSNR > 35 dB. Extensive experiments across four popular RefSR architectures (CNN, Transformer, and Mamba) on three datasets demonstrate significant performance degradation and reveal that models with higher reference similarity and those trained with perceptual/adversarial losses are more vulnerable.

## Method Summary
RefSR-Adv is a white-box adversarial attack that targets reference-based super-resolution models by perturbing only the reference image while keeping the low-resolution input fixed. The attack employs a pseudo ground-truth strategy where a clean baseline output is generated and treated as a static optimization target. Projected Gradient Descent (PGD) is used to iteratively maximize the L2 distance between adversarial and clean outputs while constraining perturbations within an L∞ bound (ε=8/255). The optimization seeks to identify high-impact reference pixels that maximize degradation when fused through the model's feature-matching mechanism. The attack maintains stealthiness by keeping perturbations imperceptible to human observers while achieving catastrophic degradation in super-resolution quality.

## Key Results
- Achieves up to 10.64 dB PSNR drops on reference-based super-resolution models
- Maintains stealthiness with reference image PSNR > 35 dB
- Models trained with perceptual/adversarial losses show higher vulnerability
- Attack effectiveness positively correlates with low-resolution and reference image similarity
- Full-resolution reference models (DATSR, SSMTF) more vulnerable than downsampled reference models (TTSR, MASA)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Perturbing only the reference image suffices to catastrophically degrade RefSR outputs.
- **Mechanism**: RefSR models transfer textures from the reference via feature matching and fusion. Perturbing the reference propagates through these mechanisms, corrupting texture synthesis while global structure (constrained by LR input) remains intact.
- **Core assumption**: Feature-matching lacks semantic verification to distinguish adversarial from legitimate textures.
- **Evidence anchors**: Abstract confirms attack manipulates only reference image; Section IV.B shows severe performance collapses in DATSR/SSMTF; corpus lacks direct evidence as no papers address reference-based adversarial attacks.

### Mechanism 2
- **Claim**: Models trained with perceptual/adversarial losses are more vulnerable than reconstruction-only models.
- **Mechanism**: Perceptual and adversarial losses encourage aggressive high-frequency texture synthesis, increasing sensitivity to reference features. Adversarial perturbations exploit this by masquerading as valid textures.
- **Core assumption**: Loss functions do not incorporate robustness constraints to prevent excessive trust in reference features.
- **Evidence anchors**: Section IV.B confirms full-loss models exhibit higher vulnerability; Section III.A explains adversarial loss increases texture sensitivity; no corpus papers compare loss-function vulnerability in RefSR.

### Mechanism 3
- **Claim**: Attack effectiveness correlates positively with LR-Ref similarity.
- **Mechanism**: At high similarity levels, RefSR models aggressively fuse reference features to maximize detail recovery, amplifying adversarial perturbation transmission. Low similarity triggers correlation filtering that rejects mismatched features.
- **Core assumption**: Feature-fusion strength scales with similarity without adversarial-aware modulation.
- **Evidence anchors**: Abstract confirms positive correlation between similarity and attack effectiveness; Section V.C shows most severe degradation at higher similarity levels; no corpus papers examine similarity-attack correlations.

## Foundational Learning

- **Concept**: **Projected Gradient Descent (PGD) for constrained optimization**
  - Why needed here: RefSR-Adv uses PGD to iteratively maximize destruction loss while keeping perturbations within ε-bounds (imperceptibility) and valid pixel range.
  - Quick check question: Given ε=8/255, α=step size, and T=50 iterations, explain why random initialization matters for escaping local maxima.

- **Concept**: **Reference-based Super-Resolution (RefSR) dual-input architecture**
  - Why needed here: Understanding how LR input constrains structure while Ref image provides texture priors clarifies why perturbing only Ref degrades textures without collapsing geometry.
  - Quick check question: Why does downsampling the reference (as in TTSR/MASA) act as a low-pass filter that mitigates high-frequency adversarial perturbations?

- **Concept**: **Destruction Loss (L₂ norm) as proxy for PSNR minimization**
  - Why needed here: The attack maximizes L₂ distance between adversarial and clean outputs, which is mathematically equivalent to minimizing PSNR.
  - Quick check question: Why use L₂ instead of L₁ for destruction loss when the goal is catastrophic pixel-level deviation?

## Architecture Onboarding

- **Component map**: LR input → structural backbone → output; Reference image → texture feature extractor → matching/fusion module → output
- **Critical path**: Reference feature extraction → feature matching → texture fusion → SR output
- **Design tradeoffs**: ε (perturbation budget) balances potency vs stealthiness; T (iterations) trades efficiency vs marginal improvement; target model architecture affects vulnerability
- **Failure signatures**: Zero gradients indicate non-differentiable matching modules; insufficient degradation suggests implementation errors; low stealthiness indicates clipping artifacts
- **First 3 experiments**:
  1. Run RefSR-Adv on TTSR (CUFED5) with ε=8/255, T=50. Verify clean output PSNR≈25.40, adversarial output PSNR≈21.84, stealthiness PSNR>35dB. Compare against random Gaussian noise at same ε to confirm attack specificity.
  2. Stratify CUFED5 by similarity levels 1-5. Measure PSNR drop per level. Confirm monotonic decrease in attack effectiveness as similarity drops (Table IV pattern).
  3. Compare attack on full-loss vs reconstruction-only checkpoints of DATSR on DRefSR. Expect full-loss model to show larger PSNR drop (10.64 vs 10.14) and verify statistical significance across multiple samples.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can RefSR-Adv be effectively adapted to black-box settings where the attacker lacks access to model parameters and gradients?
- Basis in paper: [explicit] The Conclusion states, "Future work will focus on exploring black-box attacks by integrating meta-learning or query-based optimization."
- Why unresolved: The current methodology relies entirely on white-box access to compute gradients via Projected Gradient Descent (PGD), limiting its applicability in real-world scenarios where model internals are hidden.
- What evidence would resolve it: Successful implementation of a query-based or transfer-based attack that achieves significant PSNR drops on RefSR models without accessing internal weights.

### Open Question 2
- Question: How effective are "similarity-aware" defense mechanisms at mitigating the reference-based vulnerability without compromising super-resolution performance?
- Basis in paper: [explicit] The Conclusion suggests "developing similarity-aware defense mechanisms to enhance the robustness of RefSR systems," and Section VI proposes input purification and matching gating.
- Why unresolved: The paper identifies the vulnerability and hypothesizes potential defenses but does not implement or test them, leaving their efficacy and computational cost unverified.
- What evidence would resolve it: Quantitative results showing that a specific defense method (e.g., content-based matching gating) successfully filters adversarial references while maintaining high PSNR on clean references.

### Open Question 3
- Question: Can the cross-model transferability of the attack be improved despite the architectural heterogeneity of feature matching mechanisms?
- Basis in paper: [explicit] The Conclusion notes that "the cross-model transferability of the attack remains challenging due to the architectural heterogeneity in feature matching and fusion mechanisms."
- Why unresolved: The paper demonstrates the attack works universally in white-box settings but implies that a single perturbation does not easily transfer between models (e.g., from CNN to Mamba) due to differing feature extraction logic.
- What evidence would resolve it: Generation of a universal adversarial perturbation that degrades performance across CNN, Transformer, and Mamba architectures simultaneously.

## Limitations
- Attack effectiveness heavily depends on model architecture, with downsampled reference models showing relative robustness
- Does not address real-world deployment scenarios with preprocessing or content-based filtering
- Limited cross-model transferability requires white-box access per model
- Specific perturbation budget (ε=8/255) may not generalize to stricter stealth requirements

## Confidence

**High Confidence**: Core mechanism of perturbing reference images to degrade RefSR outputs is well-validated across multiple datasets and architectures. Positive correlation between similarity and attack effectiveness is consistently demonstrated.

**Medium Confidence**: Claim about perceptual/adversarial loss vulnerability is supported but requires larger-scale statistical validation. Specific vulnerability rankings may shift with different implementation details.

**Low Confidence**: Claims about real-world attack transferability and effectiveness against production systems remain speculative without testing against models with built-in defenses or preprocessing pipelines.

## Next Checks

1. **Loss-function vulnerability verification**: Replicate the full-loss vs reconstruction-only comparison on DATSR across 50+ samples to confirm statistical significance of the 0.5 dB difference in PSNR drops.

2. **Similarity-level ablation**: Systematically test attack effectiveness across all five similarity levels on CUFED5, measuring both PSNR drops and SSIM degradation to confirm monotonic relationship.

3. **Architecture robustness comparison**: Test attack transferability from DATSR to TTSR/MASA using the same perturbation, quantifying cross-model effectiveness to validate stated architectural vulnerability differences.