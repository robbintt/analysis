---
ver: rpa2
title: 'Large Language Models and Emergence: A Complex Systems Perspective'
arxiv_id: '2506.11135'
source_url: https://arxiv.org/abs/2506.11135
tags:
- emergence
- emergent
- systems
- more
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper analyzes claims of emergent capabilities in Large Language\
  \ Models (LLMs) by examining whether these capabilities meet established criteria\
  \ for emergence from complexity science. The authors argue that emergence requires\
  \ more than just sudden improvements in performance\u2014it demands new coarse-grained\
  \ descriptions, internal compression, and novel organizational structures that screen\
  \ off microscopic details."
---

# Large Language Models and Emergence: A Complex Systems Perspective

## Quick Facts
- arXiv ID: 2506.11135
- Source URL: https://arxiv.org/abs/2506.11135
- Authors: David C. Krakauer; John W. Krakauer; Melanie Mitchell
- Reference count: 40
- One-line primary result: Emergence claims in LLMs require new compressed internal representations that screen off microscopic details, not just performance jumps

## Executive Summary
This paper critically examines claims of emergent capabilities in Large Language Models through the lens of complexity science. The authors argue that true emergence requires more than discontinuous performance improvements—it demands new coarse-grained descriptions, internal compression, and novel organizational structures that screen off microscopic details. They distinguish between "knowledge-out" emergence (complex behavior from simple components) and "knowledge-in" emergence (complex behavior from complex inputs), arguing that LLMs fall into the latter category where emergence claims are harder to establish. While acknowledging some promising evidence for emergent capabilities, the authors conclude that most current claims fall short of true emergence and emphasize the critical distinction between emergent capabilities and emergent intelligence.

## Method Summary
The paper provides a theoretical review applying complexity science criteria to evaluate emergence claims in LLMs. Rather than empirical experimentation, it synthesizes existing literature on LLM capabilities and scaling laws to assess whether they meet established emergence criteria from physics and complex systems theory. The authors develop a five-part framework examining scaling, criticality, compression, novel bases, and generalization, while distinguishing between Knowledge-Out and Knowledge-In emergence types. The analysis is primarily conceptual, drawing on examples like OthelloGPT and arithmetic capabilities to illustrate where current evidence supports or fails to support genuine emergence claims.

## Key Results
- Emergence requires new compressed internal representations that screen off microscopic details, not merely discontinuous performance improvements
- LLMs are Knowledge-In systems where emergence claims require evidence of both microscopic mechanisms and global coarse-grained properties
- Double descent phase transitions with qualitative reorganization of internal representations provide preliminary evidence for emergence
- Most current emergence claims in LLMs fall short of true emergence due to lack of internal compression evidence

## Why This Works (Mechanism)

### Mechanism 1: Coarse-Graining as the Defining Criterion for Emergence
True emergence requires new compressed internal representations that screen off microscopic details, not merely discontinuous performance improvements. A system demonstrates emergence when it develops coarse-grained variables forming "effective theories"—reduced descriptions that predict system behavior without reference to lower-level mechanisms. The value of emergence lies in descriptional efficiency, not surprise to human observers. Evidence includes the abstract's definition of emergent properties as those described by replacing high-dimensional mechanisms with lower-dimensional effective variables, and Section 3's emphasis on emergence permitting compressed models of long-term behavior. The break condition occurs if LLM capabilities rely on "massive complexity" without evidence of internal compression.

### Mechanism 2: Knowledge-In (KI) Emergence Requires Dual Evidence
LLMs, as KI systems, require evidence of both local microscopic mechanisms AND global coarse-grained properties to claim emergence—not just external behavior. Unlike Knowledge-Out systems, KI systems have parameterized components trained on complex data, making emergent claims harder to establish because capabilities may be "programmed in" by training. The more information about the world embedded in training data, the weaker emergence claims become. Evidence anchors include Section 4's requirement for describing both coarse-grained global properties and local microscopic mechanisms, and Section 5.1's argument that if global properties are being "programmed" by extensive training, the behavior hardly qualifies as emergent. The break condition occurs if new capabilities can be traced to interpolating from training data.

### Mechanism 3: Double Descent Phase Transition (Preliminary Evidence)
Neural networks may exhibit genuine emergence when covariance spectra of learned features transition from exponential to scale-free at double descent peaks. This qualitative reorganization of internal representations—not just performance change—constitutes evidence for emergent capability through a phase transition. The core assumption is that scale-free spectral structure indicates qualitatively new representational bases. Evidence includes Section 5.1's reference to Guth & Ménard (2025, in prep) showing the double descent peak occurs concomitantly with qualitative change from exponential to scale-free covariance spectra. The break condition occurs if similar spectral transitions appear in simple regression models without novel capabilities.

## Foundational Learning

- **Coarse-graining**: Understanding how high-dimensional systems can be described by lower-dimensional effective variables (e.g., fluid dynamics replacing molecular dynamics).
  - Why needed here: The paper's entire framework hinges on distinguishing true emergence (with coarse-graining) from mere performance jumps.
  - Quick check question: Can you explain why "more is different" requires new descriptions, not just more components?

- **Phase transitions and criticality**: Understanding discontinuous organizational changes in physical systems.
  - Why needed here: LLM emergence claims often invoke phase transition analogies; understanding what real phase transitions require helps evaluate these claims.
  - Quick check question: What makes water's phase transition different from a discontinuous accuracy jump in LLM benchmarks?

- **Compression and minimal description length**: Understanding how efficient representations capture regularities.
  - Why needed here: The paper argues emergence requires internal compression; distinguishing memorization from compression is critical.
  - Quick check question: Why does memorizing training examples not constitute compression in the emergence sense?

## Architecture Onboarding

- **Component map**: Five emergence conditions: (1) Scaling with new organization, (2) Criticality/phase transitions, (3) Internal compression, (4) Novel bases/manifolds, (5) Generalization across tasks -> Two emergence types: Knowledge-Out (simple components → complex behavior) vs Knowledge-In (complex inputs → complex behavior) -> Two properties to distinguish: Emergent capability vs emergent intelligence

- **Critical path**: 
  1. Identify the claimed emergent capability
  2. Check for coarse-grained internal representations (not just external behavior)
  3. Test whether representation screens off microscopic details
  4. Verify generalization beyond training distribution

- **Design tradeoffs**:
  - Probing internal representations vs. only measuring external performance
  - Testing for memorization shortcuts vs. genuine generalization
  - Computational cost of interpretability analysis vs. black-box benchmarking

- **Failure signatures**:
  - Capability appears only on benchmarks similar to training data
  - Internal analysis reveals "bag of heuristics" rather than compressed model
  - Discontinuous performance jumps explained by metric choice
  - "Overwhelmingly large causal graph" without compression

- **First 3 experiments**:
  1. Replicate OthelloGPT analysis: Probe whether claimed "world model" is causal or epiphenomenal by intervening on internal representations
  2. Test scaling breaks: Analyze covariance spectra at double descent to detect exponential → scale-free transitions
  3. Distinguish KI from KO: Compare emergence in transformer trained on synthetic data with simple rules vs. natural language corpus

## Open Questions the Paper Calls Out

### Open Question 1
Can LLMs demonstrate "instruction-mediated emergence," where verbal instructions instantly reconfigure internal network structures without further training? The authors note that in humans, understanding a verbal instruction can "configure this structure in minutes rather than through laborious training," adding, "This kind of instruction-mediated emergence has yet to be demonstrated in an LLM." Current LLMs are evaluated on capabilities derived from statistical correlation over massive datasets rather than rapid structural reorganization based on semantic understanding. Experiments showing that a prompt can induce a qualitative, causal change in an LLM's internal representational geometry without updating weights would resolve this.

### Open Question 2
Do LLMs form novel internal bases or manifolds to compress regularities, or do they rely on "bags of heuristics"? The paper states that "relatively little is known about whether or how such structures might exist in LLMs," citing conflicting evidence regarding whether internal models are parsimonious or just "bags of heuristics." High dimensionality and non-linearity make it difficult to perform the mechanistic interpretability required to verify if coarse-grained internal variables exist. Identification of low-dimensional, interpretable internal features that are causally sufficient for task performance and screen off microscopic details would resolve this.

### Open Question 3
Can we distinguish between tasks solvable by mere scaling ("more is more") versus those requiring qualitative internal reorganization ("more is different")? The authors conclude, "We do not yet have a theory with respect to capabilities themselves; we do not know a priori which tasks can be accomplished by LLMs as they just get larger... versus those that require a qualitative change." Current scaling laws predict loss but do not predict the acquisition of specific capabilities or the structural phase transitions required for them. A formal theoretical framework that predicts specific capability acquisition based on model size and data structure, verified by observing distinct phases of internal organization, would resolve this.

### Open Question 4
Does language act as a world representation, a "language of thought," or a programming language in LLMs? The authors list three possible roles of language (world representation, mentalese, or programming language) and state, "We do not have definitive evidence for any of these three claims, but they play a crucial role in any statement relating to how surprising the behavior of an LLM will be deemed." It is difficult to disentangle the information already present in the training data from the model's ability to process or reason with it. Probing studies that successfully separate the model's reliance on memorized facts from its execution of algorithmic procedures or logical reasoning would resolve this.

## Limitations

- The paper is a theoretical review lacking empirical validation for key claims about internal compression and coarse-graining
- The Knowledge-In vs Knowledge-Out distinction lacks operational criteria for practical classification
- No quantitative thresholds are provided for what constitutes sufficient "compression" or screening off of microscopic details
- The framework doesn't address the "overwhelmingly large causal graph" critique that LLMs may be better characterized as massive lookup tables

## Confidence

**High Confidence**: The theoretical framework for emergence from complexity science is well-grounded and the distinction between emergent capabilities and emergent intelligence is conceptually sound. The critique of current emergence claims as potentially conflating performance jumps with true emergence has strong support in the literature.

**Medium Confidence**: The Knowledge-In vs Knowledge-Out distinction provides a useful lens for analyzing LLM emergence, but practical application remains challenging without clearer operational definitions. The five-part framework is logically coherent but requires empirical validation.

**Low Confidence**: The specific claim about double descent phase transitions and covariance spectra transitioning from exponential to scale-free as evidence for emergence is based on unpublished work and lacks peer validation.

## Next Checks

1. Replicate OthelloGPT analysis with intervention: Test whether the claimed "world model" for Othello legality is truly causal or epiphenomenal by systematically ablating internal representations and measuring behavioral changes. This directly tests whether internal coarse-graining screens off microscopic details.

2. Quantify compression in arithmetic capabilities: Compare model performance on arithmetic tasks using two metrics: (a) exact match accuracy (current standard) and (b) continuous token probability distributions. Test whether apparent "emergence" at specific scales disappears when using metrics that don't create artificial discontinuities.

3. Cross-distribution generalization test: Train identical transformer architectures on two datasets—(a) synthetic data generated from simple rules, and (b) natural language corpus. Compare emergence patterns to isolate the effect of input complexity (KI vs KO) on claimed emergent capabilities.