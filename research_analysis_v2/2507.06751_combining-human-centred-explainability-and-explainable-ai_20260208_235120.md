---
ver: rpa2
title: Combining Human-centred Explainability and Explainable AI
arxiv_id: '2507.06751'
source_url: https://arxiv.org/abs/2507.06751
tags:
- explainability
- human-centered
- explainable
- learning
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a conceptual framework for combining human-centered
  explainability (HCx) and explainable AI (xAI) through algebraic machine learning
  (AML). The authors identify a key distinction: xAI focuses on explaining computational
  decision-making, while HCx contextualizes AI contributions within user tasks and
  understanding.'
---

# Combining Human-centred Explainability and Explainable AI

## Quick Facts
- arXiv ID: 2507.06751
- Source URL: https://arxiv.org/abs/2507.06751
- Reference count: 16
- Authors propose AML as a framework for integrating HCx and xAI through interpretable atom abstractions and constraint-based learning

## Executive Summary
This paper presents a conceptual framework for combining human-centered explainability (HCx) and explainable AI (xAI) through algebraic machine learning (AML). The authors identify a key distinction: xAI focuses on explaining computational decision-making, while HCx contextualizes AI contributions within user tasks and understanding. To bridge this gap, they propose using AML models, which offer interpretability through their simple three-layer structure (inputs-atoms-outputs), constraint maintenance, and traceability to human-defined policies. As a preliminary demonstration, they apply AML to a gesture typing application where the system interprets gesture patterns and maps them to expressive text styles. The AML model provides both xAI features (gesture speed, acceleration) and HCx explanations by generalizing gesture spaces to user contexts. The approach enables users to understand and control AI interpretations based on their current situation. While no quantitative results are presented, the paper positions AML as a promising foundation for creating AI systems that are both technically explainable and contextually meaningful to end users.

## Method Summary
The paper proposes using Algebraic Machine Learning (AML) with a three-layer architecture (inputs-atoms-outputs) and constraint-based learning via AML Description Language (AML-DL). The method involves defining human-specified constraints, training an AML model that maintains these constraints while generating interpretable atoms, and converting the atom space to a decision tree for explanation. The framework is demonstrated on a gesture typing application where gesture patterns are mapped to expressive text styles, with atoms providing both technical features (speed, acceleration) and contextual explanations for users.

## Key Results
- Conceptual framework proposed for integrating HCx and xAI through AML's three-layer architecture
- Preliminary demonstration shows AML can map gesture patterns to both xAI features and HCx context explanations
- AML's constraint-based learning preserves semantic integrity from human policies to model outputs
- No quantitative results or user studies provided to validate effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AML's three-layer architecture may enable bidirectional translation between technical xAI features and human-centered context explanations.
- Mechanism: Raw input data flows through atom abstractions that can be traced both downward to xAI features (e.g., gesture speed, acceleration) and upward to HCx contextualizations (e.g., user intent, situational relevance). The atom layer serves as a shared representation.
- Core assumption: Atom representations capture meaningful abstractions that map to both technical and conceptual user domains simultaneously.
- Evidence anchors:
  - [abstract] "The AML model provides both xAI features (gesture speed, acceleration) and HCx explanations by generalizing gesture spaces to user contexts."
  - [section] "AML-models have a fundamentally simple structure. They comprise only three layers: inputs – atoms – outputs. An atom can be linked to one or more inputs and is said to be present in an example if at least one of those inputs is present."
  - [corpus] Related work on multi-layered HCxAI frameworks (arXiv:2504.13926) supports layered decomposition for explainability, though does not validate AML specifically.
- Break condition: If atoms fail to generalize beyond training distributions, or if atom-to-context mapping requires manual encoding per use case, the mechanism degrades to feature engineering.

### Mechanism 2
- Claim: Human-defined constraints in AML-DL may preserve semantic integrity during learning, enabling explanations traceable to original policies.
- Mechanism: Users define constraints via AML Description Language (AML-DL); core algorithms check constraint maintenance as new atoms are generated, creating an auditable chain from policy to output.
- Core assumption: Constraint preservation during training ensures that learned representations remain semantically aligned with human intent.
- Evidence anchors:
  - [section] "The core AML algorithms check that the human-defined constraints are maintained in the subsequent binary relationships between inputs and outputs that are learned as new atoms are generated during training."
  - [section] "AML-DL rules can be traced back to human-defined policies, for example, those that provide the basis for human-defined work procedures."
  - [corpus] Adjacent work on argumentation-based XAI (PHAX, arXiv:2507.22009) emphasizes traceability to human rationale, but does not provide comparative validation of AML's constraint mechanism.
- Break condition: If constraint specification requires expertise comparable to traditional programming, or if constraints overly restrict model capacity, practical adoption is limited.

### Mechanism 3
- Claim: Generalized atom spaces may enable context-sensitive HCx explanations by mapping user actions to situational interpretations.
- Mechanism: The atom structure generalizes task/situation/context during learning; this allows the system to explain not just what features triggered a prediction, but how user actions relate to their current goals.
- Core assumption: Atom generalization captures task-situation-context relationships in a form humans can interpret.
- Evidence anchors:
  - [section] "The atom structure also allows us to generalize the task, situation, and context of gesturing. This allows us to provide more human-centered explainability (HCx) of how user actions directly affect the font and color selection."
  - [section] "We can provide guidance on how to control this interpretation given the current situation by displaying alternative gesture spaces for the user's intended goals based on this more general understanding of interpretation."
  - [corpus] Weak direct validation—no quantitative results in paper; corpus lacks empirical AML evaluations.
- Break condition: If generalization is too coarse (loss of nuance) or too fine (combinatorial explosion of contexts), HCx explanations become either trivial or overwhelming.

## Foundational Learning

- Concept: **xAI vs. HCx distinction**
  - Why needed here: The entire framework hinges on understanding that xAI explains computational decision-making ("how the system decided"), while HCx contextualizes outputs for user tasks ("why this matters to you"). Conflating these leads to misaligned explanation design.
  - Quick check question: Given a product recommendation, would explaining "the algorithm weighted your browsing history at 0.7" serve an xAI goal, an HCx goal, or both—and for which audience?

- Concept: **Atom as abstraction layer**
  - Why needed here: AML's explanatory power depends on atoms serving as interpretable intermediate representations between raw inputs and final outputs.
  - Quick check question: If an atom activates for both "fast swipe" and "urgent message context," is it capturing a meaningful shared property or an unintended correlation?

- Concept: **Constraint-based learning**
  - Why needed here: AML-DL constraints encode human knowledge; understanding how these persist through training is essential for debugging explanation failures.
  - Quick check question: If a learned behavior violates user intent, is the failure in constraint specification, constraint enforcement, or atom generation?

## Architecture Onboarding

- Component map:
  Raw Inputs → [AML Core: Atom Generation] → Atom Space → [CART Decision Tree] → Outputs
                    ↑                              ↓
              AML-DL Constraints           xAI Features (speed, acceleration)
                                                   ↓
                                            HCx Explanations (context, relevance)

- Critical path:
  1. Define domain and collect input data stream
  2. Specify AML-DL constraints encoding domain knowledge and policies
  3. Train AML model—atoms generated until class separation achieved under constraints
  4. Convert atom space to interpretable decision tree (CART with depth limit)
  5. Map atom activations to xAI features and HCx contextualizations for UI layer

- Design tradeoffs:
  - Constraint strictness vs. model capacity: Tighter constraints improve traceability but may underfit
  - Atom granularity vs. interpretability: More atoms capture finer distinctions but increase cognitive load
  - Tree depth vs. explanation simplicity: Shallower trees are more interpretable but may lose fidelity

- Failure signatures:
  - Atoms activate indiscriminately → constraint specification too loose or data insufficient
  - HCx explanations feel disconnected from user context → atom-to-context mapping manually defined per case, not learned
  - xAI features uninformative → input features lack discriminative power, not an AML issue
  - Model grows without convergence → conflicting constraints or class overlap in input space

- First 3 experiments:
  1. Replicate gesture typing demo with public dataset; verify atom-to-feature traceability by logging which inputs activate which atoms for known gestures.
  2. Ablate AML-DL constraints incrementally; measure impact on atom count, tree depth, and human-rated explanation quality (n≥10 users).
  3. Compare AML explanations against a post-hoc XAI baseline (e.g., SHAP) on same task; assess whether intrinsic interpretability yields measurably better HCx alignment via user study.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the strict three-layer structure (inputs-atoms-outputs) of Algebraic Machine Learning (AML) effectively scale to map complex, high-dimensional data to meaningful user contexts?
- Basis in paper: [inferred] The paper relies on a simple gesture typing example to demonstrate context mapping, leaving its applicability to more complex, semantic-heavy domains unstated.
- Why unresolved: The "preliminary" nature of the work has not yet demonstrated if the simple algebraic structure creates a bottleneck for representing nuanced human contexts.
- What evidence would resolve it: Successful application of AML in a complex domain (e.g., medical diagnosis or creative design) showing that atoms map clearly to high-level user goals.

### Open Question 2
- Question: Can AML-based explanations simultaneously satisfy the distinct needs of end-users (task relevance) and domain experts (algorithmic transparency) better than post-hoc xAI methods?
- Basis in paper: [explicit] The authors ask how to "combine system and usage context data" to create a comprehensive understanding, noting that current xAI often leads to "information fatigue" for end-users.
- Why unresolved: The paper proposes a conceptual bridge but provides no comparative user studies evaluating if AML reduces fatigue or improves understanding over standard approaches.
- What evidence would resolve it: Quantitative user studies measuring cognitive load and task performance comparing AML interfaces against traditional black-box interfaces with post-hoc explanations.

### Open Question 3
- Question: What methodologies can guide designers in defining the initial human-defined constraints (AML-DL) to ensure the resulting model aligns with explainability goals?
- Basis in paper: [explicit] The paper argues that "the goal of explainability should be considered early on" to determine necessary parameters, but does not outline a specific process for this.
- Why unresolved: Translating abstract user needs into the specific mathematical constraints required by AML is a non-trivial design challenge that lacks a formal framework.
- What evidence would resolve it: A set of design heuristics or a participatory design framework that successfully translates user requirements into AML description language rules.

## Limitations
- No quantitative results or user studies provided to validate framework effectiveness
- AML-DL implementation details and constraint specification methodology not defined
- No comparison against established XAI methods or baselines
- Demonstration limited to simple gesture typing application

## Confidence
- **High:** The distinction between xAI (computational explanation) and HCx (contextual user understanding) is clearly articulated and necessary for the framework's rationale.
- **Medium:** The proposed AML architecture could theoretically support bidirectional explainability through atom abstractions, but this requires empirical demonstration.
- **Low:** Claims about AML's superiority for HCxAI integration lack validation against baselines or real-world user studies.

## Next Checks
1. Implement AML on a public gesture dataset and measure atom interpretability via user studies (n≥10) comparing to SHAP explanations.
2. Systematically ablate AML-DL constraints to quantify the tradeoff between constraint strictness, model capacity, and explanation quality.
3. Test AML generalization across gesture contexts by evaluating explanation consistency when gesture-to-intent mappings shift.