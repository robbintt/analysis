---
ver: rpa2
title: What do language models model? Transformers, automata, and the format of thought
arxiv_id: '2508.18598'
source_url: https://arxiv.org/abs/2508.18598
tags:
- what
- format
- have
- about
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper argues that large language models (LLMs) are not models
  of human linguistic capacities, but rather models of their training corpus. The
  key insight is that cognitive science suggests human linguistic processing requires
  supralinear formats, while transformer architectures, which underlie LLMs, are at
  best linear.
---

# What do language models model? Transformers, automata, and the format of thought

## Quick Facts
- **arXiv ID:** 2508.18598
- **Source URL:** https://arxiv.org/abs/2508.18598
- **Reference count:** 5
- **Primary result:** Transformers are permutation and substring invariant, limiting them to linear formats that prevent modeling human linguistic capacities

## Executive Summary
This paper argues that large language models (LLMs) are not models of human linguistic capacities but rather models of their training corpus. The key insight is that cognitive science suggests human linguistic processing requires supralinear formats (like tree structures), while transformer architectures are fundamentally limited to linear or sublinear formats due to mathematical invariants in their computational architecture. The author demonstrates this through formal analysis showing that unmasked transformers are permutation invariant and masked transformers are substring invariant. The paper then proposes that transformers learn to emulate automata that generate the training corpus through "shortcut solutions" rather than simulating human cognitive states.

## Method Summary
The analysis uses theoretical and formal methods to examine the mathematical invariants of transformer architectures. The author analyzes the residual stream format, demonstrating permutation invariance in unmasked transformers and substring invariance in masked transformers. The study involves running forward passes through a first-generation OpenAI GPT model, extracting intermediate activations (residual stream states) after each block, and decoding top logits to visualize position decay. The theoretical framework draws on automata theory and the Krohn-Rhodes theorem to explain how transformers might learn to emulate corpus-generating automata.

## Key Results
- Unmasked transformers treat the residual stream as a sublinear "sack" of vectors, making them permutation invariant
- Masked (autoregressive) transformers are constrained to substring invariant processing, limiting them to linear formats
- Transformers may model corpora by emulating automata via "shortcut solutions" rather than simulating human cognitive states
- The format of the residual stream is fundamentally sublinear, with positional encoding providing only temporary linear structure

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Unmasked transformers treat the residual stream as a sublinear format rather than a structured sequence
- **Mechanism:** The four core operations (rescaling, weighting, comparing, summing) are either row-wise independent or all-to-all, making the architecture mathematically permutation invariant
- **Core assumption:** The format of the residual stream is defined by the mathematical invariants of the operations performed upon it
- **Evidence anchors:** Abstract mentions "certain invariants of the computational architecture," section 3.2 defines permutation invariance, corpus discusses formal analysis approach
- **Break condition:** If the model introduces row-dependent inductive biases that break the symmetry of all-to-all operations

### Mechanism 2
- **Claim:** Masked transformers are constrained to substring invariant processing
- **Mechanism:** The causal mask ensures output for first n tokens is identical to output if only those n tokens were seen, preventing supralinear structure building
- **Core assumption:** Human linguistic processing requires supralinear formats where global structure influences local components
- **Evidence anchors:** Section 3.3 defines substring invariance, section 4.2 argues masking constrains format to be at most linear
- **Break condition:** If bidirectional processing is allowed or if human thought is proven to be strictly linear

### Mechanism 3
- **Claim:** Transformers model corpora by emulating automata via shortcut solutions
- **Mechanism:** Transformers approximate FSA state transitions using Krohn-Rhodes theorem decomposition, calculating in parallel rather than serially
- **Core assumption:** Training corpus is effectively generated by FSA-like process that transformers can learn efficient shortcuts for
- **Evidence anchors:** Section 5.1 discusses Liu et al.'s speculations about shortcut automata, appendix C details Krohn-Rhodes decomposition
- **Break condition:** If required automaton contains non-solvable groups without efficient parallel shortcuts

## Foundational Learning

- **Concept: Residual Stream Format**
  - **Why needed here:** Core argument distinguishes content from format; understanding "sack of vectors" is prerequisite to grasping architectural constraints
  - **Quick check question:** Does the residual stream inherently know that token A comes before token B without positional encoding?

- **Concept: Invariants**
  - **Why needed here:** Author uses mathematical invariants to prove architectural limits; understanding permutation invariance is key
  - **Quick check question:** If I scramble input rows and get scrambled output of same values, what invariant has been demonstrated?

- **Concept: Automata & Semigroups**
  - **Why needed here:** Positive proposal relies on automata theory and decomposition into simpler algebraic components
  - **Quick check question:** How can a parallel machine simulate a serial machine without reading tokens one by one?

## Architecture Onboarding

- **Component map:** Input tokens -> Positional encodings -> Residual stream -> Masking -> Attention/MLP blocks -> Output prediction
- **Critical path:** 1) Tokens embedded and added to positional encodings (creating linear format), 2) Data enters residual stream, 3) Masking ensures substring invariance, 4) Blocks process stream to predict next token by estimating FSA state
- **Design tradeoffs:** Linearity vs. Structure (trades complex structures for training efficiency), Flexibility vs. Constraints (general purpose but limited to linear representations)
- **Failure signatures:** String reversal/palindrome failures, long-term syntactic binding difficulties, failures on automata with non-solvable groups
- **First 3 experiments:**
  1. Permutation Test: Feed sentence and scrambled version (without positional encoding) to verify permutation invariance
  2. Substring Consistency Test: Compare residual stream states for prefix vs. first n tokens of string to confirm substring invariance
  3. Position Decay Probe: Measure cosine similarity between residual stream and position embedding matrix across layers

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can transformer architectures support derived supralinear formats (like graphs or trees) despite linear constraints?
- **Basis in paper:** [explicit] Author states "That is an open question, and a difficult one to answer"
- **Why unresolved:** While base format appears sublinear, positional encoding theoretically allows complex orderings but it's unclear if operations preserve structures
- **What evidence would resolve it:** Identification of computational sub-structures within layers maintaining invariant hierarchical relationships

### Open Question 2
- **Question:** How do transformers process garden path sentences if substring invariance prevents later tokens from altering earlier structural representations?
- **Basis in paper:** [inferred] Author argues substring invariance precludes later tokens binding to earlier ones but acknowledges "sufficient cleverness" might allow linear approximations
- **Why unresolved:** Mismatch between linear constraints and apparent syntactic competence requires mechanistic explanation
- **What evidence would resolve it:** Demonstration of linear-substring algorithm approximating supralinear syntactic parsing

### Open Question 3
- **Question:** Do transformers systematically fail on formal language tasks requiring automata with non-solvable groups?
- **Basis in paper:** [explicit] Author speculates "there ought to be an important class of failures that arise when some of the automata in the decomposition are non-solvable groups"
- **Why unresolved:** Krohn-Rhodes decomposition suggests efficient shortcuts don't exist for these automata types
- **What evidence would resolve it:** Empirical benchmarking on languages generated by non-solvable group automata

## Limitations

- **Architecture specificity:** Analysis focuses on first-generation GPT architecture, but modern LLMs use different normalization schemes that may alter invariants
- **Theoretical vs. empirical:** The connection between mathematical invariants and cognitive claims requires additional empirical validation beyond theoretical analysis
- **Format categorization:** The assumption that thought format can be cleanly categorized as sublinear, linear, or supralinear may oversimplify complexity

## Confidence

- **High Confidence:** Mathematical proofs of permutation invariance for unmasked transformers and substring invariance for masked transformers
- **Medium Confidence:** Claim that human linguistic processing requires supralinear formats (active area of debate)
- **Low Confidence:** Positive proposal that transformers learn to emulate automata generating the training corpus (requires more direct empirical validation)

## Next Checks

1. **Cross-Architecture Validation:** Test permutation and substring invariance across multiple transformer architectures (GPT-1/2/3, BERT, modern variants) to determine which invariants are truly architectural

2. **Human Processing Comparison:** Design psycholinguistic experiments comparing transformer processing patterns with human reading times on sentences requiring supralinear structure resolution

3. **Automaton Emulation Verification:** Train transformers on synthetic corpora generated by known automata with different algebraic properties and test whether they successfully learn shortcut solutions only for automata with efficient parallel covers