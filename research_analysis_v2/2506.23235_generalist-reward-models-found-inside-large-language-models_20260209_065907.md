---
ver: rpa2
title: 'Generalist Reward Models: Found Inside Large Language Models'
arxiv_id: '2506.23235'
source_url: https://arxiv.org/abs/2506.23235
tags:
- reward
- learning
- language
- policy
- reinforcement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a novel theoretical framework showing that\
  \ a powerful generalist reward model is already latent within any large language\
  \ model trained via next-token prediction. By connecting this training objective\
  \ to offline inverse reinforcement learning, the authors prove that a model's logits\
  \ implicitly encode a principled reward function\u2014the endogenous reward."
---

# Generalist Reward Models: Found Inside Large Language Models

## Quick Facts
- arXiv ID: 2506.23235
- Source URL: https://arxiv.org/abs/2506.23235
- Authors: Yi-Chen Li; Tian Xu; Yang Yu; Xuqin Zhang; Xiong-Hui Chen; Zhongxiang Ling; Ningjing Chao; Lei Yuan; Zhi-Hua Zhou
- Reference count: 40
- One-line primary result: Novel theoretical framework shows powerful generalist reward model is already latent within any LLM trained via next-token prediction, enabling training-free reward extraction and superior RL self-improvement

## Executive Summary
This paper introduces a theoretical framework demonstrating that any large language model trained via next-token prediction implicitly encodes a principled reward function within its logits. By connecting this training objective to offline inverse reinforcement learning, the authors prove that these logits function as a soft Q-function, allowing direct extraction of high-quality reward signals without further training. Experiments validate that this approach outperforms both heuristic LLM-as-a-judge methods and explicitly trained reward models on preference benchmarks while achieving self-improvement on mathematical reasoning tasks.

## Method Summary
The method extracts an "endogenous reward" from any LLM by treating its logits as soft Q-functions from an inverse reinforcement learning perspective. The reward for each token is computed using the inverse soft Bellman operator: subtracting the value function (computed via log-sum-exp over next-token logits) from the current token's logit. For response classification, token-level rewards are summed with discounting to produce outcome rewards. For RL self-improvement, the extracted reward serves as the optimization signal while maintaining a KL penalty against the original model. The approach requires no additional training data or model fine-tuning beyond the RL phase.

## Key Results
- Theoretical proof that LLM logits implicitly encode a reward function equivalent to one learned through offline inverse reinforcement learning
- Endogenous reward outperforms explicitly trained reward models on RM-Bench and Multifaceted-Bench by up to 11.5% in classification accuracy
- Self-improvement experiments show EndoRM+RL achieves 85.1% on MATH-lighteval versus 73.7% for standard SFT+RL
- Theoretical error bounds prove RL using endogenous reward yields O(H) error dependence versus O(H²) for imitation learning

## Why This Works (Mechanism)

### Mechanism 1: Logits as Soft Q-Functions (Theoretical Equivalence)
The logits of an LLM trained via standard next-token prediction approximate solutions to an offline Inverse Reinforcement Learning (IRL) objective, functioning as a soft Q-function. The paper demonstrates that maximizing log-likelihood in next-token prediction is mathematically equivalent to the optimization objective of inverse soft Q-learning. Therefore, the resulting policy $\hat{\pi} = \text{softmax}(\hat{f})$ implies that the logits $\hat{f}$ satisfy the IRL objective for a Q-function. This equivalence holds when the training data consists of optimal expert demonstrations generated according to maximum entropy principles.

### Mechanism 2: Reward Extraction via Inverse Soft Bellman Operator
A principled reward function is extracted training-free by applying the inverse soft Bellman operator to the model's logits. Given the logits $\hat{Q}$, the endogenous reward is calculated as: $\hat{r}(s, a) = \hat{Q}(s, a) - \alpha \log \sum \exp(\hat{Q}(s', a')/\alpha)$. This effectively isolates the "immediate return" from the "future value" encoded in the logits. The entropy coefficient $\alpha$ corresponds to the temperature used in the softmax of the base model, and this formulation provides a unified view of generation (policy) and evaluation (reward).

### Mechanism 3: Compounding Error Reduction (RL vs. Imitation)
Fine-tuning with the extracted endogenous reward theoretically yields a superior policy with linear error dependence on the response length $O(H)$, compared to the quadratic error $O(H^2)$ of standard imitation learning. Imitation learning compounds errors because a single mistake in token prediction changes the state distribution for all subsequent steps. Theoretical analysis suggests RL using the endogenous reward corrects this distribution shift, preventing error accumulation. The environment is modeled as a token-level MDP with deterministic transitions.

## Foundational Learning

- **Concept: Inverse Reinforcement Learning (IRL)**
  - Why needed here: The paper relies on the premise that the model's training objective implicitly solves an IRL problem. Understanding IRL is necessary to grasp why the logits contain reward information at all.
  - Quick check question: Can you explain why recovering a reward function is often harder than learning a policy given a reward?

- **Concept: Soft Bellman Equation / MaxEnt RL**
  - Why needed here: The extraction mechanism uses the *inverse* soft Bellman operator. You need to understand the soft Q-function (which incorporates entropy) to invert it correctly.
  - Quick check question: How does the "soft" Bellman equation differ from the standard Bellman equation regarding exploration/entropy?

- **Concept: Behavior Cloning vs. RL Fine-Tuning**
  - Why needed here: To appreciate the claimed improvement, one must understand the failure mode of Behavior Cloning (compounding errors) and why RL is proposed as the fix.
  - Quick check question: In a sequence generation task, why does a small error early in the sequence lead to a dramatically different final output in behavior cloning?

## Architecture Onboarding

- **Component map:**
  Base LLM -> Logit Extractor -> IRL Converter -> Outcome Reward Aggregator -> RL Optimizer

- **Critical path:**
  1. Logit Extraction: Must be precise; any quantization or approximation that breaks the softmax relationship invalidates the IRL equivalence.
  2. Value Calculation: Computing $V(s') = \alpha \log \sum \exp(\dots)$ requires handling numerical stability (log-softmax trick) across the vocabulary.

- **Design tradeoffs:**
  - Temperature ($\alpha$): High temperature smooths rewards (more exploration/entropy), low temperature sharpens them. Must match base model training if possible.
  - Discounting ($\gamma$): The paper experiments with discount factors for the outcome reward ($\gamma=0.95$), balancing immediate token validity vs. long-term structure.

- **Failure signatures:**
  - Reward Hacking: The model generates high-probability gibberish to maximize the log-prob term.
  - Bias Amplification: If the base model has biases, the endogenous reward reinforces them (discussed in Limitations).
  - Iterative Stagnation: Re-extracting reward from the improved model yields no further gain (Theorem 3), so do not set up infinite self-distillation loops.

- **First 3 experiments:**
  1. Validation of IRL Link: Extract rewards from a small pre-trained model on a simple dataset (e.g., arithmetic). Verify that high-reward trajectories correspond to high-likelihood ground truth.
  2. Ablation on Outcome Reward: Implement Eq. (12) with and without the value term correction $V(s_1)$ to isolate the impact of the "shaping" potential.
  3. RL Fine-Tuning Loop: Train a policy on MATH-lighteval (as in paper) using the EndoRM. Monitor for the "compounding error" reduction (e.g., does accuracy on longer chains improve relative to SFT?).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can hybrid approaches utilizing sparse external feedback or rule-based systems effectively mitigate the risk of self-reinforcing biases and hallucinations inherent to endogenous reward optimization?
- Basis in paper: Section 7.5 (Limitations) states, "Future work should explore hybrid approaches... sparse but high-quality feedback from humans or a rule-based system could be used to provide periodic corrections."
- Why unresolved: The paper theoretically proves the endogenous reward is effective for optimization (Theorem 2), but explicitly warns that relying solely on the model's internal worldview (logits) risks the model "rewarding itself" for outputs that are consistent with training data biases but factually incorrect or harmful.
- What evidence would resolve it: Empirical results from an ablation study comparing pure EndoRM fine-tuning against EndoRM combined with sparse human intervention on safety benchmarks (e.g., TruthfulQA), showing a reduction in hallucination rates without sacrificing the efficiency of the training-free method.

### Open Question 2
- Question: Does the theoretical equivalence between next-token prediction and offline inverse reinforcement learning hold for continuous or vector-quantized token spaces in multi-modal generation?
- Basis in paper: Section 7.4 claims the mechanism is applicable to "image generation, video synthesis, and music composition" via auto-regressive structures.
- Why unresolved: The theoretical derivation in Section 3 relies on a finite vocabulary $\mathcal{V}$ and the softmax function over discrete tokens (Eq. 8). It is unclear if the Inverse Soft Bellman Operator (Eq. 7) can be approximated or re-derived for the continuous latent spaces or codebooks used in modern video/audio generation models.
- What evidence would resolve it: A formal extension of Proposition 1 to continuous action spaces, or empirical validation showing that extracting rewards from a multi-modal model (e.g., a video transformer) successfully aligns generation with text prompts without explicit reward modeling.

### Open Question 3
- Question: Can the "ineffectiveness of iterative improvement" be overcome by modifying the reference policy or potential function to enable continuous self-play?
- Basis in paper: Section 4.3 proves that iterative improvement fails because the policy $\pi_{RL}$ becomes the optimal policy for its own endogenous reward, causing convergence. However, the discussion in Section 7 implies a desire for a "paradigm shift" towards continuous improvement.
- Why unresolved: The current theoretical framework suggests a "one-shot" improvement (from SFT to RL). To achieve "AlphaGo-like" recursive self-improvement, a mechanism is needed to prevent the policy from becoming a fixed point immediately after the first RL step.
- What evidence would resolve it: A theoretical modification to the reward extraction or KL penalty framework (e.g., updating the KL reference model to a previous iteration) that allows the error bound to tighten over multiple steps, validated by multi-step RL experiments showing continued performance gains.

## Limitations

- **Temperature Calibration**: The framework critically depends on the entropy coefficient α, but the paper does not specify how to recover this from the base model, which could significantly impact reward quality.
- **Bias Amplification**: The extracted reward may amplify existing biases in the base model since it relies on the model's internal worldview rather than external feedback.
- **Computational Complexity**: The log-sum-exp operation requires processing the entire vocabulary at each token position, creating significant computational overhead compared to pre-trained reward models.

## Confidence

**High Confidence Claims**:
- The theoretical equivalence between next-token prediction and MaxEnt IRL (Section 3.2) is mathematically sound and well-supported by Proposition 1.
- The error bound improvement (O(H) vs O(H²)) follows standard RL theory and is theoretically rigorous (Theorem 2).
- Experimental results on RM-Bench and Multifaceted-Bench demonstrate clear improvements over baseline methods.

**Medium Confidence Claims**:
- The extraction mechanism works as a general solution for any LLM (the "generalist" claim). While theoretically supported, practical success depends heavily on temperature calibration and base model quality.
- The self-improvement results on mathematical reasoning tasks, while promising, use a specific evaluation protocol (MATH-lighteval) that may not generalize to all reasoning domains.

**Low Confidence Claims**:
- The assertion that this approach replaces the need for explicitly trained reward models entirely. The framework still requires significant computation during inference and may not scale to extremely large models or complex preference structures.

## Next Checks

1. **Temperature Sensitivity Analysis**: Systematically vary α across multiple orders of magnitude for different base models (Qwen2.5-7B, Llama-3-8B, etc.) and measure the impact on reward quality and downstream performance. This would clarify whether the method truly generalizes or requires careful per-model calibration.

2. **Bias Amplification Test**: Take a base model with known biases (e.g., on controversial topics or specific reasoning patterns) and measure whether the endogenous reward amplifies these biases compared to a separately trained reward model. This directly addresses the stated limitation about bias reinforcement.

3. **Computational Overhead Benchmark**: Compare wall-clock time and memory usage of endogenous reward extraction versus a distilled reward model on the same hardware. This would quantify the practical cost of avoiding reward model training and help determine when the approach is computationally viable.