---
ver: rpa2
title: Partially Observable Monte-Carlo Graph Search
arxiv_id: '2507.20951'
source_url: https://arxiv.org/abs/2507.20951
tags:
- pomcgs
- pomdps
- belief
- pomdp
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents POMCGS, a new offline POMDP solver that addresses
  scalability limitations of previous methods by constructing a policy graph (finite-state
  controller) through Monte-Carlo simulations. The algorithm folds a search tree on-the-fly
  to build a compact policy that can be executed without further computation, enabling
  use in time/energy-constrained applications.
---

# Partially Observable Monte-Carlo Graph Search

## Quick Facts
- arXiv ID: 2507.20951
- Source URL: https://arxiv.org/abs/2507.20951
- Reference count: 7
- Primary result: New offline POMDP solver using Monte-Carlo simulations to construct compact policy graphs that outperform previous methods on continuous and large domains

## Executive Summary
POMCGS is an offline POMDP solver that addresses scalability limitations by constructing a policy graph through Monte-Carlo simulations. The algorithm folds a search tree on-the-fly to build a compact Finite State Controller (FSC) that can be executed without further computation, enabling use in time/energy-constrained applications. POMCGS incorporates action progressive widening and observation clustering to handle continuous POMDPs, achieving competitive performance to state-of-the-art online planners while generating executable offline policies.

## Method Summary
POMCGS uses Monte-Carlo simulations to build a policy graph (Finite State Controller) by folding a search tree on-the-fly. The algorithm detects when simulations reach similar belief states and merges them into existing graph nodes, creating a compact representation. It handles continuous action spaces via Action Progressive Widening and continuous observation spaces via K-means clustering. Before searching, it solves the underlying MDP to provide an optimistic upper-bounding value function. The search continues until the gap between upper and lower bounds falls below a threshold, producing an executable offline policy.

## Key Results
- Generates policies that cannot be computed by previous offline algorithms
- Achieves near-optimal results on small problems (RockSample)
- Maintains strong performance on large and continuous domains (Light Dark)
- Performance competitive to state-of-the-art online planners (POMCP)

## Why This Works (Mechanism)

### Mechanism 1: Graph Construction via Belief Merging (Tree Folding)
POMCGS converts a search tree into a policy graph by merging similar belief states during simulation. When a simulation reaches a belief state similar (L1 distance ≤ ξ) to an existing graph node, it folds the trajectory into that node, creating loops in the policy structure. This allows a finite graph to represent an infinite-horizon policy, based on the assumption that optimal value functions are continuous in belief space.

### Mechanism 2: Continuous Observation Discretization via Clustering
For continuous observations, POMCGS uses K-means clustering to group observation-state pairs into discrete labels. Instead of branching on raw continuous observations, the policy graph edges are determined by cluster labels, creating a finite branching factor. This enables handling of continuous observation spaces while preserving decision-relevant information.

### Mechanism 3: MDP Heuristic Initialization
POMCGS pre-computes the value function of the underlying fully observable MDP to provide an optimistic upper bound. When creating new FSC nodes, their values are initialized using this MDP heuristic rather than zero. This guides the search and prevents value over-estimation, assuming the fully observable problem provides a useful signal for the partially observable problem.

## Foundational Learning

- **Finite State Controllers (FSC) / Policy Graphs**: Compact policy representations where nodes represent memory states and edges represent transitions based on observations. Needed because POMCGS outputs an FSC rather than a value function table. Quick check: How does a loop in a policy graph help an agent remember information over time without an infinite tree?

- **Progressive Widening**: Technique to handle continuous action spaces by initially sampling few actions and gradually adding new random actions as nodes are visited more often. Needed to limit branching factor in continuous spaces. Quick check: Why is it necessary to limit the branching factor of a node in a continuous space, and how does the visit count N(n) control this?

- **Particle Filters (Belief Approximation)**: Method to represent beliefs as unweighted particle sets for efficient computation. Needed because POMCGS represents beliefs as particles and uses them for merging and clustering. Quick check: How does the number of particles (nbparticles) affect the accuracy of the norm-1 distance calculation used for merging?

## Architecture Onboarding

- **Component map**: Simulator (Black Box) -> FSC Graph Store -> Search Controller -> Evaluator
- **Critical path**: 1. Compute V_MDP via Q-learning, 2. Run n_bsim=10^3 simulations using UCB selection, 3. Generate particles via ProcessAction, 4. Cluster observations via K-Means, 5. Merge beliefs via SearchOrInsert, 6. Backpropagate Q-value updates, 7. Estimate bounds via n_beval=10^5 simulations until convergence
- **Design tradeoffs**: Parameter ξ (Merge Threshold) - High ξ leads to smaller graphs but risks information loss; Parameter K (Clusters) - High K offers finer resolution but increases graph width
- **Failure signatures**: Stagnation in High-Dim Observations (poor lower bound on Laser Tag), Incomplete Policy (hitting leaf nodes with N(n)=0 triggering blind policy fallback)
- **First 3 experiments**: 1. RockSample(7,8) Sanity Check - Verify basic correctness on discrete benchmark, 2. Ablation on ξ - Run with ξ=0.1 vs ξ=0.5 to observe trade-off between FSC size and performance, 3. Continuous Stress Test (Light Dark) - Validate K-means clustering mechanism for observation discretization

## Open Questions the Paper Calls Out

1. Can formal convergence guarantees be established for POMCGS? The discussion section explicitly states proving convergence remains an open question due to the algorithm's structure differing from standard tree-based methods. The algorithm operates on a Particle Belief MDP but merges nodes into an FSC rather than a tree, breaking standard assumptions used in online convergence proofs.

2. How can the observation clustering process be improved to handle high-dimensional observation spaces? The authors identify handling high-dimensional observation spaces (e.g., Laser Tag) as a notable limitation and propose incorporating dimensionality reduction techniques as future work. The current K-means clustering struggles to create meaningful observation groups in high dimensions.

3. Can more effective belief comparison metrics be developed to avoid state discretization in continuous domains? The authors suggest exploring how to avoid state discretization by using more effective belief comparison metrics. The current method relies on discretizing continuous states to calculate norm-1 distance, which may introduce approximation errors.

## Limitations
- High-dimensional observation spaces (e.g., 8D in Laser Tag) challenge the K-means clustering approach
- Belief merging threshold ξ requires careful tuning to balance compactness vs information preservation
- Performance depends on the similarity between the underlying MDP and POMDP dynamics

## Confidence
- **High Confidence**: Can solve RockSample(7,8) to near-optimal levels and outperforms POMCP on several benchmarks as an offline planner
- **Medium Confidence**: Successfully handles continuous action spaces via APW and continuous observation spaces via K-means clustering in tested domains
- **Low Confidence**: Scales reliably to high-dimensional observation spaces or maintains robustness when MDP and POMDP dynamics differ significantly

## Next Checks
1. **Belief Merging Sensitivity Analysis**: Systematically vary the merge threshold ξ on RockSample and measure the trade-off between FSC size and solution quality to quantify information loss from merging

2. **High-Dimensional Observation Test**: Implement POMCGS on the Laser Tag domain and compare performance against POMCP to isolate whether observation clustering or belief merging is the primary bottleneck

3. **MDP Heuristic Quality Evaluation**: Compute the gap between V_MDP and the true POMDP value function on a small test problem to measure how much the heuristic initialization biases the search