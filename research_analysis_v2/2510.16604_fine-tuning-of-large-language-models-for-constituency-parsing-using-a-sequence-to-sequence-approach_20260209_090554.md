---
ver: rpa2
title: Fine-tuning of Large Language Models for Constituency Parsing Using a Sequence
  to Sequence Approach
arxiv_id: '2510.16604'
source_url: https://arxiv.org/abs/2510.16604
tags:
- language
- large
- parsing
- work
- corpus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explored fine-tuning large language models for constituency
  parsing of Spanish sentences via sequence-to-sequence translation. Models from Hugging
  Face (GPT-2 and Bloom variants) were fine-tuned on a corpus adapted from AnCora-ES
  to generate syntactic tree structures in Penn Treebank-like notation.
---

# Fine-tuning of Large Language Models for Constituency Parsing Using a Sequence to Sequence Approach

## Quick Facts
- arXiv ID: 2510.16604
- Source URL: https://arxiv.org/abs/2510.16604
- Authors: Francisco Jose Cortes Delgado; Eduardo Martinez Gracia; Rafael Valencia Garcia
- Reference count: 21
- GPT-2-large achieved highest F1 score (0.8141) for Spanish constituency parsing

## Executive Summary
This work demonstrates that large language models can be effectively fine-tuned for constituency parsing of Spanish sentences using a sequence-to-sequence approach. The researchers adapted the AnCora-ES corpus to generate syntactic tree structures in Penn Treebank-like notation and fine-tuned GPT-2 and Bloom variants from Hugging Face. The GPT-2-large model achieved the highest F1 score (0.8141) with acceptable inference time, while Bloom-560m offered similar accuracy with faster processing. Results confirm that LLMs can effectively learn Spanish syntactic structure, demonstrating viability for educational parsing tools like MiSintaxis.

## Method Summary
The researchers fine-tuned large language models (GPT-2 and Bloom variants) on a corpus adapted from AnCora-ES to translate input Spanish sentences into their corresponding syntactic structures using Penn Treebank-like bracketed notation. The corpus was preprocessed from XML format to bracketed syntactic representation, with GPT-2 models limited to sentences under 512 tokens (15,035 sentences) while Bloom models used the full 17,300-sentence corpus. Models were fine-tuned for 5 epochs on NVIDIA A100 40GB VRAM, with F1 score and inference time as evaluation metrics.

## Key Results
- GPT-2-large achieved highest F1 score of 0.8141 for Spanish constituency parsing
- Bloom-560m provided similar accuracy (F1=0.7963) with faster processing time (2.99s vs 5.25s)
- Larger models don't guarantee better performance (bloom-1b1 underperformed bloom-560m)
- Sequence-to-sequence approach successfully reformulates constituency parsing as translation task

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Constituency parsing can be reformulated as sequence-to-sequence translation, where a sentence maps to its bracketed syntactic representation.
- **Mechanism:** The model learns to generate output tokens (brackets, labels, words) conditioned on the input sequence, treating syntactic structure as a target "language" rather than applying discrete grammar rules.
- **Core assumption:** Syntactic tree structures are learnable as regularities in token sequences, not requiring explicit symbolic grammar encoding.
- **Evidence anchors:**
  - [abstract] "fine-tuning large language models (LLMs) to translate an input sentence into its corresponding syntactic structure"
  - [section 2] "[4] proposed that constituency parsing can be approached similarly to language translation, using a sequence-to-sequence framework"
  - [corpus] Related work (Vinyals et al.) supports seq2seq parsing; no direct corpus evidence for Spanish-specific transfer
- **Break condition:** Highly ambiguous sentences with multiple valid parses may produce inconsistent outputs (see Figure 2b showing indicative vs. imperative confusion).

### Mechanism 2
- **Claim:** Self-attention enables global context capture for syntactic structure learning.
- **Mechanism:** Attention weights allow the model to relate distant tokens, supporting long-range syntactic dependencies without recurrent processing limitations.
- **Core assumption:** Transformer attention patterns encode syntactically-relevant relationships during pre-training that transfer to parsing.
- **Evidence anchors:**
  - [section 2] "attention mechanisms...appear to learn the syntactic categories of language, as suggested by [3]"
  - [section 2] "replacing [LSTM] with a transformer architecture employing self-attention, enabling the model to capture global context"
  - [corpus] Kitaev & Klein (2018) demonstrated self-attentive encoders improve constituency parsing for English; Spanish transfer assumed
- **Break condition:** Sentences exceeding token limits (GPT-2: 512 tokens) require truncation or exclusion, potentially biasing evaluation.

### Mechanism 3
- **Claim:** Fine-tuning on task-specific corpora improves parsing accuracy over raw pre-trained models.
- **Mechanism:** Domain-specific gradient updates adjust weights to prioritize syntactic structure generation over general language modeling objectives.
- **Core assumption:** Pre-trained linguistic knowledge is retained while task-specific patterns are acquired without catastrophic forgetting.
- **Evidence anchors:**
  - [section 2] "large models must be fine-tuned to increase their effectiveness in specific application domains"
  - [section 3] Loss curves show convergence across 5 epochs (Figure 1); final loss values range 0.0175–0.0472
  - [corpus] General fine-tuning principle well-established; no corpus evidence for Spanish constituency parsing specifically
- **Break condition:** Overfitting may occur with excessive epochs; larger models (bloom-1b1) showed slightly lower F1 than bloom-560m, suggesting parameter count alone is insufficient.

## Foundational Learning

- **Concept: Constituency Parsing vs. Dependency Parsing**
  - **Why needed here:** The paper focuses on phrase-structure (constituency) trees, not dependency graphs. Misunderstanding this leads to wrong output format expectations.
  - **Quick check question:** Does the output represent nested phrases (e.g., `[NP [Det The] [N cat]]`) or head-dependent arcs?

- **Concept: Penn Treebank Notation**
  - **Why needed here:** The target format uses bracketed labels (e.g., `NP/S`, `VP/PV`) that the model must generate token-by-token.
  - **Quick check question:** Can you explain what `[PP/CN [P of] [NP/T [N cup]]]` represents structurally?

- **Concept: Token Limits and Context Windows**
  - **Why needed here:** GPT-2's 512-token limit excluded ~2,265 sentences from training, affecting corpus coverage and model comparison fairness.
  - **Quick check question:** If a sentence has 800 tokens, what happens when processed by gpt2-large-bne?

## Architecture Onboarding

- **Component map:**
  - Input: Raw Spanish sentence (tokenized)
  - Encoder-Decoder: Fine-tuned transformer (GPT-2 or Bloom)
  - Output: Bracketed syntactic string → Post-process to tree visualization
  - Training: AnCora-ES corpus (17,300 sentences, XML → Penn Treebank format)

- **Critical path:**
  1. Corpus preprocessing (XML → bracket notation, parentheses → square brackets)
  2. Tokenization alignment (verify model tokenizer handles brackets as single tokens)
  3. Fine-tuning (5 epochs, monitor loss convergence)
  4. Inference (sentence → bracketed string → parse tree)
  5. Evaluation (F1 against gold annotations)

- **Design tradeoffs:**
  - Accuracy vs. Speed: gpt2-large-bne (F1=0.8141, 5.25s) vs. bloom-560m (F1=0.7963, 2.99s)
  - Coverage vs. Compatibility: Bloom's 2048-token window uses full corpus; GPT-2's 512-token limit excludes long sentences
  - Model size vs. diminishing returns: bloom-1b1 (1B params) underperformed bloom-560m

- **Failure signatures:**
  - Ambiguity collapse: Models struggle with syntactically ambiguous sentences (Figure 2b: indicative/imperative confusion)
  - Out-of-distribution inputs: Sentences with linguistic constructs not in AnCora-ES (e.g., "Complemento Circunstancial de Compañía") will lack proper labels
  - Token overflow: Sentences >512 tokens silently fail or truncate for GPT-2 models

- **First 3 experiments:**
  1. Reproduce the F1 comparison between gpt2-large-bne and bloom-560m on the 512-token-limited test split to validate reported metrics.
  2. Test on sentences containing linguistic constructs absent from AnCora-ES to characterize out-of-distribution behavior.
  3. Ablate epoch count (3, 5, 7 epochs) to confirm 5 epochs avoids overfitting as claimed.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can combining fine-tuned LLMs with the CYK algorithm improve constituency parsing accuracy compared to the sequence-to-sequence approach alone?
- **Basis in paper:** [explicit] The conclusion states, "Future research could explore alternative methods, such as combining large language models with the CYK algorithm."
- **Why unresolved:** The current study only evaluated standalone seq2seq models without integrating traditional dynamic programming constraints.
- **What evidence would resolve it:** A benchmark comparison showing F1 scores of a hybrid LLM-CYK model against the standalone GPT-2-large baseline.

### Open Question 2
- **Question:** Does augmenting the training corpus with pre-university level sentences and missing syntactic labels (e.g., "Complemento Circunstancial de Compañía") improve parsing performance in educational contexts?
- **Basis in paper:** [explicit] The authors note the need to "enrich the corpus with additional sentences... close to pre-university syntax learning" and add missing linguistic components.
- **Why unresolved:** The current AnCora-ES corpus is based on newspaper articles and lacks labels for specific grammatical structures introduced in modern Spanish grammar.
- **What evidence would resolve it:** Evaluation results on a specialized test set containing the missing labels before and after fine-tuning on the enriched dataset.

### Open Question 3
- **Question:** Can training be extended beyond five epochs to achieve lower loss without inducing overfitting?
- **Basis in paper:** [explicit] The methodology mentions, "Five epochs were used to avoid overfitting. However, future work will explore whether further optimization is possible."
- **Why unresolved:** The authors stopped training early as a precaution, leaving the actual overfitting threshold undetermined.
- **What evidence would resolve it:** Training curves and validation metrics tracked over a larger number of epochs to identify the optimal stopping point.

## Limitations

- Limited transparency on preprocessing: Exact AnCora-ES XML-to-bracket conversion rules are unspecified
- Token limit confounding: GPT-2's 512-token constraint excludes ~2,265 sentences, affecting fair comparison
- Missing hyperparameters: Training parameters (learning rate, batch size, optimizer) not specified
- Ambiguity handling weakness: Model struggles with structurally ambiguous sentences, conflating imperative and indicative forms
- Out-of-distribution untested: Performance on linguistic constructs absent from AnCora-ES remains unknown

## Confidence

- **High confidence** in the core claim that fine-tuning LLMs for constituency parsing is feasible, supported by convergent results across multiple model sizes and the well-established seq2seq parsing framework.
- **Medium confidence** in comparative performance metrics due to unreported details on test set alignment and preprocessing consistency.
- **Low confidence** in cross-linguistic generalizability, as the study is confined to Spanish and does not benchmark against multilingual or dependency parsing approaches.

## Next Checks

1. **Reproduce F1 comparison on restricted test split**: Fine-tune gpt2-large-bne and bloom-560m on only the 512-token subset (15,035 sentences) and compare F1 scores to validate the reported performance gap while controlling for coverage bias.

2. **Test out-of-distribution robustness**: Evaluate models on Spanish sentences containing syntactic constructs absent from AnCora-ES (e.g., "Complemento Circunstancial de Compañía") to measure label generalization and identify failure patterns.

3. **Ablate training epochs**: Train models for 3, 5, and 7 epochs to confirm that 5 epochs balance convergence and overfitting, especially given the bloom-1b1 underperformance suggesting parameter count alone is insufficient.