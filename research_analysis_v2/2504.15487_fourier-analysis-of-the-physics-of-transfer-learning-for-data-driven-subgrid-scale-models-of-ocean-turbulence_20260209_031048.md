---
ver: rpa2
title: Fourier analysis of the physics of transfer learning for data-driven subgrid-scale
  models of ocean turbulence
arxiv_id: '2504.15487'
source_url: https://arxiv.org/abs/2504.15487
tags:
- data
- learning
- http
- issn
- spectral
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates transfer learning (TL) for neural network
  (NN)-based subgrid-scale (SGS) parameterization of ocean turbulence using a 9-layer
  CNN. The NN predicts subgrid forcing in a two-layer quasi-geostrophic ocean model.
---

# Fourier analysis of the physics of transfer learning for data-driven subgrid-scale models of ocean turbulence

## Quick Facts
- arXiv ID: 2504.15487
- Source URL: https://arxiv.org/abs/2504.15487
- Reference count: 40
- Primary result: Transfer learning with single-layer retraining corrects systematic spectral underestimation in neural network subgrid-scale parameterization of ocean turbulence

## Executive Summary
This study investigates transfer learning for neural network-based subgrid-scale parameterization of ocean turbulence using a 9-layer CNN. The research reveals that CNNs trained on one dynamical regime systematically underestimate output spectra when applied to out-of-distribution data, causing generalization failure. Fourier analysis shows the learned kernels converge to fundamental filter types (low-pass, Gabor, high-pass) regardless of training data isotropy. The key finding is that re-training only one layer with target system data corrects this spectral underestimation, enabling accurate predictions across different ocean turbulence regimes with minimal additional data.

## Method Summary
The study uses a two-layer quasi-geostrophic ocean model implemented via the `pyqg` library to generate high-resolution (256×256) velocity fields, which are coarse-grained to 64×64 using a Gaussian filter. Subgrid forcing is computed from the difference between high-res and coarse solutions. A 9-layer CNN with 64 channels per hidden layer (5×5 kernels) is trained to predict subgrid forcing from coarse velocity inputs. Transfer learning is implemented by initializing with pre-trained weights and re-training only the second convolutional layer using 2-10% of target case data. Performance is evaluated using RMSE, correlation coefficient, and spectrum RMSE metrics.

## Key Results
- CNNs trained on one ocean turbulence regime systematically underestimate output spectra when applied to different regimes
- Re-training only one layer with target data corrects spectral underestimation and improves generalization
- CNN kernels consistently learn low-pass, Gabor, and high-pass filters regardless of training data isotropy
- Transfer learning with just 2-10% of target data produces predictions matching target spectra

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A CNN trained on one dynamical regime fails to generalize to another because its learned weights and biases systematically underestimate the activation spectra of out-of-distribution data, with this error propagating and amplifying through the network's layers.
- Mechanism:
    1. A CNN is trained on source data, and its weights and biases become tuned to that system's spectral characteristics.
    2. When applied to target data with different spectral properties, these pre-trained weights and biases interact with the new input spectra.
    3. This interaction causes a layer-wise underestimation of the activation spectra, which begins in early layers and accumulates as data passes through the network.
    4. The final output spectrum is consequently mismatched (underestimated) compared to the true target, causing poor performance.
- Core assumption: The primary generalization failure mode is systematic spectral attenuation, not spatial misalignment.
- Evidence anchors:
  - [abstract] "...the learned weights and biases from one dataset underestimate the out-of-distribution sample spectra as they pass through the network, leading to an underestimation of output spectra."
  - [page 10, section 3.2] "...underestimation of the spectral content is evident across all layers... This underestimation begins in early layers and propagates through the network, ultimately resulting in a mismatch..."
- Break condition: If the output spectrum of a transferred model is consistently overestimated or shows a different error pattern (e.g., spatial aliasing), this mechanism would not apply.

### Mechanism 2
- Claim: Re-training a single layer with a small amount of target data corrects the spectral underestimation by modifying the kernel weight spectra, shifting the network's frequency response to align with the new dynamical regime.
- Mechanism:
    1. Transfer learning is performed by re-training only the weights and biases of one hidden layer (e.g., layer 2) using a fraction of target data.
    2. This process modifies the Fourier spectra of the kernels in that layer, which function as spectral filters.
    3. Retraining adjusts these filters in two ways: (a) by increasing the spectral amplitude at dominant wavenumbers (counteracting underestimation) and (b) by shifting some filter maxima toward lower wavenumbers to better capture the large-scale features of the new system.
    4. These localized changes recalibrate the spectral response for the entire network, enabling the final output to match the target spectrum.
- Core assumption: A single layer's adaptation is sufficient to recalibrate the network's spectral response.
- Evidence anchors:
  - [abstract] "By re-training only one layer with data from the target system, this underestimation is corrected, enabling the NN to produce predictions that match the target spectra."
  - [page 11, section 3.3] "...this amplitude increases consistently, aligning with the upshift in the activation spectra..." and "...TL shifts many of these maxima toward lower wavenumbers, indicating a stronger focus on large-scale features."
- Break condition: If re-training a single layer fails to correct the output spectrum, this mechanism is insufficient.

### Mechanism 3
- Claim: Convolutional kernels in CNNs trained for ocean turbulence SGS parameterization converge to fundamental spectral filter types (low-pass, Gabor, high-pass) regardless of the isotropy or anisotropy of the training data.
- Mechanism:
    1. The CNN learns to represent subgrid forcing by applying learned convolutional filters to input fields.
    2. Fourier analysis of the trained weight matrices reveals their function as spectral operations.
    3. Clustering analysis of the Fourier-transformed kernels shows they consistently decompose into low-pass, high-pass, and Gabor filters.
    4. While the distribution and amplitude of these filters adapt to the data (per Mechanism 2), the fundamental types are an intrinsic property of the architecture and task, enabling multi-scale processing.
- Core assumption: The identified filter types represent canonical operations for this class of problems.
- Evidence anchors:
  - [abstract] "Fourier analysis of the NN kernels reveals that they learn low-pass, Gabor, and high-pass filters, regardless of whether the training data are isotropic or anisotropic."
  - [page 10, section 3.3] "As shown in Fig. 6, the cluster centers across all four cases consistently represent combinations of low-pass, Gabor, and high-pass filters."
- Break condition: If a CNN trained on a different physical system learns entirely different filter types, the claim of universality for this task would be weakened.

## Foundational Learning
- Concept: **Spectral Analysis (Fourier Transform)**
  - Why needed here: The paper's core diagnostic is analyzing the model and data in the frequency domain (spectra) to understand performance and generalization. You must understand how spatial data translates to spectral content.
  - Quick check question: Given a 2D spatial field of ocean velocity, what would its Fourier spectrum represent, and what might a "low-pass filter" do to it?
- Concept: **Transfer Learning (TL)**
  - Why needed here: The study's central technique is adapting a pre-trained model to a new system with minimal data. Understanding the difference between training from scratch and fine-tuning is essential.
  - Quick check question: Instead of randomly initializing weights for a new task, what weights do you start with in transfer learning, and which ones do you typically update?
- Concept: **Subgrid-Scale (SGS) Parameterization**
  - Why needed here: This is the physical problem being solved. It involves modeling the effect of unresolved small-scale turbulent processes on large-scale ocean dynamics.
  - Quick check question: Why can't we simply ignore the small-scale eddies in a coarse ocean model, and what is the role of an SGS model?

## Architecture Onboarding
- Component map: Input (2-channel low-res velocity) -> Layer 1 (64 channels, 5×5 kernel) -> ReLU -> Layer 2 (64 channels, 5×5 kernel) -> ... -> Layer 9 (linear, 2 channels) -> Output (2-channel subgrid forcing)
- Critical path: The flow of spectral information is critical. Input spectrum -> processed by layer 1 kernels -> ReLU -> layer 2 activations (crucial adaptation point) -> ... -> output spectrum. The "Spectrum RMSE" metric directly measures success along this path.
- Design tradeoffs:
  - **Single-layer retraining vs. full fine-tuning**: The paper demonstrates efficiency by retraining only one layer (layer 2), but this may limit performance if the target system is extremely different. Retraining more layers could improve accuracy at the cost of data and computation.
  - **Architecture Depth**: A 9-layer network is deep enough to learn complex mappings but risks the "vanishing gradient" or accumulated spectral error issues that the paper analyzes.
  - **Offline vs. Online Metrics**: Training is supervised (offline) on pre-computed SGS forcing, but the ultimate goal is stable, accurate coupled simulation (online). A model can have low offline error but cause online instability.
- Failure signatures:
  - **Spectral Mismatch / Underestimation**: If the output spectrum is significantly lower than the true spectrum, the model has not generalized.
  - **High Spectrum RMSE**: This metric is a more sensitive indicator of generalization failure than standard RMSE or correlation.
  - **Divergent Online Simulation**: If the coupled model becomes numerically unstable or produces unphysical PDFs, the parameterization is failing.
- First 3 experiments:
  1. **Baseline Training**: Train a 9-layer CNN from scratch on the "base" ocean turbulence case (e.g., eddy configuration) using the provided architecture and optimizer settings. Evaluate its performance on held-out data from the same case using RMSE, Correlation Coefficient, and Spectrum RMSE.
  2. **Generalization Test**: Take the trained model from experiment 1 and evaluate it directly (zero-shot) on data from a different "target" case (e.g., jet configuration). Observe the degradation in metrics, especially the Spectrum RMSE and the ratio of output-to-true spectra, to confirm the generalization failure described in the paper.
  3. **Transfer Learning Fix**: Initialize a new model with the weights from experiment 1. Re-train only the second convolutional layer using a small percentage (e.g., 2-10%) of data from the target case. Compare the performance of this transfer-learned model against the baseline from experiment 2 to verify the correction of the spectral underestimation.

## Open Questions the Paper Calls Out
- **Open Question 1**: How do kernels with multiple significant local spectral maxima influence the transfer learning process compared to those with a single global maximum?
  - Basis in paper: The discussion states that the current analysis assumes filters have a single global maximum, but acknowledges some kernels exhibit multiple local maxima requiring "more detailed studies."
  - Why unresolved: The study simplified the spectral analysis by focusing on the dominant wavenumber pair, leaving the impact of secondary peaks on the spectral underestimation mechanism unexplored.
  - What evidence would resolve it: A comparative ablation study isolating the effects of multi-modal kernels on activation spectra propagation and output reconstruction accuracy.

- **Open Question 2**: To what extent does accounting for kernel directionality, rather than just radial wavenumber, improve the analysis of adaptation in anisotropic flow regimes?
  - Basis in paper: The authors note that using radial wavenumber to analyze kernel spectra distribution "may overlook directionality, necessitating further research to address this limitation."
  - Why unresolved: The current methodology aggregates spectral data using radial wavenumber, which discards orientation data crucial for distinguishing physical mechanisms in the jet (anisotropic) configurations.
  - What evidence would resolve it: A directional spectral analysis retaining orientation to correlate specific kernel rotations with the alignment of anisotropic jets in the target system.

- **Open Question 3**: Can the identified spectral characteristics of effective filters be used for physics-informed weight initialization to bypass the need for large training datasets?
  - Basis in paper: The introduction posits that understanding kernel mechanisms "could reduce training costs and lessen dependence on large volumes of high-fidelity data by enabling more physics-informed initialization."
  - Why unresolved: This study focuses on post-hoc analysis of trained weights; it does not demonstrate the reverse process of initializing networks with theoretical filters to improve data efficiency.
  - What evidence would resolve it: Experiments initializing CNNs with low-pass, Gabor, and high-pass filters based on theoretical spectral properties, comparing convergence speed and data requirements against random initialization.

## Limitations
- The study focuses on a specific CNN architecture (9-layer, 64 channels) and training protocol, leaving the universality of the spectral underestimation mechanism across different architectures untested.
- The paper demonstrates successful TL with 2-10% target data but does not explore the minimum data threshold or characterize failure conditions when base and target systems are too dissimilar.
- The claim that single-layer retraining is sufficient may not hold for more extreme dynamical regime shifts between base and target systems.

## Confidence
- **High Confidence**: The existence of spectral underestimation in cross-regime generalization (supported by multiple quantitative metrics and visual evidence across layers)
- **Medium Confidence**: The mechanism that this underestimation is the primary cause of poor generalization (plausible but not definitively proven as the sole failure mode)
- **Medium Confidence**: That TL with single-layer retraining consistently corrects this underestimation (demonstrated but with limited architectural variation)
- **Low Confidence**: The universality of the canonical filter types (low-pass, Gabor, high-pass) across all possible SGS parameterization tasks

## Next Checks
1. Test the TL mechanism with different CNN architectures (varying depth, filter sizes, activation functions) to determine if the spectral underestimation mechanism and single-layer fix are architecture-dependent or universal properties.

2. Conduct ablation studies on which layer to retrain—systematically test re-training layers 1, 3, and 4 instead of layer 2 to quantify the sensitivity of the TL performance to which layer is adapted.

3. Perform online coupling experiments where the TL-improved SGS model is integrated into the full two-layer QG model simulation, measuring not just spectral accuracy but also stability, energy conservation, and prediction skill over extended time horizons.