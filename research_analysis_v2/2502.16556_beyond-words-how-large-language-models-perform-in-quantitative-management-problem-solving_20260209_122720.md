---
ver: rpa2
title: 'Beyond Words: How Large Language Models Perform in Quantitative Management
  Problem-Solving'
arxiv_id: '2502.16556'
source_url: https://arxiv.org/abs/2502.16556
tags:
- performance
- accuracy
- distance
- significant
- binary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated Large Language Models (LLMs) on quantitative
  management decision tasks across 900 observations from 5 models, 20 scenarios, 3
  formats, and 3 iterations. Contrary to expectations, presentation format and text
  length showed no significant effect on accuracy, while scenario complexity (constraints,
  irrelevant parameters) significantly degraded performance.
---

# Beyond Words: How Large Language Models Perform in Quantitative Management Problem-Solving

## Quick Facts
- arXiv ID: 2502.16556
- Source URL: https://arxiv.org/abs/2502.16556
- Reference count: 2
- Primary result: LLMs achieved only 28.8% binary accuracy on quantitative management tasks, with scenario complexity significantly degrading performance

## Executive Summary
This study evaluates Large Language Models (LLMs) on quantitative management decision tasks across 900 observations from 5 models, 20 scenarios, 3 formats, and 3 iterations. Contrary to expectations, presentation format and text length showed no significant effect on accuracy, while scenario complexity (constraints, irrelevant parameters) significantly degraded performance. Binary accuracy was only 28.8%, though models handled multi-step tasks better than expected. No learning effects emerged across iterations. Significant model performance differences were observed, with Claude-3.5-sonnet outperforming others in binary accuracy. These findings suggest LLMs show promise for quantitative management tasks but face limitations in precision and are sensitive to scenario complexity.

## Method Summary
The study used 20 management scenarios with known optimal numerical solutions, each presented in three formats (direct, narrative, tabular). Five LLMs (Claude-3.5-sonnet, GPT-4o, Gemini-1.5-pro, Grok, Llama-3.3-70b) were tested in zero-shot settings via API with standardized prompts requesting only numerical answers. Each scenario-format-model combination was tested three times with 24-hour intervals. Accuracy was measured through binary exact match and logarithmic distance from optimal solutions, with analysis focusing on effects of format, complexity (constraints, solution steps, irrelevant parameters), and iteration.

## Key Results
- Binary accuracy across all models and conditions was only 28.8%
- Scenario complexity significantly degraded performance (p < .001 for irrelevant parameters)
- Multi-step tasks showed unexpectedly positive effects on accuracy
- No significant effects of presentation format or text length on accuracy
- Claude-3.5-sonnet achieved the highest binary accuracy (40.0%) while Llama-3.3-70b performed worst (17.5%)

## Why This Works (Mechanism)

### Mechanism 1: Irrelevant Parameter Interference
- LLMs lack robust filtering for task-irrelevant numerical information; when presented with parameters not needed for correct calculation, models may incorporate them into reasoning chains, leading to suboptimal solutions.
- Core assumption: Models do not reliably distinguish between relevant and irrelevant parameters during zero-shot inference.
- Evidence anchors:
  - "scenario complexity — particularly in terms of constraints and irrelevant parameters — strongly influenced performance, often degrading accuracy"
  - "The number of irrelevant parameters demonstrated the expected negative effect (β = 1.2982, SE = 0.15403, p < .001), strongly supporting H2c"
- Break condition: If models are explicitly instructed to identify and ignore irrelevant parameters before computing, this degradation may diminish.

### Mechanism 2: Format Robustness via Unified Tokenization
- Modern LLMs tokenize and attend to numerical information similarly across formats; the underlying token sequences for key numbers are preserved regardless of surface structure, reducing format-dependent variance.
- Core assumption: Tokenization preserves numerical salience across surface formats; models attend to numbers based on positional and semantic cues, not formatting.
- Evidence anchors:
  - "Contrary to prior findings, we observed no significant effects of text presentation format (direct, narrative, or tabular) or text length on accuracy"
  - "neither narrative (β = 0.0999, SE = 0.43899, p = 0.820) nor tabular formats (β = -0.1287, SE = 0.23410, p = 0.582) significantly affected binary accuracy"
- Break condition: If tasks require parsing of complex table structures or cross-referencing multiple visual elements, format effects may re-emerge.

### Mechanism 3: Multi-Step Reasoning Facilitation
- Multi-step tasks may trigger more explicit chain-of-thought reasoning in LLMs, leading to better intermediate computation and error self-correction compared to single-step estimations.
- Core assumption: Zero-shot models spontaneously decompose multi-step problems into intermediate calculations, improving traceability.
- Evidence anchors:
  - "the models handled tasks requiring multiple solution steps more effectively than expected"
  - "Solution steps showed a significant positive effect on binary accuracy (β = 0.4246, SE = 0.11583, p < .001), contrary to the hypothesized negative relationship"
- Break condition: If step count exceeds a threshold (e.g., 4+ steps with high constraint interaction), performance may degrade non-linearly.

## Foundational Learning

- Concept: Zero-shot prompting
  - Why needed here: The entire experimental design relies on models receiving no examples or task-specific fine-tuning; understanding this baseline is essential for interpreting the 28.8% binary accuracy.
  - Quick check question: If you provided two worked examples before the target problem, would you expect accuracy to increase, decrease, or stay the same? Why?

- Concept: Logarithmic distance as accuracy metric
  - Why needed here: Binary accuracy alone (28.8%) understates model performance; logarithmic distance captures how close approximate answers are to optimal solutions.
  - Quick check question: Two models give answers 10% and 2× off from optimal respectively. Which has the higher logarithmic distance?

- Concept: Non-parametric testing for non-normal distributions
  - Why needed here: The paper uses Kruskal-Wallis and Friedman tests because accuracy distributions are right-skewed; standard t-tests or ANOVA would be inappropriate.
  - Quick check question: If accuracy data were normally distributed, what parametric test would replace Kruskal-Wallis for comparing models?

## Architecture Onboarding

- Component map: 20 scenarios → 3 formats each → 5 models → 3 iterations → accuracy metrics
- Critical path:
  1. Define quantitative scenarios with known optimal solutions
  2. Control for irrelevant parameters and constraint counts
  3. Run zero-shot prompts in isolated sessions (no cross-contamination)
  4. Measure both exact match and proportional deviation
- Design tradeoffs:
  - Zero-shot vs. few-shot: Zero-shot provides cleaner isolation of format/complexity effects but yields lower accuracy
  - Binary vs. continuous metrics: Binary is interpretable but loses granularity; log distance preserves proportional error
  - 20 base scenarios limits variable range but enables controlled replication
- Failure signatures:
  - High constraint count + high irrelevant parameters → accuracy drop (ε² = 0.1078 for log_distance)
  - Repeated iterations without improvement (χ² = 0.0208, p = 0.990) → no self-correction across attempts
  - Tabular format shows non-significant trend toward worse performance (M = 0.305 vs. 0.225 for direct)
- First 3 experiments:
  1. Replicate with explicit "ignore irrelevant information" instruction to test if parameter interference is attention-based
  2. Add few-shot condition (2 examples) to quantify gap between zero-shot and in-context learning
  3. Expand scenario set to 100+ to enable wider constraint/step variation and test non-linear complexity thresholds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does increasing the number of solution steps positively correlate with accuracy in this context?
- Basis in paper: The authors state the "observed positive correlation between solution steps and accuracy is not fully understood and requires further investigation."
- Why unresolved: The finding contradicts hypothesis H2b and prior literature; the study's reliance on only 20 base scenarios prevents identifying the mechanism behind this counter-intuitive result.
- What evidence would resolve it: Controlled experiments analyzing intermediate reasoning traces across a broader set of scenarios to determine if explicit multi-step structures aid model logic.

### Open Question 2
- Question: Can advanced prompting techniques significantly improve the low binary accuracy found in zero-shot settings?
- Basis in paper: The discussion suggests future research "prioritize the exploration of... advanced prompting techniques," noting the 28.8% accuracy rate may be attributed to the zero-shot constraint.
- Why unresolved: The study intentionally excluded advanced prompting to maintain consistency, leaving the potential for accuracy gains via prompt engineering untested.
- What evidence would resolve it: Comparative performance testing of the same scenarios using few-shot or Chain-of-Thought prompting versus the zero-shot baseline.

### Open Question 3
- Question: Does the null effect of presentation format on accuracy persist with greater scenario diversity?
- Basis in paper: The authors note the "constrained diversity of tested scenarios" limited the range of independent variables, potentially masking subtle effects.
- Why unresolved: With only 20 base scenarios, the study may lack sufficient statistical power to detect interactions between specific formats and problem types.
- What evidence would resolve it: Replication of the study using a significantly larger dataset of management scenarios (e.g., >100) to validate the robustness of the format-accuracy relationship.

## Limitations
- The 20 base scenarios may not capture full complexity and variability of real-world quantitative management problems
- Operational definitions of "constraints" and "solution steps" remain unclear, potentially limiting reproducibility
- Focus on exact numerical matching (binary accuracy) may underestimate practical utility where approximate answers suffice
- Lack of few-shot or chain-of-thought prompting conditions leaves questions about whether limitations are inherent to LLMs or specific to zero-shot approach

## Confidence

**High Confidence:** The finding that scenario complexity (constraints, irrelevant parameters) degrades performance is well-supported by statistical analysis (p < .001 for irrelevant parameters). The lack of learning effects across iterations (p = 0.990) and significant model performance differences are also robustly demonstrated.

**Medium Confidence:** The observation that multi-step tasks show positive effects on accuracy warrants caution, as the mechanism is not fully explored and could be sensitive to specific problem types used. The claim about format robustness, while statistically supported, shows a non-significant trend toward worse performance in tabular format that deserves attention.

**Low Confidence:** The assertion that LLMs handle multi-step tasks "more effectively than expected" lacks a clear baseline for comparison, as no prior study specifically examined this relationship in management contexts.

## Next Checks
1. Replicate the study with explicit instructions to "identify and ignore irrelevant information before computing the answer" to determine if parameter interference is attention-based rather than structural.

2. Add a few-shot condition with two worked examples before each target problem to quantify the gap between zero-shot and in-context learning performance.

3. Increase the scenario set to 100+ problems with wider variation in constraint counts and solution steps to test for non-linear complexity thresholds and validate the observed peak at four steps.