---
ver: rpa2
title: 'FERD: Fairness-Enhanced Data-Free Robustness Distillation'
arxiv_id: '2509.20793'
source_url: https://arxiv.org/abs/2509.20793
tags:
- robustness
- adversarial
- robust
- student
- distillation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of robust fairness in data-free
  adversarial robustness distillation (DFRD), where existing methods overlook disparities
  in model robustness across different classes. The authors propose FERD, a framework
  that improves fairness by adjusting the proportion and distribution of adversarial
  examples.
---

# FERD: Fairness-Enhanced Data-Free Robustness Distillation

## Quick Facts
- arXiv ID: 2509.20793
- Source URL: https://arxiv.org/abs/2509.20793
- Authors: Zhengxiao Li; Liming Lu; Xu Zheng; Siyuan Liang; Zhenghan Chen; Yongbin Zhou; Shuchao Pang
- Reference count: 9
- Key outcome: FERD achieves state-of-the-art worst-class robustness in data-free adversarial robustness distillation, improving FGSM and AutoAttack worst-class robustness by 15.1% and 6.4% respectively using MobileNet-V2 on CIFAR-10.

## Executive Summary
This paper addresses fairness in data-free adversarial robustness distillation (DFRD), where existing methods overlook disparities in model robustness across different classes. The authors propose FERD, a framework that improves fairness by adjusting the proportion and distribution of adversarial examples. FERD employs a robustness-guided class reweighting strategy to synthesize more samples for less robust categories and generates Fairness-Aware Examples (FAEs) and Uniform-Target Adversarial Examples (UTAEs) to ensure balanced representation and attack targets. Experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet demonstrate that FERD achieves state-of-the-art worst-class robustness, improving FGSM and AutoAttack worst-class robustness by 15.1% and 6.4% respectively using MobileNet-V2 on CIFAR-10.

## Method Summary
FERD improves fairness in data-free robustness distillation through three key mechanisms. First, it uses a robustness-guided class reweighting strategy that calculates adversarial margins for each class and adjusts synthetic sample proportions to favor less robust categories. Second, it generates Fairness-Aware Examples (FAEs) by suppressing class-specific non-robust features through a uniformity constraint on feature-level predictions. Third, it creates Uniform-Target Adversarial Examples (UTAEs) by enforcing a uniform distribution of attack targets during training. The framework trains a generator to create synthetic data, which is then used to distill robustness from a pre-trained teacher model to a student model, with the goal of achieving balanced worst-class robustness across all categories.

## Key Results
- FERD achieves state-of-the-art worst-class robustness on CIFAR-10, CIFAR-100, and Tiny-ImageNet
- Improves FGSM worst-class robustness by 15.1% and AutoAttack worst-class robustness by 6.4% using MobileNet-V2 on CIFAR-10
- Demonstrates significant improvement in Normalized Standard Deviation (NSD) metric, indicating more balanced class-wise robustness
- Shows consistent gains across different student architectures including ResNet-18 and MobileNet-V2

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Adjusting synthetic sample proportions to favor vulnerable classes improves worst-class robustness.
- **Mechanism:** The framework calculates an "adversarial margin" for each class based on the teacher model's vulnerability. It uses a softmax function on these margins to create a non-uniform sampling distribution $p_c$. This forces the generator to synthesize more data for classes with low robustness, implicitly balancing the training data seen by the student.
- **Core assumption:** Increasing sample count for a vulnerable class directly reduces the class-wise robustness gap.
- **Evidence anchors:** [Abstract] "...adopts a robustness-guided class reweighting strategy to synthesize more samples for the less robust categories..."; [Section: Class-Aware Sample Reweighting] Eq. (1) & (2) define the adversarial margin and the transformation into sampling probability $p_c$.
- **Break condition:** If the student model's capacity is saturated, adding more synthetic samples for weak classes may reduce overall average accuracy without fixing the worst-case robustness.

### Mechanism 2
- **Claim:** Suppressing class-specific non-robust features in synthetic data prevents the dominance of specific adversarial targets.
- **Mechanism:** The generator creates "Fairness-Aware Examples" (FAEs) by identifying "non-robust channels" in the intermediate features using a noise-injection Information Bottleneck approach. It applies a loss term ($L_{uni}$) that forces the KL divergence between the predictions of these non-robust features and a uniform distribution to be low.
- **Core assumption:** Non-robust features are the primary drivers of class-specific adversarial vulnerability.
- **Evidence anchors:** [Abstract] "...enforcing a uniformity constraint on feature-level predictions, which suppress the dominance of class-specific non-robust features..."; [Section: Non-Robust Feature Suppression] Eq. (7) explicitly minimizes $KL(U, f^T_{l+}(Z_{nr}))$.
- **Break condition:** If the "non-robust feature" identification (via variance thresholding in Eq. 6) is inaccurate, the uniformity constraint may inadvertently degrade the semantic content of the synthetic samples.

### Mechanism 3
- **Claim:** Enforcing a uniform distribution of attack targets during training improves defense against diverse attacks.
- **Mechanism:** The framework generates "Uniform-Target Adversarial Examples" (UTAEs) by modifying the standard PGD attack objective. It adds a term ($-\gamma \cdot KL(U, f^T(x^t_U))$) that encourages the adversarial perturbation to push the model's prediction toward a *uniform* distribution rather than a specific wrong class.
- **Core assumption:** Training on adversarial examples with uniformly distributed targets generalizes to defending against specific, targeted attacks.
- **Evidence anchors:** [Abstract] "...applying a uniform target class constraint to avoid biased attack directions..."; [Section: Label-Space Attack Diversification] Eq. (10) modifies the PGD update step with the uniformity constraint.
- **Break condition:** If the constraint weight $\gamma$ is too high, the attack strength is diluted (perturbations become too weak), reducing the effectiveness of robustness distillation.

## Foundational Learning

- **Concept:** Data-Free Knowledge Distillation (DFKD)
  - **Why needed here:** FERD is built on top of DFRD (Data-Free Robustness Distillation). You must understand how a generator can invert a teacher model to create synthetic data before understanding how FERD modifies that generation for fairness.
  - **Quick check question:** How does a generator learn to produce images that "fool" a teacher model into revealing its knowledge without real data?

- **Concept:** Adversarial Robustness vs. Fairness
  - **Why needed here:** The core problem is not just robustness (defense against attack) but *fairness* (consistent defense across all classes). You need to distinguish "Average Robustness" from "Worst-class Robustness."
  - **Quick check question:** Why might a model with 90% average accuracy still be considered "unfair" or "unsafe" in an adversarial setting?

- **Concept:** Information Bottleneck
  - **Why needed here:** Mechanism 2 uses a variational approximation of the Information Bottleneck to separate "robust" and "non-robust" features.
  - **Quick check question:** How does compressing the mutual information between input and features help isolate the features that cause adversarial vulnerability?

## Architecture Onboarding

- **Component map:**
  Teacher ($f^T$) -> Generator -> FAE Module -> UTAE Module -> Student ($f^S$)

- **Critical path:**
  1. **Reweighting:** Teacher evaluates current synthetic batch $\to$ Calculate Margins $\to$ Update Class Weights.
  2. **Generation:** Generator creates FAEs using new weights + Non-robust Uniformity Loss.
  3. **Attack:** Construct UTAEs from FAEs using Uniform-Target constraint.
  4. **Distillation:** Train Student on FAEs (as benign) and UTAEs (as adversarial).

- **Design tradeoffs:**
  - **$\gamma$ (Uniform Target Strength):** Low $\gamma$ results in biased attacks (unfairness); High $\gamma$ results in weak perturbations (lower overall robustness).
  - **Reweighting:** Improves worst-class robustness but may slightly compromise average accuracy.

- **Failure signatures:**
  - **Model Collapse:** Generator produces unrecognizable noise.
  - **Stagnant Robustness:** If $\gamma$ is too high (>0.7), worst-class robustness improves but average robustness drops significantly.
  - **High NSD:** Normalized Standard Deviation remains high if the reweighting strategy fails to identify the correct weak classes.

- **First 3 experiments:**
  1. **Sanity Check (Reweighting):** Run distillation with uniform sampling vs. reweighting. Plot the robustness of the weakest class to verify the gain.
  2. **Ablation ($\gamma$):** Sweep $\gamma$ from 0.0 to 0.9. Plot both Average and Worst-class robustness to find the "fairness sweet spot" (identified as 0.5 in paper).
  3. **Visual Inspection:** Visualize synthetic FAEs vs. standard synthetic samples. Verify that FAEs maintain class discriminability.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the trade-off between worst-class robustness and average clean accuracy be mitigated?
- **Basis in paper:** [inferred] Table 3 shows that the robustness-guided reweighting strategy improves worst-class performance but causes a slight drop in average clean accuracy (e.g., 80.47% to 79.86%).
- **Why unresolved:** The paper establishes the trade-off as a consistent finding with previous research but does not propose a mechanism to eliminate the loss in average performance while maintaining fairness.
- **What evidence would resolve it:** A modified reweighting or distillation loss that maintains or improves average clean accuracy while achieving the same worst-class gains.

### Open Question 2
- **Question:** Is the extraction of non-robust features from the last convolutional layer optimal for all architectures?
- **Basis in paper:** [inferred] The implementation details state that intermediate features are extracted from the "last convolutional layer" for the information bottleneck, but no ablation is provided regarding the choice of layer depth.
- **Why unresolved:** Different architectures store non-robust features at varying depths; assuming the final convolutional layer is the universal source for $Z_{nr}$ may not hold for deeper or residual-heavy networks.
- **What evidence would resolve it:** An ablation study showing the impact of extracting non-robust features from different layers (e.g., middle layers vs. final layers) on the quality of FAEs.

### Open Question 3
- **Question:** Can the uniform target constraint hyperparameter $\gamma$ be optimized adaptively?
- **Basis in paper:** [inferred] Figure 6 demonstrates that $\gamma$ controls a sensitive trade-off where high values suppress attack strength, yet the paper relies on empirical selection (0.5).
- **Why unresolved:** A fixed $\gamma$ may not suit the dynamic nature of training where the required constraint strength might change as the student model evolves.
- **What evidence would resolve it:** The development of a dynamic scheduling function for $\gamma$ that adjusts based on the convergence of the student's robustness or the entropy of the attack targets.

## Limitations
- The framework relies on a pre-trained robust teacher model, limiting its effectiveness to the teacher's inherent robustness distribution.
- The uniform-target constraint hyperparameter $\gamma$ represents a sensitive trade-off that requires careful tuning and may not generalize across all threat models.
- The framework's impact on real-world attack scenarios and adaptive, class-specific adversarial attacks requires further validation.

## Confidence

- **High Confidence:** The core mechanism of robustness-guided class reweighting (Mechanism 1) is well-supported by quantitative results showing consistent worst-class robustness improvements across all datasets and student models.
- **Medium Confidence:** The non-robust feature suppression approach (Mechanism 2) is conceptually sound and supported by ablation studies, but the practical impact of this constraint on synthetic data quality and semantic preservation is less thoroughly validated.
- **Medium Confidence:** The uniform-target adversarial example generation (Mechanism 3) is novel but relies on assumptions about generalization from uniform to specific attack targets that require more extensive empirical validation.

## Next Checks
1. **Sensitivity Analysis:** Conduct a comprehensive sweep of the uniform target constraint parameter Î³ (0.1 to 0.9) and plot the tradeoff curve between average robustness and worst-class robustness to identify optimal operating points for different threat models.
2. **Real Attack Generalization:** Test FERD-trained models against adaptive, class-specific adversarial attacks that deliberately target the previously vulnerable classes to verify that the fairness improvements hold against realistic threat models.
3. **Semantic Preservation:** Implement quantitative metrics for synthetic data quality (e.g., Inception Score, FID if real data available for reference) and conduct user studies to verify that FAEs maintain class discriminability while achieving fairness objectives.