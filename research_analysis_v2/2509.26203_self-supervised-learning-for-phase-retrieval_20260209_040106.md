---
ver: rpa2
title: Self-supervised learning for phase retrieval
arxiv_id: '2509.26203'
source_url: https://arxiv.org/abs/2509.26203
tags:
- pour
- phase
- nous
- reconstruction
- perte
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of self-supervised learning for
  phase retrieval, a nonlinear inverse problem where high-quality ground truth data
  is often unavailable. The authors propose a method that leverages the natural translation
  invariance of images to learn from measurement data alone, without requiring paired
  ground truth images.
---

# Self-supervised learning for phase retrieval

## Quick Facts
- arXiv ID: 2509.26203
- Source URL: https://arxiv.org/abs/2509.26203
- Authors: Victor Sechaud; Patrice Abry; Laurent Jacques; Julián Tachella
- Reference count: 0
- Primary result: Self-supervised equivariant imaging achieves phase retrieval performance comparable to supervised methods at sampling rates above 0.5, using only measurement data without ground truth.

## Executive Summary
This paper addresses phase retrieval, a nonlinear inverse problem where only intensity measurements are available but phase information is required. The authors propose a self-supervised learning approach that exploits translation invariance in natural images to learn reconstruction without ground truth data. By combining measurement consistency loss with equivariance constraints, the method learns to reconstruct signals from their intensity measurements alone. Experiments on MNIST demonstrate that this self-supervised approach matches supervised performance at sampling rates above 0.5, though it performs significantly worse at lower sampling rates.

## Method Summary
The method combines two self-supervised losses: measurement consistency (ensuring reconstructed images match observed measurements when passed through the forward operator) and equivariance (exploiting translation invariance of images to provide additional constraints). A U-Net architecture learns to reconstruct complex signals from intensity measurements. The measurement consistency loss uses amplitude-based formulation (√y - √|Af_θ(y)|²), while the equivariance loss enforces that applying translation before or after reconstruction yields consistent results. The final loss is a weighted combination of these terms, with λ=1 in experiments.

## Key Results
- Self-supervised equivariant imaging achieves cosine similarity comparable to supervised methods when sampling rate α > 0.5
- Performance degrades significantly at lower sampling rates (α < 0.3), where self-supervised method substantially underperforms supervised
- Amplitude-based consistency loss outperforms intensity-based loss at low sampling rates
- U-Net architecture with combined loss successfully learns to recover phase information without ground truth data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-supervised learning can recover signals from nonlinear measurements without ground truth by enforcing consistency between reconstructions and observed measurements.
- Mechanism: The measurement consistency loss (L_A or L_CM) ensures that when the network's reconstruction is passed through the forward operator h, it produces measurements matching the observed y. For phase retrieval specifically: ||√y_i - √h(f_θ(y_i))||² is minimized, where h(x) = |Ax|². This anchors reconstructions to physically valid solutions.
- Core assumption: The forward operator h is known and differentiable (at least locally), allowing gradient-based optimization through the measurement model.
- Evidence anchors:
  - [abstract] "proposes a method that leverages the natural translation invariance of images to learn from measurement data alone"
  - [section 4] "Cette fonction de perte est aussi connue dans le domaine de reconstruction de phase comme la perte d'intensité... des observations empiriques suggèrent qu'il est parfois préférable d'utiliser la perte d'amplitude"
  - [corpus] Related paper "Self-Supervised Learning from Noisy and Incomplete Data" addresses similar unsupervised inverse problem frameworks
- Break condition: If measurements are severely undersampled (α < 0.3) or noise dominates, consistency alone admits too many solutions.

### Mechanism 2
- Claim: Translation equivariance provides additional supervision signal by exploiting the statistical structure of natural image distributions.
- Mechanism: If the signal space X is invariant to translations (T_g X = X for all g ∈ G), then for any signal x and its translated version T_g x, the network should reconstruct both correctly. The equivariance loss enforces: f(h(T_g x)) ≃ T_g f(h(x)). This means applying translation before reconstruction should equal applying translation after reconstruction—creating a self-consistency constraint that resolves ambiguities in the null space.
- Core assumption: The measurement operator A does NOT commute with transformations (ker(A) ≠ ker(AT_g)), otherwise equivariance provides no new information.
- Evidence anchors:
  - [abstract] "combines a consistency loss... with an equivariance loss (exploiting the invariance property to improve reconstruction)"
  - [section 3.2] "T_g doit modifier le noyau de A, c'est-à-dire que ker(A) ≠ ker(AT_g) pour tout g ∈ G. Une condition nécessaire est que l'opérateur A ne commute pas avec les transformations"
  - [corpus] Limited direct corpus support for equivariant imaging in nonlinear problems; most related work (Equivariant Imaging by Chen et al.) focuses on linear operators
- Break condition: If the forward operator commutes with transformations (e.g., circular convolution with periodic boundary), equivariance loss provides zero additional constraint.

### Mechanism 3
- Claim: The combined loss (amplitude + equivariance) enables phase recovery by resolving the global phase ambiguity inherent in intensity-only measurements.
- Mechanism: Phase retrieval has inherent ambiguity: |A(e^(iφ)x)|² = |Ax|² for any φ. The cosine similarity metric (CS) used in the equivariance loss is phase-invariant, allowing the network to learn consistent reconstructions up to global phase. The combined loss L(θ) = L_A(θ) + λL_EI(θ) balances measurement fidelity with structural consistency, where λ controls the tradeoff.
- Core assumption: The signal distribution has sufficient structure (piecewise smoothness, sparsity) that equivariance constraints meaningfully reduce the solution space.
- Evidence anchors:
  - [section 4] "La fonction de perte finale est donc une combinaison des pertes de consistence des mesures et d'équivariance: L(θ) = L_A(θ) + λL_E(θ)"
  - [section 5, Figure 1] Shows amplitude loss (LA + LEI) outperforms intensity loss (LCM + LEI) at lower sampling rates
  - [corpus] "Generalizable Holographic Reconstruction via Amplitude-Only Diffusion Priors" addresses related amplitude-only phase retrieval but uses diffusion models rather than equivariance
- Break condition: At very low sampling rates (α < 0.5), insufficient measurements leave too many degrees of freedom for equivariance to resolve.

## Foundational Learning

- Concept: **Inverse Problems and Ill-Posedness**
  - Why needed here: Phase retrieval is the canonical nonlinear ill-posed problem—multiple signals produce identical intensity measurements. Understanding why additional constraints (priors, equivariance) are necessary is prerequisite.
  - Quick check question: Given y = |Ax|², can you explain why knowing y alone is insufficient to uniquely determine x?

- Concept: **Equivariance vs Invariance**
  - Why needed here: The method exploits that signal distributions are invariant (T_g X = X) to impose equivariance on the reconstruction function (f∘h should commute with T_g). Confusing these concepts will prevent understanding the mechanism.
  - Quick check question: If f is a reconstruction network and T_g is translation, what does "equivariance" require: f(T_g(x)) = T_g(f(x)) or f(T_g(x)) = f(x)?

- Concept: **Null Space of Linear Operators**
  - Why needed here: Self-supervised learning succeeds when transformations modify the null space (ker(A) ≠ ker(AT_g)), providing new information. Without this, equivariance constraints are redundant.
  - Quick check question: For a measurement matrix A that is translation-invariant (e.g., circular convolution), would the equivariance loss provide additional constraints?

## Architecture Onboarding

- Component map:
  - Forward operator A: Complex random matrix (m × n) modeling diffusive optical media; elements drawn from N(0, 1/2m) + iN(0, 1/2m)
  - Reconstruction network f_θ: U-Net with 4 encoder/decoder levels
  - Measurement consistency branch: Computes √y - √|Af_θ(y)|²
  - Equivariance branch: Samples 2 random translations per image, computes CS(T_g f_θ(y), f_θ(h(T_g f_θ(y))))
  - Combined loss: L = L_A + λL_EI with λ = 1

- Critical path:
  1. Input: intensity measurements y ∈ R^m
  2. Network produces complex reconstruction f_θ(y) ∈ C^n
  3. Forward operator applied: Af_θ(y) ∈ C^m
  4. Intensity computed: |Af_θ(y)|² ∈ R^m
  5. Amplitude loss: ||√y - √|Af_θ(y)|²||²
  6. Equivariance: translate network output → re-measure → re-reconstruct → compare with CS

- Design tradeoffs:
  - Amplitude loss vs intensity loss: Paper finds amplitude loss (LA) more robust at low sampling rates than intensity loss (LCM)
  - Number of transformations: Authors sample only 2 translations per image for computational efficiency; more may improve performance
  - Sampling rate α: Performance degrades significantly below α = 0.5; method requires m > 0.5n measurements

- Failure signatures:
  - α < 0.3: Self-supervised performance substantially worse than supervised (see Figure 1)
  - Operator commutes with transformations: Equivariance loss provides no constraint
  - Scale ambiguity in CS metric: Must pair with measurement consistency to fix global scale

- First 3 experiments:
  1. **Baseline sanity check**: Implement supervised training with L_SUP = -Σ CS(x_i, f_θ(y_i)) to establish upper bound on achievable performance
  2. **Ablation on loss components**: Train with (a) consistency only, (b) equivariance only, (c) combined to quantify contribution of each mechanism
  3. **Sampling rate sweep**: Evaluate cosine similarity vs α ∈ {0.2, 0.4, 0.5, 0.6, 0.8, 1.0} to identify critical threshold where self-supervised matches supervised

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the equivariance-based self-supervised framework be extended to transformation groups beyond translations (e.g., rotations, scalings)?
- Basis in paper: [explicit] Conclusion states: "D'autres travaux peuvent être réalisés pour étendre cette approche à divers groupes de transformations ou d'autre type de signaux" (Other work can extend this approach to various transformation groups or other signal types).
- Why unresolved: Experiments only tested translation invariance on MNIST; theoretical and empirical validation for other groups remains unexplored.
- What evidence would resolve it: Successful reconstruction results on datasets using rotation or scaling equivariance, with analysis of which transformations satisfy the kernel-modification requirement.

### Open Question 2
- Question: Can theoretical guarantees for reconstruction feasibility be established for the proposed self-supervised phase retrieval method?
- Basis in paper: [explicit] Conclusion explicitly calls for "un cadre théorique assurant la faisabilité de la reconstruction" (a theoretical framework ensuring reconstruction feasibility).
- Why unresolved: Current work provides empirical demonstrations but no formal proofs of convergence or recovery guarantees.
- What evidence would resolve it: Theoretical analysis bounding reconstruction error under specific conditions on sampling rate α, operator A properties, and transformation group G.

### Open Question 3
- Question: How can self-supervised performance be improved at low sampling rates (α < 0.5)?
- Basis in paper: [inferred] Figure 1 shows significant performance gap between self-supervised and supervised methods at α < 0.5, with particularly poor results below α = 0.3.
- Why unresolved: The equivariance constraint may provide insufficient information about the signal null space at high compression ratios.
- What evidence would resolve it: Modified loss functions or architectural changes that close the performance gap at low α, potentially validated on diverse datasets.

### Open Question 4
- Question: Does the method generalize to real-world measurement operators beyond random Gaussian matrices?
- Basis in paper: [inferred] Experiments use only random complex Gaussian matrices, though the abstract mentions "Fourier-type measurements" and real optical imaging uses diverse operators.
- Why unresolved: The non-commutation requirement between operator A and transformations may not hold for all practical measurement schemes.
- What evidence would resolve it: Experiments on Fourier-based phase retrieval, coded diffraction patterns, or other physically-motivated measurement operators.

## Limitations

- Sampling rate threshold: Self-supervised performance matches supervised only at α > 0.5, with significant degradation below this threshold
- Limited experimental scope: Results based solely on MNIST dataset and random Gaussian measurement operators
- Theoretical gaps: No formal proofs of reconstruction feasibility or convergence guarantees for the proposed method

## Confidence

- **High confidence**: The self-supervised framework with measurement consistency is valid for phase retrieval. The mathematical formulation of amplitude and equivariance losses is correct.
- **Medium confidence**: The experimental results showing comparable performance to supervised learning at high sampling rates. Results are limited to MNIST and may not generalize to other image distributions or measurement models.
- **Low confidence**: The claim about broad applicability to "general nonlinear inverse problems." Experimental validation is limited to a specific class of problems with translation-invariant signal distributions.

## Next Checks

1. **Cross-dataset validation**: Test the method on non-MNIST datasets (e.g., CIFAR-10, natural images) to verify that performance patterns hold across different image distributions and structural properties.
2. **Operator commutation analysis**: Systematically test equivariance loss effectiveness across measurement operators with varying degrees of commutation with translations (e.g., random vs circulant matrices) to quantify when the constraint provides meaningful information.
3. **Sampling rate sensitivity**: Conduct a more granular analysis of the transition region (α = 0.4-0.6) to precisely characterize where self-supervised learning begins to match supervised performance and identify failure modes at the threshold.