---
ver: rpa2
title: 'Understanding In-Context Learning Beyond Transformers: An Investigation of
  State Space and Hybrid Architectures'
arxiv_id: '2510.23006'
source_url: https://arxiv.org/abs/2510.23006
tags:
- heads
- stream
- layers
- function
- random
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper investigates in-context learning (ICL) capabilities
  across transformer, state-space, and hybrid architectures, focusing on two types
  of knowledge-based tasks: parametric knowledge retrieval and contextual knowledge
  understanding. The study employs behavioral probing and intervention-based methods
  to analyze model internals, revealing that while different architectures exhibit
  similar ICL performance, their internal mechanisms differ significantly.'
---

# Understanding In-Context Learning Beyond Transformers: An Investigation of State Space and Hybrid Architectures

## Quick Facts
- arXiv ID: 2510.23006
- Source URL: https://arxiv.org/abs/2510.23006
- Reference count: 40
- Hybrid architectures' ICL capabilities are dominated by FV heads in self-attention layers, regardless of parallel or sequential stacking

## Executive Summary
This paper investigates In-Context Learning (ICL) capabilities across transformer, state-space, and hybrid architectures, focusing on two types of knowledge-based tasks: parametric knowledge retrieval and contextual knowledge understanding. Using behavioral probing and intervention-based methods, the study reveals that while different architectures exhibit similar ICL performance, their internal mechanisms differ significantly. The research demonstrates that hybrid models' ICL capabilities are dominated by function vectors (FVs) in self-attention layers, with less contribution from Mamba layers, and that Mamba2 likely uses a different mechanism entirely for ICL.

## Method Summary
The study employs behavioral probing and intervention-based methods to analyze model internals across seven 1B-parameter models (MAMBA, MAMBA2, GEMMA, LLAMA, QWEN, HYMBA, ZAMBA2). Researchers use label randomization experiments to establish ICL behavior, then compute Average Indirect Effect (AIE) to identify function vector (FV) heads. They conduct steering experiments by adding extracted FVs to model outputs and mean-ablation interventions to measure causal impact. For hybrid models, attention and Mamba streams are analyzed separately. The study uses 33 datasets split between parametric knowledge retrieval (17 datasets) and contextual knowledge understanding (16 datasets).

## Key Results
- Function vectors responsible for ICL are concentrated in self-attention layers of hybrid models, with less contribution from Mamba layers
- FVs are more critical for parametric knowledge retrieval tasks than contextual understanding tasks, and the two task types do not share the same set of FVs
- Mamba2 uses a different mechanism from FVs for ICL, as steering experiments show minimal impact on its performance
- Hybrid models' ICL capabilities are dominated by FV heads in self-attention layers regardless of whether self-attention and SSM layers are stacked in parallel or sequentially

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In hybrid architectures, In-Context Learning (ICL) capabilities are primarily driven by "Function Vectors" (FVs) located in self-attention layers rather than the State Space Model (SSM) layers.
- Mechanism: Specific attention heads extract a task representation (the Function Vector) from the few-shot demonstrations. This vector is added to the final token's residual stream, effectively "steering" the model toward the correct output distribution without weight updates. The SSM layers appear to provide less direct causal influence on the specific task logic compared to these attention heads.
- Core assumption: The causal intervention method (steering/ablation) accurately captures the necessity of these heads and does not disrupt correlated but non-causal activities.
- Evidence anchors:
  - [abstract] "Function vectors (FVs)... are concentrated in self-attention layers of hybrid models, with less contribution from Mamba layers."
  - [section 5.4] "For hybrid models, their ICL capabilities are primarily controlled by FV heads at self-attention layers."
  - [corpus] Neighbor "Trained Mamba Emulates Online Gradient Descent" suggests SSMs have distinct learning dynamics (gradient descent emulation), potentially explaining why they don't rely on the discrete FV mechanism found in attention.
- Break condition: If steering the Mamba stream in isolation improves performance significantly more than the attention stream, this mechanism is weakened (Data shows the opposite: steering Mamba streams often decreases performance or yields marginal gains).

### Mechanism 2
- Claim: The internal mechanism for "Parametric Knowledge Retrieval" (e.g., capital cities) is distinct from and relies more heavily on FVs than "Contextual Knowledge Understanding" (e.g., sentiment analysis).
- Mechanism: Parametric tasks require activating specific, pre-trained associations. FVs in attention heads act as effective "keys" or "triggers" for these associations. Contextual understanding requires processing relationships within the prompt, which appears to rely on distributed processing or mechanisms other than localized FVs.
- Core assumption: The datasets used cleanly separate these two cognitive modes and are representative of general ICL tasks.
- Evidence anchors:
  - [abstract] "FVs are more critical for parametric knowledge retrieval tasks than contextual understanding tasks, and the two task types do not share the same set of FVs."
  - [section 5.2] "Top FV heads are highly consistent... for parametric knowledge retrieval ICL but not contextual understanding ICL."
  - [corpus] "Do different prompting methods yield a common task representation..." supports the finding that task representations vary significantly based on the nature of the elicitation.
- Break condition: If steering identified FVs significantly improves performance on *both* task types equally, the separation of mechanisms claim fails.

### Mechanism 3
- Claim: Pure Mamba2 architectures likely utilize a different mechanism for ICL that does not rely on localized Function Vectors.
- Mechanism: Unlike Transformers or hybrid models, Mamba2's ICL behavior is not significantly altered by steering specific heads. The paper hypothesizes this is due to multi-head dimensions being too small to capture task-specific info at the head level, implying a more distributed or state-dependent mechanism.
- Core assumption: The lack of steering effect is due to the absence of the mechanism, rather than the failure of the probing method to locate the correct vector.
- Evidence anchors:
  - [abstract] "Mamba2 uses a different mechanism from FVs for ICL, as steering experiments show minimal impact on its performance."
  - [section 5.4] "MAMBA2-1.3B-HF does not seem to be significantly influenced by steering... uses its heads for other purposes."
  - [corpus] "Trained Mamba Emulates Online Gradient Descent" provides a theoretical anchor that Mamba models might implement ICL via implicit gradient descent in the state dynamics, rather than vector activation.
- Break condition: If future research identifies a specific head configuration or state dimension in Mamba2 that acts as an FV and responds to steering.

## Foundational Learning

- Concept: **Function Vectors (FVs)**
  - Why needed here: This is the core unit of analysis for the paper. You must understand that FVs are the causal "task encodings" found in specific attention heads (based on Todd et al., 2024).
  - Quick check question: Can you explain how "steering" a model by adding a vector to a hidden state differs from standard fine-tuning?

- Concept: **Hybrid Architectures (Hymba/Zamba)**
  - Why needed here: The paper compares models that stack Attention and SSM layers sequentially (Zamba) vs. in parallel (Hymba). Understanding how these streams combine is vital for interpreting the "Attention Stream" vs. "Mamba Stream" results.
  - Quick check question: If a hybrid layer takes the mean of an attention block and an SSM block, how does ablating one block affect the signal magnitude?

- Concept: **Average Indirect Effect (AIE)**
  - Why needed here: The paper uses AIE to quantify which heads are "FV heads." This metric measures the change in logit probability when a head is intervened upon.
  - Quick check question: Why is it important to measure the effect on the "corrupted" prompt (random demos) rather than just the clean prompt when calculating AIE?

## Architecture Onboarding

- Component map:
  - **Hymba (Parallel):** Input → [Attention Head + Mamba Block] → Mean → Output
  - **Zamba (Sequential):** Input → Mamba Layers … → Attention Layer → Linear → Mamba Layers …
  - **Identified FV Heads:** Located primarily in the Attention blocks (layers 10-20 typically)

- Critical path: For ICL in hybrids, the **Attention Stream** is the critical path. The Mamba stream provides sequence context but seems less responsible for the specific "task logic" retrieval (Figure 8 vs Figure 9).

- Design tradeoffs:
  - **Sequential (Zamba):** Attention is sparse (every 6th layer). FV heads are highly concentrated in these specific layers.
  - **Parallel (Hymba):** Attention is dense but windowed. FV heads are distributed but remain dominant over the parallel Mamba head.
  - *Assumption:* The sparse attention in Zamba forces the model to compress more task information into fewer heads, making them distinct targets for intervention.

- Failure signatures:
  - **Contextual Task Failure:** If a model fails to improve with ICL on sentiment analysis but succeeds on fact retrieval, check if the architecture relies heavily on the specific FVs that the paper shows are less effective for contextual tasks.
  - **Mamba2 Non-compliance:** If applying standard FV-steering techniques to a pure Mamba2 model yields no improvement, this is expected behavior (Mechanism 3), not a method failure.

- First 3 experiments:
  1. **Behavioral Probe (Label Randomization):** Replicate Figure 3. Test the model with 0%, 50%, and 100% correct labels to establish if the model is actually performing ICL or just relying on priors.
  2. **AIE Heatmap Generation:** Extract head outputs for a known parametric task (e.g., Country-Capital). Calculate AIE to visualize which heads light up. Verify they are in the Attention stream (Figure 5).
  3. **Steering Validation:** Take the "No Demo" baseline for a task, add the calculated FV to the identified top heads, and measure the performance jump. Compare this against steering random heads to confirm causality.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the specific mechanism Mamba2 uses to perform In-Context Learning (ICL) if it does not rely on function vectors (FVs)?
- **Basis in paper:** [explicit] The authors state they "speculate that Mamba2 uses a different mechanism from FVs to perform ICL" because steering experiments showed minimal impact on performance compared to other architectures.
- **Why unresolved:** The current study focused on identifying FVs using methods designed for transformers. The hypothesis that multi-head dimensions limit head-level task capture in Mamba2 remains unverified.
- **What evidence would resolve it:** A mechanistic analysis of Mamba2 internals using activation patching or causal tracing to locate ICL circuits distinct from the head-based FV mechanism.

### Open Question 2
- **Question:** What role do induction heads play in In-Context Learning within State Space Models (SSMs) and hybrid architectures relative to FVs?
- **Basis in paper:** [explicit] The authors acknowledge in the Limitations section: "In this study, we only focus on one potential mechanism, namely, FV heads. Yet there might be other mechanisms, including induction heads."
- **Why unresolved:** The analysis isolated FVs but did not extend to identifying or intervening on induction heads in SSM/hybrid layers to see if they drive the performance that FVs could not explain.
- **What evidence would resolve it:** Applying induction head detection methods (e.g., pattern matching for previous token heads) to Mamba/Hybrid models and comparing their causal effects to FVs.

### Open Question 3
- **Question:** How do pretraining data and procedures influence the emergence and layer distribution of function vector heads across different model architectures?
- **Basis in paper:** [inferred] The study notes that models like Qwen and Llama "exhibit slightly different internals despite using similar self-attention layers," and suggests "pretraining materials and procedures could also impact ICL capabilities."
- **Why unresolved:** The study relied on pretrained checkpoints without controlling for training data or hyperparameters, making it impossible to attribute internal differences solely to architecture rather than training variance.
- **What evidence would resolve it:** A controlled study training SSMs and hybrids on identical datasets with fixed hyperparameters to observe if FV locations and density are consistent or architecture-dependent.

## Limitations
- The study focuses on 1B-parameter models, limiting generalizability to larger-scale systems where different computational regimes may apply
- The finding that Mamba2 uses a different mechanism is based on negative evidence (steering doesn't work) rather than positive identification of the actual mechanism
- The separation between parametric and contextual task mechanisms could reflect dataset-specific artifacts rather than fundamental architectural differences

## Confidence
- **High confidence**: The behavioral finding that ICL performance varies significantly with demonstration quality across all architectures. The AIE method successfully identifies attention heads that correlate with ICL performance in hybrid models.
- **Medium confidence**: The claim that hybrid models' ICL capabilities are dominated by FV heads in self-attention layers. While steering experiments support this, the magnitude of effects varies across task types and architectures.
- **Low confidence**: The assertion that Mamba2 uses a fundamentally different mechanism from FVs. This is based primarily on the absence of steering effects rather than identification of the actual mechanism.

## Next Checks
1. **Cross-task FV transferability**: Test whether steering the same FV heads improves performance on both parametric and contextual tasks simultaneously. If FVs work equally well across task types, the claim of distinct mechanisms weakens significantly.

2. **Mamba2 state analysis**: Conduct intervention experiments targeting specific state dimensions in Mamba2 rather than heads, testing whether the mechanism involves distributed state representations rather than localized head activations.

3. **Architectural scaling study**: Repeat the FV analysis on larger (>7B parameter) hybrid and pure SSM models to determine whether the dominance of attention-layer FVs persists at scale, or whether SSM layers become more influential in larger systems.