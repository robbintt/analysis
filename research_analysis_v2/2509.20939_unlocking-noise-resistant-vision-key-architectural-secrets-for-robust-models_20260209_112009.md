---
ver: rpa2
title: 'Unlocking Noise-Resistant Vision: Key Architectural Secrets for Robust Models'
arxiv_id: '2509.20939'
source_url: https://arxiv.org/abs/2509.20939
tags:
- noise
- vitbasepatch16clip224
- openai
- pooling
- inception
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how architectural design choices impact
  the robustness of vision models to additive Gaussian noise. Through extensive experiments
  on 1,174 pretrained models, the authors identify four key design patterns that improve
  robustness: larger stem kernels, smaller input resolutions, average pooling instead
  of max pooling, and supervised vision transformers (ViTs) over CLIP ViTs.'
---

# Unlocking Noise-Resistant Vision: Key Architectural Secrets for Robust Models

## Quick Facts
- **arXiv ID:** 2509.20939
- **Source URL:** https://arxiv.org/abs/2509.20939
- **Reference count:** 40
- **Key outcome:** This paper investigates how architectural design choices impact the robustness of vision models to additive Gaussian noise. Through extensive experiments on 1,174 pretrained models, the authors identify four key design patterns that improve robustness: larger stem kernels, smaller input resolutions, average pooling instead of max pooling, and supervised vision transformers (ViTs) over CLIP ViTs. Theoretical analysis explains these findings, showing that noise attenuation scales quadratically with kernel size and downsampling factor, average pooling is unbiased with lower variance compared to max pooling, and CLIP preprocessing amplifies pixel-space sensitivity due to smaller normalization standard deviations. The study provides actionable guidelines for designing more robust vision models, demonstrating up to 506 rank improvements and 21.6% accuracy gains under Gaussian noise.

## Executive Summary
This paper systematically investigates how architectural design choices impact vision model robustness to additive Gaussian noise. Through extensive experiments on 1,174 pretrained models and theoretical analysis, the authors identify four key design patterns that improve noise resistance: larger stem kernels, smaller input resolutions, average pooling instead of max pooling, and supervised ViTs over CLIP ViTs. The theoretical framework explains these findings through quadratic noise attenuation scaling laws, unbiased vs. biased pooling statistics, and Lipschitz sensitivity analysis of normalization constants.

## Method Summary
The study evaluates 1,174 pretrained models from the timm library, applying Gaussian noise during evaluation using Albumentations. For controlled experiments, five datasets (ImageNet-1K plus four specialized datasets) are used with a 70:15:15 train-validation-test split. Noise is applied only during evaluation with std_range=(0.1, 0.22) for controlled datasets and (0.2, 0.44) for ImageNet-1K. ResNet training uses SGD with momentum 0.9, cosine annealing, and weight decay 1e-2 for 200 epochs. ViT fine-tuning employs AdamW with learning rate 5e-4 and weight decay 0.05 for 400 epochs. The evaluation metric is top-1 accuracy under noise, with RankDiff quantifying robustness improvements.

## Key Results
- Larger stem kernels improve noise robustness with gain decaying quadratically (∝ 1/k²)
- Smaller input resolutions provide significant noise attenuation (∝ 1/s²)
- Average pooling outperforms max pooling under Gaussian noise due to unbiased statistics
- CLIP ViTs are more sensitive to noise than supervised ViTs due to smaller normalization constants
- Up to 506 rank improvements and 21.6% accuracy gains achieved through architectural modifications

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Larger stem kernels and smaller input resolutions attenuate additive Gaussian noise with energy decaying approximately quadratically with kernel size and downsampling factor.
- **Mechanism:** Low-pass convolution kernels act as spatial averaging operators. For a kernel of size k with radial low-pass envelope, the per-pixel noise gain γ(k) ≤ C/k². Anti-aliased downsampling by factor s similarly yields γ↓(s) ≤ C'/s². Doubling the stem kernel quarters output noise energy (≈−6dB).
- **Core assumption:** The stem kernel satisfies a radial low-pass envelope condition: |bKk(ω)| ≤ (1 + βkr)^(-1-δ) for constants β, δ > 0.
- **Evidence anchors:**
  - [Abstract] "We prove that low-pass stem kernels attenuate noise with a gain that decreases quadratically with kernel size and that anti-aliased downsampling reduces noise energy roughly in proportion to the square of the downsampling factor."
  - [Section 3.2, Theorem 1 & 2] Formal proofs with k^(-2) and s^(-2) bounds.
  - [Table 1, Figure 1] Empirical validation across ViTs and ResNets showing larger patch sizes and smaller resolutions improve robustness.
- **Break condition:** Non-low-pass kernels or non-anti-aliased downsampling (e.g., strided convolution without filtering) may not achieve quadratic decay.

### Mechanism 2
- **Claim:** Average pooling is unbiased with variance scaling as σ²/k, while max pooling introduces positive bias and mean-squared error that grows logarithmically with window size.
- **Mechanism:** Average pooling computes E[δavg] = 0, Var[δavg] = σ²/k. Max pooling takes the maximum of k noisy samples, yielding E[δmax] ≥ 0 and E[δ²max] ≤ σ²(2 log(2k) + 2) in the uniform-signal case. Max pooling also has higher worst-case Lipschitz sensitivity (ℓ₂ norm ≤ 1 vs. k^(-1/2) for average).
- **Core assumption:** Noise η ∼ N(0, σ²I) is i.i.d. and additive.
- **Evidence anchors:**
  - [Section 4.2, Theorem 3] Complete theoretical derivation comparing both pooling operators.
  - [Table 3] Empirical results showing AvgPool consistently outperforms MaxPool and NNPool under Gaussian noise across 5 datasets.
  - [Corpus] Limited direct evidence in corpus; related work "Robust Noise Attenuation via Adaptive Pooling" examines transformer output pooling but not low-level pooling comparisons.
- **Break condition:** Non-Gaussian noise (e.g., salt-and-pepper) may favor median or trimmed-mean pooling; max pooling may retain advantages for invariance to non-noise transformations.

### Mechanism 3
- **Claim:** CLIP preprocessing's smaller normalization standard deviations amplify worst-case pixel-space sensitivity by up to 1.91× relative to Inception-style preprocessing.
- **Mechanism:** The pixel-space Lipschitz bound satisfies ∥Fμ,σ∥Lip ≤ Lz/σmin where σmin = minc σc. CLIP's σCLIP ≈ (0.269, 0.261, 0.276) vs. Inception's σINCEPTION = (0.5, 0.5, 0.5) yields Lz/0.261 ÷ (Lz/0.5) ≈ 1.91× larger sensitivity bound.
- **Core assumption:** The backbone f is globally ℓ₂-Lipschitz with constant Lz (holds for bounded spectral norms with ReLU/GELU/LayerNorm).
- **Evidence anchors:**
  - [Section 5.2, Theorem 4] Formal Lipschitz bound derivation.
  - [Table 5] Ablation showing CLIP ViTs achieve 9-18 percentage point accuracy gains under noise when switching from OPENAI to INCEPTION normalization constants.
  - [Corpus] No direct corpus evidence on preprocessing normalization effects.
- **Break condition:** Bound is worst-case; mean-squared sensitivity may differ. Effect may vary with non-Gaussian perturbations.

## Foundational Learning

- **Concept:** Discrete Fourier Transform (DFT) and Parseval's theorem
  - **Why needed here:** Theorem 1 proof uses Parseval identity to convert spatial noise energy to frequency domain for bounding via low-pass envelope.
  - **Quick check question:** Given a k×k box filter, what is ∥Kk∥²_F and how does it relate to per-pixel noise gain?

- **Concept:** Lipschitz continuity and spectral norms
  - **Why needed here:** Theorem 4 bounds end-to-end sensitivity via Lipschitz constants; requires understanding how normalization rescales the Lipschitz bound.
  - **Quick check question:** If a network has Lipschitz constant Lz = 3 on normalized inputs, what is the pixel-space Lipschitz constant with σmin = 0.26?

- **Concept:** Order statistics of Gaussian samples
  - **Why needed here:** Understanding max pooling's positive bias requires knowing that max of k standard Gaussians has E[Mk] ≈ √(2 log k).
  - **Quick check question:** Why does max pooling have positive bias under additive zero-mean noise while average pooling is unbiased?

## Architecture Onboarding

- **Component map:** Input → Preprocessing (choose σ values carefully) → Stem with kernel size k and optional pooling → Downsampling stages with anti-aliasing filters → Backbone feature extraction
- **Critical path:** 1. Input → Preprocessing (choose σ values carefully) 2. Stem with kernel size k and optional pooling 3. Downsampling stages with anti-aliasing filters 4. Backbone feature extraction
- **Design tradeoffs:** Larger kernels → better noise robustness but potentially coarser spatial features; Smaller resolution → better noise robustness but may reduce accuracy on clean data; Average pooling → unbiased noise suppression but may dilute discriminative features (vs. max pooling's invariance properties); Inception vs. OPENAI normalization → better noise robustness but may require retraining or fine-tuning
- **Failure signatures:** Clean accuracy high but dramatic drop (>20%) under light noise → check preprocessing σ values, pooling choice; Increasing resolution improves clean accuracy but degrades noise robustness → expected per Theorem 2; Max pooling in stem with uniform-signal regions → positive bias accumulation visible in feature maps
- **First 3 experiments:**
  1. **Baseline robustness audit:** Evaluate existing model on ImageNet validation set with Gaussian noise (std_range 0.1–0.4); measure rank change across noise levels
  2. **Stem ablation:** Replace MaxPool in stem with AvgPool; compare noise accuracy on same dataset with identical training recipe
  3. **Normalization swap test:** Replace OPENAI normalization with INCEPTION constants (no retraining); evaluate noise robustness gain

## Open Questions the Paper Calls Out

- **Open Question 1:** Do the identified noise-resistant design patterns (larger kernels, average pooling) transfer effectively to non-Gaussian corruptions such as weather effects, blur, or adversarial perturbations?
  - **Basis in paper:** [explicit] Section F (Limitations) explicitly states the study "does not encompass all real-world corruptions, such as adversarial perturbations, weather effects, or sensor-specific artifacts."
  - **Why unresolved:** The theoretical proofs in Theorems 1-4 rely specifically on properties of Gaussian noise (e.g., i.i.d. nature, variance reduction), and the empirical evaluation was restricted to Gaussian noise injection.
  - **What evidence would resolve it:** Evaluating the suite of 1,174 models on diverse corruption benchmarks (e.g., ImageNet-C non-noise categories) and adversarial robustness metrics.

- **Open Question 2:** How do these architectural insights generalize to specialized domains with distinct data structures, such as medical imaging or video processing?
  - **Basis in paper:** [explicit] Section F highlights that the results derived from `timm` models and standard datasets "may represent a limitation in their generalizability to other domains like medical imaging or video processing."
  - **Why unresolved:** The "unlocking" of robustness relies on standard computer vision architectures and ImageNet-scale data; it is unknown if the noise attenuation scaling laws hold for 3D medical volumes or temporal video streams.
  - **What evidence would resolve it:** Applying the specific design patterns (e.g., larger stem kernels, anti-aliased downsampling) to domain-specific backbones (like U-Nets or Video Transformers) and measuring performance under domain-relevant noise.

- **Open Question 3:** How can practitioners optimally balance the identified trade-off between high clean accuracy (favored by larger resolutions) and noise robustness (favored by smaller resolutions)?
  - **Basis in paper:** [explicit] Section 3.1 observes that the recommendation for smaller resolutions "is contrary to the common practice of scaling up resolution to improve general performance."
  - **Why unresolved:** The paper identifies the conflict and provides rules for robustness, but does not offer a theoretical or empirical framework for maximizing joint performance on clean and noisy data.
  - **What evidence would resolve it:** A multi-objective optimization analysis plotting Pareto frontiers for clean accuracy vs. robustness across varying resolutions and kernel sizes.

## Limitations

- The study focuses exclusively on additive Gaussian noise and does not address other real-world corruptions like adversarial perturbations, weather effects, or sensor-specific artifacts.
- Results derived from standard computer vision architectures and ImageNet-scale data may not generalize to specialized domains like medical imaging or video processing.
- The trade-off between clean accuracy and noise robustness (favoring smaller resolutions) conflicts with common practice of scaling up resolution for better general performance.

## Confidence

- **High**: Larger stem kernels improve noise robustness (strong empirical + theoretical support)
- **Medium**: Average pooling outperforms max pooling (solid theory, moderate empirical evidence)
- **Medium**: CLIP normalization amplifies sensitivity (theoretical bound supported by ablation)
- **Low**: Claims generalize to non-Gaussian noise (only tested on Gaussian perturbations)

## Next Checks

1. **Cross-noise validation**: Test proposed robust architectures (large stem kernels, average pooling) on salt-and-pepper and speckle noise to verify architectural benefits extend beyond Gaussian perturbations.
2. **Real-world dataset testing**: Evaluate on noisy real-world datasets (e.g., CIFAR-10-C, ImageNet-C) to validate that controlled-experiment findings transfer to practical scenarios.
3. **Ablation of preprocessing**: Systematically compare OPENAI vs INCEPTION normalization constants across multiple architectures to quantify the 1.91× sensitivity amplification effect with statistical significance.