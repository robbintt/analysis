---
ver: rpa2
title: 'Towards Open Foundation Language Model and Corpus for Macedonian: A Low-Resource
  Language'
arxiv_id: '2506.09560'
source_url: https://arxiv.org/abs/2506.09560
tags:
- macedonian
- language
- arxiv
- data
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the lack of resources and models for the Macedonian
  language in NLP. The authors created a 40GB corpus with 3.5B words from 12 sources,
  including new document-to-text extraction, and a 106k-instance instruction dataset
  with culturally grounded content.
---

# Towards Open Foundation Language Model and Corpus for Macedonian: A Low-Resource Language

## Quick Facts
- arXiv ID: 2506.09560
- Source URL: https://arxiv.org/abs/2506.09560
- Reference count: 23
- Primary result: An 8B-parameter Macedonian language model that outperforms all existing 8B-parameter models and achieves performance comparable to models up to 10x larger

## Executive Summary
This work addresses the critical shortage of resources for the Macedonian language in NLP by creating both a comprehensive 40GB corpus with 3.5B words and a novel 106k-instance instruction dataset. The authors developed domestic-yak, an 8B-parameter model trained through continued pretraining and supervised fine-tuning, and constructed a Macedonian evaluation suite with seven benchmarks. The model demonstrates superior performance across all benchmarks compared to existing 8B-parameter models while maintaining cultural and grammatical appropriateness according to native speaker evaluations.

## Method Summary
The authors built domestic-yak through a two-stage training process: continued pretraining from Llama 3.1 8B Instruct on a carefully filtered 1.47B-word Macedonian corpus, followed by supervised fine-tuning on a hybrid instruction dataset combining translated, post-edited, and synthetic culturally-grounded examples. The corpus was assembled from 12 sources including web crawls, documents, Wikipedia, and new document-to-text extraction, then processed through a multi-stage filtering pipeline removing low-quality content, duplicates, and non-Macedonian text. The instruction dataset was refined using GPT-4o-mini post-editing to correct translation artifacts and supplemented with synthetic examples addressing cultural knowledge gaps.

## Key Results
- Model outperforms all existing 8B-parameter Macedonian language models across all seven benchmarks
- Achieves performance comparable to models up to 10× larger (65B-175B parameters)
- Native speaker evaluation shows 81.6% prefer domestic-yak for grammatical consistency, 60% for natural phrasing, and 37% for cultural appropriateness

## Why This Works (Mechanism)

### Mechanism 1: Continued Pretraining from Multilingual Backbone
Continuing pretraining from an existing multilingual model yields better low-resource language adaptation than training from scratch by exploiting latent cross-lingual knowledge in the base model. The base model already encodes transferable linguistic representations that can be specialized to Macedonian without losing general capabilities.

### Mechanism 2: High-Quality Data Filtering with Aggressive Deduplication
Multi-stage filtering that removes ~58% of raw data improves downstream performance by increasing the effective signal-to-noise ratio. The multistage pipeline (PII removal → C4 filtering → Gopher filtering → language identification → deduplication) ensures only high-quality Macedonian text enters training.

### Mechanism 3: Hybrid Instruction Dataset with Cultural Grounding
Post-editing translated instruction data and adding synthetic culturally-grounded examples outperforms direct translation by correcting grammatical artifacts and filling cultural knowledge gaps. LLM-assisted refinement combined with synthetic examples addresses limitations of translation-based datasets.

## Foundational Learning

- **Continued Pretraining vs. Instruction Tuning**
  - Why needed here: The ablation shows pretraining (+5 avg) provides larger gains than instruction tuning (+2 avg); understanding this distinction is critical for resource allocation.
  - Quick check question: Why might pretraining improve factual knowledge benchmarks (PIQA +8) more than reading comprehension (BoolQ +1)?

- **Translation Artifacts ("Translationese")**
  - Why needed here: The paper explicitly addresses how direct translation creates unnatural phrasing and cultural mismatches that degrade model quality.
  - Quick check question: What happens to word order when translating English multiple-choice questions to Macedonian without context?

- **Template-Based Benchmark Translation**
  - Why needed here: The paper translates benchmarks via Serbian (linguistically closer) and uses placeholders to preserve grammatical structure.
  - Quick check question: Why translate through Serbian rather than directly from English?

## Architecture Onboarding

- **Component map:**
  Data Sources → Filtering Pipeline → Continued Pretraining → SFT → Evaluation
  Web crawls → PII/C4/Gopher → Llama 3.1 8B → Instruction → 7 benchmarks
  Documents → LangID/Dedup → (1 epoch) → (3 epochs) → (via Serbian)
  Wikipedia → (3.5B→1.47B)

- **Critical path:** Data quality (filtering) has the largest impact; the ablation shows pretraining gains (+5 avg) exceed SFT gains (+2 avg). Prioritize corpus construction over instruction dataset size.

- **Design tradeoffs:**
  - Tokenizer: Retained Llama's original tokenizer (avoids re-tokenization complexity, may be suboptimal for Macedonian)
  - Context window: 8192 for pretraining, 4096 for SFT (limits long-form capability)
  - Sampling: 2:1 ratio favoring human/synthetic over translated examples

- **Failure signatures:**
  - Reading comprehension (BoolQ) shows minimal pretraining gains (+1) — suggests this skill transfers well from base model
  - WinoGrande shows no SFT improvement (pronoun resolution may plateau during pretraining)
  - 4k context limit restricts multi-document tasks

- **First 3 experiments:**
  1. Replicate the filtering pipeline on a sample: run C4 → Gopher → LangID → MinHash dedup on 100k documents and measure retention rate; should approximate 42% retention.
  2. Ablate the instruction dataset: train with translated-only vs. translated + post-edited + synthetic; expect 1-2 point avg difference.
  3. Test template-based translation: compare naïve translation vs. placeholder-based translation on 50 benchmark examples; validate grammatical correctness with a native speaker.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the domestic-yak training pipeline yield comparable performance gains when applied to other low-resource languages?
- Basis in paper: [explicit] The authors state the release aims to provide a "reproducible blueprint for similar efforts in other languages."
- Why unresolved: The study validates the methodology only for Macedonian; the transferability of this specific data-to-parameter ratio is unknown.
- What evidence would resolve it: Replicating the pipeline on a distinct low-resource language and measuring the performance gap against multilingual baselines.

### Open Question 2
- Question: How does the model perform in specialized domains such as law, medicine, or finance?
- Basis in paper: [explicit] The authors note the model "has not been evaluated nor adapted for niche domains" and encourage future work in these areas.
- Why unresolved: The current evaluation is restricted to general-purpose reasoning and reading comprehension benchmarks.
- What evidence would resolve it: Benchmarking the model on domain-specific Macedonian datasets or conducting qualitative expert evaluation.

### Open Question 3
- Question: What performance improvements result from expanding the context window beyond 4,096 tokens?
- Basis in paper: [explicit] The authors identify the "current 4k context-length limitation" and state future work will address this to support applications like multi-document summarization.
- Why unresolved: The current training configuration limits the model's utility for tasks requiring long-term context.
- What evidence would resolve it: Fine-tuning the model with context extension techniques (e.g., RoPE scaling) and evaluating on long-context tasks.

### Open Question 4
- Question: To what extent do additional datasets like COPA-MK and OPUS contribute to model robustness?
- Basis in paper: [explicit] The authors state they "plan to incorporate additional datasets, such as COPA-MK... to further improve model robustness."
- Why unresolved: These resources were identified for future integration but were not included in the current training or ablation studies.
- What evidence would resolve it: An ablation study comparing the current model against a version retrained with these specific datasets.

## Limitations
- Evaluation scope is narrow, focusing on classification/reading comprehension while ignoring generation quality, factuality, or multilingual capabilities
- Claims about grammatical correctness and cultural appropriateness are supported by limited native speaker testing (15 participants)
- The relative contribution of each filtering stage to final model quality remains unquantified

## Confidence
- **High Confidence**: Model outperforms all existing 8B-parameter Macedonian models on established benchmarks; corpus size and composition are well-documented; evaluation protocol is transparent
- **Medium Confidence**: Claims about grammatical correctness and cultural appropriateness are supported by limited native speaker testing (15 participants); pretraining vs. SFT ablation results show expected trends but small sample size
- **Low Confidence**: The mechanism by which continued pretraining from multilingual models specifically benefits Macedonian (versus other low-resource languages) is not empirically isolated; the 10× performance comparison to larger models lacks statistical significance testing

## Next Checks
1. Conduct ablation study isolating each filtering stage (PII removal, C4 filtering, Gopher filtering, language identification, deduplication) to quantify individual contributions to model performance
2. Expand qualitative evaluation with larger participant pool (n≥50) and blind comparison against commercial Macedonian models to validate native speaker preference claims
3. Test model on multilingual tasks (cross-lingual transfer, translation quality) to assess whether continued pretraining maintains or enhances multilingual capabilities of the base model