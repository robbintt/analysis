---
ver: rpa2
title: Continual Personalization for Diffusion Models
arxiv_id: '2510.02296'
source_url: https://arxiv.org/abs/2510.02296
tags:
- concept
- neurons
- concepts
- continual
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Concept Neuron Selection (CNS), a method
  for continual personalization of diffusion models. CNS addresses the challenge of
  incrementally learning new visual concepts in text-to-image diffusion models while
  preventing catastrophic forgetting and preserving zero-shot generation capabilities.
---

# Continual Personalization for Diffusion Models

## Quick Facts
- **arXiv ID:** 2510.02296
- **Source URL:** https://arxiv.org/abs/2510.02296
- **Authors:** Yu-Chien Liao; Jr-Jen Chen; Chi-Pin Huang; Ci-Siang Lin; Meng-Lin Wu; Yu-Chiang Frank Wang
- **Reference count:** 40
- **Primary result:** CNS achieves state-of-the-art performance on multi-concept personalization tasks, outperforming existing methods in image and text alignment metrics while requiring minimal parameter updates and eliminating the need for fusion operations.

## Executive Summary
This paper introduces Concept Neuron Selection (CNS), a method for continual personalization of diffusion models that incrementally learns new visual concepts while preventing catastrophic forgetting and preserving zero-shot generation capabilities. The core innovation is identifying and updating only a small set of concept-specific neurons in cross-attention layers, distinguished from general neurons used for image generation. By fine-tuning these concept-specific neurons incrementally and applying targeted regularization, CNS achieves superior performance on multi-concept personalization tasks compared to existing methods, with significant computational efficiency gains due to minimal parameter updates and elimination of fusion operations.

## Method Summary
CNS operates by first identifying "concept neurons" in the cross-attention layers (W^k, W^v) of diffusion models through a scoring mechanism that combines weight magnitudes with text embedding norms. These are distinguished from "general neurons" used for base image generation by analyzing activation patterns across diverse calibration prompts. The method then fine-tunes only the concept-specific neurons while applying regularization to prevent forgetting of previously learned concepts. This approach requires minimal parameter updates (approximately 0.13% of total parameters) and eliminates the need for fusion operations or storing additional model weights, resulting in significant computational efficiency improvements over existing personalization methods.

## Key Results
- CNS outperforms existing methods (Mix-of-Show, DreamBooth variants) in both image alignment (CLIP-I) and text alignment (CLIP-T) metrics on multi-concept personalization tasks
- The method achieves superior performance while requiring only ~0.13% of total parameters to be updated, resulting in significant computational efficiency
- CNS successfully prevents catastrophic forgetting, maintaining zero-shot generation capabilities for generic prompts while learning new concepts
- Human evaluation confirms CNS produces higher quality images with better concept alignment compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1: Sparse Localization via Magnitude-Activation Scoring
- **Claim:** Identifying "concept neurons" allows the model to isolate parameters responsible for specific visual semantics, minimizing interference during updates.
- **Mechanism:** CNS calculates an importance score for neurons in the cross-attention layers ($W^k, W^v$) by taking the element-wise product of weight magnitudes and the $\ell_2$ norm of text embeddings. This isolates "Base Neurons" highly responsive to the target concept.
- **Core assumption:** High activation magnitude combined with semantic input correlates directly to the neuron's functional importance for that concept.
- **Evidence anchors:** Section 4.2 defines the scoring function $S(W^k, c) = |W^k| \odot (\mathbf{1} \cdot \|c\|_2)$; abstract notes CNS "uniquely identifies neurons... closely related to the target concepts."

### Mechanism 2: Functional Subtraction of General Generative Capability
- **Claim:** Removing "General Neurons" from the candidate set preserves the model's zero-shot generation capabilities.
- **Mechanism:** CNS identifies neurons activated by a diverse set of calibration prompts (unrelated to the target). It logically subtracts these ($\neg M_{general}$) from the base neurons to find the specific "Concept Neurons."
- **Core assumption:** There exists a distinct set of neurons responsible for general image synthesis that can be mathematically separated from concept-specific neurons.
- **Evidence anchors:** Section 4.2 and Figure 3 show ~53% overlap in neurons across prompts; Equation 6 ($M_{concept} = M_{base} \land \neg M_{general}$) formalizes the subtraction.

### Mechanism 3: Selective Regularization for Catastrophic Prevention
- **Claim:** Regularizing only the overlapping weights between new concepts and previous states prevents forgetting without freezing the entire model.
- **Mechanism:** The loss $L_{reg}$ applies a constraint specifically to the intersection of current concept neurons and previously tuned neurons ($M_{reg}$). It pulls weights toward both the previous step ($W_{m-1}$) and the initial pre-trained state ($W_0$).
- **Core assumption:** Concept interference is localized to the overlapping neurons; non-overlapping neurons can be updated freely without side effects.
- **Evidence anchors:** Section 4.3 defines $L_{reg}$ (Eq. 7) and the mask $M_{reg}$; abstract claims mitigation of "catastrophic forgetting problems while preserving zero-shot... ability."

## Foundational Learning

- **Concept: Cross-Attention Layers in Diffusion (U-Net)**
  - **Why needed here:** CNS operates exclusively on the Key ($W^k$) and Value ($W^v$) mappings of cross-attention layers, where text prompts condition the image generation.
  - **Quick check question:** Can you explain why $W^k$ and $W^v$ are targeted instead of the Query ($W^q$) or fully connected layers?

- **Concept: Catastrophic Forgetting in Incremental Learning**
  - **Why needed here:** The paper defines its success by solving this specific failure mode. Understanding that updating weights for "Concept B" traditionally degrades performance for "Concept A" is required to grasp the utility of $L_{reg}$.
  - **Quick check question:** What happens to the loss for "Concept A" if we fine-tune the model purely on "Concept B" images without regularization?

- **Concept: LoRA (Low-Rank Adaptation) vs. Full Finetuning**
  - **Why needed here:** The paper explicitly contrasts itself against the "LoRA fusion" paradigm (e.g., Mix-of-Show). CNS updates the base model weights directly rather than creating adapter modules.
  - **Quick check question:** Why does the paper claim "fusion-free" operation is a computational advantage over LoRA-based personalization methods?

## Architecture Onboarding

- **Component map:** Input (few-shot images + Captioner) -> Base Selector (scores neurons, generates $M_{base}$) -> General Selector (identifies shared neurons, generates $M_{general}$) -> Mask Generator (logic gate: $M_{base}$ AND NOT $M_{general}$ -> Output $M_{concept}$) -> Training Loop (diffusion loss + $L_{reg}$ applied to weights masked by $M_{reg}$)

- **Critical path:** The **General Neuron Selection** is the most sensitive component. If your calibration prompts aren't diverse enough, you will fail to filter out the 53% of "general" neurons, accidentally updating them, and destroying the model's zero-shot capabilities.

- **Design tradeoffs:**
  - **Thresholding (Top 30%):** The paper selects the top 30% of neurons by score. Lowering this increases sparsity (less forgetting risk) but risks underfitting the concept. Raising it improves concept capture but risks damaging prior knowledge.
  - **Prompt Diversity (K=20):** Using fewer calibration prompts speeds up selection but risks misclassifying specific neurons as "general."

- **Failure signatures:**
  - **Zero-shot degradation:** Generated images look broken or like noise for generic prompts (e.g., "a dog"). Cause: $M_{general}$ mask was too small; you updated neurons essential for base generation.
  - **Concept Bleeding/Confusion:** Generated images show attributes of Concept A when prompting for Concept B. Cause: $M_{reg}$ failed to regularize the intersection of neurons between concepts.
  - **Underfitting:** The model ignores the few-shot examples. Cause: $M_{concept}$ is too sparse.

- **First 3 experiments:**
  1. **Verify General Neuron Overlap:** Before training, visualize the heatmap of selected neurons for 5 distinct prompts (as in Fig 3). If you don't see high overlap (~50%), your scoring function is likely broken.
  2. **Single-Concept Ablation:** Train on one concept with $M_{concept}$ active but $L_{reg}=0$. Check if generic prompts (e.g., "a car") still work. This validates the "General Neuron" subtraction mechanism.
  3. **Two-Concept Interference:** Train Concept A, then Concept B. Generate Concept A. If quality drops, debug $M_{reg}$ (check if intersection neurons are actually being regularized).

## Open Questions the Paper Calls Out
- **Open Question 1:** Can the Concept Neuron Selection (CNS) framework be effectively extended to tasks such as knowledge editing and machine unlearning across different data modalities? (The conclusion states the proposed scheme could "possibly be extended to tackle knowledge editing and unlearning tasks, not limited to particular data modality.")
- **Open Question 2:** How does high semantic similarity between sequential concepts impact the efficacy of the regularization mask? (The paper relies on a regularization mask ($M_{reg}$) and assumes concepts are relatively distinct.)
- **Open Question 3:** Is the identification of "general neurons" robust against variations in the calibration prompt set? (The paper does not analyze if the selection of "general neurons" changes significantly if the calibration prompts are skewed toward specific domains.)

## Limitations
- The method relies heavily on the assumption that concept-specific neurons can be cleanly separated from general generative neurons, which may not generalize to all architectures or concept types
- The sensitivity to the threshold for neuron selection (top 30%) and calibration prompt diversity is not extensively explored
- The approach may struggle with concepts that naturally overlap with general image generation capabilities or when concepts are semantically very similar

## Confidence
- **High confidence:** Catastrophic forgetting mitigation through selective regularization is well-grounded in continual learning literature
- **Medium confidence:** Superiority over existing methods is demonstrated empirically but relies on specific benchmark choices
- **Low confidence:** Generalizability of the neuron scoring mechanism across different diffusion model architectures and robustness to concept types with high overlap with general generative capabilities

## Next Checks
1. **Architecture Transfer Test:** Apply CNS to a different diffusion model architecture (e.g., SDXL or a latent diffusion variant) and measure concept learning performance and zero-shot preservation compared to the original results.
2. **Concept Overlap Analysis:** Systematically test the method with concepts that have high semantic overlap with general image generation (e.g., "photorealistic detail," "natural lighting") to identify failure modes and refine the general neuron selection process.
3. **Threshold Sensitivity Study:** Conduct an ablation study varying the top-k percentage (10%, 20%, 30%, 50%) for neuron selection to quantify the tradeoff between concept learning capacity and zero-shot preservation.