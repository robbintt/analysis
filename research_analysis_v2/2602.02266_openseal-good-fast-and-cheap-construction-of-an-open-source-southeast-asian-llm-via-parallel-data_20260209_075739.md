---
ver: rpa2
title: 'OpenSeal: Good, Fast, and Cheap Construction of an Open-Source Southeast Asian
  LLM via Parallel Data'
arxiv_id: '2602.02266'
source_url: https://arxiv.org/abs/2602.02266
tags:
- data
- parallel
- training
- multilingual
- southeast
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the use of parallel data for extending\
  \ large language models (LLMs) to new languages. The authors conduct controlled\
  \ experiments comparing different training strategies\u2014multilingual, mixed,\
  \ parallel-first, parallel-last, and parallel-only\u2014during continual pretraining."
---

# OpenSeal: Good, Fast, and Cheap Construction of an Open-Source Southeast Asian LLM via Parallel Data

## Quick Facts
- arXiv ID: 2602.02266
- Source URL: https://arxiv.org/abs/2602.02266
- Reference count: 12
- Primary result: Parallel data alone outperforms multilingual and mixed data approaches for extending LLMs to new languages

## Executive Summary
This paper investigates the use of parallel data for extending large language models (LLMs) to new languages. The authors conduct controlled experiments comparing different training strategies—multilingual, mixed, parallel-first, parallel-last, and parallel-only—during continual pretraining. Their findings show that using only parallel data is the most effective approach for improving multilingual performance. Based on this, they develop OpenSeal, a fully open-source Southeast Asian LLM, by continually pretraining an English-only LLM on just 34.7B tokens of parallel data. OpenSeal achieves competitive or superior performance compared to existing Southeast Asian LLMs trained on much larger datasets, demonstrating that parallel data alone can efficiently extend LLMs to new languages. The work highlights the effectiveness of bilingual alignment in cross-lingual transfer and provides a transparent, open-source model for further research.

## Method Summary
The authors continually pretrain an English-only LLM (OLMo 2) on parallel data from nine Southeast Asian languages using a PARALLELONLY strategy. They use a 25% replay of original English pretraining data to prevent catastrophic forgetting. The model is trained for 34.7B tokens with a fixed 25% replay ratio across all experimental settings. The parallel data consists of 403.8M sentence pairs from NLLB and Thai sources, formatted as "source lang: sentence\ntarget lang: sentence" blocks of 262,144 tokens. Training uses the WSD scheduler, AdamW optimizer, and global batch sizes of 512 (1B model) or 1024 (7B model).

## Key Results
- Parallel-only training (OpenSeal) significantly outperforms multilingual and mixed data approaches on translation tasks
- OpenSeal achieves competitive or superior performance compared to models trained on 200-500B tokens using only 34.7B parallel tokens
- The 25% replay ratio effectively prevents catastrophic forgetting while enabling efficient cross-lingual learning

## Why This Works (Mechanism)

### Mechanism 1: Cross-Lingual Alignment via Parallel Data
Continual pretraining on parallel data provides explicit, sentence-level alignment between source and target languages, allowing the model to directly map its existing English semantic knowledge to new languages without inferring alignments from raw monolingual text.

### Mechanism 2: Efficient Token Utilization via Focused Training
Parallel data serves dual purposes—reinforcing source language while teaching target language through direct mapping. This is more sample-efficient for cross-lingual tasks than monolingual data, which requires independent learning of language structure before implicit alignment.

### Mechanism 3: Stability from High-Ratio Replay Data
A fixed 25% replay of original English pretraining data provides sufficient grounding to prevent catastrophic forgetting during aggressive continual pretraining, allowing the model to retain strong reasoning capabilities while learning new languages.

## Foundational Learning

- **Continual Pretraining (CPT)**: Adapting a frozen, pretrained model to new domains/languages; faster and more data-efficient than training from scratch but risks catastrophic forgetting and overfitting.
  - Quick check: What are the two primary risks when performing CPT on an LLM with new data? (Answer: Catastrophic forgetting of original knowledge and overfitting to the new data)

- **Parallel vs. Monolingual Data**: Parallel data provides sentence-aligned pairs offering direct cross-lingual mapping, unlike monolingual data which requires the model to independently learn language structure.
  - Quick check: In this context, what is the primary proposed advantage of parallel data over monolingual data for extending an LLM to a new language? (Answer: It provides explicit cross-lingual alignment, enabling more efficient knowledge transfer from the source language)

- **Catastrophic Forgetting**: The core problem where a model loses its original capabilities when trained on new data.
  - Quick check: How does the inclusion of replay data in the training mixture mitigate catastrophic forgetting? (Answer: By continually exposing the model to data from its original distribution, it anchors the model's weights and prevents overwriting of core knowledge)

## Architecture Onboarding

- **Component map**: Base Model (OLMo 2) -> Data Formatter (creates 262K token blocks) -> Parallel Data + Replay Data streams -> Tokenizer (fixed OLMo 2) -> Training Loop (WSD scheduler, AdamW)
- **Critical path**: Parallel Data Quality - the entire mechanism relies on high-quality sentence alignments; misaligned or noisy parallel data will produce flawed cross-lingual mapping
- **Design tradeoffs**: 
  - Parallel-Only vs. General Fluency: optimizes for cross-lingual transfer over deep monolingual cultural knowledge
  - Simplicity vs. Complexity: uses simple data mixing instead of complex model expansion or parameter freezing
  - Replay Budget: 25% of compute spent on replay data to ensure stability
- **Failure signatures**:
  - English Performance Collapse: poor English benchmarks indicate replay ratio too low
  - Poor Translation BLEU Scores: low scores suggest parallel data quality issues or tokenizer inefficiency
  - Incoherent Multilingual Output: gibberish production indicates overfitting or excessive learning rate
- **First 3 experiments**:
  1. Sanity Check: Parallel-Only vs. Multilingual (1B scale) on small subset (2B tokens) to confirm parallel-only advantage
  2. Ablate the Replay Ratio: test different ratios (0%, 10%, 25%, 50%) on 1B Parallel Only model to find optimal balance
  3. Tokenizer Analysis: measure tokens-per-word efficiency on SEA languages vs. baseline tokenizer to quantify vocabulary bottleneck

## Open Questions the Paper Calls Out

1. How does parallel-only continual pretraining interact with subsequent instruction tuning, and does the parallel-only advantage persist through this additional training stage?
2. Does the effectiveness of parallel-only training generalize beyond Southeast Asian languages to other typologically diverse, low-resource language families?
3. How does parallel-only continual pretraining affect model safety and value alignment, particularly regarding cross-lingual consistency of harmful refusal behaviors?

## Limitations

- The parallel corpus may not fully represent linguistic diversity within SEA languages and may miss culturally-specific knowledge not present in English translations
- The effectiveness of parallel-only training may not extend to all language families, particularly those with very different typological features from English
- Evaluation relies heavily on translation and reasoning benchmarks, which may not comprehensively capture real-world multilingual capabilities

## Confidence

**High Confidence** - The empirical finding that parallel-only training outperforms multilingual and mixed approaches for translation tasks is well-supported by experimental results.

**Medium Confidence** - The broader claim that parallel data alone is "most effective" for extending LLMs to new languages is supported within the studied context but requires additional validation.

**Low Confidence** - The assertion that this approach will generalize to all language families or provide a universally optimal solution lacks sufficient evidence.

## Next Checks

1. Cross-Linguistic Generalization Test: Evaluate OpenSeal's performance when extending to languages from different families (Turkish, Finnish, Japanese) to test whether parallel-only approach maintains effectiveness across typologically diverse languages.

2. Cultural Knowledge Assessment: Design benchmark tasks testing cultural knowledge and idiomatic expressions unique to SEA languages not present in English translations to determine whether parallel-only training creates blind spots in culturally-specific understanding.

3. Optimal Replay Ratio Analysis: Conduct systematic ablation studies varying replay ratio (0%, 10%, 25%, 50%, 75%) while controlling for total training tokens to identify whether 25% is truly optimal or if different ratios provide better trade-offs.