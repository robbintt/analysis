---
ver: rpa2
title: Asymptotically Optimal Problem-Dependent Bandit Policies for Transfer Learning
arxiv_id: '2509.19098'
source_url: https://arxiv.org/abs/2509.19098
tags:
- prior
- bound
- transfer
- proof
- regret
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies non-contextual multi-armed bandits with prior
  data, extending classical regret bounds to incorporate transfer parameters. It derives
  a problem-dependent asymptotic lower bound for regret that explicitly depends on
  the prior sample sizes, distance bounds, and source-target closeness.
---

# Asymptotically Optimal Problem-Dependent Bandit Policies for Transfer Learning

## Quick Facts
- **arXiv ID:** 2509.19098
- **Source URL:** https://arxiv.org/abs/2509.19098
- **Reference count:** 40
- **Primary result:** Derives a problem-dependent asymptotic lower bound for regret in non-contextual multi-armed bandits with prior data and proposes KL-UCB-Transfer, an index policy matching this bound in the Gaussian case.

## Executive Summary
This paper studies the non-contextual multi-armed bandit problem where the learner has access to prior data from a related source task. The authors derive a new problem-dependent asymptotic lower bound for regret that explicitly incorporates transfer parameters such as prior sample sizes, distance bounds, and source-target closeness. They propose KL-UCB-Transfer, a simple index policy that achieves this lower bound in the Gaussian case by adding a KL-penalty term for prior data. Theoretical analysis shows the algorithm achieves optimal logarithmic regret, and simulations demonstrate significant regret reductions when priors on suboptimal arms are accurate.

## Method Summary
The method addresses non-contextual stochastic multi-armed bandits with Gaussian rewards and known variances, where the learner receives prior data (N'_k samples) from source distributions ν'_k before playing the target bandit. The KL-UCB-Transfer algorithm selects arms by computing an upper confidence bound (index) U_a(t) for each arm, which is the solution to a constrained optimization that maximizes a candidate mean q subject to a weighted sum of divergences. This sum includes the standard online divergence term and a new penalty term f^+_a(q) derived from the prior source samples. The algorithm's index computation has an explicit closed-form formula with three cases based on the relationship between empirical means and prior bounds.

## Key Results
- Derives a problem-dependent asymptotic lower bound for regret that extends the classical Lai-Robbins result to incorporate transfer parameters (prior sample sizes N'_k, distance bounds L_k).
- Proposes KL-UCB-Transfer, an index policy that matches this new lower bound in the Gaussian case by adding a KL-penalty term for prior data.
- Shows through simulations that regret reductions are significant when priors on suboptimal arms are accurate, with no improvement when priors are too loose.
- Demonstrates that placing priors on suboptimal arms is safer than on the optimal arm, as optimistic priors on the optimal arm can increase short-term regret.

## Why This Works (Mechanism)

### Mechanism 1: KL-Penalty Index Construction
The algorithm adds a KL-penalty term for prior source data to the standard KL-UCB index, allowing it to achieve provably optimal asymptotic regret in the transfer setting. The index U_a(t) maximizes a candidate mean q subject to a weighted sum of divergences, including the standard online divergence term and a new penalty term f^+_a(q) derived from prior samples. This penalty term effectively shrinks the index towards the upper bound of the plausible source mean, forcing the index of suboptimal arms to be lower when priors are accurate and abundant, reducing unnecessary exploration.

### Mechanism 2: Transfer-Specific Asymptotic Lower Bound
The fundamental limit of any consistent transfer learning algorithm is strictly tighter than the classical Lai-Robbins bound, governed by transfer parameters. The new lower bound involves a term K_inf that captures the minimum divergence of the source distribution from an alternate distribution consistent with the target under the known distance constraint. If L_k is small and N'_k is large, this K_inf term inflates the denominator of the regret coefficient, proving a lower (better) asymptotic regret is achievable and required.

### Mechanism 3: Conditional Exploration via Hardship Parameter
Exploration of a suboptimal arm is determined by a "hardship" parameter η := L_1 - (μ'_1 - μ_1), capturing the alignment between the prior's uncertainty and the true optimal arm's mean. A tight optimistic prior on the optimal arm (small η) can increase short-term regret by artificially suppressing the optimal arm's index, causing more exploration of suboptimal arms. Conversely, a pessimistic but loose prior (large η) allows for faster initial convergence.

## Foundational Learning

- **Concept: KL-Divergence & Kinf Function**
  - Why needed here: The algorithm's core logic and its optimality proof are built on KL-divergence, with the penalty term being a squared divergence and the lower bound defined via a constrained KL-infimum (K_inf).
  - Quick check question: How does K_inf(ν'_k; d_k, ν̃_k, L_k) change if the distance bound L_k increases while holding all other parameters constant?

- **Concept: Stochastic Multi-Armed Bandits (MAB) & Consistency**
  - Why needed here: The entire problem is framed within the stochastic MAB setting, and the notion of a "consistent" algorithm (sub-polynomial regret) is prerequisite for the lower bound derivation to hold.
  - Quick check question: Why is a consistent algorithm required for the asymptotic lower bound to be meaningful? What would happen to the bound for an inconsistent algorithm?

- **Concept: Index Policies & Upper Confidence Bounds (UCB)**
  - Why needed here: KL-UCB-Transfer is an index policy that assigns a score (index) to each arm and greedily pulls the arm with the highest score, with the modification involving changing how this index is computed to incorporate prior data.
  - Quick check question: In the classical KL-UCB algorithm, what does the index U_a(t) represent, and how does adding a positive penalty term f^+_a(q) change its value?

## Architecture Onboarding

- **Component map:** Input Module -> Penalty Calculator -> Index Optimizer -> Policy Selector -> Online Estimator -> (back to Penalty Calculator)
- **Critical path:** 1) Initialization with priors, 2) Per-round calculation of indices for all arms, 3) Arm selection and environment interaction, 4) Update of online statistics
- **Design tradeoffs:**
  - Prior Accuracy vs. Gain: No benefit if priors are too loose (L_k large) or samples too few (N'_k small); tighter priors yield greater regret reduction
  - Optimal Arm Prior Risk: Placing tight priors on the optimal arm can increase short-term regret if the prior is optimistic; safer to place priors on suboptimal arms
  - Exploration Bonus: Choice of δ_t affects finite-time performance; smaller ε reduces over-exploration but might violate theoretical guarantees for extremely large T
- **Failure signatures:**
  1. Higher regret than no-transfer KL-UCB: Likely due to highly inaccurate priors (true d_k exceeding assumed L_k)
  2. Slower convergence: A very optimistic prior on the optimal arm causes over-exploration of suboptimal arms initially
  3. No improvement: Priors are too loose (L_k large) or prior sample sizes N'_k are too small to provide statistical advantage
- **First 3 experiments:**
  1. Sanity Check: Replicate Simulation 1, run KL-UCB-Transfer vs. standard KL-UCB on 6-armed Gaussian bandit with increasingly accurate priors, verify regret reduction correlates with prior tightness
  2. Stress Test: Test failure condition by running algorithm where true distance exceeds assumed radius L_k, measure performance degradation
  3. Ablation on Optimal Arm Prior: Replicate Simulation 2, systematically vary hardship parameter η by changing prior mean and radius for optimal arm, plot regret curves to observe short-term trade-off

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the optimality guarantees of KL-UCB-Transfer be extended to exponential families or bounded reward distributions?
- **Basis in paper:** The conclusion states it would be "interesting to extend our analysis beyond Gaussian rewards to exponential or bounded families."
- **Why unresolved:** Theoretical proofs currently rely on Gaussian properties and specific KL-divergence forms.
- **What evidence would resolve it:** A proof extending Theorem 6 to non-Gaussian classes or a counter-example showing sub-optimality.

### Open Question 2
- **Question:** How does the regret bound change if the prior data comes from a previous online bandit process rather than i.i.d. samples?
- **Basis in paper:** The conclusion identifies the study of prior data from "another online bandit process" as a "promising direction for future work."
- **Why unresolved:** The current analysis assumes prior samples N'_k are non-random and i.i.d., which may not hold for data from an adaptive process.
- **What evidence would resolve it:** A regret analysis for the transfer setting where source data exhibits dependencies induced by a bandit policy.

### Open Question 3
- **Question:** Can an algorithm maintain optimality if the distance bounds L_k are unknown and must be learned online?
- **Basis in paper:** The method assumes the learner knows the upper bounds L_k, which is a strict assumption in transfer learning.
- **Why unresolved:** The KL-UCB-Transfer index explicitly uses L_k in the penalty term f^+_a(q); incorrect values could violate theoretical guarantees.
- **What evidence would resolve it:** An adaptive algorithm that estimates the source-target distance while pulling arms and retains the asymptotic optimality of Theorem 6.

## Limitations

- Theoretical results rely heavily on the Gaussian assumption and correctly specified distance bounds L_k; if source and target distributions are not truly within L_k, performance may degrade below no-transfer baselines.
- The regret analysis is asymptotic; finite-time behavior shows that overly optimistic priors on the optimal arm can cause short-term regret spikes that may be significant in practical applications.
- The paper does not extensively test robustness to prior misspecification or provide complete characterization of how the hardship parameter affects regret over very long horizons.

## Confidence

- **High Confidence:** The core mechanism of adding a KL-penalty term to incorporate prior data is well-founded and the asymptotic optimality proof is rigorous within the Gaussian framework.
- **Medium Confidence:** The practical performance gains depend critically on having accurate priors (small L_k, large N'_k); the paper demonstrates this in simulations but real-world scenarios may have more uncertainty about prior accuracy.
- **Low Confidence:** The long-term impact of the optimal arm's prior (the η parameter) is less explored; the paper notes a short-term trade-off but does not provide complete characterization of how this affects regret over very long horizons or across different problem structures.

## Next Checks

1. **Robustness Test:** Systematically vary the true distance |μ_k - μ'_k| beyond the assumed L_k and measure performance degradation to quantify sensitivity to prior misspecification.
2. **Finite-Time Analysis:** Extend the simulation to very short horizons (e.g., T = 10^3) to characterize the short-term regret impact of the hardship parameter η and identify when the asymptotic regime begins.
3. **Non-Gaussian Extension:** Implement a simple variant of the algorithm for Bernoulli rewards (using KL-divergence between Bernoulli distributions) and test on a synthetic problem to assess the generality of the penalty term approach beyond the Gaussian case.