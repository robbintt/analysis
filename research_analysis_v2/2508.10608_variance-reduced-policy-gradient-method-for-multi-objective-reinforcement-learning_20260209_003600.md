---
ver: rpa2
title: Variance Reduced Policy Gradient Method for Multi-Objective Reinforcement Learning
arxiv_id: '2508.10608'
source_url: https://arxiv.org/abs/2508.10608
tags:
- lemma
- algorithm
- sample
- assumption
- proof
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses sample inefficiency in policy gradient methods
  for multi-objective reinforcement learning (MORL). The authors propose MO-TSIVR-PG,
  a variance-reduced policy gradient algorithm that achieves better sample complexity
  than previous methods while maintaining scalability to large state-action spaces.
---

# Variance Reduced Policy Gradient Method for Multi-Objective Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2508.10608
- **Source URL:** https://arxiv.org/abs/2508.10608
- **Reference count:** 40
- **Primary result:** Achieves ε-stationary convergence with Õ(M³/ε³) samples, improving sample complexity over prior MORL methods

## Executive Summary
This paper addresses sample inefficiency in policy gradient methods for multi-objective reinforcement learning (MORL). The authors propose MO-TSIVR-PG, a variance-reduced policy gradient algorithm that achieves better sample complexity than previous methods while maintaining scalability to large state-action spaces. The key innovation is implementing variance reduction without explicitly tracking occupancy measures, enabling operation in continuous action spaces. Theoretical analysis shows MO-TSIVR-PG achieves ε-stationary convergence with Õ(M³/ε³) samples and global convergence with Õ(M⁵/ε²) samples, improving upon prior work. Experiments on DeepSeaTreasure and Server Queues environments demonstrate superior performance compared to existing methods.

## Method Summary
The MO-TSIVR-PG algorithm samples two trajectory batches per iteration to independently estimate gradients and rewards, uses importance sampling for variance reduction, and projects value estimates onto the valid domain. The method operates in epochs where each epoch performs multiple variance-reduced gradient steps using incremental updates. Unlike previous approaches, it avoids tracking occupancy measures by leveraging the structure of MORL settings, enabling scalability to continuous action spaces. The algorithm uses projected gradient descent with a truncation radius to control importance sampling variance.

## Key Results
- Achieves ε-stationary convergence with Õ(M³/ε³) samples versus Õ(M⁴/ε⁴) for non-variance-reduced methods
- Global convergence with Õ(M⁵/ε²) samples improving upon prior MORL approaches
- Superior performance on DeepSeaTreasure and Server Queues environments compared to MO-PG baseline
- Maintains scalability to continuous action spaces by avoiding explicit occupancy measure tracking

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Variance reduction improves sample complexity from Õ(M⁴/ε⁴) to Õ(M³/ε³) for stationary convergence.
- **Mechanism:** Importance sampling weights enable off-policy gradient estimation without tracking occupancy measures λ. By storing J and g estimates from previous iterations and computing incremental corrections via IS-weighted trajectory differences, variance accumulates as Σ||θ^{j'-1} - θ^{j'}||²/B rather than growing linearly with iterations.
- **Core assumption:** Policy log-likelihood is G-Lipschitz and B-smooth (Assumption 2); ||θ₁ - θ₂|| ≤ δ for bounded IS weight variance (Lemma 3).
- **Evidence anchors:** [abstract] "implementing variance reduction techniques to reduce the sample complexity"; [Page 3] "without the need to explicitly keep track of the occupancy measure λ by leveraging the structure of MORL settings"; [Page 4] Lemma 5 shows variance bound includes C₃/B · Σ E[||θ^{j'-1} - θ^{j'}||²] term enabling control through batch size B; [corpus] Related work (Zhang et al. 2021) tracks occupancy measures, losing scalability—this paper explicitly avoids that constraint.

### Mechanism 2
- **Claim:** Sampling two independent trajectory batches per iteration yields better dependency on number of objectives M.
- **Mechanism:** Batch N₁ estimates J^i_j (cumulative rewards); batch N₂ estimates g^i_j (policy gradient). Independence decouples estimation error in J from gradient variance. Theorem 2 achieves Õ(M³/ε³) vs Theorem 1's Õ(M⁴/ε⁴).
- **Core assumption:** Trajectories are sampled independently; reward functions bounded in [0,1] (Assumption 1).
- **Evidence anchors:** [Page 4] "At every iteration, we sample two batches of trajectories instead of just one. This allows us to estimate the gradient and the sum of discounted rewards independently, leading to a better bound in Lemma 5"; [Page 13-14] Lemma C.3 proof explicitly separates J estimation variance from gradient variance.

### Mechanism 3
- **Claim:** Projecting value estimates onto Ω = [0, 1/(1-γ)]^M enables natural scalarization function assumptions.
- **Mechanism:** P^i_j = Proj_Ω(J^i_j) ensures ∇f(P^i_j) remains well-defined. Without projection, IS-weighted J estimates could exceed theoretical bounds, causing gradient estimator instability.
- **Core assumption:** Scalarization f has locally L_f-Lipschitz partial derivatives on Ω (Assumption 3); f need not extend beyond Ω.
- **Evidence anchors:** [Page 3-4] "P^i_j = Proj_Ω(J^i_j)... allows for more natural assumptions on f, which does not need to be defined outside the domain Ω. At the same time, the use of P^i_j prevents importance sampling from giving an unrealistic estimate of J"; [Page 10] Corollary 1 uses non-expansiveness of projection for variance bounds.

## Foundational Learning

- **Concept: Importance Sampling in Policy Gradient**
  - Why needed here: Core technique enabling variance reduction without occupancy measure tracking. Must understand how ω_t reweights trajectories from old policy π_{θ₁} to estimate expectations under new policy π_{θ₂}.
  - Quick check question: Given trajectory sampled under π_{θ₁}, write the IS weight for time step t=3.

- **Concept: Multi-Objective Scalarization**
  - Why needed here: The algorithm optimizes f(J(θ)) where J ∈ ℝ^M. Understanding how gradient ∇_θ f(J(θ)) = (∇_θ J)ᵀ ∇_J f decomposes is essential for implementing the gradient estimator.
  - Quick check question: If f(J₁, J₂) = √(J₁ + σ) + √(100 + J₂ + σ), compute ∂f/∂J₁.

- **Concept: Variance Reduction via Control Variates**
  - Why needed here: Algorithm 2 computes g^i_j = g(θ^i_j, P^i_j) - g(θ^i_j, θ^i_{j-1}, P^i_{j-1}) + g^i_{j-1}. This is a control variate structure where the IS-adjusted difference reduces variance compared to naive estimation.
  - Quick check question: Why does subtracting g(θ^i_j, θ^i_{j-1}, P^i_{j-1}) and adding g^i_{j-1} preserve unbiasedness while reducing variance?

## Architecture Onboarding

- **Component map:**
  Outer Loop (Epochs i=0..T-1) -> Inner Loop (Iterations j=0..m-1) -> j=0 branch: Full batch sampling -> j>0 branch: Incremental variance reduction -> θ update: θ^{j+1} = Proj_{B(θ^j, δ)}(θ^j + η g^j)

- **Critical path:**
  1. Implement gradient estimator g(τ|θ, Ĵ) from Equation on Page 3—this is the core computation
  2. Implement IS weight ω_t computation with numerical stability (log-space for long horizons)
  3. Implement Proj_Ω and Proj_{B(θ,δ)} projections
  4. Wire batch sampling with independent random streams for N^i_1, N^i_2 (or B^i_{j,1}, B^i_{j,2})

- **Design tradeoffs:**
  - **δ (gradient truncation radius):** Smaller δ → tighter IS variance bounds but slower convergence. Paper uses δ = 1/(2GH) where H is horizon.
  - **B vs N:** Large N reduces initial variance; large B enables longer epoch length m. Theorem 2 sets B = M^{3/2}ε^{-1}, N = B² = M³ε^{-2}.
  - **Batch doubling:** Two batches per iteration doubles sample cost per iteration but improves M-dependency from M⁴ to M³.

- **Failure signatures:**
  - **Exploding IS weights:** ω_t > 1e6 indicates ||θ^{j-1} - θ^j|| too large—reduce η or increase δ
  - **NaN in gradient:** f undefined at P^i_j—check projection implementation and scalarization domain
  - **No convergence improvement vs baseline:** Verify IS weights computed correctly (should have E[ω_t] = 1)

- **First 3 experiments:**
  1. **Validate IS weight statistics:** Sample trajectories under θ₁, compute E[ω_t(τ|θ₁, θ₂)] for various ||θ₁-θ₂||. Should equal 1 (Lemma B.2). Deviation indicates implementation error.
  2. **Compare Algorithm 1 vs 2 on small MDP:** Use DeepSeaTreasure with fixed hyperparameters from paper (T=1000, N=288 for MO-PG; N=144, B=12, m=13 for MO-TSIVR-PG). Plot optimality gap vs epochs.
  3. **Sweep M (number of objectives):** Run Server Queues with M ∈ {8, 16, 32, 64}. Verify sample complexity scales as M³ (Theorem 2) not M⁴ (Theorem 1) by measuring epochs to reach fixed ε.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical bounds rely heavily on Assumptions 1-3 (bounded rewards, Lipschitz continuity, gradient truncation)
- IS variance bounds show exponential growth in horizon H and gradient norm G, suggesting struggles with long-horizon tasks
- Experiments only validate on two environments (DeepSeaTreasure and Server Queues) with M ≤ 8 objectives
- Strong requirement that scalarization function has locally Lipschitz partial derivatives limits applicability

## Confidence

- **High confidence:** Sample complexity bounds (Õ(M³/ε³) for stationary convergence, Õ(M⁵/ε²) for global convergence) - directly derived from theorems with clear proofs
- **Medium confidence:** Mechanism claims - the variance reduction mechanism is theoretically sound, but practical performance depends on sensitive hyperparameters (δ, η, B)
- **Medium confidence:** Scalability claims - the algorithm avoids explicit occupancy measure tracking, but IS weight computation still scales with trajectory length

## Next Checks

1. **IS Weight Stability Test:** For varying ||θ₁ - θ₂|| values, measure E[ω_t] and V[ω_t] on Server Queues with M=32. Verify exponential growth predicted by Lemma A.1 and confirm implementation handles numerical stability.

2. **M-Scaling Experiment:** Run Server Queues with M ∈ {8, 16, 32, 64} and measure epochs to reach ε-stationary point. Fit power law to confirm M³ scaling rather than M⁴.

3. **Gradient Truncation Impact:** Run DeepSeaTreasure with different δ values (0.1, 0.05, 0.01) while keeping η fixed. Measure convergence speed and IS weight magnitudes to quantify tradeoff between stability and convergence rate.