---
ver: rpa2
title: 'Language Matters: How Do Multilingual Input and Reasoning Paths Affect Large
  Reasoning Models?'
arxiv_id: '2505.17407'
source_url: https://arxiv.org/abs/2505.17407
tags:
- reasoning
- language
- english
- prefill
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how language choice affects the reasoning
  processes of large reasoning models (LRMs). The authors find that despite multilingual
  training, LRMs predominantly default to reasoning in high-resource languages (primarily
  English and Chinese) regardless of the input language.
---

# Language Matters: How Do Multilingual Input and Reasoning Paths Affect Large Reasoning Models?

## Quick Facts
- arXiv ID: 2505.17407
- Source URL: https://arxiv.org/abs/2505.17407
- Reference count: 40
- Multilingual LRMs default to high-resource language reasoning regardless of input, causing up to 30pp accuracy drops for low-resource languages.

## Executive Summary
This study investigates how language choice affects reasoning processes in large reasoning models (LRMs). Despite multilingual training, LRMs predominantly default to reasoning in high-resource languages (primarily English and Chinese) regardless of input language. When constrained to reason in the same language as input, model performance significantly declines, especially for low-resource languages. Conversely, reasoning in high-resource languages generally preserves or improves performance on reasoning tasks, while the opposite effect occurs in non-reasoning tasks like cultural understanding and safety evaluations. The study introduces a segmentation-classification method to analyze reasoning patterns and identifies that language-specific prefill tokens correlate with distinct reasoning strategies.

## Method Summary
The authors evaluate multilingual LRMs on reasoning (MATH-500, MMMLU) and non-reasoning tasks (CulturalBench-Hard, LMSYS-Toxic) across eight languages. They implement language steering via prefill tokens (e.g., "Okay" for English, "嗯" for Chinese) prepended to prompts to control reasoning language. A segmentation-classification pipeline (ModernBERT + Gemini-2.0-flash) analyzes reasoning patterns into four habits plus "Others." Models include DeepSeek-R1-Distill-Llama-8B, DeepSeek-R1-Distill-Qwen-14B, QwQ-32B, and Qwen3-30B-A3B, evaluated under controlled decoding parameters.

## Key Results
- LRMs default to English/Chinese reasoning >90% of the time regardless of input language
- Input-language reasoning degrades performance by up to 30pp on reasoning tasks for low-resource languages
- Chinese prefill correlates with subgoal setting (r=0.51), English with backward chaining (r=0.30)

## Why This Works (Mechanism)

### Mechanism 1: Reasoning Hub Default Behavior
- **Claim**: LRMs internally default to "reasoning hub" languages (English, Chinese) for chain-of-thought generation regardless of input language, while maintaining output language alignment.
- **Mechanism**: During RL training, models exploit the language allowing most effective CoT generation to maximize reward. Since reasoning language is typically not an explicit reward component, models converge toward the base model's strongest language (usually English). The internal "thinking" language becomes decoupled from external "responding" language.
- **Core assumption**: Base model pre-training creates language-specific reasoning capabilities that RL training leverages as shortcuts.
- **Evidence anchors**:
  - [abstract] "LRMs tend to default to reasoning in high-resource languages (e.g., English) at test time, regardless of the input language."
  - [Section 3, Figure 2] Shows QwQ-32B and Qwen3-30B-A3B reason in English >90% of cases across all input languages, while answer sections maintain input language.
  - [Appendix H, Table 14] Base model (Qwen3-30B-A3B without post-training) achieves pass@8 of 0.267 with English phrases vs 0.133-0.200 for other languages on AIME-2024.

### Mechanism 2: Performance-Safety Trade-off via Language Selection
- **Claim**: Reasoning language choice creates asymmetric effects: hub languages improve reasoning task accuracy but can increase toxicity and reduce cultural understanding; non-hub languages have opposite effects.
- **Mechanism**: High-resource languages have richer reasoning pattern representations, enabling better decomposition and verification. However, safety guardrails and cultural knowledge may be more robustly encoded in native language contexts where these concepts were originally trained.
- **Core assumption**: Safety training and cultural knowledge transfer differently across languages than reasoning capabilities.
- **Evidence anchors**:
  - [abstract] "input-language reasoning degrades performance on reasoning tasks but benefits cultural tasks, while safety evaluations exhibit language-specific behavior"
  - [Table 1] MATH-500: Swahili reasoning degrades by 31.5 percentage points vs English reasoning; Japanese by 18.5 pp.
  - [Table 3] LMSYS-Toxic: QwQ-32B shows lower ASR (attack success rate) when reasoning in native language for Japanese (9.5% vs 9.9%), Korean (3.8% vs 4.6%), Chinese (7.4% vs 7.7%).

### Mechanism 3: Language-Activated Reasoning Strategy Schemas
- **Claim**: Different language prefill tokens activate distinct reasoning strategies: Chinese promotes subgoal setting and verification; English promotes backward chaining; low-resource languages show negative correlations with structured reasoning patterns.
- **Mechanism**: Language-specific tokens prime culturally embedded problem-solving schemas. Chinese's correlation with subgoal setting (r=0.51) may reflect structural emphasis in Chinese mathematical education; English's correlation with backward chaining (r=0.30) may reflect Anglo-Saxon proof-based reasoning traditions.
- **Core assumption**: These patterns reflect training data correlations between language and reasoning approaches, not inherent linguistic properties.
- **Evidence anchors**:
  - [abstract] "Chinese promoting subgoal setting (Pearson's r = 0.51) and English encouraging backward chaining (Pearson's r = 0.30)"
  - [Section 5.2, Figure 5] Correlation matrix shows Chinese prefill with subgoal setting (r=0.50, p<0.001) and verification (r=0.41, p<0.001); English with backward chaining (r=0.34, p<0.01); Swahili negatively correlated with subgoal setting (r=-0.35, p<0.01).

## Foundational Learning

- **Concept**: Chain-of-Thought (CoT) Reasoning in LLMs
  - **Why needed here**: The entire paper assumes understanding of "thinking" vs "answering" phases in LRMs. Without grasping that models generate explicit intermediate reasoning steps before final answers, the hub phenomenon makes little sense.
  - **Quick check question**: Can you explain why forcing a model to "show its work" might change which language it uses internally?

- **Concept**: Token-Level Language Control via Prefilling
  - **Why needed here**: The paper's intervention method—prepending language-specific tokens to steer reasoning language—requires understanding how autoregressive models condition on initial tokens.
  - **Quick check question**: Why would starting a response with "Okay" vs "嗯" shift the entire subsequent generation's language distribution?

- **Concept**: Reinforcement Learning from Verifiable Rewards (RLVR)
  - **Why needed here**: The paper hypothesizes that hub emergence stems from RL training exploiting highest-reward languages. Understanding reward optimization is essential for diagnosing why this happens.
  - **Quick check question**: If reasoning language isn't part of the reward function, why would models converge on specific languages during RL training?

## Architecture Onboarding

- **Component map**: Input Prompt → Tokenizer → [Prefill Token Injection Point] → LRM Backbone → Thinking Phase (CoT) → Answer Phase → Language Detection (validation)
- **Critical path**: The prefill injection point (between `<assistant>/experiment_tokens/` and model generation) is the single control lever. Incorrect tokenization or placement breaks the intervention entirely. The segmentation-classification pipeline (ModernBERT → Gemini-2.0-flash) is analysis-only, not inference-critical.
- **Design tradeoffs**:
  - **Prefilling vs Token Masking**: Prefilling is cross-lingually robust but requires finding representative phrases; masking is more direct but fails for languages with few tokens (e.g., Japanese has only 1,410 tokens in Llama-3 tokenizer vs 4,225 for Chinese).
  - **Hub language default vs forced alignment**: Defaulting to English maximizes reasoning accuracy (up to +42pp on MATH-500 for Japanese) but sacrifices cultural nuance and can increase toxicity. No single setting optimizes all tasks.
- **Failure signatures**:
  - **Silent language drift**: Model appears to follow prefill but switches mid-generation. Verify with language detection on full thinking trace, not just first tokens.
  - **Tokenization mismatch**: Prefill phrase tokenizes differently than expected (especially for non-Latin scripts). Always verify token IDs.
  - **Over-attribution**: Attributing performance changes to reasoning language when input language changed simultaneously. Control for both independently.
- **First 3 experiments**:
  1. **Establish baseline hub behavior**: Run MATH-500 in 3 languages (EN, ZH, SW) without prefilling. Use language detection to confirm natural reasoning language distribution. Expected: >90% English reasoning for low-resource inputs.
  2. **Validate prefill control**: For same 3 languages, apply prefill tokens (Table 9 phrases). Measure thinking language alignment (>95% target language) and answer language preservation (should match input). Expected: Successful steering with minimal answer degradation.
  3. **Quantify performance-safety trade-off**: Run English vs native-language prefilling on both MATH-500 and LMSYS-Toxic for 2 languages (ES, RU). Expected: English prefill improves math (Table 1 pattern) but may increase toxicity (Table 3 pattern shows language-specific effects).

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but the limitations section suggests several important directions: the phenomenon may evolve at larger scales (>30B parameters), the analysis is limited to medium-scale LRMs, and the authors cannot establish causal relationships between language and reasoning strategies without more controlled experiments.

## Limitations
- **Tokenization Sensitivity**: Prefilling approach depends on finding frequent first-phrases, which is harder for morphologically rich or character-based writing systems (Japanese tokenizer has only 1,410 tokens vs 4,225 for Chinese).
- **Base Model Quality Differences**: Performance differences between languages may be confounded by underlying base model's pre-training language distribution rather than reasoning-specific capabilities.
- **Single Dataset Validation**: Safety and cultural understanding findings rely on single datasets (LMSYS-Toxic, CulturalBench-Hard) which may not generalize across different cultural contexts or toxicity definitions.

## Confidence

**High Confidence**: The observation that LRMs default to high-resource languages for reasoning regardless of input language (supported by consistent patterns across multiple models and tasks in Tables 1-2, and Figure 2). The performance degradation when forcing low-resource language reasoning (up to 30-40 percentage points drops on MATH-500, documented in Table 1).

**Medium Confidence**: The correlation between language-specific prefill tokens and reasoning strategies (Pearson correlations of 0.30-0.51, statistically significant in Section 5.2). While statistically robust, the causal mechanism linking token priming to strategy activation requires further validation.

**Low Confidence**: The safety trade-off mechanism - that hub language reasoning improves reasoning accuracy but increases toxicity. This requires more extensive safety evaluation across diverse cultural contexts to establish as a general principle rather than dataset-specific artifact.

## Next Checks

1. **Cross-Tokenizer Validation**: Replicate the prefilling experiment using a tokenizer with more balanced token distributions across languages (e.g., SentencePiece with larger vocabulary). Compare steering efficacy and reasoning quality to validate that current results aren't tokenizer artifacts.

2. **Reasoning-Only Ablation**: Create a controlled experiment where the same reasoning task is presented with and without the requirement for explicit CoT generation. Compare language drift patterns to determine if hub behavior is specific to CoT-enabled LRMs versus general LLMs.

3. **Safety Generalization Test**: Extend the safety evaluation to multiple toxicity datasets (e.g., RealToxicityPrompts, Jigsaw) and cultural understanding benchmarks across different cultural dimensions. Test whether the hub language safety trade-off holds across diverse cultural contexts and toxicity definitions.