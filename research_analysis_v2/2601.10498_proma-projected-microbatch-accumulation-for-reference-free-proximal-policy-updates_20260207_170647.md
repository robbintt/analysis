---
ver: rpa2
title: 'PROMA: Projected Microbatch Accumulation for Reference-Free Proximal Policy
  Updates'
arxiv_id: '2601.10498'
source_url: https://arxiv.org/abs/2601.10498
tags:
- policy
- gradient
- proma
- microbatch
- grad
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper tackles the instability of proximal policy updates that\
  \ rely on a reference policy and likelihood\u2011ratio clipping, which can cause\
  \ excessive KL drift or premature entropy collapse. PROMA solves this by, during\
  \ backward\u2011pass accumulation, projecting the partially accumulated gradient\
  \ onto the orthogonal complement of the sequence\u2011wise log\u2011probability\
  \ gradients of the current microbatch; the projection is applied layer\u2011wise\
  \ and can be approximated with O(kd) cost."
---

# PROMA: Projected Microbatch Accumulation for Reference‑Free Proximal Policy Updates  

## Quick Facts  
- **arXiv ID:** 2601.10498  
- **Source URL:** https://arxiv.org/abs/2601.10498  
- **Reference count:** 15  
- **Primary result:** PROMA achieves validation performance on GSM8K comparable to or better than GRPO while keeping global KL drift far lower than unclipped REINFORCE and preserving policy entropy longer.  

## Executive Summary  
PROMA addresses the instability of proximal policy updates that rely on a reference policy and likelihood‑ratio clipping. By projecting the partially accumulated gradient onto the orthogonal complement of the sequence‑wise log‑probability gradients of the current microbatch, the method removes components that would otherwise drive excessive KL divergence or premature entropy collapse. An intra‑microbatch variant (Intra‑PROMA) further reduces overhead by using low‑rank approximations of activation and gradient subspaces, enabling efficient data‑parallel training. Experiments fine‑tuning Qwen‑3 0.6B on GSM8K show that PROMA matches or exceeds the GRPO baseline in validation score, while maintaining a much smaller KL drift and higher entropy throughout training.  

## Method Summary  
PROMA modifies the backward pass of policy‑gradient training. As microbatches are processed, gradients are accumulated but, before each accumulation step, the gradient is projected layer‑wise onto the orthogonal complement of the current microbatch’s log‑probability gradient vectors. This projection removes directions that would increase the KL distance to the current policy, effectively enforcing a reference‑free proximal constraint. The projection can be computed in O(k d) time (k = microbatch size, d = hidden dimension) and is applied per layer. Intra‑PROMA approximates the required subspaces with low‑rank factorizations, allowing the projection to be performed in parallel across data‑parallel workers with minimal communication.  

## Key Results  
- Validation scores on GSM8K are on par with or exceed the GRPO baseline.  
- Global KL divergence stays far below that of unclipped REINFORCE throughout training.  
- Policy entropy decays more slowly, indicating reduced risk of premature collapse.  

## Why This Works (Mechanism)  

### Mechanism 1 – Orthogonal Gradient Projection  
- **Claim:** Removing gradient components aligned with the sequence‑wise log‑probability gradients limits KL drift.  
- **Mechanism:** The projection onto the orthogonal complement eliminates updates that would increase the likelihood ratio beyond a safe region, acting as a reference‑free proximal constraint.  
- **Core assumption:** The dominant directions of KL increase are captured by the log‑probability gradients of the current microbatch.  
- **Evidence anchors:** Empirical KL curves show PROMA’s KL remains substantially lower than REINFORCE; entropy curves remain higher, supporting smoother updates.  

### Mechanism 2 – Layer‑wise, Low‑Complexity Computation  
- **Claim:** The projection can be performed with O(k d) cost per layer, making it scalable.  
- **Mechanism:** By computing the Gram matrix of the k log‑probability gradients per layer and solving a small linear system, the orthogonal component is obtained without full‑matrix operations.  
- **Core assumption:** k (microbatch size) is much smaller than d (layer width), so the Gram matrix is cheap to invert.  
- **Evidence anchors:** Reported runtime overhead is modest for the 0.6 B‑parameter model; no wall‑clock analysis is provided for larger models.  

### Mechanism 3 – Low‑Rank Subspace Approximation (Intra‑PROMA)  
- **Claim:** Approximating activation and gradient subspaces with low rank enables parallel data‑parallel training with negligible accuracy loss.  
- **Mechanism:** Singular‑value decomposition (or similar) yields a compact basis; projections are performed in this reduced space, cutting communication and compute.  
- **Core assumption:** The dominant subspace captures most of the KL‑relevant information.  
- **Evidence anchors:** Intra‑PROMA is described as “parallel‑friendly,” but quantitative speed‑up or accuracy trade‑off data are not presented.  

## Foundational Learning  
- **Proximal Policy Optimization (PPO) & KL control** – needed to understand why limiting KL divergence stabilizes policy updates. *Quick check:* Can you write the PPO surrogate loss with a KL penalty?  
- **Orthogonal projection in high‑dimensional spaces** – required to grasp how removing specific gradient components affects the update direction. *Quick check:* What is the formula for projecting a vector onto the orthogonal complement of a subspace spanned by matrix A?  
- **Microbatch gradient accumulation** – essential for appreciating why the projection is applied incrementally rather than after full‑batch accumulation. *Quick check:* How does accumulating gradients over microbatches differ from a single large batch in terms of memory usage?  
- **Low‑rank matrix approximation** – underpins Intra‑PROMA’s efficiency gains. *Quick check:* What error bound does a rank‑r SVD provide for approximating a matrix?  
- **Entropy regularization in RLHF** – relevant for interpreting the reported entropy preservation. *Quick check:* How does adding an entropy bonus to the loss affect policy exploration?  

## Architecture Onboarding  
- **Component map:** Data loader → Microbatch sampler → Forward pass (policy) → Log‑probability gradient extractor → Orthogonal projection module (layer‑wise) → Gradient accumulator → Parameter update → (Intra‑PROMA adds low‑rank subspace estimator & parallel synchronizer)  
- **Critical path:** Forward pass → Log‑probability gradient extraction → Orthogonal projection → Gradient accumulation → Optimizer step. The projection step is the new bottleneck but remains O(k d).  
- **Design tradeoffs:**  
  - *Accuracy vs. overhead*: Exact projection guarantees maximal KL control but adds compute; low‑rank approximation reduces cost at possible loss of control.  
  - *Microbatch size*: Larger k improves projection fidelity but increases O(k d) cost.  
  - *Layer‑wise granularity*: Per‑layer projection offers fine control but requires repeated subspace computations.  
- **Failure signatures:**  
  - Sudden spikes in KL despite projection → subspace approximation too coarse.  
  - Training slowdown > 30 % → projection implementation not efficiently vectorized.  
  - Entropy collapse early → orthogonal complement not correctly computed (e.g., numerical instability).  
- **First 3 experiments:**  
  1. Replicate the GSM8K fine‑tuning of Qwen‑3 0.6B using PROMA, reporting mean ± std of validation score, KL, and entropy over ≥ 5 seeds.  
  2. Ablate the projection cost by varying microbatch size k and measuring wall‑clock time and KL control.  
  3. Compare exact projection vs. low‑rank Intra‑PROMA (different rank choices) on the same task to assess trade‑off between speed and KL/entropy behavior.  

## Open Questions the Paper Calls Out  
- Does the orthogonal‑complement projection provide a formal guarantee of bounded KL or prevent entropy collapse?  
- How does the method scale to models substantially larger than 0.6 B parameters (e.g., 2 B, 6 B)?  
- Will the KL‑control and performance benefits generalise to other RL‑style tasks beyond GSM8K (e.g., summarisation, dialogue)?  
- What is the impact of the low‑rank approximation rank on stability and final performance?  
- Is the code and full hyper‑parameter set publicly available to enable exact reproducibility?  

## Limitations  
- No formal proof that the orthogonal projection bounds KL or guarantees entropy preservation.  
- Scalability analysis is limited to a 0.6 B‑parameter model; larger models are not evaluated.  
- Experiments are confined to a single task (GSM8K) and a single model size, limiting evidence of generality.  

## Confidence  
- **PROMA reduces global KL drift relative to unclipped REINFORCE** → *Medium*  
- **PROMA matches or exceeds GRPO validation scores** → *Low‑Medium*  
- **Layer‑wise orthogonal projection can be computed in O(k d) and is effective** → *Medium*  
- **Intra‑PROMA enables parallel data‑parallel training with low‑rank approximations** → *Low*  

## Next Checks  
1. **Replication on GSM8K:** Re‑run the Qwen‑3 0.6B fine‑tuning experiment, reporting mean ± std of validation score, global KL, and entropy across ≥ 5 random seeds; perform statistical tests against GRPO and REINFORCE baselines.  
2. **Cross‑task generalisation:** Apply PROMA (and Intra‑PROMA) to a distinct RL‑style dataset (e.g., summarisation with RL‑HF) and evaluate whether KL control and performance gains persist.  
3. **Scalability & overhead audit:** Measure wall‑clock time, GPU memory, and FLOPs for the projection step across microbatch sizes (8, 16, 32) and model sizes (0.6 B, 2 B, 6 B) to verify the claimed O(k d) scaling.