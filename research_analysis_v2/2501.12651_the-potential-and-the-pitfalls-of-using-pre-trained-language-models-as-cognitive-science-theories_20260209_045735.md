---
ver: rpa2
title: The potential -- and the pitfalls -- of using pre-trained language models as
  cognitive science theories
arxiv_id: '2501.12651'
source_url: https://arxiv.org/abs/2501.12651
tags:
- plms
- human
- cognitive
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies key pitfalls when using pre-trained language
  models (PLMs) as cognitive science theories. It classifies pitfalls into two categories:
  those of commission (methodological mistakes such as distal linking hypotheses)
  and those of omission (neglecting broader contexts like psychometric and developmental
  data).'
---

# The potential -- and the pitfalls -- of using pre-trained language models as cognitive science theories

## Quick Facts
- arXiv ID: 2501.12651
- Source URL: https://arxiv.org/abs/2501.12651
- Reference count: 18
- This paper identifies key pitfalls when using pre-trained language models (PLMs) as cognitive science theories, classifying them into methodological mistakes (commission) and neglected contexts (omission).

## Executive Summary
This paper provides a comprehensive analysis of the challenges in using pre-trained language models as cognitive science theories. The authors systematically identify pitfalls that researchers face when attempting to map PLM behaviors to human cognitive processes, categorizing them into methodological errors and neglected contextual factors. The work reviews various linking hypotheses used to connect model outputs to human performance measures, highlighting limitations in each approach including tokenization issues, context sensitivity problems, and interpretability challenges. The authors propose specific criteria for evaluating PLMs as credible cognitive models, emphasizing the need for multiple experiments, transparency in linking hypotheses, and consideration of developmental trajectories. The paper serves as a guide for researchers to design robust experiments and accurately interpret PLM behaviors in cognitive science contexts.

## Method Summary
The paper employs a systematic review and analytical approach to identify pitfalls in using PLMs as cognitive science theories. The authors classify pitfalls into two categories: those of commission (methodological mistakes) and those of omission (neglected contexts). They review various linking hypotheses used to map model outputs to human performance measures, including similarity computations, surprisal values, and prompting methods. The analysis involves examining the limitations of each approach through theoretical arguments and examples from existing literature. The authors also propose criteria for evaluating and developing PLMs as credible cognitive models, emphasizing the need for multiple experiments, transparency in linking hypotheses, and consideration of developmental trajectories.

## Key Results
- Classification of pitfalls into methodological mistakes (commission) and neglected contexts (omission) provides a structured framework for evaluating PLM use in cognitive science
- Review of linking hypotheses reveals fundamental limitations including tokenization effects, context sensitivity issues, and lack of interpretability in mapping model outputs to human cognitive measures
- Proposal of evaluation criteria emphasizing multiple experiments, transparent linking hypotheses, and developmental considerations as essential for credible cognitive modeling with PLMs

## Why This Works (Mechanism)
None

## Foundational Learning
1. **Linking hypotheses**: Formal mappings between model outputs and human cognitive measures
   - Why needed: Without explicit linking hypotheses, researchers cannot validly interpret PLM behaviors as cognitive phenomena
   - Quick check: Can you clearly state the mathematical or logical relationship between your model's output and the human cognitive measure?

2. **Tokenization constraints**: How models segment text into processing units
   - Why needed: Tokenization fundamentally shapes how models process and represent language, affecting cognitive modeling validity
   - Quick check: Does your cognitive task involve units smaller than your model's minimum token size?

3. **Developmental trajectory analysis**: Examining how model performance changes during training
   - Why needed: Understanding training progression helps distinguish genuine cognitive progressions from artifacts of curriculum design
   - Quick check: Can you access intermediate checkpoints to verify developmental patterns?

## Architecture Onboarding
- **Component map**: PLM architecture (layers, attention heads) -> Tokenization layer -> Output layer -> Linking hypothesis -> Cognitive measure
- **Critical path**: Tokenization → Layer activations → Linking hypothesis → Cognitive prediction
- **Design tradeoffs**: Depth vs. interpretability (deeper models show better performance but harder to interpret), tokenization granularity vs. processing efficiency
- **Failure signatures**: Tokenization mismatches causing spurious correlations, context sensitivity leading to inconsistent predictions, lack of interpretability preventing validation of cognitive claims
- **First experiments**: 1) Test model performance across different tokenization schemes for the same cognitive task, 2) Compare multiple linking hypotheses on identical cognitive measures, 3) Analyze model behavior with controlled context variations

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Are the developmental trajectories observed in PLMs genuine cognitive progressions or merely artifacts of the pre-training curriculum design?
- **Basis in paper:** [explicit] The authors state that "Studies are needed to analyze training corpora and assess the impact of training curriculum design" to distinguish genuine progressions from training artifacts.
- **Why unresolved:** Resource constraints and the unavailability of intermediate checkpoints often limit the analysis of developmental fidelity, making it difficult to isolate the effects of data ordering.
- **What evidence would resolve it:** Experiments that systematically vary the sequence and nature of pre-training data and compare the resulting model trajectories against established human developmental milestones.

### Open Question 2
- **Question:** Do PLMs replicate the specific patterns of cross-task correlations (psychometric structure) observed in human cognitive abilities?
- **Basis in paper:** [explicit] The authors propose a new evaluation paradigm to "evaluate whether the pattern of cross-task correlations observed in humans is also produced by PLMs."
- **Why unresolved:** Current studies typically focus on isolated cognitive abilities (the experimental approach) rather than the differential approach which analyzes correlations across a broad range of abilities.
- **What evidence would resolve it:** Comprehensive benchmarking that generates correlation matrices for suites of cognitive tests (e.g., mathematical, verbal, spatial) performed by the same model, compared against human psychometric data.

### Open Question 3
- **Question:** Can the functional units of a PLM be validly mapped to the functional units of the human brain?
- **Basis in paper:** [explicit] The paper notes that despite attempts to map model components to brain regions, "its viability (which requires a correspondence between NLP software and neural hardware) remains an open question."
- **Why unresolved:** There are fundamental architectural differences between biological neural networks and artificial ones implemented in silicon, and current research in this mapping is in its infancy.
- **What evidence would resolve it:** Successful identification of consistent, functional correspondences between specific PLM layers or attention heads and brain regions during matched cognitive tasks.

## Limitations
- The classification of pitfalls into commission and omission categories is somewhat subjective and the boundary between these categories can be blurry in practice
- The paper's treatment of tokenization effects, while important, doesn't fully explore how this fundamental architectural constraint systematically biases cognitive modeling efforts
- Discussion of interpretability focuses on general concerns without providing concrete frameworks for assessing when a PLM's internal representations might be meaningful as cognitive representations

## Confidence
- Claims about the utility of the pitfall classification framework: Medium
- Claims about specific linking hypothesis limitations: Medium
- Claims about developmental trajectory concerns: Medium

## Next Checks
1. Conduct a systematic literature review documenting actual cases where the identified pitfalls led to erroneous cognitive conclusions, providing empirical grounding for the theoretical critiques
2. Design benchmark experiments that directly compare different linking hypotheses on the same cognitive tasks to empirically evaluate which approaches are most robust to the identified limitations
3. Develop and validate a standardized checklist or framework that researchers can use to evaluate whether their specific use of PLMs as cognitive models adequately addresses the identified pitfalls