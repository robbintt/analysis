---
ver: rpa2
title: Vision Language Models as Values Detectors
arxiv_id: '2501.03957'
source_url: https://arxiv.org/abs/2501.03957
tags:
- images
- image
- annotators
- elements
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigated how well large language models with visual
  input can detect value-relevant elements in home environment images compared to
  human annotators. Researchers created 12 scenario images and collected annotations
  from 14 participants, then compared these to outputs from five models including
  GPT-4o and four LLaVA variants.
---

# Vision Language Models as Values Detectors

## Quick Facts
- arXiv ID: 2501.03957
- Source URL: https://arxiv.org/abs/2501.03957
- Reference count: 11
- Primary result: VLMs showed poor alignment with human value detection in home environment images, with best model achieving only 0.42 alignment score

## Executive Summary
This study evaluated how well vision language models (VLMs) can detect value-relevant elements in domestic images compared to human annotators. Researchers created 12 scenario images and collected annotations from 14 participants, then compared these to outputs from five models including GPT-4o and four LLaVA variants. Results showed models were not well-aligned with human perception, with the best model (LLaVA 34B) achieving only 0.42 alignment score. Analysis revealed models often provided descriptive rather than value-aware responses, though they occasionally detected nuanced elements humans missed. The study suggests VLMs have potential for value detection with improved training and prompting, which could enhance applications in social robotics, assistive technologies, and human-computer interaction.

## Method Summary
The study used 12 diffusion-model-generated images of domestic scenarios (10 value-laden, 2 control) and collected annotations from 14 participants who identified the most relevant element in each image. These human-derived elements served as ground truth for comparison. Five VLMs (GPT-4o and four LLaVA variants) were evaluated using a standardized prompt asking them to identify elements needing attention. Each model generated three completions per image at temperature=1, with responses truncated to the first sentence. Alignment was scored as binary (0 or 1) by comparing the first sentence to the human-derived ground truth, then averaged across all images and runs. Statistical significance between models was tested using Cochran's Q test.

## Key Results
- VLMs showed poor alignment with human value detection, with the best model (LLaVA 34B) achieving only 0.42 alignment score
- Models consistently defaulted to descriptive responses (identifying objects) rather than value-aware responses (expressing concern or awareness), accounting for 36% of misalignments
- Larger models performed better, with LLaVA 34B (34B parameters) scoring 0.42 versus LLaVA Mistral 7B (7B parameters) scoring 0.14, though statistical significance was not achieved
- In 16% of misaligned responses, models provided value-aware responses where humans gave descriptive ones, suggesting occasional complementary detection

## Why This Works (Mechanism)

### Mechanism 1: Visual-Textual Feature Projection for Value Detection
VLMs can detect value-relevant elements by projecting visual features into language model embedding space, enabling semantic reasoning about image content. A pre-trained visual encoder (e.g., CLIP) extracts visual features from images, which are then projected into the word embedding space of a language model (e.g., LLaMA) via a trained projection matrix. This allows the language model to reason about visual content using its textual understanding capabilities.

### Mechanism 2: Prompt-Guided Attention Steering
Simple prompts can shift model behavior from descriptive responses toward value-aware identification, though current effectiveness is limited. The prompt "Identify what is the element that needs attention in this image" was designed to avoid purely descriptive outputs. The instruction to answer "none" if nothing is relevant aims to reduce false positives.

### Mechanism 3: Model Scale Correlates with Value Alignment (Conditional)
Larger parameter count may modestly improve value-relevant element detection, though statistical significance was not achieved. LLaVA 34B (largest tested) achieved 0.42 alignment versus 0.14 for LLaVA Mistral 7B (smallest). The paper speculates that greater model capacity enables more nuanced reasoning about contextual and value-laden elements.

### Mechanism 4: Occasional Complementary Detection
Models sometimes detect value-laden elements that human annotators miss, suggesting non-overlapping sensitivity patterns. In 16% of misaligned responses, models provided value-aware responses (type 2) where annotators gave descriptive responses (type 1). Examples include detecting health concerns in medication scenarios or second-hand smoke risks.

## Foundational Learning

- **Vision-Language Model Architecture**
  - Why needed here: Understanding how visual and textual modalities are combined is essential for interpreting why models produce descriptive versus value-aware outputs.
  - Quick check question: Can you explain how CLIP visual features are projected into a language model's embedding space?

- **Value-Relevant vs. Descriptive Responses**
  - Why needed here: The paper's core finding is that models default to description (type 1) rather than value-aware reasoning (type 2). Recognizing this distinction is critical for prompt and training design.
  - Quick check question: Given an image of a child drawing on a wall, what is the difference between a type 1 response ("the child") and a type 2 response ("the child is writing on the wall")?

- **Alignment Evaluation Methodology**
  - Why needed here: The binary alignment scoring and Cochran's Q test determine whether observed differences are meaningful.
  - Quick check question: Why does a p-value of 0.077 prevent concluding that model performance differs statistically?

## Architecture Onboarding

- **Component map**:
  - Image input → CLIP visual encoder → feature vector
  - Feature vector → projection matrix → embedding-space representation
  - Embedded visual features + prompt tokens → language model → text output
  - Output → first sentence extraction → binary alignment scoring

- **Critical path**:
  1. Image input → CLIP visual encoder → feature vector
  2. Feature vector → projection matrix → embedding-space representation
  3. Embedded visual features + prompt tokens → language model → text output
  4. Output → first sentence extraction → binary alignment scoring

- **Design tradeoffs**:
  - **Temperature = 1**: Balances repeatability with response variability; higher values increase diversity but reduce consistency
  - **100-token cutoff**: Forces concise answers; may truncate nuanced reasoning
  - **No system prompt**: Ensures comparability across models (Mistral variant lacks support); likely reduces potential performance

- **Failure signatures**:
  - **Descriptive default**: Model identifies objects without value context (36% of misalignments); suggests training data bias
  - **False negative**: Model answers "none" when value-relevant elements exist (29% of misalignments)
  - **Quality commentary**: Model critiques image framing rather than content (type 4 responses)
  - **Hallucinated concern**: Model focuses on irrelevant elements (e.g., bathroom cleanliness when man is crying in shower)

- **First 3 experiments**:
  1. **Prompt engineering test**: Add explicit value-aware framing (e.g., "consider health, safety, and social concerns") to the prompt; measure alignment improvement against baseline.
  2. **System prompt ablation**: For models supporting system prompts, compare performance with/without detailed value-detection instructions; isolate prompt architecture effects.
  3. **Fine-tuning on value-laden datasets**: Create a small dataset of images with annotated value-relevant elements; fine-tune LLaVA 7B and compare alignment gains versus parameter scaling to 34B.

## Open Questions the Paper Calls Out

- **Can prompt engineering and fine-tuning on value-laden datasets significantly improve VLM alignment with human value detection in images?**
  - Basis in paper: "By using fine-tuning datasets that emphasise value-laden contexts, vision LLMs can be better aligned with human perceptions. Correctly structured prompts that explicitly guide the model to consider value-related aspects could improve performance."
  - Why unresolved: The study used a single simple prompt and no fine-tuning; the authors suggest but do not test whether targeted interventions would improve the 0.42 maximum alignment score.
  - What evidence would resolve it: A follow-up study comparing baseline performance against models fine-tuned on value-laden image datasets and/or using systematically varied prompt formulations.

- **How does VLM value detection performance change when evaluated on real-world images rather than AI-generated scenarios?**
  - Basis in paper: "In the future, we plan to deploy and test this technology in the field, evaluating its behaviour with real images."
  - Why unresolved: All 12 test images were generated using diffusion models, which may have different visual characteristics or artifacts that affect model perception differently than natural photographs.
  - What evidence would resolve it: A comparative evaluation using matched pairs of synthetic and real photographs depicting similar scenarios, measuring alignment differences.

- **How does cultural background affect both human and model value detection alignment in home environment images?**
  - Basis in paper: "The alignment thus depends on the cultural background of the annotators in this case, and of the users in real-life scenarios" and reports annotators from three nationality groups, but does not analyze whether cultural differences correlate with annotation patterns or model alignment.
  - Why unresolved: With only 14 annotators across 3 nationality groups, the study lacks statistical power to examine cultural variation in value perception or whether models align better with certain cultural perspectives.
  - What evidence would resolve it: A larger-scale study with sufficient participants from distinct cultural backgrounds to enable statistical comparison of annotation patterns and model alignment across groups.

## Limitations

- **Sample size and generalizability**: The study tested only five VLMs on 12 hand-crafted images, creating uncertainty about performance on real-world scenarios. The controlled image set may not capture the full diversity of value-relevant situations that VLMs would encounter in deployment.

- **Statistical power**: With only 12 images and 168 total annotations, the sample size limits the ability to detect statistically significant differences between models. The Cochran's Q test yielded a p-value of 0.077, just above the conventional 0.05 threshold, preventing strong conclusions about performance differences.

- **Human baseline reliability**: The human alignment score of 0.78 includes within-group agreement variation. This suggests that even human annotators don't perfectly agree on what constitutes a value-relevant element, potentially making perfect model alignment impossible.

## Confidence

- **High Confidence**: The finding that VLMs generally default to descriptive rather than value-aware responses (36% of misalignments). This is consistently observed across all tested models and is supported by both quantitative alignment scores and qualitative analysis of response types.

- **Medium Confidence**: The claim that larger models perform better, with LLaVA 34B achieving 0.42 alignment versus 0.14 for LLaVA Mistral 7B. While a clear trend exists, the lack of statistical significance (p=0.077) and absence of direct evidence for why scale would improve value detection prevents stronger claims.

- **Low Confidence**: The assertion that VLMs "occasionally detected nuanced elements humans missed" (16% of misalignments). This is based on manual analysis of a small number of cases and may reflect model hallucinations or attention to irrelevant details rather than genuine value detection.

## Next Checks

1. **Scale-Performance Validation**: Test the scale-value alignment relationship by including additional model sizes (e.g., LLaVA 7B, 13B, 34B) on a larger, more diverse image set (50+ images). Measure whether the performance gap widens and becomes statistically significant.

2. **Prompt Engineering Study**: Design and test five different prompt variants that explicitly encourage value-aware reasoning (e.g., "consider health, safety, and social implications"). Compare alignment scores across models to determine if prompt design can overcome the descriptive bias.

3. **Cross-Domain Generalization**: Apply the same evaluation framework to non-domestic images (workplace, public spaces, medical settings) to assess whether the descriptive bias persists across different context types and whether model performance correlates with domain familiarity in training data.