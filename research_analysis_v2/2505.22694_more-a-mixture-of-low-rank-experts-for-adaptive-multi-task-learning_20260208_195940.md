---
ver: rpa2
title: 'MoRE: A Mixture of Low-Rank Experts for Adaptive Multi-Task Learning'
arxiv_id: '2505.22694'
source_url: https://arxiv.org/abs/2505.22694
tags:
- task
- lora
- more
- tasks
- rank
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inefficiency of LoRA in multi-task scenarios
  where different tasks require different ranks for optimal performance. MoRE introduces
  a Mixture of Low-Rank Experts approach that treats different ranks within a single
  LoRA module as specialized experts and uses an adaptive rank selector to dynamically
  assign appropriate ranks to tasks.
---

# MoRE: A Mixture of Low-Rank Experts for Adaptive Multi-Task Learning

## Quick Facts
- **arXiv ID:** 2505.22694
- **Source URL:** https://arxiv.org/abs/2505.22694
- **Reference count:** 14
- **Primary result:** MoRE achieves up to 88.8% average accuracy on multi-task benchmarks compared to 87.8% for standard LoRA methods

## Executive Summary
MoRE introduces a Mixture of Low-Rank Experts approach for multi-task learning that addresses LoRA's inefficiency when different tasks require different ranks. The method treats different ranks within a single LoRA module as specialized experts and uses an adaptive rank selector to dynamically assign appropriate ranks to tasks. By incorporating task embeddings learned through contrastive learning and a balanced data sampling strategy, MoRE significantly outperforms traditional LoRA and its variants while maintaining comparable inference efficiency on GLUE and commonsense reasoning tasks.

## Method Summary
MoRE extends LoRA by structuring low-rank matrices A and B as mixtures of experts at different ranks (r=1 to r_max). An adaptive rank selector uses task embeddings (learned via contrastive loss) and a gating network to dynamically choose the appropriate rank for each task. The method employs balanced data sampling to prevent high-data tasks from dominating training and applies linear scaling to lower-rank weights for stable updates. The approach maintains LoRA's parameter efficiency while enabling task-specific rank allocation.

## Key Results
- Achieves 88.8% average accuracy on GLUE benchmark with LLaMA2-7B
- Outperforms standard LoRA (87.8%) and LoRA variants on multi-task settings
- Shows clear clustering patterns in task embeddings indicating meaningful rank-task alignment
- Maintains comparable inference efficiency to standard LoRA despite MoE structure

## Why This Works (Mechanism)

### Mechanism 1
Treating different ranks within a single LoRA module as specialized experts enables parameter sharing while preserving task-specific adaptation. The low-rank matrices A and B are structured so that lower ranks contain shared information across tasks, while higher ranks capture task-specific nuances. When task T_t is assigned rank r_t, it uses A[:r_t,:] and B[:,:r_t], meaning higher-rank tasks inherit all lower-rank parameters plus additional capacity. Tasks in multi-task scenarios share common low-dimensional subspaces that can be captured by lower ranks.

### Mechanism 2
Contrastive learning-based task embeddings enable accurate rank selection by clustering similar tasks in embedding space. Task embeddings e_t are learned via contrastive loss that pulls sample representations toward their task embedding while pushing them away from other task embeddings. A gating network G(e_t) = softmax(W_g·e_t + b_g) maps task embeddings to rank probabilities, selecting argmax during forward pass. Task characteristics can be summarized in a fixed-dimensional embedding that correlates with optimal rank requirements.

### Mechanism 3
Linear scaling on lower-rank weights combined with balanced data sampling prevents overfitting to high-data tasks and stabilizes training. Since lower ranks are updated more frequently across tasks, linear scaling (r_t/|T|) reduces their effective learning rate. Balanced sampling weights each dataset inversely to its size: ϕ_t ∝ exp(|D_t|/Σ|D_i|), ensuring smaller datasets contribute proportionally. Gradient updates from different tasks should be balanced to prevent dominant tasks from overwhelming shared parameters.

## Foundational Learning

- **Concept: LoRA Low-Rank Decomposition**
  - **Why needed here:** MoRE builds directly on LoRA's BA decomposition; understanding that ΔW = BA with r ≪ min(m,d) is prerequisite to grasping rank-as-expert.
  - **Quick check question:** Can you explain why LoRA's parameter count is r(m+d) instead of md?

- **Concept: Mixture-of-Experts Gating**
  - **Why needed here:** The adaptive rank selector uses softmax-based gating; understanding sparse expert selection (and STE for differentiability) is essential.
  - **Quick check question:** Why does argmax require Straight-Through Estimator during backpropagation?

- **Concept: Contrastive Learning Objectives**
  - **Why needed here:** Task embeddings are learned via InfoNCE-style contrastive loss; understanding positive/negative pair construction is necessary.
  - **Quick check question:** In Eq. 8, what serves as the "positive" and what serves as "negatives" for a sample from task T_t?

## Architecture Onboarding

- **Component map:**
  Input → [Task Embedding e_t] → [Gating Network G(·)] → Rank Selection r_t → Frozen W_0 + [LoRA A[:r_t,:], B[:,:r_t]] → Output

- **Critical path:**
  1. Initialize task embeddings E via Kaiming initialization
  2. For each batch, sample task t with probability ϕ_t
  3. Compute gate probabilities p_t = softmax(W_g·e_t + b_g)
  4. Select r_t = argmax(p_t), truncate LoRA to rank r_t
  5. Apply STE for gradient flow: h = W_0·x + STE(p_t)[r_t] · (r_t/|T|) · B_t·A_t·x
  6. Backprop with L = L_gen + λL_con (λ=0.1)

- **Design tradeoffs:**
  - **Max rank r:** Higher r = more expert capacity but more parameters; paper uses r=8 (GLUE) or r=16 with 768-dim task embeddings
  - **Task embedding dimension h:** Too small fails to capture task nuance; too large requires more data; paper finds h=768 optimal
  - **λ for contrastive loss:** Paper finds λ=0.1 stable; higher values cause loss oscillation

- **Failure signatures:**
  - All tasks converging to same rank → task embeddings not differentiating (check embedding visualization)
  - Small-dataset tasks underperforming → balanced sampling not working (verify ϕ_t distribution)
  - Training instability on lower ranks → linear scaling missing or λ too high

- **First 3 experiments:**
  1. **Sanity check:** Single-task LoRA baseline with r∈{1,2,4,8,16} on your target tasks to verify rank-task sensitivity exists (reproduce Table 1 pattern)
  2. **Ablation:** Train MoRE without contrastive loss (λ=0) and verify performance drops (Table 5 shows 87.3→86.3 on GLUE)
  3. **Expert allocation visualization:** After training, plot rank distribution per task (Figure 2a style) to confirm meaningful differentiation vs. uniform allocation

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does MoRE scale to significantly larger Large Language Models (e.g., 13B or 70B parameters) compared to the T5-base and LLaMA2-7B models tested?
- **Basis in paper:** [explicit] The authors explicitly state in the Limitations section (Section 7) that "due to GPU device limitations, we do not apply MoRE to larger LLMs, such as 13B, 75B, etc."
- **Why unresolved:** The current experimental scope was restricted by computational resources to smaller or mid-sized backbone models.
- **What evidence would resolve it:** Benchmark results on standard multi-task datasets (like GLUE or SuperGLUE) using LLaMA-13B or larger variants, comparing performance stability and parameter efficiency relative to the 7B baseline.

### Open Question 2
- **Question:** Can MoRE consistently outperform baselines on Natural Language Generation (NLG) tasks, or is its primary advantage restricted to Natural Language Understanding (NLU)?
- **Basis in paper:** [explicit] Section 7 notes that while preliminary attempts were made on generation tasks (Appendix A), "detailed experiments are needed to better verify the effectiveness of MoRE" in this domain.
- **Why unresolved:** The main paper focuses heavily on NLU classification benchmarks (GLUE), and the NLG results provided show MoRE is merely comparable to full fine-tuning rather than superior.
- **What evidence would resolve it:** Comprehensive evaluation on a wider variety of generation datasets (beyond DART, E2E, WebNLG) showing statistically significant improvements over standard LoRA and full fine-tuning.

### Open Question 3
- **Question:** How can the inference latency introduced by the Mixture-of-Experts (MoE) structure be minimized without sacrificing the adaptive rank selection capabilities?
- **Basis in paper:** [explicit] Section 7 highlights that "since our approach is based on the MoE structure, which cannot be merged with the original model, it results in latency during inference."
- **Why unresolved:** Unlike standard LoRA where $BA$ can be merged into $W_0$, the dynamic routing in MoRE requires separate computation during inference, partially negating the efficiency gains.
- **What evidence would resolve it:** A modified architectural design or knowledge distillation technique that enables the low-rank experts to be merged into the backbone weights post-training, resulting in zero inference latency.

## Limitations
- **GPU constraints:** Current experiments limited to T5-base and LLaMA2-7B due to computational resources, leaving scaling to larger models unverified
- **NLG performance uncertainty:** Preliminary NLG results show MoRE is comparable to full fine-tuning rather than superior, requiring more comprehensive evaluation
- **Inference latency:** MoE structure prevents merging with backbone weights, introducing latency that partially negates efficiency gains

## Confidence

**High confidence (Claims well-supported by evidence):**
- Multi-task LoRA inefficiency requiring task-specific ranks
- Contrastive learning enables meaningful task embedding clustering
- Balanced sampling prevents high-data task domination
- Inference efficiency comparable to standard LoRA

**Medium confidence (Claims partially supported):**
- Shared-expert structure enables parameter efficiency gains
- Linear scaling on lower ranks improves stability
- Task embedding dimension (768) is optimal

**Low confidence (Claims lacking direct evidence):**
- Specific rank-task alignment patterns without extensive ablation
- Generalization to NLG tasks (only 3 tasks tested)

## Next Checks

1. **Rank sensitivity baseline:** Run single-task LoRA with r∈{1,2,4,8,16} on 3-4 target tasks to verify that different tasks indeed prefer different ranks before implementing MoRE.

2. **Expert allocation sanity check:** After training MoRE, visualize rank distribution per task (like Figure 2a). If all tasks cluster at same rank, task embeddings aren't differentiating properly.

3. **Contrastive loss ablation:** Train MoRE with λ=0 (no contrastive loss) and compare task embedding visualization and performance to verify task embeddings are necessary for rank selection.