---
ver: rpa2
title: Leveraging Large Language Models for Building Interpretable Rule-Based Data-to-Text
  Systems
arxiv_id: '2502.20609'
source_url: https://arxiv.org/abs/2502.20609
tags:
- triples
- code
- training
- text
- rule
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method that uses a large language model
  (LLM) to automatically generate interpretable rule-based Python code for data-to-text
  tasks. The approach trains a rule-based system by prompting an LLM to write code
  snippets that convert input RDF triples into fluent text, ensuring correctness via
  execution and testing.
---

# Leveraging Large Language Models for Building Interpretable Rule-Based Data-to-Text Systems

## Quick Facts
- arXiv ID: 2502.20609
- Source URL: https://arxiv.org/abs/2502.20609
- Reference count: 7
- Primary result: LLM-generated Python rules produce interpretable, GPU-free data-to-text with fewer hallucinations than fine-tuned BART, competitive BLEU/BLEURT

## Executive Summary
This paper introduces a method that uses a large language model (LLM) to automatically generate interpretable rule-based Python code for data-to-text tasks. The approach trains a rule-based system by prompting an LLM to write code snippets that convert input RDF triples into fluent text, ensuring correctness via execution and testing. Additional synthetic training instances are created for better generalization. The resulting system, written in pure Python, is fully interpretable, requires no GPU at inference, and produces text almost instantaneously on a single CPU. Experiments on WebNLG show that this rule-based system produces fewer hallucinations and runs faster than both fine-tuned BART and prompted LLM baselines, while achieving competitive BLEU and BLEURT scores. Human evaluation confirms fewer major errors and high controllability.

## Method Summary
The method uses an LLM (Llama 3 70B) to generate Python code snippets that convert RDF triples into fluent text. The process involves prompting the LLM with input triples and reference text, executing the generated code in a sandbox with timeout, and validating outputs using Levenshtein distance (threshold=5) against references. Rules are organized by predicate signatures, with a greedy splitting mechanism for unmatched inputs and a fallback template for unprocessable cases. Synthetic training data is generated by clustering co-occurring predicates in a graph and creating artificial (triples, reference) pairs via LLM. The system trains on 3,408 original + ~110k synthetic rules in ~7 hours on dual L40 GPUs, then runs inference in pure Python on CPU.

## Key Results
- Rule-based system produces fewer hallucinations than fine-tuned BART and prompted LLM baselines
- Inference runs ~83x faster than BART on GPU, with near-instantaneous response on single CPU
- BLEU score of 42.51 (vs 53.28 for BART) and BLEURT of 0.372 (vs 0.456 for BART) on WebNLG
- Human evaluation confirms fewer major errors (hallucinations, omissions) while maintaining high controllability

## Why This Works (Mechanism)

### Mechanism 1: LLM-as-Compiler for Rule Extraction
- Claim: Prompting an LLM to write executable Python code rather than directly generate text yields more reliable, interpretable outputs.
- Mechanism: The LLM receives input RDF triples and reference text, then generates parameterized Python functions that generalize to new subjects/objects while preserving the predicate-to-text mapping. Code is constrained by syntax and execution semantics, reducing the search space compared to free-form text generation.
- Core assumption: The LLM's code generation capability transfers to the structured data-to-text domain, and execution success correlates with semantic correctness.
- Evidence anchors:
  - [abstract] "trains a rule-based system by prompting an LLM to write code snippets that convert input RDF triples into fluent text, ensuring correctness via execution and testing"
  - [section 3.1, Step 2] "The code is then executed in a separate process with a predefined timeout. If the code terminates before the timeout, does not throw an error, and the Levenshtein distance between the output text and the reference is within a predefined range, the rule is considered correct"
  - [corpus] Weak direct corpus support; neighbor paper "LLM Agents Implement an NLG System from Scratch" (arXiv:2512.18360) explores a similar multi-agent rule generation framework.
- Break condition: If LLM code generation quality degrades significantly on out-of-domain predicates or complex linguistic phenomena (e.g., subordinate clauses), rule coverage gaps will accumulate.

### Mechanism 2: Predicate-Triggered Rule Selection with Greedy Splitting
- Claim: Organizing rules by predicate signatures enables compositional coverage even when exact rule matches do not exist.
- Mechanism: Input triples are matched against rule specifications (expected predicates + count). If no exact match exists, a greedy algorithm iteratively selects rules processing the largest subset of remaining triples. A fallback template handles unprocessable cases.
- Core assumption: Most real inputs decompose into predicate combinations seen during training, and greedy splitting approximates optimal decomposition.
- Evidence anchors:
  - [section 2] "If there is no matching rule, the input is split into several parts by a splitting mechanism that aims to minimize the number of splits by applying greedy search"
  - [section 2] "If no rule can be found by further splitting, the triples are converted to text by a default rule '{subject} {predicate} {object}'"
  - [corpus] No direct corpus comparison for this specific splitting heuristic.
- Break condition: If inputs contain many novel predicate combinations not covered by synthetic augmentation, the fallback template will dominate, reducing fluency.

### Mechanism 3: Synthetic Example Generation via Graph-Based Predicate Clustering
- Claim: Generating artificial training instances for co-occurring predicate pairs/triples/quadruples improves generalization to unseen combinations.
- Mechanism: A graph is built where nodes are predicates and edges connect co-occurring predicates. Connected components form clusters; large clusters (>20 nodes) are pruned. The LLM is then prompted to create synthetic (triples, reference text) pairs for predicate tuples within clusters, which feed back into rule generation.
- Core assumption: Co-occurrence patterns in training data predict test-time combinations, and LLM-generated synthetic references are sufficiently accurate for rule induction.
- Evidence anchors:
  - [section 3.2] "Each connected component in such a constructed graph represents an initial cluster of predicates"
  - [section 4.2] "We found the effect [of additional rules] on metrics to be minimal (BLEU gain of 0.3%, BLEURT and METEOR stay within 0.001). Nevertheless, we still retain these rules to increase fluency for predicate combinations unseen in training data"
  - [corpus] No direct corpus validation of this clustering approach for NLG.
- Break condition: Synthetic references may contain hallucinations that propagate into rules; the paper acknowledges this error accumulation pathway.

## Foundational Learning

- Concept: **RDF Triple Structure (subject, predicate, object)**
  - Why needed here: All rules operate on lists of RDF triples; understanding this representation is prerequisite to reading/writing rule code.
  - Quick check question: Given triples `(Paris, capital_of, France)` and `(Paris, population, 2.1M)`, what predicate list would a rule need to match?

- Concept: **Template-Based Natural Language Generation**
  - Why needed here: Generated rules typically fill string templates with extracted values; understanding template slots and variable binding is essential.
  - Quick check question: If a template is `"{subj} was born in {year}"`, what output results from `subj="Einstein", year="1879"`?

- Concept: **Levenshtein Distance for String Similarity**
  - Why needed here: Rule correctness is thresholded by Levenshtein distance (threshold=5) between generated and reference text.
  - Quick check question: What is the Levenshtein distance between "Mozart born 1756" and "Mozart was born in 1756"?

## Architecture Onboarding

- Component map:
  - **Rule Store**: Python list of `(predicate_signature, code_snippet)` pairs
  - **Rule Selector**: Matches input predicate set to rules; triggers greedy splitting if no exact match
  - **LLM Rule Generator**: Prompts LLM with (triples, reference) to produce candidate code; handles retry/correction loop
  - **Synthetic Data Module**: Clusters predicates, generates artificial (triples, reference) pairs via LLM, feeds to Rule Generator
  - **Executor Sandbox**: Isolated process with timeout for safe code execution and validation

- Critical path:
  1. Training: LLM Rule Generator → Executor Sandbox (validate) → Rule Store (accumulate)
  2. Synthetic augmentation: Predicate Clustering → LLM Synthetic Generator → LLM Rule Generator → Rule Store
  3. Inference: Rule Selector retrieves matching rule → execute in sandbox → return text

- Design tradeoffs:
  - **Interpretability vs. fluency**: Pure Python rules are fully auditable but may lack the surface variation of neural models
  - **Coverage vs. hallucination risk**: More synthetic rules improve coverage but may embed LLM hallucinations in generated templates
  - **Speed vs. quality**: CPU-only inference is ~83x faster than GPU-based BART, but BLEU scores are lower (42.51 vs. 53.28)
  - **Assumption**: The paper does not prove this tradeoff generalizes beyond WebNLG

- Failure signatures:
  - **Overfitted rules**: Rules that hardcode specific entity names (e.g., Figure 6d shows `product_obj.lower() == 'world wide web'` workaround)
  - **Missing predicates**: Out-of-domain predicates trigger fallback template, producing disfluent output
  - **Error accumulation**: Synthetic references with hallucinations may produce rules that repeat those errors

- First 3 experiments:
  1. **Reproduce WebNLG baseline**: Train the rule-based system on WebNLG train split; compare BLEU/BLEURT against reported numbers (target: ~42 BLEU)
  2. **Ablate synthetic rules**: Train without synthetic augmentation; measure coverage drop on held-out predicate combinations
  3. **Out-of-domain probe**: Select 20 test instances with predicates not in training; manually assess fallback template output quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can synthetic data generation be effectively adapted to handle unseen, out-of-domain predicates without requiring access to an LLM during inference?
- Basis in paper: [explicit] "Currently, our approach does not allow the generation of rules for unseen, i.e. out-of-domain predicates... In future work, we will extend the synthetic data generation to out-of-domain situations."
- Why unresolved: The current system relies on clustering predicates found in the training data; it fails when encountering novel relations at runtime unless the system is retrained or the LLM is accessed on-the-fly.
- What evidence would resolve it: Performance metrics (BLEU, hallucination rates) on the "unseen" category of the WebNLG benchmark or a completely new domain dataset after applying the proposed synthetic extension.

### Open Question 2
- Question: Does the inclusion of sentence-level rules (e.g., for aggregating information into subordinate clauses) improve the structural complexity and fluency of the output?
- Basis in paper: [explicit] "We also plan to include new types of rules, such as rules operating at the sentence level (e.g. adding subordinate clauses)."
- Why unresolved: The current system primarily processes triples based on matching predicate lists, which may result in repetitive or disjointed sentence structures compared to neural models that naturally learn discourse connectives.
- What evidence would resolve it: Ablation studies comparing the current predicate-focused rules against sentence-level rules, evaluated using discourse metrics (e.g., diversity of connectives) or human assessment of flow.

### Open Question 3
- Question: Can the performance gap between large (70B) and small (7B) models in writing NLG rules be bridged through specialized fine-tuning or prompt engineering?
- Basis in paper: [inferred] The paper notes a "significant performance gap" between Llama 3 70B and smaller models like Mistral 7B, concluding that the task is "quite challenging" for them, but does not test remediation strategies.
- Why unresolved: The feasibility of this approach for users without access to large models remains unknown; smaller models may lack the reasoning to generate generalizable Python code from few-shot examples without further training.
- What evidence would resolve it: Experiments fine-tuning smaller code-specific models (e.g., CodeLlama) on a dataset of (triples, text, code) tuples to see if they can match the rule generation quality of the 70B model.

### Open Question 4
- Question: Does filtering the LLM-generated synthetic references for factual consistency reduce the rate of major hallucinations in the final rule-based system?
- Basis in paper: [inferred] The authors hypothesize that the system's major hallucinations are a result of "error accumulation" from training on "silver-standard, LLM-generated references that may contain hallucinations."
- Why unresolved: While the authors identified the probable cause of hallucinations, they did not implement or test a verification step to clean the synthetic training data before rule generation.
- What evidence would resolve it: A comparison of hallucination rates between systems trained on raw synthetic data versus systems trained on data validated against the input triples.

## Limitations

- The approach's performance on more complex or out-of-domain inputs remains unclear, as WebNLG represents a relatively constrained domain with limited predicate diversity
- The synthetic rule generation process introduces potential error accumulation through LLM hallucinations in reference texts, which may propagate into final outputs
- The greedy splitting mechanism's optimality is unproven, and the fallback template "{subject} {predicate} {object}" may dominate in real-world applications with diverse predicate combinations

## Confidence

- **High confidence**: The technical feasibility of using LLMs to generate executable Python code for data-to-text tasks, verified through execution and Levenshtein distance validation. The reported inference speed improvements (~83x faster than BART) are well-supported by the CPU-only architecture.
- **Medium confidence**: The generalization capability of the rule-based system, as evidenced by competitive BLEU/BLEURT scores on WebNLG test data. However, the limited impact of synthetic rules on automatic metrics suggests potential overfitting to the training distribution.
- **Low confidence**: The robustness of the approach to out-of-domain predicates and complex linguistic phenomena. The paper provides minimal evidence beyond WebNLG, and the fallback template solution appears insufficient for maintaining fluency in diverse real-world scenarios.

## Next Checks

1. **Out-of-Domain Stress Test**: Evaluate the rule-based system on a diverse set of 100+ RDF triples from multiple domains (e.g., Wikidata, DBpedia) with predicates not present in WebNLG training data. Measure the percentage of inputs requiring fallback templates and conduct human evaluation of resulting fluency and accuracy.

2. **Synthetic Error Propagation Analysis**: Implement a systematic error analysis framework that traces hallucinations from synthetic reference generation through to final output. For 100 synthetic rules, compare the LLM-generated references against ground truth (if available) or expert annotations, then measure how often these errors manifest in final generated text.

3. **Predicate Coverage Gap Analysis**: Create a predicate frequency distribution for a large-scale RDF knowledge base. Identify the top 50 most frequent predicates absent from WebNLG training. For each, attempt to generate rules using the paper's methodology and measure success rates, code quality, and generalization ability to unseen subjects/objects.