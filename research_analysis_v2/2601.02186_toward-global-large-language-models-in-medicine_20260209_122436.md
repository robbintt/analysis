---
ver: rpa2
title: Toward Global Large Language Models in Medicine
arxiv_id: '2601.02186'
source_url: https://arxiv.org/abs/2601.02186
tags:
- wolof
- zulu
- swahili
- chinese
- french
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Toward Global Large Language Models in Medicine

## Quick Facts
- arXiv ID: 2601.02186
- Source URL: https://arxiv.org/abs/2601.02186
- Reference count: 0
- One-line primary result: A benchmark reveals severe multilingual performance gaps in medical LLMs, with fine-tuned models improving low-resource language accuracy.

## Executive Summary
This paper introduces GlobMed-Bench, a comprehensive multilingual benchmark for evaluating large language models (LLMs) on medical tasks across 12 languages. It exposes significant performance disparities, especially for low-resource languages, even among state-of-the-art models. The authors propose and validate GlobMed-LLMs, a family of fine-tuned models that substantially improve accuracy on low-resource languages, demonstrating the feasibility of more equitable global medical AI systems.

## Method Summary
The study constructs GlobMed-Bench by translating six established English medical NLP benchmarks into 11 additional languages, covering six medical tasks and a range of resource levels. Fifty-six LLMs, including both proprietary and open-weight models, are evaluated zero-shot. The authors then fine-tune selected models on a multilingual medical dataset to create GlobMed-LLMs, which are re-evaluated to measure improvement, particularly for low-resource languages.

## Key Results
- The top-performing proprietary model, o4-mini, achieved 86.99% average accuracy, while the best open-weight model, LLaMA-4-Maverick, reached 82.37%.
- All models showed substantial performance drops for low-resource languages (e.g., Wolof, Swahili).
- GlobMed-LLMs improved overall accuracy from 41.64% to 66.15%, with dramatic gains for low-resource languages (e.g., Wolof from 14.52% to 69.63%).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A systematically designed multilingual benchmark can expose severe performance gaps in medical LLMs, especially for low-resource languages, which monolingual evaluations mask.
- Mechanism: The benchmark spans 6 diverse medical tasks (NLI, QA, explanation, knowledge) and 12 languages with varying resource levels. By translating gold-standard datasets from high-resource languages to low-resource ones, performance disparities become quantifiable, revealing how well models generalize linguistic and medical knowledge.
- Core assumption: Translation preserves task semantics; performance gaps reflect language capability, not translation artifacts.
- Evidence anchors:
  - [abstract] "We introduce GlobMed-Bench, a benchmark spanning 12 languages... reveals persistent language disparities, with performance dropping substantially for low-resource languages."
  - [section] "To construct our benchmark, we translated 6 established English medical benchmarks... into 11 additional target languages... using a professional translation service."
  - [corpus] Related work on multilingual medical NLP (e.g., Multilingual BERT for medical tasks) shows language gaps persist but often lacks systematic evaluation across diverse low-resource languages.
- Break condition: If translation quality is poor or introduces systematic biases, performance gaps may reflect translation errors rather than model limitations.

### Mechanism 2
- Claim: Large-scale evaluation across diverse model families (proprietary vs. open-weight, general vs. medical-specialized) establishes a clear performance hierarchy and identifies which design choices generalize best across languages.
- Mechanism: Evaluating 56 LLMs under identical zero-shot conditions creates a fair comparison. Results show proprietary models (e.g., o4-mini) lead, but open-weight models (e.g., LLaMA-4-Maverick) approach parity in high-resource languages, while all struggle with low-resource languages.
- Core assumption: Zero-shot evaluation reflects real-world deployment capability; models were not exposed to benchmark data during training.
- Evidence anchors:
  - [abstract] "We evaluated 56 LLMs... the top-performing model, o4-mini, achieved 86.99% average accuracy, while the best open-weight model, LLaMA-4-Maverick, reached 82.37%."
  - [section] "All experiments were conducted in zero-shot settings to assess the models' intrinsic multilingual medical capabilities without task-specific training."
  - [corpus] Papers on cross-lingual transfer (e.g., Cross-Lingual Transfer for Low-Resource NLP) highlight similar hierarchies but often focus on general NLP, not medical domains.
- Break condition: If some models were inadvertently trained on benchmark data (contamination), rankings would be biased.

### Mechanism 3
- Claim: Targeted fine-tuning on multilingual medical data, especially using small but diverse models, can significantly close performance gaps for low-resource languages.
- Mechanism: GlobMed-LLMs (e.g., GlobMed-Qwen3-4B) were fine-tuned on GlobMed-Bench training data. This improved overall accuracy from 41.64% to 66.15% and boosted low-resource language performance dramatically (e.g., Wolof from 14.52% to 69.63%).
- Core assumption: Fine-tuning data covers linguistic and medical diversity; model capacity is sufficient to learn multilingual medical patterns without catastrophic forgetting.
- Evidence anchors:
  - [abstract] "GlobMed-LLMs... improve overall accuracy... particularly in low-resource languages such as Swahili, Wolof, Yoruba, and Zulu."
  - [section] "GlobMed-Qwen3-4B... increased from 41.64% to 66.15%... improvements were consistent across all benchmarks and languages."
  - [corpus] Related work on multilingual medical QA (e.g., MKG-Rank) shows similar gains via knowledge augmentation, but focuses on retrieval rather than fine-tuning.
- Break condition: If fine-tuning overfits to benchmark specifics, gains may not generalize to real-world medical text.

## Foundational Learning
- Concept: **Multilingual NLP Challenges**
  - Why needed here: Understanding why models struggle with low-resource languages (tokenization, data scarcity, cultural nuances) is critical for interpreting benchmark results and designing improvements.
  - Quick check question: Why might a model perform well on English medical QA but fail on Swahili even with accurate translation?

- Concept: **Medical NLP Specificities**
  - Why needed here: Medical text has unique properties (domain-specific terminology, negation, temporal reasoning) that exacerbate multilingual challenges, requiring specialized evaluation.
  - Quick check question: How does the presence of medical jargon affect translation quality and model performance across languages?

- Concept: **Benchmark Evaluation Methodology**
  - Why needed here: Rigorous evaluation (zero-shot, multiple runs, statistical significance) ensures results are reliable and actionable for model development.
  - Quick check question: Why is zero-shot evaluation preferred for assessing intrinsic model capabilities over few-shot prompting?

## Architecture Onboarding
- Component map:
  - Original datasets -> Translation -> Validation -> Task/language splits -> Model loading -> Zero-shot inference -> Accuracy calculation -> Statistical analysis -> Fine-tuning on multilingual medical data -> Re-evaluation
- Critical path:
  1. Select diverse medical benchmarks and target languages.
  2. Translate and validate datasets.
  3. Evaluate all candidate models under identical conditions.
  4. Analyze performance disparities and identify improvement targets.
  5. Fine-tune selected models on multilingual medical data.
  6. Re-evaluate to quantify gains.
- Design tradeoffs:
  - **Translation vs. Native Data**: Using translations enables broad language coverage but may introduce artifacts; native data is scarce for low-resource languages.
  - **Model Size vs. Efficiency**: Larger models (e.g., 70B+ parameters) perform better but are computationally expensive; smaller fine-tuned models offer a practical balance.
  - **Zero-shot vs. Few-shot**: Zero-shot provides cleaner capability assessment; few-shot may boost performance but confounds evaluation with prompting.
- Failure signatures:
  - **Performance collapse on low-resource languages**: Indicates insufficient multilingual pre-training or poor cross-lingual transfer.
  - **Inconsistent performance across runs**: Suggests model instability or prompt sensitivity.
  - **Fine-tuned model underperforms base model**: Signals overfitting or data quality issues.
- First 3 experiments:
  1. Replicate evaluation of top-3 proprietary and top-3 open-weight models on GlobMed-Bench to validate reported rankings.
  2. Ablate fine-tuning data by language (e.g., train only on high-resource languages) to measure impact on low-resource performance.
  3. Apply GlobMed-LLMs to a held-out medical QA dataset (not in benchmark) to assess generalization beyond GlobMed-Bench.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What was the specific composition, including size, source domains (e.g., clinical notes, PubMed abstracts), and language distribution, of the multilingual medical training dataset used to fine-tune the GlobMed-LLMs?
- Basis in paper: [explicit] The paper states in Section S3.1 that GlobMed-LLMs were "trained on a multilingual medical dataset" but does not provide a breakdown of its contents, scale, or the proportion of high-resource versus low-resource language data.
- Why unresolved: The reported significant performance gains (e.g., GlobMed-MedGemma-4B improving average accuracy from 50.02% to 81.02% on MedNLI) cannot be fully attributed to the fine-tuning strategy without knowing if the improvements stem from data scale, quality, or specific domain alignment, which hampers reproducibility.
- What evidence would resolve it: A detailed datasheet for the fine-tuning corpus, specifying token counts per language, provenance of the medical texts, and the methodology used for translation or curation of low-resource language data (Swahili, Wolof, Yoruba, Zulu).

### Open Question 2
- Question: Why does the GlobMed fine-tuning strategy lead to a significant performance regression on the HeadQA benchmark (e.g., GlobMed-MedGemma-4B English accuracy dropping from 69.39% to 61.40%) while simultaneously achieving state-of-the-art improvements on BioNLI and MedNLI?
- Basis in paper: [explicit] A comparison of Table 135 (HeadQA) with Tables 123 (BioNLI) and 129 (MedNLI) reveals a striking divergence; GlobMed-MedGemma-4B substantially underperforms its baseline on HeadQA across multiple languages (e.g., Spanish dropping from 62.57% to 43.57%) but outperforms it on the other benchmarks.
- Why unresolved: The paper does not analyze the characteristics of HeadQA (e.g., its reliance on factual recall vs. inference) versus the other benchmarks, leaving the mechanism of this negative transfer and the potential loss of specific medical knowledge unexplained.
- What evidence would resolve it: An ablation study measuring the retention of specific medical knowledge domains (e.g., pharmacology, anatomy) post-fine-tuning, or a linguistic analysis comparing the distribution of the GlobMed training data with the HeadQA evaluation set.

### Open Question 3
- Question: Would the GlobMed fine-tuning strategy yield similar multilingual performance gains if applied to other model architectures, such as the LLaMA or Mistral families, which showed different baseline strengths?
- Basis in paper: [inferred] The paper demonstrates that GlobMed fine-tuning is effective for the MedGemma and Qwen3 families, but the baseline performance of other models like LLaMA-3.1-70B or Mistral-7B (Section S2) suggests they may have different inductive biases or multilingual capabilities that could respond differently to the same data.
- Why unresolved: The paperâ€™s conclusions about the efficacy of the GlobMed approach are currently limited to the specific model families tested, and it is unknown if the improvements are robust to architectural variations like the different attention mechanisms or tokenizer vocabularies found in other popular models.
- What evidence would resolve it: Applying the identical GlobMed fine-tuning pipeline to a diverse set of baseline models (e.g., LLaMA-3.1-8B, Mistral-Small-3.1-24B) and reporting the comparative delta in multilingual medical performance.

## Limitations
- Translation quality and potential semantic drift remain uncertain, especially for low-resource languages.
- The exact composition, licensing, and size of the multilingual training data used for GlobMed fine-tuning are unspecified.
- The benchmark focuses on accuracy, not robustness, bias, or real-world deployment feasibility.

## Confidence
- **High Confidence**: The benchmark construction methodology (translating established datasets to 12 languages) is clearly specified and reproducible with access to translations. The evaluation pipeline (zero-shot, 5-run averages, statistical testing) is rigorous and well-documented.
- **Medium Confidence**: The performance rankings of proprietary vs. open-weight models are robust, but exact accuracy values may vary due to API access restrictions and potential contamination from training data overlap. Fine-tuning results are convincing, but training data specifics and hyperparameters are underspecified.
- **Low Confidence**: Claims about real-world deployment readiness and clinical utility are not directly supported, as the study focuses on controlled benchmark evaluation rather than deployment trials or bias analysis.

## Next Checks
1. Replicate evaluation of top-3 proprietary and top-3 open-weight models on GlobMed-Bench to confirm reported performance hierarchies and check for potential contamination or translation artifacts.
2. Ablate fine-tuning data by language (e.g., train only on high-resource languages) to measure impact on low-resource performance.
3. Apply GlobMed-LLMs to a held-out medical QA dataset (not in benchmark) to assess whether fine-tuning gains transfer beyond GlobMed-Bench.