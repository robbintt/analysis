---
ver: rpa2
title: Span-Agnostic Optimal Sample Complexity and Oracle Inequalities for Average-Reward
  RL
arxiv_id: '2502.11238'
source_url: https://arxiv.org/abs/2502.11238
tags:
- span
- have
- which
- lemma
- log2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper resolves a long-standing problem in average-reward\
  \ reinforcement learning: how to learn an \u03B5-optimal policy without prior knowledge\
  \ of the optimal bias function's span. The authors develop two algorithms that achieve\
  \ the minimax optimal span-based complexity."
---

# Span-Agnostic Optimal Sample Complexity and Oracle Inequalities for Average-Reward RL

## Quick Facts
- **arXiv ID**: 2502.11238
- **Source URL**: https://arxiv.org/abs/2502.11238
- **Reference count**: 40
- **Primary result**: First algorithms achieving minimax optimal sample complexity in average-reward RL without prior knowledge of optimal bias span H

## Executive Summary
This paper resolves a fundamental problem in average-reward reinforcement learning: how to learn ε-optimal policies without knowing the optimal bias function's span. The authors develop algorithms that automatically calibrate the discount factor to balance approximation and estimation errors, achieving the optimal O(SAH/ε²) sample complexity where H is the optimal bias span. The key innovation is horizon calibration - automatically tuning the effective horizon based on empirical confidence intervals or lower bounds on performance. The span penalization approach can further improve upon this rate when near-optimal policies have much smaller span than the worst case.

## Method Summary
The algorithms combine discounted reduction with automatic horizon calibration. For each candidate discount factor γ, the method solves the empirical discounted MDP and computes either a lower bound on the gain or a confidence interval width. It selects the γ that optimizes this observable objective, effectively adapting to the problem's hidden complexity. A span-constrained variant explicitly penalizes the span of the value function during planning, competing with policies having the best tradeoff between complexity and suboptimality. The approach relies on localization bounds that control the span of empirical value functions recursively, preventing variance explosion.

## Key Results
- First span-agnostic algorithms achieving optimal Õ(SAH/ε²) sample complexity without prior knowledge of H
- Algorithm 4 satisfies an oracle inequality, potentially outperforming the worst-case rate when low-span near-optimal policies exist
- Theoretical analysis proves the algorithms match the optimal complexity and provide proper confidence intervals for policy selection

## Why This Works (Mechanism)

### Mechanism 1: Agnostic Horizon Calibration
The algorithm iterates over a geometric range of effective horizons (1/(1-γ)). For each candidate γ, it solves the empirical discounted MDP and computes an objective function - either a lower bound on the gain or a confidence interval width. It selects the γ that optimizes this observable objective, effectively adapting the horizon to the problem's hidden complexity. The key assumption is that the MDP is weakly communicating, ensuring the optimal gain is constant and the bias span H is finite.

### Mechanism 2: Localization via Empirical Variance Control
The error from approximating the true MDP with an empirical one is controlled recursively through a localization bound. This shows that the span of the empirical value function is controlled by the true optimal span H plus an estimation term, preventing the variance of the empirical solution from dominating the sample complexity. The algorithm relies on concentration inequalities for the empirical transition kernel holding uniformly over the policy class.

### Mechanism 3: Span-Constrained Optimization (Oracle Inequality)
By explicitly penalizing the span of the value function during planning with a span truncation operator, the algorithm can outperform the worst-case minimax rate if simple (low-span) near-optimal policies exist. This span penalization operator restricts the search to policies with value function span ≤ M, and by tuning M via an empirical span penalization objective, the algorithm adapts to the complexity of the best available policy.

## Foundational Learning

**Concept: Average-Reward MDP & Bias Span**
- **Why needed here**: The entire paper is defined by the "span of the optimal bias function," denoted H or ||h*||span. You must understand that h* measures the "range" of relative value (max state advantage - min state advantage) and acts as the horizon or diameter of the problem.
- **Quick check**: If an MDP has a uniform optimal gain ρ* (all states yield the same long-term average reward), does the bias h* have to be zero everywhere? (Answer: No, h* measures the transient advantage, not the average).

**Concept: Discounted Reduction**
- **Why needed here**: The algorithms solve a discounted MDP as a proxy for the average-reward MDP. You need to know that (1-γ)V*γ approximates ρ*, but incurs an approximation error of roughly (1-γ)H. Tuning γ balances this error.
- **Quick check**: Why does increasing the discount factor γ → 1 increase the "approximation error" term in a standard reduction if not calibrated correctly? (Answer: While (1-γ)H decreases, the estimation error of V*γ grows as 1/(1-γ), creating a tradeoff).

**Concept: Generative Model (Simulator)**
- **Why needed here**: The sample complexity bounds assume access to a "generative model" where you can query any state-action pair P(·|s,a) independently. This is distinct from online interaction where you must follow trajectories.
- **Quick check**: Does the generative model assumption make the sample complexity higher or lower compared to online RL for the same accuracy? (Answer: Generally lower/faster, because you can explore the state space uniformly without navigation constraints).

## Architecture Onboarding

**Component map**: Sampler -> DMDP Solver -> Calibrator -> Span-Constrained Solver
- Sampler: Draws n samples per state-action pair to build empirical transition kernel P̂
- DMDP Solver: Standard Value Iteration to compute V̂γ and π̃γ
- Calibrator (Algorithm 1/2): Loop over γ ∈ {2^-k}, computes bounds L̂(γ), Û(γ)
- Span-Constrained Solver (Algorithm 3): Value Iteration modified with Clip_M operator for Oracle variant

**Critical path**:
1. Fix sample size n or target error ε
2. Generate P̂ from n samples per (s,a) pair
3. Parallelize/Iterate: For various γ (horizons), run DMDP solver
4. Evaluate: Compute performance bounds for each γ
5. Select: Return policy π̃ corresponding to best bound

**Design tradeoffs**:
- Grid Density: Geometric grid for γ increases compute cost linearly but theoretically guarantees better calibration (log factors only). Uses powers of 2 for efficiency.
- Solver Accuracy: SolveDMDP needs error ≤ 1/n. Precise convergence is required, impacting iteration count.

**Failure signatures**:
- Stagnation: If bounds L̂(γ) never exceed threshold (in fixed-ε mode), implies n is insufficient or H is massive, requiring more data.
- Inconsistent Horizons: If selected γ̂ fluctuates wildly with small data changes, indicates high variance in estimates (breakdown of localization).

**First 3 experiments**:
1. Validation on "Hard" Instances: Construct RiverSwim MDP where H is large. Verify sample complexity matches Õ(SAH/ε²) without hardcoding H.
2. Benefit of Oracle Inequality: Design MDP where optimal policy has high span H, but near-optimal policy has span H' << H. Run Algorithm 4 to confirm it selects simpler policy and achieves better sample complexity.
3. Horizon Sensitivity: Plot selected effective horizon 1/(1-γ̂) as function of sample size n. Verify it scales roughly as √(n)H as predicted by analysis.

## Open Questions the Paper Calls Out

**Open Question 1**: Can the span-agnostic optimal sample complexity results be generalized to general/multichain MDPs?
- **Basis in paper**: [explicit] The conclusion states: "Future directions of interest include generalizing to general/multichain MDPs..."
- **Why unresolved**: All theoretical results assume weakly communicating MDPs where the optimal gain ρ* is constant; general multichain MDPs have state-dependent gains requiring fundamentally different analysis.
- **What evidence would resolve it**: An algorithm with provable sample complexity bounds for general MDPs that match or approach the span-based rate without prior knowledge of complexity parameters.

**Open Question 2**: Can these results be extended beyond the tabular simulator/generative model setting to online or model-free reinforcement learning?
- **Basis in paper**: [explicit] The conclusion states: "...and extending beyond the tabular simulator setting."
- **Why unresolved**: The analysis fundamentally relies on having n independent samples from each state-action pair via a generative model, which enables the empirical transition kernel construction and associated concentration bounds.
- **What evidence would resolve it**: An online algorithm achieving comparable span-agnostic regret or PAC guarantees without a simulator, or a reduction showing impossibility in the online setting.

**Open Question 3**: Can tighter concentration bounds be obtained by optimizing the α(δ,n) function used in the algorithms?
- **Basis in paper**: [explicit] Remark 5 states: "we conjecture that a smaller function α may be sufficient for their inequalities to hold, in which case the improvement could be carried over to our work in a black-box manner."
- **Why unresolved**: The authors adopted existing concentration bounds from prior work without attempting to optimize constants or logarithmic factors.
- **What evidence would resolve it**: A refined analysis yielding tighter α(δ,n) with improved constants or log factors, validated through matching upper and lower bounds.

## Limitations
- The analysis assumes bounded rewards in [0,1] and weakly communicating MDPs, which may not cover all practical scenarios.
- The algorithms require a generative model (simulator) providing independent samples from each state-action pair, limiting applicability to online settings.
- The constants in the sample complexity bounds are not fully specified, making precise implementation and comparison difficult.

## Confidence

**High Confidence**: The fundamental mechanism of horizon calibration through geometric search over discount factors is well-established. The localization argument controlling empirical variance is mathematically rigorous.

**Medium Confidence**: The span penalization approach achieving oracle inequalities depends on the existence of low-span near-optimal policies, which may not hold in all MDPs. The benefit is conditional rather than universal.

**Low Confidence**: The constants in the sample complexity bounds (particularly in the confidence interval terms) are not fully specified, making precise implementation and comparison with prior work difficult.

## Next Checks

1. **Implementation Validation**: Implement Algorithm 1 and verify that the selected discount factor scales as predicted with sample size n. Test on a known hard instance where H is large but unknown.

2. **Oracle Inequality Verification**: Construct an MDP with both high-span optimal policy and low-span near-optimal policy. Run Algorithm 4 to confirm it achieves better sample complexity than Algorithm 1 by exploiting the simpler policy.

3. **Span Constraint Behavior**: Analyze Algorithm 3's performance across different span constraints M. Verify that it correctly interpolates between the worst-case complexity and the oracle rate when low-span policies exist.