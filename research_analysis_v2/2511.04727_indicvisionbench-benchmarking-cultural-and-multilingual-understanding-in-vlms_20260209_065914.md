---
ver: rpa2
title: 'IndicVisionBench: Benchmarking Cultural and Multilingual Understanding in
  VLMs'
arxiv_id: '2511.04727'
source_url: https://arxiv.org/abs/2511.04727
tags:
- question
- arxiv
- languages
- image
- cultural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces IndicVisionBench, a large-scale benchmark
  designed to evaluate vision-language models (VLMs) on culturally and linguistically
  diverse Indian content. It includes 5,000 images and over 37,000 question-answer
  pairs across 13 culturally grounded topics in English and 10 Indian languages, covering
  tasks like VQA, OCR, and multimodal translation.
---

# IndicVisionBench: Benchmarking Cultural and Multilingual Understanding in VLMs

## Quick Facts
- arXiv ID: 2511.04727
- Source URL: https://arxiv.org/abs/2511.04727
- Reference count: 40
- This paper introduces a large-scale benchmark evaluating vision-language models on culturally and linguistically diverse Indian content.

## Executive Summary
IndicVisionBench is a large-scale benchmark designed to evaluate vision-language models (VLMs) on culturally and linguistically diverse Indian content. It includes 5,000 images and over 37,000 question-answer pairs across 13 culturally grounded topics in English and 10 Indian languages, covering tasks like VQA, OCR, and multimodal translation. Evaluation of 8 models shows significant performance gaps, especially in low-resource languages and culturally specific content, with Gemini-2.5 outperforming others but still struggling on adversarial questions. The benchmark highlights the need for more inclusive, culturally aware multimodal models.

## Method Summary
IndicVisionBench evaluates VLMs on three multimodal tasks: Visual Question Answering (VQA) with 6 types including adversarial questions, Multimodal Machine Translation (MMT), and Optical Character Recognition (OCR) across 10 Indic languages. The benchmark uses 4,117 images labeled with 13 cultural topics, with VQA questions generated by Gemini models and human-corrected. OCR uses 876 Wikisource page images, and MMT uses 106 image-caption pairs. Evaluation employs task-specific metrics including Exact Match, LLM-as-a-Judge, BLEU, RIBES, ANLS, WER, and CER across 8 models (4 proprietary, 4 open-weights).

## Key Results
- Visual grounding is necessary for culturally grounded VQA performance, with substantial accuracy drops when images are removed
- Performance degrades systematically along language resource level and question adversariality axes
- Gemini-2.5 outperforms other models but still struggles with adversarial questions, scoring only 5.79/10 in English

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Visual grounding appears necessary for culturally grounded VQA performance.
- **Mechanism:** Removing image inputs leads to substantial performance drops in short-answer VQA tasks, suggesting models rely on visual context for culturally specific recognition and reasoning.
- **Core assumption:** The performance drop is primarily due to loss of visual information, not confounding factors like prompt changes.
- **Evidence anchors:**
  - [section 6] "Removing images leads to a substantial drop in accuracy, most pronounced for short-answer tasks... showcasing that visual grounding is necessary."
  - [section 6, Table 5] Shows scores with vs. without images for Chitrarth-1, Gemma-3, and Gemini-2.5 across languages.
- **Break condition:** If models trained on extensive text-only cultural knowledge bases matched visual performance, visual grounding might be less critical.

### Mechanism 2
- **Claim:** Model performance degrades systematically along two axes: language resource level and question adversariality.
- **Mechanism:** Lower-resource languages and adversarial questions (with false assumptions) expose gaps in both linguistic representation and robust cultural reasoning capabilities.
- **Core assumption:** The benchmark's adversarial questions and lower-resource language samples are representative of real-world difficulty.
- **Evidence anchors:**
  - [section 5, Table 1] Shows Gemini-2.5 scoring 5.79/10 on adversarial questions vs. >8.5 on other types in English.
  - [section 5, Table 2] Shows adversarial scores dropping to ~0–2 for most models in lower-resource languages.
- **Break condition:** If models were fine-tuned on diverse adversarial examples across all target languages, this degradation could reduce.

### Mechanism 3
- **Claim:** Current VLMs lack systematic cultural alignment between visual content regions and linguistic query contexts.
- **Mechanism:** Performance varies inconsistently across region-language pairs; no model consistently performs better when the query language matches the dominant language of the depicted region.
- **Core assumption:** The regional mapping (states as cultural proxies) is valid and comprehensive.
- **Evidence anchors:**
  - [section 6] "No clear alignment between the region depicted in the image and the language of the query."
  - [section 6, Figure 7] Shows inconsistent performance patterns across state-language combinations for open-weight models.
- **Break condition:** If training data were region-language balanced or explicitly aligned, systematic cultural alignment could emerge.

## Foundational Learning

- **Concept:** Vision-Language Model (VLM) Grounding
  - **Why needed here:** The benchmark evaluates how well VLMs integrate visual and textual cultural knowledge.
  - **Quick check question:** Can a VLM answer "What festival is shown?" without seeing the image? Why or why not?

- **Concept:** Low-Resource Multilingual Challenges in OCR/MT
  - **Why needed here:** The OCR and MMT tracks specifically test performance on underrepresented Indic scripts and translation pairs.
  - **Quick check question:** Why might OCR for Malayalam be harder than for Hindi?

- **Concept:** Adversarial Evaluation for Cultural Knowledge
  - **Why needed here:** Adversarial questions probe whether models can identify and correct culturally incorrect assumptions.
  - **Quick check question:** How should a model respond to "How is this Pongal dish prepared for Eid?" if the image shows a Diwali sweet?

## Architecture Onboarding

- **Component map:** IVB-VQA-EN (4,117 images → VLM-generated QAs → Human verification) → IVB-VQA-Indic/Parallel (translated into 10 languages → Human correction) → IVB-OCR (876 Wikisource images → Paired text) → IVB-MMT (106 image-caption pairs → Translated into 10 languages)

- **Critical path:** The VQA-English creation pipeline (Figure 1) is central: image collection → VLM QA generation → human review → translation/correction for Indic tracks.

- **Design tradeoffs:**
  - **VLM-generated QAs:** Faster and cheaper than fully human-authored, but may propagate VLM biases (mitigated by human review)
  - **State-based cultural mapping:** Practical proxy, but may oversimplify intra-state diversity
  - **LLM-as-Judge:** Enables nuanced evaluation, but introduces another model's potential biases

- **Failure signatures:**
  - **OCR hallucination/repetition:** LLaMA-4 producing repetitive text up to max length (Figure 8)
  - **Adversarial question failure:** Models accepting false assumptions without correction (Table 2)
  - **Cross-lingual inconsistency:** Large performance variance across languages for the same visual content (Figure 4)

- **First 3 experiments:**
  1. **Vision necessity test:** Evaluate models on VQA-Parallel with and without images; quantify drop
  2. **Cross-lingual robustness:** Compare performance on VQA-Indic vs. VQA-English for the same images
  3. **OCR reliability analysis:** Compare ANLS, WER, CER metrics; identify language-specific failure modes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does visual grounding significantly improve multimodal machine translation (MMT) performance for low-resource Indic languages, contrary to prior findings that suggest minimal impact?
- Basis in paper: [explicit] The Limitations section states, "Although we systematically study the impact of visual modality for VQA, we have not yet explored this effect for the MMT track... We plan to cover this as part of the future work."
- Why unresolved: While the paper systematically evaluates the visual modality for VQA, the specific utility of the image context for the MMT task within this benchmark remains untested, leaving a gap in understanding the necessity of vision for translation in this cultural context.
- What evidence would resolve it: A comparative evaluation of MMT model performance with and without visual input (text-only baseline) on the IndicVisionBench-MMT track, measuring the delta in BLEU/RIBES scores.

### Open Question 2
- Question: How can evaluation metrics be refined to accurately capture cultural grounding and nuanced multimodal reasoning, moving beyond the limitations of current LLM-as-a-Judge and deterministic scores?
- Basis in paper: [explicit] The Limitations section notes that "Existing evaluation metrics and LLM-based scoring may not fully capture cultural grounding or multimodal reasoning, highlighting opportunities for more nuanced evaluation approaches."
- Why unresolved: The current reliance on GPT-4o as a judge and standard metrics like BLEU/ANLS may fail to penalize culturally hallucinated or superficially correct answers, creating a need for metrics specifically sensitive to cultural fidelity.
- What evidence would resolve it: The development and validation of a novel evaluation framework or metric that correlates better with human expert judgments on cultural nuance than current automated metrics.

### Open Question 3
- Question: What architectural or training interventions are required to mitigate the specific failure modes in Optical Character Recognition (OCR) for Indic scripts, such as repetitive hallucinations and unbounded error rates?
- Basis in paper: [inferred] The discussion on OCR evaluation highlights that standard metrics like WER/CER can over-penalize or fail to capture edge cases like repetitive outputs (e.g., LLaMA-4 in Malayalam), suggesting current models and metrics struggle with these anomalies.
- Why unresolved: Current metrics like ANLS are used to mitigate unbounded penalties, but the models themselves (including frontier open-weights models) still exhibit degenerative behaviors like repetition, indicating a lack of robustness in the decoding or architecture for these specific scripts.
- What evidence would resolve it: The application of a specific decoding strategy or fine-tuning objective that eliminates repetitive outputs without manual truncation, resulting in lower, more stable WER/CER distributions.

### Open Question 4
- Question: How can Vision-Language Models be improved to detect and reject false assumptions in adversarial queries, particularly in low-resource languages where current performance drops near zero?
- Basis in paper: [inferred] Table 2 shows that even the best models (Gemini-2.5, GPT-4o) struggle significantly with adversarial questions (scores ~2-5 out of 10), and this performance drops further for languages like Kannada and Malayalam.
- Why unresolved: The substantial performance gap in adversarial settings indicates that models rely heavily on surface-level patterns rather than grounded cultural knowledge, and this weakness is exacerbated in multilingual contexts.
- What evidence would resolve it: A training method or dataset augmentation strategy that significantly improves the "assumption identification" score (part of the LLM-as-a-Judge rubric) across all 11 languages in the VQA-Parallel track.

## Limitations
- The benchmark may not fully represent linguistic diversity within each language (regional dialects, sociolects) or cultural practices across India's vast geography
- The sample of 8 models may not represent the full spectrum of current VLMs, potentially attributing performance gaps to cultural knowledge rather than architectural differences
- The LLM-as-Judge approach introduces potential biases from the judging model that may not perfectly capture nuanced cultural understanding

## Confidence

**High Confidence:**
- Visual grounding is necessary for culturally grounded VQA performance
- Performance systematically degrades along language resource level and question adversariality axes
- OCR repetition loops are a real failure mode in some models

**Medium Confidence:**
- Current VLMs lack systematic cultural alignment between visual content regions and linguistic query contexts
- The adversarial questions effectively probe cultural knowledge gaps
- State-based cultural mapping provides reasonable proxy for regional diversity

**Low Confidence:**
- The LLM-as-Judge evaluation perfectly captures nuanced cultural understanding
- The benchmark comprehensively represents all aspects of Indian cultural diversity
- Observed performance gaps are solely due to cultural knowledge limitations

## Next Checks

1. **Cross-Validation with Alternative Judging Methods:** Re-evaluate a subset of adversarial questions using human experts or alternative LLM judges to assess the consistency and reliability of the LLM-as-Judge approach.

2. **Model Fine-Tuning Experiment:** Fine-tune a subset of VLMs on a balanced corpus of culturally diverse images and questions across all 10 languages, then re-evaluate on IndicVisionBench to isolate the impact of cultural alignment training.

3. **Dialect and Regional Subgroup Analysis:** Collect additional data points for regional dialects and minority languages within the 10 major languages, then analyze performance patterns to identify whether gaps are language-wide or specific to certain subgroups.