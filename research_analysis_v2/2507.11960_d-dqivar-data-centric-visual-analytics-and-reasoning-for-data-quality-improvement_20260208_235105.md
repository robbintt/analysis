---
ver: rpa2
title: 'd-DQIVAR: Data-centric Visual Analytics and Reasoning for Data Quality Improvement'
arxiv_id: '2507.11960'
source_url: https://arxiv.org/abs/2507.11960
tags:
- data
- system
- performance
- quality
- issues
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents d-DQIVAR, a visual analytics system designed
  to improve data quality (DQ) for machine learning tasks. The system integrates both
  data-driven and process-driven strategies, offering comprehensive methods for data
  preprocessing (imputation, outlier detection, deletion, format standardization,
  duplicate removal, and feature selection) and process-driven strategies (DQ dimension
  evaluation, ML model performance evaluation, and Kolmogorov-Smirnov test).
---

# d-DQIVAR: Data-centric Visual Analytics and Reasoning for Data Quality Improvement

## Quick Facts
- arXiv ID: 2507.11960
- Source URL: https://arxiv.org/abs/2507.11960
- Reference count: 40
- Primary result: Visual analytics system integrating data-driven preprocessing with process-driven evaluation to improve data quality for ML tasks while reducing expert knowledge dependency

## Executive Summary
d-DQIVAR addresses the challenge of data quality improvement (DQI) for machine learning by combining data-driven preprocessing methods with process-driven evaluation strategies. The system generates and evaluates multiple DQI procedures using DQ dimension scores and ML model performance, filtering candidates through Kolmogorov-Smirnov tests to preserve original data distributions. Through visual reasoning capabilities including coordinated views and interactive customization, d-DQIVAR reduces reliance on expert knowledge while ensuring data integrity and enhancing usability for regression tasks.

## Method Summary
The system integrates data-driven strategies (imputation, outlier detection, deletion, format standardization, duplicate removal, feature selection) with process-driven evaluation (DQ dimension assessment, ML model performance, K-S test filtering). Users upload CSV data, select target features and models, and the computing server generates all viable DQI procedure permutations, filtering those that distort distributions (p > 0.05). ML performance is evaluated across 25 models and 5 metrics, with results displayed through coordinated visualizations including donut charts for DQ dimensions, radar charts for model performance, and heatmaps for issue distributions. Users can customize recommended procedures based on domain knowledge.

## Key Results
- User study showed d-DQIVAR reduced cognitive load compared to baseline T2 system
- System achieved 98.46% average DQ dimension score across tested datasets
- Sampling approach (1,000 records) reduced computation time with MAPE differences of 0.1296 and 0.1050
- Case studies demonstrated effective DQI on Beijing air quality and Boston housing datasets

## Why This Works (Mechanism)

### Mechanism 1
Integrating process-driven evaluation with data-driven preprocessing enables context-sensitive DQI that better aligns with ML model performance goals. The system generates candidate DQI procedures, evaluates each using DQ dimension scores and ML model cross-validation performance, then filters procedures via Kolmogorov-Smirnov test (p > 0.05) to exclude those distorting original data distributions. Users receive ranked recommendations rather than a single automated output.

### Mechanism 2
Visual reasoning with coordinated views reduces cognitive load and expert knowledge dependency by externalizing DQ assessment into interpretable visual forms. Donut charts encode DQ dimension scores; radar charts compare multi-metric ML performance; heatmaps and correlation matrices reveal issue distributions at feature/record granularity; eCDF overlays show before/after distribution shifts. Interactive linking between views supports iterative hypothesis formation without requiring statistical expertise.

### Mechanism 3
User-in-the-loop customization of DQI procedures preserves domain-specific semantics that automated pipelines would otherwise violate. After system-recommended procedures are presented, users can modify individual steps via configuration panels. The system recalculates ML performance and K-S test results for customized pipelines, providing immediate feedback on tradeoffs.

## Foundational Learning

- **DQ Dimensions as Standardized Metrics**: The system aggregates DQ into six dimensions (completeness, outlier detection, homogeneity, duplicates, correlation detection, feature relevance) scored 0-100. If a dataset scores low on "correlation detection," investigate multicollinearity issues among features.

- **Kolmogorov-Smirnov (K-S) Test for Distribution Similarity**: The K-S test (p > 0.05 threshold) filters DQI procedures that distort original data characteristics. A DQI procedure yielding p = 0.03 on the K-S test indicates the procedure significantly alters the data distribution and should be rejected.

- **Multi-Objective ML Model Evaluation**: The system supports 25 models and 5 metrics (MAE, RMSE, R², RMSLE, MAPE); radar charts display tradeoffs across metrics. For a regression task where explained variance matters, prefer the procedure that maintains R² even if RMSE differences are similar.

## Architecture Onboarding

- **Component map**: Frontend (React.js + D3.js + Apexchart.js) -> Backend/Computing Server (Python-Flask + PyCaret) -> Configuration Panel -> DQ Assessment View -> Data Overview -> DQI View -> Data Change View

- **Critical path**: 1) User uploads data and selects target feature/models/metrics → Configuration Panel; 2) Computing server generates all viable DQI procedures (filtered by K-S test p > 0.05); 3) Server computes quality-improved datasets and ML performance for each procedure; 4) DQ Assessment View displays DQ dimensions and model performance comparisons; 5) User selects or customizes a DQI procedure → Data Change View shows impact; 6) User iterates until satisfied with DQ-ML performance tradeoff

- **Design tradeoffs**: Comprehensiveness vs. Latency (O(n!) scaling requires sampling with ΔMAPE ~0.10-0.13); Automation vs. Control (user customization prevents domain-agnostic errors but increases cognitive load); Generality vs. Specificity (currently limited to regression and numeric CSV data)

- **Failure signatures**: All DQI procedures rejected by K-S test (requires threshold relaxation); user reports "implicit visualization" (addressed with explicit encoding but may recur with visual clutter); scalability bottleneck for datasets >800K records (sampling introduces approximation error)

- **First 3 experiments**: 1) Reproduce case study workflow with Beijing air quality dataset, verify recommendation of procedure index 257 and K-S test p > 0.05; 2) Test customization edge case with Boston housing dataset, reject outlier removal on PRICE feature and observe ML performance degradation; 3) Validate sampling approximation on large dataset (Stock Market with 851K records), compare ΔMAPE and K-S results with/without sampling

## Open Questions the Paper Calls Out

### Open Question 1
How can visual analytics systems for DQI be effectively extended to support classification tasks with DQ dimensions such as class overlap, label purity, and class parity? The authors state that since d-DQIVAR focuses on regression, DQ dimensions such as class overlap or label purity are not addressed, and plan to add DQI methods not covered by the system for future extensions.

### Open Question 2
What visualization and interaction techniques are most effective for DQI on non-tabular data types such as images and text? The authors note the system only supports DQ for numeric data in CSV format and plan to expand to handle data such as images and texts, requiring fundamentally different representations of DQ issues.

### Open Question 3
How can an adaptive DQI framework dynamically adjust procedures based on real-time monitoring of data and model changes? The authors aim to develop an adaptive framework that dynamically monitors DQI impact and adjusts procedures accordingly, moving beyond the current batch-oriented approach.

### Open Question 4
How does the sampling-based DQI approximation approach perform across heterogeneous data distributions with varying DQ issue densities? The authors note sampling reduced computation time but with MAPE differences of 0.1296 and 0.1050, and that performance may vary depending on data distribution and task context.

## Limitations

- System currently limited to regression tasks and numeric CSV data, excluding classification-specific DQ dimensions and categorical data handling
- Computational complexity of generating all DQI procedure permutations requires sampling for large datasets, introducing approximation error
- Visual reasoning mechanisms may suffer from information overload with high-dimensional datasets, potentially obscuring patterns in aggregated visualizations

## Confidence

- **High Confidence**: Integration of process-driven evaluation with data-driven preprocessing is technically sound and well-supported by K-S test filtering mechanism
- **Medium Confidence**: User-in-the-loop customization mechanism's effectiveness depends heavily on user expertise and may not scale well for novice users
- **Low Confidence**: System's generalizability to domains beyond regression and numeric data remains unproven; scalability of visual reasoning for high-dimensional data not empirically validated

## Next Checks

1. **Reproduce K-S Test Edge Cases**: Create synthetic datasets with known distributional properties (bimodal, skewed, heavy-tailed) and verify d-DQIVAR correctly identifies and filters DQI procedures that distort these distributions while maintaining appropriate ML performance

2. **Stress Test Sampling Approximation**: Systematically vary dataset sizes and sampling rates to quantify the relationship between sample size, approximation error (ΔMAPE), and K-S test reliability, establishing clear thresholds for when sampling becomes inadequate

3. **Validate Cross-Domain Applicability**: Apply d-DQIVAR to at least two non-numeric datasets (one with categorical features, one with mixed types) and document whether the system's assumptions about data characteristics hold or require modification