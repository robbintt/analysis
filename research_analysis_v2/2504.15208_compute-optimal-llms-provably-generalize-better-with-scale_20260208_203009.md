---
ver: rpa2
title: Compute-Optimal LLMs Provably Generalize Better With Scale
arxiv_id: '2504.15208'
source_url: https://arxiv.org/abs/2504.15208
tags:
- bound
- generalization
- quantization
- loss
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides generalization theory to explain why larger
  language models trained in the compute-optimal regime generalize better. The authors
  introduce a novel empirical Freedman-type martingale concentration inequality that
  tightens existing bounds by accounting for the variance of the loss function.
---

# Compute-Optimal LLMs Provably Generalize Better With Scale

## Quick Facts
- arXiv ID: 2504.15208
- Source URL: https://arxiv.org/abs/2504.15208
- Authors: Marc Finzi; Sanyam Kapoor; Diego Granziol; Anming Gu; Christopher De Sa; J. Zico Kolter; Andrew Gordon Wilson
- Reference count: 40
- This paper provides generalization theory to explain why larger language models trained in the compute-optimal regime generalize better.

## Executive Summary
This paper introduces a novel generalization theory explaining why larger language models trained on the compute-optimal frontier generalize better. The authors develop a new empirical Freedman-type martingale concentration inequality that accounts for loss variance, providing tighter bounds than traditional approaches. They demonstrate that as models scale, both the loss variance and quantization error decrease, leading to smaller generalization gaps. The work bridges empirical observations about compute-optimal scaling with theoretical guarantees.

## Method Summary
The paper develops a generalization bound for compute-optimal language models using a novel empirical Freedman-type martingale concentration inequality. The method involves: (1) selecting checkpoints on the compute-optimal frontier where parameters-to-tokens ratio N/D ≈ 1/20, (2) estimating loss variance on a 10^4 token subsample using model-resampling, (3) measuring quantization gaps using 4-bit GPTQ compression, and (4) evaluating the bound's components across model scales. The bound decomposes into complexity, loss variance, and quantization terms, with theoretical arguments showing these components improve with model size.

## Key Results
- Introduces a novel empirical Freedman-type martingale concentration inequality that tightens generalization bounds by accounting for loss variance
- Shows loss variance decreases with model size as Σ ≈ 0.27 + 8337/N^0.54, improving concentration rates from O(√C) to O(C)
- Demonstrates model information content grows sublinearly with dataset size on the compute-optimal frontier (O(D^{1-β}) ≈ O(D^0.63)), reducing complexity per token

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Variance-aware concentration bounds tighten generalization guarantees as loss variance decreases with scale.
- Mechanism: The authors introduce a novel empirical Freedman-type martingale concentration inequality that incorporates empirical loss variance Σ rather than worst-case bounds. When loss variation is small, concentration to the mean happens at rate O(C) rather than O(√C), where C is the complexity term.
- Core assumption: The loss deviation can be bounded using a pre-visible proxy Y_k that conditions on prior context but captures variation under model resampling.
- Evidence anchors:
  - [abstract] "introduce a novel, fully empirical Freedman-type martingale concentration inequality that tightens existing bounds by accounting for the variance of the loss function"
  - [Section 3.1] Shows Σ ≤ 2√(1/n Σ(X_k - Y_k)²), explicitly relating to empirical variance
  - [corpus] Related work "VASSO: Variance Suppression for Sharpness-Aware Minimization" connects variance reduction to generalization, but in a different (sharpness-aware) context
- Break condition: If token-wise loss variance does not decrease with model size, the O(C) concentration rate advantage would not materialize.

### Mechanism 2
- Claim: Loss variance decreases predictably with model scale, tightening the variance term in generalization bounds.
- Mechanism: As models improve and approach the irreducible error, per-token prediction variance decreases. The authors empirically fit loss variation as Σ ≈ 0.27 + 8337/N^0.54 (approximately 1/√N with offset), converging to a minimum related to varentropy of natural text.
- Core assumption: The variance proxy using model-resampled tokens adequately approximates true conditional variance under the data distribution.
- Evidence anchors:
  - [abstract] "both the loss variance and the quantization error decrease, implying that larger models should have smaller generalization gaps"
  - [Section 4, Figure 2 center] Empirical fit showing loss variation decreases with model size
  - [corpus] "Ranked Set Sampling-Based Multilayer Perceptron" explores variance-based generalization bounds in MLPs, suggesting broader relevance of variance-accuracy tradeoffs
- Break condition: If larger models exhibited higher loss variance, the bound would loosen rather than tighten with scale.

### Mechanism 3
- Claim: Model information content grows sublinearly with dataset size on the compute-optimal frontier, reducing complexity per data point.
- Mechanism: Using prequential coding, the authors bound K(h) ≤ Σ[R(h_{k-1}) - R(h_D)] which scales as O(D^{1-β}) ≈ O(D^0.63) theoretically or O(D^0.5) empirically. Since complexity C = K(h)/D decreases with dataset size, larger compute-optimal models have lower complexity per token despite constant N/D ratio.
- Core assumption: Prequential codelength upper bounds Kolmogorov complexity; Chinchilla scaling law R(N,D) = E + A/N^α + B/D^β holds along training trajectory.
- Evidence anchors:
  - [abstract] "showing that the rate at which they can integrate new information grows more slowly than their capacity on the compute-optimal frontier"
  - [Section 5.2, Equation 6] Shows K(h) → β/(1-β) D^{1-β} = O(D^{1-β}) asymptotically
  - [corpus] "Leveraging Flatness to Improve Information-Theoretic Generalization Bounds for SGD" connects flatness to IT bounds, but corpus lacks direct evidence on prequential coding for LLMs
- Break condition: If information accumulated linearly with data (K(h) ∝ D), complexity C would remain constant rather than decreasing.

## Foundational Learning

- Concept: **Martingale concentration inequalities** (Azuma, Freedman)
  - Why needed here: Token predictions form a filtered sequence where X_k depends on X_{<k}; IID bounds don't apply. Freedman-type bounds incorporate variance for tighter concentration.
  - Quick check question: Given a sequence of bounded random variables adapted to a filtration, how does the variance-aware concentration rate compare to worst-case?

- Concept: **Chinchilla scaling laws** (compute-optimal frontier)
  - Why needed here: Establishes that N/D ≈ 1/20 is constant on the compute-optimal frontier, allowing isolation of variance and quantization effects.
  - Quick check question: Under fixed compute budget C ≈ 6ND, how do optimal N*(C) and D*(C) scale, and what is their ratio?

- Concept: **Kolmogorov complexity and prequential coding**
  - Why needed here: Provides theoretical foundation for measuring model information content without explicit compression schemes.
  - Quick check question: How does prequential coding bound K(X,h), and why does this imply K(h) grows sublinearly with D?

## Architecture Onboarding

- Component map:
  ```
  Training tokens (D) → Model (N params) → Checkpoints along frontier
                                      ↓
                     Loss variance Σ (empirically measured)
                                      ↓
                     Quantization gap (GPTQ at b bits)
                                      ↓
                     Complexity C = (N/D)·b + log terms
                                      ↓
                     Generalization bound: R_sq ≤ R̂_h + C·log(V) + Σ√C + √(2C) + quant_gap
  ```

- Critical path:
  1. Select compute-optimal checkpoints (N/D ≈ 1/20 from Chinchilla)
  2. Estimate loss variance Σ via token resampling under model distribution
  3. Quantize model (e.g., GPTQ at 4 bits) and measure quantization gap
  4. Evaluate bound terms; verify decreasing trend with scale

- Design tradeoffs:
  - **Quantization bitrate b**: Lower b reduces C·log(V) but increases quantization gap; optimize jointly
  - **Prequential vs. quantization complexity**: Prequential scales better asymptotically (O(D^{-β})) but is looser at current scales; quantization is tighter now but doesn't capture sublinear growth
  - **Variance proxy choice**: Model-resampling is tractable but approximate; true conditional variance requires data distribution knowledge

- Failure signatures:
  - Bound exceeds log(V) → vacuous (random guess level)
  - Quantization gap doesn't decrease with scale → check Hessian spectrum decay; may indicate optimization issues
  - Loss variance increases with scale → model instability or data distribution shift
  - 12B Pythia model "failed to quantize properly" → checkpoint taken at end of training with learning rate drop; avoid final checkpoints

- First 3 experiments:
  1. **Replicate Figure 2**: On Pythia models, measure Σ, quantization gap at b=4 bits, and evaluate full bound across scales to verify decreasing trend.
  2. **Bitrate optimization sweep**: For each model size, sweep b ∈ {2,3,4,8} and plot bound vs. bitrate to find optimal quantization level per scale.
  3. **Alternative variance proxy**: Compare model-resampling variance against held-out validation set variance to assess proxy quality and potential bias.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the smoothing term contribution be reduced from √C to O(C) through a more sophisticated approach to handling unbounded loss?
- Basis in paper: [explicit] The authors state "the √2C smoothing term seems pessimistic and could likely be improved with a different approach" and note that removing this term would let the O(D^{-β}) scaling "shine through."
- Why unresolved: Current prediction smoothing technique, while analytically optimized, may be overly conservative for the NLL objective's tail behavior.
- What evidence would resolve it: A revised bound formulation with √C dependence eliminated, validated empirically on scaled models showing tighter bounds that improve with model size.

### Open Question 2
- Question: What theoretical mechanism explains why the loss variation term Σ scales as 1/√N plus a constant offset?
- Basis in paper: [explicit] Authors state "it seems likely that the 1/√N in the loss variation term could be explained theoretically."
- Why unresolved: The empirical scaling 0.27 + 8337N^{-0.54} is observed but lacks theoretical grounding in terms of model architecture or data distribution properties.
- What evidence would resolve it: A theoretical derivation connecting model capacity, data distribution varentropy, and the observed loss variance scaling law.

### Open Question 3
- Question: Why does the Hessian spectrum decay sufficiently rapidly with model size to enable improved quantizability?
- Basis in paper: [explicit] The authors note that the Hessian-based argument "explains why larger models are more quantizable given the scaling of spectrum of the Hessian, but why the Hessian spectrum has this empirical behavior remains unexplained."
- Why unresolved: The connection between optimization dynamics, architecture, and Hessian spectral decay on natural data is not understood.
- What evidence would resolve it: A theoretical framework predicting Hessian spectral density scaling from architectural inductive biases or data distribution properties.

### Open Question 4
- Question: How do token-wise generalization bounds translate to generalization on complete sequences and downstream task performance?
- Basis in paper: [explicit] Authors acknowledge: "While it is intuitive that generalizing well on next token prediction over the training contexts should imply generalization on the full sequences, this gap remains to be more fully understood."
- Why unresolved: The paper bounds per-token generalization but real-world performance depends on multi-token coherent generation and task-specific metrics.
- What evidence would resolve it: Theoretical or empirical analysis connecting token-wise generalization gaps to sequence-level and downstream task performance degradation.

## Limitations

- The variance proxy using model-resampling may introduce bias compared to true conditional variance under the data distribution
- The 12B Pythia model failed to quantize properly, suggesting potential issues with using final training checkpoints
- The prequential coding bounds are loose at current scales, limiting practical tightness despite theoretical soundness

## Confidence

- Martingale concentration framework (Mechanism 1): High confidence - novel but built on established theory
- Loss variance scaling (Mechanism 2): High confidence - empirical fit is strong but could be model-specific
- Information-theoretic sublinear growth (Mechanism 3): Medium confidence - theoretically sound but bounds are loose at current scales
- Quantization gap measurements: Medium confidence - affected by 12B model failure

## Next Checks

1. **Alternative Variance Estimation**: Compare model-resampling variance against held-out validation set variance to assess potential bias in the variance proxy and validate the O(C) concentration advantage.

2. **Cross-Model Validation**: Test the scaling relationships (loss variance, quantization gap, information content) across different model families beyond Pythia to determine if the patterns are universal or Pythia-specific.

3. **Non-Optimal Frontier Analysis**: Evaluate the generalization bound along non-compute-optimal trajectories (different N/D ratios) to confirm that the constant N/D ratio is necessary for the observed scaling advantages.