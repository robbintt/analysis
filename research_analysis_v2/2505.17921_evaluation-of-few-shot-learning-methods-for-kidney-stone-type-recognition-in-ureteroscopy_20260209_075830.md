---
ver: rpa2
title: Evaluation of Few-Shot Learning Methods for Kidney Stone Type Recognition in
  Ureteroscopy
arxiv_id: '2505.17921'
source_url: https://arxiv.org/abs/2505.17921
tags:
- data
- kidney
- networks
- prototypical
- stone
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates Few-Shot Learning (FSL) methods for kidney
  stone type recognition in ureteroscopy. Prototypical Networks are used to classify
  six kidney stone subtypes using limited training data, addressing the challenge
  of data scarcity in medical imaging.
---

# Evaluation of Few-Shot Learning Methods for Kidney Stone Type Recognition in Ureteroscopy

## Quick Facts
- arXiv ID: 2505.17921
- Source URL: https://arxiv.org/abs/2505.17921
- Reference count: 14
- Primary result: Prototypical Networks achieve up to 88.77% accuracy on surface views and 95.22% on section views using only 25% training data

## Executive Summary
This study evaluates Few-Shot Learning (FSL) methods for classifying six kidney stone subtypes in ureteroscopy using limited training data. The approach uses Prototypical Networks with a ResNet-34 backbone to extract discriminative features from endoscopic images. The method demonstrates that FSL can achieve performance comparable to or better than traditional deep learning models trained with full datasets, addressing the critical challenge of data scarcity in medical imaging. With only 25% of available training data, the model achieved high accuracy across different view types, suggesting strong potential for clinical applications where labeled data is limited.

## Method Summary
The method uses Prototypical Networks with episodic training to classify six kidney stone subtypes from endoscopic images. A ResNet-34 backbone pre-trained on ImageNet serves as the feature extractor, with the final fully connected layer replaced by a flatten layer. The model is trained using 6-way-10-shot episodes with cross-entropy loss and Adam optimizer (learning rate 0.0001). Three datasets are created from 409 ex-vivo images: SUR (surface views), SEC (section views), and MIX (both views combined). Images are resized to 256×256 patches, standardized per channel, and split 80/20 for training and testing while ensuring patches from the same image remain in the same split.

## Key Results
- Prototypical Networks achieved 88.77% accuracy on surface views using only 25% training data
- Section views achieved higher accuracy (95.22%) than surface views with the same 25% data
- Model outperformed traditional deep learning approaches when trained with ≤25% of available data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prototypical Networks enable accurate kidney stone classification with limited training data by learning class prototypes in embedding space.
- Mechanism: The model computes a prototype (mean embedding) for each class from support set examples, then classifies query samples by finding the nearest prototype using Euclidean distance. This episodic training approach teaches the model to generalize from few examples rather than memorize a large dataset.
- Core assumption: The embedding space learned by the backbone captures discriminative features such that samples from the same stone subtype cluster together and different subtypes separate, even with limited examples.
- Evidence anchors: [abstract]: "The method extracts discriminative features using a ResNet-34 backbone, achieving performance comparable to or better than traditional deep learning models trained with full datasets." [section]: "The goal of Prototypical Networks is to learn an embedding space where classes are distinct even with few examples." [corpus]: Corpus evidence supports CNN promise for kidney stone classification but highlights data scarcity as a persistent challenge; related papers do not specifically address FSL approaches.

### Mechanism 2
- Claim: Transfer learning from ImageNet pre-training accelerates convergence and improves feature quality in low-data medical imaging scenarios.
- Mechanism: ImageNet pre-trained weights initialize the ResNet backbone with general visual feature detectors (edges, textures, shapes), which are then fine-tuned on kidney stone images. This transfers knowledge from natural images to the medical domain.
- Core assumption: Low-level and mid-level visual features learned from natural images (ImageNet) transfer meaningfully to endoscopic medical images despite domain differences.
- Evidence anchors: [section]: "When only a small amount of training data is available, it is more effective to initialize the model with pretrained weights than with random values. Accordingly, we use ImageNet-pretrained ResNet models as backbones." [corpus]: Related work (Vision Transformers paper) also explores transfer learning approaches for kidney stone classification, suggesting broader acceptance of this paradigm.

### Mechanism 3
- Claim: ResNet-34 provides the optimal depth balance for feature extraction across all three view types (SUR, SEC, MIX).
- Mechanism: The 34-layer residual network captures sufficient hierarchical features (texture, color, morphology) without overfitting to limited training data or suffering from vanishing gradients. Deeper networks (ResNet-50) may overfit; shallower networks (ResNet-18) may underfit.
- Core assumption: The complexity of kidney stone subtype discrimination aligns with ResNet-34's representational capacity.
- Evidence anchors: [abstract]: "extracts discriminative features using a ResNet-34 backbone" [section]: "the best overall performance for the SUR, SEC, and MIX views corresponds to ResNet-34. This architecture provides consistent performance across all metrics (accuracy, precision, recall, and F1-Score)." [corpus]: Related papers report using ResNet architectures for kidney stone classification, though optimal depth varies by study.

## Foundational Learning

- Concept: **Few-Shot Learning (FSL) paradigm**
  - Why needed here: The paper's core contribution is applying FSL to address data scarcity in medical imaging. Understanding episodic training, support/query set splits, and N-way K-shot terminology is essential.
  - Quick check question: Why does FSL use episodic training instead of standard batch gradient descent?

- Concept: **Metric-based meta-learning**
  - Why needed here: Prototypical Networks belong to metric-based FSL. Understanding how distance metrics (Euclidean) in embedding space enable classification without explicit class boundaries is critical.
  - Quick check question: How does the model compute the probability that a query sample belongs to a given class?

- Concept: **Transfer learning and domain shift**
  - Why needed here: The paper uses ImageNet pre-trained weights for medical images. Understanding source/target domain relationships helps assess generalization claims.
  - Quick check question: What assumptions must hold for ImageNet features to transfer effectively to endoscopic kidney stone images?

## Architecture Onboarding

- Component map: Input (256×256 patch) -> ResNet-34 backbone (ImageNet weights, FC replaced by Flatten) -> Feature embedding (512-dim for ResNet-34) -> Support set prototypes (mean per class) -> Distance computation (Euclidean) -> Softmax over distances -> Class prediction

- Critical path:
  1. Patch extraction from endoscopic images
  2. Forward pass through ResNet-34 encoder
  3. Prototype computation from support set (mean of K embeddings per class)
  4. Query classification via nearest prototype

- Design tradeoffs:
  - **Ways-shots configuration**: 6-way-10-shot performs best, but 6-way-15-shot is competitive. Higher shots increase memory; lower shots reduce prototype stability.
  - **View type**: SEC (section) views achieve higher accuracy (95.22%) than SUR (surface) views (88.77%) with 25% data—likely due to more distinctive internal morphology.
  - **Backbone depth**: ResNet-34 outperforms both ResNet-18 (underfitting) and ResNet-50 (potential overfitting with limited data).

- Failure signatures:
  - Performance drops significantly when transitioning from ex-vivo to in-vivo images (mentioned as future work).
  - Confusion matrix shows overlap between visually similar subtypes (e.g., Whewellite vs. Weddellite).
  - Training loss plateaus early with 6-way-5-shot due to insufficient prototype diversity.

- First 3 experiments:
  1. **Reproduce baseline**: Train Prototypical Networks with ResNet-34 on 25% of SEC dataset using 6-way-10-shot. Target: ~89-95% accuracy. Verify episodic training loop uses correct support/query splits.
  2. **Backbone ablation**: Compare ResNet-18, ResNet-34, ResNet-50 on SUR view with 50% data. Confirm ResNet-34 achieves highest mean accuracy across metrics.
  3. **Data efficiency curve**: Plot accuracy vs. training data percentage (25%, 50%, 75%, 100%) for both ProtoNet and traditional ResNet-34. Verify ProtoNet maintains stable performance as data decreases while traditional model degrades.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Prototypical Networks maintain high classification accuracy when applied to in-vivo endoscopic images compared to the ex-vivo phantom environment used in this study?
- Basis in paper: [explicit] The authors state in the Conclusion that "further testing on other datasets is required... such as in-vivo endoscopic images," noting that the current study relied on ex-vivo images.
- Why unresolved: The visual characteristics of kidney stones in a living patient (in-vivo) differ significantly from ex-vivo phantom images due to factors like blood, urine opacity, variable lighting, and camera motion, which creates a domain shift not addressed by the current ex-vivo dataset.
- What evidence would resolve it: A study evaluating the trained Prototypical Network model on a labeled dataset of ureteroscopy videos taken during actual surgical procedures.

### Open Question 2
- Question: How does the performance of Prototypical Networks change when trained on full-resolution images rather than the 256x256 patches utilized in this study?
- Basis in paper: [explicit] The Conclusion lists "training models on full images rather than patches to explore the impact on performance" as a specific direction for future research.
- Why unresolved: The study relied on patch extraction to artificially increase the dataset size (generating 6,000 patches from 409 images). It is unclear if the contextual information lost by cropping improves or hinders the model's ability to generalize compared to training on whole images.
- What evidence would resolve it: A comparative ablation study training the FSL model on full endoscopic images versus the cropped patches to measure the trade-off between context and data augmentation.

### Open Question 3
- Question: Do other Few-Shot Learning architectures (e.g., Matching Networks, MAML) outperform Prototypical Networks in the context of kidney stone classification?
- Basis in paper: [explicit] The Abstract and Conclusion explicitly propose "comparing additional FSL models" as necessary future work to evaluate alternative approaches.
- Why unresolved: The study focused exclusively on Prototypical Networks. While effective, different FSL methods utilize distinct mechanisms (e.g., metric learning vs. optimization-based meta-learning) that might capture the textural features of kidney stones more effectively.
- What evidence would resolve it: A benchmark comparison of multiple FSL algorithms (such as Relation Networks or MAML) using the same dataset splits and backbone architectures to establish state-of-the-art performance for this specific task.

## Limitations
- Ex-vivo dataset may not generalize to in-vivo clinical scenarios with blood, urine opacity, and variable lighting
- Limited to six stone subtypes, excluding rare subtypes that are most relevant for FSL approaches
- No statistical significance testing between model configurations or comparisons

## Confidence

- High confidence: Prototypical Networks outperform traditional models with ≤25% training data (well-supported by comparative results)
- Medium confidence: ResNet-34 is optimal backbone depth (supported by ablation but could vary with different dataset sizes)
- Low confidence: Results generalize to in-vivo clinical practice (extrapolation beyond tested conditions)

## Next Checks

1. **Statistical validation**: Perform paired t-tests between ProtoNet and traditional model performances across all data fractions to establish statistical significance of observed improvements.

2. **In-vivo generalization test**: Apply the trained model to a small in-vivo endoscopic dataset to quantify performance degradation and identify specific failure modes when transitioning from ex-vivo to clinical settings.

3. **Cross-view confusion analysis**: Generate detailed confusion matrices for each view type (SUR, SEC, MIX) to identify which stone subtypes are most frequently confused, informing potential improvements in feature extraction or data augmentation strategies.