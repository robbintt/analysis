---
ver: rpa2
title: Vocabulary embeddings organize linguistic structure early in language model
  training
arxiv_id: '2510.07613'
source_url: https://arxiv.org/abs/2510.07613
tags:
- words
- training
- embeddings
- vocabulary
- frequency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how input vocabulary embeddings in large\
  \ language models (LLMs) structure themselves over training, using representational\
  \ similarity analysis (RSA) to track geometric evolution against semantic, syntactic,\
  \ and frequency-based metrics. The authors find that semantic and syntactic structures\
  \ emerge rapidly\u2014correlations with similarity judgments and part-of-speech\
  \ groupings peak within ~15% of training\u2014while frequency-based organization\
  \ develops more gradually, with high-frequency words stabilizing faster than low-frequency\
  \ ones."
---

# Vocabulary embeddings organize linguistic structure early in language model training

## Quick Facts
- arXiv ID: 2510.07613
- Source URL: https://arxiv.org/abs/2510.07613
- Authors: Isabel Papadimitriou; Jacob Prince
- Reference count: 40
- Primary result: Vocabulary embeddings structure themselves around semantic, syntactic, and frequency-based linguistic features early in training, with distinct convergence patterns for different word types

## Executive Summary
This paper investigates how input vocabulary embeddings in large language models (LLMs) structure themselves over training, using representational similarity analysis (RSA) to track geometric evolution against semantic, syntactic, and frequency-based metrics. The authors find that semantic and syntactic structures emerge rapidly—correlations with similarity judgments and part-of-speech groupings peak within ~15% of training—while frequency-based organization develops more gradually, with high-frequency words stabilizing faster than low-frequency ones. Low-frequency words retain traces of their random initialization, unlike high-frequency words which completely shed it. After linguistic features stabilize, embeddings continue changing, primarily bringing rare technical words closer to their morphological inflections.

## Method Summary
The authors analyze vocabulary embedding matrices from Pythia-12B (153 checkpoints) and OLMo-7B (186 checkpoints) using two variants of Representational Similarity Analysis (RSA). Hypothesis RSA computes Kendall's τ correlation between model Representational Dissimilarity Matrices (RDMs) and linguistic RDMs (semantic similarity scores, syntactic class memberships, frequency ranks). Convergence RSA correlates each checkpoint's RDM with the final checkpoint RDM for vocabulary subsets. They focus on full-word tokens (~28k of 50k total vocabulary) and track 11 frequency bins from the most to least frequent words.

## Key Results
- Semantic and syntactic structures emerge rapidly, peaking within ~15% of training before stabilizing
- High-frequency words converge faster and completely shed their random initialization, while low-frequency words retain initialization traces
- Post-stabilization (after 15% training), embeddings continue changing, primarily bringing rare technical words closer to their morphological inflections

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Semantic and syntactic structures in the embedding space emerge and peak within the first ~15% of training.
- **Mechanism:** The optimization process (Next Token Prediction) rapidly aligns the geometry of input embeddings with linguistic patterns to minimize loss on frequent co-occurrences. This "early convergence" suggests that coarse linguistic features are low-hanging fruit for the optimizer compared to fine-grained frequency distributions.
- **Core assumption:** The model's ability to process language depends fundamentally on establishing this geometric structure early; without it, later layers cannot effectively manipulate representations.
- **Evidence anchors:**
  - [Abstract]: "semantic and syntactic structures emerge rapidly—correlations... peak within ~15% of training"
  - [Section 4.2]: "the semantic structure of vocabulary representations is established very early in training."
  - [Corpus]: *Geometry of Semantics in Next-Token Prediction* supports the premise that NTP implicitly organizes these representations.
- **Break condition:** If training data is shuffled to destroy local linguistic coherence (e.g., random word salad), this early structural peak would likely flatten or disappear.

### Mechanism 2
- **Claim:** High-frequency tokens converge to their final representations faster and shed initialization bias, while low-frequency tokens retain "traces" of their random initialization.
- **Mechanism:** Gradient updates are proportional to token occurrence. High-frequency words (e.g., "the") receive massive update signal, overwriting their random start. Low-frequency words receive sparse updates, leaving them stranded near their initialization vector.
- **Core assumption:** The random initialization of the embedding matrix creates a "bias" or "structure" that persists unless explicitly overwritten by significant gradient flow.
- **Evidence anchors:**
  - [Abstract]: "Low-frequency words retain traces of their random initialization, unlike high-frequency words which completely shed it."
  - [Section 5.2]: "Less frequent words preserve stronger connections to their initialization vectors even at training completion."
  - [Corpus]: Corpus neighbors focus on data efficiency and learning curves but do not explicitly contradict this frequency-dependent overwriting mechanism.
- **Break condition:** If a heavy regularization penalty is applied specifically to deviation from initialization, even high-frequency words might retain initial bias.

### Mechanism 3
- **Claim:** Post-stabilization drift (after the 15% mark) is driven by the consolidation of rare, technical words and their morphological inflections.
- **Mechanism:** Once broad semantic categories are established, the optimizer refines the space for rare tokens. It moves rare technical words (e.g., "qubit") closer to their inflections ("qubits") to share statistical strength and improve prediction accuracy on low-frequency patterns.
- **Core assumption:** This late-stage movement is not random noise but a specific refinement to align morphological variants that appear in similar contexts.
- **Evidence anchors:**
  - [Abstract]: "primarily bringing rare technical words closer to their morphological inflections."
  - [Section 6]: "The word pairs that have the greatest increase in representational similarity are a strikingly consistent set: rare, technical nouns and their plurals."
  - [Corpus]: No direct corpus evidence for this specific late-stage morphological mechanism; it appears to be a novel observation in this work.
- **Break condition:** If the vocabulary uses a stem-based tokenizer (no inflections), this specific drift mechanism would not be observable.

## Foundational Learning

- **Concept:** **Representational Similarity Analysis (RSA)**
  - **Why needed here:** The paper does not analyze raw weights; it analyzes the *relationships* between weights. RSA converts embedding matrices into distance matrices (RDMs) to compare against linguistic hypotheses.
  - **Quick check question:** Can you explain why comparing two Representational Dissimilarity Matrices (RDMs) is often better than comparing raw vectors when analyzing structure?

- **Concept:** **Spearman Distance vs. Cosine Distance**
  - **Why needed here:** The authors chose Spearman distance to mitigate the influence of "rogue dimensions" (oversized dimensions that distort distance metrics). This is critical for reproducing their geometric analysis.
  - **Quick check question:** Why would a standard Euclidean or Cosine distance fail if the embedding matrix has a few dimensions with disproportionately large values?

- **Concept:** **Zipfian Distribution (Frequency)**
  - **Why needed here:** The entire trajectory of training is stratified by frequency. Understanding that a few words dominate the corpus while a "long tail" is rare is essential to understanding why low-frequency words remain under-trained.
  - **Quick check question:** How does the power-law distribution of word frequency explain the difference in convergence speed between "the" and "hedgehog"?

## Architecture Onboarding

- **Component map:** Input Embedding Matrix ($V \times d$) -> Representational Dissimilarity Matrix (Spearman distances) -> Hypothesis RSA / Convergence RSA
- **Critical path:**
  1. Load Checkpoints: Access intermediate snapshots (e.g., Pythia's 153 checkpoints)
  2. Compute RDMs: Calculate pairwise Spearman distances for specific vocabulary subsets (e.g., top 1000 words)
  3. Hypothesis RSA: Correlate model RDMs with linguistic RDMs (e.g., SimLex for semantics, Part-of-Speech for syntax)
  4. Convergence RSA: Correlate checkpoint $t$ RDM with final checkpoint RDM
- **Design tradeoffs:**
  - Untied Embeddings: Pythia/OLMo separate input/output embeddings. This allows the model to learn different representations for "reading" vs. "predicting," but the paper shows they converge similarly.
  - Checkpoint Granularity: The authors note resource constraints limited OLMo analysis to every 3rd checkpoint. Finer granularity might reveal sharper phase transitions.
- **Failure signatures:**
  - Under-trained Tokens: If a token never appears in training, it stays at random initialization (a known issue, but this paper quantifies the gradient for *rare* tokens).
  - Initialization "Echo": Finding high correlation between final embeddings and random init for specific word groups indicates failed learning or insufficient data exposure.
- **First 3 experiments:**
  1. Reproduce the "Early Peak": Take a small LM (e.g., Pythia-70m), compute RSA against a simple semantic dataset (like WordSim) at steps 0, 1000, 5000, and final. Verify if the correlation plateaus early.
  2. Test the Frequency Bias: Identify the top-10 and bottom-10 frequency tokens in a model. Compute the correlation between their vectors at step 0 and the final step. Confirm that low-freq tokens correlate higher with their init.
  3. Visualize the Morphological Drift: Pick a rare technical word (e.g., "tensor") and plot its distance to its plural ("tensors") across training steps. Verify if the distance decreases significantly in the later 85% of training.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the evolution of vocabulary geometry facilitate specific capability gains during model training?
- **Basis in paper:** [explicit] The abstract states that the findings "motivate a deeper study of how the evolution of vocabulary geometry may facilitate specific capability gains."
- **Why unresolved:** The current study maps the trajectory of geometric organization (e.g., early syntactic peaks) but does not link these changes to performance on downstream tasks or the emergence of specific model abilities.
- **Evidence:** Correlating temporal geometric shifts with performance metrics on specific benchmarks (e.g., reasoning vs. grammaticality) across training checkpoints.

### Open Question 2
- **Question:** Can distinct training dynamics in the lexicon be isolated as effects beyond the role of word frequency?
- **Basis in paper:** [explicit] Section 7 notes that frequency is highly correlated with other factors and identifies a "promising avenue for future work" to isolate signatures "beyond the role of frequency."
- **Why unresolved:** The authors note that the experimental paradigm groups words by frequency, making it difficult to distinguish whether effects (like convergence speed) are due to frequency or correlated features like lexical function.
- **Evidence:** Tracking the representational changes of individual words rather than grouped clusters to dissociate frequency effects from other linguistic variables.

### Open Question 3
- **Question:** Why does OLMo attain stronger part-of-speech (POS) correlations than Pythia, even when other results are nearly identical?
- **Basis in paper:** [explicit] Section 4.2 observes this divergence and explicitly states, "understanding why OLMo attains stronger POS correlations than Pythia... is an interesting direction for future work."
- **Why unresolved:** While the paper documents the anomaly where OLMo's POS correlation continues to rise while Pythia's plateaus, it does not identify the architectural or data-driven cause.
- **Evidence:** Ablation studies comparing the architectures, optimization strategies, or training data distributions of Pythia and OLMo specifically regarding syntactic encoding.

## Limitations

- RSA measures statistical associations rather than causal mechanisms, so early geometric structure emergence cannot definitively prove it represents "learning to read" versus next-token prediction optimization
- The morphological drift mechanism for rare words is inferred from correlation patterns rather than demonstrated through intervention
- The frequency stratification analysis assumes the Zipfian distribution remains stable across training, which could affect interpretation of low-frequency word trajectories

## Confidence

- **High confidence**: Semantic and syntactic structure emergence within first 15% of training; frequency-dependent convergence rates (high-frequency words converge faster, low-frequency retain initialization bias)
- **Medium confidence**: Post-stabilization refinement of rare technical words and their morphological inflections; similarity between input and output embedding trajectories
- **Low confidence**: Causal mechanisms linking optimization dynamics to specific geometric changes; generalizability beyond English and the specific training corpora used

## Next Checks

1. **Intervention test**: Modify the training objective mid-training (e.g., switch from next-token prediction to masked language modeling at 15% training) to determine if the early geometric structure is task-specific or emergent from any gradient-based optimization on language data.

2. **Frequency bias verification**: Train a model with class-balanced sampling where all words appear with equal frequency, then measure if low-frequency words still retain initialization bias or if the frequency-dependent convergence pattern disappears.

3. **Morphological mechanism isolation**: Create a synthetic corpus with controlled morphological patterns (e.g., rare technical words and their inflections appear in identical contexts) and verify if the late-stage morphological drift emerges predictably versus requiring the full complexity of natural language data.