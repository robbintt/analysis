---
ver: rpa2
title: A Selective Quantization Tuner for ONNX Models
arxiv_id: '2507.12196'
source_url: https://arxiv.org/abs/2507.12196
tags:
- quantization
- arxiv
- accuracy
- seqto
- onnx
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SeQTO is a framework for selective quantization of ONNX models,
  addressing the challenge of balancing accuracy and efficiency in deep neural networks.
  It leverages ONNX Quantizer to selectively quantize layers based on error analysis,
  and supports deployment on both CPU (via ONNX Runtime) and GPU (via Apache TVM).
---

# A Selective Quantization Tuner for ONNX Models

## Quick Facts
- arXiv ID: 2507.12196
- Source URL: https://arxiv.org/abs/2507.12196
- Reference count: 40
- Primary result: Achieves up to 54.14% lower accuracy loss while maintaining up to 98.18% of size reduction compared to fully quantized models.

## Executive Summary
SeQTO is a framework for selective quantization of ONNX models that addresses the challenge of balancing accuracy and efficiency in deep neural networks. It leverages ONNX Quantizer to selectively quantize layers based on error analysis, and supports deployment on both CPU (via ONNX Runtime) and GPU (via Apache TVM). The framework uses layer-wise sensitivity analysis and Pareto Front-based optimization to identify high-quality quantized models. Experiments on four classification models show that SeQTO achieves significant improvements in accuracy retention while preserving most size reduction benefits.

## Method Summary
SeQTO implements a selective post-training quantization approach for ONNX models. The framework loads pre-trained models and performs layer-wise sensitivity analysis using two error metrics: QDQ Error (isolated layer quantization impact) and XModel Error (layer's contribution to full-model accuracy loss). These metrics are normalized and averaged equally to rank layers by quantization sensitivity. The framework then iteratively excludes high-sensitivity layers from quantization, generating candidate models that are benchmarked for accuracy and size. Pareto Front optimization identifies optimal trade-off solutions between accuracy loss and model size. The system supports both static and dynamic quantization and can deploy models to CPU (ONNX Runtime) or GPU (Apache TVM with RPC for mobile devices).

## Key Results
- Achieved up to 54.14% lower accuracy loss compared to fully quantized models
- Maintained up to 98.18% of size reduction while improving accuracy
- Successfully identified Pareto-optimal solutions across four different classification architectures
- Demonstrated hardware deployment on both CPU and mobile GPU platforms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Layer-wise error analysis reduces the search space for identifying which layers to exclude from quantization.
- Mechanism: SeQTO computes two error metrics per layer—QDQ Error and XModel Error—then normalizes and averages them equally to rank layers by quantization sensitivity.
- Core assumption: Layers with higher combined error metrics correlate with greater accuracy degradation when quantized.
- Evidence anchors:
  - [Section 3.3]: "SeQTO reduces the search space by fully quantizing the model and using a small, configurable dataset to compare activations and identify the most affected layers."
  - [Section 3.3]: "error_metric = 0.5 * norm_xmodel_err + 0.5 * norm_qdq_err"
- Break condition: If layer-level error metrics do not correlate with end-to-end accuracy loss, ranking-based exclusion may not improve results.

### Mechanism 2
- Claim: Selective quantization via layer exclusion can preserve most of the size reduction benefits of full quantization while significantly reducing accuracy loss.
- Mechanism: ONNX Quantizer accepts an exclusion list; SeQTO iteratively excludes high-sensitivity layers from quantization.
- Core assumption: The size contribution of excluded layers is small relative to total model size.
- Evidence anchors:
  - [Abstract]: "achieving up to 54.14% lower accuracy loss while maintaining up to 98.18% of size reduction compared to fully quantized models."
  - [Table 2]: MobileNetV2 GPU dynamic quantization preserves 98.18% of size reduction.
- Break condition: If excluded layers dominate model parameter count or storage, size reduction will degrade significantly.

### Mechanism 3
- Claim: Pareto Front optimization over accuracy loss and model size identifies trade-off candidates without requiring manual threshold specification.
- Mechanism: After generating multiple selectively quantized variants, SeQTO applies Non-Dominated Sorting (NDS) to find Pareto-optimal solutions.
- Core assumption: The discrete set of generated candidates sufficiently covers the objective space.
- Evidence anchors:
  - [Section 3.5]: "computes the top-K Pareto-optimal solutions using NonDominatedSorting (NDS) from the Pymoo library, minimizing a number of objectives."
  - [Figure 3]: Visual representation of Pareto-optimal candidates across excluded-layer configurations.
- Break condition: If the search space is undersampled, Pareto-optimal solutions may be suboptimal.

## Foundational Learning

- Concept: Post-training quantization (PTQ)
  - Why needed here: SeQTO operates on pre-trained ONNX models without retraining; understanding PTQ vs. quantization-aware training clarifies why accuracy loss occurs and how selective exclusion mitigates it.
  - Quick check question: Can you explain why dynamic quantization quantizes activations at runtime while static quantization requires a calibration dataset?

- Concept: Pareto optimality
  - Why needed here: SeQTO outputs multiple Pareto-optimal candidates; users must interpret trade-offs between competing objectives (accuracy vs. size).
  - Quick check question: Given two models where A has lower accuracy loss but larger size than B, which could be Pareto-optimal, and why?

- Concept: ONNX model graph and opset versions
  - Why needed here: SeQTO relies on ONNX Quantizer and graph traversal; opset version compatibility affects quantization support and potential crashes on hardware.
  - Quick check question: What might happen if a model contains an operation not supported by the target hardware's quantized execution path?

## Architecture Onboarding

- Component map:
  - Model Orchestrator -> Selective Quantization Module -> Layer Activation Analysis -> Runner Module -> Metrics Benchmarking -> Objectives Visualizer

- Critical path:
  1. Load ONNX model → 2. Run layer activation analysis (full quantization + calibration) → 3. Generate ranked exclusion list → 4. Iteratively create selectively quantized variants → 5. Deploy and benchmark each variant → 6. Compute Pareto front → 7. Visualize and report top candidates.

- Design tradeoffs:
  - Static vs. dynamic quantization: Static requires calibration data but may yield better inference efficiency; dynamic avoids calibration but may have higher runtime overhead.
  - Number of exclusion iterations: More variants improve Pareto coverage but increase runtime exponentially.
  - Calibration dataset size: Paper used 50 images; too few may yield unstable QDQ error estimates.

- Failure signatures:
  - Crash on hardware: Likely due to unsupported quantized operation; SeQTO attempts to identify and exclude responsible layers.
  - Minimal accuracy improvement despite exclusions: May indicate error metrics not correlating with actual loss, or inter-layer error interactions.
  - Size reduction drops sharply: Excluded layers may contain most parameters; reconsider exclusion strategy.

- First 3 experiments:
  1. Baseline comparison: Run SeQTO on MobileNetV2 with both static and dynamic quantization on CPU; compare fully quantized vs. top Pareto candidate for accuracy loss and size.
  2. Hardware sensitivity: Deploy the same selectively quantized ResNet50 variant on CPU (ONNX Runtime) and GPU (Apache TVM on Android); observe if Pareto-optimal candidates differ by device.
  3. Layer-ranking validation: Manually invert the exclusion order (exclude low-error layers first) and measure whether accuracy degrades relative to SeQTO's ranking—tests the core assumption of error-metric correlation.

## Open Questions the Paper Calls Out

- Question: How does SeQTO perform when applied to non-classification architectures, such as object detectors (e.g., Tiny YOLOv3) and transformers (e.g., T5, GPT-2), where layer sensitivity patterns may differ significantly from the tested CNNs?
  - Basis in paper: [explicit] The authors state in the "Conclusions and Future Work" section: "Future work will extend SeQTO to additional architectures, including object detectors... and transformers," and note the need to "support evaluation metrics such as F1 and BLEU."
  - Why unresolved: The current evaluation is limited to four classification models using Top-K accuracy. Object detection and language tasks involve complex output structures and different layer types.
  - What evidence would resolve it: Experimental results showing SeQTO's ability to generate Pareto-optimal models for Tiny YOLOv3 or GPT-2 using task-specific metrics.

- Question: Can SeQTO effectively navigate the search space for mixed-precision quantization (e.g., INT4 vs. INT8) rather than the current binary selective approach (FP32 vs. INT8)?
  - Basis in paper: [explicit] The authors explicitly list plans to "add explicit support for hardware bit-width constraints (e.g., INT8, INT4, mixed precision)" in future work.
  - Why unresolved: SeQTO currently relies on ONNX Quantizer to exclude specific layers (keeping them at FP32) or quantize them (defaulting to INT8). The framework does not yet implement logic to select between multiple integer bit-widths for a single layer.
  - What evidence would resolve it: An extension of the SeQTO framework that includes bit-width as a tunable parameter and resulting accuracy/size trade-offs comparing INT4, INT8, and mixed-precision configurations.

- Question: How does the Pareto-optimal selective quantization configuration determined by SeQTO impact actual inference latency and throughput on resource-constrained hardware?
  - Basis in paper: [explicit] The authors note that "additional quantization metrics, such as time overhead... systematic evaluation is left for future work."
  - Why unresolved: The current multi-objective optimization minimizes accuracy loss and model size (static storage). While size often correlates with speed, specific hardware accelerators may have non-linear performance characteristics.
  - What evidence would resolve it: Profiling data from the "Runner Module" showing the correlation between the selected optimal model size candidates and their actual end-to-end inference time or FPS on the target CPU/GPU devices.

## Limitations
- Error metrics are computed on a small calibration set (50 images), which may not generalize across diverse model architectures or data distributions.
- Layer-wise sensitivity ranking assumes independence, but error propagation through residual connections or skip layers may invalidate this assumption.
- The Pareto Front analysis is constrained by the discrete set of exclusion configurations generated; the true continuous Pareto-optimal frontier may lie between candidates.

## Confidence

- **High**: Selective quantization can preserve size reduction while reducing accuracy loss (empirically supported by size and accuracy metrics in Table 2).
- **Medium**: Layer-wise error ranking correlates with actual accuracy degradation (indirectly supported; ranking correlates with exclusion impact, but no explicit ablation of error metric components).
- **Low**: Pareto Front optimization reliably identifies best trade-offs (no direct comparison to exhaustive or alternative optimization methods).

## Next Checks

1. **Error Metric Ablation**: Run SeQTO with only QDQ error or only XModel error (50/50 weight removed) and compare Pareto Front quality to validate the hybrid metric assumption.

2. **Calibration Set Sensitivity**: Vary calibration set size (e.g., 10, 50, 100 images) and measure stability of layer rankings and resulting accuracy/size trade-offs.

3. **Hardware-Specific Pareto Divergence**: Deploy identical selectively quantized models on CPU vs. GPU; if Pareto-optimal candidates differ significantly, this indicates hardware-dependent quantization sensitivity.