---
ver: rpa2
title: Learning Diffusion Policy from Primitive Skills for Robot Manipulation
arxiv_id: '2601.01948'
source_url: https://arxiv.org/abs/2601.01948
tags:
- diffusion
- skill
- tasks
- policy
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of generating precise, short-term\
  \ actions from high-level language instructions in robot manipulation. The authors\
  \ propose SDP, a skill-conditioned diffusion policy that explicitly learns primitive\
  \ skills\u2014fine-grained manipulation primitives like \"move up\" and \"close\
  \ the gripper\"\u2014and uses them to guide action generation."
---

# Learning Diffusion Policy from Primitive Skills for Robot Manipulation

## Quick Facts
- arXiv ID: 2601.01948
- Source URL: https://arxiv.org/abs/2601.01948
- Reference count: 11
- Primary result: Skill-conditioned diffusion policy achieves higher success rates and better generalization than state-of-the-art methods on CALVIN and LIBERO benchmarks.

## Executive Summary
This paper proposes SDP, a skill-conditioned diffusion policy that learns to generate precise, short-horizon actions for robot manipulation from high-level language instructions. The key innovation is decomposing tasks into discrete primitive skills (e.g., "move up", "close gripper") that guide action generation. SDP uses a vision-language model to extract visual and instruction embeddings, a lightweight router to select the most relevant primitive skill at each state, and a skill-conditioned diffusion policy that generates skill-aligned actions. Experimental results on two simulation benchmarks (CALVIN and LIBERO) and real-world robot deployments show consistent performance improvements over state-of-the-art methods.

## Method Summary
SDP learns a diffusion policy conditioned on primitive skills to bridge the semantic gap between high-level language instructions and low-level continuous actions. The method extracts visual and instruction embeddings using a vision-language model, dynamically selects one of eight predefined primitive skills using a router network, and generates actions through a diffusion transformer whose feed-forward network is modulated by the selected skill. The architecture employs a LoRA-like hypernetwork mechanism to generate specialized weights for each skill, enabling the policy to produce fine-grained, task-specific actions. The approach is trained end-to-end with denoising score matching loss and an orthogonal loss to encourage skill diversity.

## Key Results
- SDP outperforms state-of-the-art methods on CALVIN benchmark, achieving higher success rates for multi-task sequences (1-5 tasks) and better average completed sequence length.
- On LIBERO benchmark, SDP shows superior performance across spatial, object, goal, and long-term manipulation suites.
- Real-world experiments demonstrate improved multi-task learning and visual generalization, with interpretable visualizations of learned primitive skills.
- Ablation studies validate the effectiveness of skill conditioning, dynamic weight generation, and compositional prompt ensemble for router stability.

## Why This Works (Mechanism)

### Mechanism 1: Intermediate Skill Conditioning Reduces Semantic Gap
If the policy decomposes tasks into discrete primitive skills rather than conditioning on global instructions, the diffusion model receives more precise constraints for denoising. This reduces ambiguity in multi-modal action distributions by constraining the action space to specific modes (e.g., vertical motion). The core assumption is that 8 abstracted skills are sufficient to compose all required tasks without significant information loss.

### Mechanism 2: Dynamic FFN Parameterization Creates Specialized Experts
The architecture uses a LoRA-like hypernetwork mechanism where the assigned skill embedding generates weights for an additional FFN layer. This ensures the transformer's feature extraction is mathematically specialized for specific skills (e.g., "grasping" features differ from "translating" features), rather than relying on a generic conditioned backbone. The core assumption is that low-rank adaptation generated by the MLP is sufficiently expressive to modulate policy behavior.

### Mechanism 3: Compositional Prompt Ensemble Stabilizes Router Selection
By fixing text prompts for skills using templates and ensembling them, the system creates distinct, orthogonal embeddings in the VLM space. This guides the router to select skills based on semantic alignment with visual observations, reducing classification error. The core assumption is that the frozen CLIP text encoder can effectively separate these 8 skill prompts into distinct regions of the embedding space.

## Foundational Learning

- **Concept: Diffusion Policy (DDIM)**
  - **Why needed here:** This is the base controller that predicts the "score" (gradient of the data distribution) or noise for an action sequence, iteratively denoising it.
  - **Quick check question:** Can you explain why a diffusion policy is preferred over a deterministic MLP for multi-modal action distributions (e.g., pushing an object left vs. right)?

- **Concept: Vision-Language Models (VLM/CLIP)**
  - **Why needed here:** The router relies on the joint embedding space of images and text for alignment.
  - **Quick check question:** How does the "Compositional Prompt Ensemble" differ from a standard zero-shot classification prompt in standard CLIP usage?

- **Concept: Hypernetworks / Dynamic Weight Generation**
  - **Why needed here:** Mechanism 2 relies on generating weights for the FFN, distinct from standard conditioning like FiLM.
  - **Quick check question:** In Eq. 4, $W_z$ is generated from skill embedding $z$. Does this increase inference FLOPs significantly compared to a fixed FFN, and why might the trade-off be worth it?

## Architecture Onboarding

- **Component map:** CLIP Image Encoder + BERT/CLIP Text Encoder -> Tokenizer -> VLM Backbone Transformer $\Phi$ -> Router (AvgPool + MLP + Softmax + Top-1) -> Skill Adapter (MLP) -> DiT (12-block Diffusion Transformer with AdaLN + Cross-Attention + Skill-Modulated FFN)

- **Critical path:** The Router is the single point of failure. If the Top-1 selection is wrong for the current state, the generated FFN weights will force the policy to execute the wrong primitive.

- **Design tradeoffs:** Fixed Skills (8) vs. Latent Skills sacrifices flexibility of learning infinite latent skills for debuggability and data efficiency. Top-1 vs. Soft Routing uses hard switching, which is cleaner for interpretability but prone to oscillation compared to soft blending.

- **Failure signatures:** Skill Oscillation occurs when the robot jitters due to rapid router flips between conflicting skills. Semantic Drift happens when the robot executes skills correctly but fails to stop because the "Stop" skill isn't triggered by the VLM.

- **First 3 experiments:**
  1. Router Validation: Visualize router's skill selection over time on CALVIN to ensure alignment with human intuition.
  2. Ablate FFN Modulation: Replace skill-conditioned FFN with simple addition to quantify performance drop.
  3. Generalization Test: Place unseen objects (distractors) in the scene to test VLM embedding robustness.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several remain unresolved based on the analysis: How can SDP be improved to generalize to objects with significantly different geometries? Does the "Top-1" skill selection constraint limit dexterous manipulations requiring simultaneous skill compositions? Is the fixed set of eight manually abstracted primitive skills sufficient for scaling to highly diverse robotic manipulation datasets?

## Limitations
- The fixed set of 8 primitive skills may limit generalization to tasks requiring novel or finer-grained motions not expressible by the skill vocabulary.
- Router training relies on implicit supervision without explicit skill labels, raising questions about stability and correctness of skill assignment during long or complex sequences.
- The paper does not quantify the semantic gap between language instructions and skill selection, nor does it provide ablation studies on the sufficiency of the current skill set.

## Confidence

- **High confidence:** The proposed diffusion policy architecture and training pipeline are clearly specified and grounded in established methods. The quantitative improvements over baselines on both CALVIN and LIBERO benchmarks are well-documented.
- **Medium confidence:** The mechanism by which skill-conditioned FFN modulation improves action precision is supported by ablation studies, but the exact contribution of the LoRA-like weight generation versus other architectural choices is not fully isolated.
- **Low confidence:** The robustness of the router's skill selection under visual ambiguity or rapid state changes is not thoroughly validated; the paper relies on high-level success metrics without granular analysis of router behavior over time.

## Next Checks

1. Router stability validation: Visualize the router's skill selection over time on a known trajectory to confirm that it aligns with human intuition and does not oscillate rapidly between conflicting skills.
2. Skill vocabulary ablation: Systematically reduce or expand the set of primitive skills and measure the impact on task success rates to quantify the semantic gap and sufficiency of the current skill set.
3. Visual generalization stress test: Introduce complex distractors and occlusions in the environment and evaluate whether the VLM embeddings remain robust enough to maintain correct skill selection and policy execution.