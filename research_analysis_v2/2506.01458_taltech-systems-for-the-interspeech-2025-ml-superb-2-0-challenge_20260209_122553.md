---
ver: rpa2
title: TalTech Systems for the Interspeech 2025 ML-SUPERB 2.0 Challenge
arxiv_id: '2506.01458'
source_url: https://arxiv.org/abs/2506.01458
tags:
- language
- speech
- languages
- data
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "TalTech\u2019s system for the ML-SUPERB 2.0 Challenge used a hybrid\
  \ approach combining a language embedding model and a generative classifier for\
  \ spoken language identification (LID), achieving 86.8% LID accuracy and 27.4% mean\
  \ CER across 154 languages. The system first identified the utterance language,\
  \ then selected the best ASR model (SeamlessM4T, MMS-1B-all, or MMS-zeroshot) based\
  \ on language-specific performance."
---

# TalTech Systems for the Interspeech 2025 ML-SUPERB 2.0 Challenge

## Quick Facts
- arXiv ID: 2506.01458
- Source URL: https://arxiv.org/abs/2506.01458
- Reference count: 0
- Primary result: Ranked 1st overall in ML-SUPERB 2.0 Challenge with 86.8% LID accuracy and 27.4% mean CER across 154 languages

## Executive Summary
TalTech's system for the ML-SUPERB 2.0 Challenge combined a hybrid language identification approach with hierarchical ASR model selection. The system achieved first place overall by achieving strong performance across six official metrics, excelling particularly in LID accuracy (86.8%) and mean CER (27.4%). The architecture leveraged a combination of acoustic embeddings and generative phonotactic models for LID, while routing each language to the most appropriate ASR model based on development set performance.

## Method Summary
The system uses a hybrid LID approach combining a language embedding model with a generative phonotactic classifier. The embedding model uses a frozen SeamlessM4T encoder with learned weighted layer aggregation and multi-resolution multi-head attention pooling, followed by a classifier trained on VoxLingua107. The generative model uses MMS-zeroshot to produce uroman character posteriors, which are decoded using language-specific bigram language models. LID scores are interpolated uniformly. For ASR, three models are used hierarchically: SeamlessM4T (finetuned), MMS-1B-all with custom adapters, and MMS-zeroshot for text-only languages. Each language is assigned to a single model based on development set CER.

## Key Results
- Achieved 86.8% LID accuracy and 27.4% mean CER across 154 languages
- Ranked 1st overall in the challenge across all six official metrics
- LID hybrid approach reduced errors by 23-31% compared to single models
- Successfully supported languages without audio data using text-based language models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining acoustic embedding-based LID with generative phonotactic LID reduces errors by 23-31% compared to either model alone
- Mechanism: Embedding model captures broad acoustic patterns via frozen W2V-BERT encoder with weighted layer aggregation; generative model captures phonotactic constraints via CTC decoding with language-specific bigrams; uniform interpolation combines complementary information
- Core assumption: Acoustic embeddings and phonotactic constraints provide non-redundant language-discriminative information
- Evidence anchors: Development set results show 89.9% accuracy (hybrid) vs 85.3% (embedding) and 70.7% (generative); 84.9% vs 80.5% and 73.2% on dialect data

### Mechanism 2
- Claim: Using uroman characters enables zero-shot LID for languages with only textual data
- Mechanism: MMS-zeroshot produces CTC posteriors over uroman characters; each language has bigram LM built from text corpora; decoding proceeds in parallel for all languages; highest-scoring decode identifies language
- Core assumption: Uroman romanization reliably captures phonotactic patterns without language-specific dictionaries
- Evidence anchors: System supports 5 languages with no audio data; inference takes ~1 second per utterance; eliminates need for pronunciation lexicons

### Mechanism 3
- Claim: Hierarchical ASR model selection optimizes mean CER by assigning each language to best-performing model
- Mechanism: Three ASR models cover different language sets; language-to-model mapping determined by development set CER; only one model runs per utterance
- Core assumption: Development set performance generalizes to test distribution
- Evidence anchors: Optimized combination achieves 7.9% CER (oracle LID) and 11.7% CER (predicted LID) on Dev; explicit language-to-model mapping in Table 5

## Foundational Learning

- **Phonotactic Language Identification**
  - Why needed here: Understanding how phoneme/character sequence probabilities under language-specific N-gram models enable LID is essential to grasp the generative classifier's design
  - Quick check question: Given a sequence of uroman characters "nakom ispaal as ta," how would bigram LMs for Afrikaans, Amharic, and Russian produce different scores?

- **CTC Decoding with External Language Models**
  - Why needed here: The generative LID and MMS-zeroshot ASR rely on combining CTC posteriors with n-gram LMs; understanding this integration is critical for implementation
  - Quick check question: How does a CTC posterior sequence interact with a bigram LM during beam search decoding?

- **Weighted Layer Aggregation in Transformer Encoders**
  - Why needed here: The embedding model learns dimension-specific weights to aggregate outputs from 24 Conformer layers, rather than using only the final layer
  - Quick check question: What is the shape of the learnable weight matrix for aggregating 24 layers of 1024-dimensional outputs?

## Architecture Onboarding

- **Component map:**
  1. **LID Branch A (Embedding):** Frozen SeamlessM4T encoder → Weighted layer aggregation → Multi-resolution multi-head attention pooling → 2×512 ReLU layers → Softmax classifier
  2. **LID Branch B (Generative):** MMS-zeroshot CTC encoder → Posteriors over uroman characters → Parallel decoding with 154 language-specific bigram LMs + lexicons → Score selection and normalization
  3. **LID Fusion:** Uniform interpolation of embedding and generative likelihoods
  4. **ASR Router:** Rule-based lookup table maps 154 languages to one of: SeamlessM4T (finetuned), MMS-1B-all (with custom adapters), or MMS-zeroshot (text-only adaptation)
  5. **ASR Engines:** Three separate models with distinct tokenizers, vocabularies, and inference pipelines

- **Critical path:**
  1. Load and freeze SeamlessM4T encoder; train embedding classifier head with data augmentation (reverb, noise) for 4 epochs
  2. Load MMS-zeroshot; build bigram LMs and lexicons from GlotLID Corpus text data using sentencepiece (10k vocab) and uroman mapping
  3. Run both LID branches on development data; tune interpolation (uniform or optimized weights)
  4. Run all three ASR models on development data per language; record CER and build language-to-model mapping table
  5. Integrate LID prediction → model lookup → ASR inference in a single pipeline; validate under 24GB VRAM constraint

- **Design tradeoffs:**
  - **Accuracy vs. Latency:** Parallel bigram decoding for 154 languages adds CPU overhead (~1s/utterance), but is still faster than running multiple full ASR models
  - **Coverage vs. Specialization:** SeamlessM4T covers high-resource languages well; MMS-1B-all with adapters extends to low-resource languages; MMS-zeroshot handles languages with no audio data but may have higher CER
  - **Simplicity vs. Optimization:** Uniform interpolation of LID models was chosen over optimized weights/temperature scaling for simplicity, with only slight performance loss

- **Failure signatures:**
  - Unexpected LID confusions between linguistically distant pairs (e.g., Greek → Highland Puebla Nahuatl, Arabic → Northeastern Thai) indicate poor-quality web-sourced training data or insufficient model robustness
  - High CER variance across languages (Table 6: STD CER = 23.9%, worst-15 CER = 82.5%) suggests certain languages remain underserved by current models
  - Dialectal speech LID accuracy drops (56.6% dialect LID) vs. general LID (86.8%), indicating embeddings or phonotactic models underfit dialectal variation

- **First 3 experiments:**
  1. **Ablate LID branches:** Run embedding-only, generative-only, and interpolated LID on held-out dialect data; quantify error reduction and identify language pairs benefiting most from each branch
  2. **ASR model reallocation:** For languages with high CER in the current mapping, test alternative models (e.g., switch from MMS-1B-all to SeamlessM4T if supported); measure CER change
  3. **Data quality audit:** Investigate the six languages requiring web-scraped text data (Batak, Fulah, Min Nan Chinese, Southern Thai, Northeastern Thai, Yoloxochitl Mixtec); inspect text corpus size, uroman mapping quality, and confusion patterns to diagnose errors

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can linguistic constraints be integrated into spoken language identification systems more efficiently than the current generative decoding approach?
- Basis in paper: [explicit] The Conclusion states, "Future work could focus on developing more efficient ways to leverage linguistic constraints in LID."
- Why unresolved: The current generative classifier, while accurate, requires running language-specific decoders for every utterance, which is computationally heavier than the embedding model
- What evidence would resolve it: A novel architecture that incorporates phonotactic constraints directly into the embedding space or a faster decoder that maintains the 89.9% development set accuracy with reduced latency

### Open Question 2
- Question: To what extent does noise in web-scraped training data cause unexpected confusion between linguistically distant languages?
- Basis in paper: [explicit] Section 4.1 notes that unexpected confusions (e.g., Greek identified as Highland Puebla Nahuatl) suggest the model "hasn't learned robust representations," potentially due to the "quality of the web-sourced training data"
- Why unresolved: The authors identified the anomaly and hypothesized a cause (data quality), but did not perform the analysis required to confirm if data contamination or acoustic noise is the primary driver
- What evidence would resolve it: A detailed analysis of the training data for the confused language pairs, followed by re-training with filtered data to see if the "unexpected confusions" disappear

### Open Question 3
- Question: What targeted modeling or data strategies are most effective for reducing the performance gap in the "tail" of worst-performing languages?
- Basis in paper: [explicit] The Conclusion notes that "significant performance variations across languages indicate that developing targeted approaches for the most challenging languages remains an important area for improvement"
- Why unresolved: While the system ranked 1st overall, it ranked 5th in the "CER 15 worst" metric, indicating the current hierarchical model selection and fine-tuning are insufficient for the hardest languages
- What evidence would resolve it: A comparative study on the 15 worst-performing languages testing specialized data augmentation or language-specific architecture adaptation against the current generalized fine-tuning approach

## Limitations
- The hybrid LID fusion uses uniform interpolation rather than optimized weights, potentially leaving accuracy on the table
- MMS-zeroshot ASR for text-only languages depends on uroman romanization quality, which may not preserve phonemic distinctions for all scripts
- Hierarchical ASR model selection assumes test distribution matches development data, potentially degrading with unseen dialects or acoustic conditions
- Six languages required web-scraped data due to corpus gaps, introducing uncertainty about data quality and representativeness
- High CER variance across languages (STD=23.9%, worst-15 CER=82.5%) indicates unequal performance across the 154 languages

## Confidence
- **High confidence:** LID hybrid mechanism (interpolation reduces errors by 23-31% on dev data), ASR model selection strategy (optimized combination outperforms individual models on dev), and overall system ranking (1st place in challenge)
- **Medium confidence:** Claims about zero-shot LID support for text-only languages (mechanism is sound but uroman quality is unverified), and coverage of all 154 languages (depends on web-scraped data quality)
- **Low confidence:** Generalization to unseen dialects or acoustic conditions (not tested), and robustness of uroman romanization for non-Latin scripts (no direct validation)

## Next Checks
1. **Ablate LID branches:** Run embedding-only, generative-only, and interpolated LID on held-out dialect data; quantify error reduction and identify language pairs benefiting most from each branch
2. **ASR model reallocation:** For languages with high CER in the current mapping, test alternative models (e.g., switch from MMS-1B-all to SeamlessM4T if supported); measure CER change
3. **Data quality audit:** Investigate the six languages requiring web-scraped text data (Batak, Fulah, Min Nan Chinese, Southern Thai, Northeastern Thai, Yoloxochitl Mixtec); inspect text corpus size, uroman mapping quality, and confusion patterns to diagnose errors