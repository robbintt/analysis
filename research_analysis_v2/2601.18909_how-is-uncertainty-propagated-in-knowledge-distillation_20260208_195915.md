---
ver: rpa2
title: How Is Uncertainty Propagated in Knowledge Distillation?
arxiv_id: '2601.18909'
source_url: https://arxiv.org/abs/2601.18909
tags:
- teacher
- student
- distillation
- uncertainty
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Knowledge distillation is inherently stochastic: teacher outputs,
  student training, and student inference can all introduce uncertainty. Collapsing
  these uncertainties to a single point estimate distorts what is learned.'
---

# How Is Uncertainty Propagated in Knowledge Distillation?

## Quick Facts
- **arXiv ID:** 2601.18909
- **Source URL:** https://arxiv.org/abs/2601.18909
- **Reference count:** 40
- **Primary result:** Knowledge distillation is inherently stochastic; collapsing uncertainties to point estimates distorts learning. Variance-aware corrections (averaging teacher responses, inverse-variance weighting) reduce noise and improve student stability.

## Executive Summary
Knowledge distillation is inherently stochastic due to randomness in teacher outputs, student training, and inference. Collapsing these uncertainties to single point estimates distorts what is learned. This work systematically studies how uncertainty propagates through knowledge distillation across three model classes—linear regression, neural networks, and LLMs—and proposes variance-aware corrections. The authors distinguish inter-student uncertainty (variance across independently distilled students) from intra-student uncertainty (variance of a single student's predictive distribution), showing that standard distillation suppresses intra-student variance while leaving substantial inter-student variability. Two variance-aware strategies are introduced: averaging multiple teacher responses (which reduces noise at rate O(1/k)) and variance-weighting (which combines teacher and student estimates via inverse-variance weighting to yield a minimum-variance estimator). Formal guarantees are provided in linear regression, validated in neural networks, and demonstrated empirically in LLM distillation, including reduced systematic noise and hallucination. These results reframe knowledge distillation as an uncertainty transformation and show that variance-aware distillation produces more stable students that better reflect teacher uncertainty.

## Method Summary
The paper introduces variance-aware knowledge distillation to address uncertainty propagation in the distillation process. For linear regression, theoretical analysis shows inter-student variance scales linearly with teacher variance. Two correction methods are proposed: (1) averaging k independent teacher responses to reduce noise at O(1/k) rate, and (2) variance-weighting that combines teacher and student estimates using inverse-variance weighting. The variance-weighting weights are computed as w_T = (1/σ²_T)/(1/σ²_T + 1/σ²_S) and w_S = 1 − w_T. These methods are validated across three model classes: linear regression (Boston Housing), neural networks (MLP on Digits/Wine/Breast Cancer/MNIST/Covertype), and LLMs (GPT2→DistilGPT2 on BioASQ). For LLMs, teacher variance is estimated from k sampled responses, and student variance from multiple forward passes. Distillation targets are constructed using either averaging or variance-weighting, and students are trained to predict these targets.

## Key Results
- Standard distillation suppresses intra-student variance (overconfidence) while leaving substantial inter-student variability across independent students.
- Averaging k teacher responses reduces variance at O(1/k) rate, with empirical validation showing MSE decay and variance reduction.
- Variance-weighting produces minimum-variance estimators by appropriately combining teacher and student estimates based on their respective uncertainties.
- In LLM distillation, variance-aware methods reduce systematic noise and hallucinations compared to single-response distillation.
- Theoretical guarantees in linear regression are empirically validated across neural networks and LLMs, demonstrating robustness across architectures.

## Why This Works (Mechanism)
Knowledge distillation propagates uncertainty from teacher to student through multiple stochastic channels. When teachers produce probabilistic outputs (via temperature scaling or sampling), each response carries sampling noise. During training, student optimization introduces additional variance through random initialization and optimization dynamics. Inference adds another layer of stochasticity. Standard distillation collapses all this uncertainty to a single target, forcing the student to fit a noisy point estimate rather than the underlying distribution. Variance-aware methods address this by explicitly modeling and accounting for these uncertainty sources. Averaging multiple teacher responses reduces sampling noise through the law of large numbers. Variance-weighting optimally combines teacher and student estimates by weighting each according to its reliability (inverse variance), ensuring the most trustworthy source dominates. This transforms distillation from fitting point estimates to learning uncertainty-aware representations.

## Foundational Learning
- **Inter-student variance:** Variance of predictions across independently distilled students from the same teacher. Needed to quantify distillation stability and the effect of teacher noise. Quick check: Train 100 students with different random seeds and compute variance of their predictions on held-out data.
- **Intra-student variance:** Variance of predictions from a single student across multiple forward passes (epistemic uncertainty). Needed to measure whether students are overconfident. Quick check: Perform multiple forward passes with dropout or sampling and compute predictive entropy.
- **Inverse-variance weighting:** Statistical technique that combines estimates by weighting each by the inverse of its variance. Needed to optimally combine teacher and student estimates when both have uncertainty. Quick check: Combine two estimates with known variances and verify minimum-variance property.
- **Stochastic teacher outputs:** Teacher models generating probabilistic responses (via temperature or sampling) rather than deterministic predictions. Needed to model uncertainty in the teacher's knowledge. Quick check: Vary teacher temperature and observe changes in output variance.
- **Variance estimation:** Empirical computation of variance from k samples. Needed for both averaging and variance-weighting methods. Quick check: Estimate variance from k samples and verify O(1/k) convergence to true variance.
- **Knowledge distillation as uncertainty transformation:** Viewing distillation as a process that transforms uncertainty from teacher to student rather than just transferring knowledge. Needed to understand why variance-aware methods improve stability. Quick check: Compare uncertainty (entropy) before and after distillation.

## Architecture Onboarding

- Component map:
  Teacher Model -> Sampling Module -> Variance Estimator -> Target Constructor -> Student Model -> Distillation Loss

- Critical path:
  1. For each input prompt, generate k teacher responses.
  2. Estimate teacher mean and variance from the k samples.
  3. (Optional) Estimate student variance via multiple forward passes.
  4. Construct the distillation target using averaging or inverse-variance weighting.
  5. Train the student model to predict this target.
  6. Evaluate student using alignment (MSE/cosine similarity) and inter-student variance.

- Design tradeoffs:
  - Averaging vs. Variance-weighting: Averaging is simpler but assumes teacher variance is the dominant source of noise. Variance-weighting is more complex but can downweight a noisy teacher or student.
  - Number of samples (k): Higher k reduces noise at O(1/k) but increases computational cost for teacher inference.
  - Variance estimation: Accurate estimation requires more samples but improves the robustness of the weighting scheme.

- Failure signatures:
  - Inter-student variance remains high: Initialization uncertainty dominates; increase k or use variance-weighting.
  - Intra-student variance is too low (overconfidence): Distillation target is too sharp; ensure teacher has non-zero temperature and consider multi-response training.
  - Variance-weighting fails to improve over averaging: Student variance is difficult to estimate reliably; simplify to averaging.

- First 3 experiments:
  1. **Baseline Test:** Perform standard single-response distillation on a regression task with synthetic teacher noise. Measure inter-student variance and MSE to confirm theoretical scaling.
  2. **Averaging Ablation:** Apply k-response averaging (k=3, 5) on the same task. Plot MSE and variance decay against k to validate the O(1/k) rate.
  3. **LLM Pilot:** Distill a small LLM (e.g., DistilGPT2) from GPT-2 on a QA dataset. Compare single-response vs. averaged-response distillation using cosine similarity to ground truth and a qualitative check for hallucinations.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do variance-aware distillation strategies scale to state-of-the-art LLMs (e.g., 70B+ parameters) and larger, more diverse datasets beyond the BioASQ benchmark?
- **Basis in paper:** [inferred] The paper validates methods on DistilGPT2 (small) and GPT2 (medium), but acknowledges that "future work should explore uncertainty across scales and domains."
- **Why unresolved:** Computational constraints limited experiments to smaller models; it remains unknown whether the O(1/k) variance reduction and variance-weighting gains persist at scale where optimization dynamics differ.
- **What evidence would resolve it:** Experiments distilling LLaMA-70B or similar to 7B students using multiple teacher responses, measuring alignment, hallucination rates, and inter-student variance.

### Open Question 2
- **Question:** How do variance-aware methods interact with token-level (logit-based) knowledge distillation, which remains dominant in practice?
- **Basis in paper:** [inferred] The paper focuses exclusively on sequence-level distillation for LLMs; Section 2.4 defines sequence-level p_T(y|x) but does not analyze token-level soft-label distillation where uncertainty manifests differently.
- **Why unresolved:** Token-level distillation operates on softmax distributions directly, which may propagate uncertainty differently than sampling-based sequence supervision.
- **What evidence would resolve it:** Comparative study applying averaging and variance-weighting to token-level KL divergence objectives, measuring whether intra-student uncertainty suppression still occurs.

### Open Question 3
- **Question:** How sensitive is variance-weighting to errors in empirical variance estimation, particularly when k (number of teacher responses) is small?
- **Basis in paper:** [inferred] Theorem 7.3 assumes accurate variance estimates, but the paper uses empirical estimates from limited samples; no sensitivity analysis is provided for estimation noise.
- **Why unresolved:** Inverse-variance weighting is optimal with true variances but can degrade substantially when estimates are noisy—common with small k or high-dimensional outputs.
- **What evidence would resolve it:** Ablation studies injecting controlled noise into variance estimates and measuring resulting student alignment degradation.

### Open Question 4
- **Question:** Can theoretical guarantees for averaging and variance-weighting be extended beyond linear regression to non-convex neural network settings?
- **Basis in paper:** [explicit] The paper states: "We provide formal guarantees in linear regression" and validates empirically in neural networks, but theorems 7.1-7.4 are specific to linear models.
- **Why unresolved:** Non-convex optimization landscapes introduce additional variance sources (local minima, optimization trajectories) not captured by linear theory.
- **What evidence would resolve it:** Theoretical analysis under local linearization assumptions or neural tangent kernel regimes, with bounds on variance reduction.

## Limitations

- Theoretical guarantees rely on linear regression assumptions (Gaussian noise, linear relationships) that may not hold in complex neural architectures.
- Variance-weighting requires reliable estimation of both teacher and student variances, which becomes computationally expensive for high-dimensional outputs and may be unstable when variances are near zero.
- Inter-student variance metric doesn't directly measure alignment with ground truth—a student with low inter-student variance could still systematically deviate from true distribution if teacher is biased.

## Confidence

- **High Confidence:** Linear regression theoretical results showing inter-student variance scales linearly with teacher variance; empirical observation that standard distillation suppresses intra-student variance while leaving substantial inter-student variability; computational complexity claims (O(1/k) variance reduction with k samples).
- **Medium Confidence:** Variance-weighting estimator achieving minimum-variance bounds in practice across different architectures; LLM results showing reduced systematic noise and hallucination through variance-aware distillation; general claim that uncertainty-aware distillation produces more stable students.
- **Low Confidence:** Specific quantitative gains reported for LLM hallucination reduction (subjective evaluation); precise characterization of when variance-weighting outperforms simple averaging in practice; claim that variance-aware distillation "better reflects teacher uncertainty" without establishing ground truth uncertainty calibration.

## Next Checks

1. **Ground Truth Uncertainty Calibration:** Design an experiment where the "true" posterior distribution is known (e.g., Bayesian linear regression with known prior), then evaluate whether variance-aware distillation produces students whose uncertainty estimates better match the true posterior compared to standard distillation, using proper scoring rules (e.g., log-likelihood, CRPS).

2. **Variance Estimation Stability Analysis:** Systematically vary k (number of teacher samples) and measurement noise in variance estimation for the variance-weighting method. Measure the sensitivity of final student performance to variance estimation errors, and determine the minimum k required for reliable weighting across different architectures and output dimensionalities.

3. **Teacher Bias vs. Variance Trade-off:** Create controlled experiments where the teacher is intentionally biased but has low variance, or unbiased with high variance. Compare standard vs. variance-aware distillation in these regimes to quantify when each approach is preferable, and whether variance-weighting appropriately downweights a biased teacher versus a noisy one.