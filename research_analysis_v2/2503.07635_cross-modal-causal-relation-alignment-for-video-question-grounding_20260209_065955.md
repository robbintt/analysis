---
ver: rpa2
title: Cross-modal Causal Relation Alignment for Video Question Grounding
arxiv_id: '2503.07635'
source_url: https://arxiv.org/abs/2503.07635
tags:
- video
- causal
- grounding
- question
- videoqg
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of spurious cross-modal correlations
  in Video Question Grounding (VideoQG), where models may rely on unfaithful visual
  grounding rather than causal visual evidence. The proposed Cross-modal Causal Relation
  Alignment (CRA) framework introduces three key components: Gaussian Smoothing Grounding
  (GSG) for estimating time intervals via cross-modal attention with adaptive Gaussian
  filtering, Cross-Modal Alignment (CMA) for bidirectional contrastive learning between
  estimated video segments and QA features, and Explicit Causal Intervention (ECI)
  for multimodal deconfounding using front-door intervention for vision and back-door
  intervention for language.'
---

# Cross-modal Causal Relation Alignment for Video Question Grounding

## Quick Facts
- **arXiv ID**: 2503.07635
- **Source URL**: https://arxiv.org/abs/2503.07635
- **Reference count**: 40
- **Primary result**: Achieves 18.2% Acc@GQA on NextGQA and 26.8% Acc@GQA on STAR, outperforming state-of-the-art methods by significant margins.

## Executive Summary
This paper addresses the problem of spurious cross-modal correlations in Video Question Grounding (VideoQG), where models may rely on unfaithful visual grounding rather than causal visual evidence. The proposed Cross-modal Causal Relation Alignment (CRA) framework introduces three key components: Gaussian Smoothing Grounding (GSG) for estimating time intervals via cross-modal attention with adaptive Gaussian filtering, Cross-Modal Alignment (CMA) for bidirectional contrastive learning between estimated video segments and QA features, and Explicit Causal Intervention (ECI) for multimodal deconfounding using front-door intervention for vision and back-door intervention for language. Extensive experiments on NextGQA and STAR datasets demonstrate CRA's superiority in discovering visually grounded content and achieving robust question reasoning.

## Method Summary
The CRA framework addresses VideoQG by estimating grounded video segments as mediators in a causal framework. It uses CLIP and RoBERTa encoders to extract visual and text features, respectively. The Gaussian Smoothing Grounding module applies cross-modal attention with adaptive Gaussian filtering to estimate temporal weights and grounded video features. Cross-Modal Alignment provides bidirectional contrastive learning between these grounded features and QA features. Explicit Causal Intervention applies front-door adjustment for vision (using the grounded segment as mediator) and back-door adjustment for language (using semantic graph features). The model is trained with Cross-Entropy and InfoNCE losses on NextGQA and STAR datasets.

## Key Results
- CRA achieves 18.2% Acc@GQA on NextGQA, outperforming state-of-the-art methods by significant margins.
- On STAR dataset, CRA achieves 26.8% Acc@GQA, demonstrating superior performance in discovering visually grounded content.
- Extensive ablation studies show that each component (GSG, CMA, ECI) contributes significantly to the overall performance gains.
- The framework effectively reduces spurious correlations, with qualitative examples showing improved visual grounding quality.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Explicitly modeling the grounded video segment as a mediator in a front-door causal framework isolates the causal visual evidence from the confounded full video context.
- **Mechanism:** The framework utilizes Front-door Causal Intervention (ECI). Instead of directly predicting an answer $a$ from the full video $V$ (which admits spurious correlations via confounder $Z$), the model predicts a temporal segment $M$ (the mediator) from $V$, and then predicts $a$ from $M$. By aggregating $P(a|M) \times P(M|V)$, the causal effect of $V$ on $a$ is computed while blocking the back-door path $V \leftarrow Z \rightarrow a$, assuming $M$ fully mediates the effect.
- **Core assumption:** The estimated video segment $M$ acts as a valid mediator that blocks all spurious paths from the video $V$ to the answer $a$, and there are no unblocked back-door paths from $V$ to $M$.
- **Evidence anchors:** [Abstract]: "Explicit Causal Intervention (ECI)... involves front-door intervention for vision." [Section 3.3]: Eq. 8-11 detail the derivation of $P(a|do(V), do(L))$ using the mediator $M=v_t$. [Corpus]: The paper "Deconfounded Reasoning for Multimodal Fake News Detection" supports the general efficacy of causal intervention in multimodal tasks, though it does not validate this specific front-door architecture.
- **Break condition:** The mechanism fails if the Gaussian Smoothing Grounding (GSG) module produces a segment $M$ that is causally irrelevant (noise); in this case, the front-door path calculates effects based on a poor mediator, yielding unreliable results.

### Mechanism 2
- **Claim:** Structuring linguistic inputs into semantic graphs allows back-door adjustment to mitigate language priors (e.g., always selecting "baby" when "woman" is mentioned).
- **Mechanism:** The Linguistic Causal Intervention (LCI) module applies back-door adjustment. It identifies linguistic confounders $Z_l$ (subject, verb, object entities) via a semantic structure graph. By marginalizing over these confounders, the model computes $P(a|V, do(L))$, blocking the path $L \leftarrow Z_l \rightarrow a$.
- **Core assumption:** The semantic structure graph constructed from the Question-Answer pair accurately captures the critical confounders $Z_l$ responsible for the linguistic bias.
- **Evidence anchors:** [Abstract]: "back-door intervention for language." [Section 3.3]: Eq. 7 shows the summation over confounders $Z_l$ derived from semantic clusters. [Figure 3(b)]: Illustrates the semantic structure graph construction. [Corpus]: Weak support; corpus papers focus on visual grounding causality rather than specific linguistic back-door implementations.
- **Break condition:** If the syntactic parsing fails to identify the true confounders (e.g., complex idiomatic biases not captured by Subject-Verb-Object graphs), the back-door adjustment will be incomplete.

### Mechanism 3
- **Claim:** Adaptive Gaussian filtering over cross-modal attention weights stabilizes temporal grounding by enforcing continuity and suppressing transient noise.
- **Mechanism:** The Gaussian Smoothing Grounding (GSG) module generates attention weights $w$. A learnable Gaussian filter is applied to $w$ before pooling. This acts as a denoiser, ensuring that the estimated time interval $t$ and grounded feature $v_t$ represent a continuous visual event rather than scattered, high-activation frames.
- **Core assumption:** The visual evidence relevant to the question appears in temporally contiguous segments rather than isolated, distant frames.
- **Evidence anchors:** [Abstract]: "estimating the time interval via cross-modal attention, which is de-noised by an adaptive Gaussian filter." [Section 3.1]: Eq. 2 defines the filter $G(\cdot)$ with learnable parameters. [Table 6]: Shows performance drop when Gaussian smoothing is removed (GSG w/o GS).
- **Break condition:** If the visual evidence is inherently discontinuous (e.g., editing cuts between two simultaneous sub-events), the Gaussian smoothing may erroneously merge or dilute the attention peaks.

## Foundational Learning

- **Concept: Structural Causal Models (SCMs) & Do-Calculus**
  - **Why needed here:** The core contribution relies on transforming observational probabilities $P(a|V)$ into interventional probabilities $P(a|do(V))$ using front-door and back-door criteria. Without this, the "deconfounding" logic is opaque.
  - **Quick check question:** Can you explain why standard correlation $P(a|V)$ fails when a confounder $Z$ influences both $V$ and $a$?

- **Concept: Cross-Modal Contrastive Learning**
  - **Why needed here:** The CMA module uses InfoNCE loss to align grounded video features with QA features. This provides the weak supervision signal required to train the GSG module without explicit grounding labels.
  - **Quick check question:** In a bidirectional contrastive loss, what defines a "positive" pair versus a "negative" pair in the context of VideoQA?

- **Concept: Temporal Attention Mechanisms**
  - **Why needed here:** The GSG relies on cross-attention between the question embedding and video frame features to identify the temporal location of the answer.
  - **Quick check question:** How does the Gaussian Smoothing layer modify the raw attention distribution generated by the cross-attention mechanism?

## Architecture Onboarding

- **Component map:** CLIP (Visual) -> Temporal Encoder -> GSG (Cross-Attention + Gaussian Filter) -> ECI (Front-door Adjustment) -> Answer Prediction; RoBERTa (Text) -> LCI (Semantic Graph + Back-door Adjustment) -> CMA (Contrastive Alignment) -> Answer Prediction
- **Critical path:** The extraction of the mediator $M$ (grounded feature $v_t$) via the GSG module. The entire causal intervention logic (Mechanism 1) depends on $v_t$ being a high-quality representation of the video segment. If $v_t$ is inaccurate, the front-door intervention propagates noise.
- **Design tradeoffs:**
  - One-stage vs. Two-stage Grounding: The paper uses a one-stage approach (direct estimation) which is faster but harder to optimize for long videos compared to proposal-based two-stage methods.
  - Gaussian vs. Raw Attention: Gaussian smoothing assumes event continuity. This sacrifices precision for highly discontinuous events in favor of robustness against attention jitter.
- **Failure signatures:**
  - Over-smoothing: If the Gaussian kernel size is too large, the model predicts the entire video duration (low IoU).
  - Language Bias Dominance: If LCI fails, the model answers correctly (high Acc@VQA) but highlights wrong segments (low Acc@GQA).
  - Cluster Mismatch: ECI relies on visual cluster centers $e_v$. If these clusters do not cover the test distribution, the approximation of $P(V)$ fails.
- **First 3 experiments:**
  1. Ablation on Grounding Noise: Run GSG with and without the Gaussian filter to visualize the stability of attention weights and the resulting IoU scores (verify Table 6).
  2. Qualitative SCM Validation: Visualize the "Unfaithful Grounding" examples (Figure 1). Run the model with/without ECI to confirm that the visual attention shifts from the confounder (e.g., just the "baby") to the causal interaction (e.g., "baby and woman").
  3. Back-door Effectiveness: Test on datasets with high linguistic bias (checking if LCI improves performance on "what/who" questions where correlations are strong).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the Gaussian Smoothing Grounding (GSG) module be adapted to effectively estimate longer, non-Gaussian temporal intervals that span distinct sub-events?
- **Basis in paper:** [inferred] Section 4.4 (Qualitative Analysis) notes that the model tends to generate short intervals (~2.5s) and often "neglects the estimation of longer intervals" found in the ground truth, suggesting a limitation of the single-peak Gaussian assumption.
- **Why unresolved:** The current GSG mechanism relies on a Gaussian filter which inherently favors narrow, bell-curve attention weights, failing to capture continuous but varied evidence over extended periods.
- **What evidence would resolve it:** An extension of the framework that utilizes multi-modal or uniform attention distributions, demonstrating improved IoU on video samples with ground-truth intervals longer than 20 seconds.

### Open Question 2
- **Question:** To what extent does the choice of clustering algorithm and semantic parser affect the reliability of the Explicit Causal Intervention (ECI)?
- **Basis in paper:** [inferred] Section 3.3 describes the construction of confounders ($e_V$ and $e_L$) using specific tools (K-means clustering centers, Stanza parser) without analyzing the sensitivity of the model to the quality or granularity of these representations.
- **Why unresolved:** If the selected confounders (e.g., 512 visual clusters) do not accurately reflect the true dataset biases or spurious correlations, the causal intervention (Eq. 11) may fail to deconfound the features effectively.
- **What evidence would resolve it:** An ablation study analyzing performance variance when varying the number of visual clusters or using different dependency parsers to construct the semantic structure graphs.

### Open Question 3
- **Question:** Can the Cross-modal Causal Relation Alignment framework maintain its efficacy in reducing spurious correlations when applied to significantly larger foundational models (e.g., LLaMA-based video agents)?
- **Basis in paper:** [explicit] Section 4.3.3 states that the results on Feasibility questions "motivate further development of the CRA framework with even larger models to enhance its capability to handle such complex reasoning tasks."
- **Why unresolved:** While the paper tests FrozenBiLM, modern Large Video-Language Models (LVLMs) possess different emergent biases and reasoning capabilities; it is uncertain if the current front-door/back-door intervention strategy scales to these more complex architectures without modification.
- **What evidence would resolve it:** Experiments integrating the CRA modules into a contemporary LVLM (e.g., Video-LLaVA) and measuring the reduction in hallucination or bias on complex logical reasoning benchmarks.

## Limitations
- The framework's effectiveness depends on three unverified assumptions: Gaussian-smoothed attention captures causal segments, SVO parsing comprehensively captures language bias, and cluster-based approximations of P(V) and P(L) are valid across test distributions.
- The paper does not report runtime efficiency or memory consumption, which could be substantial given the multiple Transformer layers and clustering operations.
- The assumption that Gaussian smoothing preserves causal continuity rather than potentially diluting important visual evidence lacks empirical validation through ablation studies.

## Confidence
- **High confidence**: The experimental methodology and dataset construction are clearly specified, with reproducibility largely achievable for the core components.
- **Medium confidence**: The mathematical formulation of causal interventions (front-door and back-door) follows established theory, though the specific implementation details for cluster-based approximations introduce uncertainty.
- **Low confidence**: The assumption that Gaussian smoothing preserves causal continuity rather than potentially diluting important visual evidence lacks empirical validation through ablation studies.

## Next Checks
1. **Ablation Study on Gaussian Kernel Size**: Systematically vary the Gaussian filter width in the GSG module to quantify the trade-off between smoothing-induced stability and precision loss in temporal grounding.

2. **Confounder Coverage Analysis**: Design synthetic questions that explicitly test whether the SVO-based linguistic graph captures all major sources of language bias, including idiomatic expressions and implicit entity relationships.

3. **Cluster Distribution Validation**: Analyze the visual and linguistic cluster distributions to verify that test-time data points fall within the convex hull of training clusters, ensuring the intervention approximations remain valid.