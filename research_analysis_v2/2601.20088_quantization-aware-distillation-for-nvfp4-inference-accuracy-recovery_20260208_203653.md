---
ver: rpa2
title: Quantization-Aware Distillation for NVFP4 Inference Accuracy Recovery
arxiv_id: '2601.20088'
source_url: https://arxiv.org/abs/2601.20088
tags:
- data
- nemotron
- training
- arxiv
- nvfp4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Quantization-aware distillation (QAD) is presented as an effective
  method for recovering accuracy of NVFP4-quantized large language models and vision-language
  models. QAD distills a full-precision teacher model into a quantized student model
  using KL divergence loss, differing from standard quantization-aware training (QAT)
  which uses task-specific loss functions.
---

# Quantization-Aware Distillation for NVFP4 Inference Accuracy Recovery

## Quick Facts
- arXiv ID: 2601.20088
- Source URL: https://arxiv.org/abs/2601.20088
- Reference count: 10
- Primary result: QAD recovers NVFP4-quantized LLM accuracy to near-BF16 levels, outperforming QAT on RL-trained models

## Executive Summary
Quantization-aware distillation (QAD) is presented as an effective method for recovering accuracy of NVFP4-quantized large language models and vision-language models. QAD distills a full-precision teacher model into a quantized student model using KL divergence loss, differing from standard quantization-aware training (QAT) which uses task-specific loss functions. The method shows remarkable effectiveness and stability for models trained through multi-stage post-training pipelines, including supervised fine-tuning, reinforcement learning, and model merging, where QAT faces engineering complexity and training instability. QAD is robust to data quality and coverage, enabling accuracy recovery without full training data. Across multiple post-trained models including AceReason Nemotron, Nemotron 3 Nano, Nemotron Nano V2, Nemotron Nano V2 VL, and Llama Nemotron Super v1, QAD consistently recovers accuracy to near-BF16 levels. The method achieves near-BF16 accuracy across various benchmarks including MATH500, AIME25, GPQA-D, and LiveCodeBench, demonstrating effectiveness on both SFT-heavy and RL-heavy models. QAD also shows cross-domain knowledge transfer capabilities, maintaining performance even when trained with partial domain data.

## Method Summary
QAD uses KL divergence loss between teacher and student token distributions to recover NVFP4 quantization accuracy. The method distills a frozen BF16 teacher into an NVFP4-quantized student using KL divergence loss with temperature T=1. Training data from SFT, RL-generated, or mixed sources (0.3-6B tokens) is used with learning rates of 1e-6 for SFT-heavy models and 1e-5 for RL-heavy models. The method is robust to data quality and coverage, enabling accuracy recovery without full training data. QAD consistently recovers accuracy to near-BF16 levels across multiple benchmarks and model types, showing particular effectiveness for RL-trained models where QAT faces training instability.

## Key Results
- QAD recovers NVFP4 accuracy to near-BF16 levels across multiple models and benchmarks
- QAD outperforms QAT on RL-trained models, recovering accuracy degraded by QAT (Nemotron 3 Nano AA-LCR: 31.3→24.8 with QAT vs 31.3→34.3 with QAD)
- QAD is robust to data quality and coverage, maintaining performance when trained with partial domain data (e.g., code-only data recovers strong math performance)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: QAD preserves the original model's output distribution better than QAT by matching soft probability distributions rather than hard labels.
- Mechanism: KL divergence loss between teacher and student token distributions provides implicit regularization and lower-variance gradient signals compared to cross-entropy on ground-truth labels. The teacher distribution encodes uncertainty and inter-class relationships that survive quantization better than point estimates.
- Core assumption: The full-precision teacher's output distribution represents the optimal target for the quantized student, regardless of the data domain used for training.
- Evidence anchors: [abstract] "QAD distills a full-precision teacher model into a quantized student model using a KL divergence loss"; [section 3.1] Table 1 shows QAT achieves 0.311 KL divergence vs BF16 while QAD achieves 0.004, despite similar cross-entropy losses; [corpus] Related work (Liu et al. 2023b) shows data-free distillation effectiveness for LLM quantization
- Break condition: If the teacher model has significant degradation in certain domains, distillation may propagate those errors. Also, if quantization error is so severe that output distributions become unmatchable (very small models, extremely aggressive quantization beyond FP4).

### Mechanism 2
- Claim: QAD enables cross-domain knowledge transfer—the student recovers capabilities in domains not represented in training data.
- Mechanism: The teacher's output distributions over the full vocabulary encode implicit knowledge about all learned domains. By matching these distributions on partial-domain inputs, the student learns to approximate teacher behavior across the full capability space. This works because token probabilities for domain-adjacent concepts are correlated.
- Core assumption: Teacher output distributions contain extractable cross-domain information even when activated by single-domain inputs.
- Evidence anchors: [abstract] "QAD is robust to data quality and coverage, enabling accuracy recovery without full training data"; [section 3.3] Table 4: QAD trained with code-only data recovers strong math performance (AIME24: 71.0 vs full data 71.7); [corpus] No direct corpus evidence for cross-domain transfer in quantization context; this appears novel
- Break condition: If domains are completely orthogonal with no shared vocabulary or reasoning patterns, transfer may fail. Also breaks if training data is too narrow to activate relevant latent knowledge.

### Mechanism 3
- Claim: QAD avoids capability destruction in RL-trained models by not requiring re-exposure to RL training dynamics.
- Mechanism: RL training shifts models away from their SFT initialization in ways that are difficult to replicate. QAT with SFT data or generated samples acts as an additional training stage that overwrites RL-learned behaviors. QAD instead directly copies the final RL model's distribution without requiring understanding of the RL objective or prompts.
- Core assumption: The RL-trained model's final distribution is the correct target, and any deviation from it (as QAT causes) represents capability loss.
- Evidence anchors: [abstract] "QAD shows remarkable effectiveness and stability for models trained through multi-stage post-training pipelines, including...reinforcement learning"; [section 3.2] Table 3: Nemotron 3 Nano QAT degrades AA-LCR from 31.3 (PTQ) to 24.8; QAD recovers to 34.3 (near BF16's 35.9); [corpus] Corpus mentions RL training but no corpus papers address quantization of RL-trained models
- Break condition: If the RL model has severe distribution collapse or overfitting, QAD may distill those artifacts. Also, if QAD learning rate is too high for well-converged SFT models, it can cause instability.

## Foundational Learning

- Concept: **KL Divergence for Distribution Matching**
  - Why needed here: Core loss function; understanding why KL outperforms MSE requires grasping that KL measures probabilistic distance with better gradient properties for token distributions
  - Quick check question: Why does KL divergence provide better gradients than MSE for matching softmax outputs over a vocabulary?

- Concept: **NVFP4 Format (Block Floating Point)**
  - Why needed here: The quantization target format has specific properties (16-element blocks, E4M3 scales, FP32 secondary scale) that determine where quantization error concentrates
  - Quick check question: How does NVFP4's 16-element block size differ from MXFP4's 32-element blocks, and why does this matter for outlier handling?

- Concept: **Post-Training Quantization (PTQ) vs Quantization-Aware Training (QAT)**
  - Why needed here: QAD is positioned as an alternative to both; understanding when PTQ is sufficient vs when active training is needed informs deployment decisions
  - Quick check question: For a 250B parameter model, would you expect PTQ to be sufficient, and why might this differ for a 7B model?

## Architecture Onboarding

- Component map:
  - Teacher (BF16 model, frozen) -> Forward pass on input batch -> Softmax with T=1 -> Teacher logits
  - Student (NVFP4 quantized model, trainable) -> Quantized forward pass -> Softmax with T=1 -> Student logits
  - KL divergence computation: sum over vocabulary of p_teacher * log(p_teacher / p_student)
  - Backprop through student only (gradients remain FP32, only weights/activations quantized)

- Critical path:
  1. Load BF16 teacher checkpoint, freeze all parameters
  2. Initialize student from same checkpoint, apply NVFP4 quantization to forward pass
  3. Prepare distillation data (0.3-6B tokens depending on model size; can use SFT data, generated data, or mixtures)
  4. Set learning rate: 1e-6 for SFT-heavy models (at or below original LR), 1e-5 for RL-heavy models
  5. Train with batch size/sequence length matching original post-training
  6. Evaluate top 10 checkpoints by validation loss, select best on held-out benchmarks

- Design tradeoffs:
  - **KL divergence vs MSE**: KL consistently outperforms (Table 8); use KL
  - **Teacher selection**: Original model as teacher outperforms larger teacher from same family (Table 9); use same model
  - **Data source**: SFT data, RL-generated data, and mixtures all work; random tokens don't break the model but give worse recovery
  - **Learning rate**: Too high causes divergence in SFT models; too low under-recovers in RL models
  - **Quantization scope**: Can selectively keep attention layers or first/last layers in BF16 for better PTQ baseline

- Failure signatures:
  - QAT on RL models: Accuracy drops below PTQ baseline (e.g., Nemotron 3 Nano AA-LCR: 31.3 PTQ → 24.8 QAT)
  - High KL divergence to BF16 despite low cross-entropy: Indicates distribution shift (QAT pattern in Table 1)
  - Training instability with LR > 1e-5 on well-converged SFT models
  - Incomplete recovery on reasoning benchmarks (AIME, GPQA-D) while instruction-following recovers: Suggests undertraining

- First 3 experiments:
  1. **Baseline PTQ evaluation**: Quantize model with NVFP4 max calibration, evaluate on target benchmarks to establish gap from BF16
  2. **QAD vs QAT comparison on held-out data**: Train both with same SFT data mixture (~1B tokens), compare KL divergence to BF16 teacher and benchmark accuracy
  3. **Learning rate sweep**: Test [1e-6, 5e-6, 1e-5] on validation loss and benchmark accuracy to find optimal LR for your specific model type (SFT vs RL-heavy)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can quantization-aware RL (QARL), which incorporates quantization directly into the RL training loop, be developed as an alternative to post-hoc QAD for RL-trained models?
- Basis in paper: [explicit] "An alternative approach would be to incorporate QAT into the RL training process itself, which remains an active research topic... existing work on quantized RL has focused on accelerating training throughput rather than post-hoc inference accuracy recovery."
- Why unresolved: QARL is mentioned as an open direction but not implemented; the paper only addresses post-hoc recovery via QAD.
- What evidence would resolve it: A study comparing QARL-integrated RL training against post-hoc QAD on RL-heavy models like AceReason Nemotron.

### Open Question 2
- Question: What is the theoretical mechanism behind QAD's cross-domain knowledge transfer, where training on partial domain data (e.g., code-only) recovers performance on unseen domains (e.g., math)?
- Basis in paper: [inferred] The paper states "the teacher's output distributions encode implicit knowledge about all domains" but provides only an intuitive explanation without formal analysis.
- Why unresolved: The phenomenon is empirically demonstrated (Table 4) but not theoretically explained; understanding this could inform optimal data selection strategies.
- What evidence would resolve it: Analysis of attention patterns or intermediate representations during QAD to identify how domain-agnostic knowledge is preserved and transferred.

### Open Question 3
- Question: Does QAD's effectiveness generalize to other 4-bit formats beyond NVFP4, such as INT4 or MXFP4?
- Basis in paper: [inferred] The paper evaluates only NVFP4 quantization; while NVFP4 has specific properties (smaller block size, two-level scaling), QAD's format-agnostic applicability is untested.
- Why unresolved: Different quantization formats have different error characteristics; QAD's KL divergence matching may interact differently with each.
- What evidence would resolve it: Comparative experiments applying QAD to INT4 and MXFP4-quantized models across the same benchmarks.

## Limitations
- Cross-domain knowledge transfer mechanism lacks direct experimental validation and theoretical explanation
- RL training stability claims are based on observed accuracy improvements rather than understanding underlying distribution dynamics
- Method's performance on extremely small models (7B parameters) remains unclear as results focus on 8B-70B parameter range

## Confidence
- **High Confidence**: The core empirical results showing QAD consistently outperforms QAT on tested models and benchmarks
- **Medium Confidence**: The mechanism explanations for why QAD works better than QAT, supported by empirical evidence but somewhat heuristic
- **Low Confidence**: The cross-domain knowledge transfer claims and assertion that QAD avoids RL capability destruction, inferred from observed patterns rather than directly measured

## Next Checks
1. **Direct Cross-Domain Transfer Test**: Train QAD with data from one domain (e.g., code only) and explicitly measure performance on completely disjoint domains (e.g., math reasoning) to quantify the actual transfer effect versus simple robustness to training data coverage

2. **Selective Layer Sensitivity Analysis**: Systematically test which layers benefit most from remaining in BF16 versus NVFP4 (attention vs feed-forward vs first/last layers) to optimize the hybrid quantization approach and validate the selective quantization claims

3. **Extreme Quantization Boundary Test**: Apply QAD to a 7B parameter model with NVFP2 quantization (more aggressive than NVFP4) to determine the method's limits and whether the claimed robustness extends to smaller models and more aggressive quantization schemes