---
ver: rpa2
title: 'MariNER: A Dataset for Historical Brazilian Portuguese Named Entity Recognition'
arxiv_id: '2506.23051'
source_url: https://arxiv.org/abs/2506.23051
tags:
- entity
- portuguese
- https
- dataset
- historical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MariNER, the first gold-standard dataset
  for named entity recognition (NER) in early 20th-century Brazilian Portuguese, containing
  over 9,000 manually annotated sentences across four entity types: PERSON, LOCATION,
  ORGANIZATION, and DATE. The dataset was built from historical travel records and
  articles, digitized and normalized to modern Portuguese before manual annotation.'
---

# MariNER: A Dataset for Historical Brazilian Portuguese Named Entity Recognition

## Quick Facts
- arXiv ID: 2506.23051
- Source URL: https://arxiv.org/abs/2506.23051
- Authors: João Lucas Luz Lima Sarcinelli; Marina Lages Gonçalves Teixeira; Jade Bortot de Paiva; Diego Furtado Silva
- Reference count: 30
- Primary result: First gold-standard NER dataset for early 20th-century Brazilian Portuguese with 9,649 manually annotated sentences

## Executive Summary
MariNER introduces the first gold-standard dataset for named entity recognition in early 20th-century Brazilian Portuguese, containing over 9,000 manually annotated sentences across four entity types: PERSON, LOCATION, ORGANIZATION, and DATE. The dataset was built from historical travel records and articles, digitized and normalized to modern Portuguese before manual annotation. State-of-the-art models were evaluated, with XLM-RoBERTa achieving the highest micro-F1 score of 0.922. Large language models performed significantly worse, with the best LLM (Gemma2) reaching only 0.648 F1. Cross-dataset experiments showed that models trained on MariNER outperformed those trained on general or legal-domain Portuguese datasets by approximately 20%, underscoring the importance of domain-specific training data for historical texts.

## Method Summary
The study developed MariNER through a three-stage process: digitization and normalization of historical texts from early 20th-century Brazilian travel records, manual annotation using BIO tagging scheme, and evaluation of three NER approaches. The annotation team manually labeled 9,649 sentences across four entity types. Three model families were tested: BiLSTM-CRF with Flair embeddings, BERT-based transformers (BERTimbau and XLM-RoBERTa) with linear or CRF classifiers, and LLM-based zero-shot text-to-text approaches. Models were evaluated using entity-level micro-F1 with CoNLL-2002 evaluation scripts, and cross-dataset experiments compared MariNER-trained models against those trained on HAREM and LeNER-BR datasets.

## Key Results
- XLM-RoBERTa-Large with linear classifier achieved 0.922 micro-F1, the highest performance
- All BERT-based models outperformed LLM approaches by approximately 27% F1
- Models trained on MariNER outperformed those trained on general/legaldomain Portuguese datasets by approximately 20%
- ORGANIZATION entity type showed severe performance degradation (best F1=0.640) due to class imbalance

## Why This Works (Mechanism)

### Mechanism 1: Fine-tuned Encoder Superiority via Task-Specific Optimization
Fine-tuning transformer encoders for the specific sequence classification task allows direct optimization of weights for BIO tagging, while LLM approaches using zero-shot text-to-text prompting lack this task-specific weight adjustment. This optimization gap explains the substantial performance difference, though alternative prompting or fine-tuning strategies for LLMs could potentially narrow this gap.

### Mechanism 2: In-Domain Training Data Dominance
Historical texts contain unique entity distributions and linguistic patterns not captured by modern or legal-domain corpora. Training on in-domain historical data provides significant performance gains (~20%) by addressing this domain shift, though massive pre-training could potentially reduce this dependency over time.

### Mechanism 3: CRF Layer for Sequence Consistency
CRF layers learn transition probabilities between BIO tags, enforcing valid tag sequences and providing marginal performance gains. While XLM-RoBERTa with linear head slightly outperformed its CRF counterpart, the CRF's ability to correct locally inconsistent predictions remains valuable, though very large encoders may implicitly learn these constraints.

## Foundational Learning

- **Concept: Sequence Labeling / BIO Tagging**
  - Why needed here: Core task format requiring token-level classification using Begin-Inside-Outside tags rather than text generation
  - Quick check question: For "President John F. Kennedy," what are the correct BIO tags? (Answer: O, B-PER, I-PER, I-PER)

- **Concept: Domain Shift in NLP**
  - Why needed here: Explains why models trained on modern/legal text fail on historical travel logs due to linguistic and conceptual drift
  - Quick check question: Why would the entity "Parahyba-Hotel" be challenging for a model trained only on modern Wikipedia?

- **Concept: Entity-level vs. Token-level Evaluation**
  - Why needed here: Micro-F1 metric requires correct prediction of all tokens in an entity for credit, a stricter measure than per-token accuracy
  - Quick check question: If a model predicts "Rio Grande do Sul" as "B-LOC, I-LOC, O, O", does it get partial credit in entity-level F1? (Answer: No, it is a false positive for "Rio Grande" and a false negative for the full entity)

## Architecture Onboarding

- **Component map:** Historical texts -> Digitization & Normalization -> Manual BIO Annotation -> Train/Test Split -> XLM-RoBERTa-Large with Linear Classifier -> Entity-level Micro-F1 Evaluation
- **Critical path:** Load pre-processed, BIO-tagged sentences -> Initialize XLM-RoBERTa-Large with token classification head -> Fine-tune using AdamW optimizer (lr=3e-5, 15 epochs) -> Evaluate using entity-level micro-F1 on test set
- **Design tradeoffs:** XLM-RoBERTa (multilingual) slightly outperformed BERTimbau (Portuguese-specific); linear head marginally better than CRF; zero-shot LLMs offer easier deployment but substantial performance penalty
- **Failure signatures:** ORGANIZATION entity severely underperforms (F1=0.640); LLMs show high recall but very low precision causing hallucinated entities; k-shot LLM degradation likely due to context window limitations
- **First 3 experiments:** 1) Reproduce XLM-RoBERTa baseline (~0.92 F1) 2) Address ORGANIZATION imbalance through oversampling/augmentation 3) Test alternative LLM prompting strategies for Gemma2

## Open Questions the Paper Calls Out
- Does expanding entity definitions to include sub-types improve model performance for finer-grained historical extraction tasks?
- How does increasing source document diversity from different Brazilian regions affect NER model robustness and generalizability?
- Why does few-shot prompting degrade LLM performance, and can alternative strategies mitigate this?
- To what extent does manual normalization to modern Portuguese influence NER accuracy compared to raw historical text?

## Limitations
- Dataset not yet publicly available, preventing immediate replication
- Severe class imbalance for ORGANIZATION entity type (231 examples) causing performance degradation
- LLM methodology using text-to-text approach may not represent optimal performance

## Confidence
- **High confidence:** Fine-tuned transformer encoders achieving 0.922 F1; domain shift causing ~20% degradation; CRF layers providing marginal gains
- **Medium confidence:** LLM zero-shot performance being significantly worse; cross-dataset transferability results
- **Low confidence:** Generalizability to other historical Portuguese corpora; long-term stability of LLM prompting

## Next Checks
1. Reproduce XLM-RoBERTa baseline: Fine-tune on MariNER training set and verify ~0.92 F1 on test set once available
2. Address ORGANIZATION imbalance: Apply class weighting or synthetic data augmentation and re-evaluate
3. Test alternative LLM approaches: Experiment with fine-tuned LLM variants or different prompt engineering strategies