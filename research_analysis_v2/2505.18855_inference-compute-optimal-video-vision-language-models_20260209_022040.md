---
ver: rpa2
title: Inference Compute-Optimal Video Vision Language Models
arxiv_id: '2505.18855'
source_url: https://arxiv.org/abs/2505.18855
tags:
- scaling
- video
- data
- inference
- compute
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses optimal allocation of inference compute in
  video vision language models (VLMs) across three scaling factors: language model
  size, frame count, and visual tokens per frame. The authors conduct large-scale
  training sweeps (100k A100 hours) to collect performance data and fit parametric
  models that capture how task performance depends on these scaling factors and finetuning
  data size.'
---

# Inference Compute-Optimal Video Vision Language Models

## Quick Facts
- arXiv ID: 2505.18855
- Source URL: https://arxiv.org/abs/2505.18855
- Reference count: 27
- Optimal allocation of inference compute across LM size, frame count, and visual tokens requires joint scaling at varying rates

## Executive Summary
This work addresses optimal allocation of inference compute in video vision language models (VLMs) across three scaling factors: language model size, frame count, and visual tokens per frame. The authors conduct large-scale training sweeps (100k A100 hours) to collect performance data and fit parametric models that capture how task performance depends on these scaling factors and finetuning data size. They solve a constrained discrete optimization problem to identify the compute-optimal frontier, finding that optimal performance requires jointly scaling all three factors at varying rates. The finetuning data size significantly influences the optimal allocation: as more data becomes available, it is optimal to allocate less compute to LM size and more to video visual representations. The parametric model achieves strong in-distribution fit (R²=0.98) and reasonable extrapolation performance (average relative error 1.33%), though accuracy varies across tasks.

## Method Summary
The authors conduct two types of training sweeps to collect performance data: a star sweep varying one scaling factor at a time from a center point (7.5B LM, 32 frames, 196 tokens/frame) across multiple finetuning data sizes, and an isoFLOP sweep at fixed compute budgets. They fit an additive power-law parametric model with interaction terms between scaling factors and finetuning data size. Using bootstrap aggregation with median aggregation, they stabilize parameter estimates and solve a constrained discrete optimization problem to find the compute-optimal frontier. The approach extends Chinchilla-style compute-optimal scaling to the multimodal setting while accounting for vision model compute costs.

## Key Results
- Compute-optimal performance requires jointly scaling LM size, frame count, and visual tokens at varying rates
- As finetuning data increases, optimal allocation shifts from LM size to visual representations
- Parametric model achieves R²=0.98 in-distribution fit with average 1.33% extrapolation error
- Vision model can contribute ~50% of inference FLOPs for typical configurations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task performance follows an additive power-law form with interaction terms between scaling factors and finetuning data size
- Mechanism: The `add-interact` parametric model decomposes error into factor-specific irreducible terms, data-reducible terms with factor-data interaction, and baseline data scaling
- Core assumption: Error components combine additively rather than multiplicatively
- Evidence anchors: Strong in-distribution fit (R²=0.98) outperforming multiplicative and simple additive forms
- Break condition: If b_k exponents were consistently near zero or if multiplicative error decomposition fit better

### Mechanism 2
- Claim: Joint scaling of (x_N, x_T, x_V) at varying rates is necessary for optimal performance
- Mechanism: Single-factor scaling creates bottlenecks; underscaled factors limit returns from others
- Core assumption: Vision model compute cost is non-negligible
- Evidence anchors: IsoFLOP curves and compute-optimal frontier show joint scaling requirement
- Break condition: If vision encoder cost were truly negligible or tasks uniformly LM-bottlenecked

### Mechanism 3
- Claim: Finetuning data size shifts the compute-optimal frontier toward visual representations
- Mechanism: Elasticity analysis shows negative elasticity for x_N and positive for x_T and x_V
- Core assumption: Add-interact model extrapolates reliably beyond sweep range
- Evidence anchors: Elasticity plots show consistent shift across tasks
- Break condition: If tasks were uniformly LM-knowledge-bottlenecked rather than visual-perception-bottlenecked

## Foundational Learning

- **Power-law scaling relationships**: Loss/performance scales as x^{-a} for various factors
  - Why needed here: The entire parametric modeling framework is built on power-law assumptions
  - Quick check question: Given f(n) ∝ n^{-d} with d=0.3, how much does error reduce when doubling data?

- **Constrained discrete optimization**: Finding optimal x given budget constraint over discrete search space
  - Why needed here: Equation 4 formulates the core problem requiring brute-force search
  - Quick check question: Why can't the compute-optimal frontier be found analytically when vision model cost is included?

- **Bootstrap aggregation (bagging)**: Stabilizing parameter estimates from high-variance training runs
  - Why needed here: Figure 3 shows parameter variance; bagging reduces extrapolation MSE
  - Quick check question: Why might median aggregation outperform mean aggregation for this fitting problem?

## Architecture Onboarding

- **Component map**: Vision encoder (SoViT-400m/14) -> Projector (2-layer MLP) -> Language model (Llama-3.2)

- **Critical path**: Star sweep -> fit add-interact model -> compute elasticity -> brute-force optimize Equation 4

- **Design tradeoffs**: Star sweep is compute-efficient but may miss interactions; add-interact fits best but has many parameters requiring bagging

- **Failure signatures**: Extrapolation breakdown on LVB/NQA tasks (>5% error); discrete frontier jaggedness; task heterogeneity requiring per-task optimization

- **First 3 experiments**:
  1. Replicate star sweep for single factor (x_T) with 3 data sizes to verify power-law fit
  2. Fit add-interact model on star sweep data only, evaluate on isoFLOP held-out points
  3. For fixed compute (10 TFLOPs), compute x*(10T; n) for n ∈ {1M, 5M, 10M} and verify elasticity directions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inference compute-optimal frontier shift when the vision model size is jointly scaled with the language model and video representation factors?
- Basis in paper: Components Not Accounted For - we did not explore scaling the size of the vision model
- Why unresolved: Experimental design fixed the vision encoder to isolate other factors
- What evidence would resolve it: Training sweeps varying vision model parameters alongside x_N, x_T, x_V

### Open Question 2
- Question: Does the identified parametric scaling law accurately generalize to inference compute budgets 10x larger than tested?
- Basis in paper: Uncertain how well predicted frontier generalizes to significantly higher inference compute
- Why unresolved: Computational constraints limited sweeps to specific FLOP range
- What evidence would resolve it: Extending training sweeps to include 10x current compute budgets

### Open Question 3
- Question: Is the shift toward visual representations caused by higher complexity of learning visual details?
- Basis in paper: Hypothesis that detailed visual representations are more complex to learn and require more data
- Why unresolved: Paper observes shift but doesn't isolate underlying mechanism
- What evidence would resolve it: Controlled experiments analyzing convergence rates across data regimes

## Limitations

- Parametric model extrapolation accuracy varies significantly across tasks (LVB/NQA show >5% error)
- Discrete optimization creates jagged frontiers that may not reflect true compute-optimal tradeoffs
- Additive error decomposition assumption may break down for tasks with multiplicative interactions

## Confidence

- **High confidence**: Joint scaling necessity - strongly supported by isoFLOP experimental data
- **Medium confidence**: Data-size elasticity effects - supported by fitted model but extrapolation uncertainty exists
- **Medium confidence**: Parametric model form - strong in-distribution fit but moderate extrapolation errors

## Next Checks

1. Replicate star sweep for single factor (x_T) with 3 data sizes to verify power-law relationships are recoverable

2. Fit add-interact model using only star sweep data, then evaluate prediction accuracy on held-out isoFLOP points

3. For fixed compute (10 TFLOPs), compute x*(10T; n) for multiple data sizes (n ∈ {1M, 5M, 10M}) and verify theory-predicted elasticity directions hold