---
ver: rpa2
title: Near-Optimal Sample Complexity Bounds for Constrained Average-Reward MDPs
arxiv_id: '2509.16586'
source_url: https://arxiv.org/abs/2509.16586
tags:
- policy
- constraint
- algorithm
- reward
- optimal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work establishes the first near-optimal sample complexity
  bounds for learning in constrained average-reward Markov decision processes (CAMDPs)
  under a generative model. The authors propose a model-based algorithm that achieves
  near-optimal performance in both relaxed feasibility (allowing small constraint
  violations) and strict feasibility (enforcing exact constraint satisfaction) settings.
---

# Near-Optimal Sample Complexity Bounds for Constrained Average-Reward MDPs

## Quick Facts
- arXiv ID: 2509.16586
- Source URL: https://arxiv.org/abs/2509.16586
- Reference count: 40
- First near-optimal sample complexity bounds for constrained average-reward MDPs under generative model

## Executive Summary
This paper establishes the first near-optimal sample complexity bounds for learning in constrained average-reward Markov decision processes (CAMDPs) under a generative model. The authors propose a model-based algorithm that achieves near-optimal performance in both relaxed feasibility (allowing small constraint violations) and strict feasibility (enforcing exact constraint satisfaction) settings. The algorithm uses empirical transition estimation combined with a primal-dual optimization framework, achieving sample complexities that depend polynomially on the problem parameters including bias span, transient time, and the Slater constant.

## Method Summary
The method employs a model-based approach where for each state-action pair, the algorithm collects independent samples from the true transition kernel to form an empirical transition matrix. This empirical model is then used within a primal-dual optimization framework that alternates between solving unconstrained average-reward MDPs (primal updates) and performing projected gradient descent on the Lagrange multiplier (dual updates). The algorithm operates in two settings: relaxed feasibility allows small constraint violations, while strict feasibility enforces exact constraint satisfaction at the cost of additional sample complexity proportional to the inverse square of the Slater constant.

## Key Results
- Relaxed feasibility sample complexity: $\tilde{O}\left(\frac{S A (B+H)}{ \epsilon^2}\right)$
- Strict feasibility sample complexity: $\tilde{O}\left(\frac{S A (B+H)}{\epsilon^2 \zeta^2}\right)$
- Matching lower bounds of $\tilde{\Omega}\left(\frac{S A (B+H)}{\epsilon^2 \zeta^2}\right)$ for strict feasibility
- First minimax-optimal bounds for CAMDPs, closing the sample complexity gap

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Estimating the transition model from a generative model and planning on the empirical MDP achieves near-optimal sample complexity.
- Mechanism: For each state-action pair, collect N independent samples from the true transition kernel P(·|s,a). Form empirical transition matrix $\hat{P}(s'|s,a) = \frac{N(s'|s,a)}{N}$. When N scales as $\tilde{O}(\frac{SA(B+H)}{\epsilon^2})$, the empirical value functions concentrate around true values, enabling planning on $\hat{P}$ instead of P.
- Core assumption: The generative model provides i.i.d. samples from the true transition kernel for any queried (s,a). The bounded transient time assumption (parameter B) holds uniformly across policies.
- Evidence anchors:
  - [abstract] "propose a model-based algorithm that operates under two settings... achieves sample complexities of $\tilde{O}\left(\frac{SA(B+H)}{\epsilon^2}\right)$"
  - [section 3] "For each (s, a) pair, we collect N independent samples from P(·|s, a) and form an empirical transition matrix"
  - [corpus] Similar model-based approaches appear in "Sample Complexity Bounds for Linear Constrained MDPs" and Vaswani et al. (2022) for discounted CMDPs

### Mechanism 2
- Claim: A primal-dual formulation with projected gradient descent converts the constrained problem into iterative unconstrained MDP solves.
- Mechanism: Reformulate constrained optimization as saddle-point problem $\max_\pi \min_{\lambda \geq 0} [\rho^\pi_r(s) + \lambda(\rho^\pi_c(s) - b')]$. Alternate between: (1) primal update—solve unconstrained AMDP with reward $\tilde{r} + \lambda_t \tilde{c}$; (2) dual update—gradient descent on $\lambda$ with projection to [0, U]. The mixture policy $\hat{\pi} = \frac{1}{T}\sum_{t=0}^{T-1}\hat{\pi}_t$ converges to feasibility.
- Core assumption: Strong duality holds for the CAMDP; the Slater constant ζ > 0 ensures bounded dual variables; U bounds the optimal Lagrange multiplier.
- Evidence anchors:
  - [abstract] "model-based algorithm that operates under two settings: (i) relaxed feasibility... (ii) strict feasibility"
  - [section 3] "The primal update at iteration t is given as: $\hat{\pi}_t = \arg\max[\rho^\pi_{\tilde{r}} + \lambda_t \rho^\pi_{\tilde{c}}]$"
  - [corpus] "Primal-Dual Sample Complexity Bounds for Constrained MDPs" and "Global Convergence for Average Reward Constrained MDPs" confirm primal-dual approaches for constrained settings

### Mechanism 3
- Claim: Discounting with γ = 1 - ε/(4(B+H)) and reward perturbation enables reduction from average-reward to discounted analysis.
- Mechanism: Average-reward MDPs lack natural contraction properties. By setting discount factor γ ≈ 1 and adding small uniform perturbation Z(s,a) ~ Unif(0, ω) to rewards, the algorithm connects average-reward optimality to discounted value concentration. The bias span H and transient time B control how close γ must be to 1.
- Core assumption: The bias span H and transient time B are known or estimable a priori; the perturbation magnitude ω < 1 preserves near-optimality.
- Evidence anchors:
  - [section 3] "Set discount factor γ = 1 - ε_opt/(4(B+H))"
  - [section 3] "Perturb the rewards to form r_p(s,a) = r(s,a) + Z(s,a) where Z(s,a) ~ Unif(0, ω)"
  - [corpus] Average-reward Q-learning paper mentions similar analytical challenges; Zurek & Chen (2024) introduced span-based analysis for unconstrained AMDPs

## Foundational Learning

- Concept: Bias function and span bound in average-reward MDPs
  - Why needed here: The bias function $h^\pi_r(s)$ captures cumulative deviation from average reward; its span H determines long-term planning difficulty. Sample complexity depends on H rather than discount-dependent horizon.
  - Quick check question: Can you explain why average-reward sample complexity depends on bias span H rather than the mixing time or diameter alone?

- Concept: Slater condition and feasibility margin
  - Why needed here: The Slater constant ζ = max_π ρ^\pi_c(s) - b measures slack in the constraint. Strict feasibility requires $O(1/\zeta^2)$ more samples because smaller feasible regions amplify estimation errors.
  - Quick check question: Why does the gap between relaxed and strict feasibility depend on ζ² rather than ζ?

- Concept: Primal-dual optimization with projected gradient descent
  - Why needed here: Constrained optimization requires balancing reward maximization against constraint satisfaction. The Lagrange multiplier λ encodes this tradeoff; gradient descent on λ with projection keeps dual variables bounded.
  - Quick check question: What happens to convergence if the dual variable bound U is set smaller than |λ*|?

## Architecture Onboarding

- Component map: Sample Collector → Empirical Transition Matrix (P̂) → Perturbed Reward Construction (r_p, c_p) → Primal-Dual Optimizer (T iterations) → Mixture Policy Output (π̂)

- Critical path: Sample complexity is dominated by empirical model estimation. The primal-dual iterations contribute $O(1/\epsilon^4)$ for relaxed, $O(1/\epsilon^2 \zeta^4)$ for strict, but this is absorbed in the transition estimation term.

- Design tradeoffs:
  - Strict vs. relaxed feasibility: Strict requires tightening constraint RHS to b' > b, increasing sample cost by $O(1/\zeta^2)$
  - Known vs. unknown ζ: Algorithm requires ζ for parameter tuning; can use conservative estimate at sample cost
  - Black-box solver choice: Any AMDP planner works; policy iteration suggested but value iteration also viable

- Failure signatures:
  - Constraint violation > ε in relaxed setting → insufficient samples or underestimated H/B
  - Zero constraint violation but reward suboptimality > ε in strict setting → ζ overestimated, b' too tight
  - Dual variable diverges → Slater condition violated or U too small

- First 3 experiments:
  1. Validate on small tabular CAMDP with known H, B, ζ. Verify relaxed feasibility achieves $\tilde{O}(SA(B+H)/\epsilon^2)$ scaling by varying ε.
  2. Compare strict vs. relaxed on instance with small ζ (tight constraint). Confirm strict requires ~1/ζ² more samples.
  3. Stress test on weakly communicating MDP with varying bias span H. Verify sample complexity tracks H rather than diameter.

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis assumes exact knowledge of problem parameters H, B, and ζ which may not be available in practice
- Model-based approach requires $O(SA)$ samples per state-action pair, which can be prohibitive for large state spaces
- The strict feasibility regime's $1/\zeta^2$ penalty may be severe for problems with tight constraints

## Confidence
- Sample complexity bounds (relaxed): High - Standard concentration arguments for generative models
- Sample complexity bounds (strict): Medium - $1/\zeta^2$ penalty is theoretically sound but practically impactful
- Primal-dual algorithm correctness: Medium - Structure is standard but implementation details not fully specified

## Next Checks
1. **Parameter estimation impact**: Evaluate how estimation errors in H, B, and ζ affect the actual sample complexity in practice. Test whether conservative overestimation of these parameters leads to unnecessary sample waste.

2. **Scalability analysis**: Compare the $O(SA)$ per-state-action sampling requirement against model-free approaches on problems with large state spaces. Quantify the sample complexity trade-off as state space size grows.

3. **Constraint structure dependence**: Test whether the sample complexity bounds hold for CAMDPs with disconnected feasible regions or non-convex constraint sets. The current analysis assumes standard Slater conditions without examining pathological constraint geometries.