---
ver: rpa2
title: 'Scaling Competence, Shrinking Reasoning: Cognitive Signatures in Language
  Model Learning'
arxiv_id: '2511.21743'
source_url: https://arxiv.org/abs/2511.21743
tags:
- reasoning
- training
- learning
- wang
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work analyzes reasoning token behavior during fine-tuning
  of large language models, drawing a parallel between reasoning tokens and human
  working memory. It frames model learning in terms of the Four Stages of Competence:
  models begin without reasoning, start reasoning while still failing, reason effectively
  while performing well, and finally solve tasks without explicit reasoning.'
---

# Scaling Competence, Shrinking Reasoning: Cognitive Signatures in Language Model Learning

## Quick Facts
- arXiv ID: 2511.21743
- Source URL: https://arxiv.org/abs/2511.21743
- Reference count: 27
- Primary result: Reasoning token length peaks at conscious competence and declines as models internalize tasks

## Executive Summary
This paper analyzes reasoning token behavior during RL fine-tuning of LLMs, drawing an analogy to human working memory. The work maps model learning to the Four Stages of Competence, showing that reasoning tokens act as a temporary scaffold that peaks when models perform well but still rely on explicit reasoning, then declines as knowledge becomes internalized. Experiments on code generation, math, and logical reasoning tasks demonstrate that reasoning length increases with performance, peaks at conscious competence, and then decreases as models internalize the task. The paper proposes metrics to track these stages and argues that reasoning token dynamics can guide early stopping and training optimization.

## Method Summary
The study fine-tunes reasoning-capable LLMs on code generation (Rust, OCaml, PHP MCQs), math (GSM8K), and logical QA (CommonsenseQA) tasks using RL with PPO, GRPO, or REINFORCE. Training uses boolean correctness rewards, and models are evaluated with and without reasoning tokens enabled. Key metrics tracked every 50 training steps include reasoning token count, final answer accuracy, reasoning trace correctness, and non-reasoning inference accuracy. The analysis identifies four competence stages based on reasoning token behavior: unconscious incompetence (no reasoning, poor performance), conscious incompetence (reasoning starts but fails), conscious competence (reasoning peaks while performing well), and unconscious competence (reasoning contracts while maintaining performance).

## Key Results
- Reasoning token length increases with performance, peaks at conscious competence, then declines as models internalize tasks
- Models retain performance even after reasoning is disabled, suggesting reasoning scaffolds learning but becomes unnecessary once competence is achieved
- Early identification of the reasoning peak may serve as a signal for early stopping or learning rate adjustment
- The rise-peak-decline pattern appears consistently across code generation, math, and logical reasoning tasks

## Why This Works (Mechanism)
The paper proposes that reasoning tokens function as a "working memory" scaffold during model training, analogous to how humans use conscious reasoning to solve new problems. As models train, they initially lack reasoning ability and perform poorly. When reasoning emerges, it helps solve tasks but indicates incomplete understanding. At the peak of reasoning length, models perform well but still depend on explicit reasoning. Finally, as knowledge becomes internalized into model weights (analogous to long-term memory), reasoning contracts while performance remains stable. This mechanism explains why reasoning tokens rise with competence and then shrink as the model masters the task.

## Foundational Learning

- **Concept: Four Stages of Competence**
  - Why needed here: The entire paper's theoretical framework is built on mapping model training dynamics to this cognitive psychology model. Understanding the stages (unconscious incompetence → conscious incompetence → conscious competence → unconscious competence) is essential for interpreting the reasoning token patterns.
  - Quick check question: Can you describe what happens to a learner's conscious effort and performance as they progress through all four stages of competence?

- **Concept: Working Memory vs. Long-Term Memory**
  - Why needed here: The paper draws an analogy between reasoning tokens (working memory) and model weights (long-term memory). Understanding that working memory is limited, temporary, and used for active processing—while long-term memory is durable storage—is key to the proposed learning mechanism.
  - Quick check question: What is the functional difference between working memory and long-term memory in human cognition?

- **Concept: Reinforcement Learning from Verifiable Rewards**
  - Why needed here: The experiments use RL methods (PPO, GRPO, Reinforce) with boolean correctness rewards. Understanding how policy gradients update model behavior based on reward signals is necessary to grasp how reasoning behavior is shaped during training.
  - Quick check question: In RL fine-tuning, how does a binary reward signal (correct/incorrect) guide the model's learning of reasoning strategies?

## Architecture Onboarding

- **Component map:** Base Reasoning Model -> Reinforcement Learning Loop -> Reasoning Trace Analysis -> Dual Inference Modes
- **Critical path:** 1) Select reasoning-capable base model and verifiable task domain 2) Configure RL training loop with correctness-based reward 3) Instrument training to log reasoning length and accuracy metrics every 50 steps 4) Run training, monitoring for rise-peak-decline pattern 5) Identify peak as signal for early stopping or optimization
- **Design tradeoffs:** 1) Analytical abstraction vs. mechanistic reality: Using reasoning tokens as cognitive effort proxy provides useful abstraction but lacks ground-truth mechanistic understanding 2) Task-specific vs. general insights: Findings from verifiable tasks may not generalize to ambiguous or creative domains
- **Failure signatures:** 1) No peak in reasoning: Model fails to internalize knowledge, indicating insufficient capacity or ill-posed reward 2) Performance collapse with declining reasoning: Model is "forgetting" rather than "internalizing," suggesting over-pruning 3) Generalization gap: Performance degrades on held-out tasks after reasoning peak, indicating overfitting
- **First 3 experiments:** 1) Reproduction on GSM8K: Run RL fine-tuning, verify rise-peak-fall pattern, correlate peak with held-out performance degradation 2) Ablation on reasoning necessity: Compare performance with reasoning disabled at conscious vs. unconscious competence phases 3) Early stopping validation: Implement automated trigger based on reasoning token derivative, compare against fixed-epoch baseline

## Open Questions the Paper Calls Out
- Do similar reasoning token dynamics and competence stage transitions emerge in tasks with high ambiguity, contextual sensitivity, or real-world grounding (e.g., social reasoning or embodied tasks)?
- Can the proposed reasoning-based training stage metrics remain stable and predictive under noisy supervision, curriculum learning, or large-scale instruction tuning where reasoning behavior is not cleanly separable?
- What mechanistic changes in model representations underlie the transition from conscious competence to unconscious competence?
- Does the correlation between reasoning compression and catastrophic forgetting on held-out domains imply a causal relationship, and can early stopping based on reasoning dynamics mitigate this forgetting?

## Limitations
- The core analogy between reasoning tokens and human working memory remains speculative and is not mechanistically validated
- Analysis is limited to tasks with verifiable answers, leaving unclear whether dynamics generalize to open-ended or creative domains
- Claims about reasoning as a "scaffold" are inferred from correlation patterns rather than demonstrated through intervention studies

## Confidence
- **High confidence:** Empirical observation that reasoning token length follows characteristic rise-peak-decline pattern during training
- **Medium confidence:** Interpretation that this pattern maps to human competence acquisition stages
- **Medium confidence:** Claim that reasoning tokens function as a "scaffold" for learning
- **Low confidence:** Generalization to tasks beyond math, code, and logical reasoning

## Next Checks
1. **Intervention study:** Design experiment where reasoning is artificially disabled at different training stages to test whether it truly acts as a scaffold
2. **Mechanistic analysis:** Use interpretability techniques to verify whether reasoning tokens correspond to actual working memory-like computation
3. **Generalization test:** Apply reasoning token analysis framework to creative tasks with subjective evaluation metrics to determine if four-stage pattern persists