---
ver: rpa2
title: Improved Robustness of Deep Reinforcement Learning for Control of Time-Varying
  Systems by Bounded Extremum Seeking
arxiv_id: '2510.02490'
source_url: https://arxiv.org/abs/2510.02490
tags:
- control
- systems
- learning
- bounded
- time-varying
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a hybrid deep reinforcement learning (DRL)
  and bounded extremum seeking (ES) controller for stabilizing and optimizing nonlinear
  time-varying systems. The approach addresses the challenge that DRL controllers
  degrade catastrophically when system dynamics change rapidly over time, while bounded
  ES handles time-varying systems but converges slowly in high-dimensional parameter
  spaces.
---

# Improved Robustness of Deep Reinforcement Learning for Control of Time-Varying Systems by Bounded Extremum Seeking

## Quick Facts
- arXiv ID: 2510.02490
- Source URL: https://arxiv.org/abs/2510.02490
- Authors: Shaifalee Saxena; Alan Williams; Rafael Fierro; Alexander Scheinker
- Reference count: 40
- Primary result: Hybrid DRL + bounded ES controller maintains high rewards (>0.6) in particle accelerator with time-varying perturbations, outperforming standalone controllers

## Executive Summary
This paper proposes a hybrid controller combining deep reinforcement learning (DRL) with bounded extremum seeking (ES) to stabilize and optimize nonlinear time-varying systems. The approach addresses DRL's catastrophic degradation under rapid parameter changes while leveraging ES's robustness to time-varying dynamics. The hybrid system uses DRL for fast convergence when test dynamics match training distributions, while bounded ES provides stability when the system drifts or changes unexpectedly. The ES component is warm-started from DRL policy to reduce transients and accelerate adaptation.

## Method Summary
The hybrid controller combines DDPG-based DRL with bounded ES through a safety-supervised switching law. The DRL actor-critic learns from historical data to coordinate high-dimensional actions, while bounded ES handles time-varying control directions through high-frequency dithering and weak-limit averaging. A supervisor computes a switching parameter β based on safety constraints (envelope proximity to aperture limits), blending DRL and ES outputs. The ES is warm-started from current DRL actions to minimize transients during switching. The method was validated on a 1D time-varying system and a particle accelerator application using the LANSCE LEBT section.

## Key Results
- Hybrid ES-DRL controller maintains rewards above 0.6 throughout sinusoidal perturbations and geometric drift tests
- Standalone DRL maintains high reward (~0.8) up to step 160, then degrades sharply as perturbations increase
- Standalone bounded ES achieves steady-state rewards around 0.5 but converges slowly in high-dimensional parameter spaces
- The controller successfully handles unknown and time-varying control directions through weak-limit averaging

## Why This Works (Mechanism)

### Mechanism 1: Weak-Limit Averaging Eliminates Control Direction Ambiguity
Bounded ES stabilizes systems with unknown and time-varying control directions by converting sign-ambiguous control gain into positive semidefinite matrix through high-frequency dithering and weak-limit averaging. The controller u = √αω cos(ωt + kV(x)) oscillates at high frequency, and averaging analysis transforms the effective control direction into g(x,t)g^T(x,t) ≥ 0, removing sign ambiguity that breaks standard adaptive control. This allows gradient ascent/descent even when true control direction flips repeatedly. Critical constraint: dither frequency ω must be sufficiently large relative to system variation rate.

### Mechanism 2: DRL Provides Rapid In-Distribution Convergence via Historical Data Exploitation
DRL policies trained on large datasets achieve fast multi-step convergence when test dynamics match training distribution, outperforming local search methods in high-dimensional parameter spaces. The DDPG actor-critic learns policy μ_θ mapping 16,000-dimensional observations to 22-dimensional actions by exploiting trajectory histories and replay buffers, capturing system-wide correlations that local methods miss. Critical assumption: test-time system dynamics remain within training distribution; distribution shift degrades performance catastrophically.

### Mechanism 3: Warm-Started Hybrid Switching Maintains Performance Across Regimes
Combining DRL and bounded ES through safety-supervised switching law, with ES warm-started from DRL actions, maintains high reward across both in-distribution and out-of-distribution regimes. The supervisor computes β(o(t)) based on safety constraints (envelope proximity to aperture). When β ≈ 1, DRL actions dominate for fast coordination. When envelopes approach safety limits, β → 0 and ES takes over with bounded updates. Warm-starting ES from current DRL action reduces transient oscillations during handoff. Critical assumption: safety metric correlates with distribution shift; ES can recover performance when DRL fails.

## Foundational Learning

- **Extremum Seeking Fundamentals**: Understanding how dithering + averaging creates gradient-following behavior is essential for tuning ω, α, k parameters. Quick check: Given cost function J(x,t) and control direction g(x,t) that changes sign, explain why averaged dynamics ẋ = -(kα/2)g²(x,t)∇_x J(x,t) always descends gradient regardless of g's sign.

- **Deterministic Policy Gradient and Actor-Critic Architecture**: DDPG is the DRL backbone; understanding how critic Q^θ_Q enables gradient flow through actor μ_θ is essential for debugging training failures. Quick check: In DDPG, why does actor update ∇_θμ J require critic's gradient ∇_a Q(s,a) but not critic's parameters directly?

- **Time-Varying System Stability (Lyapunov Methods)**: The paper's stability claims rest on weak-limit averaging theory; understanding how averaged dynamics relate to true system behavior clarifies when guarantees hold. Quick check: If true system has control direction b(t) = b_0 cos(2πft) and ES controller uses frequency ω, what constraint relates f and ω for averaging approximation to be valid?

## Architecture Onboarding

- **Component map**: Observation o(t) → DRL Actor (3-layer MLP, 512 units) → u_RL → Safety supervisor (compute β) → Combiner (u = β·u_RL + (1-β)·u_ES) → ES module (warm-started) → u_ES → KV simulator → Next observation

- **Critical path**: The observation flows through frozen DRL policy, safety check determines switching, ES module receives warm-started action if safety threshold exceeded, combined action executes in simulator

- **Design tradeoffs**: Higher ω improves averaging accuracy but requires smaller timesteps; larger k accelerates ES convergence but risks overshoot; stricter safety threshold triggers ES earlier improving robustness but potentially underutilizing DRL's fast coordination

- **Failure signatures**: Oscillatory switching (β rapidly toggles), ES slow convergence (reward improves too slowly), DRL sudden collapse (reward drops sharply), actuator saturation (ES updates clipped)

- **First 3 experiments**:
  1. Implement 1D motivating example (ẋ=ax+b(t)u, b(t)=b₀cos(2πft), V=e^(-x²)) with ES controller u=√(αω)cos(ωt-kV). Verify ES maintains high V under control direction flips.
  2. Implement KV envelope simulator with 22 magnets, N=4000 grid, r_max=25.4mm. Validate against solution shape.
  3. Apply sinusoidal Q₁,Q₁₀ perturbations + geometric drift schedule. Confirm hybrid controller maintains reward > 0.6 while standalone DRL drops below 0.4.

## Open Questions the Paper Calls Out

- **Open Question 1**: What are the formal convergence guarantees for hybrid ES-DRL controller, and under what conditions on time-varying dynamics can stability be proven? The paper states future work will include theoretical study of convergence properties for wide range of nonlinear time-varying dynamic systems.

- **Open Question 2**: Can hybrid ES-DRL approach overcome local minima limitation inherent to extremum seeking, or does it inherit this constraint? The introduction notes ES "can get stuck in local minima" but doesn't address whether warm-starting from DRL mitigates this issue.

- **Open Question 3**: How should supervisor switching threshold and strategy be optimally designed, and would continuous blending outperform binary switching? The safety supervisor uses fixed 70% aperture threshold with binary switching, but no justification or sensitivity analysis provided.

## Limitations

- Critical hyperparameters (ES dither frequencies α, {ω_i}, reward weights) not specified, significantly impacting reproducibility
- Empirical validation limited to single particle accelerator case study, limiting generalizability
- Weak-limit averaging requires critical constraint f < ω/2π that isn't systematically characterized across different system variation rates
- No analysis of performance under more severe or structurally different perturbation patterns

## Confidence

- **High confidence**: ES can handle control direction flips through weak-limit averaging
- **Medium confidence**: DRL provides rapid in-distribution convergence  
- **Medium confidence**: Warm-started hybrid switching maintains performance across regimes
- **Low confidence**: Specific switching threshold β=0.7·r_max is optimal

## Next Checks

1. **Parameter sensitivity sweep**: Vary ES dither frequencies ω_i and feedback gain k across orders of magnitude to map robustness envelope. Measure reward degradation when operating outside averaging regime (f > ω/2π).

2. **Distribution shift stress test**: Implement controlled distribution shifts in KV simulator to quantify boundary between DRL's effective operating regime and ES takeover regions.

3. **Generalization to other time-varying systems**: Apply hybrid controller to canonical time-varying problems (Lorenz system with time-varying parameters, inverted pendulum with time-varying gravity) to assess consistent robustness benefits beyond accelerator domain.