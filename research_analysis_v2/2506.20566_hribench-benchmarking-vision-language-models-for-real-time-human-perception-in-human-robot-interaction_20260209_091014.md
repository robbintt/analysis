---
ver: rpa2
title: 'HRIBench: Benchmarking Vision-Language Models for Real-Time Human Perception
  in Human-Robot Interaction'
arxiv_id: '2506.20566'
source_url: https://arxiv.org/abs/2506.20566
tags:
- human
- vlms
- hribench
- understanding
- interaction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces HRIBench, a comprehensive visual question-answering
  (VQA) benchmark designed to evaluate vision-language models (VLMs) for real-time
  human perception in human-robot interaction (HRI). The benchmark covers five key
  domains: non-verbal cue understanding, verbal instruction understanding, human-robot-object
  relationship understanding, social navigation, and person identification, totaling
  1,000 questions curated from real-world HRI environments and existing datasets.'
---

# HRIBench: Benchmarking Vision-Language Models for Real-Time Human Perception in Human-Robot Interaction

## Quick Facts
- arXiv ID: 2506.20566
- Source URL: https://arxiv.org/abs/2506.20566
- Authors: Zhonghao Shi; Enyu Zhao; Nathaniel Dennler; Jingzhen Wang; Xinyang Xu; Kaleen Shrestha; Mengxue Fu; Daniel Seita; Maja Matarić
- Reference count: 27
- Primary result: No evaluated VLM achieved both satisfactory accuracy (>50%) and real-time latency (<0.7s) for human-robot interaction tasks.

## Executive Summary
This paper introduces HRIBench, a comprehensive visual question-answering benchmark designed to evaluate vision-language models (VLMs) for real-time human perception in human-robot interaction (HRI). The benchmark covers five key domains: non-verbal cue understanding, verbal instruction understanding, human-robot-object relationship understanding, social navigation, and person identification, totaling 1,000 questions curated from real-world HRI environments and existing datasets. The authors conducted extensive evaluations of 11 state-of-the-art VLMs, including both closed-source and open-source models. Results showed that despite strong zero-shot generalization capabilities, current VLMs struggle with fundamental perceptual challenges in HRI, including understanding fine-grained multimodal cues, resolving ambiguous language-visual instructions, and performing real-world spatial reasoning. None of the evaluated models demonstrated satisfactory performance-latency trade-offs suitable for real-time deployment, with latency exceeding the critical 0.7-second threshold for natural human interaction. The findings highlight the need for future research to develop smaller, low-latency VLMs with improved human perception capabilities for effective HRI applications.

## Method Summary
The authors created HRIBench by curating 1,000 VQA questions across five HRI domains: non-verbal cues, verbal instructions, human-robot-object relationships, social navigation, and person identification. They evaluated 11 VLMs (6 closed-source: o3, GPT-4o, GPT-4o-mini, Gemini-2.5-pro, Gemini-1.5-pro, Gemini-1.5-flash; 5 open-source: Llama3.2-10.60B, InternVL2.5-8B/4B/2B/1B) on accuracy and latency metrics. Open-source models ran on 2x RTX 3090 GPUs with single-run evaluation, while closed-source models used API inference. The primary metric was average accuracy across domains, with latency measured per question and compared against the 0.7-second real-time HRI threshold. Human and random baselines were established for comparison.

## Key Results
- No VLM achieved both satisfactory accuracy (>50%) and real-time latency (<0.7s) for HRI deployment
- Top-performing models (o3, Gemini-2.5-pro) used test-time computation for reasoning but incurred 10-40× latency penalty
- Smaller models (e.g., InternVL2.5-1B) showed severe accuracy degradation despite lower latency
- VLMs struggled with fine-grained multimodal cues like eye gaze direction and left-right spatial reasoning
- OpenAI models refused to answer person identification questions due to policy constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Test-time computation enables stronger reasoning on HRI tasks but creates a fundamental latency penalty unsuitable for real-time deployment.
- Mechanism: Models like o3 and Gemini-2.5-pro allocate additional compute during inference to perform multi-step reasoning, improving accuracy on spatial and instruction-understanding tasks at the cost of response time.
- Core assumption: The performance gain is attributable to test-time computation rather than other architectural differences.
- Evidence anchors:
  - [abstract] "none of the models within our experiments demonstrated a satisfactory performance-latency trade-off suitable for real-time deployment"
  - [Page 5] "Both o3 and Gemini-2.5-pro leverage test-time computation, enabling them to significantly outperform other closed- and open-source models that lack this capability. However, this comes at the cost of high latency"
  - [corpus] Limited direct evidence in corpus for test-time computation in HRI; related work focuses on VLA architectures
- Break condition: If deployment tolerates non-real-time planning, or if test-time compute efficiency improves dramatically.

### Mechanism 2
- Claim: General-purpose VLM training data lacks sufficient coverage of fine-grained HRI cues, causing poor transfer to human perception tasks.
- Mechanism: Internet-scale pretraining emphasizes common objects and scenes but underrepresents subtle non-verbal cues (eye gaze, pointing direction, perspective-taking) critical for HRI.
- Core assumption: The performance gap stems from training data distribution, not fundamental architectural constraints.
- Evidence anchors:
  - [Page 6] "while the models are already proficient at recognizing gestures alone, they struggled to understand fine-grained behaviors such as eye gaze direction"
  - [Page 6-7] "VLMs appeared to be incapable of reasoning from the human's field of vision and often failed to distinguish between left and right directions"
  - [corpus] IntentionVLA notes "current SOTA VLAs are primarily pretrained on multimodal tasks with limited relevance to embodied scenarios"
- Break condition: Targeted fine-tuning or synthetic HRI data generation could close the distribution gap.

### Mechanism 3
- Claim: Reducing VLM parameter count to lower latency disproportionately degrades HRI-specific perceptual capabilities.
- Mechanism: Smaller models lack capacity to encode fine-grained visual-spatial features and multi-modal reasoning patterns needed for HRI tasks.
- Core assumption: Architecture design, not just scale, shapes the performance-latency trade-off curve.
- Evidence anchors:
  - [Table 1] InternVL2.5-1B achieves only 0.10 average accuracy vs. 0.31 for InternVL2.5-8B, despite 5× lower latency
  - [Page 7] "smaller model sizes resulted in a lack of fundamental perceptual capabilities needed for reliable deployment"
  - [corpus] FAM-HRI and related work suggest multi-modal integration efficiency matters but corpus evidence on scaling laws is limited
- Break condition: Architectural innovations (efficient attention, specialized encoders) may preserve capabilities at smaller scales.

## Foundational Learning

- **Concept: Vision-Language Model (VLM) inference pipeline**
  - Why needed here: Understanding how VLMs jointly process images and text is essential for diagnosing failures on HRI tasks.
  - Quick check question: How does a VLM encode an image and align it with a text prompt to produce an answer?

- **Concept: Test-time computation / inference-time scaling**
  - Why needed here: o3 and Gemini-2.5-pro achieve top accuracy through this mechanism; understanding it is critical for interpreting results.
  - Quick check question: What is test-time computation, and how does it differ from standard single-pass inference?

- **Concept: HRI latency threshold (0.7 seconds)**
  - Why needed here: This is the key criterion for real-time deployment; all evaluated models exceed it.
  - Quick check question: Why is 0.7 seconds considered the critical threshold for natural human-robot interaction?

## Architecture Onboarding

- **Component map**: HRIBench dataset (5 domains × 200 questions = 1,000 VQA questions) -> Image frame(s) + text prompt -> VLM -> answer selection -> Accuracy and latency measurement

- **Critical path**: 1. Define HRI perceptual task -> 2. Curate domain-specific VQA questions -> 3. Run VLM inference -> 4. Measure accuracy and latency -> 5. Compare against human baseline and 0.7s threshold

- **Design tradeoffs**:
  - Accuracy vs. latency: No model achieves >50% accuracy under 0.7s latency
  - Closed-source vs. open-source: Higher accuracy with API dependency vs. lower accuracy with local control
  - Test-time compute: Better reasoning vs. 10-40× latency increase (o3: 41.6s, GPT-4o: 13.8s)

- **Failure signatures**:
  - Eye gaze misinterpretation despite gesture recognition
  - Left/right confusion in spatial reasoning from human perspective
  - Rigid instruction following without contextual correction
  - OpenAI model refusals on person identification (policy constraints)

- **First 3 experiments**:
  1. Reproduce InternVL2.5-8B baseline: Verify reported ~1.4s latency and 31% average accuracy on HRIBench using RTX 3090s.
  2. Latency breakdown by domain: Profile each domain to identify which perceptual task causes highest latency (hypothesis: human-robot-object relationships).
  3. Targeted fine-tuning test: Train smallest viable model on synthetic non-verbal cue data only; measure accuracy-latency shift for that single domain.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can smaller, low-latency VLMs be developed that achieve accuracy comparable to current large models on human perception tasks for HRI?
- Basis in paper: [explicit] The conclusion states: "These results highlight the need for future research to improve understanding of fine-grained behavioral cues and to develop smaller, low-latency VLMs capable of supporting real-time human perception in HRI."
- Why unresolved: No current model achieved the 0.7-second latency threshold while maintaining satisfactory accuracy. The best trade-off (Gemini-1.5-pro) still exceeds this limit at 2.66 seconds.
- What evidence would resolve it: A VLM achieving >80% accuracy on HRIBench with <0.7 second latency per question.

### Open Question 2
- Question: How can VLMs be improved to reliably understand fine-grained multimodal cues such as eye gaze direction in HRI contexts?
- Basis in paper: [explicit] The discussion notes that "while the models are already proficient at recognizing gestures alone, they struggled to understand fine-grained behaviors such as eye gaze direction, which is a crucial non-verbal cue for successful HRI."
- Why unresolved: Current VLMs showed inconsistent performance correlating hand gestures with gaze direction, a capability fundamental to multi-party HRI.
- What evidence would resolve it: Consistently high accuracy on the non-verbal cue domain of HRIBench, particularly on questions requiring gaze inference.

### Open Question 3
- Question: What architectural or training improvements would enable VLMs to better perform real-world spatial reasoning and physical understanding for HRI?
- Basis in paper: [explicit] The paper states: "This limitation may stem from a fundamental lack of physical world understanding also observed in other recent VLM benchmark studies."
- Why unresolved: Even top-performing models (o3, Gemini-2.5-pro) "frequently failed to comprehend the relative spatial relationships" needed for tasks like social navigation.
- What evidence would resolve it: Significant improvement on the human-robot-object relationship and social navigation domains, demonstrating left-right discrimination and perspective-taking.

## Limitations
- Single-run evaluation design introduces measurement noise, particularly for latency on closed-source APIs where network variability is uncontrolled
- Person identification domain uses YouTube videos of varying quality, which may not generalize to diverse real-world HRI scenarios
- Study assumes static model pool; rapid advances in VLM architectures could quickly shift the performance-latency frontier

## Confidence
- **High confidence**: The core finding that current VLMs cannot achieve satisfactory performance-latency trade-offs for real-time HRI deployment
- **Medium confidence**: The diagnosis that test-time computation drives the latency-accuracy tradeoff for top-performing models
- **Medium confidence**: The claim that training data distribution limits HRI perceptual capabilities
- **Low confidence**: The scaling law inference that smaller models disproportionately lose HRI capabilities

## Next Checks
1. **Multi-run latency validation**: Repeat all latency measurements for each model across 10 runs to establish confidence intervals and identify whether observed performance differences are statistically significant.
2. **Targeted ablation study**: Create a minimal HRI-focused pretraining corpus and fine-tune a base VLM to test whether the reported HRI capability gaps stem primarily from data distribution or architectural limitations.
3. **Real-world deployment pilot**: Deploy the highest-performing VLM (considering latency constraints) in a controlled HRI scenario with human participants to validate whether benchmark accuracy translates to effective interaction quality.