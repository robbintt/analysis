---
ver: rpa2
title: 'CodeNER: Code Prompting for Named Entity Recognition'
arxiv_id: '2507.20423'
source_url: https://arxiv.org/abs/2507.20423
tags:
- codener
- vanilla
- entity
- label
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CodeNER, a code-based prompting method designed
  to enhance the named entity recognition (NER) capabilities of large language models
  (LLMs). By embedding structured code prompts that explicitly integrate the BIO output
  schema and input sentence information, CodeNER addresses the inherent gap between
  the text-in-text-out schema of LLMs and the text-in-span-out nature of NER tasks.
---

# CodeNER: Code Prompting for Named Entity Recognition

## Quick Facts
- arXiv ID: 2507.20423
- Source URL: https://arxiv.org/abs/2507.20423
- Reference count: 24
- Primary result: Code-based prompting improves NER F1 scores by 2.41 points (closed models) and 3.07 points (open models) over text-based prompting

## Executive Summary
This paper introduces CodeNER, a novel code-based prompting method that enhances named entity recognition (NER) performance in large language models (LLMs) by embedding structured programming code prompts. The method addresses the fundamental mismatch between LLMs' text-in-text-out schema and NER's text-in-span-out requirement by explicitly incorporating BIO output schema and input sentence information within Python code structures. CodeNER leverages LLMs' inherent ability to comprehend programming language structures to better capture entity boundaries. Experimental results across ten benchmark datasets in English, Arabic, Finnish, Danish, and German demonstrate consistent improvements over traditional text-based prompting approaches, with an average F1 score improvement of 2.41 points on closed models (GPT-4) and 3.07 points on open models (Llama-3-8B and Phi-3-mini-128k-instruct).

## Method Summary
CodeNER works by embedding NER tasks within structured programming code prompts that explicitly define both the input tokens and the BIO output schema. The method constructs prompts containing a tokenized sentence list, a dictionary mapping numeric IDs to entity labels with descriptive comments, and a function with a for-loop that iterates through tokens to assign BIO tags. This structure forces sequential token-by-token processing, preventing overlapping labels and ensuring each token receives exactly one tag. The approach can be combined with chain-of-thought prompting for additional gains. The method was tested across ten NER benchmarks using both closed models (GPT-4 via API) and open models (Llama-3-8B and Phi-3-mini-128k-instruct locally), with outputs parsed back from generated code to standard BIO format for evaluation.

## Key Results
- CodeNER achieves an average F1 score improvement of 2.41 points on closed models (GPT-4) and 3.07 points on open models (Llama-3-8B and Phi-3-mini-128k-instruct)
- The method outperforms text-based prompting across all ten benchmark datasets in English, Arabic, Finnish, Danish, and German
- Combining CodeNER with chain-of-thought prompting yields additional performance gains
- Python and C++ prompts perform comparably, while Java prompts fail due to limited pre-training knowledge

## Why This Works (Mechanism)

### Mechanism 1: Sequential Processing Constraint via For-Loop Structure
Embedding a for-loop in the code prompt enforces token-by-token sequential processing, preventing overlapping labels. The Python for-loop structure constrains the model to iterate over each token exactly once, assigning exactly one BIO tag per token. This eliminates ambiguity about which tokens to label when words appear multiple times or when entities span complex sequences. Core assumption: LLMs pre-trained on code will respect the sequential semantics of for-loop control flow when generating outputs. Evidence: CodeNER outperforms Vanilla on repeated tokens (Borrower appears twice, CodeNER labels both occurrences), and achieves higher B/O tag accuracy than context-only approaches.

### Mechanism 2: Explicit BIO Schema-Context Integration
Structuring both the label schema and input tokens as typed variables with explicit mappings clarifies the input-output relationship for NER. The ner_tag_labels dictionary with numeric IDs and descriptive comments, combined with the tokenized sentence list, provides a unified framework where the model sees both the classification schema and the data in a single, parseable structure. Core assumption: The model can leverage both syntactic structure (variable types, function definitions) and semantic content (comments, variable names) simultaneously. Evidence: CodeNER outperforms GoLLIE and GNER on B/O tag accuracy, suggesting the combined schema-context structure helps identify entity boundaries better than context-only or schema-only approaches.

### Mechanism 3: Programming Language Inductive Bias Transfer
LLMs' pre-training on code enables better structured reasoning for tasks requiring explicit step-by-step processing like NER. Code pre-training exposes models to long-range dependencies (variable scope, function structure), explicit type systems, and control flow—all of which transfer to NER's requirement for tracking entity boundaries across sequences while maintaining label consistency. Core assumption: Structured reasoning patterns learned from code transfer to natural language tasks that require similar constraint satisfaction. Evidence: Python and C++ prompts yield comparable results on CoNLL03 (69.48 vs 68.43 F1) but differ on FIN (41.13 vs 44.44), suggesting code knowledge transfers but varies by programming language and dataset.

## Foundational Learning

- **Concept: BIO Tagging Scheme (Beginning-Inside-Outside)**
  - Why needed: CodeNER explicitly embeds the BIO schema into code prompts. You must understand that B- marks the first token of an entity, I- marks continuation tokens, and O marks non-entity tokens to interpret outputs and debug failures.
  - Quick check: For the sentence "Apple Inc. is in Cupertino," what are the BIO tags if "Apple Inc." is an organization and "Cupertino" is a location? (Answer: B-ORG I-ORG O O B-LOC)

- **Concept: In-Context Learning (Zero-shot/Few-shot)**
  - Why needed: CodeNER operates without parameter updates. Understanding that the model performs tasks solely from prompt context—leveraging its pre-trained knowledge—is essential for debugging why certain prompts work and others fail.
  - Quick check: How does in-context learning differ from fine-tuning in terms of model parameter updates? (Answer: In-context learning doesn't update parameters; fine-tuning does)

- **Concept: Text-in-Span-Out vs Text-in-Text-Out Mismatch**
  - Why needed: The paper frames its contribution as bridging the gap between LLMs' natural text-in-text-out interface and NER's requirement for structured span outputs with explicit boundaries.
  - Quick check: Why might asking an LLM to "list all entities" in natural language produce worse boundary detection than asking it to fill in a structured code template? (Answer: Natural language prompts lack explicit boundary markers and may produce ambiguous or overlapping labels)

## Architecture Onboarding

- **Component Map:**
  - sentence -> ner_tag_labels dictionary -> find_ner_tags() function with for-loop -> ner_tags_dict_tags output list

- **Critical Path:**
  1. Format input sentence as tokenized Python list
  2. Define label schema in dictionary with numeric IDs and descriptive comments
  3. Write function signature with for-loop over sentence tokens
  4. Include print statement to trigger output generation
  5. Parse model's code output back to BIO format for evaluation

- **Design Tradeoffs:**
  - Python vs C++ vs Java: Python most reliable; C++ sometimes better for specific entity types (FIN: 44.44 vs 41.13); Java near-zero due to limited pre-training
  - With vs without label descriptions: Similar performance on average, suggesting structure matters more than verbose descriptions
  - CodeNER vs Vanilla: CodeNER excels at token-level precision and repeated tokens; Vanilla may outperform on long-span labels with many function words

- **Failure Signatures:**
  - Duplicate labels on same token (Vanilla issue, resolved by CodeNER's sequential constraint)
  - Single label for repeated tokens (Vanilla labels only first occurrence; CodeNER labels each)
  - Special characters attached to words mishandled (Vanilla struggles; CodeNER processes them as separate tokens via tokenization)
  - Performance drop on sentence-level spans dominated by function words (both struggle, Vanilla may be better)

- **First 3 Experiments:**
  1. Baseline comparison: Run CodeNER vs Vanilla on CoNLL03 with GPT-4, measuring per-label F1 (PER/LOC/ORG/MISC) to identify where code structure helps most
  2. Ablation on code elements: Test variants removing (a) for-loop, (b) label descriptions, (c) function signature on FIN dataset to isolate driving factors
  3. Cross-language code prompting: Implement identical logic in Python, C++, and Java prompts; test on Arabic A with Phi-3 to measure sensitivity to programming language syntax and model code knowledge

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can integrating auxiliary tasks focused on identifying function words or syntactic structures improve CodeNER's performance on long text spans?
- Basis: The authors state in the Conclusion and Limitations: "In the future, we plan to pre-train our CodeNER by incorporating a related auxiliary task that focuses on identifying function words or syntactic structures across various datasets."
- Why unresolved: The current CodeNER method reinforces token-level information, which hinders its ability to label long spans containing function words (e.g., "Plot" labels in the MIT-Movie dataset) compared to text-based prompting
- Evidence needed: Experimental results showing improved F1 scores on long-span entity datasets (like MIT-Movie) after pre-training CodeNER with the proposed auxiliary tasks compared to the current zero-shot implementation

### Open Question 2
- Question: To what extent does the performance of CodeNER depend on the specific programming language pre-training of the underlying LLM?
- Basis: The paper notes that Java prompts resulted in near-zero performance, hypothesizing a lack of pre-training knowledge, whereas Python and C++ succeeded. The variability between C++ and Python performance on different datasets (e.g., FIN) suggests the prompt language choice is a non-trivial factor
- Why unresolved: The study shows inconsistent results across programming languages (Java failed, C++ sometimes better than Python), but does not isolate whether this is due to syntax familiarity, tokenization efficiency, or specific structural logic inherent to the languages
- Evidence needed: A controlled ablation study using LLMs trained on identical data mixtures but differing only in their exposure to specific programming languages, testing identical NER tasks across these languages

### Open Question 3
- Question: Can a hybrid prompting approach be developed that combines the boundary precision of CodeNER with the contextual flexibility of text-based prompting?
- Basis: The "Trade-off Discussion" highlights that while CodeNER excels at boundary detection and avoiding duplicate labels, text-based "Vanilla" prompts are better for datasets where entities are long text spans with many function words
- Why unresolved: The paper presents CodeNER and Vanilla as competing methods, revealing a trade-off where each outperforms the other depending on entity characteristics (short boundaries vs. long spans), but does not propose a mechanism to switch or combine them dynamically
- Evidence needed: A new prompting architecture that dynamically selects code-based or text-based strategies based on sentence structure, or a combined prompt structure that demonstrates superior performance across both short and long-span entity types

## Limitations

- The method's effectiveness depends on LLMs' pre-training knowledge of specific programming languages, with Java failing completely due to limited exposure
- Performance may degrade on long-span entities containing many function words, where text-based approaches can be superior
- Cross-linguistic generalizability is untested for languages with significantly different morphological structures or entity recognition conventions
- The sequential constraint mechanism lacks systematic ablation studies to isolate its contribution from other structural elements

## Confidence

**High Confidence**: The empirical observation that CodeNER outperforms text-based prompting on average across benchmark datasets. The F1 score improvements (2.41 points for closed models, 3.07 points for open models) are statistically measurable and reproducible.

**Medium Confidence**: The claim that sequential processing via for-loops prevents overlapping labels. While the paper shows case studies where Vanilla fails on repeated tokens, it doesn't provide systematic analysis of when and why this failure mode occurs across all datasets.

**Low Confidence**: The mechanism by which programming language inductive bias transfers to NER performance. The paper observes performance differences between Python, C++, and Java prompts, but doesn't establish whether these differences reflect genuine code knowledge transfer or simply syntactic familiarity.

## Next Checks

1. **Ablation study on code structural elements**: Systematically remove and test individual components (for-loop, function signature, label descriptions, print statement) to quantify each element's contribution to performance gains. This would isolate whether sequential constraint or schema-context integration drives improvements.

2. **Analysis of failure cases across label types**: Examine per-label F1 scores for each entity type (PER, LOC, ORG, MISC) across all datasets to identify whether CodeNER consistently helps with specific entity boundaries (e.g., organizations vs locations) or if benefits vary by entity type and dataset characteristics.

3. **Cross-linguistic morphological robustness test**: Apply CodeNER to languages with significantly different morphological and syntactic structures (e.g., Turkish, Finnish, Chinese) to test whether the programming language inductive bias generalizes beyond Indo-European languages and similar NER conventions.