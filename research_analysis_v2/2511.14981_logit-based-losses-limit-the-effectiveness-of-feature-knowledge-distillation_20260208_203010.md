---
ver: rpa2
title: Logit-Based Losses Limit the Effectiveness of Feature Knowledge Distillation
arxiv_id: '2511.14981'
source_url: https://arxiv.org/abs/2511.14981
tags:
- layer
- knowledge
- student
- layers
- distillation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues that logit-based losses limit the effectiveness
  of feature knowledge distillation (FKD) and proposes a framework that trains student
  backbones using only feature-based losses, without logit-based losses. The authors
  introduce a geometry-aware metric to identify high-quality teacher layers for distillation
  and demonstrate state-of-the-art performance across three datasets and four student-teacher
  pairs, achieving up to 15% accuracy improvement over standard methods.
---

# Logit-Based Losses Limit the Effectiveness of Feature Knowledge Distillation

## Quick Facts
- arXiv ID: 2511.14981
- Source URL: https://arxiv.org/abs/2511.14981
- Reference count: 40
- Key outcome: Proposes removing logit-based losses from feature knowledge distillation, using only feature losses to train student backbones, achieving up to 15% accuracy improvement over standard methods.

## Executive Summary
This paper challenges the standard practice of combining logit-based and feature-based losses in knowledge distillation by demonstrating that logit losses can actually limit the effectiveness of feature knowledge transfer. The authors propose a novel framework that trains student backbones using only feature-based losses while separating classifier training with cross-entropy loss. They introduce a geometry-aware metric (Q) to identify optimal teacher layers for distillation, showing that layers at the extraction-compression transition exhibit the highest "knowledge quality" and yield best performance across three datasets and four student-teacher pairs.

## Method Summary
The method trains student backbones using only feature-based losses, removing both cross-entropy and KL divergence losses from backbone optimization. A geometry-aware metric Q(R) = S(R) + √(I(R)×E(R)) combines separation, information, and efficiency properties to identify high-quality teacher layers for distillation. The top-k=4 teacher layers are selected based on Q, with student layers mapped accordingly. Training uses Adam optimizer with One Cycle LR scheduler for 50 epochs, employing a single-layer projector for dimensional alignment between teacher and student features. The student classifier is trained separately with cross-entropy loss, while the backbone optimizes only feature alignment loss.

## Key Results
- Proposes removing logit-based losses from feature knowledge distillation
- Introduces geometry-aware metric Q to identify optimal teacher layers
- Achieves state-of-the-art performance with up to 15% accuracy improvement
- Demonstrates effectiveness across three datasets and four student-teacher pairs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Feature-based losses can transfer richer knowledge than logit-based losses due to dimensional differences.
- Mechanism: Feature losses compute distances in high-dimensional ambient spaces, while logit losses compute in class-space (dimension = number of classes C). The lower-dimensional bottleneck of logit losses dilutes the potential effectiveness of feature knowledge transfer.
- Core assumption: The dimensionality of the optimization space directly correlates with the richness of transferable knowledge.
- Evidence anchors: [section 1] states feature-based losses capture richer information in high-dimensional spaces; [corpus] provides weak direct support from related work.

### Mechanism 2
- Claim: Removing logit losses from backbone training eliminates gradient misalignment between loss terms.
- Mechanism: When both logit and feature losses backprop through the backbone, their gradients can conflict—pulling representations in incompatible directions. By stopping L_CE at the classifier and removing L_KL entirely, the backbone optimizes purely for feature alignment.
- Core assumption: Gradient misalignment is the primary cause of suboptimal FKD performance.
- Evidence anchors: [section 2] cites prior work showing FKD methods suffer from mis-alignment; [section 3.2] describes the loss recipe separating backbone and classifier training.

### Mechanism 3
- Claim: Layers at the extraction-compression transition exhibit peak "knowledge quality" and yield best distillation performance.
- Mechanism: The Q metric combines separation S, information I, and efficiency E to identify optimal layers. Transition layers balance high information from extraction with emerging separation from compression.
- Core assumption: The three geometric properties capture essential qualities of distillable knowledge.
- Evidence anchors: [section 3.2] reveals transition layers exhibit highest knowledge quality; [section 4.1] shows Q peaks at different relative depths for various architectures.

## Foundational Learning

- Concept: **Vanilla KD (VKD) vs. Feature KD (FKD)**
  - Why needed here: Understanding what knowledge is transferred—softmax distributions (VKD) or intermediate representations (FKD)—is essential to grasp why removing logit losses matters.
  - Quick check question: Can you explain why a 1000-class logit space is "lower-dimensional" than a 512-neuron layer representation?

- Concept: **Neural Collapse and Extraction/Compression Phases**
  - Why needed here: The Q metric builds on findings that models first expand representation dimensionality then compress toward class means. Recognizing this pattern helps interpret Q curves.
  - Quick check question: In a classifier's final layers, would you expect separation S to increase or decrease, and why?

- Concept: **Gradient Flow and Backpropagation Stopping**
  - Why needed here: The method stops L_CE backprop at l^S_final. You must understand where gradients flow to implement the loss recipe correctly.
  - Quick check question: If L_CE is computed on logits but stopped before the backbone, which parameters still receive L_CE gradients?

## Architecture Onboarding

- Component map: Student backbone (layers 1 to l^S_final) -> Student classifier (layers after l^S_final) -> Projector (single-layer dimensional alignment) -> Loss aggregator (L_FKD = L_CE + Σ A_ij × L_F)

- Critical path:
  1. Compute Q for all teacher layers on training set
  2. Select top-4 teacher layers by Q; map to 4 student layers
  3. Build projectors for each teacher-student pair
  4. Training loop: forward pass → L_F on backbone, L_CE on classifier (detached from backbone) → backward

- Design tradeoffs:
  - Q metric formulation: Authors combined S + √(I×E) empirically; alternative formulations unexplored
  - k=4 layers: Follows prior work; sensitivity not extensively tested
  - Single-layer projector: Minimal design vs. deeper projectors; may limit alignment quality

- Failure signatures:
  - Convergence failure: Standard layer selection + no logit losses → student fails to train (6/12 cases)
  - Unstable training: High-norm representations correlate with gradient instability
  - No improvement: ViT teachers with monotonically increasing Q may work better with logit-based methods

- First 3 experiments:
  1. Baseline sanity check: Train student with standard FKD to verify baseline performance
  2. Ablation on loss recipe: Remove only L_KL, then remove all logit losses
  3. Q metric validation: Plot S, I, E, Q curves for your teacher before full distillation runs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed framework scale effectively to large-scale datasets like ImageNet, and what are the associated computational costs?
- Basis in paper: Authors state in Limitations section that they "did not experiment with larger scale datasets such as Image Net" due to "computationally intensive nature of our geometric analysis."
- Why unresolved: Empirical validation restricted to CIFAR and Tiny ImageNet to manage computational overhead.
- What evidence would resolve it: Benchmarks of method's performance on standard ImageNet configurations compared to existing SOTA KD methods.

### Open Question 2
- Question: How does the geometry-based layer selection strategy transfer to dense prediction tasks or non-visual modalities?
- Basis in paper: Authors list exploring "computer vision tasks such as image segmentation" and "classification tasks involving modalities such as text and audio" as future work.
- Why unresolved: Study focused exclusively on image classification; geometric properties in segmentation or NLP models might behave differently.
- What evidence would resolve it: Experiments applying Q metric and logit-free backbone training to object detection or language modeling tasks.

### Open Question 3
- Question: Can the Q metric formulation be derived or improved via principled theoretical analysis?
- Basis in paper: Authors note they "combined S, I, and E into the Q metric based on empirical evidence" and "suspect that the formulation could be improved with principled theoretical analysis."
- Why unresolved: Current metric relies on heuristic combination (S + √(I×E)) without formal theoretical guarantee of optimality.
- What evidence would resolve it: Theoretical framework linking geometric properties of latent representations to distillation upper bounds.

### Open Question 4
- Question: To what extent does student layer selection influence the effectiveness of the framework?
- Basis in paper: Conclusion lists "studying how student layer selection influences distillation" as primary future work.
- Why unresolved: Method focuses on automating teacher layer selection while relying on standard or fixed mapping strategies for student layers.
- What evidence would resolve it: Ablation studies varying number and depth of student layers used in distillation loss.

## Limitations

- Method sometimes fails to converge (6/12 cases in experiments) when using standard layer selection without logit losses
- Q metric formulation (S + √(I×E)) was empirically chosen without systematic exploration of alternatives
- Critical implementation details remain underspecified including exact feature loss function type, projector architecture specifics, and student layer selection methodology
- Results primarily from standard vision datasets and architectures, limiting generalizability to other domains

## Confidence

- **High confidence**: That FKD methods can suffer from gradient misalignment between logit and feature losses
- **Medium confidence**: That the Q metric effectively identifies high-quality distillation layers
- **Medium confidence**: That removing logit losses improves distillation performance
- **Low confidence**: That this approach generalizes beyond tested vision architectures and datasets

## Next Checks

1. **Convergence stability test**: Run 12 configurations with standard layer selection (no Q metric) and no logit losses; verify the 6/12 failure rate reported in the paper.

2. **Metric formulation ablation**: Implement alternative Q formulations (weighted combinations of S, I, E) and compare layer selection performance to validate that S + √(I×E) is optimal.

3. **Domain generalization**: Apply the method to a non-vision task (e.g., NLP sequence classification) to test whether Q-based layer selection and no-logit training transfers beyond image classification.