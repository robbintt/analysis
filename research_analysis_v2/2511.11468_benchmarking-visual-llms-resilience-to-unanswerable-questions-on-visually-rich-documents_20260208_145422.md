---
ver: rpa2
title: Benchmarking Visual LLMs Resilience to Unanswerable Questions on Visually Rich
  Documents
arxiv_id: '2511.11468'
source_url: https://arxiv.org/abs/2511.11468
tags:
- explicit
- document
- questions
- question
- elements
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents VRD-UQA, a new evaluation framework for assessing
  Visual Large Language Models (VLLMs) on their ability to detect unanswerable questions
  in multi-page Visually Rich Documents (VRDs). The framework systematically corrupts
  answerable questions by swapping related NLP entities, document elements, or layout
  positions, and verifies unanswerability using a VLLM-as-a-judge approach.
---

# Benchmarking Visual LLMs Resilience to Unanswerable Questions on Visually Rich Documents

## Quick Facts
- **arXiv ID**: 2511.11468
- **Source URL**: https://arxiv.org/abs/2511.11468
- **Reference count**: 40
- **Primary result**: VRD-UQA framework reveals significant VLLM weaknesses in detecting unanswerable questions in multi-page VRDs, with structural corruptions and long contexts being particularly challenging

## Executive Summary
This paper introduces VRD-UQA, a novel evaluation framework designed to assess Visual Large Language Models' (VLLMs) ability to detect unanswerable questions in multi-page Visually Rich Documents (VRDs). The framework systematically corrupts answerable questions by replacing entities with same-type alternatives from different pages or document elements, then verifies unanswerability using a VLLM-as-a-judge approach. Experiments across 12 VLLMs show substantial performance drops on unanswerable questions, particularly for structural corruptions and larger context windows. The study demonstrates that in-context learning strategies combining OCR integration and explicit unanswerability instructions significantly improve performance, highlighting the importance of document comprehension beyond visual understanding.

## Method Summary
The VRD-UQA framework evaluates VLLMs through a multi-stage pipeline: first, auxiliary models (DocLayout-YOLO, GOT-OCR 2, Qwen 2.5 VL) extract layout, text, and visual element information from VRDs; second, GliNER identifies named entities which are then systematically corrupted by swapping with same-type entities from different document regions; third, Qwen 2.5 rephrases corrupted questions for grammatical correctness; fourth, Gemini 2.5 Flash verifies unanswerability through VLLM-as-a-judge approach; finally, target VLLMs are evaluated using specific prompt templates with varying in-context learning strategies. The framework uses two multi-page VQA datasets (MPDocVQA and DUDE) and measures both document-level and page-level accuracy, with explicit instructions that "unanswerable" is a valid response option.

## Key Results
- VLLMs show significantly lower performance on unanswerable questions compared to answerable ones, with document-level accuracy dropping from ~75% to ~40% on average
- Structural entity corruptions (swapping document elements like "Figure" for "Table") are most challenging, while numeric corruptions are relatively easier to detect
- Increasing context window size from 1 to 3 pages consistently degrades performance due to noise introduction
- Qwen and Gemma models demonstrate superior resilience, while GPT-4o, GPT-4o-mini, and Llama 3.2 show the lowest performance
- Combining OCR integration with explicit unanswerability instructions yields synergistic performance improvements

## Why This Works (Mechanism)

### Mechanism 1: Semantic Entity Corruption via Same-Type Swaps
Replacing specific natural language entities with same-type alternatives from within the document creates plausible yet unanswerable scenarios that test model robustness. This forces VLLMs to rely on precise factual verification rather than superficial semantic matching, as grammatical correctness is preserved while answer validity is invalidated.

### Mechanism 2: In-Context Learning (ICL) Synergy
Providing explicit textual context through OCR combined with clear instructions that unanswerability is a valid outcome significantly improves VLLM performance. OCR reduces visual recognition errors while explicit instructions shift the model's probability distribution to allow for refusal rather than forced answering.

### Mechanism 3: Context Noise Dilution
Increasing page window size introduces information noise that degrades VLLM accuracy in detecting unanswerable questions. As context grows, models struggle to isolate specific lack of evidence, leading to "lost in the middle" errors where the absence of information becomes buried in larger contexts.

## Foundational Learning

**Visually Rich Documents (VRDs) vs. Standard Text**: VRDs require understanding layout and visual elements, not just text. A standard NLP approach misses spatial relationships between entities. *Quick check*: Does the answer depend on reading a paragraph, or finding a specific cell in a table located in the bottom-right corner of a page?

**Negative Constraint in VQA**: Most VQA models maximize correct answer probability. Learning to say "No Answer" requires distinct confidence threshold calibration. *Quick check*: If the document says "Sales in 2009" but the question asks for "Sales in 2011", should the model guess the closest number or refuse to answer?

**Named Entity Recognition (NER) Types**: The corruption pipeline relies on "Same-Type" swaps (Temporal, Numeric, Structural). Understanding these categories is crucial to diagnosing why a model failed. *Quick check*: Is "Figure 3" a location, a number, or a structural entity in the context of this document?

## Architecture Onboarding

**Component map**: VRD Input -> DocLayout-YOLO (Layout) + GOT-OCR 2 (Text) + Qwen 2.5 VL (Visual Captioning) -> GliNER (Entity Extraction) + Rule-based Swapper + Qwen 2.5 (Rephrasing) -> Gemini 2.5 Flash (VLLM-as-a-Judge) -> Target VLLMs (Qwen, Gemma, etc.)

**Critical path**: Augmentation must succeed first, as OCR/Layout failures render subsequent corruption nonsensical. Verification is essential to filter out corruption attempts that accidentally became answerable (approximately 30% filtered).

**Design tradeoffs**: Window size balancing context vs. noise - higher w provides more information but introduces noise that degrades unanswerability detection. VLLM-as-a-judge is cost-effective but introduces model-specific dependencies.

**Failure signatures**: Structural Blindness (failing on document element swaps), In-Page Distractibility (low accuracy when misleading entities are physically close), Hallucinated Grounding (answering with nearby valid information).

**First 3 experiments**: 1) Baseline Robustness: Run VRD-UQA on target model without OCR or explicit instructions to measure raw hallucination rate. 2) ICL Ablation: Compare OCR-only vs. Explicit-only vs. OCR+Explicit to identify dominant improvement factor. 3) Corruption Sensitivity: Evaluate model separately on Numeric vs. Structural entity corruptions to identify specific weaknesses.

## Open Questions the Paper Calls Out

**Open Question 1**: How does VLLM fine-tuning on unanswerable questions compare to zero-shot in-context learning in mitigating performance drops on long documents? The paper notes this as a limitation, having only tested zero-shot settings.

**Open Question 2**: Can specialized prompting strategies be designed to specifically target the low resilience observed in structural entity corruptions? General-purpose ICL strategies help but structural corruptions remain challenging.

**Open Question 3**: Does the "VLLM-as-a-judge" verification method systematically fail to detect subtle semantic unanswerability in specific document domains? The 96.97% precision rate implies potential systematic biases in automated verification.

## Limitations

**Verification Dependency**: The evaluation relies on a VLLM-as-a-judge (Gemini 2.5 Flash) to filter unanswerable questions, introducing model-specific bias and potential verification failures that could leak answerable questions into the benchmark.

**Sampling Transparency**: The selection of a "representative sample of 300 questions" lacks specification of random seed or stratification methodology, limiting reproducibility of exact results.

**Long-Context Generalization**: The observed degradation with larger window sizes may not generalize to VLLMs specifically trained for long contexts, though the paper suggests this is a general trend.

## Confidence

**High Confidence**: VLLMs show significant performance degradation on unanswerable question detection compared to answerable questions; In-context learning strategies provide measurable improvements; Structural entity corruptions are consistently more difficult than numeric or temporal corruptions; Document-level accuracy decreases with increasing page window size.

**Medium Confidence**: Qwen and Gemma models demonstrate superior performance across all metrics; The VLLM-as-a-judge approach provides sufficient verification quality for benchmark creation; Approximately 30% of corrupted questions are filtered as not unanswerable.

**Low Confidence**: The specific degradation patterns would hold for all multi-page VRD datasets; The identified failure modes are universal across all VLLM architectures.

## Next Checks

1. **Verification Independence Test**: Run the corruption pipeline with a different VLLM-as-a-judge (e.g., GPT-4o) and compare the set of retained unanswerable questions to assess verification dependency.

2. **Prompt Engineering Ablation**: Systematically vary the explicit unanswerability instruction format (different phrasings, positions) while keeping OCR constant to quantify prompt sensitivity.

3. **Long-Context Model Comparison**: Test the same framework on a VLLM specifically trained for long contexts (e.g., LongRoPE variants) to determine if the observed window size degradation is architecture-dependent.