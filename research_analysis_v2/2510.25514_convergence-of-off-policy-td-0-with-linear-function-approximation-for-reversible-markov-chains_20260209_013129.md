---
ver: rpa2
title: Convergence of off-policy TD(0) with linear function approximation for reversible
  Markov chains
arxiv_id: '2510.25514'
source_url: https://arxiv.org/abs/2510.25514
tags:
- markov
- learning
- convergence
- chain
- off-policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proves convergence of off-policy TD(0) with linear function
  approximation for reversible Markov chains, addressing the well-known divergence
  issue that occurs when combining off-policy learning with function approximation.
  The key insight is to analyze the standard algorithm under the mild assumption of
  reversibility, rather than modifying the algorithm as previous approaches have done.
---

# Convergence of off-policy TD(0) with linear function approximation for reversible Markov chains

## Quick Facts
- arXiv ID: 2510.25514
- Source URL: https://arxiv.org/abs/2510.25514
- Reference count: 16
- The paper proves convergence of off-policy TD(0) with linear function approximation for reversible Markov chains, addressing the well-known divergence issue that occurs when combining off-policy learning with function approximation.

## Executive Summary
This paper addresses the notorious divergence problem of off-policy temporal difference learning with linear function approximation. The authors prove that standard off-policy TD(0) converges for reversible Markov chains under a specific, explicit bound on the discount factor γ < 2/(c+1), where c bounds the ratio between off-policy and on-policy transition probabilities. This result is significant because it provides a convergence guarantee for the standard algorithm without requiring modifications like Gradient TD, and improves upon previous results that only showed convergence for sufficiently small discount factors.

## Method Summary
The paper analyzes the standard off-policy TD(0) algorithm using stochastic approximation theory. The key insight is that convergence depends on the negative definiteness of matrix A = Φᵀ(γQ̂P - Q̂)Φ, where Φ is the feature matrix, P is the on-policy transition matrix, and Q̂ is the off-policy expected reward matrix. Under the reversibility assumption, the authors derive a specific condition for negative definiteness that translates to an explicit bound on γ. The proof follows the classic stochastic approximation framework, showing that the algorithm converges to the unique fixed point of the projected Bellman operator with zero projected Bellman error.

## Key Results
- Standard off-policy TD(0) converges with probability one for reversible Markov chains when γ < 2/(c+1)
- The convergence achieves projected Bellman error equal to zero
- This explicit bound improves upon previous results that only guaranteed convergence for "sufficiently small" discount factors
- The reversibility assumption enables tighter bounds compared to the general non-reversible case

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Standard off-policy TD(0) with linear function approximation converges with probability one for reversible Markov chains under a specific, explicit bound on the discount factor.
- Mechanism: The core mechanism relies on proving that the expected update matrix A = Φᵀ(γQ̂P - Q̂)Φ is negative definite, a condition required by the stochastic approximation framework (Theorem 17 from Benveniste et al.) for convergence. The reversibility assumption enables the derivation of a specific condition for negative definiteness: γ < 2/(c+1), where c quantifies the maximum ratio between off-policy and on-policy transition probabilities.
- Core assumption: The Markov chain being evaluated is reversible (satisfies detailed balance) and has a finite state space. The feature matrix Φ has full column rank.
- Evidence anchors:
  - [abstract] "We demonstrate convergence under this mild reversibility condition... In particular, we establish a convergence guarantee under an upper bound on the discount factor..."
  - [section 3] Theorem 2: "Let c ≥ 1 satisfy (1/c)ρᵢⱼ ≤ ρ̂ᵢⱼ ≤ cρᵢⱼ... Then wₜ converges to w* with probability 1."
  - [corpus] This paper directly addresses the known divergence issue of off-policy TD, providing a formal guarantee where general ones do not exist.

- Break condition: If the Markov chain is not reversible or if the discount factor γ violates the bound γ < 2/(c+1), the matrix A may cease to be negative definite, and the algorithm can diverge.

### Mechanism 2
- Claim: The reversibility assumption permits a tighter, explicit bound on the discount factor compared to general results that only guarantee stability for a "sufficiently small" γ.
- Mechanism: The proof uses the reversibility property to manipulate the term ∑ ˆqⱼpⱼᵢ found in the negative definiteness condition (Lemma 1). This manipulation allows the convergence requirement to be expressed as a clear inequality between γ and a single scalar c that captures the "distance" between the on-policy and off-policy distributions. Without reversibility, this term is far more difficult to bound, leading to looser or non-existent guarantees.
- Core assumption: The ratio between off-policy and on-policy transition probabilities can be uniformly bounded by a constant c.
- Evidence anchors:
  - [abstract] "...This improves upon known results in the literature that state that convergence holds for a sufficiently small discount factor by establishing an explicit bound."
  - [section 3] Lemma 1 provides the state-wise condition for negative definiteness, which is then simplified to Theorem 2 using the global bound c.
  - [corpus] N/A - The derivation of this explicit bound is a primary contribution of this work.

- Break condition: The derived bound γ < 2/(c+1) is a sufficient, not necessary, condition. The paper acknowledges a gap between this theoretical bound and the empirically observed maximum γ for convergence.

### Mechanism 3
- Claim: The algorithm converges to the unique fixed point of the projected Bellman operator, which has a projected Bellman error (PBE) of zero.
- Mechanism: By satisfying the conditions of the stochastic approximation theorem, the iterates wₜ are guaranteed to converge to the unique solution w* of the equation Aw* + b = 0. This equation is algebraically equivalent to the condition that the projected Bellman error is zero, which is the standard fixed-point for on-policy TD learning.
- Core assumption: The step sizes αₜ satisfy the Robbins-Monro conditions: Σαₜ = ∞ and Σαₜ² < ∞.
- Evidence anchors:
  - [abstract] "Convergence is with probability one and achieves projected Bellman error equal to zero."
  - [appendix B] The proof concludes by showing PBE(w*) = ||Φw* - Φw*|| = 0.
  - [corpus] Related work (e.g., "A Concentration Bound for TD(0)") also analyzes error relative to this same TD fixed point.

- Break condition: This is a guarantee of convergence to the TD fixed point, not to the optimal value function. The "break" condition is inherent to linear TD: the solution is only optimal within the subspace spanned by the features Φ.

## Foundational Learning

- Concept: **Reversible Markov Chains**.
  - Why needed here: This is the central, restrictive assumption of the paper. Understanding detailed balance (qᵢpᵢⱼ = qⱼpⱼᵢ) is essential for evaluating whether the theorem applies to a given problem.
  - Quick check question: For a two-state chain with P(A→B)=0.9 and P(B→A)=0.4, is the chain reversible when in its stationary distribution?

- Concept: **The "Deadly Triad" in RL**.
  - Why needed here: The paper addresses one corner of the deadly triad (off-policy learning + function approximation + bootstrapping). Knowing why this combination is typically unstable clarifies the significance of a convergence result.
  - Quick check question: What are the three components of the "deadly triad" that can lead to divergence in reinforcement learning?

- Concept: **Stochastic Approximation / ODE Method**.
  - Why needed here: The convergence proof is built on this classic framework. The key insight is that an iterative stochastic algorithm converges if its expected update points in a stable direction (governed by matrix A).
  - Quick check question: What are the Robbins-Monro conditions for a step-size sequence (αₜ), and what do they ensure?

## Architecture Onboarding

- Component map:
  - **Algorithm**: Standard off-policy TD(0) update: `wₜ₊₁ = wₜ + αₜ(r + γwₜᵀx' - wₜᵀx)x`
  - **Key Stability Matrix (A)**: `A = Φᵀ(γQ̂P - Q̂)Φ`. The algorithm's fate hinges on this matrix being negative definite.
  - **Perturbation Bound (c)**: A scalar estimating the maximum relative difference in transition ratios between the behavior and target policies.
  - **Convergence Condition**: `γ < 2/(c+1)`. The design constraint for stability.

- Critical path:
  1. **Verify Domain**: Confirm the target environment can be modeled as a reversible Markov chain.
  2. **Estimate Perturbation**: Estimate or bound the parameter `c` based on the known difference between your behavior and target policies.
  3. **Configure Discount Factor**: Select a discount factor `γ` that satisfies `γ < 2/(c+1)`.
  4. **Run Standard TD(0)**: Use the standard TD(0) algorithm with linear features, avoiding more complex convergent variants like GTD or TDC.

- Design tradeoffs:
  - **Guarantee vs. Performance**: The bound on γ is conservative. Adhering to it guarantees stability but may force a smaller discount factor than is practically usable, potentially slowing learning or undervaluing long-term rewards.
  - **Simplicity vs. Generality**: You can use the simpler, standard TD(0) algorithm but must accept the restriction to reversible chains. For general chains, you must switch to more complex, modified algorithms (e.g., Gradient TD).

- Failure signatures:
  - **Weight Divergence**: The weight vector `w` grows unbounded (e.g., NaN or Inf values), the classic failure mode of unstable off-policy TD.
  - **Oscillation**: In less severe violations, weights may oscillate wildly instead of converging.

- First 3 experiments:
  1. **Reversible Chain Validation**: Implement a simple random walk on a weighted graph (a known reversible chain). Run off-policy TD(0) and verify that weights converge to a stable fixed point.
  2. **Condition Sensitivity**: Systematically increase `c` (by making the behavior policy more different from the target) and adjust `γ` to hover around the theoretical bound. Confirm divergence when `γ > 2/(c+1)`.
  3. **Non-Reversible Ablation**: Apply the same algorithm to a non-reversible chain (e.g., a circular random walk). Observe if divergence occurs, demonstrating the necessity of the reversibility assumption.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can convergence of off-policy TD(0) be established for non-reversible Markov chains, where the perturbed chain satisfies only the condition that reverse transitions exist?
- Basis in paper: [explicit] "It is worth investigating to extend our result to the case where the perturbed chain is also not reversible, and instead satisfies the assumption that the reverse transition exists."
- Why unresolved: The proof technique relies on reversibility to manipulate the sum Σⱼ ˆqⱼpⱼᵢ; without reversibility, correction terms would be needed, complicating the bound on γ.
- What evidence would resolve it: A modified proof showing A remains negative definite under relaxed reversibility assumptions, with revised bounds on γ.

### Open Question 2
- Question: What is the gap between the sufficient condition γ < 2/(c+1) and the necessary condition for matrix A to be negative definite?
- Basis in paper: [explicit] "These experiments showed a significant gap between the numerically required values for γ and those obtained from our error bounds... Future steps would be to derive all the terms that make up the gap, and to quantify the differences between the bounds for various settings."
- Why unresolved: Lemma 1 provides a sufficient (diagonal dominance) but not necessary condition for negative definiteness.
- What evidence would resolve it: Analytical characterization of when A is negative definite beyond diagonal dominance, or counterexamples showing the bound is tight.

### Open Question 3
- Question: Can finite-time error bounds be derived for off-policy TD(0) under the reversible setting with diminishing step sizes?
- Basis in paper: [explicit] "Another extension is to consider the rate of convergence instead of the asymptotic convergence shown in this paper... The main challenge will be to derive corresponding assumptions for our setting."
- Why unresolved: Existing finite-time results (Srikant and Ying, 2019) assume constant step sizes or require technical assumptions involving mixing times that need adaptation.
- What evidence would resolve it: A theorem providing bounds on E[‖wₜ − w*‖²] under Robbins-Monro step sizes.

### Open Question 4
- Question: Can the convergence results be extended to infinite state spaces or to dependent (non-linearly independent) basis functions?
- Basis in paper: [explicit] "Our work may be extended to more general settings by eliminating various assumptions at the cost of complexity of the proof. Think of the assumption of a finite state space, or linear independence of the basis functions..."
- Why unresolved: The current proof relies on finite-dimensional matrix analysis and the full column rank assumption of Φ.
- What evidence would resolve it: Convergence proofs using operator-theoretic methods for infinite-dimensional function spaces.

## Limitations
- The reversibility assumption is restrictive and excludes many practical RL scenarios where the evaluation chain is not reversible.
- The theoretical bound γ < 2/(c+1) is conservative, with a gap between the analytical guarantee and empirically observed convergence thresholds.
- The proof assumes a finite state space and full column rank of Φ, which may not hold in infinite-horizon or high-dimensional settings.

## Confidence

- **High Confidence**: Convergence proof framework and derivation of the explicit discount factor bound under reversibility assumptions. The negative definiteness condition and its connection to the stochastic approximation framework are rigorously established.
- **Medium Confidence**: Practical applicability given the restrictive reversibility assumption and conservative theoretical bounds. The paper acknowledges this gap but does not provide methods to tighten the bounds.
- **Low Confidence**: Extension to infinite state spaces or scenarios where Φ lacks full column rank. The behavior of the algorithm under approximate reversibility or near-critical γ values remains unclear.

## Next Checks
1. Implement numerical experiments comparing the theoretical bound γ < 2/(c+1) against empirically determined convergence thresholds across different reversible chain structures.
2. Test the algorithm on non-reversible chains (e.g., circular random walks) to verify that divergence occurs when the reversibility assumption is violated.
3. Explore whether the bound can be tightened by incorporating additional problem structure beyond reversibility, such as specific transition probability symmetries.