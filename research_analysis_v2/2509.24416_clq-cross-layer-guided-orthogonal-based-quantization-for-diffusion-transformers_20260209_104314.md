---
ver: rpa2
title: 'CLQ: Cross-Layer Guided Orthogonal-based Quantization for Diffusion Transformers'
arxiv_id: '2509.24416'
source_url: https://arxiv.org/abs/2509.24416
tags:
- quantization
- layer
- generation
- which
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CLQ, a post-training quantization framework
  for diffusion transformers (DiTs) used in visual generation. The authors observe
  that DiTs contain extreme values in weights and activations that significantly degrade
  quantization performance when compressing to ultra-low bit-widths like W4A4.
---

# CLQ: Cross-Layer Guided Orthogonal-based Quantization for Diffusion Transformers

## Quick Facts
- **arXiv ID**: 2509.24416
- **Source URL**: https://arxiv.org/abs/2509.24416
- **Reference count**: 4
- **Primary result**: CLQ achieves 3.98x memory saving and 3.95x speedup with negligible performance degradation for DiT compression to W4A4

## Executive Summary
This paper introduces CLQ, a post-training quantization framework specifically designed for diffusion transformers (DiTs) used in visual generation tasks. The authors identify that DiTs contain extreme values in weights and activations that severely degrade quantization performance when compressing to ultra-low bit-widths like W4A4. CLQ addresses this through three key innovations: cross-block calibration to accurately capture quantization drift, orthogonal-based smoothing using Hadamard transforms to handle outliers, and cross-layer parameter searching to optimize quantization bounds. The method successfully compresses both image generation (PixArt-α) and video generation (OpenSora) models while maintaining high-quality outputs, outperforming state-of-the-art PTQ methods on GenEval and VBench benchmarks.

## Method Summary
CLQ employs three main techniques to enable effective ultra-low-bit quantization of DiTs. Cross-Block Calibration (CBC) sequentially quantizes transformer blocks, using the output of previously quantized blocks as input for calibrating subsequent layers to capture accumulated quantization error. Orthogonal-Based Smoothing (OBS) detects and smooths activation outliers by applying block Hadamard transforms after channel permutation, reducing dynamic range without information loss. Cross-Layer Parameter Searching (CLPS) optimizes quantization clipping bounds by analyzing the most sensitive downstream layers within a 3-block window, performing grid search to minimize reconstruction error at the target layer. The framework processes models layer-by-layer, applying OBS and CLPS during the CBC calibration loop to achieve W4A4 quantization with minimal performance degradation.

## Key Results
- CLQ achieves 3.98x memory saving and 3.95x speedup when compressing DiTs to W4A4
- Outperforms state-of-the-art PTQ methods (ViDiT-Q, SmoothQuant, QuaRot) on both GenEval and VBench benchmarks
- Successfully maintains high-quality outputs for both image generation (PixArt-α) and video generation (OpenSora) models
- Ablation studies show each component (CBC, OBS, CLPS) contributes measurable improvements to overall performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Quantizing previous transformer blocks before calibrating the current layer reduces distribution mismatch.
- **Mechanism:** Standard calibration uses full-precision (FP) inputs, but the deployed model sees quantized inputs (with accumulated error). Cross-Block Calibration (CBC) enforces a block-sequential process: quantize Block $k-1$ before collecting calibration data for Block $k$. This ensures the calibration data includes the "accumulated quantization error" present in real inference.
- **Core assumption:** The distribution shift caused by quantizing previous layers is significant enough to degrade calibration if ignored, but stable enough to be approximated block-wise.
- **Evidence anchors:** [abstract] "...calibration data used by most of the PTQ methods can not honestly represent the distribution... propose cross-block calibration (CBC)..." [section 3.2] "Therefore, the input has changed to $\hat{X}$ when calibrating the target layer... leading to a significant calibration error."
- **Break condition:** If quantization error accumulates non-linearly across blocks in a way that block-wise updates cannot track, calibration data may still drift from true inference distribution.

### Mechanism 2
- **Claim:** Channel swapping followed by a Block Hadamard transform smooths activation outliers more efficiently than learned rotations.
- **Mechanism:** Orthogonal-Based Smoothing (OBS) detects channels with high "outlier scores" (e.g., absolute max). It constructs a permutation matrix to group high-magnitude channels. A Block Hadamard matrix is then applied. The Hadamard transform rotates the vector space, distributing outlier energy across other channels (smoothing), reducing the dynamic range required for quantization.
- **Core assumption:** Outliers are primarily a distributional nuisance rather than carrying semantic "outlier" features that must remain isolated in specific channels.
- **Evidence anchors:** [abstract] "...leverages block Hadamard matrix to smooth the outliers with negligible overhead." [section 3.3] "The column-swapping matrix gathers the outliers together, and the Hadamard matrix could effectively smooth the activation matrix..."
- **Break condition:** If semantic information is strictly localized to specific activation channels, rotation might obscure these features, though the paper notes orthogonal transforms are lossless in norm.

### Mechanism 3
- **Claim:** Optimizing quantization parameters (clipping bounds) by minimizing error in a sensitive downstream layer is superior to local optimization.
- **Mechanism:** Cross-Layer Parameter Searching (CLPS) rejects local min-max clipping. Instead, it perturbs the current layer to find the subsequent layer (within 3 blocks) most sensitive to input changes (L1 norm). It then performs a grid search on the current layer's clipping bounds ($l, r$) to minimize the L2 error of the *target layer's* output.
- **Core assumption:** The "most influenced" layer within a local window (3 blocks) serves as a sufficient proxy for the network's global sensitivity to quantization error at the current layer.
- **Evidence anchors:** [section 3.4] "We limit the target layer to be within the subsequent three blocks... determine the target layer as the most influenced layer... perform a grid search."
- **Break condition:** If the critical error propagation path extends beyond the 3-block window, the selected target layer will be a poor proxy, potentially yielding sub-optimal clipping bounds.

## Foundational Learning

- **Concept:** Post-Training Quantization (PTQ) & Dynamic Range
  - **Why needed here:** The core problem is compressing FP16 to INT4. You must understand why "extreme values" (outliers) expand the quantization range ($r - l$), drastically reducing precision (higher rounding error) for normal values.
  - **Quick check question:** If an activation tensor has a range of [-100, 100] but 99% of values lie in [-1, 1], why does min-max quantization destroy precision for the majority?

- **Concept:** Orthogonal/Hadamard Matrices
  - **Why needed here:** OBS relies on the Hadamard transform. You need to know that these matrices preserve vector norm (energy) while "mixing" values, effectively flattening spikes.
  - **Quick check question:** Does applying a Hadamard transform change the magnitude of the vector $\mathbf{x}$ (i.e., $||\mathbf{x}||_2$)? Why is this property critical for preserving information during smoothing?

- **Concept:** Calibration Data Drift
  - **Why needed here:** CBC addresses the mismatch between training-time calibration and inference-time reality.
  - **Quick check question:** Why would calibrating Layer 5 using FP16 inputs from Layer 4 result in a biased scale factor if the deployed model runs Layer 4 in INT4?

## Architecture Onboarding

- **Component map:** Calibration Loop (CBC) -> OBS Module -> CLPS Optimizer
- **Critical path:** The sequential dependency in CBC is the bottleneck. You cannot calibrate Block $N$ until Block $N-1$ is fully quantized (weights processed and frozen).
- **Design tradeoffs:**
  - Smoothing vs. Clipping: OBS rotates outliers (preserves info) vs. standard methods that clip outliers (loses info).
  - Search Cost vs. Quality: CLPS uses a grid search. It improves W4A4 quality significantly but increases the one-time calibration overhead compared to static min-max scaling.
- **Failure signatures:**
  - Model Collapse: At W4A4 without CLQ, models generate "random noise" (QuaRot) or "black content" (SmoothQuant).
  - Drift: If CBC is skipped, later layers diverge significantly from calibration statistics, resulting in color/consistency failures (e.g., lower scores in "Imaging Quality" or "Consistency").
- **First 3 experiments:**
  1. Sanity Check (W8A8): Run CLQ on PixArt-α at W8A8. Verify it matches FP16 performance (Total Score ~0.55 vs 0.54) to ensure the pipeline is stable.
  2. Ablation (W4A4): Disable OBS and run W4A4. Confirm visual collapse or metric drop (Table 1 suggests score drops to ~0.45). This isolates the outlier problem.
  3. Target Layer Analysis: Visualize CLPS target selection (Fig 3). Verify that for specific layers (e.g., `attn1.to_q`), the "most influenced" target is indeed the subsequent projection layer (`attn1.to_out` or `attn2.to_q`) to validate the cross-layer sensitivity assumption.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the CLQ framework be effectively extended to sub-4-bit quantization (e.g., W2A4 or W2A2) while maintaining generative capabilities? The paper states "Future work will focus on lower bit-width and further improving performance," but pushing to 2-bit weights typically introduces sign-bit errors and capacity collapse that current smoothing techniques may not handle.

- **Open Question 2:** How can the "critical information" encoded in extreme values be preserved during quantization without relying solely on smoothing them into the normal distribution? The paper notes "how to properly handle extreme values remains an open problem," as they "encode critical information" but cause severe rounding errors.

- **Open Question 3:** Can the Cross-Layer Parameter Searching (CLPS) be reformulated to avoid the computational overhead of grid searching and perturbation-based target layer identification? The paper describes CLPS using a "grid search" over bounds and perturbations, which is computationally discrete and potentially inefficient for scaling to larger models.

## Limitations

- The CBC mechanism assumes block-wise sequential quantization accurately captures cumulative quantization error, but this may not hold for models with highly non-linear error accumulation patterns or skip connections.
- OBS assumes rotating outlier channels preserves semantic information, but the paper provides no ablation showing performance degradation when this assumption fails.
- CLPS's limitation to analyzing only 3 subsequent blocks may miss important error propagation paths in deeper networks, and the paper doesn't explore how performance scales with different block window sizes.

## Confidence

**High Confidence:** The empirical results showing CLQ outperforming baseline methods on both GenEval and VBench benchmarks, with measurable improvements in Total Score metrics across ablation studies.

**Medium Confidence:** The theoretical justification for why each mechanism works. While the paper provides plausible explanations, these are primarily asserted rather than rigorously proven.

**Low Confidence:** The reproducibility of results without access to the exact calibration datasets, specific Hadamard matrix dimensions, and precise perturbation magnitudes used in CLPS.

## Next Checks

1. **Cross-Block Calibration Drift Analysis:** Measure the actual distribution shift between FP inputs and quantized-block inputs at each layer to quantify how much calibration error CBC prevents.

2. **OBS Information Preservation Test:** Create synthetic activation patterns where specific channels contain unique semantic information, then apply OBS and measure information loss via reconstruction error or downstream task performance.

3. **CLPS Window Size Sensitivity:** Systematically vary the CLPS search window from 1 to 10 blocks and measure the impact on quantization quality and calibration time.