---
ver: rpa2
title: 'Dual-Phase Continual Learning: Supervised Adaptation Meets Unsupervised Retention'
arxiv_id: '2406.13653'
source_url: https://arxiv.org/abs/2406.13653
tags:
- learning
- data
- dosapp
- forgetting
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel continual learning setting where test-time
  data can be used to reduce forgetting without relying on replay buffers or task
  boundaries. The key method, DoSAPP, uses a Teacher-Student framework with dual-momentum
  exponential moving average (EMA) updates, where the teacher's parameters are updated
  based on an affine transformation of the student's sparse parameter updates.
---

# Dual-Phase Continual Learning: Supervised Adaptation Meets Unsupervised Retention

## Quick Facts
- arXiv ID: 2406.13653
- Source URL: https://arxiv.org/abs/2406.13653
- Reference count: 40
- Proposes DoSAPP: Dual-phase continual learning with sparse parameter updates and dual-momentum EMA

## Executive Summary
This paper introduces a novel continual learning setting where test-time data can be used to reduce forgetting without replay buffers or task boundaries. The DoSAPP method employs a Teacher-Student framework with sparse parameter updates and dual-momentum exponential moving average (EMA) updates. During supervised phases, only the top 10% of parameters by gradient magnitude are updated, while the unsupervised test-time learning phase uses a pseudo-labeling strategy based on maximum logit scores from both teacher and student models. The method achieves state-of-the-art results on five vision datasets, demonstrating effectiveness in handling imbalanced test data and long sequences of tasks with domain shifts.

## Method Summary
DoSAPP combines sparse parameter updates with a Teacher-Student framework using dual-momentum EMA. During supervised phases, gradient magnitude scores identify the top 10% of parameters for local updates, restricted to first MLP layers of transformer blocks. The teacher parameters update via dual-momentum: fast for active parameters (γ=0.8) and slow for frozen parameters (δ=0.9999). In the unsupervised test-time phase, pseudo-labels are generated by comparing maximum logits from teacher and student, selecting the higher-confidence expert. The teacher's intermediate momentum (λ=0.9) handles noisier online updates while maintaining stability.

## Key Results
- Outperforms existing continual learning methods on five vision datasets
- Achieves state-of-the-art results in average accuracy and forgetting metrics
- Demonstrates robustness to imbalanced test data and long task sequences
- Single-momentum EMA significantly degrades performance compared to dual-momentum approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sparse parameter updates (top 10% by gradient magnitude) reduce interference with previously learned knowledge while enabling task-specific adaptation
- Mechanism: Gradient-based scoring identifies parameters most relevant to current loss reduction; only these are updated in first MLP layers
- Core assumption: Gradient magnitude correlates with parameter importance for the task, and important parameters are largely task-specific
- Evidence: Table 6 shows sparse parameters improve accuracy on 4/5 datasets; restricting updates to first MLP layers preserves generic representations

### Mechanism 2
- Claim: Dual-momentum EMA updates (δ, γ, λ) decouple adaptation rates for masked vs. unmasked parameters
- Mechanism: Teacher parameters for active parameters move faster (γ=0.8) while frozen parameters update slowly (δ=0.9999); λ=0.9 handles test-time updates
- Core assumption: Single-momentum EMA creates "dissonance" when some parameters are masked
- Evidence: Table 7 shows performance significantly drops when γ=δ (single momentum EMA)

### Mechanism 3
- Claim: Max-logit expert selection for pseudo-labeling leverages teacher (stable, past-task expert) and student (plastic, recent-task expert) as complementary specialists
- Mechanism: For each test sample, compare max logits l_T = max(M_T(x)) and l_S = max(M_S(x)); assign pseudo-label from whichever expert yields higher confidence
- Core assumption: Maximum unnormalized logit is a reliable in-distribution score for CLIP
- Evidence: Table 12 shows pseudo-labeling from teacher-only degrades performance vs. dual-expert selection

## Foundational Learning

- **Exponential Moving Average (EMA) for model averaging**
  - Why needed: Teacher-Student framework relies on EMA to maintain stable reference model
  - Quick check: If δ = 0.9999, how many steps for teacher to reflect ~63% of student update? (Answer: ~10,000 steps)

- **Gradient-based parameter importance scoring**
  - Why needed: DoSAPP uses gradient magnitude to select sparse parameters
  - Quick check: Why might gradient magnitude fail for parameters critical to loss landscape curvature but not immediate loss reduction?

- **Pseudo-labeling and self-training**
  - Why needed: Unsupervised test-time phase depends on generating labels from model predictions
  - Quick check: What happens to pseudo-label quality when model is systematically overconfident on out-of-distribution samples?

## Architecture Onboarding

- **Component map:**
  - CLIP ViT-B/16 backbone (Student Model) -> Gradient Scoring Function -> Parameter Mask Manager -> First MLP Layers
  - Student Model -> Dual-Momentum EMA -> Teacher Model -> Max-Logit Expert Selection
  - Teacher Model + Student Model -> Pseudo-label Router -> Student Update

- **Critical path:**
  1. Supervised phase: Compute F(θ) → select top-10% → update M_S → update M_T via dual-momentum EMA
  2. Unsupervised phase: Load m_u → compare l_T vs l_S → assign pseudo-label → update M_S → update M_T with λ-momentum
  3. Evaluation: Report M_T accuracy on held-out D_e

- **Design tradeoffs:**
  - Sparsity (c=0.1): Lower values reduce capacity; higher values increase forgetting risk
  - Momentum values (δ=0.9999, γ=0.8, λ=0.9): Higher δ stabilizes teacher but slows adaptation
  - Teacher-Student overhead: Requires maintaining two models (2× memory), though sparse updates mitigate compute cost

- **Failure signatures:**
  - Negative forgetting on Cars but high forgetting on Aircraft (Table 5)
  - Performance drop when γ=λ (Table 7)
  - Degradation with teacher-only pseudo-labels (Table 12)

- **First 3 experiments:**
  1. Run DoSAPP on single task with sparsity c ∈ {0.01, 0.1, 0.5}; verify accuracy correlates with Table 11 trends
  2. Compare (δ=0.9999, γ=0.8) vs. (δ=γ=0.9999) on 2-task sequence; expect ~5-10% accuracy drop
  3. Reduce D_u to 25%, 50%, 75% per Table 10; confirm graceful degradation rather than collapse

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can DoSAPP's mechanisms generalize to generative or other transformer-based tasks?
- Basis: Conclusion states experiments focus exclusively on CIL classification
- Why unresolved: Current study validates only on discriminative vision-language tasks
- What evidence would resolve: Empirical results on generative models (e.g., LLMs) or standard ViTs without CLIP pre-training

### Open Question 2
- Question: How can the framework be made robust to extreme class imbalance in unsupervised test-time stream?
- Basis: Appendix 8.4 identifies test data quality as primary bottleneck
- Why unresolved: Demonstrates robustness to moderate imbalance but suggests extreme cases require future mitigations
- What evidence would resolve: Modifications maintaining high accuracy when specific classes are heavily overrepresented or absent

### Open Question 3
- Question: Can stability be achieved without maintaining two separate models?
- Basis: Appendix 8.4 lists computational overhead of two models as limitation
- Why unresolved: Method relies on discrepancy between slow-moving teacher and fast-adapting student
- What evidence would resolve: Single-model approach achieving comparable forgetting metrics with reduced memory footprint

### Open Question 4
- Question: How does DoSAPP perform with significant proportion of Out-Of-Distribution (OOD) samples in test-time data?
- Basis: Methodology assumes D^u_t is drawn from "distributions of all previous tasks"
- Why unresolved: Real-world deployment streams often contain unknown classes
- What evidence would resolve: Experiments evaluating degradation when test-time stream contains classes outside training distribution

## Limitations
- Gradient-based parameter selection may fail when tasks share critical parameters or gradients are noisy
- Dual-momentum EMA's superiority lacks external corpus validation
- Max-logit expert routing assumes teacher remains well-calibrated for old tasks over long task sequences
- Requires maintaining two models (2× memory) despite sparse updates mitigating compute cost

## Confidence

- **High**: Sparse parameter updates improve average accuracy (validated by Table 6 and 11)
- **Medium**: Dual-momentum EMA reduces forgetting (supported by Table 7, lacks external validation)
- **Medium**: Max-logit expert selection outperforms teacher-only pseudo-labeling (confirmed by Table 12, routing mechanism unverified)

## Next Checks

1. **Gradient importance validation**: Replace gradient magnitude scoring with random parameter selection at sparsity c=0.1; measure accuracy drop to quantify gradient's contribution beyond sparsity alone

2. **Long-sequence stability**: Run DoSAPP on 10+ task sequences with domain shifts; track negative forgetting accumulation to test teacher calibration decay

3. **Transfer vs. interference analysis**: After training on task pairs (A→B, B→A), compute parameter overlap in frozen vs. updated sets; measure correlation with forgetting to validate sparse update assumptions