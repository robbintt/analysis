---
ver: rpa2
title: Cross-Attention is Half Explanation in Speech-to-Text Models
arxiv_id: '2509.18010'
source_url: https://arxiv.org/abs/2509.18010
tags:
- cross-attention
- attention
- https
- speech
- saliency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes cross-attention as an explanation mechanism
  in speech-to-text models by comparing cross-attention scores to input saliency maps
  derived from feature attribution. The study evaluates monolingual, multilingual,
  and multitask models across ASR and ST tasks.
---

# Cross-Attention is Half Explanation in Speech-to-Text Models

## Quick Facts
- arXiv ID: 2509.18010
- Source URL: https://arxiv.org/abs/2509.18010
- Authors: Sara Papi; Dennis Fucci; Marco Gaido; Matteo Negri; Luisa Bentivogli
- Reference count: 40
- Cross-attention explains only ~50% of input relevance in S2T models

## Executive Summary
This paper investigates whether cross-attention weights in speech-to-text models provide a complete explanation of model behavior. Through systematic correlation analysis between cross-attention scores and input saliency maps, the study reveals that cross-attention captures only about half of the relevant input features. The research spans monolingual, multilingual, and multitask architectures across automatic speech recognition and speech translation tasks. Results show that while cross-attention provides valuable insights, it represents an incomplete explanation mechanism that should complement rather than replace formal explainability methods.

## Method Summary
The study evaluates cross-attention as an explanation mechanism by correlating attention scores with input saliency maps derived from feature attribution methods. Researchers analyze monolingual, multilingual, and multitask models across ASR and ST tasks, computing Pearson correlation coefficients between cross-attention weights and saliency maps. The analysis examines correlations across different decoder layers, attention heads, and model architectures. To isolate context mixing effects, the study also evaluates correlations between encoder output representations and saliency maps. Multiple correlation thresholds and averaging strategies are employed to assess the robustness of findings.

## Key Results
- Cross-attention correlates moderately to strongly with saliency maps (up to 0.588) when averaged across heads and layers
- Cross-attention captures only about 50% of input relevance, explaining 52-75% of saliency patterns
- Last decoder layers show strongest alignment with saliency patterns
- Even when isolating context mixing effects via encoder outputs, cross-attention explains just half of saliency

## Why This Works (Mechanism)
The paper demonstrates that cross-attention provides partial but incomplete explanations of speech-to-text model behavior. The mechanism works by showing that while attention weights do capture some aspects of input relevance, they miss significant portions of the model's reasoning process. The correlation analysis reveals that attention patterns align with saliency maps to varying degrees depending on architectural choices and averaging strategies, but never achieve complete explanatory coverage.

## Foundational Learning
- **Cross-attention mechanism**: How decoder attends to encoder outputs in encoder-decoder architectures - why needed: central to understanding model's context utilization; quick check: verify attention patterns match expected linguistic phenomena
- **Saliency maps**: Gradient-based feature attribution methods for input importance - why needed: provides ground truth for evaluating attention explanations; quick check: confirm saliency patterns align with linguistic intuition
- **Correlation analysis**: Statistical methods for measuring relationship strength - why needed: quantifies explanatory completeness of attention; quick check: validate Pearson coefficient calculations
- **Encoder-decoder architecture**: Standard sequence-to-sequence model structure - why needed: context for understanding cross-attention role; quick check: trace information flow from encoder to decoder
- **Feature attribution**: Methods for identifying input feature importance - why needed: enables quantitative evaluation of explanations; quick check: verify attribution method sensitivity

## Architecture Onboarding
- **Component map**: Audio input -> Encoder -> Cross-attention (Decoder) -> Output predictions
- **Critical path**: Audio features → Encoder representations → Cross-attention weights → Decoder predictions
- **Design tradeoffs**: Complete explanations require full saliency analysis vs. lightweight attention inspection
- **Failure signatures**: Low correlation between attention and saliency indicates incomplete explanations
- **First experiments**: 1) Compute cross-attention vs saliency correlation, 2) Analyze layer-wise correlation patterns, 3) Compare multilingual vs monolingual alignment

## Open Questions the Paper Calls Out
### Open Question 1
- **Question:** Do the correlation trends between cross-attention and saliency maps hold for architectures based on Speech Foundation Models (SFMs) paired with Large Language Models (LLMs)?
- **Basis in paper:** [explicit] The authors state in Appendix F (Limitations) that they "focus on models trained from scratch but do not include architectures based on Speech Foundation Models (SFMs) paired with large language models (LLMs)."
- **Why unresolved:** SFMs and LLMs often utilize different scaling laws, attention mechanisms, or decoder configurations (e.g., no encoder-decoder attention in decoder-only setups) that could fundamentally alter the relationship between attention weights and input relevance.
- **What evidence would resolve it:** Replicating the correlation analysis (Pearson $\rho$ between cross-attention and SPES maps) on modern SpeechLLM architectures while strictly controlling for training data contamination.

### Open Question 2
- **Question:** Can auxiliary training losses that explicitly align cross-attention with saliency maps improve performance on downstream tasks?
- **Basis in paper:** [explicit] The Discussion section suggests that "similar training-time strategies–such as auxiliary losses that align attention with saliency–could further benefit downstream applications, enhancing both interpretability and task performance."
- **Why unresolved:** The current study is observational (measuring correlation) rather than interventional; it establishes a link but does not test whether enforcing this link via regularization acts as a beneficial inductive bias.
- **What evidence would resolve it:** Training new S2T models with a regularization term penalizing divergence between cross-attention and saliency, followed by an evaluation of timestamp estimation accuracy and translation quality.

### Open Question 3
- **Question:** Is the explanatory gap of cross-attention consistent across structurally diverse languages, such as tonal or morphologically rich languages?
- **Basis in paper:** [explicit] Appendix F notes, "Our multilingual analysis is limited to English (a Germanic language) and Italian (a Romance language), due to the high computational cost..."
- **Why unresolved:** Tonal languages convey lexical meaning via pitch (frequency features), which may be attended to differently than the temporal features dominant in English/Italian, potentially changing the alignment with frequency-agnostic cross-attention.
- **What evidence would resolve it:** Extending the experimental methodology to datasets covering typologically diverse languages (e.g., Mandarin for tone, Finnish for morphology) to compare correlation coefficients.

## Limitations
- Analysis relies on alignment-based evaluation without investigating causal relationships
- Depends on gradient-based saliency methods as ground truth, which have known limitations
- Focuses on specific model architectures and tasks, limiting generalizability

## Confidence
- Cross-attention explains ~50% of input relevance: Medium
- Cross-attention should complement formal explainability methods: High
- Correlation findings are robust across architectures: Medium

## Next Checks
1. Conduct ablation studies to establish causal relationships between cross-attention patterns and model outputs
2. Validate correlation findings using multiple explanation methods including perturbation-based approaches and human evaluation
3. Extend the analysis to diverse model architectures and tasks to assess generalizability of the "half explanation" finding