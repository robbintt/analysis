---
ver: rpa2
title: What is the Best Process Model Representation? A Comparative Analysis for Process
  Modeling with Large Language Models
arxiv_id: '2507.11356'
source_url: https://arxiv.org/abs/2507.11356
tags:
- process
- bpmn
- pmrs
- dataset
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares different process model representations (PMRs)
  for use with large language models (LLMs) in process modeling tasks. The authors
  evaluate nine PMRs across six criteria including token compactness, expressiveness,
  human readability, visualization, usability, and extensibility.
---

# What is the Best Process Model Representation? A Comparative Analysis for Process Modeling with Large Language Models

## Quick Facts
- arXiv ID: 2507.11356
- Source URL: https://arxiv.org/abs/2507.11356
- Reference count: 20
- Primary result: Mermaid is the most suitable PMR for LLM-based process modeling, while BPMN text excels at process model generation

## Executive Summary
This paper evaluates nine process model representations (PMRs) for large language model (LLM) use in process modeling tasks. The authors introduce the PMo Dataset with 55 process descriptions and corresponding models in all nine PMRs. Through qualitative assessment and quantitative experiments, they find Mermaid performs best overall for general process modeling, while BPMN text achieves superior results for process model generation tasks. The study reveals that LLMs tend to undergenerate process elements, particularly gateways, but this tendency is partially mitigated when using branching-structured PMRs.

## Method Summary
The authors evaluated nine PMRs across six criteria: token compactness, expressiveness, human readability, visualization, usability, and extensibility. They introduced the PMo Dataset containing 55 process descriptions with corresponding models in all nine PMRs. Quantitative experiments compared LLM-generated models against ground truth using element count analysis and process model element (PME) similarity with semantic matching. PMRs were tested using both graph-based (Graphviz, Mermaid, PlantUML, BPEL, RDF) and branching-structured (BPMN text, POWL code, JSON branches, XML branches) representations.

## Key Results
- Mermaid achieves highest overall PMo score and 93% token reduction compared to BPMN XML
- BPMN text performs best for process model generation with 0.54 PME similarity score
- LLMs undergenerate gateways significantly (-2.99 average deficit), but branching PMRs partially mitigate this issue
- POWL code achieves excellent token compactness but scores lowest on expressiveness (71% coverage) and has 40% formatting error rate

## Why This Works (Mechanism)

### Mechanism 1
Branching-structured PMRs partially mitigate LLM undergeneration of gateways by encoding control flow explicitly through nested structures rather than requiring models to infer gateway placement from edge relationships. This reduces cognitive burden for tracking parallel topologies. Evidence shows BPMN text has only -0.41 exclusive gateway deficit vs. -3.82 for Graphviz. Break condition: If branching PMRs show no improvement on models with complex nested loops or multiple start/end events.

### Mechanism 2
Token compactness correlates with PMo suitability but not necessarily PMG accuracy. Compact representations reduce context window pressure and generation costs, though compression can obscure structural cues for correct topology generation. Evidence: Mermaid achieves highest PMo score while BPMN text delivers best PMG results. Break condition: If larger models with extended context windows show no PMG accuracy difference between compact and verbose PMRs.

### Mechanism 3
Descriptive node identifiers improve gateway generation quality by reducing the need for LLM to maintain identifier-to-semantics mapping. Evidence: Graphviz achieves 0.60 gateway decision similarity vs. 0.16-0.18 for other graph-based PMRs due to use of descriptive node names. Break condition: If controlled experiments show no difference between descriptive and abstract identifiers with schema enforcement.

## Foundational Learning

- **BPMN 2.0 core elements**: Tasks, events, gateways, sequence flows - needed as all PMRs map to/from BPMN; quick check: Which elements cannot POWL code represent compared to full BPMN?
- **Tokenization and context window constraints**: Understanding why 93% reduction matters requires grasping LLM input limitations; quick check: Why might a 4000-token BPMN model be problematic for a model with 8K context when including prompt and examples?
- **Semantic similarity via embeddings**: PME similarity evaluation uses embedding-based semantic matching with 0.7 threshold; quick check: What failure mode does semantic matching address that exact string comparison would miss?

## Architecture Onboarding

- **Component map**: Input layer (process description) -> PMR selection -> Generation layer (LLM) -> Validation layer (PME extraction -> semantic matching -> element count comparison) -> Output (PMR model)
- **Critical path**: PMR choice drives token budget, expressiveness ceiling, and generation accuracy; prompt must include PMR-specific formatting examples; branching PMRs require pre-validation that input process fits representable topology
- **Design tradeoffs**: Mermaid offers best PMo suitability and direct visualization but lower PMG element similarity (0.48); BPMN text excels at PMG similarity (0.54) and human readability but lacks direct visualization and extensibility; POWL code provides excellent token compactness and executability but has 40% formatting errors and lowest expressiveness
- **Failure signatures**: Gateway undergeneration (50-65% fewer gateways in graph-based PMRs), high formatting error rates in code-based PMRs (40% for POWL code), conversion failures for branching PMRs (2/3 of BPMN models cannot be converted)
- **First 3 experiments**:
  1. Baseline PMG comparison: Generate 10 processes using Mermaid vs. BPMN text with identical prompts; measure element counts and PME similarity
  2. Gateway stress test: Create 5 synthetic processes with varying gateway complexity; compare generation accuracy across branching vs. graph PMRs
  3. Identifier naming ablation: Modify Mermaid prompt to use descriptive node names vs. abstract IDs; measure gateway decision similarity

## Open Questions the Paper Calls Out

- How does LLM performance vary when generating process models containing advanced elements such as swimlanes or data objects? The study focused on a reduced subset and future research should investigate richer models.
- To what extent does manual expert evaluation align with automated similarity metrics regarding the practical usability of generated models? The study relied on automated metrics rather than human assessment.
- Does a multi-expert review validate the authors' subjective grading of PMRs for criteria such as human readability and extensibility? The qualitative scoring lacks validation from a broader panel of experts.

## Limitations

- The PMo Dataset contains only 55 process descriptions, limiting statistical power and raising questions about generalizability from German-language processes
- Finding that branching PMRs mitigate gateway undergeneration may reflect selection bias as the dataset contains processes that fit branching topologies
- The study identifies gateway undergeneration but does not distinguish between exclusive, inclusive, and parallel gateways in the analysis

## Confidence

- **High confidence**: Comparative methodology for evaluating PMRs is sound with clear metrics and transparent scoring; token compactness measurements are objective and reproducible
- **Medium confidence**: Identification of Mermaid as most suitable PMR for general process modeling is well-supported, though individual task preferences may shift with different datasets or LLM models
- **Low confidence**: Proposed mechanisms explaining why branching PMRs mitigate gateway undergeneration and why descriptive identifiers improve generation are speculative without direct causal testing

## Next Checks

1. **Representation bias validation**: Create 20 synthetic process models spanning simple to complex topologies; test which PMRs can represent each topology; measure gateway generation accuracy only for representable models to control for topology bias

2. **Identifier naming experiment**: Design A/B test where same LLM generates models using identical PMRs except for identifier style (descriptive vs. abstract); use Graphviz-style prompts with both naming conventions across multiple runs to isolate identifier effect

3. **Gateway type disaggregation**: Re-analyze gateway undergeneration data by gateway type (exclusive, parallel, inclusive); create process descriptions specifically designed to require each gateway type; measure whether undergeneration patterns differ by type or PMR