---
ver: rpa2
title: Imitation Learning as Return Distribution Matching
arxiv_id: '2509.12026'
source_url: https://arxiv.org/abs/2509.12026
tags:
- policy
- expert
- return
- learning
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies risk-sensitive imitation learning (IL) where
  the goal is to match not only the expected return but also the full return distribution
  of an expert. The authors propose return distribution matching (RDM) formulated
  as minimizing the Wasserstein distance between the expert's and learner's return
  distributions.
---

# Imitation Learning as Return Distribution Matching

## Quick Facts
- **arXiv ID:** 2509.12026
- **Source URL:** https://arxiv.org/abs/2509.12026
- **Reference count:** 40
- **One-line primary result:** Introduces return distribution matching (RDM) for risk-sensitive imitation learning using non-Markovian policies, with RS-KT achieving lower sample complexity than RS-BC by exploiting transition knowledge.

## Executive Summary
This paper addresses the limitations of standard imitation learning (IL) which focuses on matching expected returns, by proposing return distribution matching (RDM) to capture the full return distribution of an expert. The authors demonstrate that standard Markovian policies are insufficient for this task and introduce a class of non-Markovian policies based on discretized cumulative rewards. Two algorithms are developed: RS-BC for unknown transitions and RS-KT for known transitions, both with polynomial sample complexity. Experiments confirm that non-Markovian policies outperform standard IL methods in matching return distributions, particularly for large horizons.

## Method Summary
The paper formulates risk-sensitive imitation learning as minimizing the Wasserstein distance between the expert's and learner's return distributions. It introduces non-Markovian policies that condition on both state and discretized cumulative reward, effectively augmenting the state space. RS-BC estimates a non-Markovian policy from expert trajectories using frequency counts in the augmented state space, while RS-KT formulates policy search as a linear program when transitions are known. Both algorithms achieve polynomial sample complexity, with RS-KT being more sample-efficient in large MDPs due to its ability to exploit transition knowledge.

## Key Results
- Demonstrates that Markovian policies cannot express certain risk profiles, with any Markovian policy having Wasserstein distance ≥0.5 from a non-Markovian expert in a specific MDP
- Proves polynomial sample complexity for both RS-BC and RS-KT algorithms
- Shows RS-KT achieves substantially lower sample complexity than RS-BC by exploiting dynamics information
- Experiments confirm non-Markovian policies outperform standard IL methods (BC, MIMIC-MD) in matching return distributions

## Why This Works (Mechanism)

### Mechanism 1: State-Augmentation for Non-Markovian Expressivity
- **Claim:** Standard Markovian policies are inherently insufficient to replicate the risk profile (return distribution) of a non-Markovian expert, even if they match the expected return.
- **Mechanism:** The method utilizes a policy class $\Pi(r)$ that conditions actions not just on the current state $s$, but also on the cumulative reward collected so far $G(\omega; r)$. This effectively converts the non-Markovian decision process into a Markovian one on an augmented state space $\mathcal{S} \times \mathcal{G}_r$.
- **Core assumption:** The expert's risk attitude is encoded in the history of rewards, specifically the running total.
- **Evidence anchors:**
  - [Section 3, Proposition 3.2]: Demonstrates a specific MDP where any Markovian policy has a Wasserstein distance $\ge 0.5$ from the expert.
  - [Abstract]: "After demonstrating the limited expressivity of Markovian policies for this task, we introduce an efficient... subclass of non-Markovian policies..."
- **Break condition:** If the expert's risk profile depends on the sequence of rewards rather than just the sum, or if the horizon is extremely long causing memory issues.

### Mechanism 2: Discretization for Tractable Sample Complexity
- **Claim:** Matching the return distribution over a continuous space is statistically intractable; discretizing the cumulative reward allows for polynomial sample complexity.
- **Mechanism:** The algorithm defines a $\theta$-covering of the return space ($Y_\theta$). It replaces the true reward $r$ with a discretized version $r_\theta$ where values snap to the grid. This bounds the size of the policy class ($|\Pi(r_\theta)|$) while controlling approximation error by $\theta$.
- **Core assumption:** The approximation error introduced by $\theta$-discretization is bounded linearly by the horizon $H$.
- **Evidence anchors:**
  - [Section 4.1, Lemma 4.2]: Proves $W(\eta^{\pi^{r_\theta}}, \eta^{\pi^E}) \le H\theta$, linking granularity directly to error.
  - [Section 4.1]: Explicitly constructs the discretized reward $r_\theta$ to reduce policy class size.
- **Break condition:** If $\theta$ is set too large, the approximation bias dominates; if $\theta$ is too small, sample complexity explodes.

### Mechanism 3: Distribution Matching via Linear Programming (RS-KT)
- **Claim:** When environment dynamics (transitions) are known, finding the optimal risk-sensitive policy reduces to a convex optimization problem (Linear Program).
- **Mechanism:** RS-KT constructs an augmented MDP where states are $(s, g)$. It then solves for an occupancy measure $d$ that minimizes the Wasserstein distance between the induced return distribution and the empirical expert distribution $\hat{\eta}$. This avoids the need to learn a behavior policy directly from data counts.
- **Core assumption:** The transition model $p$ is known exactly, and the LP solver finds the global optimum.
- **Evidence anchors:**
  - [Section 4.3, Eq. (10)]: Formulates the policy search as minimizing $W(\eta, \hat{\eta})$ subject to flow constraints.
  - [Abstract]: "RS-KT achieves substantially lower sample complexity... by exploiting dynamics information."
- **Break condition:** If the transition model is misspecified or unknown, RS-KT cannot be applied; RS-BC must be used instead.

## Foundational Learning

- **Concept: Wasserstein Distance**
  - **Why needed here:** This is the specific metric used to quantify the "distance" between the learner's return distribution and the expert's. It is chosen over Total Variation because it has polynomial sample complexity in this setting (TV requires exponential samples).
  - **Quick check question:** Why is Wasserstein distance preferred over Total Variation distance for estimating return distributions from finite samples?

- **Concept: Non-Markovian Policies in RL**
  - **Why needed here:** Understanding that optimal risk management (e.g., CVaR) usually requires memory. The learner must realize that looking at *only* the current state is insufficient to decide "how much risk to take."
  - **Quick check question:** In a stochastic environment, why might a policy need to know its "budget" (cumulative reward so far) to act optimally?

- **Concept: Augmented State MDPs**
  - **Why needed here:** The theoretical trick used to make the problem solvable. By adding the cumulative reward to the state vector, we can use standard Markovian DP/LP tools on a larger state space.
  - **Quick check question:** If the reward space is discretized to $k$ values and the horizon is $H$, what is the approximate size of the augmented state space?

## Architecture Onboarding

- **Component map:** Expert dataset $D_E$ and reward $r_E$ -> Preprocessor (discretize cumulative rewards) -> RS-BC (frequency counts) OR RS-KT (LP solver) -> Policy extractor -> Learner's policy
- **Critical path:** The estimation of the joint probability $P(s, G(\omega), a)$ (for RS-BC) or the return distribution $\hat{\eta}$ (for RS-KT). Errors in this estimation propagate directly into the Wasserstein distance.
- **Design tradeoffs:**
  - **$\theta$ (Granularity):** Low $\theta$ = High accuracy, High sample cost. High $\theta$ = Low accuracy, Low sample cost.
  - **Algorithm Selection:** RS-BC requires no environment model but scales poorly with $S, A$. RS-KT requires a known model but scales independently of $S, A$.
- **Failure signatures:**
  - **Persistent Bias:** If using Markovian policies (standard BC), the variance/tail risk will not decrease even with infinite data.
  - **"Blocky" Distributions:** If $\theta$ is too large, the learned return distribution will look artificially discrete and may miss narrow risk regions.
  - **LP Infeasibility (RS-KT):** If the expert data is inconsistent with the provided transition model $p$, the constraints in Eq. (10) may become infeasible.
- **First 3 experiments:**
  1. **Expressivity Test:** Compare RS-BC against standard BC on a gridworld with a known non-Markovian expert (e.g., the "safe" vs "risky" path counterexample). Verify that BC fails to match the variance.
  2. **Sensitivity to $\theta$:** Run RS-BC with $\theta \in \{0.1, 0.05, 0.01\}$. Plot the final Wasserstein distance to confirm the $H\theta$ error bound relationship.
  3. **Model Exploitation:** Compare RS-KT vs RS-BC in a large state space ($S=300$). Verify that RS-KT achieves lower error with fewer samples due to exploiting the transition model.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Are the sample complexity upper bounds provided for RS-BC and RS-KT information-theoretically tight?
- Basis: Explicit limitation noted in Section 7 regarding the lack of lower bounds to confirm the optimality of the derived rates (e.g., the $\tilde{O}(H^6)$ dependence).
- Why unresolved: The theoretical analysis is currently one-sided; without lower bounds, it is unknown if the polynomial rates are minimal or can be improved.
- What evidence would resolve it: Deriving matching lower bounds for the return distribution matching problem in the tabular setting.

### Open Question 2
- Question: Can a practical, computationally tractable algorithm be designed for the unknown-reward setting?
- Basis: Explicit call in Section 7, as Section 5 relies on an oracle to solve the min-max optimization over all rewards.
- Why unresolved: Solving the robust RDM objective over the entire class of non-Markovian policies is currently treated as a black-box oracle, and a concrete implementation is missing.
- What evidence would resolve it: An algorithm that approximates the solution to Eq. (12) without exhaustive enumeration or oracle access.

### Open Question 3
- Question: Can the proposed approach be extended to large or continuous state-action spaces using function approximation?
- Basis: Explicit limitation in Section 7 noting the focus on the tabular setting and the need for "scalable versions."
- Why unresolved: The discretization of cumulative rewards and the linear program in RS-KT scale polynomially with state/action counts, preventing direct application to complex environments.
- What evidence would resolve it: An algorithmic extension using neural networks or other function approximators that maintains the non-Markovian policy structure.

## Limitations

- The theoretical sample complexity bounds, while polynomial, may still be impractical for large horizons or fine reward discretization
- Experimental validation is limited to relatively small tabular MDPs and synthetic non-Markovian experts
- The synthetic expert generation method in Appendix E.1 lacks full specification

## Confidence

- **High Confidence:** The fundamental insight that Markovian policies cannot express certain risk profiles is well-supported by Proposition 3.2 and the accompanying counterexample. The discretization error analysis in Lemma 4.2 is mathematically sound.
- **Medium Confidence:** The sample complexity bounds for both RS-BC and RS-KT are theoretically derived, but their tightness and practical implications for larger problems remain unclear. The LP formulation for RS-KT assumes perfect transition knowledge.
- **Low Confidence:** The experiments comparing against MIMIC-MD (which requires access to the expert policy, not just trajectories) may not be entirely fair for a trajectory-only IL setting. The synthetic expert generation method in Appendix E.1 lacks full specification.

## Next Checks

1. **Expressivity Validation:** Implement the counterexample from Proposition 3.2 (a simple MDP where Markovian policies must have Wasserstein distance ≥0.5) to empirically verify that standard BC cannot match the expert's return distribution while RS-BC can.

2. **Scalability Test:** Scale the experiments to larger state spaces (S > 100) and compare RS-KT's performance advantage over RS-BC as the transition model knowledge becomes more valuable, verifying the theoretical claim about sample complexity scaling.

3. **Distribution Fidelity Analysis:** For various values of θ, plot the learned return distributions against the expert's true distribution to empirically verify the Hθ error bound relationship stated in Lemma 4.2, and determine the practical trade-off between discretization granularity and sample complexity.