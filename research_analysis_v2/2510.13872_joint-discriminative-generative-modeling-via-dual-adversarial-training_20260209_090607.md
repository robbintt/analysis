---
ver: rpa2
title: Joint Discriminative-Generative Modeling via Dual Adversarial Training
arxiv_id: '2510.13872'
source_url: https://arxiv.org/abs/2510.13872
tags:
- training
- generative
- adversarial
- data
- standard
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of achieving robust classification
  and high-fidelity generative modeling simultaneously within a single framework.
  The authors propose Dual Adversarial Training (DAT), which integrates adversarial
  training principles for both discriminative robustness and stable generative learning.
---

# Joint Discriminative-Generative Modeling via Dual Adversarial Training

## Quick Facts
- arXiv ID: 2510.13872
- Source URL: https://arxiv.org/abs/2510.13872
- Reference count: 40
- First EBM-based hybrid to scale to high-resolution datasets with high training stability, achieving state-of-the-art discriminative and generative performance on ImageNet 256×256.

## Executive Summary
This paper introduces Dual Adversarial Training (DAT), a framework that unifies robust classification and high-fidelity generative modeling within a single energy-based model. DAT addresses the instability of traditional JEM training by replacing SGLD sampling with adversarial training using PGD-generated contrastive samples and BCE loss. A two-stage training strategy decouples normalization requirements between discriminative and generative tasks. Experiments on CIFAR-10/100 and ImageNet demonstrate DAT achieves state-of-the-art performance in both domains while providing unique capabilities like robust counterfactual explanations.

## Method Summary
DAT employs a two-stage training approach where Stage 1 trains a robust classifier using standard adversarial training, then Stage 2 jointly optimizes discriminative and generative objectives. The generative component uses PGD attacks initialized from out-of-distribution data to generate contrastive samples, optimizing a BCE loss that pushes energy down for real data and up for contrastive samples. This replaces unstable SGLD-based JEM training. The discriminative component maintains adversarial robustness. Batch normalization statistics are frozen during Stage 2 to prevent generative training instability. This approach implicitly provides R1 regularization through adversarial training, eliminating the need for explicit gradient penalties.

## Key Results
- First EBM-based hybrid model to scale to high-resolution datasets (ImageNet 256×256) with high training stability
- Achieves state-of-the-art discriminative and generative performance on ImageNet 256×256
- Combines generative quality with adversarial robustness for applications like robust counterfactual explanations
- Functions as a competitive standalone generative model matching autoregressive methods and surpassing diffusion models

## Why This Works (Mechanism)

### Mechanism 1
Replacing Stochastic Gradient Langevin Dynamics (SGLD) with Adversarial Training (AT) for sampling stabilizes Joint Energy-Based Model (JEM) learning. Instead of using unstable MCMC sampling to estimate the data distribution gradient, this method uses Projected Gradient Descent (PGD) to generate "contrastive samples" from out-of-distribution (OOD) data. It optimizes a Binary Cross-Entropy (BCE) loss that forces the energy of real data down and the energy of these PGD-generated contrastive samples up.

### Mechanism 2
Adversarial training for classification implicitly regularizes the energy function, removing the need for explicit gradient penalties (R1). Adversarial training constrains the spectral norm of the network's Jacobian (operator norm regularization). Since the R1 gradient norm is bounded by this spectral norm, AT naturally keeps gradients bounded, preventing the explosion common in EBM training without constraining model expressiveness like explicit penalties do.

### Mechanism 3
A two-stage training strategy decouples the conflicting normalization requirements of discriminative and generative tasks. Batch Normalization (BN) is unstable during generative training because EBM sampling breaks the i.i.d. assumption of batch statistics. By training a robust classifier first (Stage 1) with BN enabled, then freezing the BN statistics (setting to eval mode) for joint training (Stage 2), the model retains the benefits of normalization for robustness without the instability during generation.

## Foundational Learning

- **Joint Energy-Based Models (JEM)**
  - Why needed here: This method modifies JEM. You must understand that JEM reinterprets a standard classifier's logits as an energy function $E(x, y)$ to unify classification and generation.
  - Quick check question: How does JEM derive the marginal probability $p(x)$ from a classifier's logits?

- **Projected Gradient Descent (PGD)**
  - Why needed here: PGD is used in two distinct ways here: 1) generating adversarial examples for robustness, and 2) generating samples for generative learning. Understanding the update rule (iterative gradient ascent + projection) is essential.
  - Quick check question: How does the PGD update step differ when trying to *maximize* classification loss versus *minimize* energy?

- **R1 Gradient Penalty**
  - Why needed here: The paper argues it avoids this penalty implicitly. You need to know that R1 penalizes the gradient of the discriminator (or energy function) with respect to the input to enforce Lipschitz continuity.
  - Quick check question: Why does "gradient explosion" in an EBM prevent it from generating valid samples?

## Architecture Onboarding

- **Component map:**
  Backbone (ResNet/ConvNeXt) -> Logits -> Energy Head (computes $E(x,y)$ and $E(x)$) -> PGD Attackers (discriminative and generative) -> Loss Aggregator (AT-CE + BCE)

- **Critical path:**
  1. Stage 1: Train standard Adversarial Training (AT) classifier. (Can use pre-trained weights).
  2. Freeze Norm: Set BatchNorm layers to `eval` mode (freeze running stats).
  3. Stage 2 (Joint): Batch real data -> PGD attack -> AT-CE Loss. Batch OOD data -> PGD attack (Ancestral Sampling) -> BCE Loss. Backprop total loss.

- **Design tradeoffs:**
  - **PGD Iterations ($T$)**: Increasing $T$ improves generative quality (FID) but degrades classification accuracy. This is a tunable knob.
  - **OOD Dataset**: Requires an auxiliary dataset (e.g., 80M Tiny Images or OpenImages). Performance scales with OOD diversity.

- **Failure signatures:**
  - **Loss NaN/Divergence in Stage 2**: Check if BatchNorm is frozen. If using LayerNorm (ConvNeXt), ensure it is *not* frozen.
  - **High FID / Poor Samples**: Check OOD data pipeline (is it distinct from training data?) or increase PGD steps $T$.
  - **Gradient Explosion**: Check if adversarial training on the discriminative side is active (it provides implicit R1).

- **First 3 experiments:**
  1. CIFAR-10 Reproduction: Train Stage 2 from a provided robust checkpoint. Plot the trade-off between Robust Accuracy and FID by varying $T$.
  2. Ablation on Implicit R1: Train a version without the AT component on the discriminative loss and log the R1 gradient norm to verify the explosion claim (referencing Appendix A.3).
  3. Counterfactual Generation: Use the trained model to perform targeted attacks on real images to visualize the "robust counterfactuals" described in Section 4.3.2.

## Open Questions the Paper Calls Out

### Open Question 1
Can a hybrid objective combining DAT's generative loss with RATIO's explicit OOD detection term simultaneously achieve state-of-the-art OOD detection and generative quality? Section C.3 identifies an OOD detection performance gap compared to RATIO and suggests that "developing a hybrid objective that combines our generative loss with RATIO's" could address this limitation.

### Open Question 2
Is it possible to modify the DAT formulation to learn relative data densities rather than just modeling the data support? Section A.2 states the model "cannot distinguish between common and rare examples" because it learns a constant marginal energy on the support, trading off full density estimation for stability.

### Open Question 3
Does the application of persistent Markov chains significantly improve training efficiency or sample diversity compared to the current finite-step PGD approach? The Conclusion suggests "improving training efficiency with persistent markov chains" as a specific direction for future work.

## Limitations
- Dependency on large, diverse out-of-distribution datasets (80M Tiny Images, OpenImages) which may not be readily available for all domains
- Curriculum learning schedule for gradually increasing PGD steps during training is mentioned but not fully specified
- Implicit R1 regularization mechanism requires empirical validation across different architectures

## Confidence
- **High Confidence**: The core DAT framework combining adversarial training with energy-based modeling, the two-stage training procedure with frozen BatchNorm statistics, and the empirical performance improvements on CIFAR-10/100 and ImageNet
- **Medium Confidence**: The theoretical claims about implicit R1 regularization through adversarial training and the mechanism by which PGD from OOD data effectively learns the data distribution
- **Medium Confidence**: The claimed advantages in adversarial robustness for counterfactual explanations, as this application is less extensively validated compared to standard metrics

## Next Checks
1. **Ablation Study on R1 Regularization**: Train a version without the adversarial component on the discriminative loss and measure the R1 gradient norm over training iterations to empirically verify the implicit regularization claim.
2. **OOD Dataset Sensitivity Analysis**: Systematically evaluate model performance using progressively smaller or less diverse OOD datasets to quantify the robustness of the generative quality to OOD data quality.
3. **Robustness to Counterfactuals**: Beyond qualitative examples, quantitatively assess the model's robustness to targeted adversarial attacks in the counterfactual generation task using metrics like attack success rate and perturbation magnitude.