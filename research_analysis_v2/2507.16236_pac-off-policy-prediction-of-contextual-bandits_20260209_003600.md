---
ver: rpa2
title: PAC Off-Policy Prediction of Contextual Bandits
arxiv_id: '2507.16236'
source_url: https://arxiv.org/abs/2507.16236
tags:
- prediction
- policy
- algorithm
- data
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses off-policy evaluation in contextual bandits,
  aiming to construct prediction intervals for a target policy's reward using data
  collected under a different behavior policy. The core method, PACOPP, builds on
  rejection sampling to create a subset of data resembling the target distribution,
  then applies a conformal prediction framework with modified thresholds to ensure
  probably approximately correct (PAC) coverage guarantees.
---

# PAC Off-Policy Prediction of Contextual Bandits

## Quick Facts
- **arXiv ID**: 2507.16236
- **Source URL**: https://arxiv.org/abs/2507.16236
- **Reference count**: 40
- **Primary result**: PACOPP constructs prediction intervals for a target policy's reward using rejection sampling and PAC conformal prediction, achieving probably approximately correct coverage with only modest interval length increases compared to marginally valid alternatives.

## Executive Summary
This paper addresses off-policy evaluation in contextual bandits, aiming to construct prediction intervals for a target policy's reward using data collected under a different behavior policy. The core method, PACOPP, builds on rejection sampling to create a subset of data resembling the target distribution, then applies a conformal prediction framework with modified thresholds to ensure probably approximately correct (PAC) coverage guarantees. Theoretical results include finite-sample validity with coverage converging to the target level and asymptotic efficiency approaching the oracle interval. Experiments show that PACOPP achieves desired coverage with only modest interval length increases compared to marginally valid alternatives, especially when high confidence is required.

## Method Summary
PACOPP constructs (ε, δ)-PAC prediction intervals for contextual bandits by first transforming observational data from behavior policy π_b to resemble target policy π_e through rejection sampling using importance weights. The method then applies split conformal prediction: quantile regression models are trained on the transformed data to estimate conditional reward quantiles, non-conformity scores are computed on a calibration set, and a PAC-valid threshold is determined using binomial distribution quantiles. The final prediction interval is formed by symmetrically expanding the base quantile estimates by the PAC threshold, ensuring that with probability at least 1-δ over datasets, the conditional coverage is at least 1-ε.

## Key Results
- PACOPP achieves finite-sample PAC validity: P[P(R_{n+1} ∈ Ĉ(S_{n+1}) | D] ≥ 1-ε] ≥ 1-δ
- Asymptotic efficiency approaches oracle intervals: L(Ĉ_τ̃(S_{n+1}) ∆ C^{oracle}(S_{n+1})) = o_P(1)
- Empirically achieves desired coverage with modest interval length increases compared to marginally valid alternatives
- Coverage degradation from unknown π_b estimation is bounded by O(√(log|Π|/n))

## Why This Works (Mechanism)

### Mechanism 1: Rejection Sampling for Distribution Alignment
Rejection sampling transforms data collected under behavior policy π_b into samples that appear drawn from target policy π_e. For each observation (S_i, A_i, R_i), compute importance weight w(s,a) = π_e(a|s)/π_b(a|s). Accept the sample with probability (1/B)·w(s,a) where B = sup_{(s,a)} w(s,a). Accepted pairs (S_i, R_i) are provably i.i.d. from P^{π_e}. This works because the rejection mechanism corrects for the distribution shift between policies. Core assumption: The weight ratio w(s,a) is bounded (B < ∞); π_b has support where π_e has support. Break condition: If B is very large (highly dissimilar policies), acceptance rate 1/B becomes very low, reducing effective sample size dramatically.

### Mechanism 2: PAC-Valid Threshold Selection via Binomial Quantiles
Using binomial distribution quantiles instead of empirical quantiles provides PAC coverage guarantees conditional on the calibration dataset. Standard split CP uses the (1-ϵ)-th empirical quantile of non-conformity scores. PACOPP uses threshold τ̃ = τ_{(M−k(M,ϵ,δ))} where k(M,ϵ,δ) = max{k : F_{Bin(M,ϵ)}(k) ≤ δ}. This ensures P[P(R_{n+1} ∈ Ĉ(S_{n+1}) | D] ≥ 1−ϵ] ≥ 1−δ. Core assumption: No ties among non-conformity scores (almost surely); calibration samples are conditionally i.i.d. given N_rs. Break condition: If M (calibration set size) is too small relative to δ, k(M,ϵ,δ) = −1 and the method becomes vacuously conservative.

### Mechanism 3: Quantile Regression Base with Symmetric Expansion
Learning conditional quantiles q̂_{ϵ_lo}(S) and q̂_{ϵ_up}(S) provides context-adaptive intervals that expand symmetrically based on non-conformity. Train quantile estimators on the rejection-sampled training set. Define candidate intervals Ĉ_τ(S) = [q̂_{ϵ_lo}(S) − τ, q̂_{ϵ_up}(S) + τ]. Non-conformity score τ_i = max{q̂_{ϵ_lo}(S_i) − R_i, R_i − q̂_{ϵ_up}(S_i)} measures how far outside the base interval the reward falls. Core assumption: Quantile estimators are consistent (converge to true conditional quantiles as training size increases). Break condition: Poor quantile estimation (model misspecification, insufficient data) leads to inefficient intervals, though PAC coverage is maintained.

## Foundational Learning

- **Importance Sampling / Propensity Scores**
  - Why needed here: The core challenge is distribution shift between behavior and target policies. Understanding how importance weights π_e/π_b enable off-policy correction is essential.
  - Quick check question: If π_b(a|s) = 0.1 and π_e(a|s) = 0.5, what is the importance weight? (Answer: 5.0)

- **Conformal Prediction Fundamentals**
  - Why needed here: PACOPP builds on split conformal prediction. You need to understand exchangeability, non-conformity scores, and how empirical quantiles provide finite-sample coverage.
  - Quick check question: In standard split CP with failure rate ϵ=0.1 and 100 calibration points, which quantile of non-conformity scores is used? (Answer: 90th percentile)

- **PAC Learning Framework**
  - Why needed here: The key innovation is PAC validity—coverage guarantees that hold with probability 1−δ over datasets, not just marginally. This distinction matters for safety-critical applications.
  - Quick check question: What does "(ϵ, δ)-PAC" mean? (Answer: With probability ≥1−δ over datasets, the coverage is ≥1−ϵ)

## Architecture Onboarding

- **Component map**: Observational data D -> Rejection sampling (π_b, π_e) -> D_rs -> Split (D_rs^1, D_rs^2) -> Quantile regression (D_rs^1) -> Non-conformity scores (D_rs^2) -> PAC threshold -> Output interval

- **Critical path**:
  1. Estimate or obtain π_b (if unknown, this introduces coverage degradation Δ_w)
  2. Compute B = sup w(s,a) — this determines acceptance rate
  3. Run rejection sampling — monitor N_rs; if too small, method fails
  4. Train quantile regressors — quality affects efficiency, not validity
  5. Compute PAC threshold — requires binomial CDF computation

- **Design tradeoffs**:
  - **Known vs. unknown π_b**: Unknown π_b requires estimation, adding coverage degradation Δ_w (Theorem 6). With MLE estimation, degradation is O(√(log|Π|/n)).
  - **Calibration ratio γ**: Larger γ → more calibration data → tighter confidence bounds C√n in Theorems 2-3, but less training data for quantile estimation.
  - **Confidence δ**: Lower δ → stricter PAC guarantee → larger intervals. Experiments show interval length grows "only modestly" even at δ=0.01.

- **Failure signatures**:
  - **Very low N_rs**: If policies are highly dissimilar (B large), rejection sampling yields few samples. Symptom: extremely wide intervals or vacuous coverage.
  - **Quantile estimator failure**: Poor q̂ estimates cause inefficient (overly wide) intervals, but PAC coverage still holds.
  - **Ties in non-conformity scores**: Theoretical guarantees assume no ties; in practice, add small noise or use randomized tie-breaking.

- **First 3 experiments**:
  1. **Validate PAC coverage**: Fix ϵ=0.2, δ=0.1. Run 1000 simulations, each with 10000 test points. Verify that ≥90% of simulations achieve ≥80% coverage. Compare against theoretical bounds in Theorem 2.
  2. **Compare with COPP baseline**: Replicate the synthetic experiment (Section 4.2) with Gaussian mixture rewards. Measure: (a) empirical coverage, (b) average interval width. PACOPP should match or exceed COPP in coverage reliability.
  3. **Stress test with unknown π_b**: Vary the policy estimation error Δ_w by controlling training data for π̂_b. Verify coverage degradation matches Theorem 6 bound (coverage should be ≥1−ϵ−Δ_w with probability ≥1−δ).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the PACOPP framework be effectively extended to general sequential decision-making scenarios, such as Markov Decision Processes (MDPs)?
- Basis in paper: [explicit] The Conclusion states that "extending PACOPP to general sequential decision-making scenarios beyond contextual bandits would be an promising direction for future research."
- Why unresolved: The current method relies on i.i.d. assumptions inherent in contextual bandits; temporal dependencies and horizon lengths in full RL settings introduce complexity not addressed by the current rejection sampling mechanism.
- What evidence would resolve it: A modified algorithm that maintains PAC validity in sequential settings with theoretical guarantees on coverage and efficiency comparable to the bandit setting.

### Open Question 2
- Question: How can data utilization be improved to reduce the conservativeness of the prediction intervals caused by rejection sampling?
- Basis in paper: [explicit] The Conclusion notes that addressing distribution shift via rejection sampling comes "with the cost of reduction of available sample size... Enhancing the data utilization in this setting remains a challenge."
- Why unresolved: The rejection sampling step discards data to correct distribution shift, directly reducing the calibration set size and increasing interval width.
- What evidence would resolve it: A modified sampling or weighting strategy that uses a larger proportion of the offline data while retaining PAC guarantees, resulting in empirically shorter intervals.

### Open Question 3
- Question: Can rigorous error bounds be established for the coverage degradation term when using non-MLE algorithms to estimate the behavior policy?
- Basis in paper: [explicit] Section 3.3 notes that for general learning algorithms, "theoretical guarantees for bounding the error term (12) remain unknown," limiting Theorem 7 to the MLE case.
- Why unresolved: The coverage guarantee depends on bounding the weight estimation error ($\Delta_w$), which is currently only quantified for the specific parametric case of MLE within a finite policy class.
- What evidence would resolve it: Derivation of a high-probability bound for $\Delta_w$ applicable to non-parametric or black-box estimators (e.g., neural networks) commonly used in practice.

## Limitations

- **Policy estimation bottleneck**: When π_b is unknown, estimating it from data introduces coverage degradation Δ_w = O(√(log|Π|/n)), which may be significant in practice despite being asymptotically negligible.
- **Rejection sampling sample size**: The method's viability critically depends on N_rs being sufficiently large. With highly dissimilar policies (large B), acceptance rates become impractically low, potentially rendering the method vacuous despite theoretical validity.
- **Quantile estimator quality**: While PAC guarantees hold regardless of quantile estimator quality, poor estimation leads to inefficient intervals. The paper assumes consistent estimators but doesn't analyze the finite-sample efficiency gap.

## Confidence

**High Confidence**: Rejection sampling correctly transforms the distribution (Theorem 1); PAC threshold selection via binomial quantiles provides the claimed coverage guarantees (Theorem 3); asymptotic efficiency approaches oracle bounds (Theorem 5).

**Medium Confidence**: Coverage degradation from policy estimation (Theorem 6) is theoretically bounded but may be significant in practice; the assumption of no ties among non-conformity scores is crucial but not robust to practical violations.

**Low Confidence**: The trade-off between calibration ratio γ and training efficiency is not thoroughly characterized; the impact of quantile estimator choice (neural network architecture, etc.) on finite-sample performance is not explored.

## Next Checks

1. **Finite-Sample Policy Estimation Impact**: Systematically vary the amount of data available for estimating π̂_b and measure the actual coverage degradation Δ_w, comparing against the theoretical bound.

2. **Rejection Sampling Robustness**: Test PACOPP on pairs of policies with varying degrees of dissimilarity (B values). Quantify the relationship between B, N_rs, and both coverage and interval width.

3. **Quantile Estimator Sensitivity**: Compare PACOPP performance using different quantile estimation approaches (linear models vs. neural networks vs. kernel methods) on the same datasets, measuring both coverage reliability and interval efficiency.