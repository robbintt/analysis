---
ver: rpa2
title: Enabling Off-Policy Imitation Learning with Deep Actor Critic Stabilization
arxiv_id: '2511.07288'
source_url: https://arxiv.org/abs/2511.07288
tags:
- learning
- policy
- target
- expert
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the sample inefficiency problem in adversarial
  imitation learning by introducing an off-policy algorithm with deep actor-critic
  stabilization. The authors replace GAIL's on-policy TRPO with an off-policy actor-critic
  framework (inspired by DDPG) that leverages a replay buffer for sample reuse.
---

# Enabling Off-Policy Imitation Learning with Deep Actor Critic Stabilization

## Quick Facts
- arXiv ID: 2511.07288
- Source URL: https://arxiv.org/abs/2511.07288
- Reference count: 22
- This work achieves expert-level performance on BipedalWalker-v2 in just 200,000 timesteps using off-policy imitation learning.

## Executive Summary
This paper addresses the sample inefficiency of adversarial imitation learning by introducing an off-policy algorithm with deep actor-critic stabilization. The authors replace GAIL's on-policy TRPO with an off-policy actor-critic framework (inspired by DDPG) that leverages a replay buffer for sample reuse. They further stabilize training through clipped double Q-learning and eliminate the separate discriminator network by incorporating reward learning directly into the critic objective using Jensen-Shannon divergence. The algorithm achieves expert-level performance on BipedalWalker-v2 in just 200,000 timesteps—significantly faster than the on-policy GAIL baseline—while learning from demonstrations without access to environment rewards.

## Method Summary
The method combines off-policy actor-critic learning with imitation learning objectives. It uses a twin-critic architecture with clipped double Q-learning (inspired by TD3) and incorporates Jensen-Shannon divergence into the critic loss to implicitly learn rewards without a separate discriminator network. The actor is updated via policy gradients using the reparameterization trick with noise input, and training leverages a replay buffer containing both expert demonstrations and agent experiences. The critic targets are computed using the minimum of two target critics to mitigate overestimation bias.

## Key Results
- Achieves expert-level performance on BipedalWalker-v2 (return ~300) within 200,000 timesteps
- Outperforms on-policy GAIL baseline which requires 2+ million timesteps
- Successfully learns from demonstrations without access to environment rewards

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Replacing on-policy policy optimization (TRPO) with off-policy actor-critic improves sample efficiency by enabling data reuse.
- **Mechanism:** An off-policy algorithm stores transitions in a replay buffer, allowing multiple gradient updates from the same environment interaction. The policy gradient uses the reparameterized form: ∇θJ(θ) = E[∇aQ(s,a)|a=πθ(s,z) · ∇θπθ(s,z)], where noise z provides stochasticity while maintaining differentiability through the actor.
- **Core assumption:** The policy improvement step does not strictly require on-policy data; the Q-function can provide meaningful gradients even when trained on off-policy distribution.
- **Evidence anchors:**
  - [abstract] "leverages a replay buffer for sample reuse"
  - [Section 3.2] "By using an off-policy method, we can leverage a replay buffer... significantly increasing the data available for gradient updates"
  - [Section 4.1] Details the off-policy actor-critic formulation with deterministic policy reparameterization
  - [corpus] Related work "Unlocking the Potential of Soft Actor-Critic for Imitation Learning" similarly explores off-policy actor-critic for IL, suggesting convergent validation of this direction
- **Break condition:** If distribution shift between behavior policy and current policy becomes too large, off-policy bias may destabilize learning.

### Mechanism 2
- **Claim:** Clipped Double Q-learning stabilizes training by mitigating overestimation bias in the critic.
- **Mechanism:** Two independent critic networks Qν1 and Qν2 are maintained with corresponding target networks. The Bellman target uses the minimum: Qtarget(s',a') = min(Qν1,target, Qν2,target). Both critics are trained to regress to this conservative target via soft updates.
- **Core assumption:** Q-learning with neural function approximation exhibits systematic overestimation that can be reduced by taking a minimum over independent estimates.
- **Evidence anchors:**
  - [Section 5] Explicitly adopts "Clipped Double Q-learning technique from TD3 (Fujimoto et al., 2018)"
  - [Section 5] "This prevents a single network from developing an overly optimistic estimate"
  - [corpus] Weak direct corpus validation; stabilization techniques are standard in off-policy RL but specific JSD + TD3 combination is novel here
- **Break condition:** If both critics converge to similar overestimates, the minimum provides no benefit; underestimation bias could slow learning.

### Mechanism 3
- **Claim:** Embedding reward learning directly into the critic objective eliminates the discriminator network and reduces instability from multi-network interactions.
- **Mechanism:** The critic is trained to minimize Jensen-Shannon divergence between its output distribution and a target distribution that implicitly encodes the reward: expert pairs target Pν(πE) = 1, non-expert pairs target Pν(πE) = 0.5. The combined loss: L = Eexpert[DJS(Pν || P'target)] + Epolicy[DJS(Pν || P'target/2)].
- **Core assumption:** The optimal reward function can be represented as r* = 1 for expert and r* = 0.5 for non-expert, which maximizes entropy for uncertain pairs while maintaining discrimination.
- **Evidence anchors:**
  - [Section 4.4] "By embedding the optimal reward values directly into the critic's target distributions, the critic update step now implicitly learns the reward function"
  - [Equation 27] Shows the final JSD-based objective without discriminator parameters ω
  - [corpus] No direct corpus validation of this specific JSD-based reward integration
- **Break condition:** If the binary expert/non-expert distinction is insufficient for dense reward shaping, the policy may fail to distinguish fine-grained quality differences.

## Foundational Learning

- **Concept:** Actor-Critic Methods (specifically DDPG/TD3)
  - **Why needed here:** The algorithm builds directly on DDPG's off-policy actor-critic structure and TD3's clipped double Q-learning. Without understanding how Q(s,a) provides gradients to the actor via the chain rule, the reparameterization trick in Equation 13 will be opaque.
  - **Quick check question:** Can you explain why DDPG uses a deterministic policy but still achieves exploration?

- **Concept:** Generative Adversarial Imitation Learning (GAIL)
  - **Why needed here:** The paper positions itself as fixing GAIL's sample inefficiency. Understanding the discriminator-generator game (Equation 10) and why it requires on-policy collection is essential to appreciate what's being replaced.
  - **Quick check question:** In GAIL, what role does the discriminator play in generating the reward signal?

- **Concept:** Jensen-Shannon Divergence
  - **Why needed here:** The critic loss is formulated as JSD minimization (Equation 24, 27). JSD is symmetric and bounded unlike KL divergence, which matters for training stability.
  - **Quick check question:** What property of JSD makes it preferable to KL divergence for this critic objective?

## Architecture Onboarding

- **Component map:** 
  - Actor πθ(s,z) → tanh-bounded action → Environment
  - Environment → (s, a, s', done) → Agent replay buffer Bβ
  - Expert buffer BE (static)
  - Two critics Qν1, Qν2 → (s, a) → log-probability
  - Two target critics → Soft updates from Qν1, Qν2

- **Critical path:**
  1. Sample action a ~ πθ(s, z), execute, store transition in Bβ
  2. Sample mini-batches from both BE and Bβ
  3. Compute target Q using minimum of both target critics on next states
  4. Update both critics by minimizing JSD loss (Equation 27)
  5. Update actor via policy gradient using Qν1 (Equation 13)
  6. Soft-update target networks: νi,target ← τνi + (1-τ)νi,target

- **Design tradeoffs:**
  - Off-policy efficiency vs. potential bias from stale buffer data
  - Bounded tanh actor eliminates wasted clipped actions but constrains exploration distribution
  - JSD-based critic removes discriminator instability but fixes reward structure to binary expert/non-expert distinction
  - Two critics add memory/compute cost but provide stability

- **Failure signatures:**
  - High variance in episodic returns during training (explicitly noted in Future Work)
  - Critic Q-values diverging or collapsing to extremes (suggests target network τ too large or JSD loss unstable)
  - Policy failing to improve despite stable Q-values (suggests actor gradient flow issue or exploration insufficient)
  - Expert/non-expert distinction blurring (suggests buffer contamination or expert data quality issues)

- **First 3 experiments:**
  1. **Sanity check on BipedalWalker-v2:** Train with access to environment reward (as auxiliary signal) to verify actor-critic components work before testing imitation-only mode. Compare to standard DDPG/TD3 baseline.
  2. **Buffer composition ablation:** Vary the ratio of expert to policy samples in each mini-batch. The paper uses equal sampling but does not ablate this. Test if imbalance affects convergence speed or final performance.
  3. **Critic-only validation:** Freeze the actor and train only the critic on fixed expert/policy data to verify the JSD loss converges and produces meaningful Q-value distinctions between expert and random actions.

## Open Questions the Paper Calls Out

- **Question:** Can hybrid on-policy/off-policy updates effectively mitigate the high variance in policy returns observed during training?
  - **Basis in paper:** [explicit] The authors state that the results show high variance and propose exploring hybrid algorithms to combine stable on-policy dynamics with off-policy sample efficiency.
  - **Why unresolved:** The current implementation is purely off-policy, which often suffers from high variance, and the proposed hybrid approach remains untested.
  - **What evidence would resolve it:** Empirical comparison of variance metrics and convergence speed between the current method and a hybrid update rule.

- **Question:** Can the algorithm be extended to learn effectively from suboptimal demonstrations by incorporating environment rewards?
  - **Basis in paper:** [explicit] The authors identify learning from suboptimal demonstrations as an "important and orthogonal extension," proposing the integration of environment rewards to provide ground truth.
  - **Why unresolved:** The current formulation assumes the provided demonstrations are near-optimal and relies solely on occupancy measure matching without environment rewards.
  - **What evidence would resolve it:** Successful policy learning on tasks where the demonstration dataset contains noisy or suboptimal trajectories, utilizing a combined reward signal.

- **Question:** Does the removal of the explicit discriminator and the use of JSD-based critic targets scale to high-dimensional continuous control tasks?
  - **Basis in paper:** [inferred] The evaluation is restricted to the BipedalWalker-v2 environment, leaving the algorithm's efficacy on more complex benchmarks (e.g., MuJoCo Humanoid) unknown.
  - **Why unresolved:** Stabilization techniques that work on lower-dimensional tasks often fail to scale without tuning, and the "value learning without reward inference" approach is only validated on one task.
  - **What evidence would resolve it:** Benchmark results on standard high-dimensional continuous control suites (e.g., OpenAI Gym MuJoCo tasks).

## Limitations
- The binary expert/non-expert reward structure (r=1 for expert, r=0.5 for non-expert) may be insufficient for complex tasks requiring fine-grained reward shaping.
- Network architecture details and hyperparameters remain unspecified, creating potential reproducibility barriers.
- No ablation studies validate whether improvements come specifically from the off-policy framework, clipped double Q-learning, or JSD-based reward integration.

## Confidence

**High confidence**: The core claim that off-policy actor-critic improves sample efficiency over on-policy GAIL is well-supported by the 200k timestep result versus GAIL's 2M+ timesteps. This mechanism (data reuse via replay buffer) is theoretically sound and empirically validated in prior RL work.

**Medium confidence**: The effectiveness of clipped double Q-learning for stabilization in this specific JSD-based framework is reasonable but not extensively validated. While TD3-style stabilization is standard, the interaction with JSD objectives is novel and could have unexpected dynamics.

**Medium confidence**: The elimination of the separate discriminator network through JSD integration is elegant but untested in ablation. The binary reward structure may work for BipedalWalker-v2 but could fail on tasks requiring more nuanced reward signals.

## Next Checks

1. **Ablation of core components**: Run variants with (a) on-policy policy updates but same JSD critic, (b) clipped double Q-learning removed, (c) discriminator restored instead of JSD integration. Compare sample efficiency and final performance to isolate which mechanisms drive improvements.

2. **Reward structure validation**: Test alternative reward formulations beyond the binary expert/non-expert split—e.g., continuous density ratios or multi-class expert quality levels. Verify whether the JSD-based approach can handle more granular reward signals.

3. **Distributional shift analysis**: Monitor the policy distribution shift from the expert demonstrations over training. Quantify how much the off-policy actor-critic relies on the expert buffer versus agent-generated data, and test buffer composition effects (e.g., 1:1 vs 10:1 expert:policy sampling ratios).