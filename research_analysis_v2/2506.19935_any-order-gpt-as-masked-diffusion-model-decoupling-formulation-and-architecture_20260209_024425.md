---
ver: rpa2
title: 'Any-Order GPT as Masked Diffusion Model: Decoupling Formulation and Architecture'
arxiv_id: '2506.19935'
source_url: https://arxiv.org/abs/2506.19935
tags:
- decoder-only
- diffusion
- ao-gpt
- training
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the architectural bias in masked diffusion
  models (MDMs), which are typically encoder-only, while autoregressive (AR) models
  are decoder-only. This design difference makes fair comparisons difficult, as differences
  in performance may stem from either the modeling formulation or the architecture.
---

# Any-Order GPT as Masked Diffusion Model: Decoupling Formulation and Architecture

## Quick Facts
- **arXiv ID**: 2506.19935
- **Source URL**: https://arxiv.org/abs/2506.19935
- **Reference count**: 40
- **Primary result**: Decoder-only MDMs achieve ~25× faster generation speed with comparable perplexity to encoder-only MDMs when using appropriate temperature annealing.

## Executive Summary
This paper addresses the architectural bias in masked diffusion models (MDMs), which are typically encoder-only, while autoregressive (AR) models are decoder-only. This design difference makes fair comparisons difficult, as differences in performance may stem from either the modeling formulation or the architecture. To address this, the authors propose AO-GPT, a decoder-only model that implements the MDM (or Any-Order AR) formulation, allowing for a fairer comparison. They find that decoder-only MDMs converge slower than AR models initially, likely because many permutations are less informative than the natural left-to-right order in language. Incorporating a small fraction (10%) of left-to-right data improves both left-to-right and any-order performance. The authors also compare encoder-only and decoder-only MDMs, showing that encoder-only models model fewer conditional probabilities but can perform better without context-order ensembling. Decoder-only models, however, achieve about 25× faster generation speed and comparable perplexity with appropriate temperature annealing, despite modeling a larger conditional probability space.

## Method Summary
The authors propose AO-GPT, a decoder-only implementation of masked diffusion models that decouples the Any-Order AR formulation from the decoder architecture. The model uses adaptive layer normalization (adaLN) conditioned on target positions, exponential moving average (EMA) weight averaging, and efficient sampling (Lemma 1) that combines Bernoulli masking decisions with parallel generation attention masks. Training uses a hybrid order distribution: 90% uniform permutations and 10% left-to-right order. The decoder-only architecture leverages KV-caching for efficient generation, reducing complexity from O(n²) to O(n) through two-stage sampling and cached context representations.

## Key Results
- Decoder-only MDMs converge slower than AR models initially, but 10% left-to-right data improves both convergence and final perplexity
- Encoder-only MDMs model n·2^(n-1) order-invariant conditionals vs. decoder-only's e·n! order-dependent conditionals, but ensembling closes the performance gap
- Decoder-only MDMs achieve ~25× faster generation speed than encoder-only MDMs through KV-caching and efficient sampling
- Appropriate temperature annealing (T=0.7, Top-p=0.95) reduces generation perplexity gap from 2-3× to comparable levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Uniform averaging over all token permutations slows convergence relative to left-to-right ordering in language modeling.
- Mechanism: The AO-AR objective averages over n! permutations. Many permutations are less informative because language exhibits inherent left-to-right sequential structure; random orders dilute the learning signal by forcing the model to learn representations effective across orders that lack linguistic coherence.
- Core assumption: Natural language has stronger left-to-right statistical dependencies than reverse or random orders.
- Evidence anchors:
  - [abstract] "many permutations appear less informative compared to the language's inherent left-to-right structure"
  - [section 3] Finding 1: "Any-Order GPT converges significantly slower in the initial training stages compared to its standard GPT counterpart"
  - [corpus] Related work "Improving Discrete Diffusion Unmasking Policies" confirms MDM performance is highly sensitive to unmasking order choice.

### Mechanism 2
- Claim: Encoder-only MDMs model a smaller conditional probability space than decoder-only MDMs, but ensembling over context orders can close the performance gap.
- Mechanism: Encoder-only attention is permutation-invariant to context tokens, computing p_θ(x_j|x_E) regardless of input order. Decoder-only causal attention produces order-dependent predictions p_θ(x_j|x_E, σ_E). The decoder thus must learn ~e·n! distinct conditionals vs. n·2^(n-1) for encoder. Ensembling over context permutations marginalizes order.
- Core assumption: The perplexity gap between architectures stems primarily from order-sensitivity, not fundamental capacity differences.
- Evidence anchors:
  - [section 4.1] Finding 4: exact counts of conditional probability spaces
  - [section 4.2] Figure 3: "ensemble on order context fills the gap"
  - [corpus] "Understanding the Reversal Curse Mitigation" analyzes attention dynamics in MDMs vs AR models.

### Mechanism 3
- Claim: Decoder-only MDMs achieve ~25× faster generation than encoder-only MDMs through KV-cache and efficient two-stage sampling.
- Mechanism: Encoder-only MDMs require T iterations with O(n) full-attention passes each. Decoder-only leverages KV-cache avoiding recomputation and Lemma 1 efficient sampling—first sample Bernoulli for mask/unmask decision, only compute q_{0|t}(·|x_t) for positions to unmask.
- Core assumption: Number of generation steps T scales with sequence length n.
- Evidence anchors:
  - [section 4.3] Finding 6: complexity analysis; Finding 7.1: "AO-GPT can achieve 25× speedup on generation compared with SEDD"
  - [section 4.3] Figure 4: empirical timing comparison showing ~25× speedup at 1024 tokens
  - [corpus] "Esoteric Language Models" notes MDMs "lack key inference-time efficiency features, most notably KV caching."

## Foundational Learning

- **Autoregressive factorization and chain rule for probability**
  - Why needed here: The paper reformulates MDMs as Any-Order AR models; understanding that -log p(x) = Σ_i -log p(x_i|x_{<i}) underlies both standard AR and AO-AR objectives.
  - Quick check question: Can you explain why the entropy lower bound H(x) is identical for any factorization order?

- **Causal vs. bidirectional attention masks**
  - Why needed here: The architectural comparison hinges on decoder-only (causal, each token attends only to prior positions in sequence order) vs. encoder-only (full attention to all positions).
  - Quick check question: Draw the attention mask for a 4-token sequence under causal attention vs. full attention.

- **ELBO (Evidence Lower Bound) for diffusion training**
  - Why needed here: The MDM objective L_MDM is derived as an ELBO on data likelihood; understanding this connects diffusion training to probabilistic modeling.
  - Quick check question: Why does optimizing ELBO approximate maximum likelihood?

## Architecture Onboarding

- **Component map**: Standard GPT decoder backbone -> Target position encoding (concatenation or AdaLN) -> Parallel generation attention mask -> Efficient sampling (Lemma 1) -> KV-cache for generation

- **Critical path**:
  1. Implement standard decoder-only GPT backbone
  2. Add target position encoding input (concatenated or via AdaLN)
  3. Modify training to sample permutation σ per sequence; compute loss on reordered prediction targets
  4. Implement EMA weight tracking
  5. For inference: implement KV-cache + Lemma 1 sampling + parallel attention mask

- **Design tradeoffs**:
  - Encoder-only vs. decoder-only: Encoder gives lower perplexity without ensembling; decoder gives 25× faster inference. Choose based on inference budget vs. quality requirements.
  - Uniform vs. L2R-biased order distribution: 90% uniform + 10% L2R improves both convergence and final perplexity; pure uniform slower but maximally flexible.
  - Ensembling at inference: 8-64 ensemble passes close perplexity gap but adds compute; trade quality vs. speed.

- **Failure signatures**:
  - Slow convergence in early training: Likely using pure uniform permutation without L2R bias or missing EMA
  - Higher perplexity than expected (decoder-only): Check if using appropriate temperature annealing (Top-p 0.95, Temp 0.7); without annealing, generation perplexity 2-3× higher
  - Memory issues at long sequences: KV-cache grows linearly; encoder-only may be preferable if memory-constrained

- **First 3 experiments**:
  1. **Baseline convergence comparison**: Train identical decoder-only architectures with (a) pure L2R order, (b) pure any-order uniform, (c) 90/10 hybrid. Measure loss curves to reproduce Findings 1 and 3. Expected: hybrid within 5-10% of L2R final loss, uniform 15-20% higher.
  2. **Ensemble ablation on perplexity**: Evaluate decoder-only model with ensemble sizes M ∈ {1, 2, 4, 8, 16, 32, 64}. Plot perplexity vs. M (Figure 3 replication). Expected: diminishing returns after M=16, approaching encoder-only baseline.
  3. **Generation speed benchmark**: Measure wall-clock time for 1024-token generation with (a) encoder-only MDM (SEDD), (b) decoder-only with KV-cache only, (c) decoder-only with KV-cache + Lemma 1. Expected: (c) ~25× faster than (a).

## Open Questions the Paper Calls Out

- **Why does incorporating 10% left-to-right data improve both L2R and any-order performance?**
  - Question: Why does incorporating a small fraction (10%) of left-to-right training data improve performance not only on left-to-right evaluation but also on any-order evaluation in decoder-only MDMs?
  - Basis in paper: [explicit] The authors state in Section 3: "This phenomenon itself warrants dedicated investigation beyond our initial exploration, as a comprehensive understanding of this benefit is a promising direction for future work."
  - Why unresolved: The paper reports the empirical finding but does not provide theoretical or mechanistic explanations for why structured L2R patterns enhance generalization across arbitrary permutations.
  - What evidence would resolve it: Ablation studies varying the L2R fraction systematically; analysis of learned representations to identify what inductive biases L2R data introduces.

- **Do findings generalize to larger model scales?**
  - Question: Do the findings comparing decoder-only and encoder-only MDMs generalize to significantly larger model scales (e.g., 7B+ parameters)?
  - Basis in paper: [explicit] The limitations section states: "Our experiments were conducted on models of up to medium size (e.g., 350M parameters). Whether these observations generalize to significantly larger computational scales remains an open question."
  - Why unresolved: Scaling behavior in language models can introduce emergent properties; the 25× speedup and perplexity trade-offs observed at 350M may shift at billion-parameter scales.
  - What evidence would resolve it: Training and evaluating AO-GPT and encoder-only MDM baselines at 1B, 7B, and larger scales.

- **What is the optimal non-uniform order distribution?**
  - Question: What is the optimal non-uniform distribution over token prediction orders for masked diffusion language models?
  - Basis in paper: [explicit] The conclusion states: "This suggests future MDM research could benefit from exploring non-uniform order distributions, balancing modeling power with data alignment and efficiency."
  - Why unresolved: The paper demonstrates that uniform sampling over all permutations is suboptimal, but does not characterize what order distribution optimally balances flexibility with language's inherent left-to-right structure.
  - What evidence would resolve it: Systematic search over order distributions (e.g., weighting L2R more heavily, block-wise orders, or learned order distributions); evaluation of convergence speed, final perplexity, and generation quality.

## Limitations
- The 25× speedup claim is measured against SEDD specifically, not all encoder-only MDMs
- The performance improvement from 10% left-to-right data is empirical but lacks theoretical explanation
- The parallel generation mask is conceptually described but implementation details are sparse
- AdaLN architecture details are underspecified, making exact replication challenging

## Confidence

- **High Confidence**: Decoder-only MDMs are ~25× faster than encoder-only MDMs (Finding 7.1) - supported by direct complexity analysis and empirical timing
- **High Confidence**: 10% L2R data mixing improves both convergence and final perplexity (Finding 3) - replicated across architectures
- **Medium Confidence**: AO-AR convergence is slower than standard AR due to uniform permutation averaging (Finding 1) - strong empirical evidence but mechanistic explanation assumes linguistic sequential bias without formal proof
- **Medium Confidence**: Encoder-only MDMs model fewer conditionals than decoder-only (Finding 4) - exact counts provided but assumes specific definition of conditional space
- **Low Confidence**: Context-order ensembling fully closes the perplexity gap between encoder-only and decoder-only MDMs (Finding 5) - shown empirically but computational cost makes this impractical
- **Low Confidence**: AdaLN outperforms naive PE concatenation (Finding 5) - statistically significant but MLP architecture underspecified

## Next Checks

1. **Convergence sensitivity analysis**: Systematically vary the L2R data fraction from 0% to 100% in 10% increments to identify the optimal trade-off point between convergence speed and any-order flexibility. Expected: peak performance around 5-15% L2R data.

2. **Scaling study on order sensitivity**: Train decoder-only AO-GPT on tasks with varying sequential bias (e.g., natural language vs. biological sequences) to test whether the uniform permutation disadvantage holds across domains. Expected: significant convergence gap for language, minimal gap for permutation-invariant tasks.

3. **KV-cache memory scaling analysis**: Measure actual memory usage of decoder-only MDMs at sequence lengths 1K, 2K, 4K, 8K to determine when memory overhead becomes prohibitive compared to encoder-only. Expected: memory scales linearly until exceeding GPU capacity, at which point encoder-only becomes preferable despite slower inference.