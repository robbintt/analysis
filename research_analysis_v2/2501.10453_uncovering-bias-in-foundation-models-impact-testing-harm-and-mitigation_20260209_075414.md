---
ver: rpa2
title: 'Uncovering Bias in Foundation Models: Impact, Testing, Harm, and Mitigation'
arxiv_id: '2501.10453'
source_url: https://arxiv.org/abs/2501.10453
tags:
- biases
- bias
- test
- probes
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Trident Probe Testing (TriProTesting) to\
  \ uncover biases in foundation models across social attributes like gender, race,\
  \ age, and occupation. It uses three types of probes\u2014negative, positive, and\
  \ neutral\u2014to detect explicit and implicit biases."
---

# Uncovering Bias in Foundation Models: Impact, Testing, Harm, and Mitigation

## Quick Facts
- arXiv ID: 2501.10453
- Source URL: https://arxiv.org/abs/2501.10453
- Authors: Shuzhou Sun; Li Liu; Yongxiang Liu; Zhen Liu; Shuanghui Zhang; Janne Heikkilä; Xiang Li
- Reference count: 40
- Primary result: Introduces TriProTesting framework and AdaLogAdjustment mitigation to detect and reduce social biases in foundation models

## Executive Summary
This paper presents a comprehensive framework for uncovering and mitigating social biases in foundation models across attributes like gender, race, age, and occupation. The authors introduce Trident Probe Testing (TriProTesting), which uses negative, positive, and neutral text probes to detect both explicit and implicit biases in zero-shot classification. They evaluate four major models (CLIP, ALIGN, BridgeTower, OWLv2) across 240 single-attribute and 180 mixed-attribute scenarios, revealing pervasive biases. To address these biases, they propose Adaptive Logit Adjustment (AdaLogAdjustment), a post-processing technique that redistributes probability power without retraining, achieving up to 70% bias reduction and improving fairness in over 99% of test cases.

## Method Summary
The study employs TriProTesting to detect biases by appending three types of semantic probes (negative, positive, neutral) to classification labels and measuring the probability of images being classified as these probes. The framework tests four foundation models across curated datasets for gender (CelebA), age (UTKFace), race (FairFace), and occupation (IdenProf), with extended intersectional datasets created by auto-labeling gender. For mitigation, AdaLogAdjustment learns multiplicative adjustment factors applied to output logits via Adam optimization on small labeled subsets (N=20), redistributing probability mass to improve fairness without retraining the underlying models.

## Key Results
- AdaLogAdjustment achieves bias reduction in 99.17% of single-attribute and 98.89% of mixed-attribute scenarios
- Up to 70% reduction in bias across tested models and attributes
- Reveals intersectional biases that are qualitatively different from single-attribute biases
- OWLv2 exhibits stronger biases than other models despite being adapted from object detection

## Why This Works (Mechanism)

### Mechanism 1: Trident Probe Testing (TriProTesting)
If specific semantic probes are appended to the classification label set, the model's propensity to classify an image as a "probe" rather than a ground-truth label reveals explicit and implicit biases. The framework injects three types of text prompts (Negative, Positive, Neutral) into the zero-shot classification pipeline. By calculating the probability P(class → probe), it distinguishes explicit bias (association with negative/positive terms) from implicit bias (association with generic neutral terms like "person" or "stranger"), effectively turning the classifier into a bias detector.

### Mechanism 2: Probability Power Redistribution (AdaLogAdjustment)
If bias is viewed as an unfair concentration of "probability power," it can be mitigated by learning a set of multiplicative adjustment factors (α) applied to output logits without retraining. The method reframes bias mitigation as a "power redistribution" problem (borrowing from sociology). It uses a small labeled set (N=20) to optimize a vector of adjustment factors α = [α₁, ..., αC] via Adam optimizer to minimize cross-entropy loss. The adjusted logit is zα = z · α.

### Mechanism 3: Intersectional Bias Amplification
Combining social attributes (e.g., Gender × Race) reveals biases that are qualitatively different or more severe than those observed in single-attribute tests. The paper creates "Extended Datasets" (e.g., UTKFACE) by annotating intersectional labels (e.g., "elderly woman"). It observes that while single attributes might show moderate bias, their combination can lead to "contradictory biases" (high scores on both positive and negative probes), suggesting the model is confused or relying heavily on stereotypes when identity cues conflict.

## Foundational Learning

- **Zero-Shot Classification with Vision-Language Models (VLMs)**: The paper tests "Foundation Models" (CLIP, ALIGN, etc.) which operate by aligning images with text prompts. You must understand that the "prediction" is simply the text prompt with the highest similarity score to the image. *Quick check*: If you prompt a model with ["a photo of a doctor", "a photo of a nurse", "a photo of a criminal"], and input an image of a man in scrubs, which logit score would indicate a "correct" prediction vs. a "biased" prediction?

- **Min-Max Normalization**: The paper normalizes probability data to a [0, 100] range for visualization (Figure 2 & 3). This prevents models with generally low output probabilities from being ignored in the visual analysis. *Quick check*: Why might comparing raw probability scores between CLIP and OWLv2 be misleading without normalization?

- **Macro Average Accuracy**: This is the primary metric for evaluating the AdaLogAdjustment mitigation. Unlike standard accuracy, it weights all classes equally, preventing the metric from being inflated by high performance on over-represented groups. *Quick check*: If a dataset has 90% "White" faces and 10% "Black" faces, why would "Overall Accuracy" be a bad metric for measuring fairness improvements?

## Architecture Onboarding

- **Component map**: Image I + Class Labels L + Probes P → VLM Encoder → Bias Detection (P(class → probe)) → Visualization → AdaLogAdjustment (α optimization) → Adjusted Logits zα → Corrected Predictions

- **Critical path**: 1. Probe Design: Define Negative (e.g., "criminal"), Neutral (e.g., "person"), and Positive (e.g., "genius") prompts. 2. Inference: Pass images through the VLM to get logits for all classes + probes. 3. Analysis: Calculate the probability of images being mapped to the probes (bias detection). 4. Optimization: Select N=20 samples per class to learn α factors that maximize macro average accuracy. 5. Correction: Apply α to logits during inference to redistribute probability mass.

- **Design tradeoffs**: Efficiency vs. Overfitting: The method uses a tiny training set (N=20). While efficient (no retraining), the paper's ablation study (Section S2.1) shows performance drops if N is too small (underfitting) or too large (overfitting the adjustment factors). Post-hoc vs. In-training: As a post-processing technique, it cannot fix underlying feature representations (unlike KLAAD in the corpus which modifies attention), meaning it applies a "band-aid" to the output distribution rather than curing the model's internal bias.

- **Failure signatures**: Contradictory Bias: High probability for both negative and positive probes (e.g., "White" group associated with both "genius" and "criminal"). This suggests the model is over-representing this group in all semantic directions. Neutral Drift: A group consistently predicted as "person" or "individual" rather than their specific occupation indicates implicit devaluation (erasure of identity). Mitigation Collapse: If AdaLogAdjustment results in a decrease in Macro Average Accuracy (as seen in 0.8% of cases in Fig 4), the optimizer may have found a local minimum that unfairly penalizes specific groups.

- **First 3 experiments**: 1. Reproduce Single-Bias Bubbles: Test CLIP on CelebA with the 15 probes. Verify if the "man" group is predicted as "genius" more often than the "woman" group (Explicit Bias). 2. Verify OWLv2 Adaptation: Since OWLv2 is an object detector, implement the mean-logit adaptation described in Supplementary S2.3 to force it into a classification mode, then check if it still exhibits social bias. 3. Ablate N for Mitigation: Run the AdaLogAdjustment optimization with N=10, 20, 100 on a specific biased subset to observe the trade-off between sample efficiency and mitigation stability.

## Open Questions the Paper Calls Out

- How can Foundation Models be architected to minimize intrinsic biases during pre-training, rather than relying on post-processing mitigation techniques? [explicit] The conclusion explicitly poses the question: "How can models be designed to intrinsically minimize biases from the outset?"

- What are the long-term societal impacts and ethical trade-offs of deploying AdaLogAdjustment in high-stakes environments like healthcare or education? [explicit] The authors ask: "What are the long-term societal impacts of deploying debiased models in high-stakes environments like healthcare or education?"

- Does the probability redistribution in AdaLogAdjustment degrade the model's feature representation quality for general downstream tasks? [inferred] While the paper reports improved accuracy on bias test scenarios, it does not analyze if adjusting logits to prioritize fairness compromises the model's general capability as a feature extractor for non-social tasks.

## Limitations

- The effectiveness of neutral probes for detecting implicit bias remains theoretically contentious - it's unclear whether "person" truly captures devaluation versus mere semantic ambiguity
- OWLv2 adaptation from object detection to zero-shot classification requires mean pooling of bounding box logits, but the paper doesn't specify handling of background or multiple objects
- The small training set (N=20) for AdaLogAdjustment raises concerns about generalizability, though ablation studies suggest N=20 is optimal

## Confidence

- **High**: TriProTesting framework design and probe classification methodology
- **Medium**: AdaLogAdjustment efficacy claims (99.17% success rate), as results depend on specific dataset splits and random initialization
- **Medium**: Intersectional bias findings, as auto-labeling via CLIP introduces potential circularity in bias measurement

## Next Checks

1. **Probe validity test**: Run TriProTesting with synthetically biased datasets where ground truth associations are known to verify that probe probabilities correctly detect explicit bias
2. **OWLv2 adaptation validation**: Compare mean pooling vs. max pooling strategies for aggregating bounding box logits to ensure robust bias detection across different object detection outputs
3. **AdaLogAdjustment stability**: Test the optimization across 10 different random seeds for N=20 training set selection to measure variance in α factors and mitigation performance