---
ver: rpa2
title: 'AdaDocVQA: Adaptive Framework for Long Document Visual Question Answering
  in Low-Resource Settings'
arxiv_id: '2508.13606'
source_url: https://arxiv.org/abs/2508.13606
tags:
- document
- retrieval
- arxiv
- data
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'AdaDocVQA presents a hybrid text retrieval architecture combining
  TF-IDF and semantic embeddings for document segmentation, an automated data augmentation
  pipeline with multi-level quality verification, and adaptive ensemble inference
  with dynamic configuration generation. The framework achieves state-of-the-art performance
  on Japanese document VQA benchmarks: 83.04% accuracy on Yes/No questions, 52.66%
  on factual questions, and 44.12% on numerical questions in JDocQA, plus 59% accuracy
  on LAVA dataset.'
---

# AdaDocVQA: Adaptive Framework for Long Document Visual Question Answering in Low-Resource Settings

## Quick Facts
- arXiv ID: 2508.13606
- Source URL: https://arxiv.org/abs/2508.13606
- Reference count: 37
- Primary result: State-of-the-art accuracy on Japanese document VQA: 83.04% Yes/No, 52.66% factual, 44.12% numerical questions

## Executive Summary
AdaDocVQA presents a hybrid text retrieval architecture combining TF-IDF and semantic embeddings for document segmentation, an automated data augmentation pipeline with multi-level quality verification, and adaptive ensemble inference with dynamic configuration generation. The framework achieves state-of-the-art performance on Japanese document VQA benchmarks while providing a scalable foundation for low-resource language processing. The method demonstrates significant improvements over existing approaches through intelligent data augmentation and adaptive inference strategies.

## Method Summary
The framework combines OCR enhancement using Qwen2.5-VL with structure-preserving prompts, hybrid retrieval combining TF-IDF (α=0.6) and semantic embeddings (β=0.4) with adaptive selection, LoRA fine-tuning on augmented QA pairs (10,282 total), and ensemble inference with early stopping (confidence threshold 0.8). The system processes documents through a pipeline that maintains hierarchical structure, generates reasoning question types, and applies multi-layer quality verification before fine-tuning the 72B parameter model.

## Key Results
- Achieves 83.04% accuracy on Yes/No questions, 52.66% on factual questions, and 44.12% on numerical questions in JDocQA benchmark
- Demonstrates 59% accuracy on LAVA dataset, a 5% improvement over baseline methods
- Shows significant performance gains through ensemble inference (54%→59%) and fine-tuning (45%→54%)

## Why This Works (Mechanism)

### Mechanism 1
Hybrid retrieval combining lexical and semantic matching improves document segment identification for Japanese text over single-method approaches. TF-IDF provides precise term matching while multilingual-e5-large embeddings capture paraphrased semantic relationships. Scores are fused via weighted combination (α·S_tfidf + β·S_semantic) with adaptive threshold-based selection (minimum 3, maximum 7 pages, threshold 0.3). Core assumption: Japanese document queries require both morphological precision and conceptual flexibility; neither method alone sufficiently covers query-document relevance.

### Mechanism 2
VLM-based data augmentation with multi-level verification generates training-quality QA pairs that improve low-resource model performance. VLM performs structured OCR extraction maintaining hierarchy/tables/numerical precision → Answer feasibility analysis validates derivability → Five reasoning question types (comparative, computational, conditional, causal, comprehensive) generated with templates → Multi-layer quality checks (length, complexity, support, deduplication). Core assumption: Generated QA pairs with reasoning complexity transfer to real test distributions; verification filters sufficiently remove low-quality samples.

### Mechanism 3
Ensemble inference with dynamic configuration sampling and early stopping improves accuracy while reducing unnecessary computation. 20 decoding configurations (greedy + varied temperature/top-p/top-k sampling) → Generate responses → Track answer consistency → Stop when confidence ≥0.8 with ≥10 responses → Final answer via majority voting. Core assumption: Model uncertainty correlates with answer quality; diverse sampling captures knowledge distribution meaningfully.

## Foundational Learning

- Concept: TF-IDF vectorization and BM25 variants
  - Why needed here: Hybrid retrieval requires understanding sparse lexical matching fundamentals before combining with dense embeddings
  - Quick check question: Can you explain why TF-IDF alone struggles with synonym queries?

- Concept: Sentence embeddings and semantic similarity
  - Why needed here: Multilingual-e5-large produces 1024-dimensional dense vectors; understanding embedding space properties is essential for retrieval tuning
  - Quick check question: Why would cosine similarity be preferred over Euclidean distance for semantic matching?

- Concept: Ensemble methods and voting strategies
  - Why needed here: Adaptive ensemble inference relies on sampling diversity and consensus mechanisms
  - Quick check question: What conditions cause majority voting to fail even with diverse ensemble members?

## Architecture Onboarding

- Component map: OCR Enhancement (VLM) → Data Quality Filter → QA Generator → Hybrid Retrieval (TF-IDF + Semantic) → LoRA Fine-tuning (Qwen2.5-VL-72B) → Ensemble Inference (Dynamic Config → Early Stopping → Voting)

- Critical path: Retrieval quality directly determines inference feasibility (Table 3: without retrieval → OOM). Fine-tuning provides largest accuracy gain (45%→54%). Ensemble adds final boost (54%→59%).

- Design tradeoffs: TF-IDF weight (0.6) vs semantic weight (0.4) tuned for Japanese; higher α favors precision, higher β favors recall. Early stopping threshold (0.8) balances efficiency vs accuracy—lower threshold saves compute but may miss convergence.

- Failure signatures: OOM errors indicate retrieval insufficient or absent; low factual/numerical accuracy vs Yes/No suggests reasoning data augmentation gaps; high inference variance indicates ensemble configuration issues.

- First 3 experiments:
  1. Validate retrieval in isolation: measure recall@k on held-out documents with ground-truth relevant pages; test α/β weight variations.
  2. Assess augmentation quality: human-evaluate 100 generated QA pairs across 5 reasoning types; measure filtering rejection rates.
  3. Profile ensemble efficiency: plot accuracy vs inference count; identify early stopping threshold that maintains 95% of full-ensemble performance.

## Open Questions the Paper Calls Out

### Open Question 1
How effectively does AdaDocVQA generalize to other low-resource languages beyond Japanese, and does the hybrid TF-IDF plus semantic embedding retrieval architecture require language-specific re-optimization? The conclusion states future research directions include extending the framework to additional languages, yet the framework is evaluated exclusively on Japanese benchmarks with no cross-lingual transfer experiments.

### Open Question 2
What retrieval mechanisms can scale AdaDocVQA to extremely long document collections (hundreds or thousands of pages) without degrading accuracy or incurring prohibitive computational costs? The adaptive selection mechanism uses fixed minimum (3) and maximum (7) page limits, which may not generalize to documents with hundreds of pages where relevant information is more sparsely distributed.

### Open Question 3
What architectural or training modifications would close the substantial performance gap between numerical reasoning questions (44.12%) and other question types (Yes/No: 83.04%, factual: 52.66%)? Table 1 reveals a consistent 8-39 percentage point deficit on numerical questions across all models, suggesting a fundamental limitation in numerical reasoning capabilities.

### Open Question 4
Should the retrieval fusion weights (α=0.6 for TF-IDF, β=0.4 for semantic) and similarity threshold (τ=0.3) adapt dynamically based on query characteristics, rather than remaining fixed across all inputs? Query complexity varies significantly, yet the hybrid retrieval system uses static weights and thresholds.

## Limitations

- Data Quality Verification Gaps: Automated quality filters effectiveness versus human evaluation remains unclear without explicit validation against human-annotated gold standards
- Retrieval Architecture Scalability: Hybrid system relies on Jieba morphological analysis and multilingual-e5-large embeddings that may not generalize to other low-resource languages lacking quality morphological analyzers
- Ensemble Inference Overhead: 20-decoding configuration ensemble with early stopping increases computational costs significantly without detailed latency or cost comparisons against single-inference baselines

## Confidence

**High Confidence (Mechanism 1 - Hybrid Retrieval)**: Well-established in information retrieval literature with clearly specified implementation details and reasonable parameter choices for Japanese text.

**Medium Confidence (Mechanism 2 - Data Augmentation)**: Described with clear procedural steps but lacks direct validation against human-annotated data quality benchmarks; theoretical soundness but limited empirical verification.

**Medium Confidence (Mechanism 3 - Ensemble Inference)**: Follows established practices in uncertainty quantification with logically sound early stopping mechanism, but specific decoding configuration parameters and confidence calculation formula are underspecified.

## Next Checks

1. Conduct ablation studies measuring recall@k on held-out documents with ground-truth relevant pages across different α/β weight combinations (e.g., 0.4/0.6, 0.7/0.3, 0.5/0.5) to determine optimal hybrid balance for Japanese documents.

2. Implement human evaluation of 200 randomly sampled generated QA pairs (100 original, 100 augmented) across all five reasoning types, measuring precision, relevance, and answer derivability to quantify augmentation filter effectiveness.

3. Run controlled experiments varying the early stopping confidence threshold (0.6, 0.7, 0.8, 0.9) while measuring accuracy degradation and inference latency to identify the optimal efficiency-accuracy trade-off point.