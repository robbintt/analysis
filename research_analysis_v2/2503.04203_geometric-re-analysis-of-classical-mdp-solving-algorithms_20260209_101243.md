---
ver: rpa2
title: Geometric Re-Analysis of Classical MDP Solving Algorithms
arxiv_id: '2503.04203'
source_url: https://arxiv.org/abs/2503.04203
tags:
- policy
- value
- iteration
- state
- convergence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work analyzes classical MDP algorithms using a geometric\
  \ framework, focusing on Value Iteration (VI) and Policy Iteration (PI). It introduces\
  \ a discount factor transformation that can improve convergence by modifying the\
  \ discount parameter \u03B3 while preserving algorithm dynamics."
---

# Geometric Re-Analysis of Classical MDP Solving Algorithms

## Quick Facts
- arXiv ID: 2503.04203
- Source URL: https://arxiv.org/abs/2503.04203
- Reference count: 20
- Primary result: Introduces geometric framework for analyzing MDP algorithms, proving VI converges faster than γ when optimal policy MRP is irreducible/aperiodic and bounding PI iterations by action count in 2-state MDPs

## Executive Summary
This work provides a geometric re-analysis of classical MDP algorithms, Value Iteration (VI) and Policy Iteration (PI), using a novel framework that represents MDP actions as vectors and policies as hyperplanes. The key insight is that VI convergence benefits from both discount factor contraction and a "rotation component" arising from mixing properties of the optimal policy's Markov chain. The authors introduce a discount factor transformation that can reduce the effective discount parameter γ while preserving algorithm dynamics, leading to improved convergence bounds. For PI, they prove that in 2-state MDPs, the number of iterations needed is bounded by the number of actions.

## Method Summary
The method represents MDP actions as (n+1)-dimensional vectors with reward and transition coefficients, and policies as hyperplanes orthogonal to value vectors. A discount factor transformation Jγ's modifies the discount parameter while preserving advantages and spans, potentially improving convergence. For VI, the analysis tracks span contraction through both γ-contraction and mixing properties of the optimal policy MRP. For PI, the geometric approach uses slope comparisons and inefficiency zones to bound iterations in 2-state MDPs. The framework also introduces learning rate variants and span-based stopping criteria.

## Key Results
- Value Iteration exhibits faster-than-γ convergence when optimal policy MRP is irreducible and aperiodic, with span contracting as sp(VN) ≤ γ^N τ sp(V0)
- In 2-state MDPs, Policy Iteration converges in at most m iterations (number of actions)
- Discount factor transformation can reduce effective discount γ_eff < γ when coefficient conditions allow, improving convergence bounds
- For VI with span-based stopping, convergence improves from O((log 1/ε + log(1/(1-γ)))/log(1/γ)) to O((log(1/ε) + log(1/(1-γ)))/(log(1/γ) + log(1/τ)/N))

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A discount factor transformation Jγ's can improve convergence of VI and PI while preserving algorithm dynamics
- Mechanism: The transformation shifts the coordinate c_s corresponding to state s by moving the zero-level hyperplane, changing values on all states while preserving advantages adv(a,π) and the span sp(Vπ). This allows reducing the effective discount factor γ_eff below γ when action coefficients permit, directly improving convergence bounds.
- Core assumption: The transformation can be applied "safely"—at least one state i exists where all coefficients c_a^i for actions a in other states are positive
- Evidence anchors:
  - [abstract] "introduces a discount factor transformation that can improve convergence by modifying the discount parameter γ while preserving algorithm dynamics"
  - [section 3] Theorem 3.1 proves advantage and span preservation; Corollaries 3.3-3.4 give improved bounds using γ_eff
  - [corpus] Rank-One Modified Value Iteration explores related approximation-based acceleration but uses different rank-one structure
- Break condition: No state exists with universally positive cross-state coefficients; transformation would violate coefficient constraints (make multiple entries negative)

### Mechanism 2
- Claim: Value Iteration exhibits faster-than-γ convergence when the optimal policy's MRP is irreducible and aperiodic
- Mechanism: VI has two convergence sources: (1) γ-contraction and (2) a "rotation component" from information exchange between states via the mixing properties of P*. The span contracts as sp(VN) ≤ γ^N τ sp(V0) where τ ∈ (0,1) depends on the minimum entry ω of (P*)^N and advantage gap δ. This "skews" the value vector toward horizontal faster than pure γ-contraction.
- Core assumption: Assumption 5.1—the MRP induced by the unique optimal policy π* is irreducible and aperiodic
- Evidence anchors:
  - [abstract] "identifies a rotation component in the VI method...asymptotic convergence rate of value iteration is strictly smaller than γ"
  - [section 5.2] Theorem 5.2 proves sp(VN) ≤ γ^N τ sp(V0); Corollary 5.4 gives improved iteration bound O((log(1/ε) + log(1/(1-γ)))/(log(1/γ) + log(1/τ)/N))
  - [corpus] Unrolling Dynamic Programming via Graph Filters also exploits structural properties but through graph filter unrolling rather than mixing analysis
- Break condition: Optimal policy MRP is reducible or periodic; states don't exchange information through P* dynamics, eliminating the τ < 1 factor

### Mechanism 3
- Claim: In 2-state MDPs, Policy Iteration converges in at most m iterations (number of actions)
- Mechanism: Geometric analysis shows that for any action set A with |A| ≥ 3 spanning both states, at least one action lies in an "inefficiency zone"—bounded by extreme-slope policies. Such actions cannot be chosen by PI since another action has higher advantage across all policies in the set. Each iteration eliminates ≥1 action.
- Core assumption: Two-state MDP with deterministic stationary policies
- Evidence anchors:
  - [section 4] Theorem 4.1 proves existence of inefficient action; Corollary 4.2 bounds iterations by m-n
  - [corpus] From Optimization to Control: Quasi Policy Iteration analyzes PI acceleration but doesn't address state-dimension bounds
- Break condition: Extension to n > 2 states not proven; inefficiency zone argument relies on 2D geometry

## Foundational Learning

- Concept: **Markov Reward Process (MRP) mixing properties**
  - Why needed here: The τ convergence factor depends on P* being irreducible and aperiodic; you need to understand when powers of stochastic matrices have all positive entries
  - Quick check question: Given transition matrix P, does there exist N such that all entries of P^N are strictly positive?

- Concept: **Span semi-norm and value function geometry**
  - Why needed here: Convergence is measured via sp(V) = max(V) - min(V); the paper uses geometric hyperplane interpretations where span measures vertical extent
  - Quick check question: Why is span more appropriate than ∞-norm for normalized MDPs with zero-mean optimal values?

- Concept: **Advantage function and policy improvement**
  - Why needed here: PI selects actions maximizing adv(a,π) = r_a + Σ c_a^i V_π(i); geometric interpretation uses inner products between action and policy vectors
  - Quick check question: What is the geometric relationship between an action vector, a policy hyperplane, and the advantage value?

## Architecture Onboarding

- Component map:
  - Geometric Engine: Converts MDP actions to (n+1)-dimensional vectors a^+ with reward coordinate and transition coefficients; policies to hyperplanes H_π orthogonal to V_π^+
  - Discount Transformer: Applies Jγ's to reduce γ_eff when coefficient constraints allow; validates safety conditions
  - Convergence Analyzer: Tracks sp(V_t), computes τ from P* structure, determines stopping criteria
  - Action Filter (optional): Eliminates provably suboptimal actions using advantage bounds

- Critical path:
  1. Normalize MDP to M* (optimal values → 0) using L transformations
  2. Attempt discount factor reduction → compute γ_eff
  3. Run VI/PI with span-based stopping: sp(V_t - V_{t-1}) ≤ ε(1-γ)/γ
  4. Output ε-optimal policy

- Design tradeoffs:
  - Computing γ_eff requires scanning all cross-state coefficients O(nm); may not always provide improvement
  - Span-based stopping is independent of unknown τ; practical but potentially conservative
  - Learning rate α < 1 trades γ-contraction for faster mixing (N_α ≤ n-1 vs N ≤ n²-2n+2)

- Failure signatures:
  - No convergence improvement: γ_eff ≈ γ (all states have at least one negative cross-coefficient)
  - Slow VI convergence: Optimal policy MRP is weakly connected (τ close to 1)
  - Action filter removes optimal action: Advantage bounds violated; initialization or normalization incorrect

- First 3 experiments:
  1. **Validate transformation preservation**: Generate random 2-state MDPs, apply Jγ's, verify adv(a,π) unchanged before/after; measure achieved γ_eff reduction
  2. **Characterize τ empirically**: For MDPs with known irreducible/aperiodic optimal policies, run VI and fit observed span decay to γ^N τ; compare to theoretical bound using computed ω and δ
  3. **Test 2-state PI bound**: Construct MDPs with varying action counts; count PI iterations until convergence; verify never exceeds m

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the geometric proof technique for PI be extended from 2-state MDPs to obtain tight bounds for general n-state MDPs?
- Basis: explicit - The paper states "We show that in the case of a 2-state MDP, the number of iterations required by PI to reach the optimal policy is upper bounded by the number of actions."
- Why unresolved: The 2-state proof relies on slope comparisons and "inefficiency zones" that may not generalize directly to higher dimensions.
- What evidence would resolve it: A geometric proof technique for n-state MDPs yielding a bound independent of γ, or a counterexample showing polynomial dependence on γ is unavoidable.

### Open Question 2
- Question: What is the computational complexity of finding the effective discount factor γ_eff, and can it be computed efficiently in practice?
- Basis: inferred - Definition 3.2 introduces γ_eff as the minimum achievable discount factor through safe transformations, but no algorithm or complexity analysis is provided.
- Why unresolved: Computing γ_eff requires characterizing all valid transformation sequences, which could be combinatorially complex.
- What evidence would resolve it: A polynomial-time algorithm for computing γ_eff, or hardness results showing the problem is computationally difficult.

### Open Question 3
- Question: Can tighter VI convergence bounds be established when the optimal policy MRP is reducible or periodic?
- Basis: explicit - Assumption 5.1 requires the optimal policy MRP to be "irreducible and aperiodic," which the improved bounds depend on.
- Why unresolved: The proof uses mixing properties (positive entries in (P*)^N) guaranteed by this assumption; reducible or periodic structures break this argument.
- What evidence would resolve it: Analysis showing whether the rotation-based improvement persists, degrades, or disappears in reducible/periodic cases.

### Open Question 4
- Question: How should the learning rate α be optimally selected to balance γ-contraction against mean-reversion in VI with learning rates?
- Basis: inferred - The paper notes that learning rate creates "a trade-off between these two sources of convergence" but provides no guidance on optimal selection.
- Why unresolved: The optimal α depends on mixing properties τ_α that depend on unknown optimal policy structure.
- What evidence would resolve it: Adaptive learning rate schedules or theoretical characterization of optimal α given MDP structure.

## Limitations
- The discount factor transformation requires specific coefficient structures that may not be achievable in all MDPs
- Improved VI convergence bounds depend on the optimal policy MRP being irreducible and aperiodic, which may not hold in practice
- The PI iteration bound is only proven for 2-state MDPs and may not generalize to higher dimensions
- Computing the effective discount factor γ_eff requires solving the MDP first, limiting practical applicability

## Confidence

- Mechanism 1 (discount transformation): Medium - theoretically sound but practical applicability depends on MDP structure
- Mechanism 2 (VI rotation component): High - rigorous mathematical derivation with clear geometric interpretation
- Mechanism 3 (2-state PI bound): High - tight proof for specific case, though limited generalizability

## Next Checks

1. **Empirical τ characterization**: Generate MDPs with varying optimal policy structures and measure actual span decay rates versus theoretical bounds. Compare observed convergence to γ^N τ scaling.

2. **Transformation feasibility analysis**: Systematically test discount factor transformations across MDP families (random, structured, pathological) to quantify how often γ_eff < γ can be achieved and by how much.

3. **Cross-state coefficient conditions**: Derive necessary and sufficient conditions for safe discount transformations, identifying MDP properties that enable or prevent improvement.