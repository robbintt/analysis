---
ver: rpa2
title: RAKG:Document-level Retrieval Augmented Knowledge Graph Construction
arxiv_id: '2504.09823'
source_url: https://arxiv.org/abs/2504.09823
tags:
- knowledge
- graph
- entity
- rakg
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RAKG, a framework for document-level knowledge
  graph construction that addresses challenges in traditional methods like entity
  disambiguation and cross-document knowledge integration. RAKG extracts pre-entities
  from text chunks and uses them as queries for retrieval-augmented generation (RAG),
  mitigating long-context forgetting in large language models (LLMs) and simplifying
  coreference resolution.
---

# RAKG:Document-level Retrieval Augmented Knowledge Graph Construction

## Quick Facts
- arXiv ID: 2504.09823
- Source URL: https://arxiv.org/abs/2504.09823
- Reference count: 33
- Key outcome: RAKG achieves 95.91% accuracy on MINE dataset, a 6.2 percentage point improvement over GraphRAG baseline (89.71%).

## Executive Summary
RAKG is a document-level knowledge graph construction framework that addresses challenges in traditional methods like entity disambiguation and cross-document knowledge integration. The approach extracts pre-entities from text chunks and uses them as queries for retrieval-augmented generation (RAG), mitigating long-context forgetting in large language models and simplifying coreference resolution. By integrating corpus-retrospective and graph-structure retrieval, RAKG enhances global information capture and node interconnection. The framework incorporates RAG evaluation techniques to filter hallucinations and improve graph quality.

## Method Summary
RAKG processes documents through chunking, pre-entity extraction via sentence-level NER, entity disambiguation using vector similarity and LLM verification, dual retrieval (corpus and graph structure), relation network generation, LLM-as-judge hallucination filtering, and knowledge graph fusion. The framework uses Qwen2.5-72B LLM for extraction and BGE-M3 embeddings for vectorization. Evaluation uses MINE dataset with 15 facts per article, measuring accuracy, Entity Coverage (0.8752), and Relation Network Similarity (0.7998) against GraphRAG baseline.

## Key Results
- Achieves 95.91% accuracy on MINE dataset, 6.2 percentage points higher than GraphRAG baseline
- Demonstrates higher Entity Coverage (0.8752) and Relation Network Similarity (0.7998) compared to baseline
- Shows improved entity density, relationship richness, and entity coverage metrics
- Robust performance in entity fidelity and relation network similarity

## Why This Works (Mechanism)

### Mechanism 1: Pre-Entity Extraction as RAG Queries
Extracting pre-entities from text chunks and using them as RAG queries mitigates LLM long-context forgetting and simplifies coreference resolution by localizing LLM attention to entity-relevant context rather than full-document processing. Sentence-by-sentence NER ensures high accuracy entity identification, with retrieved context containing sufficient signals for relation extraction.

### Mechanism 2: Dual Retrieval (Corpus Retrospective + Graph Structure)
Combining corpus retrospective retrieval with graph structure retrieval improves global information capture and entity interconnection by integrating textual context with structural priors from the initial knowledge graph. For each entity, related text chunks and graph nodes/edges are retrieved and integrated to generate complete relation networks.

### Mechanism 3: LLM-as-Judge for Hallucination Filtering
Adapting RAG evaluation frameworks to KGC filters hallucinated entities and relations by having an LLM evaluate each entity and relation against retrieved source text/graph, assigning credibility scores. Low-scoring triples are filtered out, reducing false positives from LLM hallucination generation.

## Foundational Learning

- **Named Entity Recognition (NER)**: Essential for RAKG's first stage of extracting pre-entities via sentence-level NER; errors cascade through all downstream steps. Quick check: Can you explain why chunk-level NER might outperform full-document NER for long texts?

- **Coreference Resolution**: RAKG claims pre-entity-based retrieval simplifies coreference by linking mentions via chunk-id and similarity, avoiding traditional expensive resolution. Quick check: How does vector similarity-based entity disambiguation differ from rule-based coreference chains?

- **Knowledge Graph Construction (KGC)**: RAKG's goal is document-level KGC; understanding triple extraction, entity merging, and relation schema is essential. Quick check: What are the tradeoffs between open-schema KGC vs. fixed-ontology KGC for document-level extraction?

## Architecture Onboarding

- **Component map**: Document Chunking & Vectorization -> Pre-Entity Extraction -> Entity Disambiguation -> Dual Retrieval -> Relation Network Generation -> LLM-as-Judge Evaluation -> Knowledge Graph Fusion

- **Critical path**: Chunking → Pre-Entity Extraction → Entity Disambiguation → Dual Retrieval → Relation Generation → Judge Filtering → KG Fusion. Errors in early stages compound downstream.

- **Design tradeoffs**: Chunk size (smaller improves NER precision but fragments relations), retrieval threshold (higher improves precision but may miss context), judge strictness (aggressive filtering reduces hallucinations but may discard valid relations).

- **Failure signatures**: Low Entity Coverage (NER missing entities), Low Relation Fidelity (irrelevant retrieval context), High hallucination rate (judge LLM miscalibrated).

- **First 3 experiments**: 1) Reproduce MINE dataset accuracy on subset, measuring accuracy, Entity Coverage, Relation Network Similarity vs. GraphRAG. 2) Ablate dual retrieval: run with only corpus retrieval, only graph retrieval, then both, comparing EC and RNS. 3) Tune retrieval threshold: sweep similarity threshold values and plot Entity Fidelity vs. Relation Richness.

## Open Questions the Paper Calls Out

### Open Question 1
How does RAKG performance generalize to noisy, human-authored corpora or significantly longer documents compared to the short, LLM-generated articles in the MINE dataset? The experimental validation is restricted to the MINE dataset of approximately 1000-word LLM-generated articles, potentially lacking real-world noise and complexity.

### Open Question 2
To what extent does the "LLM-as-judge" evaluation mechanism align with human expert assessment in detecting subtle hallucinations and semantic errors? The framework relies on LLM filtering without comparative study against human-annotated ground truth for the filtering process.

### Open Question 3
What is the computational latency and financial cost overhead of RAKG's multi-stage pipeline compared to single-pass extraction baselines? The framework requires multiple LLM inference calls for every entity and relation network, but doesn't analyze inference time or API costs.

## Limitations

- Heavy dependency on LLM performance for NER, disambiguation, and evaluation, with critical prompt templates and hyperparameters unspecified
- Dataset dependency on MINE dataset, which appears proprietary with unclear ground truth construction methodology
- Computational overhead from multi-stage pipeline requiring sequential LLM calls for every entity and relation network

## Confidence

**High Confidence**: The general architecture of using pre-entities as RAG queries is theoretically sound; LLM-as-judge for hallucination filtering is valid; MINE dataset evaluation protocol is methodologically reasonable.

**Medium Confidence**: The 6.2 percentage point accuracy improvement is credible but dataset-dependent; Entity Coverage and Relation Network Similarity improvements suggest meaningful gains but depend on ideal KG construction; simplification of coreference resolution is plausible but not empirically validated.

**Low Confidence**: Sentence-level NER achieving "nearly perfect" entity identification is unverified; independence assumption between generation and evaluation errors may not hold; scalability claims are not tested beyond 105-article MINE corpus.

## Next Checks

1. **Ablation Study on Dual Retrieval Components**: Implement RAKG with only corpus retrieval, only graph structure retrieval, and both components. Measure Entity Coverage and Relation Network Similarity on MINE dataset to quantify individual and combined contributions.

2. **Prompt Template and Hyperparameter Sensitivity Analysis**: Systematically vary vector similarity thresholds, LLM sampling parameters (temperature, top_p), and chunk size parameters. Plot performance metrics against these parameters to identify optimal operating ranges and assess robustness.

3. **Cross-Dataset Validation**: Apply RAKG to a publicly available document-level KGC dataset with established evaluation protocols. Compare performance against GraphRAG and other baselines to assess whether improvements generalize beyond MINE dataset.