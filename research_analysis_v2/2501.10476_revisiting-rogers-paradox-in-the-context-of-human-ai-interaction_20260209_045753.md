---
ver: rpa2
title: Revisiting Rogers' Paradox in the Context of Human-AI Interaction
arxiv_id: '2501.10476'
source_url: https://arxiv.org/abs/2501.10476
tags:
- learning
- world
- social
- system
- individual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper revisits Rogers\u2019 Paradox in the context of human-AI\
  \ interaction. Rogers\u2019 Paradox describes a counterintuitive result where the\
  \ availability of cheap social learning (learning from others) does not increase\
  \ population fitness compared to individual learning in a dynamic, uncertain environment."
---

# Revisiting Rogers' Paradox in the Context of Human-AI Interaction

## Quick Facts
- arXiv ID: 2501.10476
- Source URL: https://arxiv.org/abs/2501.10476
- Reference count: 29
- Key outcome: AI presence does not change population equilibrium of collective understanding, even with different costs or learning success rates

## Executive Summary
This paper extends Rogers' Paradox—which shows that cheap social learning doesn't improve population fitness in dynamic environments—to human-AI interaction networks. The authors simulate populations of agents who can learn individually, socially from other humans, or socially from an AI that learns from the population mean. Their primary finding is that adding an AI learner doesn't alter the population equilibrium of collective world understanding. However, strategies like critical social learning (allowing humans to override AI outputs) and enabling AI individual learning can improve population understanding under certain conditions. The study also introduces a novel extension showing that AI interactions can negatively impact human individual learning efficacy, reducing collective understanding unless humans have alternative learning sources.

## Method Summary
The authors build an agent-based simulation extending Rogers' Paradox to include AI-human interaction. The simulation models 1000 agents in a dynamic environment with changing world states (probability u=0.01). Agents choose between individual learning (cost c_I=0.05, success z_i=0.66) or social learning from random agents or an AI (cost c_S=0). The AI learns by snapping to the population mean each timestep. Fitness is measured as the proportion of adapted agents over time. Multiple extensions explore critical social learning strategies, variable AI update schedules, AI individual learning with different costs/success rates, and negative feedback where AI interactions reduce human individual learning efficacy by 10% per interaction.

## Key Results
- AI presence does not change population equilibrium of collective understanding compared to social-only learning
- Critical social learning (humans can override AI outputs) improves population understanding when world changes rapidly (u ≥ 0.1)
- Negative feedback where AI interactions reduce human individual learning efficacy decreases collective understanding unless alternative learning sources exist

## Why This Works (Mechanism)
The mechanism underlying Rogers' Paradox in human-AI contexts is that AI systems, when learning from the population, simply reinforce existing population knowledge without adding new adaptive information. Since the AI snaps to the population mean each timestep, it doesn't improve the rate at which the population discovers correct behaviors in changing environments. The critical social learning strategy works by allowing humans to bypass potentially outdated AI information when their own social learning fails, effectively maintaining a pathway for discovering new adaptive behaviors. The negative feedback mechanism demonstrates how over-reliance on AI can create a de-skilling effect, reducing humans' capacity for individual learning and thus their ability to adapt when AI outputs become obsolete.

## Foundational Learning
- **Rogers' Paradox**: Why needed - forms the theoretical foundation; Quick check - verify equilibrium fitness matches individual-only baseline (~0.58)
- **Agent-based modeling**: Why needed - simulates population dynamics and learning strategy evolution; Quick check - confirm agents reproduce based on survival probability
- **Social learning dynamics**: Why needed - models how information propagates through populations; Quick check - verify social learning inherits delayed (previous timestep) information
- **Population fitness equilibrium**: Why needed - measures collective adaptation success; Quick check - ensure fitness stabilizes over final 50,000 timesteps
- **Learning strategy evolution**: Why needed - determines which strategies dominate in population; Quick check - confirm mutation rate allows strategy diversity

## Architecture Onboarding

### Component Map
Agents (Individual/Social/AI) -> Learning Strategy Selection -> Fitness Assessment -> Reproduction -> Population Evolution

### Critical Path
1. Agents choose learning strategy each timestep
2. Learning outcomes determine survival probabilities
3. Survivors reproduce with inherited/mutated strategies
4. AI updates from population mean
5. Fitness converges to equilibrium

### Design Tradeoffs
- AI learning mechanism: snapping to population mean vs. more complex models
- Strategy selection: random choice vs. history-based optimization
- Fitness measurement: immediate vs. time-averaged outcomes

### Failure Signatures
- Equilibrium fitness ≠ 0.58 baseline indicates incorrect survival probability application
- No strategy evolution suggests mutation rate too low or reproduction mechanics broken
- Critical social learning shows no improvement suggests agents cannot assess learning success

### First Experiments
1. Validate base Rogers' Paradox simulation matches individual-only equilibrium (~0.58)
2. Add AI agent and verify paradox persists with same equilibrium
3. Implement critical social learning and test under varying world change rates

## Open Questions the Paper Calls Out
- How does AI system transparency (uncertainty communication, explanations) affect human decisions to engage in social versus individual learning, and does this modify population-level equilibria?
- Under what conditions do compositions of strategies (e.g., critical social learning combined with AI individual learning) yield higher collective understanding than either strategy alone?
- If learning from AI can degrade individual learning efficacy (negative feedback), can positive feedback loops—where co-learning with AI boosts human individual learning—be designed, and what are their equilibrium effects?

## Limitations
- Simplified agent learning model (snapping to population mean) may not capture real-world AI learning complexity
- Incomplete specification of agent initialization and reproduction mechanics affects reproducibility
- No empirical validation with human subjects to confirm simulation predictions

## Confidence
- AI presence doesn't change population equilibrium (High confidence)
- Critical social learning improves outcomes under high environmental change (Medium confidence)
- Negative feedback reduces collective understanding (Medium confidence)

## Next Checks
1. Replicate base Rogers' Paradox simulation with explicit agent initialization and multiple runs to establish confidence intervals for equilibrium fitness
2. Implement critical social learning strategy with varying world change rates to verify threshold effects and parameter sensitivity
3. Test negative feedback extension with alternative decay functions (e.g., exponential) to assess robustness of findings