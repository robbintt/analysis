---
ver: rpa2
title: 'Fast-Slow-Thinking: Complex Task Solving with Large Language Models'
arxiv_id: '2504.08690'
source_url: https://arxiv.org/abs/2504.08690
tags:
- task
- step
- answer
- thinking
- they
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a new task decomposition method termed \u201C\
  Fast-Slow-Thinking\u201D (FST) for complex task solving with Large Language Models\
  \ (LLMs). Inspired by human thinking modes, FST combines Fast Thinking (FT) and\
  \ Slow Thinking (ST) steps to enable LLMs to consider complex problems from coarse\
  \ to fine."
---

# Fast-Slow-Thinking: Complex Task Solving with Large Language Models

## Quick Facts
- **arXiv ID**: 2504.08690
- **Source URL**: https://arxiv.org/abs/2504.08690
- **Reference count**: 40
- **Primary result**: Introduces Fast-Slow-Thinking (FST) method that improves LLM performance on complex tasks through coarse-to-fine decomposition, achieving up to 15.85% accuracy gain on GSM8K.

## Executive Summary
This paper introduces Fast-Slow-Thinking (FST), a task decomposition method for complex problem solving with Large Language Models. Inspired by human dual-process thinking, FST divides the reasoning process into Fast Thinking (FT) and Slow Thinking (ST) steps. In FT, the LLM simplifies the original complex task by removing constraints to create a general version. In ST, the LLM improves the simplified solution to meet the original task's requirements. The method is evaluated across three task types—math reasoning, long-content answering, and constrained story generation—demonstrating consistent performance improvements over baseline approaches like Zero-Shot Chain-of-Thought.

## Method Summary
FST is a three-step prompt engineering pipeline that leverages the same LLM sequentially without training. The Fast Thinking step simplifies complex tasks by removing constraints using few-shot examples, generating a general solution. The Slow Thinking step then refines this solution by re-introducing the original constraints. An Output Inspection step verifies the final answer against the original requirements. The method is implemented through carefully constructed prompts that guide the LLM through this coarse-to-fine reasoning process, with temperature settings varying by model (GPT-3.5: 0.7, Llama-3.1: 0.6, Gemini-pro: 0.9).

## Key Results
- On GSM8K math dataset, FST increases Result Accuracy by up to 15.85% compared to Zero-Shot-CoT
- On CommonGen-Hard story generation, FST increases All Present Rate by up to 10%
- Ablation studies show the Output Inspection step adds 2-3% accuracy improvement
- FST demonstrates consistent improvements across different LLM backbones (GPT-3.5, Llama-3.1, Gemini-pro)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Simplifying tasks via constraint removal aligns the input distribution with the LLM's pre-training data.
- **Mechanism:** Fast Thinking strips "fussy constraints," transforming complex prompts into generic ones, allowing the model to leverage robust patterns learned during pre-training before tackling edge cases.
- **Core assumption:** The model possesses base knowledge for general tasks but struggles when synchronizing knowledge with multiple strict constraints simultaneously.
- **Evidence anchors:** [Section 3.1]: "simplification to a concise and general task can better stimulate the power of LLMs"; [Abstract]: "In FT, LLMs are prompted to remove the constraints... simplifying it to a general and concise one."

### Mechanism 2
- **Claim:** A coarse-to-fine approach mitigates "attention degeneration" caused by complex logic.
- **Mechanism:** Separating generation prevents cognitive load from juggling logic and constraints simultaneously. FT establishes a valid semantic framework; ST fills in details, avoiding illogical content seen in baselines.
- **Core assumption:** LLMs fail on complex tasks not due to lack of capability, but inability to attend to global logic and local constraints in a single forward pass.
- **Evidence anchors:** [Page 2-3]: Existing methods result in solutions that "deviate from the original purpose" or contain "illogical content"; [Figure 1]: Shows GPT-3.5-turbo failing without FST, while FST succeeds.

### Mechanism 3
- **Claim:** Explicit output inspection triggers post-hoc self-correction capabilities.
- **Mechanism:** The Output Inspection step uses a checklist prompt to force the model to verify requirements, acting as a "System 2" check on previous outputs.
- **Core assumption:** The model can recognize errors in its own output when explicitly prompted to verify them, even if it couldn't avoid generating them initially.
- **Evidence anchors:** [Section 3.3]: "We further introduce an Output Inspection (OI) step... to verify the correctness"; [Table 7]: Adding OI to FT+ST improves GSM8K accuracy from 87.35% to 90.05% for Llama-3.1.

## Foundational Learning

- **Concept:** **Chain-of-Thought (CoT) Prompting**
  - **Why needed here:** FST is structurally an evolution of CoT. Understanding that LLMs perform better when "thinking step by step" is prerequisite to understanding why FST extends this to "thinking coarse, then thinking fine."
  - **Quick check question:** Can you explain why asking an LLM to "think step by step" improves math reasoning accuracy?

- **Concept:** **In-Context Learning (Few-Shot Prompting)**
  - **Why needed here:** The FST implementation relies on providing "simplification examples" in the prompt. You must understand how to structure these examples to guide the model's behavior without weight updates.
  - **Quick check question:** How do you select examples for a prompt to ensure the model learns the *pattern* of simplification rather than just copying the content?

- **Concept:** **Prompt Sensitivity**
  - **Why needed here:** The paper explicitly tests "character-level" and "word-level" disturbances. Understanding how minor prompt changes affect LLM stability is crucial for diagnosing why FST might fail in production.
  - **Quick check question:** What is the likely result if the "simplification examples" provided in the Fast Thinking prompt contain syntax errors?

## Architecture Onboarding

- **Component map:** FT Agent -> ST Agent -> OI Agent (can be same LLM called sequentially)
- **Critical path:** The Task Simplification in the FT step. If the model fails to identify the correct "General Task," the subsequent ST and OI steps will only refine an irrelevant response.
- **Design tradeoffs:**
  - **Latency vs. Accuracy:** FST requires 3 LLM calls (FT, ST, OI), tripling latency compared to Zero-Shot-CoT.
  - **Token Usage:** The "Slow Thinking" step requires injecting full original constraints and previous answer into context window, increasing input token costs.
- **Failure signatures:**
  - **Constraint Drift:** ST step generates logical story that fails to include required words.
  - **Logic Break:** FT produces logical story, but ST creates "weird" logical bridges to force words in, resulting in illogical content.
  - **Verification Failure:** OI step incorrectly validates a wrong answer (false positive).
- **First 3 experiments:**
  1. **Ablation Run:** Run FST pipeline on 10 math problems using only FT step (skip ST/OI). Observe if model solves "general" version correctly but fails specific one.
  2. **Stability Test:** Introduce typos into "simplification examples" (Table E4 style) and run on constrained story generation task to see if OI step catches resulting logic errors.
  3. **Constraint Scaling:** Apply FST to story generation task with 10 required words vs. 30 required words. Plot "All Present Rate" to see if FST degrades gracefully compared to standard CoT.

## Open Questions the Paper Calls Out
The paper explicitly states in Section 4.3 that "task-specific checks are not considered in our experiments," despite the method's goal of general applicability. This raises questions about how well FST would perform when verification logic must be tailored to specific task domains, and whether the current generic inspection approach limits the method's effectiveness on specialized tasks requiring domain-specific validation criteria.

## Limitations
- **Constraint Extraction Dependency**: The core innovation relies on Fast Thinking step correctly identifying and removing constraints, but the paper provides no algorithmic specification for automatically extracting the required placeholder for the ST prompt.
- **Evaluation Scope**: While improvements are demonstrated on GSM8K and CommonGen-Hard, generalizability to arbitrary complex reasoning tasks remains untested, with ablation study limited to Llama-3.1 and small datasets.
- **Latency and Cost**: FST requires three sequential LLM calls versus one for Zero-Shot-CoT, tripling inference latency and token usage, with no discussion of practical trade-offs between overhead and accuracy gains.

## Confidence

**High Confidence**: The coarse-to-fine decomposition mechanism (FT→ST) is theoretically sound and mathematical reasoning experiments show consistent improvements across different LLMs (GPT-3.5, Llama-3.1, Gemini-pro).

**Medium Confidence**: The story generation results (APR/MCR improvements) are promising but based on a small dataset (200 samples for CommonGen-Hard). Effectiveness may depend heavily on quality of few-shot examples.

**Low Confidence**: The claim that FST "better stimulates the power of LLMs" by aligning with pre-training data distribution is speculative, with no analysis showing how simplified tasks relate to the LLM's training corpus.

## Next Checks

1. **Constraint Extraction Validation**: Implement and test multiple algorithms for automatically extracting constraints from raw input text, then measure FST performance degradation when using different extraction methods versus manual annotation.

2. **Generalization Stress Test**: Apply FST to a multi-hop scientific reasoning dataset (e.g., QASC or OpenBookQA) and measure whether the FT→ST mechanism maintains effectiveness on tasks requiring integration of multiple knowledge domains.

3. **Latent Failure Analysis**: Systematically induce errors at each stage (FT, ST, OI) in the pipeline on 50 GSM8K problems and measure: (a) whether OI can recover from ST errors, (b) whether ST can recover from FT errors, and (c) the compounding effect when multiple stages fail.