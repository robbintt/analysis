---
ver: rpa2
title: Recreating Neural Activity During Speech Production with Language and Speech
  Model Embeddings
arxiv_id: '2505.14074'
source_url: https://arxiv.org/abs/2505.14074
tags:
- speech
- uni00000013
- neural
- embeddings
- activity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigated whether embeddings from large-scale, self-supervised\
  \ language and speech models can effectively reconstruct high-gamma neural activity\
  \ during speech production recorded via stereo EEG. The method mapped embeddings\
  \ from FastText, GPT-2.0, and Wav2Vec 2.0 onto high-gamma features using ElasticNet\
  \ regression, then evaluated reconstruction quality against ground truth using Pearson\
  \ correlation and R\xB2 metrics."
---

# Recreating Neural Activity During Speech Production with Language and Speech Model Embeddings

## Quick Facts
- **arXiv ID**: 2505.14074
- **Source URL**: https://arxiv.org/abs/2505.14074
- **Reference count**: 0
- **Primary result**: High-gamma neural activity during speech production can be reconstructed from embeddings of large-scale self-supervised language and speech models.

## Executive Summary
This study demonstrates that high-gamma neural activity recorded via stereo EEG during speech production can be effectively reconstructed using embeddings from large-scale, self-supervised language and speech models. The method involves mapping embeddings from FastText, GPT-2.0, and Wav2Vec 2.0 onto high-gamma features using ElasticNet regression, then evaluating reconstruction quality against ground truth using Pearson correlation and R² metrics. Results showed effective reconstruction across all participants, with Pearson correlation coefficients ranging from 0.79 to 0.99. FastText and GPT-2.0 generally outperformed Wav2Vec 2.0, with FastText yielding the highest proportion of trials in the 0.9-1.0 R² range. The findings suggest that neural activity during speech production can be reliably modeled using machine learning embeddings, supporting potential applications in speech neuroprostheses.

## Method Summary
The study used stereo EEG recordings from 10 Dutch participants reading 100 words aloud. High-gamma power (70-170 Hz) was extracted from sEEG signals and aligned to word onsets. Embeddings were extracted from FastText (300-d static), GPT-2.0 (768-d contextualized, penultimate layer), and Wav2Vec 2.0 XLS-R (PCA-reduced speech embeddings). These embeddings were mapped to high-gamma features using ElasticNet regression with leave-one-out cross-validation. Reconstruction quality was evaluated using Pearson correlation and R² metrics.

## Key Results
- High-gamma neural activity was effectively reconstructed from embeddings of large-scale self-supervised language and speech models.
- FastText and GPT-2.0 generally outperformed Wav2Vec 2.0 in reconstruction quality.
- FastText yielded the highest proportion of trials in the 0.9-1.0 R² range.

## Why This Works (Mechanism)

### Mechanism 1: High-Gamma Band as Cortical Speech Encoding
High-gamma power (70-170 Hz) extracted from sEEG signals carries sufficient information about speech production to be predicted from linguistic embeddings. Band-pass filtering isolates high-gamma activity; Hilbert transform extracts envelope power in 50 ms windows (10 ms shift), yielding temporal features that co-vary with articulatory and lexical processing. The high-gamma envelope captures a linearly decodable representation of speech-related cortical dynamics.

### Mechanism 2: Pre-trained Embeddings as Surrogate Neural Representations
Static and contextualized embeddings from text (FastText, GPT-2.0) and audio (Wav2Vec 2.0) models encode linguistically relevant structure that linearly maps to high-gamma activity. FastText provides subword-informed static vectors (d=300); GPT-2.0 provides averaged penultimate-layer transformer embeddings (d=768); Wav2Vec 2.0 provides contextualized speech embeddings reduced via PCA (90% variance). These embeddings serve as compressed semantic/acoustic codes aligned with neural patterns.

### Mechanism 3: Sparse Linear Mapping via ElasticNet Regularization
A regularized linear regression (ElasticNet) suffices to map embedding vectors to multi-channel high-gamma features across subjects. ElasticNet combines L1 (sparsity) and L2 (multicollinearity handling) penalties to learn stable mappings from n=99 training words to m electrode channels, minimizing overfitting with leave-one-out evaluation. The embedding-neural relationship is approximately linear within the restricted stimulus set.

## Foundational Learning

- **Concept: High-gamma neural oscillations (70-170 Hz)**
  - Why needed here: This band is the target signal being reconstructed; understanding its extraction and significance is essential for interpreting results.
  - Quick check question: Explain why high-gamma power (not raw voltage) is used as the neural feature and how the Hilbert transform extracts it.

- **Concept: Self-supervised representation learning (FastText, GPT-2, Wav2Vec 2.0)**
  - Why needed here: The study's core hypothesis depends on these models encoding brain-aligned structure without task-specific supervision.
  - Quick check question: Contrast static embeddings (FastText) with contextualized embeddings (GPT-2, Wav2Vec 2.0) and state which layer the study extracts from each model.

- **Concept: ElasticNet regularization (L1 + L2)**
  - Why needed here: The mapping model must balance sparsity (electrode selection) with stability (multicollinearity) across variable channel counts.
  - Quick check question: Given the optimization objective, describe how λ₁ and λ₂ affect which electrodes contribute to prediction.

## Architecture Onboarding

- **Component map**: Raw sEEG (1024 Hz) + synchronized audio (16 kHz) -> Detrend -> Notch filter (50 Hz harmonics) -> Butterworth bandpass (70-170 Hz) -> Hilbert envelope -> 50 ms window power -> ElasticNet regression -> Pearson correlation/R²

- **Critical path**: sEEG preprocessing -> high-gamma extraction must complete before any regression; embedding extraction is parallel and can be cached. Regression training is the bottleneck for hyperparameter tuning.

- **Design tradeoffs**: Text vs. audio embeddings: Text (FastText, GPT-2) outperformed audio (Wav2Vec 2.0) likely due to temporally misaligned acoustic features and speaker variability. Electrode coverage vs. reconstruction quality: Dense Broca/Wernicke coverage (S04) yielded R²=0.99; sparse coverage led to R² as low as -0.29 (S09, Wav2Vec 2.0). Dimensionality reduction: PCA on Wav2Vec 2.0 embeddings retained 90% variance but may have discarded neural-relevant dimensions.

- **Failure signatures**: Negative R² (e.g., S09 with Wav2Vec 2.0): Model predictions worse than mean baseline; indicates embedding-neural misalignment or insufficient electrode coverage. High standard deviation in PCC (e.g., S09: 0.69 for Wav2Vec 2.0): Word-specific reconstruction failure, possibly due to OOV tokens or articulatory variability. Consistent underperformance across embeddings: Likely electrode placement outside language-critical regions.

- **First 3 experiments**: 1) Replicate the preprocessing pipeline on a single subject: validate high-gamma extraction by visualizing envelope power aligned to word onset markers. 2) Train ElasticNet on FastText embeddings for one subject; sweep λ₁ and λ₂ to observe sparsity patterns in electrode weights. 3) Compare reconstruction quality across embedding types for the best (S04) and worst (S09) subjects; analyze whether text embeddings consistently outperform audio or if failure is subject-specific.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does combining text and audio embeddings (multimodal integration) improve neural reconstruction accuracy compared to unimodal approaches? The authors state future work will investigate "how the integration of multimodal embeddings affects reconstruction accuracy." This remains unresolved as FastText, GPT-2.0, and Wav2Vec 2.0 embeddings were evaluated independently.

- **Open Question 2**: Does information transfer across deep learning model layers mirror neural information transfer regarding temporal dynamics? The conclusion proposes exploring "whether the transfer of information from lower-level to higher-level layers... mirrors the neural information transfer." This remains unresolved as the current analysis utilized embeddings only from the final or penultimate layers.

- **Open Question 3**: Can this reconstruction methodology generalize to continuous, naturalistic speech production? The study utilized a dataset comprising only 100 isolated words, which lacks the coarticulation and syntactic planning of continuous speech. This limits immediate applicability to real-time neuroprostheses.

## Limitations
- **Electrode coverage variability**: Reconstruction quality is strongly dependent on electrode placement, with dense Broca/Wernicke coverage achieving R²=0.99 while sparse coverage yielded R² as low as -0.29.
- **Temporal alignment precision**: The paper lacks explicit detail on how word boundaries from audio align to 50 ms envelope windows, a critical step for supervised regression.
- **Hyperparameter specification**: ElasticNet λ₁ and λ₂ values are not reported, preventing exact replication.

## Confidence
- **High confidence**: Pearson correlation coefficients (0.79-0.99) and R² metrics are directly computed from leave-one-out predictions versus ground truth.
- **Medium confidence**: Claims that FastText and GPT-2.0 outperform Wav2Vec 2.0 are supported by aggregate statistics, but the mechanism is speculative without ablation studies.
- **Low confidence**: The assertion that self-supervised embeddings "share geometric structure" with brain activity lacks quantitative validation within this study.

## Next Checks
1. **Temporal synchronization validation**: Cross-correlate envelope power peaks with word onset markers across all subjects; flag subjects with >25 ms offset for potential realignment before retraining models.
2. **Hyperparameter sensitivity sweep**: For S04 (highest R²) and S09 (lowest R²), train ElasticNet across a grid of λ₁/λ₂ values (0.001-1.0) and report how reconstruction quality varies with sparsity.
3. **Embedding-brain alignment quantification**: Compute centered kernel alignment (CKA) between averaged embedding vectors and high-gamma activity vectors per word; test whether CKA scores correlate with reconstruction quality across subjects and embedding types.