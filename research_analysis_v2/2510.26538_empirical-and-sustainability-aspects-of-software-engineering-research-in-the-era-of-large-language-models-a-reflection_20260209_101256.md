---
ver: rpa2
title: 'Empirical and Sustainability Aspects of Software Engineering Research in the
  Era of Large Language Models: A Reflection'
arxiv_id: '2510.26538'
source_url: https://arxiv.org/abs/2510.26538
tags:
- papers
- ieee
- software
- engineering
- icse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study systematically analyses empirical software engineering
  research using large language models (LLMs) from 2023-2025, focusing on four critical
  challenges: benchmarking rigour, data contamination, replicability, and sustainability.
  The analysis of 177 ICSE papers reveals increasing use of commercial models like
  GPT-4 and GPT-3.5, with only 56.7% of 2025 papers benchmarking against non-LLM baselines.'
---

# Empirical and Sustainability Aspects of Software Engineering Research in the Era of Large Language Models: A Reflection

## Quick Facts
- arXiv ID: 2510.26538
- Source URL: https://arxiv.org/abs/2510.26538
- Reference count: 33
- Authors systematically analyze 177 ICSE papers from 2023-2025 to identify four critical challenges in LLM-based SE research: benchmarking rigour, data contamination, replicability, and sustainability.

## Executive Summary
This systematic literature review examines 177 ICSE papers from 2023-2025 to identify critical challenges in empirical software engineering research using large language models. The study reveals increasing dependence on commercial models like GPT-4 and GPT-3.5, with only 56.7% of 2025 papers benchmarking against non-LLM baselines. While data contamination is acknowledged by 42.2% of recent papers, only three studies implement specific mitigation strategies. Replicability remains poor with just 18.6% earning "Artifact Available" badges, and sustainability reporting is inadequate with only 20% documenting computational costs. A user study of 57 researchers found widespread uncertainty about sustaining commercial model costs long-term.

## Method Summary
The authors conducted a three-step manual selection process on 692 ICSE technical research track papers from 2023-2025, filtering through 15 keywords to identify 304 candidates, then manually reviewing to yield 174 papers plus 3 from random sampling. Data extraction followed a pilot phase with 4 papers per author, then independent extraction with iterative taxonomy consolidation. A questionnaire was distributed to 177 paper authors (57 responses) with ethics approval. The analysis focused on four research questions examining model usage patterns, contamination mitigation, replicability infrastructure, and sustainability reporting.

## Key Results
- Commercial model dominance: 58.8% of 2025 papers use GPT-4 and GPT-3.5, up from 39.5% in 2023
- Contamination awareness: 42.2% of 2025 papers acknowledge contamination as a threat, but only 3 studies implement mitigation
- Poor replicability: Only 18.6% of LLM studies earned "Artifact Available" badges vs. 41.4% for non-LLM papers
- Sustainability gap: Just 20% report both computational environment and time; no studies directly report energy or CO2 emissions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data contamination in LLM evaluation produces inflated performance estimates that may not reflect genuine model capability.
- Mechanism: When benchmark datasets overlap with training corpora, models may succeed via memorization rather than task competence; performance gains persist until contamination is detected through temporal filtering or ablation studies.
- Core assumption: Models can distinguish between recalled training examples and novel problem-solving, which remains unproven for closed-model training data.
- Evidence anchors:
  - [abstract] "potential data leakage from training corpora can skew performance metrics"
  - [RQ2 section] "only three articles...specifically tackle the problem of contamination/memorisation" and "data contamination or leakage refers to the scenario where a model is evaluated using data present in its training set"
  - [corpus] Related work (Golchin & Surdeanu 2025) addresses contamination detection tools; corpus evidence is limited on proven mitigation efficacy.
- Break condition: If training corpora become fully transparent or models are proven incapable of memorization, this mechanism weakens.

### Mechanism 2
- Claim: Shifting from open to commercial models reduces replicability due to API deprecation and configuration opacity.
- Mechanism: Commercial models update frequently without version locking; studies citing specific API versions become non-replicable when those versions are deprecated. Only explicit configuration logging and artifact preservation mitigate this.
- Core assumption: API providers will continue deprecating older versions—a pattern observed but not guaranteed.
- Evidence anchors:
  - [RQ1 section] "replicability is hindered as these models are frequently deprecated when newer versions are released"
  - [RQ3 section] Only 18.6% of LLM studies earned "Artifact Available" badges vs. 41.4% for non-LLM papers; "nine papers contain expired links"
  - [corpus] Corpus does not provide direct evidence on API deprecation timelines.
- Break condition: If commercial providers offer permanent version-locked APIs or frozen model snapshots, replicability improves.

### Mechanism 3
- Claim: Incomplete cost reporting obscures financial and environmental barriers, creating a sustainability gap in LLM-based SE research.
- Mechanism: Without mandatory reporting of compute hours, token counts, and hardware requirements, researchers cannot accurately estimate reproduction costs or environmental impact; this compounds as commercial API usage grows.
- Core assumption: Reported costs correlate with actual environmental impact, though no direct energy/CO2 measurements exist in the surveyed papers.
- Evidence anchors:
  - [abstract] "address the financial and environmental costs of LLM-based SE"
  - [RQ4 section] "we found no study that directly reported the energy consumed or CO2 emissions"; only 20% reported both computational environment and time; user study shows 56% uncertain about sustaining commercial model costs
  - [corpus] Related sustainability papers (e.g., "Greening AI-enabled Systems with Software Engineering") advocate for green SE practices but do not provide empirical LLM cost data.
- Break condition: If conferences mandate energy/cost disclosure, the mechanism becomes an enforced constraint rather than a voluntary practice.

## Foundational Learning

- Concept: **Data Contamination (Leakage)**
  - Why needed here: Central validity threat for LLM evaluation; understanding that models may have "seen" test data during training is prerequisite to interpreting any benchmark result.
  - Quick check question: Can you explain why temporal filtering (using data created after a model's training cutoff) might reduce but not eliminate contamination risk?

- Concept: **Replicability vs. Reproducibility**
  - Why needed here: The paper distinguishes these (replicability = independent reproduction; reproducibility = same data/code yields same results). LLM stochasticity complicates both.
  - Quick check question: If a paper provides prompts but not temperature/seed parameters, which is compromised—replicability, reproducibility, or both?

- Concept: **Inference Configuration Parameters**
  - Why needed here: Only 57.8% of 2025 papers report these (temperature, top-p, max tokens, seed). Missing values make result comparison unreliable.
  - Quick check question: Why would two experiments using the same model and prompt produce different outputs?

## Architecture Onboarding

- Component map: Benchmarking Rigour → Contamination Detection → Replicability Artifacts → Sustainability Disclosure
- Critical path: Benchmarking Rigour → Contamination Mitigation → Replicability Artifacts → Sustainability Disclosure. Each stage builds trust; gaps in early stages compound downstream.
- Design tradeoffs: Open models (CodeBERT, CodeLlama) offer transparency and replicability but require hardware investment; commercial models (GPT-4, Claude) offer convenience and performance but reduce control and long-term viability. Hybrid approaches (benchmarking both) mitigate but increase experiment complexity.
- Failure signatures:
  - Paper reports only LLM vs. LLM comparisons → no baseline validity
  - No configuration parameters logged → non-replicable
  - Single programming language evaluated → limited generalizability
  - Artifact link returns 404 → reproducibility broken
  - No cost/hardware details → sustainability unassessable
- First 3 experiments:
  1. Audit your current LLM research pipeline against the four pillars; log which parameters from the paper's taxonomy are missing.
  2. Implement contamination detection by applying temporal filtering to your benchmark (collect post-cutoff data) and run deduplication against known training corpora sources.
  3. Create a cost-tracking shim for your LLM API calls that logs tokens, latency, and estimated compute cost per experiment; validate against the 20% reporting threshold the paper identifies.

## Open Questions the Paper Calls Out

- Question: Does the high cost of LLM infrastructure create a measurable digital divide in research output and quality between well-resourced and low-resourced institutions?
  - Basis in paper: [explicit] The authors explicitly state they "aim to tackle additional ethical and social factors, including whether the costs and infrastructure demands of LLMs are enabling a digital divide in the field, particularly between well-resourced and low-resourced institutions."
  - Why unresolved: This study focused on ICSE publications, which may not fully capture barriers faced by researchers unable to publish at top venues due to resource constraints. The questionnaire respondents were already successful authors, potentially missing those excluded by cost barriers.
  - What evidence would resolve it: A comparative study of submission rates, acceptance rates, and research impact across institutions with varying levels of LLM-related funding and infrastructure access, controlling for other factors.

- Question: To what extent do LLM-based SE techniques trained or evaluated on a single programming language generalize to other programming languages?
  - Basis in paper: [inferred] The paper notes only 22.2% of 2025 papers evaluate with more than one programming language, down from 31.2% in 2023, despite evidence that "LLMs have been shown to perform better on some PLs than others, implying that results reported on one PL will not necessarily generalise to other PLs or contexts."
  - Why unresolved: The decreasing trend in multi-language evaluation suggests researchers may be prioritizing speed over generalizability, but the actual performance degradation when transferring techniques across languages remains unquantified.
  - What evidence would resolve it: Systematic replication of LLM-based SE techniques across multiple programming languages with performance comparison, identifying which techniques generalize well and which are language-specific.

- Question: What is the actual effectiveness of combining multiple contamination mitigation strategies (temporal filtering, deduplication, cross-dataset validation, ablation studies) in reducing inflated LLM performance estimates?
  - Basis in paper: [inferred] Only 3 of 177 papers specifically address contamination, yet the authors recommend "a combination of temporal filtering, benchmark design consideration, deduplication, cross-dataset validation, and ablation studies" without empirical evidence of effectiveness. The paper notes "no strategy can entirely eliminate the risk of contamination when the training dataset is unavailable."
  - Why unresolved: While individual strategies are described, their combined effectiveness and optimal configuration remain unknown, and the field lacks standardized contamination detection and reporting protocols.
  - What evidence would resolve it: Controlled experiments comparing LLM performance on known contaminated versus cleaned datasets, with and without various mitigation strategy combinations, measuring the reduction in artificially inflated metrics.

## Limitations

- Manual literature review methodology may have missed relevant papers despite keyword filtering and random sampling validation
- Lack of explicit inter-rater reliability metrics and detailed extraction schemas limits reproducibility of the review methodology itself
- Sustainability analysis constrained by limited empirical data on actual energy consumption and carbon emissions
- User study generalizability uncertain due to 57/177 response rate and potential self-selection bias among authors

## Confidence

- High confidence: Descriptive statistics about model usage trends and artifact badge compliance
- Medium confidence: Contamination mitigation effectiveness claims, as few actual implementations identified
- Low confidence: Sustainability cost estimates due to absence of direct energy/CO2 measurements in corpus

## Next Checks

1. Reconstruct the full extraction taxonomy from the figshare dataset to verify the categorization of contamination mitigation strategies and sustainability reporting items
2. Validate the keyword search coverage by replicating the candidate filtering process across multiple digital library interfaces
3. Conduct a follow-up survey with non-respondents to assess potential response bias in the sustainability perception data