---
ver: rpa2
title: Leveraging Large Language Models for Zero-shot Lay Summarisation in Biomedicine
  and Beyond
arxiv_id: '2501.05224'
source_url: https://arxiv.org/abs/2501.05224
tags:
- article
- summaries
- judges
- llms
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the application of Large Language Models
  to zero-shot Lay Summarisation, aiming to generate summaries that effectively communicate
  technical research findings to non-expert audiences. A novel two-stage framework
  is proposed, inspired by real-world practices, which involves a question-answering
  stage followed by summary generation.
---

# Leveraging Large Language Models for Zero-shot Lay Summarisation in Biomedicine and Beyond

## Quick Facts
- arXiv ID: 2501.05224
- Source URL: https://arxiv.org/abs/2501.05224
- Reference count: 20
- Primary result: A two-stage LLM framework generates lay summaries increasingly preferred by humans as model size grows, with LLM judges effectively approximating human preferences.

## Executive Summary
This paper introduces a novel two-stage framework for zero-shot lay summarisation that decomposes the task into question-answering and summary generation phases. The approach simulates real-world scientific communication practices and shows that larger language models (46B+ parameters) generate significantly more useful summaries for non-expert audiences compared to smaller models or simpler approaches. The framework demonstrates strong generalization from biomedical to NLP domains without domain-specific fine-tuning.

## Method Summary
The proposed method uses a two-stage LLM prompting framework where a first-stage "Author" agent extracts structured answers to four key questions about background, research questions, findings, and significance from the source article. A second-stage "Writer" agent then generates the final lay summary (300-400 words) using both the article and the QA responses as input. The framework is evaluated on biomedical articles from eLife and NLP papers from ACL 2023, comparing against simple baseline prompts and using both automatic metrics and human/LLM judge preferences.

## Key Results
- LLM judges reliably approximate human preferences for lay summaries
- Two-stage framework shows increasing preference for larger models (>46B parameters)
- Abstract-only input outperforms full-text input for summary generation
- Framework generalizes from biomedical to NLP domains without domain-specific adaptation

## Why This Works (Mechanism)

### Mechanism 1: Task Decomposition via Question-Answering
- Decomposing lay summarization into question-answering and generation stages improves summary utility by explicitly surfacing information that may be implicit in the source article. This allows the model to focus on clarity and accessibility rather than information retrieval.
- Core assumption: The QA stage questions reliably capture lay readers' information needs and the model can generate accurate answers.
- Evidence: Human judges increasingly prefer summaries from larger models using this framework, suggesting the QA stage effectively structures information for the generation stage.
- Break condition: If the model's QA answers are factually incorrect or irrelevant, final summary quality degrades.

### Mechanism 2: Scaling-Dependent Emergence of Utility
- Larger models show significantly stronger performance with the two-stage framework due to greater emergent capabilities in instruction following and complex reasoning.
- Core assumption: Model size is the primary driver of improvement, not architecture or training data differences.
- Evidence: Judges prefer two-stage summaries for models ≥46B parameters, with preference strength increasing with model size.
- Break condition: Framework fails on smaller models (≤7B) where the task complexity overwhelms the model.

### Mechanism 3: Proxy Evaluation via LLM-as-Judge Panels
- Panels of smaller LLMs can effectively approximate lay human judge preferences by internalizing patterns of clarity, coherence, and relevance from training data.
- Core assumption: LLM judge preferences align sufficiently with human notions of utility for lay summaries.
- Evidence: LLM judges prove effective in approximating human preferences across different model sizes and domains.
- Break condition: LLM-as-judge fails if systematic divergence exists between LLM preferences and genuine human utility.

## Foundational Learning

- **Zero-Shot Learning**: Needed because the framework generates summaries for new domains (NLP) without specific training data. Quick check: Can the model perform based solely on instructions and pre-existing knowledge?

- **Prompt Engineering / Task Decomposition**: The framework is advanced prompt engineering that breaks down complex tasks into sequential roles and steps. Quick check: How does structuring tasks into sub-tasks affect model performance?

- **LLM Evaluation Metrics**: The paper critiques traditional metrics like ROUGE for failing to capture human preference. Quick check: Why might ROUGE fail to correlate with human judgment of summary "utility"?

## Architecture Onboarding

- **Component map**: Author Agent (extracts QA answers) -> Writer Agent (generates summary) -> Panel of LLM Evaluators (judges quality)

- **Critical path**: The quality of intermediate QA output is critical; if the Author agent fails to extract correct information, the Writer cannot produce a high-quality summary.

- **Design tradeoffs**: Two-stage approach increases inference cost (two LLM calls) vs. single-prompt methods; abstract-only input sometimes outperforms full text; model size tradeoff between cost and performance.

- **Failure signatures**: Performance degradation on smaller models (≤7B), high automatic metric scores with poor human preference indicating metric-summary misalignment.

- **First 3 experiments**:
  1. Test two-stage framework against simple baseline on small (Mistral-7B) and large (Llama-3-70B) models; expect two-stage to fail on small and succeed on large.
  2. Run ablation study removing "Author" and "Writer" role descriptions to measure impact on quality.
  3. Test cross-domain generalization by applying best configuration to new scientific domain (e.g., materials science) and evaluate with LLM judges.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise model size threshold at which the two-stage QA-based framework becomes beneficial?
- Basis: Experiments only test discrete sizes (2.7B, 7B, 46B, 70B, 132B), leaving gap between 7B and 46B unexplored.
- Resolution: Systematic evaluation across continuous range (10B, 20B, 30B, 40B) using same protocol.

### Open Question 2
- Question: Why do standard automatic metrics like ROUGE fail to capture aspects driving human preferences?
- Basis: Paper identifies problem but doesn't investigate which qualitative dimensions (accessibility, structure, jargon reduction) humans value that metrics miss.
- Resolution: Correlation analysis between fine-grained human judgments and broader range of automatic metrics.

### Open Question 3
- Question: To what extent does the framework generalize to other scientific domains beyond Biomedicine and NLP?
- Basis: Only two domains tested; effectiveness on fields with different discourse structures (physics, social sciences) remains unknown.
- Resolution: Application to diverse scientific domains with equivalent human evaluation protocols.

### Open Question 4
- Question: What is the optimal composition and size of LLM evaluator panels for approximating human preferences?
- Basis: Low individual agreement among LLM judges (κ = -0.067) suggests panel composition matters, but only one configuration tested.
- Resolution: Ablation studies varying panel size, model diversity, and aggregation strategies against human preference data.

## Limitations

- Narrow domain scope: Only tested on biomedical and NLP domains, limiting claims about broader generalization.
- Reliance on LLM judges: Assumes LLM preferences generalize across reader demographics without reporting evaluator characteristics.
- Model size barrier: Smaller models (<46B) fail with the framework, creating practical accessibility issues.

## Confidence

- **High Confidence**: Larger models (≥46B) significantly outperform smaller models with two-stage framework, supported by quantitative comparisons and consistent human evaluation.
- **Medium Confidence**: Two-stage framework provides benefits specifically for large models, though precise mechanism and thresholds remain unclear.
- **Medium Confidence**: LLM judges effectively approximate human preferences, but depth of alignment across evaluation dimensions not fully characterized.
- **Low Confidence**: Strong cross-domain generalization claims, as testing only one additional domain (NLP) provides insufficient evidence.

## Next Checks

1. **Cross-Domain Generalization Test**: Apply best configuration to three diverse scientific domains (materials science, environmental science, economics) and evaluate using both LLM judges and new human evaluators to assess performance consistency.

2. **LLM Judge Calibration Study**: Conduct systematic comparison between LLM judges and human evaluators across multiple dimensions (clarity, accuracy, completeness, accessibility) to identify systematic biases and establish confidence intervals.

3. **Small Model Optimization**: Systematically explore prompt engineering techniques (few-shot examples, chain-of-thought, intermediate reasoning) to determine if smaller models can be made effective with two-stage framework, expanding practical applicability.