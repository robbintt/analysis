---
ver: rpa2
title: 'Causal-LLaVA: Causal Disentanglement for Mitigating Hallucination in Multimodal
  Large Language Models'
arxiv_id: '2505.19474'
source_url: https://arxiv.org/abs/2505.19474
tags:
- visual
- https
- causal
- semanticscholar
- corpusid
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of object hallucinations in multimodal
  large language models (MLLMs), where models generate descriptions of objects inconsistent
  with or absent from the input. The authors propose a causality-driven disentanglement
  framework that mitigates hallucinations through causal intervention.
---

# Causal-LLaVA: Causal Disentanglement for Mitigating Hallucination in Multimodal Large Language Models

## Quick Facts
- **arXiv ID**: 2505.19474
- **Source URL**: https://arxiv.org/abs/2505.19474
- **Reference count**: 40
- **Primary result**: 22.6% increase in MME-Perception score and reduction in CHAIR metrics, indicating notable reduction in hallucinations while maintaining strong performance across several benchmarks.

## Executive Summary
This paper addresses the problem of object hallucinations in multimodal large language models (MLLMs), where models generate descriptions of objects inconsistent with or absent from the input. The authors propose a causality-driven disentanglement framework that mitigates hallucinations through causal intervention. The core method idea involves introducing a Causal-Driven Projector in the visual pathway and a Causal Intervention Module integrated into the final transformer layer of the language model. These components work together to reduce spurious correlations caused by biased training data. Experimental results show a 22.6% increase in the MME-Perception score and a reduction in CHAIR metrics, indicating a notable reduction in hallucinations while maintaining strong performance across several benchmarks.

## Method Summary
The proposed method introduces two key modules to LLaVA: a Causal-Driven Projector in the visual pathway and a Causal Intervention Module in the final transformer layer of the language model. The projector disentangles visual features of an object from frequently co-occurring objects via causal intervention using cross-attention with a pre-computed "confounder dictionary" (averaged visual features per object category). The intervention module purifies hidden states in the final LLM layer using cross-attention with visual and textual confounder dictionaries to remove confounding influence before final word prediction. The framework addresses the structural causal model by intervening at two key points: the projector (blocking `S ← Dv → h`) and the final transformer layer (blocking `h ← Dv/Dt → W`). The synergy ensures spurious correlations are addressed throughout the representation learning and prediction pipeline.

## Key Results
- **22.6% increase** in MME-Perception score compared to LLaVA-1.5
- **Reduction in CHAIR metrics** (CHAIRs, CHAIRi) indicating fewer hallucinations
- **Slight improvement** in VQAv2 and GQA while maintaining performance on MM-Vet, MMBench, VizWiz, TextVQA, and ScienceQA

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Causal-Driven Projector mitigates hallucinations by reducing representational entanglement in the visual pathway.
- Mechanism: The projector disentangles visual features of an object from frequently co-occurring objects via causal intervention. It employs cross-attention between input visual features and a pre-computed "confounder dictionary" (averaged visual features per object category). This intervention, based on a Normalized Weighted Geometric Mean (NWGM) approximation, aims to subtract the spurious correlation signal, preventing the model from activating an absent object's representation solely due to co-occurrence.
- Core assumption: The entanglement of object representations in the projector's output is a primary cause of object hallucinations, and these representations can be effectively disentangled via a learnable causal intervention module using a fixed confounder dictionary.
- Evidence anchors:
  - [abstract] "Our approach includes a Causal-Driven Projector in the visual pathway...These components work together to reduce spurious correlations caused by biased training data."
  - [section 4.3, Eq. 6, 7, 8] The equations detail the Causal-Driven Projector as `gf(fv) + gz(Ez[z])`, where `Ez[z]` is approximated by cross-attention with a confounder dictionary `D`.
  - [corpus] Corpus evidence on this specific projector mechanism is weak; related works like CLAIM and SAVE focus on attention intervention or visual enhancement in later stages.
- Break condition: If the confounder dictionary does not accurately capture spurious correlations (e.g., due to noise or dataset shift), the intervention may fail or introduce new biases.

### Mechanism 2
- Claim: The Causal Intervention Module in the final transformer layer purifies hidden states to block bias propagation into token predictions.
- Mechanism: This module operates in the final LLM layer, applying causal intervention to hidden states `h`. Similar to the projector, it uses cross-attention with separate visual and textual confounder dictionaries (`Dv`, `Dt`) to approximate the expectation `E[d|h]`. This process, grounded in the NWGM approximation of `P(W|do(h))`, aims to remove confounding influence of both visual and textual priors before final word prediction.
- Core assumption: Biases from co-occurrence patterns persist and affect final token predictions, and an intervention at the final layer can effectively deconfound these predictions without degrading general language capabilities.
- Evidence anchors:
  - [abstract] "and a Causal Intervention Module integrated into the final transformer layer of the language model."
  - [section 4.2, 4.3] The paper models the causal path `h → W` affected by confounders `Dv` and `Dt`, and proposes the intervention `CausalIntervention(h) = CrossAttn(h, Dv, Dv) + CrossAttn(h, Dt, Dt)`.
  - [corpus] Weak corpus evidence; related works address hallucination through contrastive alignment or self-augmentation, not this specific final-layer causal intervention.
- Break condition: If final-layer hidden states `h` no longer contain strong entangled representations (i.e., previous layers have already disentangled them), this module may provide marginal benefit or introduce instability.

### Mechanism 3
- Claim: The combined dual-path intervention across projector and final transformer layer provides complementary and more effective bias mitigation than either component alone.
- Mechanism: The framework addresses the structural causal model by intervening at two key points: the projector (blocking `S ← Dv → h`) and the final transformer layer (blocking `h ← Dv/Dt → W`). Early intervention in the projector prevents initial bias injection, while late intervention in the LLM cleanses residual correlations during contextual reasoning. The synergy ensures spurious correlations are addressed throughout the representation learning and prediction pipeline.
- Core assumption: The described causal structure accurately reflects how hallucinations are generated in MLLMs, and bias propagates sequentially through visual and linguistic pathways, making dual-path intervention necessary and effective.
- Evidence anchors:
  - [abstract] "These components work together to reduce spurious correlations..."
  - [section B, Table 4] Ablation study shows that the "Both-causal" model achieves the best performance (lowest CHAIR scores, highest MME-P) compared to "Only-transformer" or "Only-projection" variants.
  - [corpus] Related works like Deconfounded Reasoning for Multimodal Fake News Detection also apply causal intervention, but across different tasks and architectures, providing only tangential support.
- Break condition: If the two modules interfere with each other's operations (e.g., the projector's disentanglement creates representations that the final-layer module's confounder dictionaries cannot properly interpret), overall performance may degrade.

## Foundational Learning

- **Concept**: Causal Inference & Backdoor Adjustment
  - **Why needed here**: The core theoretical justification. The method relies on do-calculus to block "backdoor paths" in a causal graph, which represent how confounders (e.g., object co-occurrence frequency) create spurious correlations. Understanding this is necessary to interpret the paper's problem framing and solution.
  - **Quick check question**: Can you explain how a confounder variable creates a spurious correlation between an input and an output?

- **Concept**: Representation Disentanglement
  - **Why needed here**: The paper's goal is to disentangle hidden state representations of frequently co-occurring objects. The PCA visualizations are direct evidence of this mechanism. Understanding what disentanglement means in the context of neural network embeddings is critical.
  - **Quick check question**: In a high-dimensional embedding space, what does it mean for the representations of two objects to be "entangled"?

- **Concept**: Normalized Weighted Geometric Mean (NWGM) Approximation
  - **Why needed here**: The paper uses NWGM to make causal intervention (specifically, computing an expectation over all confounders) computationally tractable. It's the mathematical bridge between causal theory and practical neural network implementation using cross-attention.
  - **Quick check question**: How does the NWGM approximation simplify the backdoor adjustment formula for use in a neural network layer?

## Architecture Onboarding

- **Component map**:
  - **Input Image** → **CLIP Vision Encoder** (frozen) → produces `Fv` (visual features).
  - `Fv` → **Causal-Driven Projector** → produces disentangled visual tokens `S`.
    - Projector's core: `gf(fv)` (standard MLP) + `gz(Ez[z])` (cross-attention with visual confounder dictionary `D`).
  - `S` + Text Embeddings → **LLM (e.g., LLaMA-2)**.
  - **LLM Final Layer** → **Causal Intervention Module** → takes hidden states `h`, performs cross-attention with visual (`Dv`) and textual (`Dt`) confounder dictionaries → produces purified hidden states.
  - Purified `h` → **LM Head** → outputs word probabilities.

- **Critical path**:
  1. **Confounder Dictionary Construction:** Build first by running partial training on a non-causal model and aggregating features. Prerequisite for main training.
  2. **Causal-Driven Projector Training:** Active from pretraining start, shaping initial visual tokens.
  3. **Causal Intervention Module in Final Layer:** Active during all training phases, making final prediction deconfounded.

- **Design tradeoffs**:
  - **Accuracy vs. Compute:** NWGM-based cross-attention is more efficient than full marginalization but may be less precise.
  - **Modularity vs. Complexity:** Designed as modular addition to LLaVA, but introduces new components (dictionaries, attention layers) increasing model and training complexity.
  - **Performance on Different Tasks:** Improves perception and reduces hallucinations, but improvements on some reasoning tasks (TextVQA, ScienceQA) are marginal, suggesting potential tradeoff or limitation in scope.

- **Failure signatures**:
  - **Training Instability:** Ablation study shows independent projection matrices in cross-attention can cause large training losses; requires specific configurations (shared `Wk`/`Wv`).
  - **Noisy Confounder Estimation:** Confounders may be affected by noisy variables, leading to ineffective intervention.
  - **High Resource Requirements:** Substantial computational resource requirements listed as primary limitation.

- **First 3 experiments**:
  1. **Verify Confounder Dictionary Visualization:** Perform PCA on learned visual confounder dictionary `D`. Confirm frequently co-occurring objects (from training data) are not clustered together, validating dictionary structure.
  2. **Ablation on Dictionary Size/Composition:** Train models with different confounder dictionary sizes (e.g., K=50, 100, 500 objects) or different sampling strategies for dictionary entries to measure impact on hallucination metrics (CHAIR) and general performance (MME-P).
  3. **Layer-wise Entanglement Analysis:** For a trained Causal-LLaVA model, visualize (using PCA) hidden state distributions of "dining table" and its top co-occurring objects at different LLM layers (e.g., layers 5, 15, 40). Compare to original LLaVA visualizations in the paper (Figures 2, 3) to confirm progressive disentanglement.

## Open Questions the Paper Calls Out

- **Open Question 1**: How sensitive is the framework's performance to the selection of the confounder dictionary, specifically regarding the reliance on an intermediate checkpoint from a non-causal model?
  - **Basis in paper**: [explicit] Section 5.1 states, "The confounder dictionary estimation utilizes an intermediate checkpoint from a non-causal model trained for 0.1 epoch," but does not test other checkpoints or sources.
  - **Why unresolved**: It is unclear if the specific 0.1-epoch checkpoint is optimal or merely sufficient, and whether a fully trained or dynamically updated dictionary would improve or destabilize the disentanglement.
  - **What evidence would resolve it**: Ablation studies comparing hallucination metrics (CHAIR/POPE) when the confounder dictionary is derived from fully trained checkpoints or different training stages.

- **Open Question 2**: To what extent does the static nature of the confounder dictionary limit the model's robustness when encountering "noisy potential variables" in out-of-distribution environments?
  - **Basis in paper**: [explicit] Section 7 (Limitations) notes that "confounders may be affected by noisy potential variables, which are related with different environments and different datasets."
  - **Why unresolved**: The current method uses a fixed dictionary estimated from specific training data; it remains untested how well this static intervention generalizes to environments where the confounding variables differ significantly from the estimation set.
  - **What evidence would resolve it**: Evaluation of Causal-LLaVA on out-of-distribution datasets (e.g., distinct cultural contexts or synthetic data) to measure if hallucination rates increase due to confounder mismatch.

- **Open Question 3**: Does the assumption that frozen CLIP encoders are free from visual confounders hold for complex scenes, potentially leaving a residual bias pathway unblocked?
  - **Basis in paper**: [inferred] Section 4.2 claims the path $F_v \leftarrow D_v$ is blocked because CLIP is frozen and trained on diverse data, but this assumes the encoder itself has not learned spurious correlations that the causal projector cannot remove.
  - **Why unresolved**: If the frozen visual features $F_v$ already contain entangled semantic representations from CLIP's pre-training, the Causal-Driven Projector may be insufficient to fully disentangle them.
  - **What evidence would resolve it**: Analyzing the representation separability of object features before the projector (post-CLIP) on biased subsets of data to confirm the visual encoder output is truly unbiased.

## Limitations

- **Confounder Dictionary Validity**: The method's effectiveness depends critically on the confounder dictionary accurately capturing spurious correlations, but validation of the sampling strategy and construction process is limited.
- **Generalization Across Datasets**: Improvements are demonstrated on COCO-based metrics, but effectiveness on non-COCO domains with different object co-occurrence patterns remains untested.
- **Computational Overhead**: While "no significant overhead" is claimed, detailed timing and memory usage comparisons, particularly for the confounder estimation phase, are not provided.

## Confidence

- **High Confidence**: The core causal framework (identifying confounders, backdoor adjustment, NWGM approximation) is theoretically sound and well-grounded in established causal inference literature. The empirical improvements on hallucination metrics (MME-P increase of 22.6%, CHAIR reduction) are clearly demonstrated.
- **Medium Confidence**: The specific implementation details (confounder dictionary construction, cross-attention parameterization) appear reasonable but haven't been extensively validated. The ablation study supports the dual-module approach but doesn't explore all design dimensions.
- **Low Confidence**: The method's performance on complex reasoning tasks (TextVQA, ScienceQA-IMG) shows only marginal improvements, suggesting potential limitations in addressing deeper semantic understanding issues beyond surface-level co-occurrence biases.

## Next Checks

1. **Confounder Dictionary Sensitivity Analysis**: Systematically vary the number of confounder dictionary entries (K=50, 100, 200, 500) and evaluate the impact on hallucination metrics and computational efficiency to determine optimal trade-offs.
2. **Cross-Dataset Generalization Test**: Evaluate Causal-LLaVA on non-COCO datasets (e.g., medical imaging datasets, custom hallucination benchmarks) to verify the method's effectiveness beyond its training distribution.
3. **Layer-wise Causal Effect Measurement**: Use causal mediation analysis to quantify how much hallucination reduction occurs at each intervention point (projector vs. final layer) and whether the effects are truly complementary or potentially redundant.