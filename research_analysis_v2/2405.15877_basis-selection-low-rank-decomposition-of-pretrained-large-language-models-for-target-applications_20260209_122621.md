---
ver: rpa2
title: 'Basis Selection: Low-Rank Decomposition of Pretrained Large Language Models
  for Target Applications'
arxiv_id: '2405.15877'
source_url: https://arxiv.org/abs/2405.15877
tags:
- basel
- size
- compression
- accuracy
- llama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of deploying large language
  models (LLMs) on resource-constrained devices by introducing Basel, a low-rank decomposition
  method that compresses pretrained LLMs while maintaining target application performance.
  The core idea is to treat weight matrices as linear combinations of bases, retrain
  the importance of these bases using target application data, prune redundant bases,
  and add new bases to handle data distribution differences.
---

# Basis Selection: Low-Rank Decomposition of Pretrained Large Language Models for Target Applications

## Quick Facts
- arXiv ID: 2405.15877
- Source URL: https://arxiv.org/abs/2405.15877
- Authors: Yang Li; Daniel Agyei Asante; Changsheng Zhao; Ernie Chang; Yangyang Shi; Vikas Chandra
- Reference count: 30
- Primary result: Up to 2.7× additional model size reduction while maintaining accuracy on math/code tasks

## Executive Summary
This paper introduces Basel, a low-rank decomposition method that compresses pretrained LLMs while maintaining target application performance. The core idea is to treat weight matrices as linear combinations of bases, retrain the importance of these bases using target application data, prune redundant bases, and add new bases to handle data distribution differences. The method significantly outperforms existing low-rank compression techniques, achieving up to 2.7× additional model size reduction while maintaining comparable accuracy on mathematical reasoning and code generation tasks, and up to 4× greater size reduction on language modeling while improving performance.

## Method Summary
Basel compresses LLMs by decomposing weight matrices using SVD, retraining singular values on target application data, pruning unimportant bases, and adding new learnable bases to capture task-specific semantics. The method operates in phases: decomposition (SVD of linear layers), keeping (1 epoch retraining learnable parameters), pruning (100 iterative steps with 2 epochs retraining), recomposition (low-rank layer replacement), and post-fine-tuning (3 epochs). Unlike standard SVD, Basel freezes the original bases and only retrains their importance weights (singular values) plus learns new bases to compensate for pruning losses and distributional differences between pretraining and target data.

## Key Results
- Achieves up to 2.7× additional model size reduction compared to FWSVD while maintaining comparable accuracy on GSM8K and MATH
- Reaches 4× greater size reduction on WikiText-2 language modeling while improving perplexity
- Shows 100-step iterative pruning consistently outperforms 2-step pruning for compression ratios above 4×

## Why This Works (Mechanism)

### Mechanism 1
Retraining singular values on target application data identifies which bases are semantically relevant for that specific task, enabling selective pruning with controlled performance loss. SVD decomposes weight matrices W = USV^T = Σ s_i u_i v_i^T. Basel freezes the bases (u_i, v_i) from the pretrained model but relearns the singular values s_i using target application training data. Bases associated with small retrained singular values are pruned; those with large values are retained. This works because the retraining process amplifies task-relevant bases while diminishing irrelevant or harmful ones.

### Mechanism 2
Adding learnable new bases during compression compensates for information loss from pruning and captures semantics absent from the pretrained model. Beyond pruning original bases, Basel introduces learnable vectors ũ_j, ṽ_j trained alongside the retrained singular values. These serve two purposes: (1) recovering collective information lost when many individually minor bases are pruned, and (2) learning task-specific semantics not present in the original pretrained model due to distribution shift between pretraining and target data.

### Mechanism 3
Gradual multi-step pruning with interleaved fine-tuning allows the model to adapt progressively to parameter reduction, achieving deeper compression than single-step approaches. Algorithm 1 performs 100 pruning iterations, each time removing bases whose singular values fall below a threshold. After each pruning step, remaining learnable parameters are fine-tuned to compensate. This iterative adaptation enables the model to redistribute information across remaining bases rather than losing it irreversibly.

## Foundational Learning

- **Singular Value Decomposition (SVD)**
  - Why needed here: Basel fundamentally relies on SVD to express weight matrices as linear combinations of orthonormal bases. Understanding that W = Σ s_i u_i v_i^T, where s_i are importance weights and u_i v_i^T are direction matrices, is prerequisite to grasping the pruning logic.
  - Quick check question: Given a 4096×4096 weight matrix with rank 4096, if you retain only the top 500 singular values, what is the compressed parameter count compared to the original?

- **Low-Rank Factorization for Neural Networks**
  - Why needed here: Basel replaces each original linear layer with two smaller layers (S'V'^T and U'), reducing parameters from nm to (n+m)r'. Understanding this architectural transformation is essential for implementation.
  - Quick check question: For a weight matrix of size 4096×11008 (Llama MLP up-projection), what rank r' achieves 8× compression in parameter count?

- **Basis Interpretation as Semantic Filters**
  - Why needed here: The paper claims bases carry semantic meaning (e.g., Python tokens, math symbols). This conceptualization justifies why selective basis retention preserves task performance—different tasks activate different semantic filters.
  - Quick check question: If a basis de-embeds to tokens like "def", "class", "import", would you expect it to be important for a mathematical reasoning task? What would the retrained singular value likely show?

## Architecture Onboarding

- **Component map:**
Input: Pretrained LLM → SVD Decomposition (per layer)
                           ↓
                    Linear Combination Form: W = Σ s_i u_i v_i^T + Σ ũ_j ṽ_j^T
                           ↓
                    Retraining Phase (KeepingEpoch + PruningEpoch)
                    - Learnable: s̃_i (singular values), ũ_j, ṽ_j (new bases)
                    - Frozen: u_i, v_i (original bases)
                    - Iterative pruning based on singular value magnitude
                           ↓
                    Final SVD & Layer Replacement
                    - Compute final low-rank W̃
                    - Replace original layer with two sequential low-rank layers
                           ↓
                    Post-FineTuning Phase
                           ↓
Output: Compressed Model M'

- **Critical path:**
  1. SVD decomposition of all linear layers (excluding embeddings)
  2. Singular value retraining on target data (freezing bases)
  3. Progressive basis pruning (100 iterations recommended)
  4. Post-compression fine-tuning (3 epochs)

- **Design tradeoffs:**
  - **KeepRatio vs. Performance**: Lower ratios (5-30%) enable deeper compression but require more careful tuning; paper shows Basel maintains accuracy at ratios where SVD/FWSVD collapse
  - **Additional Dimension (r̃)**: Default 32; higher values improve accuracy recovery under extreme compression but reduce net compression gain
  - **Pruning Frequency**: 100 steps vs. 2 steps shows significant accuracy difference above 4× compression (Figure 14)
  - **Freezing vs. Free Bases**: Freezing bases (recommended) reduces training time by 33% and improves accuracy (Figure 15)

- **Failure signatures:**
  - **Accuracy collapse below baseline**: Indicates KeepRatio too aggressive for task complexity; increase ratio or add additional dimension
  - **Perplexity explosion (>1000)**: Seen in SVD/FWSVD baselines at high compression; Basel should maintain perplexity <20 even at 10× compression (Table 12)
  - **Training instability during retraining**: May indicate learning rate too high for singular value updates; reduce by 10×

- **First 3 experiments:**
  1. **Sanity check on single layer**: Apply Basel to one attention layer (e.g., W_O) with KeepRatio=50%, verify SVD reconstruction error is lower than standard SVD after retraining on 1000 samples of target data
  2. **Compression sweep with baseline comparison**: Run Basel on Llama-2-7B for GSM8K with KeepRatio ∈ {70%, 50%, 30%, 15%, 5%}, compare accuracy curve against SVD and FWSVD; expect Basel to match baseline at 70% and diverge positively below 30%
  3. **Ablation on additional dimension**: Fix KeepRatio=15%, vary r̃ ∈ {0, 16, 32, 64} on GSM8K; expect plateau or degradation beyond 32, confirming r̃=32 is near-optimal

## Open Questions the Paper Calls Out

### Open Question 1
Does jointly pruning the W_V and W_O attention matrices yield better compression results than the current independent pruning strategy? The current implementation treats these matrices independently, potentially missing structural optimizations inherent to the attention mechanism. Evidence: A comparative study measuring accuracy and model size when applying joint decomposition versus independent pruning on the same LLM architectures.

### Open Question 2
How accurately can the semantic meaning of specific bases be interpreted and mapped to model capabilities? While the paper shows individual bases correspond to tokens (e.g., Python keywords), a comprehensive mapping between bases and semantic concepts remains unexplored. Evidence: Application of interpretability frameworks (e.g., network dissection) to quantify the semantic alignment of the retained bases.

### Open Question 3
Can combining Basel's basis selection with layer-wise rank optimization techniques achieve superior compression performance? Basel currently focuses on selecting important bases globally without explicitly optimizing the rank distribution across different layers. Evidence: Experiments integrating Basel with a per-layer rank optimizer to determine if the combined approach outperforms either method in isolation.

## Limitations

- Missing optimizer details (learning rates, schedules) that significantly affect reproducibility of singular value retraining
- Unclear implementation details of baseline methods (SVD/FWSVD) making fair comparison difficult
- Claims of "improving performance while compressing" on WikiText-2 contradict typical compression trade-offs and require careful validation

## Confidence

- **High Confidence**: The core mechanism of SVD-based basis representation and the general framework of retraining singular values while freezing bases is well-established and theoretically sound
- **Medium Confidence**: The empirical results showing Basel outperforming baselines on GSM8K and MATH at high compression ratios are convincing but lack detailed hyperparameter specifications
- **Low Confidence**: The claim of "improving performance" while compressing (particularly on WikiText-2) is surprising and would require careful examination of the evaluation methodology

## Next Checks

1. **Optimizer Sensitivity Analysis**: Run Basel with varying learning rates (1e-4, 5e-5, 1e-5) during the singular value retraining phase on a single layer to determine the optimal setting, then verify that the reported compression gains are consistent across these settings.

2. **Baseline Implementation Verification**: Implement FWSVD and SVD baselines with identical training configurations (optimizer, learning rate schedule, batch size) as Basel, then reproduce the compression-vs-accuracy curves on GSM8K to verify the 2.7× improvement claim is robust to implementation details.

3. **Pruning Step Granularity Test**: Systematically vary the number of pruning iterations (2, 10, 50, 100) while keeping total pruning budget constant, measuring accuracy degradation at each compression ratio to validate that the 100-step approach provides measurable benefits over fewer steps.