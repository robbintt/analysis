---
ver: rpa2
title: 'Neural Actor-Critic Methods for Hamilton-Jacobi-Bellman PDEs: Asymptotic Analysis
  and Numerical Studies'
arxiv_id: '2507.06428'
source_url: https://arxiv.org/abs/2507.06428
tags:
- control
- actor
- critic
- problem
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a neural actor-critic method for solving high-dimensional
  Hamilton-Jacobi-Bellman (HJB) PDEs from stochastic control theory. The approach
  combines a critic network trained via the Q-PDE algorithm to approximate the value
  function and an actor network trained by minimizing the integral of the estimated
  Hamiltonian over the domain.
---

# Neural Actor-Critic Methods for Hamilton-Jacobi-Bellman PDEs: Asymptotic Analysis and Numerical Studies

## Quick Facts
- **arXiv ID:** 2507.06428
- **Source URL:** https://arxiv.org/abs/2507.06428
- **Reference count:** 38
- **Primary result:** Neural actor-critic method achieves mean square errors as low as 1.38 × 10⁻⁸ for critic and 5.97 × 10⁻³ for actor in 200-dimensional LQR problems

## Executive Summary
This paper introduces a neural actor-critic framework for solving high-dimensional Hamilton-Jacobi-Bellman (HJB) PDEs arising in stochastic control theory. The method combines a critic network trained via the Q-PDE algorithm to approximate the value function with an actor network trained to minimize the estimated Hamiltonian. The critic architecture incorporates hard boundary constraints through a specific network design, while the training dynamics are analyzed in the infinite-width limit. Theoretical guarantees show convergence to optimal solutions under a convexity-like assumption on the Hamiltonian, with numerical experiments demonstrating accurate solutions in up to 200 dimensions.

## Method Summary
The method solves HJB PDEs using a two-network architecture: a critic Q_ϕ(x) = Z_ϕ(x)·η(x) + g(x) that approximates the value function with hard boundary constraints, and an actor U_θ(x) that approximates the optimal control. The critic is trained using a biased Q-PDE gradient to reduce computational cost, while the actor minimizes the integral of the estimated Hamiltonian. Both networks are one-hidden-layer MLPs trained iteratively with Adam optimizer. The theoretical analysis shows that as network widths approach infinity, the training dynamics converge to an infinite-dimensional ODE, and under a convexity-like assumption on the Hamiltonian, any fixed point corresponds to the optimal solution.

## Key Results
- Achieves mean square errors as low as 1.38 × 10⁻⁸ for the critic and 5.97 × 10⁻³ for the actor in 200-dimensional LQR problems
- Demonstrates accurate solutions in up to 200 dimensions, significantly outperforming existing approaches
- Proves that under a convexity-like assumption on the Hamiltonian, any fixed point of the infinite-width training dynamics corresponds to the true optimal solution
- Shows that the hard-constrained boundary architecture eliminates the need for separate loss terms to enforce boundary conditions

## Why This Works (Mechanism)

### Mechanism 1: Hard-Constrained Boundary Architecture
The critic network Q_ϕ(x) = Z_ϕ(x)·η(x) + g(x) guarantees exact satisfaction of boundary conditions without requiring separate loss terms. The factorization forces Q_ϕ|_∂Ω = g automatically by construction.

### Mechanism 2: Biased Q-PDE Gradient with Gradient Clipping
The critic uses a simplified gradient estimator dϕ/dt ∝ -∫F^N*(L[U]Q_ϕ(x))·∇_ϕ(-Q_ϕ)dμ that reduces computational cost by avoiding second-order derivatives in the backward pass.

### Mechanism 3: Wide-Network Convergence to Limiting ODE with Hamiltonian Convexity
Under a convexity-like condition on the Hamiltonian, any fixed point of the infinite-width training dynamics ODE corresponds to the true optimal solution, as formalized in Theorem 3.13.

## Foundational Learning

- **Hamilton-Jacobi-Bellman PDEs and Verification Theorem:**
  - Why needed: Solutions to HJB equations characterize optimal value functions and controls in stochastic control
  - Quick check question: If V ∈ C² solves the HJB equation with boundary condition V|_∂Ω = g, what does the verification theorem guarantee about V?

- **Neural Tangent Kernel (NTK) Regime:**
  - Why needed: Theoretical guarantees rely on analyzing training dynamics as network widths → ∞, where neural networks behave like kernel methods with the NTK
  - Quick check question: In the infinite-width limit, what type of mathematical object does the training dynamics converge to for this actor-critic method?

- **Feynman-Kac Formula and Linear PDEs:**
  - Why needed: For a fixed control u, the value function V^u satisfies a linear PDE via Feynman-Kac, which the critic learns to approximate
  - Quick check question: How does the Feynman-Kac theorem connect linear PDEs to expectations of stochastic processes?

## Architecture Onboarding

- **Component map:**
  ```
  Critic Q_ϕ(x) = Z_ϕ(x) · η(x) + g(x)
    └── Z_ϕ(x): 1-hidden-layer network, width N*, parameters {c_i, w_i, b_i}
    └── η(x): Auxiliary function (domain-specific, e.g., R² - ||x||² for ball domains)
    └── g(x): Interpolation of boundary condition

  Actor U_θ(x): 1-hidden-layer network, width N, parameters {h_i, v_i, z_i}
    └── Output dimension matches action space A
  ```

- **Critical path:**
  1. Initialize actor U_θ₀ and critic Q_ϕ₀ (parameters satisfy Assumption 3.5)
  2. Each iteration: Critic step (fix U_θ, update ϕ via Monte Carlo estimate of clipped Q-PDE gradient), then Actor step (fix Q_ϕ, update θ via Monte Carlo estimate of clipped Hamiltonian gradient)
  3. Repeat until terminal time Λ or convergence

- **Design tradeoffs:**
  - **Hard vs. soft boundary constraint:** Hard constraint (η·Z + g) eliminates boundary loss but requires explicit η; soft constraint (add boundary loss term) is more flexible but introduces another hyperparameter
  - **Online vs. offline learning:** Online (α_t, ω_t > 0 simultaneously) may be less stable; offline (alternate training) is more controlled but slower
  - **Scaling parameter β ∈ (1/2, 1):** Affects convergence rate and width requirements; paper uses β in this range for theoretical guarantees

- **Failure signatures:**
  - **Non-convex Hamiltonian (Problem 2A):** Actor converges to local minimum far from optimal; critic may appear accurate but actor MSE remains high (~3.35 vs. true optimum)
  - **High drift sensitivity (Problem 3):** Actor outputs explode to action space boundaries; modify actor loss to `max(H(U,Q)(x) - γQ(x), δ)` to prevent over-minimization
  - **Insufficient width for high-dim actor:** Actor learning R^d → R^d function is harder than scalar critic; relative errors increase with dimension (Table 1 shows actor RE: 6.12×10⁻⁴ → 1.64×10⁻² from 10d to 200d)

- **First 3 experiments:**
  1. **10D LQR problem** (Section 4.1): Start with known analytic solution; validate that critic MSE reaches ~10⁻⁷ and actor MSE ~10⁻⁴; compare against baseline [31] method
  2. **Problem 2A vs 2B comparison** (Section 4.2.2-4.2.3): Demonstrate importance of convexity assumption; 2A should fail (actor stuck at local min), 2B should succeed (convex Hamiltonian)
  3. **Problem 3 with modified Hamiltonian** (Section 4.2.4): Test handling of sensitive drift; compare standard loss vs. Equation (36) modified loss to prevent actor explosion

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the convergence of the infinite-dimensional limit ODE (Equation 25) to a fixed point be rigorously proven? The authors state "proving it remains an open problem," suggesting it may be possible if the operator family {L_u}_{u ∈ U} is uniformly strongly negatively monotone.

- **Open Question 2:** Do the asymptotic convergence guarantees hold for HJB equations with controlled jumps or in multi-agent settings? Section 1.1 mentions the mathematical tools "can easily be extended to multi-agent settings and to settings containing jumps."

- **Open Question 3:** Is the modified actor loss function (Equation 36) theoretically sound, or does it introduce approximation errors relative to the true optimal control? The modification is presented as a practical fix for numerical instability without analysis of whether it preserves the fixed-point properties guaranteed in Theorem 3.13.

## Limitations

- The theoretical analysis relies heavily on the convexity-like assumption (Assumption 3.11) on the Hamiltonian, which represents a significant restriction
- Numerical results are limited to specific test cases (LQR problems) with known solutions, leaving open questions about performance on more complex, realistic control problems
- Computational efficiency claims lack comprehensive benchmarking against other neural PDE solvers
- Analysis focuses on infinite-width limit, with the gap between theory and finite-width practice partially unquantified

## Confidence

- **High confidence:** The hard-constrained boundary architecture (Mechanism 1) and the critic's biased gradient formulation (Mechanism 2) are well-specified and empirically validated
- **Medium confidence:** The theoretical convergence analysis in the infinite-width limit (Mechanism 3) is rigorous, but its practical implications for finite-width networks require further investigation
- **Medium confidence:** The numerical results demonstrate significant improvements over baseline methods, but the limited scope of test problems and lack of comprehensive benchmarking reduce confidence in general applicability claims

## Next Checks

1. **Convexity assumption validation:** Systematically test the algorithm on problems with varying degrees of Hamiltonian convexity/non-convexity to quantify the impact of Assumption 3.11 on solution quality and convergence behavior

2. **Dimensionality scaling study:** Extend numerical experiments beyond 200 dimensions to characterize the method's scalability limits and compare against alternative approaches for very high-dimensional HJB PDEs

3. **Robustness to initialization and hyperparameters:** Conduct ablation studies varying initial network parameters, learning rates, and the scaling parameter β to assess the method's sensitivity to these choices and identify robust default configurations