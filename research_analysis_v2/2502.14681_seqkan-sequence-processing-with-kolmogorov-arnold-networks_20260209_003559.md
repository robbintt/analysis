---
ver: rpa2
title: 'seqKAN: Sequence processing with Kolmogorov-Arnold Networks'
arxiv_id: '2502.14681'
source_url: https://arxiv.org/abs/2502.14681
tags:
- seqkan
- functions
- network
- more
- sequence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces seqKAN, a new KAN architecture for processing
  sequences that is more faithful to the KAN framework by avoiding MLP-inspired structures
  like recurrence cells. Unlike prior KAN approaches for sequence processing, seqKAN
  achieves superior performance while maintaining interpretability.
---

# seqKAN: Sequence processing with Kolmogorov-Arnold Networks

## Quick Facts
- arXiv ID: 2502.14681
- Source URL: https://arxiv.org/abs/2502.14681
- Reference count: 10
- Primary result: seqKAN outperforms RNN, LSTM, TKAN, and symbolic regression on physics-based sequence tasks while maintaining interpretability

## Executive Summary
This paper introduces seqKAN, a Kolmogorov-Arnold Network (KAN) architecture for sequence processing that avoids MLP-inspired recurrence cells while achieving superior extrapolation performance. Unlike prior approaches that hybridize KANs with RNN/LSTM structures, seqKAN maintains pure KAN principles with learnable spline-based activation functions on edges. The architecture demonstrates strong performance on a complex physics-based pendulum task with variable string length, achieving ROC-AUC scores of 0.96 and 0.90 on energy increasing and close-to-equilibrium labels respectively in extrapolation scenarios.

## Method Summary
seqKAN processes sequences through a hidden-state layer that maps current inputs and previous hidden state to a new hidden state, followed by an output layer that maps the hidden state and previous inputs to outputs. The architecture uses KAN layers with edge-based learnable activation functions (weighted sums of SiLU and B-splines) rather than node-based activations. A key design choice is routing previous timestep inputs directly to the output layer, preserving hidden-state capacity for longer-term dependencies. The model is trained on a pendulum dataset with variable string length as a hidden variable, using binary cross-entropy loss and evaluated via ROC-AUC and PR-AUC metrics.

## Key Results
- seqKAN achieves ROC-AUC scores of 0.96 (energy increasing) and 0.90 (close to equilibrium) on extrapolation tasks
- Outperforms RNN, LSTM, TKAN, and symbolic regression baselines on both interpolation and extrapolation scenarios
- Maintains interpretability through transparent spline structure that reveals learned physical dependencies
- Demonstrates capability to capture underlying physics rather than surface correlations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pure KAN recurrency without MLP-style gated cells can achieve superior extrapolation performance while maintaining interpretability
- Core assumption: Forcing all knowledge into learned spline functions rather than opaque MLP weights produces representations that capture underlying causal structure
- Evidence: seqKAN outperforms hybrid architectures like TKAN; avoids MLP-inspired structures while maintaining performance
- Break condition: Tasks requiring complex multiplicative gating that cannot be decomposed into additive combinations of univariate functions

### Mechanism 2
- Claim: Explicitly routing previous inputs directly to output layer improves performance over relying on hidden state to preserve them
- Core assumption: Immediate temporal dependencies are universally important and should not compete for hidden-state representation
- Evidence: Preliminary experiments showed seqKAN/wide wasted degrees of freedom learning label-specific functions rather than shared structure
- Break condition: Tasks where t-1 inputs are not meaningfully more important than earlier timesteps

### Mechanism 3
- Claim: KAN's interpretable spline structure enables discovery of hidden variables governing non-stationary dynamics
- Core assumption: True underlying dynamics can be reasonably approximated by additive combinations of univariate functions
- Evidence: Learned expressions correlate well with analytically-derived physics formulas; maintains high performance on extrapolation
- Break condition: When target function requires non-additive compositions that KAN's structure cannot directly represent

## Foundational Learning

- **Kolmogorov-Arnold Representation Theorem (KAT)**
  - Why needed: KANs are theoretically grounded in KAT, which proves multivariate functions can be decomposed into sums of univariate functions
  - Quick check: Can you explain why KANs use learnable activation functions on edges instead of fixed activations at nodes?

- **RNN Hidden-State Dynamics**
  - Why needed: seqKAN borrows hidden-state recurrence pattern from RNNs; understanding h_t = f(X_t, h_{t-1}) is essential
  - Quick check: What information does an RNN hidden state carry, and what happens if it becomes a bottleneck?

- **B-Spline Function Approximation**
  - Why needed: KAN activation functions are weighted sums of B-splines and SiLU; understanding spline control points is essential for interpretation
  - Quick check: How does a spline's number of control points affect its expressive capacity vs. overfitting risk?

## Architecture Onboarding

- **Component map**: θ_t, ω_t, h_{t-1} -> Φ_h (hidden layer) -> h_t; h_t, θ_{t-1}, ω_{t-1} -> Φ_o (output layer) -> y_t
- **Critical path**: Prepare sequence input -> Compute hidden state via Φ_h -> Compute outputs via Φ_o using hidden state + previous inputs -> Visualize learned splines for analysis
- **Design tradeoffs**: seqKAN vs seqKAN/wide (hardwired vs. wider hidden state); network depth vs. interpretability; spline grid resolution vs. expressivity
- **Failure signatures**: seqKAN/wide shows good interpolation but poor extrapolation with label-isolated functions; additive limitations prevent exact physics matching; hybrid architectures may store knowledge in opaque MLP portions
- **First 3 experiments**: 1) Reproduce pendulum task and verify ROC-AUC > 0.90 on extrapolation with physics-revealing splines; 2) Compare seqKAN vs seqKAN/wide on same data to measure performance gap and inspect learned functions; 3) Test on synthetic dataset with known formula to assess representation fidelity

## Open Questions the Paper Calls Out
- How does seqKAN performance and interpretability compare to Transformer-inspired KAN architecture on real-world sequence tasks?
- To what extent does the additive structure of seqKAN limit its ability to model systems dominated by multiplicative interactions?
- Can human-computer interaction methods effectively assist non-expert operators in extracting symbolic expressions from seqKAN networks?

## Limitations
- The learned additive expressions only approximately correlate with true physics rather than structurally matching them
- Claim of being "more faithful to KAN framework" lacks corpus comparison data for validation
- Architectural choice of routing previous inputs directly to output has limited validation beyond preliminary experiments
- Symbolic regression baseline uses only final timestep inputs rather than full sequences

## Confidence
- **High Confidence**: seqKAN achieves superior interpolation and extrapolation performance compared to RNN/LSTM/TKAN baselines on pendulum task
- **Medium Confidence**: seqKAN's interpretable spline structure reveals meaningful physical dependencies in pendulum system
- **Low Confidence**: seqKAN's extrapolation success necessarily implies discovery of underlying physical laws

## Next Checks
1. **Physics Formula Fidelity Test**: Create synthetic sequence datasets with known ground-truth formulas and quantitatively compare learned spline expressions to exact ground truth
2. **Hidden Variable Recovery Analysis**: Systematically vary string length formula L(t) and measure how seqKAN's extrapolation performance degrades
3. **Architectural Ablation Study**: Implement and test alternative seqKAN variants including no previous-input routing, deeper hidden state stacks, and explicit hidden variable inputs