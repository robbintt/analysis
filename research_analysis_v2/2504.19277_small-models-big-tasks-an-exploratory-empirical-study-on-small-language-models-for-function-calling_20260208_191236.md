---
ver: rpa2
title: 'Small Models, Big Tasks: An Exploratory Empirical Study on Small Language
  Models for Function Calling'
arxiv_id: '2504.19277'
source_url: https://arxiv.org/abs/2504.19277
tags:
- function
- prompt
- slms
- arxiv
- edge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates small language models (SLMs) for function\
  \ calling in resource-constrained environments. The authors benchmark five top-performing\
  \ SLMs (1.35B\u20133.82B parameters) using zero-shot, few-shot, and fine-tuned approaches\
  \ on a diverse 60K-sample dataset, supplemented with edge-device experiments (Snapdragon\
  \ 8 Gen 2)."
---

# Small Models, Big Tasks: An Exploratory Empirical Study on Small Language Models for Function Calling

## Quick Facts
- arXiv ID: 2504.19277
- Source URL: https://arxiv.org/abs/2504.19277
- Authors: Ishan Kavathekar; Raghav Donakanti; Ponnurangam Kumaraguru; Karthik Vaidhyanathan
- Reference count: 40
- One-line primary result: Small language models improve function calling performance from zero-shot to fine-tuning, but most struggle with structured output compliance despite achieving up to 99.44% JSON parsability post-fine-tuning.

## Executive Summary
This study benchmarks five small language models (1.35B–3.82B parameters) for function calling in resource-constrained environments. The authors evaluate zero-shot, few-shot, and fine-tuned approaches on a 60K-sample dataset, supplemented with edge-device experiments on Snapdragon 8 Gen 2. Results show that while SLMs significantly improve performance through fine-tuning, they face challenges with JSON output compliance and high latency on edge devices. Deepseek-Coder-1.3B-instruct achieved the highest performance (99.44% JSON parsability, 85.43% task accuracy) post-fine-tuning, but edge deployment remained impractical with latencies up to 70.55s.

## Method Summary
The study evaluates five SLMs (Deepseek-Coder-1.3B-instruct, Phi-3-mini-4k-instruct, Phi-2, Starcoder2-3B, and Stable-code-3B) on the Salesforce XLAM Function Calling dataset (60,000 samples). The models are tested using zero-shot, few-shot (3 examples), and fine-tuned approaches. Fine-tuning employs LoRA (rank=8, alpha=8, dropout=0.05) with 2 epochs on a single RTX A5000 GPU. Edge experiments use GGUF Q4_K_M quantization on Snapdragon 8 Gen 2. Performance is measured across six custom metrics: JSON Parsability, Task Accuracy (F1), Correct Ratio, Function Selection Performance, Argument Completeness Score, and Argument Value Correctness.

## Key Results
- Deepseek-Coder-1.3B-instruct achieved 99.44% JSON parsability and 85.43% task accuracy post-fine-tuning, the highest among all models
- Most SLMs showed significant improvement from zero-shot to fine-tuning, with Deepseek-Coder improving by 67–80% across metrics
- Edge deployment resulted in extreme latency penalties (up to 20x slower than servers), with Phi-3-mini requiring 364.51s compared to 3.22s on servers
- Prompt injection caused minor performance degradation, indicating robustness in fine-tuned models

## Why This Works (Mechanism)

### Mechanism 1: Parameter-Efficient Fine-Tuning for Schema Alignment
Fine-tuning resolves uncontrolled generation and formatting issues by constraining the output distribution to the required JSON schema. LoRA (rank 8) updates a small subset of weights, shifting the model's prior from general code completion to the syntactic rigidity of function calling. This process likely reduces the probability of generating conversational filler or malformed code.

### Mechanism 2: Few-Shot Prompting as a Contextual Anchor
Providing task-specific examples (few-shot) guides the model to map natural language queries to function signatures more effectively than zero-shot. The inclusion of three query-response examples serves as a "demonstration" mechanism, activating in-context learning capabilities and providing templates for both logic and syntax.

### Mechanism 3: Latency Scaling on Edge Hardware
Inference latency on edge devices scales non-linearly with model size and precision. While quantization reduces memory footprint (5x reduction), the lack of powerful GPUs/NPUs creates bottlenecks that increase latency by 10x-20x compared to server-grade GPUs, primarily due to memory bandwidth limitations.

## Foundational Learning

- **Concept:** **LoRA (Low-Rank Adaptation)**
  - **Why needed here:** The study relies on LoRA to fine-tune SLMs efficiently on a single GPU (RTX A5000), making the "Fine-tuned" setting possible without full parameter updates.
  - **Quick check question:** Why does updating low-rank matrices reduce the computational cost of fine-tuning compared to full parameter updates?

- **Concept:** **Quantization (GGUF)**
  - **Why needed here:** Edge device experiments depend entirely on compressing models to 4-bit precision (Q4_K_M) to fit into the limited memory of the Snapdragon device.
  - **Quick check question:** How does converting model weights from float16 to 4-bit integer precision affect the model's theoretical "knowledge" capacity versus its memory footprint?

- **Concept:** **Function Calling Metrics (JSON Parsability vs. Task Accuracy)**
  - **Why needed here:** The paper distinguishes between syntactically generating valid JSON (Parsability) and semantically selecting correct arguments (Task Accuracy), which are often confused in production pipelines.
  - **Quick check question:** If a model achieves 100% Task Accuracy but only 50% JSON Parsability, why is it likely useless in a production software pipeline?

## Architecture Onboarding

- **Component map:** Prompt Constructor -> SLM (fp16 or Q4_K_M GGUF) -> JSON Parser & Validator -> Tool Executor
- **Critical path:** The transition from Zero-Shot to Fine-Tuning is critical for reliability, as JSON Parsability is near zero for most models without fine-tuning.
- **Design tradeoffs:**
  - Few-Shot vs. Fine-Tuning: Few-shot offers flexibility but is fragile to prompt injection; fine-tuning offers robustness but requires retraining.
  - Server vs. Edge: Server deployment offers high speed (3-5s) but requires data transmission; edge deployment offers privacy but suffers extreme latency (60s+).
- **Failure signatures:**
  - Uncontrolled Generation: Model generates correct JSON but continues with conversational text or new queries.
  - Format Hallucination: Model produces Python dictionaries instead of JSON or misses closing brackets.
  - Injection Drift: Random noise in prompt causes incorrect argument selection in few-shot mode.
- **First 3 experiments:**
  1. Run Deepseek-Coder-1.3B in zero-shot mode to confirm <10% JSON parsability rate.
  2. Implement 3-shot prompting to verify ~89% JSON parsability increase.
  3. Introduce prompt injection string to measure performance drop between few-shot and fine-tuned models.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can constrained decoding strategies (e.g., grammar-constrained sampling) effectively mitigate uncontrolled generation and improve JSON format adherence in SLMs?
  - **Basis:** Page 8 suggests "grammar-constrained sampling" as needed advancement for decoding strategies.
  - **Why unresolved:** Current approaches resulted in low JSON parsability due to models' tendency to generate beyond required answers.
  - **Evidence needed:** Experiments applying constrained decoding to poorly performing models showing improved metrics without fine-tuning.

- **Open Question 2:** How resilient are fine-tuned SLMs to complex adversarial attacks, such as multi-turn prompt injection, compared to simple direct perturbations tested?
  - **Basis:** Page 8 notes that "multi-turn prompt injection has the potential to manipulate the model" and calls for robust defense mechanisms.
  - **Why unresolved:** Study limited analysis to simple injection of random characters, leaving susceptibility to sophisticated attacks unknown.
  - **Evidence needed:** Evaluation against multi-turn adversarial benchmarks to measure performance degradation and potential leakage.

- **Open Question 3:** Does performance (latency and memory usage) of SLMs for function calling generalize across diverse edge hardware architectures beyond Snapdragon 8 Gen 2?
  - **Basis:** Page 10 suggests "Exploring additional edge devices" as future work; Page 9 acknowledges results may differ on different configurations.
  - **Why unresolved:** Study restricted to single Qualcomm Innovation Development Kit, limiting generalizability.
  - **Evidence needed:** Benchmarking GGUF quantized models on various chipsets (Apple Silicon, other NPUs) to determine if high latency persists.

- **Open Question 4:** Can hybrid edge-cloud architectures successfully balance low latency requirements with high reliability of server-based inference?
  - **Basis:** Page 9 discusses "hybrid deployment strategies" and "edge-cloud collaboration" to reduce latency while maintaining robustness.
  - **Why unresolved:** Paper analyzed edge and server environments in isolation, highlighting severe latency penalty on edge devices.
  - **Evidence needed:** System-level experiments measuring end-to-end Task Accuracy and latency in setups routing simple calls to on-device SLM and complex queries to cloud.

## Limitations
- Dataset filtering process for context length varies by model, potentially introducing comparison biases
- Prompt templates for zero-shot and few-shot settings are not fully specified, affecting reproducibility
- Edge device experiments use single hardware configuration (Snapdragon 8 Gen 2), limiting generalizability
- Study focuses on structured function calling tasks and may not generalize to open-ended applications

## Confidence

**High Confidence:**
- JSON parsability improvements from zero-shot to fine-tuning are well-supported by quantitative metrics
- Latency scaling patterns between server and edge deployments are robustly demonstrated

**Medium Confidence:**
- Attribution of performance gains to LoRA learning JSON schema structure (not memorization) is plausible but untested
- Claim that prompt injection causes minor performance degradation is supported but requires real-world validation

**Low Confidence:**
- Mechanism explaining Phi-3-mini's extreme 364.51s edge latency is not sufficiently explored
- Study doesn't investigate whether different LoRA ranks or fine-tuning durations would yield better results

## Next Checks
1. **Schema Transferability Test:** Evaluate fine-tuned Deepseek-Coder-1.3B-instruct on held-out functions with different argument patterns to verify LoRA learns general JSON schema structure rather than dataset-specific patterns.

2. **Prompt Template Variation:** Systematically test alternative prompt structures to quantify sensitivity of few-shot performance to template design, particularly for Phi-3-mini.

3. **Hardware Acceleration Impact:** Repeat edge device latency measurements with NPU acceleration enabled on Snapdragon 8 Gen 2 to determine if latency penalty is computational or software optimization related.