---
ver: rpa2
title: 'Nash Policy Gradient: A Policy Gradient Method with Iteratively Refined Regularization
  for Finding Nash Equilibria'
arxiv_id: '2510.18183'
source_url: https://arxiv.org/abs/2510.18183
tags:
- nash
- policy
- games
- equilibrium
- nashpg
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Nash Policy Gradient (NashPG), a regularization-based
  algorithm for finding Nash equilibria in two-player zero-sum imperfect-information
  games. Unlike prior methods that decay regularization strength, NashPG fixes a large
  regularization coefficient and iteratively refines the reference policy, ensuring
  strictly monotonic improvement and convergence to a Nash equilibrium.
---

# Nash Policy Gradient: A Policy Gradient Method with Iteratively Refined Regularization for Finding Nash Equilibria

## Quick Facts
- **arXiv ID**: 2510.18183
- **Source URL**: https://arxiv.org/abs/2510.18183
- **Reference count**: 40
- **Primary result**: A policy gradient algorithm (NashPG) that finds Nash equilibria in two-player zero-sum imperfect-information games by fixing a large regularization coefficient and iteratively refining the reference policy, achieving lower exploitability than prior model-free methods.

## Executive Summary
Nash Policy Gradient (NashPG) introduces a novel approach to finding Nash equilibria in two-player zero-sum imperfect-information games. Unlike previous methods that decay regularization strength, NashPG fixes a large regularization coefficient and iteratively refines the reference policy, ensuring strictly monotonic improvement and convergence to a Nash equilibrium. The algorithm demonstrates lower exploitability than prior model-free methods on benchmark games and scales to large domains like Battleship and No-Limit Texas Hold'em, where it attains higher Elo ratings.

## Method Summary
NashPG is a regularization-based algorithm that finds Nash equilibria by iteratively updating a reference policy rather than decaying regularization strength. The method uses a KL-regularized objective where the policy maximizes payoff while being penalized for deviating from a reference policy. After a fixed number of inner-loop policy gradient steps, the reference policy is updated to the current policy. This approach ensures stable convergence even in stochastic environments where traditional methods with decaying regularization often diverge. The algorithm is implemented as a minor modification to standard policy gradient frameworks like PPO.

## Key Results
- Achieves lower exploitability than prior model-free methods on seven benchmark games including Kuhn Poker, Leduc Poker, and No-Limit Texas Hold'em
- Scales to large domains where traditional methods fail, achieving 1800+ Elo rating in No-Limit Texas Hold'em
- Demonstrates theoretical convergence guarantees through iterative refinement of reference policies
- Maintains stability in stochastic gradient settings where decaying regularization leads to divergence

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Iteratively updating the reference policy guarantees strictly monotonic improvement toward a Nash equilibrium, bypassing the need to decay regularization strength.
- **Mechanism**: The paper defines a "Regularized VI Operator" M(ρ) which returns the solution to a regularized game anchored at ρ. By updating the anchor ρ_{t+1} ← M(ρ_t), the distance (Bregman divergence) to any Nash equilibrium strictly decreases at every step.
- **Core assumption**: The regularizer ψ is μ-strongly convex, and iterates remain in the interior of the domain (Assumption 1).
- **Evidence anchors**: [abstract]: "fix the regularization strength at a large value... and achieve convergence by iteratively refining the reference policy." [section 4.2]: Theorem 3 proves the sequence z_t converges to a Nash equilibrium with strictly decreasing Bregman divergence.
- **Break condition**: If the inner-loop solver for M(ρ) is inaccurate, the theoretical monotonic improvement guarantee may not hold in practice.

### Mechanism 2
- **Claim**: Fixing the regularization coefficient α at a large value stabilizes learning in stochastic environments, whereas decaying α causes divergence.
- **Mechanism**: Decaying α requires the step size η to shrink to satisfy convergence constraints (α ≥ μ η L²), leading to negligible progress or noise-dominated updates. NashPG keeps α large, ensuring the operator remains strongly monotone and robust to gradient noise, while the iterative reference update handles the bias removal.
- **Core assumption**: Gradient noise is the primary source of instability when α is small.
- **Evidence anchors**: [section 3.3]: "Shrinking α → 0... forcing either vanishingly small stepsizes... or risking divergence." [appendix c.1]: Empirical study showing that annealing α leads to divergent exploitability in Kuhn Poker.
- **Break condition**: If α is set too large without sufficient inner-loop refinement steps, the policy may become overly conservative, stalling convergence.

### Mechanism 3
- **Claim**: The theoretical Iterative M Method can be approximated by a practical Policy Gradient algorithm (NashPG) using KL-regularized objectives.
- **Mechanism**: NashPG maps the theoretical mirror descent update to a behavioral strategy update. It maximizes an objective g(π) = Payoff - α · E_o~π[DKL(π || ρ)]. This allows the use of standard RL components (like PPO) to estimate gradients via trajectory sampling.
- **Core assumption**: Trajectory sampling sufficiently approximates the exact gradients required by the theory.
- **Evidence anchors**: [section 4.3]: "We develop a practical algorithm... preserving the generalizability of policy gradient methods while relying solely on the current and reference policies." [algorithm 2]: Explicitly defines the inner loop using gradient estimates and KL divergence.
- **Break condition**: High variance in gradient estimation or sparse rewards in complex games may require extensive tuning of inner-loop steps (K) to approximate the theoretical operator well enough.

## Foundational Learning

- **Concept: Bregman Divergence**
  - **Why needed here**: This is the distance metric used to prove convergence. The paper proves B_ψ(z*; z_t) > B_ψ(z*; z_{t+1}), meaning the current policy gets "closer" to the Nash equilibrium relative to the regularizer ψ.
  - **Quick check question**: Can you explain why Euclidean distance might be a poor choice for measuring distance between probability distributions compared to KL Divergence?

- **Concept: Variational Inequality (VI)**
  - **Why needed here**: The paper frames finding a Nash Equilibrium not as optimization, but as solving a VI, VI(Z, F). Understanding this helps explain why "strong monotonicity" (added by regularization) is required to make the problem solvable.
  - **Quick check question**: How does adding a regularization term change a "monotone" operator into a "strongly monotone" one, and why does that guarantee a unique solution?

- **Concept: Policy Gradient (PPO)**
  - **Why needed here**: NashPG is implemented as a wrapper around PPO. You need to understand how PPO calculates advantages and updates networks to implement the inner loop of NashPG.
  - **Quick check question**: In standard PPO, where does the "clip" objective apply, and how would you modify the loss function to include the negative KL penalty term required by NashPG?

## Architecture Onboarding

- **Component map**: Policy Network (π_θ) -> Reference Network (ρ) -> Environment -> Optimizer
- **Critical path**:
  1. Outer Loop: Set Reference ρ ← π_θ
  2. Inner Loop (Repeat K times):
     - Sample trajectories using π_θ (and opponent)
     - Compute Payoff (R) and KL Divergence (DKL(π || ρ))
     - Calculate gradient of Objective = R - α · DKL
     - Update π_θ (e.g., via PPO step)
  3. Repeat: Update ρ again

- **Design tradeoffs**:
  - **α (Regularization Strength)**: High α = stable but slow convergence; Low α = faster but risks divergence/noise
  - **K (Inner Steps)**: High K = better approximation of theoretical operator M(ρ) but slower wall-clock time; Low K = risk of "greedy" errors
  - **Reference Update Frequency**: Updating too frequently (small K) might destabilize the monotonic improvement property

- **Failure signatures**:
  - **Divergence/Spiking Loss**: Check if α is too low relative to learning rate
  - **Stagnation/High Bias**: Policy collapses to deterministic strategy prematurely; check if α is too high or K is insufficient
  - **Negative Exploitability**: In complex games, measuring exploitability via PPO might fail because the "best response" learner is weaker than the target policy

- **First 3 experiments**:
  1. **Kuhn Poker Sanity Check**: Implement the inner loop on Kuhn Poker. Verify that as you run inner steps (K), the KL divergence term keeps the policy close to ρ while the payoff term tries to improve it.
  2. **Ablation on α**: Run NashPG on Leduc Poker with α ∈ {0.1, 0.2, 0.4}. Confirm that α=0.2 matches the paper's reported sweet spot for stability vs. convergence speed.
  3. **Annealing vs. Fixed**: Implement a version where α decays to 0 (standard MMD) vs. the proposed fixed α with reference updating. Plot exploitability to confirm the annealing version diverges while NashPG stabilizes.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the heuristic approximations used to derive the practical Nash Policy Gradient (NashPG) algorithm—specifically the shift from mixed to behavioral strategies and the KL regularization formulation—be theoretically grounded to ensure convergence guarantees?
  - **Basis in paper**: [explicit] The conclusion states, "On the theoretical side, our heuristic approximations would benefit from theoretically grounded justification."
  - **Why unresolved**: The theoretical convergence proof (Theorem 3) applies to the idealized Iterative M Method using mixed strategies, whereas the practical implementation relies on behavioral strategies and sampling approximations that deviate from these assumptions.
  - **What evidence would resolve it**: A formal proof showing that the behavioral strategy approximation and the specific regularization dependency on the opponent's strategy retain the monotonic improvement and convergence properties of the idealized operator.

- **Open Question 2**: Does the NashPG framework scale effectively to continuous-action domains and high-dimensional environments such as Atari?
  - **Basis in paper**: [explicit] The authors explicitly list extending the method to "continuous-action domains and to high-dimensional environments, such as Atari," as a direction for future work to test scalability.
  - **Why unresolved**: The current experiments are restricted to finite action spaces, and the algorithm's reliance on KL divergence regularization may face computational or stability challenges in continuous or high-dimensional action spaces.
  - **What evidence would resolve it**: Empirical results demonstrating NashPG's performance and stability when applied to standard continuous-control benchmarks or Atari games.

- **Open Question 3**: How does NashPG compare against domain-specific state-of-the-art algorithms like Pluribus in large-scale poker settings?
  - **Basis in paper**: [explicit] The conclusion notes that "benchmarking NashPG against domain-specific state-of-the-art methods, such as Pluribus... would further clarify its strengths."
  - **Why unresolved**: While the paper compares NashPG against general model-free baselines, it has not yet been validated against highly specialized, superhuman agents optimized specifically for poker.
  - **What evidence would resolve it**: A comparative study measuring exploitability or win rates of NashPG agents against Pluribus-style agents in large games like No-Limit Texas Hold'em.

## Limitations

- The theoretical convergence guarantees apply to the idealized Iterative M Method using mixed strategies, while the practical implementation uses behavioral strategies with sampling approximations that lack formal theoretical grounding.
- The algorithm's performance in continuous-action domains and high-dimensional environments like Atari remains untested, limiting assessment of its scalability.
- While NashPG scales to large games like No-Limit Hold'em, the exploitability metric becomes unreliable for very complex games, limiting theoretical guarantees in practice.

## Confidence

- **High**: Theoretical convergence proof (Theorem 3) and monotonic improvement property
- **Medium**: Empirical results showing lower exploitability and higher Elo ratings compared to baselines
- **Medium**: Claim that fixed regularization is more stable than decaying regularization, based on limited ablation studies

## Next Checks

1. **Reproduce Kuhn Poker baseline**: Implement NashPG and MMD on Kuhn Poker to verify the exploitability curves match Figure 1a, particularly the divergence of MMD with annealing.
2. **Ablation on α**: Run NashPG on Leduc Poker with α ∈ {0.1, 0.2, 0.4} to confirm the reported sweet spot for stability vs. convergence speed.
3. **Annealing vs. Fixed ablation**: Implement both variants on Leduc Poker and plot exploitability to confirm that annealing causes divergence while fixed α stabilizes learning.