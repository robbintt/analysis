---
ver: rpa2
title: Bellman Error Centering
arxiv_id: '2502.03104'
source_url: https://arxiv.org/abs/2502.03104
tags:
- uni00000013
- uni00000011
- uni00000018
- uni00000014
- uni00000015
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Bellman error centering (BEC) as a fundamental
  principle underlying value-based reward centering, clarifying that while simple
  reward centering operates on immediate rewards, value-based centering actually centers
  Bellman errors. The authors establish centered fixpoints for both tabular and linear
  function approximation settings, and develop on-policy CTD and off-policy CTDC algorithms
  with convergence guarantees under standard assumptions.
---

# Bellman Error Centering

## Quick Facts
- arXiv ID: 2502.03104
- Source URL: https://arxiv.org/abs/2502.03104
- Authors: Xingguo Chen; Yu Gong; Shangdong Yang; Wenhao Wang
- Reference count: 23
- Primary result: Bellman error centering (BEC) clarifies value-based reward centering operates on Bellman errors, enabling stable on-policy CTD and off-policy CTDC algorithms with convergence guarantees.

## Executive Summary
This paper establishes Bellman error centering (BEC) as the fundamental principle underlying value-based reward centering in reinforcement learning. The authors prove that while simple reward centering operates on immediate rewards, value-based centering actually centers Bellman errors through a two-timescale stochastic approximation framework. They develop both on-policy CTD and off-policy CTDC algorithms with provable convergence under standard assumptions, demonstrating superior stability compared to traditional TD methods, especially in challenging off-policy scenarios.

## Method Summary
The paper introduces CTD (Centered Temporal Difference) for on-policy policy evaluation and CTDC (Centered Temporal Difference with Correction) for off-policy settings. Both algorithms use two-timescale stochastic approximation where a faster timescale update tracks the expected Bellman error ω while a slower timescale updates value weights θ. The off-policy CTDC adds a third timescale for gradient correction using importance sampling ratios. The methods operate on centered Bellman errors δ - ω rather than raw temporal differences, creating unique fixpoint properties that enable convergence where traditional TD methods fail.

## Key Results
- Bellman error centering transforms standard Bellman operators into zero-mean centered operators with unique fixpoints
- CTD and CTDC algorithms converge under standard assumptions with proper timescale separation (αₖ = o(βₖ))
- CTDC successfully solves Baird's 7-state counterexample where off-policy TD diverges
- Experimental validation demonstrates stability across policy evaluation tasks with challenging off-policy scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Value-based reward centering operates on Bellman errors, not raw rewards, creating a centered Bellman operator with unique fixpoint properties.
- Mechanism: The centering operator C removes the expected component from any variable: Cx(s) = x(s) − E[x(s)]. When applied to Bellman errors δ = r + γV(s') − V(s), the update δ̄ = δ − ω where ω → E[δ] effectively computes Cδ. This transforms the standard Bellman operator into TᶜπCV = CRπ + γPπCV, which has zero-mean centered solutions.
- Core assumption: The ω parameter converges to E[δ] before θ converges; requires i.i.d. sampling with bounded second moments and non-singular covariance matrix A = Cov(ϕ, ϕ − γϕ').
- Evidence anchors: [abstract] "VRC is essentially Bellman error centering (BEC)"; [Page 3, Section 3.1] "δₜ − r̄ₜ ≈ Cδₜ... value-based reward centering is no more, in a strict sense, reward centering"

### Mechanism 2
- Claim: Two-timescale stochastic approximation with semi-gradient descent yields provable convergence to centered TD fixpoint under linear function approximation.
- Mechanism: CTD maintains two recursions: ω updates on the faster timescale (βₖ) to track E[δ], while θ updates on slower timescale (αₖ) using centered gradients. The ODE analysis shows θ converges to A⁻¹b where A = Cov(ϕ, ϕ − γϕ') and b = Cov(r, ϕ). This is the covariance form of the TD fixpoint.
- Core assumption: Matrix A must be non-singular (positive definite); step-sizes satisfy Σαₖ = Σβₖ = ∞, Σαₖ² < ∞, Σβₖ² < ∞, and αₖ = o(βₖ).
- Evidence anchors: [Page 4, Theorem 4.1] Full convergence proof using Borkar's two-timescale theorem; [Page 5, Eq. 40] Shows A is positive definite when Cov(ϕ,ϕ) and Cov(ϕ−γϕ', ϕ−γϕ') are semi-positive definite

### Mechanism 3
- Claim: Off-policy CTDC uses three-timescale updates with projected centered Bellman error to prevent divergence from importance sampling variance.
- Mechanism: Off-policy TD diverges due to off-policy counterexamples (Baird's). CTDC adds a secondary weight vector u that estimates the gradient correction term, operating on an intermediate timescale. The MSPCBE objective: min ||Π(TπVθ − Vθ − ω1)||²D uses projection to ensure convergence. Update order: ω (fastest) → u (medium) → θ (slowest).
- Core assumption: Three-timescale conditions αₖ = o(ζₖ) = o(βₖ); matrices A and C = E[ϕϕᵀ] must be non-singular.
- Evidence anchors: [Page 4-5, Section 5] Full three-timescale convergence proof; [Page 7, Figure 1(b)(c)] Experimental validation on 2-state and 7-state counterexamples showing CTDC converges while off-policy TD diverges

## Foundational Learning

- **Concept: Centering operator and covariance decomposition**
  - Why needed here: Understanding why subtracting E[δ] fundamentally changes the optimization landscape
  - Quick check question: If V is constant across all states, what happens to the centered Bellman operator?

- **Concept: Two-timescale stochastic approximation (Borkar 1997)**
  - Why needed here: The entire convergence proof rests on timescale separation—ω must "see" θ as quasi-static
  - Quick check question: Why must αₖ = o(βₖ) rather than just αₖ < βₖ?

- **Concept: Off-policy TD divergence and Baird's counterexample**
  - Why needed here: Without understanding why off-policy TD diverges, you cannot appreciate why CTDC's projection and gradient correction are necessary
  - Quick check question: In Baird's 7-state counterexample, what causes the weights to grow unboundedly?

## Architecture Onboarding

- **Component map:**
  ```
  CTD (on-policy):
  ├── θ (value weights, m-dim) ← slower timescale αₖ
  └── ω (scalar centering offset) ← faster timescale βₖ

  CTDC (off-policy):
  ├── θ (value weights, m-dim) ← slowest timescale αₖ
  ├── u (gradient correction, m-dim) ← medium timescale ζₖ
  └── ω (scalar centering offset) ← fastest timescale βₖ
  ```

- **Critical path:**
  1. Initialize θ₀, ω₀ (and u₀ for CTDC) to zero
  2. Sample transition (s, a, r, s')
  3. Compute δ = r + γθᵀϕ(s') − θᵀϕ(s)
  4. Update ω ← ω + β(δ − ω) [CTD] or ω ← ω + βρ(δ − ω) [CTDC]
  5. For CTD: θ ← θ + α(δ − ω)ϕ
  6. For CTDC: u ← u + ζ[δ − ω − ϕᵀu]ϕ, then θ ← θ + α[(δ − ω)ϕ − γϕ'(ϕᵀu)]

- **Design tradeoffs:**
  - **CTD vs CTDC:** CTD is simpler but unproven for off-policy; CTDC has guaranteed off-policy convergence but requires tuning three learning rates
  - **Learning rate ordering:** Larger β means faster ω convergence but potentially noisier centering estimate; standard choice is βₖ ≈ 10× ζₖ ≈ 10× αₖ
  - **Assumption:** A and C non-singular requires diverse features—avoid collinear or near-collinear feature vectors

- **Failure signatures:**
  - θ oscillating/diverging → check A singularity (reduce feature dimensionality)
  - ω not stabilizing → βₖ too small relative to data variance
  - CTDC still diverging in off-policy → verify ζₖ and βₖ satisfy timescale ordering; check importance sampling ratio bounds

- **First 3 experiments:**
  1. **Boyan's chain (on-policy):** Implement CTD, vary α/β ratios {0.01, 0.1, 0.5}, plot RMSCBE vs iterations. Validate θ → A⁻¹b.
  2. **2-state counterexample (off-policy):** Compare TD, TDC, CTD, CTDC. Expect: TD diverges, TDC/CTD/CTDC converge. Test importance sampling ratio sensitivity.
  3. **Baird's 7-state (off-policy):** Validate CTDC on canonical divergence example. Sweep (α, ζ, β) grid to find stable region; verify timescale ordering necessity.

## Open Questions the Paper Calls Out

- **Extension to control:** The paper calls for extending CTD and CTDC algorithms to learning for control, especially episodic problems, as current analysis and experiments are restricted to policy evaluation tasks.

- **Integration with policy methods:** Future work includes extending Bellman error centering to policy gradient and actor-critic RL algorithms, which remains unexplored given the current focus on value-based methods.

- **Off-policy CTD convergence:** While CTDC is proven to converge off-policy, the paper notes that CTD's empirical stability in off-policy counterexamples is surprising and lacks formal proof for this setting.

- **λ-return extension:** The paper explicitly lists extending Bellman error centering to λ returns as a specific future direction, leaving the multi-step generalization unexplored.

## Limitations

- The convergence guarantees rely on specific timescale separation assumptions (αₖ = o(βₖ)) that may be difficult to verify in practice, particularly for complex off-policy scenarios.

- The theoretical framework assumes i.i.d. sampling with bounded second moments, but real-world reinforcement learning data often exhibits temporal correlations that could violate these assumptions.

- The three-timescale analysis for CTDC, while theoretically sound, introduces significant practical complexity in tuning ζₖ relative to αₖ and βₖ, making hyperparameter selection challenging.

## Confidence

- **High confidence:** The mechanism connecting value-based reward centering to Bellman error centering (Mechanism 1) is well-established with clear mathematical derivation from δₜ − r̄ₜ ≈ Cδₜ.

- **Medium confidence:** The two-timescale convergence proof for CTD (Mechanism 2) follows standard stochastic approximation theory but lacks corpus validation for the specific centering case.

- **Medium confidence:** The three-timescale CTDC convergence proof (Mechanism 3) is theoretically rigorous but represents a novel combination of centering with off-policy corrections that hasn't been validated in the broader literature.

## Next Checks

1. **Timescale Sensitivity Analysis:** Systematically vary the ratio α/β across multiple orders of magnitude (0.01 to 10) for CTD on Boyan's chain to empirically verify the o(relationship) requirement from Theorem 4.1.

2. **Feature Correlation Impact:** Design experiments with progressively more correlated features to identify when the matrix A becomes singular, testing the robustness of the convergence guarantees under realistic feature engineering scenarios.

3. **Importance Sampling Ratio Stress Test:** For CTDC on the 2-state counterexample, systematically vary the behavior policy to generate importance sampling ratios ranging from 0.1 to 10, measuring both convergence speed and final RMSCBE to identify practical bounds on the theoretical assumptions.