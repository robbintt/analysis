---
ver: rpa2
title: Scaling Sparse and Dense Retrieval in Decoder-Only LLMs
arxiv_id: '2502.15526'
source_url: https://arxiv.org/abs/2502.15526
tags:
- retrieval
- sparse
- dense
- https
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically compares the scaling behavior of sparse
  and dense retrieval models in decoder-only LLMs across different model scales (1B,
  3B, 8B) and training objectives (contrastive loss, knowledge distillation, and their
  combination). Using Llama-3 models and MSMARCO dataset, the study evaluates performance
  on in-domain (MSMARCO, TREC DL) and out-of-domain (BEIR) benchmarks.
---

# Scaling Sparse and Dense Retrieval in Decoder-Only LLMs

## Quick Facts
- arXiv ID: 2502.15526
- Source URL: https://arxiv.org/abs/2502.15526
- Reference count: 40
- This paper systematically compares the scaling behavior of sparse and dense retrieval models in decoder-only LLMs across different model scales (1B, 3B, 8B) and training objectives (contrastive loss, knowledge distillation, and their combination).

## Executive Summary
This paper systematically compares the scaling behavior of sparse and dense retrieval models in decoder-only LLMs across different model scales (1B, 3B, 8B) and training objectives (contrastive loss, knowledge distillation, and their combination). Using Llama-3 models and MSMARCO dataset, the study evaluates performance on in-domain (MSMARCO, TREC DL) and out-of-domain (BEIR) benchmarks. The key findings show that scaling benefits only emerge with contrastive loss, while knowledge distillation shows minimal improvement across scales. Sparse retrieval consistently outperforms dense retrieval across all benchmarks and demonstrates greater robustness to imperfect supervision. The combination of contrastive loss and knowledge distillation achieves the best performance, enabling an 8B sparse retrieval model (Lion-SP-8B) to achieve state-of-the-art results, outperforming ColBERTv2 by 10.6% in BEIR and RepLlama by 4.1% in TREC DL.

## Method Summary
The study uses Llama-3 models (1B, 3B, 8B) with bidirectional attention masks instead of causal masks. Models undergo MNTP pre-training on MSMARCO passages (10K steps, 20% masking) followed by LoRA fine-tuning with three objectives: contrastive loss (CL), knowledge distillation (KD), or their combination. Sparse retrieval uses vocabulary projection to 128K dimensions with FLOP regularization, while dense retrieval uses mean pooling to 4096 dimensions. Training uses 4×A100 80GB GPUs with varying batch sizes and epochs inversely proportional to model size. The KD teacher is a cross-encoder (MiniLM-L-6-v2), and CL+KD uses KL-Divergence with 0.5 weighting.

## Key Results
- Scaling benefits only emerge with contrastive loss, while knowledge distillation shows minimal improvement across scales
- Sparse retrieval consistently outperforms dense retrieval across all benchmarks and demonstrates greater robustness to imperfect supervision
- The combination of contrastive loss and knowledge distillation achieves the best performance, with Lion-SP-8B achieving state-of-the-art results

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Contrastive loss (CL) enables reliable scaling behavior across model sizes, while knowledge distillation (KD) plateaus early.
- **Mechanism:** CL directly optimizes the retrieval objective using in-batch negatives, allowing larger models to learn more nuanced representations without external bottlenecks. KD constrains the student model to mimic a fixed teacher, capping potential gains when student capacity approaches or exceeds teacher capacity.
- **Core assumption:** The training data provides sufficient signal for CL to exploit added capacity; the teacher model's quality is the limiting factor for KD.
- **Evidence anchors:** [abstract] "Scaling behaviors emerge clearly only with CL, where larger models achieve significant performance gains, whereas KD-trained models show minimal improvement" [section 3] "when the fine-tuning objective is CL the retrieval model's performance...increases significantly as the model size grows...In contrast, when KD loss is used the performance improvement is far less pronounced"
- **Break condition:** If teacher model capacity significantly exceeds student capacity, KD may show stronger scaling; if training data quality degrades, CL scaling benefits may diminish.

### Mechanism 2
- **Claim:** Sparse retrieval provides superior out-of-domain generalization compared to dense retrieval, particularly under imperfect supervision.
- **Mechanism:** Sparse representations project token embeddings to vocabulary space, preserving lexical signals and enabling term expansion. This explicit representation is less prone to overfitting to training distribution peculiarities than dense semantic embeddings.
- **Core assumption:** Lexical overlap and term expansion signals remain informative across domains; vocabulary coverage is adequate for out-of-domain terms.
- **Evidence anchors:** [abstract] "Sparse retrieval consistently outperforms dense retrieval across both in-domain...and out-of-domain (BEIR) benchmarks, and they demonstrate greater robustness to imperfect supervised signals" [section 3] "sparse-KD improves steadily as model size increases...dense-KD, which suffers from overfitting to the teacher model"
- **Break condition:** In domains with severe vocabulary mismatch or where semantic similarity doesn't correlate with lexical overlap, dense retrieval may outperform.

### Mechanism 3
- **Claim:** Combining CL and KD losses achieves optimal performance by leveraging complementary strengths—KD benefits small models while CL benefits large models.
- **Mechanism:** Small models (1B) gain from teacher-provided soft labels that compensate for limited capacity. Large models (8B) exceed teacher capacity and benefit more from direct CL optimization. The combination allows smooth interpolation across scales.
- **Core assumption:** Teacher model quality is good but imperfect; CL and KD gradients don't conflict destructively.
- **Evidence anchors:** [abstract] "The combination of contrastive loss and knowledge distillation achieves the best performance" [section 3.1] "KD loss significantly boosts the performance of small-scale (1B) retrieval models, while CL provides greater benefits for large-scale (8B) models"
- **Break condition:** If teacher model is significantly better/worse than assumed, optimal loss weighting may shift; if CL and KD objectives fundamentally conflict, combination may underperform.

## Foundational Learning

- **Concept: Contrastive Learning for Retrieval**
  - **Why needed here:** Core training objective that enables scaling. Understanding how in-batch negatives create the ranking signal is essential for debugging performance.
  - **Quick check question:** Given a batch with 1 query and 16 negatives, how does the softmax distribution change as model capacity increases?

- **Concept: Knowledge Distillation (MarginMSE vs. KL-Divergence)**
  - **Why needed here:** Paper uses different KD losses for different configurations. MarginMSE for pure KD, KL-Divergence when combining with CL.
  - **Quick check question:** Why does MarginMSE work with triplets while KL-Divergence requires listwise distributions?

- **Concept: Sparse vs. Dense Representations**
  - **Why needed here:** Fundamental architectural choice with different scaling properties. Sparse uses vocabulary projection; dense uses mean pooling.
  - **Quick check question:** If vocabulary size is 50k and hidden dimension is 4096, what's the storage difference between sparse and dense representations for 1M documents?

## Architecture Onboarding

- **Component map:** Llama-3 Backbone (1B/3B/8B) -> Bidirectional Attention Mask (replaces causal) -> MNTP Pre-training (10k steps on MSMARCO) -> LoRA Fine-tuning (CL / KD / CL+KD) -> Representation Head (Dense: Mean pooling → 4096-dim vector / Sparse: Project to vocab → 128k-dim sparse vector) -> Scoring: Dot product

- **Critical path:**
  1. Replace causal attention mask with bidirectional mask (enables token-aware sparse projection)
  2. Run MNTP pre-training (20% masking, predict from position i-1)
  3. Fine-tune with LoRA (r=16, α=32) using CL+KD with 0.5 weighting
  4. For sparse: apply FLOP regularization (0.05 query, 0.04 doc)

- **Design tradeoffs:**
  - **Sparse vs. Dense:** Sparse gives +4-10% BEIR performance but larger index size (vocabulary dim vs. hidden dim)
  - **CL vs. KD:** Use KD for 1B models (+20% in-domain), CL for 8B models (+7% out-of-domain)
  - **Compute budget:** Fixed ~40-44 hours on 4×A100 across all configurations; adjust epochs inversely to model size

- **Failure signatures:**
  - **Dense-KD overfitting:** Performance degrades at 3B/8B on BEIR (table 3 shows -5% to -7% vs. teacher)
  - **Scaling not emerging:** If CL doesn't improve from 1B→8B, check batch size/negative count (table 5)
  - **Sparse not sparsifying:** FLOP regularization coefficient too low; vectors remain dense

- **First 3 experiments:**
  1. **Baseline verification:** Reproduce Lion-SP-1B with CL+KD on MSMARCO Dev (target: MRR@10 ≥0.410)
  2. **Ablation: CL vs. KD at 1B:** Train sparse-1B with CL only and KD only; verify KD outperforms CL in-domain, CL outperforms KD out-of-domain
  3. **Scaling test:** Train sparse-3B with CL; verify ~3% improvement over sparse-1B-CL on BEIR (0.528 vs. 0.496)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does increasing the size and capacity of the teacher model restore the effectiveness of knowledge distillation (KD) for large-scale (8B) student models?
- Basis in paper: [explicit] The authors state in the conclusion: "In future work, we aim to expand this analysis by exploring teacher models with varying sizes... to gain deeper insights into their impact on retrieval performance."
- Why unresolved: The study utilized a single, fixed teacher (MiniLM-L-6-v2). The observed lack of scaling for KD at the 8B level may be due to the student's capacity exceeding the teacher's, rather than a fundamental limitation of the KD objective.
- Evidence: A set of experiments training 1B, 3B, and 8B student models using teachers of increasing size (e.g., BERT-large, Llama-3-8B as cross-encoder) to see if the scaling gap closes.

### Open Question 2
- Question: How do alternative supervision strategies beyond MarginMSE and KL-Divergence impact the scaling dynamics of retrieval models?
- Basis in paper: [explicit] The conclusion explicitly lists "different supervision strategies" as a future direction to explore alongside teacher model sizes.
- Why unresolved: The current study was limited to specific distillation loss implementations (MarginMSE for pure KD, KL-Div for CL+KD). It is unclear if other strategies could mitigate the overfitting observed in dense retrieval or further boost sparse retrieval scaling.
- Evidence: Comparative scaling experiments using different distillation methods (e.g., specialized hard-negative sampling or token-level distillation) across the 1B, 3B, and 8B model scales.

### Open Question 3
- Question: What is the underlying mechanism that allows sparse retrieval to maintain robustness and outperform the teacher model, while dense retrieval consistently underperforms the teacher at larger scales?
- Basis in paper: [inferred] Section 3.2 and Table 3 show that sparse retrieval consistently outperforms the teacher reranker, whereas dense retrieval falls behind. The paper observes this "divergence" but does not fully explain the mechanism behind sparse retrieval's robustness to imperfect supervision compared to dense retrieval.
- Why unresolved: The paper documents the phenomenon where dense models overfit to the teacher's imperfections while sparse models do not, but it does not provide a theoretical or empirical analysis of why the sparse representation space handles noisy supervision better.
- Evidence: An analysis of the gradient updates or representation geometry (e.g., uniformity/alignment) for sparse vs. dense projection heads when trained on identical noisy/distilled labels.

### Open Question 4
- Question: Do the observed scaling laws for contrastive loss (CL) and knowledge distillation (KD) generalize to training corpora other than MSMARCO?
- Basis in paper: [inferred] The experimental setup is restricted to "Using MSMARCO passages as the training dataset." The authors note in the introduction that scaling behavior is a key interest, but they do not test if the specific data domain or size of MSMARCO influenced the finding that KD fails to scale.
- Why unresolved: It is possible that KD requires more data diversity to scale effectively, or that the "overfitting" observed is specific to the MSMARCO domain.
- Evidence: Replicating the 1B vs. 8B scaling experiments on a different large-scale dataset (e.g., Natural Questions or a web-scale corpus) and evaluating the CL vs. KD performance gap.

## Limitations

- **Limited Generalization Beyond MS MARCO Domain:** Results are primarily based on MS MARCO passage data, which may not capture full diversity of real-world retrieval scenarios.
- **Teacher Model Dependency for KD:** Performance ceiling for KD-trained models is inherently limited by teacher model quality and may propagate systematic biases.
- **Fixed Training Infrastructure and Compute Constraints:** Experiments conducted with specific hardware configurations and fixed training durations, which may have introduced implicit constraints on training dynamics.

## Confidence

**High Confidence Claims:**
- Sparse retrieval consistently outperforms dense retrieval across all evaluated benchmarks (in-domain and out-of-domain)
- Contrastive loss enables reliable scaling behavior, with larger models achieving significant performance gains
- Knowledge distillation shows minimal scaling benefits and can lead to overfitting

**Medium Confidence Claims:**
- The combination of contrastive loss and knowledge distillation achieves optimal performance
- Sparse retrieval demonstrates greater robustness to imperfect supervision
- Scaling benefits are specific to contrastive loss and don't emerge with knowledge distillation alone

**Low Confidence Claims:**
- These findings will generalize to non-MS MARCO domains and datasets
- The observed scaling patterns would remain consistent with different teacher models or longer training durations
- The specific performance advantages would hold with different hardware configurations or batch sizes

## Next Checks

1. **Domain Generalization Test:** Evaluate the best-performing sparse and dense models (particularly Lion-SP-8B) on additional retrieval benchmarks beyond BEIR, such as specialized domains (medical, legal, technical) or larger document collections to assess whether the observed performance patterns hold across diverse data distributions.

2. **Teacher Quality Ablation:** Systematically vary the teacher model quality used for knowledge distillation (e.g., different architectures, training data, or performance levels) to determine how teacher quality affects the scaling behavior of KD-trained models and whether the observed plateauing is truly due to capacity limitations or teacher quality constraints.

3. **Extended Training Duration Analysis:** Conduct longer training runs for the 8B models with contrastive loss to determine if the scaling benefits continue to increase or if they plateau, which would help validate whether the observed scaling patterns represent true asymptotic behavior or are constrained by the fixed training duration.