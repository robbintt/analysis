---
ver: rpa2
title: 'MuBench: Assessment of Multilingual Capabilities of Large Language Models
  Across 61 Languages'
arxiv_id: '2506.19468'
source_url: https://arxiv.org/abs/2506.19468
tags:
- score
- language
- languages
- consistency
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MUBENCH, a comprehensive benchmark for evaluating
  multilingual large language models across 61 languages and diverse capabilities
  including natural language understanding, commonsense reasoning, factual recall,
  and technical reasoning. The benchmark features aligned test samples across languages
  and includes code-switched variants for mixed-language evaluation.
---

# MuBench: Assessment of Multilingual Capabilities of Large Language Models Across 61 Languages

## Quick Facts
- arXiv ID: 2506.19468
- Source URL: https://arxiv.org/abs/2506.19468
- Reference count: 40
- Authors: Wenhan Han, Yifan Zhang, Zhixun Chen, Binbin Liu, Haobin Lin, Bingni Zhang, Taifeng Wang, Mykola Pechenizkiy, Meng Fang, Yin Zheng
- Primary result: Introduces MUBENCH, a benchmark evaluating multilingual LLMs across 61 languages with aligned test samples, code-switched variants, and human-validated translation quality

## Executive Summary
MUBENCH introduces a comprehensive evaluation framework for assessing multilingual large language models across 61 languages, addressing the critical gap in systematic multilingual assessment. The benchmark features aligned test samples across languages, code-switched variants for mixed-language evaluation, and human validation of translation quality. Through extensive experiments with state-of-the-art multilingual models, the authors reveal significant performance gaps between English and low-resource languages that persist even in larger models. The study introduces Multilingual Consistency (MLC) as a metric to analyze cross-lingual knowledge transfer, finding stable inter-language correlation patterns. Pretraining experiments with 1.2B-parameter models demonstrate that parallel corpora improve both accuracy and consistency, though excessive parallel data can reduce information diversity.

## Method Summary
The MUBENCH framework was constructed through an automated translation pipeline that leverages translation models and Chain-of-Thought reasoning to generate aligned test samples across 61 languages. The pipeline includes automatic filtering for culturally sensitive content, with human evaluation confirming translation quality comparable to human-translated benchmarks. The benchmark covers four core capabilities: natural language understanding, commonsense reasoning, factual recall, and technical reasoning. Code-switched variants were generated by mixing languages within test samples to evaluate robustness in mixed-language contexts. The evaluation methodology includes standard accuracy metrics plus the novel Multilingual Consistency (MLC) metric to measure cross-lingual knowledge transfer. Pretraining experiments were conducted using 1.2B-parameter models with varying proportions of parallel versus monolingual data to study the impact on multilingual performance.

## Key Results
- Significant performance gaps persist between English and low-resource languages even in larger multilingual models
- Pretraining with parallel corpora improves both accuracy and consistency, particularly when one language dominates training
- Multilingual Consistency (MLC) metric reveals stable inter-language correlation patterns across different model sizes
- Code-switching evaluation shows that improvements in standard multilingual understanding do not translate to better handling of mixed inputs
- Excessive parallel data during pretraining can reduce information diversity due to redundancy

## Why This Works (Mechanism)
The effectiveness of MUBENCH stems from its comprehensive coverage of 61 languages and its innovative approach to evaluating cross-lingual knowledge transfer through aligned test samples and code-switched variants. The benchmark's strength lies in its ability to measure not just absolute performance but also the consistency of knowledge transfer across languages. The Multilingual Consistency (MLC) metric captures the correlation between language pairs, revealing how well models transfer knowledge from high-resource to low-resource languages. The pretraining experiments demonstrate that parallel corpora serve as a crucial bridge for cross-lingual transfer, though the optimal balance with monolingual data remains an open question due to the trade-off between transfer efficiency and information diversity.

## Foundational Learning
- **Cross-lingual knowledge transfer**: The ability of models to apply knowledge learned in one language to another. Why needed: Essential for understanding how multilingual models generalize across language barriers. Quick check: Compare performance on aligned tasks across language pairs.
- **Code-switching evaluation**: Testing models on mixed-language inputs where multiple languages appear in the same sample. Why needed: Real-world multilingual communication often involves language mixing. Quick check: Measure accuracy degradation when switching languages mid-sample.
- **Multilingual Consistency (MLC)**: A metric measuring correlation between language pairs in model performance. Why needed: Captures cross-lingual transfer quality beyond absolute accuracy. Quick check: Calculate Pearson correlation between performance scores across language pairs.
- **Parallel corpus impact**: The effect of bilingual aligned text data on multilingual model training. Why needed: Critical for understanding how to optimize pretraining for low-resource languages. Quick check: Compare model performance with varying ratios of parallel to monolingual data.
- **Cultural sensitivity filtering**: Automated detection and removal of culturally biased or sensitive content. Why needed: Ensures fair evaluation across diverse cultural contexts. Quick check: Validate filtered samples don't disproportionately affect certain language groups.
- **Translation quality validation**: Human evaluation of automatically translated benchmark samples. Why needed: Ensures benchmark reliability despite automated construction. Quick check: Compare automatic translations against human-translated reference samples.

## Architecture Onboarding
Component map: Translation Pipeline -> Benchmark Construction -> Evaluation Framework -> Pretraining Experiments
Critical path: Automated translation generation → Cultural sensitivity filtering → Human validation → Multi-task evaluation → Cross-lingual consistency analysis
Design tradeoffs: The automated translation approach enables rapid benchmark creation across 61 languages but may introduce subtle biases; parallel corpus pretraining improves transfer but risks reducing diversity
Failure signatures: Performance gaps between high-resource and low-resource languages; inconsistent cross-lingual transfer patterns; code-switching robustness failures
First experiments:
1. Evaluate baseline multilingual model performance across all 61 languages using standard accuracy metrics
2. Calculate Multilingual Consistency (MLC) scores to analyze cross-lingual transfer patterns
3. Test model robustness on code-switched variants to identify mixed-language processing limitations

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How can the trade-off between improving cross-lingual transfer with parallel data and reducing information diversity be optimized?
- Basis in paper: The authors observe that while parallel corpora aid transfer, excessive amounts can reduce information diversity due to redundancy, creating a conflict between transfer efficiency and knowledge breadth.
- Why unresolved: The pretraining experiments confirm the existence of this trade-off but do not identify the specific data ratios or curriculum strategies required to balance it effectively.
- What evidence would resolve it: Experiments varying parallel data ratios while simultaneously measuring cross-lingual alignment scores and factual diversity metrics across diverse tasks.

### Open Question 2
- Question: Why does robustness in mixed-language (code-switched) contexts fail to improve with increased model scale?
- Basis in paper: The evaluation reveals that improvements in standard multilingual understanding do not translate to better handling of mixed inputs, as larger models do not consistently exhibit greater robustness in code-switched settings.
- Why unresolved: The paper documents the surprising lack of scaling laws in this specific domain but does not investigate the architectural or representational reasons for this limitation.
- What evidence would resolve it: Ablation studies analyzing the internal representations and attention patterns of large-scale models specifically when processing code-switched inputs versus monolingual ones.

### Open Question 3
- Question: How does the exclusion of culturally sensitive samples quantitatively impact the evaluation and downstream behavior of multilingual LLMs?
- Basis in paper: The authors state in the appendix, "The impact of these samples on model behavior will be further investigated in future work," referring to samples filtered out by their cultural sensitivity check.
- Why unresolved: While the pipeline successfully filters these samples to reduce Western-centric bias, the specific effect of this removal on model performance and fairness has not yet been measured.
- What evidence would resolve it: Comparative evaluations of models trained and assessed on datasets with and without the culturally sensitive samples to measure shifts in bias and accuracy.

## Limitations
- Small-scale pretraining experiments (1.2B parameters) may not fully represent state-of-the-art model behavior
- Benchmark construction relies on automatic translation quality that may introduce subtle biases despite human validation
- Code-switching evaluation represents only a subset of possible multilingual scenarios
- Performance gaps between high-resource and low-resource languages may be partially attributable to data quality issues in underlying datasets

## Confidence
- **High confidence**: Findings about multilingual consistency patterns and cross-lingual transfer demonstrate robust statistical analysis and replication across multiple language pairs
- **Medium confidence**: Observations about pretraining benefits from parallel corpora based on limited experimental scope may not generalize to larger model architectures
- **Medium confidence**: Claims about performance gaps between high-resource and low-resource languages influenced by multiple confounding factors including data quality and evaluation methodology

## Next Checks
1. Scale the pretraining experiments to 10B+ parameters to verify whether the observed benefits of parallel corpora persist at larger model sizes
2. Conduct error analysis on benchmark translations to identify and quantify potential bias sources
3. Expand code-switching evaluation to include more diverse language pairs and switching patterns to better understand cross-lingual transfer in mixed-language contexts