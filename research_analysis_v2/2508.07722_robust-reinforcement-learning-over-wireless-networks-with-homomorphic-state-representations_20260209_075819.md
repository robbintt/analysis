---
ver: rpa2
title: Robust Reinforcement Learning over Wireless Networks with Homomorphic State
  Representations
arxiv_id: '2508.07722'
source_url: https://arxiv.org/abs/2508.07722
tags:
- receiver
- state
- learning
- hr3l
- transmitter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HR3L, a novel architecture for training remote
  reinforcement learning (RL) agents over lossy or delayed wireless networks. The
  key innovation is using homomorphic representations to enable the transmitter to
  send compressed, task-relevant features of the environment state, which the receiver
  uses to learn a control policy without requiring gradient exchanges across the wireless
  channel.
---

# Robust Reinforcement Learning over Wireless Networks with Homomorphic State Representations

## Quick Facts
- arXiv ID: 2508.07722
- Source URL: https://arxiv.org/abs/2508.07722
- Reference count: 40
- Primary result: HR3L achieves robustness to 92.5% packet loss and 2-3 step delays while reducing data rate by an order of magnitude versus image compression baselines

## Executive Summary
This paper introduces HR3L, a novel architecture for training remote reinforcement learning agents over lossy or delayed wireless networks. The key innovation is using homomorphic representations to enable the transmitter to send compressed, task-relevant features of the environment state, which the receiver uses to learn a control policy without requiring gradient exchanges across the wireless channel. This avoids the communication overhead and synchronization issues of prior approaches like deep joint source-channel coding.

## Method Summary
HR3L consists of a transmitter that encodes states into successor feature representations and a receiver that learns a policy using PPO over this compressed representation. The transmitter selects the most informative features to send based on estimation error, and the receiver can predict missing or delayed states using a learned transition model. Training occurs in rounds where the transmitter learns encoding parameters (ϕ, α, M, w) using eligibility traces, and the receiver updates its policy using PPO over the estimated features.

## Key Results
- HR3L maintains performance under 92.5% packet loss rates versus PPO degradation
- The architecture achieves performance comparable to state-of-the-art image compression methods while reducing required data rate and computational delay by an order of magnitude
- Sample efficiency matches or exceeds PPO baseline in ideal channel conditions across DeepMind Control Suite tasks

## Why This Works (Mechanism)

### Mechanism 1: Successor Feature Representation (SFR) for Task-Relevant Compression
Encoding states into successor feature representations enables compressed transmission while preserving task-relevant information required for policy optimization. The transmitter learns basis functions ϕ and α that map states and actions to a feature space where rewards decompose linearly: r(s,a) = [ϕ(s)α(a)]^T w. This follows MDP homomorphism theory—if multiple states yield equivalent optimal behavior, they can map to the same representation. The receiver learns policies directly in this compressed space without needing raw state reconstruction.

### Mechanism 2: Receiver-Side Prediction via Learned Transition Model
A learned transition model in feature space enables the receiver to estimate current states during packet losses or delays without requiring retransmission. The transmitter jointly learns a transition matrix M such that ϕ(st+1) ≈ z^T(s,a)(t) M. When packets are lost, the receiver predicts forward: ẑs(t) = [ẑs(t-1) α(at-1)]^T M. When packets arrive (possibly delayed), the receiver blends received and predicted features using the binary mask.

### Mechanism 3: Error-Based Feature Selection for Adaptive Bandwidth
Prioritizing features with highest prediction error maximizes information gain under fixed bandwidth constraints. The transmitter estimates the receiver's belief ẑ(t) and computes per-feature error e(t) = (z(t) - ẑ(t))². Only the G features with largest error are transmitted, accompanied by a binary mask g indicating selection. This targets "surprise"—information the receiver cannot predict autonomously.

## Foundational Learning

- **Concept: Markov Decision Processes (MDPs) and Value Functions**
  - Why needed here: HR3L is built on MDP theory; understanding Q^π(s,a), policies, and reward maximization is essential to follow the homomorphism and SFR decomposition.
  - Quick check question: Can you explain what Q^π(s,a) represents and why discount factor γ matters for long-horizon control?

- **Concept: Successor Feature Representations (SFR)**
  - Why needed here: The core encoding mechanism; SFR enables linear reward decomposition and avoids reconstructing raw states.
  - Quick check question: Given r(s,a) = ϕ(s)^T w, how would you interpret w and what does it mean for transfer learning across tasks?

- **Concept: Model-Based vs. Model-Free RL Tradeoffs**
  - Why needed here: HR3L uses a learned transition model M for prediction; understanding MBRL benefits (planning, handling missing data) and risks (compounding errors) clarifies design choices.
  - Quick check question: What is the primary advantage of model-based RL over model-free, and what failure mode is unique to model-based approaches?

## Architecture Onboarding

- **Component map:**
  - Transmitter: State encoder ϕ (FNN or CNN), action encoder α, transition matrix M (F+A × F), reward weights w, feature selector (mask g)
  - Receiver: PPO policy π operating on estimated features ẑt, state estimator using M and α
  - Channel: Gilbert-Elliott two-state model (Good/Bad) with configurable loss probability; fixed delay d; capacity limit L bits
  - Feedback path: Receiver → Transmitter sends action/reward history at round end (assumed ideal per paper)

- **Critical path:**
  1. Round n begins: Transmitter sends updated Mn and αn to receiver
  2. Each step t: Transmitter observes st, computes zt = ϕn(st), selects top-G features via mask g, transmits mt
  3. Channel applies delay d and/or loss
  4. Receiver: If received, blend ẑ*t-d using Eq. 18; predict forward to t via Eq. 19. If lost, predict from prior estimate via Eq. 17
  5. Policy π samples action at from ẑt; environment returns rt
  6. Round end: Receiver sends a(n-1)T:nT, r(n-1)T:nT to transmitter; transmitter updates ϕ, α, M, w via Eq. 15; receiver updates π via PPO
  7. Repeat for N rounds

- **Design tradeoffs:**
  - Round length T: Longer → more stable feature space for receiver; shorter → faster encoder adaptation
  - Feature dimension F: Higher → better representation capacity; increases bandwidth and risk of overfitting
  - Features transmitted G: Higher → better state accuracy; increases data rate (Eq. 20: |mt| = Df·G + F bits)
  - On-policy (PPO) vs. off-policy: PPO chosen for robustness; off-policy may be more sample-efficient but less stable under changing encodings

- **Failure signatures:**
  - Sharp performance drop at specific loss rates → transition model M failing to generalize; check prediction error growth
  - Gradual degradation with compression → F too small or G too aggressive; verify mask overhead isn't dominating small messages
  - Training instability, oscillating rewards → round length T too short; encoder changes faster than policy can adapt
  - Consistent underperformance vs. PPO (ideal) → encoder ϕ not learning meaningful features; check loss Eq. 15 convergence

- **First 3 experiments:**
  1. **Ideal channel baseline:** Replicate Fig. 3 on 2–3 DeepMind Control tasks; verify SFR learning converges and matches/exceeds standard PPO
  2. **Controlled packet loss:** Test GE 95.5 and GE 92.5 on same tasks; confirm receiver prediction maintains performance (Fig. 5 pattern); ablate prediction by forcing zero-update on loss
  3. **Compression sweep:** On image-based task (e.g., cheetah-run), vary G to target 0.3–0.8 Mb/s; plot reward vs. bitrate; compare mask selection vs. random feature selection to validate error-based prioritization

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can HR3L maintain robustness when transmission delays are stochastic rather than deterministic and constant?
- **Basis in paper:** Section II-B states the system currently assumes deterministic delays, but authors plan to adjust the solution for stochastic delays in future work.
- **Why unresolved:** The receiver’s state estimation (Eq. 17) relies on a fixed delay horizon $d$, which may not hold if delay varies unpredictably.
- **What evidence would resolve it:** Simulation results under a stochastic delay model (e.g., random steps) showing the receiver can successfully predict missing states without performance collapse.

### Open Question 2
- **Question:** How does performance degrade if the feedback channel for actions and rewards is imperfect or lossy?
- **Basis in paper:** Footnote 2 notes that the current work assumes an ideal feedback channel, leaving the scenario of an imperfect feedback channel for a future extension.
- **Why unresolved:** The transmitter’s loss function (Eq. 15) requires the history of actions and rewards ($D_n$) to update the encoding; missing this data breaks the training loop.
- **What evidence would resolve it:** A modified HR3L implementation that handles lost feedback packets, analyzing the trade-off between feedback loss rate and policy convergence speed.

### Open Question 3
- **Question:** Can the architecture be adapted for scenarios where the reward signal is unavailable or only partially available to the transmitter?
- **Basis in paper:** Footnote 1 mentions that future work will study the challenging scenario where reward feedback is limited or not possible.
- **Why unresolved:** The current transmitter optimization relies on explicit reward signals to learn the reward contribution vector $w$ and update model parameters.
- **What evidence would resolve it:** A novel training framework (e.g., using unsupervised representation learning) that allows the transmitter to learn meaningful state embeddings without access to the reward signal $r_t$.

## Limitations
- The paper's theoretical guarantees for MDP homomorphism under learned successor features are not rigorously proven
- The receiver's ability to track the transmitter's belief state for error-based feature selection lacks implementation details
- The impact of round length T on stability versus sample efficiency is not thoroughly explored across the task suite
- The computational overhead of the learned transition model M at the receiver versus potential compression gains is not quantified

## Confidence
- **High confidence** in the core architecture design and empirical results showing robustness to packet loss and delay
- **Medium confidence** in the error-based feature selection mechanism, as the belief tracking mechanism is underspecified
- **Medium confidence** in the scalability claims to image-based states, given that only one pixel-based task is shown
- **Low confidence** in theoretical claims about MDP homomorphism preservation under the learned representations

## Next Checks
1. **Ablation study on receiver prediction:** Disable the transition model M and force zero-update on packet loss to quantify the contribution of model-based prediction to robustness
2. **Belief tracking validation:** Implement and evaluate different strategies for the transmitter to obtain the receiver's state estimate (feedback vs. internal model) to validate the mask selection mechanism
3. **Round length sensitivity:** Systematically vary T across tasks to identify the optimal tradeoff between encoding stability and sample efficiency