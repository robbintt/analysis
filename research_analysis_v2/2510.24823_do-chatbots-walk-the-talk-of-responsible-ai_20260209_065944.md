---
ver: rpa2
title: Do Chatbots Walk the Talk of Responsible AI?
arxiv_id: '2510.24823'
source_url: https://arxiv.org/abs/2510.24823
tags:
- responsible
- google
- openai
- https
- technical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examined whether leading AI chatbot companies implement
  the responsible AI principles they publicly advocate. Using a mixed-methods approach,
  the authors analyzed four major chatbots (ChatGPT, Gemini, DeepSeek, and Grok) across
  company websites, technical documentation, and direct chatbot evaluations.
---

# Do Chatbots Walk the Talk of Responsible AI?

## Quick Facts
- arXiv ID: 2510.24823
- Source URL: https://arxiv.org/abs/2510.24823
- Reference count: 0
- Primary result: Companies frequently mention safety and privacy but rarely address accountability, human rights, or public interest in technical documentation (391/97,854 words = 0.004%)

## Executive Summary
This study examined whether leading AI chatbot companies implement the responsible AI principles they publicly advocate. Using a mixed-methods approach, the authors analyzed four major chatbots (ChatGPT, Gemini, DeepSeek, and Grok) across company websites, technical documentation, and direct chatbot evaluations. They found significant gaps between corporate rhetoric and practice: while companies frequently mentioned safety, privacy, and openness, they rarely addressed other responsible AI concepts like accountability, human rights, or public interest. Technical documents showed minimal use of responsible AI terminology, with only 391 relevant mentions out of 97,854 total words (.004%). Chatbot responses often provided broad claims without concrete examples. The study concludes that despite public commitments to responsible AI, companies have not made it a genuine priority, suggesting a need for international definitions and stronger accountability mechanisms.

## Method Summary
The authors used a mixed-methods approach analyzing four chatbots (ChatGPT/GPT-4o, Gemini 2.5, DeepSeek V3, and Grok 3) across three sources: company websites, technical documentation, and direct chatbot responses. They employed keyword frequency analysis (391 mentions / 97,854 words = 0.004%) using NVivo software to search technical documents for responsible AI terminology. Five standardized questions were posed to each chatbot to evaluate whether they could provide concrete examples of responsible AI practices. The analysis triangulated findings across these three channels to identify inconsistencies between public commitments and documented practices.

## Key Results
- Technical documentation contained only 391 responsible AI keyword mentions out of 97,854 total words (.004%)
- Companies mentioned safety 184 times but human rights, public interest, and accountability never appeared in technical documents
- Chatbot responses provided broad claims without concrete examples when prompted for specifics
- No company addressed how responsible AI commitments shape chatbot design, development, and deployment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Keyword frequency analysis in technical documentation reveals actual development priorities versus rhetorical commitments.
- Mechanism: By counting responsible AI terminology across 97,854 words in technical documents, low frequency signals low operational priority regardless of public messaging.
- Core assumption: Word frequency in technical documentation correlates with implementation emphasis during model development.
- Evidence anchors: Technical documents showed minimal use of responsible AI terminology (391/97,854 words = .004%); Table 2 shows "Safety" appears 184 times while "Human Rights," "Sustainable," "Public Good," "Equitable," and "Human-centered" each appear 0 times.
- Break condition: If companies implement robust responsible AI practices without using standard terminology in technical docs, the keyword-based detection mechanism would produce false negatives.

### Mechanism 2
- Claim: Mixed-methods triangulation (websites + documentation + direct evaluation) exposes gaps any single method would miss.
- Mechanism: Each data source captures different audiences and commitments—public websites for stakeholders, technical docs for engineers, chatbot responses for end-users—enabling cross-source inconsistency detection.
- Core assumption: Inconsistencies across these three channels indicate genuine gaps rather than audience-appropriate communication strategies.
- Evidence anchors: Using a mixed-methods approach, the authors analyzed four major chatbots across company websites, technical documentation, and direct chatbot evaluations; No company addressed how their responsible AI commitments shape their approaches to chatbot design, development, and deployment.
- Break condition: If companies legitimately tailor messaging by audience without implementation gaps, the triangulation mechanism would over-detect inconsistencies.

### Mechanism 3
- Claim: Probing chatbots with standardized questions reveals whether responsible AI principles are embedded in model behavior or merely surface-level claims.
- Mechanism: Asking for concrete examples tests whether training data, fine-tuning, and RLHF actually incorporate responsible AI principles versus producing plausible but unsupported assertions.
- Core assumption: Chatbot responses reflect training priorities and data composition rather than purely stochastic word prediction.
- Evidence anchors: Chatbot responses often provided broad claims without concrete examples; All four bots provided broad descriptions of why concerns such as human rights or democratic values were important, but when prompted for further details or specific examples, the bots provided few specifics.
- Break condition: If chatbots cannot articulate training processes regardless of actual implementation quality, the evaluation mechanism would produce false negatives.

## Foundational Learning

- Concept: **Responsible AI vs. AI Safety distinction**
  - Why needed here: The paper shows companies conflate safety (harm prevention) with responsible AI (broader principles including accountability, fairness, human rights, public interest). Understanding this distinction is critical for evaluating whether companies "walk the talk."
  - Quick check question: If a company has robust safety testing but no accountability mechanisms for harms, are they practicing responsible AI?

- Concept: **Voluntary vs. Mandated AI Governance**
  - Why needed here: Companies emphasized "mandated aspects" (safety, privacy) over "soft-law issues" (accountability, explainability), suggesting compliance-driven rather than values-driven approaches.
  - Quick check question: Why might companies prioritize safety terminology over accountability terminology in technical documentation?

- Concept: **Alignment in LLMs**
  - Why needed here: Multiple companies referenced "alignment" with human values, but the paper notes "no one really knows if a particular model is aligned with societal needs"—critical for understanding the gap between claimed and verified responsible AI.
  - Quick check question: What evidence would distinguish a truly "aligned" model from one that produces plausible alignment-related language?

## Architecture Onboarding

- Component map: Corporate Messaging Layer (websites, public commitments) -> Technical Implementation Layer (training pipelines, RLHF, safety filters) -> Model Behavior Layer (chatbot responses to user prompts) -> Evaluation Layer (standardized probing questions, keyword analysis)

- Critical path:
  1. Define responsible AI taxonomy (keywords, principles) before evaluation
  2. Collect data from all three sources simultaneously to prevent post-hoc adjustments
  3. Apply consistent coding framework across websites, documentation, and chatbot responses
  4. Cross-reference findings to identify gaps between rhetoric and practice

- Design tradeoffs:
  - Keyword-based analysis: Fast, replicable, but misses context and may miss synonyms/paraphrases
  - Direct chatbot evaluation: Captures actual behavior but subject to stochastic variability
  - Sample size (4 chatbots): Enables deep analysis but limits generalizability

- Failure signatures:
  - Chatbot provides broad claims but cannot furnish specific examples when prompted
  - Technical documentation mentions safety frequently but accountability/human rights never
  - Website commits to responsible AI but no named accountable person or remediation process
  - Privacy opt-out mechanisms exist but user conversations are web-scrapable (as noted with OpenAI/xAI)

- First 3 experiments:
  1. Keyword expansion test: Add synonyms and related terms to the keyword list, re-run NVivo analysis to test whether low frequency reflects terminology choices versus genuine gaps.
  2. Multi-turn probing: After initial broad questions, ask 2-3 follow-up questions demanding specifics to distinguish trained knowledge from plausible generation.
  3. Longitudinal tracking: Re-run evaluation quarterly after major model updates to test whether responsible AI integration improves over time or remains rhetorical.

## Open Questions the Paper Calls Out

- **Open Question 1**: Do regulatory transparency mandates, such as the EU AI Act, effectively incentivize companies to increase the frequency and specificity of responsible AI terminology in their technical documentation?
  - Basis in paper: The authors suggest policymakers should "examine whether transparency mechanisms such as the EU AI Act and Code of Practices might incentivize a stronger focus on responsible AI."
  - Why unresolved: The study provides a baseline of current practices but does not track changes in corporate behavior following the implementation of specific legal frameworks.
  - What evidence would resolve it: A longitudinal study comparing technical documentation word counts and content specificity before and after the enforcement of transparency regulations.

- **Open Question 2**: Does the observed disconnect between corporate responsible AI rhetoric and technical practice persist across a statistically representative sample of global AI developers?
  - Basis in paper: The authors caution that "these four chatbots are not necessarily a representative sample, but they provide sufficient variation in their approach to responsible AI."
  - Why unresolved: The findings are based on a small sample (n=4) of market-leading US and Chinese firms, leaving the behavior of smaller or non-US/Chinese firms unknown.
  - What evidence would resolve it: Replication of the mixed-methods analysis (website, documentation, and chatbot evaluation) across a broader, randomized set of international AI firms.

- **Open Question 3**: How can researchers validly assess a company's adherence to principles like "human rights" when standard keyword analysis returns zero matches in technical documentation?
  - Basis in paper: The authors note they were "unable to ascertain how the firms translated these terms into responsible chatbot behavior" because terms like "public interest" and "human rights" were completely absent from the analyzed text.
  - Why unresolved: Reliance on keyword frequency assumes that companies use specific academic or policy terminology when implementing these concepts, which may lead to false negatives.
  - What evidence would resolve it: Development of behavioral testing frameworks (red-teaming) that probe for specific responsible AI outcomes without relying on the presence of explicit terminology in documentation.

## Limitations
- Temporal validity: Data collection dates are unspecified, creating uncertainty about reproducibility as companies regularly update materials and models.
- Keyword coverage: Analysis relies on predefined keyword list that may miss relevant terminology, paraphrases, or emerging responsible AI concepts.
- Sample generalizability: With only four chatbots analyzed, findings may not represent the broader AI chatbot industry.

## Confidence
- High confidence: Companies mention safety and privacy far more frequently than accountability, human rights, or public interest in technical documentation (391/97,854 words = 0.004%).
- Medium confidence: Chatbots provide broad claims without concrete examples when probed for specifics.
- Medium confidence: Technical documentation uses minimal responsible AI terminology, though this may miss non-standard terminology.

## Next Checks
1. Keyword expansion test: Expand keyword list to include synonyms and related terms, then re-run NVivo analysis to determine whether low frequency reflects terminology choices versus genuine gaps.
2. Multi-turn probing: After initial broad questions, administer 2-3 follow-up questions demanding specific examples to distinguish trained knowledge from plausible generation.
3. Longitudinal tracking: Re-run complete evaluation quarterly after major model updates to test whether responsible AI integration improves over time or remains primarily rhetorical.