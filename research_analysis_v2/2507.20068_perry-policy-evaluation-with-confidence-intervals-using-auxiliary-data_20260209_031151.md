---
ver: rpa2
title: 'PERRY: Policy Evaluation with Confidence Intervals using Auxiliary Data'
arxiv_id: '2507.20068'
source_url: https://arxiv.org/abs/2507.20068
tags:
- policy
- intervals
- confidence
- state
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces two methods for constructing valid confidence\
  \ intervals in off-policy evaluation (OPE) when using data augmentation. The first,\
  \ CP-Gen, provides state-conditioned intervals via a novel conformal prediction\
  \ approach that handles high-dimensional MDPs through an \u03F5-approximation strategy."
---

# PERRY: Policy Evaluation with Confidence Intervals using Auxiliary Data

## Quick Facts
- arXiv ID: 2507.20068
- Source URL: https://arxiv.org/abs/2507.20068
- Reference count: 40
- One-line primary result: Introduces two methods for constructing valid confidence intervals in off-policy evaluation using data augmentation.

## Executive Summary
This paper addresses the challenge of constructing valid confidence intervals for off-policy evaluation (OPE) when using data augmentation through generative models. The authors introduce CP-Gen, a conformal prediction method for state-conditioned intervals, and DR-PPI, a prediction-powered inference approach for unconditional policy value estimation. Both methods are theoretically grounded and empirically validated across four domains including real healthcare data. The key insight is that naive use of augmented data in OPE fails to provide valid uncertainty quantification, necessitating principled correction mechanisms.

## Method Summary
The paper proposes two complementary methods for OPE with augmented data. CP-Gen uses conformal prediction with an ε-approximation strategy to handle high-dimensional MDPs, generating state-conditioned intervals that are valid within a bounded margin of error. DR-PPI combines doubly robust estimation with prediction-powered inference to estimate unconditional policy value, using cross-fitting to reduce variance. Both methods split the behavior dataset into training and calibration sets, use generative models to simulate trajectories, and apply correction mechanisms to ensure valid coverage. The framework addresses a fundamental gap in uncertainty quantification for OPE when working with augmented datasets.

## Key Results
- CP-Gen and DR-PPI achieve valid coverage across four domains (Inventory Control, Sepsis simulator, HalfCheetah, MIMIC-IV) while baselines that use augmented data without correction fail
- Both methods produce confidence intervals that are often tighter than standard importance sampling and direct method baselines
- CP-Gen provides state-conditioned intervals suitable for personalized decision-making, while DR-PPI gives unconditional population estimates
- The ε-approximation strategy makes CP-Gen computationally tractable for high-dimensional state spaces

## Why This Works (Mechanism)
The core mechanism addresses a fundamental problem in OPE: when we generate synthetic trajectories using a model trained on behavior data, we need to account for the fact that these trajectories are not independent samples from the behavior policy. CP-Gen solves this by using conformal prediction to construct intervals that are valid conditional on the initial state, while DR-PPI uses prediction-powered inference to correct for the bias introduced by model-based rollouts. Both methods effectively create a correction set that provides the statistical leverage needed to maintain valid coverage when working with augmented data.

## Foundational Learning
- **Off-Policy Evaluation (OPE) & Importance Sampling (IS)**: Essential because the paper estimates target policy value using data from a different behavior policy, with IS re-weighting the data for unbiased estimation. Quick check: If you have data from a policy that explores randomly, how would you estimate the value of a greedy policy?
- **Conformal Prediction**: The statistical framework used by CP-Gen to generate finite-sample, distribution-free confidence intervals. Quick check: How does conformal prediction differ from a confidence interval derived from a standard normal distribution?
- **Doubly Robust (DR) Estimation**: Forms the basis of DR-PPI's architecture, combining model-based estimates with IS-based corrections for robustness. Quick check: A DR estimator remains consistent if either the reward model is correct or the importance weights are correct, but what happens if both are slightly wrong?

## Architecture Onboarding
- **Component map**: Data Splitter -> Generative Model (T) -> CP-Gen Engine (state-conditioned) or DR-PPI Engine (unconditional)
- **Critical path**: 1) Data split (train/calibration), 2) Generative model training, 3) Trajectory generation for calibration and rollouts, 4) CP-Gen builds conformal interval for given initial state, 5) DR-PPI builds asymptotic interval for policy's average value
- **Design tradeoffs**: CP-Gen vs DR-PPI (state-conditioned vs unconditioned), ε-approximation (tractability vs bounded error), clipping IS weights (stability vs bias)
- **Failure signatures**: Invalid CP-Gen intervals (Lipschitz violation), wide DR-PPI intervals (poor model quality), all intervals invalid (extreme IS ratios)
- **First 3 experiments**: 1) Validate CP-Gen and DR-PPI on Inventory Control (check coverage and compare widths), 2) Ablate generative model quality in HalfCheetah/Sepsis (measure interval width/coverage changes), 3) Stress test with distribution shift (compare against AugIS/AugDR baselines)

## Open Questions the Paper Calls Out
- Can alternative generative architectures, specifically diffusion models, improve confidence interval tightness or validity compared to VAEs and feed-forward networks?
- How can the framework be modified to mitigate the impact of low-fidelity generated trajectories that significantly degrade estimator performance?
- Is it possible to derive valid coverage guarantees for CP-Gen without relying on the assumption that behavior and target policies are Lipschitz continuous?

## Limitations
- CP-Gen relies on Lipschitz continuity assumptions that may not hold in environments with discontinuous dynamics or abrupt policy changes
- Both methods inherit importance sampling instability when target and behavior policies have low overlap
- Empirical results for DR-PPI on continuous control domains are limited, with heavy reliance on lower-dimensional Inventory and Sepsis domains
- High-dimensional scalability of CP-Gen, while addressed through ε-approximation, lacks empirical validation in very high-dimensional domains like image observations

## Confidence
- **Validity of CP-Gen and DR-PPI**: High confidence (rigorous theoretical proofs and multi-domain empirical validation)
- **Efficiency Over Baselines**: Medium confidence (tighter intervals shown primarily on Inventory domain)
- **Advantage Over Naive Augmentation**: High confidence (clear demonstration that AugIS/AugDR fail while PERRY methods succeed)

## Next Checks
1. **Check Lipschitz Continuity**: In Sepsis, perturb state by small ε and measure policy action change to verify Assumption 2 holds
2. **Stress Test Importance Sampling Overlap**: In HalfCheetah, create target policy with actions rarely taken by behavior policy, measure IS weight variance and coverage
3. **Analyze Generative Model Impact**: In MIMIC-IV, vary generative model quality (training data, capacity) and plot resulting DR-PPI interval width and coverage