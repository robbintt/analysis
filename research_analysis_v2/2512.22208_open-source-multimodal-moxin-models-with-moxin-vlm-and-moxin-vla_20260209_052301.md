---
ver: rpa2
title: Open-Source Multimodal Moxin Models with Moxin-VLM and Moxin-VLA
arxiv_id: '2512.22208'
source_url: https://arxiv.org/abs/2512.22208
tags:
- arxiv
- preprint
- language
- data
- moxin
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Moxin, a fully open-source large language
  model (LLM) developed in compliance with the Model Openness Framework (MOF) to promote
  transparency and reproducibility. To expand its capabilities, the authors develop
  three multimodal variants: Moxin-VLM (vision-language model), Moxin-VLA (vision-language-action
  model), and Moxin-Chinese (enhanced Chinese language model).'
---

# Open-Source Multimodal Moxin Models with Moxin-VLM and Moxin-VLA

## Quick Facts
- arXiv ID: 2512.22208
- Source URL: https://arxiv.org/abs/2512.22208
- Authors: Pu Zhao; Xuan Shen; Zhenglun Kong; Yixin Shen; Sung-En Chang; Arash Akbari; Timothy Rupprecht; Lei Lu; Enfu Nan; Changdi Yang; Yumei He; Weiyan Shi; Xingchen Xu; Yu Huang; Wei Jiang; Wei Wang; Yue Chen; Yong He; Yanzhi Wang
- Reference count: 40
- Primary result: Moxin-VLM achieves 64.68 average accuracy on vision-language benchmarks, outperforming other open-source VLMs

## Executive Summary
This paper introduces Moxin, a fully open-source large language model developed in compliance with the Model Openness Framework to promote transparency and reproducibility. The authors extend Moxin with three multimodal variants: Moxin-VLM (vision-language model), Moxin-VLA (vision-language-action model), and Moxin-Chinese (enhanced Chinese language model). The Moxin-VLM uses a dual visual backbone approach with DINOv2 and SigLIP, achieving superior performance across multiple vision-language benchmarks compared to other open-source VLMs. The Moxin-VLA extends the VLM for robotic control by fine-tuning on the Open X-Embodiment dataset using the OpenVLA-OFT recipe, demonstrating state-of-the-art performance in simulation environments. The Moxin-Chinese model enhances Chinese language understanding and translation by extending the vocabulary and fine-tuning on Chinese datasets.

## Method Summary
The Moxin-VLM is trained using the Prismatic VLMs framework with DINOv2 and SigLIP as frozen visual backbones, trained for two epochs on the LLaVa v1.5 data mixture (558K captioning samples + 665K instruction tuning examples). The Moxin-VLA extends the VLM for robotic control by fine-tuning on the Open X-Embodiment dataset using the OpenVLA-OFT recipe with parallel action chunking for temporal coherence. The Moxin-Chinese model extends the vocabulary from ~32K to ~57K tokens and fine-tunes on Chinese datasets. All models are publicly released to support further research and innovation.

## Key Results
- Moxin-VLM achieves 64.68 average accuracy on vision-language benchmarks vs. 62.83 for Mistral-based VLM and 61.43 for Llama-2-based VLM
- Moxin-VLA achieves 91.95% average success rate on Open X-Embodiment tasks vs. 87.9% for NORA-Long and 81.1% for CoT-VLA
- Moxin-VLA without robotics pre-training achieves 92.5% average success rate, slightly outperforming the pre-trained version

## Why This Works (Mechanism)

### Mechanism 1: Complementary Visual Feature Fusion
Fusing DINOv2 and SigLIP visual backbones provides complementary representations that improve VLM performance over single-backbone approaches. DINOv2 captures low-level spatial properties while SigLIP captures higher-level semantic properties trained with vision-language contrastive objectives. SigLIP additionally contains internet-sourced images (sketches, diagrams, animated graphics) not present in ImageNet or DINOv2 pretraining data, creating feature complementarity. Assumption: The fusion of spatial and semantic visual features generalizes better than either alone across diverse VLM tasks.

### Mechanism 2: Single-Stage Projection Training with Frozen Visual Encoders
Training only the vision-language projector and LLM while freezing visual modules achieves effective cross-modal alignment efficiently. The pre-trained visual backbones (DINOv2, SigLIP) remain frozen while the projection layer maps patch features into the LLM embedding space. Training for two epochs on the LLaVa v1.5 data mixture aligns modalities without destabilizing visual representations. Assumption: Pre-trained visual encoders are sufficiently general that fine-tuning them provides marginal benefit relative to computational cost.

### Mechanism 3: Parallel Action Chunking for Temporal Coherence
Predicting action "chunks" (multiple future timesteps simultaneously) reduces control latency and improves temporal coherence compared to sequential autoregressive prediction. Standard VLA models predict action dimensions sequentially for a single timestep. OpenVLA-OFT recipe replaces this with parallel decoding—predicting an entire trajectory chunk at once. This enforces temporal coherence across timesteps and reduces cumulative error from sequential predictions. Assumption: Robot manipulation benefits more from temporally consistent trajectories than from per-timestep adaptive correction.

## Foundational Learning

- **Vision-Language Projector Architecture**
  - Why needed here: Understanding how visual patch features map to LLM embedding space is essential for debugging alignment failures and extending to new modalities
  - Quick check question: Given a 7B LLM with embedding dimension 4096 and a vision encoder outputting 1024 tokens × 1024 dims, what projection layer design minimizes parameter count while preserving spatial information?

- **LoRA (Low-Rank Adaptation) Fine-Tuning**
  - Why needed here: VLA experiments use LoRA (rank=32) for efficient adaptation; understanding this is critical for reproducing results or extending to new robot platforms
  - Quick check question: If base model weight matrix W is 4096×4096 and LoRA rank is 32, how many trainable parameters does LoRA add? What is the compression ratio?

- **Vocabulary Extension for Multilingual Models**
  - Why needed here: Moxin-Chinese extends vocabulary from ~32K to ~57K tokens to improve Chinese encoding efficiency; this pattern applies to any language adaptation
  - Quick check question: Why does extending vocabulary improve both encoding efficiency and generation quality for non-English languages? What is the tradeoff in model size?

## Architecture Onboarding

- **Component map:**
  Moxin-7B (base LLM)
      │
      ├── Moxin-VLM branch:
      │   ├── DINOv2 (frozen) ──┬──> Feature Fusion ──> Projector ──> Moxin-7B-LLM
      │   └── SigLIP (frozen) ──┘
      │       (trained 2 epochs on LLaVa v1.5 mixture)
      │
      ├── Moxin-VLA branch:
      │   └── Moxin-VLM + Action Head
      │       (OpenVLA-OFT recipe: action chunking, parallel decoding)
      │       (LoRA r=32, 50k steps on OXE mixture)
      │
      └── Moxin-Chinese branch:
          └── Extended vocab (57k tokens) + Chinese pretraining + translation finetuning

- **Critical path:** Base Moxin-7B → VLM training → VLA fine-tuning. Each stage builds on the previous; VLA cannot be trained without VLM checkpoint.

- **Design tradeoffs:**
  - VLA pre-training: With OXE pre-training achieves 91.95% avg success but requires 2 weeks on 8×H100. Without pre-training drops to 92.5% on some tasks but trains faster—contradictory results suggest task-dependent benefit.
  - FiLM module: Paper reports "no benefit to task success rates in the single-arm robot setting" and excludes it—do not add feature-wise linear modulation for Franka-style manipulators.
  - Training steps: 50k vs. 150k steps showed "negligible performance improvements"—early stopping is valid.

- **Failure signatures:**
  - Low RefCOCO/OCID-Ref scores → visual grounding failure, check projector alignment
  - High VizWiz "unanswerable" errors → overconfident generation, may need calibration
  - VLA jittery motion → action chunking not active or chunk size too small
  - Chinese tokenization fragmentation → vocabulary extension failed, tokens falling back to character-level

- **First 3 experiments:**
  1. **Reproduce VLM baseline:** Train Moxin-VLM using Prismatic framework on LLaVa v1.5 data, validate on GQA and RefCOCO+. Target: match reported 64.88 GQA / 71.3 RefCOCO+ within ±2 points.
  2. **Ablate visual backbones:** Train two variants—DINOv2-only and SigLIP-only—to quantify fusion benefit. Expect 1-3 point drop on spatial tasks (RefCOCO, OCID-Ref) for SigLIP-only.
  3. **VLA transfer test:** Fine-tune Moxin-VLA on a single LIBERO task (e.g., Spatial) with and without OXE pre-training to validate the claimed pre-training benefit. Use 50k steps, LoRA r=32, batch size 64.

## Open Questions the Paper Calls Out

### Open Question 1
What specific factors determine the trade-offs between generalist pre-training on the Open X-Embodiment dataset versus direct fine-tuning for specific robotic tasks? The authors investigated two distinct training strategies to evaluate the trade-offs between generalist pre-training and direct task adaptation, testing the hypothesis that semantic priors alone are sufficient. Table 3 shows the model without robotics pre-training achieved a higher average success rate (92.5% vs 91.95%), suggesting pre-training might not always be necessary, yet the pre-trained model excelled in specific categories like spatial reasoning.

### Open Question 2
Does the exclusion of the FiLM (Feature-wise Linear Modulation) module hinder performance in complex robotic settings outside of single-arm manipulation? The authors explicitly excluded the FiLM module because it "provided no benefit to task success rates in the single-arm robot setting," leaving its utility in other robotic architectures unverified. The evaluation was limited to single-arm tasks within the LIBERO simulation, so the module's effect on multi-arm or bimanual coordination is unknown.

### Open Question 3
Can the simulation-optimized Moxin-VLA maintain its high performance when transferred to real-world physical robotics? The evaluation methodology in Section 6.2 is strictly confined to the "LIBERO simulation environment," and the paper does not mention physical deployment or sim-to-real transfer results. Simulation benchmarks often fail to capture the latency constraints, sensor noise, and complex physical dynamics present in real-world robotics.

## Limitations

- **Weak direct evidence**: The specific claim that DINOv2+SigLIP fusion provides complementary representations is supported only by weak indirect evidence from benchmark performance rather than ablation studies isolating each backbone's contribution.
- **Domain shift assumptions**: The claim that freezing visual encoders provides sufficient adaptation for diverse VLM tasks assumes limited domain shift, which may break for specialized domains like medical or satellite imaging.
- **Simulation-only validation**: Moxin-VLA performance is evaluated exclusively in simulation environments, with no evidence of real-world deployment or sim-to-real transfer capabilities.

## Confidence

- **High confidence**: Benchmark performance claims (GQA, RefCOCO+, VSR scores) - directly measurable and reproducible
- **Medium confidence**: Model architecture descriptions and training procedures - well-documented but some hyperparameters unspecified
- **Low confidence**: Claims about mechanism superiority (visual fusion complementarity, frozen encoder efficiency, action chunking benefits) - based primarily on this paper's internal experiments without external validation

## Next Checks

1. **Ablation study of visual backbones**: Train three variants (DINOv2-only, SigLIP-only, fused) on spatial reasoning tasks (RefCOCO, OCID-Ref) to quantify the claimed complementarity. Measure performance drop when using single backbones.

2. **Domain shift stress test**: Fine-tune Moxin-VLM on a medical imaging dataset (e.g., MIMIC-CXR) with frozen vs. unfrozen visual encoders to validate the frozen-encoder efficiency claim across domain shifts.

3. **Robotic platform generalization**: Transfer Moxin-VLA from Franka Emika Panda to a different manipulator (e.g., UR5) and evaluate whether action chunking maintains its temporal coherence benefits across different kinematic structures.