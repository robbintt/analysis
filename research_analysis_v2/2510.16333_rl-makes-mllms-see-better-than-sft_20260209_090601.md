---
ver: rpa2
title: RL makes MLLMs see better than SFT
arxiv_id: '2510.16333'
source_url: https://arxiv.org/abs/2510.16333
tags:
- vision
- mllm
- encoder
- mllms
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how different training strategies affect\
  \ both Multimodal Large Language Models (MLLMs) and their underlying vision encoders.\
  \ While it's widely assumed that MLLM performance is inherited from their large\
  \ language model backbones, this work demonstrates that the training method\u2014\
  specifically supervised fine-tuning (SFT) versus reinforcement learning with direct\
  \ preference optimization (DPO)\u2014significantly impacts the vision encoder's\
  \ visual representations and localization ability."
---

# RL makes MLLMs see better than SFT

## Quick Facts
- arXiv ID: 2510.16333
- Source URL: https://arxiv.org/abs/2510.16333
- Authors: Junha Song; Sangdoo Yun; Dongyoon Han; Jaegul Choo; Byeongho Heo
- Reference count: 40
- Primary result: DPO-trained vision encoders produce stronger, more localized visual features than SFT, achieving state-of-the-art performance with <1% of vision pretraining cost

## Executive Summary
This paper challenges the assumption that MLLM performance is inherited from large language model backbones by demonstrating that the training method fundamentally reshapes the vision encoder's representations. Through controlled experiments comparing supervised fine-tuning (SFT) with reinforcement learning using direct preference optimization (DPO), the authors show that DPO produces stronger, more precisely localized visual features. They introduce PIVOT, a simple training recipe that applies DPO to vision encoders, achieving state-of-the-art performance while requiring less than 1% of the computational cost of standard vision pretraining.

## Method Summary
The authors employ a two-stage training pipeline. Stage 1 involves projector-only pre-training on 558K caption/VQA pairs followed by full end-to-end pre-training on 3.2M data. Stage 2 conducts controlled post-training comparisons between SFT (LR=1×10⁻⁵) and DPO (LR=1×10⁻⁶) using 20K preference pairs from the MMPR-1.2 dataset. The architecture consists of SigLIP2 vision encoders (B/16 to g/16) with Qwen2.5 LLM heads (0.5B-7B) and 2-layer MLP projectors. PIVOT specifically trains vision encoders via DPO with an LLM head, then freezes the encoder for new MLLM evaluation. The study evaluates on 16 VQA benchmarks, ImageNet linear probing, and ADE20K segmentation probing.

## Key Results
- DPO-trained vision encoders show higher ImageNet accuracy (+3.7%) and segmentation recall than SFT-trained encoders when detached from LLMs
- Grad-CAM visualizations demonstrate DPO gradients are more precisely localized to question-relevant regions compared to scattered SFT signals
- Scaling the LLM head from 0.5B to 7B increases vision encoder ImageNet accuracy by +4.4%, confirming larger LLMs transmit more informative signals
- PIVOT achieves state-of-the-art performance on vision-centric tasks with <1% of vision pretraining computational cost

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** DPO provides more precise, localized gradient signals to the vision encoder than SFT, enhancing fine-grained visual understanding.
- **Mechanism:** DPO's contrastive objective forces the model to attend to specific visual details that differentiate correct answers from plausible but incorrect ones, sharpening the feature map in the encoder.
- **Core assumption:** Rejected responses in preference pairs contain specific visual errors that backpropagate corrective signals to relevant image patches.
- **Evidence anchors:** Abstract statement on localized representations; Grad-CAM visualizations in Section 4.2 showing DPO gradients align with question-relevant regions versus scattered SFT signals.
- **Break condition:** If preference data consists primarily of knowledge errors rather than visual errors, gradient signals may not localize to image patches.

### Mechanism 2
- **Claim:** MLLM post-training physically reshapes the vision encoder's representations, making them more effective even when detached from the LLM.
- **Mechanism:** Backpropagation from the LLM head through the projector updates ViT weights to produce features that are linearly separable for downstream tasks better than original pre-trained features.
- **Core assumption:** The vision encoder is not frozen during training, allowing the "LLM preference signal" to modify the visual feature space.
- **Evidence anchors:** Abstract statement on representation reshaping; Section 4.2 results showing DPO-trained encoders have higher ImageNet accuracy and segmentation recall than SFT-trained ones.
- **Break condition:** If learning rate for vision encoder is too low or frozen, representation remains static and benefits are limited to projector/LLM.

### Mechanism 3
- **Claim:** Larger LLM backbones transmit more informative optimization signals to the vision encoder.
- **Mechanism:** A larger LLM has superior capacity to model complex decision boundaries, translating into higher-quality gradient estimates regarding why visual features are preferred.
- **Core assumption:** The informativeness of the backward pass scales with parameter count and model capability.
- **Evidence anchors:** Section 4.2 results showing scaling from 0.5B to 7B LLM increases vision encoder ImageNet accuracy by +4.4%.
- **Break condition:** If LLM is already over-parameterized for task complexity, scaling further yields diminishing returns for encoder.

## Foundational Learning

- **Concept: Direct Preference Optimization (DPO)**
  - **Why needed here:** DPO is the core driver of the "localization" effect, creating precise visual gradients through binary preference pairs.
  - **Quick check question:** Can you explain why optimizing a binary preference pair (chosen vs. rejected) creates a different gradient signal than maximizing the probability of a single correct answer (SFT)?

- **Concept: Vision Encoder-LLM Coupling**
  - **Why needed here:** The paper challenges the assumption that the encoder is static, showing how gradients flow from LLM head back through projector to ViT.
  - **Quick check question:** If you freeze the vision encoder during DPO, which mechanism described in this paper is disabled?

- **Concept: Linear Probing & Representation Quality**
  - **Why needed here:** To prove the encoder itself improved, authors use linear probing to isolate visual representation quality from LLM reasoning capability.
  - **Quick check question:** Why is linear probing on ImageNet a valid proxy for measuring the quality of a vision encoder meant for an LLM?

## Architecture Onboarding

- **Component map:** SigLIP2 Vision Encoder -> 2-layer MLP Projector -> Qwen2.5 LLM Head
- **Critical path:**
  1. Stage 1: Pre-train projector and warm up model on caption/VQA data
  2. Stage 2 (PIVOT): Apply DPO using preference pairs with non-zero Vision Encoder learning rate
  3. Evaluation: Detach encoder for probing OR evaluate full MLLM on vision-centric benchmarks
- **Design tradeoffs:**
  - SFT vs. DPO: SFT is robust for knowledge tasks; DPO excels at vision-centric tasks
  - LLM Scale vs. Cost: 7B LLM head yields better visual features than 1.5B but increases compute cost
- **Failure signatures:**
  - Gradient Scattering: Grad-CAM shows attention everywhere, indicating DPO may be failing
  - Knowledge Regression: Performance on Knowledge VQA drops, suggesting visual focus at expense of semantic breadth
- **First 3 experiments:**
  1. **Gradient Visualization:** Overfit single preference pair using SFT vs. DPO and visualize Grad-CAM on encoder patches
  2. **Linear Probe Ablation:** Train two encoders (SFT vs. DPO), freeze them, and train linear layer on ImageNet
  3. **LLM Scaling Signal:** Train same vision encoder with 0.5B vs. 7B LLM head using DPO; measure ImageNet accuracy delta

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do alternative reinforcement learning algorithms, such as PPO and GRPO, alter the visual representations of the vision encoder compared to DPO?
- **Basis in paper:** Section 6 (Broader Impact) explicitly states examining other RL algorithms would be insightful
- **Why unresolved:** Study focused exclusively on DPO as representative RL method
- **What evidence would resolve it:** Comparative analysis of gradient visualizations and segmentation probing results for PPO and GRPO versus DPO and SFT

### Open Question 2
- **Question:** Does DPO superiority for visual representation learning generalize to other MLLM architectures (e.g., InternVL, Qwen-VL) and LLM backbones (e.g., LLaMA, Gemma)?
- **Basis in paper:** Section E.2 (Future Work) asks to investigate generalization to other architectures and backbones
- **Why unresolved:** Experimental results derived using LLaVA-OneVision framework and Qwen2.5 backbones
- **What evidence would resolve it:** Replicating scaling experiments and encoder analysis on architectures like InternVL or with LLaMA backbones

### Open Question 3
- **Question:** Can novel dataset formats be designed to better leverage DPO for learning stronger visual representations?
- **Basis in paper:** Section E.2 lists exploring novel dataset formats as promising future direction
- **Why unresolved:** Current study utilized standard preference datasets (MPO, MMPR)
- **What evidence would resolve it:** Designing datasets with specific visual contrastive pairs and evaluating resulting PIVOT-enhanced encoder

### Open Question 4
- **Question:** Why does DPO performance advantage diminish significantly for Knowledge VQA tasks compared to vision-centric tasks?
- **Basis in paper:** Section E.1 notes low performance gap on Knowledge VQA and hypothesizes chosen responses contain sufficient knowledge
- **Why unresolved:** Paper observes phenomenon but lacks empirical evidence confirming rejected responses lack comparative signal for factual knowledge
- **What evidence would resolve it:** Ablation study analyzing gradient magnitudes on factual vs. visual tokens

## Limitations
- DPO β (temperature) hyperparameter not specified, assumed 0.1 based on standard practice
- Training epochs/steps for each stage are unspecified beyond learning rates and batch sizes
- Exact sampling strategy for 20K MMPR subset lacks reproducibility details

## Confidence
- **High confidence**: Empirical finding that DPO-trained encoders outperform SFT-trained encoders on vision-intensive tasks and ImageNet linear probing
- **Medium confidence**: Localization mechanism claim, relying on Grad-CAM visualizations that may have interpretation limitations
- **Medium confidence**: Representation reshaping claim, supported by probing results but requiring further validation on diverse vision tasks

## Next Checks
1. Verify localization hypothesis by training on single preference pair and visualizing Grad-CAM differences between SFT and DPO
2. Confirm representation quality improvements by conducting linear probing on multiple vision datasets with encoders frozen after SFT vs DPO training
3. Test LLM scaling hypothesis by training same vision encoder with different LLM sizes using DPO and measuring resulting encoder performance deltas on vision tasks