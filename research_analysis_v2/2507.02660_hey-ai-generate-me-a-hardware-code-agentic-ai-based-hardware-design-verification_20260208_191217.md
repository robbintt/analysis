---
ver: rpa2
title: Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design & Verification
arxiv_id: '2507.02660'
source_url: https://arxiv.org/abs/2507.02660
tags:
- verification
- design
- agents
- agent
- hitl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a multi-agent system (MAS) for end-to-end hardware
  design and verification using Large Language Models (LLMs). The method decomposes
  RTL design and verification into specialized tasks executed by collaborating agents,
  with iterative refinement and human-in-the-loop (HITL) intervention.
---

# Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design & Verification

## Quick Facts
- arXiv ID: 2507.02660
- Source URL: https://arxiv.org/abs/2507.02660
- Reference count: 22
- Primary result: Multi-agent system achieves >95% coverage with reduced verification time compared to zero-shot LLM prompting

## Executive Summary
This paper presents a multi-agent system (MAS) for end-to-end hardware design and verification using Large Language Models. The system decomposes RTL design and verification into specialized tasks executed by collaborating agents, with iterative refinement and human-in-the-loop intervention. Evaluated on five open-source designs, the MAS approach achieved over 95% coverage while outperforming zero-shot LLM methods, demonstrating superior performance, adaptability, and configurability.

## Method Summary
The methodology uses the AutoGen framework with specialized agents including design lead, verification lead, RTL agents, formal verification agents, critic agents, code executor agents, and a group chat manager for orchestration. Agents work in sequential coordination through deliberation loops with a 5-iteration threshold before escalating to human intervention. The system generates SystemVerilog RTL modules and formal properties (SVAs), then uses EDA tools for linting and formal verification to achieve high coverage metrics.

## Key Results
- MAS achieved 86.21% initial coverage vs 69.85% for zero-shot LLM methods
- Final coverage reached 97.73% with HITL intervention compared to 76.07% for zero-shot
- HITL interventions averaged under 30 minutes per design
- Generated lint-error-free RTL with high formal coverage across all five test designs

## Why This Works (Mechanism)

### Mechanism 1
Specialized multi-agent decomposition outperforms zero-shot LLM prompting for hardware design tasks. The system breaks down RTL design and verification into specialized roles, reducing LLM attention deficits and hallucination through constrained agent scopes. Structured coordination via group chat manager enables sequential interactions that improve output quality.

### Mechanism 2
Iterative deliberation with self-correction loops improves RTL and property quality through autonomous refinement. Agents engage in propose-critique-revise cycles with a 5-iteration threshold, allowing critic agents to flag errors and trigger re-generation. This iterative approach converges on higher quality outputs compared to single-shot generation.

### Mechanism 3
Human-in-the-loop intervention at bounded escalation points achieves near-complete coverage with minimal human effort. HITL is triggered when autonomous resolution fails, allowing humans to resolve ambiguities, remove placeholder code, and refine specifications. This targeted intervention reduces cognitive load compared to full manual design.

## Foundational Learning

- **Register Transfer Level (RTL) Design and SystemVerilog**: Understanding RTL syntax, synthesizability, lint errors, and FSM design is essential for evaluating AI-generated outputs and providing meaningful HITL feedback. Quick check: Can you identify why a generated RTL module with unresolved placeholder signals would fail linting?

- **Formal Verification and SystemVerilog Assertions (SVA)**: The methodology generates formal properties and uses JasperGold for proving them. Understanding coverage metrics, counterexamples, and property types is crucial for verification work. Quick check: What does a counterexample (CEX) from a formal tool indicate about the relationship between an assertion and the RTL design?

- **Multi-Agent System (MAS) Coordination Patterns**: The architecture uses pipeline decomposition, deliberation loops, and role-based group chat orchestration. Understanding agent roles and interaction topologies is required to configure and debug the system. Quick check: In a group chat manager topology, what happens if two agents attempt to communicate simultaneously without sequential orchestration?

## Architecture Onboarding

- **Component map**: Specification document -> Planning (microarchitecture + vPlan) -> Development (RTL + SVA generation with critic review) -> Execution (lint + formal prove + coverage analysis) -> HITL escalation -> Sign-off with lint-error-free RTL and coverage report

- **Critical path**: Specification document → Planning (microarchitecture + vPlan) → Development (RTL + SVA generation with critic review) → Execution (lint + formal prove + coverage analysis) → HITL escalation (if thresholds exceeded) → Sign-off with lint-error-free RTL and coverage report

- **Design tradeoffs**: Temperature setting affects output consistency vs diversity; iteration threshold balances refinement vs stagnation; agent specialization depth impacts quality vs coordination complexity; hot-swappable LLMs allow model updates without architecture changes

- **Failure signatures**: Placeholder code in RTL, syntax/lint errors persisting after iteration limit, coverage plateau below threshold, agent loop stagnation, non-synthesizable constructs in RTL

- **First 3 experiments**:
  1. Baseline with simple design: Run CRC at temperature 0.2; measure lint errors, iterations to convergence, and initial coverage
  2. Temperature sweep: Run ECC or FIFO at temperatures 0.2, 0.5, 0.8; compare logical correctness categories and HITL effort
  3. Zero-shot vs. MAS comparison: For same design, run single-prompt zero-shot generation and MAS methodology; compare initial coverage and human intervention time

## Open Questions the Paper Calls Out

- Can the methodology be effectively extended to generate efficient simulation-based testbenches and stimuli for more intricate designs? The current study focuses on static linting and formal property verification, leaving dynamic simulation domain unexplored.

- To what degree can explainable AI techniques reduce the necessity of Human-in-the-Loop (HITL) interventions? The current framework relies on HITL to resolve ambiguities that automated agents cannot fix, limiting full autonomy.

- Does the multi-agent coordination scale efficiently to complex, industrial-grade SoCs compared to the small open-source IP blocks evaluated? The evaluation is limited to five relatively simple designs, and it's unclear if agent overhead remains manageable at larger scales.

## Limitations

- Evaluation relies on five relatively simple designs (4-11 state FSMs, no clock domain crossing), leaving effectiveness on complex industrial designs unproven
- Agent configuration sensitivity noted but not systematically studied—performance appears contingent on optimal agent count and role definition
- Reliance on GPT-4o limits reproducibility and generalizability to other LLM models or open-source alternatives

## Confidence

- **High confidence**: MAS outperforms zero-shot LLM prompting for RTL design and verification (supported by direct comparative metrics showing 86.21% vs 69.85% initial coverage, and 97.73% vs 76.07% final coverage with HITL)
- **Medium confidence**: HITL intervention at bounded escalation points achieves near-complete coverage with minimal human effort (based on average <30 minutes per design, but lacking systematic measurement of reviewer expertise and cognitive load)
- **Medium confidence**: Iterative deliberation with self-correction loops improves RTL and property quality (supported by iteration data showing convergence patterns, but error detection capability of critic agents not independently validated)

## Next Checks

1. **Scalability validation**: Apply the MAS methodology to industrial designs with complex timing requirements (multiple clock domains, asynchronous interfaces) to verify effectiveness beyond simple FSMs.

2. **Agent configuration sensitivity study**: Systematically vary agent count, specialization depth, and iteration thresholds across multiple design types to quantify performance tradeoffs and identify optimal configurations.

3. **Model generalizability test**: Replace GPT-4o with open-source LLM alternatives (e.g., Llama3.1-70B) while maintaining identical agent architectures to assess dependency on specific model capabilities.