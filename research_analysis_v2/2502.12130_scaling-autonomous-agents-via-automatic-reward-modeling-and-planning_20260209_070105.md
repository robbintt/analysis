---
ver: rpa2
title: Scaling Autonomous Agents via Automatic Reward Modeling And Planning
arxiv_id: '2502.12130'
source_url: https://arxiv.org/abs/2502.12130
tags:
- reward
- task
- action
- data
- environment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of improving decision-making
  capabilities of large language model (LLM) agents in multi-step tasks requiring
  environmental feedback, such as online shopping, scientific reasoning, and mathematical
  problem-solving. The authors propose ARMAP, a framework that automatically learns
  a reward model from the environment without human annotations.
---

# Scaling Autonomous Agents via Automatic Reward Modeling And Planning

## Quick Facts
- arXiv ID: 2502.12130
- Source URL: https://arxiv.org/abs/2502.12130
- Reference count: 40
- Primary result: ARMAP achieves 66.8% success rate on Webshop with MCTS versus 52.0% sampling baseline

## Executive Summary
This paper addresses the challenge of improving LLM agent performance on multi-step tasks requiring environmental feedback by automatically learning reward models without human annotations. The ARMAP framework generates synthetic trajectory data through LLM exploration, trains a reward model to evaluate task success, and integrates this model with planning algorithms like MCTS to guide decision-making. Experiments on Webshop, ScienceWorld, and Game of 24 demonstrate consistent improvements across different LLM backbones and planning strategies, with the largest gains observed when combining ARMAP with MCTS.

## Method Summary
ARMAP automatically learns reward models through a three-stage process: (1) an LLM agent explores the environment to generate diverse trajectories, (2) a second LLM refines task descriptions and synthesizes negative trajectories alongside correct ones, and (3) a reward model is trained on these triplets using pairwise ranking loss. The trained reward model is then integrated with LLM agents and planning algorithms including Best-of-N, Reflexion, and MCTS to improve task-solving performance. The approach leverages a VILA-3B vision-language model as the reward backbone, fine-tuned with LoRA adapters on synthetic instruction-trajectory pairs, and uses tree search to explore high-reward action sequences during inference.

## Key Results
- ARMAP with MCTS achieves 66.8% success rate on Webshop versus 52.0% for sampling baseline with Llama70B
- On ScienceWorld seen tasks, ARMAP improves performance from 49.6% (greedy) to 61.2% (ARMAP-M with Llama70B)
- The framework enables controllable generation through custom reward targets, reducing Webshop trajectory length from 4.5 to 4.0 actions while maintaining success rates

## Why This Works (Mechanism)

### Mechanism 1: Evaluation-Easier-Than-Generation Decomposition
Separating trajectory evaluation from generation reduces learning burden when tasks permit reliable post-hoc evaluation. Rather than training a policy model to directly predict optimal actions, ARMAP trains a lightweight reward model to score trajectories, transferring complexity from sequential generation to single-step classification. This works when target tasks have clear success criteria that can be evaluated after the fact.

### Mechanism 2: Synthetic Trajectory Contrastive Learning
Training a reward model on synthetic positive-negative trajectory pairs produces usable evaluation signals when the synthetic data distribution approximates real failure modes. An LLM agent randomly explores, a second LLM refines task descriptions to match observed trajectories, and negative trajectories are generated by modifying successful actions. The reward model learns via pairwise comparison to distinguish good from bad trajectories.

### Mechanism 3: Inference-Time Reward-Guided Planning
Integrating a learned reward model with tree search improves action selection when reward scores for partial trajectories correlate with eventual task success. Planning algorithms use reward model scores to prioritize promising branches, with MCTS tracking visit counts and maximum predicted rewards to balance exploitation and exploration. This approach requires reward signals that are predictive early in trajectories rather than only at terminal states.

## Foundational Learning

- **Partial Observable Markov Decision Process (POMDP) Formulation**: ARMAP frames agent tasks as POMDPs (X, S, A, O, T); understanding this formalization is prerequisite to grasping how reward models interface with planning. Quick check: Given a Webshop task, can you identify the observation space O and how it differs from the underlying state S?

- **Bradley-Terry / Pairwise Comparison Loss**: The reward model optimization uses pairwise comparison loss from RLHF literature; familiarity prevents misinterpreting this as simple binary classification. Quick check: Why might pairwise comparison outperform binary classification for reward learning? (Hint: see Table 7 ablation.)

- **Monte Carlo Tree Search (MCTS) Mechanics**: MCTS is ARMAP's strongest planning variant; understanding selection, expansion, simulation, and backpropagation is essential for debugging integration issues. Quick check: In ARMAP's MCTS, what two quantities does the algorithm track per node, and how do they influence exploration?

## Architecture Onboarding

- **Component map**: Data Generation Pipeline (LLM agent → Environment → Trajectories → Second LLM → Refined instructions + synthetic negatives) → Reward Model (VILA-3B + linear head → scalar score) → Planning Module (Best-of-N / Reflexion / MCTS) → Environment Wrappers (Webshop, ScienceWorld, Game of 24)

- **Critical path**: 1) Verify environment setup (docker containers from AgentBench for Webshop/ALFWorld) 2) Run data generation pipeline (expect 2,436–37,885 trajectory pairs depending on task) 3) Train reward model (VILA-3B with LoRA, ~1,100–1,500 steps depending on task) 4) Integrate with policy LLM and planning algorithm 5) Evaluate with max 10 trajectories (Webshop/ScienceWorld) or 100 (Game of 24)

- **Design tradeoffs**: Reward model size vs. efficiency (VILA-3B vs. VILA-13B; larger models improve scores but increase latency), data volume vs. quality (1/25 of training data still yields functional models), planning algorithm choice (MCTS strongest but most expensive vs. Best-of-N simpler but limited by sampling diversity)

- **Failure signatures**: Reward model ignores multi-condition instructions (focuses on price/size but misses product attributes), mis-calibrated condition priorities (overweights color vs. size), missing domain knowledge (cannot assess biological facts without external knowledge), visual input ablation (removing visual observations drops Webshop performance)

- **First 3 experiments**: 1) Reproduce Webshop baseline: Set up AgentBench docker, run Llama70B greedy (~50.4%), then ARMAP-B (~62.0%) to validate pipeline 2) Ablate synthetic data quality: Train reward model with data from smaller LLM (Phi-3.8B) vs. Llama70B; compare ScienceWorld scores 3) Test custom reward targets: Implement length penalty on Webshop; verify trajectory length reduction (4.5 → 4.0) with maintained task success

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the automatic data generation pipeline be explicitly modified to prioritize instructions with multiple constraints to prevent the reward model from overlooking specific task requirements? The authors identify this as a common error but don't implement the suggested data augmentation strategy.

- **Open Question 2**: How can external knowledge be effectively integrated into the reward modeling process to rectify commonsense reasoning errors? The authors propose this as a strategy to fix failure cases involving domain knowledge, but the mechanism for external intervention remains undefined.

- **Open Question 3**: How robust is the reward model synthesis process when the initial "explorer" agent generates incoherent or extremely low-quality trajectories? The method assumes exploration data is sufficient for refinement but doesn't analyze lower bounds of explorer agent capability required for successful reward learning.

## Limitations

- **Synthetic Data Quality Dependence**: The approach relies on LLM-generated negative trajectories, but no validation is provided that these negatives represent realistic failure modes rather than artificial patterns that could mislead the reward model.

- **Task Domain Restriction**: The evaluation assumes tasks with verifiable post-hoc evaluation (online shopping, scientific reasoning, math), but the paper doesn't establish generalization to domains where success criteria are subjective or require extensive domain expertise.

- **Reward Model Calibration Uncertainty**: While the reward model shows improved performance, there's no analysis of whether the learned reward signals are properly calibrated across all tasks.

## Confidence

- **High Confidence**: The core architectural framework (automatic reward modeling + planning integration) and its implementation details are well-specified and reproducible.
- **Medium Confidence**: The claimed performance improvements over baselines, though supported by results, may depend heavily on the specific synthetic data generation process which isn't fully specified.
- **Low Confidence**: The mechanism-level claims about evaluation being "easier than generation" and synthetic contrastive learning effectiveness lack direct empirical validation or comparison to alternative data generation approaches.

## Next Checks

1. **Data Quality Validation**: Conduct ablation studies comparing reward model performance when trained on LLM-generated negatives versus hand-curated or randomly sampled negatives to assess synthetic data quality impact.

2. **Domain Generalization Test**: Apply ARMAP to a task with more subjective evaluation criteria (e.g., creative writing or open-ended problem solving) to test whether the evaluation-generation asymmetry claim holds beyond structured domains.

3. **Calibration Analysis**: Measure and report reward score distributions and their correlation with actual task success rates across all benchmarks to assess whether the reward model provides reliable guidance beyond simple ranking.