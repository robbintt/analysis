---
ver: rpa2
title: Solving Zero-Sum Convex Markov Games
arxiv_id: '2506.16120'
source_url: https://arxiv.org/abs/2506.16120
tags:
- gradient
- convex
- function
- mins
- games
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides the first provable convergence guarantees for
  independent policy gradient methods in two-player zero-sum convex Markov games.
  The authors address the fundamental challenge of non-convexity in these games by
  introducing a simple nonconvex regularization technique that transforms the min-max
  optimization problem into a nonconvex-proximal Polyak-Lojasiewicz (NC-pPL) objective.
---

# Solving Zero-Sum Convex Markov Games

## Quick Facts
- **arXiv ID:** 2506.16120
- **Source URL:** https://arxiv.org/abs/2506.16120
- **Reference count:** 40
- **Primary result:** First provable convergence guarantees for independent policy gradient methods in two-player zero-sum convex Markov games

## Executive Summary
This paper provides the first provable convergence guarantees for independent policy gradient methods in two-player zero-sum convex Markov games. The authors address the fundamental challenge of non-convexity in these games by introducing a simple nonconvex regularization technique that transforms the min-max optimization problem into a nonconvex-proximal Polyak-Lojasiewicz (NC-pPL) objective. This regularization stabilizes policy gradient updates and ensures convergence to Nash equilibria with polynomial sample and iteration complexity.

## Method Summary
The method employs two algorithms: nested policy gradient (Nest-PG) and alternating policy gradient descent-ascent (Alt-PGDA). Both algorithms use direct tabular policy parametrization with ε-greedy exploration and operate on a perturbed utility that includes a nonconvex regularization term \(-\frac{\mu}{2}\|\lambda_2(x,y)\|^2\). The algorithms rely on stochastic gradient estimators using trajectory samples and require oracle access to the gradient of the utility with respect to occupancy measures. The regularization parameter μ is tuned to O(ϵ) to balance convergence speed against bias from the true equilibrium.

## Key Results
- Alt-PGDA achieves O(1/ϵ⁶) polynomial iterations and samples to reach ϵ-approximate Nash equilibrium
- Nest-PG achieves O(1/ϵ^{11/2}) polynomial iterations and samples for general concave utilities
- Hidden convexity of convex Markov games enables gradient-domination and best-response Lipschitz continuity
- Novel Lipschitz continuity results for best-response mappings in NC-pPL games established

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Nonconvex regularization transforms the nonconvex-nonconcave min-max objective into a structured NC-pPL landscape that enables convergence.
- Mechanism: The paper adds a term \(-\frac{\mu}{2}\|\lambda_2(x,y)\|^2\) to the utility, making the transformed utility hidden-strongly-concave for the maximizing player. This implies the proximal-PL condition with respect to policy variables.
- Core assumption: Original utility is concave in occupancy measures; occupancy-to-policy mapping is invertible with Lipschitz continuous inverse.
- Break condition: If utility not concave in occupancy measures or mapping not sufficiently regular, hidden-strong-concavity → pPL implication fails.

### Mechanism 2
- Claim: Hidden convexity of cMG utilities enables gradient-domination and best-response Lipschitz continuity, which stabilize alternating updates.
- Mechanism: For fixed opponent policy, utility equals concave function composed with occupancy mapping. Hidden-strong-convexity implies pPL, and Theorem 4.1 shows best-response mapping is Lipschitz, bounding opponent response shifts.
- Core assumption: Utility F is concave in each player's marginal occupancy measure; occupancy-to-policy mapping is invertible and Lipschitz.
- Break condition: If occupancy mapping not invertible or Lipschitz constants scale poorly, convergence rates degrade.

### Mechanism 3
- Claim: Alternating and nested policy gradient updates converge to ϵ-NE with polynomial complexity by exploiting NC-pPL and two-sided pPL structure.
- Mechanism: Alt-PGDA updates x then y sequentially using stochastic inexact gradients. Under NC-pPL, stationarity proxy decreases on average; under two-sided pPL, Lyapunov function contracts linearly.
- Core assumption: Access to gradient oracle for ∇_λ F; ε-greedy exploration controls variance; utility is smooth; step-sizes are properly tuned.
- Break condition: If gradients too noisy, inexactness too large, or step-sizes violate tuning conditions, descent/contraction inequalities fail.

## Foundational Learning

- **Policy Gradient Methods in MDPs**: Understanding single-agent policy gradient theorem and stochastic estimator is prerequisite for applying these methods to multi-agent settings. Quick check: Can you derive ∇_x U(x,y) via chain rule from ∇_λ₁ F · ∇_x λ₁(x,y)?
- **Markov Games (Zero-Sum) and Nash Equilibrium**: The target is ϵ-approximate Nash equilibrium; zero-sum structure is essential for min-max formulation. Quick check: State definition of ϵ-NE: U(x*, y) - ϵ ≤ U(x*, y*) ≤ U(x, y*) + ϵ for all x,y.
- **Proximal Polyak-Łojasiewicz (pPL) Condition**: Core structural result is that regularized cMGs satisfy NC-pPL. Quick check: How does pPL differ from strong convexity? (pPL is gradient-domination without requiring convexity.)
- **Occupancy Measures in MDPs/MGs**: Hidden convexity is in terms of occupancy measures λ₁, λ₂; Lemma 2.1's continuity properties are used repeatedly. Quick check: What is relationship between policy x and state-action occupancy measure λ₁(x,y)? (Occupancy measure is expected discounted state-action frequency induced by policy.)

## Architecture Onboarding

- **Component map**: Game Interface -> Policy Parametrization -> Regularization Module -> Stochastic Gradient Estimator -> Optimization Core -> Best-Iterate Selector
- **Critical path**:
  1. Regularizer tuning: μ = O(ϵ) trades off bias vs. stability
  2. Step-size tuning: Must satisfy τ_x ≤ 1/(5ℓ_Φ) or similar conditions
  3. Batch-size & horizon: M_x, M_y control variance; H controls truncation bias
  4. Exploration parameter: ε in ε-greedy controls gradient estimator variance
- **Design tradeoffs**:
  - Nest-PG vs. Alt-PGDA: Nest-PG better iteration dependency on ϵ for strongly concave utilities but requires inner-loop iterations
  - Regularization strength: Larger μ gives faster convergence but larger bias from true equilibrium
  - Batch vs. trajectory length: More samples reduce variance but increase sample complexity
- **Failure signatures**:
  - Oscillating/cycling policies: Likely step-sizes too large or missing regularization
  - Convergence to biased equilibrium: Regularizer μ too large
  - High variance/no convergence: ε too small in ε-greedy or batch sizes insufficient
  - Numerical instability in occupancy inversion: If min_s ϱ(s) extremely small, inverse mapping becomes ill-conditioned
- **First 3 experiments**:
  1. Warm-up on iterated RPS: Replicate Figure 1 experiment with Alt-PGDA on small state game, vary μ to observe bias-speed tradeoff
  2. Hyperparameter sweep for step-sizes: On small cMG, grid search τ_x, τ_y around theoretical values to validate sensitivity
  3. Variance & horizon study: Fix cMG, run Alt-PGDA with varying batch sizes M_x, M_y and trajectory horizons H; plot sample complexity vs. final exploitability

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the independent policy gradient convergence guarantees be extended to general-sum convex Markov games with more than two players? Basis: The conclusion states "We anticipate fascinating explorations of multi-player cMG interactions." Why unresolved: Current analysis relies heavily on two-player zero-sum structures which may not hold in general-sum settings. What evidence would resolve it: A convergence proof for independent policy gradient in n-player general-sum cMGs or convergence to correlated equilibria.

- **Open Question 2**: Is it possible to construct a provably convergent gradient estimator that relies entirely on trajectory samples rather than oracle access to ∇_λ F? Basis: Appendix E.2, Remark 4 states "Designing a gradient estimator that entirely relies on samples lies well-beyond the scope of our paper." Why unresolved: Current theoretical guarantees assume a "black-box" gradient oracle which is often unavailable in practical RL. What evidence would resolve it: A sample-based algorithm that converges to an ϵ-NE without oracle access, or a lower bound proving such estimation is intractable.

- **Open Question 3**: Can these methods be adapted to control equilibrium selection or guarantee specific equilibrium performance metrics? Basis: The conclusion identifies "equilibrium selection, and notions of equilibrium performance" as meaningful, challenging problems. Why unresolved: Paper guarantees convergence to an approximate Nash equilibrium but provides no mechanism to select a specific one based on utility or performance. What evidence would resolve it: An algorithm that provably converges to the global optimum or Pareto-efficient equilibrium among the set of possible solutions.

## Limitations

- Reliance on oracle access to gradient of utility with respect to occupancy measures, which may not be available in practical applications
- Assumes full support initial distribution and ε-greedy exploration, which may not hold in practice and could affect gradient variance bounds
- Optimal regularization parameter μ depends on problem structure and may require careful tuning in practice

## Confidence

- **High Confidence**: Core structural result that cMGs satisfy NC-pPL with regularization; convergence rates for Alt-PGDA and Nest-PG under NC-pBL and two-sided pBL
- **Medium Confidence**: Lipschitz continuity of best-response mapping; sample complexity bounds depending on gradient estimator variance and bias
- **Low Confidence**: Exact practical performance and sensitivity to hyperparameters like step-sizes and batch sizes

## Next Checks

1. **Bias-Stability Tradeoff Experiment**: Reproduce iterated RPS experiment with varying μ to empirically validate predicted bias-speed tradeoff; plot exploitability decay for multiple μ values and verify optimal μ aligns with theoretical prediction of μ = O(ϵ).

2. **Step-Size Sensitivity Analysis**: Conduct grid search over step-sizes τ_x, τ_y for a small cMG and plot convergence curves; verify optimal step-sizes align with theoretical guidelines and convergence degrades when step-sizes are too large or too small.

3. **Gradient Variance and Horizon Study**: For fixed cMG, run Alt-PGDA with varying batch sizes M_x, M_y and trajectory horizons H; plot sample complexity against final exploitability to empirically verify polynomial dependencies predicted by theorems.