---
ver: rpa2
title: An Empirical Study for Representations of Videos in Video Question Answering
  via MLLMs
arxiv_id: '2510.12299'
source_url: https://arxiv.org/abs/2510.12299
tags:
- video
- visual
- audio
- subtitles
- mllms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates the impact of different video representation\
  \ modalities\u2014visual frames, subtitles, and audio signals\u2014on the performance\
  \ of multimodal large language models (MLLMs) for video question answering (VideoQA).\
  \ Through systematic evaluation on VideoMME and LongVideoBench benchmarks, the research\
  \ reveals that while visual frames significantly enhance accuracy, they also impose\
  \ heavy computational costs in terms of GPU memory and inference latency."
---

# An Empirical Study for Representations of Videos in Video Question Answering via MLLMs

## Quick Facts
- **arXiv ID**: 2510.12299
- **Source URL**: https://arxiv.org/abs/2510.12299
- **Authors**: Zhi Li; Yanan Wang; Hao Niu; Julio Vizcarra; Masato Taya
- **Reference count**: 18
- **Primary result**: Visual frames enhance accuracy but impose heavy computational costs in MLLM-based VideoQA; subtitles offer a lightweight yet effective alternative, especially for long videos.

## Executive Summary
This empirical study systematically investigates the impact of different video representation modalities—visual frames, subtitles, and audio signals—on multimodal large language model (MLLM) performance for video question answering (VideoQA). Through evaluation on VideoMME and LongVideoBench benchmarks using models like LLaVA-Video and Qwen2.5-VL, the research reveals a clear trade-off: visual frames significantly boost accuracy but at substantial costs in GPU memory and inference latency, while subtitles provide a more efficient alternative, particularly for long videos. The findings highlight the importance of balancing accuracy and computational efficiency when designing MLLM-based VideoQA systems.

## Method Summary
The study evaluates MLLMs (LLaVA-Video and Qwen2.5-VL) on two benchmarks: VideoMME (900 videos, 3,000 4-choice questions) and LongVideoBench Val (753 videos, 1,337 5-choice questions). Subtitles are extracted using WebRTC VAD for speech segmentation followed by Whisper-Large-v3 ASR. Video representations include uniformly sampled visual frames (processed via a visual encoder and projector), subtitles, and raw audio embeddings. Input configurations tested are question-only (Q), Q+Subtitles (Q+S), Q+Visual (Q+V), Q+Audio (Q+A), and combinations like Q+S+V. Performance is measured via accuracy, GPU VRAM usage, and inference latency using lmms-eval with default hyperparameters.

## Key Results
- Visual frames substantially enhance accuracy but impose heavy computational costs (GPU memory and latency).
- Subtitles provide a lightweight yet effective alternative, particularly for long videos.
- The combination of subtitles and visual frames achieves the best performance in most cases, outperforming audio-visual combinations.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Textual subtitles derived via ASR provide a lightweight yet effective semantic representation for long video QA, particularly outperforming visual frames when token length constraints limit frame processing.
- Mechanism: ASR converts raw audio speech signals into text. This text, along with the question, is tokenized and processed by the LLM's text encoder. The LLM leverages its strong language understanding capabilities to perform reasoning over the textual content, which compresses the video's semantic information without the high token cost of raw visual data.
- Core assumption: The spoken content in a video carries the necessary semantic information required to answer the questions, and the ASR model is sufficiently accurate to produce a reliable transcript.
- Evidence anchors:
  - [abstract] "subtitles provide a lightweight yet effective alternative, particularly for long videos."
  - [section 3.2] "In VideoMME, subtitles are more effective than visual frames... for long videos. This is because MLLMs can process a small number of frames due to the limitation of token length."
  - [corpus] No directly comparable mechanism found in corpus neighbors. Corpus papers focus on reasoning enhancements (ReasVQA) and temporal dynamics (Video Flow) rather than modality cost-benefit tradeoffs.
- Break condition: The video content is primarily visual (e.g., silent films, non-speech action) or the dialogue is irrelevant to the visual events being queried. ASR accuracy is also a bottleneck.

### Mechanism 2
- Claim: Combining visual frame representations with textual subtitles yields superior performance by integrating complementary information sources—visual for spatial/action details, textual for semantic context.
- Mechanism: Visual frames are encoded by a visual encoder and projected into the LLM's embedding space. Subtitles are encoded by the text encoder. The LLM then performs cross-modal reasoning, attending to information from both the visual tokens (E_V) and the text tokens (S) alongside the question (Q) to generate an answer.
- Core assumption: The visual and textual modalities are not perfectly redundant and provide non-overlapping, useful information. The LLM's attention mechanism can effectively fuse these different token types without being overwhelmed.
- Evidence anchors:
  - [abstract] "...while visual frames substantially enhance accuracy..."
  - [section 3.2] "The combination of subtitles and visual frames achieves the best performance in most cases, especially outperforming the combination of audio signal and visual frames..."
  - [corpus] The corpus neighbor "OmniVideoBench" mentions evaluating "synergistic reasoning capabilities across audio and visual modalities," aligning with the principle of multi-modal combination.
- Break condition: The modalities provide conflicting information, or one modality is extremely noisy. The model's context window may also be exceeded by the combined token count of long subtitles and many frames.

### Mechanism 3
- Claim: Visual frame processing is the primary driver of computational cost (GPU VRAM and inference latency) in MLLM-based VideoQA.
- Mechanism: Processing visual frames requires storing and computing activations for a high-dimensional tensor through a visual encoder and an MLP projector. This generates a large number of visual tokens, which are then processed by the autoregressive LLM. The attention complexity scales with sequence length, making long visual token sequences computationally expensive.
- Core assumption: The model architectures use standard transformer attention mechanisms where complexity scales with the number of input tokens. Visual tokens vastly outnumber text tokens in typical video inputs.
- Evidence anchors:
  - [abstract] "...visual frames substantially enhance accuracy but impose heavy costs in GPU memory and inference latency..."
  - [section 3.2] "...visual frames substantially dominate memory demand, nearly doubling the usage for 7B-scale models..." and "...inclusion of visual frames substantially dominates inference latency..."
  - [corpus] Corpus papers are not focused on computational cost analysis of visual tokens. Paper [6] "BIMBA" is cited for "selective-scan compression," implying the cost problem is recognized.
- Break condition: Models use extremely aggressive frame sampling or highly efficient visual token compression techniques not covered in this study. Hardware with vast memory bandwidth could mitigate latency issues.

## Foundational Learning
- Concept: **Multimodal Token Alignment & Fusion**
  - Why needed here: To understand how a model trained primarily on text processes visual or audio information. Encoders for non-text modalities produce embeddings that a "projector" (often an MLP) maps into the LLM's embedding space, making video frames look like another language to the LLM.
  - Quick check question: If an MLLM has a vocabulary of 100k text tokens, how does it "understand" a video frame tensor? (Answer: The visual encoder and projector convert the frame into a sequence of embeddings in the same vector space as text embeddings, which the LLM treats as special tokens.)

- Concept: **Computational Trade-offs in Sequence Modeling**
  - Why needed here: The study highlights the high cost of visual frames. This cost stems from Transformer-based LLMs, where computational complexity and memory usage are heavily influenced by sequence length. Video frames create very long token sequences.
  - Quick check question: Why does feeding 100 frames to an LLM take more memory than feeding a 1000-word transcript? (Answer: Each frame is broken down into many visual tokens. 100 frames can result in thousands of visual tokens, whereas 1000 words are just 1000 tokens, creating a much longer sequence for the video.)

- Concept: **Automatic Speech Recognition (ASR) Pipeline**
  - Why needed here: The paper proposes using ASR as a core video representation method. Understanding the two-stage pipeline—Voice Activity Detection (VAD) for segmentation followed by ASR for transcription—clarifies how subtitle text is generated and its potential failure modes.
  - Quick check question: In the paper's methodology, why is a Voice Activity Detector (VAD) used before the ASR model? (Answer: To segment the raw audio into clips containing only speech, improving transcription accuracy and efficiency by avoiding processing silence or non-speech noise.)

## Architecture Onboarding
- Component map: Representation Extraction Module (Visual Encoder + Projector, Audio Encoder + Projector, ASR Pipeline) -> VideoQA Module (Frozen LLM with Text Encoder) -> LLM Inference (Question + Modality Tokens -> Answer)

- Critical path:
  1. **Modality Selection:** Choose which input(s) to use (Question only, Q+Subtitles, Q+Visual, Q+Audio, or combinations).
  2. **Representation Extraction:**
      - If Subtitles: Run audio through VAD -> ASR (Whisper) -> Get text S.
      - If Visual: Sample frames from video -> Pass through Visual Encoder -> Pass through Projector -> Get visual tokens E_V.
  3. **Tokenization & Encoding:** Encode the question Q and any text subtitles S using the LLM's text encoder.
  4. **LLM Inference:** Concatenate tokens from all selected modalities into a single sequence. Feed this sequence into the frozen LLM to perform autoregressive generation and produce the answer Y.

- Design tradeoffs:
  - **Accuracy vs. Efficiency:** Visual frames provide the highest accuracy gain but at the greatest computational cost (high GPU memory, long latency). Subtitles are much more efficient but may miss visual-only details. The choice depends on the deployment budget and task requirements.
  - **Complementarity:** Subtitles and visual frames are complementary. Subtitles excel at long-video semantic understanding where visual token limits hinder frame-based analysis. Visual frames excel at capturing fine-grained spatial and action details. Combining them (Q+S+V) yields the best performance but combines the costs.
  - **Alternative Audio:** The study finds Q+S+V often outperforms Q+A+V, suggesting that for many VideoQA tasks, the textual transcription of speech is a more effective representation for the LLM than raw audio embeddings.

- Failure signatures:
  - **Visual-Only on Long Videos:** Expect a significant drop in accuracy compared to subtitle or combined approaches, as the limited number of sampled frames fails to capture the full temporal context.
  - **Question-Only Reliance:** Performance may be deceptively high due to the LLM's internal commonsense knowledge, leading to "hallucinated" answers not grounded in the video evidence.
  - **Resource Exhaustion with Video:** Processing visual frames (especially Q+S+V) on long videos will likely cause GPU Out-Of-Memory (OOM) errors or have prohibitively long inference times.

- First 3 experiments:
  1. **Baseline Efficiency Test:** Using the Qwen2.5-VL-7B model, run inference on a subset of VideoMME with three input configurations: Question-only, Q+Subtitles, and Q+Visual. Measure and compare GPU VRAM usage and inference latency per sample to validate the primary cost driver.
  2. **Modality Ablation on Long Videos:** Using the LongVideoBench validation set (specifically the 600s and 3600s categories), compare the accuracy of Q+S, Q+V, and Q+S+V. The goal is to confirm the paper's finding that subtitles become more competitive or superior for long-duration videos.
  3. **Audio Representation Substitution:** For a model that supports audio (like Video-SALMONN 2), compare the accuracy of Q+A+V against Q+S+V on a subset of VideoMME. This test will validate the finding that textual subtitles can be a more effective representation of the speech signal than raw audio embeddings for QA tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can adaptive modality selection strategies—dynamically choosing between subtitles, frames, or combinations based on video content characteristics or query type—achieve near-optimal accuracy while minimizing computational costs?
- Basis in paper: [inferred] The paper identifies clear trade-offs between effectiveness and efficiency but evaluates only fixed modality combinations, suggesting room for context-aware selection.
- Why unresolved: The study provides static comparisons but does not explore decision policies that switch modalities per-video or per-question.
- What evidence would resolve it: Experiments with a modality-selection policy (e.g., routing based on video duration, speech density, or question type) showing competitive accuracy with reduced average GPU memory and latency.

### Open Question 2
- Question: How do different visual frame sampling strategies (uniform vs. content-aware vs. keyframe-based) alter the accuracy-efficiency frontier for MLLM-based VideoQA?
- Basis in paper: [inferred] The methodology section notes that most MLLMs uniformly sample a limited number of frames due to GPU memory constraints, but alternative sampling approaches are not evaluated.
- Why unresolved: Uniform sampling is assumed; the impact of smarter sampling on the identified trade-offs remains unknown.
- What evidence would resolve it: Systematic comparison of sampling methods across VideoMME and LongVideoBench, reporting accuracy, GPU memory, and latency.

### Open Question 3
- Question: For speech-heavy videos, does raw audio input provide complementary or redundant information compared to ASR-generated subtitles, and under what conditions does audio outperform text?
- Basis in paper: [inferred] Audio experiments are limited (Video-SALMONN results missing on LongVideoBench), and subtitles are found highly effective, leaving the relative value of raw audio underexplored.
- Why unresolved: The paper does not isolate audio-only conditions or directly compare audio vs. subtitles on matched content.
- What evidence would resolve it: Controlled comparisons of audio-only, subtitle-only, and combined conditions on speech-centric video subsets, with matched questions and efficiency metrics.

### Open Question 4
- Question: Do the modality trade-offs identified on VideoMME and LongVideoBench generalize to open-ended VideoQA tasks and diverse real-world video domains?
- Basis in paper: [inferred] The study uses only multiple-choice benchmarks and specific video sources (YouTube, web videos), leaving generalization uncertain.
- Why unresolved: Different task formats (e.g., generative answers) and domains may exhibit different modality effectiveness patterns.
- What evidence would resolve it: Evaluation across additional benchmarks with open-ended generation and diverse domains (e.g., educational, surveillance, medical), repeating the modality ablation analysis.

## Limitations
- The exact number of frames sampled per video is not specified, which impacts accuracy and efficiency results.
- Specific prompt formats and delimiters for multimodal inputs are not documented, potentially influencing model performance.
- Hardware configurations and optimization settings for Video-SALMONN 2 results are not provided, hindering comparison of efficiency metrics.

## Confidence
- **High Confidence:** Visual frames enhance accuracy but impose heavy computational costs; subtitles are a lightweight yet effective alternative, especially for long videos.
- **Medium Confidence:** Q+S+V generally outperforms Q+A+V; relative performance of different model scales (7B vs 72B).
- **Low Confidence:** Specific numerical efficiency values (VRAM and latency) for Video-SALMONN 2 due to lack of hardware details; effectiveness of subtitles for videos with minimal or no spoken content.

## Next Checks
1. **Modality Ablation on Long Videos:** Using the LongVideoBench validation set (specifically the 600s and 3600s duration categories), conduct a focused experiment to compare the accuracy of the Question-only, Q+Subtitles, Q+Visual, and Q+S+V input configurations. This will validate the study's finding that subtitles become more competitive or superior for long-duration videos and provide insight into the trade-off between accuracy and the number of visual frames that can be processed.

2. **Reproduce Efficiency Scaling:** Using a Qwen2.5-VL-7B model, run a series of experiments on a subset of VideoMME with the Question-only, Q+Subtitles, and Q+Visual configurations. Measure and record the peak GPU VRAM usage and end-to-end inference latency for each configuration. The goal is to reproduce the reported efficiency scaling, specifically the finding that visual frames nearly double the memory demand for 7B-scale models.

3. **Audio vs. Subtitle Representation:** For a model that natively supports audio input (such as Video-SALMONN 2), design an experiment to compare the accuracy of Q+A+V against Q+S+V on a subset of the VideoMME benchmark. This will directly test the study's finding that textual subtitles can be a more effective representation of the speech signal than raw audio embeddings for VideoQA tasks.