---
ver: rpa2
title: 'Sub-MoE: Efficient Mixture-of-Expert LLMs Compression via Subspace Expert
  Merging'
arxiv_id: '2506.23266'
source_url: https://arxiv.org/abs/2506.23266
tags:
- expert
- experts
- sub-moe
- merging
- compression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Sub-MoE, a novel compression framework for
  Mixture-of-Experts (MoE) large language models that addresses the fundamental challenge
  of parameter conflicts during expert merging. The core innovation lies in performing
  joint Singular Value Decomposition (SVD) on concatenated expert weights to extract
  shared U-matrices while enabling effective merging of expert-specific V components,
  thereby aligning and fusing experts in a shared subspace.
---

# Sub-MoE: Efficient Mixture-of-Expert LLMs Compression via Subspace Expert Merging

## Quick Facts
- **arXiv ID:** 2506.23266
- **Source URL:** https://arxiv.org/abs/2506.23266
- **Reference count:** 40
- **Primary result:** Maintains 96% of original performance with 25% expert reduction and 86% with 50% reduction on Mixtral-8x7B.

## Executive Summary
Sub-MoE introduces a novel compression framework for Mixture-of-Experts (MoE) large language models that addresses the fundamental challenge of parameter conflicts during expert merging. The core innovation lies in performing joint Singular Value Decomposition (SVD) on concatenated expert weights to extract shared U-matrices while enabling effective merging of expert-specific V components, thereby aligning and fusing experts in a shared subspace. Extensive experiments on Mixtral, DeepSeek, and Qwen MoE models demonstrate that Sub-MoE significantly outperforms existing expert pruning and merging methods across multiple model architectures and compression ratios.

## Method Summary
Sub-MoE operates through a two-stage process: first, Adaptive Expert Clustering groups functionally coherent experts using K-means clustering based on output similarity, then Subspace Expert Merging performs union SVD decomposition followed by frequency-based V-matrix merging and expert reconstruction. The method concatenates weight matrices of clustered experts, applies joint SVD to extract a shared orthogonal basis (U-matrix) while preserving individual expert characteristics in V-matrices, and reconstructs merged experts using frequency-weighted averaging of V components. This approach minimizes parameter conflicts by aligning diverse experts to a shared orthogonal basis before merging, preserving functional behavior better than weight-based grouping methods.

## Key Results
- Achieves 96% of original performance with 25% expert reduction (8→6 experts) on Mixtral-8x7B
- Maintains 86% performance with 50% expert reduction (8→4 experts) on zero-shot benchmarks
- Outperforms state-of-the-art methods including HC-SMoE and simple frequency pruning by substantial margins
- Demonstrates consistent performance across Mixtral, DeepSeek, and Qwen MoE model architectures

## Why This Works (Mechanism)

### Mechanism 1
Aligning diverse experts to a shared orthogonal basis minimizes parameter conflicts during merging. The method performs joint Singular Value Decomposition (SVD) on vertically concatenated weight matrices of experts within a cluster, forcing them to share a common left singular vector matrix (U) while retaining their individuality in the right singular vector matrices (V). By reconstructing the merged expert as $W_{merged} = U\Sigma V_{merged}^T$, the model projects distinct experts into a unified subspace before fusing them.

### Mechanism 2
Weighting the merge by activation frequency prioritizes the model's dominant capabilities over rarely used features. Instead of uniform averaging, the framework calculates frequency scores for each expert based on how often the router selects it in calibration data. The merged matrix is a frequency-weighted sum, ensuring that experts handling common patterns influence the final weights more than those handling edge cases.

### Mechanism 3
Clustering by output similarity rather than weight similarity preserves functional behavior better than structural grouping. The framework groups experts using K-means on cosine similarity of their output vectors rather than their raw parameters. This ensures that experts are merged only if they perform similar transformations on the data, regardless of their internal weight configurations.

## Foundational Learning

- **Singular Value Decomposition (SVD)**: Essential for understanding how the method decomposes weight matrices into shared U-matrices and expert-specific V-matrices. *Quick check:* If two experts have different weight matrices but identical U matrices, what does that imply about their functional relationship?

- **Mixture-of-Experts (MoE) Routing**: Critical for understanding why frequency weighting works. The sparsity of MoE (top-k routing) means not all experts are active for a given token. *Quick check:* How does the "Top-k" selection mechanism in MoE justify the assumption that we can drop or merge low-frequency experts?

- **Parameter Conflict / Representation Alignment**: This is the specific problem the paper solves. Standard averaging fails because expert weights occupy different regions in parameter space. *Quick check:* Why does simply averaging the weights of two specialized experts (e.g., one for math, one for coding) typically result in a model that is bad at both?

## Architecture Onboarding

- **Component map:** Calibration Stage -> Clustering Module -> Subspace Decomposition -> Merging & Reconstruction -> (Optional) Intra-Expert Compression
- **Critical path:** The quality of the Shared U-matrix. If the joint SVD does not find a robust shared basis, the subsequent reconstruction of any merged expert will be flawed regardless of clustering quality.
- **Design tradeoffs:** Cluster Granularity (merging more experts increases compression but risks collapsing the subspace) vs. Calibration Size (small calibration sets speed up compression but may miss rare experts).
- **Failure signatures:** Perplexity Spike (massive jump in WikiText-2 perplexity indicates shared U-matrix rank is too low) or Catastrophic Forgetting (merged model loses specific capabilities if low-frequency experts were under-weighted).
- **First 3 experiments:** 1) Baseline Validation: Reduce 8 experts to 4 on Mixtral-8x7B, compare against HC-SMoE. 2) Ablation on Similarity: Switch clustering metric from "Expert Output" to "Weight Similarity" to quantify performance drop. 3) Intra-Expert Stress Test: Apply Sub-MoE† with 20% SVD truncation on merged experts to verify shared subspace holds under additional low-rank approximation.

## Open Questions the Paper Calls Out
- Can Sub-MoE be adapted to function effectively without calibration data?
- Does the subspace alignment approach apply effectively to non-LLM MoE architectures or dense model merging?
- Does frequency-based merging inadvertently suppress long-tail knowledge by prioritizing frequently activated experts?

## Limitations
- **Calibration Data Dependency**: Performance critically depends on calibration dataset distribution matching deployment tasks
- **Scalability Uncertainty**: Behavior on state-of-the-art MoE models (>256 experts) remains untested
- **Single-Stage Compression**: Method performs one-pass compression without iterative refinement or fine-tuning

## Confidence
- **High Confidence (4/5)**: Core mathematical framework using joint SVD is sound and well-supported by empirical results across multiple architectures
- **Medium Confidence (3/5)**: Robustness to calibration data distribution shifts and scalability to larger models (>64 experts) are only partially addressed
- **Low Confidence (2/5)**: Handling of bias terms during SVD concatenation/reconstruction is not specified

## Next Checks
1. **Calibration Data Ablation**: Test compressed model performance using calibration datasets from different domains to quantify sensitivity to distribution mismatch
2. **Iterative Compression Test**: Apply Sub-MoE in multiple rounds (8→6→4 experts) to measure if performance exceeds single-stage results
3. **Memory Complexity Analysis**: Profile computational and memory requirements of joint SVD for models with varying expert counts (16, 32, 64 experts) to determine practical scaling limits