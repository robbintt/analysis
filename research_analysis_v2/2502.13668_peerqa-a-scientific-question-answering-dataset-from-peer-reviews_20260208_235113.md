---
ver: rpa2
title: 'PeerQA: A Scientific Question Answering Dataset from Peer Reviews'
arxiv_id: '2502.13668'
source_url: https://arxiv.org/abs/2502.13668
tags:
- answer
- question
- questions
- evidence
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'PeerQA is a scientific question answering dataset sourced from
  peer reviews, with answers annotated by original paper authors. It supports three
  key tasks: evidence retrieval, unanswerable question classification, and answer
  generation.'
---

# PeerQA: A Scientific Question Answering Dataset from Peer Reviews

## Quick Facts
- arXiv ID: 2502.13668
- Source URL: https://arxiv.org/abs/2502.13668
- Reference count: 40
- PeerQA is a scientific QA dataset from peer reviews with 579 QA pairs from 208 papers, supporting evidence retrieval, unanswerable question classification, and answer generation tasks.

## Executive Summary
PeerQA is a scientific question answering dataset derived from peer review comments and annotated answers by original paper authors. The dataset contains 579 QA pairs from 208 scientific papers across ML/NLP and other domains, with an average length of 12k tokens per paper. The authors establish baseline performance for three key tasks: evidence retrieval, unanswerable question classification, and answer generation. Experiments demonstrate that decontextualization significantly improves retrieval performance, RAG approaches outperform full-text context for generation, and models exhibit systematic biases in answerability prediction. The dataset serves as a challenging benchmark for developing practical QA systems for scientific literature.

## Method Summary
The PeerQA dataset was constructed by extracting peer review comments from papers with available peer reviews and having the original authors provide answers. Scientific papers were processed using GROBID to extract text organized into paragraphs. Three tasks were defined: evidence retrieval (finding relevant paragraphs/sentences), unanswerable question classification (binary classification), and free-form answer generation. The authors established baselines using SPLADEv3 for retrieval, various LLMs for generation and classification, and multiple evaluation metrics including MRR, Recall@k, Rouge-L, AlignScore, and Prometheus-2.

## Key Results
- Decontextualization (title prepending) consistently improves retrieval performance across architectures
- RAG with top-k retrieved passages outperforms full-text context for answer generation, with GPT-4o showing stable performance across increasing context sizes
- Models exhibit systematic bias toward predicting questions as either answerable or unanswerable, independent of context size
- SPLADEv3 achieves best retrieval performance with MRR of 0.48 for paragraphs and 0.34 for sentences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decontextualization of passages improves evidence retrieval performance across architectures
- Mechanism: Scientific document passages contain pronoun references, domain-specific abbreviations, and contextual dependencies that require external information to resolve. Prepending document titles provides grounding context that enables retrieval models to match queries to passages more accurately, particularly when questions reference concepts that appear indirectly within paragraphs.
- Core assumption: The semantic gap between question phrasing and passage content stems partly from missing contextual signals rather than vocabulary mismatch alone.
- Evidence anchors:
  - [abstract] "even simple decontextualization approaches consistently improve retrieval performance across architectures"
  - [section 5.1] "Appending the title to the paragraph further improves results (except ColBERTv2's MRR)... showing that decontextualizing the paragraphs from the paper helps"
  - [corpus] Related work on document-aware passage retrieval (Wang et al., 2024) shows similar benefits for title prepending
- Break condition: When passages are already self-contained or when titles provide no discriminative information (e.g., generic titles like "Introduction"), decontextualization adds noise without benefit.

### Mechanism 2
- Claim: Retrieval-augmented generation with top-k passages outperforms full-text context for answer generation, except for models with strong long-context capabilities
- Mechanism: Long documents contain substantial irrelevant content that creates attention dilution—LLMs must allocate computational capacity across many irrelevant tokens, reducing precision on evidence synthesis. Retrieval acts as a pre-filter, concentrating model capacity on relevant passages. However, advanced models like GPT-4o appear to have improved attention mechanisms that maintain performance even with noisy context.
- Core assumption: The performance degradation with full-text is primarily due to attention/processing limitations rather than missing information.
- Evidence anchors:
  - [abstract] "using retrieved paragraphs outperforms full-text context, with GPT-4o showing stable performance across increasing context sizes"
  - [section 5.3] "LLMs perform better in RAG, with fewer but relevant contexts, than in the full-text setting... it is more effective to employ a retriever filtering relevant information than leaving this step to the internal workings of the LLM"
  - [corpus] ScIRGen and related RAG work supports retrieval-first approaches for scientific domains
- Break condition: When retrieval fails to surface critical evidence (low recall), full-text may outperform RAG if the model can locate the missed information. Section F shows SPLADEv3 recall@100 reaches 1.0, but lower k values have substantial gaps.

### Mechanism 3
- Claim: Models exhibit systematic class bias in answerability prediction, independent of context size
- Mechanism: Instruction-tuned models develop default response tendencies based on their training data distribution and alignment. Some models (Mistral, Command-R) default to attempting answers even with insufficient context, while others (Llama, GPT-3.5) default to refusal. This bias persists across context configurations, suggesting it's encoded in model priors rather than responsive to evidence.
- Core assumption: The bias originates from instruction-tuning rather than architectural limitations.
- Evidence anchors:
  - [abstract] "models show bias toward predicting questions as either answerable or unanswerable"
  - [section 5.2] "Mistral and Command-R tend to predict an answer more often, while Llama and GPT tend to predict the question is unanswerable, showing that all models have a bias towards one of the classes"
  - [corpus] Limited corpus evidence on this specific phenomenon; concurrent work on scientific QA does not address answerability bias
- Break condition: Calibration techniques or few-shot examples demonstrating balanced answerable/unanswerable cases may reduce but likely won't eliminate this bias.

## Foundational Learning

- Concept: **Retrieval evaluation metrics (MRR, Recall@k)**
  - Why needed here: Evidence retrieval is the first pipeline stage; understanding these metrics is essential to diagnose generation failures. The paper uses MRR for ranking quality and Recall@10 for coverage assessment.
  - Quick check question: If a retriever has MRR=0.45 and Recall@10=0.67, what proportion of questions have their first relevant passage in position 1 versus having at least one relevant passage in the top 10?

- Concept: **RAG architecture for single-document QA**
  - Why needed here: Unlike corpus-level RAG, PeerQA retrieves from within a single long document. This requires passage segmentation strategies (paragraph vs. sentence level) and understanding how retrieval granularity affects downstream generation.
  - Quick check question: Given a 12k-token paper, what trade-offs exist between paragraph-level retrieval (~120 passages) versus sentence-level retrieval (~400+ passages) for MRR and computational cost?

- Concept: **Long-form QA evaluation challenges**
  - Why needed here: Free-form scientific answers cannot be evaluated with simple lexical overlap. The paper uses three metrics (Rouge-L, AlignScore, Prometheus-2) against two ground truths (free-form answers, evidence paragraphs), each capturing different quality aspects.
  - Quick check question: Why does the paper evaluate generated answers against both the free-form reference AND the annotated evidence paragraphs? What different failure modes does each comparison detect?

## Architecture Onboarding

- Component map: GROBID PDF extraction -> Paragraph segmentation -> Title prepending (decontextualization) -> SPLADEv3 retrieval -> LLM generation/classification -> Multi-metric evaluation
- Critical path: Retrieval quality determines generation ceiling. Section S.1 shows positive correlation (r=0.27-0.42) between retriever recall and generation metrics. Invest in retrieval optimization before generation fine-tuning.
- Design tradeoffs:
  - **Paragraph vs. sentence retrieval**: Paragraphs yield higher MRR (0.48 vs. 0.34 for SPLADEv3) but sentences provide finer granularity for evidence localization
  - **Context window vs. retrieval count**: Larger k improves recall but increases cost and potentially dilutes attention; optimal k appears to be 20-50 based on Figure 4 performance plateaus
  - **Model selection**: GPT-4o provides best generation quality and stable long-context performance but at 6x+ cost of RAG-20 setup; open-source alternatives (Llama-3-8B-32k) offer cost savings with moderate quality degradation
- Failure signatures:
  - **Low MRR with high Recall**: Model retrieves relevant content but ranks poorly—investigate query-passage similarity calibration
  - **High answerability precision, low recall on unanswerable class**: Model bias toward one class—requires calibration or balanced few-shot prompting
  - **High Rouge-L but low AlignScore**: Model generates surface-level matches without factual consistency—investigate hallucination
  - **Generation quality drops with increasing context k**: Attention dilution—consider re-ranking retrieved passages before generation
- First 3 experiments:
  1. **Retrieval ablation**: Implement SPLADEv3 with title-prepended paragraphs, measure MRR and Recall@10 on PeerQA dev split. Compare against BM25 baseline to quantify neural vs. lexical retrieval gap.
  2. **Context size sweep**: Using top-performing retriever, test generation with k ∈ {10, 20, 50, 100} passages on Llama-3-8B. Plot generation metrics (Rouge-L, AlignScore) against k to identify performance/cost sweet spot.
  3. **Answerability calibration**: Evaluate model bias by computing precision/recall for both classes. Test if few-shot prompting with balanced answerable/unanswerable examples reduces systematic bias.

## Open Questions the Paper Calls Out

- How can evaluation metrics be adapted to account for author-provided answers that rely on knowledge external to the paper's text?
- To what extent does the heavy dominance of ML/NLP papers in PeerQA impact the generalizability of models to other scientific domains?
- How can the systematic bias in LLMs towards predicting questions as answerable or unanswerable be mitigated?

## Limitations
- Dataset construction relies on author-annotated answers, potentially introducing bias toward paper-specific terminology
- Relatively small size (579 QA pairs) limits statistical power for certain analyses
- Evaluation depends heavily on LLM-based metrics whose reliability for scientific content assessment remains uncertain

## Confidence
- **High Confidence**: Decontextualization improves retrieval performance; RAG outperforms full-text context; systematic answerability bias exists
- **Medium Confidence**: GPT-4o's superior long-context performance; optimal retrieval count (k=20-50); correlation between retrieval quality and generation performance
- **Low Confidence**: Domain generalization beyond ML/NLP; reliability of LLM-based evaluation metrics; effectiveness on papers with non-standard structure

## Next Checks
1. **Cross-domain validation**: Test the complete pipeline on PeerQA's Geoscience subset to assess domain transfer performance and identify domain-specific failure modes.
2. **Human evaluation study**: Conduct systematic human evaluation of generated answers using Prometheus-2 rubric to validate LLM-based metric reliability and quantify evaluation error rates.
3. **Retrieval architecture ablation**: Systematically compare SPLADEv3 with modern hybrid retrieval approaches on PeerQA to identify whether reported performance gaps persist with current state-of-the-art methods.