---
ver: rpa2
title: 'EVaDE : Event-Based Variational Thompson Sampling for Model-Based Reinforcement
  Learning'
arxiv_id: '2501.09611'
source_url: https://arxiv.org/abs/2501.09611
tags:
- layer
- simple
- evade
- layers
- noisy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Event-based Variational Distributions for
  Exploration (EVaDE), a method for improving exploration in model-based reinforcement
  learning, particularly for object-based domains. The core idea is to use variational
  distributions induced by three types of noisy convolutional layers (event interaction,
  weighting, and translation) to guide exploration towards high-reward states.
---

# EVaDE : Event-Based Variational Thompson Sampling for Model-Based Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2501.09611
- **Source URL:** https://arxiv.org/abs/2501.09611
- **Reference count:** 40
- **Key result:** EVaDE achieves 0.682 mean human-normalized score on Atari 100K vs 0.525 for baseline SimPLe

## Executive Summary
EVaDE introduces a method for improving exploration in model-based reinforcement learning by leveraging variational distributions induced through structured noise injection into convolutional neural networks. The core innovation is the use of three types of "event-based" noisy convolutional layers that perturb object interactions, event weighting, and spatial translations in the reward prediction network. These layers enable Thompson sampling-style exploration by sampling plausible alternate reward functions, encouraging the agent to discover high-reward states that deterministic models might miss. The method demonstrates significant performance gains on the challenging Atari 100K benchmark, achieving 0.682 mean human-normalized score compared to 0.525 for the baseline.

## Method Summary
EVaDE builds on the SimPLe model-based RL framework by inserting three types of event-based noisy convolutional layers into the reward network branch. These layers (Interaction, Weighting, and Translation) apply Gaussian multiplicative dropout to induce variational distributions over the reward function, enabling Thompson sampling-style exploration. During training, the agent samples a perturbed reward model at the start of each imaginary training phase and optimizes its policy against this sampled model. The method maintains the original SimPLe transition network unchanged to avoid destabilizing dynamics learning. EVaDE is specifically designed for object-based domains where spatial features and object interactions can be meaningfully perturbed.

## Key Results
- EVaDE achieves 0.682 mean human-normalized score on Atari 100K benchmark
- Significant improvement over baseline SimPLe (0.525) and other model-based methods
- Ablation studies show that combining all three EVaDE layers yields the best performance (IQM of 0.739)
- The method is particularly effective on games with complex visual features and object interactions

## Why This Works (Mechanism)

### Mechanism 1: Variational Thompson Sampling via Gaussian Dropout
Gaussian multiplicative dropout applied to reward network weights induces variational distributions that approximate posterior sampling. By sampling perturbed reward models, the agent explores trajectories that maximize returns under specific samples rather than expected mean rewards, encouraging discovery of high-reward states missed by deterministic models.

### Mechanism 2: Inductive Bias for Object-Based Exploration
EVaDE layers inject noise into geometric structures of convolutional filters to target semantically meaningful variations. The Interaction Layer perturbs feature co-occurrence (object collisions), Weighting Layer perturbs channel importance (event types), and Translation Layer perturbs spatial positions, encouraging exploration of object-centric variations.

### Mechanism 3: Imaginary Rollouts with Sampled Rewards
The agent trains PPO policies inside simulated environments using sampled reward models rather than deterministic predictions. This causes the policy to exploit "potential" high-reward areas identified by noise samples, with real environment interaction correcting misleading samples and reinforcing accurate ones.

## Foundational Learning

- **Posterior Sampling for Reinforcement Learning (PSRL)**
  - *Why needed:* EVaDE is explicitly an approximation of PSRL; understanding PSRL explains why model parameters are sampled rather than using uncertainty for intrinsic motivation
  - *Quick check:* How does Thompson sampling differ from optimism-in-the-face-of-uncertainty (OFU) in terms of computational complexity during planning?

- **Variational Inference (VI) & Gaussian Dropout**
  - *Why needed:* The paper relies on Gal & Ghahramani's interpretation of dropout as variational inference; without this, noisy layers appear as ad-hoc noise injection
  - *Quick check:* In variational inference, what does the KL-divergence term in the ELBO optimize relative to the true posterior?

- **Convolutional Filter Equivariance**
  - *Why needed:* The Event Interaction Layer relies on convolutional filters detecting the same patterns regardless of location
  - *Quick check:* If you apply a translation to an image and then apply a convolution, how does the result compare to applying the convolution first and then translating the output feature map?

## Architecture Onboarding

- **Component map:** Environment Model (SimPLe) -> EVaDE Layers (Reward Network) -> Policy Network (PPO)
- **Critical path:** Real Interaction -> Train Model (with VI) -> Sample Model -> Train Policy in Imagination -> Repeat
- **Design tradeoffs:** Perturbing only the reward function (not transition) to avoid destabilizing dynamics learning; using all three layers yields best IQM but increases parameter count
- **Failure signatures:** Reward hacking (agent finds simulation glitch), variance collapse (σ parameters go to zero)
- **First 3 experiments:** 1) Identity initialization sanity check, 2) Single layer ablation on 12-game subset, 3) Variance parameter monitoring over training iterations

## Open Questions the Paper Calls Out
- Can EVaDE layers be effectively integrated with state-of-the-art tree-search methods like EfficientZero to further improve sample efficiency?
- Does the EVaDE architecture generalize to non-spatial or continuous control domains that lack discrete object interactions?
- Can exploration efficiency be improved by adaptively selecting specific EVaDE layers rather than using all three simultaneously?

## Limitations
- Implementation details of base SimPLe model and optimization hyperparameters are not fully specified
- Theoretical framework guarantees identity function representability but doesn't prove convergence to optimal exploration strategies
- Individual layer contributions and potential redundancy are not fully characterized through extensive ablation

## Confidence
- **High:** Core mechanism of using Gaussian dropout to induce variational distributions for exploration is well-established
- **Medium:** Empirical improvements on Atari 100K benchmark are significant and reproducible
- **Medium:** Architectural design of three event-based layers is novel and theoretically sound

## Next Checks
1. Monitor evolution of variance parameters (σ) during training to ensure they don't collapse to zero
2. Conduct more extensive ablation study testing each EVaDE layer type individually and in pairs
3. Analyze qualitative behavior of learned policies comparing event types experienced by EVaDE vs vanilla SimPLe agents