---
ver: rpa2
title: Summaries as Centroids for Interpretable and Scalable Text Clustering
arxiv_id: '2502.09667'
source_url: https://arxiv.org/abs/2502.09667
tags:
- k-means
- clustering
- k-llmmeans
- cluster
- k-nlpmeans
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces k-NLPmeans and k-LLMmeans, text clustering
  methods that periodically replace numeric centroids with textual summaries. The
  approach maintains standard k-means assignments in embedding space while producing
  interpretable, human-readable cluster prototypes.
---

# Summaries as Centroids for Interpretable and Scalable Text Clustering

## Quick Facts
- arXiv ID: 2502.09667
- Source URL: https://arxiv.org/abs/2502.09667
- Reference count: 40
- Primary result: k-NLPmeans and k-LLMmeans improve text clustering accuracy and interpretability while maintaining k-means computational efficiency.

## Executive Summary
This paper introduces k-NLPmeans and k-LLMmeans, text clustering methods that periodically replace numeric centroids with textual summaries while maintaining standard k-means assignments in embedding space. Two variants are proposed: k-NLPmeans uses classical extractive summarizers for LLM-free operation, while k-LLMmeans employs LLMs under a fixed budget that does not scale with dataset size. Experiments across multiple datasets and embedding models show consistent improvements over traditional k-means and topic modeling baselines, with k-LLMmeans achieving accuracy and NMI scores competitive with advanced LLM-based clustering methods at much lower computational cost. The methods also extend to mini-batch k-means for streaming text clustering. A StackExchange-derived benchmark is released for evaluating streaming text clustering.

## Method Summary
The approach maintains standard k-means assignments in embedding space while periodically replacing numeric centroids with textual summaries. k-NLPmeans uses extractive summarizers (TextRank, Centroid, or LSA) to generate q=5 sentence summaries from clustered documents, which are then re-embedded to serve as new centroids. k-LLMmeans samples m=10-50 documents per cluster using k-means++ sampling and summarizes them with an LLM under a fixed budget. Both methods run for T=120 iterations with summarization steps at l=20 (multiple) or l=60 (single). The approach extends to mini-batch k-means for streaming scenarios and includes a newly released StackExchange benchmark with 205,943 posts from 35 sites (2020-2023).

## Key Results
- k-NLPmeans and k-LLMmeans consistently outperform vanilla k-means and BERTopic baselines across BANK77, CLINC, GoEmo, and MASSIVE datasets
- k-LLMmeans achieves accuracy and NMI scores competitive with advanced LLM-based clustering methods while using far fewer LLM calls
- k-LLMmeans maintains computational efficiency by fixing LLM calls per dataset rather than scaling with dataset size
- The methods successfully extend to mini-batch k-means for streaming text clustering scenarios

## Why This Works (Mechanism)
The method works by periodically replacing numeric centroids with textual summaries that better capture the semantic essence of clusters. By re-embedding these summaries back into the same vector space, the approach maintains k-means' computational efficiency while producing interpretable, human-readable cluster prototypes. The fixed LLM budget in k-LLMmeans ensures scalability regardless of dataset size, while extractive summarizers in k-NLPmeans provide a no-LLM alternative that still improves over numeric centroids.

## Foundational Learning
- **Text embeddings**: Dense vector representations capturing semantic meaning; needed to represent documents in continuous space for clustering. Quick check: Visualize document embeddings with t-SNE to verify semantic clustering.
- **k-means clustering**: Partitioning algorithm assigning points to nearest centroid; needed as the underlying assignment mechanism. Quick check: Verify convergence by monitoring centroid movement across iterations.
- **Text summarization**: Process of condensing text while preserving key information; needed to generate interpretable cluster prototypes. Quick check: Evaluate summary coherence with human judges or ROUGE scores.
- **SVD for extractive summarization**: Matrix factorization identifying important sentences; needed for LSA-based summarization in k-NLPmeans. Quick check: Compare top singular vectors' contribution scores across clusters.
- **k-means++ initialization**: Smart seeding algorithm for better centroid initialization; needed to improve clustering stability. Quick check: Compare with random initialization on same datasets.
- **Hungarian algorithm**: Bipartite matching for optimal label assignment; needed to compute clustering accuracy against ground truth. Quick check: Verify perfect matching when clusters align with labels.

## Architecture Onboarding

**Component map**: Document embeddings -> k-means assignment -> summarization (extractive/LLM) -> re-embedding -> new centroid -> next iteration

**Critical path**: Embedding computation → k-means assignment → summarization → re-embedding → centroid update

**Design tradeoffs**: Fixed summarization frequency (l=20/60) vs. adaptive timing based on cluster stability; extractive summarizers (no LLM cost) vs. LLM summarizers (better quality); sample size (m=10-50) vs. summary representativeness.

**Failure signatures**: Over-generic summaries that echo the prompt rather than cluster content; extractive summarizers producing incoherent summaries on noisy datasets; computational overhead from frequent summarization steps.

**Three first experiments**:
1. Implement k-NLPmeans (LSA variant) on BANK77 with T=120 iterations, l=20 summarization frequency, and compare ACC/NMI against vanilla k-means
2. Test k-LLMmeans with m=10 sampled documents per cluster on CLINC dataset, varying summarization frequency to find optimal l
3. Evaluate both methods on StackExchange streaming benchmark with mini-batch k-means extension

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on fixed summarization intervals (l=20 or l=60) introduces an unprincipled hyperparameter
- Quality of summaries from only 10-50 sampled documents per cluster may be insufficient for highly diverse datasets
- Preprocessing pipeline for noisy community text in StackExchange benchmark is underspecified
- Method's transferability to non-English languages or domains with very different text characteristics is untested

## Confidence

**High confidence**: The core algorithmic approach is technically sound and well-implemented; computational efficiency claims for k-LLMmeans are well-supported.

**Medium confidence**: Superiority claims over BERTopic and other baselines may be dataset-dependent; mini-batch extension is theoretically valid but minimally validated.

**Low confidence**: Claims of competitiveness with advanced LLM-based clustering methods lack direct comparison to most recent hierarchical approaches.

## Next Checks

1. **Ablation study on summarization frequency**: Systematically vary l from 10 to 100 iterations to quantify trade-off between cluster quality and computational overhead.

2. **Stress test on noisy data**: Evaluate both methods on intentionally degraded versions of StackExchange to identify failure modes and determine when numeric centroids outperform textual summaries.

3. **Direct comparison to hierarchical LLM clustering**: Implement a simplified version of HERCULES or similar hierarchical LLM clustering and compare end-to-end performance and computational cost against k-LLMmeans.