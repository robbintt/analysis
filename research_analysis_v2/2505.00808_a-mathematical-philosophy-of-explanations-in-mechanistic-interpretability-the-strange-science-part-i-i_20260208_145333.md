---
ver: rpa2
title: A Mathematical Philosophy of Explanations in Mechanistic Interpretability --
  The Strange Science Part I.i
arxiv_id: '2505.00808'
source_url: https://arxiv.org/abs/2505.00808
tags:
- interpretability
- explanations
- neural
- explanation
- explanatory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a philosophical framework for mechanistic interpretability
  by arguing that neural networks contain implicit explanations of their behavior
  that can be extracted and understood. The authors define mechanistic interpretability
  as producing Model-level, Ontic, Causal-Mechanistic, and Falsifiable explanations
  of neural networks, and introduce the "Explanatory View Hypothesis" which suggests
  neural networks naturally admit explanations through their internal structures.
---

# A Mathematical Philosophy of Explanations in Mechanistic Interpretability -- The Strange Science Part I.i

## Quick Facts
- **arXiv ID**: 2505.00808
- **Source URL**: https://arxiv.org/abs/2505.00808
- **Reference count**: 40
- **Primary result**: Provides philosophical framework arguing neural networks contain implicit explanations extractable through internal structures, defines MI as producing Model-level, Ontic, Causal-Mechanistic, and Falsifiable explanations

## Executive Summary
This paper presents a philosophical framework for mechanistic interpretability (MI) that defines what constitutes a valid explanation of neural network behavior. The authors argue that neural networks contain implicit explanations ("ur-explanations") of their behavior through learned internal structures that can potentially be extracted and understood. They formalize MI as producing explanations that are model-level (not system-level), ontic (about real mechanisms), causal-mechanistic (tracing continuous causal chains), and falsifiable (testable through intervention). The paper introduces the "Principle of Explanatory Optimism" - the conjecture that neural network algorithms are human-understandable - as foundational to the MI project.

## Method Summary
This is a theoretical philosophy paper that introduces conceptual definitions and arguments rather than empirical methods. The authors propose the Explanatory View Hypothesis that neural networks contain implicit explanations, define explanatory faithfulness through layer-wise activation matching, and establish criteria for valid mechanistic explanations. No training procedures, data inputs, or quantitative metrics are specified. The framework references empirical MI work as motivation but conducts no experiments itself.

## Key Results
- Defines mechanistic interpretability as producing Model-level, Ontic, Causal-Mechanistic, and Falsifiable explanations
- Introduces "Explanatory View Hypothesis" suggesting neural networks naturally admit explanations through internal structures
- Formulates "Principle of Explanatory Optimism" - the conjecture that learned algorithms are human-understandable
- Discusses limitations including value-ladenness and theory-ladenness of explanations

## Why This Works (Mechanism)

### Mechanism 1: The Explanatory View Hypothesis
Neural networks may contain implicit explanations ("ur-explanations") of their behavior through learned internal structures that can potentially be extracted. Training produces internal computational structure (representations and operations) that encodes how the model generalizes; these structures constitute an implicit explanation waiting to be articulated.

### Mechanism 2: Explanatory Faithfulness as Layer-wise Activation Matching
An explanation's faithfulness can be assessed by comparing intermediate activations predicted by the explanation against actual model activations. Given explanation E and model M, explanatory faithfulness requires that intermediate activations at each layer given by E closely match the intermediate activations of M.

### Mechanism 3: The Principle of Explanatory Optimism (Foundational Conjecture)
The project of mechanistic interpretability rests on the conjecture that neural network algorithms are human-understandable. The paper distinguishes between Strong EO (everything important is understandable) and Weak EO (most important behavior is understandable).

## Foundational Learning

- **Marr's Three Levels of Analysis**: The paper locates mechanistic interpretability at the Algorithmic/Representational level, distinct from Implementation (matrix operations) and Computational (goal-level). Understanding this hierarchy is essential to grasp what MI targets.
  - Quick check: Can you explain why knowing all matrix multiplications (implementation level) doesn't automatically provide algorithmic understanding?

- **Causal-Mechanistic vs. Statistical Explanation**: The paper defines MI specifically as producing causal-mechanistic explanations that trace "continuous causal chains" rather than statistical correlations. This demarcation is central to distinguishing MI from other interpretability paradigms.
  - Quick check: Given that ice cream sales correlate with shark attacks, why would a statistical explanation of this correlation fail to satisfy the causal-mechanistic criterion?

- **Representation Criteria (Information, Use, Misrepresentation)**: The paper adopts Harding's three criteria for when activations qualify as representations. These criteria underpin the argument that models contain meaningful structure rather than random computation.
  - Quick check: Why is the "misrepresentation" criterion—being able to causally intervene and produce incorrect outputs—necessary for a pattern to count as a representation?

## Architecture Onboarding

- **Component map**: Ur-explanation (target implicit explanation) -> Proposed explanation (E) -> Explanatory faithfulness metric (layer-wise activation matching) -> Model vs. System boundary
- **Critical path**: 1) Identify behavior to explain, 2) Formulate mechanism hypothesis, 3) Test explanatory faithfulness via activation matching, 4) Causal intervention validation, 5) Iterate
- **Design tradeoffs**: Precision vs. Comprehension ("no such thing as interpreting a neural network, only interpreting at a given scale of precision"), Model-level vs. System-level focus, Theory-ladenness shaping what explanations emerge
- **Failure signatures**: Behavioral but not explanatory faithfulness, features not satisfying "use" criterion, alien concepts, value-laden precision mismatches
- **First 3 experiments**: 1) Validate causal intervention on known circuits via activation patching, 2) Test layer-wise activation matching on simple behaviors, 3) Probe theory-ladenness by comparing SAE architectures

## Open Questions the Paper Calls Out

- **Open Question 1**: How can we formally define explanatory complexity classes and explanatory universality?
  - Basis: Authors explicitly request formalization of explanatory complexity classes and universality analogous to computational universality
  - Why unresolved: Lacks formal mathematical machinery to compare explanatory capacity of humans versus neural networks
  - Evidence needed: Theoretical framework defining "explanatory complexity" enabling proofs about equivalence or universality

- **Open Question 2**: Is the Principle of Explanatory Optimism (strong or weak) true?
  - Basis: Paper states "Here we provide no arguments for the truth of Explanatory Optimism (EO) as a conjecture"
  - Why unresolved: Remains unproven whether learned algorithms are actually human-understandable
  - Evidence needed: Mathematical proof or disproof, or evidence of untranslatable vital concepts

- **Open Question 3**: How can we quantitatively operationalize what it means to understand "most" of a model's behavior?
  - Basis: Call to Action highlights need to "operationalise the quantitative notion of understanding 'most' of a model's behavior"
  - Why unresolved: No rigorous metric to measure proportion of behavior successfully explained
  - Evidence needed: Robust evaluation metric quantifying explanatory coverage relative to capabilities

## Limitations

- The Explanatory View Hypothesis lacks empirical validation that ur-explanations are well-defined or discoverable in practice
- The Principle of Explanatory Optimism is explicitly presented as unproven conjecture
- Layer-wise activation matching for explanatory faithfulness lacks mathematical formalization or operationalization
- Assumes clean model/system boundary that may not hold for complex LM architectures

## Confidence

- **High confidence**: Philosophical framing and conceptual definitions are internally consistent and well-argued; explicit acknowledgment of theory-ladenness demonstrates rigor
- **Medium confidence**: Explanatory View Hypothesis as conceptual target is reasonable but confidence drops regarding whether ur-explanations are actually well-defined
- **Low confidence**: Principle of Explanatory Optimism is explicitly unproven conjecture; practical utility of explanatory faithfulness remains hypothetical

## Next Checks

1. Test the causal intervention criterion: Apply activation patching to established circuits and verify whether intervention effects on outputs match predictions
2. Operationalize explanatory faithfulness: Develop quantitative metrics comparing explanation-predicted activations versus actual model activations across layers
3. Probe alien concepts: Systematically test whether explanations require concepts fundamentally inaccessible to human cognition by attempting to translate extracted features