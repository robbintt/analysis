---
ver: rpa2
title: 'MATA: A Trainable Hierarchical Automaton System for Multi-Agent Visual Reasoning'
arxiv_id: '2601.19204'
source_url: https://arxiv.org/abs/2601.19204
tags:
- agent
- hyper
- reasoning
- wang
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MATA, a hierarchical multi-agent system for
  visual reasoning that combines a trainable hyper agent with rule-based sub-agents
  in a finite-state automaton framework. Unlike prior approaches that use fixed pipelines
  or single agents, MATA learns high-level transitions between complementary and overlapping
  agents, enabling both collaboration and competition.
---

# MATA: A Trainable Hierarchical Automaton System for Multi-Agent Visual Reasoning

## Quick Facts
- **arXiv ID**: 2601.19204
- **Source URL**: https://arxiv.org/abs/2601.19204
- **Reference count**: 36
- **Primary result**: State-of-the-art accuracy on visual reasoning benchmarks (GQA 64.9%, OK-VQA 76.5%) using a trainable hyper agent to learn transitions between rule-based sub-agents.

## Executive Summary
This paper introduces MATA, a hierarchical multi-agent system for visual reasoning that combines a trainable hyper agent with rule-based sub-agents in a finite-state automaton framework. Unlike prior approaches that use fixed pipelines or single agents, MATA learns high-level transitions between complementary and overlapping agents, enabling both collaboration and competition. Each agent maintains a rule-based sub-automaton for reliable micro-control, while the hyper agent—supervised via a dataset of transition trajectories (MATA-SFT-90K)—selects the next agent based on shared memory context. This design improves interpretability and robustness against hallucinations. MATA achieves state-of-the-art accuracy across multiple visual reasoning benchmarks, including GQA (64.9%), OK-VQA (76.5%), and referring expression tasks, outperforming both monolithic and compositional baselines.

## Method Summary
MATA formalizes visual reasoning as a Mealy machine M_θ = (S, S₀, Σ, Λ, δ_θ, Γ), where δ_θ is a learned transition function implemented by an LLM state controller. The system generates a dataset MATA-SFT-90K by expanding bounded trajectory trees for each image-query pair, scoring leaf nodes with task metrics, and propagating scores upward to label optimal next-state decisions. A Qwen3-4B LLM is then fine-tuned on this dataset to learn transition policies. The architecture consists of three specialized sub-agents (SPECIALIZED for perception, ONESHOT for single-pass reasoning, STEPWISE for multi-step deliberation) each with rule-based sub-automata, coordinated by the trainable hyper agent that reads shared append-only memory to make routing decisions.

## Key Results
- Achieves state-of-the-art accuracy on GQA (64.9%), OK-VQA (76.5%), and RefCOCO variants
- Ablation shows LLM+SFT outperforms random transition (57.1%) and pretrained LLM without SFT (58.5%)
- Domain-transfer results show <1% gap between general and domain-specific models
- Three-agent setup provides diminishing returns beyond two agents (0.4% gain on GQA)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A trainable hyper agent learning state transitions outperforms hand-crafted pipelines by dynamically routing to specialized agents based on context.
- Mechanism: The system models visual reasoning as a Mealy machine M_θ = (S, S₀, Σ, Λ, δ_θ, Γ), where δ_θ is a learned transition function implemented by an LLM state controller. This controller reads shared memory snapshots and selects among collaborative agents (SPECIALIZED, ONESHOT, STEPWISE) and lifecycle states (INITIAL, FINAL, FAILURE), enabling both collaboration (agents build on prior context) and competition (overlapping agents can retry failed tasks).
- Core assumption: Cross-agent transition decisions depend on task context in ways that are learnable from trajectory data but difficult to hand-code, particularly for functionally overlapping agents.
- Evidence anchors:
  - [abstract] "unlike prior approaches that use fixed pipelines or single agents, MATA learns high-level transitions between complementary and overlapping agents"
  - [Section 4.2, Table 5] Ablation shows LLM+SFT (64.9% GQA) outperforms random transition (57.1%) and pretrained LLM without SFT (58.5%)
  - [corpus] Weak direct evidence; corpus papers on hierarchical multi-agent systems (MASTER, MARS) focus on different domains without validating MATA's specific transition-learning mechanism
- Break condition: When state space grows large, exhaustive trajectory tree generation becomes intractable—paper acknowledges this limitation with current three agents.

### Mechanism 2
- Claim: Supervised finetuning on transition-trajectory trees produces transferable agent-selection policies without requiring end-to-end VLM training.
- Mechanism: For each image-query pair, expand a bounded trajectory tree exploring all state transitions. Score leaf nodes with task metrics (IoU for grounding, accuracy for VQA), then propagate scores upward via V(s) = max_{s'∈Child(s)} V(s'). Label each decision node with the child leading to the highest-scoring subtree, generating memory-to-next-state pairs for SFT.
- Core assumption: The optimal next-state decision at each timestep can be determined by comparing downstream task outcomes, and this decision policy generalizes to unseen queries.
- Evidence anchors:
  - [Section 3.4, Eq. 3-4] Formal definition of bottom-up node scoring and optimal next-state selection
  - [Section 4.2, Table 6] Domain-transfer results (GQA→OK-VQA: 75.8% vs domain-specific 76.5%) show the transition policy transfers with <1% gap
  - [corpus] No corpus papers directly validate trajectory-tree-based training; training methods like M-GRPO in corpus operate on different principles
- Break condition: If agent capabilities shift significantly (e.g., new tools, degraded VLM), the learned policy may become misaligned with actual agent strengths.

### Mechanism 3
- Claim: Shared append-only memory enables transparent, auditable reasoning while providing sufficient context for transition decisions.
- Mechanism: All agents read/write to structured memory m_t accumulating variables, perception results, code history, and verifier feedback. Memory updates as m_{t+1} = m_t ∪ Δm_t. The hyper agent's prompt x_t is derived from m_t via a template exposing task description, query, feedback, code, variables, and state history.
- Core assumption: A textual summary of execution history captures the essential information for deciding which agent should act next.
- Evidence anchors:
  - [Section 3.2] "Memory is append-only so the full reasoning process is auditable and visible to the hyper agent"
  - [Prompt 3.1] Concrete template showing what memory fields are exposed to the LLM controller
  - [corpus] Corpus papers on multi-agent systems (VIKI-R, GenEscape) similarly assume shared state but do not empirically validate this design choice for MATA's architecture
- Break condition: If memory grows too large (many iterations, verbose code), prompt context limits may force truncation, potentially losing critical decision-relevant information.

## Foundational Learning

- Concept: **Finite-State Automata (Mealy Machines)**
  - Why needed here: MATA's entire control flow is formalized as a Mealy machine where outputs (agent transitions) depend on both current state and input (memory).
  - Quick check question: Can you trace one complete inference cycle from INITIAL through two agent states to FINAL, labeling all transitions?

- Concept: **Supervised Finetuning of LLMs**
  - Why needed here: The hyper agent's transition policy δ_θ is learned via SFT on MATA-SFT-90K, requiring understanding of instruction-completion pair formatting.
  - Quick check question: Given a trajectory tree node with three child branches scoring 0.7, 0.5, and 0.9, what would be the SFT label for that node's memory snapshot?

- Concept: **System 1 vs System 2 Reasoning**
  - Why needed here: The three agents map to this dual-process framework—SPECIALIZED for fast perception, STEPWISE for slow deliberative reasoning—informing when transitions should occur.
  - Quick check question: For a query like "Count the red cups on the left table," which agent should the hyper agent likely select first, and what failure signal would trigger a transition?

## Architecture Onboarding

- Component map:
  Hyper Automaton (M_θ)
  ├── States: INITIAL, SPECIALIZED, ONESHOT, STEPWISE, FINAL, FAILURE
  ├── Hyper Agent
  │   ├── Memory Prompter (converts m_t → text prompt x_t)
  │   ├── LLM State Controller (finetuned Qwen3-4B)
  │   └── Transition Verifier (validates output format)
  ├── Shared Memory m_t (append-only: variables, code, feedback, history)
  └── Sub-Agents (each with rule-based sub-automaton):
      ├── SPECIALIZED: VLM/detector → Prediction Verifier
      ├── ONESHOT: Code Generator → Code Verifier → Code Interpreter
      └── STEPWISE: Instruction Generator → Code Generator → Code Interpreter (loops)

- Critical path:
  1. Initialize m_0 with image-query pair, enter INITIAL state
  2. Hyper agent reads m_t, generates next-state prediction s_{t+1}
  3. If s_{t+1} ∈ S_agent: execute sub-automaton, append results to m_t, return to step 2
  4. If s_{t+1} = FINAL or t > T: extract answer via Γ(FINAL, m_t)
  5. If s_{t+1} = FAILURE: remove failed agent from candidates, retry

- Design tradeoffs:
  - **Three agents vs more**: Paper shows diminishing returns (2→3 agents yields only 0.4% gain on GQA, Figure 4)
  - **Rule-based sub-automata vs learned intra-agent control**: Rules provide reliability for micro-steps (verifiers, code execution) while learning handles ambiguous cross-agent selection
  - **Domain-specific vs general SFT**: General model (all data) vs domain-specific show <1% difference, suggesting single general model may suffice (Table 6)
  - **LLM size for controller**: 0.6B–1.7B models competitive with domain-specific SFT; 4B chosen as default for cross-task generalization (Figure 3)

- Failure signatures:
  - Persistent state cycling: Hyper agent repeatedly selects same agent → check if FAILURE state properly removes failed agent from candidates
  - Empty/wrong format outputs from controller: Transition Verifier should catch; if recurrent, inspect prompt template rendering
  - Code execution errors in STEPWISE: Should trigger regeneration loops; if exhausted, should transition to FAILURE state
  - Domain-transfer degradation >2%: May indicate training data distribution mismatch; consider augmenting trajectory data for target domain

- First 3 experiments:
  1. **Reproduce ablation**: Train hyper agent with random transitions, pretrained LLM (no SFT), and full SFT on a single dataset split (e.g., GQA train→testdev). Confirm the 6–7 percentage point gap reported in Table 5.
  2. **Test domain transfer**: Train on RefCOCO subset only, evaluate on GQA. Verify that cross-task transfer achieves within 1–2% of domain-specific baseline (compare to Table 6 pattern).
  3. **Probe agent selection patterns**: Log transition decisions on 50 held-out queries. Manually inspect whether easy queries preferentially route to SPECIALIZED first and escalate to STEPWISE on failure, as claimed in Section 4.1.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the transition-trajectory data generation pipeline be adapted to remain efficient as the number of agent states increases significantly?
- Basis in paper: [explicit] The Conclusion states the current near-exhaustive search is "tractable with the current three agents but may become costly as the number of states grows."
- Why unresolved: The current MATA-SFT-90K generation relies on a bounded trajectory tree that expands combinatorially, making data collection potentially prohibitive for complex systems with many specialized agents.
- What evidence would resolve it: A demonstration of a sampling-based or reinforcement learning data generation method that maintains transition policy accuracy while reducing computational cost for systems with >5 agents.

### Open Question 2
- Question: Does the performance of the trainable hyper agent degrade or improve when the diversity of specialized agents scales beyond the current three-agent setup?
- Basis in paper: [inferred] The ablation study (Figure 4) shows diminishing returns when increasing from 2 to 3 agents, but does not test whether adding more agents (e.g., specific tools for depth, OCR, segmentation) improves robustness or confuses the transition policy.
- Why unresolved: It is unclear if the learned transition policy effectively manages a larger action space or if the "diminishing improvements" suggest a ceiling for this specific architecture.
- What evidence would resolve it: Experiments varying the agent count (e.g., 5 to 10 agents) to measure if the hyper agent can successfully learn to route among highly granular capabilities.

### Open Question 3
- Question: Can the rigid, rule-based sub-automata be replaced by learnable policies without sacrificing the system's reliability and interpretability?
- Basis in paper: [inferred] The methodology relies on rule-based sub-automata for "reliable micro-control," assuming learned micro-control is less dependable, but offers no ablation comparing rule-based versus learned intra-agent transitions.
- Why unresolved: While rules ensure reliability, they may lack the flexibility to handle novel tool errors or edge cases that a learned internal policy might navigate.
- What evidence would resolve it: An ablation study replacing the rule-based sub-automata with small trained policies to compare error recovery rates and execution stability.

## Limitations

- **Scalability**: The trajectory tree expansion approach becomes computationally intractable as agent count increases beyond three
- **Memory sufficiency**: No ablation study validates whether the shared memory schema captures all decision-relevant information
- **Verifier robustness**: Rule-based verifiers may not handle sophisticated adversarial inputs or domain-specific edge cases

## Confidence

- **High Confidence**: The ablation results showing SFT significantly outperforms random transitions and pretrained-only models are well-supported by direct experimental evidence in Table 5
- **Medium Confidence**: The reported accuracy improvements over baselines are substantial, but comparison to other hierarchical multi-agent systems is limited since most corpus papers operate in different domains
- **Low Confidence**: The paper does not provide empirical evidence that the shared memory contains sufficient context for all possible transition decisions, nor does it address how the approach would scale to more than three agents or continuous state spaces

## Next Checks

1. **Memory Ablation Study**: Systematically remove individual memory fields (variables, code history, feedback, etc.) and measure the impact on transition accuracy and final task performance to validate whether the current memory schema captures all necessary decision information.

2. **Scalability Experiment**: Extend the agent set from 3 to 5-7 agents (e.g., adding specialized agents for different modalities) and measure the degradation in transition learning quality and inference speed to test the trajectory tree scalability limits mentioned in the paper.

3. **Adversarial Verifier Testing**: Create a benchmark of adversarial inputs designed to bypass the rule-based verifiers, measuring false acceptance rates to assess the robustness claims and identify potential failure modes in the verification pipeline.