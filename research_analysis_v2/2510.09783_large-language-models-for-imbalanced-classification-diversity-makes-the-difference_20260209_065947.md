---
ver: rpa2
title: 'Large Language Models for Imbalanced Classification: Diversity makes the difference'
arxiv_id: '2510.09783'
source_url: https://arxiv.org/abs/2510.09783
tags:
- samples
- minority
- minor
- methods
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses imbalanced classification in tabular data
  by proposing an LLM-based oversampling method called ImbLLM. The key innovation
  is enhancing diversity in synthetic minority samples through three improvements:
  conditioning sample generation on both minority labels and features, permuting features
  while fixing the minority label to ensure attention, and fine-tuning the LLM on
  minority and interpolated samples only.'
---

# Large Language Models for Imbalanced Classification: Diversity makes the difference

## Quick Facts
- arXiv ID: 2510.09783
- Source URL: https://arxiv.org/abs/2510.09783
- Reference count: 33
- Primary result: LLM-based oversampling method ImbLLM outperforms 8 SOTA baselines, achieving best results in 5/10 cases with up to 8% improvement over non-oversampling

## Executive Summary
This paper addresses imbalanced classification in tabular data by proposing an LLM-based oversampling method called ImbLLM. The key innovation is enhancing diversity in synthetic minority samples through three improvements: conditioning sample generation on both minority labels and features, permuting features while fixing the minority label to ensure attention, and fine-tuning the LLM on minority and interpolated samples only. Extensive experiments on 10 datasets show ImbLLM outperforms 8 SOTA baselines, achieving best results in 5 cases with up to 8% improvement over non-oversampling and 3% over the next best baseline. Generated samples are shown to be both realistic and diverse, with high coverage and close probability scores. Theoretical analysis using entropy confirms the method promotes diversity.

## Method Summary
ImbLLM converts tabular data into sentence format ("X1 is v1, ..., XM is vM, Y is y"), then applies three key innovations during LLM fine-tuning and generation. First, it uses dual-conditioning where prompts combine minority labels with randomly sampled feature-value pairs to increase diversity. Second, it employs fixed-label permutation (fix_y) that places the minority label at the beginning of sequences while permuting features, ensuring all features attend to the label during training. Third, it fine-tunes exclusively on minority samples and their interpolated variants (no majority samples), using linear interpolation for continuous features. The method uses DistilGPT-2 fine-tuned on minority+interpolated data, generates synthetic samples via autoregressive generation with temperature T=0.7, and evaluates using XGBoost classifiers on the rebalanced datasets.

## Key Results
- ImbLLM achieves best F1/AUC in 5 out of 10 datasets compared to 8 SOTA baselines
- Coverage scores consistently above 0.8, indicating high diversity in synthetic samples
- Close probability scores near 1.0, showing synthetic samples are realistic and close to unseen test minority samples
- Theoretical entropy analysis proves each mechanism increases diversity in synthetic generation
- Outperforms non-oversampling baselines by up to 8% and next best baseline by up to 3%

## Why This Works (Mechanism)

### Mechanism 1: Dual-Conditioning for Prompt Diversity
- Claim: Conditioning synthetic sample generation on both minority labels and features increases output diversity compared to label-only conditioning.
- Mechanism: Standard LLM-based methods use identical prompts "Y is y_minor" for all samples, causing the softmax function to repeatedly select high-probability tokens. By sampling a random feature-value pair "X_i is v_i" and concatenating it with the label, prompts become diverse (e.g., "Income is ≥200K, Edu is Bachelor" vs. "Income is ≥200K, Job is Doctor"), forcing the model to sample from a mixture of distributions rather than a single fixed distribution.
- Core assumption: The LLM's output distribution is sufficiently context-sensitive that different feature-value suffixes produce meaningfully different conditional distributions p(G|C⊕R_i).
- Evidence anchors:
  - [abstract]: "conditioning synthetic sample generation on both minority labels and features"
  - [section III-A1]: "Our prompts are much more diverse as they combine the minority label with different features and values. We call our strategy condition_yx."
  - [section IV-A, Proposition 1]: "Injecting randomness into the condition C by concatenating random combinations {R_i} increases the diversity... total entropy of sampling from the resulting mixture of distributions is greater than the entropy of repeatedly sampling from a single, fixed distribution."
  - [corpus]: Weak—corpus neighbors focus on SMOTE-style interpolation and kernel-based methods; no direct evidence on prompt conditioning strategies for LLMs.
- Break condition: If the sampled features are highly deterministic given the minority label (strong conditional dependence), prompt variations may not translate to meaningfully different output distributions.

### Mechanism 2: Fixed-Label Permutation for Full Attention Coverage
- Claim: Permuting features while fixing the minority label at the sequence beginning ensures all features can attend to the label during training, improving conditional generation.
- Mechanism: Decoder-only LLMs use causal attention masks where token i can only attend to tokens j ≤ i. Existing methods permute both X and Y together (permute_xy), which can place Y in the middle or end of the sequence, preventing earlier features from attending to it. The fix_y strategy always places "Y is y_minor" at the beginning, ensuring all subsequent feature tokens attend to the label context.
- Core assumption: Learning the conditional distribution p(X|Y) requires Y to be in the attention window of all feature positions during fine-tuning.
- Evidence anchors:
  - [abstract]: "permuting features while fixing the minority label to ensure attention"
  - [section III-A2, Figure 2]: Visual comparison showing permute_xy results in sparse attention (Y only attends to later features), while fix_y enables dense attention from Y to all features.
  - [section IV-B, Proposition 2]: "The conditional entropy of the distribution learned by the 'begin' model, when prompted with C, is greater than that of the 'middle' model."
  - [corpus]: No corpus evidence on attention-aware permutation strategies.
- Break condition: If using bidirectional architectures (e.g., encoder-only transformers), this mechanism does not apply; also irrelevant for non-autoregressive generation.

### Mechanism 3: Minority-Only Fine-Tuning with Interpolation
- Claim: Fine-tuning exclusively on minority samples and their interpolated variants prevents majority-class bias and improves coverage of continuous feature spaces.
- Mechanism: Training on both majority and minority samples biases the LLM toward majority patterns, causing synthetic minority samples to drift toward majority characteristics. By training only on D_minor ∪ D_inter (where D_inter contains interpolated continuous features via x'_i = x^con_i + ε(x^con_j - x^con_i)), the model learns a smoother, broader representation of minority space without requiring an unreliable verification step.
- Core assumption: Linear interpolation between real minority continuous features produces plausible synthetic samples that lie within the true minority manifold.
- Evidence anchors:
  - [abstract]: "fine-tuning the LLM on minority and interpolated samples only"
  - [section III-A3, Equation 5]: Interpolation formula for continuous variables.
  - [section IV-C, Proposition 3]: "The entropy of the augmented model's distribution is greater than that of the original model's distribution: H(Q^aug_θ) > H(Q^minor_θ)."
  - [corpus]: Partial—corpus neighbors (SMOTE variants) validate interpolation as effective for oversampling, but not specifically for LLM fine-tuning.
- Break condition: If minority continuous features have multimodal distributions with disjoint clusters, linear interpolation may generate unrealistic samples between modes.

## Foundational Learning

- **Concept: Causal Attention Masking in Decoder-Only LLMs**
  - Why needed here: Mechanism 2 depends on understanding why token ordering affects what the model can learn—token i cannot attend to tokens after it.
  - Quick check question: In GPT-2, can the 5th token in a sequence attend to the 8th token? (Answer: No—only tokens 1-5.)

- **Concept: Softmax Temperature and Output Diversity**
  - Why needed here: Mechanism 1 addresses the softmax bottleneck where identical prompts lead to repetitive outputs; temperature T controls distribution sharpness.
  - Quick check question: What happens to output diversity as temperature T → 0? (Answer: The distribution becomes deterministic, always selecting the highest-probability token.)

- **Concept: Entropy as a Diversity Measure**
  - Why needed here: The paper's theoretical analysis (Propositions 1-3) uses entropy to formally prove each mechanism increases diversity.
  - Quick check question: If H(p) > H(q), which distribution has higher uncertainty/diversity? (Answer: p.)

## Architecture Onboarding

- **Component map:**
  Raw tabular data → Sentence serialization → fix_y permutation → Interpolation (continuous features) → LLM fine-tuning → Diverse prompt construction → Autoregressive generation → Post-processing (verify format/validity) → Synthetic minority dataset

- **Critical path:**
  Raw tabular data → Sentence serialization → fix_y permutation → Interpolation (continuous features) → LLM fine-tuning → Diverse prompt construction → Autoregressive generation → Post-processing (verify format/validity) → Synthetic minority dataset

- **Design tradeoffs:**
  - Minority-only training removes majority bias but risks overfitting; interpolation mitigates this by expanding continuous feature coverage.
  - Higher interpolation ratio (r) increases entropy/diversity but may generate out-of-distribution samples if r approaches 1 on sparse data.
  - Temperature T=0.7 balances coherence (realistic samples) vs. diversity (coverage); lower T yields more repetitive outputs.

- **Failure signatures:**
  - Low Coverage score (<0.8): Synthetic samples cluster around few modes, failing to capture minority variance.
  - High DCR mode (~1.0): Synthetic samples far from unseen test minority samples (see Figure 7—ImbLLM mode near 0, others near 1).
  - Verification-based methods (TapTap, Pred-LLM) underperform when the imbalanced classifier has low F1—the verifier is unreliable (Table V).

- **First 3 experiments:**
  1. **Ablation on permutation strategy:** Compare fix_y vs. permute_xy on attention coverage (visualize attention matrices) and downstream F1-score. Expected: fix_y shows denser attention, higher F1.
  2. **Interpolation ratio sweep:** Test r ∈ {0, 0.25, 0.5, 0.75, 1.0} × |D_major|. Monitor Close probability (quality) vs. Coverage (diversity). Expected: Sweet spot around r=0.75-1.0 for most datasets.
  3. **Cross-validation on imbalance ratio:** Vary q ∈ {0.1, 0.2, 0.3, 0.4, 0.5} (fraction of minority samples used for training). Expected: Performance gap between ImbLLM and baselines widest at low q (severe imbalance).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ImbLLM perform on datasets with multiple distinct minority classes rather than a binary or "one-vs-rest" setup?
- Basis in paper: [inferred] The Background section (II-A) explicitly states, "To simplify the problem, we consider Y as a binary variable" and defines the minority class as the class with the lowest number of samples, effectively collapsing multi-class problems into binary ones for the experiments.
- Why unresolved: The proposed permutation strategy (fix_y) fixes a single minority label at the beginning of the sequence to ensure attention; it is unclear if this mechanism scales efficiently when the model must switch contexts between multiple distinct minority labels simultaneously.
- What evidence would resolve it: Experiments on standard multi-class imbalanced benchmarks (e.g., image or text classification with many classes) where the method generates samples for all non-majority classes without binary reduction.

### Open Question 2
- Question: Can the interpolation strategy be extended to categorical variables to further improve performance in sparse feature spaces?
- Basis in paper: [inferred] Section III-A3 notes that the method "only interpolate[s] continuous variables" based on the assumption that "categorical values are still good enough to cover the categorical domains," leaving the potential for categorical interpolation unexplored.
- Why unresolved: While categorical domains are finite, minority samples may still exhibit rare co-occurrences of categorical values that simple interpolation of continuous features cannot capture, potentially limiting diversity in specific domains.
- What evidence would resolve it: An ablation study applying techniques like categorical mixing or embedding interpolation alongside the continuous interpolation, measured against the "Coverage" metric.

### Open Question 3
- Question: Is the reported performance gain robust across different downstream classifiers, or is it dependent on the specific behavior of XGBoost?
- Basis in paper: [inferred] The Evaluation Metric section (V-A2) and all subsequent experiments rely exclusively on XGBoost to evaluate the quality of the synthetic data, citing only that it is "popular" for tabular data.
- Why unresolved: LLM-generated tabular data often differs statistically from real data (e.g., in distribution tails or discrete value frequencies); it is possible that tree-based models handle this specific noise better than neural networks or linear models.
- What evidence would resolve it: Re-evaluating the F1-scores of the rebalanced datasets using diverse classifiers such as Multi-Layer Perceptrons (MLP), Logistic Regression, or Random Forests.

## Limitations

- The dual-conditioning mechanism's benefit may saturate with more complex prompts, and excessive randomness could reduce sample realism.
- The fixed-label permutation strategy is limited to decoder-only architectures and doesn't apply to bidirectional transformers commonly used in tabular tasks.
- Linear interpolation between continuous features may produce unrealistic samples when minority distributions are multimodal with disjoint clusters.

## Confidence

**High confidence:** The experimental superiority of ImbLLM over 8 SOTA baselines is well-supported by quantitative metrics (F1, AUC) and diversity measures (Coverage, Close probability). The theoretical entropy analysis provides rigorous proof for each mechanism's contribution to diversity.

**Medium confidence:** The mechanisms' contributions are individually proven via entropy analysis, but the interaction effects between the three improvements (dual-conditioning, fixed-label permutation, minority-only fine-tuning) are not fully isolated. The interpolation ratio sweep suggests 0.75-1.0 is optimal, but this may be dataset-dependent.

**Low confidence:** The assumption that DistilGPT-2's autoregressive generation produces realistic synthetic samples for arbitrary tabular domains is not thoroughly validated. The paper shows samples are "realistic" via Close probability but doesn't examine feature distribution shifts or semantic coherence across diverse datasets.

## Next Checks

1. **Prompt complexity ablation:** Systematically vary the number of sampled feature-value pairs in condition_yx (1, 2, 3, 5 pairs) and measure the trade-off between diversity (Coverage) and realism (Close probability). This will determine if the current single-pair approach is optimal or if more complex prompts yield better results.

2. **Architecture generalization test:** Implement ImbLLM using a bidirectional transformer (e.g., DistilBERT) and compare performance to the decoder-only version. This will validate whether the fix_y permutation strategy's benefits are architecture-specific or if bidirectional attention can achieve similar diversity gains through different mechanisms.

3. **Interpolation validity analysis:** For each dataset, visualize the minority feature space and mark interpolated samples. Measure the percentage of interpolated points that fall in low-density regions (e.g., regions with <5% of minority samples). This will quantify how often interpolation produces unrealistic samples and inform whether adaptive interpolation strategies are needed.