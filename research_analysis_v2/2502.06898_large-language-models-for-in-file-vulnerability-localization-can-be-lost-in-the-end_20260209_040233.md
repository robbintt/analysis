---
ver: rpa2
title: Large Language Models for In-File Vulnerability Localization Can Be "Lost in
  the End"
arxiv_id: '2502.06898'
source_url: https://arxiv.org/abs/2502.06898
tags:
- vulnerability
- llms
- file
- size
- vulnerabilities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Recent advances in LLMs have enabled the processing of larger inputs,
  leading developers to increasingly rely on chat-based LLMs like GPT-3.5 and GPT-4
  for in-file vulnerability detection. However, it remains unclear whether these models
  can effectively analyze large file-sized inputs.
---

# Large Language Models for In-File Vulnerability Localization Can Be "Lost in the End"

## Quick Facts
- arXiv ID: 2502.06898
- Source URL: https://arxiv.org/abs/2502.06898
- Authors: Francesco Sovrano; Adam Bauer; Alberto Bacchelli
- Reference count: 40
- Key outcome: LLM effectiveness for in-file vulnerability detection is strongly influenced by both vulnerability location and input size, with a "lost-in-the-end" effect causing significant underperformance when detecting vulnerabilities at the end of larger files

## Executive Summary
This study evaluates how well state-of-the-art chat-based LLMs (GPT-3.5, GPT-4, Llama 3, Mixtral) detect in-file vulnerabilities, revealing that performance is strongly influenced by both vulnerability location and input size. The researchers identify a "lost-in-the-end" effect where LLMs significantly underperform when detecting vulnerabilities located toward the end of larger files, regardless of vulnerability type. They also demonstrate that adjusting input size through chunking can improve recall by over 37% on average across all models, and provide a simple strategy for identifying optimal input sizes for different vulnerability types.

## Method Summary
The researchers conducted extensive experiments using 5,000 vulnerable and non-vulnerable PHP files from three common CWE types (XSS, SQL Injection, Path Traversal). They tested four state-of-the-art chat-based LLMs with various input sizes, systematically analyzing performance across different vulnerability types, input sizes, and vulnerability positions within files. The study employed logistic regression to analyze factors affecting detection probability and used a "code-in-the-haystack" experiment to isolate the positional effect. They also explored optimal chunking strategies to maximize detection effectiveness.

## Key Results
- LLMs show significant performance degradation for vulnerabilities at the end of large files ("lost-in-the-end" effect)
- There is a negative correlation between input size and detection probability
- Chunking improves recall by over 37% on average across all tested models
- Optimal input sizes vary by CWE type (500-1500 chars for XSS, 3000-6500 chars for SQL Injection)

## Why This Works (Mechanism)

### Mechanism 1: Positional Attention Bias ("Lost-in-the-End")
The models appear to apply a positional heuristic or attention weighting that prioritizes earlier tokens, causing them to "give up" or hallucinate before reaching the end of the context window. The LLMs incorrectly predict that bug positions occur within the initial 10,000 characters, skewing toward the file's beginning.

### Mechanism 2: Input Size Toxicity
As context length increases, the "signal" of the vulnerability is diluted by the "noise" of surrounding code, overwhelming the model's effective reasoning capacity even if within the nominal context window. The relationship is roughly linear or monotonic regarding the degradation of performance.

### Mechanism 3: Context Restoration via Chunking
Chunking resets the "positional clock" and reduces context noise, making the vulnerability more prominent relative to the local window. The recall gain outweighs the loss of global context required to understand data flow, though this causes false positives due to missing context from other chunks.

## Foundational Learning

- **Concept: Top-k Accuracy & Recall vs. Precision**
  - Why needed here: The paper emphasizes *Recall* (finding the bug) over *Precision* (avoiding false alarms) because in security, missing a vulnerability (False Negative) is often costlier than investigating a false lead.
  - Quick check question: If a model flags 100 lines as vulnerable in a file with only 1 real bug, is it High Recall or High Precision? (Answer: Low Precision, potentially High Recall if it caught the real one).

- **Concept: Context Window vs. Effective Context**
  - Why needed here: Developers often assume that if a file fits in the token limit (e.g., 128k tokens), it is "processed." This paper proves the *effective* reasoning window is much smaller (e.g., 4k-16k chars) for this specific task.
  - Quick check question: Does a file fitting in the context window guarantee accurate analysis? (Answer: No, "Lost in the End" proves otherwise).

- **Concept: CWE (Common Weakness Enumeration)**
  - Why needed here: The paper segments findings by CWE types (79, 89, 22). Understanding that different bug classes (XSS vs. SQLi) have different optimal input sizes is crucial for system design.
  - Quick check question: Why might XSS (CWE-79) require smaller chunk sizes than Path Traversal (CWE-22) according to the paper? (Answer: XSS is more context-dependent/sensitive to "lost-in-the-end" effects).

## Architecture Onboarding

- **Component map:** Input Processor -> Chunking Service -> LLM Interface -> Result Aggregator
- **Critical path:**
  1. Receive file content and target CWE type
  2. Determine optimal chunk size $K$ (e.g., 1500 for CWE-79, 6500 for CWE-22)
  3. Split file into chunks $\le K$
  4. Query LLM for each chunk with the specific CWE prompt
  5. Aggregate hits; report if any chunk returns positive
- **Design tradeoffs:**
  - Cost vs. Accuracy: Chunking increases API calls (cost) and latency but is required for $>37\%$ recall improvement
  - Recall vs. Precision: Chunking boosts recall but lowers precision (more false positives) because chunks lack the full file context
- **Failure signatures:**
  - "Lost in the End" Failure: Vulnerabilities at line $N$ in a file of size $2N$ are missed if $N$ is large
  - Context Severance: False positives where a chunk contains `query = "..." + user_input` but the sanitization logic was in a previous chunk
- **First 3 experiments:**
  1. Baseline Check: Feed a set of known vulnerable files (whole) to the model and measure the drop-off rate as file size increases to validate the "Lost in the End" effect locally
  2. Chunking Threshold Search: Run a dichotomic search on chunk sizes (e.g., 500, 1500, 3000, 6500) for your specific target CWE to find the "sweet spot" for recall
  3. Positional Stress Test (Code-in-the-Haystack): Insert a known vulnerable snippet at the start, middle, and end of a large dummy file and measure detection rates to confirm positional bias

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the "lost-in-the-end" phenomenon extend to vulnerability types other than XSS, SQL Injection, and Path Traversal (CWE-79, CWE-89, CWE-22)?
- Basis in paper: [explicit] The authors state, "While we did not analyze additional types of vulnerabilities, the trends observed suggest that the ‚Äòlost-in-the-end‚Äô issue may also affect other vulnerability types."
- Why unresolved: The study was constrained to three specific CWE types to ensure sufficient statistical power (ùõΩ ‚â• .8) and due to the high cost of running experiments.
- What evidence would resolve it: Replicating the "code-in-the-haystack" experiment across a wider variety of CWE types (e.g., buffer overflows, cryptographic failures) to see if detection probability similarly declines at the end of files.

### Open Question 2
- Question: Can training LLMs on datasets with uniformly distributed bug positions mitigate the "lost-in-the-end" effect?
- Basis in paper: [explicit] The authors hypothesize that models use code position as a heuristic because real-world data is skewed. They suggest "training LLMs with more uniformly distributed bug positions... could potentially mitigate the ‚Äòlost-in-the-end‚Äô phenomenon."
- Why unresolved: The study evaluates off-the-shelf models and does not perform new fine-tuning or training experiments.
- What evidence would resolve it: Training a new model (or fine-tuning an existing one) on a vulnerability dataset where bugs are randomly distributed throughout file lengths, then testing for the "lost-in-the-end" pattern.

### Open Question 3
- Question: Can permutation-invariant neural networks, such as graph neural networks, prevent the "lost-in-the-end" issue by making detection robust to code reordering?
- Basis in paper: [explicit] Noting that code is not linear, the authors propose that "future research should explore more advanced architectures... including permutation-invariant neural networks... where reordering a function‚Äôs position does not affect the model‚Äôs ability."
- Why unresolved: Current LLMs (e.g., GPT, Llama) process input linearly, but the study did not test architectures that handle code non-linearly.
- What evidence would resolve it: Applying the "code-in-the-haystack" methodology to a graph-based code model to determine if its detection accuracy remains constant regardless of the vulnerable function's location in the input.

### Open Question 4
- Question: Do LLMs underperform on frequent vulnerabilities like XSS because their training data contains unfixed instances of these bugs, causing the models to learn them as normal code?
- Basis in paper: [explicit] The authors hypothesize: "It is possible that the most frequently fixed CWE vulnerability types (CWE-79) also more frequently appear unfixed in production... If the model cannot handle a large context effectively, it might learn these vulnerabilities as normal non-buggy code."
- Why unresolved: Analyzing the massive, opaque training corpora of commercial models to find unfixed vulnerabilities is currently infeasible.
- What evidence would resolve it: A controlled study training models on codebases with known ratios of fixed vs. unfixed vulnerabilities to measure the impact on detection sensitivity.

## Limitations

- Generalizability of Chunking Heuristics: The identified optimal input sizes were derived from a PHP corpus and may not transfer to other languages without recalibration
- Prompt Sensitivity: The study uses a single prompt template per CWE, but minor phrasing changes could significantly alter the "lost-in-the-end" effect magnitude
- False Positive Calibration: The chunking strategy improves recall but degrades precision, and the paper doesn't provide a systematic method to balance them

## Confidence

**High Confidence Claims:**
- LLMs show significant performance degradation for vulnerabilities at the end of large files
- There is a negative correlation between input size and detection probability
- Chunking improves recall across all tested models

**Medium Confidence Claims:**
- Specific optimal chunk sizes for different CWE types (requires validation on different corpora)
- The relationship between input size and performance is monotonic
- Recall improvement averages 37% across models

**Low Confidence Claims:**
- The exact mechanisms causing "lost-in-the-end" (attention bias vs. training data distribution)
- Prompt sensitivity and generalizability across different vulnerability types
- Long-term effectiveness as models scale to larger context windows

## Next Checks

1. **Cross-Language Chunking Validation:** Apply the identified optimal chunk sizes (500-1500 for CWE-79, 3000-6500 for CWE-89) to a Python/JavaScript vulnerability corpus and measure recall degradation.

2. **Positional Recovery Experiment:** For files where vulnerabilities are missed at the end, systematically move the same vulnerability to the beginning/middle and re-run detection to quantify the "lost-in-the-end" effect magnitude.

3. **Chunking Granularity Sweep:** Instead of fixed optimal sizes, test a logarithmic sweep (250, 500, 1000, 2000, 4000, 8000 chars) on a representative sample to map the full recall-precision tradeoff curve.