---
ver: rpa2
title: Comparing how Large Language Models perform against keyword-based searches
  for social science research data discovery
arxiv_id: '2601.19559'
source_url: https://arxiv.org/abs/2601.19559
tags:
- search
- keyword
- tool
- datasets
- returned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study compares a large language model (LLM)-based semantic
  search tool with a traditional keyword search for data discovery in social science
  research. Using 131 frequently used search terms from the Consumer Data Research
  Centre (CDRC), the authors assess differences in result volume, overlap, ranking,
  and relevance.
---

# Comparing how Large Language Models perform against keyword-based searches for social science research data discovery

## Quick Facts
- arXiv ID: 2601.19559
- Source URL: https://arxiv.org/abs/2601.19559
- Authors: Mark Green; Maura Halstead; Caroline Jay; Richard Kingston; Alex Singleton; David Topping
- Reference count: 1
- Key outcome: LLM semantic search returns more results than keyword search (median 30 vs. 7) and better handles complex/misspelled queries, though overlap is moderate

## Executive Summary
This study compares LLM-based semantic search with traditional keyword search for data discovery in social science research using 131 real-world search terms from the Consumer Data Research Centre. The semantic search tool consistently returned more results than keyword search and handled complex, misspelled, or obscure queries better, often retrieving geographically relevant datasets missed by keyword search. While exact dataset overlap was moderate (median 54.6%), semantic similarity scores were high (median cosine similarity 0.94), indicating the LLM returned relevant but different datasets. Rankings of top results differed substantially between tools.

## Method Summary
The study used 131 frequently used search terms from CDRC search logs, comparing results from a custom LLM-based semantic search tool against CDRC's keyword search. The semantic search used BERT embeddings stored in Pinecone vector database, with manual entry for semantic search and scraping for keyword results. Four hypotheses were tested using result count comparison, exact dataset overlap percentage, Jaccard similarity on dataset titles, and cosine similarity via BERT embeddings on titles. CDRC-specific filtering was applied post-hoc to both search methods.

## Key Results
- Semantic search returned more results than keyword search (median 30 vs. 7)
- Exact dataset overlap was moderate (median 54.6%) but semantic similarity was high (median cosine similarity 0.94)
- Semantic search better handled complex, misspelled, or obscure queries
- Geographic reasoning enabled retrieval of relevant datasets without explicit place-name keywords

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic embedding-based matching retrieves conceptually related datasets that keyword matching misses
- Mechanism: LLM encodes queries and metadata into dense vector representations where semantically similar concepts occupy nearby positions in embedding space
- Core assumption: BERT-derived embeddings capture domain-relevant semantic relationships for social science metadata
- Evidence anchors: High cosine similarity scores (median 0.94) despite low Jaccard similarity; embedding approach described in methodology
- Break condition: If metadata is too sparse or terminology diverges significantly from pretraining corpus

### Mechanism 2
- Claim: LLM semantic search infers geographic and contextual relevance beyond explicit metadata tags
- Mechanism: Model leverages learned geographic knowledge to connect queries to datasets lacking explicit place-name keywords
- Core assumption: LLM pretraining includes sufficient geographic knowledge for valid contextual inferences
- Evidence anchors: "Searching for 'Sheffield' picked up datasets collected for 'Yorkshire' or 'South Yorkshire'"; demonstrates regional context interpretation
- Break condition: If geographic relationships are obscure or counterintuitive

### Mechanism 3
- Claim: Semantic matching provides robustness to spelling errors and natural language query variations
- Mechanism: Embedding representations capture sub-word and character patterns, allowing noisy inputs to map close to correct semantic region
- Core assumption: Misspellings preserve enough similarity for embedding proximity
- Evidence anchors: "All the 21 incorrect spellings of 'retail' produced zero results on keyword search"; LLM robust to most incorrect spellings
- Break condition: Severely corrupted inputs or domain-specific acronyms without training exposure

## Foundational Learning

- Concept: **Vector embeddings and cosine similarity**
  - Why needed here: Understanding how text becomes numerical vectors and how similarity is computed is essential for debugging matches
  - Quick check question: If two dataset titles have cosine similarity of 0.94 but Jaccard similarity of 0.20, what does this tell you about their relationship?

- Concept: **Keyword vs. semantic search tradeoffs**
  - Why needed here: The paper explicitly compares these paradigms; knowing when each fails helps set realistic expectations
  - Quick check question: Why might a keyword search outperform semantic search for the query "data" (a very general term)?

- Concept: **LLM pretraining knowledge and transfer**
  - Why needed here: Geographic reasoning and typo tolerance depend on knowledge encoded during pretraining
  - Quick check question: Would you expect this system to handle newly coined acronyms or very recent geographic boundary changes well? Why or why not?

## Architecture Onboarding

- Component map: Query → embedding generation → Pinecone vector similarity search → CDRC filtering → ranked results with optional AI explanations
- Critical path: Query → embedding generation (latency-critical) → vector similarity search against Pinecone index → filter to target catalog and apply relevance cutoff → return ranked results
- Design tradeoffs: Result limit (500 vs. 5000/10000) due to Pinecone API rate issues; LLM excludes low-relevance results vs. keyword returns all matches; semantic expansion trades keyword-match recall for conceptual relevance
- Failure signatures: Generic queries return fewer results than keyword; place-specific queries occasionally return comparative datasets without the place itself; complex phrasing like "topic about education" returned no CDRC results
- First 3 experiments: 1) Hybrid retrieval test combining keyword and semantic results; 2) Geographic expansion calibration measuring clicked vs. ignored inferred-region datasets; 3) Relevance threshold sensitivity varying similarity cutoff and measuring precision@10

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does LLM-based semantic search improve user task success and satisfaction in data discovery compared to keyword-based search?
- Basis: Study relies on proxy metrics for relevance rather than measuring actual user performance or feedback
- Why unresolved: High semantic similarity scores don't necessarily equate to datasets being more useful for specific analytical needs
- What evidence would resolve it: Controlled user study comparing tools using task completion rates and user satisfaction scores

### Open Question 2
- Question: How can LLM semantic search tools be refined to better handle domain-specific acronyms or fragmented natural language queries?
- Basis: Tool showed "low comprehension" for acronym 'pdv' and failed to return results for "topic about education"
- Why unresolved: LLMs may struggle with obscure acronyms lacking semantic richness or with query structures lacking clear semantic coherence
- What evidence would resolve it: Benchmarking performance on targeted test set of domain-specific acronyms and complex queries before and after fine-tuning

### Open Question 3
- Question: To what extent are datasets missed by the LLM (but found via keywords) relevant false negatives?
- Basis: LLM missed ~45% of keyword results, but missed results assessed only by semantic similarity, not utility
- Why unresolved: Unclear if LLM's low-relevance filter inadvertently excludes viable data users expect to find
- What evidence would resolve it: Qualitative expert review of specific datasets returned by keyword but excluded by LLM to determine practical relevance

## Limitations
- Exact LLM/embedding model variant not specified, making reproduction difficult
- Geographic reasoning capability lacks systematic validation beyond observational evidence
- Single data source (CDRC) with potentially limited catalog diversity raises generalizability questions
- Study doesn't test hybrid keyword+semantic approaches that might combine strengths

## Confidence

- **High confidence**: Semantic search returns more results than keyword search (median 30 vs. 7) for tested query set
- **Medium confidence**: Semantic search handles misspellings and complex queries better, depending on embedding model quality
- **Medium confidence**: Geographic inference capabilities are promising but not systematically validated
- **Low confidence**: Claims about general superiority without hybrid approaches, as paper doesn't test combined methods

## Next Checks

1. **Geographic reasoning validation**: Create test suite of place-based queries with known correct regional relationships and measure precision of inferred geographic matches
2. **Cross-domain generalizability**: Replicate comparison using metadata from different social science data repository to assess results beyond CDRC
3. **Hybrid retrieval experiment**: Implement combined keyword+semantic approach using reciprocal rank fusion and measure whether recall improves without sacrificing precision on 131 test queries