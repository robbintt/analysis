---
ver: rpa2
title: Large Language Models Are Read/Write Policy-Makers for Simultaneous Generation
arxiv_id: '2501.00868'
source_url: https://arxiv.org/abs/2501.00868
tags:
- generation
- policy
- simultaneous
- translation
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel LLM-driven Simultaneous Generation
  (LSG) framework, which allows the off-the-shelf LLM to decide the generation timing
  and produce output concurrently. Specifically, LSG selects the generation policy
  that minimizes latency as the baseline policy.
---

# Large Language Models Are Read/Write Policy-Makers for Simultaneous Generation

## Quick Facts
- **arXiv ID:** 2501.00868
- **Source URL:** https://arxiv.org/abs/2501.00868
- **Reference count:** 32
- **Primary result:** LSG achieves SOTA simultaneous generation using open-source LLMs across translation and ASR tasks.

## Executive Summary
This paper proposes the LSG (LLM-driven Simultaneous Generation) framework that enables off-the-shelf LLMs to act as policy-makers for simultaneous generation tasks. The framework allows the LLM to decide when to read more input versus when to write output, effectively balancing latency and generation quality. LSG uses a wait-1 policy as baseline and computes KL divergence between current and baseline distributions to make READ/WRITE decisions. Experiments on simultaneous translation and streaming ASR show state-of-the-art performance using open-source LLMs.

## Method Summary
LSG transforms LLMs into policy-makers that decide generation timing for simultaneous tasks. At each step, the framework computes two probability distributions: one from the current input (p(yi|x≤j, y<i)) and one from the baseline input (p(yi|x≤i, y<i)). A WRITE action is triggered when the KL divergence between these distributions exceeds threshold δ or when the maximum probability exceeds confidence threshold α. The method operates under a range constraint [L+i-1, L+i-1+U]. For translation, Llama2-7B-chat is fine-tuned with LoRA on 100k samples; for speech tasks, Qwen-Audio is used. Inference uses greedy search with dataset-specific hyperparameters for δ and α.

## Key Results
- LSG achieves SOTA performance on simultaneous text-to-text and speech-to-text translation using open-source LLMs
- The framework effectively balances latency (measured by Average Lagging) and quality (measured by BLEU/WER)
- Wait-1 baseline policy combined with LLM policy-making outperforms specialized simultaneous translation systems

## Why This Works (Mechanism)
The framework works by leveraging LLMs' understanding of language structure and context to make intelligent READ/WRITE decisions. Instead of using fixed policies, LSG allows the LLM to assess whether the current input provides enough information for confident generation by comparing probability distributions across different read positions. This adaptive approach enables better latency-quality trade-offs than static policies.

## Foundational Learning

**Simultaneous Generation:** Real-time processing where decisions to read or write must be made at each step. Needed to understand the streaming nature of the task. Quick check: Can you explain why fixed wait-k policies might struggle with language pairs having different word orders?

**KL Divergence in Policy Making:** Measures the difference between probability distributions from current and baseline inputs. Needed to quantify how much new information the LLM gains from reading more input. Quick check: Can you compute KL divergence between two small probability distributions?

**Range Constraint [L, U]:** Limits how far ahead or behind the policy can deviate from baseline. Needed to prevent the policy from making extreme decisions that could harm quality. Quick check: What happens to translation quality if U is set to 0?

## Architecture Onboarding

**Component Map:** Input Stream -> LLM Policy-Maker -> READ/WRITE Decision -> Output Stream

**Critical Path:** At each generation step: (1) Get current distribution from LLM, (2) Get baseline distribution, (3) Compute KL divergence, (4) Compare against thresholds, (5) Execute READ or WRITE action

**Design Tradeoffs:** Using wait-1 as baseline provides simple comparison but may not be optimal for all language pairs. The dual-distribution computation doubles inference workload but enables better decision-making.

**Failure Signatures:** Excessive latency indicates thresholds are too conservative (reduce δ or increase α). Poor translation quality suggests range constraints are too narrow (increase U). Inconsistent policies may result from improper prompt formatting.

**3 First Experiments:**
1. Implement KL divergence computation between two probability distributions from a pre-trained LLM
2. Test READ/WRITE decision logic on a single sentence with fixed thresholds
3. Compare policy quality using eflomal alignments against wait-1 baseline

## Open Questions the Paper Calls Out

**Open Question 1:** Is the fixed wait-1 baseline policy optimal for all language directions, or could LSG benefit from dynamic or language-specific baseline policies? The authors acknowledge that generation states are influenced by language characteristics and word reordering, but only evaluate with wait-1 policy.

**Open Question 2:** Can the decision thresholds for KL divergence (δ) and confidence (α) be made adaptive to input complexity rather than requiring fixed dataset-specific tuning? The reliance on manually tuned hyperparameters suggests the policy-making logic may not generalize without careful calibration.

**Open Question 3:** Does the dual-distribution calculation required for policy-making impose a computational bottleneck that limits scalability in strict real-time applications? While computation-aware latency is reported, the paper doesn't isolate the overhead of the policy-making module itself.

## Limitations

- LoRA fine-tuning procedure uses 100k samples without specifying exact data composition or sampling methodology
- Implementation details for obtaining probability distributions from LLMs (logits extraction, vocabulary alignment) are underspecified
- Prompt engineering for streaming scenarios lacks detailed specification of prompt structure variations

## Confidence

**High Confidence:** The core algorithmic framework using LLMs as policy-makers is sound and well-specified. The evaluation methodology using AL and BLEU/WER metrics is standard and appropriate.

**Medium Confidence:** Translation quality improvements over baseline policies are convincing, but the exact contribution of LLM-based policy versus fine-tuning procedure is difficult to isolate.

**Low Confidence:** Absolute performance numbers are hard to verify without access to exact fine-tuning data and implementation details. The interaction between confidence threshold (α) and KL divergence threshold (δ) is complex and may require extensive hyperparameter tuning.

## Next Checks

1. Implement and test the full inference pipeline on a small subset of WMT15 De⇒En data to verify KL divergence computation and policy execution logic.

2. Conduct an ablation study on the range constraint [L, U] by systematically varying these parameters and measuring both sufficiency rate and translation quality.

3. Replicate the LoRA fine-tuning procedure using publicly available parallel data to create 100k training samples and experiment with different sampling strategies.