---
ver: rpa2
title: Thompson Sampling in Online RLHF with General Function Approximation
arxiv_id: '2505.23927'
source_url: https://arxiv.org/abs/2505.23927
tags:
- function
- regret
- rlhf
- learning
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a model-free Thompson sampling algorithm
  for online reinforcement learning from human feedback (RLHF) with general function
  approximation. The algorithm addresses the challenge of learning optimal policies
  using preference feedback rather than explicit reward signals, which is particularly
  relevant for aligning large language models with human preferences.
---

# Thompson Sampling in Online RLHF with General Function Approximation

## Quick Facts
- arXiv ID: 2505.23927
- Source URL: https://arxiv.org/abs/2505.23927
- Authors: Songtao Feng; Jie Fu
- Reference count: 40
- Primary result: First Thompson sampling algorithm for online RLHF with general function approximation achieving O(√T) regret

## Executive Summary
This paper introduces a model-free Thompson sampling algorithm for online reinforcement learning from human feedback (RLHF) with general function approximation. The algorithm learns optimal policies using preference feedback rather than explicit reward signals, which is particularly relevant for aligning large language models with human preferences. The key innovation lies in applying posterior sampling to RLHF through maximum likelihood estimation, enabling tractable exploration in the general function approximation setting. The authors establish the first eluder-type regret bound for Thompson sampling in RLHF, achieving O(√T) regret with multiplicative factors depending on horizon, Bellman eluder dimension, and log-bracketing number.

## Method Summary
The method implements model-free Thompson sampling for online RLHF by maintaining a posterior distribution over Q-functions using Bayes' rule. At each episode, the algorithm samples a Q-function from the posterior, computes a greedy policy, and executes it to generate trajectories. Preference feedback between trajectory pairs updates the posterior. The key theoretical contribution is showing that posterior sampling achieves similar regret guarantees to confidence-set methods using Bellman eluder dimension. The analysis requires completing the function class to ensure completeness under posterior sampling and establishing concentration-type inequalities for squared Bellman error using MLE generalization bounds.

## Key Results
- Establishes O(√T) regret bound with multiplicative factors depending on horizon H, Bellman eluder dimension d, and log-bracketing number
- First eluder-type regret bound for Thompson sampling in RLHF setting
- Demonstrates that using trajectories from different policies balances exploration and exploitation
- Completes function class to ensure completeness under posterior sampling
- Establishes concentration-type inequality for squared Bellman error using MLE generalization bounds

## Why This Works (Mechanism)

### Mechanism 1: Posterior Sampling Enables Thompson-Style Exploration
Posterior sampling provides principled exploration by sampling Q-functions from the posterior distribution rather than selecting optimistic upper bounds. The algorithm constructs a posterior over Q-functions using Bayes' rule, combining a prior with likelihood from preference observations. At each episode, a Q-function is sampled from this posterior, and a greedy policy is computed. The key insight is that sampled Q-functions and the true Q-function are identically distributed given history, enabling Bayesian regret analysis. This requires a differentiable link function Φ satisfying κ bounds and generalized completeness for the completed function class.

### Mechanism 2: MLE Generalization Bounds Yield Concentration of Squared Bellman Error
The posterior sampling procedure implicitly performs MLE, enabling concentration bounds on squared Bellman error. The paper establishes a confidence set such that with high probability, the true Q-function is contained within it and cumulative squared Bellman error is bounded. The concentration bound depends on the log-bracketing number of the function class. This requires the function class to be parameterized such that MLE generalization bounds apply, connecting preference observations to Bellman error concentration.

### Mechanism 3: Distributional Eluder Dimension Bounds Error Accumulation via Pigeonhole Principle
Low Bellman eluder dimension ensures cumulative Bellman error scales as O(√(d·β·T)) rather than O(T). The distributional eluder dimension captures how "independent" new state-action distributions are from past observations. Applied to Bellman residuals, this bounds how Bellman errors can accumulate across episodes. The function class must have low Bellman eluder dimension, which is established for linear models but remains open for general neural networks.

## Foundational Learning

- Concept: **Thompson Sampling for Bandits**
  - Why needed here: Extends classical Thompson sampling from bandits to full RLHF with function approximation
  - Quick check question: Given a 3-armed bandit with posterior means [0.3, 0.5, 0.4] and variances [0.1, 0.01, 0.1], which arm is Thompson sampling more likely to select than UCB?

- Concept: **Bellman Equations and Value Function Decomposition**
  - Why needed here: Core analysis relies on decomposing regret into cumulative Bellman errors via policy loss decomposition
  - Quick check question: For a 2-step MDP with Q*(s₁,a₁)=0.5, Q*(s₂,a₂)=1.0, and T₁Q*(s₂)(s₁,a₁) = r(s₁,a₁) + E[V*(s₂)], what must be true about the reward at (s₁,a₁)?

- Concept: **Bracketing and Covering Numbers**
  - Why needed here: Regret bound explicitly depends on log-bracketing number through β_G
  - Quick check question: For a class of bounded 1-Lipschitz functions on [0,1] with ||f||∞ ≤ 1, how does the covering number scale with precision ε?

## Architecture Onboarding

- Component map: Preference Data -> Likelihood Computation -> Posterior Update -> Posterior Sampling -> Planning Oracle -> Policy Pair Selection -> Environment Rollout

- Critical path:
  1. **Posterior computation** - Computational bottleneck requiring variational inference approximation
  2. **Function class completion** - Must complete F to G for generalized completeness
  3. **Transition kernel estimation** - Adds Õ(√(1/t)) error if kernel is not known

- Design tradeoffs:
  - Exact vs. approximate posterior: Exact has theoretical guarantees but may be intractable
  - Model-free vs. model-based: Model-free avoids transition dynamics estimation but may be less sample-efficient
  - Policy pair selection: Using previous policy as comparator balances exploration and exploitation

- Failure signatures:
  - Regret not decreasing: Check preference noise κ, verify function class BE dimension
  - Posterior collapse: Monitor posterior entropy for exploration cessation
  - Bias in MLE: Verify transition kernel estimation doesn't corrupt reward computation
  - Completeness violation: Monitor bracketing number growth

- First 3 experiments:
  1. **Tabular validation**: Implement on small MDP (|S|≤10, |A|≤3) with known BE dimension, verify O(√T) regret scaling using exact posterior
  2. **Linear function approximation stress test**: Use linear Q-functions, systematically vary dimension d̃, preference noise κ, and horizon H, verify scaling O(H√(d̃·κ²·log(T)·T))
  3. **Approximate posterior comparison**: Compare variational inference vs. exact posterior in tabular setting, quantify computational speedup and regret degradation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do known tractable RL function classes remain tractable after undergoing the completion procedure required for posterior sampling?
- Basis in paper: Page 7 states: "We leave as our future work whether known tractable classes after completion are still tractable."
- Why unresolved: The completion process expands function class F to G to accommodate posteriors, potentially altering complexity measures like Bellman eluder dimension.
- What evidence would resolve it: A theoretical proof demonstrating that Bellman eluder dimension of completed class G remains bounded by original class F dimension for standard model classes.

### Open Question 2
- Question: Can the Thompson sampling algorithm maintain regret guarantees if the generalized completeness assumption is violated?
- Basis in paper: Page 7 notes: "It is also worth exploring the case when generalized completeness is violated, and we leave this as our future work."
- Why unresolved: Current analysis relies heavily on closedness of function class under Bellman operator to bound estimation errors.
- What evidence would resolve it: Deriving regret bound under relaxed assumptions or proposing modified algorithm handling incomplete function classes.

### Open Question 3
- Question: Can approximate posterior sampling methods provide same O(√T) regret guarantees as exact algorithm?
- Basis in paper: Page 8 acknowledges while approximation methods exist, they currently lack theoretical guarantees.
- Why unresolved: Regret proof assumes exact sampling from posterior distribution, which is often computationally intractable for general function approximation.
- What evidence would resolve it: Theoretical analysis establishing that specific approximate inference techniques satisfy concentration inequalities required by proof.

## Limitations

- Bellman eluder dimension analysis only established for linear function classes, remains open for neural networks
- Log-bracketing number dependence creates potential computational intractability in high-dimensional settings
- Transition kernel P_h assumed known, though estimation adds only Õ(√(1/t)) error

## Confidence

- High confidence: Posterior sampling mechanism and connection to Thompson sampling is well-established
- Medium confidence: MLE generalization bounds yielding concentration of squared Bellman error is novel but relies on bracketing number assumptions
- Medium confidence: Distributional eluder dimension analysis provides elegant theory but practical implications for neural networks unclear

## Next Checks

1. **BE dimension verification**: Implement with linear function approximation, verify regret scales as O(H√(d̃·log(T)·T)) by systematically varying dimension d̃
2. **Approximation robustness**: Compare exact posterior sampling versus variational inference approximation in tabular settings to quantify regret degradation
3. **Noise sensitivity**: Systematically vary preference noise κ to empirically validate polynomial dependence of regret bounds on κ⁻¹