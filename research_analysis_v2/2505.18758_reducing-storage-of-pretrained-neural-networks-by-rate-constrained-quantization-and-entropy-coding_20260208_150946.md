---
ver: rpa2
title: Reducing Storage of Pretrained Neural Networks by Rate-Constrained Quantization
  and Entropy Coding
arxiv_id: '2505.18758'
source_url: https://arxiv.org/abs/2505.18758
tags:
- compression
- equation
- quantization
- weight
- entropy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CERWU, a novel post-training compression
  framework that combines rate-aware quantization with entropy coding. The method
  extends layer-wise loss by adding a quadratic rate estimation and provides locally
  exact solutions using the Optimal Brain Surgeon framework.
---

# Reducing Storage of Pretrained Neural Networks by Rate-Constrained Quantization and Entropy Coding

## Quick Facts
- arXiv ID: 2505.18758
- Source URL: https://arxiv.org/abs/2505.18758
- Authors: Alexander Conzelmann; Robert Bamler
- Reference count: 40
- One-line result: CERWU achieves 20-40% bit rate reduction over NNCodec while maintaining accuracy and enabling sub-1-bit weights through entropy coding

## Executive Summary
This paper introduces CERWU, a post-training compression framework that combines rate-aware quantization with entropy coding to reduce neural network storage requirements. The method extends layer-wise loss by adding a quadratic rate estimation and provides locally exact solutions using the Optimal Brain Surgeon framework. CERWU achieves superior compression performance compared to existing methods, particularly excelling at producing compressible quantized weights that can be encoded at fractional bit rates below 1 bit per weight.

## Method Summary
CERWU compresses neural networks through a rate-distortion optimization that balances reconstruction error against bit rate during quantization. The algorithm computes layer-wise Hessians from activation data, then iteratively quantizes weights while updating unquantized weights in the same row using entropy-regularized OBS updates. After quantization, DeepCABAC entropy coding is applied to achieve sub-1-bit per weight rates. The method supports arbitrary quantization grids and maintains extremely fast decoding, making it particularly effective for resource-constrained devices.

## Key Results
- Achieves 20-40% decrease in bit rate at same performance level as NNCodec
- Maintains extremely fast decoding while enabling fractional bit rates below 1 bit per weight
- Superior compression performance demonstrated on ResNet, VGG16, and MobileNetV3 architectures
- Entropy coding allows individual weights to contribute less than 1 bit to total bit rate through amortization

## Why This Works (Mechanism)

### Mechanism 1
Adding a quadratic rate estimation to the layer-wise loss produces quantized weights that are more compressible under entropy coding. The objective L_λ(Ŵ) = ||WX - ŴX||²_2 + λR(Ŵ) balances reconstruction error against bit rate, with the rate term R(Ŵ) = -log₂ P_Ŵ(Ŵ) approximated quadratically as λγ/2||Ŵ||²_2. This penalizes weights that would require more bits to encode. Core assumption: Gaussian approximation to entropy model is sufficiently accurate. Break condition: If γ→0 or λ→0, rate awareness disappears.

### Mechanism 2
Entropy-regularized OBS updates optimally compensate for quantization error while maintaining compressibility of remaining weights. After quantizing W'_ij → Ŵ_ij, unquantized weights in the same row are updated via ΔW'_{i,>j} = -(W'_ij - Ŵ_ij)/C'_jj · C'_{j,>j}. The entropy-regularized Hessian H' = H + λγI prevents weight magnitude growth that would hurt compressibility. Core assumption: Quadratic approximation L'_λ dominates second-order effects on non-quadratic rate term. Break condition: If Hessian is poorly conditioned or calibration set is unrepresentative.

### Mechanism 3
Entropy coding with autoregressive models enables sub-1-bit per weight amortized bit rates. Each weight's contribution to total bit rate is -log₂ P_Ŵ(Ŵ_ij | Ŵ_{<(ij)}), which can be fractional. Arithmetic/range/ANS coding approaches this theoretical limit within <1 bit overhead. Aggregating many weights allows individual contributions below 1 bit. Core assumption: Autoregressive entropy model accurately captures weight statistics. Break condition: If entropy model is mismatched to weight distribution, actual bit rates exceed predictions.

## Foundational Learning

- Concept: **Rate-distortion theory and Lagrangian optimization**
  - Why needed here: The entire method hinges on minimizing D + λR tradeoffs; understanding what λ controls is essential
  - Quick check question: If you increase λ by 10x, would you expect bit rate to increase, decrease, or stay the same? Why?

- Concept: **Optimal Brain Surgeon (OBS) and inverse Hessian updates**
  - Why needed here: CERWU's weight updates derive from OBS; understanding why only same-row weights are updated requires knowing how the Hessian factorizes
  - Quick check question: Why does OBS use the inverse Hessian rather than the gradient for weight updates?

- Concept: **Entropy coding fundamentals (information content, arithmetic/ANS coding)**
  - Why needed here: The "fractional bits" claim only makes sense if you understand that entropy coders pack symbols together, making per-symbol bit allocations amortized rather than literal
  - Quick check question: Can a single weight ever literally occupy 0.3 bits on disk? Explain.

## Architecture Onboarding

- Component map: Calibration pass -> Hessian computation -> Cholesky decomposition -> Iterative quantization loop -> Entropy coding
- Critical path: Hessian computation (O(m³)) dominates for small grids; grid search + entropy model calls (O(nmk)) dominates for large grids
- Design tradeoffs:
  - λ: Higher → more compression, more accuracy loss; sweep logarithmically (10⁻⁸ to 10⁻¹)
  - Grid size k: Larger → more faithful quantization, slower encoding; k ≤ 31 typically sufficient
  - Scan order: Row-major vs. column-major; minor performance difference, try both
  - Entropy model: DeepCABAC is default; other models possible but not tested
- Failure signatures:
  - Accuracy collapse at moderate λ: Hessian may be poorly estimated; increase calibration set size
  - No compression improvement vs. RTN+EC: Likely λ too small or γ incorrectly computed
  - Encoding extremely slow for large grids: Expected; consider early stopping in grid search or smaller k
  - Transformer/LLM underperformance: Paper explicitly notes this regime is not targeted
- First 3 experiments:
  1. Baseline comparison on ResNet-18/ImageNet: Run CERWU, CERWU-γ=0, CERWU-λ=0, NNCodec, RTN+EC with λ sweep and k∈{16,32,64}. Plot rate-accuracy Pareto front.
  2. Ablation on calibration set size: Repeat (1) with calibration sets of {1k, 5k, 20k, 40k} images. Characterize Hessian stability and accuracy variance.
  3. Scan order sensitivity: For MobileNetV3-small, compare row-major vs. column-major scan order at fixed λ and k. Quantify bit rate and accuracy differences.

## Open Questions the Paper Calls Out

### Open Question 1
Can the CERWU framework be effectively adapted for transformer-based architectures and Large Language Models (LLMs) despite observed performance degradation in preliminary tests? The authors explicitly state in the Limitations section: "we do not include an analysis for transformer-based models" and note that "quick experiments... indicate worse performance." This remains unresolved because the method is currently validated only on computer vision CNNs, while transformers exhibit different weight distributions that may require specific entropy models not yet tested.

### Open Question 2
Can the computational complexity of the encoding step be reduced through grid search optimization techniques like early stopping? The paper notes that encoding runtimes are higher than baselines and suggests "further work to speed up the grid search might be helpful (such as introducing early stopping)." While the authors provide a complexity analysis, they treat the optimization of this search process as future work rather than implementing it in the current method.

### Open Question 3
Can CERWU-compressed representations be decoded on-the-fly on a GPU to effectively reduce RAM-to-GPU bandwidth bottlenecks? The Conclusion suggests that "given a suitable entropy model, our method can also produce compressible representations that could be decoded on-the-fly on a GPU... [to] reduce the required communication bandwidth." The current work focuses on storage reduction and CPU-based decoding; the proposed application of streaming decompression to alleviate memory bandwidth limits is theoretical and untested.

## Limitations

- Performance on transformer-based architectures and LLMs has not been validated and preliminary results indicate worse performance
- Encoding step computational complexity can be high for large quantization grids, requiring potential optimization
- DeepCABAC entropy model details are underspecified, potentially affecting reproducibility and performance with alternative entropy coders

## Confidence

- High: CERWU's mathematical framework (OBS + quadratic rate estimation) is sound and reproducible
- Medium: 20-40% bit rate reduction claim is empirically supported but depends on entropy coder implementation
- Medium: Sub-1-bit per weight rates are theoretically valid but practically contingent on entropy model quality
- Low: Generalization to non-vision architectures and datasets lacks empirical validation

## Next Checks

1. **Entropy coder ablation**: Replace DeepCABAC with generic arithmetic coding and measure bit rate degradation; determine if claimed compression gains are entropy coder-specific
2. **Architecture stress test**: Apply CERWU to ViT or BERT-small; measure if 20-40% reduction holds or if architectural peculiarities (layernorm, attention) break the mechanism
3. **Out-of-distribution calibration**: Calibrate CERWU on CIFAR10 then evaluate on a different vision dataset (e.g., STL-10); quantify Hessian stability and compression robustness to calibration set mismatch