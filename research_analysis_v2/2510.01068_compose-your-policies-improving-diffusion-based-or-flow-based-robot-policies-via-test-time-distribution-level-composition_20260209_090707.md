---
ver: rpa2
title: Compose Your Policies! Improving Diffusion-based or Flow-based Robot Policies
  via Test-time Distribution-level Composition
arxiv_id: '2510.01068'
source_url: https://arxiv.org/abs/2510.01068
tags:
- diffusion
- arxiv
- policy
- preprint
- policies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel method to improve diffusion-based
  and flow-based robot policies through test-time distribution-level composition,
  without requiring additional model training. The key insight is that convex combinations
  of distributional scores from multiple pre-trained policies can yield a provably
  superior functional objective, which translates into better trajectory sampling
  through stable dynamics.
---

# Compose Your Policies! Improving Diffusion-based or Flow-based Robot Policies via Test-time Distribution-level Composition

## Quick Facts
- arXiv ID: 2510.01068
- Source URL: https://arxiv.org/abs/2510.01068
- Reference count: 40
- Key outcome: Improves diffusion-based and flow-based robot policies through test-time distribution-level composition without additional training

## Executive Summary
This paper introduces General Policy Composition (GPC), a training-free framework that improves robot policies by composing pre-trained diffusion or flow-based policies at test time. The key insight is that convex combinations of distributional scores from multiple policies can yield provably superior functional objectives. Through extensive experiments on Robomimic, PushT, RoboTwin, and real-world tasks, GPC demonstrates consistent performance gains of up to +7.55% in simulation and +10% in real-world tasks, validating its effectiveness across different modalities, architectures, and VLA/VA settings.

## Method Summary
GPC works by combining score functions from pre-trained policies through convex combinations at test time, followed by discrete weight search to find optimal task-specific compositions. The method requires only compatible action spaces between policies and operates without additional training. During inference, GPC evaluates candidate weights on validation rollouts, selects the optimal weight configuration based on success rates, and deploys the composed policy for the task. The framework supports both diffusion-based and flow-based models, with implementation details provided for converting between different prediction types (noise, sample, or velocity predictions) to a unified score representation.

## Key Results
- Average improvement of +7.55% in simulation tasks and +10% in real-world tasks
- Consistently outperforms single policies across Robomimic, PushT, and RoboTwin benchmarks
- Effective across different modalities (RGB, point cloud), architectures (CNN, Transformer), and VLA/VA settings
- Logical AND/OR operators can yield even larger gains (+25.73%) but require per-step weight recomputation

## Why This Works (Mechanism)

### Mechanism 1: Single-Step Score Error Reduction via Convex Combination
A convex combination of two score estimators can achieve strictly lower mean-squared error than either individual estimator. Each score estimator deviates from the true score in different ways, and convex combination cancels uncorrelated errors. The error function Q(w) is convex quadratic in weight w, guaranteeing an interior minimizer exists when errors are not perfectly aligned.

### Mechanism 2: Stable Error Propagation Through Sampling Dynamics
Single-step score error reductions propagate to trajectory-level improvements through stable sampling dynamics. Using Grönwall's inequality, trajectory deviation is bounded by the integrated score error multiplied by an exponential factor determined by Lipschitz constants. Reducing per-step score error directly tightens this bound.

### Mechanism 3: Task-Dependent Weight Search at Test Time
Optimal composition weights are task-dependent and can be found via discrete search at inference time. The paper searches over weights w ∈ {0.0, 0.1, ..., 0.9, 1.0}, evaluating success rates to find w*. This adapts the policy mixture to the specific task distribution rather than using fixed weights.

## Foundational Learning

- **Concept: Score Functions and Diffusion/Flow Matching**
  - Why needed here: GPC operates by composing score functions ∇_τ log p(τ) from different policies. Understanding how scores relate to noise prediction (ε-prediction), data prediction (x₀-prediction), and v-prediction is essential (Appendix G).
  - Quick check question: Given a diffusion model outputting predicted noise ε_θ(τ_t, t), how do you convert it to a score s_θ(τ_t, t)? (Answer: s = -ε/σ_t)

- **Concept: Convex Optimization Basics**
  - Why needed here: The core theoretical result depends on the convexity of Q(w) as a quadratic function. Understanding why convexity guarantees a unique minimizer helps interpret Proposition 4.1.
  - Quick check question: For Q(w) = Aw² + Bw + C with A > 0, what is the minimizer? (Answer: w* = -B/(2A))

- **Concept: Grönwall's Inequality**
  - Why needed here: This mathematical tool bounds how errors grow over time in differential equations. It's the bridge from single-step improvement to trajectory-level guarantees.
  - Quick check question: For ϕ'(t) ≤ L(t)ϕ(t) + g(t) with ϕ(0) = 0, what does Grönwall tell us about ϕ(T)? (Answer: ϕ(T) ≤ ∫₀^T exp(∫_t^T L(τ)dτ) g(t) dt)

## Architecture Onboarding

- **Component map:** Pre-trained policies -> Score extractor -> Score composer -> Denoiser -> Weight search module
- **Critical path:**
  1. Load ≥2 pre-trained policies with compatible action spaces
  2. Align prediction types (convert to score or noise space) via Appendix G formulas
  3. For each candidate weight w ∈ grid:
     - Run full denoising with composed scores
     - Execute trajectory, record success/failure
  4. Select w* = argmax_w R(w)
  5. Deploy with fixed w* for task

- **Design tradeoffs:**
  - Grid granularity vs. search cost: Finer grid (0.01 steps) improves weight accuracy but multiplies evaluation cost. Paper uses 0.1 steps as practical default.
  - Logical AND/OR vs. convex combination: AND/OR operators (Section 5.3) can yield larger gains (+25.73% in Table 4) but require per-step weight recomputation and don't apply to flow-based models.
  - Number of policies: Dual-policy composition is studied; scaling to N policies increases compute O(N) per denoising step and O(grid^N) search space.

- **Failure signatures:**
  - No improvement over best parent: Check if policies have highly correlated errors or one is much weaker. Try filtering to only combine policies with comparable performance.
  - Optimal weight at endpoint (w=0 or w=1): Indicates one policy dominates; composition provides no benefit for this task.
  - Inconsistent weights across similar tasks: May indicate evaluation noise; increase rollouts per weight setting.

- **First 3 experiments:**
  1. Sanity check with identical policies: Compose a policy with itself at w=0.5. Should recover same performance (verifies composition logic doesn't introduce bugs).
  2. Cross-modality composition: Combine DP_img (RGB-based) with DP_pcd (point cloud) on RoboTwin tasks. Compare optimal weights across tasks to validate task-dependence claim.
  3. Architecture heterogeneity: Compose VLA (RDT) with VA (DP3) to test if language grounding and pure vision policies provide complementary signals. Analyze failure modes to understand when language helps vs. hurts.

## Open Questions the Paper Calls Out

### Open Question 1
How can the framework move beyond fixed discretization to enable adaptive or automatic weight searching?
The authors state that test-time weight search is currently restricted by fixed discretization and suggest exploring adaptive or automatic search strategies as future work. Fixed grid search may miss optimal weight values, and manual tuning does not scale across diverse tasks.

### Open Question 2
How can the method scale efficiently from dual-policy to multi-policy composition?
The discussion notes that increasing the number of policies increases computation, suggesting feature sharing or compact representations as potential solutions. The current experimental focus is on dual-policy composition; naive extension to N policies creates a high-dimensional search space.

### Open Question 3
Can superposition-based composition operators be effectively applied to flow-based models?
The analysis of superposition notes it has clear limitations and is "not directly applicable to flow-based models." Logical AND/OR operators rely on probabilistic formulations inherent to diffusion models, which differ from flow-matching dynamics.

## Limitations
- Theoretical guarantees about trajectory-level improvements lack direct experimental validation
- Test-time weight search requires evaluating multiple weight configurations, increasing inference time
- Performance gain depends on having multiple complementary policies; composition may not help when one policy dominates or errors are highly correlated

## Confidence

- **High Confidence:** The core empirical results showing performance improvements (up to +7.55% in simulation, +10% in real-world) are well-supported by experimental data across multiple benchmarks.
- **Medium Confidence:** The claim that single-step improvements propagate to trajectory-level gains relies on the Grönwall inequality bound, which is mathematically sound but has not been rigorously validated against actual trajectory error propagation in experiments.
- **Low Confidence:** The generality claim that GPC works "across different modalities, architectures, and VLA/VA settings" is supported by composition examples but lacks systematic ablation studies showing which architectural differences matter most for composition effectiveness.

## Next Checks

1. **Lipschitz Constant Verification:** Measure the actual Lipschitz constants of both the dynamics F(t, x, s) and score estimators s_θ across the trajectory space for a representative task. Compare these measured values against the theoretical bounds assumed in Proposition 4.2.

2. **Weight Search Granularity Study:** Systematically vary the weight search grid (0.05 vs 0.1 vs 0.01 steps) on 3-5 tasks and quantify the performance difference. This would validate whether the 0.1 step grid is sufficient or if finer grids could yield additional gains.

3. **Error Correlation Analysis:** For pairs of policies where composition fails to improve performance, measure the correlation between their score estimation errors across trajectories. This would empirically validate the assumption that composition works best when errors are uncorrelated.