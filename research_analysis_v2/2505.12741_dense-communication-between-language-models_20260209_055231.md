---
ver: rpa2
title: Dense Communication between Language Models
arxiv_id: '2505.12741'
source_url: https://arxiv.org/abs/2505.12741
tags:
- lmnet
- llms
- language
- training
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LMNet, a novel paradigm for building collective
  intelligence by enabling direct dense vector communication between LLMs. Unlike
  existing methods that rely on natural language communication, LMNet strips the embedding
  and de-embedding layers from pre-trained transformers, allowing them to communicate
  directly via dense vectors.
---

# Dense Communication between Language Models

## Quick Facts
- arXiv ID: 2505.12741
- Source URL: https://arxiv.org/abs/2505.12741
- Reference count: 40
- Primary result: LMNet-1B achieves competitive performance with existing 1B-scale LLMs using <0.1% of typical training cost

## Executive Summary
This paper introduces LMNet, a novel paradigm for building collective intelligence by enabling direct dense vector communication between LLMs. Unlike existing methods that rely on natural language communication, LMNet strips the embedding and de-embedding layers from pre-trained transformers, allowing them to communicate directly via dense vectors. The system connects these stripped transformers with trainable sequence-to-sequence modules, forming a network similar in structure to MLPs.

As a case study, the authors construct and train LMNet-1B using 0.5B parameters from a pre-trained Qwen2.5-0.5B as vertex models and 0.6B randomly initialized parameters for communication edges. Trained on public datasets with less than 0.01T tokens, LMNet-1B achieves performance competitive with or superior to existing open-source LLMs of similar size, while requiring less than 0.1% of the typical training cost.

## Method Summary
LMNet connects pre-trained LLMs by removing their embedding and de-embedding layers and inserting trainable sequence-to-sequence edge modules between them. Each vertex is a stripped transformer backbone, and edges are single-layer attention blocks. The architecture forms a layer-wise fully connected network with summation aggregation. The system is trained end-to-end with an auto-regressive loss, first freezing vertex parameters then unfreezing all components. For LMNet-1B, the authors use a Qwen2.5-0.5B backbone shared across all vertices, with 1→4→4→4→1 vertices per layer and total parameters of 1.14B.

## Key Results
- LMNet-1B achieves 53.9% accuracy on MMLU compared to 44.3% for base Qwen2.5-0.5B and 32.2% for Llama3.2-1B
- The system requires less than 0.1% of typical training cost while achieving competitive performance
- LMNet demonstrates significant performance improvements for customizing LLMs with limited data compared to fine-tuning and other parameter-efficient methods
- Visualization of attention weights reveals that different edges pass distinct messages, suggesting the system learns diverse communication patterns

## Why This Works (Mechanism)

### Mechanism 1: Dense Vector Communication Bypasses Token Bottleneck
Removing embedding/de-embedding layers between LLMs enables richer information transfer than natural language mediation. Pre-trained transformers are stripped of their input embedding and output de-embedding layers. Instead of LLM₁ outputting tokens → LLM₂ re-embedding them, LLM₁ passes hidden states (dense vectors) directly to LLM₂ via trainable seq2seq edge modules. Dense vectors encode more task-relevant information per unit than discrete tokens; language is optimized for human communication, not machine-to-machine transfer.

### Mechanism 2: End-to-End Differentiability Enables Joint Optimization
The fully differentiable pathway allows gradient-based optimization across all vertices and edges simultaneously. With continuous dense communication, gradients flow from output loss through de-embedding → vertices → edges → embedding via chain rule. All parameters receive gradients and are trainable. Pre-trained vertex parameters contain useful knowledge that can be preserved or refined; edge parameters can be learned from limited data to route information appropriately.

### Mechanism 3: Layer-Wise Fully Connected Topology Supports Emergent Specialization
A multi-layer, fully connected structure allows edges to learn diverse communication patterns without collapsing to simpler topologies. L layers with nₗ vertices per layer; each vertex connects to all vertices in the next layer via independent edge modules. Aggregation uses summation. Different edges learn to pass different messages (observed via attention weight visualization).

## Foundational Learning

- **Transformer embedding/de-embedding layers**: Why needed here - LMNet explicitly removes these to enable dense communication; understanding what information is lost/gained is critical. Quick check: Can you explain why token → dense → token round-trip loses information compared to dense → dense direct transfer?

- **Sequence-to-sequence attention modules**: Why needed here - Each edge is a seq2seq module (implemented as 1-layer attention block); understanding attention is required to interpret edge behavior. Quick check: How does a single attention layer transform a sequence of dense vectors, and what inductive biases does it introduce?

- **Gradient flow through composed functions**: Why needed here - The core advantage of LMNet is differentiability; understanding chain rule through multiple transformers is essential for debugging training. Quick check: If gradients vanish at layer 3 of a 5-layer LMNet, what components would you suspect first?

## Architecture Onboarding

- **Component map**: Input vertex (keeps E₁¹) → Edges → Hidden vertices (stripped transformers) → Edges → Output vertex (keeps Dᴸ₁)

- **Critical path**: Token input → E₁¹ → dense Xin → For each layer l: Xᵢˡ → edges → sum → vertex vᵢˡ⁺¹ → Xᵢˡ⁺¹ → Final XL₁ → Dᴸ₁ → output tokens → logits → loss

- **Design tradeoffs**:
  - Width vs. depth: More layers = longer sequential compute; more vertices/layer = more parallelism but more edge parameters
  - Vertex sharing: Sharing one pre-trained LLM across all vertices is parameter-efficient but limits diversity; different LLMs per vertex increases expressiveness
  - Freeze vs. fine-tune vertices: Freezing vertices is parameter-efficient for limited data; full fine-tuning may improve performance but risks overfitting and catastrophic forgetting

- **Failure signatures**:
  - Edge collapse: All edges produce similar outputs → check attention weight diversity
  - Unintelligible intermediate representations: De-embedding intermediate dense vectors yields non-English → expected due to summation fusion, not a bug
  - Overfitting on small data: Performance degrades on test set → freeze vertices, train edges only

- **First 3 experiments**:
  1. **Minimal reproduction**: Build LMNet-1/2/1 (3 layers, 1-2-1 vertices) with a small pre-trained LLM (e.g., GPT-2 small). Train edges only on a single dataset (e.g., E2E). Verify loss decreases and outputs are coherent.
  2. **Ablation on topology**: Compare LMNet-1/1 (equivalent to deeper single model) vs. LMNet-1/2/1 vs. LMNet-1/4/1 on the same task. Measure whether wider layers improve performance.
  3. **Edge diversity check**: After training, visualize attention weights or parameter matrices across edges. Confirm edges are distinct from initialization and from each other; if uniform, investigate learning rate or initialization issues.

## Open Questions the Paper Calls Out

- **Alternative topologies**: How do alternative topologies, such as residual connections or message-passing graphs, affect the efficiency and emergent capabilities of LMNet? The current study only implements a layer-wise fully connected structure to prove the basic concept.

- **Intermediate communication interpretability**: Can the intermediate dense vector communications be decoded or interpreted back into human-understandable semantic concepts? The authors failed to de-embed intermediate vectors, finding they produced text that was "not even English."

- **Heterogeneous vertex specialization**: Does employing heterogeneous, specialized LLMs as vertices yield superior performance compared to the current weight-sharing setup? The authors utilized weight-sharing for parameter efficiency, leaving the integration of diverse model expertise unexplored.

## Limitations

- Scalability beyond 1B parameters is unclear - computational overhead of maintaining dense communication across many vertices could become prohibitive
- The paper doesn't address how LMNet would handle tasks requiring very long-range dependencies
- Dense vector communication efficiency compared to token-based communication is theoretically plausible but not empirically proven

## Confidence

- **High Confidence**: The core architectural innovation of stripping embedding/de-embedding layers and enabling direct dense vector communication is technically sound
- **Medium Confidence**: The claim that LMNet achieves competitive performance with <0.1% of typical training cost is supported by reported results
- **Medium Confidence**: The assertion that dense vector communication is inherently more efficient than token-based communication is theoretically plausible but not empirically proven

## Next Checks

1. **Scalability Test**: Construct LMNet with varying numbers of vertices per layer (e.g., 1→8→8→8→1 and 1→16→16→16→1) and measure whether performance improvements scale sublinearly with parameter count.

2. **Communication Efficiency Analysis**: Implement a token-based communication baseline where LLMs communicate via generated tokens and compare both training efficiency and downstream performance on identical tasks.

3. **Long-Range Dependency Test**: Evaluate LMNet on tasks specifically designed to test long-range context understanding (e.g., passkey-style tasks or book-length comprehension) to determine whether the dense communication introduces bottlenecks for tasks requiring extended context.