---
ver: rpa2
title: Data Augmentation for Deep Learning Regression Tasks by Machine Learning Models
arxiv_id: '2501.03654'
source_url: https://arxiv.org/abs/2501.03654
tags:
- data
- augmentation
- learning
- datasets
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes a data augmentation (DA) method for improving
  deep learning (DL) regression on tabular data by using machine learning (ML) models.
  The method generates synthetic data points with noise and uses an ML model to predict
  their labels, then combines these with the original training data to train a DL
  model.
---

# Data Augmentation for Deep Learning Regression Tasks by Machine Learning Models

## Quick Facts
- arXiv ID: 2501.03654
- Source URL: https://arxiv.org/abs/2501.03654
- Reference count: 40
- Primary result: Machine learning-based data augmentation improves deep learning regression on tabular data by over 10% average across 30 datasets.

## Executive Summary
This study proposes a data augmentation method that leverages machine learning models to improve deep learning regression performance on tabular data. The approach generates synthetic data points with noise and uses an ML model to predict their labels, then combines these with the original training data to train a DL model. Across 30 datasets and three AutoDL frameworks (AutoKeras, H2O, AutoGluon), the method achieved an average performance improvement of over 10% compared to baselines without augmentation.

## Method Summary
The method involves training an AutoML model (TPOT) on the original training data, then generating synthetic samples by adding Gaussian noise to the training features. The trained ML model predicts labels for these synthetic samples, creating an augmented dataset that combines original and synthetic data. This augmented dataset is then used to train a deep learning model. The approach assumes the ML model captures the underlying data-generating function better than the DL model can learn from limited data alone, enabling knowledge transfer through synthetic labeling.

## Key Results
- Average performance improvement of over 10% across 30 datasets compared to baselines without augmentation
- Outperformed existing DA techniques like C-Mixup and ADA
- Improvements most pronounced in smaller datasets but still significant in larger ones (3% gain at 50K training rows)
- Both augmentation and knowledge distillation from the ML model contributed to the gains

## Why This Works (Mechanism)

### Mechanism 1: Knowledge Distillation from ML to DL
A classical ML model (often tree-based) learns a robust approximation of the data-generating function. This teacher model labels synthetic data points, transferring its learned inductive biases to the DL student model, which regularizes the student and improves generalization beyond what the small original dataset would allow.

### Mechanism 2: Noise-Injected Synthetic Data for Regularization
Generating synthetic samples with noise expands the training set and forces the DL model to learn smoother decision boundaries. The per-feature Gaussian noise ensures perturbations are statistically consistent with the original data distribution, preventing unrealistic points while combating overfitting.

### Mechanism 3: AutoML-Selected Teacher Model
Using an AutoML framework to select and tune the teacher model ensures a strong labeler. This automates the search for the best performing classical model, directly linking the success of the augmentation to the state-of-the-art in classical ML for that dataset.

## Foundational Learning

- **Knowledge Distillation**: Why needed: This is the mechanism for transferring learned "knowledge" from a strong classical teacher to a DL student, enabling the student to generalize better. Quick check: Why might a student model learning from a teacher's soft predictions on synthetic data outperform a student trained only on a small set of ground-truth hard labels?

- **Inductive Bias of Tree vs. Neural Models**: Why needed: The method exploits the relative strength of tree-based models (sharp decision boundaries, better on heterogeneous features) to train neural networks (smooth function approximators) on tabular data. Quick check: What are the fundamental differences in how a Random Forest and a Multi-Layer Perceptron partition the feature space?

- **Data Augmentation for Regression**: Why needed: It is critical to understand that regression augmentation is non-trivial; unlike image classification, a perturbed input must map to a new output. This paper's solution is to learn the mapping function with an ML model. Quick check: Why is it insufficient to simply add noise to the input features of a regression dataset without also adjusting the target variable?

## Architecture Onboarding

- **Component map**: TPOT (AutoML) -> Synthetic Generator -> Oracle Labeler (TPOT predictions) -> Data Combiner -> AutoDL Trainer
- **Critical path**: The Oracle Labeler is the single point of failure. The quality of synthetic labels is paramount. Monitoring the teacher model's validation RMSE is the most important health metric.
- **Design tradeoffs**: Teacher complexity vs. speed (longer AutoML search may yield better teachers but delays training), augmentation volume (more data helps only if labels are accurate), noise scale (controls exploration vs. creating out-of-distribution samples).
- **Failure signatures**: Degraded student performance (strong signal of label noise from poor teacher), stagnant loss (potentially caused by conflicting synthetic labels), small gains on large data (benefits diminish as student has sufficient data).
- **First 3 experiments**: 1) Baseline & Oracle Check: Run AutoDL student on raw data and AutoML teacher separately to compare RMSE. 2) Ablation on Noise: Vary noise levels (η ∈ [0.01, 0.05, 0.1]) with fixed simple teacher. 3) Volume Scaling: Vary augmented rows (0.5x, 1x, 2x training size) to find diminishing returns.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the proposed ML-based labeling strategy be effectively adapted for non-tabular data domains such as computer vision, or does the "curse of dimensionality" in feature space prevent the teacher ML model from generating valid synthetic labels?
- **Open Question 2**: Is there a quantifiable performance threshold or "competency limit" for the AutoML teacher model below which the synthetic labels degrade the student DL model's performance beyond the baseline?
- **Open Question 3**: Does the simple Gaussian noise injection used to generate synthetic features limit the exploration of the decision boundary compared to deep generative models like Variational Autoencoders (VAEs)?

## Limitations
- Heavy dependence on quality of AutoML-selected teacher model, introducing variability across datasets
- Does not fully address categorical feature handling or provide details on TPOT's specific configuration
- Assumes TPOT's default settings are sufficient, which may not hold across all problem domains

## Confidence
- **High Confidence**: Empirical results showing average >10% improvement across 30 datasets are well-supported by methodology and statistical testing
- **Medium Confidence**: Knowledge distillation mechanism is logically sound and supported by ablation study, but corpus provides weak direct evidence for tabular regression
- **Low Confidence**: Robustness to different AutoML configurations and search times is not thoroughly explored

## Next Checks
1. **Teacher Model Validation**: Systematically evaluate TPOT-selected teacher model performance across all 30 datasets, comparing test RMSE to AutoDL baseline
2. **Ablation on Augmentation Volume**: Conduct detailed scaling study (V = 0.1x to 10x training size) on representative subset to identify diminishing returns point
3. **Noise Scale Sensitivity**: Perform grid search over η on datasets with varying feature distributions to determine optimal noise scale and identify failure modes