---
ver: rpa2
title: 'Dimensional Collapse in Transformer Attention Outputs: A Challenge for Sparse
  Dictionary Learning'
arxiv_id: '2508.16929'
source_url: https://arxiv.org/abs/2508.16929
tags:
- attention
- dead
- features
- subspace
- output
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reveals that transformer attention outputs have a surprisingly
  low effective dimensionality (~60% of full space), far lower than MLP outputs (~90%),
  and shows this is a key cause of dead features in sparse dictionary learning methods.
  The authors propose Active Subspace Initialization (ASI), which initializes SAE
  features within the active subspace of activations.
---

# Dimensional Collapse in Transformer Attention Outputs: A Challenge for Sparse Dictionary Learning

## Quick Facts
- arXiv ID: 2508.16929
- Source URL: https://arxiv.org/abs/2508.16929
- Reference count: 40
- Primary result: Transformer attention outputs have ~60% effective dimensionality, causing >80% dead features in sparse autoencoders without specialized initialization

## Executive Summary
This paper reveals that transformer attention outputs occupy a surprisingly low-dimensional subspace (~60% of full space) compared to MLP outputs (~90%), a phenomenon driven by the output projection matrix $W_O$. This dimensional collapse is the primary cause of dead features in sparse dictionary learning methods like TopK SAEs. The authors propose Active Subspace Initialization (ASI), which projects SAE feature vectors onto the active singular vectors of attention activations. ASI reduces dead features from 87% to below 1% while improving reconstruction quality, and generalizes to other sparse replacement models like Lorsa.

## Method Summary
The paper identifies dimensional collapse in attention outputs as the cause of dead features in sparse dictionary learning. To address this, ASI computes the SVD of collected attention activations to identify the active subspace (top $d_{init}$ singular vectors). SAE decoder weights are initialized randomly but then projected onto this active subspace using $W_D \leftarrow W_D V_{active} V_{active}^T$. The encoder is tied: $W_E = W_D^T$. This initialization ensures features begin in directions with non-zero activation variance, providing gradient signals from the start. The method is tested on Llama-3.1-8B layer 15 with $d_{init}=768$ and dictionary size 32768, achieving >99% feature activation rates.

## Key Results
- Attention outputs have ~60% effective rank versus ~90% for MLP outputs
- Standard random initialization yields 87% dead features for 1M-feature SAEs on attention outputs
- ASI reduces dead features to <1% while improving MSE and LM loss delta
- ASI generalizes to Lorsa sparse attention replacement, reducing dead features from 52% to 3%
- The low-rank structure is primarily driven by the output projection matrix $W_O$, not the attention heads themselves

## Why This Works (Mechanism)

### Mechanism 1: Dimensional Collapse via Output Projection
Attention outputs reside in a low-dimensional subspace (~60% effective rank) due to the output projection matrix $W_O$ compressing multi-head activations into a lower-rank subspace. While concatenated head outputs theoretically span higher dimensions, $W_O$ introduces anisotropy that collapses the space. This contrasts with MLP outputs which utilize ~90% of available space.

### Mechanism 2: Dead Feature Genesis via Geometric Mismatch
Standard random initialization scatters feature vectors uniformly across the full $d_{model}$ space. Since ~40% of this space is "dead" (inactive) for attention outputs, features initialized in these directions receive near-zero activation signals and gradients, failing to update during training.

### Mechanism 3: Active Subspace Initialization (ASI) Recovery
ASI computes SVD to identify the top-$d_{init}$ singular vectors of activations and projects SAE decoder weights onto this active subspace. This ensures every feature has a component in a direction with non-zero variance, guaranteeing a gradient signal. The projection is $W_D \leftarrow W_D V_{active} V_{active}^T$ with $W_E = W_D^T$.

## Foundational Learning

- **Singular Value Decomposition (SVD) & Effective Rank**
  - Why needed here: Core diagnostic tool for quantifying dimensional collapse and implementing ASI
  - Quick check question: If a $4096 \times 4096$ activation matrix has only 100 large singular values and the rest are near zero, what is its approximate effective rank relative to full rank?

- **Sparse Autoencoders (TopK SAE)**
  - Why needed here: Object of study is failure modes in these models
  - Quick check question: In a TopK SAE with $k=50$ and $1M$ features, what happens to a feature that is never in the top 50 activations for any training sample?

- **Transformer Residual Stream vs. Sublayer Outputs**
  - Why needed here: Critical distinction—Attention writes to a specific subspace of the Residual Stream
  - Quick check question: Why does the low rank of Attention Output not imply the Residual Stream is low rank?

## Architecture Onboarding

- **Component map:** Collect activations (Attention Output) -> Buffer (10M tokens) -> Initialization Unit (SVD on activations) -> Project SAE weights -> Sparse Learner (TopK SAE) -> Optimizer (SparseAdam)

- **Critical path:** 1) Collect activations from Attention Output 2) Run SVD to extract $V_{active}$ (top $d_{init}$ components) 3) Initialize $W_D$ randomly, project: $W_D \leftarrow W_D V_{active} V_{active}^T$ 4) Set $W_E = W_D^T$ 5) Train using reconstruction loss

- **Design tradeoffs:**
  - $d_{init}$ selection: Too high ($\approx d_{model}$) → no effect; Too low → reconstruction suffers. Paper suggests 1024-2048 for Llama-3.1-8B works well
  - Activation Site: ASI is critical for Attention Output. Applying it to Residual Stream yields marginal gains

- **Failure signatures:**
  - High Dead Features (>80%): Training standard TopK SAE on Attention Output without ASI
  - ASI Ineffective: Using JumpReLU with high-initial $\ell_0$ schedule (features drift out of subspace)
  - Slow Convergence: Using standard Adam instead of SparseAdam at large scales

- **First 3 experiments:**
  1. Compute effective rank of Attention Output vs. MLP Output to confirm ~60% vs ~90% split
  2. Train baseline TopK SAE on Attention Output and log dead feature percentage (expect high)
  3. Implement ASI with $d_{init}=768$ and verify dead features drop below 1% on short run

## Open Questions the Paper Calls Out
- What is the precise causal mechanism linking low effective dimensionality of attention outputs to dead features in sparse dictionary learning?
- Would explicitly constraining attention mechanisms to maintain higher effective rank improve LLM capacity or performance?
- Why does ASI fail to maintain benefits when training starts in a high-sparsity regime ($\ell_0$) that decreases over time?

## Limitations
- The causal mechanism linking dimensional collapse to dead features is not definitively proven—correlation does not equal causation
- ASI assumes the active subspace remains static throughout training, which may not hold under certain sparsity schedules
- Analysis focuses on a single architecture (Llama-3.1-8B), layer, and dataset, limiting generalizability

## Confidence

- **High Confidence:** Dimensional collapse phenomenon itself (attention ~60% vs MLP ~90% effective rank) is well-established through direct computation
- **Medium Confidence:** Causal mechanism linking collapse to dead features is plausible but not definitively proven—optimization dynamics remain possible
- **Medium Confidence:** ASI's effectiveness in reducing dead features is demonstrated, but claims about optimal capacity utilization require further validation

## Next Checks

1. **Ablation on initialization geometry:** Train SAEs with various initialization strategies (random, orthogonal, low-rank structured) on attention outputs while holding all other factors constant to isolate whether dead features are purely geometric

2. **Active subspace stability analysis:** Track how feature vectors evolve during training relative to the initially identified active subspace and monitor whether features drift significantly from $V_{active}$

3. **Cross-architecture generalization test:** Apply the full pipeline (effective rank analysis → ASI implementation) to attention outputs from other architectures (GPT-style, Vision Transformers, hybrid models) to verify transfer beyond Llama-3.1-8B