---
ver: rpa2
title: Do Large Language Models Grasp The Grammar? Evidence from Grammar-Book-Guided
  Probing in Luxembourgish
arxiv_id: '2510.24856'
source_url: https://arxiv.org/abs/2510.24856
tags:
- grammatical
- grammar
- translation
- language
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating large language
  models' (LLMs) grammatical understanding, particularly in low-resource languages
  like Luxembourgish, where traditional translation metrics do not capture deep syntactic
  competence. The authors propose a grammar-book-guided evaluation pipeline that systematically
  extracts grammatical rules from a grammar reference, generates controlled example
  sentences, and constructs minimal pairs for fine-grained testing.
---

# Do Large Language Models Grasp The Grammar? Evidence from Grammar-Book-Guided Probing in Luxembourgish

## Quick Facts
- arXiv ID: 2510.24856
- Source URL: https://arxiv.org/abs/2510.24856
- Reference count: 0
- Primary result: Weak positive correlation between translation performance and grammatical understanding in Luxembourgish; reasoning-enhanced models show improved grammatical competence

## Executive Summary
This paper addresses the challenge of evaluating large language models' (LLMs) grammatical understanding, particularly in low-resource languages like Luxembourgish, where traditional translation metrics do not capture deep syntactic competence. The authors propose a grammar-book-guided evaluation pipeline that systematically extracts grammatical rules from a grammar reference, generates controlled example sentences, and constructs minimal pairs for fine-grained testing. They evaluate LLMs across four tasks: identifying instantiated grammar in sentences, matching sentences to grammar categories, inferring grammar from paragraphs, and detecting grammatically acceptable sentences. Results show only a weak positive correlation between translation performance and grammatical understanding, indicating that strong translations do not imply deep grammatical competence. Larger models perform better overall but still struggle with minimal pair tasks, while reasoning-enhanced models show more promise in grammatical understanding.

## Method Summary
The authors develop a four-stage grammar-book-guided evaluation pipeline for assessing LLM grammatical competence in Luxembourgish. The pipeline extracts grammatical rules from a reference grammar book, generates controlled sentence pairs that instantiate specific rules, creates minimal pairs (grammatical vs. ungrammatical variants), and tests models across four increasingly difficult tasks. They evaluate multiple LLM families (GPT-5, Gemma, Llama, Qwen) on translation metrics (CometScore, spBLEU, ChrF++) and grammatical understanding tasks, computing correlations to assess whether translation performance predicts grammatical competence. The approach addresses the limitation that translation metrics alone cannot capture deep syntactic knowledge, particularly in low-resource languages where training data is limited.

## Key Results
- Weak positive correlation between translation performance and grammatical understanding, suggesting strong translations don't imply deep grammatical competence
- Larger models generally outperform smaller ones on grammatical tasks, but all models struggle with minimal pair identification
- Reasoning-enhanced models (Qwen3-Next-80B-Thinking) show superior grammatical understanding compared to standard models
- Task 4 (minimal pair detection) is the most challenging, with models performing near chance level
- Grammar-book-guided evaluation reveals gaps in LLM grammatical knowledge not captured by translation metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A grammar-book-guided pipeline can systematically extract and evaluate grammatical competence in low-resource languages where traditional translation metrics fail to capture syntactic understanding.
- Mechanism: The pipeline extracts explicit grammatical rules from a reference grammar book, generates controlled example sentences that instantiate specific rules, creates minimal pairs (grammatical vs. ungrammatical variants), and tests LLMs across four tasks of increasing difficulty—identifying grammar in sentences, matching sentences to grammar categories, inferring grammar from paragraphs, and detecting acceptable sentences.
- Core assumption: Grammar books provide comprehensive, structured coverage of linguistic rules that can be systematically converted into testable probes, and LLM performance on these controlled tasks reflects genuine grammatical competence rather than surface pattern matching.
- Evidence anchors:
  - [abstract] "We propose a Grammar Book Guided evaluation pipeline intended to provide a systematic and generalizable framework for grammar evaluation consisting of four key stages"
  - [section 3.2] Pipeline structure with Material Inspector, Phrasing Atelier, Twin Forge, and Proof Stand components
  - [corpus] Related work on grammaticality judgments (arxiv 2512.10453) confirms minimal pair testing as a valid paradigm for probing grammatical knowledge

### Mechanism 2
- Claim: Translation performance and grammatical understanding are only weakly correlated—strong translation does not imply deep grammatical competence.
- Mechanism: Translation relies heavily on semantic mapping and fluency, which can be achieved through statistical pattern matching in training data. Grammatical competence, especially at the morphological and syntactic level (tested via minimal pairs), requires explicit rule learning and character-level analysis that translation metrics don't capture.
- Core assumption: The correlation gap reflects a genuine difference in underlying competencies, not just measurement artifact.
- Evidence anchors:
  - [abstract] "Results show a weak positive correlation between translation performance and grammatical understanding, indicating that strong translations do not imply deep grammatical competence"
  - [section 4.2] Spearman and Pearson correlation analyses show Tasks 1-3 (grammar identification) correlate strongly with each other but weakly with Task 4 (minimal pair) and translation metrics
  - [corpus] Evidence is consistent—related work (arxiv 2510.16227) notes probability and grammaticality are distinct notions in linguistics

### Mechanism 3
- Claim: Reasoning-enhanced models outperform standard models on grammatical understanding, suggesting that explicit reasoning capacity helps bridge the gap between semantic fluency and syntactic competence.
- Mechanism: Grammatical analysis requires logical inference about abstract rules and their application—capacity that benefits from chain-of-thought style reasoning. Models with enhanced reasoning (e.g., Qwen3-Next-80B-Thinking) can decompose grammatical judgments into explicit steps rather than relying on implicit pattern matching.
- Core assumption: The performance gain is due to reasoning architecture, not simply model scale or training data differences.
- Evidence anchors:
  - [abstract] "strong reasoning ability offers a promising way to enhance their grammatical understanding"
  - [section 4.3] "reasoning model 'Qwen3-Next-80B-A3B-Thinking' demonstrate higher performance on most grammar understanding tasks, even outperforming GPT-5"
  - [corpus] Related work (arxiv 2509.00425) on metalinguistic reasoning supports the role of explicit reasoning in grammatical tasks

## Foundational Learning

- Concept: **Minimal Pairs Testing**
  - Why needed here: Task 4 (identifying grammatically acceptable sentences from minimal pairs) is the hardest task and reveals the gap between semantic fluency and syntactic precision. Understanding this paradigm is essential for interpreting results.
  - Quick check question: Can you explain why two sentences that differ by only one morpheme might have identical semantics but different grammaticality?

- Concept: **Correlation vs. Causation in Benchmark Metrics**
  - Why needed here: The paper's central finding is about weak correlation between translation metrics (COMET, BLEU) and grammatical tasks. Understanding what correlation does and doesn't tell you about underlying mechanisms is critical.
  - Quick check question: If model A outperforms model B on translation, what can you conclude about their relative grammatical competence?

- Concept: **Controlled Generation with Grammar Constraints**
  - Why needed here: The Phrasing Atelier and Twin Forge stages require generating sentences that obligatorily instantiate specific grammatical rules. This is a constrained decoding problem.
  - Quick check question: How would you prompt an LLM to generate a Luxembourgish sentence that must use a specific case-marking pattern?

## Architecture Onboarding

- Component map: Grammar book PDF -> OCR/text extraction -> Grammar point extraction -> Controlled sentence generation -> Minimal pair creation -> Task evaluation

- Critical path: Grammar book PDF → OCR/text extraction → Grammar point extraction → Controlled sentence generation → Minimal pair creation → Task evaluation. The quality of OCR and grammar point extraction determines downstream validity.

- Design tradeoffs:
  - Using GPT-5 for pipeline construction may introduce circularity (evaluating LLMs with LLM-generated data), mitigated by human validation (98% grammar instantiation confirmed)
  - Grammar book coverage may not match natural language distribution—trades ecological validity for controlled probing
  - Focus on Luxembourgish (low-resource) limits immediate generalization but reduces data contamination concerns

- Failure signatures:
  - Task 4 near chance-level performance indicates models lack fine-grained grammatical discrimination
  - Large performance drops when increasing candidate set sizes in Tasks 1-3 suggest shallow grammatical representations
  - Phonological rules (e.g., "Eifeler Regel") show 32% violation rate—models lack phonological awareness

- First 3 experiments:
  1. **Baseline correlation check**: Run all four grammar tasks + translation metrics on your model set; compute Pearson/Spearman correlations to verify the weak correlation finding reproduces
  2. **Ablation by model scale**: Test 2-3 model families across size variants (e.g., Gemma 2B/9B/27B) to confirm translation improves faster than grammar as scale increases
  3. **Reasoning enhancement test**: Compare standard vs. thinking/reasoning variants (e.g., Qwen3-Instruct vs. Qwen3-Thinking) on Task 4 minimal pairs to validate reasoning mechanism hypothesis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Chain-of-Thought reasoning or dedicated reasoning architectures significantly improve LLM performance on fine-grained grammatical minimal pair tasks?
- Basis in paper: [explicit] The authors explicitly list exploring "Chain-of-Thought" and "adopting dedicated reasoning models" as future work to address models' struggles with Task 4 (Minimal Pairs).
- Why unresolved: Current models rely heavily on semantic cues, failing when tasks require rigorous character-level verification rather than sentence-level meaning.
- What evidence would resolve it: Demonstrating that reasoning-enhanced models achieve above-chance accuracy on minimal pair tasks without corresponding gains in translation fluency, indicating improved syntactic reasoning.

### Open Question 2
- Question: Does the Grammar-Book-Guided evaluation pipeline generalize effectively to low-resource languages with diverse typological features?
- Basis in paper: [explicit] The authors identify "conducting grammatical evaluations across multiple languages" as a future direction to establish a broader benchmark.
- Why unresolved: The current study is validated only on Luxembourgish (a Germanic language), leaving the pipeline's robustness unproven for non-Germanic or polysynthetic languages.
- What evidence would resolve it: Successful replication of the pipeline's extraction and probing stages on structurally diverse languages, yielding valid minimal pairs and consistent correlation metrics.

### Open Question 3
- Question: Can LLMs overcome limitations in learning phonologically-conditioned grammatical rules without explicit acoustic training?
- Basis in paper: [inferred] The authors note in Section 4.1 that rules like the "Eifeler Regel" (spelling based on pronunciation) are "especially difficult," resulting in a 32% violation rate.
- Why unresolved: Text-only models appear to lack the phonological awareness necessary to internalize orthographic rules that depend on sound rather than just syntax.
- What evidence would resolve it: Experiments showing that multimodal models (trained on both audio and text) significantly reduce error rates on phonologically-conditioned grammar tasks compared to text-only baselines.

## Limitations

- The evaluation pipeline relies on a single grammar reference (Gilles, 2023), which may not capture the full complexity of Luxembourgish grammar, particularly phonological rules
- Using GPT-5 for both pipeline construction and evaluation creates potential circularity, though human validation provides some mitigation
- Translation-grammar correlation findings may be influenced by the low-resource nature of Luxembourgish, limiting generalizability to high-resource languages
- The evaluation focuses on grammatical knowledge as encoded in grammar books rather than naturalistic language use, potentially testing rule-book knowledge rather than linguistic competence

## Confidence

- **High confidence**: The weak correlation between translation performance and grammatical understanding is robustly demonstrated across multiple models and correlation measures. The finding that larger models perform better on grammatical tasks but still struggle with minimal pairs is consistently observed.
- **Medium confidence**: The claim that reasoning-enhanced models show superior grammatical understanding requires further validation, as the comparison is limited to a single reasoning variant (Qwen3-Next-80B-Thinking). The mechanism by which reasoning capacity enhances grammatical competence is plausible but not definitively established.
- **Low confidence**: The generalizability of these findings to other low-resource languages remains uncertain, as the evaluation pipeline and results are specific to Luxembourgish. The extent to which grammar book knowledge maps to real-world linguistic competence is not fully established.

## Next Checks

1. **Cross-linguistic validation**: Apply the grammar-book-guided pipeline to at least two other low-resource languages with available grammar references to test whether the weak translation-grammar correlation generalizes beyond Luxembourgish.

2. **Reasoning architecture ablation**: Compare multiple reasoning-enhanced model variants (e.g., different reasoning techniques across model families) on the minimal pair task to determine whether the observed improvements are due to reasoning capacity specifically or other factors like model scale.

3. **Ecological validity test**: Evaluate models on naturalistic Luxembourgish text (e.g., news articles, social media) using the same grammatical categories to assess whether grammar book-based competence translates to real-world language understanding.