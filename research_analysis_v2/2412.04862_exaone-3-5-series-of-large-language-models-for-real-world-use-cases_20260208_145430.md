---
ver: rpa2
title: 'EXAONE 3.5: Series of Large Language Models for Real-world Use Cases'
arxiv_id: '2412.04862'
source_url: https://arxiv.org/abs/2412.04862
tags:
- language
- exaone
- answer
- long
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LG AI Research developed EXAONE 3.5, a series of instruction-tuned
  language models (32B, 7.8B, 2.4B parameters) with long-context processing up to
  32K tokens. Built using a two-stage pre-training approach with extended context
  via positional interpolation and rigorous decontamination, the models were fine-tuned
  using supervised learning and preference optimization.
---

# EXAONE 3.5: Series of Large Language Models for Real-world Use Cases

## Quick Facts
- arXiv ID: 2412.04862
- Source URL: https://arxiv.org/abs/2412.04862
- Reference count: 40
- LG AI Research developed 32B, 7.8B, and 2.4B parameter models with 32K context length, achieving state-of-the-art results on real-world use-case benchmarks and competitive performance on general tasks

## Executive Summary
LG AI Research presents EXAONE 3.5, a series of instruction-tuned language models designed for real-world applications with long-context processing capabilities up to 32K tokens. The models were developed through a two-stage pre-training approach that extends context length while preserving general capabilities, followed by supervised fine-tuning and preference optimization. Evaluation results demonstrate state-of-the-art performance on seven real-world use-case benchmarks, top results on four long-context benchmarks, and competitive performance on nine general-domain tasks. The 2.4B model notably outperforms larger baselines, highlighting the efficiency of the architecture and training methodology.

## Method Summary
EXAONE 3.5 employs a two-stage pre-training pipeline where stage one trains on diverse bilingual corpus (6.5T tokens for 32B/2.4B, 9T for 7.8B) with 4K context limits, followed by stage two extending to 32K context using full-length documents mixed with replayed stage one data to prevent catastrophic forgetting. Post-training includes supervised fine-tuning on evolved instruction-response pairs, then multi-stage preference optimization using DPO/SimPO with dual reward model validation to filter preference data. The architecture uses Grouped Query Attention (8 KV heads), SwiGLU activation, and RoPE with theta=1M, with careful decontamination via substring matching (S=50, N=10) against benchmark test sets.

## Key Results
- State-of-the-art performance on all seven real-world use-case benchmarks (MT-Bench, LiveBench, Arena-Hard, AlpacaEval, IFEval, KoMT-Bench, LogicKor)
- Top results on four long-context benchmarks (NIAH, LongBench, LongRAG variants)
- Competitive performance on nine general-domain tasks (GSM8K, MATH, HumanEval, MBPP, MMLU, KMMLU, GPQA, ARC-C, BBH)
- 2.4B model outperforms larger baseline models on certain tasks, demonstrating efficiency

## Why This Works (Mechanism)

### Mechanism 1
Two-stage pre-training with replay-based continuation enables context extension while preserving learned knowledge. First-stage trains on general corpus with documents truncated to 4K tokens. Second-stage uses full-length documents up to 32K tokens, mixing long-context data with replayed first-stage data to prevent catastrophic forgetting. Replay ratio balances context extension gains against retention of general capabilities.

### Mechanism 2
Multi-stage preference optimization with dual reward model validation mitigates over-optimization. Sequential DAA training (M0 → M1 → M2) where each stage uses preference data filtered by agreement between two reward models. This prevents reward hacking while aligning to human preferences. Agreement threshold reliably identifies high-quality preference pairs.

### Mechanism 3
Substring-level decontamination with strict matching preserves benchmark validity. Extract all S=50-character substrings from normalized test sets. Sample N=10 substrings from each training example and flag contamination if any match. This is stricter than GPT-4's original approach (N=1). 50-character substring matches indicate meaningful contamination rather than coincidental overlap.

## Foundational Learning

- **Grouped Query Attention (GQA)**: Reduces KV cache memory for 32K context. All three models use 8 KV heads regardless of total attention heads. Quick check: For the 32B model with 40 heads and 8 KV heads, what is the compression ratio for KV cache compared to standard multi-head attention?

- **Positional Interpolation / Long-context Fine-tuning**: Extends context from 4K → 32K tokens. RoPE theta=1,000,000 suggests extended base frequency for position encoding. Quick check: Why does the paper use "long-context fine-tuning" rather than pure positional interpolation, and what role does replay data play?

- **Direct Alignment Algorithms (DPO/SimPO)**: Post-SFT alignment without training a separate reward model. Paper mentions both DPO and SimPO but doesn't specify which is used per stage. Quick check: What is the key difference between DPO and SimPO regarding reference model requirements, and why might staged training help with over-optimization?

## Architecture Onboarding

- **Component map**: Input → BBPE Tokenizer (102,400 vocab) → Transformer Decoder (SwiGLU + GQA + RoPE) → Output
- **Critical path**: Load pre-trained checkpoint from HuggingFace (LGAI-EXAONE organization) → Verify context window (32K) and RoPE configuration → For fine-tuning: SFT → multi-stage preference optimization → Apply decontamination if training on new data
- **Design tradeoffs**: 2.4B model has tied embeddings (saves memory) vs. 32B/7.8B with untied (better performance); Training token counts vary: 6.5T (32B), 9T (7.8B), 6.5T (2.4B) — suggests compute-optimal scaling not strictly followed; RoPE θ=1M enables long context but may impact short-context precision
- **Failure signatures**: Long-context retrieval fails at specific depths → check NIAH heatmap patterns in Figure 3; Korean language degradation → verify 50/50 vocab split is preserved during fine-tuning; Over-optimization in preference training → model outputs become verbose or repetitive
- **First 3 experiments**: Baseline sanity check: Run provided evaluation prompts (Appendix D.3) on GSM8K and MMLU with 0-shot CoT; verify scores within ±2% of reported; Long-context stress test: Run Needle-in-a-Haystack at 32K tokens, Korean and English; confirm >95% retrieval accuracy across all depth positions; Fine-tuning pilot: SFT on 1K instruction-response pairs with replay mixing; measure retention on 3 general-domain benchmarks vs. baseline

## Open Questions the Paper Calls Out

### Open Question 1
What specific mechanisms or techniques are required to effectively advance the explainability of EXAONE 3.5's decision-making process? The paper identifies the lack of transparency as a risk that reduces trust but does not propose or evaluate specific explainability methods in the current model release. A study applying interpretability techniques (e.g., attention visualization, probing classifiers) to EXAONE 3.5 to validate decision pathways would resolve this.

### Open Question 2
How can safety alignment techniques be optimized for smaller parameter models (e.g., 2.4B) to close the performance gap with larger models on trustworthiness benchmarks? The paper demonstrates the existence of a safety performance gap based on model size but does not detail a specific methodology for aligning smaller models to match the safety standards of the 32B model. Comparative analysis showing that specific data augmentation or targeted preference optimization can raise the 2.4B model's safety scores to near-parity with the 32B model would resolve this.

### Open Question 3
Does the strict "training efficiency" strategy (using fewer tokens than competitors) result in a measurable trade-off specifically in general reasoning and coding capabilities? The paper asserts high performance relative to compute cost but does not investigate whether the reduced token count specifically limits the model's capacity for abstract reasoning compared to instruction following. Ablation studies training EXAONE models with increasing token counts to determine if the "General Domain" scores scale linearly with data volume independent of model size would resolve this.

## Limitations

- Lack of transparency regarding training hyperparameters including learning rate schedules, batch sizes, and optimizer configurations across different model sizes
- Exact composition and size of the pre-training corpus remain unspecified, making it difficult to assess whether the claimed 6.5T or 9T training tokens represent optimal scaling
- Decontamination process, while methodologically sound, lacks external validation to confirm it effectively prevents benchmark contamination without removing legitimate training data

## Confidence

**High Confidence Claims:**
- Architectural design using GQA, SwiGLU activation, and RoPE with theta=1M is technically sound and well-documented
- Two-stage pre-training approach with context extension from 4K to 32K tokens follows established methodology
- Multi-stage preference optimization framework with dual reward model validation represents a reasonable approach to prevent over-optimization

**Medium Confidence Claims:**
- Reported state-of-the-art performance on real-world use-case benchmarks is plausible given the extensive post-training pipeline
- 2.4B model outperforming larger baselines on certain tasks is consistent with efficient architecture design
- Long-context understanding capabilities demonstrated on Needle-in-a-Haystack and LongBench are supported by the architectural choices

**Low Confidence Claims:**
- Claim of being "the first to achieve state-of-the-art performance on all seven real-world use-case benchmarks" cannot be fully verified without access to exact evaluation protocols
- Assertion that the 2.4B model's performance on certain tasks surpasses 7.8B and 32B models may depend on specific evaluation conditions not fully disclosed

## Next Checks

1. **Benchmark Contamination Verification**: Conduct independent decontamination analysis by applying the substring-matching method (S=50, N=10) to all training data against test sets for MT-Bench, LiveBench, Arena-Hard, and AlpacaEval. Calculate the exact overlap percentage and assess whether reported scores are inflated due to contamination.

2. **Component Ablation Study**: Implement controlled experiments isolating the contributions of each major component - (a) two-stage pre-training vs. single-stage with positional interpolation only, (b) replay-based continuation vs. pure long-context fine-tuning, and (c) staged preference optimization vs. single-stage DPO. Measure performance deltas on GSM8K, MMLU, and NIAH benchmarks.

3. **Cross-Lingual Consistency Analysis**: Given the 50/50 Korean-English vocabulary split, evaluate the 32K context capabilities separately for Korean and English on Needle-in-a-Haystack tasks. Measure retrieval accuracy at varying depths (1K, 4K, 16K, 32K) and identify any systematic degradation patterns that might indicate language-specific limitations in the long-context processing pipeline.