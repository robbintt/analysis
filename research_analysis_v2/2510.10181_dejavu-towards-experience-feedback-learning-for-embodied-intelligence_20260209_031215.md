---
ver: rpa2
title: 'Dejavu: Towards Experience Feedback Learning for Embodied Intelligence'
arxiv_id: '2510.10181'
source_url: https://arxiv.org/abs/2510.10181
tags:
- experience
- policy
- action
- residual
- bank
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EFN addresses the inability of deployed embodied agents to improve
  task performance without retraining. It augments a frozen VLA policy with an Experience
  Feedback Network that retrieves semantically relevant prior trajectories and predicts
  residual corrections.
---

# Dejavu: Towards Experience Feedback Learning for Embodied Intelligence

## Quick Facts
- arXiv ID: 2510.10181
- Source URL: https://arxiv.org/abs/2510.10181
- Reference count: 40
- Primary result: EFN improves frozen VLA policies post-deployment via retrieval and residual corrections, outperforming baselines in both simulation and real-robot settings.

## Executive Summary
Deployed embodied agents often cannot improve task performance without retraining. EFN augments a frozen VLA policy with an Experience Feedback Network that retrieves semantically relevant prior trajectories and predicts residual corrections. Training uses similarity-shaped rewards and SAC to align predicted actions with past successful behaviors. During deployment, EFN continually grows its experience bank with successful rollouts, enabling "learning from experience." Experiments on LIBERO and a real-world AgiBot-G1 platform show EFN consistently outperforms frozen VLA baselines and retrieval-only, residual-only, retrieval-RL, and test-time training methods, improving both success rates and efficiency under the same backbone and interaction budget.

## Method Summary
EFN augments a frozen VLA policy with an Experience Feedback Network that retrieves semantically relevant prior trajectories and predicts residual corrections. The experience bank stores (visual feature, instruction, key, base action) tuples. Keys are generated via mean–max pooling and ℓ2 normalization. During deployment, EFN retrieves top-k past trajectories using cosine similarity, applies a temperature-smoothed softmax to sample, and uses the retrieved context to condition a residual action prediction Δa_t added to the frozen policy's base output a^(0)_t. Training employs a shaped reward combining absolute action similarity, progress, momentum, anti-idling penalties, and time penalties, optimized via SAC with a frozen backbone. The network architecture uses MAP blocks, a Transformer encoder, and outputs residual actions in a shared latent space.

## Key Results
- EFN consistently outperforms frozen VLA baselines and retrieval-only, residual-only, retrieval-RL, and test-time training methods in both LIBERO simulation and real-world AgiBot-G1 tasks.
- Success rates and average steps to completion improve with EFN under the same interaction budget and backbone.
- Continual growth of the experience bank with successful rollouts enables ongoing adaptation without retraining.

## Why This Works (Mechanism)
EFN addresses the inability of deployed embodied agents to improve task performance without retraining. It augments a frozen VLA policy with an Experience Feedback Network that retrieves semantically relevant prior trajectories and predicts residual corrections. Training uses similarity-shaped rewards and SAC to align predicted actions with past successful behaviors. During deployment, EFN continually grows its experience bank with successful rollouts, enabling "learning from experience." Experiments on LIBERO and a real-world AgiBot-G1 platform show EFN consistently outperforms frozen VLA baselines and retrieval-only, residual-only, retrieval-RL, and test-time training methods, improving both success rates and efficiency under the same backbone and interaction budget.

## Foundational Learning
- **Experience Bank**: Stores successful trajectories for retrieval; needed to provide semantically relevant past experiences for current tasks.
  - Quick check: Verify bank stores (ℓ_τ, F_t, k_t, a^(0)_t) with correct key generation.
- **Retrieval via Cosine Similarity**: Finds past experiences similar to current state; needed to condition corrections on relevant contexts.
  - Quick check: Ensure top-k shortlist and temperature-smoothed softmax sampling work as specified.
- **Residual Action Prediction**: Predicts Δa_t to correct frozen policy output; needed to adapt without retraining backbone.
  - Quick check: Confirm a_t = a^(0)_t + Δa_t is used, not direct action copying.
- **Shaped Reward with Anti-Idling**: Combines similarity, progress, momentum, and lazy penalties; needed to encourage efficient, non-stuttering behavior.
  - Quick check: Validate anti-idling term w_lazy·(s^next_t·n_t·s^stay_t) penalizes stasis without harming success.
- **SAC on Frozen Backbone**: Trains EFN while freezing VLA; needed to adapt residual policy without updating pretrained features.
  - Quick check: Monitor training stability and confirm frozen backbone weights remain unchanged.

## Architecture Onboarding

**Component Map**: Visual features → Experience Bank → Retrieval (cosine similarity, top-k, softmax) → Context Encoder → Residual Prediction → Final Action (a^(0)_t + Δa_t)

**Critical Path**: Retrieval of relevant past experiences → Conditioning of residual correction → Application to frozen policy output.

**Design Tradeoffs**: 
- Using residuals (Δa_t) instead of direct action prediction allows adaptation without destabilizing the frozen backbone, but may limit expressiveness if corrections are too small.
- Storing raw visual features and instructions enables flexible retrieval but requires efficient similarity search and sufficient memory.

**Failure Signatures**:
- Agent idles with high s^next but negligible progress—anti-idling penalty may be too weak or incorrectly computed.
- Direct-action EFN collapses (success≈0)—residual parameterization may not be used or base action not added to Δa_t.
- kNN-RAG degrades performance—EFN may be copying actions instead of predicting residuals.

**3 First Experiments**:
1. Set up LIBERO simulation with a frozen OpenVLA checkpoint; construct initial experience bank from ~300–1000 mixed rollouts using mean–max visual keys and stored instruction embeddings.
2. Implement the retrieval pipeline (top-k with τ=0.05 softmax) and Sinkhorn-based similarity sim(·,·); build the shaped reward with anti-idling terms and default weights.
3. Train EFN (MAP + 2-layer encoder) with SAC on top of frozen VLA outputs using residual parameterization a_t = a^(0)_t + Δa_t; freeze backbone; evaluate success rate and steps.

## Open Questions the Paper Calls Out
- **Open Question 1**: Can coupling EFN with world models to retrieve experiences based on predicted future trajectories improve long-horizon adaptation?
  - Basis in paper: Section D.5 states that "predictive retrieval opens the door to uncertainty-aware guidance and more robust long-horizon adaptation."
  - Why unresolved: The current implementation retrieves based on current visual features rather than predicted futures, limiting proactive correction.
  - What evidence would resolve it: Experiments demonstrating that retrieving matches from a predicted latent rollout increases success rates on long-horizon tasks compared to current-view retrieval.

- **Open Question 2**: What memory management strategies (e.g., coreset selection) are needed to maintain performance in long-running deployments with unbounded data?
  - Basis in paper: Section D.4 identifies the limitation that "the bank will grow without bound unless additional mechanisms are introduced" for storage and retrieval efficiency.
  - Why unresolved: The paper evaluates fixed bank volumes (300/1000) but does not test online forgetting, compression, or prioritization algorithms for lifelong operation.
  - What evidence would resolve it: A study showing that a specific replacement policy maintains high success rates while keeping the memory footprint constant over thousands of episodes.

- **Open Question 3**: Can unified latent spaces or lightweight alignment layers enable cross-robot retrieval without negative transfer?
  - Basis in paper: Section D.5 proposes moving "toward shared experience repositories across tasks and robots" to allow different platforms to reuse experience.
  - Why unresolved: Current banks are specific to a single VLA backbone, preventing the transfer of skills between heterogeneous manipulators.
  - What evidence would resolve it: Demonstration of a robot successfully leveraging an experience bank populated by a physically different robot to boost its own performance.

- **Open Question 4**: Can learned, task-aware success estimators outperform simple similarity thresholds for admitting experiences to the live bank?
  - Basis in paper: Section D.1 notes the current "simple similarity threshold" is a limitation and suggests "future work could explore learned, task-aware success estimators."
  - Why unresolved: The current success detection is heuristic and may admit low-quality trajectories or reject valid successes that deviate visually.
  - What evidence would resolve it: Ablation experiments comparing the convergence speed and final policy quality of EFN using a learned estimator versus the threshold baseline.

## Limitations
- Critical architectural details (MAP pooling, context encoder) and exact offline experience seeding are unspecified, which may affect reproducibility.
- Current banks are specific to a single VLA backbone, preventing cross-robot skill transfer.
- The experience bank grows without bound unless additional memory management mechanisms are introduced for long-running deployments.

## Confidence
- **Method**: Medium — LIBERO and AgiBot-G1 experiments show gains, but key details (MAP, context encoder, experience bank seeding) are unspecified.
- **Generalization**: Medium — Ablation on reward shaping is incomplete; retrieval-RL baseline underperformance is plausible but requires exact hyperparameter adherence.
- **Reproducibility**: Low — Unspecified architectural and seeding procedures may prevent faithful reproduction.

## Next Checks
1. Reproduce EFN on LIBERO with a publicly available frozen VLA backbone, verifying the offline experience bank is constructed with the specified key-generation and initial episode mix.
2. Implement the anti-idling penalty and monitor idle episodes to confirm the lazy term effectively discourages stasis without harming task success.
3. Compare EFN against a retrieval-only ablated version and the direct-action ablated version to confirm that both components are necessary for the reported gains.