---
ver: rpa2
title: Provable Benefits of In-Tool Learning for Large Language Models
arxiv_id: '2508.20755'
source_url: https://arxiv.org/abs/2508.20755
tags:
- learning
- facts
- token
- language
- recall
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a theoretical and empirical analysis of in-tool
  versus in-weight learning in language models. It proves that the number of facts
  a model can memorize in its parameters is fundamentally limited by its size, while
  tool-augmented models can access unbounded factual knowledge through external retrieval.
---

# Provable Benefits of In-Tool Learning for Large Language Models

## Quick Facts
- arXiv ID: 2508.20755
- Source URL: https://arxiv.org/abs/2508.20755
- Reference count: 40
- Key outcome: Tool-augmented learning enables unbounded factual recall while preserving general capabilities, unlike in-weight memorization which degrades them.

## Executive Summary
This paper provides a theoretical and empirical analysis of in-tool versus in-weight learning in language models. It proves that the number of facts a model can memorize in its parameters is fundamentally limited by its size, while tool-augmented models can access unbounded factual knowledge through external retrieval. Controlled experiments confirm that in-tool models achieve stable performance regardless of dataset size, while in-weight models require increasingly more parameters. Large-scale experiments show that in-weight finetuning degrades general capabilities due to interference, whereas in-tool learning preserves them.

## Method Summary
The paper compares factual recall via in-weight memorization versus in-tool retrieval using synthetic biographical datasets with 4 attributes (birthplace, birthdate, address, occupation). Controlled experiments train small Llama3-style transformers from scratch on datasets ranging from 100 to 50k facts, measuring factual recall accuracy and parameter scaling. Large-scale experiments fine-tune pretrained models (SmolLM and Llama3.1/3.2) on the same datasets while tracking general capability retention via HellaSwag accuracy and Total Variation distance from the base model. The in-tool setting uses an external SQLite database queried through special tokens, while in-weight models memorize facts directly in parameters.

## Key Results
- In-weight models require linearly increasing parameters to memorize more facts, hitting a fundamental capacity limit.
- In-tool models exhibit a phase transition from memorization to rule-based retrieval once dataset diversity exceeds ~1K facts.
- Large-scale experiments show in-weight finetuning degrades general capabilities due to interference, while in-tool learning preserves them.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The number of distinct facts a model can memorize in its weights is fundamentally limited by its parameter count, creating a capacity bottleneck.
- Mechanism: Theoretical lower bound (Lemma 3.1, Theorem 3.2) shows that memorizing N facts requires parameters proportional to N bits. Empirical validation (Figure 2) confirms in-weight models need increasing parameters as dataset grows, following ~linear scaling.
- Core assumption: Facts are structured as independent (n, a, v) triplets; correlations between facts can reduce "effective" fact count.
- Evidence anchors:
  - [abstract] "We show that the number of facts a model can memorize solely in its weights is fundamentally limited by its parameter count."
  - [Section 3] Theorem 3.2 establishes P ≥ c·#Facts for parameter count P.
  - [corpus] Weak direct evidence; corpus focuses on tool-use evaluation and conflicts, not capacity bounds.
- Break condition: If facts are highly correlated or compressible, the linear relationship may become sublinear, reducing parameter requirements.

### Mechanism 2
- Claim: In-tool learning exhibits a phase transition from memorization to rule-based retrieval once dataset diversity exceeds a critical threshold.
- Mechanism: Models initially memorize fact-answer pairs but, after seeing sufficient fact diversity, "grok" the generalizable rule: construct a tool query to retrieve any fact. This decouples recall accuracy from model size (Figure 2, 3).
- Core assumption: The model can discover and execute the correct tool-query format via gradient-based optimization.
- Evidence anchors:
  - [abstract] "We prove that tool-use enables unbounded factual recall via a simple and efficient circuit construction."
  - [Section 5] Figure 3 shows OOD accuracy jumps sharply after a critical dataset size (~1K facts), indicating rule learning.
  - [corpus] Related work on tool-memory conflicts (arXiv:2601.09760) and multi-turn tool evaluation supports the existence of distinct learning regimes.
- Break condition: If tool interfaces are inconsistent or queries require complex reasoning beyond pattern copying, the phase transition may not occur.

### Mechanism 3
- Claim: In-weight finetuning on new facts interferes with existing capabilities due to parameter capacity limits, while in-tool learning preserves them.
- Mechanism: In-weight updates shift the model's output distribution (measured by Total Variation) to encode new facts, overwriting prior knowledge. In-tool learning offloads storage, keeping the base distribution stable (Figures 5, 6).
- Core assumption: General capabilities (e.g., HellaSwag) rely on the same parameter space used for factual memorization.
- Evidence anchors:
  - [abstract] "Large-scale experiments show that in-weight finetuning degrades general capabilities due to interference, whereas in-tool learning preserves them."
  - [Section 6] Figure 5 shows HellaSwag accuracy drops for in-weight models as facts increase; in-tool models remain stable.
  - [corpus] Tool-Memory Conflict work (arXiv:2601.09760) documents similar interference between parametric and tool knowledge.
- Break condition: If new facts are highly structured or redundant, interference may be mitigated, reducing capability loss.

## Foundational Learning

### Parametric vs. External Knowledge Storage
- Why needed here: The paper's central comparison hinges on understanding that weights have fixed capacity while external tools (databases) are unbounded.
- Quick check question: Can you explain why increasing parameter count linearly increases memorization capacity but not tool-use capability?

### Phase Transitions in Learning
- Why needed here: The paper documents a sharp shift from memorization to rule-learning in in-tool models (Figure 3), analogous to "grokking."
- Quick check question: What dataset characteristic triggers the transition from memorizing facts to learning a retrieval rule?

### Distributional Interference
- Why needed here: In-weight learning changes the model's output distribution (TV distance), while in-tool learning does not; this underpins capability degradation.
- Quick check question: Why does Total Variation distance grow with facts memorized in-weight but stay low for in-tool learning?

## Architecture Onboarding

### Component map
Transformer backbone (Llama3-style, 2-8 layers, 2-8 heads, embedding dim 4-128) -> External database (SQLite with biographical facts) -> Tool interface (special tokens <DB>, </DB>) -> Chat template (structured dialog format)

### Critical path
1. User query Q = φ1(a) ◦ φ2(n) ◦ φ3(a) enters transformer.
2. Transformer generates tool query T = χ1(a) ◦ χ2(n) (assumes in-tool mode).
3. Database returns ξ(f, a, n) = v or error.
4. Transformer formats final answer A = ψ1(n) ◦ ψ2(a) ◦ ψ3(v).

### Design tradeoffs
- **In-weight**: Lower inference latency (no external call), but parameter scaling required for more facts; risks capability interference.
- **In-tool**: Stable performance regardless of fact count; preserves capabilities; but higher latency and requires tool infrastructure.
- **Rule vs. fact teaching**: Teaching tool-use rules (e.g., "query DB for birthplace") is sample-efficient; teaching individual facts is not.

### Failure signatures
- **In-weight**: HellaSwag accuracy drops >5% as facts increase; TV distance >0.1 indicates distribution shift.
- **In-tool**: OOD accuracy <50% if model hasn't reached phase transition; unstable tool-use training requires early stopping.
- **General**: Correlation between facts (α > 0) reduces parameter needs for in-weight, but in-tool still preferred for scalability.

### First 3 experiments
1. Validate parameter bound: Train in-weight models on 1K-100K facts; plot parameters needed for 95% recall vs. #Facts. Expect linear trend.
2. Trigger phase transition: Train in-tool models on increasing dataset sizes; measure OOD accuracy. Expect sharp jump at ~1K facts.
3. Measure capability degradation: Finetune pretrained LLMs (1B-8B) on 500-50K facts in-weight vs. in-tool; track HellaSwag and TV distance. Expect degradation only for in-weight.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the theoretical scalability of in-tool learning hold for complex, non-retrieval tools such as code interpreters, reasoning traces, or learnable memory modules?
- Basis in paper: [explicit] The Limitations section states the exploration was restricted to structured databases and explicitly calls for "extending our analysis to these richer settings" as a promising direction.
- Why unresolved: The current proofs rely on a specific circuit construction for query-based retrieval; tools with stochastic outputs or complex state management may not admit the same bounded parameter upper bounds.
- What evidence would resolve it: Empirical scaling laws or theoretical proofs demonstrating that learning to use code execution or reasoning tools requires fixed model capacity regardless of the complexity of the tool's state space.

### Open Question 2
- Question: What statistical or mechanistic factors explain the sublinear parameter scaling observed in in-weight models for small dataset sizes?
- Basis in paper: [explicit] The authors note in Section 5 that empirical scaling deviates from the linear lower bound for small datasets, suggesting "subtle phenomena not captured by the theorem" to be addressed in future work.
- Why unresolved: Theorem 3.2 provides a worst-case linear bound, but does not account for the compressibility of the synthetic data or optimization dynamics that might allow for more efficient storage at low capacities.
- What evidence would resolve it: A refined theoretical bound incorporating data distribution priors or interpretability analysis showing how models compress small clusters of facts more efficiently than isolated random facts.

### Open Question 3
- Question: How do optimization dynamics and training interference affect the achievability of the theoretical capacity bounds for in-weight learning?
- Basis in paper: [explicit] The Limitations section explicitly notes that "optimization dynamics are not considered in our theoretical bounds," creating a gap between static capacity and learnability.
- Why unresolved: The paper proves that a model *of size P* can store *P* facts, but it does not prove that standard gradient descent will successfully converge on this configuration without getting stuck or overwriting previous facts.
- What evidence would resolve it: Studies analyzing the loss landscape of memorization tasks or experiments measuring the "optimization gap" between the theoretical maximum capacity and the actual capacity achieved via standard training loops.

## Limitations
- The theoretical parameter bounds assume facts are independent triplets, but real-world knowledge often has hierarchical or graph structure that could reduce effective parameter requirements through compression.
- The phase transition from memorization to rule-based retrieval is observed empirically but not formally proven; the exact conditions and universality across different tool interfaces remain unclear.
- Capability degradation measurements rely on HellaSwag as a proxy for general capabilities, but this may not capture all types of knowledge interference that could occur.

## Confidence
- **High confidence**: In-weight models require increasing parameters for more facts (Mechanism 1) - supported by both theory and controlled experiments.
- **Medium confidence**: In-tool models show phase transition to rule-based retrieval (Mechanism 2) - empirical evidence strong but theoretical proof incomplete.
- **Medium confidence**: In-tool learning preserves general capabilities better than in-weight (Mechanism 3) - supported by large-scale experiments but depends on HellaSwag as proxy.

## Next Checks
1. Test correlation parameter α systematically: Generate datasets with varying fact correlations (α from 0 to 1) and measure how this affects the linear scaling relationship between parameters and facts memorized in-weight.
2. Validate phase transition across architectures: Repeat the controlled experiments with different model sizes and architectures to confirm the universality of the memorization-to-rule-learning transition.
3. Measure interference across capability types: Use multiple capability benchmarks beyond HellaSwag (e.g., MMLU, GSM8K) to verify that in-tool learning preserves diverse general capabilities while in-weight degrades them.