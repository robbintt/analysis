---
ver: rpa2
title: Performance of Large Language Models in Answering Critical Care Medicine Questions
arxiv_id: '2509.19344'
source_url: https://arxiv.org/abs/2509.19344
tags:
- performance
- questions
- across
- domains
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated the performance of two Meta-Llama 3.1 models
  (8B and 70B parameters) on 871 real-world Critical Care Medicine board-style questions.
  Using in-context learning with chain-of-thought reasoning, multi-shot examples,
  and varied temperatures, the Llama3.1:70B model achieved an average accuracy of
  60%, outperforming the 8B model by 30% across all domains.
---

# Performance of Large Language Models in Answering Critical Care Medicine Questions

## Quick Facts
- arXiv ID: 2509.19344
- Source URL: https://arxiv.org/abs/2509.19344
- Reference count: 0
- Two Llama 3.1 models (8B and 70B) achieved 60% and 30% average accuracy on CCM board-style questions respectively

## Executive Summary
This study evaluated two Meta-Llama 3.1 models (8B and 70B parameters) on 871 real-world Critical Care Medicine board-style questions using in-context learning with chain-of-thought reasoning. The 70B model achieved 60% average accuracy, outperforming the 8B model by 30 percentage points across all domains. Performance varied significantly by specialty, with highest accuracy in Research/Ethics (68.4%) and Surgery/Trauma (68.3%), and lowest in Renal (47.9%) and Gastrointestinal (53.1%). The 70B model maintained stable accuracy (59-60%) across different shot numbers and temperature settings, while the 8B model showed more variable performance.

## Method Summary
The study used Meta-Llama 3.1 8B and 70B models to answer 871 CCM board-style multiple-choice questions. Evaluation employed in-context learning with chain-of-thought reasoning and multi-shot examples (0-5 shots). Temperature settings of 0, 0.2, 0.5, and 1.0 were tested. The non-public question bank covered 12 clinical domains. Accuracy was measured as the percentage of correct answers across domains, shot counts, and temperature settings.

## Key Results
- Llama3.1:70B achieved 60% average accuracy, outperforming 8B by 30 percentage points
- Highest performance in Research/Ethics (68.4%) and Surgery/Trauma (68.3%) domains
- Lowest performance in Renal (47.9%) and Gastrointestinal (53.1%) domains
- 70B model maintained stable 59-60% accuracy across shot numbers and temperature settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scale-dependent performance gains manifest in subspecialty medical reasoning.
- Mechanism: The 70B parameter model encodes more nuanced representations of complex clinical relationships, enabling better discrimination among plausible distractors in board-style questions.
- Core assumption: Training corpora contained sufficient CCM-adjacent content for latent knowledge encoding.
- Evidence anchors:
  - [abstract]: "Llama3.1:70B outperformed 8B by 30%, with 60% average accuracy."
  - [results]: "Llama3.1:70B consistently outperformed Llama3.1:8B in all domains."
- Break condition: If training data overlap exists between pre-training corpus and question bank, observed gains may partially reflect memorization rather than reasoning.

### Mechanism 2
- Claim: Domain-specific knowledge density in pre-training data predicts subspecialty accuracy.
- Mechanism: Higher performance in Research/Ethics and Surgery/Trauma suggests these domains have denser representation in training corpora.
- Core assumption: Board-style questions accurately sample domain complexity.
- Evidence anchors:
  - [abstract]: "Performance varied across domains, highest in Research (68.4%) and lowest in Renal (47.9%)."
  - [results]: "It had the lowest accuracies in Renal (47.9%), Gastrointestinal (53.1%)."
- Break condition: If question difficulty varies systematically by domain, performance differences may reflect reasoning complexity rather than knowledge density.

### Mechanism 3
- Claim: Larger models exhibit prompt-configuration stability; smaller models remain sensitive.
- Mechanism: The 70B model maintains consistent 59-60% accuracy across shot numbers and temperature settings, suggesting robust internal representations that resist prompt perturbation.
- Core assumption: Temperature and shot variations adequately test robustness.
- Evidence anchors:
  - [results]: "Llama3.1:70B maintained stable performance at 60% regardless of the number of shots."
  - [results]: "Llama3.1:8B showed fluctuating accuracy lowest at two shots (26.5%) and highest at 0 shots (36.1%)."
- Break condition: If chain-of-thought prompting was applied uniformly without ablation, we cannot isolate which prompt component drives stability.

## Foundational Learning

- Concept: **In-context learning with chain-of-thought reasoning**
  - Why needed here: The study uses multi-shot examples plus CoT prompting to elicit reasoning; understanding this technique is prerequisite to interpreting why temperature/shot experiments were structured this way.
  - Quick check question: Can you explain why adding reasoning examples before the target question might improve accuracy on multi-step clinical problems?

- Concept: **Temperature as sampling diversity control**
  - Why needed here: Temperature was varied (0, 0.2, 0.5, 1) to test output stability; low temperature yields deterministic outputs while higher temperature increases sampling variance.
  - Quick check question: If a model's accuracy drops at temperature 1.0 vs 0.0, what does this suggest about the model's confidence distribution over answer tokens?

- Concept: **Parameter scale vs. task complexity matching**
  - Why needed here: The 30-point gap between 8B and 70B models suggests subspecialty board questions exceed the reasoning capacity of smaller models; this informs deployment decisions.
  - Quick check question: Would you expect similar scale-dependent gaps on medical student-level vs. subspecialty board questions? Why or why not?

## Architecture Onboarding

- Component map: Meta-Llama 3.1 (8B, 70B) -> Chain-of-thought prompting + multi-shot examples -> Temperature settings (0, 0.2, 0.5, 1.0) -> 871 CCM board-style MCQs -> Accuracy metric

- Critical path:
  1. Load question bank with domain labels
  2. Format each question with CoT prompt template and N-shot examples
  3. Run inference at specified temperature
  4. Parse model output to extract answer selection
  5. Compute accuracy stratified by domain, shot count, temperature

- Design tradeoffs:
  - **Non-public question bank**: Reduces contamination risk but limits reproducibility
  - **Single model family (Llama 3.1)**: Controls for architecture but limits generalizability to other model families
  - **No fine-tuning**: Establishes base performance but may underrepresent achievable accuracy with domain adaptation

- Failure signatures:
  - Accuracy below random guessing (25% for 4-option MCQ) suggests systematic bias or parsing failure
  - High variance across temperature for 8B model indicates unstable reasoning paths
  - Domain-specific cliffs (e.g., Renal at 47.9%) may indicate systematic knowledge gaps

- First 3 experiments:
  1. **Contamination check**: Test whether questions or similar content appear in Llama 3.1 pre-training corpus via membership inference or n-gram overlap analysis.
  2. **Error analysis by domain**: Manually review 50 incorrect answers in Renal and GI domains to characterize failure modes (knowledge gap, reasoning error, misinterpretation).
  3. **Fine-tuning pilot**: Fine-tune 8B model on CCM case explanations and re-evaluate to measure domain adaptation gap relative to 70B base performance.

## Open Questions the Paper Calls Out

- Can the current performance metrics be replicated on an independent, external Critical Care Medicine dataset? [explicit] The authors state, "From here, we plan to validate our findings using an independent dataset."
- Does domain-specific fine-tuning significantly improve accuracy in low-performing subspecialties like Nephrology and Gastroenterology? [explicit] The discussion highlights the "need for improving those bases models to specific subspeciality domains" and plans to "determine whether further model training or fine tuning will be necessary."
- How does static question-answering performance correlate with performance in dynamic simulated clinical environments? [explicit] The authors propose to "incorporate simulated testing" in future work.

## Limitations

- Non-public dataset prevents independent validation and reproducibility
- Single model family evaluation limits generalizability to other architectures
- No fine-tuning comparison leaves open whether observed gaps are inherent to model scale or addressable through adaptation

## Confidence

- **High confidence**: Scale-dependent performance gains (70B vs 8B) and domain-specific accuracy variation
- **Medium confidence**: Claims about prompt-configuration stability without ablation studies
- **Low confidence**: Inferences about pre-training data domain density without corpus analysis

## Next Checks

1. **Contamination audit**: Conduct n-gram overlap analysis between the CCM question bank and publicly available Llama 3.1 pre-training data fragments to rule out memorization effects driving the 30-point performance gap.
2. **Error mode characterization**: Perform blinded review of 100 incorrect answers (50 each from lowest-performing domains) to classify failure modes: knowledge gaps vs. reasoning complexity vs. prompt misinterpretation.
3. **Fine-tuning impact study**: Fine-tune Llama 3.1 8B on CCM case explanations for 2-3 epochs and re-evaluate to quantify the adaptation gap versus the 70B base model.