---
ver: rpa2
title: 'Language Models Are Borrowing-Blind: A Multilingual Evaluation of Loanword
  Identification across 10 Languages'
arxiv_id: '2510.26254'
source_url: https://arxiv.org/abs/2510.26254
tags:
- language
- loanwords
- languages
- loanword
- lexical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates the ability of pretrained language models\
  \ to identify loanwords versus native vocabulary across 10 languages using the ConLoan\
  \ dataset. Despite explicit instructions and contextual information, models\u2014\
  including large language models\u2014perform poorly, with average F1-scores below\
  \ 0.70."
---

# Language Models Are Borrowing-Blind: A Multilingual Evaluation of Loanword Identification across 10 Languages

## Quick Facts
- **arXiv ID:** 2510.26254
- **Source URL:** https://arxiv.org/abs/2510.26254
- **Reference count:** 0
- **Primary result:** Pretrained language models perform poorly at loanword identification across 10 languages, with F1-scores below 0.70 even with fine-tuning.

## Executive Summary
This study evaluates the ability of pretrained language models to identify loanwords versus native vocabulary across 10 languages using the ConLoan dataset. Despite explicit instructions and contextual information, models—including large language models—perform poorly, with average F1-scores below 0.70. Prompt-based LLMs like Gemini, OpenAI, and Llama achieve F1-scores ranging from 0.23 to 0.47, while fine-tuned multilingual encoders such as XLM-RoBERTa reach up to 0.85. Fine-tuning substantially improves performance over zero-shot baselines. Analysis reveals systematic errors: models frequently misclassify code-switches as loanwords, struggle with named entities and scientific terminology, and rely heavily on orthographic and etymological cues rather than deeper linguistic integration. These findings highlight that loanword identification remains a challenging, unsolved task, even with task-specific training, underscoring the need for models to better capture sociolinguistic context and lexical assimilation.

## Method Summary
The study evaluates loanword identification as a sequence labeling task using BIO tags (O, B-LOAN, I-LOAN) across 10 languages (Chinese, French, German, Greek, Icelandic, Italian, Northern Kurdish, Portuguese, Russian, Spanish). Two approaches are tested: (1) LLM prompting with three prompt variants (minimal, etymological, usage-based) using zero-shot and few-shot settings; (2) fine-tuning multilingual encoders (mBERT, XLM-RoBERTa-base/large, ELECTRA) with 10 epochs, batch size 16, AdamW optimizer, and cross-entropy loss. The ConLoan dataset provides sentence-level loanword span annotations with 80/20 train/test splits. Evaluation uses seqeval span-level F1-score with strict and relaxed protocols for LLMs.

## Key Results
- Fine-tuned multilingual encoders achieve F1 up to 0.85, substantially outperforming zero-shot baselines (F1 < 0.2)
- Prompt-based LLMs (Gemini, OpenAI, Llama) achieve F1-scores ranging from 0.23 to 0.47
- Models systematically misclassify code-switches as loanwords and struggle with named entities and scientific terminology
- Fine-tuning substantially improves performance over zero-shot baselines across all tested architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning multilingual encoders on labeled loanword data substantially improves identification performance over zero-shot baselines.
- Mechanism: Task-specific training enables models to learn boundary detection patterns for borrowed vs. native vocabulary through supervised signal on token-level BIO tags.
- Core assumption: Labeled loanword data adequately represents the distribution of borrowing phenomena across languages and integration levels.
- Evidence anchors: [abstract] Fine-tuning substantially improves performance over zero-shot baselines; fine-tuned XLM-RoBERTa reaches up to 0.85 F1. [section 4.2] Zero-shot models average F1 < 0.2; fine-tuned XLM-RL achieves 0.8513.

### Mechanism 2
- Claim: Models rely primarily on orthographic and etymological surface cues rather than sociolinguistic integration signals when identifying loanwords.
- Mechanism: Pretrained representations capture superficial phonological/orthographic patterns associated with foreign origin but lack grounding in speaker intent, monolingual usage, and lexical assimilation.
- Core assumption: Surface features correlate imperfectly with true borrowing status; deeper integration signals are not well-captured in pretraining.
- Evidence anchors: [abstract] Analysis reveals systematic errors: models rely heavily on orthographic and etymological cues. [section 5.1] LLMs misclassify code-switches as loanwords; fail to distinguish 'really' in Kurdish (code-switch) from 'deal' in French (loanword).

### Mechanism 3
- Claim: Prompt formulation with explicit definitions has mixed effects on LLM performance for loanword identification.
- Mechanism: Detailed definitions may bias models toward etymological reasoning (over-detection) or, in usage-based formulations, help distinguish code-switching from borrowing—though overall task remains difficult.
- Core assumption: LLMs can operationalize definitional nuances when prompted; failure stems from representation limitations rather than comprehension.
- Evidence anchors: [section 3.2] Three prompt variants tested: minimal, etymological, usage-based. [section 4.1] Prompt 1 (minimal) yields higher F1 (0.398) than Prompt 2 (etymological, 0.337).

## Foundational Learning

- **Sequence Labeling with BIO Tagging**
  - Why needed here: Loanword identification is formulated as token-level classification (B-LOAN, I-LOAN, O) to handle multi-word borrowings.
  - Quick check question: Can you explain why span-level F1 (seqeval) penalizes boundary errors differently than token-level accuracy?

- **Code-Switching vs. Lexical Borrowing Distinction**
  - Why needed here: Models systematically conflate these phenomena; understanding the difference (alternation vs. integration, bilingual vs. monolingual usage) is essential for interpreting errors.
  - Quick check question: In the sentence "really ha, nizanim..." (Kurdish), why is 'really' a code-switch rather than a loanword?

- **Multilingual Encoder Architectures**
  - Why needed here: Performance varies dramatically between encoder-only models (XLM-R, mBERT) and generative LLMs; understanding token classification heads vs. generation is critical for implementation.
  - Quick check question: Why would XLM-R with a token classification head outperform GPT-4 on this span-detection task despite GPT-4's larger scale?

## Architecture Onboarding

- **Component map**: ConLoan dataset (10 languages, BIO-tagged spans) -> Tokenization with respective model tokenizers -> Fine-tuning (XLM-RoBERTa-large) with token classification head OR Prompting (LLMs) with 3 variants -> Evaluation with seqeval span-level F1

- **Critical path**: Start with ConLoan data -> tokenize with BIO tags -> fine-tune XLM-RL for token classification -> evaluate with seqeval. For LLMs, format prompts with explicit instructions -> parse string-list outputs -> apply strict/relaxed matching.

- **Design tradeoffs**: LLMs: Flexible prompting, no training required, but low F1 (0.23-0.47) and inconsistent output parsing; Fine-tuned encoders: Higher F1 (up to 0.85), but requires labeled data and training compute; Prompt choice: Minimal definitions yield higher recall; usage-based definitions improve code-switch precision but risk over-correction.

- **Failure signatures**: Code-switches misclassified as loanwords (e.g., 'really' in Kurdish); Named entities tagged as borrowings (e.g., PISA-Studie, NASA); Greco-Latin scientific terms either over-detected (LLMs) or under-detected (fine-tuned) based on etymological cues; Llama output format breaks under certain few-shot prompts, collapsing F1 to near-zero.

- **First 3 experiments**: 1) Replicate zero-shot baseline with XLM-R-base on ConLoan to confirm F1 < 0.02; then fine-tune and verify improvement to ~0.83. 2) Test Gemini with Prompt 1 (minimal) on 50 held-out examples per language; manually inspect false positives for named entities and code-switches. 3) Evaluate XLM-RL fine-tuned model on out-of-domain scientific texts to probe generalization of Greco-Latin term handling.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can models be trained to classify loanwords along an integration continuum (from fresh borrowings to fully assimilated vocabulary) rather than using binary labels?
- Basis in paper: "Future work should consider a more fine-grained identification task where loanwords are categorized based on their integration status or within a continuum (Poplack and Sankoff, 1984)."
- Why unresolved: Current BIO tagging treats loanwords as binary; the analysis shows models fail to capture degrees of lexical assimilation, particularly for Greco-Latin scientific terms that are historically borrowed but synchronically native.

### Open Question 2
- Question: What architectural or training modifications would enable models to reliably distinguish code-switches from established loanwords?
- Basis in paper: Analysis shows both LLMs and fine-tuned models "struggle to operationalize the nuanced boundary between code-switching and lexical borrowing" and "over-rely on orthographic cues and fail to account for sociolinguistic integration."
- Why unresolved: Prompt 3's usage-based definition improved precision for some models but caused others (e.g., OpenAI) to overcorrect, misclassifying established loanwords like French "deal" as code-switches.

### Open Question 3
- Question: How does automatic replacement of loanwords with native alternatives affect downstream NLP task performance?
- Basis in paper: "Additionally, creating and analyzing models with controlled vocabulary where loanwords are automatically replaced by native alternatives should be explored."
- Why unresolved: Prior ConLoan work showed NMT quality degrades with native replacements, but systematic evaluation across tasks (sentiment analysis, NER, summarization) and languages remains unexplored.

## Limitations
- The ConLoan dataset's construction details remain underspecified, particularly regarding annotation consistency across languages and annotator guidelines for distinguishing highly integrated loanwords from native vocabulary.
- The study's evaluation focuses on 10 geographically and linguistically diverse languages, but the extent to which findings generalize to other language families or low-resource languages remains unknown.
- While the paper identifies systematic error patterns, the root causes—whether they stem from pretraining data composition, architectural limitations, or evaluation methodology—are not definitively established.

## Confidence
- **High confidence**: Fine-tuned multilingual encoders substantially outperform zero-shot LLMs on loanword identification (F1 up to 0.85 vs 0.23-0.47)
- **Medium confidence**: Models rely on orthographic/etymological cues rather than integration-level signals; systematic errors with code-switches and named entities
- **Low confidence**: The specific mechanisms by which prompt formulation affects performance, particularly the counterintuitive effects of few-shot examples on certain prompt variants

## Next Checks
1. Conduct cross-linguistic analysis to determine whether error patterns correlate with specific language families or borrowing histories, testing the hypothesis that model failures reflect dataset biases rather than universal limitations
2. Implement controlled experiments varying prompt specificity and exemplar selection to isolate the effects of instruction-following versus representation quality on LLM performance
3. Test fine-tuned models on out-of-domain scientific and technical texts to evaluate whether training on ConLoan enables generalization to domain-specific borrowing patterns or merely memorizes training distribution