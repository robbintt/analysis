---
ver: rpa2
title: 'From Word to World: Can Large Language Models be Implicit Text-based World
  Models?'
arxiv_id: '2512.18832'
source_url: https://arxiv.org/abs/2512.18832
tags:
- world
- action
- agent
- door
- environment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates whether large language models can serve
  as effective world models for text-based agents, enabling them to learn from simulated
  interaction. The authors formalize world modeling as next-state prediction under
  a fixed interaction protocol and evaluate it across five text-based environments.
---

# From Word to World: Can Large Language Models be Implicit Text-based World Models?

## Quick Facts
- arXiv ID: 2512.18832
- Source URL: https://arxiv.org/abs/2512.18832
- Reference count: 40
- This work investigates whether large language models can serve as effective world models for text-based agents, enabling them to learn from simulated interaction.

## Executive Summary
This work investigates whether large language models can serve as effective world models for text-based agents, enabling them to learn from simulated interaction. The authors formalize world modeling as next-state prediction under a fixed interaction protocol and evaluate it across five text-based environments. They find that LLMs trained with supervised fine-tuning exhibit coherent latent state, scale predictably with data and model size, and improve agent performance via action verification, synthetic data generation, and warm-starting reinforcement learning. However, gains depend critically on behavioral coverage and environment complexity, with open-ended settings requiring substantially more data and larger models to achieve robustness.

## Method Summary
The method involves fine-tuning LLM base models (Qwen2.5-7B or Llama-3.1-8B) on trajectories collected from text-based environments using a GPT-4o policy. Trajectories are formatted as multi-turn dialogues with initialization context for structured environments. The fine-tuning objective is next-state prediction: given dialogue history and current action, predict the environment response. Training uses LLaMA-Factory with batch size 128, learning rate 1e-5, 5 epochs, and BF16 precision. Evaluation measures single-step accuracy, multi-step consistency ratios, and downstream agent utility across five environments (ALFWorld, SciWorld, TextWorld, WebShop, StableToolBench).

## Key Results
- Supervised fine-tuning on interaction trajectories enables LLMs to internalize environment dynamics, achieving high-fidelity next-state prediction that prompting alone cannot match
- Joint training across multiple text environments yields shared dynamics representations that transfer across domains, accelerating learning compared to isolated training
- World model rollouts provide practical agent utility through action verification, synthetic data generation, and RL warm-starting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised fine-tuning on interaction trajectories enables LLMs to internalize environment dynamics, achieving high-fidelity next-state prediction that prompting alone cannot match.
- Mechanism: The world model is trained to predict the next environment response conditioned on dialogue history and current action (Eq. 2). This dynamics-aligned supervision transfers the model's general language capabilities into domain-specific transition prediction.
- Core assumption: Trajectory data collected by a capable policy (GPT-4o) provides sufficient coverage of valid state transitions for the model to learn transferable dynamics rather than memorize specific paths.
- Evidence anchors:
  - [section] Table 1 shows SFT models achieving 99%/98% accuracy on ALFWorld/SciWorld vs. ~50-65% for few-shot prompting; StableToolBench improves from ~13% F1 (few-shot) to ~79% F1 (SFT)
  - [section 5.1] "Supervised fine-tuning yields substantial improvements... robust world modeling requires dynamics-aligned training: prompting alone cannot capture the full diversity of transition patterns"
  - [corpus] Related work (Wang et al., 2024; Xie et al., 2024) shows limited accuracy with prompting-only approaches, consistent with this finding
- Break condition: When behavioral coverage is narrow (single expert policy only), the world model fails to generalize to out-of-distribution agents with different action patterns (Table 3 shows consistency drops from 0.81 to 0.49 for weaker agents trained on expert-only data).

### Mechanism 2
- Claim: Joint training across multiple text environments yields shared dynamics representations that transfer across domains, accelerating learning compared to isolated training.
- Mechanism: Mixed training (Mix3, Mix4, Mix5 configurations) exposes the model to shared physical, procedural, and narrative dynamics across environments. The model internalizes reusable transition patterns rather than environment-specific rules.
- Core assumption: Text-based environments share underlying structure (spatial reasoning, object manipulation, causal dependencies) that can be abstracted and transferred.
- Evidence anchors:
  - [section 6.4] Figure 5 shows mixed training consistently accelerates learning with "particularly strong gains in TextWorld and WebShop"
  - [section 6.4] "Mixed data provides stable positive gains and, importantly, enables practical deployments where a single world model can robustly serve multiple environments"
  - [corpus] No direct corpus evidence on cross-environment transfer for world models; this appears novel
- Break condition: Environments with fundamentally incompatible dynamics (e.g., StableToolBench's single-turn, schema-centric structure) show degraded performance under mixed training compared to specialized models.

### Mechanism 3
- Claim: World model rollouts provide practical agent utility through three pathways: action verification, synthetic data generation, and RL warm-starting.
- Mechanism: (1) Before irreversible actions, the agent simulates outcomes in the world model and only executes if success is predicted; (2) Successful WM trajectories substitute for real experience in agent SFT; (3) Pre-training agents on world-model objectives (predicting next states) provides useful inductive bias before policy learning.
- Core assumption: World model predictions remain sufficiently accurate over the planning horizon to guide real decisions, and simulated success correlates with real-world success.
- Evidence anchors:
  - [section 7.1] Table 4 shows verification improves WebShop success by +1.5 to +15.6 percentage points depending on agent and verification budget
  - [section 7.2] Figure 6 shows synthetic data matching real data quality in SciWorld; mixed real+synthetic achieves best results
  - [section 7.3] Figure 7 shows early experience pipeline achieving higher final success rates with faster convergence
  - [corpus] LLM-Driven Policy Diffusion (arxiv 2509.00347) supports offline RL generalization gains from learned dynamics, though in different context
- Break condition: In open-ended environments (WebShop), world model consistency degrades due to high output diversity (CR ~0.56-0.82 vs. ~0.95+ in structured environments), limiting verification reliability. Excessive verification (>10 checks) can induce distribution shift that destabilizes behavior.

## Foundational Learning

- Concept: **Partially Observable Markov Decision Processes (POMDPs)**
  - Why needed here: Text-based environments are inherently POMDPs—the true state exceeds textual observations (e.g., contents of closed drawers). Understanding this framing explains why world models need initialization context beyond agent observations.
  - Quick check question: Can you explain why providing full initial state information (Figures 8-9) to the world model improves prediction accuracy in ALFWorld but isn't possible in WebShop?

- Concept: **Supervised Fine-Tuning (SFT) for LLMs**
  - Why needed here: The paper's core intervention is SFT on trajectory data. Understanding training dynamics (data scaling, epoch selection, overfitting risks) is essential for reproducing results.
  - Quick check question: Why does Figure 2 show structured environments saturating at ~20K trajectories while open-ended environments show no saturation at 160K?

- Concept: **Consistency Ratio (W2R/Real) as fidelity metric**
  - Why needed here: Single-step accuracy doesn't capture long-horizon reliability. The CR metric measures whether trajectories generated in the world model remain executable when replayed in the real environment.
  - Quick check question: What does CR > 1.0 indicate (observed in some ALFWorld settings), and is this desirable or a measurement artifact?

## Architecture Onboarding

- Component map:
  - Agent (ReAct-style) -> World Model -> Environment
  - World Model: Takes (initialization context, action history, current action) -> predicts (next state, reward)
  - Environment: Five testbeds—ALFWorld (embodied), SciWorld (science experiments), TextWorld (narrative), WebShop (web navigation), StableToolBench (API tools)

- Critical path:
  1. Collect ~40K trajectories per environment using AgentGym framework (GPT-4o policy, temperature=1.0, max 50 turns)
  2. Format trajectories as multi-turn dialogues; for ALFWorld/SciWorld, prepend initial state info
  3. Train with LLaMA-Factory: batch_size=128, lr=1e-5, 5 epochs, warmup=10 steps
  4. Evaluate: (a) single-step EM accuracy, (b) multi-step consistency ratio, (c) agent utility gains

- Design tradeoffs:
  - **Model size vs. environment complexity**: 1.5B sufficient for structured environments; open-ended settings require 7B+ (Figure 3)
  - **Data diversity vs. training efficiency**: Expert-only data faster to collect but fails on OOD agents; mixed-agent trajectories improve generalization (Table 3: GPT-4o-mini CR improves 0.49→0.81)
  - **Verification budget**: 2-10 checks optimal; excessive verification causes distribution shift and diminishing returns (Table 4)

- Failure signatures:
  - **Low consistency in open-ended environments**: WebShop CR typically <0.8 due to search result diversity; mitigate by grounding rollouts with real observations
  - **OOD agent failure**: When test agent behavior diverges from training policy, CR drops sharply; requires diverse training trajectories
  - **Drift in long rollouts**: Multi-step error accumulation in WebShop/StableToolBench; requires periodic real-environment anchoring

- First 3 experiments:
  1. **Baseline fidelity check**: Train Qwen2.5-7B on 40K ALFWorld trajectories; verify EM accuracy approaches 99%. If <95%, check initialization context formatting and trajectory quality.
  2. **Consistency stress test**: Run 100 rollouts each with GPT-4o and GPT-4o-mini agents; compute CR. If CR <0.7 for weaker agents, expand training data to include diverse agent policies.
  3. **Utility validation**: Implement verification on WebShop checkout action with budget=4; measure success rate improvement. If <2% gain, inspect WM prediction accuracy on checkout-specific transitions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the fidelity and agent utility of LLM-based world models observed in text environments be effectively transferred to multimodal or embodied domains?
- Basis in paper: [explicit] The Conclusion states the work "opens the door to extending these ideas beyond text to richer, multimodal, and embodied domains."
- Why unresolved: The current study is strictly limited to text-based environments (ALFWorld, WebShop, etc.) and does not test visual or physical grounding.
- What evidence would resolve it: Empirical results showing that fine-tuning on multimodal trajectories maintains coherent latent states and improves agent performance in visual navigation or robotics tasks.

### Open Question 2
- Question: What are the precise data and model scaling laws required to close the performance gap between structured and open-ended environments?
- Basis in paper: [explicit] The Abstract and Findings note that "open-ended settings require substantially more data and larger models to achieve robustness" compared to structured environments which saturate quickly.
- Why unresolved: The paper identifies the gap (structured settings saturate at ~20K trajectories while open-ended settings do not saturate at 160K) but does not propose a solution for data efficiency in open-ended domains.
- What evidence would resolve it: A scaling law analysis or a novel training method (e.g., curriculum learning) that allows open-ended world models to saturate performance with data volumes comparable to structured settings.

### Open Question 3
- Question: How can world models maintain long-horizon consistency in open-ended environments without relying on frequent grounding in real-world observations?
- Basis in paper: [inferred] Section 5.2 notes that consistency in open-ended environments like WebShop is low (often <80%) and errors are "substantially mitigated by grounding model rollouts with real observations."
- Why unresolved: While grounding helps, it defeats the purpose of a pure simulator for "imagined interaction." The paper does not solve the compounding error drift in open-ended simulation without external anchoring.
- What evidence would resolve it: Mechanisms that improve internal state tracking (e.g., external memory or state regularization) allowing high consistency ratios (W2R) in open-ended tasks without intermediate real-environment feedback.

## Limitations
- Data requirements for open-ended environments remain prohibitive, with WebShop showing no saturation even at 160K trajectories
- The study only explores supervised fine-tuning, not alternative approaches like retrieval-augmented generation or advanced prompting
- The assertion that 7B+ models are required for open-ended environments extrapolates from limited experiments without establishing clear scaling laws

## Confidence
- **High confidence**: The supervised fine-tuning mechanism works reliably for structured environments with near-perfect single-step accuracy and high consistency ratios
- **Medium confidence**: Cross-environment transfer benefits from mixed training are promising but only demonstrated on a subset of environments
- **Low confidence**: The 7B model size requirement for open-ended environments is based on limited empirical data points and may represent gradual improvement rather than a true threshold

## Next Checks
1. **Scaling validation**: Test intermediate model sizes (3B, 5B) on WebShop to establish whether the 7B requirement is a true threshold or gradual improvement
2. **Coverage stress test**: Systematically vary training data diversity by mixing trajectories from different agent types and measure consistency ratio degradation
3. **Alternative architecture comparison**: Implement a retrieval-augmented world model and compare consistency ratios against the pure SFT approach