---
ver: rpa2
title: The Uneven Impact of Post-Training Quantization in Machine Translation
arxiv_id: '2508.20893'
source_url: https://arxiv.org/abs/2508.20893
tags:
- quantization
- language
- languages
- translation
- gguf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents the first large-scale evaluation of post-training
  quantization (PTQ) on machine translation across 55 languages using five LLMs ranging
  from 1.7B to 70B parameters. The research compares four quantization techniques
  (AWQ, BitsAndBytes, GGUF, and AutoRound) at 2-bit and 4-bit precisions.
---

# The Uneven Impact of Post-Training Quantization in Machine Translation

## Quick Facts
- arXiv ID: 2508.20893
- Source URL: https://arxiv.org/abs/2508.20893
- Authors: Benjamin Marie; Atsushi Fujita
- Reference count: 19
- Primary result: First large-scale evaluation of PTQ on 55 languages shows 2-bit quantization severely degrades low-resource and typologically diverse languages while 4-bit preserves quality for high-resource languages and large models.

## Executive Summary
This study presents the first comprehensive evaluation of post-training quantization (PTQ) on machine translation across 55 languages using five large language models ranging from 1.7B to 70B parameters. The research compares four quantization techniques at 2-bit and 4-bit precisions, revealing that while aggressive quantization saves memory, it disproportionately harms low-resource and typologically diverse languages. GGUF variants provide the most consistent performance, and language-matched calibration offers benefits primarily in low-bit scenarios. The findings highlight critical tradeoffs between compression efficiency and translation quality, particularly for underrepresented languages.

## Method Summary
The study evaluates four PTQ methods (AWQ, BitsAndBytes, GGUF, and AutoRound) on five LLM models (Qwen3-1.7B/8B/32B, Llama-3.1-8B-Instruct, Llama-3.3-70B) using the WMT24++ benchmark with 55 languages. Quantization was applied to linear layers only, with GGUF using importance matrix estimation from 20k WikiText samples. Translation quality was measured using COMET (primary), chrF, and BLEU scores across 110 translation directions (En→Xx and Xx→En). Language-matched calibration was tested at 2-bit precision, and inference used greedy decoding with temperature and top-p sweeps.

## Key Results
- GGUF consistently yields smallest quality loss across all quantization settings
- 4-bit quantization preserves translation quality for high-resource languages while 2-bit causes significant degradation
- Low-resource and typologically diverse languages (particularly Indic scripts) suffer disproportionate quality loss under quantization
- Models <10B parameters show clear degradation for low-resource languages even at 4-bit
- Language-matched calibration benefits primarily appear at 2-bit precision

## Why This Works (Mechanism)

### Mechanism 1: Importance-Guided Error Minimization
Quantization methods utilizing importance matrices (e.g., GGUF) preserve translation quality better by allocating higher precision to weights that significantly impact output. This reduces quantization error in critical transformer layers compared to uniform scaling. The mechanism assumes calibration data represents translation feature distributions across languages.

### Mechanism 2: Long-Tail Feature Vulnerability
Low-resource languages degrade faster because quantization acts as a low-pass filter that disproportionately prunes rare linguistic features. These features, being less frequent during training, sit on the periphery of weight distributions and are most susceptible to rounding errors during quantization.

### Mechanism 3: Calibration-Constrained Reconstruction
Language-matched calibration mitigates degradation only at extreme 2-bit compression where the grid is too coarse to support multiple scripts simultaneously. At 4-bit, the grid is dense enough to approximate multiple distributions without specific alignment.

## Foundational Learning

- **Post-Training Quantization (PTQ) vs. Quantization-Aware Training (QAT)**: PTQ applies quantization to frozen weights post-hoc without gradient updates, unlike QAT where the model learns to recover precision during training.
  - *Quick check*: Does the model undergo gradient updates during the quantization process? (Answer: No)

- **Group-wise Quantization**: Weights are split into groups (e.g., 128 channels) with independent scale factors to handle varying distributions across channels/heads, as single scale factors for entire weight tensors are insufficient.
  - *Quick check*: Why is a single scale factor for an entire weight tensor insufficient for LLMs? (Answer: Weight distributions vary widely across channels/heads)

- **Metric Fragility in MT**: COMET scores are not interval-scaled and are biased by language morphology, making cross-language averaging methodologically unsound.
  - *Quick check*: Why is reporting a single average COMET score across 55 languages methodologically unsound? (Answer: Metrics are not calibrated across languages and penalize morphologically rich languages disproportionately)

## Architecture Onboarding

- **Component map**: WMT24++ dataset -> LLM Transformer (1.7B-70B params) -> Quantization Wrapper (Weight-only) -> Greedy/Sampling Decoder

- **Critical path**: 1) Model Selection: >10B for low-resource safety, 2) Quantization Choice: GGUF default, avoid BnB for 70B+, 3) Calibration: language-matched for 2-bit

- **Design tradeoffs**: Memory vs. low-resource accuracy (2-bit on <8B eliminates Indic/African language capability), generic vs. specific calibration (adds complexity for extreme compression)

- **Failure signatures**: Intra-model asymmetry (Latin scripts high, Indic scripts collapse), language elimination (incoherent output for specific low-resource languages)

- **First 3 experiments**: 1) Baseline stability: Qwen3-8B Q4_K_M vs GGUF for Japanese vs Bengali, 2) Calibration ablation: Llama-3.1-8B Q2_K with English vs target language calibration, 3) Scale stress test: Llama-3.3-70B vs Qwen3-1.7B at 4-bit

## Open Questions the Paper Calls Out

1. Does joint activation and KV cache quantization exacerbate or mitigate disproportionate degradation for low-resource languages under weight-only PTQ?

2. Can domain-specific calibration using in-domain parallel corpora reduce quantization-induced errors for low-resource languages compared to generic Wikipedia-based calibration?

3. Do new quantization formats (MXFP4, NVFP4) preserve multilingual translation quality more effectively than current methods, particularly for smaller models under 10B parameters?

4. Does fine-grained linguistic error analysis reveal systematic patterns in how quantization degrades morphological agreement, syntactic structure, or lexical choice differently across language families?

## Limitations

- Exact translation prompt text is unspecified, potentially affecting model behavior across languages
- COMET metric reliability varies across languages and may not reflect true quality differences
- Calibration data domain (Wikipedia) may not adequately represent specialized translation domains

## Confidence

**High Confidence**:
- GGUF variants provide more consistent performance across quantization settings
- 4-bit quantization generally preserves translation quality better than 2-bit for high-resource languages
- Low-resource languages experience disproportionate degradation under quantization, particularly in 2-bit settings
- Model size below 10B parameters shows clear degradation for low-resource languages

**Medium Confidence**:
- Language-matched calibration provides benefits primarily in 2-bit scenarios
- The interaction between model size, quantization algorithm, and language resource level determines overall robustness
- GGUF's importance matrix estimation mechanism is the primary driver of its superior performance

**Low Confidence**:
- Exact quantitative thresholds for when specific quantization methods fail
- The extent to which calibration language choice affects 2-bit performance across all language families
- Whether the observed patterns hold for non-LLM transformer architectures

## Next Checks

1. **Prompt Ablation Study**: Systematically vary the translation prompt and measure COMET score stability across languages, particularly focusing on low-resource languages that showed the most degradation.

2. **Calibration Domain Extension**: Test language-matched calibration with domain-specific calibration sets (e.g., legal texts) to measure whether improvements extend beyond WikiText-derived calibration.

3. **Architecture Transfer**: Apply the same quantization and evaluation pipeline to non-LLM architectures (e.g., M2M-100) to determine if uneven impact patterns are architecture-specific or general to transformer-based MT systems.