---
ver: rpa2
title: A General Framework for Off-Policy Learning with Partially-Observed Reward
arxiv_id: '2506.14439'
source_url: https://arxiv.org/abs/2506.14439
tags:
- rewards
- policy
- target
- secondary
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of off-policy learning (OPL) in
  contextual bandits when rewards are only partially observed, a common issue in real-world
  applications like recommender systems and e-commerce platforms. The core method,
  HyPeR (Hybrid Policy Optimization for Partially-Observed Reward), leverages both
  partially observed target rewards and fully observed secondary rewards to estimate
  policy gradients with reduced variance while maintaining unbiasedness.
---

# A General Framework for Off-Policy Learning with Partially-Observed Reward

## Quick Facts
- arXiv ID: 2506.14439
- Source URL: https://arxiv.org/abs/2506.14439
- Reference count: 38
- Authors: Rikiya Takehi; Masahiro Asami; Kosuke Kawakami; Yuta Saito
- Primary result: HyPeR framework outperforms existing methods in off-policy learning when target rewards are partially observed, by leveraging secondary rewards for variance reduction

## Executive Summary
This paper addresses the challenge of off-policy learning (OPL) in contextual bandits when rewards are only partially observed, a common issue in recommender systems and e-commerce platforms. The authors propose HyPeR (Hybrid Policy Optimization for Partially-Observed Reward), a framework that leverages both partially observed target rewards and fully observed secondary rewards to estimate policy gradients with reduced variance while maintaining unbiasedness. The approach combines target and secondary rewards in a principled way, introducing strategic weight tuning to further improve performance by balancing bias and variance. Experiments on synthetic and real-world datasets demonstrate that HyPeR consistently outperforms existing methods, particularly in challenging scenarios with sparse target reward observations and small training data sizes.

## Method Summary
HyPeR addresses OPL with partially-observed rewards by introducing a hybrid gradient estimator that conditions on secondary rewards. The method requires training two q-function models: an unconditional estimator ˆq(x,a) and a secondary-reward-conditioned estimator ˆq(x,a,s). An observation probability model p(o|x) is also trained to handle partial observations. The gradient estimator combines target and secondary reward gradients with weight γ, which can be tuned via bootstrap optimization. The framework maintains unbiasedness while reducing variance through the conditional q-function, and strategic weight tuning allows trading small bias for larger variance reduction when target observations are sparse.

## Key Results
- HyPeR outperforms existing OPL methods (r-IPS, r-DR, s-IPS, s-DR) across varying observation rates, particularly at low p(o|x) where target rewards are sparse
- The variance reduction mechanism depends on secondary rewards correlating with target rewards, with performance improving as correlation increases
- Data-driven weight tuning (γ ≠ β) consistently improves performance by shifting weight toward the lower-variance secondary term in sparse observation regimes
- On KuaiRec real-world dataset, HyPeR shows robust performance improvements over baselines for optimizing both target and combined policy values

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conditioning the q-function estimate on secondary rewards reduces policy gradient variance when secondary rewards correlate with the target.
- Mechanism: HyPeR replaces the standard q-function estimator ˆq(x, a) with a conditional estimator ˆq(x, a, s) that incorporates secondary reward information. The variance reduction (Theorem 2) depends on the gap between estimation errors: if ˆq(x, a, s) better approximates q(x, a, s) than ˆq(x, a) does, variance decreases proportionally to ρ²/p(o|x)² · w(x,a)² · gθ(x,a)² · (∆q,ˆq¬s² − ∆q,ˆq²).
- Core assumption: Secondary rewards have non-trivial correlation with the target reward (λ > 0 in synthetic experiments).
- Evidence anchors:
  - [section 4] Theorem 2 explicitly derives the variance reduction formula conditioned on ˆq(x, a, s) being more accurate than ˆq(x, a).
  - [section 5.1] Figure 3 shows HyPeR performance improves as λ (target-secondary correlation) increases.
  - [corpus] Related work on multi-stage rewards and implicit-explicit feedback (Wan & McAuley, 2018; Liu et al., 2010) supports the premise that secondary signals often carry predictive information.
- Break condition: If secondary rewards are uncorrelated with target rewards (λ → 0), ˆq(x, a, s) offers no advantage over ˆq(x, a), and variance reduction collapses to zero or becomes negative.

### Mechanism 2
- Claim: The HyPeR gradient estimator maintains unbiasedness while leveraging all available data points, including those with unobserved target rewards.
- Mechanism: The estimator (Eq. 10) decomposes into three terms: (1) a baseline expectation under πθ using ˆq(x, a), (2) an IPS-style correction using the conditional q-function ˆq(x, a, s) for all samples, and (3) an IPS-style correction using observed target rewards weighted by o/p(o|x). The structure ensures that unobserved rewards (o=0) still contribute via the secondary-reward-conditioned term, while observed rewards provide unbiased correction.
- Core assumption: The observation probability p(o|x) is either known or accurately estimable; the logging policy π₀ has full support (Condition 1).
- Evidence anchors:
  - [section 4] Theorem 1 proves unbiasedness: E[∇θ ˆVr(πθ; D)] = ∇θVr(πθ).
  - [section 2.3] Eqs. 4-5 show baseline r-IPS/r-DR discard data when o=0, motivating the hybrid approach.
  - [corpus] The DOLCE paper (arXiv:2505.00961) addresses related issues of insufficient action overlap, but does not handle partial reward observation directly.
- Break condition: If p(o|x) is misspecified or π₀ lacks full support, unbiasedness is compromised.

### Mechanism 3
- Claim: Intentionally using a weight γ ≠ β in the gradient estimator can improve finite-sample policy performance by trading small bias for larger variance reduction.
- Mechanism: The combined policy gradient (Eq. 11) mixes target and secondary reward gradients with weight γ. When target reward observations are sparse, the target-gradient term has higher variance. Increasing γ shifts weight toward the lower-variance secondary term, reducing overall estimation variance at the cost of bias relative to the true objective weight β. The optimal γ is selected via bi-level optimization (Eq. 14) using bootstrapped training sets to maintain proper variance estimation.
- Core assumption: The secondary reward gradient estimator has lower variance than the target reward gradient estimator (typically true when p(o|x) < 1).
- Evidence anchors:
  - [section 4.1] The weight tuning procedure explicitly optimizes γ* = argmax V(πθ(·; γ, D); β).
  - [section 5.1] Figure 4 and Table 2 show Tuned ˆγ* consistently outperforms γ = β, with ˆγ* often exceeding β (e.g., ˆγ* = 0.56 vs. β = 0.0).
  - [corpus] Corpus lacks direct precedents for intentional weight mismatch in OPL; this appears novel.
- Break condition: If secondary rewards are extremely noisy (high σs) or nearly uncorrelated (low λ), increasing γ may introduce harmful bias without sufficient variance benefit.

## Foundational Learning

- Concept: **Inverse Propensity Score (IPS) estimation**
  - Why needed here: IPS is the foundation for unbiased off-policy gradient estimation from logged bandit data. HyPeR's estimator builds on IPS-style corrections but extends them to handle partial observations and secondary rewards.
  - Quick check question: Can you explain why dividing by π₀(a|x) enables unbiased estimation under a different policy πθ?

- Concept: **Doubly Robust (DR) estimation**
  - Why needed here: HyPeR's core estimator (Eq. 10) follows the DR structure, combining a direct q-function model with IPS-style residuals. Understanding DR clarifies why HyPeR can remain unbiased even with imperfect q-function estimates.
  - Quick check question: What happens to DR's variance when the q-function model perfectly predicts rewards?

- Concept: **Bias-variance trade-off in finite-sample estimation**
  - Why needed here: The strategic weight tuning mechanism (γ ≠ β) is justified by finite-sample bias-variance considerations. Without this intuition, the idea of using an "incorrect" weight seems counterintuitive.
  - Quick check question: If you have two estimators, one unbiased with high variance and another slightly biased with much lower variance, which might you prefer with n=100 samples? What about n=100,000?

## Architecture Onboarding

- Component map:
  - Q-function models: Two regressors needed—ˆq(x, a) for unconditional expectation and ˆq(x, a, s) for secondary-reward-conditioned expectation. Both trained via squared error on observed (x, a, s, r) tuples.
  - Observation probability model: Classifier regressing p(o|x) from context features; required for all methods handling partial rewards (including baselines).
  - Secondary reward model: ˆf(x, a) predicting expected secondary rewards, used in the secondary-gradient term.
  - Policy network: πθ(a|x) parameterized by θ, updated via gradient ascent using the HyPeR estimator.
  - Weight tuner: Bi-level optimization loop (Eq. 14) with bootstrapped D′_tr and validation set Dval.

- Critical path:
  1. Train ˆq(x, a), ˆq(x, a, s), ˆf(x, a), and p(o|x) models on logged data D.
  2. Compute importance weights w(x, a) = πθ(a|x)/π₀(a|x) for each logged action.
  3. Evaluate HyPeR gradient (Eq. 11) with chosen γ (either β or tuned).
  4. Update θ via gradient ascent; iterate until convergence.
  5. For weight tuning: bootstrap D′_tr, train policy for each candidate γ, estimate value on Dval, select best γ.

- Design tradeoffs:
  - **Model complexity vs. variance reduction**: More expressive ˆq(x, a, s) models reduce variance but risk overfitting to limited observed rewards; regularize appropriately.
  - **Bootstrapping vs. simple split**: The tuning procedure uses bootstrapping to maintain |D′_tr| = |D|, avoiding variance underestimation, but adds computational cost.
  - **γ search granularity**: Fine-grained γ search improves tuning but increases compute; coarse search may miss optimal values.

- Failure signatures:
  - **No improvement over r-DR**: Check if secondary rewards actually correlate with target (compute empirical correlation); if near zero, HyPeR's advantage disappears.
  - **Tuned γ degrades performance**: Likely p(o|x) poorly estimated or bootstrap samples too noisy; validate observation model calibration.
  - **High variance despite HyPeR**: Ensure logging policy has reasonable support (π₀(a|x) not too small for actions favored by πθ).

- First 3 experiments:
  1. **Baseline comparison on varying observation rates**: Replicate Figure 1 on your data—plot combined/target/secondary policy values vs. p(o|x) for r-DR, s-DR, HyPeR(γ=β), HyPeR(γ=0). Expect HyPeR to dominate at low p(o|x).
  2. **Ablation on target-secondary correlation**: Similar to Figure 3, vary the correlation strength (if you can control or proxy it) to confirm HyPeR's advantage scales with correlation.
  3. **Weight tuning validation**: Compare HyPeR(γ=β) vs. HyPeR(Tuned ˆγ*) across different β values; verify that tuned weights consistently improve combined policy value, and inspect whether ˆγ* > β in sparse-observation regimes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can HyPeR be effectively extended from contextual bandits to full offline reinforcement learning with sequential decision-making and delayed rewards across multiple timesteps?
- Basis in paper: [explicit] "In future work, it would be valuable to extend our formulation and method to the problem of offline reinforcement learning."
- Why unresolved: The current formulation assumes single-step contextual bandit settings where rewards are observed per action. Sequential RL introduces temporal credit assignment, function approximation across states, and more complex partial observation patterns.
- What evidence would resolve it: A theoretical extension showing unbiasedness and variance properties in MDPs, plus experiments on sequential benchmarks demonstrating HyPeR's advantage over existing offline RL methods under partial reward observations.

### Open Question 2
- Question: How does HyPeR perform when secondary rewards are also partially observed rather than fully dense?
- Basis in paper: [inferred] The method assumes "secondary rewards that are more densely observed" and treats them as fully available. Real systems may have missing clicks, incomplete dwell time logs, or delayed secondary metrics.
- Why unresolved: The variance reduction proof relies on secondary rewards being observable for all samples. Partial observation of secondary rewards could undermine the variance benefits while potentially introducing additional bias.
- What evidence would resolve it: Theoretical analysis of HyPeR's properties under partial secondary reward observation, and experiments varying observation rates for both reward types.

### Open Question 3
- Question: What is the computational and statistical cost of the bi-level optimization procedure for data-driven weight tuning (γ*), particularly for large-scale industrial deployments?
- Basis in paper: [inferred] Section 4.1 introduces bootstrapping and bi-level optimization for tuning γ. The procedure trains policies on bootstrapped datasets D′_tr and evaluates on validation sets, which multiplies computational cost.
- Why unresolved: The paper doesn't analyze computational complexity or provide scaling experiments. Industrial recommender systems often have millions of samples and actions, where repeated policy training may be prohibitive.
- What evidence would resolve it: Complexity analysis of the tuning procedure and experiments showing how runtime and performance scale with dataset size, action space dimensionality, and number of hyperparameter candidates.

## Limitations
- The variance reduction benefits critically depend on secondary rewards correlating with target rewards; performance degrades when correlation is low
- The method assumes secondary rewards are fully observed, which may not hold in real-world scenarios where secondary metrics also have observation gaps
- The bi-level optimization procedure for weight tuning increases computational cost and may not scale well to industrial-sized datasets

## Confidence
- **High confidence** in the unbiasedness claims (Theorem 1) and variance reduction mechanism (Theorem 2), as these are mathematically proven results.
- **Medium confidence** in the practical performance improvements, based on controlled experiments but limited to specific synthetic and real-world settings.
- **Medium confidence** in the strategic weight tuning benefits, as this is the most novel component with limited comparative precedent.

## Next Checks
1. **Correlation sensitivity analysis**: Systematically vary the target-secondary reward correlation (λ) on real-world datasets to confirm HyPeR's advantage scales with correlation as predicted by theory.
2. **Observation probability misspecification test**: Evaluate HyPeR performance when p(o|x) is misspecified or estimated with error to understand robustness boundaries.
3. **Extreme sparsity stress test**: Push observation rates below p(o|x)=0.1 to identify the operational limits of HyPeR's variance reduction benefits compared to baselines.