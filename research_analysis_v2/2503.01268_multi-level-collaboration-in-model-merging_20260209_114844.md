---
ver: rpa2
title: Multi-Level Collaboration in Model Merging
arxiv_id: '2503.01268'
source_url: https://arxiv.org/abs/2503.01268
tags:
- performance
- merging
- ensembling
- neulig
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the potential for achieving performance
  consistency between parameter-level model merging and prediction-level model ensembling
  in multi-model collaboration scenarios. The authors theoretically demonstrate that
  under certain conditions, the performance difference between merging and ensembling
  is of second-order smallness, regardless of model scale and number of collaborating
  models.
---

# Multi-Level Collaboration in Model Merging

## Quick Facts
- arXiv ID: 2503.01268
- Source URL: https://arxiv.org/abs/2503.01268
- Reference count: 40
- Primary result: Achieved 0.02% performance gap between merging and ensembling in 5 ViT model collaboration (95.44% vs 95.46%)

## Executive Summary
This paper investigates the theoretical and practical potential for achieving performance consistency between parameter-level model merging and prediction-level model ensembling in multi-model collaboration scenarios. The authors theoretically demonstrate that under certain conditions, the performance difference between merging and ensembling is of second-order smallness, regardless of model scale and number of collaborating models. To validate this finding, they introduce Neural Ligand (NeuLig), a framework that employs a single-layer network called Portland to generate Cooperative Vectors for each model on each data. Experimental results show that NeuLig effectively enables performance consistency between merging and ensembling, with minimal performance gaps observed across various model configurations.

## Method Summary
NeuLig introduces a learnable "Portland" network that generates per-sample Cooperative Vectors to enable merging-ensembling consistency. The framework works by pre-computing parameter offsets between models and their average, then training Portland to produce weighting coefficients that both align these offsets to zero (via alignment loss) and optimize ensemble performance (via boosting loss). The method uses a single linear layer with softmax to output T-dimensional vectors, taking concatenated model outputs as input. During training, Portland is optimized while models remain frozen, and the resulting CoopVecs enable both weighted parameter merging and prediction ensembling with minimal performance difference.

## Key Results
- Achieved 0.02% performance gap between merging and ensembling for 5 ViT models (95.44% vs 95.46%)
- Demonstrated robust consistency across different model scales, quantities, and initialization methods
- Showed distinct behavioral patterns between ViT (orthogonal CoopVec Maps) and ResNet (task-dependent weighting) models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: When weighted parameter offsets sum to zero, the difference between ensembling and merging outputs becomes second-order small.
- Mechanism: A Taylor expansion around the merged parameters shows that the first-order term vanishes precisely when $\sum_{t=1}^T(\beta_t \xi_t^\top) = 0$, leaving only $O(\Delta^2)$ residuals (Equation 3, Proposition 1). This aligns the functional behavior of the merged model with the ensemble at each data point.
- Core assumption: The model function $f_\theta(x, y)$ is at least twice differentiable in parameter space, which holds for standard architectures with softmax/cross-entropy outputs.
- Evidence anchors:
  - [section 3.2]: "the difference between $\tilde{f}$ and $f_{\tilde{\theta}}$ is of second-order smallness if and only if $\sum_{t=1}^T(\beta_t\xi_t^\top) = 0$"
  - [corpus]: Related work on NTK regime (e.g., Fort et al., Jacot et al.) shows linear mode connectivity emerges late in training—consistent with second-order approximation validity.
- Break condition: If models diverge too far from a common point (large $\xi_t$), higher-order terms dominate and the approximation degrades.

### Mechanism 2
- Claim: The alignment loss directly minimizes the first-order discrepancy condition from Proposition 1.
- Mechanism: The alignment term $L_a = \sum_{t=1}^T(\Psi_\phi(\cdot)_t \cdot \xi_t^*)^2$ is an MSE loss targeting zero; it trains CoopVecs to cancel weighted parameter offsets across all dimensions, satisfying the theoretical condition for merging-ensembling consistency.
- Core assumption: Parameter offsets $\xi_t$ are pre-computable and stable per model; the simplification of using an unweighted average initialization does not significantly bias the optimization.
- Evidence anchors:
  - [section 3.3]: "The alignment term ($L_a$) is designed to bring the performance of merging in line with that of ensembling... which can be viewed as a MSE loss on each data with a target value of 0"
  - [section 4.3]: "the performance gap with precision to the second decimal place" for ViT models validates the mechanism empirically
- Break condition: If $\xi_t^*$ scaling is unstable or gradients through softmax saturate, alignment may underfit.

### Mechanism 3
- Claim: The boosting term ensures that the converged solution is not just consistent but also high-performing.
- Mechanism: $L_b$ (supervised cross-entropy or semi-supervised entropy minimization) pushes the ensemble toward low loss on each task; combined with $L_a$, it yields CoopVecs that are both aligned and effective.
- Core assumption: Entropy minimization correlates positively with task loss (validated in AdaMerging [38] via Spearman correlation).
- Evidence anchors:
  - [abstract]: "merging: 95.44% vs. ensembling: 95.46%" shows consistency without performance sacrifice
  - [section 4.4, Figure 5]: CoopVec Maps converge rapidly for ViTs (strong orthogonality), slower for ResNets, but final performance remains stable
- Break condition: If tasks are highly conflicting or data is severely out-of-distribution, entropy minimization may not track true loss.

## Foundational Learning

- **Concept: Taylor Expansion in Parameter Space**
  - Why needed here: The entire theoretical justification rests on approximating $f_{\theta_t}$ around $f_{\tilde{\theta}}$. Without understanding why first-order terms cancel, the mechanism is opaque.
  - Quick check question: Given $\theta_1, \theta_2$, what is the first-order approximation of $f_{\theta_1} - f_{(\theta_1+\theta_2)/2}$?

- **Concept: Model Merging via Task Vectors**
  - Why needed here: NeuLig operates on offsets $\xi_t = \theta_t - \tilde{\theta}$. Understanding Task Arithmetic, Ties-Merging, and RegMean provides context for why NeuLig's learnable CoopVecs are novel.
  - Quick check question: Why does simple averaging of fine-tuned ViTs often outperform averaging of ResNets?

- **Concept: Ensemble vs. Merged Model Inference**
  - Why needed here: The paper's goal is consistency between two fundamentally different inference paths. You must distinguish prediction-level aggregation (ensemble) from parameter-level combination (merged model).
  - Quick check question: For $T$ models, what is the computational difference between running $T$ forward passes and one forward pass with merged weights?

## Architecture Onboarding

- **Component map**: Model outputs -> Concatenation -> Portland (Linear + Softmax) -> CoopVec -> Merging & Ensembling

- **Critical path**:
  1. Pre-compute $\tilde{\theta}_{init} = \frac{1}{T}\sum_t \theta_t$ and offsets $\xi_t = \theta_t - \tilde{\theta}_{init}$
  2. Scale $\xi_t$ by standard deviation to obtain $\xi_t^*$
  3. Forward pass each model on batch -> concatenate outputs
  4. Portland generates CoopVec -> compute $L_b$ (ensemble loss) and $L_a$ (offset alignment)
  5. Backprop through Portland only (models frozen)
  6. At test time, use trained Portland for per-sample CoopVecs

- **Design tradeoffs**:
  - Data-level CoopVec (per-sample) vs. Dataset-level CoopVec Map (per-task): The former maximizes consistency but requires Portland at inference; the latter enables static merging after training
  - Semi-supervised (entropy) vs. supervised (labels): Semi-supervised is more practical but assumes entropy-loss correlation holds
  - $\alpha : \beta$ ratio: Too much alignment may sacrifice task performance; too much boosting may widen merge-ensemble gap

- **Failure signatures**:
  - **Gap remains large (>5%)**: Check if $L_a$ is actually decreasing; may need to increase $\beta$ or reduce learning rate
  - **Both merge and ensemble perform poorly**: $L_b$ may be under-optimized; verify loss scale and check for gradient conflicts
  - **ResNet models fail to converge**: Known issue—ResNets require more epochs (Figure 6). Consider longer training or pre-normalization
  - **Diverse-origin models yield random performance**: Previous methods fail here entirely; NeuLig should still show consistency (Table 7b), but absolute performance may be low

- **First 3 experiments**:
  1. **Toy validation (2 models, 2 tasks)**: Replicate Figure 3 with GTSRB + CIFAR100. Verify merge-ensemble gap < 1% and both exceed 80% accuracy
  2. **Ablation on loss terms**: Train with $L_b$ only, $L_a$ only, and both. Confirm that both terms are necessary for consistency + performance
  3. **CoopVec Map extraction**: After 1 epoch, extract peak CoopVec values per dataset (Figure 4). Verify that ViT produces near-orthogonal maps while ResNet shows task-dependent weighting. Use this for dataset-level merging and compare to per-sample performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do ResNet-based models show a dependency on complex datasets in the CoopVec Map while ViT-based models exhibit strong orthogonality?
- Basis in paper: [explicit] The authors explicitly note "distinct behaviors" in the analysis, observing that ResNets assign higher weights to models fine-tuned on complex data, whereas ViTs prioritize the specific task model (orthogonality).
- Why unresolved: The paper empirically documents these divergent dynamics but does not provide a theoretical explanation linking architectural inductive biases (e.g., convolutional locality vs. global attention) to the merging landscape.
- What evidence would resolve it: A theoretical analysis connecting the model's loss landscape curvature or feature space geometry to the optimal coefficient distribution for merging.

### Open Question 2
- Question: Can the Cooperative Vectors (CoopVec) required for consistency be derived analytically without training a router network?
- Basis in paper: [inferred] The proposed NeuLig framework relies on a trainable "Portland" network to find the coefficients that satisfy the alignment condition ($\sum \beta_t \xi_t = 0$).
- Why unresolved: The paper validates the *existence* of these vectors through optimization but does not explore if a closed-form or zero-shot solution exists, leaving the efficiency of the discovery process unoptimized.
- What evidence would resolve it: A method that mathematically solves for the optimal $\beta$ using only pre-trained weights and gradients without backpropagation on test data.

### Open Question 3
- Question: Does the observed performance consistency generalize to Large Language Models (LLMs) or other modalities?
- Basis in paper: [inferred] The experiments are strictly confined to CLIP-based vision encoders (ViT and ResNet), despite the theoretical assumptions being architecture-agnostic.
- Why unresolved: While the Taylor-expansion theory applies broadly, the "second-order smallness" relies on specific model behaviors (e.g., linear mode connectivity) which may differ in the non-convex landscapes of LLMs.
- What evidence would resolve it: Experimental results applying NeuLig to standard NLP benchmarks (e.g., merging instruction-tuned Llama models) to verify if the alignment term functions identically.

## Limitations

- Theoretical foundation assumes twice-differentiable model functions and small parameter offsets, which may not hold for highly divergent model states
- Reliance on a single linear layer (Portland) to capture complex inter-model relationships represents a significant architectural simplification
- Empirical validation focuses primarily on CLIP-based vision models, with limited testing across diverse model families and domains

## Confidence

- Theoretical claims (Proposition 1): High - derivation is mathematically sound given stated assumptions
- Empirical results on ViT models: Medium - strong results but limited to specific model architectures
- ResNet performance claims: Low - documented convergence issues and requirement for significantly more training epochs
- Generalization to non-CLIP models: Low - insufficient empirical validation across diverse architectures

## Next Checks

1. Verify alignment loss convergence empirically by monitoring $L_a$ during training across different model pairs and architectures
2. Test theoretical second-order approximation validity by measuring performance gap as a function of parameter divergence magnitude
3. Evaluate the semi-supervised entropy assumption by comparing performance when using true labels versus entropy minimization across diverse task types