---
ver: rpa2
title: Vision LLMs Are Bad at Hierarchical Visual Understanding, and LLMs Are the
  Bottleneck
arxiv_id: '2505.24840'
source_url: https://arxiv.org/abs/2505.24840
tags:
- hierarchical
- visual
- vllms
- classification
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work evaluates the hierarchical visual understanding capabilities\
  \ of state-of-the-art vision-language models (VLLMs) using one million four-choice\
  \ visual question-answering tasks derived from six taxonomies and four hierarchical\
  \ image datasets. It reveals that many VLLMs struggle with hierarchical consistency\u2014\
  often correctly identifying fine-grained categories but failing at higher taxonomic\
  \ levels\u2014even when their leaf-level accuracy is high."
---

# Vision LLMs Are Bad at Hierarchical Visual Understanding, and LLMs Are the Bottleneck

## Quick Facts
- arXiv ID: 2505.24840
- Source URL: https://arxiv.org/abs/2505.24840
- Reference count: 40
- Primary result: Vision-language models can identify objects correctly but fail at higher-level taxonomic reasoning, with LLMs identified as the bottleneck.

## Executive Summary
This paper investigates hierarchical visual understanding in vision-language models (VLLMs) using one million four-choice visual question-answering tasks derived from six taxonomies and four hierarchical image datasets. The study reveals that many VLLMs achieve high leaf-level accuracy but fail to maintain hierarchical consistency across taxonomic levels. Through systematic probing, the authors demonstrate that visual encoders and projectors retain rich hierarchical information, but the LLM component fails to decode it. Vision-language tuning on hierarchical tasks improves both VLLM and LLM taxonomy knowledge, with larger gains observed in the LLM component. The findings suggest that VLLMs cannot achieve fully hierarchical visual understanding until their underlying LLMs possess robust taxonomy knowledge.

## Method Summary
The study evaluates VLLMs using 4-choice hierarchical VQA tasks constructed from datasets like CUB-200, iNaturalist-2021, and ImageNet-1K. Distractors are selected based on image-text similarity using SigLIP. The authors compute Hierarchical Consistent Accuracy (HCA) and Leaf-level Accuracy (Accleaf) to measure performance. Linear probes are trained on visual tokens at encoder, projector, and LLM layers to assess information retention. LoRA finetuning is applied to the LLM component of Qwen2.5-VL-7B using iNat21-Plant data to evaluate transfer learning effects on hierarchical understanding.

## Key Results
- VLLMs often achieve high Accleaf but low HCA, indicating correct object identification without consistent higher-level taxonomy reasoning.
- Linear probes on visual encoder and projector tokens outperform full VLLM HCA, showing hierarchical information is preserved but not utilized by the LLM.
- Text-only LLM HCA strongly correlates with VLLM visual HCA (ρ up to 0.95), identifying the LLM as the bottleneck.
- Vision-language tuning improves both VLLM and LLM taxonomy knowledge, with larger gains observed in the LLM component.

## Why This Works (Mechanism)

### Mechanism 1
Visual embeddings from encoder and projector retain discriminative hierarchical cues that are not lost through the forward pass. The visual encoder extracts features, the projector maps them to the LLM's token space, and linear probes trained on these embeddings achieve higher hierarchical consistent accuracy (HCA) than the full VLLM, suggesting the information is present but not utilized by the LLM decoder. Probing accuracy reflects the presence of usable information, not just memorization.

### Mechanism 2
The LLM component fails to decode hierarchical structure from visual tokens, lacking robust taxonomy knowledge even in text-only settings. When LLMs are evaluated on text-only hierarchical QA (images replaced by leaf labels), their (text) HCA is low and strongly correlates with the VLLM's (visual) HCA (ρ up to 0.95), indicating that the LLM's taxonomy knowledge limits overall performance. Text-only performance is a proxy for the LLM's inherent capacity to reason over taxonomic hierarchies.

### Mechanism 3
Vision-language tuning on hierarchical VQA tasks improves both VLLM and LLM taxonomy knowledge, with larger gains in the text-only LLM. LoRA finetuning on visual hierarchical VQA increases LLM (text) HCA more than VLLM (visual) HCA (e.g., +20.66 vs. +11.67 on iNat21-Plant), suggesting that visual tasks can transfer to text-only hierarchical reasoning, partially alleviating the LLM bottleneck. The improvement is not due to task-specific overfitting but reflects genuine knowledge transfer.

## Foundational Learning

- **Concept: Hierarchical classification vs. flat classification**
  - Why needed here: The paper evaluates hierarchical consistency, where labels are organized in a taxonomy (e.g., Animal → Mammal → Dog → Terrier → Boston Terrier), unlike standard flat classification.
  - Quick check question: Can you explain why hierarchical consistent accuracy (HCA) is stricter than leaf-level accuracy (Accleaf)?

- **Concept: Linear probing of internal representations**
  - Why needed here: Probing visual tokens at encoder, projector, and LLM layers isolates where hierarchical information is preserved or lost.
  - Quick check question: What does it mean if a linear probe outperforms the full model on a task?

- **Concept: Vision-language tuning (e.g., LoRA)**
  - Why needed here: The paper uses LoRA to finetune VLLMs on hierarchical VQA, showing improvements in both visual and text-only hierarchical consistency.
  - Quick check question: Why might tuning on visual tasks improve text-only taxonomy reasoning?

## Architecture Onboarding

- **Component map:** Visual encoder (e.g., SigLIP, OpenCLIP) -> Projector -> LLM decoder -> Probing classifiers
- **Critical path:** Image input -> visual encoder -> visual tokens -> projector -> language-space tokens -> LLM + text prompt -> output tokens
- **Design tradeoffs:**
  - Closed-set (4-choice VQA) vs. open-set generation: closed-set reduces prediction space and ambiguity, but open-set is more realistic
  - Probing vs. end-to-end evaluation: probing isolates information retention; end-to-end reflects full system behavior
  - Visual-only vs. text-only hierarchical evaluation: text-only isolates LLM taxonomy knowledge; visual-only reflects integrated performance
- **Failure signatures:**
  - High leaf accuracy (Accleaf) but low HCA: correct fine-grained predictions but inconsistent higher-level taxonomy
  - High probing accuracy but low VLLM HCA: visual embeddings retain structure, but LLM fails to decode it
  - Low text-only LLM HCA: LLM lacks taxonomy knowledge, likely bottleneck for VLLM
- **First 3 experiments:**
  1. Reproduce the 4-choice hierarchical VQA evaluation on a subset (e.g., CUB-200) for baseline VLLM and compute HCA/Accleaf
  2. Run linear probing on visual encoder and projector outputs to verify if hierarchical cues are preserved
  3. Evaluate the LLM in text-only mode on the same hierarchical QA to measure (text) HCA and compare with VLLM (visual) HCA

## Open Questions the Paper Calls Out

### Open Question 1
Can VLLMs achieve full hierarchical visual understanding through post-hoc finetuning, or is explicit hierarchical knowledge injection strictly necessary during the LLM pre-training phase? The authors conjecture that this shortcoming can only be fixed in the pretraining stage rather than the "tail patching" finetuning stage, despite showing that LoRA finetuning yields improvements. The paper does not empirically test whether extensive pre-training modifications would succeed where finetuning fails.

### Open Question 2
What mechanistic factors prevent LLMs from decoding hierarchical structures that are demonstrably present in their intermediate embeddings? Section 3.3.2 reveals a paradox where linear probes on LLM embeddings achieve near-perfect hierarchical consistency, yet the model's generative output fails to reflect this knowledge. The paper identifies the "encoding vs. decoding" gap but does not isolate the specific attention or feed-forward mechanisms responsible for the decoding failure.

### Open Question 3
Does the identified LLM bottleneck generalize to hierarchical understanding in other modalities, such as video or 3D data? The study focused on images due to resource constraints but conjectures that VLLMs would likely perform poorly on video or 3D hierarchies, potentially with different underlying causes. The current work relies entirely on static image datasets; it is unknown if the temporal or spatial hierarchies in video/3D data exacerbate the visual encoder issues or if the LLM remains the sole bottleneck.

## Limitations
- The study relies on GPT-4 for taxonomy extraction in CUB-200, introducing potential variability in hierarchical label generation that could affect HCA calculations.
- The closed-set 4-choice VQA format may inflate performance metrics compared to open-ended generation tasks.
- The correlation between text-only LLM HCA and visual VLLM HCA, while strong, does not establish causation and may be influenced by shared pretraining data.

## Confidence

- **High Confidence:** The core finding that VLLMs can achieve high leaf accuracy while failing at hierarchical consistency is well-supported by multiple datasets and systematic probing.
- **Medium Confidence:** The claim that LLMs are the primary bottleneck is compelling but relies on the assumption that text-only HCA is a valid proxy for taxonomy knowledge.
- **Medium Confidence:** The transfer learning results showing vision-language tuning improves LLM taxonomy knowledge are promising but based on a single fine-tuning experiment with limited scope.

## Next Checks

1. **Cross-Validation of Taxonomy Generation:** Independently verify the GPT-4-extracted taxonomy for CUB-200 using human annotators or alternative automated methods to ensure HCA calculations are not artifact-driven.
2. **Open-Set Hierarchical Evaluation:** Replicate key HCA/Accleaf findings using open-ended generation instead of 4-choice VQA to assess whether closed-set format artificially boosts performance.
3. **Ablation of Visual vs. Text Bottlenecks:** Conduct controlled experiments where LLM components are swapped between VLLMs with different visual backbones to isolate whether taxonomy knowledge deficits persist regardless of visual encoder quality.