---
ver: rpa2
title: Convergent Reinforcement Learning Algorithms for Stochastic Shortest Path Problem
arxiv_id: '2508.13963'
source_url: https://arxiv.org/abs/2508.13963
tags:
- algorithms
- algorithm
- function
- policy
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes three convergent reinforcement learning algorithms
  for the Stochastic Shortest Path (SSP) problem, which is characterized by reaching
  a terminal state. The authors present two tabular algorithms (Actor-Critic and Critic-Actor)
  and one function approximation algorithm (Actor-Critic).
---

# Convergent Reinforcement Learning Algorithms for Stochastic Shortest Path Problem

## Quick Facts
- **arXiv ID**: 2508.13963
- **Source URL**: https://arxiv.org/abs/2508.13963
- **Reference count**: 40
- **Key outcome**: Three convergent RL algorithms (two tabular, one function approximation) for Stochastic Shortest Path problem with asymptotic almost-sure convergence guarantees

## Executive Summary
This paper addresses the Stochastic Shortest Path (SSP) problem, where an agent must reach a terminal state while minimizing expected total cost. The authors propose three reinforcement learning algorithms with rigorous convergence guarantees: a tabular Actor-Critic (AC), a tabular Critic-Actor (CA), and an Actor-Critic with function approximation (AC-FA). All algorithms are proven to converge asymptotically almost surely under the Basic Assumption that every policy is proper (reaches terminal state with probability 1 within |S| steps). The key innovation is extending convergent RL algorithms to the SSP setting, where previous methods like Q-Learning and SARSA may diverge or exhibit undesirable behavior.

## Method Summary
The paper presents three reinforcement learning algorithms for the Stochastic Shortest Path problem. The tabular Actor-Critic (AC) uses two-timescale stochastic approximation with a fast critic tracking the value function and a slow actor updating policy parameters. The tabular Critic-Actor (CA) reverses the timescales while maintaining convergence through Lyapunov function analysis. The function approximation Actor-Critic (AC-FA) replaces tabular value estimates with linear function approximation, using differential equations to prove convergence when approximation error is sufficiently small. All algorithms use softmax policies and require properness of all policies as a fundamental assumption.

## Key Results
- Tabular AC and CA algorithms converge asymptotically almost surely to ε-neighborhoods of optimal value functions
- AC-FA algorithm converges on diagnostic MDPs where Q-Learning with linear function approximation diverges and SARSA with linear function approximation exhibits chattering
- Empirical results show superior performance of tabular algorithms compared to Q-Learning and SARSA with theoretically-backed exploration strategies
- Convergence proofs establish asymptotic almost-sure convergence for all proposed algorithms under Basic Assumption

## Why This Works (Mechanism)

### Mechanism 1
The tabular Actor-Critic algorithm converges by separating critic and actor updates into two timescales. The fast critic tracks the ODE $\dot{V} = R_\theta + P_\theta V - V$, which has a globally asymptotically stable equilibrium $V^\theta = (I - P_\theta)^{-1}R_\theta$ under the Basic Assumption. The slow actor then follows a projected gradient descent ODE with Lyapunov function $\sum_i V^\theta(i)$, converging to near-optimal policies when the projection bound is large enough. This mechanism relies on the assumption that every stationary randomized policy is proper (reaches terminal state with probability 1 within |S| steps).

### Mechanism 2
The Critic-Actor algorithm achieves convergence by reversing the timescales while maintaining stability through differential inclusion analysis. The actor updates fast, converging to the set of greedy-like policies, while the critic updates slow via a differential inclusion $\dot{V}(i) \in \bar{K}_i(V)$. A Lyapunov function involving the operator norm of transition matrices ensures convergence. This mechanism also requires the Basic Assumption of proper policies and uses projection bounds for stability.

### Mechanism 3
The function approximation Actor-Critic converges to an ε-neighborhood of a stationary point when approximation error is small. The algorithm uses linear function approximation $V^\pi(i) \approx v^T \phi(i)$ with features $\phi$. The critic tracks a stable ODE $\dot{v} = A_1(\theta)v + b_1(\theta)$ where $A_1(\theta)$ is negative definite. The actor follows a projected ODE using policy gradients. Convergence is guaranteed when the approximation error $\sup_\theta ||e'_1(\theta)||$ is below a threshold, requiring linearly independent features and proper policies.

## Foundational Learning

- **Stochastic Shortest Path (SSP) formulation**: Essential for understanding the problem setup where a terminal state must be reached. Quick check: Can you explain why a discounted infinite-horizon MDP can be reformulated as an SSP problem?

- **Two-timescale stochastic approximation**: Critical for understanding the separation of critic and actor updates with different step-sizes. Quick check: Why does the faster timescale "see" the slower timescale as quasi-static?

- **Linear function approximation in RL**: Necessary for understanding the AC-FA algorithm's stability conditions. Quick check: What happens if the feature matrix $\Phi$ has linearly dependent columns?

## Architecture Onboarding

- **Component map**: 
  - Tabular AC/CA: Value table $V_n(i)$ and policy table $\theta_n(i,u)$ with softmax updates
  - AC-FA: Linear approximation $v^T \phi(i)$ replacing value table, with TD error and policy gradient updates

- **Critical path**:
  1. Verify MDP satisfies Basic Assumption (all policies proper)
  2. Choose appropriate step-sizes satisfying Assumption 3 or 4
  3. For AC-FA, design features ensuring linear independence and low approximation error
  4. Run episodes until terminal state, updating critic and actor per episode

- **Design tradeoffs**:
  - AC vs CA: Choice between policy iteration (fast critic) and value iteration (fast actor) behavior
  - Projection bound $\theta_0$: Larger bounds reduce bias but may slow convergence
  - Feature design for AC-FA: More features reduce approximation error but increase computational cost

- **Failure signatures**:
  - Divergence of $V_n$ or $v_n$: Check for improper policies or near-singular matrices in AC-FA
  - Slow convergence or oscillation: Verify step-size conditions and timescale separation
  - Suboptimal final policy: Check approximation error bounds for AC-FA or projection bounds for tabular methods

- **First 3 experiments**:
  1. FrozenLake 4x4 with tabular AC/CA vs Q-Learning and SARSA with softmax exploration
  2. Diagnostic MDP for AC-FA vs Q-LFA with divergence verification
  3. Diagnostic MDP for AC-FA vs SARSA-LFA with chattering behavior comparison

## Open Questions the Paper Calls Out

- **Natural Actor-Critic algorithms**: The paper suggests developing Natural Actor-Critic algorithms for SSP to improve convergence behavior, but does not investigate natural gradient methods.

- **Constrained optimization algorithms**: The authors propose exploring constrained optimization algorithms for SSP analogous to existing solutions for Discounted and Average Cost problems.

- **Standard Assumption extension**: The paper hypothesizes that algorithms can be extended to the Standard Assumption (where improper policies may exist) using projection, but rigorous proofs are not provided.

- **Feature selection methodology**: The convergence of AC-FA depends on approximation error bounds, but the paper provides no method for systematically selecting features that satisfy these theoretical requirements.

## Limitations

- Theoretical guarantees rely heavily on the Basic Assumption that all policies are proper, which may not hold in practical environments
- Function approximation algorithm's convergence depends critically on feature quality with no clear guidance for feature selection
- Tabular algorithms' performance comparisons use specific exploration strategies that may not generalize to all environments
- Convergence proofs are asymptotic with no finite-time bounds provided

## Confidence

- **High confidence**: Convergence of tabular AC/CA algorithms under Basic Assumption (supported by rigorous ODE analysis)
- **Medium confidence**: AC-FA convergence guarantees when approximation error is small (theoretical support exists but relies on idealized conditions)
- **Medium confidence**: Superior empirical performance of tabular algorithms (supported by experiments but limited to specific benchmarks)

## Next Checks

1. Test tabular AC algorithm on a modified FrozenLake environment with cycles to verify properness assumption and detect divergence when violated

2. Implement AC-FA on diagnostic MDPs with varying feature quality to map the boundary between convergent and divergent behavior as approximation error increases

3. Compare tabular AC/CA against Q-Learning/SARSA on a diverse set of SSP environments including those with absorbing non-terminal states to evaluate robustness beyond the proper policy assumption