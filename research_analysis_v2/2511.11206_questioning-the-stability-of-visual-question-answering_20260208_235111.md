---
ver: rpa2
title: Questioning the Stability of Visual Question Answering
arxiv_id: '2511.11206'
source_url: https://arxiv.org/abs/2511.11206
tags:
- visual
- perturbations
- stability
- question
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically evaluates the stability of visual language
  models (VLMs) under benign, semantically preserving perturbations including pixel-level
  image shifts, light rotations, padding/cropping, and paraphrasing or multilingual
  rewrites of questions. Across multiple models and datasets, the study finds that
  VLMs are highly sensitive to such minor perturbations, with substantial fractions
  of samples changing predicted answers under at least one perturbation type.
---

# Questioning the Stability of Visual Question Answering

## Quick Facts
- **arXiv ID:** 2511.11206
- **Source URL:** https://arxiv.org/abs/2511.11206
- **Reference count:** 40
- **Primary result:** VLMs show high sensitivity to benign perturbations; stability strongly predicts correctness and can forecast closed-source model accuracy.

## Executive Summary
This paper systematically evaluates the stability of visual language models (VLMs) under semantically preserving perturbations, including minor pixel-level image shifts, rotations, padding/cropping, and paraphrasing or multilingual rewrites of questions. Across multiple models and datasets, the study finds that VLMs are highly sensitive to such perturbations, with substantial fractions of samples changing predicted answers under at least one perturbation type. For example, even leading closed-source models like GPT-4o and Gemini 2.0 Flash frequently fail under shifts as small as a few pixels or harmless rephrasings. The research demonstrates that sample-level stability is strongly correlated with correctness—stable samples are consistently far more likely to be answered correctly—and that stability patterns from small open-source models can be used to predict the correctness of much larger closed-source models with high precision. These findings reveal fundamental fragility in current VLMs, highlighting the need for robustness evaluations beyond adversarial perturbations, focusing on invariances models should reliably uphold.

## Method Summary
The study evaluates VLM stability under two main perturbation types: visual (pixel shifts, rotations, padding/cropping, text overlays) and textual (paraphrasing, translation). Each sample is perturbed with 27 visual variants and up to 11 textual paraphrases/translations. For each perturbed sample, the model generates an answer, and answer entropy is computed across perturbations to measure stability. The study analyzes correlations between stability and accuracy, and trains a linear predictor on stability features from small open-source models to forecast the correctness of larger closed-source models. Experiments use NaturalBench, TextVQA, DocVQA, and SeedBench IMG datasets, covering models like Qwen2.5-VL, LLaVA, InternVL, Phi-3.5-Vision, Gemini 2.0 Flash, and GPT-4o.

## Key Results
- VLMs exhibit high instability under benign perturbations; e.g., GPT-4o and Gemini 2.0 Flash frequently fail under minor shifts or rephrasing.
- Sample-level stability is strongly correlated with correctness; stable samples are much more likely to be answered correctly.
- Stability patterns from small, open-source models can predict the correctness of larger, closed-source models with high precision, outperforming native confidence scores.
- Internal activations diverge more for perturbations that cause answer changes, suggesting a link between instability and decision boundaries.

## Why This Works (Mechanism)

### Mechanism 1: Stability as a Correctness Proxy
- Claim: Sample-level stability under benign perturbations serves as a strong, model-agnostic indicator of prediction correctness.
- Mechanism: If a model's prediction changes when the image is shifted by a few pixels or the question is rephrased, it indicates the model is relying on spurious features or is near a decision boundary. Conversely, if the model is robust to these semantic-preserving changes, it is likely relying on the correct underlying reasoning or features. The paper demonstrates that visual and textual entropy (instability) are negatively correlated with accuracy.
- Core assumption: Benign perturbations strictly preserve semantics and do not degrade the image quality to the point of non-recognition.
- Evidence anchors:
  - [abstract] "stable samples are consistently far more likely to be answered correctly."
  - [section 5.2] Table 7 shows P(Acc|Stable) is significantly higher than baseline accuracy (e.g., 0.91 vs 0.787 for "All" perturbations).
  - [corpus] "Visual CoT Makes VLMs Smarter but More Fragile" supports the general theme that advanced reasoning in VLMs correlates with increased fragility.
- Break condition: The correlation weakens if the model is consistently "confidently wrong" and robust to perturbations on false premises, or if the "benign" perturbation accidentally removes a critical token (e.g., cropping out a small object).

### Mechanism 2: Cross-Model Stability Transfer
- Claim: Stability patterns from small, open-source VLMs can predict the correctness of larger, closed-source VLMs with higher precision than the large model's own confidence scores.
- Mechanism: VLMs share common failure modes and sensitivities to specific visual or textual shifts because they are trained on similar data distributions with similar architectures (Transformer-based). If a sample is inherently ambiguous or "adversarial" enough to destabilize a small model, it is likely to exploit similar vulnerabilities in a larger model.
- Core assumption: The smaller model (e.g., Qwen-2.5-VL-3B) and the larger target (e.g., GPT-4o) share a non-trivial set of semantic representations and failure modes.
- Evidence anchors:
  - [abstract] "stability patterns of small, accessible open-source models can be used to predict the correctness of much larger closed-source models with high precision."
  - [section 5.2] Figure 6 shows the learned predictor from open-source models achieves 92% precision on Gemini 2.0 Flash, doubling the recall of native confidence at the same precision.
  - [corpus] Corpus signals generally discuss VLM fragility but do not explicitly contradict this specific transfer learning mechanism.
- Break condition: The mechanism fails if the large model has specifically addressed the instabilities of the small model (e.g., via robustness training or different data curation) such that their failure modes diverge.

### Mechanism 3: Activation Divergence Under Instability
- Claim: Perturbations that cause an answer flip result in measurably larger divergences in internal layer activations compared to perturbations where the answer remains constant.
- Mechanism: The model processes a perturbation; if the change in input pushes the internal representation past a critical threshold (a decision boundary), the activations diverge significantly in later layers to produce a new token. If the perturbation is effectively "ignored," the activations remain closer to the original.
- Core assumption: The L2 norm of activation differences is a sufficient proxy for the functional divergence of the network.
- Evidence anchors:
  - [section 5.1] "perturbations that cause the answer to change produce larger changes in the network activations."
  - [section 5.1] Figure 4 visualizes the layer-wise normalized L2 distance, showing a distinct gap between changed and unchanged samples.
  - [corpus] No direct corpus evidence found for this specific internal activation mechanism; related papers focus on output-level robustness.
- Break condition: This observation is empirically derived but not proven as causal; it could break if residual connections maintain high similarity in early layers despite functional divergence, or if noise in the activations obscures the signal.

## Foundational Learning

- Concept: **Semantic Invariance vs. Corruptions**
  - Why needed here: The paper deliberately distinguishes "benign" perturbations (which shouldn't change the answer, like a 2-pixel shift) from "corruptive" ones (like heavy noise). Understanding this distinction is required to interpret the "stability" metric not as robustness to noise, but as logical consistency.
  - Quick check question: Does a cyclic shift of an image change the semantic content of the scene?

- Concept: **VQA Answer Entropy**
  - Why needed here: The paper quantifies "instability" using the entropy of the answer distribution across perturbations ($H_S^i$). You must understand discrete entropy to interpret their results (0 entropy = stable, high entropy = volatile).
  - Quick check question: If a model answers "Yes" 9 times and "No" 1 time across 10 rephrasings, is the sample strictly "V-stable" or "T-stable" according to the paper's definition?

- Concept: **Visual Prompt Injection**
  - Why needed here: The paper identifies a specific failure mode where text overlaid on images (e.g., "Answer Yes") distracts the model. This is crucial for understanding the "Text Overlay" perturbation results and their security implications.
  - Quick check question: Why might a VLM obey text rendered inside an image more than the user's explicit text prompt?

## Architecture Onboarding

- Component map: Perturbation Engine -> Inference Target (VLM) -> Stability Scorer -> Predictor Model
- Critical path: The generation of "semantically preserving" textual paraphrases is the most fragile step; if the LLM rephrasing prompt accidentally alters the question's meaning, the "instability" metric becomes invalid.
- Design tradeoffs: The paper evaluates 27 visual perturbations per image. Increasing this number improves the resolution of the stability estimate but incurs a quadratic inference cost relative to the dataset size.
- Failure signatures:
  - **Pixel-Shift Sensitivity:** Answer changing based on cyclic shifts of 1-2 pixels (Figure 1).
  - **Overlay Compliance:** High susceptibility to text overlays (e.g., GPT-4o changing answers based on "Answer No" text in image).
  - **Rotation Confusion:** Instability on rotation-invariant questions (e.g., "Is there an elephant?").
- First 3 experiments:
  1. **Sanity Check (Shift):** Run a standard VQA sample through the pipeline with only horizontal translations (±16 pixels). Plot answer entropy to confirm >30% of samples are unstable.
  2. **Paraphrase Consistency:** Generate 10 paraphrases for a set of 100 questions using an LLM. Verify that the VLM answer changes on at least 10-15% of the rephrasings.
  3. **Proxy Validation:** Train a logistic regression on the stability features of Qwen-2.5-VL-3B to predict the accuracy of Qwen-2.5-VL-7B. Aim to reproduce the precision lift seen in the paper.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What mechanisms explain the remaining ~25% of mutual information between visual and textual stability that cannot be attributed to model confidence?
- Basis in paper: [explicit] Section 5.2.1 explicitly computes I(H^V, H^L|C)/I(H^V, H^L) = 0.276, concluding "roughly one quarter of the mutual information remains unexplained by confidence, suggesting a direct relationship between the two modalities."
- Why unresolved: The paper identifies the gap but does not investigate what underlying factors—architectural, representational, or data-driven—account for this shared instability across modalities.
- What evidence would resolve it: Ablation studies isolating vision encoder, language model, and cross-modal attention components; analysis of training data composition and its effect on joint stability.

### Open Question 2
- Question: Can targeted training interventions (e.g., stability-augmented data, consistency regularization) substantially improve VLM robustness to benign perturbations?
- Basis in paper: [inferred] The paper demonstrates pervasive instability across all tested models but does not explore remediation strategies. The conclusion calls for "robustness evaluations that go beyond adversarial perturbations, focusing instead on invariances that models should reliably uphold."
- Why unresolved: No experiments address whether fine-tuning, data augmentation with semantic-preserving perturbations, or architectural changes can mitigate the observed fragility.
- What evidence would resolve it: Training models with explicit stability objectives (e.g., consistency losses under paraphrasing and pixel shifts) and measuring reductions in answer entropy.

### Open Question 3
- Question: Do the stability findings generalize to open-ended generation tasks (e.g., captioning, visual dialogue) beyond multiple-choice VQA?
- Basis in paper: [inferred] The study restricts evaluation to benchmarks with "a small number of fixed answers" to enable entropy-based stability metrics (Section 3.2). This design choice limits conclusions to constrained output spaces.
- Why unresolved: Free-form generation may exhibit different instability patterns; semantic equivalence is harder to assess but critical for real-world deployment.
- What evidence would resolve it: Extending stability analysis to generative tasks using semantic similarity metrics (e.g., embedding-based equivalence, human evaluation) across perturbed inputs.

## Limitations

- The study assumes all evaluated perturbations are truly "benign" and "semantically preserving," but some may introduce ambiguity (e.g., cropping out critical objects).
- The entropy-based stability metric does not distinguish between semantically equivalent answer variations ("Yes" vs "Yep") and true errors, though normalization is applied.
- The cross-model stability transfer claim is empirically validated but not theoretically grounded; the correlation could weaken with different architectures or training data.

## Confidence

- **High Confidence:** The empirical finding that VLM predictions change under minor visual shifts and text paraphrasing is well-supported and directly observable. The negative correlation between stability and accuracy is also robust.
- **Medium Confidence:** The mechanism explaining *why* stability predicts correctness (spurious feature reliance vs. correct reasoning) is plausible but not definitively proven. The claim about internal activation divergence is descriptive of observed data but not proven causal.
- **Medium Confidence:** The cross-model stability transfer mechanism is validated on the tested datasets but its generalizability to other domains or more diverse model pairs is uncertain.

## Next Checks

1. **Semantic Preservation Validation:** Manually annotate a subset of paraphrased questions to verify they are truly semantically equivalent to the originals, ensuring the "benign" label is accurate.
2. **Activation Causality Test:** Perform ablation studies on the activation divergence results—e.g., does masking specific layers or attention heads reduce the stability-accuracy correlation?
3. **Domain Transfer Experiment:** Test whether the small-model stability predictor works on a completely different VQA dataset (e.g., medical images or a different language) to validate the universality of the transfer learning claim.