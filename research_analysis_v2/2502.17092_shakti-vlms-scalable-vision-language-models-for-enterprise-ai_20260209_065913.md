---
ver: rpa2
title: 'Shakti-VLMs: Scalable Vision-Language Models for Enterprise AI'
arxiv_id: '2502.17092'
source_url: https://arxiv.org/abs/2502.17092
tags:
- training
- visual
- performance
- tasks
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Shakti-VLM, a family of efficient vision-language
  models (1B and 4B parameters) designed to address data efficiency challenges in
  multimodal learning. Unlike existing models that rely on extensive training data,
  Shakti-VLM leverages architectural innovations including QK-Normalization for attention
  stability, hybrid normalization techniques, and enhanced positional encoding.
---

# Shakti-VLMs: Scalable Vision-Language Models for Enterprise AI

## Quick Facts
- arXiv ID: 2502.17092
- Source URL: https://arxiv.org/abs/2502.17092
- Reference count: 36
- Primary result: Shakti-VLM-4B achieves 59.78% on MMMU validation and 2340.99 points on MME

## Executive Summary
Shakti-VLM introduces a family of efficient vision-language models (1B and 4B parameters) designed to address data efficiency challenges in multimodal learning. The models employ architectural innovations including QK-Normalization for attention stability, hybrid normalization techniques, and enhanced positional encoding. Through a three-stage training strategy, Shakti-VLM achieves competitive performance on complex multimodal reasoning benchmarks while using fewer training tokens than comparable models.

## Method Summary
Shakti-VLM uses a three-stage training strategy: (1) decoder-only pretraining with extended context, (2) frozen-decoder alignment of visual features through an MLP projection layer, and (3) full fine-tuning with instruction tuning and RLHF/DPO. The architecture features a vision encoder (36-48 layers) with QK-Normalization applied to query and key vectors, hybrid normalization combining Pre-LayerNorm in early layers with RMSNorm in later layers, and Rotary Position Embeddings with 2D positional bias. Training uses 487B tokens for the 1B model and 782B tokens for the 4B model across diverse datasets including Dolma, The Stack, LAION-400M, and specialized multimodal instruction datasets.

## Key Results
- Shakti-VLM-1B achieves 42.5% on MMMU validation and 1910.62 points on MME
- Shakti-VLM-4B achieves 59.78% on MMMU validation and 2340.99 points on MME
- Both models outperform larger parameter models like SmolVLM-2.25B on multiple benchmarks
- Strong performance in document understanding, visual reasoning, OCR extraction, and general multimodal reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** QK-Normalization may stabilize attention computations in deeper vision-language architectures, potentially reducing gradient instability during training.
- **Mechanism:** RMS normalization applied specifically to query and key vectors before attention score computation prevents extreme magnitude differences that can cause gradient vanishing/explosion in multi-head attention layers.
- **Core assumption:** Attention instability is a limiting factor in VLM training efficiency, and normalizing Q/K vectors directly addresses this bottleneck rather than other normalization points.
- **Evidence anchors:**
  - [abstract] "Key advancements include QK-Normalization for attention stability"
  - [Section 3.1] "To stabilize attention mechanisms, we utilized the QK-Norm... which applies RMS normalization specifically to query and key vectors... This rare optimization prevents gradient vanishing/explosion"
  - [corpus] Limited direct corpus validation; related VLM papers do not specifically address QK-normalization efficacy
- **Break condition:** If loss curves show oscillating or divergent behavior despite QK-Norm, the assumption that attention instability is the primary training bottleneck may be incorrect.

### Mechanism 2
- **Claim:** Hybrid normalization (Pre-LayerNorm in early layers, RMSNorm in later layers) may balance training stability with representational capacity.
- **Mechanism:** Pre-LayerNorm in early layers provides gradient flow stability during initial feature extraction, while post-normalization with RMSNorm in later layers allows more flexible representation learning for complex multimodal reasoning.
- **Core assumption:** Different transformer layers benefit from different normalization strategies, and a fixed split point (12/24 for 1B, 18/30 for 4B) captures this variation optimally.
- **Evidence anchors:**
  - [abstract] "hybrid normalization techniques"
  - [Section 3.1] "Pre-LayerNorm is applied to the initial layers, while RMSNorm governs the remaining layers... In Shakti-VLM-1B, Pre-LayerNorm is utilized for the first 12 layers, with RMSNorm applied to the next 24 layers"
  - [corpus] No direct corpus evidence comparing hybrid vs. uniform normalization strategies
- **Break condition:** If ablation studies show uniform normalization performs equally well, the architectural complexity of hybrid normalization may not justify its use.

### Mechanism 3
- **Claim:** The three-stage training strategy (decoder pretraining → frozen-decoder alignment → full fine-tuning) may enable data-efficient multimodal learning by establishing strong language foundations before cross-modal integration.
- **Mechanism:** Stage 1 builds extended-context language understanding; Stage 2 aligns visual features to the frozen language space without disrupting learned representations; Stage 3 refines the integrated system with task-specific data including RLHF/DPO.
- **Core assumption:** Decoupling language pretraining from multimodal alignment reduces interference and allows each component to specialize before integration.
- **Evidence anchors:**
  - [abstract] "A three-stage training strategy further optimizes learning efficiency"
  - [Section 4.1-4.3] Detailed stage descriptions with frozen decoder in Stage 2, full fine-tuning in Stage 3
  - [Section 2.4] "Shakti models achieve competitive performance despite using fewer training tokens than comparable models"
  - [corpus] Related papers (Qwen-VL, InternVL) also use multi-stage training, suggesting broad adoption but not direct validation of efficiency claims
- **Break condition:** If single-stage end-to-end training with equivalent tokens matches performance, the decoupling assumption may not hold.

## Foundational Learning

- **Concept: Rotary Position Embeddings (RoPE) with 2D positional bias**
  - **Why needed here:** Shakti-VLM augments RoPE with 2D absolute positional bias for spatial encoding. Understanding how RoPE encodes relative positions and why 2D extension matters for images is essential.
  - **Quick check question:** Can you explain why standard 1D position embeddings struggle with 2D spatial relationships in vision transformers?

- **Concept: Layer Normalization variants (Pre-LN, Post-LN, RMSNorm)**
  - **Why needed here:** The hybrid normalization strategy combines different variants. Understanding their gradient flow properties explains why the split matters.
  - **Quick check question:** What is the primary gradient flow difference between Pre-LayerNorm and Post-LayerNorm in transformer residual streams?

- **Concept: Vision-Language Alignment via Projection Layers**
  - **Why needed here:** The MLP projection layer transforms visual features to the decoder's embedding space. Understanding this cross-modal bridge is critical for debugging alignment failures.
  - **Quick check question:** Why might a frozen decoder during Stage 2 alignment prevent catastrophic forgetting of language capabilities?

## Architecture Onboarding

- **Component map:**
  Input Image → Vision Encoder (ViT backbone, 36-48 layers) → Dynamic Patch Extraction (14×14 to 32×32) → QK-Normalized Attention + Hybrid Norm layers → Projection Layer (MLP) → Visual Tokens ⊕ Text Embeddings → Decoder (Shakti-500M / Shakti-2.5B) → Output

- **Critical path:** Vision Encoder → Projection Layer → Decoder alignment. The projection layer is the narrowest point where visual features must map correctly to language space. Monitor Stage 2 loss carefully here.

- **Design tradeoffs:**
  - Smaller model (1B) uses shorter context (16K tokens) vs. 4B (32K tokens)—affects document processing capacity
  - Dynamic patch sizes improve resolution flexibility but add preprocessing complexity
  - Hybrid normalization split points (12/24 vs. 18/30) are fixed, not learned—may not generalize across architectures

- **Failure signatures:**
  - Stage 1: High final loss (>2.0) indicates insufficient language foundation
  - Stage 2: Flat loss curve suggests projection layer not learning alignment
  - Stage 3: Performance drop on text-only tasks suggests catastrophic forgetting during multimodal fine-tuning

- **First 3 experiments:**
  1. **Baseline validation:** Reproduce reported MMMU and DocVQA scores on validation sets to confirm checkpoint integrity.
  2. **Ablation—Normalization strategy:** Train with uniform Pre-LN vs. hybrid normalization on a subset (e.g., 50B tokens) to isolate normalization contribution.
  3. **Alignment probe:** Visualize projection layer outputs using t-SNE—check if visual tokens cluster meaningfully with related text embeddings after Stage 2.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do the architectural innovations in Shakti-VLM (such as QK-Normalization and hybrid normalization) maintain their training efficiency advantages when scaled to models significantly larger than 4B parameters?
- **Basis in paper:** [explicit] The Future Works section states, "Scaling to larger models beyond 4B parameters could help assess whether the architectural innovations continue to yield efficiency improvements at greater scales."
- **Why unresolved:** The current study only validates the 1B and 4B variants; the scaling behavior of these specific normalization techniques relative to standard architectures at larger scales (e.g., 7B or 70B) remains undemonstrated.
- **What evidence would resolve it:** Benchmark results and training token requirements for a Shakti-VLM-7B or larger variant compared against parameter-matched state-of-the-art models.

### Open Question 2
- **Question:** Can the integration of curriculum learning or contrastive learning pretraining strategies further reduce the reliance on large-scale datasets?
- **Basis in paper:** [explicit] The Future Works section suggests, "Enhancing data efficiency by exploring additional pretraining strategies, such as curriculum learning and contrastive learning, may reduce reliance on large-scale datasets."
- **Why unresolved:** The current three-stage training methodology optimizes the existing pipeline but does not experiment with these specific advanced pretraining techniques to test data minimization limits.
- **What evidence would resolve it:** Ablation studies comparing standard Shakti pretraining against curriculum-based approaches, measuring performance retention when dataset size is reduced.

### Open Question 3
- **Question:** How can the current architecture be effectively extended to support temporal modalities like video and audio processing?
- **Basis in paper:** [explicit] The authors list as future work "expanding Shakti's VLM models capabilities to support multilingual and multimodal inputs, such as audio and video processing."
- **Why unresolved:** The current Vision Encoder utilizes a Vision Transformer (ViT) optimized for static images (patch sizes 14x14 to 32×32), which lacks inherent mechanisms for temporal understanding or audio feature extraction.
- **What evidence would resolve it:** Performance metrics on video-understanding benchmarks (e.g., ActivityNet or Video-QA) using a modified Shakti architecture incorporating temporal encoders or audio adapters.

## Limitations

- No ablation studies to determine whether QK-Normalization or hybrid normalization independently contribute to performance gains
- Lack of training curves, convergence analysis, or computational cost comparisons to validate data efficiency claims
- No evaluation of potential biases in training data or fairness across different demographic groups for enterprise applications

## Confidence

- **High Confidence:** The architectural description (QK-Normalization, hybrid normalization, three-stage training) is well-specified and internally consistent. The benchmark evaluation methodology (using established metrics like MMMU, MME, DocVQA) follows standard practices.
- **Medium Confidence:** The comparative performance claims against other models appear reasonable given the parameter sizes, but the lack of ablation studies and statistical significance testing reduces confidence in attributing success to specific innovations.
- **Low Confidence:** The data efficiency claims and practical deployment advantages are asserted but not empirically validated. The absence of training cost analysis or inference benchmarking makes it difficult to assess real-world applicability.

## Next Checks

1. **Ablation study on normalization strategies:** Train three variants on a 50B token subset - uniform Pre-LN, uniform RMSNorm, and the proposed hybrid approach. Compare convergence speed and final performance to isolate the contribution of each normalization technique.

2. **Single-stage vs. three-stage comparison:** Train a model end-to-end with the same total tokens (487B/782B) and compare both convergence behavior and final performance to the staged approach. This directly tests the data efficiency claims.

3. **Inference efficiency benchmark:** Measure actual inference latency and memory usage on representative enterprise workloads (document processing, OCR extraction) across different batch sizes. Compare against claimed parameter-efficient models to validate practical deployment advantages.