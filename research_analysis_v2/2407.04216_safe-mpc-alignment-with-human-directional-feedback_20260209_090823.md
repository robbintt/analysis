---
ver: rpa2
title: Safe MPC Alignment with Human Directional Feedback
arxiv_id: '2407.04216'
source_url: https://arxiv.org/abs/2407.04216
tags:
- learning
- human
- robot
- safety
- corrections
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Safe MPC Alignment, a certifiable method for
  learning safety constraints in model predictive control (MPC) from human directional
  feedback. The approach enables a robot to interactively learn safety constraints
  by receiving online directional corrections from human supervisors, using only the
  direction (not magnitude) of corrections to update the safety parameter space.
---

# Safe MPC Alignment with Human Directional Feedback

## Quick Facts
- **arXiv ID:** 2407.04216
- **Source URL:** https://arxiv.org/abs/2407.04216
- **Reference count:** 40
- **Primary result:** A certifiable method for learning safety constraints in MPC from human directional feedback achieves 89.3% success in simulations and 84.0% in real-world experiments using only tens of corrections.

## Executive Summary
This paper introduces Safe MPC Alignment, a method that enables robots to learn safety constraints through interactive human feedback. The approach uses directional corrections (where only the direction, not magnitude, of human input matters) to progressively eliminate invalid safety parameters in a convex polytope representation. By selecting parameters as the center of the Maximum Volume Ellipsoid within the remaining valid space, the method guarantees exponential convergence within finite corrections or certifies when constraints cannot be learned due to misspecification.

## Method Summary
Safe MPC Alignment learns parameterized safety constraints $g_\theta$ for Model Predictive Control from online human directional corrections. The algorithm operates by maintaining a hypothesis space $\Theta$ as a convex polytope, where each human correction provides a hyperplane cut that removes incompatible parameter regions. The next query parameter is selected as the center of the Maximum Volume Ellipsoid inscribed in the current polytope, ensuring exponential volume reduction per step. The method uses log-barrier penalty-based MPC to execute robot motions and can certify misspecification when the hypothesis space converges to the boundary of the initial parameter bounds.

## Key Results
- **Efficiency:** Successfully learns safety constraints using only tens of human corrections (average 10.7 corrections in simulations).
- **Performance:** Achieves 89.3% success rate in simulation games and 84.0% in real-world water-pouring experiments.
- **Certification:** Can detect and certify when safety constraints fall outside the predefined feature space or initial bounds.

## Why This Works (Mechanism)

### Mechanism 1: Hypothesis Space Cutting via Directional Cuts
Using only the direction of human corrections allows the system to progressively eliminate invalid safety parameters while remaining robust to human scaling errors. The algorithm treats the parameter space $\Theta$ as a convex polytope and translates each correction into a linear inequality that removes incompatible regions.

### Mechanism 2: Maximum Volume Ellipsoid (MVE) Centering
Selecting the next query parameter as the center of the Maximum Volume Ellipsoid inscribed in the current hypothesis space ensures convergence with a bounded number of corrections. This geometric approach guarantees significant volume reduction per cut, preventing slow convergence seen in gradient descent.

### Mechanism 3: Misspecification Certification
The system can detect if the safety constraint cannot be learned because it falls outside the predefined feature space or initial bounds. If the hypothesis space shrinks to a zero-volume set on the boundary of the initial $\Theta_0$, the algorithm infers that the user's intent is incompatible with the provided features.

## Foundational Learning

- **Concept: Model Predictive Control (MPC) with Barrier Functions**
  - *Why needed:* The paper uses penalty-based MPC where safety constraints are converted into logarithmic barrier terms in the cost function. Understanding how $\gamma$ trades off task cost vs. safety constraint strictness is crucial.
  - *Quick check:* If $\gamma \to 0$, does the solution approach the original constrained problem or a relaxed version?

- **Concept: Convex Geometry (Half-spaces and Polytopes)**
  - *Why needed:* The core algorithm is geometric, iteratively slicing a convex shape (polytope) using hyperplanes derived from human feedback.
  - *Quick check:* Does intersecting a convex set with a half-space always result in a convex set?

- **Concept: Feature-based Constraint Representation**
  - *Why needed:* The safety constraint $g_\theta$ is a linear combination of features $\phi$ (e.g., RBFs or polynomials). The method assumes the "true" safety is linear in these features.
  - *Quick check:* If the true safety constraint is a complex nonlinear shape not captured by $\phi$, can this method learn it?

## Architecture Onboarding

- **Component map:** MPC Controller -> Feedback Interface -> Alignment Module (Cutter -> MVE Solver -> Updator) -> MPC Controller
- **Critical path:** Execute MPC → User provides correction → Compute gradient of barrier function → Derive hyperplane cut → Solve for MVE center → Update MPC weights
- **Design tradeoffs:**
  - Direction vs. Magnitude: Using direction makes the system robust to "overshooting" but ignores the information density of magnitude
  - Feature Selection: The system is "certifiable" only if the features can represent the constraint; poor features lead to misspecification
  - Parameter $\rho_H$: Acts as a convergence threshold; too small requires many corrections, too large causes premature convergence
- **Failure signatures:**
  - Oscillation/No Convergence: Human corrections are inconsistent
  - Boundary Convergence: MVE center drifts to the edge of the initial box (wrong features or bounds)
  - Conservative Motion: If $\gamma$ is too high, robot moves too slowly
- **First 3 experiments:**
  1. **Sanity Check (Pendulum):** Implement the 2D inverted pendulum example and verify hypothesis volume decreases linearly
  2. **Misspecification Test:** Force a failure case by setting initial bounds outside the ground truth parameter and verify "misspecification" declaration
  3. **Robustness to Noise:** Introduce Gaussian noise to simulated corrections and compare convergence speed against baseline

## Open Questions the Paper Calls Out
1. Can the method be extended to learn the structure of safety features $\phi$ online rather than relying on fixed, pre-defined features?
2. How can the learning process be modified to ensure learned safety constraints generalize across the entire boundary of the safe set?
3. How does introducing robustness mechanisms like relaxation factors or voting systems affect theoretical guarantees of finite convergence and misspecification certification?

## Limitations
- The MVE optimization becomes computationally intensive in high-dimensional feature spaces, potentially limiting real-time applicability
- The directional feedback assumption may not hold for all human supervisors, particularly those without technical background
- The misspecification detection mechanism relies on heuristic thresholds that may lead to false positives in high-dimensional spaces

## Confidence
- **High confidence:** The geometric cutting mechanism and MVE centering approach are mathematically sound with established convergence proofs
- **Medium confidence:** The simulated correction model reasonably approximates human feedback but may not capture all real-world variability
- **Medium confidence:** The real-world water-pouring experiment demonstrates practical feasibility though with limited trial numbers

## Next Checks
1. **Runtime benchmarking:** Measure MVE solver computation time across varying feature dimensions (2D to 10D) to establish practical limits for real-time deployment
2. **Human supervisor variability:** Conduct experiments with diverse user groups (technical vs. non-technical) to validate the directional feedback assumption across different supervision styles
3. **Feature sensitivity analysis:** Systematically test the algorithm with different feature types (polynomial, RBF, neural) on the same task to quantify sensitivity to feature selection