---
ver: rpa2
title: MCP-Orchestrated Multi-Agent System for Automated Disinformation Detection
arxiv_id: '2508.10143'
source_url: https://arxiv.org/abs/2508.10143
tags:
- agent
- data
- information
- news
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents an MCP-orchestrated multi-agent system for\
  \ automated disinformation detection. The system combines four specialized agents\u2014\
  logistic regression, Wikipedia knowledge checking, coherence detection, and web-scraped\
  \ data analysis\u2014working together to detect disinformation in news articles."
---

# MCP-Orchestrated Multi-Agent System for Automated Disinformation Detection

## Quick Facts
- arXiv ID: 2508.10143
- Source URL: https://arxiv.org/abs/2508.10143
- Reference count: 40
- Primary result: 95.3% accuracy with F1 score of 0.964 using weighted aggregation of four specialized agents

## Executive Summary
This paper presents a multi-agent system for automated disinformation detection that combines four specialized agents—logistic regression, Wikipedia knowledge checking, coherence detection, and web-scraped data analysis—working together via the Model Context Protocol (MCP). The system achieves 95.3% accuracy on disinformation detection tasks, significantly outperforming individual agents and traditional approaches. Using relation extraction and shared context, the agents collaborate to evaluate content through multiple lenses, with weighted aggregation based on individual misclassification rates proving superior to algorithmic threshold optimization.

## Method Summary
The system implements four specialized agents orchestrated through MCP for disinformation detection in news articles. The Classic ML agent uses HashingVectorizer with SGD logistic regression, the Wikipedia agent performs entity extraction and knowledge base verification, the Coherence Detector analyzes logical consistency via Llama3 prompts, and the Web Scraped Data Analyzer extracts semantic relations from search results. Agents process input sequentially with shared context updates, and outputs are combined using weights derived from individual misclassification rates (19%, 23%, 17%, 41% respectively) with a threshold of 0.41.

## Key Results
- Achieves 95.3% accuracy and 0.964 F1 score on disinformation detection
- Weighted aggregation based on misclassification rates outperforms algorithmic threshold optimization
- Modular architecture allows for scalability while maintaining transparency
- Individual agent accuracies: Classic ML (61%), Wikipedia (67%), Coherence (64%), Scraped Data (88%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weighted aggregation based on misclassification rates outperforms algorithmic threshold optimization for combining multi-agent outputs.
- Mechanism: Each agent's weight is derived from its historical accuracy (wi = ci/Σcj, where ci = 1 - mi), giving higher influence to agents with lower misclassification rates. The Scraped Data Analyzer (41% weight) dominates due to 88% accuracy, while the Coherence Detector (17% weight) has minimal influence due to 64% accuracy.
- Core assumption: Agent misclassification rates measured on evaluation data generalize to production distributions; agent errors are not perfectly correlated.
- Evidence anchors: [abstract] "The weighted aggregation method, mathematically derived from individual agent misclassification rates, proves superior to algorithmic threshold optimization." [Section V.A] Weight formula and final agent weights: Classic ML (19%), Wikipedia (23%), Coherence (17%), Scraped Data (41%)

### Mechanism 2
- Claim: Shared context via Model Context Protocol (MCP) enables incremental learning across agents by passing extracted relations through the pipeline.
- Mechanism: MCP maintains a common context that each agent updates as it processes input. The Classic ML agent can update weights with web-scraped data before predicting; downstream agents receive enriched context from predecessors' relation extraction results.
- Core assumption: Context updates are beneficial and do not introduce cascading errors; shared memory does not become a bottleneck.
- Evidence anchors: [abstract] "The system is orchestrated via the Model Context Protocol (MCP), offering shared context and live learning across components." [Section IV.A] "This agent uses the context of the Web Scraper Module in order to obtain new data based on user input, which it then uses to update its own weights before making a prediction (i.e. online learning)."

### Mechanism 3
- Claim: Complementary agent specialization provides cross-validation across different detection modalities (statistical patterns, knowledge base verification, coherence, real-time web evidence).
- Mechanism: Four agents operate on different signal types—Classic ML captures surface patterns, Wikipedia agent verifies against structured knowledge, Coherence detects logical disjointedness, Scraped Data analyzer fetches real-time evidence. Errors in one modality are often caught by another.
- Core assumption: Disinformation exhibits detectable signals across at least one of these modalities; agents make independent errors that aggregation can correct.
- Evidence anchors: [abstract] "combines four specialized agents—logistic regression, Wikipedia knowledge checking, coherence detection, and web-scraped data analysis—working together to detect disinformation" [Table I] Individual agent accuracies: Classic ML (61%), Wikipedia (67%), Coherence (64%), Scraped Data (88%); ensemble reaches 95.3%

## Foundational Learning

- Concept: **Relation Extraction (RE)**
  - Why needed here: The core technical approach identifies semantic relationships between entities (subject-relation-object triplets) to verify claims against knowledge bases and web data.
  - Quick check question: Given "Marie Curie discovered radium," can you extract the triplet (Marie Curie, discovered, radium)?

- Concept: **Model Context Protocol (MCP)**
  - Why needed here: Understanding orchestration requires knowing how MCP standardizes agent communication, context sharing, and tool discovery across heterogeneous components.
  - Quick check question: How does MCP differ from a simple function call API in terms of context persistence and tool discovery?

- Concept: **Named Entity Recognition (NER)**
  - Why needed here: The Wikipedia agent relies on NER to extract entities (people, organizations, events) for querying the knowledge base.
  - Quick check question: What entity types would NER extract from "The WHO announced new guidelines in Geneva"?

## Architecture Onboarding

- Component map: Input → Orchestrator (MCP) → [4 agents in sequence, context updated after each] → Aggregator (weighted threshold 0.41) → Final verdict with confidence

- Critical path: Input → Orchestrator (MCP) → [4 agents in sequence, context updated after each] → Aggregator (weighted threshold 0.41) → Final verdict with confidence

- Design tradeoffs:
  - HashingVectorizer vs TF-IDF: Lower memory, supports streaming, but trades some accuracy
  - Static weights vs adaptive: Mathematically derived weights are interpretable but don't adapt to domain shifts
  - Sequential pipeline vs parallel: Shared context requires sequencing; limits parallelization

- Failure signatures:
  - Out-of-domain input: Classic ML accuracy drops from 92% to 61% (see Table II)
  - API failures: DuckDuckGo/Wikipedia outages disable Scraped Data and Wiki agents
  - Recursive disinformation: If search results are manipulated, web-based verification fails
  - Coherent fake news: Well-written disinformation evades coherence detection

- First 3 experiments:
  1. Reproduce single-agent baselines on Kaggle/Snopes data to verify Table I metrics and establish your evaluation pipeline.
  2. Ablate one agent at a time to measure contribution; start with removing the lowest-weighted Coherence Detector and observe F1 change.
  3. Test on adversarial samples: take real news, introduce controlled relation flips (e.g., swap entities), verify Scraped Data Analyzer catches them.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust is the static weighted aggregation mechanism against adversarial attacks designed to exploit specific agent misclassification rates?
- Basis in paper: [explicit] The authors state: "The current architecture has not been extensively tested against adversarial attacks... static weight aggregation... may prove vulnerable to attackers who study and exploit the system’s patterns."
- Why unresolved: The system uses fixed weights derived from baseline misclassification rates, which creates a static attack surface that has not been evaluated.
- What evidence would resolve it: Performance evaluation of the ensemble under adversarial conditions where inputs are specifically perturbed to mislead the highest-weighted agents.

### Open Question 2
- Question: How can the recursive feedback loop be mitigated when web search results are themselves contaminated by the disinformation the system aims to detect?
- Basis in paper: [explicit] The authors identify a "recursive problem where disinformation might influence the search results used to detect it," suggesting the need for more robust verification methods that do not depend solely on web searches.
- Why unresolved: The Web Scraped Data Analyzer relies on external search APIs (DuckDuckGo), which may return poisoned or biased results, creating a circular dependency.
- What evidence would resolve it: Testing system accuracy using search results intentionally seeded with conflicting disinformation versus verified clean knowledge bases.

### Open Question 3
- Question: To what extent does detection performance vary across specific knowledge domains, such as political news versus scientific or health content?
- Basis in paper: [explicit] The authors note that while mixed datasets were used, "we did not systematically analyze performance variations by domain," acknowledging that different content types present unique challenges.
- Why unresolved: The paper reports aggregate metrics (95.3% accuracy) but lacks stratified analysis to determine if the Wikipedia agent or ML agent fails on specific topics like emerging technical fields.
- What evidence would resolve it: Ablation studies presenting F1 scores stratified by distinct domains (e.g., politics, health, science) rather than single aggregate scores.

## Limitations

- Static weighted aggregation may be vulnerable to adversarial attacks targeting specific agent misclassification rates
- Performance relies heavily on external API availability (DuckDuckGo, Wikipedia, Ollama) creating single points of failure
- Sequential pipeline architecture limits parallelization and could become a bottleneck at scale

## Confidence

- System architecture and component design: **High**
- Performance metrics and comparison baselines: **High**
- Weighted aggregation mathematical framework: **High**
- Real-world deployment robustness: **Medium**
- Generalization to unseen disinformation patterns: **Low**

## Next Checks

1. **Ablation study on domain adaptation**: Systematically remove the Scraped Data Analyzer (highest-weighted component) and test on out-of-domain datasets to quantify its contribution to the 95.3% accuracy and identify vulnerability to distribution shifts.

2. **Error correlation analysis**: Measure pairwise correlation coefficients between agent errors to verify the assumption of independence that underlies the weighted aggregation mechanism—high correlations would invalidate the mathematical framework.

3. **API dependency stress testing**: Simulate 30%, 60%, and 90% failure rates for DuckDuckGo, Wikipedia, and Ollama APIs to measure cascading performance degradation and identify the system's operational threshold under degraded conditions.