---
ver: rpa2
title: Adversarial Attacks Leverage Interference Between Features in Superposition
arxiv_id: '2510.11709'
source_url: https://arxiv.org/abs/2510.11709
tags:
- features
- adversarial
- layer
- input
- superposition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work proposes that adversarial vulnerability in neural networks\
  \ can arise from superposition\u2014the representation of more features than available\
  \ dimensions\u2014rather than from learning flaws or non-robust input features.\
  \ The authors demonstrate that adversarial perturbations exploit interference between\
  \ superposed features, with attack patterns being predictable from feature arrangements."
---

# Adversarial Attacks Leverage Interference Between Features in Superposition

## Quick Facts
- arXiv ID: 2510.11709
- Source URL: https://arxiv.org/abs/2510.11709
- Reference count: 40
- Primary result: Adversarial vulnerability arises from interference between superposed features rather than from non-robust input features or flawed learning.

## Executive Summary
This work proposes that adversarial vulnerability in neural networks can arise from superposition—the representation of more features than available dimensions—rather than from learning flaws or non-robust input features. The authors demonstrate that adversarial perturbations exploit interference between superposed features, with attack patterns being predictable from feature arrangements. They show that input correlations constrain feature geometry, creating consistent arrangements across models that explain attack transferability. Using synthetic models and a vision transformer on CIFAR-10, they establish that superposition is sufficient (but not necessary) for adversarial vulnerability. The findings suggest that adversarial vulnerability can be a byproduct of networks' representational compression, offering a mechanistic framework that reconciles architectural constraints with feature properties in adversarial vulnerability.

## Method Summary
The paper investigates adversarial vulnerability through three experimental settings: (1) synthetic toy models with controlled superposition, (2) ViT on CIFAR-10 with engineered bottleneck, and (3) modular addition task with orthogonal representations. The synthetic models use a two-layer bottleneck architecture where an encoder compresses input to fewer dimensions than features, forcing superposition. Adversarial examples are generated using PGD attacks, and their properties are analyzed relative to theoretical predictions. The ViT experiments add a post-hoc bottleneck layer to create superposition, while the modular addition experiments test algorithmic brittleness in orthogonal representations. Geometric similarity between models is measured using pairwise cosine similarity of encoder weight columns.

## Key Results
- Adversarial perturbations align with the interference between superposed features, specifically following the direction W_e^T(v_k - v_j)
- Input correlations force models into similar superposition geometries, explaining high attack transferability (94% vs 18% for uncorrelated data)
- Superposition is sufficient but not necessary for adversarial vulnerability; algorithmic brittleness exists independently
- Attack transferability correlates with geometric similarity of encoder weight arrangements across models

## Why This Works (Mechanism)

### Mechanism 1: Interference from Superposition Geometry
Adversarial vulnerability emerges from efficient encoding through superposition, where perturbations exploit non-orthogonality of feature representations. When representing M features in d dimensions (M > d), features are stored as non-orthogonal directions. Adversarial perturbations align with W_e^T(v_k - v_j), leveraging interference between unrelated features to cross decision boundaries efficiently.

### Mechanism 2: Correlations Constrain Geometry and Enable Transfer
Attack transferability between independently trained models is driven by input correlations forcing similar superposition geometries. Strong correlations in training data constrain possible arrangements of feature directions, causing different initializations to converge to similar geometric arrangements. Shared geometry leads to shared vulnerabilities and high transferability.

### Mechanism 3: Algorithmic Brittleness (Non-Superposition Vulnerability)
Superposition is sufficient but not necessary for adversarial vulnerability. In tasks like modular addition, models learn specific algorithms (e.g., trigonometric features) that can be targeted independently of feature interference. Attacks can exploit these algorithmic components rather than relying on superposition geometry.

## Foundational Learning

- **Linear Representation Hypothesis (LRH):** Features are represented as linear directions in activation space. Why needed: The entire analysis depends on features being linear directions for "interference" (dot products) and "superposition geometry" to be defined. Quick check: If features were represented as complex non-linear manifolds, how would "feature interference" change?

- **Superposition & Almost-Orthogonality:** Networks can store more features than dimensions by sacrificing orthogonality. Why needed: This "packing" creates the vulnerability surface. Quick check: Why does sparsity assist in superposition? (Answer: It allows disambiguation of non-orthogonal features during reconstruction)

- **Projected Gradient Descent (PGD):** Standard tool for generating adversarial examples. Why needed: The paper analyzes PGD output properties, requiring understanding of its iterative constraint mechanism. Quick check: How does ℓ_∞ constraint in PGD differ from ℓ_2 constraint in terms of resulting perturbation shape?

## Architecture Onboarding

- **Component map:** Input Layer -> Encoder (W_e) -> Latent Space (h) -> Decoder/Head
- **Critical path:** The bottleneck dimension m is the primary control knob. Reducing m increases superposition pressure (k/m ratio), increasing vulnerability and geometric consistency.
- **Design tradeoffs:** Efficiency vs. Robustness (lower m improves parameter efficiency but degrades robust accuracy), Consistency vs. Transfer Risk (correlated data forces consistent geometries but increases transferability)
- **Failure signatures:** High Transferability (>80% success between seeds), Algorithmic Brittleness (certified robustness increases attack budget but doesn't change attack direction)
- **First 3 experiments:**
  1. Train toy model with m=k (no superposition) and verify standard adversarial success drops to near zero
  2. Train identical architectures on "Global" vs "Uncorrelated" synthetic data, measure geometric similarity via cosine similarity of W_e columns
  3. Run PGD attacks on bottlenecked model, compare perturbation vector δ to theoretical optimum W_e^T(v_k - v_j)

## Open Questions the Paper Calls Out
- How does robust or adversarial training reshape the geometry of superposed features?
- Can constrained adversarial attacks (e.g., universal perturbations, patch attacks) exploit superposition interference?
- What is the quantitative "adversarial cost" of superposition relative to the accuracy-robustness trade-off?

## Limitations
- The superposition mechanism is sufficient but not necessary, potentially not explaining all attack vectors
- Results rely heavily on synthetic data with limited validation on diverse real-world architectures
- Geometric similarity measurements depend on pairwise cosine similarity of encoder weight columns

## Confidence
- **High confidence:** Superposition geometry directly influences attack directions - supported by analytical derivation and strong empirical alignment
- **Medium confidence:** Input correlations drive geometric convergence and transferability - robust across multiple correlation conditions but relies on synthetic data
- **Medium confidence:** Algorithmic brittleness exists independently of superposition - demonstrated in modular addition but may not generalize

## Next Checks
1. Test correlation-geometry-transferability relationship on ResNet and ConvNeXt architectures beyond ViT experiments
2. Design attacks that explicitly avoid W_e^T(v_k - v_j) direction to quantify superposition-specific vulnerability surface
3. Track feature geometry and interference patterns during training to validate whether models actively optimize for minimal interference or adapt to data correlations