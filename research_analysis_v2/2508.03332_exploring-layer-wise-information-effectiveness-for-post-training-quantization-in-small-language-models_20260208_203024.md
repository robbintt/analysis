---
ver: rpa2
title: Exploring Layer-wise Information Effectiveness for Post-Training Quantization
  in Small Language Models
arxiv_id: '2508.03332'
source_url: https://arxiv.org/abs/2508.03332
tags:
- arxiv
- quantization
- layer
- lieq
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "LieQ addresses the problem of severe accuracy loss in post-training\
  \ quantization of small language models (under 8B parameters) at extreme low-bit\
  \ precision, particularly 2-bit. The core method idea is to leverage a discovered\
  \ correlation between layer-wise functional saliency and representational compactness\u2014\
  layers with higher training-induced energy concentration are more critical and require\
  \ higher precision."
---

# Exploring Layer-wise Information Effectiveness for Post-Training Quantization in Small Language Models

## Quick Facts
- arXiv ID: 2508.03332
- Source URL: https://arxiv.org/abs/2508.03332
- Authors: He Xiao; Qingyao Yang; Dirui Xie; Wendong Xu; Zunhai Su; Runming yang; Wenyong Zhou; Haobo Liu; Zhengwu Liu; Ngai Wong
- Reference count: 15
- Primary result: LieQ achieves state-of-the-art 2.0-bit post-training quantization for small language models by leveraging geometric layer sensitivity proxies.

## Executive Summary
Post-training quantization (PTQ) of small language models (under 8B parameters) faces severe accuracy loss at extreme low-bit precision, particularly 2-bit. LieQ addresses this by discovering a strong correlation between layer-wise functional saliency and representational compactness—layers with higher training-induced energy concentration are more critical and require higher precision. The method uses a purely geometry-driven sensitivity proxy to automatically allocate bit-widths, preserving uniform bit-width within each layer while mixing precision across layers for hardware efficiency.

## Method Summary
LieQ is a post-training quantization method that automatically allocates mixed precision across layers based on geometric sensitivity proxies. The core approach computes singular value entropy on projected activations to measure representational compactness, comparing trained weights to random initialization. Layers are ranked by their compactness reduction (Δr_ℓ), with the top-k layers assigned 4-bit precision and remaining layers 2-bit. This preserves standard multiplication kernels while achieving ~2.0 average bits, significantly outperforming naive 2-bit quantization on Qwen3 and LLaMA3.x families.

## Key Results
- LieQ reduces perplexity gap from orders of magnitude to near-FP16 levels at ~2.05 bits on Qwen3-4B and LLaMA-3.x models
- Strong correlation (Spearman ρ > 0.8) between layer-wise functional saliency and representational compactness across multiple datasets
- Data-free layer selection using only weight statistics, eliminating expensive inference-based perplexity probing
- Hardware-friendly uniform bit-width within layers while mixing precision across layers

## Why This Works (Mechanism)

### Mechanism 1: Representational Compactness as Static Sensitivity Proxy
The paper claims a layer's functional indispensability can be predicted from its geometric weight structure without inference-based probing. Training concentrates information into lower-rank manifolds, detectable via reduction in singular value entropy. The core assumption is that training-induced compression of representational manifold correlates with functional importance. Evidence shows Spearman correlation ρ > 0.8 between ΔPPL_ℓ and Δr_ℓ across depths on WikiText2, Dolly-15k, HH-RLHF, and C4 datasets.

### Mechanism 2: Layer-Ranked Mixed-Precision Allocation
Allocating high precision (4-bit) to only the top-K most compact layers and 2-bit to the rest recovers most accuracy loss at ~2.0 average bits. The core assumption is that protecting geometrically critical layers is sufficient while other layers tolerate aggressive quantization. Evidence shows at ~2.05 bits, LieQ reduces perplexity gap from orders of magnitude to near-FP16 levels on Qwen3-4B and LLaMA-3.x.

### Mechanism 3: Data-Free Layer Selection
Layer importance can be determined purely from weight statistics without validation set inference or gradient computation. The core assumption is that the geometric signature captured by SVD on one batch generalizes to full dataset. Evidence shows LieQ's "purely geometry-driven sensitivity proxy" enables automatic bit-width allocation without expensive gradient updates or inference-based perplexity probing.

## Foundational Learning

- **Post-Training Quantization (PTQ)**: The baseline problem (accuracy collapse at 2-bit) is essential. Quick check: How does PTQ differ from Quantization-Aware Training (QAT) in terms of computational overhead?

- **Singular Value Decomposition (SVD) and Spectral Analysis**: The core proxy relies on singular value entropy. Quick check: Given a matrix with singular values [10, 1, 0.1, 0.01], how would you compute the normalized energy distribution p_k?

- **Shannon Entropy**: Compactness is defined as exponential of negative Shannon entropy of the energy distribution. Quick check: What is the entropy of a uniform distribution vs. a concentrated distribution? Which has higher compactness?

## Architecture Onboarding

- **Component map**: SVD Module -> Compactness Scorer -> Layer Ranker -> Budget Allocator -> Quantization Backend
- **Critical path**:
  1. Load pretrained model and capture random initialization state
  2. Run single forward pass on calibration data to collect hidden states
  3. Compute SVD on Z = W·h for each projection; compute Compact(Z) and Compact(Z̃)
  4. Compute Δr_ℓ per layer; rank and select top-K for 4-bit
  5. Apply GPTQ or AWQ quantization at per-layer assigned precision
  6. Export quantized model with metadata indicating layer-wise bit-widths

- **Design tradeoffs**:
  - Within-layer uniformity vs. accuracy: Sacrifices per-group/element precision for hardware regularity
  - Single proxy vs. multi-metric: Uses only compactness; Top-k energy serves as validation
  - Calibration sensitivity: Depends on representative forward pass; unusual input distributions may skew scores

- **Failure signatures**:
  1. Persistently high perplexity (>100) at ~2.0 bits → Check if top-ranked layers align with known "super weight" layers
  2. Inconsistent rankings across calibration datasets → May indicate model with unstable geometry
  3. OOM during SVD computation → Reduce batch size or compute SVD on weight matrices directly

- **First 3 experiments**:
  1. Correlation validation: Compute both ΔPPL_ℓ and Δr_ℓ for all layers; verify Spearman ρ > 0.7 before proceeding
  2. Ablation on K: Sweep number of 4-bit layers from 1 to 16; plot perplexity vs. average bits
  3. Backend integration test: Apply LieQ with GPTQ-4bit on high-precision layers and GPTQ-2bit on others

## Open Questions the Paper Calls Out

### Open Question 1
Does the geometric compactness proxy generalize to Mixture-of-Experts (MoE) architectures where routing mechanisms may alter layer-wise information flow? The paper evaluates only dense Transformer models but does not test MoE models where expert specialization may change representational geometry.

### Open Question 2
What is the theoretical mechanism linking training-induced energy concentration to quantization vulnerability? The paper demonstrates strong empirical correlation but states the behavior "must stem from underlying weight structures" without formal proof.

### Open Question 3
Can the representational compactness metric guide activation quantization, or is it specific to weight-only compression? The paper acknowledges "Significant room remains for optimization in practical engineering" and focuses solely on weight quantization.

### Open Question 4
How does LieQ interact with rotation-based quantization methods that fundamentally alter weight distributions? The paper compares against QuIP#/AQLM but does not explore combining LieQ's layer selection with Hadamard rotations or codebook methods.

## Limitations
- Correlation between compactness and functional saliency may not generalize to all model families or training regimes
- Performance depends on unspecified calibration dataset characteristics and backend hyperparameters
- Hardware regularity constraint may sacrifice potential accuracy gains from finer-grained mixed-precision allocation

## Confidence

**High Confidence**: The geometric proxy (singular value entropy) is mathematically well-defined and the layer-ranked mixed-precision allocation mechanism is clearly specified.

**Medium Confidence**: The correlation between compactness and functional saliency (ρ > 0.8) is the foundational assumption. While demonstrated across multiple datasets, its universality across all model families is uncertain.

**Low Confidence**: The absolute performance numbers cannot be fully verified without the unspecified calibration data and backend hyperparameters.

## Next Checks

1. **Correlation Validation on Target Model**: On your specific model family, compute both functional saliency (drop-one-layer perplexity increase, ΔPPL_ℓ) and geometric compactness (Δr_ℓ) for all layers. Verify Spearman correlation ρ > 0.7 before applying LieQ.

2. **Calibration Data Sensitivity Analysis**: Test LieQ's layer rankings and resulting perplexity using multiple calibration datasets (different domains, sequence lengths, batch sizes). If rankings vary significantly or perplexity fluctuates by >20%, the method may be sensitive to calibration data choice.

3. **Backend Hyperparameter Ablation**: Apply LieQ with varying GPTQ/AWQ settings (group sizes: 32, 64, 128; activation orders: 0, 1, 2) while keeping the LieQ layer ranking fixed. If perplexity varies by more than 15% across settings, the backend's role is substantial.