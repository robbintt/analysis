---
ver: rpa2
title: Fine-Tuning Adversarially-Robust Transformers for Single-Image Dehazing
arxiv_id: '2504.17829'
source_url: https://arxiv.org/abs/2504.17829
tags:
- adversarial
- dehazing
- robustness
- fine-tuning
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the vulnerability of state-of-the-art single-image
  dehazing transformers to adversarial attacks, showing that even minimal pixel-level
  perturbations can significantly degrade dehazing performance (e.g., up to 2.8 dB
  PSNR loss with 1-pixel changes). To enhance robustness, the authors propose two
  lightweight fine-tuning strategies: Last Layer (LL), Scale-and-Bias (SB), and LINEar
  ADaptation (LINEAD).'
---

# Fine-Tuning Adversarially-Robust Transformers for Single-Image Dehazing

## Quick Facts
- **arXiv ID**: 2504.17829
- **Source URL**: https://arxiv.org/abs/2504.17829
- **Reference count**: 22
- **Primary result**: Lightweight fine-tuning strategies (SB, LINEAD) with adversarial training improve transformer robustness to adversarial attacks while maintaining clean-image dehazing performance.

## Executive Summary
This paper addresses the vulnerability of state-of-the-art single-image dehazing transformers to adversarial attacks, showing that even minimal pixel-level perturbations can significantly degrade dehazing performance (e.g., up to 2.8 dB PSNR loss with 1-pixel changes). To enhance robustness, the authors propose two lightweight fine-tuning strategies: Last Layer (LL), Scale-and-Bias (SB), and LINEar ADaptation (LINEAD). These methods are applied in conjunction with adversarial training (AT) and TRADES regularization to improve resistance against both ℓ∞ and ℓ0 adversarial attacks. Experimental results on RESIDE-Outdoor and remote sensing datasets (HazyDet UA V, RICE-I) demonstrate that the fine-tuned models maintain comparable clean-image performance while achieving substantially higher robustness against adversarial noise, including out-of-distribution data, with LINEAD yielding the best protection despite higher computational cost. The findings highlight the feasibility of enhancing dehazing model reliability without retraining from scratch.

## Method Summary
The authors propose fine-tuning pre-trained dehazing transformers using lightweight adapter modules (Scale-and-Bias or LINEAD) while keeping the base model frozen. Scale-and-Bias adds trainable scaling factors after attention blocks, acting as pseudo-normalizers, while LINEAD inserts 3x3 convolutional layers initialized as identity operators. These adapters are trained with adversarial training (AT) or TRADES regularization, which enforces similarity between clean and adversarial predictions. The approach maintains the pre-trained haze-removal features while adapting them to be more resistant to adversarial perturbations, evaluated against both ℓ∞ and ℓ0 attacks.

## Key Results
- Fine-tuned models maintain clean PSNR (32.19 dB with LINEAD+TRADES) while improving adversarial robustness
- LINEAD with TRADES achieves highest robustness (26.59 dB PSNR under ℓ∞ attack vs 17.39 dB for base model)
- SB method provides lightweight protection (<0.15% additional parameters) with good robustness
- Models show improved robustness on out-of-distribution remote sensing data (HazyDet UAV, RICE-I)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Injecting lightweight, trainable modules (Scale-and-Bias or Linear Layers) into a frozen pre-trained Transformer allows the architecture to adapt to adversarial distributions without catastrophic forgetting of the dehazing task.
- **Mechanism**: The proposed Scale-and-Bias (SB) method introduces scaling factors after attention blocks, acting as "pseudo-normalizers" to adjust feature distributions disrupted by adversarial noise. The LINEar ADaptation (LINEAD) method adds capacity via 3x3 convolutions. By freezing the bulk of the model, the pre-trained haze-removal features are preserved, while the new parameters learn to "correct" or stabilize representations when adversarial perturbations are present.
- **Core assumption**: The pre-trained Transformer has already learned robust, generalizable features for dehazing that do not need relearning; the vulnerability lies primarily in the activation scaling or output projection of these features.
- **Evidence anchors**:
  - [Section III-A]: "...SB... adds a trainable scaling factor... initialized with 1.0... act as pseudo-normalizers."
  - [Section III-A]: "...LINEAD... initialize these layers with identity operators, starting from a similar performance in the first iterations..."
  - [Corpus]: "Are Fast Methods Stable in Adversarially Robust Transfer Learning?" supports the viability of efficient fine-tuning for robustness but highlights stability risks.
- **Break condition**: If the adversarial noise affects low-level pixel statistics in a way that corrupts the input to the first layer irreparably, tuning only intermediate/output layers (SB/LINEAD) may fail to recover the clean signal.

### Mechanism 2
- **Claim**: Enforcing similarity between predictions of clean and adversarial images via TRADES regularization explicitly forces the network to ignore imperceptible perturbations.
- **Mechanism**: By adding a regularization term $L_{reg}(T(x_i), T(x_i + z_i))$ to the loss function, the optimizer minimizes the distance between the latent representations or outputs of a clean image and its attacked counterpart. This theoretically flattens the loss landscape around data points, reducing the efficacy of gradient-based attacks.
- **Core assumption**: The perturbation $z$ is small enough ($\|z\|_p \leq \epsilon$) that the "correct" dehazed output should ideally remain identical to the clean prediction.
- **Evidence anchors**:
  - [Section II-B]: "TRADES... adds a term to increase the similarity between predictions over clean and adversarial data..."
  - [Table II]: Shows LINEAD + TRADES ($\lambda=0.5$) maintaining high clean PSNR (32.19) while improving robustness compared to pure AT in some configurations.
  - [Corpus]: Weak direct evidence in neighbors for TRADES specifically in dehazing, but "Adversarially Robust CLIP Models" discusses robust feature alignment, analogous to this mechanism.
- **Break condition**: If the regularization weight $\lambda$ is too high, the model may underfit the dehazing task (smoothing out details) to satisfy the robustness constraint; if too low, it provides insufficient protection.

### Mechanism 3
- **Claim**: Initializing adaptation layers (LINEAD) as identity operators prevents the "chaotic" degradation of performance at the start of fine-tuning, ensuring a stable optimization path.
- **Mechanism**: When adding new layers to a deep network, random initialization introduces noise into the forward pass, destroying the pre-trained features. Identity initialization ensures the modified network behaves exactly like the base model at $t=0$, allowing the gradients to selectively "activate" the new layers only where they reduce the adversarial loss.
- **Core assumption**: The base model is a valid starting point (local minimum) from which one can traverse the loss landscape to a robust minimum without passing through a region of high error.
- **Evidence anchors**:
  - [Section III-A]: "...initialize these layers with identity operators... avoiding chaotic initial behaviors that would hinder the frozen model performance..."
  - [Section IV-B]: "LL exhibited inferior performance... SB and LINEAD allow the newly formed architecture to easily adapt..."
- **Break condition**: If the new layers are inserted in a way that creates singularities or vanishing gradients when initialized as identity (less likely with Conv2d/Linear but possible in complex branches), training may stall.

## Foundational Learning

- **Concept**: **$\ell_p$-norm Adversarial Attacks ($\ell_0$ vs $\ell_\infty$)**
  - **Why needed here**: The paper distinguishes between budget attacks (changing *any* pixel by a tiny amount, $\ell_\infty$) and sparse attacks (changing *few* pixels drastically, $\ell_0$). Understanding this difference is crucial because the defense strategies (AT/TRADES) are primarily optimized against $\ell_\infty$, while $\ell_0$ results demonstrate a specific vulnerability to "One Pixel" attacks.
  - **Quick check question**: Does an $\ell_\infty$ constraint limit the *magnitude* of change per pixel or the *count* of changed pixels?

- **Concept**: **Image-to-Image (I2I) Translation Metrics (PSNR/SSIM)**
  - **Why needed here**: Unlike classification (accuracy), dehazing quality is continuous. The paper reports success in dB (PSNR). You must understand that a drop of 2.8 dB (from $\approx$33 to $\approx$30 or lower) is visually significant and structurally damaging (SSIM drop), validating the severity of the attack.
  - **Quick check question**: If a dehazing model outputs an image identical to the input hazy image, would the PSNR be high or low?

- **Concept**: **Adversarial Training (AT) vs. TRADES**
  - **Why needed here**: The paper contrasts two robustness strategies. Standard AT trains on attacked data (changing the data distribution), while TRADES adds a regularization term (changing the loss landscape). The results show a trade-off where TRADES often preserves clean performance better than pure AT.
  - **Quick check question**: Which method modifies the *dataset* during training, and which modifies the *loss function*?

## Architecture Onboarding

- **Component map**:
  - Pre-trained Backbone (DehazeFormer/MB-TaylorFormer) -> Adapter Modules (SB/LINEAD) -> Output Layer
  - SB: Scale Layer (init 1.0) + Bias after every Attention/Conv block
  - LINEAD: 3x3 Conv2d (init Identity) after every Attention/Conv block

- **Critical path**:
  1. Load pre-trained weights (Freeze all)
  2. Insert SB or LINEAD modules (Initialize correctly: 1.0 for Scale, Identity for Conv)
  3. **Batch Generation**: For every batch, generate adversarial examples $x_{adv}$ using the *current* model state (iterative PGD)
  4. **Forward Pass**: Compute output for both $x_{clean}$ and $x_{adv}$
  5. **Loss Calculation**: Compute $L_{total} = L_1(x_{clean}, y) + \lambda \cdot L_{reg}(x_{clean}, x_{adv})$
  6. Backpropagate updates *only* to the new adapter modules

- **Design tradeoffs**:
  - **SB vs. LINEAD**: SB is extremely lightweight ($<0.15\%$ params) and fast, suitable for quick deployment. LINEAD offers higher robustness (highest PSNR under attack) but increases parameters by $\approx 40\%$ and slows training.
  - **Robustness vs. Clean Performance**: Aggressive adversarial training (high $\lambda$ or strong attacks) tends to drop clean image PSNR slightly (e.g., from 33.7 to 30.9).

- **Failure signatures**:
  - **Identity Collapse**: If LINEAD layers are not initialized as identity, the initial validation PSNR will be random/low, and the model may fail to converge to the pre-trained baseline.
  - **Obfuscated Gradients**: If the robustness looks "too good" (e.g., 0% attack success) but visual results are noisy, check for gradient masking (though AT usually avoids this).
  - **Overfitting to Noise**: If the model performs well on Gaussian noise but fails on PGD noise, it has learned specific noise patterns rather than robust features.

- **First 3 experiments**:
  1. **Vulnerability Audit**: Run the pre-trained base model (no fine-tuning) against the $\ell_0$ (1-pixel) and $\ell_\infty$ ($\epsilon=4/255$) attacks to reproduce the baseline drops (Table II) and confirm the threat model.
  2. **Efficient Validation (SB)**: Implement the **Scale-and-Bias (SB)** method with Adversarial Training. Train for 15 epochs. Plot Clean PSNR vs. Adversarial PSNR to verify if robustness improves with minimal parameter cost.
  3. **Capacity Test (LINEAD + TRADES)**: Implement **LINEAD** with TRADES ($\lambda=0.5$). Compare the "1-pixel attack" robustness against the SB model to determine if the added computational cost of LINEAD is justified for your specific safety requirements.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can certified defense methods be effectively adapted for image-to-image dehazing transformers without incurring prohibitive computational costs?
- Basis in paper: [explicit] The authors explicitly exclude certified methods in Section II.B, noting their high computational overhead limits network complexity, and state they are "leaving their usage in dehazing as a future endeavor."
- Why unresolved: Certified robustness often requires constrained architectures, which conflict with the high capacity needed for state-of-the-art image restoration tasks.
- What evidence would resolve it: A certified defense implementation for a dehazing transformer that maintains feasible training times and competitive PSNR on clean data.

### Open Question 2
- Question: Do the proposed lightweight fine-tuning strategies (LL, SB, LINEAD) generalize to other low-level vision tasks, such as super-resolution or deraining?
- Basis in paper: [explicit] The conclusion states the authors plan on "going further with their usage in different applications."
- Why unresolved: The current study is restricted to single-image dehazing; it is unknown if the Scale-and-Bias or LINEAD adaptations are effective for tasks with different degradation models.
- What evidence would resolve it: Applying the fine-tuning strategies to non-dehazing restoration architectures and observing similar preservation of clean performance alongside increased adversarial robustness.

### Open Question 3
- Question: Does incorporating $\ell_0$-attacks into the adversarial training loop significantly improve robustness against sparse perturbations compared to training solely with $\ell_\infty$ attacks?
- Basis in paper: [inferred] The authors note in Section IV.A that they did not assess training under $\ell_0$-attacks due to "substantial computational cost," despite showing that $\ell_0$-attacks effectively degrade model performance.
- Why unresolved: It is unclear if the observed robustness against $\ell_0$-attacks is merely a byproduct of $\ell_\infty$ training or if specific $\ell_0$ optimization is required for optimal defense.
- What evidence would resolve it: A comparison of models trained with $\ell_0$ perturbations versus the proposed $\ell_\infty$-trained models when evaluated against sparse (pixel-change) attacks.

## Limitations

- **Unknown hyperparameters**: Critical PGD attack parameters (iterations, step size) are not specified, affecting reproducibility of robustness claims.
- **Computational cost**: LINEAD increases parameters by ~40%, potentially limiting real-time deployment on resource-constrained hardware.
- **Limited domain validation**: While tested on remote sensing datasets, robustness generalization to other degradation types (fog, smoke, sensor noise) remains unverified.

## Confidence

- **High Confidence**: The vulnerability of dehazing transformers to adversarial attacks (PSNR drops of 1-2.8 dB) and the general mechanism of fine-tuning with lightweight adapters (SB/LINEAD) to improve robustness are well-supported by the experimental results and align with established principles in transfer learning.
- **Medium Confidence**: The specific quantitative claims about PSNR improvements under attack (e.g., LINEAD achieving 26.59 dB vs 17.39 dB for base under ℓ∞) are plausible but depend critically on the unreported PGD hyperparameters. The TRADES regularization's effectiveness in preserving clean performance is supported but requires careful λ-tuning.
- **Low Confidence**: The generalizability of these robustness gains to all remote sensing applications is uncertain without broader testing across different imaging conditions and degradation types beyond haze.

## Next Checks

1. **Hyperparameter Sensitivity Audit**: Systematically vary PGD attack parameters (iterations: 5, 10, 20; step size: 0.5, 1, 2) during both training and evaluation to determine the sensitivity of reported robustness gains and establish a baseline for fair comparison.

2. **Real-Time Inference Impact**: Profile the inference speed and memory usage of SB vs. LINEAD on representative hardware to quantify the trade-off between the minimal parameter increase of SB and the higher robustness of LINEAD.

3. **Cross-Domain Robustness Transfer**: Evaluate the fine-tuned models on additional remote sensing datasets with different degradation types (e.g., fog, smoke, sensor noise) to test whether the adversarial training generalizes beyond the RESIDE-Outdoor distribution.