---
ver: rpa2
title: Inference on Optimal Policy Values and Other Irregular Functionals via Smoothing
arxiv_id: '2507.11780'
source_url: https://arxiv.org/abs/2507.11780
tags:
- have
- treatment
- where
- optimal
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of constructing confidence\
  \ intervals for the value of an optimal treatment policy, which is non-differentiable\
  \ due to the max operator involved. The authors propose a softmax smoothing-based\
  \ estimator that achieves \u221An convergence rates without requiring parametric\
  \ assumptions on outcome regressions or unrealistic margin assumptions."
---

# Inference on Optimal Policy Values and Other Irregular Functionals via Smoothing

## Quick Facts
- **arXiv ID:** 2507.11780
- **Source URL:** https://arxiv.org/abs/2507.11780
- **Reference count:** 40
- **Primary result:** √n-consistent inference for optimal policy values using softmax smoothing without parametric assumptions

## Executive Summary
This paper tackles the fundamental challenge of constructing confidence intervals for optimal treatment policy values, which are irregular functionals due to the non-differentiability of the max operator. Traditional approaches require either parametric assumptions or unrealistic margin conditions that are often violated in practice. The authors introduce a novel softmax smoothing technique that transforms the irregular problem into a regular one while maintaining computational efficiency. Their method achieves √n convergence rates without requiring the nuisance estimators to converge at prohibitively fast rates in the supremum norm.

The approach is broadly applicable beyond optimal policy values to other irregular functionals where standard semiparametric efficiency theory fails. By carefully controlling first-order bias through a Neyman-orthogonal score and bounding second-order remainder terms, the authors demonstrate that their smoothed estimator provides valid inference even when the density of sub-optimality gaps near zero is positive. This work bridges a significant gap between theoretical possibility and practical feasibility in policy learning from observational data.

## Method Summary
The method employs softmax smoothing to create a differentiable approximation of the optimal policy value functional. The key innovation is constructing a Neyman-orthogonal score that uses the softmax transformation of estimated Q-values combined with a debiasing term involving the derivative of the softmax function and inverse propensity weights. The estimator is implemented using K-fold cross-fitting to prevent overfitting, where nuisance functions (outcome regressions and propensity scores) are estimated on training folds and evaluated on held-out data. The smoothing parameter β_n is carefully calibrated to balance bias and variance, with theoretical requirements linking its growth rate to the density of sub-optimality gaps near zero. The final estimator achieves asymptotic normality through a combination of smooth approximation theory and empirical process techniques.

## Key Results
- √n-consistent confidence intervals for optimal policy values without parametric assumptions on outcome regressions
- Achieves statistical efficiency while requiring only moderate convergence rates (o_P(n^{-1/4}β_n^{-1/2})) for nuisance estimators
- Valid inference under realistic conditions where traditional methods fail due to positive density of near-optimal decisions
- Extends to general irregular functionals beyond optimal policy values

## Why This Works (Mechanism)
The method works by transforming the non-differentiable max operator into a smooth approximation via softmax, enabling standard asymptotic theory to apply. The Neyman orthogonality ensures that first-order bias from nuisance estimation vanishes, while careful control of the smoothing parameter β_n ensures that second-order bias remains negligible. The cross-fitting procedure prevents overfitting to the data used for final estimation, and the specific form of the debiasing term accounts for the particular structure of the smoothed score. By allowing the density of near-optimal decisions to be positive (violating traditional margin conditions), the method applies to more realistic settings while still achieving √n rates through the smoothing bias-variance tradeoff.

## Foundational Learning
- **Softmax Transformation:** Converts discrete max operations into smooth, differentiable functions; needed for applying differential calculus to irregular functionals; quick check: verify softmax approximates argmax as β → ∞
- **Neyman Orthogonality:** Property ensuring first-order bias from nuisance estimation vanishes; needed to achieve √n rates without requiring Donsker-class convergence for nuisances; quick check: confirm score is orthogonal to first-order nuisance perturbations
- **Cross-fitting:** Sample splitting technique to prevent overfitting in double/debiased machine learning; needed to maintain √n rates when using flexible ML for nuisance estimation; quick check: ensure each observation is used only once for final estimation
- **Riesz Representation:** Characterizes the influence function of the target functional; needed to construct the debiasing correction term; quick check: verify the representer satisfies the appropriate inner product condition
- **Smooth Approximation Theory:** Provides bounds on the error between softmax and max operators; needed to control the bias introduced by smoothing; quick check: confirm bias bounds scale appropriately with β_n
- **Empirical Process Techniques:** Tools for analyzing uniform convergence of estimated nuisances; needed to control the remainder terms in the asymptotic expansion; quick check: verify Donsker conditions hold for the nuisance classes

## Architecture Onboarding

**Component Map:**
Data → Cross-fitting Split → Nuisance Estimation (Q, p) → Smoothed Score Construction → Aggregation → Variance Estimation → Confidence Interval

**Critical Path:**
1. Split data into K folds
2. Estimate Q and p on training folds
3. Construct smoothed scores on held-out fold
4. Aggregate across folds for final estimate
5. Compute variance and form CI

**Design Tradeoffs:**
- Smoothing parameter β_n: Larger β reduces bias but increases variance and computational cost
- Cross-fitting folds: More folds reduce overfitting but increase computational burden
- Nuisance model complexity: More flexible models reduce misspecification bias but may require stronger convergence rate conditions

**Failure Signatures:**
- Coverage rates below nominal: Indicates β_n too small or nuisance convergence too slow
- Extremely wide confidence intervals: Suggests β_n too large or variance estimates inflated
- Numerical instability: Points to near-zero propensity estimates or exploding Hessian terms

**First Experiments:**
1. Implement softmax function and verify it approximates max as β increases
2. Construct the Neyman-orthogonal score and test orthogonality numerically
3. Perform single-fold estimation to debug the smoothed score calculation

## Open Questions the Paper Calls Out
- **Data-adaptive smoothing parameter selection:** How to develop methods for choosing the β_n sequence when the density of CATE near zero is unknown, given that theoretical convergence rates depend on this unknown parameter
- **Lower bounds for irregular laws:** What are the non-parametric and semi-parametric lower bounds when the probability of non-response (non-unique maximizers) is non-zero, since existing efficiency theory primarily covers regular laws
- **Relaxing polynomial density assumptions:** Can the polynomial density assumption on sub-optimality gaps be relaxed while maintaining √n convergence rates, as the current results rely heavily on this specific structure

## Limitations
- Practical choice of smoothing parameter β_n remains theoretically and empirically challenging, requiring knowledge of the unknown density parameter δ
- Reliance on propensity scores creates vulnerability to positivity violations, potentially causing numerical instability when estimated propensities are near zero
- Theoretical framework assumes bounded densities and smooth gaps, which may not hold in many real-world applications

## Confidence
- **High Confidence:** Theoretical framework for smoothed estimator and Neyman orthogonality
- **Medium Confidence:** Asymptotic normality result and √n convergence rate under stated conditions
- **Low Confidence:** Practical implementation details, particularly β_n choice and validation of smoothness assumptions

## Next Checks
1. **Simulation Study with Known δ:** Implement method on simulated data where sub-optimality gap smoothness δ is known, varying δ to validate theoretical growth rate requirements for β_n
2. **Sensitivity Analysis of β_n:** Perform grid search around theoretical rate n^{1/4} to empirically determine coverage rate sensitivity to smoothing parameter
3. **Propensity Score Robustness Test:** Evaluate estimator performance under varying propensity score estimation error, including near-positivity violation scenarios, to assess practical robustness of inverse propensity weighting correction