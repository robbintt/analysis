---
ver: rpa2
title: 'First Ask Then Answer: A Framework Design for AI Dialogue Based on Supplementary
  Questioning with Large Language Models'
arxiv_id: '2508.08308'
source_url: https://arxiv.org/abs/2508.08308
tags:
- fata
- information
- user
- questions
- while
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the First Ask Then Answer (FATA) framework
  to address the problem of incomplete user queries in LLM interactions. FATA proactively
  generates multi-dimensional supplementary questions to gather comprehensive user
  information before generating responses, improving response quality and relevance.
---

# First Ask Then Answer: A Framework Design for AI Dialogue Based on Supplementary Questioning with Large Language Models

## Quick Facts
- arXiv ID: 2508.08308
- Source URL: https://arxiv.org/abs/2508.08308
- Reference count: 2
- Primary result: FATA improves response quality by 40% and reduces coefficient of variation by 8% compared to baseline methods

## Executive Summary
This paper introduces the First Ask Then Answer (FATA) framework to address the problem of incomplete user queries in LLM interactions. FATA proactively generates multi-dimensional supplementary questions to gather comprehensive user information before generating responses, improving response quality and relevance. Unlike reactive clarification methods, FATA employs a single-turn approach to reduce interaction overhead. Experiments across 12 domains show FATA outperforms baseline methods by 40% and exhibits 8% lower coefficient of variation, indicating superior stability.

## Method Summary
The FATA framework operates through a two-stage process: first generating supplementary questions using an expert persona prompt, then generating final responses with enriched context from user answers. The evaluation uses 300 user cases across 12 industries, comparing three conditions: B-Prompt (incomplete baseline), C-Prompt (expert-level complete context), and FATA (two-stage question-then-answer). Responses are evaluated across 9 dimensions using multiple LLM evaluators (ChatGPT-O3, Claude 4 Opus, DeepSeek R1). The framework generates questions in a single turn, aggregates user responses with the original query, and produces final answers with improved accuracy and completeness.

## Key Results
- FATA outperforms baseline methods by 40% across weighted aggregate scores
- FATA exhibits 8% lower coefficient of variation, indicating superior stability
- FATA achieves 52.7% gain in Persona Recall score, demonstrating effective context integration
- Single-turn batching reduces interaction overhead while maintaining quality

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Proactive expert scaffolding enables non-expert users to articulate latent requirements.
- **Mechanism:** By prompting the LLM to adopt an "expert perspective," the model is conditioned to retrieve domain-specific priors regarding necessary constraints, externalizing the expert's internal checklist to guide users.
- **Core assumption:** The underlying LLM possesses sufficient parametric domain knowledge to generate relevant, high-value questions without external retrieval.
- **Evidence anchors:** [abstract] FATA "scaffold user expression, enabling non-expert users to formulate more comprehensive and contextually relevant queries." [section 3.1] FATA transforms implicit expert knowledge into explicit questioning frameworks.

### Mechanism 2
- **Claim:** Multi-dimensional questioning reduces solution-space entropy.
- **Mechanism:** The framework treats supplementary questions as intersecting constraints, systematically narrowing the set of valid solutions and reducing hallucination probability.
- **Core assumption:** Users provide accurate answers to generated questions.
- **Evidence anchors:** [section 3.2] "Multiple well-targeted supplementary questions create intersecting constraints... achieving H(Solution|Query) > H(Solution|Query + Supplementary Info)." [table 2] Shows up to 52.7% gain in Persona Recall.

### Mechanism 3
- **Claim:** Single-turn batching reduces interaction overhead while maintaining quality.
- **Mechanism:** FATA generates all questions simultaneously in a "checklist" approach, minimizing cognitive load of repeated exchanges and stabilizing dialogue flow.
- **Core assumption:** Users are willing to engage in structured, form-filling interaction rather than fluid chat.
- **Evidence anchors:** [abstract] "Adopts a single-turn strategy: all clarifying questions are produced at once, thereby reducing dialogue length." [section 5.3] Discusses "Conciseness paradox" of upfront efficiency.

## Foundational Learning

- **Concept: Prompt Chaining & Two-Stage Generation**
  - **Why needed here:** FATA is fundamentally a state-machine masquerading as a prompt, requiring understanding of how to maintain state between "Ask" and "Answer" phases.
  - **Quick check question:** How does the system handle the transition if the user refuses to answer the supplementary questions?

- **Concept: Persona-Based Prompting**
  - **Why needed here:** Question quality depends entirely on the model successfully "role-playing" a specific domain expert.
  - **Quick check question:** Does the prompt explicitly define the "expert persona" or ask the model to infer it from the query?

- **Concept: Coefficient of Variation (CV)**
  - **Why needed here:** The paper emphasizes stability (8% lower CV) over raw score improvements, requiring understanding of CV for production deployment evaluation.
  - **Quick check question:** Why is a lower CV arguably more important for production systems than higher mean score with high variance?

## Architecture Onboarding

- **Component map:** Input Sanitizer -> Stage 1 (F1) - Question Generator -> User Interaction Layer -> Context Aggregator -> Stage 2 (F2) - Response Generator
- **Critical path:** The Context Aggregator logic. If user answers are not clearly delimited from the original query when fed back into the LLM, the model may hallucinate facts or ignore new constraints.
- **Design tradeoffs:** Latency vs. Accuracy (adds full LLM inference round-trip + human interaction time) and Simplicity vs. Control (Prompt-Only approach allows immediate deployment but offers less granular control).
- **Failure signatures:** "Conciseness Paradox" (users complaining response is too long), Over-Questioning (asking for sensitive data despite constraints), Context Drift (user answers off-topic).
- **First 3 experiments:**
  1. Baseline Comparison: Run 50 ambiguous queries through standard Chat Completion API vs. FATA prompt chain, compare "Actionability" scores manually.
  2. Refusal Handling: Test "Quality Control" component by inputting vague answers ("I don't know") to supplementary questions, verify if model defaults to safe, generic response.
  3. Persona Drift: Input query spanning two domains (e.g., "legal implications of medical treatment"), check if generated questions favor one "expert persona" over the other or mix them incoherently.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does FATA retain its performance advantage when users provide low-quality or incomplete answers to supplementary questions, compared to simulated user profiles?
- **Basis in paper:** [explicit] Authors note simulated responses likely overestimate robustness to noisy or ambiguous human input.
- **Why unresolved:** Current evaluation relies on idealized, simulated responses that don't capture real-world human behavior patterns.
- **What evidence would resolve it:** User study with actual participants measuring performance degradation when supplementary answers are imperfect.

### Open Question 2
- **Question:** Can an adaptive mechanism be developed to dynamically trigger FATA only when necessary, rather than applying proactive questioning to all queries?
- **Basis in paper:** [explicit] Authors list "Adaptive Interaction Strategy" as future work, suggesting need for model choosing optimal questioning strategy based on real-time feedback.
- **Why unresolved:** Current framework always generates supplementary questions, which may be redundant for simple or well-specified queries.
- **What evidence would resolve it:** Classifier or heuristic predicting "information gap" severity, evaluating whether selective application maintains accuracy while reducing interaction overhead.

### Open Question 3
- **Question:** Does single-turn questioning strategy cause information overload or cognitive fatigue in highly complex scenarios with numerous interrelated variables?
- **Basis in paper:** [explicit] Authors identify "Scalability in Complex Systems" as limitation, noting single-turn questioning may lead to "information overload or gaps" in complex domains.
- **Why unresolved:** Paper evaluates broad range of domains but doesn't isolate "highly complex" scenarios to test user tolerance for long lists of simultaneous questions.
- **What evidence would resolve it:** Experiments with high-complexity variables measuring user dropout rates and answer quality relative to number of supplementary questions.

## Limitations

- **Prompt Design Variability:** FATA's effectiveness is tightly coupled to prompt engineering quality, with minor changes potentially significantly impacting performance in ways not captured in reported results.
- **Evaluation Model Bias:** Using LLM evaluators introduces potential circularity, as these models may favor responses generated by similar architectural approaches, making evaluation not fully objective.
- **Simulation Gap:** User response simulation using ground-truth profiles doesn't capture real-world human behavior patterns, potentially overestimating practical effectiveness in actual user interactions.

## Confidence

- **High Confidence:** 40% performance improvement over baseline methods and 8% reduction in coefficient of variation are supported by systematic evaluation across 12 domains with statistically significant paired t-tests.
- **Medium Confidence:** Mechanism claims about expert scaffolding and entropy reduction are theoretically sound but rely heavily on underlying LLM's domain knowledge, which isn't independently verified.
- **Low Confidence:** Single-turn approach's superiority over multi-turn clarification is asserted but not rigorously tested against reactive clarification methods in comparable interaction scenarios.

## Next Checks

1. **Human Evaluation Pilot:** Conduct small-scale study with actual users interacting with FATA-generated questions to validate whether simulated user responses accurately predict real human behavior patterns and response quality.
2. **Cross-LLM Validation:** Test FATA framework across multiple LLM architectures (not just ChatGPT variants) to determine if performance gains are model-dependent or represent genuine framework improvements.
3. **Interaction Cost Analysis:** Measure actual time-to-resolution and user satisfaction for FATA versus multi-turn clarification methods in real-world scenarios to validate claimed efficiency benefits beyond theoretical interaction counts.