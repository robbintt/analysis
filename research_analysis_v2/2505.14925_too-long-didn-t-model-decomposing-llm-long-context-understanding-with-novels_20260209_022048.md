---
ver: rpa2
title: 'Too Long, Didn''t Model: Decomposing LLM Long-Context Understanding With Novels'
arxiv_id: '2505.14925'
source_url: https://arxiv.org/abs/2505.14925
tags:
- narrative
- long-context
- performance
- long
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Too Long, Didn't Model (TLDM) benchmark evaluates large language
  models' ability to process and understand long novels beyond simple retrieval tasks.
  It tests summarization, storyworld tracking, and narrative time estimation across
  novels ranging from 32k to over 128k tokens using various input treatments.
---

# Too Long, Didn't Model: Decomposing LLM Long-Context Understanding With Novels

## Quick Facts
- arXiv ID: 2505.14925
- Source URL: https://arxiv.org/abs/2505.14925
- Reference count: 33
- Primary result: All tested frontier LLMs show significant long-context comprehension degradation beyond 64k tokens, with no model maintaining stable understanding at 128k+ tokens

## Executive Summary
TLDM benchmark evaluates LLM long-context understanding on novels beyond simple retrieval tasks. Testing seven frontier models across summarization, storyworld tracking, and narrative time estimation tasks, TLDM finds consistent performance below 64k tokens but significant degradation beyond that threshold. Open-weight models show steeper declines than commercial models. The benchmark demonstrates that despite million-token context windows, current LLMs struggle with complex long-context comprehension, particularly for non-summary tasks.

## Method Summary
The TLDM benchmark uses 40 public domain English novels binned by token length into four ranges (<32k, 32-64k, 64-128k, >128k tokens). Five treatment combinations are applied: title-only, unaltered text, chapter-separated messages, truncated window, and shuffled chapters. Three tasks are evaluated: summarization (one sentence per chapter), storyworld tracking (report last known location of all characters), and narrative time estimation (predict elapsed time). Performance is measured by comparing full-novel predictions against concatenated chapter-level predictions using semantic similarity, Jaccard + semantic similarity, and absolute relative error metrics.

## Key Results
- All seven tested frontier LLMs exhibit consistent performance below 64k tokens but show significant degradation beyond that threshold
- No model maintains stable understanding at 128k+ tokens, with open-weight models like Gemma 3 and Qwen 3 showing steepest declines
- Entity tracking (storyworld) proves more fragile than summarization across long contexts, with near-zero performance at >128k tokens
- Sequential narrative structure aids comprehension; shuffling chapters reduces performance even at shorter text windows

## Why This Works (Mechanism)

### Mechanism 1
Long-context understanding degrades non-linearly beyond 64k tokens despite advertised context windows. Models rely on localized pattern matching that scales poorly when semantic dependencies must span tens of thousands of tokens. They can retrieve information but fail to integrate information across long contexts for complex tasks.

### Mechanism 2
Sequential narrative structure aids long-context processing; disrupting order impairs comprehension. Models develop position-sensitive representations that leverage narrative coherence. When chapters are shuffled, models cannot reconstruct temporal/causal relationships effectively.

### Mechanism 3
Entity tracking (storyworld state) is more fragile than summarization across long contexts. Storyworld description requires maintaining explicit state representations that must be updated across the full text. This state-tracking requirement appears more brittle than extractive summarization.

## Foundational Learning

- **Needle-in-a-Haystack vs. Integration Tasks**: Why needed - TLDM critiques NIAH benchmarks for only testing retrieval, not comprehension. Quick check - Can you explain why retrieving a passkey from 100k tokens differs from summarizing a 100k-token narrative?

- **State Tracking in Sequence Models**: Why needed - Storyworld task tests whether models track entity state over time. Quick check - Given a 5-chapter story where a character moves between locations each chapter, can a transformer track the final location without architectural support?

- **Self-Consistency as Evaluation Proxy**: Why needed - TLDM uses chapter-level vs. novel-level prediction consistency as its metric. Quick check - Why might a model's chapter-level predictions be more reliable than its whole-novel predictions, even if neither matches human performance?

## Architecture Onboarding

- **Component map**: Novel selection -> token binning -> treatment application -> model inference -> chapter-level baseline generation -> full-text comparison -> similarity scoring

- **Critical path**: Novel selection → token binning → treatment application → model inference → chapter-level baseline generation → full-text comparison → similarity scoring

- **Design tradeoffs**: No human ground truth → relies on internal consistency (valid for degradation detection, weaker for absolute capability claims); English-only → limits multilingual conclusions; Chapter-based decomposition → assumes chapters are valid units of analysis

- **Failure signatures**: Sharp drop at 64k+ tokens indicates context integration failure; Storyworld near-zero at >128k indicates entity state tracking collapse; T5 (shuffled) performance drop indicates order-dependent processing; Open-weight steeper decline indicates scale-dependent long-context capability

- **First 3 experiments**:
  1. Replicate T2 (unaltered) on a single novel across all 7 models to validate reported degradation curves before expanding
  2. Test T4 (truncated window) with 25% vs 100% window on >128k texts to confirm the paper's finding that even truncated analysis degrades with longer source texts
  3. Add a "T6" hybrid treatment: pass chapter summaries (not full text) to isolate whether failures are due to input length vs. integration complexity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do LLMs utilize representational mechanisms for narrative state tracking that are similar to human cognitive processes?
- Basis in paper: The authors explicitly encourage using "mechanistic interpretability" to determine "whether their representational mechanisms are similar to those of humans."
- Why unresolved: The benchmark evaluates output stability rather than internal model states or cognitive alignment
- What evidence would resolve it: Interpretability studies mapping model attention heads or internal states to narrative entities and comparing them to human reading patterns

### Open Question 2
- Question: Can we identify specific model features that predict success or failure for distinct long-context tasks?
- Basis in paper: The conclusion states the need to "develop strategies for better predicting which long-context tasks LLMs are most appropriate for."
- Why unresolved: The study shows performance varies by task and model scale but does not isolate the architectural or training causes
- What evidence would resolve it: Ablation studies on context-extension methods and pre-training data mixtures to correlate specific features with task performance

### Open Question 3
- Question: How does LLM performance on the TLDM benchmark compare to human-level accuracy?
- Basis in paper: The "Limitations" section notes the lack of "validated human ground truth" prevents "robust assessments of model vs. human capabilities."
- Why unresolved: The benchmark relies on internal consistency (comparing long vs. short context) rather than an external human standard
- What evidence would resolve it: A human annotation phase to generate gold-standard summaries, storyworld states, and time estimates for the test novels

## Limitations
- Reliance on internal consistency metrics rather than human-annotated ground truth limits absolute capability claims
- English-only corpus and focus on narrative novels restricts generalizability to other domains or languages
- Treatment design assumes chapter boundaries represent natural processing units, which may not hold for all narrative structures

## Confidence

- **High Confidence**: Claims about relative performance degradation across token thresholds (particularly the 64k token inflection point) are well-supported by systematic treatment design and consistent patterns across seven models
- **Medium Confidence**: Assertions about architectural implications (e.g., why open-weight models show steeper declines) are plausible but not directly tested—the paper documents correlations without establishing causation
- **Medium Confidence**: The claim that sequential narrative structure specifically aids comprehension is supported but could reflect broader position-sensitive learning rather than narrative-specific mechanisms

## Next Checks

1. **Ground Truth Validation**: Run a subset of the benchmark (5 novels, 3 models) with human-annotated summaries and storyworld tracking to establish whether the internal consistency metric correlates with absolute performance, particularly around the 64k token threshold

2. **Cross-Domain Generalization**: Apply the same benchmark structure to non-narrative long-form text (e.g., technical documentation or academic papers) to determine whether observed degradation patterns are specific to narrative comprehension or reflect more general long-context limitations

3. **Architectural Intervention Test**: Test a model with explicit state-tracking mechanisms (such as recurrent memory or structured attention) on the storyworld task to determine whether architectural modifications can mitigate the observed entity-tracking failures beyond 128k tokens