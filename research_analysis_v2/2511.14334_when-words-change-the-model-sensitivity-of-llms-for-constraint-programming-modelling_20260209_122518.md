---
ver: rpa2
title: 'When Words Change the Model: Sensitivity of LLMs for Constraint Programming
  Modelling'
arxiv_id: '2511.14334'
source_url: https://arxiv.org/abs/2511.14334
tags:
- problem
- llms
- constraint
- each
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines how Large Language Models (LLMs) perform in
  automatically generating constraint programming (CP) models from natural language
  descriptions. To assess whether LLMs rely on memorization or genuine reasoning,
  the authors modified well-known CP problems from CSPlib by altering their context
  and introducing misleading elements while preserving their underlying structure.
---

# When Words Change the Model: Sensitivity of LLMs for Constraint Programming Modelling

## Quick Facts
- **arXiv ID:** 2511.14334
- **Source URL:** https://arxiv.org/abs/2511.14334
- **Reference count:** 6
- **Key outcome:** LLMs perform well on standard CP benchmarks but show significant sensitivity to linguistic variations, suggesting reliance on memorization rather than abstract reasoning

## Executive Summary
This study examines how Large Language Models (LLMs) perform in automatically generating constraint programming (CP) models from natural language descriptions. To assess whether LLMs rely on memorization or genuine reasoning, the authors modified well-known CP problems from CSPlib by altering their context and introducing misleading elements while preserving their underlying structure. Three representative LLMs (GPT-4, Claude 4, and DeepSeek-R1) were evaluated on both original and modified problem descriptions, with model quality assessed using correctness and executability metrics. The results show that all LLMs perform well on original descriptions but their performance drops significantly under contextual and linguistic variation, revealing shallow understanding and sensitivity to wording. Mathematical formulations in problem descriptions help anchor reasoning, while distracting information leads to incorrect interpretations.

## Method Summary
The study evaluated three LLMs (GPT-4, Claude 4, DeepSeek-R1) on 11 CSPLib problems, testing them under three conditions: original descriptions, context-modified, and context+distraction-modified. Models generated MiniZinc code with temperature=0, and outputs were assessed for correctness (whether the model represents the intended problem) and executability (whether it runs in MiniZinc without major manual fixes). The authors used a single attempt per model with no in-context examples, comparing performance across the different prompt conditions to measure sensitivity to linguistic variations.

## Key Results
- All LLMs showed high performance on original CSPlib descriptions but significant degradation under context modifications
- Mathematical formulations in prompts acted as "reasoning anchors," improving reliability across all models
- LLMs often over-relied on memorized patterns, producing syntactically valid but semantically incorrect models when prompts were altered
- The presence of specific keywords (like "maximize") could trigger incorrect problem interpretations even in satisfaction problems

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** High performance on standard benchmarks likely reflects data contamination (memorization) rather than abstract reasoning.
- **Mechanism:** When input prompts match the statistical distribution of training data (e.g., standard CSPLib descriptions), the model retrieves cached solution patterns. Performance degrades when surface features change because the retrieval pathway is disrupted, forcing the model to rely on weaker generalization capabilities.
- **Core assumption:** The evaluated LLMs have ingested standard benchmark datasets during pre-training.
- **Evidence anchors:**
  - [abstract] "much of this apparent success may derive from data contamination rather than genuine reasoning"
  - [section] "GPT-4 and Claude 4 may have prior knowledge of, respectively, four and five problems... DeepSeek-R1 appears to have even a higher likelihood of contamination"
  - [corpus] Related work in DCP-Bench-Open focuses on evaluation, but this paper specifically targets the validity of such evaluations via contamination checks.

### Mechanism 2
- **Claim:** LLMs treat specific lexical cues (like "maximize") or irrelevant context as "hard constraints," often overriding the problem's actual structural requirements.
- **Mechanism:** The attention mechanism overweights specific tokens (e.g., "maximise") or narrative flourishes (e.g., "misalignment"), causing the model to hallucinate constraints or shift the problem class (e.g., from satisfaction to optimization) based on semantic association rather than logical necessity.
- **Core assumption:** LLMs lack a robust internal "irrelevance filter" to distinguish between problem-defining logic and narrative noise.
- **Evidence anchors:**
  - [section] "This modification led the LLMs to incorrectly model the task as an optimisation problem... We conjecture here that the presence of the word 'maximize' triggered the LLMs"
  - [section] "Claude 4 and GPT-4 incorporated additional constraints to account for potential misalignment issues" in the Template Design problem despite them being non-essential.
  - [corpus] "Guided Perturbation Sensitivity" (corpus) discusses detecting adversarial text via word importance, supporting the view that specific tokens can disproportionately influence model behavior.

### Mechanism 3
- **Claim:** Explicit mathematical formulations in prompts act as "reasoning anchors," reducing sensitivity to linguistic noise.
- **Mechanism:** Symbolic notation (e.g., `a * b = c`) provides unambiguous tokens that constrain the generation space, preventing the "drift" caused by natural language ambiguity. This forces the model into a formal reasoning mode rather than associative text completion.
- **Core assumption:** The model has strong associations between mathematical symbols and formal logic operators.
- **Evidence anchors:**
  - [section] "LLMs seem particularly reliable when the problem description includes an explicit mathematical formulation... clearly stated formal structure strongly anchors the modelling process"
  - [section] "All-Interval Series... task was straightforward... likely because the problem description was highly detailed [and explicit]"
  - [corpus] ConstraintLLM (corpus) proposes neuro-symbolic frameworks, aligning with the finding that formal structure improves reliability.

## Foundational Learning

- **Concept: Constraint Programming (CP) Formulation**
  - **Why needed here:** To distinguish between a syntactically valid model and a semantically correct one. The paper evaluates correctness based on variables, domains, and constraints (e.g., Car Sequencing requires handling demand arrays, not global quantities).
  - **Quick check question:** If a prompt asks for "scheduling," but the model outputs a single variable for total demand instead of a sequence, which constraint category is likely violated (Domain or Relation)?

- **Concept: Data Contamination / Memorization**
  - **Why needed here:** To interpret benchmark results critically. The paper argues that high accuracy on standard benchmarks is a false positive for reasoning capability.
  - **Quick check question:** Why is "temperature=0" (determinism) critical when testing for memorization vs. reasoning?

- **Concept: Semantic vs. Syntactic Validity**
  - **Why needed here:** The paper notes models often produce "syntactically valid" code that compiles but fails the "correctness" metric due to wrong objectives or missing constraints.
  - **Quick check question:** A generated MiniZinc model runs without errors but seeks to maximize efficiency when the prompt asked for a feasible schedule. Is this a syntactic or semantic failure?

## Architecture Onboarding

- **Component map:** Natural Language Problem Description (Original vs. Context-Modified vs. Distractor) -> LLM (GPT-4, Claude 4, DeepSeek-R1) with Temperature=0 -> MiniZinc code -> MiniZinc compiler/runner (checks "Run" metric) + Human Inspector (checks "Correct" metric)

- **Critical path:** The prompt specification is the primary failure point. The transition from natural language to MiniZinc syntax is where "shallow understanding" manifests (e.g., misinterpreting "maximize" keywords).

- **Design tradeoffs:**
  - **Zero-shot (used in paper):** Fast, isolates reasoning capability, but highly brittle to wording.
  - **Iterative Refinement (proposed in paper):** Robustness increases by allowing the model to fix compiler errors, but evaluation becomes "contaminated" by the loop.
  - **Math-Anchored Prompts:** High reliability, but requires user expertise to formulate the math first.

- **Failure signatures:**
  - **Objective Drift:** The model hallucinates an optimization goal (e.g., "maximize") in a satisfaction problem due to a single keyword.
  - **Constraint Hallucination:** The model adds constraints not in the prompt (e.g., "balanced workload") because they are statistically associated with the context (e.g., "scheduling").
  - **Type Mismatch:** Converting arrays into scalars (e.g., Car Sequencing demand) when the context changes from familiar to novel.

- **First 3 experiments:**
  1. **Contamination Baseline:** Run the "completion task" (give half the prompt, check if the model completes it with the canonical CSPLib text) to estimate memorization levels for your specific model version.
  2. **Sensitivity Stress Test:** Take a working prompt and introduce a single "distractor" sentence or keyword (e.g., change "find a schedule" to "find a schedule to maximize throughput") to measure semantic fragility.
  3. **Symbolic Anchoring A/B Test:** Compare model performance on the same problem described in pure text vs. text + explicit mathematical notation to quantify the "anchor" effect.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can iterative refinement frameworks where LLMs receive feedback from constraint solvers significantly improve the correctness of generated models?
- **Basis in paper:** [explicit] The authors state interest in "developing interactive and in-context learning frameworks that allow LLMs to iteratively refine their formulations under controlled feedback while preserving interpretability."
- **Why unresolved:** Each model in this study was given only a single attempt with no opportunity for refinement or correction.
- **What evidence would resolve it:** Experiments comparing single-shot generation against multi-turn refinement loops where solver errors are fed back to the LLM for correction.

### Open Question 2
- **Question:** What prompt engineering strategies that embed formal mathematical structure within natural language most effectively improve LLM robustness for CP modelling?
- **Basis in paper:** [explicit] The authors note "Another promising line concerns prompt-engineering strategies that integrate formal structure within natural language, helping the models to capture abstract relations rather than surface patterns."
- **Why unresolved:** The study observed that explicit mathematical formulations anchor reasoning but did not systematically test different prompting strategies.
- **What evidence would resolve it:** Controlled experiments varying the level and format of mathematical notation in prompts, measuring impact on model correctness under distraction conditions.

### Open Question 3
- **Question:** How would hybrid LLM-solver pipelines perform where the solver verifies, corrects, or guides refinement of candidate models?
- **Basis in paper:** [explicit] The authors aim "to explore hybrid pipelines in which LLMs collaborate with domain-specific solvers, where the LLM proposes candidate models and the solver verifies, corrects, or guides refinement."
- **Why unresolved:** Current work evaluated standalone LLM output without solver-in-the-loop feedback mechanisms.
- **What evidence would resolve it:** Implementation of systems where solver diagnostics are automatically translated to natural language feedback for the LLM, compared against baseline single-shot performance.

## Limitations
- The study's findings are based on a relatively small sample of 11 problems from CSPLib, which may limit generalizability to the broader space of constraint programming tasks
- The evaluation methodology relies on human judgment for the "correctness" metric, introducing potential subjectivity despite the systematic approach
- The assessment focuses solely on MiniZinc output, potentially missing model quality variations that might emerge with other CP modeling languages

## Confidence
- **High Confidence:** The core finding that LLMs exhibit significant sensitivity to linguistic variations in problem descriptions is well-supported by the experimental results
- **Medium Confidence:** The claim about data contamination (memorization) is supported by evidence but could benefit from more systematic verification across different model versions and training timeframes
- **Low Confidence:** The specific mechanism by which attention mechanisms overweight certain tokens remains somewhat speculative, as the paper doesn't provide direct attention visualization or ablation studies

## Next Checks
1. **Cross-Problem Generalization Test:** Evaluate the same sensitivity patterns on a broader set of CP problems to determine whether the observed LLM brittleness is consistent across different problem structures and domains

2. **Attention Mechanism Analysis:** Conduct controlled experiments that systematically vary specific keywords while keeping other factors constant, combined with attention weight visualization to directly observe whether these tokens receive disproportionate weight

3. **Iterative Refinement Efficacy Study:** Compare the performance of a single-shot approach versus an iterative refinement process where models can correct compilation errors, to quantify the tradeoff between evaluation purity and practical robustness