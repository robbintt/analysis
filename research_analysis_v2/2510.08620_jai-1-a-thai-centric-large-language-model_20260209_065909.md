---
ver: rpa2
title: 'JAI-1: A Thai-Centric Large Language Model'
arxiv_id: '2510.08620'
source_url: https://arxiv.org/abs/2510.08620
tags:
- thai
- data
- performance
- language
- jai-1
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "JAI-1 is a Thai-centric large language model with 75B parameters,\
  \ developed through an upscaling strategy that expands a smaller English model\u2019\
  s parameter space to integrate Thai linguistic knowledge. Unlike prior Thai models\
  \ that fine-tune existing architectures and risk catastrophic forgetting, JAI-1\
  \ uses depth-up-scaling, MoE, and tokenizer adaptation to create a distinct architecture\
  \ optimized for Thai."
---

# JAI-1: A Thai-Centric Large Language Model

## Quick Facts
- arXiv ID: 2510.08620
- Source URL: https://arxiv.org/abs/2510.08620
- Reference count: 33
- Primary result: Thai-centric 75B LLM using upscaling approach outperforms Typhoon2-70B on Thai benchmarks and matches GPT-3.5 on general tasks

## Executive Summary
JAI-1 is a Thai-centric large language model with 75B parameters, developed through an upscaling strategy that expands a smaller English model's parameter space to integrate Thai linguistic knowledge. Unlike prior Thai models that fine-tune existing architectures and risk catastrophic forgetting, JAI-1 uses depth-up-scaling, MoE, and tokenizer adaptation to create a distinct architecture optimized for Thai. Pre-training on 1.5T tokens (300B Thai) and post-training on 600K examples yielded strong performance: JAI-1 outperformed Typhoon2-70B on Thai benchmarks (IFEval-TH, MT-Bench-TH, JAI-Hall-Bench) and achieved GPT-3.5-level performance on general knowledge tasks. The approach preserves baseline capabilities while enhancing Thai cultural and linguistic understanding, advancing representation for underrepresented languages.

## Method Summary
JAI-1 uses a three-phase upscaling approach: tokenizer adaptation (32K→64K vocabulary with embedding merging), depth-up-scaling (DUS.v2) expanding 40→64 layers with skip connections, and MoE with 4 experts initialized from training snapshots. Pre-training occurs in three phases: English recovery, Thai injection (27% Thai data), and context expansion (4K→32K). Post-training includes SFT with model merging and DPO alignment. The method avoids catastrophic forgetting by creating new parameter capacity rather than repurposing existing weights.

## Key Results
- Outperformed Typhoon2-70B on Thai benchmarks: IFEval-TH, MT-Bench-TH, JAI-Hall-Bench
- Achieved GPT-3.5-level performance on general knowledge tasks
- Demonstrated 71% reduction in tokens-per-character for Thai text (0.2913 vs 1.0311)
- Maintained English capabilities while integrating Thai knowledge through upscaling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Upscaling creates dedicated parameter capacity for Thai knowledge without overwriting existing capabilities.
- Mechanism: Depth-Up-Scaling (DUS.v2) expands the base model from 40 to 64 layers by duplicating blocks with skip connections, creating new parameter space rather than repurposing existing weights. This provides "fresh" capacity for Thai token embeddings and linguistic patterns.
- Core assumption: New parameters can absorb Thai knowledge without interfering with pre-trained English representations if the original information flow is preserved through skip connections.
- Evidence anchors:
  - [abstract]: "expanded its parameter space and utilized the newly allocated capacity to systematically integrate Thai-language knowledge"
  - [Section 3.1.2]: "DUS.v2 incorporates additional skip connections... This approach preserves the base model's inherent information flow within the duplicated blocks"
  - [corpus]: Limited direct evidence—related Thai LLM papers (OpenThaiGPT, Typhoon) do not evaluate upscaling vs. fine-tuning approaches comparatively

### Mechanism 2
- Claim: MoE with snapshot-based expert initialization reduces routing bias and enables language-specific specialization.
- Mechanism: Four experts are initialized from training snapshots at 0K, 2K, 4K, and 6K steps. This avoids the "one pretrained, three random" asymmetry that causes disproportionate expert selection, allowing more balanced routing for Thai vs. English tasks.
- Core assumption: Weights from different training stages represent meaningfully diverse functional capabilities that can specialize.
- Evidence anchors:
  - [Section 3.1.3]: "By leveraging learned weights from different training stages, we reduce routing bias and ensure balanced expert selection"
  - [Section 3.1.3]: "Internal evaluations confirmed that this modification significantly improves training stability"
  - [corpus]: No external validation—this snapshot initialization technique is not evaluated in related Thai LLM literature

### Mechanism 3
- Claim: Thai-optimized tokenization with semantic embedding initialization improves sample efficiency and representation quality.
- Mechanism: Vocabulary expands from 32K to 64K tokens using Thai BPE. New token embeddings are initialized by averaging constituent subword embeddings from the original tokenizer, grounding Thai tokens in existing semantic space.
- Core assumption: Semantic relationships between Thai tokens and existing subwords transfer meaningfully to new embeddings.
- Evidence anchors:
  - [Section 3.1.1]: "Each new token is decomposed into constituent subwords... The embeddings of these origin tokens are averaged to initialize the new token's embedding"
  - [Table 1]: JAI tokenizer achieves 0.2913 tokens-per-character vs. Phi-3's 1.0311 (71% reduction)
  - [corpus]: Consistent with OpenThaiGPT and Typhoon approaches using Thai-adapted tokenizers, but no comparative ablation isolating tokenizer impact

## Foundational Learning

- Concept: **Catastrophic Forgetting**
  - Why needed here: JAI-1's entire architecture is designed to mitigate this. Continual pre-training on Thai data can overwrite optimized English weights.
  - Quick check question: Can you explain why adding Thai data to a pre-trained English model might degrade English benchmark performance?

- Concept: **Mixture of Experts (MoE) Routing**
  - Why needed here: JAI-1 uses top-K=2 routing from M=4 experts. Understanding load balancing and expert collapse is essential for debugging.
  - Quick check question: What happens if the router always selects the same expert? How might snapshot initialization prevent this?

- Concept: **BPE Tokenization**
  - Why needed here: The tokenizer adaptation relies on Byte Pair Encoding. Understanding merge rules and vocabulary construction is prerequisite.
  - Quick check question: Why does a larger vocabulary size typically improve compression for underrepresented scripts?

## Architecture Onboarding

- Component map:
  Phi-3-Medium (14B, 40 layers) -> Tokenizer Adaptation: 32K → 64K vocab -> DUS.v2: Block duplication + skip connections → 64 layers -> MoE: 1 FFN → 4 experts per layer, top-2 routing -> Pre-training: 1.5T tokens, 3 phases -> SFT: 600K examples, recipe blending -> DPO alignment -> JAI-1 (75B)

- Critical path:
  1. Tokenizer training on Thai corpus → embedding merge initialization
  2. DUS.v2 block duplication with skip-connection scaling (α parameters)
  3. MoE expert initialization from training snapshots (0K, 2K, 4K, 6K)
  4. Three-phase pre-training (English recovery → Thai injection → context expansion)

- Design tradeoffs:
  - DUS.v2 vs. standard DUS: More flexible but introduces more disruption points; requires skip connections to compensate
  - Block duplication pattern: Middle blocks (4-7) duplicated 2-3x; input/output blocks kept at 1x to preserve stability
  - Thai data ratio: 27% Thai during Phase 2; higher ratios risk English degradation
  - Model merging: SFT.1 + SFT.2 merged 1:1; trades peak performance on single benchmarks for robustness

- Failure signatures:
  - Early training instability after DUS → likely skip-connection scaling issue
  - Router collapse to single expert → expert initialization too similar; add diversity
  - MMLU score stuck at ~62 after upscaling → English recovery phase data recipe insufficient
  - Thai fluency poor despite high Thai-H6 → SFT cultural data under-represented

- First 3 experiments:
  1. Tokenizer ablation: Compare JAI tokenizer vs. original Phi-3 tokenizer on Thai text compression and downstream Thai-H6. Measure tokens-per-character and training throughput.
  2. DUS.v2 skip-connection sensitivity: Vary α scaling factors (0.0, 0.3, 0.5, 0.7) and measure training loss stability in first 1K steps. Check for gradient explosion/vanishing.
  3. MoE expert initialization strategies: Compare snapshot-based vs. random vs. uniform initialization. Track expert selection distribution and routing entropy over training.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal expert initialization strategy for MoE layers when upscaling from a dense model, and how does the snapshot-based heuristic compare to alternatives?
- Basis in paper: [explicit] The paper states: "A key challenge in MoE initialization arises when one expert is initialized with pretrained FFN weights... while the remaining three are randomly initialized. This asymmetry can introduce routing bias... we employ a heuristic strategy" using snapshots from 2K step intervals.
- Why unresolved: The snapshot interval (2K steps) and number of snapshots (3) were chosen heuristically without systematic comparison to other initialization methods such as noise injection, knowledge distillation, or different checkpoint selection criteria.
- What evidence would resolve it: Ablation studies comparing routing balance and downstream performance across different expert initialization strategies, including varying snapshot intervals, using different training phases, or alternative initialization techniques.

### Open Question 2
- Question: How does the scaling factor α_n,(d) in DUS.v2 skip connections affect training dynamics and final model performance?
- Basis in paper: [explicit] The paper introduces skip connections with a scaling factor α_n,(d) in equation (1), stating it is "a scaling factor for the original information process," but does not report what values were used or how they were determined.
- Why unresolved: The paper confirms internal evaluations showed improved stability but provides no ablation on the scaling factor values, leaving unclear whether the factor is learned, fixed, or empirically tuned.
- What evidence would resolve it: Experiments varying α_n,(d) (e.g., fixed values like 0.1, 0.5, 1.0 vs. learned parameters) and measuring training loss curves, convergence speed, and benchmark performance.

### Open Question 3
- Question: Does the embedding averaging strategy for tokenizer adaptation fully preserve semantic relationships for Thai morphological variants and compound words?
- Basis in paper: [inferred] The paper states new token embeddings are initialized by averaging constituent subword embeddings, claiming this "preserves semantic consistency." However, Thai has complex morphological processes (compounding, reduplication) where simple averaging may not capture compositional semantics.
- Why unresolved: No evaluation specifically targets whether tokens initialized via averaging perform equivalently to those learned from scratch, nor whether semantic similarity is preserved for Thai-specific linguistic phenomena.
- What evidence would resolve it: Probing tasks measuring semantic similarity between new Thai tokens and their constituent subwords, or comparison against alternative initialization methods (e.g., contextualized embeddings, multilingual transfer) on Thai linguistic benchmarks.

## Limitations

- Data Composition Uncertainty: The 1.5T token pre-training corpus composition is incompletely specified, with unclear distribution across sources and languages
- Hyperparameter Opacity: Critical training parameters (skip-connection scaling factors, learning rates, batch sizes) are absent, preventing exact reproduction
- Evaluation Scope Limitations: Performance on non-Thai languages, complex reasoning tasks beyond MT-Bench, and long-form generation quality are not thoroughly examined

## Confidence

- High Confidence (8-10/10): The core architectural innovation of DUS.v2 with skip connections is well-described and the reported tokenizer efficiency improvement (0.2913 tokens-per-character vs. 1.0311) is verifiable through direct calculation
- Medium Confidence (5-7/10): The claim that upscaling prevents catastrophic forgetting relies on indirect evidence; comparative ablation studies isolating upscaling vs. fine-tuning effects are absent
- Low Confidence (1-4/10): The Thai cultural knowledge integration through SFT recipe blending is described qualitatively but lacks quantitative measures of cultural competency

## Next Checks

1. Ablation Study on Upscaling Strategy: Train two models from identical initial checkpoints—one using standard fine-tuning on Thai data, another using the JAI-1 upscaling approach. Compare English capability retention (MMLU, BBH) and Thai performance (Thai-H6, IFEval-TH) after equivalent training compute.

2. MoE Expert Diversity Analysis: Track expert utilization entropy and individual expert performance throughout training. Compute the KL divergence between expert selection distributions at initialization versus final converged states.

3. Tokenizer Efficiency Validation: Measure actual tokens-per-character on Thai Wikipedia and news articles using both JAI tokenizer and original Phi-3 tokenizer. Compute training throughput (tokens/second) and memory usage for equivalent batch sizes.