---
ver: rpa2
title: 'A Practitioner''s Guide to Building ASR Models for Low-Resource Languages:
  A Case Study on Scottish Gaelic'
arxiv_id: '2506.04915'
source_url: https://arxiv.org/abs/2506.04915
tags:
- data
- training
- speech
- gaelic
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares fine-tuning of multilingual end-to-end ASR
  models (such as Whisper) against hybrid HMM systems for low-resource languages,
  using Scottish Gaelic as a case study. It demonstrates that hybrid HMMs, when combined
  with self-supervised pre-training, subword units, and semi-supervised training,
  can outperform fine-tuned end-to-end models by 32% relative WER (WER 12.8% vs 19.0%)
  and improve upon the previous best model by 54%.
---

# A Practitioner's Guide to Building ASR Models for Low-Resource Languages: A Case Study on Scottish Gaelic

## Quick Facts
- arXiv ID: 2506.04915
- Source URL: https://arxiv.org/abs/2506.04915
- Reference count: 0
- Key outcome: Hybrid HMM systems with SSL features, BPE units, and semi-supervised training outperform fine-tuned end-to-end models by 32% relative WER (12.8% vs 19.0%) on Scottish Gaelic ASR.

## Executive Summary
This paper compares fine-tuning multilingual end-to-end ASR models (such as Whisper) against hybrid HMM systems for low-resource languages, using Scottish Gaelic as a case study. It demonstrates that hybrid HMMs, when combined with self-supervised pre-training, subword units, and semi-supervised training, can outperform fine-tuned end-to-end models by 32% relative WER and improve upon the previous best model by 54%. Key technical advances include using SSL features from XLS-R and XEUS, replacing graphemes with BPE acoustic units, integrating large RNN language models, and leveraging untranscribed data through noise augmentation and semi-supervised training. The results show that carefully optimized hybrid approaches remain highly effective for low-resource ASR, even in the presence of strong multilingual E2E models.

## Method Summary
The study fine-tunes Whisper-Turbo with LoRA and 8-bit quantization as an E2E baseline, then builds a hybrid system using self-supervised features from XLS-R/XEUS (layer 18), BPE acoustic units (1000 tokens), TDNN-F acoustic models trained with LF-MMI, and RNN-LM rescoring. The hybrid pipeline includes continued pre-training of SSL models on target audio, noise augmentation (3×), and semi-supervised training with pseudo-labels from untranscribed data. Text resources include 86M words of web-scale Gaelic text and 18M words of broadcast text. The final model achieves 12.8% WER versus 19.0% for the E2E baseline across four test sets.

## Key Results
- Hybrid HMM system achieves 12.8% WER versus 19.0% for fine-tuned Whisper (32% relative improvement)
- Replacing MFCC with SSL features reduces WER by 33.5%–37.7% relative (28.4% → 18.9% with XLS-R)
- Continued pre-training of XLS-R on Gaelic data improves WER from 18.9% to 17.7%
- BPE acoustic units achieve 6.7%–9.6% lower WER than graphemes, especially on code-switched utterances
- Semi-supervised training with 184h pseudo-labeled data and noise augmentation reduces WER from 13.7% to 12.8%

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Self-supervised multilingual features (XLS-R, XEUS) provide stronger acoustic representations than MFCCs for low-resource languages.
- **Mechanism**: SSL models encode universal phonetic patterns across languages; continued pre-training adapts these representations to target-language audio without requiring transcriptions.
- **Core assumption:** Target language shares acoustic patterns with pre-training languages; sufficient untranscribed audio exists for continued pre-training.
- **Evidence anchors:**
  - [section 4.3] Replacing MFCC with SSL features reduced average WER by 33.5%–37.7% relative (WER 28.4% → 18.9% with XLS-R; 18.2% with XEUS).
  - [section 5] Continued pre-training of XLS-R on Gaelic data ("XLS-R 300M CP") achieved 17.7% WER vs. 18.9% without adaptation.
  - [corpus] Neighbor papers (arXiv:2506.17459, arXiv:2602.01008) confirm cross-lingual transfer benefits for low-resource ASR but note effectiveness varies by phylogenetic distance and adaptation strategy.
- **Break condition:** If target language has acoustic properties highly divergent from SSL pre-training languages and <50 hours untranscribed audio, gains may diminish.

### Mechanism 2
- **Claim:** Hybrid HMM systems leverage text resources more effectively than E2E fine-tuning by decoupling acoustic and language modeling.
- **Mechanism**: LMs can be trained independently on web-scale text (MADLAD-400: 86M Gaelic words) and combined with acoustic models via lattice rescoring; this is harder in E2E architectures.
- **Core assumption:** Substantial text corpora exist for the target language; LM quality bounds semi-supervised training effectiveness.
- **Evidence anchors:**
  - [section 1] "Hybrid models can more easily utilize language models trained on all available text corpora... [including] web-scale text corpora."
  - [section 5] RNN-LM rescoring reduced WER from 16.0% to 15.0% (6.3% relative); largest LM (2048 hidden) performed best.
  - [corpus] Limited direct corpus evidence on hybrid vs. E2E LM integration; related work focuses on fine-tuning efficiency rather than LM decoupling.
- **Break condition:** If text corpora are extremely small (<1M words) or noisy, n-gram/RNN-LM gains may not materialize.

### Mechanism 3
- **Claim:** Semi-supervised training with pseudo-labeling complements SSL pre-training when untranscribed speech is available.
- **Mechanism**: Initial ASR generates pseudo-labels; noise augmentation increases robustness; combined with continued SSL pre-training on all audio, this expands effective training data.
- **Core assumption:** Seed model quality is sufficient to generate useful pseudo-labels; untranscribed audio matches target domain.
- **Evidence anchors:**
  - [section 4.6] 184 hours of pseudo-labeled news data added; noise augmentation applied; WER improved from 13.7% to 12.8%.
  - [section 5] "Continued pre-training on all data" + semi-supervised training yielded final 12.8% WER.
  - [corpus] arXiv:2512.07277 confirms cross-lingual unlabeled data improves low-resource ASR; arXiv:2501.14788 documents pseudo-labeling methods across languages.
- **Break condition:** If seed model WER >40%, pseudo-label noise may degrade rather than improve performance.

### Mechanism 4
- **Claim:** BPE acoustic units improve handling of code-switching compared to graphemes.
- **Mechanism**: BPE tokens are language-agnostic subword units that don't assume fixed grapheme-to-phoneme mappings; this helps when Gaelic-English code-switching occurs.
- **Core assumption:** Code-switching is present; BPE vocabulary size is tuned (too small → under-segmentation; too large → data sparsity).
- **Evidence anchors:**
  - [section 4.4] "Replacing grapheme units with contextual graphemes [BPE tokens]... models using BPE units achieve 6.7%–9.6% lower WER."
  - [section 5] Best BPE size was 1000–2000 tokens; 500 and 5000 underperformed.
  - [corpus] No direct corpus evidence on BPE acoustic units; this is a less commonly reported technique.
- **Break condition:** For monolingual data with consistent orthography, BPE gains may be smaller or negative.

## Foundational Learning

- **Concept:** Lattice-Free MMI (LF-MMI) training
  - **Why needed here:** All hybrid models use LF-MMI; understanding sequence-level objective helps debug training failures.
  - **Quick check question:** Can you explain why LF-MMI uses denominator graphs and how frame subsampling affects memory requirements?

- **Concept:** Self-supervised speech representations (wav2vec 2.0 / XLS-R)
  - **Why needed here:** Feature extraction from SSL models is central; layer selection (layer 18) and continued pre-training are key knobs.
  - **Quick check question:** What is the difference between frozen feature extraction and continued pre-training, and when would you choose each?

- **Concept:** Lattice rescoring with neural LMs
  - **Why needed here:** Final WER gains come from RNN-LM rescoring; understanding n-best lists vs. lattices is practical.
  - **Quick check question:** Why does rescoring a lattice (vs. re-decoding) preserve more hypotheses while remaining efficient?

## Architecture Onboarding

- **Component map:**
  ```
  Audio → SSL Feature Extractor (XLS-R/XEUS, layer 18, 50fps)
        → TDNN-F Acoustic Model (trained with LF-MMI on BPE units)
        → First-pass decoding with n-gram LM (4-gram BPE)
        → Lattice rescoring with RNN-LM (2048 hidden)
        → Final transcript
  ```
  Parallel path: Untranscribed audio → Continued SSL pre-training → Updated feature extractor

- **Critical path:**
  1. Normalize transcripts (fix encoding, map non-Gaelic to noise token)
  2. Train BPE tokenizer (1000–2000 tokens) on transcripts
  3. Extract SSL features (requires GPU, ~2 days for continued pre-training on 8×12GB GPUs)
  4. Train TDNN-F AM with LF-MMI
  5. Train n-gram LM on web text; train RNN-LM on all text
  6. Decode + rescore; iterate with semi-supervised data

- **Design tradeoffs:**
  - **SSL vs. MFCC:** SSL requires GPU and more engineering; MFCC is faster but underperforms by 30%+ relative.
  - **BPE size:** 1000–2000 optimal here; smaller fragments words excessively, larger increases sparsity.
  - **Whisper fine-tuning vs. hybrid:** Whisper is easier to start (HuggingFace) but 32% worse WER in this study; hybrid requires Kaldi expertise.

- **Failure signatures:**
  - High deletion errors on children's speech with background music (17.7% WER on BBC test set vs. 10.4%–12.0% on others)
  - Code-switching errors when using graphemes instead of BPE
  - LM adaptation degraded performance when interpolating with pseudo-label LM (Section 5: "adapted LM significantly degraded accuracy")

- **First 3 experiments:**
  1. Fine-tune Whisper-Turbo with LoRA (rank 256–512) on 288h transcribed data — establishes E2E baseline (expect ~19–22% WER).
  2. Train hybrid TDNN-F with MFCC + graphemes + word n-gram LM — establishes hybrid baseline (expect ~28–30% WER).
  3. Replace MFCC with frozen XLS-R features (layer 18) — should drop WER to ~18–19%; if not, check feature extraction pipeline and frame subsampling.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can fine-tuning Large Language Models (LLMs) on Gaelic text outperform the current RNN-LM rescoring approach, or can they be effectively utilized for generative error correction?
- Basis in paper: [explicit] The authors state in the conclusion: "we will attempt to fine-tune large language models on Gaelic... Such a model could be used for rescoring, generative error correction [39] or directly for speech recognition."
- Why unresolved: The current best system relies on RNN-LMs, but the authors hypothesize that LLMs could offer superior performance, a hypothesis not yet tested in the study.
- What evidence would resolve it: A comparison of WER after n-best list rescoring or generative error correction using a fine-tuned Llama-style model versus the current RNN-LM baseline.

### Open Question 2
- Question: Can unsupervised language model adaptation methods be modified to improve rather than degrade accuracy in low-resource settings where speech data outnumbers text data?
- Basis in paper: [explicit] The authors note that "languages like Gaelic might have significantly more speech data available than text data," but their initial experiment "interpolated our web n-gram LM with an n-gram LM trained on automatic transcripts... [which] significantly degraded accuracy, therefore we left this for future experiments."
- Why unresolved: The potential for unsupervised adaptation is high, but the specific implementation tested (n-gram interpolation) failed due to noise or distribution mismatch.
- What evidence would resolve it: Successful application of an unsupervised adaptation technique (e.g., using neural LLMs or filtering confidence thresholds) that yields a statistically significant WER reduction over the static web-based LM.

### Open Question 3
- Question: Would continued self-supervised pre-training of the XEUS model on target language data provide superior acoustic features compared to the continued pre-training of XLS-R used in the final system?
- Basis in paper: [explicit] The authors found XEUS to be a better base model than XLS-R initially, but noted: "Whilst we believe that XEUS would also benefit from continued pre-training, unfortunately, code for this has not been integrated into ESPnet yet."
- Why unresolved: It is unknown if the performance gap between XEUS and XLS-R would persist, widen, or close if both were subjected to the same continued pre-training regime on Gaelic data.
- What evidence would resolve it: A controlled experiment comparing WERs of hybrid models using features from XLS-R-CP versus XEUS-CP (XEUS after continued pre-training on the same Gaelic corpus).

### Open Question 4
- Question: Can the optimized hybrid model's pseudo-labels effectively be used to train an end-to-end (E2E) model that surpasses the hybrid system's own performance?
- Basis in paper: [explicit] The authors plan to "transcribe large quantities of untranscribed data with our best hybrid model" to create training data for E2E models, citing prior work [20].
- Why unresolved: While the hybrid model is currently superior, it remains untested whether the E2E architecture can leverage the massive amount of pseudo-labelled data to overcome the hybrid model's performance ceiling.
- What evidence would resolve it: Benchmarking a Whisper-style model trained from scratch or fine-tuned on the hybrid-generated pseudo-labels against the teacher hybrid model on the same test sets.

## Limitations

- Data access is a primary limitation as CLTW and BBC Alba datasets require institutional licensing and are not publicly available
- The paper lacks complete implementation details for critical components including the exact Gaelic normalization script, noise augmentation parameters, and complete Kaldi recipe hyperparameters
- The 32% relative WER improvement over Whisper is based on a single low-resource language case study, limiting generalization claims

## Confidence

- **High confidence**: SSL feature effectiveness (37.7% WER reduction confirmed with XLS-R), hybrid LM decoupling advantage (6.3% WER drop from RNN-LM), and semi-supervised training gains (12.8% final WER) are well-supported by ablation studies
- **Medium confidence**: BPE acoustic units' 6.7–9.6% WER improvement is less established, with limited corpus evidence for this specific application
- **Low confidence**: Whisper fine-tuning configuration (LoRA rank, quantization) and exact noise augmentation parameters may not reproduce the stated 19.0% WER without precise tuning

## Next Checks

1. Apply the same hybrid pipeline to a different low-resource language (e.g., Basque or Welsh) with available audio/text data to test generalization of the 32% WER advantage over Whisper

2. Quantify the WER of the seed model used for semi-supervised training and measure how pseudo-label accuracy degrades across iterations to establish break points

3. Systematically test BPE acoustic units (500, 1000, 2000, 5000) on a held-out code-switched subset to determine optimal vocabulary size and quantify the impact on code-switching errors