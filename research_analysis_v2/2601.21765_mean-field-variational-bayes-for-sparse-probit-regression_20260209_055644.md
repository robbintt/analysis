---
ver: rpa2
title: Mean-field Variational Bayes for Sparse Probit Regression
arxiv_id: '2601.21765'
source_url: https://arxiv.org/abs/2601.21765
tags:
- variational
- logp
- mcmc
- variables
- mfvb
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a mean-field variational Bayes (MFVB) approach
  for sparse probit regression with spike-and-slab priors, addressing computational
  challenges in high-dimensional binary classification. The authors develop an efficient
  coordinate ascent variational inference algorithm with closed-form updates for all
  variational factors and evidence lower bound, enabling fast posterior inference
  for variable selection and prediction.
---

# Mean-field Variational Bayes for Sparse Probit Regression

## Quick Facts
- arXiv ID: 2601.21765
- Source URL: https://arxiv.org/abs/2601.21765
- Reference count: 32
- This paper proposes a mean-field variational Bayes approach for sparse probit regression with spike-and-slab priors, achieving 2-3 orders of magnitude speedup over MCMC while maintaining comparable accuracy.

## Executive Summary
This paper develops a mean-field variational Bayes (MFVB) approach for Bayesian variable selection in high-dimensional binary classification using probit regression with spike-and-slab priors. The method employs latent variable augmentation and coordinate ascent variational inference to achieve closed-form updates for all variational factors. In extensive simulations, MFVB demonstrates computational advantages of 2-3 orders of magnitude over MCMC while maintaining comparable predictive accuracy. In high-dimensional settings, MFVB produces more parsimonious solutions by favoring variables with extreme inclusion probabilities (0 or 1), while MCMC tends to be more conservative.

## Method Summary
The method frames binary outcomes through latent Gaussian variables (Albert & Chib augmentation), enabling closed-form conditional updates. A mean-field factorization approximates the posterior as independent factors over coefficients, latent variables, and inclusion indicators. The algorithm uses coordinate ascent variational inference with closed-form updates: multivariate Gaussian for coefficients, truncated Gaussian for latent utilities, and Bernoulli for inclusion indicators. The spike-and-slab prior is implemented via binary masks on coefficients. Hyperparameters include prior variance ν² and inclusion probability ρ, with ρ selected via 5-fold cross-validation.

## Key Results
- MFVB achieves 2-3 orders of magnitude speedup over MCMC (e.g., 55s vs 81,000s)
- In high-dimensional settings (p > n), MFVB produces more parsimonious solutions with PIPs tending to 0 or 1
- Out-of-sample deviance performance comparable to MCMC while selecting fewer variables
- True positive/negative rates above 95% in low-dimensional simulations

## Why This Works (Mechanism)

### Mechanism 1: Latent Variable Augmentation for Conjugacy
The Albert & Chib (1993) data augmentation strategy introduces latent variables z_i ~ N(x_i^TΓβ, 1) truncated by binary outcomes. This transforms the non-conjugate binary likelihood into a conjugate Gaussian likelihood, enabling tractable Gaussian updates for β. The probit link function correctly relates the linear predictor to binary outcomes.

### Mechanism 2: Mean-Field Factorization for Scalability
The joint posterior p(β, z, γ | y) is approximated by q(β)q(z)∏q(γ_j), decoupling optimization of p+1 parameter blocks. This enables linear scaling with dimension p through coordinate ascent variational inference, cycling through updates using expected values of other parameters.

### Mechanism 3: Binary Masking for Variable Selection
The spike-and-slab prior uses binary masks: β̃ = Γβ. The Bernoulli variational factor for inclusion indicator γ_j produces parsimonious solutions by pushing inclusion probabilities toward 0 or 1, explicitly selecting or dropping variables.

## Foundational Learning

- **Evidence Lower Bound (ELBO)**
  - Why needed: MFVB optimizes ELBO rather than sampling posterior; essential for convergence monitoring
  - Quick check: Why does maximizing ELBO minimize KL divergence between variational approximation q and true posterior?

- **Truncated Gaussian Distribution**
  - Why needed: Core innovation is updating latent variables z, which are truncated by binary outcomes
  - Quick check: If y_i = 1, what constraint is placed on z_i, and how does the Inverse Mills Ratio affect its mean?

- **Coordinate Ascent**
  - Why needed: Algorithm iteratively updates parameters; understanding single-block optimization explains convergence behavior
  - Quick check: In Algorithm 1, why must we recalculate z̄ before updating γ inclusion probabilities?

## Architecture Onboarding

- **Component map:** X, y → Algorithm 1 (CAVI) → Variational factors q(β), q(z), q(γ) → ELBO maximization

- **Critical path:**
  1. Initialize w (inclusion probs) and μ (coefficients)
  2. E-step (z): Update z̄ using current μ, w via Inverse Mills Ratio
  3. M-step (beta): Update μ, Σ using z̄ and w
  4. M-step (gamma): Update w using μ, Σ
  5. Check Convergence: Evaluate ELBO change

- **Design tradeoffs:**
  - Speed vs. Uncertainty: Extremely fast (closed-form) but often underestimates variance vs. MCMC
  - Parsimony vs. Sensitivity: Favors extreme probabilities (0 or 1), yielding cleaner interpretation but potentially overly aggressive

- **Failure signatures:**
  - Local Minima: May converge to sub-optimal variable subsets depending on initialization
  - Underestimation of Variance: Covariance matrix Σ typically underestimates posterior uncertainty

- **First 3 experiments:**
  1. Validate vs. MCMC (Low Dim): Replicate p=200, n=1000 simulation to verify 100x speedup claim
  2. Stress Test (High Dim): Run p=1000, n=500 with correlated predictors to test variable selection trade-offs
  3. Initialization Sensitivity: Randomize starting values for w on LSVT dataset to check solution stability

## Open Questions the Paper Calls Out

### Open Question 1
Can MFVB extend to mixed-effects probit regression while preserving closed-form updates and scalability? The current derivation relies on specific hierarchical structure; incorporating random effects may break mean-field factorization's tractability.

### Open Question 2
Do more structured variational approximations (accounting for posterior correlations between β and γ) improve variable selection accuracy in high-dimensional settings? Mean-field ignores posterior correlations; whether capturing these improves performance remains unknown.

### Open Question 3
Can theoretical convergence of MFVB posterior inclusion probabilities to true indicator values be proven for probit case? Empirical degeneracy is observed but not theoretically guaranteed due to probit link's non-linear structure.

### Open Question 4
In p > n settings, does MFVB's parsimonious tendency yield better or worse out-of-sample generalization compared to MCMC's conservative inclusion? The optimal balance depends on application-specific costs of false positives versus false negatives.

## Limitations
- Mean-field assumption may underestimate posterior uncertainty compared to MCMC
- Spike-and-slab prior with binary inclusion indicators may be overly aggressive in high-dimensional settings
- Computational advantages come at the cost of potentially missing weak but relevant predictors

## Confidence
- **High confidence**: Speed improvements over MCMC (2-3 orders of magnitude), closed-form update derivations, simulation setup and results
- **Medium confidence**: Variable selection performance in high-dimensional settings, real data application conclusions
- **Low confidence**: Uncertainty quantification accuracy relative to MCMC, behavior with correlated predictors

## Next Checks
1. **Initialization sensitivity analysis**: Systematically vary initial inclusion probabilities (w) on the LSVT dataset to determine if the algorithm consistently converges to the same sparse variable subset.
2. **False negative assessment**: In the high-dimensional simulation (p=1000, n=500), compare the True Positive Rate of MFVB versus MCMC to quantify how many relevant variables are incorrectly dropped.
3. **Convergence criterion validation**: Implement the algorithm with different ELBO convergence thresholds (e.g., 1e-4, 1e-6, 1e-8) to determine the impact on solution stability and computational time.