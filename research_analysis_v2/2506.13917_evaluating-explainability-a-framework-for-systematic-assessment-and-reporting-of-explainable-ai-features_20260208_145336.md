---
ver: rpa2
title: 'Evaluating Explainability: A Framework for Systematic Assessment and Reporting
  of Explainable AI Features'
arxiv_id: '2506.13917'
source_url: https://arxiv.org/abs/2506.13917
tags:
- explanation
- explainability
- explanations
- evaluation
- fidelity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a framework for systematically assessing
  and reporting explainable AI (XAI) features in medical devices. The authors define
  four evaluation criteria: Consistency (stability of explanations to similar inputs),
  Plausibility (alignment with ground truth), Fidelity (alignment with model internal
  mechanisms), and Usefulness (impact on task performance).'
---

# Evaluating Explainability: A Framework for Systematic Assessment and Reporting of Explainable AI Features

## Quick Facts
- arXiv ID: 2506.13917
- Source URL: https://arxiv.org/abs/2506.13917
- Reference count: 30
- Primary result: Introduces a 4-criteria framework (Consistency, Plausibility, Fidelity, Usefulness) for evaluating XAI methods in medical devices

## Executive Summary
This paper introduces a systematic framework for evaluating explainable AI (XAI) features in medical devices. The framework defines four key criteria: Consistency (stability under input perturbations), Plausibility (alignment with ground truth), Fidelity (alignment with model internal mechanisms), and Usefulness (impact on task performance). The authors apply this framework to evaluate heatmaps from Eigen CAM and Ablation CAM on synthetic mammography data for breast lesion detection. Results demonstrate that Ablation CAM achieves higher plausibility and fidelity than Eigen CAM, while both methods maintain model accuracy under perturbations. The framework provides a structured approach to XAI evaluation, moving beyond anecdotal evidence to quantitative metrics.

## Method Summary
The framework evaluates XAI methods through four sequential criteria. First, Consistency is assessed by measuring explanation stability under input perturbations (noise, rotation, dose variation) using SSIM, MSE, and IoU metrics. Second, Plausibility is evaluated by comparing explanations to ground truth annotations using IoU between heatmap activation regions and lesion bounding boxes. Third, Fidelity is tested by perturbing model parameters (weight randomization, ROI masking) and measuring explanation changes using SSIM/MSE/IoU and model accuracy impacts. Finally, Usefulness is determined through human studies examining explanation impact on task performance. The evaluation is demonstrated on Eigen CAM and Ablation CAM applied to synthetic mammography data with Faster R-CNN for breast lesion detection.

## Key Results
- Ablation CAM demonstrates higher plausibility (better lesion localization) than Eigen CAM on synthetic mammography data
- Ablation CAM maintains model accuracy under perturbations while Eigen CAM shows variability in explanation stability
- The framework establishes a structured approach to evaluate XAI methods, moving beyond anecdotal evidence to quantitative metrics
- Sequential evaluation (Consistency → Plausibility → Fidelity → Usefulness) effectively filters poor explanations before costly human studies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sequential evaluation of XAI methods (Consistency → Plausibility → Fidelity → Usefulness) filters poor explanations before costly human studies.
- Mechanism: Consistency ensures stable explanations under input perturbations; plausibility aligns explanations with ground truth; fidelity confirms explanations reflect actual model reasoning. Only methods passing all three proceed to human-based usefulness evaluation.
- Core assumption: Assumption: Poor performance on early criteria predicts low usefulness in human studies.
- Evidence anchors:
  - [abstract] "recommend evaluating consistency, plausibility, and fidelity before human-based usefulness studies to optimize resource use"
  - [section 3.3] "The order of evaluation is important. Consistency must be evaluated first... Once an explainability method passes Consistency, Plausibility, and Fidelity, it is ready to move into the second stage for human-based evaluation"
  - [section 4] "Requiring a method to pass consistency, plausibility, and fidelity before involving human experts, filters out poor or misleading explanations"
- Break condition: If early criteria show high variability or misalignment, stop—human evaluation will not rescue a fundamentally flawed explanation method.

### Mechanism 2
- Claim: Fidelity and model accuracy are independent; an explanation can have high fidelity to a model's flawed reasoning even when that reasoning is incorrect.
- Mechanism: Fidelity is measured by perturbing model parameters (e.g., randomizing weights) and observing whether explanations change accordingly. If explanations remain unchanged after randomization, they do not reflect learned parameters. Accuracy measures task performance separately.
- Core assumption: Assumption: Parameter randomization disrupts learned patterns uniformly across relevant features.
- Evidence anchors:
  - [section 3.2.3] "model's accuracy and fidelity are related but distinct concepts. Fidelity refers to how well the explanation aligns with the internal workings of the model, while accuracy measures how well the model performs at its task"
  - [section 3.2.3, Figure 4 discussion] "both explanations were disrupted by perturbations, but only Ablation CAM resulted in a more consistent disruption and a change in the model's decision making"
  - [corpus] Weak direct corpus support for this specific fidelity-accuracy distinction; related work (Nauta et al. 2022, cited in paper) surveys fidelity metrics but does not test this mechanism directly.
- Break condition: If explanation does not change when model parameters are randomized, fidelity is low—regardless of accuracy.

### Mechanism 3
- Claim: Plausibility and fidelity can diverge; an explanation may align with ground truth (high plausibility) but not reflect model internals (low fidelity), or vice versa.
- Mechanism: Plausibility compares explanations to external ground truth (e.g., radiologist annotations). Fidelity compares explanations to model behavior under perturbation. These measure different alignments.
- Core assumption: Assumption: Ground truth annotations correctly capture relevant features for the task.
- Evidence anchors:
  - [section 3.2.3] "Plausibility focuses on measuring the alignment between the explanation and domain knowledge or ground truth, while fidelity focuses on measuring the alignment of the explanation with respect to the predictive model"
  - [section 3.2.3] "an explanation highlighting patient information in the background... would score low on plausibility with respect to tumor segmentation masks but could still correctly reflect the reasoning of this flawed classifier (high fidelity)"
  - [corpus] Corpus papers on XAI evaluation (e.g., "Unifying VXAI") acknowledge plausibility-fidelity tension but do not provide experimental validation of divergence rates.
- Break condition: If plausibility and fidelity metrics are conflated, evaluation will miss cases where the model is right for wrong reasons or wrong for right reasons.

## Foundational Learning

- Concept: **Perturbation testing for explanations**
  - Why needed here: Consistency and fidelity both rely on measuring how explanations change under controlled input or model perturbations.
  - Quick check question: If you rotate an input image 10 degrees, should a good heatmap change significantly? Why or why not?

- Concept: **Ground truth hierarchy in medical imaging**
  - Why needed here: Plausibility requires comparing explanations to annotations, but ground truth can be direct (lesion location) or indirect (surrounding anatomical context).
  - Quick check question: What types of ground truth would you need to evaluate a breast lesion detection heatmap beyond just lesion boundaries?

- Concept: **Post-hoc vs. intrinsic explainability**
  - Why needed here: The framework evaluates post-hoc methods (Eigen CAM, Ablation CAM) applied after model training; intrinsic methods embed explainability in architecture.
  - Quick check question: Why might a post-hoc explanation have lower fidelity than an intrinsic one?

## Architecture Onboarding

- Component map: Descriptive section -> Quantitative section -> Scorecard
- Critical path:
  1. Define context of use (task, audience, model)
  2. Run consistency tests under perturbations (noise, rotation, dose variation)
  3. Measure plausibility against ground truth annotations (IoU with lesion masks)
  4. Test fidelity via parameter randomization and ROI masking
  5. Only if all pass: design human usefulness study
- Design tradeoffs:
  - Eigen CAM provides global feature representation but lower localization accuracy (lower plausibility)
  - Ablation CAM provides localized explanations with higher fidelity but requires more computation per explanation
  - Synthetic data (M-SYNTH) enables perfect ground truth but may not generalize to real clinical images
- Failure signatures:
  - High consistency + low plausibility: Explanation is stable but highlights wrong regions
  - High plausibility + low fidelity: Explanation matches ground truth but not model reasoning (model may be right for wrong reasons)
  - Unchanged explanation after parameter randomization: Low fidelity—explanation independent of learned weights
  - Model accuracy unchanged after masking high-activation ROI: Explanation may not identify truly important features
- First 3 experiments:
  1. Run consistency evaluation on your XAI method with 3 perturbation types (noise, rotation, dose/image quality variation); compute SSIM, MSE, IoU between original and perturbed explanations.
  2. Measure plausibility using IoU between explanation heatmap peak activation region and ground truth lesion bounding boxes on a labeled validation set.
  3. Test fidelity by randomizing the final convolutional layer and comparing explanation similarity (SSIM) before/after; also mask the highest-activation ROI and check if model prediction changes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific acceptability thresholds are required for consistency, plausibility, and fidelity to ensure clinical safety?
- Basis in paper: [explicit] The authors state the framework "does not provide a threshold or any acceptability criteria" but note these must be defined for different applications.
- Why unresolved: Appropriate values likely vary significantly across clinical tasks (e.g., screening vs. diagnosis) and cannot be generalized.
- What evidence would resolve it: Domain-specific benchmarking studies that correlate specific metric scores with patient outcomes or expert consensus.

### Open Question 2
- Question: Does high performance in consistency, plausibility, and fidelity reliably predict the "Usefulness" of an explanation in clinical practice?
- Basis in paper: [inferred] The paper proposes evaluating the first three criteria before "Usefulness" to filter poor methods, but the case study did not validate this link with human subjects.
- Why unresolved: Quantitative alignment with model internals (fidelity) or ground truth (plausibility) does not guarantee the explanation reduces cognitive load or aids decision-making.
- What evidence would resolve it: Human reader studies comparing task performance using explanations that passed vs. failed the proposed quantitative metrics.

### Open Question 3
- Question: How can the plausibility criterion be adapted to account for "indirect" ground truths, such as contextual anatomical cues, rather than just direct lesion overlap?
- Basis in paper: [explicit] The text notes that evaluating plausibility "may require incorporating different levels of ground truth," such as eye-tracking data of radiologists viewing surrounding structures.
- Why unresolved: Standard metrics like Intersection over Union (IoU) focus strictly on overlap and may penalize valid reasoning that includes contextual information.
- What evidence would resolve it: Development of weighted evaluation datasets that include both lesion segmentation masks and radiologist gaze patterns.

## Limitations
- Synthetic data (M-SYNTH) may not reflect real clinical variability in image quality and anatomical diversity
- Framework tested on only two specific post-hoc methods (Eigen CAM, Ablation CAM) without comparing to intrinsic explainability approaches
- Confidence in sequential evaluation mechanism is Medium—empirical validation across diverse tasks and XAI methods remains limited

## Confidence
- Framework structure and criteria definitions: **High**
- Sequential evaluation filtering hypothesis: **Medium**
- Fidelity-accuracy independence: **Medium**
- Plausibility-fidelity divergence rates: **Low**

## Next Checks
1. Apply the framework to real clinical mammography data with imperfect ground truth to assess robustness to ground truth variability
2. Compare Ablation CAM and Eigen CAM performance on additional XAI methods (e.g., Grad-CAM, Integrated Gradients) across different medical imaging tasks
3. Quantify computational overhead of each evaluation criterion and test the sequential filtering hypothesis by running usefulness studies on methods that fail early criteria