---
ver: rpa2
title: Calibrated and uncertain? Evaluating uncertainty estimates in binary classification
  models
arxiv_id: '2508.11460'
source_url: https://arxiv.org/abs/2508.11460
tags:
- data
- learning
- uncertainty
- probability
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates uncertainty estimation methods for binary
  classification models. It compares six algorithms: neural network ensemble, neural
  network ensemble with conflictual loss, evidential deep learning, Monte Carlo Dropout,
  Gaussian process classification, and Dirichlet process mixture model.'
---

# Calibrated and uncertain? Evaluating uncertainty estimates in binary classification models

## Quick Facts
- **arXiv ID**: 2508.11460
- **Source URL**: https://arxiv.org/abs/2508.11460
- **Reference count**: 26
- **Primary result**: Deep learning uncertainty estimation methods fail to provide useful uncertainty estimates for out-of-distribution data, while Gaussian Process and Dirichlet Process Mixture Models maintain increasing uncertainty in such regions.

## Executive Summary
This paper evaluates six uncertainty estimation algorithms for binary classification: neural network ensemble, neural network ensemble with conflictual loss, evidential deep learning, Monte Carlo Dropout, Gaussian process classification, and Dirichlet process mixture model. The study uses synthetic 2D data with known generating functions to test calibration, data-size dependency, and out-of-distribution behavior. Results show all algorithms are well-calibrated for in-distribution data, but deep learning models fail to provide useful uncertainty estimates for out-of-distribution data, tending to produce extreme probability estimates with near-zero uncertainty. In contrast, Gaussian process and Dirichlet process mixture models show increasing uncertainty for out-of-distribution data. The paper concludes that while deep learning methods excel at classification accuracy, their uncertainty estimates are unreliable for scientific applications requiring uncertainty quantification.

## Method Summary
The study evaluates six uncertainty estimation algorithms on synthetic 2D binary classification data. Dataset A uses Gamma distributions with parameters (2,5) and (6,3) for the two classes, while Dataset B uses (2,3) and (4,3). The neural network architecture consists of 3 hidden layers with 200 nodes each using ReLU activation. Models are trained using AdamW optimization with specific learning rates and weight decay parameters. Evaluation metrics include calibration (Wasserstein-1 distance, Expected Calibration Error, LogLoss), data-size dependency (average uncertainty vs training set size), and out-of-distribution behavior (average uncertainty and probability estimates on extrapolation grids).

## Key Results
- All six algorithms achieve good calibration on in-distribution test data, with calibration error below 0.1.
- Deep learning models (NNE, CL, EDL, MCD) produce extreme probability estimates (near 0 or 1) with near-zero uncertainty for out-of-distribution data.
- Gaussian process and Dirichlet process mixture models maintain increasing uncertainty for out-of-distribution data points.
- Evidential deep learning shows anomalous behavior with increasing uncertainty as training data size grows.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Deep learning classifiers utilizing ReLU activations and softmax output layers likely extrapolate to extreme probabilities with near-zero uncertainty when far from training data.
- **Mechanism**: ReLU networks become piecewise linear functions. As one moves infinitely far from the training data in feature space, the network outputs approach linear functions of the input. When normalized via softmax, these linear outputs saturate, pushing class probabilities toward 0 or 1. Since uncertainty is often mathematically coupled to probability estimates (e.g., proportional to $\sqrt{\nu(1-\nu)}$), it collapses to zero.
- **Core assumption**: The failure is an architectural artifact of the function class (neural networks with ReLU) rather than an optimization failure.
- **Evidence anchors**: The paper notes that neural networks with ReLU activation functions eventually become simple linear functions, combined with softmax giving rise to class probabilities that asymptotically approach extreme values.
- **Break condition**: If input features are bounded or if the data manifold is topologically closed such that "infinite" extrapolation is impossible, this asymptotic failure may not trigger.

### Mechanism 2
- **Claim**: Non-parametric Bayesian models (Gaussian Processes, Dirichlet Process Mixture Models) revert to a defined prior state in low-density regions, maintaining high uncertainty for out-of-distribution inputs.
- **Mechanism**: These models explicitly model the joint probability $p(c, x)$ or utilize kernel functions that decay with distance. When a test point is distant from training data, the likelihood term diminishes, leaving the posterior dominated by the prior (e.g., Beta(1,1) implying 0.5 probability with high variance).
- **Core assumption**: The kernel function or mixture model structure correctly identifies "distance" or "low density" in the high-dimensional feature space.
- **Evidence anchors**: The paper shows that Gaussian process produces probability estimates of approximately 0.5 and high uncertainty for OOD data points, and Dirichlet process mixture model also produces high uncertainty for OOD data.
- **Break condition**: If the model is misspecified or if numerical errors occur in density estimation, the uncertainty signal may degrade or become unstable.

### Mechanism 3
- **Claim**: In-distribution calibration is statistically decoupled from OOD uncertainty robustness; optimizing for the former does not guarantee the latter.
- **Mechanism**: Calibration metrics measure the alignment between predicted probability and observed frequency within the distribution of the test set. Conversely, OOD robustness requires correct behavior in unobserved regions. A model can strictly fit the probability density of the training domain while possessing arbitrary function behavior outside that domain.
- **Core assumption**: The "Long-Run Frequency Distribution" used for calibration checks is only valid where data exists.
- **Evidence anchors**: The paper demonstrates that all algorithms are well-calibrated for in-distribution data, but deep learning models fail to provide useful uncertainty estimates for out-of-distribution data.
- **Break condition**: If the training data covers the entirety of the possible input space, the distinction between in-distribution and OOD vanishes, and calibration becomes the only necessary metric.

## Foundational Learning

- **Concept**: Bayesian Posterior vs. Point Estimates
  - **Why needed here**: The paper frames uncertainty as the variance of a posterior distribution. Without understanding that Neural Networks typically find a single mode while GPs find a distribution, the difference in their uncertainty behavior is opaque.
  - **Quick check question**: Can you explain why a Maximum A Posteriori (MAP) estimate inherently lacks variance information compared to a full Bayesian posterior?

- **Concept**: Calibration (Reliability Diagrams/ECE)
  - **Why needed here**: The study relies on calibration metrics to verify that models are truthful about their confidence inside the training distribution.
  - **Quick check question**: If a model predicts "70% chance of rain" on 100 days, how many days must it rain for the model to be considered "calibrated"?

- **Concept**: Inductive Bias & Architecture Constraints
  - **Why needed here**: The paper attributes the failure of Deep Learning to the specific functional form of ReLU networks. Understanding inductive bias is required to diagnose why a model fails on OOD data.
  - **Quick check question**: How does the inductive bias of a Gaussian Process (smoothness) differ from that of a ReLU Neural Network (piecewise linearity) regarding extrapolation?

## Architecture Onboarding

- **Component map**: Synthetic 2D data generator -> Train/Val/Test split -> 6 algorithms (NNE, CL, EDL, MCD, GP, DPMM) -> Evaluator (in-dist metrics: Accuracy, ECE, Wasserstein Distance; OOD metrics: Average Probability ≈ 0.5? Average Uncertainty ↑?)

- **Critical path**: Define a generative function (the "Ground Truth" LRFD) -> Train models on finite samples -> Validation Step: Compare models not just on accuracy, but on the decoupling of calibration (Q1) and OOD uncertainty (Q3)

- **Design tradeoffs**:
  - GP/DPMM: High reliability on uncertainty, theoretically grounded. Cost: Poor scaling with data size (O(N³) for GP), computational expense.
  - Deep Learning (DL): High accuracy, scalable. Cost: Unreliable uncertainty estimates; require "Toy Model" validation before deployment in science domains.

- **Failure signatures**:
  - Silent Failure: DL model reports 99% confidence with 0% uncertainty on data point x_ood, which is physically impossible/improbable.
  - Hyperparameter Instability: EDL showing increasing uncertainty with more data (potentially an implementation/optimization issue noted in results).

- **First 3 experiments**:
  1. Replicate the Tail Behavior: Train a standard ReLU MLP and a GP on the paper's Dataset A. Plot the predicted probability P(c|x) vs. Radius r for r >> r_train. Verify the ReLU saturates at 1.0 while the GP tends to 0.5.
  2. Perturb the Architecture: Replace ReLU activations with Sinh or Softplus (which have different asymptotic properties) in the Neural Network. Test if OOD uncertainty improves without sacrificing in-distribution calibration.
  3. Implicit Prior Analysis: Attempt to regularize the NN loss function to penalize deviations from 0.5 probability in low-density regions (simulating a Beta prior). Check if this "fixes" the OOD overconfidence.

## Open Questions the Paper Calls Out

- **Open Question 1**: What is the root cause of Evidential Deep Learning (EDL) producing increasing uncertainty estimates as the volume of training data grows, contrary to the expected decrease?
  - Basis in paper: The conclusion states that for EDL, average uncertainties on the test set increase with data size, "indicating that something might be wrong in assumptions or implementation."
  - Why unresolved: The paper identifies the anomalous trend but does not investigate whether it stems from the theoretical formulation of subjective logic or specific implementation details.
  - What evidence would resolve it: A theoretical analysis of the EDL loss landscape or ablation studies on the loss function components (e.g., the KL divergence term) to see if the behavior persists.

- **Open Question 2**: Can replacing ReLU activation functions with alternatives (e.g., periodic or bounded functions) mitigate the pathology where deep learning models output extreme probabilities for out-of-distribution (OOD) data?
  - Basis in paper: The authors note that ReLU networks asymptotically approach linear functions, leading to extreme OOD outputs. They call for "Further studies... to investigate these pathologies of neural networks and the effect of changing important elements like the activation function."
  - Why unresolved: The study focused exclusively on ReLU networks; whether different architectures inherently produce better uncertainty estimates in the tails remains untested.
  - What evidence would resolve it: Empirical evaluation of uncertainty estimation methods using alternative activation functions on the same synthetic OOD datasets.

- **Open Question 3**: Do deep learning uncertainty estimation methods perform better on data with discrete features or statistically independent input features compared to the continuous, correlated synthetic data used in this study?
  - Basis in paper: The authors conclude that while methods failed on their synthetic data, "they might work better for data with for example discrete or bounded distributions, or for data with statistically independent input features."
  - Why unresolved: The study was limited to continuous, unbounded distributions with overlap; the behavior on fundamentally different data types was not investigated.
  - What evidence would resolve it: Application of the same evaluation framework (calibration, data-size dependency, OOD behavior) to datasets specifically designed with discrete variables or independent features.

## Limitations

- The evaluation is restricted to binary classification on synthetic 2D data with radial symmetry, which may not capture the complexity of real-world high-dimensional problems.
- The computational cost of Gaussian Processes and Dirichlet Process Mixture Models prevents thorough testing on larger datasets, leaving uncertainty about their scalability.
- The neural network architectures are relatively simple (3 hidden layers, 200 nodes each), and deeper or more sophisticated architectures might exhibit different uncertainty behaviors.

## Confidence

- Confidence in the core finding that deep learning models fail to provide useful uncertainty estimates for out-of-distribution data: **High**
- Confidence in the conclusion that non-parametric Bayesian models maintain better uncertainty calibration for OOD data: **Medium**
- Confidence in the broader recommendation that researchers should validate model behavior using toy problems before deploying in scientific contexts: **High**

## Next Checks

1. Test the same algorithms on real-world scientific datasets (e.g., astronomical spectra or particle physics data) to verify if the OOD uncertainty failure pattern persists in higher dimensions.
2. Implement alternative neural network architectures (e.g., residual connections, attention mechanisms, or sigmoid activations) to determine if architectural modifications can improve OOD uncertainty estimates without sacrificing in-distribution calibration.
3. Compare the uncertainty estimates from these algorithms with conformal prediction methods to determine if alternative approaches can achieve both good calibration and reliable OOD detection.