---
ver: rpa2
title: Theoretical Compression Bounds for Wide Multilayer Perceptrons
arxiv_id: '2512.06288'
source_url: https://arxiv.org/abs/2512.06288
tags:
- pruning
- proposition
- where
- network
- n1n2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes theoretical bounds on the effectiveness
  of post-training pruning and quantization for wide neural networks. It introduces
  a randomized greedy compression algorithm inspired by Optimal Brain Damage, which
  iteratively replaces weights with perturbed versions to minimize loss increase.
---

# Theoretical Compression Bounds for Wide Multilayer Perceptrons

## Quick Facts
- **arXiv ID:** 2512.06288
- **Source URL:** https://arxiv.org/abs/2512.06288
- **Reference count:** 40
- **Primary result:** Proves wide MLPs can be pruned at linear sparsity rates while maintaining performance, using randomized greedy compression with Lindeberg interpolation.

## Executive Summary
This paper establishes theoretical bounds on post-training pruning and quantization for wide neural networks. The authors introduce a randomized greedy compression algorithm that iteratively replaces weights with perturbed versions to minimize loss increase. Using Lindeberg interpolation, they prove that wide networks can be compressed at linear sparsity rates while maintaining performance, bridging the gap between empirical success and theoretical justification. The results extend to structured pruning of both MLPs and CNNs, showing that networks with wide layers or tall-then-wide bottlenecks are particularly amenable to compression.

## Method Summary
The paper proposes a randomized greedy compression algorithm inspired by Optimal Brain Damage. For pruning, it replaces weights with Bernoulli random variables that have zero mean (E[t] = original weight), which cancels first-order loss changes and allows second-order terms to be bounded. For quantization, it uses quantized random variables with the same property. The algorithm iteratively selects weights based on importance scores (second-order sensitivity) and replaces them with perturbations. Lindeberg interpolation transforms the cumulative loss change into a tractable calculus problem, enabling inductive proofs over layers. The analysis extends from MLPs to structured pruning and CNNs via equivalent MLP representations.

## Key Results
- Proves wide MLPs can be compressed at linear sparsity rates (Theorem 2) with provable loss bounds
- Extends results to structured pruning of neurons and filters in CNNs (Theorem 4)
- Demonstrates empirically that compressibility improves with increased network width (Fig 1)
- Shows bottleneck layers (tall-then-wide) are particularly amenable to compression

## Why This Works (Mechanism)

### Mechanism 1: Randomized Greedy Compression with Zero-Mean Perturbations
- **Claim:** Replacing weights with random perturbations that have zero mean cancels the first-order term in the Taylor expansion of the loss change, allowing controlled loss increases to be bounded by second-order terms.
- **Mechanism:** The algorithm iteratively selects a weight to replace with a discrete random variable (e.g., Bernoulli for pruning or quantized random variable for quantization). Because the perturbation's expected value equals the original weight, the expected first-order change in loss vanishes. The remaining loss variation is governed by second-order terms, which scale with the variance of the perturbation and the curvature of the loss landscape.
- **Core assumption:** The perturbation distribution has zero mean (E[t] = original weight) and bounded variance; the network's loss is locally smooth.
- **Evidence anchors:**
  - [abstract]: "The algorithm we consider bears some similarities with Optimal Brain Damage (OBD) and can be viewed as a post-training randomized version of it."
  - [Section 3.2]: "For both pruning and quantization t satisfies Et[t] = [Wℓ]ij, which cancels the first order mean variation of loss when [Wℓ]ij is replaced by t."
- **Break condition:** If perturbations have non-zero mean or if the loss has large higher-order derivatives near the original weights, the second-order approximation may fail.

### Mechanism 2: Lindeberg Interpolation for Iterative Bound Control
- **Claim:** Lindeberg interpolation transforms the problem of bounding the cumulative loss change across many weight perturbations into a tractable calculus problem, enabling inductive proofs over layers.
- **Mechanism:** Instead of analyzing the full perturbation of a layer at once, the technique considers perturbing one weight at a time. By interpolating between the original and perturbed weights, the loss difference becomes a function of a single scalar parameter. This allows use of Taylor's theorem and concentration inequalities to bound the expected loss change per step, which can then be aggregated across steps.
- **Core assumption:** The network is sufficiently wide to apply concentration results; the activations and weights satisfy boundedness conditions.
- **Evidence anchors:**
  - [Section 3.2]: "The use of the interpolation technique provides two key benefits... Second, this technique casts the problem of bounding the variations of the loss analytically as a simple calculus problem involving a one variable scalar function."
  - [Section 7, Lemmas 8-9]: Technical lemmas bounding second derivatives of the interpolated loss function.
- **Break condition:** If the network depth is very large or weight norms are not bounded, the accumulated errors from iterative interpolation may exceed the theoretical bounds.

### Mechanism 3: Width-Compressibility Tradeoff via Dimensionality Arguments
- **Claim:** Wide layers or "tall-then-wide" bottlenecks can be pruned more aggressively because the per-weight contribution to the output norm diminishes with width, reducing the impact of pruning any single weight.
- **Mechanism:** The error bound for pruning a layer scales with terms like ν² / (width), where ν is a weight norm bound. As width increases, these terms shrink, allowing higher pruning ratios for the same error budget. For bottleneck layers, the tall-then-wide structure allows pruning both layers jointly, exploiting the fact that the intermediate high-dimensional representation has intrinsic dimensionality at most that of the input.
- **Core assumption:** Weights satisfy scaled norms ∥Wℓ∥∞ ≤ c₂√(nℓ ∨ nℓ+1) (Assumption 1, Eq. 3.11); operator norms are bounded (∥Wℓ∥ ≤ c₁, Eq. 3.10).
- **Evidence anchors:**
  - [Section 3.3, Prop. 1]: Conditions (3.13)-(3.15) explicitly link dimension ratios to achievable sparsity p.
  - [Section 6, Numerical Simulations]: "We observe a steady decrease of the error Δ(Φ, ˆΦ) in all pruning settings as w increases."
- **Break condition:** If weight norms grow faster than √(width), or if the layer is not wide relative to its input/output dimensions, the compressibility advantage diminishes.

## Foundational Learning

- **Concept: Second-order Taylor Approximation**
  - **Why needed here:** The entire compression analysis hinges on approximating loss changes using Taylor expansions up to second order; first-order terms are eliminated by randomization, leaving second-order curvature terms to bound errors.
  - **Quick check question:** Given a function f(w) with gradient ∇f and Hessian H at w₀, write the second-order Taylor approximation for f(w₀ + Δw). How does E[Δw] = 0 simplify the expected loss change?

- **Concept: Operator Norms and Weight Scaling**
  - **Why needed here:** Assumption 1 uses operator norms (∥W∥) and entrywise max norms (∥W∥∞) to bound network sensitivity to perturbations; these bounds appear in all error bounds.
  - **Quick check question:** For a weight matrix W ∈ ℝⁿˣᵐ, how is the operator norm ∥W∥ defined? If W has entries drawn i.i.d. from a distribution with variance O(1/(n ∨ m)), what is the expected operator norm?

- **Concept: Lindeberg Interpolation Principle**
  - **Why needed here:** This probabilistic technique (extended from Lindeberg's central limit theorem work) allows replacing a complex random variable with a simpler one while controlling expectations, crucial for bounding the interpolated loss function.
  - **Quick check question:** Explain how Lindeberg interpolation can be used to compare E[f(X)] and E[f(Y)] for two random variables X and Y with matched moments.

## Architecture Onboarding

- **Component map:** Randomized Greedy Algorithm (weight selection + perturbation) -> Lindeberg Interpolation (step-by-step loss bounding) -> Dimension-Dependent Sparsity Bounds (width scaling) -> Structured Pruning Extension (neurons/filters)

- **Critical path:**
  1. Understand the importance score: |E[L(Φ_t) - L(Φ)]| for weight perturbation t.
  2. Grasp how zero-mean perturbations eliminate first-order terms.
  3. Follow the inductive proof (Lemmas 8-10) that bounds cumulative error across layers.
  4. Map dimension conditions (e.g., n_{ℓ+1}/n_ℓ) to achievable sparsity.

- **Design tradeoffs:**
  - **Width vs. depth:** Wider networks allow more aggressive pruning but may increase memory/compute during training.
  - **Projection vs. spectral norm control:** The algorithm adds explicit projections to bound output norms; alternatives include regularizing spectral norms during training.
  - **Randomized vs. deterministic pruning:** Randomization simplifies theory but may be suboptimal compared to deterministic methods like magnitude pruning in practice.

- **Failure signatures:**
  - Pruning too aggressively (p too small) for given width → loss explosion.
  - Applying to networks with unbounded weight norms → violation of Assumption 1 → theoretical guarantees void.
  - Using non-zero-mean perturbations → first-order terms don't cancel → error bounds invalid.

- **First 3 experiments:**
  1. **Validate width-scaling:** Train 2-layer MLPs with varying hidden widths on California Housing. Apply randomized greedy pruning algorithm with fixed p and α. Plot normalized error Δ(Φ, ˆΦ) vs. width to confirm decreasing trend.
  2. **Test structured pruning:** Apply structured variant to a CNN on Digits dataset. Compare pruning wide layers (W) vs. bottleneck layers (B) in terms of accuracy drop and FLOPs reduction.
  3. **Stress test assumptions:** Train a network with weight decay to enforce Assumption 1 vs. one without. Compare pruning performance to see if violated assumptions degrade compressibility.

## Open Questions the Paper Calls Out

- **Question:** Can spectral norms of greedily pruned matrices be bounded probabilistically without requiring explicit projection operations?
  - **Basis in paper:** [explicit] The authors state "Bounding spectral norms of pruned matrices from our algorithm is interesting but challenging since probabilistic tools from random matrix theory are limited when it comes to greedily randomized matrices, and thus requires extensive technical work."
  - **Why unresolved:** Standard random matrix theory tools do not apply to the greedy randomization process used in the algorithm.
  - **What evidence would resolve it:** A probabilistic bound on the spectral norm shift of pruned matrices that holds with high probability under the same width conditions.

- **Question:** Can the differentiability assumption (3.9) on activation functions be removed, allowing direct extension to ReLU without smooth approximations?
  - **Basis in paper:** [inferred] The paper notes ReLU does not satisfy (3.9) and suggests replacing it with "a smooth approximation such as log(1 + e^(βx))/β." The theoretical analysis relies on twice-differentiable activations for bottleneck layers.
  - **Why unresolved:** The Lindeberg interpolation and second-order Taylor analysis fundamentally require the Hessian to exist.
  - **What evidence would resolve it:** A modified analysis framework or alternative proof technique that achieves comparable bounds for non-differentiable activations like ReLU.

## Limitations

- The theoretical framework requires careful application when layers have very different dimensions or when network depth is large
- The assumption that weight norms scale as O(√n) may not hold for all architectures, particularly those with skip connections or normalization layers
- The omission of projection steps in empirical simulations means theoretical guarantees may not fully apply to demonstrated results

## Confidence

- **High Confidence:** The core mechanism of randomized greedy compression with zero-mean perturbations is well-founded, with clear mathematical justification and consistent empirical support
- **Medium Confidence:** The width-compressibility tradeoff predictions are supported by simulations but rely on specific dimension assumptions that may not generalize to all architectures
- **Medium Confidence:** The extension from MLPs to CNNs via equivalent representation is mathematically sound but may not capture all practical considerations in real-world CNN architectures

## Next Checks

1. **Robustness to Non-Ideal Width Scaling:** Test pruning performance on networks where weight norms violate the O(√n) scaling assumption, comparing empirical results to theoretical predictions
2. **Projection Impact Study:** Implement and compare the randomized greedy algorithm with and without the theoretical projection steps to quantify their practical importance
3. **Depth Sensitivity Analysis:** Systemmatically vary network depth while keeping width constant to identify at what depth the cumulative error bounds become violated, and compare to theoretical predictions