---
ver: rpa2
title: Generative AI Uses and Risks for Knowledge Workers in a Science Organization
arxiv_id: '2501.16577'
source_url: https://arxiv.org/abs/2501.16577
tags:
- generative
- science
- llms
- data
- work
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examined generative AI use in a science organization,
  surveying 66 employees and interviewing 22 to understand current and envisioned
  applications, as well as privacy and security concerns. Usage data from Argo, a
  private LLM instance, showed small but increasing adoption across Science and Operations
  divisions.
---

# Generative AI Uses and Risks for Knowledge Workers in a Science Organization

## Quick Facts
- arXiv ID: 2501.16577
- Source URL: https://arxiv.org/abs/2501.16577
- Authors: Kelly B. Wagman; Matthew T. Dearing; Marshini Chetty
- Reference count: 40
- Primary result: Surveyed 66 employees and interviewed 22 at a science organization, finding small but growing generative AI adoption primarily for structured writing tasks, with significant concerns about reliability, overreliance, and data privacy.

## Executive Summary
This study examined generative AI use in a science organization, surveying 66 employees and interviewing 22 to understand current and envisioned applications, as well as privacy and security concerns. Usage data from Argo, a private LLM instance, showed small but increasing adoption across Science and Operations divisions. Employees primarily used generative AI for writing structured text and code, with envisioned uses focusing on extracting insights from unstructured data. Operations employees also explored workflow automation. Key concerns included reliability, overreliance, data privacy, academic publishing ethics, and potential job impacts. The findings highlight the need for domain-specific copilot and workflow agent tools, clear organizational policies on generative AI use, and strategies to address privacy and security risks.

## Method Summary
The study combined quantitative and qualitative methods across an 8-month period. Researchers deployed a survey to 80 employees, filtering to 66 valid responses, and conducted 22 semi-structured interviews (30 minutes each). They analyzed telemetry data from Argo, a private GPT-3.5 Turbo instance, capturing authenticated user counts, division metadata, and token usage from January to August 2024. Qualitative analysis used Thematic Analysis with MAXQDA, employing axial coding by two researchers. The survey instrument and codebook are detailed in Appendix A.1 and Table 6 respectively.

## Key Results
- Adoption data showed small but growing usage across Science and Operations divisions, with monthly unique users increasing over the 8-month study period.
- Employees primarily used generative AI for writing structured text and code, leveraging the ability to easily verify outputs for correctness.
- Operations employees explored workflow automation and repetitive task delegation, while Science employees focused on insight extraction from unstructured data.
- Key concerns centered on reliability and hallucination risks, overreliance leading to skill degradation, data privacy and export control compliance, academic publishing ethics, and potential job displacement.

## Why This Works (Mechanism)

### Mechanism 1: Trust-Conditional Adoption via Private Instances
- Claim: In security-sensitive organizations, adoption of GenAI appears contingent on the deployment of private, internal instances that guarantee data non-retention and firewall containment.
- Mechanism: Employees assess the risk profile of the tool against organizational security mandates (e.g., export control, PII). A private instance (Argo) lowers the perceived risk of data leakage, allowing users to experiment with sensitive tasks that would otherwise be blocked by policy or fear of reprimand.
- Core assumption: Users prioritize security compliance over raw model capability (at least initially), and they accurately perceive the private instance as secure.
- Evidence anchors:
  - [abstract] Mentions "Argo, a private LLM instance," as a core context.
  - [section 4.4.3] Participants explicitly stated they trusted Argo more than ChatGPT because it is "contained within our firewall" and chat history is not stored.
  - [corpus] The paper *How Tech Workers Contend with Hazards of Humanlikeness* suggests user trust is complex, but here it is institutionally mediated via infrastructure.
- Break condition: If the internal tool lags significantly in capability compared to public models (e.g., lacking GPT-4 features), users may bypass the secure system, breaking the security model.

### Mechanism 2: Verification-Driven Use Case Segregation
- Claim: Early adopters naturally segregate GenAI use into "Copilot" (verifiable generation) and "Agent" (delegated execution) modalities based on their ability to validate outputs efficiently.
- Mechanism: Users currently gravitate toward "structured text/code" because errors are easily spotted (low verification cost). They *envision* using agents for "unstructured data extraction" but currently hesitate due to high verification costs (hallucinations), creating a gap between current and envisioned utility.
- Core assumption: The user possesses sufficient domain expertise to verify the structured output (e.g., code, grammar) but finds it expensive to verify unstructured insights (e.g., summary of a field they don't know).
- Evidence anchors:
  - [abstract] Notes current uses for "writing structured text and code" vs. envisioned uses for "extracting insights."
  - [section 4.2.1] Participants used LLMs for emails/code where they could "easily verify is correct."
  - [corpus] *How Tech Workers Contend with Hazards of Humanlikeness* aligns with the risk of overtrust in unverified outputs.
- Break condition: If hallucination rates in unstructured tasks drop (or citations are automated), the barrier to the "envisioned" use cases lowers, shifting the primary modality from Copilot to Agent.

### Mechanism 3: Context-Augmented Workflow Automation
- Claim: Transitioning from a chat interface to a workflow agent requires the injection of domain-specific "knowledge files" to guide the LLM through complex, multi-step processes.
- Mechanism: General-purpose LLMs lack specific organizational context. By augmenting prompts with summaries of past experiences or technical parameters (RAG-like behavior), users can create semi-autonomous loops (e.g., parameter tuning for instruments) that function as "workflow agents."
- Core assumption: The LLM can effectively retrieve and apply the injected context without hallucinating the technical parameters it was fed.
- Evidence anchors:
  - [section 4.3.1] P14 describes creating "knowledge files" to convert natural language to code for data analysis, enabling a workflow agent.
  - [section 4.3.1] P1 used LLMs to generate Python scripts to control legacy software, acting as a bridge to automation.
  - [corpus] *Reliable agent engineering* supports the need for organizational principles in agent design, though specific "knowledge file" mechanics are specific to this paper's findings.
- Break condition: The workflow fails if the "knowledge files" become outdated or if the LLM's context window is insufficient to hold the necessary operational logic.

## Foundational Learning

- Concept: **Copilot vs. Workflow Agent Distinction**
  - Why needed here: To understand why users adopt chat interfaces for writing (Copilot) but require architectural scaffolding (Agents) to automate safety checks or instrument operations.
  - Quick check question: Does the task require a single turn of verifiable generation (Copilot) or a multi-step autonomous execution (Agent)?

- Concept: **Export Control & Data Sovereignty**
  - Why needed here: National labs and similar organizations operate under strict rules about where data resides. Understanding "Private Instances" vs. "Public APIs" is critical for onboarding.
  - Quick check question: If a user pastes unpublished research into the tool, does it stay within the organizational firewall?

- Concept: **Human-in-the-Loop Verification**
  - Why needed here: The paper highlights that "reliability" is the top barrier. Systems must be designed assuming the LLM will make errors, requiring user verification.
  - Quick check question: In a proposed automated workflow, at which step does the human validate the machine's decision?

## Architecture Onboarding

- Component map:
  - **User Interface**: Browser-based chat (Argo) + API access.
  - **Model Layer**: Private instance of GPT-3.5/4 (hosted internally to prevent data egress).
  - **Data Source**: Unconnected to internal databases by default (RAG is a user-side implementation in this study, e.g., P14's knowledge files).
  - **Security Layer**: VPN/Lab login required; no chat history persistence.

- Critical path:
  1. Authentication (Lab credentials) -> 2. Prompt Entry -> 3. Context Injection (if using "knowledge files") -> 4. Internal Inference -> 5. Output Generation.

- Design tradeoffs:
  - **Security vs. Utility**: The private instance secures data (Pro) but may lag behind public models in features/intelligence (Con, noted by P9/P21).
  - **Retention vs. Auditability**: Not storing chat history enhances privacy (Pro) but prevents auditing how the tool is used or recovering lost work (Con).

- Failure signatures:
  - **Feature Gap Bypass**: Users leaving the secure internal tool for more capable public versions, violating security protocols.
  - **Hallucination Drift**: Workflow agents generating plausible but incorrect parameters for scientific instruments.
  - **Prompt Engineering Fatigue**: Users abandoning the tool if "knowledge file" creation is too manual or complex.

- First 3 experiments:
  1. **"Verifiable Text" Pilot**: Roll out to Operations for drafting internal reports/emails. Measure time saved vs. editing time.
  2. **"Context-Augmented" Agent Test**: Implement a specific "knowledge file" workflow for a single instrument team (as P14 did) to automate parameter setting.
  3. **Policy Q&A Retrieval**: Connect the LLM to a sanitized database of lab policies/regulations (envisioned use) to test "unstructured data extraction" accuracy in a low-risk domain.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can internal LLM interfaces be designed to accommodate varying levels of data sensitivity, such as strictly classified information versus unpublished academic research?
- Basis in paper: [explicit] The authors state that "future work could investigate how to design this into the system, particularly for classified information," noting that current private instances do not yet fully address the nuance between different types of sensitive data.
- Why unresolved: Current implementations treat the internal tool as a secure monolith, but employees handle complex data tiers (e.g., export-controlled vs. public) that require distinct handling protocols which the interface does not currently support.
- What evidence would resolve it: A user study of a prototype LLM interface featuring dynamic, role-based data access controls that successfully prevents the input of restricted data while remaining usable for researchers.

### Open Question 2
- Question: Does feature disparity between secure internal LLMs and advanced commercial models drive employees to bypass organizational security protocols?
- Basis in paper: [explicit] The authors note that "Organizations must... make sure their internal options are competitive with external ones since otherwise employees may continue to use external generative AI with better features," a concern echoed by participants who preferred ChatGPT over the internal Argo tool.
- Why unresolved: While usage data shows Argo adoption is growing, the study could not measure the volume of "shadow IT" usage (employees using external tools against policy), leaving the trade-off between security and capability unknown.
- What evidence would resolve it: A longitudinal study comparing the adoption rates of internal tools before and after feature parity upgrades, correlated with network monitoring data for unauthorized external LLM traffic.

### Open Question 3
- Question: What design frameworks allow generalized workflow agents to be effectively adapted by non-technical staff across different organizational roles?
- Basis in paper: [explicit] The discussion calls for "future work" to determine "how to craft a generalised basic workflow agent or template agent that can be adapted by employees in multiple roles and with a range of technical skills."
- Why unresolved: The study found that current workflow automation is largely limited to technically inclined early adopters (e.g., software engineers) writing custom scripts, leaving the utility of agents for Operations or non-technical staff uncertain.
- What evidence would resolve it: A comparative analysis of task completion rates and error frequencies for non-technical employees using template-based workflow agents versus those using standard manual methods.

## Limitations
- The study's restricted scope within a single organization with demographic homogeneity limits generalizability to other contexts.
- The 8-month telemetry window captures early adoption patterns but may not reflect longer-term usage trends or organizational shifts.
- The study only measured usage of a private LLM instance and did not capture potential parallel usage of public tools like ChatGPT.

## Confidence

**High Confidence** in findings about trust-conditional adoption and security concerns, supported by direct quotes from 22 interviewees and corroborated by usage patterns showing security-sensitive users preferring the private instance. The mechanism of users naturally segregating tasks into verifiable (Copilot) versus complex (Agent) modalities is well-supported by multiple participant accounts.

**Medium Confidence** in the generalizability of use case patterns and envisioned applications. While the thematic analysis is robust, the organization-specific context (national laboratory with strict export controls) may not translate directly to commercial or academic settings where security requirements differ.

**Low Confidence** in the long-term sustainability of current adoption patterns. The study captures a snapshot during the rapid evolution of generative AI capabilities, and the feature gap between private and public instances (noted by participants) could fundamentally alter adoption trajectories as capabilities converge or diverge.

## Next Checks
1. **External Tool Usage Audit**: Conduct a controlled survey specifically asking employees about concurrent usage of public generative AI tools alongside the private instance to address the blind spot in understanding whether security measures are driving adoption or creating parallel, unmonitored usage patterns.

2. **Cross-Organizational Comparison**: Replicate the survey and interview methodology in organizations with different security postures (commercial tech companies, academic institutions, healthcare) to validate whether the trust-conditional adoption mechanism and copilot/agent segregation pattern hold across contexts with varying data sensitivity requirements.

3. **Longitudinal Adoption Study**: Extend telemetry collection for 12-24 months to track whether early adopters remain engaged, whether the 10% adoption ceiling breaks, and how the envisioned agent workflows evolve as hallucination mitigation techniques improve.