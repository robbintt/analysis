---
ver: rpa2
title: 'ResKoopNet: Learning Koopman Representations for Complex Dynamics with Spectral
  Residuals'
arxiv_id: '2501.00701'
source_url: https://arxiv.org/abs/2501.00701
tags:
- koopman
- reskoopnet
- spectral
- figure
- dictionary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ResKoopNet addresses the spectral inclusion problem in Koopman
  operator learning by directly minimizing spectral residuals over eigenpairs, enabling
  discovery of more complete spectral information compared to filtering-only approaches.
  The method employs neural networks to optimize dictionary functions for the Koopman
  invariant subspace, achieving both discrete eigenvalue computation and pseudospectrum
  analysis.
---

# ResKoopNet: Learning Koopman Representations for Complex Dynamics with Spectral Residuals

## Quick Facts
- arXiv ID: 2501.00701
- Source URL: https://arxiv.org/abs/2501.00701
- Reference count: 40
- Primary result: ResKoopNet recovers more complete Koopman spectra by minimizing spectral residuals, achieving superior accuracy on pendulum, turbulent flow, and neural dynamics data compared to filtering-only approaches.

## Executive Summary
ResKoopNet addresses the spectral inclusion problem in Koopman operator learning by directly minimizing spectral residuals over eigenpairs rather than relying on post-hoc filtering. The method uses neural networks to optimize dictionary functions for the Koopman invariant subspace, enabling discovery of both discrete eigenvalues and continuous pseudospectra. Experiments demonstrate that ResKoopNet achieves full spectrum recovery with fewer observables than ResDMD, successfully extracts fundamental structures from high-dimensional turbulent flow data, and provides superior state clustering for neural dynamics compared to existing methods.

## Method Summary
ResKoopNet learns Koopman representations by minimizing spectral residuals through alternating optimization between neural dictionary parameters and Koopman matrix updates. A feedforward network parameterizes dictionary functions, with non-trainable basis components preventing trivial solutions. The optimization minimizes J(θ) = (1/m)‖(ΨY - ΨXK)V‖²F, where V contains eigenvectors, ensuring the learned operator captures spectral structure. The method provides theoretical guarantees while maintaining computational adaptability across diverse dynamical systems, from simple pendulums to high-dimensional turbulent flows and neural data.

## Key Results
- Pendulum system: Achieves full spectrum recovery with 300 observables versus 460+ needed by ResDMD
- Turbulent flow: Successfully recovers fundamental pressure field structures missed by Hankel-DMD
- Neural dynamics: Achieves significantly better state clustering (lower Davies-Bouldin index) compared to Hankel-DMD, EDMD with RBF basis, and Kernel ResDMD

## Why This Works (Mechanism)

### Mechanism 1
Direct spectral residual minimization recovers more complete Koopman spectra than filtering-only approaches. ResKoopNet minimizes J(θ) = Σ cres(λi, ϕi)² over eigenpairs rather than post-hoc filtering. The residual cres(λ,ϕ)² = (1/m)v*[Ψ*YΨY − λ(Ψ*XΨY)* − λ̄Ψ*XΨY + |λ|²Ψ*XΨX]v quantifies deviation from true eigenpairs. By treating residual as training loss, the optimization actively explores dictionary space to satisfy spectral accuracy, unlike ResDMD which only validates precomputed spectra.

### Mechanism 2
Eigenspace-projected loss preserves spectral structure better than prediction-error loss. EDMD minimizes ∥ΨY − ΨXK∥²_F (prediction error), while ResKoopNet minimizes ∥(ΨY − ΨXK)V∥²_F where V contains eigenvectors. This projection weights errors along spectral directions, forcing learned K to capture eigenstructure rather than average prediction accuracy. The closed-form K = G†A is derived from spectral residual minimization, not least-squares regression.

### Mechanism 3
Neural dictionary learning enables adaptive basis selection without hand-specified functions. A feedforward network (3 hidden layers, tanh activation) parameterizes Ψ(x;θ). Non-trainable components (constant 1, state coordinates) prevent trivial solutions. Alternating optimization updates θ via SGD while K(θ) updates via closed-form G†A, jointly learning dictionary and Koopman matrix.

## Foundational Learning

- **Concept: Koopman Operator**
  - Why needed here: The entire method assumes understanding that the Koopman operator K acts on observables g via Kg = g∘F, converting nonlinear dynamics into linear (but infinite-dimensional) form.
  - Quick check question: Can you explain why K is linear even when F is nonlinear?

- **Concept: Spectral Residual**
  - Why needed here: The core loss function is built on spectral residual definition; understanding res(λ,ϕ)² = ∫|Kϕ − λϕ|²/∫|ϕ|² is essential.
  - Quick check question: What does a small spectral residual indicate about an eigenpair (λ,ϕ)?

- **Concept: Pseudospectrum**
  - Why needed here: For systems with continuous spectra (chaotic systems), pseudospectrum captures regions where resolvent norm is large even without discrete eigenvalues.
  - Quick check question: Why can't standard eigenvalue decomposition capture continuous spectra?

## Architecture Onboarding

- **Component map:**
  - Input layer (state x ∈ R^d) → 3 hidden layers (250-350 neurons, tanh) → Output layer (NK dictionary functions + non-trainable basis) → Koopman matrix K(θ) = (G + σI)^(-1)A → Eigenpair computation → Loss computation J(θ)

- **Critical path:**
  1. Initialize θ randomly
  2. Compute ΨX(θ), ΨY(θ) from data
  3. Compute K(θ) = G†A (closed-form)
  4. Compute eigenvectors V(θ)
  5. Evaluate J(θ)
  6. If J > ε: update θ via Adam SGD, go to step 2
  7. Output: K(θ), eigenpairs, optional pseudospectrum

- **Design tradeoffs:**
  - Dictionary size NK: Larger NK → richer spectral capture but O(N³K) matrix inversion cost
  - Hidden layer neurons: More neurons → better approximation but longer training
  - Regularization σ: Too small → numerical instability in G†; too large → biased K estimate
  - Alternating vs joint optimization: Alternating ensures stability; joint may be faster but less stable

- **Failure signatures:**
  - J(θ) plateaus above threshold: Network capacity insufficient or learning rate too small
  - Eigenvalues diverge from unit circle (for measure-preserving systems): Overfitting or inadequate basis
  - High condition number of G(θ): Increase regularization σ
  - Pseudospectrum computation times out: Reduce grid resolution nz

- **First 3 experiments:**
  1. **Pendulum spectrum recovery:** Use 90-240 initial conditions, NK=300-350. Verify eigenvalues lie on unit circle. Compare against EDMD (shows spectral pollution) and Hankel-DMD (misses continuous spectrum).
  2. **Turbulence mode extraction:** Apply truncated SVD (150 components) to 295K-dimensional pressure field. Train with NK=250 (99 NN + 150 SVD + 1 constant). Check that Mode 1 (smallest residual) recovers fundamental pressure structure.
  3. **Neural state clustering:** Reduce neural data to 24D via SVD, use NK=50. Compute eigenfunctions across trials. Quantify clustering quality using Davies-Bouldin Index; ResKoopNet should achieve lower DBI than Hankel-DMD, EDMD+RBF, Kernel-ResDMD.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the ResKoopNet framework be extended to handle stochastic dynamical systems while maintaining its spectral accuracy?
  - Basis in paper: Section 5 explicitly states that unlike stochastic approaches such as VAMP and SDMD, "ResKoopNet currently does not account for stochasticity."
  - Why unresolved: The current formulation minimizes spectral residuals for deterministic operators, lacking a mechanism to handle noise or probabilistic state transitions inherent in stochastic systems.
  - What evidence would resolve it: A modified loss function that incorporates noise terms or stochastic Koopman operator theory, validated on benchmark stochastic systems (e.g., Langevin dynamics).

- **Open Question 2:** How can physical constraints (e.g., conservation laws, symmetries) be integrated into the neural network architecture to improve the discovery of Koopman representations?
  - Basis in paper: Section 5 suggests that "incorporating such domain knowledge directly into the neural network architecture could significantly enhance the learning of Koopman spectral information."
  - Why unresolved: The current implementation uses generic feedforward networks without explicit mechanisms to enforce physical invariants, potentially leading to non-physical spectral components or modes.
  - What evidence would resolve it: A modified network architecture (e.g., physics-informed neural networks) that enforces constraints like unitary spectra for Hamiltonian systems, resulting in better adherence to physical laws than the unconstrained baseline.

- **Open Question 3:** Can the computational cost of ResKoopNet be reduced to support real-time or online model learning?
  - Basis in paper: Appendix A.6 notes that "repeated least-squares optimizations make convergence slower" and "its computational demands render it less suitable for real-time or online Koopman model learning tasks."
  - Why unresolved: The iterative optimization process scales with system dimensionality and network parameters, creating a bottleneck for streaming data applications.
  - What evidence would resolve it: The development of efficient training strategies (e.g., adaptive selection of observables or incremental updates) that achieve comparable spectral accuracy with significantly reduced runtime complexity.

## Limitations
- Dictionary capacity bounds remain empirical; precise relationship between network width and spectral recovery quality is not established
- Computational scaling to high-dimensional systems is problematic due to cubic eigenvector computation costs
- Performance on highly chaotic systems with dense continuous spectra lacks systematic validation

## Confidence
- **High confidence:** Core algorithmic framework (spectral residual minimization via alternating optimization) is mathematically sound and well-implemented. Pendulum experiments showing complete spectrum recovery versus ResDMD's filtering approach are reproducible.
- **Medium confidence:** Claims about superior performance on turbulence and neural data rely on qualitative comparisons (mode shapes, clustering quality) that are harder to reproduce without access to exact datasets.
- **Low confidence:** Theoretical guarantees about avoiding spectral pollution are stated but not quantitatively validated across diverse system classes.

## Next Checks
1. **Scalability test:** Apply ResKoopNet to a system with dimension >100 (e.g., high-dimensional Lorenz) and measure computational time versus dictionary size NK. Verify that eigenvalue computation remains tractable.
2. **Robustness to initialization:** Repeat pendulum experiments with different random seeds for network initialization. Quantify variance in recovered spectra to assess sensitivity to initialization.
3. **Cross-validation of pseudospectrum:** For a system with known continuous spectrum (e.g., Chirikov map), compare ResKoopNet's pseudospectrum against analytical predictions. Verify that high-resolvent regions align with theoretical expectations.