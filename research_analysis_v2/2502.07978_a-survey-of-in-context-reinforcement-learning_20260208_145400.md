---
ver: rpa2
title: A Survey of In-Context Reinforcement Learning
arxiv_id: '2502.07978'
source_url: https://arxiv.org/abs/2502.07978
tags:
- learning
- pretraining
- reinforcement
- policy
- icrl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey paper explores the emerging field of in-context reinforcement
  learning (ICRL), where pretrained agents can solve new tasks without parameter updates
  by conditioning on task-related context such as action-observation histories. The
  paper surveys methods for pretraining such agents, including supervised pretraining
  through behavior cloning and reinforcement pretraining using online interactions.
---

# A Survey of In-Context Reinforcement Learning

## Quick Facts
- arXiv ID: 2502.07978
- Source URL: https://arxiv.org/abs/2502.07978
- Reference count: 12
- Authors: Amir Moeini; Jiuqi Wang; Jacob Beck; Ethan Blaser; Shimon Whiteson; Rohan Chandra; Shangtong Zhang
- One-line primary result: Survey of emerging field where pretrained agents solve new tasks without parameter updates by conditioning on action-observation histories

## Executive Summary
This survey paper explores in-context reinforcement learning (ICRL), where pretrained agents can solve new tasks without parameter updates by conditioning on task-related context such as action-observation histories. The paper surveys methods for pretraining such agents, including supervised pretraining through behavior cloning and reinforcement pretraining using online interactions. Key contributions include demonstrating remarkable out-of-distribution generalization capabilities, with pretrained agents achieving performance comparable to standard RL algorithms that require parameter updates. The survey covers test-time performance, showing that ICRL agents require fewer samples to achieve similar performance to baseline RL algorithms, and examines theoretical advances in understanding how neural networks implement RL algorithms in their forward pass.

## Method Summary
ICRL methods involve pretraining agents on diverse RL tasks with cross-episode trajectory histories, then deploying them without parameter updates by conditioning on new task contexts. Two main pretraining approaches exist: supervised pretraining via behavior cloning on trajectories showing improvement patterns, and reinforcement pretraining using standard RL objectives with architectural modifications for stability. At test time, agents process action-observation histories through causal transformer backbones to generate actions for new tasks. The key insight is that pretraining on diverse tasks with curriculum-ordered trajectories enables the network to learn algorithmic patterns that generalize to novel tasks, implementing RL algorithms in the forward pass rather than memorizing task-specific policies.

## Key Results
- Pretrained agents achieve performance comparable to standard RL algorithms without parameter updates
- ICRL agents demonstrate stronger out-of-distribution generalization than traditional meta-RL approaches
- Test-time sample efficiency is significantly improved, requiring fewer samples to reach similar performance levels

## Why This Works (Mechanism)

### Mechanism 1
Pretrained neural networks can implement RL algorithms in their forward pass without parameter updates. During pretraining on diverse tasks with trajectory histories, the network learns to recognize patterns of improvement across episodes. At test time, conditioning on action-observation histories triggers the learned algorithm to process context and output improved actions—the algorithm is "frozen" into the weights. This assumes pretraining distribution contains sufficient diversity and structure for the network to learn generalizable algorithmic patterns rather than task-specific memorization.

### Mechanism 2
In-context improvement emerges when pretraining data includes curriculum-ordered trajectories showing performance progression. Cross-episode inputs ordered by difficulty, proficiency, or returns demonstrate the improvement pattern to the network. The model learns to continue this pattern at test time by attending to what changed between improving episodes and applying similar transformations. This requires the rate and structure of improvement in pretraining trajectories to transfer to test tasks.

### Mechanism 3
Reinforcement pretraining enables emergent ICRL without being explicitly trained to produce in-context learning behavior. The network optimizes only for good actions or value estimates. During training across many MDPs with history-dependent policies, implementing an RL algorithm in the forward pass emerges as a solution discovered by the network itself—no behavioral cloning required. This assumes sufficient task diversity and appropriate architecture exist for the emergent solution to be generalizable rather than overfitted.

## Foundational Learning

- **Markov Decision Processes (MDPs) and Policy Optimization**: ICRL frames test tasks as new MDPs the agent must solve. Understanding policy evaluation vs. control, value functions, and the policy improvement loop is essential to recognize what the forward pass is implementing. Quick check: Can you explain why iterating between policy evaluation and improvement generates optimal policies, and how this relates to what ICRL agents do in-context?

- **Behavior Cloning and Supervised Sequence Learning**: Supervised pretraining uses log-likelihood objectives on trajectory-action pairs. Understanding the gap between cloning a policy vs. cloning an algorithm (which generalizes) is central. Quick check: Why might cloning expert actions fail to produce improvement behavior, whereas cloning learning histories (improving trajectories) might succeed?

- **Transformer Attention and Context Processing**: Most ICRL work uses causal transformers to process long cross-episode contexts. Understanding attention, causal masking, and position encoding helps diagnose why the architecture can or cannot implement specific RL algorithms. Quick check: How does causal masking prevent information leakage, and why might quadratic attention scaling become a bottleneck for very long interaction histories?

## Architecture Onboarding

- **Component map**: Context Encoder -> Sequence Backbone (Causal Transformer) -> Action Head -> Auxiliary Modules (RTG predictor, hierarchical decision makers)

- **Critical path**: Pretraining data curation → Cross-episode trajectory ordering determines what algorithm emerges → Context window length must span multiple episodes to enable improvement → Architecture stability modifications required for long-horizon RL pretraining without collapse

- **Design tradeoffs**: Transformer vs. State Space Models (Transformers have quadratic inference but strong empirical performance; SSMs offer linear scaling but less validated for ICRL); Expert demonstrations vs. self-generated context (Expert prompts improve few-shot performance but create train/test distribution mismatch; own interactions require longer test-time budgets); Hindsight information (RTG) availability (Ground-truth RTG unavailable at test time; estimation methods introduce approximation error)

- **Failure signatures**: No in-context improvement (Performance flat across episodes → Check if pretraining used single-episode context or lacked improvement curriculum); Catastrophic forgetting during pretraining (Performance collapses mid-training → Check critic objective interference, consider attention sinks); OOD generalization failure (Works on similar tasks but not novel ones → May have learned task identification rather than algorithm)

- **First 3 experiments**: Dark Room sanity check (Train on bandit/navigation tasks with held-out goal locations; plot episode return vs. episode number within single test task to verify in-context improvement curve); Context ablation (Test with 1 episode vs. 5 episodes vs. 20 episodes of context; confirm performance scales with context length); Supervised vs. reinforcement pretraining comparison (On same task distribution, compare behavior cloning with improvement curriculum vs. online RL pretraining; measure sample efficiency and OOD generalization gap)

## Open Questions the Paper Calls Out

### Open Question 1
How does in-context reinforcement learning emerge during reinforcement pretraining in realistic settings? Current theoretical analyses use simplified models and pretraining algorithms that do not reflect the complexity of large-scale reinforcement pretraining. Resolution requires theoretical proofs showing how specific RL algorithms emerge in multi-layer transformers trained with standard RL objectives, validated by empirical analysis of learned attention patterns.

### Open Question 2
Can ICRL help close the sim-to-real gap in robotics applications? All demonstrated ICRL capabilities currently exist only in simulated environments with no empirical evidence yet showing ICRL transferring to physical robot platforms. Resolution requires empirical demonstrations showing ICRL agents pretrained in simulation achieving comparable in-context improvement rates on physical robots.

### Open Question 3
Can ICRL enable generalization to unseen teammates and opponents in multi-agent settings? Meta-RL approaches have shown only limited generalization in small-scale problems; it remains untested whether ICRL's capacity for in-context adaptation scales to the complexity of modeling and adapting to diverse agent behaviors. Resolution requires benchmarks showing ICRL agents maintaining performance when paired with novel agent types not seen during pretraining.

### Open Question 4
Why do recent ICRL methods achieve stronger out-of-distribution generalization than earlier meta-RL approaches? Multiple factors changed simultaneously between early and recent work (transformer architectures, pretraining scale, curriculum strategies), making it difficult to isolate which factors drive improved generalization. Resolution requires ablation studies systematically varying architecture, scale, and training objectives while measuring out-of-distribution performance.

## Limitations

- Theoretical foundations remain incomplete despite recent proofs for simplified models
- Pretraining data requirements and optimal curriculum construction methods are underspecified
- Architecture constraints limit scalability, with quadratic attention becoming prohibitive for very long contexts

## Confidence

**High confidence**: The fundamental observation that pretrained agents can solve new tasks without parameter updates by conditioning on context; the distinction between supervised pretraining via behavior cloning and reinforcement pretraining; the importance of cross-episode context spanning multiple episodes

**Medium confidence**: Claims about curriculum ordering directly influencing improvement rate, though generality across different task domains and optimal curriculum construction methods are not fully characterized

**Low confidence**: Theoretical claims about emergent RL algorithms in the forward pass, as existing proofs are limited to simplified single-layer linear attention transformers and the relationship between pretraining objectives and emergent algorithms is not fully mapped

## Next Checks

1. **Algorithmic specificity test**: Train two variants—one with improvement-curriculum trajectories and one with random trajectory ordering. Test both on held-out tasks requiring different RL algorithms (e.g., one task favoring TD methods, another favoring model-based planning). Measure whether the curriculum variant generalizes across algorithm types while the random variant overfits to specific patterns.

2. **Context window ablation study**: Systematically vary context window length (1, 3, 5, 10 episodes) across multiple task families. Plot in-context improvement curves and OOD generalization gaps. Identify the minimum context length threshold where algorithmic learning emerges versus task memorization, and test whether this threshold scales with task complexity.

3. **Alternative architecture comparison**: Implement the same ICRL training pipeline using state space models (Mamba/S5) versus causal transformers. Keep pretraining data, curriculum, and evaluation identical. Measure sample efficiency, OOD generalization, and inference latency. This directly tests whether transformer dominance is due to architectural superiority or historical precedent.