---
ver: rpa2
title: Named Entity Recognition of Historical Text via Large Language Model
arxiv_id: '2508.18090'
source_url: https://arxiv.org/abs/2508.18090
tags:
- historical
- entity
- llms
- methods
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the use of large language models for named
  entity recognition in historical texts, where annotated data is scarce. The approach
  uses zero-shot and few-shot prompting strategies, providing the model with minimal
  examples directly in the prompt without fine-tuning.
---

# Named Entity Recognition of Historical Text via Large Language Model

## Quick Facts
- arXiv ID: 2508.18090
- Source URL: https://arxiv.org/abs/2508.18090
- Reference count: 29
- Primary result: LLM prompting with single examples outperforms zero-shot baselines for historical NER without fine-tuning

## Executive Summary
This study investigates large language models for named entity recognition in historical texts, where annotated data is scarce. The approach uses zero-shot and few-shot prompting strategies, providing the model with minimal examples directly in the prompt without fine-tuning. Experiments on the multilingual HIPE-2022 dataset show that even a single in-context example significantly improves performance over zero-shot baselines, while adding more examples often reduces accuracy due to prompt length. Few-shot prompting outperforms zero-shot prompting, and majority voting over multiple runs offers modest improvements under fuzzy evaluation. LLM-based methods achieve competitive results, outperforming some supervised systems on one dataset, but remain below state-of-the-art supervised approaches overall.

## Method Summary
The study uses DeepSeek-V3-0324 to perform named entity recognition on historical texts through in-context learning. Rather than fine-tuning, the model receives prompts containing entity label definitions and 1-5 example sentence-entity mappings. Examples are retrieved either randomly, via TF-IDF lexical overlap, or embedding similarity. The model outputs a Python list of entity tuples, which is parsed and converted to IOB format. Performance is evaluated using strict and fuzzy micro F1 scores on the HIPE-2022 dataset across five languages.

## Key Results
- Single in-context example (1-shot) significantly outperforms zero-shot prompting
- Adding more examples (3-5 shot) often degrades performance due to context length
- Lexical retrieval performs as well as or better than embedding-based retrieval
- Majority voting over 3 runs improves fuzzy F1 scores modestly
- LLM methods outperform some supervised systems on German datasets but lag behind SOTA overall

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Providing a single labeled example (1-shot) allows the LLM to significantly outperform zero-shot baselines in historical NER.
- **Mechanism:** The model utilizes in-context learning, where the single demonstration anchors the output format and clarifies the specific entity schema without updating model weights.
- **Core assumption:** The LLM possesses sufficient pre-trained semantic knowledge of historical language variations to generalize from the schema demonstration.
- **Evidence anchors:** "even a single in-context example significantly improves performance over zero-shot baselines" [abstract]; "all few-shot prompting methods significantly outperform the zero-shot baseline" [section 5.1].

### Mechanism 2
- **Claim:** Increasing examples from one to three or five often degrades performance due to context length constraints.
- **Mechanism:** As prompt length increases, the model's attention mechanism may become diluted, reducing adherence to the task.
- **Core assumption:** Performance drop is causal due to prompt length and attention distribution, rather than specific content of added examples.
- **Evidence anchors:** "likely due to longer prompts exceeding the model's optimal context window" [section 6]; "prompt methods using only one example outperform their counterparts using three or five examples" [section 5.1].

### Mechanism 3
- **Claim:** Majority voting over multiple stochastic generations improves robustness primarily for fuzzy matching.
- **Mechanism:** Ensembling filters out spurious predictions that appear in only one run, retaining entities the model is consistently confident about.
- **Core assumption:** Model errors are uncorrelated across runs, and valid entities appear consistently even with temperature ≈ 0.
- **Evidence anchors:** "majority voting over multiple runs offers modest improvements under fuzzy evaluation" [abstract]; "leveraging repeated runs to reduce variance and filter out spurious predictions" [section 4.2].

## Foundational Learning

- **Concept: In-Context Learning (ICL)**
  - **Why needed here:** This is the engine of the methodology. It explains how the system functions without the "training" phase typical of historical NER approaches.
  - **Quick check question:** If you change the order of examples in the prompt, does the model's output change? (Answer: Likely yes, due to recency bias in ICL).

- **Concept: Token Classification vs. Text Generation**
  - **Why needed here:** The paper frames NER (typically token classification) as a text generation task (generating a Python list of tuples).
  - **Quick check question:** Does the model output a label for every token or just a list of extracted strings? (Answer: The latter; the paper converts this list to IOB format later).

- **Concept: Fuzzy vs. Strict Evaluation**
  - **Why needed here:** Historical texts have messy boundaries (OCR errors). "Strict" evaluation punishes slight boundary mismatches, while "Fuzzy" allows overlap.
  - **Quick check question:** If a model extracts "Berlin" but the ground truth is "Berlin," which metric catches this as an error? (Answer: Strict, likely; Fuzzy would count it as correct if overlap is sufficient).

## Architecture Onboarding

- **Component map:** Input Text → Example Retrieval → Prompt Construction → LLM Generation → Response Parsing → IOB Conversion → Evaluation
- **Critical path:** Example Retrieval → Prompt Construction → LLM Generation → Response Parsing
- **Design tradeoffs:**
  - 1-shot vs. 3-shot: 1-shot is favored as performance degrades with more examples likely due to context dilution
  - Lexical vs. Embedding Retrieval: Lexical (TF-IDF) is cheaper and slightly better or equal to Embedding similarity
  - Cost vs. Accuracy: LLM prompting is "cost-effective" compared to annotation but lags behind SOTA supervised models
- **Failure signatures:**
  - Empty Lists: LLM fails to generate any entities if it deems the text too noisy
  - Boundary Drift: High Fuzzy F1 but Low Strict F1 indicates the model finds the entity but gets the span wrong
  - Context Dilution: Performance drops when moving from 1-shot to 3-shot
- **First 3 experiments:**
  1. Zero-Shot Baseline: Run the prompt with no examples on the Test set to establish floor performance
  2. Shot Scaling (1 vs. 3): Run 1-shot vs. 3-shot on a small validation slice to verify the "degradation" hypothesis
  3. Format Robustness: Test if the "Python list of tuples" output format breaks on sentences with quotes or special characters

## Open Questions the Paper Calls Out

- **Open Question 1:** Can prompt compression techniques mitigate the performance degradation observed when providing multiple in-context examples? The paper suggests future work should explore prompt optimization including compression techniques to address the 3-5 shot performance drop.

- **Open Question 2:** Do retrieval strategies leveraging semantic and historical metadata outperform the random, lexical, and embedding-based methods analyzed? The authors state that more sophisticated retrieval strategies could improve in-context example quality.

- **Open Question 3:** To what extent do the findings generalize to other large language models with different architectures or context window limitations? The methodology notes experiments were restricted to a single LLM and acknowledges results may differ across models with varying architectures.

## Limitations
- Performance degradation with multiple examples is observed but the underlying mechanism (context dilution vs. attention limits) remains unconfirmed
- Results are based on a single dataset (HIPE-2022) and may not generalize to other entity schemas or historical periods
- Key implementation details are underspecified including exact entity label sets and error handling for malformed outputs

## Confidence
**High Confidence:** 1-shot prompting outperforms zero-shot baselines; LLM prompting is a viable cost-effective alternative; Majority voting provides modest improvements under fuzzy evaluation.

**Medium Confidence:** 3-5 shot prompting degrades performance due to context length; Retrieval strategy has minimal impact; LLM methods outperform some supervised systems on specific datasets.

**Low Confidence:** 1-shot is universally optimal; Fuzzy evaluation is more appropriate for historical texts; Cost-effectiveness claims relative to fine-tuning.

## Next Checks
1. **Context Window Ablation:** Systematically test performance across different prompt lengths while measuring attention distribution patterns to confirm whether degradation is due to context dilution or simple length constraints.

2. **Cross-Dataset Generalization:** Apply the same prompting strategy to MariNER or other historical NER datasets with different entity schemas and time periods to test whether the 1-shot advantage generalizes.

3. **Determinism Analysis:** Run experiments with varying temperature settings across multiple seeds to quantify actual variance in outputs and validate whether majority voting provides meaningful benefits.