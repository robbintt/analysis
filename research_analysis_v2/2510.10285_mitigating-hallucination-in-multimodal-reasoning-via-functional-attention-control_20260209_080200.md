---
ver: rpa2
title: Mitigating Hallucination in Multimodal Reasoning via Functional Attention Control
arxiv_id: '2510.10285'
source_url: https://arxiv.org/abs/2510.10285
tags:
- reasoning
- attention
- heads
- visual
- hallucination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses hallucination in multimodal large reasoning
  models (MLRMs), where models produce conclusions that conflict with visual evidence
  or their own reasoning chains. The authors identify two main causes: perceptual
  bias in shallow layers (inadequate visual attention) and reasoning drift in deeper
  layers (loss of intermediate steps).'
---

# Mitigating Hallucination in Multimodal Reasoning via Functional Attention Control

## Quick Facts
- arXiv ID: 2510.10285
- Source URL: https://arxiv.org/abs/2510.10285
- Reference count: 40
- Key outcome: A lightweight plugin that improves MLRM performance by 5-8% on average by amplifying functional attention heads without retraining

## Executive Summary
This paper addresses hallucination in multimodal large reasoning models (MLRMs), where models produce conclusions that conflict with visual evidence or their own reasoning chains. The authors identify two main causes: perceptual bias in shallow layers (inadequate visual attention) and reasoning drift in deeper layers (loss of intermediate steps). They propose a lightweight, two-step plugin: Functional Head Identification to classify attention heads as perception- or reasoning-oriented using visual-attention ratios, and Class-conditioned Rescaling to amplify contributions of these functional heads without retraining. Evaluations on three MLRMs (Kimi-VL, Ocean-R1, R1-Onevision) across six benchmarks show average improvements of 5-8% and up to 15%, with minimal computational overhead (<1%) and latency (9% of baseline). The approach is model-agnostic and enhances reliability and interpretability.

## Method Summary
The method consists of two steps: (1) Functional Head Identification computes visual-attention ratios S_v^(l)(h) for each head, then categorizes heads as perception-oriented (high visual attention in shallow layers) or reasoning-oriented (low visual attention in deep layers) using thresholds τ_perc and τ_reas; (2) Class-Conditioned Rescaling applies multiplicative gains g_perc and g_reas to identified functional heads before output projection. The approach requires no retraining and only modifies attention head outputs through simple multiplicative scaling. Key hyperparameters include layer boundaries ℓ_perc and ℓ_reas, thresholds τ_perc and τ_reas, and gains g_perc and g_reas, with specific values provided for different model configurations.

## Key Results
- Average performance improvement of 5-8% across six benchmarks (MathVista, MathVision, HallusionBench, CLEVR, MMStar, SEED-Bench)
- Up to 15% improvement on individual tasks, with Ocean-R1 showing 14.7% gain on MathVision
- Minimal computational overhead: <1% additional parameters, 9% latency increase
- Model-agnostic approach validated on Kimi-VL, Ocean-R1, and R1-Onevision architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attention heads exhibit functional specialization, with shallow heads predominantly serving perception and deeper heads shifting toward symbolic reasoning.
- Mechanism: Compute modality attention ratio S_v(h) = (1/|T_q|) Σ_{i∈T_q} Σ_{j∈T_v} a_{ij}^{(h,ℓ)} to measure visual focus per head. Heads with S_v(h) ≥ τ_perc in layers ℓ ≤ ℓ_perc are classified as perception-oriented; heads with S_v(h) ≤ τ_reas in layers ℓ ≥ ℓ_reas are reasoning-oriented.
- Core assumption: Models already contain functional heads with latent capability, but they are under-weighted relative to dominating heads that cause hallucination.

### Mechanism 2
- Claim: Hallucinations arise from compounding failures across perception-reasoning stages, not isolated modality underuse.
- Mechanism: Perceptual bias in L_perc causes incomplete visual grounding; reasoning drift in L_reas allows conclusions to deviate from established premises. The paper attributes hallucination to their synergistic interaction.
- Core assumption: Attention patterns are causally linked to output correctness, not merely correlated.

### Mechanism 3
- Claim: Selective amplification of functional heads improves grounding without retraining or architectural changes.
- Mechanism: Apply multiplicative gains g(h,ℓ) = g_perc·I[h∈H_perc] + g_reas·I[h∈H_reas] + 1·I[otherwise] to per-head outputs before concatenation. Gains compound through residual stream, influencing global reasoning trajectory.
- Core assumption: Amplifying beneficial heads is safer than attenuating potentially harmful ones (minimal editing principle).

## Foundational Learning

- **Modality-Indexed Attention**
  - Why needed here: Understanding how attention distributes across visual vs. textual tokens is prerequisite to identifying functional heads and diagnosing hallucination sources.
  - Quick check question: Given a 256-token sequence with 64 visual tokens, if head h allocates 48% of attention mass to visual tokens, what is S_v(h)?

- **Depth-Aware Functional Boundaries**
  - Why needed here: The method depends on layer boundaries (ℓ_perc, ℓ_reas) that define where perception ends and reasoning begins; misplacement would misclassify heads.
  - Quick check question: In a 28-layer model, if ℓ_perc = 7 and ℓ_reas = 3, which layers belong to both L_perc and L_reas? (Answer: layers 3-7)

- **Minimal Editing Principle**
  - Why needed here: The paper argues for amplification-only intervention to avoid collateral damage from attenuating heads with uncertain utility.
  - Quick check question: Why does Strategy A (amplify only) have a higher expected gain than Strategy C (amplify + attenuate) under the assumption E[U(H_attenuate)] ≥ 0?

## Architecture Onboarding

- **Component map**: Vision encoder → produces visual tokens T_v → Language backbone (Transformer with multi-head attention) → layers partitioned into L_perc and L_reas → Functional Head Identification module → Class-Conditioned Rescaling module → output projection

- **Critical path**: 1) During forward pass, materialize attention weights A^(h,ℓ) 2) Compute visual attention ratios S_v(h) using pre-cached visual token ranges 3) Classify heads as H_perc^(ℓ) or H_reas^(ℓ) 4) Multiply per-head outputs by g(h,ℓ) before concatenation 5) Standard output projection continues unchanged

- **Design tradeoffs**: Sparse vs. dense intervention: Lower τ_perc and higher τ_reas select fewer heads (sparser, less risk of dilution); Boundary placement: Earlier ℓ_perc captures more perception layers but risks including mixed-function layers; Gain magnitude: Higher gains increase effect size but may destabilize; optimal values (1.14-1.30) found via sweep

- **Failure signatures**: No improvement: Check if visual token ranges are correctly cached; thresholds may be too restrictive; Degraded performance: Gains may be too high; verify ℓ_perc and ℓ_reas placement matches model architecture; High latency: Ensure head classification is computed once per forward pass, not per token

- **First 3 experiments**: 1) Boundary sweep: Fix gains and thresholds; vary ℓ_perc ∈ {3,5,7,10} and ℓ_reas ∈ {2,3,5} on a held-out subset. Measure accuracy delta. 2) Gain calibration: Fix boundaries at best configuration from (1); sweep g_perc ∈ {1.05,1.10,1.14,1.20} and g_reas ∈ {1.10,1.20,1.30,1.40}. Identify stable region. 3) Ablation of perception vs. reasoning: Run with only H_perc amplified, then only H_reas amplified. Compare per-task gains to verify complementary roles (Table 2 pattern).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the perception-reasoning boundary be automatically determined for each task rather than requiring manual tuning of ℓperc and ℓreas?
- Basis in paper: [explicit] The authors observe that "performance peaks appear in bands of layers, and the optimal ℓperc and ℓreas vary across tasks," with "task-dependent bands" forming different optimal regions for visual vs. reasoning-dominant tasks. They explicitly state: "Future work could build on interpretability studies to map these transitions more precisely."
- Why unresolved: The current approach requires sweep experiments to identify optimal boundaries (e.g., 27.4% variance between best/worst settings), which is computationally expensive and task-specific.
- What evidence would resolve it: A learning-based or heuristic method that automatically detects layer-wise functional transitions without exhaustive search, validated by matching or exceeding the performance of manually-tuned boundaries.

### Open Question 2
- Question: Would input-conditional adaptive selection and scaling of attention heads improve performance beyond static functional head identification?
- Basis in paper: [explicit] The authors state: "A natural extension is to design more fine-grained classifiers that score each head by multiple signals (e.g., depth, modality ratio, consistency). An adaptive scheme could then select S(x) depending on the input x." They also note the challenge is "balancing stronger, adaptive selection with the stability and efficiency."
- Why unresolved: Current method applies uniform gains (gperc, greas) to all heads within functional categories, ignoring instance-level variation.
- What evidence would resolve it: Comparing input-conditional scaling against static scaling on diverse examples, measuring both performance gains and stability/variance across runs.

### Open Question 3
- Question: Can the transitional layer zone (layers 10-17 in their experiments) be better characterized and leveraged, rather than treating perception and reasoning as binary categories?
- Basis in paper: [inferred] Figure 4 reveals "low Acc observed in the intermediate range (ℓ∈[10,17]), when contrasted with both shallow and deep high-Acc regions," suggesting "a transitional zone where perception and reasoning functions are more intricately intertwined rather than cleanly separated."
- Why unresolved: The binary categorization (perception vs. reasoning heads) may miss nuanced roles of intermediate layers.
- What evidence would resolve it: Multi-class head categorization or continuous functional scoring that captures mixed roles, demonstrating improved hallucination mitigation over binary classification.

## Limitations

- **Methodological Generalizability**: The approach's effectiveness across diverse multimodal architectures (MoE vs dense, different vision backbones) remains untested, with reported gains primarily from three specific MLRMs trained on particular vision-language datasets.
- **Threshold Stability**: Functional head identification relies on fixed thresholds (τ_perc, τ_reas) that may not generalize across model scales or training regimes, with no sensitivity analyses for these hyperparameters across the full model spectrum.
- **Attribution vs. Correlation**: While the paper demonstrates attention modulation improves performance, the causal relationship between identified functional heads and hallucination reduction requires further validation through comparative ablations against random head modulation.

## Confidence

- **High Confidence**: The computational framework for identifying functional attention heads is technically sound and well-specified, following established attention analysis methodologies and producing consistent results across reported experiments.
- **Medium Confidence**: The claimed 5-8% average improvement and up to 15% maximum gains are well-supported within the experimental scope, but external validity across broader MLRM architectures and task domains remains uncertain.
- **Low Confidence**: The theoretical guarantees for the "minimal editing principle" are derived from simplified assumptions that may not hold in complex multimodal reasoning contexts, and the synergistic interaction model lacks direct causal evidence beyond correlation analysis.

## Next Checks

1. **Cross-Architecture Transferability**: Apply the method to two additional MLRM architectures (e.g., GPT-4V, Gemini) and evaluate on the same six benchmarks. Document whether functional head patterns and optimal thresholds transfer, or if architecture-specific calibration is required.

2. **Causal Intervention Analysis**: Design a controlled experiment where randomly selected non-functional heads are attenuated instead of functional heads being amplified. Compare performance changes to validate that the observed improvements specifically stem from functional head enhancement rather than general attention perturbation.

3. **Out-of-Distribution Robustness**: Test the method on cross-dataset generalization tasks (e.g., applying Ocean-R1-trained functional heads to MathVision evaluation without fine-tuning). Measure whether hallucination mitigation persists when visual contexts and reasoning patterns differ from training distribution.