---
ver: rpa2
title: 'LLMs as High-Dimensional Nonlinear Autoregressive Models with Attention: Training,
  Alignment and Inference'
arxiv_id: '2602.00426'
source_url: https://arxiv.org/abs/2602.00426
tags:
- training
- learning
- token
- alignment
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a unified mathematical framework for understanding
  large language models (LLMs) as high-dimensional nonlinear autoregressive models
  with attention. The key contribution is presenting LLMs through a coherent mathematical
  formulation that spans pretraining via next-token prediction, alignment training
  methods including RLHF, DPO, RSFT, and RLVR, and autoregressive generation during
  inference.
---

# LLMs as High-Dimensional Nonlinear Autoregressive Models with Attention: Training, Alignment and Inference

## Quick Facts
- arXiv ID: 2602.00426
- Source URL: https://arxiv.org/abs/2602.00426
- Reference count: 14
- Primary result: Presents a unified mathematical framework for LLMs spanning pretraining, alignment, and inference

## Executive Summary
This paper provides a unified mathematical framework for understanding large language models (LLMs) as high-dimensional nonlinear autoregressive models with attention. The key contribution is presenting LLMs through a coherent mathematical formulation that spans pretraining via next-token prediction, alignment training methods including RLHF, DPO, RSFT, and RLVR, and autoregressive generation during inference. The framework reveals self-attention as a repeated bilinear-softmax-linear composition that enables highly expressive sequence modeling. This formulation enables principled analysis of various LLM phenomena including sycophancy, hallucination, in-context learning, chain-of-thought prompting, and retrieval-augmented generation.

## Method Summary
The paper formulates LLMs as Δ-order nonlinear autoregressive models where token probabilities depend on the past Δ tokens through a transformer with attention. The mathematical framework covers three phases: pretraining (next-token prediction via cross-entropy minimization), alignment (RLHF, DPO, RSFT, RLVR as KL-regularized reward optimization), and autoregressive inference (generating tokens via softmax sampling). Self-attention is characterized as bilinear similarity scoring followed by softmax normalization and linear aggregation. KV caching is formalized as storing past key-value projections to enable efficient linear-time inference. The framework enables analysis of LLM phenomena through this unified lens.

## Key Results
- Self-attention emerges naturally as repeated bilinear-softmax-linear composition, yielding highly expressive sequence models
- Alignment methods can be unified under KL-regularized optimization with different reward formulations
- KV caching enables efficient autoregressive inference by reducing computational complexity from quadratic to linear in sequence length
- The mathematical framework enables principled analysis of phenomena including sycophancy, hallucination, in-context learning, and retrieval-augmented generation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Self-attention implements a bilinear similarity kernel followed by softmax normalization and linear state aggregation, enabling context-dependent token interaction regardless of distance.
- **Mechanism:** Each attention head computes attention scores as bilinear functions of hidden states: $a(t,s) = (M_\theta h_{t})^\top (N_\theta h_{s})$, then applies softmax to produce a probability distribution over context positions, and finally computes a weighted sum of value projections. The nonlinearity $\phi$ abstracts residual connections, layer normalization, and feedforward operations.
- **Core assumption:** Token relevance can be captured through learned bilinear similarity in projected subspaces.
- **Evidence anchors:** [abstract] "Self-attention emerges naturally as a repeated bilinear–softmax–linear composition, yielding highly expressive sequence models." [section, p.4] Equation (3) and accompanying text define the attention mechanism explicitly as softmax over bilinear scores with linear aggregation.

### Mechanism 2
- **Claim:** KV caching enables efficient autoregressive inference by storing past key-value projections as a sufficient statistic, reducing computational complexity from quadratic to linear in sequence length.
- **Mechanism:** Due to causal masking, hidden states and attention projections of past tokens are invariant to future tokens. The KV cache $S_t = \bigcup_{\ell=1}^L \{(k_{\ell,s}, v_{\ell,s})\}_{s=1}^t$ stores these projections. At each generation step, only the new query, key, and value are computed; attention uses cached keys and values.
- **Core assumption:** The model's parameters $\theta^*$ remain fixed during inference; only the state evolves.
- **Evidence anchors:** [abstract] "autoregressive generation during inference" described as part of unified framework. [section, p.13-14] "State-Space Recursive Update" defines KV cache and its role in efficient inference.

### Mechanism 3
- **Claim:** Hallucination arises from approximately neutral propagation of semantic deviations during autoregressive rollout, where early factual errors are not strongly corrected by subsequent layers.
- **Mechanism:** Once a low-probability but coherent deviation occurs at step $t$, subsequent predictions are conditioned on the perturbed prefix. The Jacobian of the hidden-state map with respect to earlier tokens remains near-identity in semantically relevant directions, allowing trajectories to drift into coherent but unsupported continuations without strong restoring forces.
- **Core assumption:** Transformers lack explicit corrective dynamics toward factual attractors; residual streams exhibit near-unit singular values in semantic directions.
- **Evidence anchors:** [abstract] "hallucination (factually unsupported generation)" listed as a phenomenon analyzable through the framework. [section, p.15] "Because the transformer map $F_{\theta^*}$ is composed of residual connections and attention mechanisms that are approximately neutral with respect to small semantic perturbations...such deviations tend to propagate rather than being quickly suppressed or corrected."

## Foundational Learning

- **Concept: Softmax and probability distributions**
  - **Why needed here:** The entire generation process samples tokens from softmax-normalized logits; understanding temperature scaling and probability vectors is essential.
  - **Quick check question:** Given logits $[2.0, 1.0, 0.1]$, what is the probability of the first class at $\tau=1$ and $\tau=2$?

- **Concept: Autoregressive models and conditional distributions**
  - **Why needed here:** LLMs are framed as $\Delta$-order nonlinear autoregressive models; the next-token distribution depends on the context window.
  - **Quick check question:** In an autoregressive model, how does the prediction of $x_{t+1}$ depend on earlier tokens? What happens when $t > \Delta$?

- **Concept: KL divergence and regularization**
  - **Why needed here:** Alignment methods (RLHF, DPO) use KL penalties to prevent deviation from reference policies while optimizing rewards.
  - **Quick check question:** Why does the KL term in RLHF prevent "catastrophic drift" from pretrained capabilities?

## Architecture Onboarding

- **Component map:** Token indices → embedding matrix → positional encoding → transformer layers (attention + FFN + LayerNorm) → logits → softmax → token sample
- **Critical path:** Embedding lookup → positional encoding → transformer layers (attention computes query/key/value, softmax, weighted value aggregation) → final hidden state → logits → softmax → token sample
- **Design tradeoffs:**
  - Context window $\Delta$: Larger windows capture longer dependencies but increase memory ($O(Ld\Delta)$) and attention cost ($O(\Delta)$ per token)
  - Temperature $\tau$: Low $\tau$ yields deterministic outputs; high $\tau$ increases diversity but raises hallucination risk
  - Alignment method: RLHF captures nuanced preferences but requires reward model training; DPO is simpler but may be less expressive; RLVR enforces objective correctness but only applies to verifiable tasks
- **Failure signatures:**
  - Hallucination: Coherent but factually unsupported outputs; mitigated by RAG or lower temperature
  - Sycophancy: Excessive agreement with user input even when incorrect; alignment-induced artifact
  - Context overflow: Prompts exceeding $\Delta$ are truncated; early tokens lose influence
  - Catastrophic forgetting: Full fine-tuning on new data degrades prior capabilities
- **First 3 experiments:**
  1. **Trace a single generation step:** Given a 3-token prompt, compute embeddings, run one forward pass, extract logits, apply softmax with $\tau=1$, and sample. Verify probabilities sum to 1.
  2. **Implement KV caching:** Profile inference latency with and without caching on a 100-token generation. Confirm linear vs quadratic scaling.
  3. **Temperature sweep:** Generate completions at $\tau \in \{0.3, 0.7, 1.0, 1.5\}$ for the same prompt. Quantify diversity (e.g., distinct n-gram ratio) and factual accuracy (manual inspection or simple verifier).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can architectural modifications be designed to introduce corrective dynamics that suppress hallucination drift, rather than the approximately neutral propagation observed in standard transformer residual streams?
- **Basis in paper:** [explicit] The paper states transformers "do not impose strong corrective dynamics toward a unique semantic trajectory" and that "residual streams in modern transformers often exhibit near-unit singular values in semantically relevant directions, leading to approximately neutral propagation of early factual errors rather than systematic amplification or rapid correction."
- **Why unresolved:** The mathematical framework characterizes hallucination as dynamical drift, but no architectural interventions are proposed to introduce contractive dynamics toward factually grounded attractors.
- **What evidence would resolve it:** Demonstration of modified attention or residual mechanisms that exhibit Jacobians with contractive properties in semantically relevant directions, reducing Hall(θ*) in verifier-based benchmarks.

### Open Question 2
- **Question:** What is the optimal form of the regularizer Ω(ϑ, θ_trainable^(k)) in continual learning that minimizes catastrophic forgetting while maximizing new knowledge acquisition?
- **Basis in paper:** [inferred] Equation (12) introduces a regularizer Ω without specifying its functional form, noting only that it "penalizes changes that would distort previously learned hidden representations."
- **Why unresolved:** The paper identifies the non-orthogonality of task gradients as the root cause of catastrophic forgetting but does not specify how Ω should be constructed to enforce orthogonality constraints or protect critical representations.
- **What evidence would resolve it:** Theoretical derivation or empirical validation of regularizers that provably maintain performance on earlier corpora D^(1),...,D^(k) while fitting D^(k+1).

### Open Question 3
- **Question:** Can the sycophancy-accuracy tradeoff be formally characterized, and does it admit a Pareto-optimal frontier in the space of alignment objectives?
- **Basis in paper:** [explicit] The paper states that "mitigating sycophancy through constitutional constraints, truthfulness rewards, or debiasing preference data—remains an active area of alignment research" and notes sycophancy arises because "human raters typically prefer responses that are polite, empathetic, non-confrontational, or validating."
- **Why unresolved:** The framework formalizes how preference optimization induces sycophancy (shifting π̂_t toward agreeable tokens), but does not address whether this bias can be removed without degrading other alignment objectives.
- **What evidence would resolve it:** Formal analysis of the multi-objective optimization landscape showing achievable tradeoffs, or demonstration of alignment procedures that simultaneously reduce sycophancy metrics and maintain helpfulness/harmlessness scores.

### Open Question 4
- **Question:** What are the necessary and sufficient conditions (model scale, data distribution, architectural depth) for in-context learning capabilities to emerge in autoregressive transformers?
- **Basis in paper:** [inferred] The paper notes ICL "emerges only at large model scale and was first clearly demonstrated in GPT-3, where zero-shot, one-shot, and few-shot performance improves smoothly with parameter count" but provides no theoretical explanation for this scaling behavior.
- **Why unresolved:** The mathematical framework describes ICL as conditioning on a prompt P with demonstration pairs, yielding p(y|P, x; θ), but does not explain why the implicit task inference capability only emerges above certain scale thresholds.
- **What evidence would resolve it:** Theoretical analysis connecting model capacity (d, L, n_head) to the ability to represent and execute arbitrary input-output mappings from context, or identification of phase transitions in ICL capability as a function of scale.

## Limitations

- The mathematical tractability of alignment methods remains theoretical without empirical validation of their high-dimensional optimization dynamics
- The hallucination mechanism explanation relies on neutral propagation assumptions that may oversimplify complex training dynamics
- The framework lacks extensive empirical validation against real LLM behavior, particularly for the phenomena it claims to analyze

## Confidence

- **High Confidence:** The mathematical formulation of self-attention as bilinear-softmax-linear composition and KV caching efficiency are well-grounded in established transformer theory
- **Medium Confidence:** The KL-regularized optimization framework for alignment methods provides coherent mathematical abstraction but lacks empirical validation
- **Low Confidence:** The hallucination dynamics explanation through neutral propagation lacks empirical support and may oversimplify the phenomenon

## Next Checks

1. **Empirical validation of alignment methods:** Implement RLHF, DPO, RSFT, and RLVR on a common task (e.g., text summarization) and measure whether their empirical behavior aligns with the unified KL-regularized framework prediction. Compare reward trajectories, KL divergence evolution, and final policy characteristics.

2. **Hallucination dynamics experiment:** Generate long completions from a base LLM, then from versions with varying degrees of alignment training (RSFT, RLVR, RLHF). Quantify the propagation of factual errors using a fact-checking oracle. Test whether alignment methods that explicitly ground in verifiable facts (RLVR) reduce neutral propagation as predicted.

3. **Mathematical tractability test:** For a small-scale transformer (e.g., nanoGPT with 2 layers), analytically compute the Jacobian of hidden states with respect to input embeddings at multiple layers. Verify whether the near-unit singular values in semantic directions that enable neutral propagation can be observed in practice.