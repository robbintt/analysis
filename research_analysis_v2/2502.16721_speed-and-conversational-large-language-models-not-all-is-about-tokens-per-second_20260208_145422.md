---
ver: rpa2
title: 'Speed and Conversational Large Language Models: Not All Is About Tokens per
  Second'
arxiv_id: '2502.16721'
source_url: https://arxiv.org/abs/2502.16721
tags:
- llms
- speed
- tokens
- task
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem that existing token-based speed
  metrics for LLMs do not accurately reflect the time needed to complete real-world
  tasks. The authors evaluated five open-weight LLMs of similar sizes (around 7 billion
  parameters) on an NVIDIA A100 GPU using three different tasks: answering multiple-choice
  questions, paraphrasing questions, and providing open answers.'
---

# Speed and Conversational Large Language Models: Not All Is About Tokens per Second

## Quick Facts
- arXiv ID: 2502.16721
- Source URL: https://arxiv.org/abs/2502.16721
- Authors: Javier Conde; Miguel González; Pedro Reviriego; Zhen Gao; Shanshan Liu; Fabrizio Lombardi
- Reference count: 0
- Key outcome: Existing token-based speed metrics do not accurately reflect the time needed to complete real-world tasks

## Executive Summary
This paper challenges the common practice of evaluating LLM speed using tokens-per-second metrics. The authors demonstrate that the time needed to complete tasks does not correlate with token generation speed, showing that models generating fewer output tokens can complete tasks faster despite being slower at generating tokens. The study evaluated five open-weight LLMs on an NVIDIA A100 GPU using three different tasks and found significant discrepancies between tokens-per-second measurements and actual task completion times.

## Method Summary
The researchers evaluated five open-weight models (~7B parameters) on a single NVIDIA A100 (40GB) GPU using 16-bit floating-point precision. They used 660 MMLU benchmark questions with three prompt formats: multiple-choice answering, question paraphrasing, and open-ended answers with explanations. The study measured both tokens-per-second and total task completion time, comparing input and output token counts across different tokenizers. The goal was to assess whether traditional speed metrics accurately reflect real-world performance.

## Key Results
- Models with lower token generation speeds completed tasks faster due to generating fewer output tokens
- LLaMa3-8B and Yi-6B were among the slowest in time-per-token but fastest in task completion
- Token-per-second metrics are unfair when comparing models with different tokenizers
- Gemma was fastest overall not due to generation speed, but because it generated substantially fewer tokens

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task completion time depends on both token generation speed AND the total number of tokens a model produces, not just tokens-per-second.
- Mechanism: Models with lower verbosity (fewer output tokens per task) can outperform faster models that generate more tokens. The paper shows LLaMa3-8B and Yi-6B were among the slowest in time-per-token but fastest in task completion because they produced significantly fewer output tokens.
- Core assumption: The task constraints (prompts) are equivalent across models, and output quality meets task requirements despite varying verbosity.
- Evidence anchors:
  - [abstract] "models that were slow in generating tokens per second were among the fastest in completing tasks due to generating fewer output tokens"
  - [section: Results for Task 1] "The two slowest models in terms of time to generate a token (LLaMa3-8B and Yi-6B) are among the fastest to complete the task... The models that produce fewer tokens, are LLaMa3-8B and Yi-6B"
  - [corpus] No direct corpus support for this specific token-verbosity mechanism; related papers focus on model construction and fine-tuning, not inference speed metrics.

### Mechanism 2
- Claim: Different tokenizers create unfair comparisons when using tokens-per-second as a speed metric.
- Mechanism: Each model uses a different tokenizer that segments text differently. The same input text produces varying token counts across models, making tokens-per-second a non-normalized metric. The paper notes input token differences across models for identical prompts.
- Core assumption: Tokenizer differences are consistent and not task-specific.
- Evidence anchors:
  - [section: Evaluating LLM Speed] "for two LLMs that have different tokenizers, it is not a fair comparison because the number of tokens for the same text are different"
  - [section: Results for Task 1] "The number of input tokens depends only on the tokenizer used by each model... the differences are small with LLaMa3-8B and Gemma using fewer tokens"
  - [corpus] No corpus papers directly address tokenizer effects on speed benchmarking.

### Mechanism 3
- Claim: Model verbosity varies significantly by task type and model, creating unpredictable task completion times.
- Mechanism: For open-ended tasks (Task 3), models produce wildly different output lengths. Gemma was fastest overall not due to generation speed, but because it generated substantially fewer tokens. LLaMa-2-7B was fast at token generation but among slowest for task completion due to verbosity.
- Core assumption: Verbosity is a model property that generalizes across prompts within a task category.
- Evidence anchors:
  - [section: Results for Task 3] "LLaMa-2-7B is fast in generating tokens, but among the slowest in completing the task. Instead, the Gemma model is the fastest because it generates substantially fewer tokens"
  - [section: Conclusion] "the time needed to complete each task does not necessarily correlate to the number of tokens that the LLM can generate per second... more importantly on the lengths of the texts generated by each model for the same task"
  - [corpus] The paper on "Fine-tuning on simulated data outperforms prompting for agent tone of voice" touches on stylistic control, which could influence verbosity, but does not directly address this mechanism.

## Foundational Learning

- Concept: **Autoregressive Token Generation**
  - Why needed here: Understanding that LLMs generate tokens sequentially, so total time = (input tokens × prefill time) + (output tokens × generation time per token).
  - Quick check question: If Model A generates 100 tokens at 50 tokens/sec and Model B generates 50 tokens at 30 tokens/sec, which completes faster?

- Concept: **Tokenization and Vocabulary Size**
  - Why needed here: Different tokenizers split text differently; larger vocabularies may produce fewer tokens per text but have larger embedding matrices affecting inference.
  - Quick check question: Why would the same sentence "The quick brown fox" produce different token counts across LLaMa, Mistral, and Gemma?

- Concept: **KV-Cache and Batched Inference**
  - Why needed here: The paper mentions batch processing for multiple prompts; understanding how KV-cache affects memory and speed is critical for deployment decisions.
  - Quick check question: How does increasing batch size affect memory usage versus throughput?

## Architecture Onboarding

- Component map:
  - Input Processing: Prompt → Tokenizer → Token IDs
  - Prefill Phase: Process all input tokens through model layers (parallelizable)
  - Generation Phase: Autoregressive token-by-token output (sequential bottleneck)
  - Output Processing: Token IDs → Detokenizer → Text
  - Measurement Points: Time-to-first-token (TTFT), tokens-per-second (TPS), total task completion time

- Critical path:
  1. Identify the deployment scenario (single-user latency vs. batched throughput)
  2. Select models with appropriate verbosity characteristics for the task
  3. Benchmark using task-specific completion time, not raw tokens-per-second
  4. Control for generation parameters (temperature, max_tokens) in comparisons

- Design tradeoffs:
  - Verbose models: May provide better explanations but slower task completion
  - Efficient tokenizers: Reduce input token count but may obscure cross-model comparison
  - Quantization (4/8-bit): Reduces memory, may affect generation speed and verbosity

- Failure signatures:
  - High TPS but slow task completion → model is verbose for your use case
  - Large variance in completion times → inconsistent generation lengths
  - Memory OOM with batched requests → KV-cache exceeding GPU memory

- First 3 experiments:
  1. **Baseline Task Timing**: Run 100+ prompts from your actual use case through candidate models, measuring wall-clock time—not tokens/sec—to identify which model is fastest for YOUR task.
  2. **Verbosity Analysis**: For the same prompts, measure output token distribution per model. High variance suggests unpredictable latency; high mean suggests verbosity issues.
  3. **Tokenizer Equivalence Check**: If comparing fine-tuned variants of the same base model, verify they share a tokenizer before trusting token-based metrics for comparison.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed task-oriented benchmarks be standardized to accurately compare LLM speed across diverse real-world scenarios like translation, summarization, and essay writing?
- Basis in paper: [explicit] The authors state, "A potential alternative is to develop speed benchmarks that are focused on specific tasks... such as translation, summarization, question answering or essay writing."
- Why unresolved: While the paper demonstrates the need for such benchmarks using three specific tasks, a standardized suite of datasets and evaluation metrics for these broader categories has not yet been defined or adopted by the community.
- What evidence would resolve it: The creation and validation of a benchmark suite that measures "time to complete task" across the specified diverse scenarios, showing a clear correlation with user-perceived latency.

### Open Question 2
- Question: Does the observed discrepancy between token-generation speed and task-completion time directly correlate with energy consumption?
- Basis in paper: [inferred] The paper notes that "existing token-based speed metrics do not necessarily correlate with the time needed to complete different tasks" and explicitly mentions that "energy dissipation" is a key issue inferred from speed.
- Why unresolved: The study measures time as a proxy for computational cost, but energy consumption involves power dynamics (e.g., memory access vs. compute intensity) that may not scale linearly with task duration or token counts.
- What evidence would resolve it: Empirical data measuring joules consumed (in addition to seconds) for the same models and tasks, determining if models that generate fewer tokens (faster task time) are also more energy-efficient.

### Open Question 3
- Question: Do quantized formats (e.g., 4-bit or 8-bit) alter the relative speed rankings of models when measured by task completion rather than tokens per second?
- Basis in paper: [inferred] The authors assume that "results and insights obtained would be similar when using other formats, for example 8 or 4 bits," but they only evaluated 16-bit floating-point models.
- Why unresolved: Quantization changes the memory footprint and inference latency. Since tokenization (vocabulary size) remains constant but compute profile changes, the balance between generation speed and verbosity might shift for quantized models.
- What evidence would resolve it: Re-running the experiment using 4-bit and 8-bit versions of the evaluated models to see if the "fastest task completion" rankings remain consistent with the 16-bit results.

### Open Question 4
- Question: To what extent do model hyperparameters, specifically temperature, influence the output token count and consequently the task completion time?
- Basis in paper: [explicit] The authors note as a limitation that "the time to complete a task for the same model may be different depending on its parameters, such as temperature."
- Why unresolved: The study likely used fixed parameters, but generative models are sensitive to temperature. Higher temperatures might lead to more verbose (and slower) outputs, disrupting the speed rankings observed.
- What evidence would resolve it: An ablation study showing the variance in output token count and total execution time for the same tasks across a range of temperature settings (e.g., 0.0 to 1.0).

## Limitations

- The study only evaluated five models in the ~7B parameter range, limiting architectural generalizability
- Only a single GPU architecture (A100 40GB) was tested, though this is reasonable given the models' memory requirements
- The effects of generation parameters on verbosity and task completion time weren't explicitly analyzed

## Confidence

- High: The demonstration that tokens-per-second does not correlate with task completion time across the tested models and tasks
- Medium: The claim that model verbosity is the primary driver of this disconnect (could be influenced by generation parameters)
- Low: Generalization to other model sizes, architectures, or deployment scenarios

## Next Checks

1. **Parameter Sensitivity Test**: Re-run the benchmark with varied generation parameters (temperature 0.0, 0.7, 1.5) to determine how much verbosity—and thus task completion time—is parameter-dependent versus model-dependent
2. **Cross-Architecture Validation**: Test the same models on CPU and smaller GPUs to verify if the task-completion vs. tokens-per-second disconnect persists across hardware constraints
3. **Quality-Qualification Check**: Measure output quality for each task (using appropriate metrics) to confirm that faster task completion from lower verbosity isn't achieved at the expense of answer accuracy or completeness