---
ver: rpa2
title: Applying Text Mining to Analyze Human Question Asking in Creativity Research
arxiv_id: '2501.02090'
source_url: https://arxiv.org/abs/2501.02090
tags:
- question
- questions
- creativity
- taxonomy
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper applies NLP to analyze the relationship between question-asking
  and creativity, focusing on three research questions: Can question creativity be
  measured with NLP techniques? How do various question characteristics influence
  creativity?'
---

# Applying Text Mining to Analyze Human Question Asking in Creativity Research

## Quick Facts
- **arXiv ID**: 2501.02090
- **Source URL**: https://arxiv.org/abs/2501.02090
- **Reference count**: 40
- **Primary result**: Text mining reveals that question complexity (via Bloom's Taxonomy) is the most reliable indicator of creative potential, while question topic and timing show no significant effect on creativity.

## Executive Summary
This paper investigates whether NLP techniques can measure and predict question creativity, examining how question characteristics influence creative potential and how questions lead to creative answers. The authors analyze four datasets across four experiments: taxonomy classification, answer-based creativity metrics, topic modeling, and interview timeline analysis. They find that Bloom's Taxonomy classification provides the most promising metric for assessing question creativity, achieving ~80% accuracy, while answer-based metrics show some potential but are less reliable. Question creativity appears influenced by length and content features rather than topic or temporal position in dialogue.

## Method Summary
The study employs RoBERTa-base embeddings with a CNN classifier for Bloom's Taxonomy classification (6 cognitive levels), and four answer-based metrics: Information (cosine distance), Content Uniqueness (bigram overlap), DSI (pairwise semantic distances), and MAD (maximum semantic distance). The authors use three sentence embedding models (DistilRoBERTa, MiniLM, MPNet) and conduct LDA/SeedTopicMine topic modeling and timeline analysis on interview data. The pipeline processes questions through classification, computes creativity metrics on Q-A pairs, and analyzes correlations across dimensions.

## Key Results
- Bloom's Taxonomy classification achieves ~80% test accuracy and serves as the most promising metric for question creativity assessment
- Answer-based metrics show Knowledge-level questions scoring highest on creativity measures, suggesting reliability concerns
- Topic distribution and question timing show no significant effect on creativity metrics across datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bloom's Taxonomy classification serves as a proxy for question creativity, where higher cognitive levels correlate with greater creative potential
- Mechanism: RoBERTa-base embeddings → CNN classifier → 6 taxonomy levels. Higher levels (Analysis, Synthesis, Evaluation) indicate more complex cognitive demands than Knowledge-level questions
- Core assumption: Question complexity and creativity are correlated constructs
- Evidence anchors: 80% test accuracy; related work found question complexity strongly positively related to creativity using Bloom's Taxonomy
- Break condition: If datasets contain predominantly factual questions, taxonomy may not capture creative potential beyond surface complexity

### Mechanism 2
- Claim: Answer-based metrics measure creativity as contextual divergence between question and combined question-answer text
- Mechanism: Information = cosine distance between Q and Q-A embeddings; Content Uniqueness = 1 - (repeated bigrams / total bigrams); DSI = pairwise semantic distances; MAD = maximum semantic distance from any question word to answer words
- Core assumption: Creative answers introduce semantically novel information that measurably shifts conversational context
- Evidence anchors: Knowledge-level questions scored highest on information and content uniqueness metrics; weak direct evidence from corpus
- Break condition: Short questions with long answers inflate metrics regardless of actual creativity

### Mechanism 3
- Claim: Question creativity is influenced by length and content features, not by topic or temporal position in dialogue
- Mechanism: LDA/SeedTopicMine for topic clustering; sequential metric tracking for interview flows
- Core assumption: Creativity resides in question formulation rather than external contextual factors
- Evidence anchors: No significant differences in creativity metrics across topics; topic and timing had no measurable effect on creativity
- Break condition: Small interview sample (n=8) limits generalizability

## Foundational Learning

- **Concept**: Bloom's Taxonomy (cognitive domain)
  - Why needed here: Core classification framework; understanding six levels (Remember → Create) is essential for interpreting experimental results
  - Quick check question: Can you explain why "evaluate" ranks higher than "apply" in the revised taxonomy?

- **Concept**: Sentence embeddings and cosine similarity
  - Why needed here: Three embedding models generate vector representations; cosine distance measures semantic divergence
  - Quick check question: If two questions have cosine similarity of 0.95, are they semantically similar or different?

- **Concept**: Lexical diversity metrics (TTR, D-metric)
  - Why needed here: Baseline complexity measures compared against taxonomy levels; helps distinguish question sophistication
  - Quick check question: Why might TTR be problematic for comparing questions of vastly different lengths?

## Architecture Onboarding

- **Component map**: Preprocessing → Bloom Classifier → Answer Metrics Module → Topic Module → Timeline Analyzer
- **Critical path**: Taxonomy Assessment → Answer-Based Assessment → cross-metric correlation analysis. Bloom classifier must be trained first; answer metrics depend on pre-trained embedders
- **Design tradeoffs**: RoBERTa vs. lighter models (accuracy vs. speed); bigrams vs. higher n-grams (simplicity vs. phrase repetition detection); five heterogeneous datasets (broader coverage vs. inconsistent labeling)
- **Failure signatures**: Knowledge-level dominance suggests ceiling effects; DSI/MAD show no correlation with taxonomy levels; 84% of STA interview data lacks speaker labels
- **First 3 experiments**: 1) Replicate Bloom classifier training; validate ~80% test accuracy 2) Apply classifier to R. Tatman dataset; compute answer-based metrics 3) Run correlation analysis between taxonomy levels and all metrics

## Open Questions the Paper Calls Out

- **Open Question 1**: Can large language models (e.g., GPT-4, Llama) with chain-of-thought prompting outperform Bloom's Taxonomy classification in assessing question creativity?
- **Open Question 2**: What constitutes a valid gold standard for question creativity that could benchmark NLP methods?
- **Open Question 3**: Does question complexity equate to question creativity, or are these distinct constructs requiring separate measurement approaches?
- **Open Question 4**: How reliable are answer-based creativity metrics for questions where answers diverge from the question topic?

## Limitations
- Exam Question Dataset contains predominantly factual, Knowledge-level questions with minimal creative questioning representation
- Answer-based metrics show Knowledge questions paradoxically scoring highest on creativity measures
- Small interview sample (n=8) and incomplete metadata limit generalizability of temporal analysis findings

## Confidence
- **High Confidence**: Bloom's Taxonomy classification as measure of question complexity/cognitive demand
- **Medium Confidence**: Question length and content influence creativity findings
- **Low Confidence**: Answer-based metrics as reliable creativity measures

## Next Checks
1. Collect expert ratings of question creativity on a subset of questions across taxonomy levels to validate correlation between higher taxonomy levels and perceived creativity
2. Conduct ablation studies on answer-based metrics by systematically varying question length while holding answer content constant to isolate length-based artifacts
3. Apply the full pipeline to a corpus specifically curated for creative questioning (e.g., design thinking workshops) to test generalizability of findings