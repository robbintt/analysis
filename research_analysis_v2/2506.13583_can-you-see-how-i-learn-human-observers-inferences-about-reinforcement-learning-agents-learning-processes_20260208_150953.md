---
ver: rpa2
title: Can you see how I learn? Human observers' inferences about Reinforcement Learning
  agents' learning processes
arxiv_id: '2506.13583'
source_url: https://arxiv.org/abs/2506.13583
tags:
- learning
- agent
- inferences
- human
- about
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a novel observational paradigm to study
  how humans infer reinforcement learning (RL) agents'' learning processes from behavior
  alone. Through two experiments with different tasks and RL algorithms, the authors
  identified four recurring themes in human observer inferences: Agent Goals, Knowledge,
  Decision Making, and Learning Mechanisms.'
---

# Can you see how I learn? Human observers' inferences about Reinforcement Learning agents' learning processes

## Quick Facts
- arXiv ID: 2506.13583
- Source URL: https://arxiv.org/abs/2506.13583
- Authors: Bernhard Hilpert; Muhan Hou; Kim Baraka; Joost Broekens
- Reference count: 40
- Primary result: Human observers consistently infer four themes (Goals, Knowledge, Decision Making, Learning Mechanisms) when watching RL agents learn

## Executive Summary
This paper investigates how humans interpret and make sense of reinforcement learning (RL) agents' learning processes through passive observation. Through two experiments with different tasks and RL algorithms, the authors identified four recurring themes in human observer inferences: Agent Goals, Knowledge, Decision Making, and Learning Mechanisms. A confirmatory study with 34 participants validated the framework, showing that all four themes were perceived as highly relevant to understanding agent learning. The findings provide a human-centered understanding of how people make sense of RL agent learning, offering actionable insights for designing interpretable RL systems and improving transparency in Human-Robot Interaction.

## Method Summary
The study used a two-phase approach: exploratory interviews (N=9) followed by a confirmatory questionnaire study (N=34). RL agents were trained on two tasks: a modified 8x8 FrozenLake navigation task using tabular Q-learning, and a PandaGym Push manipulation task using DDPG. Videos were generated from training checkpoints and segmented into early, middle, and late learning phases. Participants watched these videos and responded to open-ended questions about what they observed, followed by theme-specific relevance ratings on a 7-point scale. The analysis used iterative thematic coding to identify and validate the four inference themes.

## Key Results
- Four recurring themes emerged across experiments: Agent Goals, Knowledge, Decision Making, and Learning Mechanisms
- All four themes were rated as highly relevant to understanding agent learning in the confirmatory study
- Observers distinguished between structural assumptions (goals, knowledge) and procedural ones (decision making, learning mechanisms)
- Temporal chunking of videos led to approximately three times more statements than continuous viewing

## Why This Works (Mechanism)

### Mechanism 1: Anthropomorphic Mental Model Formation
When observing complex agent behavior, observers project human-like structure onto RL agents to make algorithmic learning interpretable. This reduces cognitive complexity by mapping unfamiliar processes onto familiar categories like goals and knowledge.

### Mechanism 2: Structural vs Procedural Inference Distinction
Observers spontaneously separate what the agent "has" (goals, knowledge) from what it "does" (decision making, learning mechanisms). This decomposition mirrors how humans understand each other's cognition.

### Mechanism 3: Temporal Chunking Enriches Inference Differentiation
Segmenting learning trajectories into phases supports richer, more differentiated observer inferences. Chunked presentation enables comparison across phases, helping observers detect change and form expectations incrementally.

## Foundational Learning

### Concept: Reinforcement Learning Fundamentals
Why needed: The paper assumes familiarity with RL agents, exploration vs. exploitation, reward functions, and policy learning. Without this, the observer inference task makes little sense.
Quick check: What does a Q-learning agent update after receiving a reward, and how does this differ from DDPG's actor-critic architecture?

### Concept: Mental Models in Human-AI Interaction
Why needed: The core contribution is characterizing the mental model observers form when watching RL agents. Understanding mental model theory is prerequisite.
Quick check: What are two ways a user's mental model of an agent could diverge from its actual implementation, and what risks does each create?

### Concept: Qualitative Thematic Analysis
Why needed: The methodology relies on iterative thematic coding of open-ended responses. Readers must understand how themes are extracted, refined, and validated.
Quick check: What safeguards prevent thematic analysis from simply reflecting the analyst's preconceptions rather than participant meanings?

## Architecture Onboarding

### Component Map
Stimulus Generation Pipeline -> Observation Interface -> Inference Elicitation Module -> Analysis Pipeline

### Critical Path
1. Select RL algorithm(s) and task environment(s) with clearly observable behavior changes across learning
2. Generate and chunk stimulus videos that show genuine learning progress (verified by reward curves)
3. Pilot question phrasing to avoid leading responses (as done with RL expert in Experiment 1)
4. Recruit participants with appropriate screening (attention checks, language fluency)
5. Collect and anonymize responses before thematic coding to reduce bias
6. Validate themes via independent relevance ratings and temporal consistency checks

### Design Tradeoffs
- Exploratory vs. Confirmatory: Exploratory interviews yield rich, undirected data but small N; confirmatory questionnaires scale up but constrain responses
- Anthropomorphic vs. Neutral Framing: Question phrasing may encourage anthropomorphism; Experiment 2 reduced this but still found anthropomorphic themes
- Split vs. Continuous Video: Split chunks yield more statements but may disrupt holistic perception; trade-off depends on granularity vs. coherence priority

### Failure Signatures
- Theme Saturation Failure: If open-ended responses cluster into fewer than four themes, stimulus may lack behavioral richness
- Learning Progress Mismatch: If perceived learning progress doesn't track actual reward curves, either the stimulus doesn't show visible improvement or the task framing is confusing
- Over-Anthropomorphism Alert: If observers exclusively attribute human reasoning without acknowledging exploration/trial-and-error, inference may be confounded with projection

### First 3 Experiments
1. Replicate with novices only: Remove RL-exposed participants entirely to test whether themes require any domain knowledge
2. Add interactive condition: Allow observers to provide feedback mid-learning; compare inference themes and accuracy to passive-observation baseline
3. Test misalignment scenarios: Intentionally design agents with reward structures that diverge from obvious task goals; measure whether observer goal inferences misalign and how this affects teaching intentions

## Open Questions the Paper Calls Out

### Open Question 1
To what extent do human observers' inferred themes (Goals, Knowledge, Decision Making, Learning Mechanisms) align or diverge from the agent's actual internal representations and policy structure? This remains unexplored as the current study focused exclusively on capturing the human observer's perspective without comparing these inferences against the ground truth of the agent's algorithmic state.

### Open Question 2
How do the identified inference themes shift when users transition from passive observation to active teaching in interactive settings? The experiments deliberately isolated observation from interaction, leaving the dynamic where the user's feedback shapes the agent unexplored.

### Open Question 3
Can a formal metric of human-centric interpretability be developed based on the four-theme framework to evaluate agent transparency? The current findings are qualitative or based on subjective relevance ratings, lacking a standardized, quantitative scale for "interpretability" grounded in these specific themes.

## Limitations
- The study relies on open-ended thematic analysis, introducing subjectivity in theme extraction and subcluster definitions without full coding manuals
- The sample size for the confirmatory study (N=34) is modest for establishing generalizability
- The exclusive use of expert-generated agents may bias toward cleaner, more interpretable learning trajectories than real-world deployments would show

## Confidence
- High confidence in the relevance of all four themes (Goals, Knowledge, Decision Making, Learning Mechanisms) to human observers' sensemaking
- Medium confidence in the anthropomorphic mechanism claim, as direct evidence is correlational rather than experimental
- Low confidence in the temporal chunking mechanism, as no corpus papers address this effect

## Next Checks
1. Inter-rater reliability test: Have independent coders apply the thematic analysis scheme to a subset of responses to verify consistency
2. Novel agent type test: Run the observational paradigm with agents trained under non-obvious reward structures to test goal inference misalignment
3. Expert vs. novice comparison: Replicate the confirmatory study with participants screened to exclude any RL exposure