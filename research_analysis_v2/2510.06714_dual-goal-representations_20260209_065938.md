---
ver: rpa2
title: Dual Goal Representations
arxiv_id: '2510.06714'
source_url: https://arxiv.org/abs/2510.06714
tags:
- goal
- learning
- representation
- dual
- representations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces dual goal representations for goal-conditioned
  reinforcement learning (GCRL), where goals are represented as functions of temporal
  distances from all other states. The method theoretically satisfies sufficiency
  (contains enough information for optimal policies) and noise invariance (discards
  exogenous noise).
---

# Dual Goal Representations

## Quick Facts
- **arXiv ID:** 2510.06714
- **Source URL:** https://arxiv.org/abs/2510.06714
- **Reference count:** 16
- **Primary result:** Dual goal representations achieve best average performance across 20 OGBench tasks, with particular effectiveness in navigation and manipulation tasks.

## Executive Summary
This paper introduces dual goal representations for goal-conditioned reinforcement learning (GCRL), where goals are represented as functions of temporal distances from all other states. The method theoretically satisfies sufficiency (contains enough information for optimal policies) and noise invariance (discards exogenous noise). A practical implementation uses parameterized temporal distance functions trained via goal-conditioned IQL, with the goal head serving as the dual representation for downstream GCRL policies. Experiments on 20 diverse OGBench tasks show consistent improvements over five prior representation learning methods, with robust performance even with noisy goal observations.

## Method Summary
The approach learns a goal representation $\phi(g)$ that encodes a goal as its temporal distance to all states via $\phi(g) \approx f(\psi(s), \phi(g)) = \psi(s)^\top \phi(g)$. First, dual representations are pre-trained using Goal-Conditioned IQL to approximate temporal distances from offline data. Then, a downstream GCRL policy is trained using the frozen goal representation as input. The method decouples representation learning from policy training, theoretically ensuring both sufficiency for optimal goal-reaching and invariance to exogenous noise.

## Key Results
- Dual representations achieve best average performance across 20 OGBench tasks compared to five prior representation learning methods
- Robust performance in navigation and manipulation tasks, even with noisy goal observations
- Particularly effective in antmaze and cube manipulation tasks where other methods struggle
- Consistent improvements across three different downstream GCRL algorithms (GCIVL, CRL, GCFBC)

## Why This Works (Mechanism)

### Mechanism 1: Policy Sufficiency via Temporal Relations
Representing a goal by its temporal distances to all other states theoretically retains sufficient information to derive an optimal goal-reaching policy. The dual representation $\phi^\vee(g)$ encodes $g$ as the functional $s \mapsto d^*(s, g)$. An optimal policy can be recovered by selecting actions that maximize the expected value of this functional at the next state. The core assumption is that the agent can accurately approximate the optimal temporal distance function from offline data.

### Mechanism 2: Noise Invariance via Latent Dynamics Filtering
Dual representations can discard exogenous noise (visual distractors, irrelevant state features) because they depend only on the intrinsic dynamics required to reach the goal. By defining the representation via $d^*$ (which is a function of reachability), observation features that do not affect transition dynamics become irrelevant. The core assumption is that the environment can be modeled as an Exogenous Block CMP where observations with identical latent dynamics have disjoint supports.

### Mechanism 3: Structured Separation of Representation and Policy
Decoupling the learning of the goal representation (via value approximation) from the learning of the downstream policy improves performance over end-to-end alternatives. The method first optimizes $\phi(g)$ to predict temporal distances, then a downstream policy learns in this simplified space. The core assumption is that a separate, unconstrained value function is better for policy extraction than direct use of the distance function.

## Foundational Learning

- **Temporal Distance / Log-Value Functions**: Needed to understand that the "distance" is derived from the optimal value function. Quick check: How does one transform the output of a Q-function trained on sparse rewards into a temporal distance?
- **Goal-Conditioned IQL (Implicit Q-Learning)**: Used to learn the temporal distance function offline. Quick check: In IQL, do we regress the value function onto the Q-function directly, or do we use an expectile loss?
- **Inner Product vs. Metric Parameterization**: The paper argues for bilinear (inner product) form over metric learning norms. Quick check: Why might an inner product formulation be more "universal" for approximating arbitrary two-variable functions than a symmetric distance metric?

## Architecture Onboarding

- **Component map:** State Encoder $\psi(s)$ -> Distance Head $f = \psi(s)^\top \phi(g)$ -> Goal Encoder $\phi(g)$ -> Downstream Policy $\pi(a|s, \phi(g))$
- **Critical path:** Pre-train $\psi, \phi$ using GC-IQL on offline data, discard $\psi$ and Q-function, keep only $\phi$, then train downstream policy using $s$ and $\phi(g)$ as inputs.
- **Design tradeoffs:** Use Inner Product for better expressivity despite Metric learning being more intuitive for "distance." For pixel-based tasks, this method forces "late fusion" (separate encoding of $s$ and $g$), which caused failure in visual puzzles.
- **Failure signatures:** Visual Puzzle Tasks (0% performance due to late fusion architecture), Direct Policy Extraction (worse than separate downstream learner).
- **First 3 experiments:** 1) Implement Lights Out experiment using exact BFS distances to verify theoretical sufficiency. 2) Compare $\psi^\top \phi$ vs $||\psi - \phi||^2$ on state-based navigation. 3) Add Gaussian noise to goal observations in antmaze and compare Dual vs. Original representations.

## Open Questions the Paper Calls Out

### Open Question 1
Can modifying the representation function to be state-aware overcome the limitations of late fusion in visual tasks? Section 6.2.2 identifies "late fusion" as the cause of failure in visual puzzle tasks and suggests future work should "modify the representation function to be aware of the current state." This remains unresolved because current methods decouple state and goal processing, failing to capture necessary relational features for complex visual puzzles.

### Open Question 2
Is the inner product parameterization universally sufficient for direct policy extraction without a separate downstream GCRL phase? Table 5 and Section 6.3 show that extracting a policy directly from the temporal distance function degrades performance, suggesting the chosen parameterization is too constrained despite being theoretically universal. This remains unresolved because while the inner product form is theoretically universal, it is observed to be "relatively less expressive" for policy extraction than a monolithic value function.

### Open Question 3
How do the sufficiency and noise invariance guarantees extend to continuous state spaces? Section 2 states that theoretical results assume discrete spaces "for the sake of simplicity" and claims results "can be extended to continuous state spaces with appropriate modifications," implying this is not formally proven. This remains unresolved because the proofs rely on discrete settings, leaving formal guarantees for continuous environments unproven.

## Limitations

- Visual task performance drops to 0% on visual puzzles due to late-fusion architecture that prevents fine-grained pixel alignment
- Noise invariance critically depends on Ex-BCMP assumption holding in practice - fails if exogenous features affect transition dynamics
- Theoretical sufficiency guarantees for continuous state spaces remain unproven despite method being applied to continuous domains

## Confidence

- **High Confidence:** Theoretical sufficiency and noise invariance claims under stated assumptions (Theorems 3.1, 3.2)
- **Medium Confidence:** Empirical superiority over baselines on OGBench tasks (controlled experiments, but potential unreported ablations)
- **Low Confidence:** Generalization to environments with dynamic exogenous noise or tasks requiring early fusion of visual observations

## Next Checks

1. **Robustness Test:** Add exogenous noise that affects dynamics (e.g., visual clutter that physically blocks movement) to verify if noise invariance breaks
2. **Architecture Ablation:** Compare inner product vs. metric parameterizations on a visual puzzle task to quantify the late-fusion limitation
3. **Transfer Experiment:** Evaluate Dual Representations on a new OGBench task (e.g., quadruped-warehouse) not in the original 20 to assess generalization