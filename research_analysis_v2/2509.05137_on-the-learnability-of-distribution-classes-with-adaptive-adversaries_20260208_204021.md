---
ver: rpa2
title: On the Learnability of Distribution Classes with Adaptive Adversaries
arxiv_id: '2509.05137'
source_url: https://arxiv.org/abs/2509.05137
tags:
- adversary
- adaptive
- vsub
- adversaries
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the learnability of distribution classes
  in the presence of adaptive adversaries, who can intercept and manipulate samples
  with full knowledge of the learner's algorithm and the ground-truth distribution.
  While previous work showed that realizable learnability implies robustness to oblivious
  additive adversaries, the authors demonstrate that this does not extend to adaptive
  adversaries.
---

# On the Learnability of Distribution Classes with Adaptive Adversaries

## Quick Facts
- arXiv ID: 2509.05137
- Source URL: https://arxiv.org/abs/2509.05137
- Reference count: 40
- Primary result: Realizable learnability does not imply robustness to adaptive adversaries; a separation exists between oblivious and adaptive robustness.

## Executive Summary
This paper establishes a fundamental separation between learnability under oblivious and adaptive adversaries in distribution learning. While realizable learnability and oblivious robustness were previously shown to be equivalent, the authors prove this equivalence breaks down for adaptive adversaries. They construct a specific distribution class that is realizably learnable but not robustly learnable under adaptive additive or subtractive adversaries, even with constant budget increases. The key insight is that adaptive adversaries are strictly more powerful than their oblivious counterparts, effectively possessing the same destructive capability regardless of whether they add or remove data points.

## Method Summary
The paper proves a separation by constructing a distribution class C_g where realizable learnability relies on observing rare indicator events that can be trivially suppressed by adaptive adversaries. The method involves (1) defining a class of distributions p_{i,j,k} with a tiny indicator component, (2) constructing a subtractive adversary that preferentially removes these indicators within budget constraints, and (3) showing this confusion technique prevents reliable learning. The authors then demonstrate that an adaptive additive adversary can achieve the same confusion by inverting the subtractive strategy, effectively bridging the gap between adding and deleting data points under adaptive control.

## Key Results
- Realizable learnability does not imply robustness to adaptive adversaries
- Adaptive additive and subtractive adversaries have equivalent power
- The constructed class C_g is realizably learnable but not robustly learnable under any constant budget adaptive adversary
- A separation exists between oblivious and adaptive robustness in distribution learning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Adaptive additive adversaries are strictly more powerful than oblivious additive adversaries, effectively possessing the same destructive capability as adaptive subtractive adversaries.
- **Mechanism:** The authors demonstrate an "inversion" technique. If a subtractive adversary can successfully remove points from sample S to make it look like sample S', an additive adversary can achieve the same confusion by adding the specific points that *would have been removed* from a different distribution to the target sample. This bridges the gap between adding and deleting data points under adaptive control.
- **Core assumption:** The adversary has full knowledge of the learner's algorithm and the ground-truth distribution p (adaptive setting), and the budget η is constant.
- **Evidence anchors:**
  - [abstract] "They construct a class of distributions... key technique involves showing that if a subtractive adversary can successfully confuse samples... an additive adversary can be constructed."
  - [section 6] "We show that given an adaptive subtractive adversary, we can construct an adaptive additive adversary by inverting the subtractive adversary."
  - [corpus] *Robust Learnability of Sample-Compressible Distributions* (Neighbor) discusses robustness under perturbations, providing context for why distinguishing additive/subtractive noise matters.
- **Break condition:** This equivalence holds specifically for **adaptive** adversaries. In the **oblivious** setting, additive and subtractive robustness are distinct (Table 1).

### Mechanism 2
- **Claim:** A class of distributions C is not robustly learnable if an adversary can make samples from two distinct distributions in C statistically indistinguishable (low Total Variation distance) while the true distributions remain far apart (high Total Variation distance).
- **Mechanism:** The "Confusion" technique (Theorem 5.2). If an adversary V transforms samples such that d_TV(V(p^m), V(q^m)) < ζ (indistinguishable) but d_TV(p, q) > γ (distinct truths), no learner can reliably output the correct ground truth with error less than γ/2.
- **Core assumption:** There exists a meta-distribution Q over the class C where all members are sufficiently distant from a target p ∈ C.
- **Evidence anchors:**
  - [section 5] Definition 5.1 defines "successfully confusing" samples; Theorem 5.2 formalizes the lower bound on error.
  - [appendix a.1] The proof shows that if samples are indistinguishable, any learner must fail on at least one of the distributions.
- **Break condition:** If the learner is allowed infinite samples or if the adversary's budget is too small to bridge the distance γ between distributions, the mechanism fails.

### Mechanism 3
- **Claim:** Realizable learnability can rely on rare "indicator" events that are trivially suppressible by an adaptive adversary with even a small budget.
- **Mechanism:** The paper utilizes a specific distribution class C_g where identifying the distribution relies on observing a unique "indicator" bit (probability ≈ 1/g(j)). An adaptive subtractive adversary V_{sub, η} targets these indicators. Since indicators are rare, the adversary rarely needs to delete many points to remove all indicators, making the remaining sample look generic.
- **Core assumption:** The function g grows superlinearly (g(j) ≫ j), ensuring the indicator probability is vanishingly small compared to the main distribution mass.
- **Evidence anchors:**
  - [section 4] Definition of C_g and p_{i,j,k}; the learner "only needs to observe a unique indicator bit."
  - [section 5.1] Proof that V_{sub, η} successfully confuses C_g by bounding the probability of repeated elements and indicator survival.
- **Break condition:** If the indicator probability was high (linear g), the adversary would exhaust its budget trying to delete all indicators, failing the attack.

## Foundational Learning

- **Concept: Total Variation (TV) Distance**
  - **Why needed here:** This is the primary metric for both the "error" of the learner (how close the estimate is to the truth) and the "power" of the adversary (how close corrupted samples are to clean ones).
  - **Quick check question:** If two distributions have a TV distance of 0.1, what is the maximum probability that a statistical test can distinguish them? (Answer: 0.5 + 0.1/2 = 0.55).

- **Concept: i.i.d. (Independent and Identically Distributed) Assumption**
  - **Why needed here:** Standard PAC learning relies on samples being i.i.d. The critical distinction in this paper is that **adaptive adversaries** break the i.i.d. nature of the sample set (by conditioning deletions/additions on observed points), whereas **oblivious adversaries** do not.
  - **Quick check question:** Does an oblivious adversary change the distribution of the *second* sample point based on the value of the *first* sample point? (Answer: No).

- **Concept: Realizable vs. Agnostic Learning**
  - **Why needed here:** The paper demonstrates a separation. A class might be learnable in the "realizable" case (truth is in class C) and even robust to "oblivious" noise, but fail in the "agnostic" equivalent created by an adaptive adversary.
  - **Quick check question:** In the realizable setting, is the ground truth distribution guaranteed to be a member of the hypothesis class C? (Answer: Yes).

## Architecture Onboarding

- **Component map:** Learner <-> Sample Stream <-> Adaptive Adversary
- **Critical path:** Implementing the "Confusion" test (Theorem 5.2). You must verify that for your constructed class and adversary, d_TV(V(p^m), V(|Q|^m)) is small.
- **Design tradeoffs:**
  - **Budget vs. Structure:** The adversary's budget η must be carefully balanced with the probability of the indicator bit. If the budget is too low, indicators survive; if too high, the attack is trivial.
  - **Strong vs. Weak Adversary:** The main theorem holds even for adversaries that do *not* know the ground truth p (only the sample), making the negative result stronger.
- **Failure signatures:**
  - **Learner Crash:** The learner outputs a distribution p' that has d_TV(p', p) > αη (exceeds the error tolerance).
  - **Adversary Failure:** The learner successfully identifies the "indicator" bit in the sample, meaning the adversary failed to scrub/add enough noise.
- **First 3 experiments:**
  1. **Verify Realizable Learnability:** Implement the simple learner for C_g (scan for indicators) and confirm it succeeds on clean samples.
  2. **Verify Subtractive Confusion:** Implement V_{sub, η}. Run samples from two different distributions in C_g through the adversary. Measure the TV distance of the resulting *samples*. Confirm they are statistically close.
  3. **Construct Universal Additive Adversary:** Implement the "Inversion" logic from Section 7. Use the subtractive strategy to generate points to *add* to a sample. Verify this additive attack prevents learning just as effectively as the subtractive one.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does a single class of distributions exist that is not learnable for all continuous, monotonically increasing robustness functions f simultaneously?
- **Basis in paper:** [explicit] Appendix C, paragraph following the proof of Theorem C.4, states: "Whether this is the case remains an open question (for each of the following versions of robustness: adaptive Additive, adaptive subtractive and oblivious subtractive)."
- **Why unresolved:** The paper proves that for any given function f, a specific class C exists that is not f-robustly learnable, but it does not establish if a single universal class can serve as a counterexample for every such function.
- **What evidence would resolve it:** A proof constructing a specific class C that fails robust learnability for all valid functions f, or a proof showing that different functions necessitate distinct counterexample classes.

### Open Question 2
- **Question:** Does the separation between adaptive and oblivious additive adversaries hold for natural, unbounded distribution classes?
- **Basis in paper:** [inferred] Page 3 contrasts the authors' "somewhat contrived" construction with Canonne et al. (2023), who showed a similar gap for "a natural problem" in hypothesis testing, implying the question remains for learning natural classes.
- **Why unresolved:** The paper's main construction (C_g) is explicitly designed to be difficult to learn and is described as contrived, leaving the behavior of standard distribution classes (e.g., Gaussians) under these specific adaptive attacks undetermined.
- **What evidence would resolve it:** A proof of separation using a standard distribution class (e.g., Gaussians or other parametric families) or a proof that natural classes are universally robust to adaptive additive attacks.

### Open Question 3
- **Question:** Is a dependence on the domain size unavoidable for the reduction from adaptive to oblivious adversaries to hold?
- **Basis in paper:** [explicit] Page 3 notes that previous work by Blanc et al. requires a "larger dataset" with size depending on the support. The paper states: "Thus our results imply that some dependence on the domain size is unavoidable for this reduction to go through for the task of distribution learning."
- **Why unresolved:** The paper focuses on distributions with unbounded support where the reduction fails, but it does not characterize the precise relationship or lower bounds on the domain size dependence for finite domains.
- **What evidence would resolve it:** A formal lower bound proving that any general reduction strategy must depend on the domain size, or an algorithm that achieves the reduction without this dependence.

## Limitations

- The separation result relies on a specifically constructed distribution class that is designed to be difficult to learn
- The proof uses conservative bounds that may not be tight in practice
- The result is limited to Total Variation distance as the error metric
- The indicator-based attack may not generalize to other distribution classes

## Confidence

- **High Confidence:** The theoretical framework and proof structure are sound. The mechanisms (inversion technique, confusion argument, indicator suppression) are mathematically rigorous and well-established.
- **Medium Confidence:** The numerical bounds and concrete parameter choices (like g(j) = j², specific η values) are conservative and likely valid, but their tightness and practical implications are uncertain.
- **Low Confidence:** The impact of different enumeration schemes for finite subsets and the generalizability of the result to other distribution classes or distance metrics are unclear without further analysis.

## Next Checks

1. **Empirical TV Distance Verification:** Implement the distribution class C_g with g(j) = j² and compute the empirical TV distance between samples from p and Q after applying the subtractive adversary. Verify the bound d_TV < 1/8 holds in practice.
2. **Enumeration Scheme Sensitivity:** Test multiple enumeration schemes for the finite subsets {B_i} and measure the resulting TV distances. Determine if the separation result is robust to this choice.
3. **Budget-Indicator Tradeoff Analysis:** Systematically vary the adversary budget η and the indicator probability 1/k to identify the precise threshold where the attack succeeds or fails. Compare empirical results to theoretical predictions.