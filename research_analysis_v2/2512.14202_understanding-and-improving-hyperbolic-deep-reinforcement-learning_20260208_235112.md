---
ver: rpa2
title: Understanding and Improving Hyperbolic Deep Reinforcement Learning
arxiv_id: '2512.14202'
source_url: https://arxiv.org/abs/2512.14202
tags:
- hyperbolic
- hyper
- learning
- euclidean
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of training stable and performant
  hyperbolic deep reinforcement learning agents. By analyzing the gradients of core
  hyperbolic operations, the authors identify that large-norm embeddings destabilize
  training, leading to trust-region violations in PPO.
---

# Understanding and Improving Hyperbolic Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2512.14202
- Source URL: https://arxiv.org/abs/2512.14202
- Reference count: 40
- Primary result: HYPER++ outperforms prior hyperbolic agents on ProcGen, reduces wall-clock time by ~30%, and achieves strong gains on Atari-5 with DDQN.

## Executive Summary
This paper addresses the challenge of training stable and performant hyperbolic deep reinforcement learning agents. By analyzing the gradients of core hyperbolic operations, the authors identify that large-norm embeddings destabilize training, leading to trust-region violations in PPO. To mitigate this, they propose HYPER++, a new hyperbolic PPO agent with three key components: (1) a categorical value loss for stable critic training, (2) feature regularization via RMSNorm and learned scaling to bound norms without the curse of dimensionality, and (3) an optimization-friendly formulation of hyperbolic network layers using the Hyperboloid model. On ProcGen, HYPER++ outperforms prior hyperbolic agents, reduces wall-clock time by ~30%, and achieves strong gains on Atari-5 with DDQN.

## Method Summary
HYPER++ combines three innovations to stabilize hyperbolic deep RL: (1) a categorical value loss (HL-Gauss) that matches the geometric properties of hyperbolic MLR layers, (2) RMSNorm + learned scaling applied to the final encoder layer to bound embedding norms while preserving representational capacity, and (3) a Hyperboloid model formulation that avoids conformal factor instability. The method is evaluated on ProcGen with PPO and Atari-5 with DDQN, showing significant performance improvements over previous hyperbolic agents while reducing training time by approximately 30%.

## Key Results
- HYPER++ outperforms prior hyperbolic agents on ProcGen benchmark
- Achieves ~30% reduction in wall-clock training time compared to spectral normalization baselines
- Demonstrates strong performance gains on Atari-5 with DDQN (off-policy) in addition to PPO (on-policy)
- Stabilizes training by preventing embedding norm explosion and trust-region violations

## Why This Works (Mechanism)

### Mechanism 1: Gradient Explosion from Large-Norm Embeddings in Hyperbolic Operations
Large-norm Euclidean embeddings destabilize hyperbolic layer gradients, causing PPO trust-region violations. The conformal factor λc_x = 2/(1−c∥x∥²) in the Poincaré Ball amplifies gradients as embeddings approach the manifold boundary (∥x∥ → 1/√c). The exponential map Jacobian for both Poincaré Ball and Hyperboloid contains terms that grow with ∥xE∥, with the Hyperboloid exhibiting O(cosh(√c∥xE∥)) growth. PPO's clipped objective constrains ratios only on sampled states; large gradient updates can cause unintended policy shifts on unseen states.

### Mechanism 2: RMSNorm + Learned Scaling Bounds Embeddings Without Dimensionality Curse
Applying RMSNorm before the final encoder activation guarantees bounded hyperbolic operations while preserving representational capacity. RMSNorm bounds ∥x̂∥₂ < 1/√d(∥f(0)∥₂ + L), which is dimension-independent for activations with fixed point 0. Learned scaling (sigmoid-gated) then expands usable manifold volume by (α/0.76)^d (e.g., ~1200× for d=32, α=0.95). Unlike SpectralNorm, which constrains every encoder layer, RMSNorm applied only to the final layer suffices.

### Mechanism 3: Categorical Value Loss Matches Hyperbolic MLR Geometry
HL-Gauss categorical loss stabilizes critic training better than MSE regression in hyperbolic agents. Hyperbolic MLR layers output signed hyperplane distances, which are classification-oriented. Categorical loss over discrete bins is a better geometric fit than continuous MSE regression, reducing gradient variance from nonstationary targets. While Euclidean linear layers naturally support MSE regression over continuous values, hyperbolic MLR layers output classification-oriented hyperplane distances, making the categorical loss over discrete bins a better geometrical fit.

## Foundational Learning

- **Hyperbolic geometry and exponential volume growth**: Hyperbolic space has exponential volume growth (vs polynomial in Euclidean), making it suited for tree-structured/hierarchical data from RL state spaces. Quick check: Can you explain why a chess game tree has exponential branching and how this relates to the choice of embedding space?

- **Riemannian manifolds, tangent spaces, and exponential maps**: The paper maps Euclidean encoder outputs to hyperbolic space via exp₀: T₀M → M. Understanding this operation is essential for grasping why norm bounds matter. Quick check: What happens to the gradient of the exponential map as the input tangent vector norm increases?

- **PPO trust region and clipped surrogate objective**: The paper diagnoses hyperbolic PPO failures through trust-region violations (KL divergence, clip fraction). Understanding PPO's heuristic trust region is prerequisite. Quick check: Why might constraining importance sampling ratios on sampled states fail to prevent large policy changes on unseen states?

## Architecture Onboarding

- **Component map**: Encoder (Impala-ResNet) → RMSNorm → TanH → Learned scaling → Exponential map → Hyperboloid embedding → MLR heads (policy/value)

- **Critical path**: 1) Encoder output → RMSNorm (bounds norms, critical for stability) 2) RMSNorm output → TanH → Learned scaling (recovers volume) 3) Scaled output → exp₀ → Hyperboloid embedding 4) Hyperboloid embedding → MLR heads → policy/value outputs 5) Critic: categorical cross-entropy with HL-Gauss targets

- **Design tradeoffs**:
  - Hyperboloid vs Poincaré Ball: Hyperboloid has no conformal factor (more stable) but requires d+1 dimensional representation; Poincaré is conformal but has λc instability. Paper empirically prefers Hyperboloid.
  - RMSNorm vs SpectralNorm: RMSNorm applies only to final layer (less restrictive, faster), SpectralNorm requires all layers (more expressive cost, power iteration overhead). ~30% wall-clock improvement with RMSNorm.
  - Categorical vs MSE loss: Better for hyperbolic geometry but adds hyperparameters (bins, min/max clip).

- **Failure signatures**:
  - Entropy collapse + high clip fraction: Indicates unregularized hyperbolic agent
  - Exploding conformal factor (Poincaré): Indicates embeddings approaching boundary
  - Complete learning failure with zero gradients: Missing RMSNorm
  - SpectralNorm failure: Applying SN only to penultimate layer doesn't bound norms; applying to all layers over-constrains expressivity

- **First 3 experiments**:
  1. Validate RMSNorm necessity: Train HYPER++ with RMSNorm removed on BigFish; expect embedding norm explosion, gradient vanishing, and reward collapse. Compare to full HYPER++.
  2. Ablate geometric model: Swap Hyperboloid for Poincaré Ball (same RMSNorm+scaling); expect modest performance drop but stable training. This isolates the conformal factor effect.
  3. Transfer to off-policy: Apply the same regularization (RMSNorm + scaling + categorical loss) to DDQN on Atari-5 (NameThisGame); verify performance gains transfer beyond PPO (as shown in Figure 9).

## Open Questions the Paper Calls Out
- Which specific environment characteristics determine whether hyperbolic representations provide benefits over Euclidean ones in deep RL?
- How do geometric model choices (Poincaré Ball vs. Hyperboloid) interact with different deep RL algorithm architectures beyond PPO and DDQN?
- What hierarchical or relational structures do trained hyperbolic embeddings actually capture, and do they correspond to meaningful environment structure?

## Limitations
- The empirical claim that RMSNorm alone suffices for norm control (vs SpectralNorm) rests on activation-specific analysis but lacks robustness checks across different encoder architectures.
- The geometric intuition for categorical loss superiority in hyperbolic spaces is compelling but untested on non-hierarchical tasks where Euclidean embeddings might suffice.
- While the Hyperboloid model is shown empirically stable, ablation comparing Poincaré Ball with equivalent regularization would strengthen the geometric design choice.

## Confidence
- High confidence in the gradient instability mechanism due to formal derivation and diagnostic plots
- Medium confidence in RMSNorm + scaling efficacy—strong ablations but limited to Impala architecture
- Medium confidence in categorical loss benefits—geometric justification holds, but numerical comparisons with MSE show mixed results

## Next Checks
1. Ablate RMSNorm dependency: Train on BigFish without RMSNorm; verify embedding norm explosion and learning collapse as predicted
2. Cross-geometry ablation: Swap Hyperboloid for Poincaré Ball with identical regularization; measure impact on gradient stability and performance
3. Cross-algorithm transfer: Apply the same RMSNorm + scaling + categorical loss pipeline to a non-PPO agent (e.g., IMPALA) on ProcGen; test if gains generalize beyond PPO-specific trust-region dynamics