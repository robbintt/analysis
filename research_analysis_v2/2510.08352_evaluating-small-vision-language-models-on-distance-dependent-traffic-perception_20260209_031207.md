---
ver: rpa2
title: Evaluating Small Vision-Language Models on Distance-Dependent Traffic Perception
arxiv_id: '2510.08352'
source_url: https://arxiv.org/abs/2510.08352
tags:
- performance
- vlms
- visual
- perception
- traffic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Small vision-language models struggle with perception in traffic
  scenes, particularly at long distances and for spatial reasoning tasks like distinguishing
  left from right. We introduce DTPQA, the first VQA benchmark focused solely on perception-based
  questions in traffic scenes with distance annotations, to evaluate these models.
---

# Evaluating Small Vision-Language Models on Distance-Dependent Traffic Perception

## Quick Facts
- arXiv ID: 2510.08352
- Source URL: https://arxiv.org/abs/2510.08352
- Reference count: 40
- Primary result: Small VLMs underperform humans (~60% vs ~85% accuracy) on traffic perception tasks, with performance degrading notably at long distances

## Executive Summary
Small vision-language models (under 4B parameters) exhibit significant limitations in traffic scene perception, particularly for distance-dependent tasks and spatial reasoning. This study introduces DTPQA, the first visual question answering benchmark focused exclusively on perception-based questions in traffic scenes with distance annotations. Testing 9 state-of-the-art small VLMs on DTPQA reveals substantial performance gaps compared to human baselines, with accuracy dropping significantly as object distance increases. While some tasks like traffic sign recognition match human performance, spatial reasoning tasks such as left/right discrimination remain especially challenging, with models performing near chance accuracy even on simple directional questions.

## Method Summary
The study evaluates 9 small VLMs (<4B parameters) on DTPQA, a new benchmark with 19,149 samples from CARLA (synthetic) and nuScenes (real) datasets. Samples are categorized by question type and annotated with object distances (5-50m). Models are evaluated using greedy decoding with a strict single-word answer prompt format. Performance is measured via macro-averaged accuracy across distance bins, compared against human baselines (~85%), and analyzed for distance-dependent degradation patterns and sensitivity to question rephrasing.

## Key Results
- Small VLMs achieve ~60% average accuracy versus ~85% for humans on DTPQA
- Performance degrades linearly with distance for most question types (5-50m range)
- Spatial reasoning tasks (left/right discrimination) show near-chance performance (~50%) across all models
- Slight question rephrasing can cause >10 percentage point accuracy swings
- Some tasks (traffic sign recognition) match human performance while others fail dramatically

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Small VLMs exhibit distance-dependent perception degradation due to visual feature loss at long range.
- Mechanism: As object distance increases, fewer pixels encode the target object, and vision encoder downsampling compounds information loss before features reach the LLM.
- Core assumption: The vision encoder's fixed resolution and patch-based processing limits fine-grained detail preservation for distant objects.
- Evidence anchors:
  - [abstract] "performance degrading notably as object distance increases"
  - [Section VI-A-2] "For some question types, the performance of most models drops almost linearly with distance (Cat.1-Synth, Cat.3-Synth, Cat.5-Synth, Cat.1-Real, and Cat.3-Real)"
  - [corpus] Related work on fine-grained VLM perception (CropVLM, Decomposing Complex Visual Comprehension) confirms perception limitations persist across VLMs, but distance-specific degradation is not extensively studied elsewhere.

### Mechanism 2
- Claim: Spatial reasoning (left/right discrimination) fails because visual embeddings lack explicit spatial structure and LLMs attend more to text than image embeddings.
- Mechanism: VLMs process spatial relationships through flattened patch embeddings without explicit 2D coordinate encoding, forcing the LLM to infer spatial relations indirectly from semantic features.
- Core assumption: Spatial reasoning requires explicit positional encoding that current projector designs do not preserve.
- Evidence anchors:
  - [abstract] "spatial reasoning tasks like distinguishing left from right" remain "especially challenging"
  - [Section VI-A-1] "questions that require distinguishing left from right seem to be the most difficult... even for InternVL3-78B, which performs around chance accuracy"
  - [corpus] Chen et al. (attention mechanism analysis) found smoothing/sharpening attention can improve spatial reasoning—suggesting attention distribution is part of the mechanism, but this is not tested on small VLMs.

### Mechanism 3
- Claim: Question rephrasing sensitivity emerges from misalignment between visual feature embeddings and text token embeddings for semantically equivalent queries.
- Mechanism: The same visual features may have different similarity scores with different word embeddings ("crossing" vs "traversing"), causing retrieval of different visual tokens during generation.
- Core assumption: Visual-linguistic alignment in small VLMs is insufficiently robust to synonym variations.
- Evidence anchors:
  - [abstract] "Even slight rephrasing of questions can affect model performance"
  - [Section VI-B] "replacing 'crossing' with 'traversing'... consistently improves performance across all models" while "replacing 'traffic light' with 'stoplight'... slightly degrades performance"
  - [corpus] SEAM benchmark evaluates cross-modal semantic consistency but does not specifically address synonym sensitivity in small VLMs.

## Foundational Learning

- Concept: **Vision Encoder → Projector → LLM Pipeline**
  - Why needed here: Understanding where perception degrades (encoder resolution, projector compression, or LLM attention) is essential for targeted improvements.
  - Quick check question: Given a 1920×1080 image processed through a ViT with 448×448 tiles, how many patches represent a pedestrian at 50 meters vs 5 meters?

- Concept: **Perception vs Reasoning Isolation in Benchmarks**
  - Why needed here: DTPQA isolates perception by excluding reasoning-heavy questions, enabling precise diagnosis of where VLMs fail.
  - Quick check question: If a model answers "What color is the traffic light?" correctly but fails "Is the pedestrian to the left or right?", is this a perception or reasoning failure?

- Concept: **Chance Accuracy and Balanced Benchmarks**
  - Why needed here: DTPQA balances answer classes at each distance to prevent models from exploiting linguistic biases.
  - Quick check question: A 3-class classification task with 33.3% chance accuracy shows model accuracy of 40%—what fraction of samples demonstrates meaningful understanding?

## Architecture Onboarding

- Component map: Image → Tiling/Resizing → Vision Encoder Patches → Projector → Visual Token Sequence → Concatenation with Text Tokens → LLM Processing → Answer Token

- Critical path: Image → Tiling/Resizing → Vision Encoder Patches → Projector → Visual Token Sequence → Concatenation with Text Tokens → LLM Processing → Answer Token

- Design tradeoffs:
  - RoPE positional embeddings (Qwen models) vs tiling with thumbnails (InternVL, Ovis): flexibility vs computational cost
  - Large projector (Ovis: 300M+ params) vs minimal MLP (Aquila: 4M params): visual embedding richness vs parameter efficiency
  - MoE LLM (DeepSeek-VL2-Tiny) vs dense LLM: sparse activation vs consistent compute

- Failure signatures:
  - **Distance blindness**: Accuracy drops >20 percentage points from 5m to 50m (Cat.2-Real: all models near chance at all distances)
  - **Left/right confusion**: Performance near chance (50%) on directional questions regardless of model size
  - **Negative sample bias**: Models default to "No" or "Zero" when uncertain at long distances (Cat.1-Synth, Cat.3-Synth drop below chance)
  - **Rephrasing volatility**: >10 percentage point accuracy swing from single-word synonym changes

- First 3 experiments:
  1. **Baseline DTPQA evaluation**: Run each target VLM on DTP-Synthetic and DTP-Real across all 6 distance bins; compute macro-averaged accuracy per category and plot degradation curves (Figure 6 style)
  2. **Spatial reasoning probe**: Isolate left/right discrimination (Cat.2-Synth, Cat.4-Synth) and test whether increasing vision encoder resolution (via adaptive tiling) improves performance
  3. **Rephrasing robustness test**: Create 3 variants per question as in Table 3; measure accuracy variance across variants to quantify prompt sensitivity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can mechanistic interpretability techniques identify exactly where and why visual information is "lost" in small VLMs during failure cases, particularly for spatial reasoning?
- Basis in paper: [explicit] The authors state in Section VII that they "did not address the question of why these models fail" and suggest future work use "attention visualization, probing experiments, and activation patching" to trace visual information flow.
- Why unresolved: The current study is purely evaluative; it establishes that performance drops at distance or during spatial tasks but does not analyze internal model states or component interactions to explain the root cause.
- What evidence would resolve it: A study mapping visual feature degradation across specific layers (projector vs. LLM) to identify bottlenecks that cause failures in long-range or directional perception tasks.

### Open Question 2
- Question: How does the modification of prompt *structures* (as opposed to simple question rephrasing) affect the reliability of small VLMs on perception tasks?
- Basis in paper: [explicit] In Section V-B, the authors note they "leave the study of the effect of prompt structure modifications on model performance for future work," having only tested variations of the question text itself.
- Why unresolved: The paper demonstrates that rephrasing questions affects performance, but it maintains a constant prompt format; the sensitivity of these models to the broader structural context of the input remains unknown.
- What evidence would resolve it: A comparative evaluation on DTPQA using diverse prompt templates (e.g., Chain-of-Thought prompting vs. direct instruction) to measure variance in accuracy and robustness.

### Open Question 3
- Question: Why do small VLMs with identical vision encoders but different LLM backbones exhibit significantly different performance on trivial perception tasks?
- Basis in paper: [inferred] In Section VI-A-4, the authors observe that Ovis2-2B and Ovis2-1B (which share a vision encoder) perform differently, concluding that "crucial processing of the visual information itself occurs within the LLM."
- Why unresolved: The paper evaluates models as black boxes; it detects the correlation between LLM size/architecture and perception scores but does not isolate the specific mechanism by which the LLM aids or hinders basic visual processing.
- What evidence would resolve it: Ablation studies or probing experiments that isolate the visual feature extraction quality from the LLM's processing to determine if the bottleneck is feature propagation or reasoning capacity.

## Limitations

- Dataset Construction Transparency: The DTPQA benchmark construction relies on nuScenes annotations with specific filtering and distance binning procedures that remain unspecified.
- Generalization Boundary: Results demonstrate performance degradation on controlled synthetic and semi-synthetic data but don't establish whether these limitations generalize to real-world driving scenarios.
- Parameter Efficiency Analysis Gap: The study identifies performance differences across architectures but doesn't systematically explore whether parameter efficiency tradeoffs explain these differences.

## Confidence

**High Confidence**: The core finding that small VLMs underperform humans on perception tasks (~60% vs ~85% accuracy) is well-supported by empirical results across multiple models and benchmark variants.

**Medium Confidence**: The mechanism explanations linking performance degradation to visual feature loss and spatial reasoning failures are plausible but not definitively proven.

**Low Confidence**: The claim that slight rephrasing affects performance due to visual-linguistic misalignment is based on limited examples and could result from tokenization differences or other factors.

## Next Checks

1. **Cross-Benchmark Validation**: Evaluate the same VLMs on independent perception benchmarks (e.g., Visiolinguistic Navigation, CropVLM) to determine whether DTPQA-specific findings generalize to other perception tasks and datasets.

2. **Architectural Ablation Study**: Systematically vary key architectural parameters (vision encoder resolution, projector size, spatial encoding methods) in a controlled VLM to isolate which components most strongly influence distance-dependent performance degradation and spatial reasoning capabilities.

3. **Real-World Deployment Test**: Conduct controlled real-world tests of small VLMs in actual traffic environments, comparing their perception accuracy against both DTPQA benchmark performance and human operators to validate the safety-critical implications claimed in the study.