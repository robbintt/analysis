---
ver: rpa2
title: An Improved Algorithm for Adversarial Linear Contextual Bandits via Reduction
arxiv_id: '2508.11931'
source_url: https://arxiv.org/abs/2508.11931
tags:
- linear
- algorithm
- regret
- bandits
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses adversarial linear contextual bandits with
  stochastic action sets, a challenging online learning setting where action sets
  vary over time. The authors present a novel reduction that transforms this problem
  into misspecification-robust adversarial linear bandits with fixed action sets.
---

# An Improved Algorithm for Adversarial Linear Contextual Bandits via Reduction

## Quick Facts
- arXiv ID: 2508.11931
- Source URL: https://arxiv.org/abs/2508.11931
- Reference count: 21
- Key outcome: First polynomial-time algorithm for adversarial linear contextual bandits achieving poly(d)√T regret without exponential dependence on action set size

## Executive Summary
This paper presents a novel reduction that transforms adversarial linear contextual bandits with stochastic action sets into misspecification-robust adversarial linear bandits with fixed action sets. The key innovation is an empirical estimation of the feasible policy space using uniform convergence bounds over linear policies, combined with a continuous exponential weights algorithm enhanced with an exploration bonus to handle misspecification. The resulting algorithm achieves O(min{d²√T, √(d³TlogK)}) regret without simulator access and O(d√L⋆) regret with simulator access, running in polynomial time poly(d,C,T) where C bounds the number of linear constraints describing action sets.

## Method Summary
The approach reduces contextual bandits to linear bandits by mapping policies to their mean actions under the context distribution, creating a feasible set Ω. The authors construct an empirical approximation bΩ from N samples of action sets and prove uniform convergence over linear policies. They then run a misspecification-robust continuous exponential weights algorithm over bΩ with a carefully designed exploration bonus. The algorithm samples from the estimated feasible set, executes the corresponding policy, and feeds back biased loss estimates to the bandit algorithm, which updates with bonus-adjusted estimators.

## Key Results
- Achieves O(min{d²√T, √(d³TlogK)}) regret without simulator access
- Achieves O(d√L⋆) regret with simulator access
- Runs in polynomial time poly(d,C,T) where C bounds linear constraints
- First polynomial-time algorithm not explicitly depending on action set size K
- Enables efficient learning in combinatorial settings with exponentially large K

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The contextual bandit problem can be reduced to linear bandits by mapping policies to their mean actions under the context distribution.
- Mechanism: Define Ψ(π) = E[A∼D][π(A)], mapping each policy to its expected action. This creates a feasible set Ω = {Ψ(π) | π ∈ Π}. The expected loss of playing policy π_t becomes ⟨Ψ(π_t), θ_t⟩, transforming the problem into linear bandits over Ω. Critically, the optimal policy is a linear classifier π_ϕ(A) = argmin_{a∈A} ⟨a, ϕ⟩, so it suffices to consider linear policies.
- Core assumption: Action sets are drawn i.i.d. from a fixed distribution D; losses are linear in action features.
- Evidence anchors:
  - [Section 3]: "if the learner draws a_t from policy π_t in round t, the expected regret may be written as E[⟨Ψ(π_t) − Ψ(π), θ_t⟩]"
  - [Section 2.1]: Shows optimal policy is attained by linear classifier policies Π_lin
  - [corpus]: Related work on stochastic linear bandits confirms reduction approach is theoretically grounded
- Break condition: If contexts are adversarial (not i.i.d.), the Ψ-mapping loses meaning; the reduction fails.

### Mechanism 2
- Claim: The true feasible set Ω can be approximated by empirical samples with uniform convergence over linear policies.
- Mechanism: Construct bΩ from N samples A_1,...,A_N ∼ D where bΩ = {(1/N)∑_i a_i : a_i ∈ conv(A_i)}. Use Rademacher complexity and Natarajan dimension to show sup_ϕ |⟨Ψ(π_ϕ) − Ψ̂(π_ϕ), θ⟩| ≤ O(√(d log(NK)/N)). This bounds the misspecification error when using bΩ instead of Ω.
- Core assumption: Action set size bounded by K; linear policies have Natarajan dimension ≤ d.
- Evidence anchors:
  - [Lemma 1, Section 3.1]: Uniform convergence bound with explicit d, K, N dependence
  - [Appendix C]: Reduction to multiclass classification via Natarajan dimension
  - [corpus]: Limited direct evidence for this specific uniform convergence technique in contextual bandits
- Break condition: If K is unbounded, log K diverges. If optimal policy is far from linear, uniform convergence over linear policies is insufficient.

### Mechanism 3
- Claim: Adding an exploration bonus to continuous exponential weights enables robustness to constant misspecification in loss feedback.
- Mechanism: The algorithm maintains distribution q_t over bΩ, samples y_t, receives biased loss c_t where |E[c_t] − ⟨y_t, θ_t⟩| ≤ ε. The bonus b_t = 8η(ε + 1/T²)∑_{τ<t}(θ̂_τ − b_τ) creates negative regret offsetting misspecification, achieving O(√d·ε·T) overhead instead of O(εT). The bonus encourages exploration where misspecification could mislead.
- Core assumption: Misspecification level ε is known; bΩ is a polytope with poly(d, C) constraints.
- Evidence anchors:
  - [Algorithm 2, Section 3.4]: Explicit bonus definition with γ = 10 log(10dT), β = T^{-4}
  - [Theorem 2]: Proves √d-misspecification-robustness
  - [corpus]: Corpus shows limited direct evidence for this specific bonus mechanism
- Break condition: If ε is unknown or varies significantly, the fixed bonus is mis-specified. If action sets cannot be described by poly(C) constraints, efficiency is lost.

## Foundational Learning

- Concept: **Linear Bandits with Exponential Weights**
  - Why needed here: The base algorithm builds on continuous exponential weights; understanding FTRL with entropic barriers is essential.
  - Quick check question: Can you explain why exponential weights achieve O(d√T) regret in linear bandits, and what role the entropic barrier plays?

- Concept: **Uniform Convergence and Rademacher Complexity**
  - Why needed here: Empirical estimation of Ω relies on uniform convergence; the proof uses Natarajan dimension.
  - Quick check question: What is the Natarajan dimension of linear multiclass classifiers with K classes, and how does it relate to uniform convergence rates?

- Concept: **Misspecification in Bandits**
  - Why needed here: The core innovation is handling biased feedback; understanding how misspecification degrades standard algorithms is crucial.
  - Quick check question: In standard linear bandits, if loss estimates are biased by ε, how does regret scale without robustness mechanisms?

## Architecture Onboarding

- Component map:
  - **Empirical Feasible Set Constructor** -> **Misspecification-Robust Linear Bandit Core** -> **Policy Executor**
  - Takes N action set samples, constructs polytope bΩ with O(NC) constraints
  - Continuous exponential weights with bonus term, runs in poly(d, C, T)
  - Decomposes y_t into vertices of bΩ, maps to linear classifier policies

- Critical path:
  1. Sample N contexts from D (or simulator if available)
  2. Construct bΩ by solving N linear programs for constraint descriptions
  3. Each round: query ALG for y_t, decompose into vertices via Carathéodory, sample vertex ψ_t, find interior point of -N(bΩ, ψ_t) via LP with O(NC) constraints, execute argmin_{a∈A_t} ⟨a, ϕ_t⟩
  4. Return loss ℓ_t to ALG which updates with bonus-adjusted estimator

- Design tradeoffs:
  - **Regret vs. Computation**: d²√T regret (vs. optimal d√T) is the price of unknown D; simulator access recovers d√L*
  - **Sample size N vs. Accuracy**: Larger N reduces misspecification ε ∝ √(d log(NK)/N) but increases oracle complexity
  - **Bonus strength vs. Exploration**: Stronger bonus handles more misspecification but increases regret variance

- Failure signatures:
  - Regret scales linearly with T: Verify ε is actually O(√(log(NK)/N)), not larger; check doubling trick implementation
  - Runtime exceeds poly(d, C, T): Confirm conv(A_i) describable by C constraints; spanning trees violate this
  - Sampled policy doesn't match target y_t: Debug vertex decomposition and normal cone interior point computation (Lines 5-6, Algorithm 1)

- First 3 experiments:
  1. **Validate reduction on known distribution**: Set D uniform over {0,1}^d action sets, verify bΩ → Ω and ε → 0 as N grows; measure empirical misspecification
  2. **Test misspecification robustness**: Inject controlled bias ε into loss feedback, confirm regret degrades as O(√d·ε·T) not O(εT); compare to non-robust baseline
  3. **Combinatorial stress test**: Run on shortest path (d edges, K = 2^Ω(d), C = O(d)), verify runtime poly(d, T) not exponential in K; track regret scaling with d

## Open Questions the Paper Calls Out

- Can the computation cost be improved to match that of Neu and Valko (2014), requiring only a linear optimization oracle for each action set without needing a polynomial number of constraints?
- Can the near-optimal Õ(d√T) regret bound be achieved with a polynomial-time algorithm without simulator access?
- Can poly(d)√L⋆ first-order regret be achieved without simulator access?
- Can the discretization approach of Hanna et al. (2023) be applied purely in analysis without restricting learner policies to discretized linear policies?

## Limitations

- The uniform convergence analysis relies on bounding the Natarajan dimension but lacks extensive empirical validation for high-dimensional action sets
- The misspecification robustness mechanism requires knowing the misspecification level ε in advance, which may be impractical in real deployments
- The reduction framework assumes action sets are efficiently describable by poly(C) linear constraints, excluding problems like shortest path with exponentially many constraints

## Confidence

- Mechanism 1 (Reduction framework): High confidence - well-established theoretical foundation with clear proofs
- Mechanism 2 (Empirical feasible set estimation): Medium confidence - rigorous uniform convergence bounds but limited empirical validation for large K
- Mechanism 3 (Misspecification robustness): Medium confidence - theoretical guarantee but untested with unknown/adaptive misspecification levels

## Next Checks

1. Implement a controlled experiment injecting varying levels of misspecification ε into the loss feedback to empirically verify the O(√d·ε·T) regret bound degrades as predicted
2. Test the algorithm on shortest path bandit instances where K = 2^Ω(d) but action sets are implicitly described, measuring actual runtime scaling with d
3. Construct action set distributions where optimal policies are far from linear classifiers to stress-test the uniform convergence bounds and identify break points