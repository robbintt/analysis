---
ver: rpa2
title: Token and Span Classification for Entity Recognition in French Historical Encyclopedias
arxiv_id: '2506.02872'
source_url: https://arxiv.org/abs/2506.02872
tags:
- entity
- entities
- nested
- lower
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates NER approaches on 18th-century French encyclopedia
  texts, testing classical CRF, spaCy CNN/span models, Flair BiLSTM-CRF, CamemBERT
  transformers, and few-shot GPT prompting. The GeoEDdA dataset contains 2,200 annotated
  paragraphs with flat and nested entities.
---

# Token and Span Classification for Entity Recognition in French Historical Encyclopedias

## Quick Facts
- arXiv ID: 2506.02872
- Source URL: https://arxiv.org/abs/2506.02872
- Reference count: 7
- This paper benchmarks NER approaches on 18th-century French encyclopedia texts, finding transformer models achieve highest performance (F1 up to 98% on frequent classes) while few-shot LLMs show promise for low-resource settings.

## Executive Summary
This paper evaluates six NER approaches on the GeoEDdA dataset of 18th-century French encyclopedia texts containing 2,200 annotated paragraphs with flat and nested entities. The study compares classical CRF models, spaCy CNN/span models, Flair BiLSTM-CRF, CamemBERT transformers, and few-shot GPT prompting. Token-level and span-level formulations are tested to handle overlapping annotations. Results show transformer-based models (CamemBERT) achieve state-of-the-art performance, particularly on nested entities, while few-shot LLMs demonstrate competitive results on well-represented entity types but underperform on rare, nested entities.

## Method Summary
The study benchmarks six NER architectures on the GeoEDdA dataset: CRF with three feature configurations, spaCy NER (token-level CNN) and Spancat (span-level), Flair BiLSTM-CRF with French embeddings, CamemBERT transformers using both joint-label and multi-label strategies, and few-shot GPT prompting. The dataset contains 12 entity types including spatial, person, and nested variants. Joint-label encoding is used to handle overlapping entities by merging them into composite labels. Models are evaluated using F1, precision, and recall metrics with micro/macro averaging across entity types.

## Key Results
- CamemBERT achieves highest performance with F1 up to 98% on frequent entity classes
- Few-shot GPT prompting with minimal examples (1-2 shots) shows competitive results on well-represented types but underperforms on rare, nested entities
- Span-level classification (Spancat) achieves higher recall on nested entities but lower precision than token-level approaches
- Joint-label encoding improves precision on overlapping entities but creates label sparsity for rare composites

## Why This Works (Mechanism)

### Mechanism 1: Deep Bidirectional Context Captures Archaic Language Patterns
- Claim: Transformer self-attention mechanisms enable superior handling of non-standardized historical orthography by modeling full-sequence context rather than local windows.
- Mechanism: CamemBERT's self-attention allows each token to attend to all other tokens in the sequence, capturing long-range dependencies that help disambiguate archaic spellings and non-standard syntax. This contrasts with CRF/CNN approaches that rely on local context windows or sequential processing.
- Core assumption: Historical French orthography follows learnable patterns that can be captured through contextual relationships across the full sequence, not just adjacent tokens.
- Evidence anchors:
  - [abstract]: "transformer-based models achieve state-of-the-art performance, especially on nested entities"
  - [Section 4.3, Table 5]: CamemBERT achieves micro-avg F1 of 91.2-91.8% vs CRF's 88.6%, with particularly strong gains on complex entities (ENE-Spatial: 92.2-92.6% vs 89.9%)
  - [corpus]: GapDNER paper (FMR=0.65) similarly finds grid-based tagging improves discontinuous entity recognition—suggesting structural modeling matters for complex entity patterns.
- Break condition: If your historical corpus has predominantly short, non-nested entities with consistent modern spelling, transformer overhead may not justify marginal gains over BiLSTM-CRF.

### Mechanism 2: Joint-Label Encoding Reduces Overprediction on Nested Entities
- Claim: Converting overlapping entity combinations into single composite labels improves precision by modeling interdependencies between entity types explicitly.
- Mechanism: Instead of predicting each entity type independently (which can produce inconsistent combinations), joint-label classification creates a single expanded label space where "NP-Spatial+ENE-Spatial" is one atomic class. This forces the model to learn valid co-occurrence patterns.
- Core assumption: Valid entity combinations follow learnable patterns; invalid combinations are sufficiently rare that expanding the label space doesn't create prohibitive sparsity.
- Evidence anchors:
  - [Section 3.2]: "Overlapping entities are merged into composite labels (e.g., 'NP-Spatial+ENE-Spatial'), allowing the problem to be recast as a flat classification task"
  - [Section 4.3]: "Joint-label model showed slightly better precision... suggesting that the joint-labeling approach may help CamemBERT better model interdependencies between overlapping entity types and avoid overprediction"
  - [corpus]: EIoU-EMC paper (FMR=0.62) addresses nested NER class imbalance with novel loss functions—suggesting label space design is an active research area for nested entities.
- Break condition: When rare entity combinations have insufficient training examples (e.g., ENE-Misc with only 255 instances), joint-labeling creates sparse composite classes that underperform multi-label alternatives.

### Mechanism 3: Few-Shot Prompting Leverages Pretrained World Knowledge for Low-Resource Settings
- Claim: Generative LLMs with minimal in-context examples can achieve competitive NER performance by leveraging pretrained knowledge about entity types, reducing need for task-specific training data.
- Mechanism: GPT models encode entity patterns from massive pretraining corpora. With just 1 labeled example and structured JSON output format, the model retrieves relevant entity recognition capabilities without gradient updates.
- Core assumption: The pretraining distribution sufficiently covers historical French entity patterns and JSON-structured output can be reliably elicited through prompting.
- Evidence anchors:
  - [Section 4.5]: GPT-o4-mini achieves 75.6% micro-avg precision and 70.5% recall with only 1-shot prompting, competitive with fine-tuned spaCy NER (83.2% F1)
  - [Section 5]: "Few-shot prompting with generative LLMs emerges as a promising alternative for low-resource or zero-shot settings"
  - [corpus]: Ground Truth Generation for Multilingual Historical NLP (FMR=0.59) uses LLMs for historical French/Chinese annotation—suggesting generative models have emerging utility for historical NLP.
- Break condition: Rare entity types (ENE-Misc: 11.2% F1) and nested structures remain challenging; few-shot underperforms fine-tuned transformers by 15-20 F1 points on underrepresented classes.

## Foundational Learning

- **Named Entity Recognition (NER) as Sequence Labeling**:
  - Why needed here: All architectures in this paper (CRF, BiLSTM-CRF, Transformers) build on the fundamental framing of NER as assigning entity labels to token sequences.
  - Quick check question: Can you explain why CRFs model "conditional probability of an entire label sequence" rather than independent token predictions?

- **Nested/Overlapping Entity Problem**:
  - Why needed here: The GeoEDdA dataset explicitly supports 3 levels of entity nesting; standard BIO tagging cannot represent overlapping spans like "ville d'Espagne" where tokens belong to multiple entities simultaneously.
  - Quick check question: Given the phrase "ville d'Espagne" annotated as both [NC-Spatial: ville] and [ENE-Spatial: ville d'Espagne], why can't standard BIO tagging represent this?

- **Token-Level vs Span-Level Classification**:
  - Why needed here: The paper compares spaCy NER (token-based CNN) vs spaCy Spancat (span-based scoring)—fundamentally different formulations with distinct precision/recall tradeoffs.
  - Quick check question: Why might span-level classification achieve higher recall (57.4% micro-avg) but lower precision (58.5%) compared to token-level approaches?

## Architecture Onboarding

- **Component map**:
  - Input: 18th-century French text → tokenizer (model-specific)
  - Feature extraction: Varies by architecture (hand-crafted features for CRF, CNN embeddings for spaCy, contextual embeddings for Flair/BERT)
  - Sequence modeling: CRF (Viterbi decoding), BiLSTM (Flair), Transformer attention (CamemBERT), or in-context learning (GPT)
  - Output layer: Token classifier (most models), span scorer (Spancat), or generative decoder (GPT)
  - Post-processing: Joint-label decoding or multi-label aggregation for nested entities

- **Critical path**:
  1. Choose task formulation: Joint-label (higher precision) vs multi-label (higher recall) based on entity overlap patterns
  2. Select architecture based on constraints: CamemBERT for best performance, Flair for resource efficiency, GPT for zero-shot prototyping
  3. Handle nested entities: Either flatten via joint encoding or use span-based models like Spancat

- **Design tradeoffs**:
  - CamemBERT joint-label: Best precision on frequent classes (99.1% on Domain-mark), but struggles with rare composites (40% on ENE-Misc)
  - Flair BiLSTM-CRF: Competitive performance (86.2 macro F1) with lighter compute, but limited long-range dependency modeling
  - GPT few-shot: Fastest to deploy, best for bootstrapping annotations, but inconsistent on rare classes and requires API access
  - spaCy Spancat: Highest recall on nested entities but precision drops on ambiguous classes

- **Failure signatures**:
  - CRF with dependency features: Adding syntactic features decreased performance (Table 3, config 3 vs 2)—indicates shallow parsing introduces noise for historical texts
  - spaCy NER on nested entities: ENE-Person recall of 2.5% (Table 4)—token-level CNN cannot capture hierarchical entity structure
  - Open-weights LLMs (Llama 3-70b, Mistral 7B): "Consistently very poor performance" due to insufficient instruction following for structured output
  - GPT-4o: High precision (77.5% micro) but severely low recall (33.4%)—overly conservative predictions

- **First 3 experiments**:
  1. Baseline comparison: Run CRF with lexical features only (config 1) vs CamemBERT multi-label on a held-out subset to quantify transformer gains for your specific historical domain
  2. Joint-label vs multi-label ablation: On CamemBERT, compare precision/recall tradeoffs for your most frequent vs rarest entity types to select output strategy
  3. Few-shot scaling test: With GPT-4o-mini, test 1-shot vs 3-shot vs 5-shot prompting to establish whether performance plateaus or continues improving with more context examples

## Open Questions the Paper Calls Out
- Can hybrid approaches combining symbolic rule-based reasoning with neural models significantly outperform purely neural methods on nested and rare entity types in historical French texts?
- Does specialized pretraining on historical French corpora improve NER performance over models pretrained on modern French text?
- Why do open-weights LLMs (Llama 3-70b, Mistral 7B) fail so dramatically on structured NER tasks compared to proprietary GPT models, and can this gap be closed?
- How does increasing the number of few-shot examples beyond one affect LLM performance on rare entity classes in historical NER?

## Limitations
- Limited dataset size (2,200 paragraphs) may not capture full diversity of 18th-century French encyclopedic writing styles and vocabulary
- Unreported hyperparameters (learning rates, batch sizes, epochs, seeds) make exact reproduction difficult
- Few-shot experiments use specific prompt templates and JSON schemas that are not fully detailed

## Confidence
- **High confidence**: Transformer-based models achieving superior performance on frequent entity classes; clear quantitative gap between token-level and span-level approaches for nested entities
- **Medium confidence**: Claims about few-shot prompting being "promising for low-resource settings" - while results show competitive performance on well-represented classes, the severe underperformance on rare and nested entities (ENE-Misc at 11.2% F1) suggests more nuanced applicability
- **Medium confidence**: The joint-label encoding strategy improving precision - evidence shows slightly better precision but also creates label sparsity issues for rare composites that are not fully addressed

## Next Checks
1. **Ablation study on label encoding**: Compare joint-label vs multi-label output strategies on CamemBERT specifically for rare entity types (ENE-Misc, ENE-Other) to quantify the precision-recall tradeoff and identify when each approach breaks down
2. **Few-shot scaling analysis**: Systematically test 1-shot vs 3-shot vs 5-shot prompting with GPT-4o-mini on the same held-out set to determine whether performance plateaus or continues improving with more context examples, and whether this differs by entity class frequency
3. **Cross-domain generalization test**: Apply the best-performing CamemBERT model to a different historical French corpus (e.g., 17th-century texts or different encyclopedia series) to assess whether performance gains transfer beyond the GeoEDdA dataset