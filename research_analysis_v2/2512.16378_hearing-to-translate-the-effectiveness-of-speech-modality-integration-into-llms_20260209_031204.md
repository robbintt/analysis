---
ver: rpa2
title: 'Hearing to Translate: The Effectiveness of Speech Modality Integration into
  LLMs'
arxiv_id: '2512.16378'
source_url: https://arxiv.org/abs/2512.16378
tags:
- tower
- speech
- gemma3
- translation
- whisper
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents Hearing to Translate, the first comprehensive
  benchmark suite evaluating the effectiveness of integrating speech modality into
  Large Language Models (LLMs) for speech-to-text translation. The study compares
  five state-of-the-art SpeechLLMs against 16 strong direct and cascaded systems across
  13 language pairs, 16 benchmarks, and 9 challenging conditions (disfluencies, noise,
  accents, code-switching, gender bias, named entities, emotion, and long-form speech).
---

# Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs

## Quick Facts
- arXiv ID: 2512.16378
- Source URL: https://arxiv.org/abs/2512.16378
- Reference count: 40
- This work presents the first comprehensive benchmark suite evaluating speech modality integration into LLMs for speech-to-text translation.

## Executive Summary
This paper introduces Hearing to Translate, a benchmark suite evaluating Speech-to-Text Translation (ST) across three paradigms: SpeechLLMs (audio→LLM→translation), Cascaded systems (SFM→LLM), and Standalone Speech Foundation Models (SFMs). The study compares five state-of-the-art SpeechLLMs against 16 strong direct and cascaded systems across 13 language pairs, 16 benchmarks, and 9 challenging conditions. Results show cascaded architectures remain most reliable overall, while SpeechLLMs only match cascades in selected settings like noise, code-switching, and disfluencies. Human evaluation confirms these trends and highlights common error patterns, validating the use of automatic quality estimation metrics for system-level comparisons.

## Method Summary
The study evaluates 21 systems (5 SpeechLLMs, 8 cascades, 8 SFMs) across 16 benchmarks covering 13 language pairs and 9 conditions including noise, accents, disfluencies, long-form speech, code-switching, gender bias, and named entities. Systems are evaluated using reference-free Quality Estimation (QE) metrics xCOMET and METRICX due to limited availability of reference translations in speech benchmarks. A strict pipeline filters off-target language generation using LINGUAPY. Human evaluation validates the QE results through pairwise comparisons of 40 samples per condition.

## Key Results
- Cascaded architectures (ASR → LLM translation) outperform direct SpeechLLMs for most speech translation tasks
- SpeechLLMs achieve parity with cascades specifically in noise, code-switching, and disfluency scenarios
- Standalone Speech Foundation Models lag behind both paradigms, indicating that LLM integration is essential for high-quality translation

## Why This Works (Mechanism)

### Mechanism 1
Cascaded architectures outperform direct SpeechLLMs because they separate acoustic processing (handled by specialized speech foundation models) from linguistic reasoning (handled by text-trained LLMs). This modularity allows each component to leverage large-scale specialized training data. Error propagation from ASR to MT is less damaging than the representation bottleneck in end-to-end models.

### Mechanism 2
SpeechLLMs achieve parity with cascades in noise, code-switching, and disfluency scenarios because direct audio access allows them to leverage prosodic and acoustic cues that are discarded in cascaded transcription. The integration adapter successfully maps acoustic representations to the LLM's semantic space without significant information loss.

### Mechanism 3
Phenomenon-specific robustness is determined by different pipeline components—encoder for accents, decoder for gender bias and named entities. Accent robustness depends on speech encoder training diversity; gender bias and entity accuracy depend on the LLM's text-based training priors and translation specialization.

## Foundational Learning

- **Cascaded vs. Direct Speech Translation Architecture**: Understanding the tradeoffs between these paradigms is essential for interpreting results. Quick check: Can you explain why cascades suffer error propagation but still outperform direct models?

- **Speech Foundation Models (SFMs)**: Whisper, SeamlessM4T, Canary, and OWSM form the backbone of both cascaded and SpeechLLM systems evaluated. Quick check: What role does the speech encoder play versus the LLM in a SpeechLLM?

- **Quality Estimation (QE) Metrics**: The paper uses xCOMET QE and METRICX QE because most speech benchmarks lack reference translations. Quick check: Why would reference-based metrics be problematic for evaluating speech translation systems?

## Architecture Onboarding

- **Component map**: Audio input → Speech encoder → Adapter → LLM embedding space → Translation output. For cascades: Audio → ASR → Text → LLM → Translation.

- **Critical path**: The integration path requires the adapter to bridge acoustic representations to LLM embedding space without information loss, while cascades benefit from specialized training at each stage.

- **Design tradeoffs**: Cascades offer better overall quality but suffer from error propagation and latency. SpeechLLMs provide lower latency potential and better noise/code-switching handling but show architecture-dependent long-form performance issues.

- **Failure signatures**: Extreme length degradation in SpeechLLMs without proper context handling (DeSTA2: 93.8% degradation), ASR hallucination propagation in noisy cascades, overtranslation in LLM-based systems, and gender stereotyping when using non-translation-specialized LLMs.

- **First 3 experiments**:
  1. Run Whisper→Tower+ cascade vs. Voxtral on FLEURS and CoVoST2 to verify generic benchmark findings.
  2. Evaluate both paradigms on NoisyFLEURS to confirm SpeechLLM advantage under acoustic degradation.
  3. Process ACL 60/60 full talks through Voxtral vs. Whisper cascade to measure length gap and identify context handling failures.

## Open Questions the Paper Calls Out

### Open Question 1
Do the comparative advantages of cascaded systems over SpeechLLMs persist when evaluating larger, proprietary models with context windows exceeding 128k tokens? The study limits scope to models with "<32B parameters," leaving performance of frontier-sized models unresolved.

### Open Question 2
How can speech modality integration in SpeechLLMs be optimized to retain the accent robustness of specialized SFMs without sacrificing linguistic quality? Current SpeechLLMs generally underperform compared to standalone SFMs on accent benchmarks.

### Open Question 3
Can specific training interventions or decoding strategies mitigate the pro-stereotypical gender bias and overtranslation tendencies identified in the LLM components of SpeechLLMs? While the paper identifies the LLM as the source of these failures, it doesn't test solutions like debiasing fine-tuning.

### Open Question 4
What specific architectural modifications are necessary to prevent catastrophic performance degradation in long-form speech translation observed in certain SpeechLLMs? Models like DeSTA2 and Qwen2-Audio show extreme length degradation, while architectures that process full context remain robust.

## Limitations
- Benchmark representativeness is skewed toward English-centric directions and Western European languages
- Evaluation metrics remain proxy measures despite human validation
- Component-level analysis may oversimplify complex interactions between speech encoder, adapter, and LLM

## Confidence
- **High Confidence**: Cascaded architectures outperform SpeechLLMs on generic benchmarks (supported by extensive quantitative data)
- **Medium Confidence**: SpeechLLMs show specific advantages in noise, code-switching, and disfluency conditions (primarily supported by this paper's data)
- **Low Confidence**: SpeechLLMs are a long-term investment requiring future integration of reasoning capabilities (speculative based on neighboring research)

## Next Checks
1. Evaluate the same SpeechLLM and cascade on a truly low-resource language pair from MuST-C to test whether reported trends hold outside the study's primary language set.

2. Systematically measure ASR error rates in cascaded systems versus end-to-end outputs for the same audio, then correlate these with translation quality differences to quantify actual error propagation costs.

3. Test Voxtral with different adapter configurations (Q-Former vs. MLP) while keeping all other components constant to isolate the adapter's contribution to performance differences, particularly for long-form content handling.