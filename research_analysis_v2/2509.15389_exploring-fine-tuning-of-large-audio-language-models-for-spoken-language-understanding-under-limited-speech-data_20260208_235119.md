---
ver: rpa2
title: Exploring Fine-Tuning of Large Audio Language Models for Spoken Language Understanding
  under Limited Speech Data
arxiv_id: '2509.15389'
source_url: https://arxiv.org/abs/2509.15389
tags:
- speech
- data
- language
- fine-tuning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores fine-tuning strategies for Large Audio Language
  Models (LALMs) in spoken language understanding (SLU) under limited speech data
  conditions. The research addresses how to effectively adapt LALMs when paired speech-label
  data is scarce while abundant text-label pairs are available.
---

# Exploring Fine-Tuning of Large Audio Language Models for Spoken Language Understanding under Limited Speech Data

## Quick Facts
- arXiv ID: 2509.15389
- Source URL: https://arxiv.org/abs/2509.15389
- Reference count: 0
- LALMs achieve 85-95% of oracle SLU performance with text-only fine-tuning under limited speech data

## Executive Summary
This study investigates how to effectively fine-tune Large Audio Language Models (LALMs) for Spoken Language Understanding (SLU) when paired speech-label data is scarce. The research evaluates three fine-tuning strategies—text-only training, direct mixing of text and speech data, and curriculum learning that progressively introduces speech data. Experiments across English, Italian, and multilingual datasets demonstrate that LALMs can achieve strong SLU performance with minimal paired speech data, particularly when using curriculum learning under data scarcity. The findings provide practical insights for real-world deployment scenarios where collecting large amounts of labeled speech data is challenging.

## Method Summary
The study fine-tunes Qwen2-Audio-7B-Instruct on SLU tasks using three schemes: text-only (freezing audio components), direct mixing (random sampling of speech data), and curriculum learning (text-only epochs followed by speech introduction). Training uses unified JSON prompts for structured output, AdamW optimizer with cosine learning rate scheduling, and beam search inference. Cross-lingual experiments combine source-language speech with target-language text and minimal target speech. Performance is evaluated using intent accuracy, entity F1, and utterance-level SLU-F1 across multiple languages and speech data budgets.

## Key Results
- Text-only fine-tuning achieves 85-95% of oracle SLU-F1 performance by leveraging pre-trained audio-text alignment
- Adding 2-5% speech data yields substantial gains, with curriculum learning particularly effective under data scarcity
- Cross-lingual SLU succeeds by combining source-language speech data with target-language text and minimal target speech

## Why This Works (Mechanism)

### Mechanism 1
Text-only fine-tuning achieves strong baseline performance (85–95% of oracle) by leveraging pre-trained audio-text alignment while freezing audio components. During text-only training, the LLM weights are updated using transcript-label pairs while the audio encoder and modality adapter remain frozen. The model exploits its pre-trained audio-text alignment to generalize from text to speech at inference.

### Mechanism 2
Curriculum learning outperforms direct mixing under limited speech data (2–10%) by stabilizing semantic learning before acoustic integration. Training proceeds in two phases—first, text-only epochs establish the semantic task mapping; second, speech data is introduced in the final epoch. This prevents the model from overfitting to scarce speech samples before the task structure is learned.

### Mechanism 3
Source-language speech data provides cross-lingual grounding that improves zero-shot and few-shot transfer to low-resource target languages. Acoustic representations learned from source-language speech strengthen the model's ability to ground speech-to-semantics mapping. This grounding transfers across languages when combined with target-language text (and minimal target speech).

## Foundational Learning

- **Concept: Spoken Language Understanding (SLU) as joint intent + slot filling**
  - Why needed here: The paper casts SLU as extracting structured semantics (scenario, action, entities) directly from speech. Understanding this task formulation is essential for interpreting the metrics (intent accuracy, entity F1, SLU-F1).
  - Quick check question: Can you explain why SLU-F1 is an utterance-level metric rather than a simple average of intent and entity scores?

- **Concept: End-to-end vs. cascaded speech processing**
  - Why needed here: LALMs avoid ASR-to-LLM cascades, eliminating transcription error propagation and preserving paralinguistic information. This design choice underpins why direct speech-to-label training is viable.
  - Quick check question: What two specific limitations of cascaded systems does the paper claim LALMs overcome?

- **Concept: Curriculum learning**
  - Why needed here: The paper's key finding is that curriculum learning (text → speech) outperforms direct mixing under data scarcity. Understanding curriculum learning as "easy-to-hard" or "text-to-audio" progression is critical for reproducing results.
  - Quick check question: In the paper's curriculum scheme, how many epochs are text-only before speech is introduced?

## Architecture Onboarding

- **Component map:**
  - Speech input → Audio Encoder → Modality Adapter → LLM → Structured JSON output
  - Text input → Tokenizer → LLM → Structured JSON output (text-only mode)

- **Critical path:**
  1. Speech input → Audio Encoder → Modality Adapter → LLM → Structured JSON output
  2. Text input → Tokenizer → LLM → Structured JSON output (text-only mode)

- **Design tradeoffs:**
  - Text-only vs. direct mixing vs. curriculum: Text-only is cheapest (freezes 2/3 components) but leaves acoustic grounding untuned. Direct mixing tunes everything but requires more speech data. Curriculum balances both under scarcity.
  - Learning rate scheduling: Curriculum uses reduced LR (3.0×10⁻⁶) in the speech epoch vs. 5.0×10⁻⁶ for text-only—stability vs. adaptation speed.
  - Cross-lingual source selection: Choosing a typologically similar source language (e.g., French for Spanish) improves transfer; dissimilar languages (e.g., French for Vietnamese) limit gains.

- **Failure signatures:**
  - Text-only baseline underperforms oracle significantly → acoustic information is task-critical; add speech data.
  - Direct mixing underperforms curriculum at 2–10% speech → model overfitting to scarce speech; switch to curriculum.
  - Cross-lingual transfer fails for specific targets → check typological distance and pre-training corpus representation; consider adding minimal target-language speech.

- **First 3 experiments:**
  1. Establish text-only baseline: Train with transcripts only, freeze audio encoder + adapter, evaluate on test set to measure gap from oracle (target: 85–95% of oracle SLU-F1).
  2. Compare curriculum vs. direct mixing at 5% speech: Run both schemes with identical speech subset, verify curriculum outperforms (check for non-overlapping 95% CIs in SLU-F1).
  3. Cross-lingual zero-shot with 10% source speech: Train on French speech + text, evaluate on 2–3 target languages (e.g., German, Spanish) to confirm 20–30% relative improvement over text-only baseline.

## Open Questions the Paper Calls Out

### Open Question 1
Do the observed fine-tuning patterns (curriculum learning effectiveness under scarcity, text-only baseline strength) generalize across different LALM architectures? Only one model architecture tested; different audio encoders or LLM backbones may exhibit different adaptation behaviors under limited speech data.

### Open Question 2
Does the effectiveness of curriculum learning and text-only baselines transfer to speech tasks beyond SLU (e.g., speech translation, emotion recognition)? SLU involves structured semantic extraction; tasks requiring different acoustic-linguistic mappings may respond differently to these fine-tuning strategies.

### Open Question 3
What causes the poor cross-lingual transfer to Vietnamese, and can it be mitigated through alternative source languages or training strategies? Vietnamese consistently underperforms across all experimental conditions, which authors attribute to "larger typological gap from French and limited representation in the model's pretraining corpus."

## Limitations
- Experiments limited to single model architecture (Qwen2-Audio-7B-Instruct), limiting generalizability
- Cross-lingual transfer effectiveness depends heavily on typological similarity and pretraining corpus coverage
- Speech data budgets tested (2-100%) may not capture extreme scarcity scenarios (<1% regime)

## Confidence
**High Confidence**: Core finding that LALMs can be effectively adapted for SLU with minimal paired speech data is well-supported across multiple languages and settings.

**Medium Confidence**: Superiority of curriculum learning under data scarcity is supported but relies on specific two-phase schedule that may not be optimal.

**Low Confidence**: Cross-lingual transfer mechanism's effectiveness for typologically distant languages is weakly supported, with absolute performance remaining low for certain targets.

## Next Checks
1. Test alternative curriculum schedules (e.g., 1 epoch text-only, or multi-phase schedules with 5-20% speech increments) to verify optimal scheduling.
2. Analyze pretraining corpus to quantify representation of low-resource target languages and determine if transfer failures stem from curriculum design or insufficient pretraining coverage.
3. Conduct experiments with speech data budgets below 2% (e.g., 0.1-1%) to test curriculum learning in extreme scarcity and identify minimum speech requirements.