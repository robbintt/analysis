---
ver: rpa2
title: 'Chain of Questions: Guiding Multimodal Curiosity in Language Models'
arxiv_id: '2508.04350'
source_url: https://arxiv.org/abs/2508.04350
tags:
- multimodal
- reasoning
- language
- questions
- framework
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the Chain of Questions (CoQ) framework to
  enhance multimodal reasoning in language models. Current models are limited to passive
  integration of sensory modalities, but humans dynamically engage multiple senses
  to reason about complex environments.
---

# Chain of Questions: Guiding Multimodal Curiosity in Language Models

## Quick Facts
- arXiv ID: 2508.04350
- Source URL: https://arxiv.org/abs/2508.04350
- Authors: Nima Iji; Kia Dashtipour
- Reference count: 27
- Key outcome: Chain of Questions (CoQ) framework improves multimodal reasoning by prompting models to generate curiosity-driven questions that guide selective sensor activation, with FLAN T5 XL achieving 76.2% match accuracy on a benchmark of 180,629 instances.

## Executive Summary
Current language models are limited to passive integration of sensory modalities, while humans dynamically engage multiple senses to reason about complex environments. The Chain of Questions (CoQ) framework addresses this by prompting models to generate targeted, curiosity-driven questions that guide selective activation of relevant sensors such as cameras, microphones, and LiDAR. Each question maps to a specific perceptual task, collecting multimodal observations that improve reasoning accuracy and interpretability. Experiments on a novel benchmark dataset assembled from WebGPT, ScienceQA, AVSD, and ScanQA show that CoQ improves a model's ability to identify and integrate relevant sensory information.

## Method Summary
The CoQ framework guides language models to generate curiosity-driven questions that identify which sensory modalities to activate for multimodal reasoning. The method uses few-shot prompting to elicit modality-specific questions mapped to predefined perceptual tasks (object detection, speech-to-text, spatial reasoning) and corresponding sensors. Observations from activated sensors are aggregated into unified context for final answer inference. The approach was evaluated on a benchmark of 180,629 instances from four datasets using FLAN-T5 and Llama-2 models with greedy, sampling, and beam search decoding.

## Key Results
- FLAN T5 XL (3B) achieved 76.2% match accuracy, outperforming Llama 2 (7B) at 43.9% despite having less than half the parameters
- Encoder-decoder architecture (FLAN T5) significantly outperformed decoder-only architecture (Llama 2) for targeted question generation
- Performance varied by model size and architecture, with larger models and encoder-decoder structures showing better modality selection accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generating curiosity-driven questions before reasoning improves modality selection accuracy.
- Mechanism: The framework introduces an intermediate questioning stage that forces the model to explicitly identify information gaps. Each question maps to a predefined perceptual task (e.g., "What do I see?" → Object Detection), which triggers sensor activation. This decomposition reduces the complexity of directly mapping prompts to sensors.
- Core assumption: Models can learn to generate semantically meaningful questions that reliably map to appropriate modalities through few-shot prompting or fine-tuning.
- Evidence anchors:
  - [abstract]: "These generated questions guide the model to selectively activate relevant modalities, thereby gathering critical information necessary for accurate reasoning."
  - [section 3.1]: "When the model receives a textual prompt (P), it first generates a series of modality-specific questions (Q = {q1, q2, . . . , qk}) that clarify what additional multimodal information is required."
  - [corpus]: Related work on uncertainty in multimodal models (FMR=0.55) suggests explicit uncertainty signaling improves modality selection, supporting the questioning mechanism.
- Break condition: If the question-to-task mapping function T(qi) is poorly defined or the model generates questions outside the predefined taxonomy, the pipeline fails to activate correct sensors.

### Mechanism 2
- Claim: Encoder-decoder architectures outperform decoder-only models for targeted question generation in multimodal contexts.
- Mechanism: FLAN T5's encoder-decoder structure separates input comprehension from output generation, potentially enabling more precise conditioned generation. The encoder processes the prompt context while the decoder generates questions with explicit conditioning on that representation.
- Core assumption: Architectural inductive biases, not just parameter count, determine question generation quality.
- Evidence anchors:
  - [section 5.2, Table 3]: FLAN T5 XL (3B) achieved 76.2% match accuracy versus LLaMA 7B at 43.9%, despite having less than half the parameters.
  - [section 5.2]: "Llama 2 (7 billion parameters), despite its larger size, underperformed relative to FLAN T5 xl, potentially due to its decoder-only architecture, which favors more diverse but less precise token generation."
  - [corpus]: Weak corpus evidence on architecture-specific multimodal reasoning; no direct architectural comparisons found.
- Break condition: Decoder-only models may generate more diverse but less targeted questions, reducing task-mapping precision.

### Mechanism 3
- Claim: Observation aggregation into unified context enables coherent multimodal reasoning.
- Mechanism: Individual sensor observations (o1, o2, ..., ok) are combined through an aggregation function before final answer inference. This creates a shared representation space where textual reasoning can reference multimodal evidence.
- Core assumption: The aggregation function preserves task-relevant information while filtering noise across modalities.
- Evidence anchors:
  - [section 3.1]: "Once all observations (O = {o1, o2, . . . , ok}) are collected, they are aggregated to form a comprehensive multimodal context (C)."
  - [section 1.2]: "The sensor-derived observations are aggregated into a coherent multimodal context, enhancing the model's ability to accurately respond to complex, context-dependent prompts."
  - [corpus]: Related work on multimodal fusion (FMR=0.55) indicates aggregation quality significantly impacts downstream reasoning performance.
- Break condition: If observations contain conflicting information or the aggregation function cannot resolve cross-modal inconsistencies, final reasoning quality degrades.

## Foundational Learning

- Concept: **Chain-of-Thought (CoT) Reasoning**
  - Why needed here: CoQ extends CoT principles from purely textual reasoning to multimodal contexts. Understanding how intermediate reasoning steps improve textual tasks provides the theoretical foundation for the questioning mechanism.
  - Quick check question: Can you explain why generating intermediate steps before a final answer improves reasoning accuracy in language models?

- Concept: **Multimodal Fusion Architectures**
  - Why needed here: The framework requires understanding how different modalities (vision, audio, spatial) can be represented and combined. Knowledge of early vs. late fusion, shared embeddings, and modality-specific encoders informs the observation aggregation design.
  - Quick check question: What are the tradeoffs between early fusion (combining modalities at input level) versus late fusion (combining at representation level)?

- Concept: **Few-Shot Prompt Engineering**
  - Why needed here: The primary implementation uses few-shot learning without fine-tuning. Understanding how to construct effective prompts with examples that elicit desired behaviors is essential for deploying CoQ.
  - Quick check question: How does the choice and ordering of few-shot examples affect model behavior in prompting?

## Architecture Onboarding

- Component map:
  - Question Generator: LLM component that produces curiosity-driven questions from input prompts
  - Task Mapper (T): Function mapping questions to perceptual tasks (lookup table or classifier)
  - Sensor Assignment (S): Function mapping tasks to hardware/software sensors
  - Task Executors: Individual perceptual models (object detection, STT, etc.) that process sensor data
  - Observation Aggregator: Combines outputs from multiple executors into unified context
  - Answer Inference (Fa): LLM-based reasoning over aggregated context to produce final response

- Critical path:
  1. Prompt input → Question Generator → Questions Q
  2. Questions Q → Task Mapper T → Tasks {T1, T2, ...}
  3. Tasks → Sensor Assignment S → Sensor activations
  4. Sensors + Executors → Observations {o1, o2, ...}
  5. Observations → Aggregator → Context C
  6. Context C + Original Prompt → Answer Inference Fa → Final Answer

- Design tradeoffs:
  - **Few-shot vs. Fine-tuning**: Few-shot requires no training but may lack consistency; fine-tuning improves accuracy but increases computational cost (section 3.2)
  - **Predefined vs. Open-ended Questions**: Predefined question taxonomy (Table 1) ensures reliable task mapping but limits flexibility
  - **Sequential vs. Parallel Sensor Activation**: Current design implies sequential; parallel could improve latency but increases complexity

- Failure signatures:
  - **Low Match % (<30%)**: Model generates questions misaligned with prompt requirements; check few-shot examples or consider fine-tuning
  - **Low Asking Rate (<40%)**: Model defaults to text-only reasoning; verify prompt instructions emphasize multimodal exploration
  - **Task Mapping Errors**: Questions fall outside predefined taxonomy; expand Table 1 or implement fuzzy matching
  - **Observation Conflicts**: Aggregated context contains contradictory information; implement conflict resolution in aggregator

- First 3 experiments:
  1. **Baseline Modality Selection**: Test FLAN T5 base with CoQ prompting on held-out subset; establish match rate baseline before architecture changes.
  2. **Question Taxonomy Ablation**: Remove specific question types (e.g., spatial questions) and measure impact on corresponding modality tasks; validates mapping function design.
  3. **Decoding Strategy Comparison**: Compare greedy vs. sampling vs. beam search on question generation quality; determines optimal inference configuration for deployment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does fine-tuning on multimodal curiosity tasks significantly improve question relevance and match accuracy compared to few-shot prompting?
- Basis in paper: [explicit] The authors state that fine-tuning "may incur higher computational costs and resource usage" but "potentially offers improved accuracy and consistency," yet only evaluate few-shot learning.
- Why unresolved: No fine-tuning experiments were conducted; the trade-off between computational cost and accuracy gains remains unquantified.
- What evidence would resolve it: A controlled comparison of few-shot vs. fine-tuned CoQ performance on the same benchmark, measuring both accuracy and training/inference costs.

### Open Question 2
- Question: Why does the encoder-decoder architecture (FLAN-T5 XL, 3B) outperform the larger decoder-only architecture (Llama 2, 7B) in generating targeted multimodal questions?
- Basis in paper: [inferred] The authors note Llama 2 "underperformed relative to FLAN T5 xl, potentially due to its decoder-only architecture, which favors more diverse but less precise token generation," but do not test this hypothesis.
- Why unresolved: Architectural factors (encoder-decoder vs. decoder-only), training objectives, and scale are confounded; no ablation isolates architecture effects.
- What evidence would resolve it: Controlled experiments comparing encoder-decoder and decoder-only models of matched scale and pretraining data on CoQ tasks.

### Open Question 3
- Question: How robust is the CoQ framework in real-time physical environments with actual sensors, as opposed to curated benchmark datasets?
- Basis in paper: [inferred] The paper evaluates CoQ on assembled benchmark datasets (WebGPT, ScienceQA, AVSD, ScanQA) but does not deploy it with real hardware sensors in dynamic environments.
- Why unresolved: Real-world noise, latency, sensor failures, and temporally changing contexts may affect question generation and modality selection differently than static benchmarks.
- What evidence would resolve it: Deployment studies in embodied AI settings (e.g., robots with cameras, microphones, LiDAR) measuring task success, latency, and failure modes.

## Limitations

- Prompt template design critically affects effectiveness, but exact few-shot prompt templates and example question–task pairs are not provided
- Ground truth labeling methodology for "correct" modality per prompt is unclear across different datasets with different annotation schemas
- The task mapping function T(qi) implementation details are missing, which is critical for whether generated questions successfully activate appropriate sensors

## Confidence

- High Confidence: The architectural insight that encoder-decoder models outperform decoder-only models for targeted question generation is well-supported by direct comparison
- Medium Confidence: The observation aggregation mechanism and its impact on reasoning quality, though specific implementation details are not provided
- Low Confidence: The framework's generalization beyond the curated benchmark to real-world, unstructured multimodal inputs

## Next Checks

1. **Prompt Template Ablation**: Systematically vary the few-shot examples and prompt structure while holding the model and evaluation constant. Measure match rate sensitivity to different prompt engineering choices to distinguish framework robustness from prompt brittleness.

2. **Cross-Dataset Generalization**: Evaluate CoQ on held-out examples from datasets not used in benchmark construction (e.g., VQA, GQA, or real-world multimodal inputs). This tests whether the framework generalizes beyond curated multimodal datasets.

3. **Error Analysis on Question Generation**: Manually examine generated questions from both high-performing (FLAN T5 XL) and low-performing (Llama 2) models. Categorize errors (irrelevant questions, missing modalities, task-mapping failures) to identify whether improvements come from better question generation or better task mapping.