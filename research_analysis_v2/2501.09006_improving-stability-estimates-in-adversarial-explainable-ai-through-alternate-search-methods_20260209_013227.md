---
ver: rpa2
title: Improving Stability Estimates in Adversarial Explainable AI through Alternate
  Search Methods
arxiv_id: '2501.09006'
source_url: https://arxiv.org/abs/2501.09006
tags:
- similarity
- perturbations
- search
- text
- explanation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of evaluating the stability of
  local surrogate models in explainable AI (XAI), specifically their vulnerability
  to adversarial attacks. Prior methods focused on demonstrating instability's existence,
  not its magnitude.
---

# Improving Stability Estimates in Adversarial Explainable AI through Alternate Search Methods

## Quick Facts
- arXiv ID: 2501.09006
- Source URL: https://arxiv.org/abs/2501.09006
- Reference count: 5
- Primary result: Genetic algorithms find fewer perturbations to achieve target instability thresholds compared to greedy search methods

## Executive Summary
This work addresses the challenge of evaluating stability of local surrogate models in explainable AI (XAI), specifically their vulnerability to adversarial attacks. Prior methods focused on demonstrating instability's existence rather than its magnitude. The authors propose using a genetic algorithm (GA) to find minimal perturbations needed to achieve fixed similarity thresholds between original and altered text explanations, allowing for more nuanced comparisons of XAI stability.

The GA approach was compared against a greedy search method using two datasets (Twitter Gender Bias and Symptoms to Diagnosis) across multiple similarity measures and thresholds. Results showed the GA successfully achieved higher attack success rates, particularly with Spearman-weighted similarity measures, and often required fewer perturbations to reach target similarity thresholds. For example, on the Symptoms to Diagnosis dataset, the GA achieved 24% success at 30% threshold under Spearman w where the greedy search had 0% success.

## Method Summary
The paper proposes using a genetic algorithm to find minimal perturbations that achieve a fixed similarity threshold between original and altered text explanations. The GA searches for perturbations that maintain the predicted class while reducing explanation similarity to a target threshold (30-60%). The approach was compared against a greedy search baseline across two datasets and nine similarity measures. Experiments measured attack success rates, mean similarity achieved, average perturbation rate, and minimum perturbations required for successful attacks.

## Key Results
- GA achieved 24% success rate at 30% threshold under Spearman w on Symptoms to Diagnosis dataset where greedy search had 0% success
- GA generally required fewer perturbations to achieve target similarity thresholds compared to greedy search
- GA successfully achieved higher attack success rates, particularly with Spearman-weighted similarity measures
- The approach provides more nuanced stability estimates by measuring how many perturbations are needed to expose instability rather than just establishing its existence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Genetic algorithms find fewer perturbations to achieve target instability thresholds compared to greedy search.
- Mechanism: Population-based evolution with selection, crossover, and mutation explores perturbation combinations non-sequentially, escaping local optima that trap greedy approaches which iterate through importance-ordered indices.
- Core assumption: The search space contains perturbation combinations that greedy sequential selection systematically misses.
- Evidence anchors:
  - [abstract] "GA achieved 24% success at 30% threshold under Spearman w where the greedy search had 0% success"
  - [section 4] "Genetic algorithms efficiently search extremely large search spaces, making them a good candidate for efficiently searching for minimal perturbations"
  - [corpus] Related work (Alzantot et al., 2018) uses GAs for parsimonious text attacks—suggests pattern generalizes
- Break condition: When computational budget constrains population size (10) and generations (10), exploration advantage diminishes against simpler search spaces.

### Mechanism 2
- Claim: Threshold-based similarity provides an objective metric for comparing stability magnitude across XAI methods.
- Mechanism: Define success as achieving a target similarity reduction (τ = 30–60%), then measure perturbations required—fewer perturbations indicates worse stability.
- Core assumption: Explanation similarity thresholds capture meaningful instability distinctions; lower perturbation counts objectively indicate inferior robustness.
- Evidence anchors:
  - [abstract] "Intuitively, a method that requires fewer perturbations to expose a given level of instability is inferior to one which requires more"
  - [section 1] "Prior work in adversarial XAI has focused on establishing the existence of instability and has yet to explore how much instability may be present"
  - [corpus] Same authors' concurrent work (Burger et al., 2024) explores similarity measure selection—confirms this is an active research direction
- Break condition: When thresholds are too aggressive (unachievable) or too lenient (all methods succeed), comparative signal collapses.

### Mechanism 3
- Claim: Weighted rank similarity measures (Spearman w, Kendall w) detect explanation changes more sensitively than unweighted versions.
- Mechanism: Weighted measures incorporate LIME feature importance values, capturing both rank reordering and weight redistribution in explanations.
- Core assumption: Feature weight values carry distinct semantic information beyond ordering alone.
- Evidence anchors:
  - [abstract] "GA successfully achieved higher attack success rates, particularly with Spearman-weighted similarity measures"
  - [section 3.2] "All non-RBO measures also have a weighted alternative based on the values calculated by the explanatory model"
  - [corpus] Weak corpus evidence for this specific weighted measure advantage—requires validation beyond LIME
- Break condition: When surrogate models produce near-uniform weights, weighted and unweighted measures converge.

## Foundational Learning

- Concept: **LIME (Local Interpretable Model-agnostic Explanations)**
  - Why needed here: The entire attack framework targets LIME's explanation stability; understanding its sampling-based surrogate training is essential.
  - Quick check question: Why does LIME's random word removal during sampling create inherent instability?

- Concept: **Genetic Algorithm Components**
  - Why needed here: Implementation requires understanding fitness evaluation, parent selection, crossover, and mutation operators.
  - Quick check question: Given population=10 and generations=10, what limits the search depth?

- Concept: **Ranked List Similarity Measures**
  - Why needed here: Selecting appropriate similarity measures (RBO, Jaccard, Kendall, Spearman) determines attack success detection.
  - Quick check question: Which measure weights top-ranked features more heavily?

## Architecture Onboarding

- Component map:
  Input Document → LIME → Base Explanation (ranked features + weights)
                                    ↓
  Perturbation Generator (TextFooler: embedding neighbors + POS + semantic constraints)
                                    ↓
  Search Controller (GA: init population → fitness eval → selection → crossover → mutation)
                                    ↓
  Constraint Checker (class preservation, top-k protection, semantic threshold δ)
                                    ↓
  Similarity Calculator (RBO/Jaccard/Kendall/Spearman × weighted variants)
                                    ↓
  Output: Minimal perturbation count achieving threshold τ

- Critical path:
  1. Generate base explanation via LIME
  2. Initialize GA population by mutating base document
  3. Per generation: rank by fitness (similarity to threshold), truncate bottom 50%, crossover, mutate children
  4. Terminate when any chromosome achieves similarity ≤ τ or generations exhausted
  5. Verify constraints: f(db) = f(dp), Sims(db, dp) ≥ δ, top-k preserved

- Design tradeoffs:
  - **GA vs. Greedy**: GA finds fewer perturbations (~20-50% reduction in some cases) but incurs ~250% time overhead
  - **Population/generation size**: Limited to 10/10 by compute; larger values likely improve results
  - **Weighted vs. unweighted measures**: Weighted more discriminative but require weight extraction from surrogate
  - **Threshold selection**: Lower thresholds (30%) harder to achieve; higher (60%) less discriminative

- Failure signatures:
  - Zero success rate at threshold → threshold too aggressive OR model genuinely stable
  - GA = Greedy results → search space lacks combinatorial complexity
  - Perturbed text semantically degraded → embedding/constraint failure
  - Class prediction flips → constraint violation during search

- First 3 experiments:
  1. Replicate Table 2 on held-out samples: Compare GA vs. Greedy success rates across all similarity measures at τ=50%
  2. Threshold sweep: Fix Spearman w, vary τ ∈ {30, 40, 50, 60}%, plot success rate and mean perturbations
  3. Compute-accuracy ablation: Test population ∈ {5, 10, 20} × generations ∈ {5, 10, 20}, measure time vs. perturbation reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the genetic algorithm approach achieve similar improvements in finding minimal perturbations on longer documents (≥100 words)?
- Basis in paper: [explicit] Authors state they "were unable to test the collection of similarity measures and thresholds on larger datasets (≥ 100 words) due to the computational expenditure required."
- Why unresolved: The computational cost of the GA (~250% increase over greedy search) made testing on larger documents infeasible with available resources.
- What evidence would resolve it: Running the same GA vs. greedy comparison on datasets with documents ≥100 words, reporting attack success rates and minimal perturbations across similarity measures.

### Open Question 2
- Question: What improvements in attack efficacy can be achieved through alternative genetic algorithm configurations (parent selection, mutation strategies, chromosome representations)?
- Basis in paper: [explicit] "Given the relative simplicity of the genetic algorithm and the large possible variety of parent selection methods, mutation strategies and chromosome representations available, further increases in efficacy seem likely."
- Why unresolved: The GA used restrictive parameters (population=10, generations=10) due to computational constraints, leaving the design space largely unexplored.
- What evidence would resolve it: Systematic ablation study varying GA hyperparameters and operators, measuring changes in attack success rates and minimal perturbations required.

### Open Question 3
- Question: Can purpose-built GPU adaptations or other computational optimizations reduce the time overhead of the genetic algorithm approach while maintaining or improving its advantages?
- Basis in paper: [explicit] "Any explanatory method used will likely require a purpose built adaptation to allow efficient computation on modern hardware, especially as existing methodologies are lacking in optimal GPU support."
- Why unresolved: Current implementations are not optimized for parallel hardware, limiting scalability and practical adoption.
- What evidence would resolve it: Developing GPU-accelerated implementations and benchmarking computational time against the current ~250% overhead while preserving attack efficacy.

### Open Question 4
- Question: Do the improved stability estimates from the GA search approach transfer to other local surrogate explanation methods beyond LIME (e.g., SHAP, Integrated Gradients)?
- Basis in paper: [inferred] The paper tests only LIME, noting it was chosen as "a familiar standard," but claims the method is "applicable to any XAI method that returns an explanation in this format."
- Why unresolved: No empirical validation was provided for other XAI methods; different surrogate methods may have different vulnerability patterns.
- What evidence would resolve it: Applying the GA search method to alternative XAI techniques and comparing minimal perturbation findings against greedy search baselines.

## Limitations
- Computational constraints limited GA experiments to 10 population members and 10 generations, creating uncertainty about scalability
- Only two datasets (Twitter Gender Bias and Symptoms to Diagnosis) were tested, limiting generalizability across domains and text lengths
- The underlying black-box classifiers and embedding spaces used for perturbations are not specified, creating potential reproducibility gaps

## Confidence

**High confidence**: The comparative advantage of GA over greedy search at finding fewer perturbations for given similarity thresholds is well-supported by experimental results across multiple similarity measures. The mechanism explaining GA's exploration advantage over greedy sequential selection is theoretically sound and empirically validated.

**Medium confidence**: The claim that weighted rank similarity measures (Spearman w, Kendall w) are more sensitive than unweighted versions is supported but requires broader validation beyond the specific LIME implementation tested. The choice of GA parameters (population=10, generations=10) represents a practical compromise but introduces uncertainty about optimal configuration.

**Low confidence**: The generalizability of these results to other surrogate explanation methods beyond LIME remains untested. Similarly, the relationship between perturbation count and meaningful stability differences across diverse XAI applications needs further investigation.

## Next Checks

1. **Scale-up validation**: Run GA experiments with population ∈ {20, 50} and generations ∈ {20, 50} to determine if perturbation reduction scales with computational budget and whether diminishing returns appear.

2. **Cross-method generalization**: Apply the GA-based stability assessment framework to other local surrogate methods (SHAP, Anchor) to verify if the search advantage transfers beyond LIME.

3. **Domain diversity test**: Evaluate on additional datasets spanning different text domains (clinical notes, legal documents, social media) and varying average lengths to assess robustness across text characteristics.