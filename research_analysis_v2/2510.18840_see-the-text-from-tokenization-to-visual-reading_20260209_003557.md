---
ver: rpa2
title: 'See the Text: From Tokenization to Visual Reading'
arxiv_id: '2510.18840'
source_url: https://arxiv.org/abs/2510.18840
tags:
- text
- tokenization
- visual-text
- languages
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SeeTok, a vision-centric tokenization method
  that replaces subword tokenization with visual rendering of text into images, processed
  by pretrained multimodal models. By leveraging strong OCR and text-vision alignment
  abilities from large-scale multimodal pretraining, SeeTok achieves performance on
  par with or exceeding subword tokenizers while requiring 4.43x fewer tokens and
  reducing FLOPs by 70.5%.
---

# See the Text: From Tokenization to Visual Reading

## Quick Facts
- arXiv ID: 2510.18840
- Source URL: https://arxiv.org/abs/2510.18840
- Authors: Ling Xing; Alex Jinpeng Wang; Rui Yan; Hongyu Qu; Zechao Li; Jinhui Tang
- Reference count: 30
- Primary result: Vision-centric tokenization achieves 4.43× compression for English and up to 13.05× for low-resource languages while maintaining or exceeding subword tokenizer performance.

## Executive Summary
This paper introduces SeeTok, a vision-centric tokenization method that renders text as images and processes them through pretrained multimodal models. By leveraging the strong OCR and text-vision alignment capabilities of large-scale multimodal pretraining, SeeTok achieves performance on par with or exceeding traditional subword tokenizers while requiring 4.43× fewer tokens and reducing FLOPs by 70.5%. The approach demonstrates substantial gains in cross-lingual generalization, robustness to typographic noise, and preservation of linguistic hierarchy across three language tasks and 13 languages.

## Method Summary
SeeTok renders text into images using Google Noto Sans font at 7px size, then processes these images through a vision encoder that extracts patch-level features. A two-layer MLP projector aggregates four neighboring patches into a single token, achieving 4× compression. The system uses LoRA adapters (rank=8, α=32, dropout=0.1) on both the vision encoder and LLM while keeping the projector frozen to preserve cross-modal alignment. The method is evaluated using pretrained MLLMs (Qwen2.5-VL, JanusPro) fine-tuned on filtered OpenHermes 2.5 instruction data.

## Key Results
- SeeTok achieves 4.43× compression for English and up to 13.05× for low-resource languages (Georgian) while maintaining task performance.
- FLOPs are reduced by 70.5% compared to text-based tokenizers.
- Cross-lingual generalization improves significantly, with +3.87 COMET-22 gains in translation quality across 13 languages.
- Robustness to character-level, word-level, and visual perturbations exceeds that of subword tokenization.

## Why This Works (Mechanism)

### Mechanism 1: Language-Agnostic Compression
Vision-centric tokenization achieves 4.43× compression for English and up to 13.05× for low-resource languages while maintaining task performance. The vision encoder's patch-based segmentation is language-agnostic, avoiding the vocabulary allocation bias of subword tokenizers toward high-resource languages. Core assumption: Pretrained vision encoder's OCR and text-vision alignment abilities transfer effectively to visual-text instruction following with minimal adaptation. Break condition: Compression gains may diminish if the vision encoder's OCR capacity is insufficient for the script or rendering parameters cause illegibility.

### Mechanism 2: Perturbation Robustness
Vision-centric tokenization exhibits stronger robustness to character-level, word-level, and visual perturbations than subword tokenization. Visual tokenization treats characters as visual units; local edits affect only nearby patch features while the overall word shape and spatial layout remain largely intact, yielding more stable representations. Core assumption: Holistic visual patterns preserved under perturbation are sufficient for downstream reasoning. Break condition: Robustness may fail under severe visual corruption that destroys word shape entirely or adversarial perturbations designed to exploit vision encoder vulnerabilities.

### Mechanism 3: LoRA Alignment
Lightweight LoRA instruction tuning on the vision encoder and LLM (with projector frozen) closes the distribution gap between visual-text and pure-text instructions, enabling comparable instruction-following. Freezing the projector preserves cross-modal alignment from large-scale pretraining; tuning it risks disrupting this alignment. Core assumption: Visual-text and pure-text inputs should be processed similarly at higher layers. Break condition: Gains may not transfer if the instruction-tuning data distribution diverges significantly from target tasks.

## Foundational Learning

- **Concept: Subword Tokenization (BPE, WordPiece, Unigram)**
  - Why needed here: SeeTok positions itself as an alternative to subword tokenization; understanding BPE's vocabulary allocation bias is essential to contextualize the claimed advantages.
  - Quick check question: Given a BPE tokenizer trained primarily on English, what happens to the tokenization of a Georgian word not in the vocabulary?

- **Concept: Vision Transformer (ViT) Patch Embeddings**
  - Why needed here: The vision encoder processes rendered text as a grid of patches; understanding patch-based feature extraction is necessary to reason about compression ratios.
  - Quick check question: If a 224×224 image is divided into 14×14 patches, what is the resolution of each patch, and how does the 4:1 aggregation affect the output token count?

- **Concept: Multimodal LLM Architecture (Encoder–Projector–LLM)**
  - Why needed here: SeeTok reuses the encoder and projector from pretrained MLLMs as a "visual tokenizer"; understanding each component's role clarifies what is reused vs. adapted.
  - Quick check question: In a typical MLLM, what is the purpose of the projector between the vision encoder and the LLM backbone?

## Architecture Onboarding

- **Component map:** Text string → Visual Renderer → Vision Encoder → MLP Projector → LLM Backbone
- **Critical path:** Render text → image (ensure font size and resolution support legibility for target scripts) → Vision encoder forward pass → patch features → Projector aggregates 4:1 → visual tokens → LLM forward pass with LoRA-adapted weights → next-token prediction loss → Backprop through LoRA parameters only; encoder and projector base weights frozen.
- **Design tradeoffs:**
  - Compression vs. fidelity: Higher compression reduces FLOPs but may lose fine-grained character information; the paper uses 4:1 empirically.
  - Frozen projector vs. end-to-end tuning: Freezing preserves pretraining alignment but limits adaptation; tuning risks catastrophic forgetting.
  - Font size vs. patch coverage: Smaller fonts fit more text per image but may reduce OCR accuracy; 7px was selected empirically.
- **Failure signatures:**
  - Performance collapse on visual-text input without LoRA tuning indicates distribution gap.
  - Degradation after tuning projector indicates disrupted cross-modal alignment.
  - Poor compression on very short inputs suggests fixed image dimensions underutilize capacity.
- **First 3 experiments:**
  1. Reproduce Table 1 (visual-text vs. pure-text baseline on TriviaQA/MMLU/SST5) with Qwen2.5-VL 3B to validate LoRA tuning recovers or exceeds pure-text performance on visual-text input.
  2. Ablate projector freezing: Compare (encoder+LLM LoRA, projector frozen) vs. (encoder+LLM+projector LoRA) on TriviaQA to confirm Table 4 findings.
  3. Perturbation robustness check: Evaluate on MMLU with character-level n-gram shuffling comparing visual-text vs. pure-text input to replicate Figure 4 trends.

## Open Questions the Paper Calls Out
None

## Limitations
- The distribution gap between pure-text and visual-text instruction-following is not fully characterized, with a 9.09-point gap on MMLU suggesting fundamental limitations.
- OCR performance on diverse scripts depends on pretraining data coverage that is not reported, raising questions about the claimed 13.05× compression for Georgian.
- Robustness claims depend on human-like shape-based character recognition without investigating whether the vision encoder's OCR is actually based on shape similarity.

## Confidence
- High Confidence: Vision-centric tokenization achieves claimed compression ratios while maintaining performance; projector freezing is critical; cross-lingual generalization benefits from visual processing; LoRA improves visual-text instruction-following.
- Medium Confidence: Robustness to perturbations exceeds subword tokenization; translation quality improvements are consistent; FLOPs reduction scales with token reduction.
- Low Confidence: The exact mechanism by which LoRA achieves pure-text level performance is fully understood; pretraining data coverage is sufficient for all target languages; OCR capabilities generalize to unseen scripts without adaptation.

## Next Checks
1. Systematically evaluate the vision encoder's OCR performance on each of the 13 target languages using character recognition accuracy on rendered text samples.
2. Design adversarial perturbations specifically targeting the vision encoder's weaknesses to establish absolute robustness limits.
3. Implement and evaluate dynamic image sizing to assess whether compression ratios and performance vary significantly with text length.