---
ver: rpa2
title: 'Multi-Objective Reinforcement Learning with Max-Min Criterion: A Game-Theoretic
  Approach'
arxiv_id: '2510.20235'
source_url: https://arxiv.org/abs/2510.20235
tags:
- policy
- max-min
- learning
- which
- morl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses multi-objective reinforcement learning (MORL)
  with a max-min criterion, reformulating it as a two-player zero-sum game and introducing
  an efficient algorithm based on mirror descent. The authors propose ERAM (Entropy-Regularized
  Adversary for Max-min MORL) and ARAM (Adaptively-Regularized Adversary for Max-min
  MORL), leveraging natural policy gradient updates for the learner and closed-form
  updates for the adversary's weight vector.
---

# Multi-Objective Reinforcement Learning with Max-Min Criterion: A Game-Theoretic Approach

## Quick Facts
- **arXiv ID:** 2510.20235
- **Source URL:** https://arxiv.org/abs/2510.20235
- **Reference count:** 40
- **Key outcome:** Reformulates max-min MORL as zero-sum game with closed-form updates, achieving up to 68% faster training and 95% less memory than baselines.

## Executive Summary
This paper addresses multi-objective reinforcement learning (MORL) with a max-min criterion, reformulating it as a two-player zero-sum game and introducing an efficient algorithm based on mirror descent. The authors propose ERAM (Entropy-Regularized Adversary for Max-min MORL) and ARAM (Adaptively-Regularized Adversary for Max-min MORL), leveraging natural policy gradient updates for the learner and closed-form updates for the adversary's weight vector. Theoretical analysis establishes global last-iterate convergence with exact and approximate policy evaluations, along with sample complexity bounds. Empirically, ERAM and ARAM significantly outperform existing baselines in various MORL environments, including traffic signal control, species conservation, and other benchmark tasks.

## Method Summary
The method reformulates the max-min MORL problem as a two-player zero-sum game where the learner updates the policy using PPO while the adversary updates a weight vector over objectives using closed-form mirror descent with entropy regularization. The key insight is that adding negative entropy regularization enables closed-form softmax updates for the adversary, eliminating iterative optimization overhead. The approach uses asymmetric learning rates (smaller for adversary, larger for learner) to ensure last-iterate convergence to Nash equilibrium. ERAM uses static regularization while ARAM adaptively adjusts regularization based on reward correlations.

## Key Results
- ERAM and ARAM achieve up to 68% reduction in training time and 95% reduction in memory usage compared to prior methods
- Theoretical guarantees of global last-iterate convergence with both exact and approximate policy evaluations
- Sample complexity bounds established for the proposed algorithms
- Significant performance improvements over baselines in traffic signal control, species conservation, and other MORL benchmark tasks

## Why This Works (Mechanism)

### Mechanism 1: Game-Theoretic Reformulation Converts Non-Differentiable Objective to Saddle-Point Problem
- Claim: The max-min MORL problem max_π min_k V^π_k can be reformulated as a two-player zero-sum game where finding a Nash equilibrium yields the optimal policy.
- Mechanism: The discrete min over k is equivalently expressed as min_{w∈Δ^K} ⟨w, V^π⟩ where w lies on the probability simplex. The paper establishes that max_π min_w ⟨w, V^π_τ⟩ = min_w max_π ⟨w, V^π_τ⟩ (Theorem 3.1), meaning a saddle point of this game corresponds to the max-min optimal policy.
- Core assumption: Entropy regularization ensures solution indeterminacy is resolved.
- Evidence anchors:
  - [abstract] "we reformulate max-min multi-objective reinforcement learning as a two-player zero-sum regularized continuous game"
  - [section 3.1] Equation (6) establishes the max-min = min-max equivalence
  - [corpus] FairDICE paper mentions max-min fairness as a common objective in MORL, though without this game-theoretic reformulation

### Mechanism 2: Negative Entropy Regularization Enables Closed-Form Adversary Updates
- Claim: Adding negative entropy regularization -τ_w H(w) to the adversary's objective yields a closed-form softmax update, eliminating iterative optimization overhead.
- Mechanism: The mirror descent update with KL-divergence and negative entropy regularization yields w_{t+1} = softmax(-(1-β)/τ_w · V^{π_t} + β log w_t) where β = 1/(λτ_w + 1). Without this regularization, the adversary would select a one-hot vector at each step, causing instability.
- Core assumption: The weight simplex forms a valid manifold where KL-divergence is the appropriate Bregman divergence.
- Evidence anchors:
  - [section 3.3] Equation (13) provides the closed-form solution
  - [section 3.2] Explains that entropy regularization spreads weight elements to incorporate multiple objectives simultaneously
  - [corpus] Weak or missing — related works on MORL do not leverage this specific closed-form structure

### Mechanism 3: Asymmetric Learning Rates Ensure Last-Iterate Convergence
- Claim: Using a smaller step size for the adversary (λ = O(ε²)) than for the learner (η = O(ε)) guarantees last-iterate convergence to the Nash equilibrium.
- Mechanism: The linear system analysis (Appendix G) shows the spectral radius of the transition matrix ρ(η, λ) < 1 only when policy updates dominate weight updates. Intuitively, the policy must adapt faster than the adversary's evaluation of which objective is worst.
- Core assumption: Tabular MOMDP with softmax policy parameterization; approximate policy evaluation has bounded error δ.
- Evidence anchors:
  - [section 4] Theorem 4.1 establishes linear convergence rates with explicit conditions on η and λ
  - [section 4] Corollary 4.2 gives iteration complexity O(1/ε² log(1/ε_acc))
  - [corpus] Convergence analysis aligns with prior work on two-player zero-sum Markov games, but with asymmetric regularization structure

## Foundational Learning

- Concept: **Mirror Descent and Natural Policy Gradient**
  - Why needed here: The learner's policy update uses NPG, which is mirror descent with the Fisher information metric. Understanding why NPG = MD with KL-divergence is essential to grasp the unified optimization framework.
  - Quick check question: Can you explain why the NPG update with softmax policy yields the closed-form π_{t+1} ∝ π_t^α exp((1-α)Q/τ)?

- Concept: **Two-Player Zero-Sum Games and Nash Equilibrium**
  - Why needed here: The entire reformulation relies on finding a Nash equilibrium where max-min equals min-max. Without this, the game-theoretic approach has no guarantee.
  - Quick check question: In a zero-sum game with payoff f(θ, w), what conditions ensure that max_θ min_w f = min_w max_θ f?

- Concept: **Entropy Regularization in RL**
  - Why needed here: Two roles: (1) resolves indeterminacy in max-min MORL by ensuring stochastic optimal policies exist; (2) enables closed-form updates for both players. The coefficient τ controls the bias-variance tradeoff.
  - Quick check question: What happens to the optimal policy as τ → 0, and why does this matter for max-min MORL?

## Architecture Onboarding

- Component map:
  - Policy Network (π_θ) -> Critic Networks (V_ϕ) -> Weight Vector (w) -> Reference Vector (c) for ARAM

- Critical path:
  1. Collect trajectories with current π_θ
  2. Compute advantage estimates Â_k for each objective k
  3. Form weighted advantage ⟨w_t, Â⟩ and update policy via PPO
  4. Update critic networks for all K objectives
  5. Compute V^{π_t}_k estimates and update w via closed-form softmax
  6. (ARAM only) Compute correlation vector c_t for adaptive regularization

- Design tradeoffs:
  - **β = 1/(λτ_w + 1)**: Controls momentum in weight updates. β near 0 ignores history (faster adaptation but unstable); β near 1 preserves history (stable but slow). Ablation shows optimal β ∈ [0.25, 0.67] for traffic tasks.
  - **τ (policy entropy coefficient)**: Larger τ increases exploration but biases the optimal policy away from true max-min solution. Paper suggests τ and τ_w should be "sufficiently small" per Remark 4.5.
  - **Memory vs. objectives**: O(K) critic heads required, but Park et al. [37] uses O(K × |S||A|) Q-networks. ERAM/ARAM achieve ~95% memory reduction.

- Failure signatures:
  - **Oscillating weights with no convergence**: β too low (λ too large or τ_w too small); weights jump between objectives
  - **Slow convergence to suboptimal policy**: β too high; adversary too conservative to identify worst objective
  - **Large max-min gap in validation**: Policy overfits to training objective distribution; need better exploration or smaller τ

- First 3 experiments:
  1. **Tabular MOMDP with (|S|,|A|,K) = (4,4,4)**: Implement Algorithm 2 with exact policy evaluation. Verify Nash gap decreases linearly. Target: suboptimality < 0.1 within 5000 iterations.
  2. **Traffic signal control with 4 objectives (Base-4 scenario)**: Integrate ERAM with PPO using Stable-Baselines3. Compare max-min return against Avg-DQN and GGF-PPO baselines. Target: match or exceed Table 1 results.
  3. **Ablation on (λ, β) hyperparameters**: Run sweep over λ ∈ {0.01, 0.1, 0.2} and β ∈ {0.25, 0.5, 0.67, 0.75}. Identify stable region where weights converge smoothly. Generate heatmap similar to Figure 8.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the ARAM (Adaptively-Regularized Adversary) algorithm possess theoretical convergence guarantees similar to those established for ERAM?
- **Basis in paper:** [explicit] Section 4 states the authors are "leaving theoretical analysis of more complicated ARAM as a possible future work."
- **Why unresolved:** ARAM utilizes a dynamic reference vector ($c_t$) for regularization, which complicates the stability analysis required for convergence proofs compared to the static regularization in ERAM.
- **What evidence would resolve it:** A formal proof showing last-iterate convergence rates for ARAM in the tabular setting, likely requiring modifications to the linear system analysis used for ERAM.

### Open Question 2
- **Question:** Can the theoretical guarantees of global last-iterate convergence be extended to general function approximation settings, such as linear function approximation?
- **Basis in paper:** [explicit] Appendix O notes that "analyzing last-iterate convergence under more general settings, such as linear function approximation, remains a valuable research direction."
- **Why unresolved:** The current theoretical analysis relies on the specific properties of the softmax parameterization (e.g., closed-form updates) and may not hold under the approximation errors inherent in linear or general function approximation.
- **What evidence would resolve it:** Theoretical bounds on the optimality gap that account for function approximation error, or convergence proofs for the NPG updates under compatible function approximation assumptions.

### Open Question 3
- **Question:** How can the max-min criterion be effectively adapted for heterogeneous objectives that have different units or scales?
- **Basis in paper:** [explicit] Appendix O states, "The max-min criterion works best when objectives are homogeneous... Extending it to heterogeneous objectives is an interesting direction for future work."
- **Why unresolved:** The max-min operator is sensitive to scale; directly minimizing values with vastly different magnitudes can cause the algorithm to focus solely on the numerically largest objective, ignoring others.
- **What evidence would resolve it:** A framework that integrates adaptive normalization or weighting into the max-min formulation to ensure meaningful trade-offs between incommensurable objectives.

## Limitations
- Theoretical guarantees rely on tabular settings with exact or bounded-approximation policy evaluation, limiting direct applicability to large-scale deep RL
- Convergence analysis assumes specific step-size relationships (λ = O(ε²), η = O(ε)) that may be sensitive to hyperparameters in practice
- Empirical results show significant improvements but comparisons are limited to relatively small-scale environments

## Confidence
- **High confidence**: Game-theoretic reformulation equivalence (max-min = min-max) and closed-form adversary updates with entropy regularization
- **Medium confidence**: Theoretical convergence rates and sample complexity bounds (based on idealized tabular assumptions)
- **Medium confidence**: Empirical performance claims (improvements demonstrated but in limited domains)

## Next Checks
1. **Hyperparameter sensitivity analysis**: Systematically vary λ and β across multiple orders of magnitude to identify stable regions and verify theoretical step-size conditions
2. **Scalability test**: Apply ERAM/ARAM to environments with K > 16 objectives to validate the claimed O(K) memory advantage over Park et al. [37]
3. **Policy evaluation quality study**: Quantify how different levels of approximation error in V^π estimation affect convergence rates and final performance, particularly for large state spaces