---
ver: rpa2
title: Learning Symbolic Task Decompositions for Multi-Agent Teams
arxiv_id: '2502.13376'
source_url: https://arxiv.org/abs/2502.13376
tags:
- agent
- task
- agents
- decomposition
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents LOTaD, a method for learning optimal task decompositions
  in multi-agent reinforcement learning without prior knowledge of environment dynamics.
  The key innovation is simultaneously learning both the optimal decomposition of
  a symbolic task (expressed as a reward machine) and policies for each agent through
  model-free interactions.
---

# Learning Symbolic Task Decompositions for Multi-Agent Teams

## Quick Facts
- arXiv ID: 2502.13376
- Source URL: https://arxiv.org/abs/2502.13376
- Reference count: 40
- Primary result: LOTaD learns optimal task decompositions in MARL without prior environment knowledge, outperforming baselines across five environments including those with codependent agent dynamics.

## Executive Summary
This paper presents LOTaD, a method for learning optimal task decompositions in multi-agent reinforcement learning without prior knowledge of environment dynamics. The key innovation is simultaneously learning both the optimal decomposition of a symbolic task (expressed as a reward machine) and policies for each agent through model-free interactions. LOTaD uses a task-conditioned policy architecture where agents share a neural network policy conditioned on both their sub-task and the overall task state, enabling them to handle dependent agent dynamics. The method employs UCB-based selection to explore different candidate decompositions and leverages value estimates to balance exploration and exploitation. Experiments across five environments demonstrate that LOTaD significantly outperforms baseline approaches.

## Method Summary
LOTaD learns optimal task decompositions by generating k candidate decompositions from a reward machine, then using UCB-based selection to explore them while simultaneously training a shared task-conditioned policy for all agents. Each agent's policy receives four inputs: MDP observation, decomposition encoding, current sub-task RM state, and overall task RM state encoding. Training uses PPO with auxiliary rewards for overall task completion, enabling coordination in codependent environments. The method estimates sub-task value from reward histories to inform decomposition selection.

## Key Results
- LOTaD significantly outperforms monolithic task approaches and decomposition selection without learning-informed selection across five environments
- Conditioning on overall task state is critical for performance in codependent dynamics environments, enabling coordination beyond individual sub-tasks
- Improved credit assignment through task decomposition provides substantial sample efficiency gains even when optimal decomposition is unknown a priori

## Why This Works (Mechanism)

### Mechanism 1: Task-Conditioned Policy Architecture
- Claim: Sharing a task-conditioned policy across all agents enables simultaneous learning of decomposition selection and policy optimization.
- Mechanism: A single feedforward neural network receives four inputs per agent: (1) MDP observation, (2) selected decomposition encoding, (3) current sub-task RM state, (4) overall task RM state encoding. This enables generalization across sub-tasks and curriculum effects—simpler sub-tasks teach reusable skills (e.g., "reach HQ") that transfer to complex sub-tasks.
- Core assumption: Sub-tasks share learnable structure; skills from simple sub-tasks transfer to complex ones.
- Evidence anchors:
  - [abstract]: "Our method uses a task-conditioned architecture to simultaneously learn an optimal decomposition and the corresponding agents' policies for each sub-task."
  - [section 3.2]: "Learning a task-conditioned policy allows us to distinguish between different sub-tasks within different decompositions while exploiting the generalization capabilities of multi-task learning as well as the natural curriculum learning inherent in decomposition exploration."
  - [corpus]: Weak/missing—corpus papers focus on LLM-based decomposition, not neural task-conditioned policies for reward machine decomposition.
- Break condition: If sub-tasks are fundamentally dissimilar (no shared structure), multi-task learning degrades; curriculum benefit lost if all decompositions are equally complex.

### Mechanism 2: UCB-Based Decomposition Selection via Value Estimates
- Claim: UCB selection balances exploration of candidate decompositions with exploitation of high-performing ones, identifying optimal decomposition without prior environment knowledge.
- Mechanism: Compute exponential weighted moving sum of rewards per sub-task as value estimate V. Score each decomposition as average of its sub-task value estimates. Apply UCB: Score(d_j) + β·√(ln(H)/n_j), where n_j is selection count. Higher β increases exploration.
- Core assumption: Value estimates correlate with true decomposition quality; early rewards are sufficiently informative.
- Evidence anchors:
  - [abstract]: "LOTaD uses UCB-based selection to explore different candidate decompositions and leverages value estimates to balance exploration and exploitation."
  - [section 3.3]: "A key insight of our work is that we can use rewards from previous executions of a sub-task to estimate the satisfaction likelihood of that sub-task."
  - [corpus]: Weak—corpus papers do not address UCB-based decomposition selection for symbolic tasks.
- Break condition: If rewards are sparse early, value estimates are noisy → poor exploration; too many candidates (large k) makes UCB sample-inefficient (Figure 6 confirms U-shaped performance).

### Mechanism 3: Global Task Conditioning for Dependent Dynamics
- Claim: Conditioning on overall task state (in addition to sub-task state) enables coordination in environments with codependent agent dynamics, which prior work could not handle.
- Mechanism: Each agent receives an encoding of the overall task RM state alongside their sub-task RM state. During training, agents receive a small auxiliary reward for overall task completion. This incentivizes agents to facilitate others' sub-tasks after completing their own (e.g., exiting a hazardous region so another agent can enter).
- Core assumption: Agents can observe shared environment events through labeling function; information sharing for global task state is practical.
- Evidence anchors:
  - [abstract]: "Notably, conditioning on the overall task state proves critical for performance, enabling agents to coordinate beyond their individual sub-tasks."
  - [section 3.4]: "We are able to relax the assumption of independent dynamics... by allowing agents to condition not only on the status of their sub-task, but on the status of the overall task as well."
  - [Figure 5]: Ablation shows LOTaD without overall task encoding fails in Repairs (codependent dynamics) and is slower in Cramped-Corridor.
  - [corpus]: Weak—corpus does not address this specific mechanism.
- Break condition: If agents cannot share event observations (fully decentralized with local sensors only), global task conditioning is impractical; paper notes this limitation in Appendix A.

## Foundational Learning

- **Concept: Reward Machines (RMs)**
  - Why needed here: RMs provide symbolic task structure that can be formally decomposed into sub-tasks. They encode objectives over high-level events (not low-level states) with temporal dependencies.
  - Quick check question: How does a task completion RM differ from a standard reward function? (Answer: RMs are Mealy machines over event alphabet Σ; reward = 1 only on transitions into goal states, 0 otherwise.)

- **Concept: Valid Decomposition of Reward Machines**
  - Why needed here: The paper only considers decompositions where parallel composition of sub-task RMs is bisimilar to original RM. This ensures sub-task satisfaction → overall task satisfaction.
  - Quick check question: What does "valid decomposition" mean formally? (Answer: Parallel composition R₁∥...∥Rₙ is bisimilar to original R; see [21] and Section 2.3.)

- **Concept: Multi-Armed Bandits and UCB**
  - Why needed here: Decomposition selection is framed as a bandit problem where each decomposition is an "arm." UCB provides principled explore-exploit balance.
  - Quick check question: How does UCB balance exploration vs exploitation? (Answer: Adds exploration bonus β·√(ln(H)/nⱼ) to mean reward estimate, favoring less-explored arms.)

## Architecture Onboarding

- **Component map:**
  ATAD decomposition generator -> Task-conditioned policy network -> UCB selector -> PPO trainer -> Labeling function L_G

- **Critical path:**
  1. Define task completion RM R for your domain
  2. Generate candidate decompositions D (k=10; vary based on task complexity)
  3. Implement shared task-conditioned policy with 4 input streams (one-hot encodings work)
  4. Each training episode: UCB selects decomposition -> agents receive sub-task RMs -> collect trajectories -> update value estimates -> PPO update
  5. Monitor: convergence on single decomposition, overall task reward

- **Design tradeoffs:**
  - k (num candidates): Too small -> may miss optimal; too large -> exploration burden (Figure 6)
  - β (UCB exploration): Higher = more exploration; lower = faster convergence but may lock onto suboptimal
  - Overall task reward weight: Small enough to not overwhelm sub-task credit assignment, but sufficient to incentivize coordination in codependent domains
  - Global task conditioning: Requires shared event observability; may not suit fully decentralized settings (Appendix A limitation)

- **Failure signatures:**
  - Performance plateaus without converging: UCB stuck exploring -> reduce k or β
  - Agents complete sub-tasks but overall task fails (codependent domains): Missing overall task conditioning -> verify ablation
  - High variance across seeds: ATAD baseline has high variance when decompositions tie; LOTaD should reduce this
  - Agent obstructs another after completing sub-task: Auxiliary overall-task reward too small or missing

- **First 3 experiments:**
  1. Replicate Four-Buttons: Simplest setting (independent dynamics). Verify LOTaD > Monolithic > ATAD. Confirm removing overall task conditioning has minimal effect.
  2. Ablation on global task conditioning in Repairs: Run LOTaD with/without overall task state encoding. Expect failure without encoding (Figure 5), validating dependent dynamics handling.
  3. Vary k in Four-Buttons: Test k∈{5,10,20,50}. Expect U-shaped curve (Figure 6). Identifies exploration burden vs coverage tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can different neural representations of Reward Machines (RMs) improve sample efficiency by exploiting semantic similarity between overlapping sub-tasks?
- Basis in paper: [explicit] The conclusion states, "We are interested in exploring how different representations of an RM may enable greater sample efficiency by exploiting semantic similarity amongst overlapping sub-tasks."
- Why unresolved: The current implementation uses simple one-hot encodings for sub-tasks, which may fail to capture shared structure between similar sub-tasks across different decompositions.
- What evidence would resolve it: A comparative study showing that embedding-based or graph-neural-network representations of RMs result in faster convergence compared to one-hot encodings.

### Open Question 2
- Question: How does the method used to generate the candidate set of decompositions ($\mathcal{D}$) affect the learning outcome and optimality of LOTaD?
- Basis in paper: [explicit] The authors note in the Appendix, "The quality of our found decomposition will only be as good as the quality of $\mathcal{D}$... Studying how the generation of $\mathcal{D}$ affects the learning outcome of LOTaD is left as a compelling direction for future work."
- Why unresolved: The framework currently relies on heuristics (ATAD) to generate $\mathcal{D}$, but it is unknown if poor candidate generation fundamentally limits the method or if a learned generation process would be superior.
- What evidence would resolve it: Experiments varying the quality and size of $\mathcal{D}$ to measure the sensitivity of the final policy performance and decomposition optimality.

### Open Question 3
- Question: Can the LOTaD framework be generalized to solve multiple distinct tasks by leveraging the curriculum learning inherent in decomposition exploration?
- Basis in paper: [explicit] The authors express interest in "generalizing our multi-agent setting to solving multiple tasks, building on prior work in goal-conditioned RL on automata-based tasks."
- Why unresolved: The current framework focuses on solving a single global task, and it is unclear if the policy architecture can effectively transfer knowledge when the overall task RM changes completely.
- What evidence would resolve it: Demonstrating that agents trained with LOTaD on a distribution of tasks can solve novel task variants faster than agents trained from scratch.

### Open Question 4
- Question: Can the requirement for a global view of the overall task be relaxed to reduce communication overhead while maintaining performance in environments with dependent dynamics?
- Basis in paper: [inferred] The Appendix notes that "Providing this global view... requires agents to receive the same information from a labeling function when environment events occur. This level of information sharing may be impractical in some cases."
- Why unresolved: While the global view solves the issue of dependent agent dynamics, it imposes a communication cost and centralized information requirement that contradicts the goal of decentralized execution in some real-world scenarios.
- What evidence would resolve it: Identifying an alternative mechanism (e.g., implicit communication or local estimation) that allows agents to coordinate in dependent-dynamics environments without explicitly sharing the full global task state.

## Limitations

- The method requires a labeling function for event detection, which demands information sharing about global task state that may not be practical in fully decentralized settings
- The computational complexity of exploring multiple decompositions (k=10) could become prohibitive in larger state spaces
- Comparisons are primarily against strong baselines rather than state-of-the-art methods specifically designed for dependent dynamics

## Confidence

- **High confidence**: The mechanism of task-conditioned policy architecture for enabling generalization across sub-tasks and the effectiveness of UCB-based decomposition selection in balancing exploration and exploitation are well-supported by experimental evidence and ablation studies.
- **Medium confidence**: The claim that global task conditioning is critical for dependent dynamics is supported by ablation studies but relies on assumptions about information sharing that may not generalize to all multi-agent settings.
- **Medium confidence**: The assertion that improved credit assignment through task decomposition provides substantial sample efficiency gains is demonstrated across multiple environments but would benefit from testing in more diverse domains.

## Next Checks

1. **Information sharing requirements**: Test LOTaD in a fully decentralized setting where agents cannot observe shared events, to quantify the performance degradation and validate the stated limitation about information sharing requirements.

2. **Scalability analysis**: Evaluate LOTaD with varying numbers of decomposition candidates (k values beyond the tested range) to better characterize the exploration-exploitation tradeoff and identify practical limits for larger state spaces.

3. **Comparison with dependent dynamics specialists**: Benchmark LOTaD against methods specifically designed for codependent agent dynamics (like Automata-Conditioned methods) to establish its relative advantage in these challenging environments.