---
ver: rpa2
title: Enhancing the Capabilities of Large Language Models for API calls through Knowledge
  Graphs
arxiv_id: '2507.10630'
source_url: https://arxiv.org/abs/2507.10630
tags:
- knowledge
- data
- kg2data
- calls
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of enabling large language models
  (LLMs) to effectively utilize API calls in knowledge-intensive domains, specifically
  meteorology. The core problem is that LLMs struggle with domain-specific knowledge
  and complex queries due to limited access to specialized information and terminology.
---

# Enhancing the Capabilities of Large Language Models for API calls through Knowledge Graphs

## Quick Facts
- arXiv ID: 2507.10630
- Source URL: https://arxiv.org/abs/2507.10630
- Authors: Ye Yang; Xue Xiao; Ping Yin; Taotao Xie
- Reference count: 0
- One-line primary result: KG2data achieves 88.57% call correctness vs. 72.14% for RAG2data and 71.43% for chat2data

## Executive Summary
This paper addresses the challenge of enabling large language models (LLMs) to effectively utilize API calls in knowledge-intensive domains, specifically meteorology. The core problem is that LLMs struggle with domain-specific knowledge and complex queries due to limited access to specialized information and terminology. The proposed solution, KG2data, integrates knowledge graphs with LLMs, ReAct agents, and tool-use technologies to create an intelligent system for data acquisition and query handling.

## Method Summary
KG2data integrates knowledge graphs with LLMs using ReAct agents and tool-use technologies to enable intelligent data acquisition. Unlike traditional approaches that rely solely on LLMs or retrieval-augmented generation (RAG), KG2data uses knowledge graphs as persistent memory to enhance content retrieval, handle complex queries, enable domain-specific reasoning, resolve semantic relationships, and integrate heterogeneous data. This approach avoids the high costs associated with fine-tuning LLMs for evolving domain knowledge.

## Key Results
- Call correctness: 88.57% (KG2data) vs. 72.14% (RAG2data) vs. 71.43% (chat2data)
- Name recognition failure: 1.43% (KG2data) vs. 16% (RAG2data) vs. 7.14% (chat2data)
- Hallucination failure: 0% (KG2data) vs. 10% (RAG2data) vs. 8.57% (chat2data)

## Why This Works (Mechanism)

### Mechanism 1: Knowledge Graph as Persistent Domain Memory
Using a knowledge graph as structured external memory reduces hallucination and name recognition failures in API calls. The KG stores domain entities, API parameter schemas, and semantic relationships, allowing the system to ground LLM outputs in verified domain knowledge rather than relying on parametric memory.

### Mechanism 2: ReAct Agent Loop for Iterative Query Decomposition
ReAct-style reasoning enables multi-step query handling that outperforms single-pass approaches. The ReAct agent decomposes complex queries into sub-tasks, queries the KG for intermediate information, formulates API calls, observes results, and iterates if needed.

### Mechanism 3: Cost Avoidance Through Externalized Knowledge
Externalizing domain knowledge to a KG avoids fine-tuning costs while preserving adaptability. Rather than encoding domain knowledge into model weights, the KG serves as editable external memory. Updates to domain knowledge or API schemas require only KG modifications, not model retraining.

## Foundational Learning

- Concept: Knowledge Graph Construction and Querying
  - Why needed here: The core innovation relies on representing domain terminology and API schemas as nodes/edges.
  - Quick check question: Given a weather API with parameters `station_id`, `metric_type`, and `time_range`, sketch how you'd represent these as KG entities with relationships.

- Concept: ReAct (Reasoning + Acting) Agent Pattern
  - Why needed here: KG2data uses ReAct agents as the orchestration layer between user queries and API execution.
  - Quick check question: Write out the trace of a ReAct agent handling "What was the average temperature at Station X last week?" — show the Thought-Action-Observation loop.

- Concept: API Parameter Binding and Validation
  - Why needed here: The system's success metric (88.57% call correctness) depends on correctly mapping natural language to API parameters.
  - Quick check question: How would you validate that "precipitation in millimeters" maps to the correct API parameter before making the call?

## Architecture Onboarding

- Component map:
User Query → LLM (language understanding) → ReAct Agent (orchestration loop) → Knowledge Graph (domain memory: terminology, API schemas, relationships) → API Call Constructor (parameter binding) → Virtual/Real API (data retrieval) → Response Formatter

- Critical path:
  1. Query parsing and entity extraction
  2. KG lookup for domain terms and valid API parameters
  3. Parameter binding validation (prevents hallucination failures)
  4. API call construction and execution
  5. Result formatting with domain context

- Design tradeoffs:
  - KG coverage vs. construction cost: A more comprehensive KG improves accuracy but requires domain expert time.
  - ReAct loop depth vs. latency: More reasoning steps improve complex query handling but increase response time.
  - Virtual API testing vs. real API risk: The paper uses a virtual API for evaluation; production requires handling real API failures.

- Failure signatures:
  - Name recognition failure (1.43% in KG2data vs. 16% in RAG2data): System doesn't recognize domain terminology → check KG entity coverage.
  - Hallucination failure (0% in KG2data): System invents non-existent parameters → check KG parameter schema enforcement.
  - Low call correctness: System selects wrong API or parameters → check KG relationship definitions between query terms and API schemas.

- First 3 experiments:
  1. Baseline replication: Implement chat2data (pure LLM) and RAG2data baselines, then verify you can reproduce the paper's reported metrics before building KG2data.
  2. KG ablation: Run KG2data with progressively reduced KG content (remove 25%, 50%, 75% of entities) to identify which KG components contribute most to accuracy.
  3. Domain transfer test: Apply the same architecture to a different knowledge-intensive domain (e.g., the genomics domain from GeneGPT in corpus) to test generalizability of the approach.

## Open Questions the Paper Calls Out

- How does KG2data performance translate to real-world API environments compared to the virtual API simulation used in the evaluation?
- Can the system maintain a 0% hallucination rate when applied to domains with less structured terminology than meteorology?
- What specific failure modes constitute the remaining 11.43% of incorrect API calls, and are they solvable via Knowledge Graph expansion?
- What is the latency overhead introduced by integrating Knowledge Graph retrieval compared to the RAG2data baseline?

## Limitations

- Evaluation relies on a virtual API rather than real-world meteorological data sources, limiting ecological validity.
- No ablation study isolates the contribution of the knowledge graph versus the ReAct agent versus the LLM itself.
- Knowledge graph schema and population methodology are not fully specified, making replication dependent on domain expertise and judgment.

## Confidence

- High confidence in the core claim that integrating knowledge graphs reduces hallucination and name recognition failures, supported by the 0% hallucination rate and 1.43% name recognition failure.
- Medium confidence in the cost avoidance claim, as the paper asserts KG maintenance is cheaper than fine-tuning but provides no quantitative comparison.
- Medium confidence in the domain transfer potential, given the success of similar architectures in genomics (GeneGPT) but without KG2data being tested outside meteorology.

## Next Checks

1. Implement the knowledge graph with the meteorology domain's terminology and API schemas, then verify that all evaluation queries can be mapped to KG entities before testing.
2. Confirm that the virtual API's parameter space and response patterns accurately reflect real meteorological data sources, as evaluation results depend on this fidelity.
3. Replace the virtual API with a live meteorological data source to test whether the 88.57% call correctness holds under real-world API failures, rate limits, and data inconsistencies.