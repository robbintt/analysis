---
ver: rpa2
title: Improving Minimax Estimation Rates for Contaminated Mixture of Multinomial
  Logistic Experts via Expert Heterogeneity
arxiv_id: '2602.00939'
source_url: https://arxiv.org/abs/2602.00939
tags:
- page
- expert
- where
- experts
- estimation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the statistical properties of the contaminated
  mixture-of-experts (MoE) model in a classification setting, where a frozen pre-trained
  expert is combined with a trainable adapter expert via a softmax gating function.
  The authors analyze both homogeneous and heterogeneous expert regimes and establish
  uniform convergence rates for maximum likelihood estimation of the model parameters.
---

# Improving Minimax Estimation Rates for Contaminated Mixture of Multinomial Logistic Experts via Expert Heterogeneity

## Quick Facts
- **arXiv ID:** 2602.00939
- **Source URL:** https://arxiv.org/abs/2602.00939
- **Reference count:** 40
- **Primary result:** Expert heterogeneity in contaminated MoE models achieves optimal $O(n^{-1/2})$ parameter estimation rates, while homogeneity degrades rates due to model collapse.

## Executive Summary
This paper analyzes the statistical properties of contaminated mixture-of-experts (MoE) models for classification, where a frozen pre-trained expert is combined with a trainable adapter expert via softmax gating. The authors establish uniform convergence rates for maximum likelihood estimation and prove that expert heterogeneity (distinct expert structures) enables parametric convergence rates of $O(n^{-1/2})$, while homogeneity (identical structures) leads to slower-than-parametric rates due to potential model collapse. The theoretical findings are validated through synthetic experiments and matched with minimax lower bounds, demonstrating optimal sample efficiency for heterogeneous configurations.

## Method Summary
The study examines MLE for contaminated MoE models with one frozen expert and one trainable adapter expert using softmax gating. Synthetic data is generated with covariates $X \sim \mathcal{N}(0, I_d)$ and ground truth parameters including $\beta^*$, $\tau^*$, and $\eta_0$. The EM algorithm with BFGS optimization estimates parameters, with convergence rates analyzed for both homogeneous (same expert structure) and heterogeneous (distinct expert structures) regimes. The analysis assumes strong identifiability and localized parameter settings where $\eta^* \to \eta_0$.

## Key Results
- Heterogeneous expert regimes achieve parametric convergence rates of $O(n^{-1/2})$
- Homogeneous expert regimes suffer slower-than-parametric rates due to model collapse when $\eta^* \to \eta_0$
- Softmax gating prevents vanishing weights unlike input-free gates
- Matching minimax lower bounds confirm theoretical optimality
- Numerical experiments validate theoretical predictions

## Why This Works (Mechanism)

### Mechanism 1: Heterogeneity Restores Strong Identifiability
- **Claim:** Distinct expert structures ensure parameter-space movements map uniquely to density-space changes
- **Core assumption:** Expert function $h$ is twice differentiable and satisfies Definition 1
- **Evidence:** Abstract states heterogeneous regimes achieve parametric order rates
- **Break condition:** If adapter expert structure converges to frozen expert structure

### Mechanism 2: Homogeneity Induces Model Collapse
- **Claim:** Identical expert structures cause parameter estimation rates to slow significantly
- **Core assumption:** Ground-truth parameters vary with sample size ($\eta^* \to \eta_0$)
- **Evidence:** Section 3.1 shows rates scale poorly with $\|\Delta \eta^*\|^{-1}$
- **Break condition:** If adapter parameters bounded away from frozen parameters

### Mechanism 3: Softmax Gating Avoids Vanishing Weights
- **Claim:** Softmax weights remain strictly positive, maintaining adapter influence
- **Core assumption:** Covariate space $\mathcal{X}$ is bounded
- **Evidence:** Page 2 states softmax weights cannot converge to zero under compact spaces

## Foundational Learning

- **Concept: Strong Identifiability (Definition 1)**
  - **Why needed:** Central constraint distinguishing fast heterogeneous from slow homogeneous regimes
  - **Quick check:** Does chosen activation function guarantee linear independence of expert derivatives and covariates?

- **Concept: Minimax Lower Bounds**
  - **Why needed:** Prove achieved rates are optimal and cannot be improved
  - **Quick check:** If estimator achieves $O(n^{-1/2})$ but minimax lower bound is $O(n^{-1/4})$, is estimator optimal?

- **Concept: Taylor Expansion of Densities**
  - **Why needed:** Decompose density discrepancy into linearly independent terms
  - **Quick check:** Why does cancellation of first-order terms in Taylor expansion lead to slower convergence rates?

## Architecture Onboarding

- **Component map:** Input $X$ → Both Experts → Softmax Gate → Weighted Prediction
- **Critical path:** Input $X$ → Frozen Expert ($h_0$) & Adapter Expert ($h$) → Softmax Gate → Weighted Prediction
- **Design tradeoffs:**
  - Homogeneity: Parameter efficient but statistically inefficient
  - Heterogeneity: Statistically efficient but requires architectural diversity
- **Failure signatures:**
  - ReLU usage: Fails Strong Identifiability due to zero second derivative
  - Parameter Collapse: In homogeneous settings when $\eta^* \approx \eta_0$
- **First 3 experiments:**
  1. Verify rates (heterogeneous): Linear frozen, Tanh adapter; plot $\log(\text{Error})$ vs $\log(n)$
  2. Verify rates (homogeneous): Tanh frozen, Tanh adapter; confirm slope $< -0.5$
  3. Activation ablation: Test ReLU vs GELU in heterogeneous setting

## Open Questions the Paper Calls Out
- **Open Question 1:** How do convergence rates change when fine-tuning with multiple adapter components instead of a single adapter?
- **Open Question 2:** Can alternative gating functions improve stability compared to softmax gating?
- **Open Question 3:** Can strong identifiability be relaxed to accommodate non-smooth functions like ReLU?

## Limitations
- Theoretical guarantees rely heavily on Strong Identifiability assumption not empirically verified across activation functions
- Homogeneous regime analysis assumes specific parameter collapse pattern that may not hold in all scenarios
- Experiments use synthetic data with controlled ground truth parameters, limiting real-world generalizability

## Confidence

**High Confidence:** Theoretical derivation of minimax lower bounds and softmax gating characterization

**Medium Confidence:** Distinction between homogeneous/heterogeneous convergence rates depends on Strong Identifiability strength

**Low Confidence:** Specific claim about ReLU failing Strong Identifiability lacks empirical validation

## Next Checks

1. **Activation Function Ablation Study:** Test ReLU, GELU, and Tanh in heterogeneous regime to verify ReLU degradation due to lack of Strong Identifiability

2. **Real-World Data Validation:** Apply MoE framework to CIFAR-10 or NLP task to assess practical performance improvements between homogeneous vs heterogeneous experts

3. **Parameter Initialization Robustness:** Systematically vary initialization strategy and measure sensitivity of convergence rates to initialization distance from ground truth