---
ver: rpa2
title: Evaluation-Time Policy Switching for Offline Reinforcement Learning
arxiv_id: '2503.12222'
source_url: https://arxiv.org/abs/2503.12222
tags:
- offline
- policy
- learning
- uncertainty
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of offline reinforcement learning,
  where an agent learns from a fixed dataset without environmental interaction. Existing
  methods struggle to balance improving behavior using dynamic programming with staying
  close to the data, often requiring extensive hyperparameter tuning.
---

# Evaluation-Time Policy Switching for Offline Reinforcement Learning

## Quick Facts
- arXiv ID: 2503.12222
- Source URL: https://arxiv.org/abs/2503.12222
- Reference count: 40
- The paper proposes a novel evaluation-time policy switching technique that dynamically combines a reinforcement learning agent with a behavioral cloning agent, demonstrating strong performance across various D4RL benchmarks without extensive hyperparameter tuning.

## Executive Summary
This paper addresses the challenge of offline reinforcement learning (RL), where an agent must learn from a fixed dataset without environmental interaction. The authors propose a novel policy switching technique that dynamically combines a reinforcement learning agent with a behavioral cloning agent at evaluation time. This approach uses epistemic uncertainty from an ensemble of critics and aleatoric uncertainty from the dataset to decide which agent's action to take. The method demonstrates strong performance across various D4RL benchmarks, outperforming individual algorithms and competing with state-of-the-art methods. Notably, it achieves this without extensive hyperparameter tuning and naturally extends to offline-to-online fine-tuning, allowing for quick and safe adaptation to new data.

## Method Summary
The method trains two separate policies: a reinforcement learning agent (TD3-N with 10 critics) and a Gaussian behavioral cloning (BC) agent. During evaluation, the system calculates a pessimistic Q-value for the RL action by subtracting a term proportional to epistemic uncertainty (standard deviation across the critic ensemble) from the median Q-value. If this adjusted value is lower than the Q-value of the BC action, the system switches to the BC action. The uncertainty penalty is scaled by a metric of dataset diversity (aleatoric uncertainty), allowing the agent to adapt its conservativeness to the quality of the offline dataset. This switching mechanism remains active during online fine-tuning, automatically annealing the reliance on BC as the agent explores and reduces epistemic uncertainty in new regions.

## Key Results
- Policy switching outperforms individual algorithms (TD3-N, BC) on D4RL benchmarks
- The method achieves competitive performance with state-of-the-art offline RL methods without extensive hyperparameter tuning
- Natural extension to offline-to-online fine-tuning enables quick and safe adaptation to new data

## Why This Works (Mechanism)

### Mechanism 1: Evaluation-Time Uncertainty Filtering
Conditionally selecting actions at evaluation time using an ensemble-based uncertainty metric mitigates the overestimation of out-of-distribution (OOD) actions common in offline RL. The system trains an RL policy and a BC policy independently. During evaluation, it calculates a "pessimistic" Q-value for the RL action by subtracting the epistemic uncertainty (standard deviation across a critic ensemble) from the median Q-value. If this adjusted value is lower than the Q-value of the BC action, the system switches to the BC action.

### Mechanism 2: Dataset Diversity Modulation (Aleatoric Uncertainty)
Scaling the uncertainty penalty by a metric of dataset diversity allows the agent to adapt its conservativeness to the quality of the offline dataset. The method computes the normalized standard deviation of returns in the dataset, which modulates the penalty function. If the dataset has low diversity, the penalty for deviating from the RL policy increases, forcing the agent to rely more on BC.

### Mechanism 3: Automatic Annealing for Online Fine-tuning
Keeping the switching mechanism active during online fine-tuning stabilizes the transition from offline to online learning without manual hyperparameter scheduling. As the agent explores online, new data is added to the replay buffer, reducing epistemic uncertainty in those regions. This naturally raises the pessimistic Q-value, causing the agent to automatically select RL actions more frequently (annealing the reliance on BC).

## Foundational Learning

- **Concept: Epistemic vs. Aleatoric Uncertainty**
  - Why needed here: The core logic relies on distinguishing "what the model doesn't know due to lack of data" (Epistemic) vs "what is inherently random in the data" (Aleatoric). Confusing these leads to improper penalty scaling.
  - Quick check question: Does the agent become more conservative if the dataset is smaller (epistemic) or if the dataset is noisier (aleatoric)?

- **Concept: Distributional Shift in Offline RL**
  - Why needed here: The paper attempts to solve the "exploitation of overestimated OOD actions." Understanding why standard Q-learning fails offline is prerequisite to grasping why the "switch" is necessary.
  - Quick check question: Why does maximizing Q-values often fail in offline RL but works in online RL? (Answer: Offline cannot correct overestimation via environmental feedback).

- **Concept: Ensemble Disagreement**
  - Why needed here: The method uses N=10 critics. Understanding how variance across models acts as a uncertainty sensor is critical for implementation.
  - Quick check question: If all 10 critics agree on a value, is the epistemic uncertainty high or low?

## Architecture Onboarding

- **Component map:**
  - Inputs: Offline Dataset
  - Parallel Trainers:
    1. RL Trainer (TD3-N): Updates Policy + Critic Ensemble
    2. BC Trainer: Updates Policy (Supervised Learning)
  - Evaluation Meta-Controller (The Switch): Runs inference loop. Takes state, computes actions, Q-values, and outputs final action

- **Critical path:**
  1. Initialization of the Critic Ensemble (size N=10)
  2. Calculation of dataset statistic
  3. The switching logic formula

- **Design tradeoffs:**
  - Inference Latency: Requires forward passes for two policies and N critics at every evaluation step
  - Training Simplicity: Decouples RL and BC training (can run concurrently) vs. Joint Training (slower, more complex gradients)
  - Sensitivity: The penalty function hyperparameters control the sensitivity

- **Failure signatures:**
  - Overconfidence Collapse: On narrow "expert" datasets, the ensemble variance collapses to near zero, causing the agent to trust the RL policy blindly
  - Stuck in BC: If online fine-tuning faces new dynamics not reflected in the ensemble variance drop, the agent may refuse to switch to RL actions

- **First 3 experiments:**
  1. Sanity Check (MuJoCo Medium): Train TD3-N, BC, and PS on halfcheetah-medium. Verify PS score > max(TD3-N, BC)
  2. Ablation (Fixed vs Dynamic Penalty): Compare PS with dynamic vs. fixed penalty to validate the aleatoric mechanism
  3. Online Annealing Visualization: Run offline-to-online fine-tuning on AntMaze. Plot the "Proportion of BC actions used" over training steps

## Open Questions the Paper Calls Out

### Open Question 1
Can alternative methods for quantifying epistemic uncertainty be integrated into the policy switching framework to support non-ensemble algorithms like IQL?
- Basis in paper: The authors state in the conclusion that "Future work could explore alternative methods for quantifying epistemic uncertainty, potentially enabling the incorporation of algorithms like IQL."
- Why unresolved: The current method relies on variance across an ensemble of critics, a feature not inherent to all RL algorithms.
- What evidence would resolve it: A demonstration of the switching technique successfully using uncertainty estimates from a non-ensemble method (e.g., dropout or distributional RL) without performance degradation.

### Open Question 2
Can the theoretical connection between evaluation-time switching and "critical state" identification be formalized to provide stronger performance guarantees?
- Basis in paper: The conclusion notes, "Future theoretical work could formalise these connections, potentially leading to stronger performance guarantees for our policy switching method."
- Why unresolved: The current work is primarily empirical; the link to theories regarding critical/non-critical states is conceptual and unproven.
- What evidence would resolve it: A theoretical proof showing that the switching mechanism reduces error propagation in critical states similarly to explicit policy constraints.

### Open Question 3
How can the policy switching mechanism be adapted to prevent performance degradation in narrow, high-quality datasets where critic ensembles suffer from overconfidence?
- Basis in paper: The paper reports struggles with "hopper-expert" datasets where "overconfidence from the ensemble of critics... prevents the ensemble from being as useful a measure of epistemic uncertainty."
- Why unresolved: The current reliance on ensemble variance fails when the data distribution is narrow, causing the epistemic uncertainty signal to be unreliable.
- What evidence would resolve it: A modification to the uncertainty quantification that maintains high uncertainty estimates for out-of-distribution actions even when trained on narrow data supports.

## Limitations

- The method struggles with extremely narrow datasets (e.g., expert data) where critic ensembles suffer from overconfidence
- Aleatoric uncertainty is estimated only from return variance, which may poorly reflect state-space coverage in sparse-reward tasks
- Exact penalty function parameters (m, α) were not reported, limiting precise reproduction

## Confidence

- Switching mechanism effectiveness: High
- Aleatoric penalty improves generalization: Medium
- Automatic online annealing without tuning: Medium

## Next Checks

1. **Ablation on penalty hyperparameters**: Sweep m and α to confirm robustness or identify narrow operating ranges
2. **Test on narrow datasets**: Evaluate PS on hopper-expert or walker-expert to check for overconfidence failure modes
3. **Compare uncertainty estimators**: Replace return-variance-based σD with trajectory-length or state-diversity metrics in AntMaze tasks