---
ver: rpa2
title: Reward Models Identify Consistency, Not Causality
arxiv_id: '2502.14619'
source_url: https://arxiv.org/abs/2502.14619
tags:
- reward
- question
- arxiv
- reasoning
- truncation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges common assumptions about how reward models
  (RMs) evaluate reasoning quality in large language models. Through systematic experiments,
  the authors demonstrate that state-of-the-art RMs prioritize structural consistency
  over true causal correctness.
---

# Reward Models Identify Consistency, Not Causality

## Quick Facts
- **arXiv ID**: 2502.14619
- **Source URL**: https://arxiv.org/abs/2502.14619
- **Reference count**: 5
- **Primary result**: State-of-the-art reward models prioritize structural consistency over true causal correctness in evaluating reasoning quality.

## Executive Summary
This paper challenges common assumptions about how reward models (RMs) evaluate reasoning quality in large language models. Through systematic experiments, the authors demonstrate that state-of-the-art RMs prioritize structural consistency over true causal correctness. Key findings show that removing the problem statement has minimal impact on reward scores, while disrupting reasoning steps or altering numerical values significantly affects evaluations. The analysis reveals that RMs primarily rely on learned reasoning patterns rather than explicit problem comprehension.

## Method Summary
The authors generate 32 solution trajectories per question using a base LLM with temperature=0.8 and top_p=1.0. They then apply various perturbations to the (question, solution) pairs including question truncation, step truncation, question shuffling, and numerical value modification. These perturbed inputs are scored using pre-trained reward models via vLLM on H100 GPUs. The primary metrics include absolute reward error, Spearman rank correlation, and Best-of-N accuracy across different numbers of candidates (N=4,8,16,32).

## Key Results
- Removing the problem statement has minimal impact on reward scores
- Altering numerical values or disrupting reasoning flow significantly affects RM outputs
- RMs exhibit strong dependence on complete reasoning trajectories—truncated steps lead to significant variations in reward assignments

## Why This Works (Mechanism)

### Mechanism 1: Pattern-Based Reward Assignment via Structural Consistency
Reward models assign high scores based on internal structural consistency of solution trajectories rather than verifying causal correctness between problem and solution. The observed robustness to question removal is interpreted as evidence of pattern matching over true comprehension.

### Mechanism 2: Trajectory-Completeness Dependency
Reward assignment critically depends on complete, ordered reasoning trajectories more than on final answers or problem statements. RMs have learned that "quality" associates with presence of full derivations, creating bias against valid but shorter reasoning.

### Mechanism 3: Semantic-Coherence Sensitivity as Proxy for Correctness
RMs are highly sensitive to surface-level semantic coherence between problem text and solution text. They leverage strong statistical associations between problem keywords and solution structures as a low-cost proxy for actual verification.

## Foundational Learning

- **Reward Models (RM) vs. Outcome/Process RMs**: Understanding the distinction is crucial—ORMs score final results while PRMs score each step. Quick check: Does a Process Reward Model (PRM) evaluate only the final answer? (No, a PRM provides a score for each step).

- **Best-of-N Selection**: This evaluation framework measures how often an RM can select a correct answer from N candidates. Quick check: In a Best-of-N experiment, what does a drop in accuracy after a perturbation indicate? (It indicates the perturbation has degraded the RM's ability to reliably identify the best solution).

- **Spearman's Rank Correlation Coefficient (ρ)**: Used to quantify how consistently an RM ranks different solutions. Quick check: If an RM assigns random scores after a perturbation, what would the Spearman correlation be compared to its original rankings? (Close to zero).

## Architecture Onboarding

- **Component map**: Input Perturbation Module -> Reward Model Backbone -> Scalar Reward Head -> Evaluation Orchestrator
- **Critical path**: Generate candidate trajectories, score with baseline RM, apply perturbation, re-score with same RM, compare distributions using absolute error and Spearman correlation
- **Design tradeoffs**: ORM vs. PRM for analysis (PRMs provide granular data but are more complex), choice of perturbation experiments (question truncation tests causal understanding, shuffling tests semantic coherence)
- **Failure signatures**: Consistency bias (high reward for well-written but irrelevant solutions), trajectory-length hacking (rewarding verbose but potentially incorrect solutions)
- **First 3 experiments**: 1) Baseline vs. question-truncation error (compute |r(x,y) - r(None,y)|), 2) Semantic coherence test (pair solutions with random questions and measure accuracy drop), 3) Trajectory completeness ablation (systematically truncate reasoning steps and measure reward score drop)

## Open Questions the Paper Calls Out

### Open Question 1
How can counterfactual reasoning tasks be effectively integrated into reward model training to shift evaluation from surface-level patterns to causal dependencies? Current models prioritize internal coherence over actual comprehension, failing to verify if reasoning corresponds to the question.

### Open Question 2
What specific training objectives or architectures can enable reward models to explicitly capture logical validity rather than just ranking correctness? Existing RMs act as "consistency checkers" rather than verifiers of logic.

### Open Question 3
To what extent does the reliance on consistency-driven evaluation hinder the ability of reward models to generalize to novel problem distributions? If RMs rely on learned patterns rather than problem comprehension, they likely fail on out-of-distribution tasks requiring novel reasoning paths.

## Limitations
- Findings primarily focus on mathematical reasoning tasks, leaving generalizability to other domains uncertain
- The interpretation of consistency bias as a "fundamental limitation" represents one theoretical framing of the results
- Experiments may not fully capture the complexity of RM training objectives and real-world usage scenarios

## Confidence
- **High Confidence**: Empirical findings regarding relative impact of different perturbations
- **Medium Confidence**: Interpretation that findings indicate a "fundamental limitation" in reward modeling
- **Medium Confidence**: Conclusion that RMs are "more effective at ranking responses based on coherence than verifying logical validity"

## Next Checks
1. Test whether the consistency bias persists in non-mathematical reasoning tasks (commonsense reasoning, causal inference) to assess generalizability
2. Train RMs with explicit counterexamples that pair coherent reasoning patterns with incorrect answers to determine if bias can be reduced
3. Compare RM rankings against human expert evaluations across different perturbation conditions to establish alignment with human judgment of reasoning quality