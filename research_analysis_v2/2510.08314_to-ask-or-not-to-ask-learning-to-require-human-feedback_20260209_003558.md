---
ver: rpa2
title: 'To Ask or Not to Ask: Learning to Require Human Feedback'
arxiv_id: '2510.08314'
source_url: https://arxiv.org/abs/2510.08314
tags:
- expert
- surr
- learning
- human
- feedback
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Learning to Ask (LtA), a framework for human-AI
  collaboration that learns when to query a human expert and integrates their feedback
  into predictions, rather than simply deferring as in Learning to Defer (LtD). The
  authors show that LtD can be suboptimal in certain data distributions and derive
  the optimal selection strategy for LtA under budget constraints.
---

# To Ask or Not to Ask: Learning to Require Human Feedback

## Quick Facts
- arXiv ID: 2510.08314
- Source URL: https://arxiv.org/abs/2510.08314
- Reference count: 40
- Learning to Ask (LtA) framework achieves up to 5% improvement over Learning to Defer (LtD) by integrating expert feedback into predictions rather than deferring

## Executive Summary
This paper introduces Learning to Ask (LtA), a framework for human-AI collaboration that learns when to query a human expert and integrates their feedback into predictions, rather than simply deferring as in Learning to Defer (LtD). The authors show that LtD can be suboptimal in certain data distributions and derive the optimal selection strategy for LtA under budget constraints. They propose two practical training approaches: a sequential regime (LtA-Seq) and a joint regime (LtA-Joint) with a novel realizable-consistent surrogate loss. Experiments on synthetic and real-world medical image classification tasks demonstrate that LtA consistently matches or outperforms LtD, achieving up to 5% improvement with richer forms of expert feedback. LtA-Seq shows sensitivity to underfitting at high coverage levels, but this can be mitigated by increasing the defer cost.

## Method Summary
The LtA framework consists of three components: a standard ML model f(X→Y), an enriched model g(X×H→Y) that conditions on expert feedback h, and a selector s(X→{0,1}) that decides when to query the expert. The enriched model is trained with additional expert feedback h ∈ H as input features via FiLM conditioning. Two training regimes are proposed: LtA-Seq trains g first then optimizes f+s via LtD losses, while LtA-Joint pretrains f and g then optimizes jointly with a novel surrogate loss. The optimal selection strategy queries g when expected loss reduction exceeds a calibrated threshold, derived from budget-constrained optimization theory. Experiments compare LtA variants against LtD baseline and individual human/machine baselines on synthetic and NIH Chest X-ray datasets.

## Key Results
- LtA consistently matches or outperforms LtD across coverage levels, with up to 5% accuracy improvement when using richer expert feedback (Unc-Feedback vs LtD-Feedback)
- LtA-Seq shows underfitting of f at high coverage (1-β ≥ 0.70) due to over-reliance on g, which can be mitigated by increasing defer cost δ
- LtA-Joint avoids underfitting through joint training regularization and outperforms LtA-Seq in regimes where f is underfit
- Both LtA variants achieve complementarity (team performance exceeds both individuals) when expert feedback contains complementary signal beyond standard features

## Why This Works (Mechanism)

### Mechanism 1
LtA achieves complementarity by allowing human feedback to augment rather than replace ML predictions. The enriched model g: X × H → Y ingests expert feedback h ∈ H as additional input features rather than treating the expert as an alternative decision-maker. When s(x) = 1, the system queries the expert for h, then feeds (x, h) to g. This allows the model to learn synergistic patterns between machine-observable features and expert-only information. Core assumption: Expert feedback provides information complementary to (not redundant with) the base features X. Evidence: Figure 3b shows LtA with LtD-Feedback (hard predictions) performs equivalently to LtD, indicating no gain when feedback lacks complementary signal.

### Mechanism 2
The optimal selection strategy queries the enriched model when the expected loss reduction exceeds a calibrated threshold. Theorem 1 derives that the optimal selector s*(x) = 1 iff ΔE(ℓ_f, ℓ_g) > τ*_β, where ΔE is the difference in expected conditional risk between f and g, and τ*_β is the (1-β)-quantile of this difference. Intuitively: query only when the enriched model is substantially more confident (lower expected loss) than the standard model, subject to budget β. Core assumption: The expected loss difference ΔE is continuous and can be estimated from calibration data. Evidence: Section 5 describes using the calibration set to estimate the deferral threshold τ_β for each coverage level.

### Mechanism 3
Joint training with the proposed surrogate loss regularizes against over-reliance on the enriched model. LtA-Joint optimizes f, g, and s simultaneously using loss Eq. 10: ℓ_f(·)ℓ_s(-s̃(x)) + ℓ_g(·)ℓ_s(s̃(x)). The multiplicative coupling means the selector must balance both terms, preventing premature convergence to s(x) = 1 everywhere. LtA-Seq trains g first without this coupling, leading to underfitting of f when g is strong. Core assumption: The surrogate losses satisfy conditions (a) and (b) in Theorem 2 (comp-sum loss for predictors, margin-based monotone loss for selector). Evidence: Figure 3a shows LtA-Seq underperforms LtD at high coverage (1-β ≥ 0.70); LtA-Joint does not.

## Foundational Learning

- **Concept: Learning to Defer (LtD) surrogate losses and Bayes-consistency**
  - Why needed here: LtA builds directly on LtD theory. The paper uses existing LtD losses for LtA-Seq and derives analogous consistency guarantees for LtA-Joint.
  - Quick check question: Can you explain why the standard 0-1 deferral loss is intractable and how surrogate losses (e.g., cross-entropy combinations) enable optimization?

- **Concept: Budget-constrained optimization and Lagrangian duality**
  - Why needed here: Theorem 1's proof relies on dual formulation with Lagrangian multiplier λ. Understanding this derivation is necessary to adapt the framework to different cost structures.
  - Quick check question: Given the optimal selector s*_λ(x) = I{ΔE > λ}, what happens to the threshold as the budget β → 0?

- **Concept: Realizable vs. Bayes-consistency**
  - Why needed here: Theorem 2 proves realizable-consistency (asymptotic recovery if a perfect hypothesis exists in the class), but the paper explicitly notes Bayes-consistency remains open.
  - Quick check question: Why is realizable-consistency weaker than Bayes-consistency, and what failure mode does it not guarantee against?

## Architecture Onboarding

- **Component map:**
  - Standard predictor f: X → Y (e.g., DenseNet for X-rays, MLP for synthetic)
  - Enriched predictor g: X × H → Y (e.g., DenseNet + FiLM conditioning layer for injecting feedback h)
  - Selector s: X → {0, 1} (logit-based threshold, Eq. 13 uses sigmoid/tanh)
  - Feedback space H: can be labels (LtD-Feedback), uncertainty vectors (Unc-Feedback), or other structured annotations

- **Critical path:**
  1. Define H and collect expert feedback for training data
  2. Pretrain f and g independently (for LtA-Joint) or train g first (for LtA-Seq)
  3. Train selector s using calibration set to estimate τ_β
  4. At inference: compute s(x); if 1, query expert for h and predict with g(x, h); else predict with f(x)

- **Design tradeoffs:**
  - Sequential vs. Joint: LtA-Seq is simpler (reuses LtD code) but risks underfitting f; LtA-Joint is more robust but requires careful loss balancing
  - Feedback richness: LtD-Feedback (labels) provides limited gains; Unc-Feedback (probabilities) yields ~5% improvement but requires richer annotations
  - Defer cost δ: mitigates LtA-Seq underfitting but can harm LtA-Joint; tune on validation set

- **Failure signatures:**
  - LtA-Seq accuracy drops at high coverage (1-β > 0.5): underfitting of f due to g dominance; increase δ
  - LtA-Joint fails to improve over LtD: feedback h may lack complementary signal; audit H content
  - Selector collapses to s(x) = 1 everywhere: calibration threshold τ_β may be too low; check calibration set size

- **First 3 experiments:**
  1. Replicate the synthetic experiment (Figure 3a) to validate your implementation; verify that LtA-Seq shows underfitting at high coverage while LtA-Joint does not
  2. Ablate feedback type on your domain: compare LtD-Feedback (labels only) vs. richer feedback (e.g., feature annotations, uncertainty) to quantify complementarity
  3. Sweep δ ∈ {0.0, 0.05, 0.1, 0.2} for LtA-Seq on your data to find the setting that closes the gap to LtA-Joint at your target coverage level

## Open Questions the Paper Calls Out

- **Question:** Can surrogate losses be designed to achieve Bayes-consistency, rather than just realizable-consistency, for the Learning to Ask (LtA) framework?
  - Basis in paper: The authors state in the Limitations section that while they characterized a realizable-consistent loss, "we still lack Bayes-consistency."
  - Why unresolved: Realizable-consistency relies on the assumption that the perfect hypothesis exists within the chosen class, which is often not true in practice.
  - What evidence would resolve it: A theoretical proof or derivation of a surrogate loss that guarantees convergence to the Bayes optimal solution without requiring the realizability assumption.

- **Question:** How can a model learn not only when to query an expert but also what type of feedback to request (e.g., labels vs. partial annotations)?
  - Basis in paper: The authors identify "understanding which type of human feedback is most useful for a given instance" as an important open question.
  - Why unresolved: The current LtA formulation relies on a fixed query structure, whereas real-world applications may require adapting the form of the query dynamically.
  - What evidence would resolve it: An extension of the LtA framework that successfully optimizes over a set of distinct feedback types and demonstrates improved efficiency in a multi-modal feedback setting.

- **Question:** Does the LtA framework maintain its performance advantages when interacting with human experts who exhibit non-stationary behavior or fatigue?
  - Basis in paper: The experiments rely on simulated experts or static consensus labels (e.g., "simulated expert," "expert consensus"), which do not capture the dynamic nature of live human feedback.
  - Why unresolved: The current evaluation assumes a fixed expert model $f'$ or static feedback distribution, ignoring the potential for human error or reliability shifts over time.
  - What evidence would resolve it: Empirical results from user studies where the framework interacts with human annotators in real-time, measuring performance stability as human reliability fluctuates.

## Limitations

- Synthetic data generation details are underspecified (feature distribution parameters, expert information gap, noise levels), making exact replication difficult
- FiLM conditioning configuration for X-ray experiments lacks details on architecture and hyperparameters
- Surrogate loss implementation details are sparse, particularly how defer cost δ is incorporated during training vs. calibration
- Generalization to other domains depends heavily on the availability and quality of expert feedback in H, which may not be as structured or rich as in the medical imaging setting

## Confidence

- **High confidence**: Theoretical framework (Theorem 1, Theorem 2) and core mechanism that enriched models with complementary feedback outperform simple deferral when feedback contains additional signal
- **Medium confidence**: Practical implementation details, particularly the sequential vs. joint training dynamics and their sensitivity to defer cost δ
- **Low confidence**: Generality of results to other data distributions or feedback types not explored in the paper (e.g., unstructured annotations, different cost structures)

## Next Checks

1. **Ablation study on feedback richness:** Systematically compare LtA performance with LtD-Feedback (labels only) vs. Unc-Feedback (probabilities) vs. custom feedback types on your domain to quantify the complementarity gain
2. **Calibration robustness analysis:** Test LtA performance with varying calibration set sizes and distributions to assess sensitivity to the threshold estimation procedure τ_β
3. **Cost structure sensitivity:** Evaluate LtA under different budget constraints (varying β) and defer costs (varying δ) to identify regimes where the joint training approach significantly outperforms sequential training