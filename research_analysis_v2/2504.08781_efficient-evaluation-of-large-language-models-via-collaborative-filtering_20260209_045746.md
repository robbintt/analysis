---
ver: rpa2
title: Efficient Evaluation of Large Language Models via Collaborative Filtering
arxiv_id: '2504.08781'
source_url: https://arxiv.org/abs/2504.08781
tags:
- performance
- llms
- instances
- evaluation
- efficient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of efficiently evaluating large
  language models (LLMs) on benchmarks, which is costly due to the large number of
  test instances and slow inference speed. The authors propose a two-stage method
  inspired by collaborative filtering from recommendation systems.
---

# Efficient Evaluation of Large Language Models via Collaborative Filtering

## Quick Facts
- **arXiv ID:** 2504.08781
- **Source URL:** https://arxiv.org/abs/2504.08781
- **Reference count:** 40
- **Primary result:** Achieves lower MAE in performance and ranking predictions than baselines like random sampling, clustering, Sort&Search, and TinyBenchmark while reducing inference overhead.

## Executive Summary
This paper addresses the challenge of efficiently evaluating large language models (LLMs) on benchmarks, which is costly due to the large number of test instances and slow inference speed. The authors propose a two-stage method inspired by collaborative filtering from recommendation systems. In the first stage, they select important test instances by treating instance selection as a product recommendation problem, choosing instances that can easily distinguish model performance. In the second stage, they predict the target LLM's performance on unselected instances by treating it as a rating prediction problem in recommendation systems, using historical evaluation results and optimal transport to synthesize additional information. Experiments on multiple LLMs and datasets demonstrate that their method can accurately estimate the target model's performance while significantly reducing inference overhead.

## Method Summary
The proposed method consists of two stages: (1) Instance Selection and (2) Performance Prediction. In Stage 1, the method calculates importance scores for each test instance based on variance of LLM performance across historical evaluations. It starts with a probe set, runs the target LLM on this subset, identifies similar historical LLMs, and iteratively re-scores and selects instances until the desired subset size is reached. In Stage 2, the method uses optimal transport to synthesize data from similar tasks and applies a hybrid collaborative filtering approach (combining item-based and user-based heuristics) to predict performance on unselected instances. The final performance estimate combines predictions from both stages.

## Key Results
- Achieves lower mean absolute error (MAE) in performance predictions compared to baselines like random sampling, clustering, Sort&Search, and TinyBenchmark
- Maintains reasonable computational efficiency while significantly reducing inference overhead
- Demonstrates robust performance across multiple LLMs and datasets with sampling ratios ranging from 0.1 to 0.5

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Selecting test instances based on performance variance across a historical model set reduces redundancy better than semantic clustering.
- **Mechanism:** The system defines an "importance score" (variance) for each instance, prioritizing instances where historical models diverge (e.g., 50% success rate) to maximize information gain regarding relative model capability.
- **Core assumption:** Historical model behavior on specific instances is predictive of the target model's behavior, and semantic similarity does not correlate with evaluation difficulty.
- **Evidence anchors:** [abstract] "In the first stage, we treat instance selection as recommending products... to choose instances that can easily distinguish model performance." [section 4.1.1] Equation 7 defines the importance score based on variance; Section 3.3 Table 1 shows historical results outperforming semantic embeddings.
- **Break condition:** If the target model possesses a capability entirely unseen in the historical set, the variance scores of historical instances may fail to identify relevant tests, leading to uninformative sampling.

### Mechanism 2
- **Claim:** Optimal Transport (OT) can synthesize missing evaluation data by mapping distributions from similar tasks, mitigating the "cold start" problem of sparse sampling.
- **Mechanism:** When the target model is evaluated on only a small subset, the method identifies "similar tasks" based on historical performance vectors and uses OT to transport the evaluation distribution from these similar tasks to the target task, creating synthetic evaluation results.
- **Core assumption:** Tasks with similar historical performance profiles share underlying evaluation distributions that can be mathematically aligned via cost matrices.
- **Evidence anchors:** [abstract] "...using historical evaluation results and optimal transport to synthesize additional information." [section 4.2.2] "We leverage the performance of sampled instances in similar tasks combined with Optimal Transport (OT) to derive meta-information."
- **Break condition:** If task correlations are low (i.e., the benchmark tasks are highly dissimilar), the transported distribution will be noisy, degrading prediction accuracy.

### Mechanism 3
- **Claim:** A hybrid Collaborative Filtering approach (combining item-based and user-based heuristics) predicts performance on unselected instances more robustly than simple averaging.
- **Mechanism:** For an unselected instance, if it is similar to selected instances (high cosine similarity), the system uses the target model's known performance on those neighbors (item-based). If it is dissimilar, it defaults to the performance of the target model's peer group (user-based, using similar LLMs).
- **Core assumption:** The target model's behavior on "hard" or "dissimilar" instances is better approximated by its peer models than by interpolating its own performance on "easy" instances.
- **Evidence anchors:** [abstract] "...predict the target LLM's behavior on unselected instances." [section 4.2.2] Describes splitting instances based on adaptive thresholds to apply either the average of similar instances or the average of similar LLMs.
- **Break condition:** If the target model is an outlier (behaviorally distinct from all historical models) and the test instances are unique, the "user-based" fallback will fail, and the "item-based" path will lack neighbors.

## Foundational Learning

- **Concept:** **Collaborative Filtering (User vs. Item-based)**
  - **Why needed here:** This is the structural metaphor for the entire system (LLMs=Users, Instances=Items). Understanding the difference between finding similar *users* (models with similar strengths) vs. similar *items* (questions of similar difficulty) is required to follow the two-stage selection and prediction logic.
  - **Quick check question:** Can you explain why the paper uses "user-based" logic to find similar models for selection, but might use "item-based" logic to predict a score on a specific test question?

- **Concept:** **Optimal Transport (OT)**
  - **Why needed here:** Used in the second stage to generate synthetic data. You need to understand OT not just as "moving data," but as aligning probability distributions (performance profiles) across different tasks to fill in sparse matrices.
  - **Quick check question:** How does using OT to map distributions from a "similar task" help when we have almost no data (0.1 ratio) for the target task?

- **Concept:** **Cold Start Problem**
  - **Why needed here:** The paper explicitly frames the low-sample evaluation problem as a cold-start scenario in recommendation systems.
  - **Quick check question:** In this context, does "cold start" refer to a new LLM entering the zoo, or a new test instance being added to the benchmark?

## Architecture Onboarding

- **Component map:** Historical Matrix (D) -> Stage 1 (Selector) -> Target Inference -> Stage 2 (Predictor) -> Final Score
- **Critical path:** The iterative loop in Stage 1 (Section 4.1.2). If the initial "Probe Set" fails to find the correct cluster of "similar LLMs," the personalized instance selection will default to generic importance scores, reducing the efficiency of the sample.
- **Design tradeoffs:** The paper explicitly rejects Semantic Embeddings (e.g., BERT-Score) for Historical Performance Vectors. The tradeoff is dependency on historical data: you cannot evaluate a model on a brand-new benchmark without prior model baselines, but you gain significant accuracy over semantic clustering (MAE drops from 0.210 to 0.035 in toy tests).
- **Failure signatures:**
  - Homogenous Model Zoo: If all historical models are identical, importance scores (variance) approach zero, breaking the instance selector.
  - Massive Distribution Shift: If a target model is strictly better/worse than all historical models on all tasks, the "similar model" lookup returns poor proxies.
- **First 3 experiments:**
  1. **Sanity Check (Toy Experiment):** Replicate Section 3.3. Cluster instances using Sentence-BERT vs. Historical Results. Confirm that Historical Results yield lower MAE.
  2. **Ablation on Optimal Transport:** Disable the OT synthesis module (as per Appendix D.1). Compare MAE at 0.1 sampling ratio to measure the contribution of "cross-task" information.
  3. **Iterative Selection Validation:** Run Stage 1 using only the initial "Probe Set" vs. the full iterative re-selection. Measure the difference in "personalization" (how well the selected set fits the target model's specific failures).

## Open Questions the Paper Calls Out
None

## Limitations
- **Hyperparameter sensitivity:** Exact values for |S|, α, iteration count, and similarity thresholds τ₀, τ₁, τ₂ are not specified in the main text, potentially affecting performance.
- **Distribution shift vulnerability:** The method may fail when the target model possesses entirely new capabilities not present in the historical set.
- **Computational bottleneck:** Similarity search for finding similar LLMs and tasks at scale is not explicitly addressed and could become problematic for very large model zoos.

## Confidence

- **High Confidence:** The core mechanism of using variance-based importance scores for instance selection over semantic embeddings is well-supported by the toy experiment results (MAE drops from 0.210 to 0.035).
- **Medium Confidence:** The hybrid CF approach for prediction (combining item-based and user-based heuristics) is plausible and supported by general CF literature, but the specific adaptive thresholding mechanism lacks direct validation in the corpus.
- **Low Confidence:** The claim that Optimal Transport can effectively synthesize missing evaluation data by mapping distributions from similar tasks is weakly supported by the corpus. Standard CF literature does not typically employ OT for this specific "meta-information" synthesis in evaluation.

## Next Checks

1. **Toy Experiment Replication:** Replicate Section 3.3 to confirm that historical results yield lower MAE than semantic embeddings (e.g., Sentence-BERT) for instance clustering. This validates the foundational choice of performance variance over semantic similarity.

2. **OT Ablation Study:** Disable the OT synthesis module (as per Appendix D.1) and compare MAE at low sampling ratios (e.g., 0.1) to measure the contribution of "cross-task" information. This isolates the impact of OT on prediction accuracy.

3. **Iterative Selection Impact:** Run Stage 1 using only the initial "Probe Set" vs. the full iterative re-selection. Measure the difference in "personalization" (how well the selected set fits the target model's specific failures). This validates the value of the iterative re-scoring loop.