---
ver: rpa2
title: 'Divide (Text) and Conquer (Sentiment): Improved Sentiment Classification by
  Constituent Conflict Resolution'
arxiv_id: '2505.06320'
source_url: https://arxiv.org/abs/2505.06320
tags:
- sentiment
- polarity
- passage
- roberta
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of degraded sentiment classification
  performance on longer passages containing multiple conflicting tones. The authors
  propose a divide-and-conquer approach that isolates conflicting sentiments by generating
  subpredictions at the constituent level (sentences or aspects) and then aggregating
  them using a Multi-Layer Perceptron (MLP).
---

# Divide (Text) and Conquer (Sentiment): Improved Sentiment Classification by Constituent Conflict Resolution

## Quick Facts
- arXiv ID: 2505.06320
- Source URL: https://arxiv.org/abs/2505.06320
- Reference count: 8
- The paper proposes a divide-and-conquer approach that significantly improves ternary sentiment classification accuracy on longer passages by isolating conflicting sentiments at constituent level and aggregating them using an MLP.

## Executive Summary
This paper addresses the problem of degraded sentiment classification performance on longer passages containing multiple conflicting tones. The authors propose a divide-and-conquer approach that isolates conflicting sentiments by generating subpredictions at the constituent level (sentences or aspects) and then aggregating them using a Multi-Layer Perceptron (MLP). The MLP approach significantly outperforms baseline models across various datasets including Amazon, Twitter, and SST, with improvements of up to 20 percentage points for models fine-tuned on different distributions. The method is particularly effective for ternary sentiment classification and delivers substantial accuracy gains at approximately 1/100 the cost of standard fine-tuning.

## Method Summary
The approach involves three main steps: (1) Constituent extraction using either sentence splitting (PySBD) or aspect detection (SetFitABSA), (2) Generating subpredictions using a ternary sentiment classifier (RoBERTa or Polarity model) for each constituent, and (3) Aggregating subpredictions using an MLP that takes 19 statistical features (mean, min, max, std, range, count of highest scores per class, plus constituent count) as input. The MLP learns to effectively combine conflicting sentiment signals that would otherwise be averaged away in traditional approaches. Training the MLP aggregator takes minutes on a T4 GPU and costs approximately 1/100 of full model fine-tuning.

## Key Results
- MLP aggregation achieved up to 20 percentage points improvement over baseline models on out-of-distribution data
- Significant accuracy gains across Amazon, Twitter, and SST datasets for ternary sentiment classification
- MLP showed minimal improvement (5pp) on Twitter when base model was already well-calibrated, but substantial gains (20pp+) when base model was out-of-distribution
- The method costs approximately 1/100 of what fine-tuning the baseline model would require

## Why This Works (Mechanism)

### Mechanism 1: Constituent-Level Sentiment Isolation
Decomposing passages into constituents (sentences or aspects) reduces noise from competing sentiment signals before aggregation. A passage containing "The staff were great, only negative was the noise" produces opposing subpredictions that are easier to reconcile when isolated than when processed as a single vector. The base classifier generates N × 3 probability vectors (N = number of constituents, 3 = sentiment classes), which preserves granular signal that would otherwise be averaged away.

### Mechanism 2: Statistical Feature Compression for Aggregation
Computing summary statistics (mean, min, max, std, range, count) over constituent predictions creates a fixed-dimensional representation that a shallow MLP can learn to aggregate more effectively than simple averaging. Rather than learning from raw N × 3 matrices (variable size), the 19-feature vector provides the MLP with distributional information about sentiment uncertainty and conflict. The MLP learns weighting schemes that simple averaging cannot express—e.g., treating outliers differently than central tendency.

### Mechanism 3: Domain Adaptation via Shallow Meta-Learning
Training an MLP aggregator on top of frozen subpredictions functions as efficient domain adaptation when the base model encounters out-of-distribution data. The Polarity model (fine-tuned on restaurant reviews) showed a 20+ percentage point improvement on Twitter data when paired with the MLP aggregator—suggesting the MLP learned to recalibrate systematic biases in the base model's predictions for the new domain.

## Foundational Learning

- **Ternary Sentiment Classification with Neutral Class**: Why needed: Binary classification is nearly solved, but the neutral class introduces ambiguity that compounds in longer passages. Understanding how neutral predictions behave in aggregation is critical. Quick check: Given subpredictions [0.1, 0.85, 0.05] and [0.3, 0.4, 0.3] (neg, neu, pos), would simple averaging produce the correct final label? What information is lost?

- **Aspect-Based Sentiment Analysis (ABSA)**: Why needed: One of the two constituent extraction methods relies on ABSA to identify sentiment-bearing aspects rather than syntactic boundaries. ABSA performance directly constrains the quality of subpredictions. Quick check: If ABSA detects "Bluetooth" as an aspect but assigns it neutral sentiment when the text clearly expresses frustration, what happens to the downstream aggregation?

- **Feature Engineering for Aggregation Functions**: Why needed: The MLP's input features are hand-crafted statistics, not learned representations. Understanding what statistics capture about distributions determines whether you can debug or extend the approach. Quick check: Why include "the number of instances when the given class had the highest score" as a feature? What aggregation pattern would this help the MLP learn that mean alone wouldn't capture?

## Architecture Onboarding

Input Passage
  ├──→ [Sentence Splitter: PySBD] ──→ N sentences
  │         │
  │         └──→ [Base Classifier: RoBERTa/Polarity] ──→ N × 3 probs
  │
  └──→ [Aspect Detector: SetFitABSA] ──→ M aspects
            │
            └──→ [Per-Aspect Classifier] ──→ M × 3 probs

Either N × 3 or M × 3 matrix
  │
  └──→ [Feature Extractor] ──→ 19-dim vector (stats + count)
            │
            └──→ [MLP: 128 hidden, 1 layer] ──→ Final 3-class prediction

Critical path: The base classifier quality is the bottleneck. If subpredictions are noisy (as with ClauCy clause splitting), no aggregation strategy recovers signal. Test base classifier accuracy on constituents before building aggregation layer.

Design tradeoffs:
- **Sentences vs. Aspects**: Sentence splitting is faster and deterministic; ABSA captures semantic units but adds model dependency and potential error propagation
- **Average vs. MLP**: Averaging requires no training data; MLP requires train/val splits but learns domain-specific calibration
- **Feature complexity**: 19 features worked; authors did not ablate to find minimal sufficient set

Failure signatures:
- Very short passages: SST has minimal splittable content; aggregation provides little benefit
- Poor constituent quality: ClauCy produced fragments like "I said" with no sentiment signal
- Domain-matched base models: RoBERTa (social media fine-tuned) gained only 5pp on Twitter vs. 20pp for Polarity (restaurant fine-tuned)
- Imbalanced constituent counts: A passage with 1 positive and 8 neutral sentences may be misclassified if aggregation overweights neutrality

First 3 experiments:
1. **Validate base classifier on constituents**: Sample 50 passages, manually verify that sentence-level predictions align with ground truth sentiment for each sentence
2. **Ablate aggregation strategies on held-out set**: Compare baseline whole-passage prediction, simple averaging, AWON, and MLP on the same test split
3. **Test distribution shift sensitivity**: Train MLP on one dataset (e.g., Amazon), evaluate on another (e.g., Twitter) to determine generalizability

## Open Questions the Paper Calls Out
None

## Limitations
- The paper does not investigate the exact cause of performance degradation on longer passages, leaving open whether conflicting sentiments are actually the primary factor
- The MLP aggregation mechanism may be learning domain-specific calibration rather than generalizable aggregation logic
- The approach shows minimal gains on SST where passages contain few splittable constituents, suggesting limited applicability to certain text types

## Confidence
- **High confidence**: The empirical observation that constituent-level subpredictions improve classification when aggregated, and the computational efficiency claim
- **Medium confidence**: The mechanism that conflicting sentiments are the primary cause of degradation, and that the MLP learns domain-agnostic aggregation
- **Low confidence**: The generalizability of the 19-feature engineering approach across different text domains and constituent extraction methods

## Next Checks
1. **Base classifier validation**: Manually verify sentence-level predictions align with ground truth sentiment for sampled passages to confirm the bottleneck is at aggregation, not subprediction quality
2. **Ablation study**: Systematically remove individual features from the 19-dimensional vector to identify which statistics contribute most to MLP performance
3. **Cross-dataset transfer**: Train the MLP aggregator on one dataset (e.g., Amazon) and evaluate on another (e.g., Twitter) to determine whether it learns generalizable aggregation logic or dataset-specific calibration