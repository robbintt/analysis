---
ver: rpa2
title: 'TULIP: Adapting Open-Source Large Language Models for Underrepresented Languages
  and Specialized Financial Tasks'
arxiv_id: '2508.16243'
source_url: https://arxiv.org/abs/2508.16243
tags:
- financial
- language
- data
- turkish
- finance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses the challenge of adapting large language models
  for specialized financial tasks in underrepresented languages, specifically Turkish.
  The core method involves a five-stage pipeline: data collection from diverse financial
  sources, continual pre-training to establish domain knowledge, synthetic data generation
  for supervised fine-tuning, and evaluation on task-specific benchmarks.'
---

# TULIP: Adapting Open-Source Large Language Models for Underrepresented Languages and Specialized Financial Tasks

## Quick Facts
- arXiv ID: 2508.16243
- Source URL: https://arxiv.org/abs/2508.16243
- Authors: İrem Demirtaş; Burak Payzun; Seçil Arslan
- Reference count: 3
- One-line primary result: TULIP models achieve up to 0.668 accuracy on academic finance exams and 0.918 on Turkish Trade Registry Gazette QA tasks

## Executive Summary
This work addresses the challenge of adapting large language models for specialized financial tasks in underrepresented languages, specifically Turkish. The core method involves a five-stage pipeline: data collection from diverse financial sources, continual pre-training to establish domain knowledge, synthetic data generation for supervised fine-tuning, and evaluation on task-specific benchmarks. The results demonstrate significant performance improvements, with the adapted models achieving accuracy scores of up to 0.668 on academic finance exams and 0.918 on Turkish Trade Registry Gazette question answering tasks, surpassing base models and highlighting the effectiveness of domain-specific adaptation for underrepresented languages in finance.

## Method Summary
The methodology employs a five-stage development pipeline involving data collection, continual pre-training (CPT), benchmark design, synthetic data generation, and supervised fine-tuning (SFT). The process begins with collecting approximately 2.19B tokens across six categories including academic sources, financial institutions, textbooks, market data, legislation, and reports. CPT uses QLoRA with rank=64 and alpha=128 on 2.19B tokens to establish domain knowledge. Synthetic SFT data is generated using GPT-4o with structured outputs across 11 task types, producing 23K instruction pairs. Quality filtering removes invalid samples before SFT training with lr=2e-6. The approach is evaluated on FINTR-EXAMS (academic finance exam questions) and Turkish Trade Registry Gazette QA tasks.

## Key Results
- TULIP models achieve 0.668 accuracy on FINTR-EXAMS academic finance exam benchmark
- Turkish Trade Registry Gazette QA task performance reaches 0.918 accuracy
- CPT-only models marginally outperform CPT+SFT variants (0.668 vs 0.659) on general benchmarks
- Native Turkish adaptation outperforms translation pipelines for domain-specific tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continual pre-training on domain-specific corpora establishes foundational knowledge that instruction-tuning alone cannot achieve.
- Mechanism: CPT on 2.19B tokens (academic 1.1B, financial institutions 150M, textbooks 200M, market data 350M, legislation 50M, reports 340M) injects domain vocabulary and conceptual patterns before task-specific training. The paper notes CPT models marginally outperformed CPT+SFT variants on benchmarks, suggesting CPT provides broader generalization while SFT may introduce overly specific formatting constraints.
- Core assumption: The base model already possesses sufficient Turkish language proficiency to build upon; starting from a model lacking fundamental Turkish knowledge would significantly expand scope.
- Evidence anchors:
  - [abstract] "The five-stage development pipeline involves data collection, continual pre-training (CPT), benchmark design, synthetic data generation and supervised fine-tuning (SFT)."
  - [section 3.2] "A total of approximately 2.19B tokens were processed during continual pre-training, distributed across various categories"
  - [corpus] Sparse Subnetwork Enhancement paper confirms targeted fine-tuning can enhance underrepresented language capabilities while preserving general performance.

### Mechanism 2
- Claim: Synthetic data generation from domain-specific raw text enables supervised fine-tuning when native instruction datasets are unavailable for underrepresented language-domain pairs.
- Mechanism: Raw text from four sources (Academic, Central Bank, News, Trade Registry Gazette) is chunked and fed to GPT-4o with structured output. For each task type (11 types including QA, summarization, NER), seed prompts incorporate reference text and request task completion. Quality checks filter samples by length, empty responses, and format compliance.
- Core assumption: GPT-4o can accurately generate Turkish financial instruction-answer pairs without introducing systematic errors or mistranslations that plagued automatic translation approaches.
- Evidence anchors:
  - [abstract] "synthetic data generation for supervised fine-tuning"
  - [section 3.1] "Using GPT-4o's structured output capability, the rephrased prompt and the produced answer were collected. Quality checks were implemented both for the rephrased prompt and the answer"
  - [corpus] MetaSynth paper notes synthetic data diversity limitations but doesn't directly validate this specific approach.

### Mechanism 3
- Claim: Specialized financial terminology acquisition in underrepresented languages requires explicit domain exposure due to loanword repurposing and morphological complexity.
- Mechanism: Turkish financial terms like "memzuç" (Ottoman-derived, meaning consolidated credit report) have specialized meanings detached from everyday usage or original etymology. CPT exposes models to these terms in context, enabling accurate interpretation where untuned models fail to recognize domain-specific usage.
- Core assumption: The frequency and contextual diversity of terminology in CPT corpus is sufficient for models to learn specialized meanings.
- Evidence anchors:
  - [section 1] "Morphological qualities like agglutination bring different mechanics... This is especially evident in smaller models."
  - [section 4.4] "Our observations indicate that general-purpose language models, not specifically fine-tuned for the financial domain, often fail to accurately comprehend the specialized meanings of these terms."
  - [corpus] TigerCoder paper confirms underrepresented languages face data scarcity for specialized tasks, but doesn't address terminology mechanisms directly.

## Foundational Learning

- Concept: **QLoRA (Quantized Low-Rank Adaptation)**
  - Why needed here: The paper uses QLoRA for memory-efficient fine-tuning of 7-8B parameter models on limited GPU resources (2x RTX A6000). Understanding rank, alpha, and quantization settings is essential for reproducing results.
  - Quick check question: Can you explain why applying LoRA to "all linear layers, as well as the head and embedding layers" differs from standard LoRA configurations that typically target attention layers only?

- Concept: **Continual Pre-training vs. Supervised Fine-Tuning Trade-offs**
  - Why needed here: The paper shows CPT-only models sometimes outperform CPT+SFT on general benchmarks (0.668 vs 0.659 for Qwen), while SFT improves task-specific formats. Understanding when to apply each is critical.
  - Quick check question: What factors would determine whether CPT alone is sufficient versus when CPT+SFT is necessary for your target application?

- Concept: **Synthetic Data Quality Assurance Pipelines**
  - Why needed here: With 23K synthetically generated instruction pairs, quality filtering (length checks, format validation, empty response detection) determines whether SFT helps or harms performance.
  - Quick check question: What quality checks would you implement beyond those mentioned (length, format, empty responses) to catch subtle synthetic data errors?

## Architecture Onboarding

- Component map: Base Model (Llama 3.1 8B / Qwen 2.5 7B) -> Data Collection (6 categories) -> Continual Pre-training (QLoRA: rank=64, alpha=128, lr=2e-5, 2.19B tokens) -> Synthetic SFT Data Generation (GPT-4o, 23K pairs, 11 task types) -> Supervised Fine-Tuning (lr=2e-6, quality-filtered pairs) -> Evaluation (FINTR-EXAMS benchmark, Trade Registry Gazette QA)

- Critical path: Data collection quality -> CPT coverage of domain terminology -> Synthetic data quality -> SFT format alignment. The paper notes CPT is "crucial" for "unlocking deeper domain adaptation and new capabilities often unachievable by only fine-tuning instruct models."

- Design tradeoffs:
  - **CPT-only vs CPT+SFT**: CPT provides better generalization (0.668 vs 0.659 on exams), SFT improves task-specific formatting but may cause "minor catastrophic forgetting"
  - **Translation vs native adaptation**: Direct Turkish adaptation outperforms translation pipelines; self-translation by local models performs worse than GPT-4o translation but is required for data confidentiality
  - **Base model selection**: Models with existing Turkish proficiency (Llama 3.1, Qwen 2.5) work better than those without (Falcon, Mistral showed "unsatisfactory" improvements)

- Failure signatures:
  - Language switching mid-response (Qwen2.5 failures show English/Chinese switching; resolved after additional training)
  - Partial answers that invalidate evaluation (TULIP-Llama3.1-IT provides explanations alongside expected answers, disqualifying responses)
  - Domain terminology misinterpretation (untuned models define "memzuç" as "not common in finance" rather than consolidated credit report)

- First 3 experiments:
  1. **Validate base model Turkish proficiency**: Run 5-shot evaluation on base Llama 3.1 and Qwen 2.5 using FINTR-EXAMS before any training. If scores are below 0.50, consider alternative base models.
  2. **CPT ablation with token budget scaling**: Train with 500M, 1B, and 2B tokens to identify minimum viable CPT budget for your target domain. Measure performance delta on terminology-heavy questions.
  3. **Synthetic data quality audit**: Before full SFT, manually review 100 randomly sampled synthetic pairs for factual accuracy, format compliance, and Turkish fluency. If error rate exceeds 10%, refine prompt templates or quality filters.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between continual pre-training (CPT) and supervised fine-tuning (SFT) for domain adaptation, given that CPT-only models marginally outperformed CPT+SFT models on the FINTR-EXAMS benchmark?
- Basis in paper: [explicit] The authors state: "In our experiments, the CPT models marginally outperformed the CPT+SFT variants on the benchmark" and suggest this may be due to "minor catastrophic forgetting or a distributional shift away from the precise competencies evaluated by the benchmark."
- Why unresolved: The paper does not conduct ablation studies on SFT data size, hyperparameters, or training sequencing to identify the cause of degraded performance.
- What evidence would resolve it: Ablation experiments varying SFT data quantity, learning rates, and intermediate evaluation checkpoints during multi-stage training.

### Open Question 2
- Question: Would the five-stage pipeline generalize effectively to other underrepresented languages with different linguistic characteristics (e.g., Arabic, Hindi), or is Turkish's specific mix of agglutination and loanword sources uniquely suited to this approach?
- Basis in paper: [inferred] The paper details Turkish-specific challenges including agglutination, diverse loanwords from Arabic/Persian/French, and domain-specific terminology, but tests only Turkish.
- Why unresolved: No experiments were conducted on other low-resource languages to validate pipeline generalizability.
- What evidence would resolve it: Applying the methodology to 2-3 other underrepresented languages with comparable domain-specific benchmarks and reporting relative performance improvements.

### Open Question 3
- Question: What specific quantitative or numerical reasoning capabilities must be added to improve performance in the Accounting and Financial Reporting (AFP) domain, which showed the lowest scores across all models?
- Basis in paper: [explicit] The authors note: "Domains like Accounting and Financial Reporting (AFP), which involve more numerical and technical questions, generally yield lower scores" because "these questions require not only domain expertise but also a strong grasp of quantitative analysis."
- Why unresolved: The paper identifies the gap but does not propose or test interventions for numerical reasoning.
- What evidence would resolve it: Targeted SFT with chain-of-thought numerical reasoning examples, followed by AFP-specific benchmark evaluation.

## Limitations
- The synthetic data generation pipeline relies entirely on GPT-4o quality, creating a single point of failure where domain-specific errors get amplified
- The CPT corpus composition lacks detailed breakdown of term frequencies and contextual diversity, making it difficult to assess whether specialized terminology exposure is sufficient
- No experiments were conducted on other low-resource languages to validate pipeline generalizability beyond Turkish

## Confidence
**High Confidence**: The five-stage pipeline methodology (data collection → CPT → synthetic SFT generation → evaluation) is well-documented and produces measurable performance improvements over base models. The empirical results showing accuracy gains (0.668 vs lower scores) and the documented trade-offs between CPT-only and CPT+SFT approaches are reproducible given access to the datasets.

**Medium Confidence**: The claim that CPT provides broader generalization while SFT introduces formatting constraints is supported by benchmark comparisons, but the sample size (limited to Llama 3.1 and Qwen 2.5) prevents generalization to other model families. The language switching failures observed in Qwen2.5 and their resolution through additional training suggest model-specific rather than universal patterns.

**Low Confidence**: The assertion that morphological complexity (agglutination) uniquely challenges Turkish financial NLP requires broader validation across other morphologically rich languages. The paper's observations about specialized terminology learning are plausible but lack systematic ablation studies showing which CPT categories contribute most to domain knowledge acquisition.

## Next Checks
1. **Base Model Proficiency Validation**: Before any training, evaluate Llama 3.1 8B and Qwen 2.5 7B on FINTR-EXAMS using 5-shot prompting. If baseline accuracy falls below 0.50, the foundational Turkish proficiency assumption is violated, requiring alternative base models or extended pre-training.

2. **Synthetic Data Quality Audit**: Manually review 100 randomly sampled synthetic instruction pairs for factual accuracy, Turkish fluency, and format compliance. Calculate error rates and trace failures back to GPT-4o prompt templates. If error rate exceeds 10%, refine quality filters or redesign synthetic generation prompts.

3. **CPT Token Budget Ablation**: Train with 500M, 1B, and 2B token budgets while measuring performance on terminology-heavy questions (e.g., "memzuç" interpretation). Identify the minimum viable CPT budget where performance plateaus, then compare CPT-only vs CPT+SFT trade-offs at each budget level.