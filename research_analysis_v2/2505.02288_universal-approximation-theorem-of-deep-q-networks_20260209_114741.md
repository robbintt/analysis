---
ver: rpa2
title: Universal Approximation Theorem of Deep Q-Networks
arxiv_id: '2505.02288'
source_url: https://arxiv.org/abs/2505.02288
tags:
- approximation
- function
- continuous
- deep
- optimal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes a theoretical foundation for Deep Q-Networks
  (DQNs) in continuous-time settings using stochastic control and FBSDEs. It proves
  that DQNs with residual blocks can approximate the optimal Q-function with arbitrary
  accuracy on compact sets with high probability, leveraging universal approximation
  theorems and large deviation bounds.
---

# Universal Approximation Theorem of Deep Q-Networks

## Quick Facts
- **arXiv ID:** 2505.02288
- **Source URL:** https://arxiv.org/abs/2505.02288
- **Reference count:** 40
- **Primary result:** Proves DQNs with residual blocks can approximate optimal Q-functions with arbitrary accuracy on compact sets with high probability in continuous-time settings.

## Executive Summary
This paper establishes a theoretical foundation for Deep Q-Networks (DQNs) in continuous-time settings using stochastic control and FBSDEs. It proves that DQNs with residual blocks can approximate the optimal Q-function with arbitrary accuracy on compact sets with high probability, leveraging universal approximation theorems and large deviation bounds. The paper also analyzes the convergence of a continuous-time Q-learning algorithm to the optimal Q-function under suitable assumptions on ergodicity, learning rates, and network architecture. The results bridge deep reinforcement learning and stochastic control theory, providing insights into DQNs' behavior in continuous-time environments relevant for physical systems and high-frequency data.

## Method Summary
The paper analyzes DQNs for continuous-time control problems using forward-backward stochastic differential equations (FBSDEs). The method combines residual network approximation theory with stochastic control techniques. The DQN architecture uses residual blocks that implement the update rule x_{l+1} = x_l + h_θ(x_l, a)Δt, which mirrors the Euler discretization of an ODE. The training procedure follows continuous-time Q-learning with experience replay, analyzing convergence through stochastic approximation theory and Lyapunov stability analysis.

## Key Results
- DQNs with residual blocks can approximate the optimal Q-function Q* with arbitrary accuracy on compact sets with high probability (1-δ)
- The continuous-time Q-learning algorithm converges almost surely to the optimal Q-function under ergodicity, learning rate, and network architecture assumptions
- Large deviation theory enables probabilistic compactification to handle unbounded state spaces by focusing on high-probability regions

## Why This Works (Mechanism)

### Mechanism 1: Residual Networks as ODE Discretizations
- **Claim:** DQNs with residual blocks can approximate Q* with arbitrary accuracy on compact sets
- **Mechanism:** The residual update rule x_{l+1} = x_l + h_θ(x_l, a)Δt mirrors the Euler scheme for solving ODEs, inheriting universal approximation capabilities of deep residual networks
- **Core assumption:** Q*(t,s,a) is continuous on [0,T] × S × A (Assumption 2.3)
- **Evidence anchors:** Abstract mentions leveraging residual network approximation theorems; Section 2.2 explicitly links update rule to ODE discretization
- **Break condition:** Q* is discontinuous or network depth/width is insufficient

### Mechanism 2: Probabilistic Compactification via Large Deviations
- **Claim:** Approximation error holds with high probability (1-δ) in unbounded spaces
- **Mechanism:** Uses Large Deviation Theory to define bounded region K_R where state-action process resides with probability ≥ 1-δ, restricting approximation guarantee to high-probability compact set
- **Core assumption:** Drift and diffusion coefficients satisfy linear growth conditions (Assumption 2.2)
- **Evidence anchors:** Abstract mentions "high probability" approximation; Section 3.1 constructs K_R based on Lemma 2.10
- **Break condition:** Process is highly unstable or non-ergodic, causing low probability of staying within K_R

### Mechanism 3: Convergence via Stochastic Approximation
- **Claim:** Continuous-time Q-learning converges almost surely to optimal Q*
- **Mechanism:** Analyzes training dynamics as stochastic approximation scheme, mapping discrete updates to ODE solution; convergence requires unique globally asymptotically stable equilibrium
- **Core assumption:** Assumption 3.6 (Negative Correlation Condition) and Assumption 3.5 (Identifiability)
- **Evidence anchors:** Section 3.2 mentions adapting stochastic approximation theory with Bellman operator treatment; Appendix A.7 details ODE method and Lyapunov analysis
- **Break condition:** Spurious equilibria exist or Negative Correlation condition fails

## Foundational Learning

- **Concept: Viscosity Solutions**
  - **Why needed here:** In stochastic control, value functions are often non-differentiable (e.g., kinks at switching points). Classical calculus cannot solve the HJB equation. Viscosity solutions provide a rigorous way to define "solutions" to PDEs that are not smooth.
  - **Quick check question:** Why is "viscosity" necessary for defining the optimal value function V* in this framework? (Hint: It handles non-smoothness).

- **Concept: Martingales and Quadratic Variation**
  - **Why needed here:** The environment noise is modeled by a square-integrable martingale M_t (generalizing Brownian motion). Understanding that ⟨M⟩_t tracks the accumulated variance (quadratic variation) is essential for modeling state evolution ds_t = ... dM_t.
  - **Quick check question:** What does the process C(t) characterize in the term d⟨M⟩_t = C(t)dt?

- **Concept: Bellman Operator Contraction**
  - **Why needed here:** The proof of convergence relies on the Bellman operator T being a contraction mapping. This ensures that repeated application of T inevitably leads to the unique fixed point Q*.
  - **Quick check question:** Which parameter ensures the contraction property of the Bellman operator in infinite horizon settings?

## Architecture Onboarding

- **Component map:** State s_t and Action a_t -> Input -> Linear(64) -> 2 Residual Blocks -> Linear(3) -> Q-value
- **Critical path:** The residual block implementation must strictly follow x + f(x)Δt. Standard feedforward layers without skip connection or Δt scaling break the ODE approximation connection.
- **Design tradeoffs:**
  - Depth (L) vs. Discretization Error: Increasing L improves approximation but requires finer time steps Δt = T/L, potentially increasing training instability
  - Assumption 3.6 Strictness: Convergence proof assumes strong gradient conditions that standard ReLU networks might not naturally satisfy
- **Failure signatures:**
  - Divergence during training: Likely indicates violation of Robbins-Monro conditions or Negative Correlation condition, leading to unstable ODE dynamics
  - Approximation Stagnation: If state-trajectory escapes K_R (high δ), approximation guarantee fails, potentially leading to poor performance on tail states
- **First 3 experiments:**
  1. **Verify Residual Necessity:** Implement both ResNet-DQN and MLP-DQN on 1D control task to empirically validate ODE-style structure approximation benefit
  2. **Large Deviation Check:** Run environment dynamics without learning to histogram state distribution and verify probability of states outside radius R_1 matches theoretical δ bound
  3. **Stability Test:** Systematically vary learning rate α_k and time step Δt to validate Robbins-Monro conditions required for convergence

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What are the explicit approximation rates for DQNs under stronger regularity conditions?
- **Basis in paper:** Conclusion states future direction is "deriving explicit approximation rates under stronger regularity conditions on Q*"
- **Why unresolved:** Theorem 3.1 proves existence based solely on continuity, noting quantitative bounds usually require higher smoothness (e.g., Sobolev spaces)
- **What evidence would resolve it:** Theoretical bounds linking network depth/width and Δt to approximation error ε, assuming higher-order differentiability of Q*

### Open Question 2
- **Question:** Can convergence guarantees be maintained while relaxing identifiability and gradient non-degeneracy assumptions?
- **Basis in paper:** Remark 3.7 states verifying Assumptions 3.5 and 3.6 "remains an open research challenge in general"
- **Why unresolved:** These assumptions are necessary for Lyapunov stability proof but are structurally strong and difficult to verify for general neural networks
- **What evidence would resolve it:** Convergence proofs relying on weaker structural conditions, or empirical studies validating properties for specific architectures

### Open Question 3
- **Question:** What is the sample complexity of learning optimal Q-function in continuous-time setting?
- **Basis in paper:** Conclusion lists "analyzing sample complexity of learning in continuous setting" as future research direction
- **Why unresolved:** Paper establishes asymptotic convergence but doesn't quantify data required to achieve specific accuracy
- **What evidence would resolve it:** Derivation of non-asymptotic bounds on samples needed to approximate Q* within error ε with high probability

### Open Question 4
- **Question:** How does framework extend to continuous-time partially observable systems?
- **Basis in paper:** Conclusion suggests extending framework to "partially observable systems" as future avenue
- **Why unresolved:** Current framework assumes fully observable state s_t for DQN input, which is not true in POMDP settings
- **What evidence would resolve it:** Theoretical extension defining belief state or utilizing memory-based architectures within FBSDE framework

## Limitations
- Strong assumptions required for theoretical guarantees, particularly Assumption 3.6 (Negative Correlation Condition) which may be too restrictive for general architectures
- Probabilistic compactification approach using large deviation theory provides high-probability bounds but doesn't guarantee performance on rare tail events
- Convergence analysis assumes ergodicity of underlying Markov process, which may not hold for highly unstable or poorly designed reward structures

## Confidence
- **High Confidence:** Universal approximation capability of residual networks is well-established in prior literature and connection to ODE discretization is mathematically rigorous
- **Medium Confidence:** Probabilistic compactification approach is theoretically sound but depends on practical applicability of large deviation bounds and linear growth conditions
- **Medium-Low Confidence:** Convergence proof relies on Assumption 3.6 which may be too restrictive for general deep network architectures and hasn't been empirically validated

## Next Checks
1. **Gradient Structure Validation:** Systematically test whether trained DQNs satisfy the "Negative Correlation" condition (Assumption 3.6) by analyzing gradient dynamics during training across multiple random seeds and architectures

2. **Tail Event Performance:** Design experiments specifically targeting rare state-action trajectories outside theoretical compact set K_R to empirically measure approximation quality degradation in these regions

3. **Assumption Relaxation Study:** Conduct ablation studies removing or relaxing Assumption 3.6 to identify which components are truly necessary for convergence versus being overly conservative theoretical artifacts