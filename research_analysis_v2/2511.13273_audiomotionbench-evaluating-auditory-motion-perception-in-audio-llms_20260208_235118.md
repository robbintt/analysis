---
ver: rpa2
title: 'AudioMotionBench: Evaluating Auditory Motion Perception in Audio LLMs'
arxiv_id: '2511.13273'
source_url: https://arxiv.org/abs/2511.13273
tags:
- audio
- motion
- spatial
- auditory
- perception
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AudioMotionBench, a benchmark designed to
  evaluate whether large audio-language models (LALMs) can perceive the motion of
  sound sources from binaural audio. The authors systematically uncover a motion perception
  deficit in current LALMs, showing that models struggle to infer the direction and
  trajectory of moving sound sources.
---

# AudioMotionBench: Evaluating Auditory Motion Perception in Audio LLMs

## Quick Facts
- arXiv ID: 2511.13273
- Source URL: https://arxiv.org/abs/2511.13273
- Authors: Zhe Sun; Yujun Cai; Jiayu Yao; Yiwei Wang
- Reference count: 4
- Primary result: Current LALMs perform at chance level (~25% MCQ, ~50% T/F) on binaural motion perception tasks

## Executive Summary
This paper introduces AudioMotionBench, a benchmark designed to evaluate whether large audio-language models (LALMs) can perceive the motion of sound sources from binaural audio. The authors systematically uncover a motion perception deficit in current LALMs, showing that models struggle to infer the direction and trajectory of moving sound sources. Their controlled experiments reveal that model accuracy remains below 50% on average, even under clean audio conditions. This performance gap persists across different noise levels and task variants, highlighting a fundamental limitation in auditory spatial reasoning. AudioMotionBench thus provides a diagnostic tool and foundational insights for developing spatially aware LALMs.

## Method Summary
The benchmark synthesizes 224 binaural audio clips representing 56 directed trajectories (N, S, E, W, NE, NW, SE, SW) × 4 noise levels (clean, 35dB, 25dB, 25dB, 15dB SNR) in a 6m × 6m space with listener at center (3.0, 3.0). Clips are paired with MCQ (4 options) and T/F questions, yielding 224 MCQ + 448 T/F samples across three task variants (SCT, SDT, TAT). Analytical binaural rendering uses ITD (propagation delay), ILD (inverse-square attenuation), Doppler shift, and direction-dependent filtering. Models tested include Qwen2-Audio-7B, Voxtral-Mini-3B-2507, Aero-1-Audio, and Kimi-Audio-7B via zero-shot evaluation with temperature T=0.

## Key Results
- LALMs achieve only ~25% accuracy on MCQ and ~50% on T/F tasks, essentially at chance levels
- Performance remains flat across SNR levels (clean to 15dB), suggesting models don't use spatial cues
- Models show no sensitivity to binaural input—mono-summed versions yield identical performance
- Conservative bias observed with high TNR but low TPR in T/F verification

## Why This Works (Mechanism)

### Mechanism 1: Binaural Cue Extraction via Analytical Rendering
- Claim: Spatial motion perception requires models to extract ITD, ILD, and Doppler shifts from binaural waveforms
- Mechanism: The benchmark synthesizes dual-channel audio where each frame encodes instantaneous source-to-ear distances, propagation delays, and frequency shifts
- Core assumption: Models that encode binaural cues will generalize to varied SNR and motion speeds
- Evidence anchors: [abstract] "infer the direction and trajectory of moving sound sources from binaural audio"; [Section 3.2] detailed ITD/ILD/Doppler calculations

### Mechanism 2: Trajectory Consistency via Temporal Integration
- Claim: Accurate motion perception requires models to maintain coherent spatial representations across time
- Mechanism: Each audio clip encodes a continuous trajectory (e.g., NW→SW); models must attend to temporal evolution of binaural cues
- Core assumption: Transformers can implicitly learn trajectory continuity if spatial cues are preserved
- Evidence anchors: [Section 4.2] "accuracies remain around 25% for MCQ and 50% for T/F—almost exactly at chance levels"

### Mechanism 3: Noise-Robust Spatial Inference
- Claim: True spatial perception should degrade gracefully as SNR drops
- Mechanism: The benchmark tests four SNR levels; robust spatial encoders would show accuracy loss as noise masks binaural differences
- Core assumption: Flat performance across SNR levels indicates models are not using spatial cues
- Evidence anchors: [Section 4.2] "none of them exhibited noticeable performance changes as the SNR decreased"

## Foundational Learning

- Concept: **Interaural Time Difference (ITD) and Interaural Level Difference (ILD)**
  - Why needed here: These are primary acoustic cues humans use to localize sound
  - Quick check question: Given a sound source at 45° azimuth, which ear receives the signal earlier and louder?

- Concept: **Binaural vs. Monaural Audio Representation**
  - Why needed here: The paper attributes model failure to training on single-channel data
  - Quick check question: If a model is trained only on mono audio, what spatial information is fundamentally unavailable at inference time?

- Concept: **Doppler Effect in Motion Perception**
  - Why needed here: The benchmark uses Doppler shift as a motion cue
  - Quick check question: A sound source moving toward you at 10 m/s emits a 440 Hz tone—will you perceive higher or lower frequency?

## Architecture Onboarding

- Component map: Audio Encoder -> Binaural Input Layer -> Spatial Feature Extractor -> Language Model Backbone -> Motion Reasoning Head
- Critical path: Dual-channel waveform → Early spatial feature extraction (ITD/ILD per frame) → Trajectory-aware temporal encoding → LM reasoning over spatial tokens → Constrained output
- Design tradeoffs:
  - Analytical rendering vs. learned HRTFs: interpretable vs. realistic
  - Explicit spatial modules vs. end-to-end learning: guaranteed cue availability vs. higher-order patterns
  - Task simplicity vs. real-world complexity: synthetic control vs. ecological validity
- Failure signatures:
  - Chance-level accuracy (~25% MCQ, ~50% T/F): Model ignores spatial cues
  - Flat performance across SNR: Indicates spatial cues are ignored, not robust
  - High TNR / Low TPR: Model defaults to "FALSE"
- First 3 experiments:
  1. Input ablation: Compare mono-summed vs. true binaural performance
  2. Explicit spatial probe: Train linear probe on encoder outputs for ITD/ILD prediction
  3. Binaural pretraining pilot: Fine-tune on synthetic binaural motion data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can incorporating explicit ITD/ILD estimators or differentiable binaural-rendering modules resolve spatial reasoning deficits?
- Basis in paper: Authors suggest "incorporating physically grounded acoustic features" as promising avenue
- Why unresolved: Transformers lack inductive biases for geometric transformations
- What evidence would resolve it: Improved accuracy on AudioMotionBench using these architectural modifications

### Open Question 2
- Question: Does pre-training on multi-microphone or ambisonic recordings enable LALMs to treat spatial cues as primary learning signal?
- Basis in paper: Discussion proposes leveraging "multi-microphone or ambisonic recordings during pre-training"
- Why unresolved: Most training corpora dominated by single-channel audio lacking interaural cues
- What evidence would resolve it: Performance gains following large-scale pre-training on spatialized audio

### Open Question 3
- Question: To what extent do real-world acoustic factors like reverberation, occlusion, and interference interact with motion perception?
- Basis in paper: "Limitations" section notes benchmark uses idealized synthesis
- Why unresolved: Current benchmark isolates spatial cues in controlled environment
- What evidence would resolve it: Evaluation results from extended benchmark with real-world acoustic artifacts

### Open Question 4
- Question: Can joint training objectives coupling motion prediction, source localization, and language reasoning induce geometrically coherent representations?
- Basis in paper: Authors ask about "joint objectives that couple motion prediction, source localization, and language-based reasoning"
- Why unresolved: Current models rely on superficial spectral cues lacking spatial supervision
- What evidence would resolve it: Success in multi-task learning where spatial grounding improves alongside semantic tasks

## Limitations

- Benchmark uses synthetic harmonic sources in anechoic conditions, potentially overestimating difficulty
- Authors don't explore whether targeted fine-tuning on binaural data could improve performance
- Causal mechanism attribution to architectural limitations not definitively proven through intervention studies
- Real-world acoustic factors like reverberation and natural sound textures not represented

## Confidence

**High Confidence**: LALMs perform at chance levels on AudioMotionBench (25% MCQ, 50% T/F) is robust and well-supported

**Medium Confidence**: Attribution of failure to lack of binaural training data and architectural limitations is reasonable but not definitively proven

**Low Confidence**: Claim that this represents a "fundamental limitation in auditory spatial reasoning" may overstate the case without intervention studies

## Next Checks

1. **Input Ablation Study**: Feed models mono-summed versions of binaural audio and compare performance to true binaural input

2. **Explicit Spatial Probe**: Freeze audio encoder and train linear regression probe to predict ITD/ILD values per frame from model's internal representations

3. **Targeted Fine-tuning Experiment**: Generate small synthetic dataset (e.g., 1000 samples) of binaural motion clips, then fine-tune baseline LALM on this data and compare zero-shot transfer performance