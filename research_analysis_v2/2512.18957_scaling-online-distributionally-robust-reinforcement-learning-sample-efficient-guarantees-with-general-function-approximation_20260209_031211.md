---
ver: rpa2
title: 'Scaling Online Distributionally Robust Reinforcement Learning: Sample-Efficient
  Guarantees with General Function Approximation'
arxiv_id: '2512.18957'
source_url: https://arxiv.org/abs/2512.18957
tags:
- robust
- learning
- dual
- function
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles online distributionally robust reinforcement
  learning (DR-RL) with general function approximation, addressing the challenge of
  learning a robust policy without prior models or offline data. The authors propose
  Robust Fitted Learning with TV-Divergence Uncertainty Set (RFL-TV), which learns
  through interaction using a novel dual-driven fitted learning approach with a global
  uncertainty quantifier over function classes.
---

# Scaling Online Distributionally Robust Reinforcement Learning: Sample-Efficient Guarantees with General Function Approximation

## Quick Facts
- arXiv ID: 2512.18957
- Source URL: https://arxiv.org/abs/2512.18957
- Reference count: 40
- Key result: Near-optimal sublinear regret bounds for online DR-RL with general function approximation under TV-divergence uncertainty sets

## Executive Summary
This paper addresses online distributionally robust reinforcement learning (DR-RL) with general function approximation, proposing a scalable algorithm called Robust Fitted Learning with TV-Divergence Uncertainty Set (RFL-TV). The method learns robust policies through interaction without requiring prior models or offline data. By reformulating the robust Bellman operator through functional dual optimization, RFL-TV avoids intractable pointwise inner minimization over transition kernels while maintaining optimism for exploration. The algorithm establishes a near-optimal regret bound of $\tilde{O}(H^5\min\{H,\sigma^{-1}\}^2 C_{rcov}^2 \epsilon^{-2})$ under a total variation uncertainty set, achieving scalability to large or continuous state-action spaces.

## Method Summary
RFL-TV learns robust policies by iteratively updating Q-function and dual network parameters using batches of collected transitions. The method reformulates the TV-robust Bellman operator as a dual optimization problem, avoiding per-state-action inner minimization. A global confidence set based on least-squares Bellman residuals controls exploration while maintaining robustness. The algorithm uses a fitted learning approach where the dual network approximates the optimal dual minimizer, and the Q-networks are updated using TD targets that incorporate dual residuals. The method maintains completeness through function class design and uses a failure-state assumption to ensure robust coverability remains finite.

## Key Results
- Establishes near-optimal regret bound of $\tilde{O}(H^5\min\{H,\sigma^{-1}\}^2 C_{rcov}^2 \epsilon^{-2})$ for general function approximation
- Achieves $\tilde{O}(H^2\min\{H,\sigma^{-1}\}\sqrt{d^2K})$ regret in linear setting, near-optimal relative to minimax lower bounds
- Scales to large/continuous state-action spaces with bounds independent of $|S|$ and $|A|$
- Demonstrates empirical superiority over non-robust baselines under various perturbations (action noise, force shifts, pole length changes)

## Why This Works (Mechanism)

### Mechanism 1: Dual-Driven Robust Bellman Operator Approximation
The TV-robust Bellman operator is approximated through functional dual optimization, avoiding intractable pointwise inner minimization over transition kernels. Instead of solving `inf_{P∈U} E_P[V]` for each (s,a), the paper reformulates this as a single optimization over dual functions `g ∈ L1(μ)`. The dual loss `Dualloss(g;f) = E_{(s,a)∼μ}[E_{s'∼P⋆}[(g(s,a) - max_a' f(s',a'))_+] - (1-σ)g(s,a)]` is minimized empirically to obtain `g_f`, which then defines the approximate robust Bellman operator `T^σ_g f`. This approximation relies on the dual function class G being sufficiently expressive to approximate the optimal dual.

### Mechanism 2: Global Confidence Sets via Least-Squares Bellman Residual
A single least-squares objective over the dual residual simultaneously approximates the robust Bellman operator and quantifies uncertainty for exploration. The confidence set `F^(k)_h = {f ∈ F_h : L^(k)_h(f_h, f_{h+1}, g_{f_{h+1}}) - min L^(k)_h ≤ β}` uses a global squared Bellman error criterion. This aggregates estimation error across all (s,a) pairs under the visitation distribution, rather than adding per-state-action bonuses. Optimism selects the most optimistic function within this set, maintaining exploration while controlling uncertainty.

### Mechanism 3: Robust Coverability Controls Distribution Mismatch
The robust coverability coefficient `C_rcov` characterizes the fundamental difficulty of learning robust policies from nominal data by quantifying the worst-case density ratio between adversarial and nominal visitations. The paper shows regret decomposes into terms measured under worst-case visitation d^{π,P^ω}, but empirical error is controlled under nominal visitation μ^π. Robust coverability `C_rcov = sup_{π,h} ||d^π_h / μ^π_h||_∞` bounds this transfer via Hölder's inequality. Failure-state assumptions ensure C_rcov < ∞, enabling sample-efficient learning.

## Foundational Learning

- **Robust Bellman Equations and RMDPs**: The entire algorithm is built on the robust Bellman operator T^σ, which differs fundamentally from standard Bellman operators due to the inner infimum over uncertainty sets. Quick check: Can you explain why `Q^⋆,σ_h(s,a) = r_h(s,a) + inf_{P∈U_σ} E_P[V^⋆,σ_{h+1}]` differs from the standard Bellman backup?

- **Total Variation Divergence and Uncertainty Sets**: The TV-divergence uncertainty set structure enables the dual reformulation; understanding the constraint `D_TV(P, P^⋆) ≤ σ` is essential for interpreting the dual loss. Quick check: What does σ=0 recover, and what happens as σ→1?

- **Fitted Value Iteration with Function Approximation**: RFL-TV follows the fitted learning template—alternating between empirical Bellman backup and projection onto function class F—extended with dual optimization and global confidence sets. Quick check: Why does fitted learning require completeness (closure under Bellman operator) but not necessarily realizability?

## Architecture Onboarding

- **Component map**: Q-networks (f ∈ F) -> Dual network (g) -> Confidence set F^(k) -> Q-updates -> Dual updates -> Target networks

- **Critical path**: 
  1. Collect trajectory under ε-greedy exploration on nominal environment
  2. Sample mini-batch; compute next-state values via target Q-networks
  3. Update dual network g by minimizing `L_g = E[(max{||dual_term|| - β, 0})^2]`
  4. Recompute dual terms with updated g; form TD targets y = r + γ(v_next + dual_term)
  5. Update Q-networks via squared TD loss; soft-update targets

- **Design tradeoffs**:
  - **Robustness radius σ**: Larger σ increases robustness to perturbations but may reduce nominal performance; experiments suggest σ ∈ [0.4, 0.6] works well for CartPole
  - **Slack parameter β**: β=0 enforces strict dual constraint (harder optimization); β>0 relaxes constraint (more stable but potentially less robust)
  - **Dual network capacity**: Larger hidden sizes reduce approximation error ξ_dual but increase compute; Figure 3 shows 128→256 hidden units yields 40-250% return gains

- **Failure signatures**:
  - **Catastrophic robustness gap**: Policy achieves high nominal return but collapses under perturbations → σ too small or dual network under-capacity
  - **Excessive conservatism**: Policy underperforms even on nominal environment → σ too large
  - **Training instability**: Dual loss diverges → β too small or learning rate too high
  - **No improvement over non-robust baselines**: Dual network may not be learning; check gradient flow through dual_term

- **First 3 experiments**:
  1. **Baseline comparison with perturbation sweep**: Train RFL-TV (σ∈{0.4,0.5,0.6}) alongside DQN/GOLF on CartPole; evaluate under action perturbations ρ∈[0.3,0.9], force-magnitude shifts η∈[0.2,0.8], and pole-length changes. Verify RFL-TV maintains ≥30% higher returns than non-robust baselines under moderate perturbations.
  2. **Ablation on dual network capacity**: Fix σ=0.5 and vary dual hidden sizes in {(64,64), (128,128), (256,256)}. Measure returns under a fixed perturbation level (e.g., ρ=0.5 action noise). Confirm monotonic improvement with capacity as shown in Figure 3.
  3. **Robustness radius calibration**: Train with σ∈{0.0,0.2,0.4,0.6,0.8} and evaluate across all three perturbation families. Identify the σ range where returns saturate without excessive nominal degradation; verify this aligns with the paper's finding of σ≈0.4-0.6 as the sweet spot.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the $\sqrt{H^3}$ gap in horizon dependence be closed for general function approximation, and can matching lower bounds be established?
- **Basis**: Page 11 states: "Closing the remaining $\sqrt{H^3}$ gap in the horizon dependence and establishing matching lower bounds for online DR-RL with general function approximation... remain important directions for future work."
- **Why unresolved**: Current upper bounds match minimax lower bounds in the linear setting only up to an additional factor of $O(\sqrt{H^3})$.
- **What evidence would resolve it**: An algorithm achieving regret bounds with improved polynomial dependence on $H$, or a theoretical lower bound proof demonstrating the necessity of the current dependence.

### Open Question 2
- **Question**: Can the analysis of RFL-TV be adapted to utilize alternative complexity measures such as Bellman rank or Bellman-Eluder dimension?
- **Basis**: Page 9, Remark 2 notes: "There is also a line of work in online RL that employs complexity measures such as Bellman rank... and BE dimension... and we expect our analysis could similarly be adapted to these notions."
- **Why unresolved**: Current theoretical guarantees rely specifically on the "robust coverability coefficient" ($C_{rcov}$) rather than other standard measures of function class complexity used in non-robust RL.
- **What evidence would resolve it**: Deriving regret bounds for the algorithm (or a variant) where the sample complexity scales with the Bellman rank or Bellman-Eluder dimension instead of coverability.

### Open Question 3
- **Question**: Is the failure-state assumption (Assumption 2) necessary for sample efficiency, or can it be relaxed?
- **Basis**: Page 6 explains that without this assumption, the sample complexity can be exponentially large due to the "support shifting issue" where worst-case dynamics differ from nominal ones.
- **Why unresolved**: The paper introduces the assumption to guarantee the finiteness of $C_{rcov}$, but this imposes a specific structural constraint on the environment dynamics which limits generality.
- **What evidence would resolve it**: An algorithm that achieves sublinear regret under strictly weaker structural assumptions, or a proof that the failure-state assumption is information-theoretically necessary for polynomial sample complexity.

## Limitations

- The theoretical guarantees assume TV-divergence uncertainty sets, which may not capture all real-world perturbations
- The robust coverability coefficient $C_{rcov}$ is bounded but not explicitly characterized for different function classes
- The dual approximation error $\xi_{dual}$ contributes additively to regret but its practical impact depends on function class expressiveness
- The failure-state assumption imposes strong structural constraints on environment dynamics

## Confidence

- **High**: Sample complexity bound under stated assumptions
- **Medium**: Near-optimality in linear setting relative to minimax lower bounds
- **Low**: Practical scalability to very large or continuous state-action spaces without further empirical validation

## Next Checks

1. **Robust Coverability Coefficient Estimation**: Empirically estimate $C_{rcov}$ for different MDPs (e.g., CartPole variants with varying dynamics) to understand its impact on sample complexity and validate the assumption of finiteness.

2. **Dual Function Class Capacity Scaling**: Systematically vary the dual network's capacity (e.g., hidden layer sizes) and measure the trade-off between $\xi_{dual}$ and regret. Quantify how much the approximation error contributes to practical performance gaps.

3. **Non-TV Uncertainty Sets**: Extend the algorithm to handle KL-divergence or Wasserstein uncertainty sets and compare regret bounds and empirical performance. Assess the robustness of the dual reformulation to different divergence structures.