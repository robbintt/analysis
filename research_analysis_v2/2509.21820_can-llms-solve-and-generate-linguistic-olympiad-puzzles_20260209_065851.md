---
ver: rpa2
title: Can LLMs Solve and Generate Linguistic Olympiad Puzzles?
arxiv_id: '2509.21820'
source_url: https://arxiv.org/abs/2509.21820
tags:
- puzzles
- puzzle
- linguistic
- uklo
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores using large language models (LLMs) to solve
  and generate linguistic puzzles used in high school competitions like the International
  Linguistics Olympiad. The authors extend the LINGOLY benchmark with puzzles on writing
  systems and test OpenAI's o1 model with reasoning capabilities.
---

# Can LLMs Solve and Generate Linguistic Olympiad Puzzles?

## Quick Facts
- arXiv ID: 2509.21820
- Source URL: https://arxiv.org/abs/2509.21820
- Reference count: 40
- LLMs generally outperform humans on most linguistic puzzle types except writing systems and understudied languages

## Executive Summary
This paper investigates the capabilities of large language models in solving and generating linguistic puzzles used in high school competitions like the International Linguistics Olympiad. The authors extend the LINGOLY benchmark with writing system puzzles and test OpenAI's o1 model with reasoning capabilities. Results show that while LLMs excel at most puzzle types by leveraging pre-training data, they struggle with writing systems and puzzles involving understudied languages. The study also explores puzzle generation, finding that LLMs can create valid but simple puzzles under specific prompt settings. The research highlights the distinction between knowledge retrieval and genuine reasoning in LLM performance.

## Method Summary
The study uses the LINGOLY benchmark (90 UKLO puzzles) and 31 writing system puzzles converted to images. OpenAI's o1 model is employed for solving with reasoning capabilities, while GPT-4o and o1 are used for generation tasks. Three prompt settings are tested: zero-shot, one-shot with Lithuanian puzzle, and few-shot with four puzzles. Solving uses exact match scoring against UKLO answer keys, while generation validity is assessed by three Olympiad participants using Zhurinsky's criteria. The approach combines linguistic puzzle theory with practical LLM evaluation to explore both solving and generation capabilities.

## Key Results
- LLMs outperform humans on phonology, morphology, syntax, semantics, numbers, and compounding puzzles
- Humans outperform LLMs on writing system puzzles at higher difficulty levels
- Generated puzzles are valid but simple, often lacking complexity found in Olympiad-level problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High performance correlates with pre-training data density rather than deductive reasoning
- Mechanism: LLMs retrieve memorized patterns for well-known languages, bypassing context-bound inference
- Core assumption: Models prioritize retrieved knowledge over context when language is familiar
- Evidence: o1 outputs "Georgia" instead of "Sakartvelo" for Georgian puzzle; reasoning model excels on high-resource languages

### Mechanism 2
- Claim: Few-shot prompts enforce self-contained validity constraints
- Mechanism: Zero-shot prompts default to generic question-answer format; few-shot examples align output to specific puzzle structure
- Core assumption: LLM cannot intuit strict puzzle format without demonstration
- Evidence: Zero-shot lacks preamble; few-shot adopts specific structure but risks context repetition

### Mechanism 3
- Claim: Writing system puzzles expose modality gap in visual encoding
- Mechanism: LLMs process script images as patterns but fail at higher-level decipherment logic
- Core assumption: Vision encoder cannot perform structural analysis of unfamiliar scripts
- Evidence: Humans outperform LLMs on highest difficulty writing system levels

## Foundational Learning

- **Zhurinsky's Criteria for Validity**
  - Why needed: Formal standard for invalidating generated puzzles with parasitic solutions or external knowledge requirements
  - Quick check: Can puzzle be solved with only provided clues?

- **Rosetta Stone vs. Match-up Formats**
  - Why needed: Distinguishes between puzzles providing explicit correspondences versus requiring inference
  - Quick check: Does puzzle provide pairings or require discovering them?

- **Data Contamination vs. Reasoning**
  - Why needed: Distinguishes between knowledge retrieval and genuine deduction in superhuman results
  - Quick check: Is model deducing from prompt or recalling from training?

## Architecture Onboarding

- **Component map**: Input Parser (PDFs â†’ images/text) -> Solver Engine (o1/GPT-4o) -> Evaluator (Exact Match/Human Review)
- **Critical path**: Transition from Solving to Generation requires understanding failure modes in reasoning vs. retrieval
- **Design tradeoffs**: o1 better at solving but prone to knowledge override; few-shot ensures validity but risks context repetition
- **Failure signatures**: "Hallucinated Expert" (real-world entity over puzzle logic), "Lazy Author" (context repetition), "Impossible Puzzle" (external knowledge requirement)
- **First 3 experiments**: 1) Test constructed languages to verify retrieval vs. reasoning, 2) Stress-test generation validity with low-resource languages, 3) Compare text vs. image formats for writing systems

## Open Questions the Paper Calls Out

1. How can creativity and difficulty of generated puzzles be formally evaluated? (No automated method exists for puzzle quality assessment)
2. Can LLMs generate valid puzzles for topics beyond morphology and at intermediate/advanced difficulty levels?
3. How can models be guided to rely on in-context reasoning rather than pre-trained language knowledge?

## Limitations
- Human expertise required for validity assessment limits automation and scalability
- Performance metrics conflate knowledge retrieval with genuine reasoning
- Generated puzzles lack the complexity of Olympiad-level problems

## Confidence
- **High Confidence**: LLMs outperform humans on most puzzle types with sufficient language data
- **Medium Confidence**: Retrieval-vs-reasoning mechanism plausible but needs constructed language testing
- **Low Confidence**: Modality gap hypothesis for writing systems lacks direct visual reasoning evidence

## Next Checks
1. Test LLMs on constructed languages with identical logical structures to high-resource puzzles
2. Develop heuristic checks for Zhurinsky's criteria to enable automated validity scoring
3. Reformulate writing system puzzles as text to isolate visual processing limitations from reasoning gaps