---
ver: rpa2
title: 'Gradient Descent as Loss Landscape Navigation: a Normative Framework for Deriving
  Learning Rules'
arxiv_id: '2510.26997'
source_url: https://arxiv.org/abs/2510.26997
tags:
- learning
- loss
- optimal
- gradient
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a normative framework for deriving learning
  rules by treating them as optimal policies for navigating partially observable loss
  landscapes. The authors cast learning as a continuous-time optimal control problem
  where the objective balances parameter change cost, loss minimization, and belief
  updating about the loss landscape.
---

# Gradient Descent as Loss Landscape Navigation: a Normative Framework for Deriving Learning Rules

## Quick Facts
- **arXiv ID**: 2510.26997
- **Source URL**: https://arxiv.org/abs/2510.26997
- **Reference count**: 40
- **Primary result**: Learning rules emerge as optimal policies for navigating loss landscapes when cast as continuous-time optimal control problems

## Executive Summary
This paper presents a normative framework that derives common learning rules by treating them as optimal policies for navigating partially observable loss landscapes. The authors formulate learning as a continuous-time optimal control problem where the objective balances parameter change costs, loss minimization, and belief updating about the landscape. Under different assumptions about planning horizon, parameter space geometry, and observability, the framework naturally recovers well-known algorithms: gradient descent from short-horizon planning, momentum from longer horizons, natural gradients from geometric considerations, and adaptive optimizers like Adam from Bayesian inference of loss shape. The work provides a principled foundation for understanding learning dynamics and suggests that evaluating learning rules should consider the computational assumptions they encode.

## Method Summary
The authors frame learning as minimizing a functional $J$ that combines kinetic energy (parameter change cost) and potential energy (loss), solved via Euler-Lagrange equations. They implement a "Ballistic" optimizer approximating square-root Hessian normalization using running gradient variance as a diagonal proxy. Experiments use a double-well potential for theoretical validation, and MNIST/CIFAR image classification tasks to compare the Ballistic rule against SGD and Adam. The framework allows interpolation between different learning rules by varying the temporal discount rate $\gamma$, metric tensor $G$, and observability assumptions about the loss landscape.

## Key Results
- Momentum emerges naturally as a consequence of multi-step optimization over longer planning horizons
- Adaptive optimizers like Adam arise from treating loss landscapes as partially observable and inferring their shape via Bayesian filtering
- Non-gradient learning rules emerge when parameter spaces have non-trivial geometry or inherent drift dynamics
- The framework provides a unified explanation for diverse learning phenomena under a single normative objective

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Momentum is a necessary consequence of optimizing over multi-step time horizons rather than single steps.
- **Mechanism**: The objective includes a term penalizing squared velocity over time. Long horizons (low $\gamma$) yield second-order dynamics (inertia), manifesting as momentum, while short horizons ($\gamma \gg 1$) recover gradient descent.
- **Core assumption**: The objective balances kinetic and potential energy, and the optimizer is farsighted.
- **Evidence anchors**: Abstract mentions "momentum from longer-horizon planning"; section 3 shows temporal discounting allows interpolation between momentum and GD.
- **Break condition**: If optimization is framed as independent single-step problems, momentum vanishes.

### Mechanism 2
- **Claim**: Adaptive learning rates emerge as optimal solutions when treating loss landscapes as partially observable.
- **Mechanism**: Modeling loss as a hidden variable, the learner maintains belief states about gradient statistics. The optimal policy yields step sizes inversely proportional to gradient variance, justifying Adam's update logic.
- **Core assumption**: The learner operates in a ballistic regime with slowly updated beliefs about landscape geometry.
- **Evidence anchors**: Abstract mentions "adaptive optimizers like Adam from online Bayesian inference"; section 5 shows effective learning rate scales as $V_t^{-1/2}$.
- **Break condition**: If landscape is fully observable, variance-adaptive terms collapse to constant or Hessian-based scaling.

### Mechanism 3
- **Claim**: Non-gradient rules arise from non-trivial parameter space geometry or inherent drift dynamics.
- **Mechanism**: Introducing drift terms and non-Euclidean metrics into the kinetic energy penalty produces optimal trajectories deviating from steepest descent, including spiraling paths when drift is rotational.
- **Core assumption**: The learner experiences uncontrollable parameter dynamics resulting in default drift.
- **Evidence anchors**: Section 4 discusses "partially controllable parameters" producing non-gradient rules; abstract mentions natural gradients from parameter geometry.
- **Break condition**: If drift is zero and metric is isotropic, rules collapse to standard gradient descent.

## Foundational Learning

- **Concept: Euler-Lagrange Equations (Calculus of Variations)**
  - **Why needed here**: Derives all learning rules by solving these equations to minimize the action integral $J$.
  - **Quick check question**: Can you explain how minimizing an integral functional leads to a second-order differential equation describing the path of least resistance?

- **Concept: Riemannian Geometry (Metric Tensors)**
  - **Why needed here**: Understanding Natural Gradient Descent requires distinguishing between Euclidean parameter space and curved spaces defined by the Fisher Information Matrix $G$.
  - **Quick check question**: How does changing the metric tensor $G$ from identity to a positive-definite matrix change the definition of the "shortest path" between two points?

- **Concept: Partial Observability & Bayesian Filtering**
  - **Why needed here**: The derivation of adaptive optimizers relies on treating the true gradient as a latent variable estimated from noisy observations.
  - **Quick check question**: In a Hidden Markov Model, how does the posterior distribution evolve as new observations arrive?

## Architecture Onboarding

- **Component map**: Objective ($J$) -> Euler-Lagrange Solver -> Continuous-time ODEs -> Discretizer -> Parameter Updates

- **Critical path**:
  1. Define constraints: Set geometry ($G$), observability, and planning horizon ($\gamma$)
  2. Formulate objective: Construct $J$
  3. Solve EL equations: Derive continuous-time ODEs
  4. Discretize: Apply time step $\Delta t$ to generate update rules

- **Design tradeoffs**:
  - Accuracy vs. Scalability: Full Hessian yields superior paths but is computationally prohibitive; diagonal approximations offer practical compromise
  - Horizon vs. Stability: Longer horizons offer faster convergence but risk instability compared to overdamped safety of standard GD

- **Failure signatures**:
  - "Spiral" Trap: Rotational drift may converge to points different from global minimum
  - Ballistic Overshoot: Inaccurate curvature estimation can cause divergence in high-inertia regime

- **First 3 experiments**:
  1. Vary planning horizon: Implement continuous-time integrator for Eq. 2, sweep $\gamma$ on quadratic loss to reproduce trajectory shift
  2. Derive "Ballistic" Adam: Implement update rule $\Delta\theta \propto -\sqrt{\eta k} V^{-1/2} m \Delta t$ using diagonal $V$ approximation, compare convergence on MNIST
  3. Induce non-gradient dynamics: Introduce rotational drift term $f(\theta)$, solve resulting EL equations, verify spiraling trajectory converges to biased equilibrium

## Open Questions the Paper Calls Out
None

## Limitations
- Continuous-time framework assumes infinitesimal time steps, but practical implementations rely on discretization that may break theoretical guarantees
- Several derivations depend on hand-specified metric tensors $G$ and drift terms $f$ rather than learning them from data
- Limited empirical validation of the continuous-to-discrete transition and non-gradient dynamics beyond toy examples

## Confidence

- **High confidence**: Momentum emergence from multi-step planning (Mechanism 1) is mathematically rigorous and well-supported
- **Medium confidence**: Bayesian inference interpretation of adaptive optimizers (Mechanism 2) provides compelling framework but limited empirical validation
- **Low confidence**: Claims about non-gradient dynamics from partial controllability (Mechanism 3) are mathematically sound but lack extensive empirical demonstration

## Next Checks

1. Test ballistic Adam variant across diverse architectures and datasets to verify $\sqrt{V_t^{-1}}$ scaling robustness beyond MNIST/CIFAR
2. Implement synthetic loss landscapes with controlled rotational drift terms to systematically study when non-gradient dynamics emerge
3. Compare computational overhead of exact Hessian-based metrics versus diagonal approximations across different model scales to quantify practical tradeoffs