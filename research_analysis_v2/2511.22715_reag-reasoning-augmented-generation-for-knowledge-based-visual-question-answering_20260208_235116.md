---
ver: rpa2
title: 'ReAG: Reasoning-Augmented Generation for Knowledge-based Visual Question Answering'
arxiv_id: '2511.22715'
source_url: https://arxiv.org/abs/2511.22715
tags:
- reasoning
- reag
- answer
- question
- passages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ReAG improves knowledge-based visual question answering by combining
  multi-level retrieval with a critic model that filters irrelevant passages, and
  a generator trained via supervised fine-tuning and reinforcement learning to produce
  explicit reasoning traces. The critic reduces noise in the retrieval pipeline, while
  the generator learns to reason over retrieved evidence and output structured answers.
---

# ReAG: Reasoning-Augmented Generation for Knowledge-based Visual Question Answering

## Quick Facts
- arXiv ID: 2511.22715
- Source URL: https://arxiv.org/abs/2511.22715
- Reference count: 40
- Primary result: State-of-the-art KB-VQA performance with up to 7.8-point gains on single-hop questions

## Executive Summary
ReAG introduces a reasoning-augmented generation framework for knowledge-based visual question answering (KB-VQA). The method combines multi-level retrieval (coarse + fine-grained) with a critic model that filters irrelevant passages, and a generator trained via supervised fine-tuning and reinforcement learning to produce explicit reasoning traces. The critic reduces noise in the retrieval pipeline, while the generator learns to reason over retrieved evidence and output structured answers. On Encyclopedic-VQA and InfoSeek, ReAG achieves state-of-the-art accuracy, outperforming prior methods by up to 7.8 points on single-hop questions and 4.3 points overall, while providing interpretable reasoning grounded in retrieved content.

## Method Summary
ReAG operates through a three-stage pipeline: retrieval, filtering, and generation. First, EVA-CLIP-8B performs coarse-grained retrieval by encoding the full query image against document images/metadata, then fine-grained retrieval using GroundingDINO to detect and crop the subject before re-retrieving. Retrieved passages are merged and ranked. Second, a critic model (Qwen2.5-VL-3B) fine-tuned on passage relevance labels filters out irrelevant passages, retaining only those exceeding a probability threshold. Third, the generator (Qwen2.5-VL) undergoes supervised fine-tuning on reasoning traces produced by a teacher model, followed by reinforcement learning (GRPO-style) to optimize a reward combining task accuracy and format adherence. The final output includes explicit reasoning steps followed by the answer.

## Key Results
- Achieves state-of-the-art accuracy on Encyclopedic-VQA and InfoSeek benchmarks
- Outperforms prior methods by up to 7.8 points on single-hop questions and 4.3 points overall
- Explicit reasoning traces improve interpretability while maintaining or improving accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Critic-based filtering reduces noise in the retrieval pipeline, improving generator accuracy by removing irrelevant passages.
- Mechanism: An autoregressive MLLM (Qwen2.5-VL-3B) fine-tuned on passage relevance labels predicts Yes/No for each retrieved passage. Only passages exceeding a probability threshold (0.1) are retained.
- Core assumption: The critic can reliably distinguish relevant from irrelevant passages even when they are semantically related (hard negatives).
- Evidence anchors:
  - [abstract] "a critic model that filters irrelevant passages, ensuring high-quality additional context"
  - [Section 4.4] Critic reduces passages from 128.6 to 5.7 on average at k=20, yielding "more than a twofold gain"
  - [corpus] VLM-PRF (arXiv:2510.14605) similarly employs external filtering, achieving strong KB-VQA results—consistent with filtering as an effective mechanism.
- Break condition: If retrieval returns no relevant passages at all, or if the critic threshold is too aggressive, the generator receives empty context and must fall back to zero-shot reasoning.

### Mechanism 2
- Claim: Explicit reasoning traces produced via multi-stage training (SFT cold start → RL refinement) improve answer accuracy over implicit reasoning.
- Mechanism: SFT exposes the generator to structured reasoning traces linking image, question, and passage. RL (GRPO-inspired) then optimizes a reward combining task accuracy and format adherence, encouraging the model to produce valid reasoning steps before the answer.
- Core assumption: Reasoning traces generated by a teacher MLLM (Qwen2.5-VL-7B) are logically coherent and serve as effective supervision.
- Evidence anchors:
  - [abstract] "reinforcement learning to enhance reasoning over retrieved content, while supervised fine-tuning serves only as a cold start"
  - [Section 3.4] "completions with above-average rewards have their likelihood increased, while those below the mean are down-weighted"
  - [corpus] GRPO-CARE (arXiv:2506.16141) shows RL-based coherence training improves multimodal reasoning—supports RL-for-reasoning mechanism, though direct replication on KB-VQA is unverified.
- Break condition: If the reward function poorly matches task success (e.g., format reward dominates over accuracy), the model may game the reward without genuine reasoning improvement.

### Mechanism 3
- Claim: Multi-level retrieval (coarse + fine-grained) increases recall of relevant passages compared to single-stage retrieval.
- Mechanism: Coarse-grained retrieval encodes the full query image against document images/metadata. Fine-grained retrieval detects the subject via GroundingDINO, crops the image, and retrieves using the focused patch. Results are merged and ranked.
- Core assumption: The detection model correctly localizes the question's subject, and the cropped region aligns with relevant document images.
- Evidence anchors:
  - [Section 3.1] "By restricting the visual input to the region of interest, this stage allows the retriever to focus on more fine-grained visual details"
  - [Section 4.4] Adding fine-grained retrieval yields additional gains on both datasets over coarse-only.
  - [corpus] OMGM (ACL 2025) similarly employs multi-granularity retrieval; ReAG outperforms OMGM even when using only OMGM's first-stage retrieval—suggesting filtering and reasoning are the dominant factors, not retrieval granularity alone.
- Break condition: If GroundingDINO fails to detect the subject (e.g., abstract questions, no clear entity), fine-grained retrieval degrades to coarse-only behavior.

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG) for multimodal inputs**
  - Why needed here: ReAG extends text-only RAG to image+text queries, requiring understanding of how visual encoders (CLIP, EVA-CLIP) map images to shared embedding spaces.
  - Quick check question: Can you explain why cosine similarity is used to rank retrieved documents against a query image embedding?

- Concept: **Reinforcement Learning from Reward Models (RL-based optimization for LLMs)**
  - Why needed here: ReAG uses GRPO-style RL to optimize reasoning traces; understanding policy gradients, advantage estimation, and reward design is essential.
  - Quick check question: What is the role of the advantage term Â in Equation 8, and why is it normalized across completions in a batch?

- Concept: **Supervised Fine-Tuning (SFT) as cold start**
  - Why needed here: ReAG uses SFT only to initialize reasoning behavior before RL, not as the final training stage.
  - Quick check question: Why might skipping SFT and training directly with RL lead to instability or poor convergence?

## Architecture Onboarding

- Component map: EVA-CLIP-8B (retriever) -> GroundingDINO (subject detector) -> Critic (Qwen2.5-VL-3B) -> Generator (Qwen2.5-VL) -> Answer + Reasoning Trace
- Critical path: 1) Encode query image → retrieve top-k documents (coarse) 2) Detect subject → crop → retrieve top-k documents (fine) 3) Merge → critic filters passages → pass P_relevant to generator 4) Generator produces reasoning steps followed by answer
- Design tradeoffs: k=20 documents balances recall vs. noise; higher k increases filtering cost without accuracy gains (Fig. 6). Threshold=0.1 is conservative; higher thresholds may discard relevant passages. RL training is computationally expensive (48h on 32-64 A100s) but yields +4-8 point gains.
- Failure signatures: Empty P_relevant: critic filters all passages → generator falls back to zero-shot, may hallucinate. Poor retrieval: neither coarse nor fine-grained retrieval finds relevant documents → answer accuracy drops sharply (Fig. 3, "w/o Evidence" bars). Reward hacking: model generates well-formatted but incorrect reasoning if format reward dominates.
- First 3 experiments: 1) Ablate the critic: Replace critic with simple threshold (e.g., keep all top-5 passages) to quantify filtering contribution on accuracy and latency. 2) Vary k and threshold: Sweep k ∈ {5, 10, 20, 40} and threshold ∈ {0.05, 0.1, 0.3, 0.5} to find optimal precision-recall tradeoff for a new KB-VQA dataset. 3) SFT-only vs. SFT+RL: Train generator with SFT only and compare against full SFT→RL pipeline to validate RL contribution on a held-out validation split.

## Open Questions the Paper Calls Out

- Question: How would ReAG perform when applied to knowledge bases beyond Wikipedia (e.g., domain-specific corpora, semi-structured databases, or live web retrieval)?
  - Basis in paper: [explicit] The paper states in Limitations: "the quality of ReAG depends on the reliability of the retrieved evidence" and notes retrieval failures can lead to incorrect reasoning.
  - Why unresolved: All experiments use static Wikipedia-derived knowledge bases (2M pages for E-VQA, 100k–6M for InfoSeek). No evaluation on other retrieval sources or live retrieval scenarios is conducted.
  - What evidence would resolve it: Experiments applying ReAG to alternative knowledge bases (e.g., scientific literature, medical databases, or web search APIs) with comparative performance metrics.

- Question: Can the reasoning trace length be dynamically controlled to balance interpretability against inference latency?
  - Basis in paper: [explicit] Limitations section notes: "the generator produces a detailed reasoning trace, which improves the explainability of the final answer but may also increase latency, as more tokens must be generated."
  - Why unresolved: The current architecture always generates full reasoning traces; no mechanism for adaptive or condensed reasoning is explored.
  - What evidence would resolve it: Implementation of controllable trace generation (e.g., via length penalties or early stopping) with latency measurements and accuracy trade-off curves.

- Question: Would scaling the critic model beyond 3B parameters yield improved filtering precision, particularly for harder negatives?
  - Basis in paper: [inferred] The critic model is fixed at Qwen2.5-VL-3B regardless of generator scale (3B or 7B). Hard negatives constitute 70% of irrelevant passages in training, yet the relationship between critic capacity and filtering quality is unexplored.
  - Why unresolved: Ablation studies only test presence/absence of the critic, not its architecture or scale. The 3B critic may lack capacity for subtle relevance distinctions.
  - What evidence would resolve it: Experiments varying critic model size (e.g., 7B, 13B) with analysis of precision/recall trade-offs and downstream answer accuracy.

## Limitations

- Limited ablation of critic vs. retrieval design: The multi-stage training pipeline makes it difficult to attribute performance gains to a single mechanism.
- Dependence on high-quality teacher traces: If teacher traces contain errors, the generator may inherit or amplify them.
- Computational cost: The full training pipeline requires 32-64 A100 GPUs for ~48 hours, making it impractical for rapid iteration.

## Confidence

- High confidence: Multi-level retrieval with critic filtering improves accuracy over single-stage retrieval baselines; RL fine-tuning after SFT provides measurable gains in reasoning quality and answer accuracy.
- Medium confidence: The explicit reasoning traces are genuinely interpretable and grounded in retrieved content (qualitative inspection supports this, but systematic human evaluation is not provided).
- Low confidence: The exact contribution of each component (critic threshold, k value, RL reward balance) is not precisely quantified; some design choices appear heuristic.

## Next Checks

1. **Component ablation study**: Train ablations that remove the critic, skip fine-grained retrieval, or use SFT-only (no RL) to quantify each stage's marginal contribution to accuracy and robustness.
2. **Cross-dataset robustness test**: Evaluate ReAG on a held-out KB-VQA dataset (e.g., OK-VQA or A-OKVQA) to test generalization beyond encyclopedic knowledge.
3. **Human evaluation of reasoning traces**: Conduct a blinded study where annotators rate the logical coherence and factual accuracy of generated reasoning traces, independent of final answer correctness.