---
ver: rpa2
title: Attention Is All You Need
arxiv_id: '1706.03762'
source_url: https://arxiv.org/abs/1706.03762
tags:
- attention
- output
- positions
- values
- sequence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper tackles the inefficiency of recurrent and convolutional\
  \ encoder\u2011decoder models, whose sequential computation limits parallelism and\
  \ inflates training time for long sequences. It introduces the Transformer, an architecture\
  \ that replaces recurrence and convolutions entirely with stacked multi\u2011head\
  \ self\u2011attention and point\u2011wise feed\u2011forward layers, using residual\
  \ connections and layer normalization."
---

# Attention Is All You Need

## Quick Facts
- **arXiv ID:** 1706.03762  
- **Source URL:** https://arxiv.org/abs/1706.03762  
- **Reference count:** 40  
- **Primary result:** Transformer achieves 28.4 BLEU (EN‑DE) and 41.8 BLEU (EN‑FR) with a single model trained in 3.5 days on eight GPUs.

## Executive Summary
The paper introduces the **Transformer**, a novel sequence‑to‑sequence architecture that eliminates recurrence and convolution in favor of stacked multi‑head self‑attention and point‑wise feed‑forward networks. By doing so, it enables constant‑time global dependency modeling and massive parallelism across all positions in a sequence. Empirical evaluation on WMT 2014 translation tasks shows state‑of‑the‑art BLEU scores—28.4 for English‑German and 41.8 for English‑French—using a single model trained in only 3.5 days on eight GPUs. The same architecture also transfers to English constituency parsing, demonstrating broader applicability.

## Method Summary
The Transformer consists of an encoder and a decoder, each built from identical layers that combine (1) **multi‑head self‑attention** to capture relationships between all token positions simultaneously, (2) **position‑wise feed‑forward networks** applied independently to each position, (3) **residual connections** and **layer normalization** to stabilize training. Positional encodings inject order information. The decoder adds a masked self‑attention sub‑layer to prevent attending to future tokens and an encoder‑decoder attention sub‑layer to incorporate source context. Training uses the Adam optimizer with a custom learning‑rate schedule and label‑smoothed cross‑entropy loss.

## Key Results
- 28.4 BLEU on WMT 2014 English‑German, >2 BLEU over previous best (including ensembles).  
- 41.8 BLEU on WMT 2014 English‑French with a single model.  
- Training completed in 3.5 days on eight GPUs, far less compute than prior recurrent/convolutional models.

## Why This Works (Mechanism)
1. **Global attention in constant time** – Multi‑head self‑attention computes pairwise token interactions in parallel, removing the sequential bottleneck of RNNs and allowing the model to learn long‑range dependencies efficiently.  
2. **Parallelizable feed‑forward layers** – Position‑wise fully connected networks operate independently on each token, further exploiting GPU parallelism.  
3. **Residual connections + layer normalization** – These stabilize gradients across deep stacks, enabling the training of very deep models without degradation.  
4. **Learnable positional encodings** – Provide the model with order information despite the lack of recurrence, preserving sequence structure.

## Foundational Learning
| Concept | Why needed | Quick check |
|--------|------------|-------------|
| Scaled dot‑product attention | Core operation that computes weighted sums of values; understanding scaling prevents gradient issues. | Can you write the formula for attention and explain the purpose of the scaling factor? |
| Multi‑head mechanism | Allows the model to attend to information from different representation subspaces jointly. | How many heads are used in the base Transformer and why? |
| Positional encoding | Supplies order information absent in pure attention. | What sinusoidal function is used and how does it encode position? |
| Label smoothing | Regularizes the classifier, improving generalization. | What smoothing value is reported and what effect does it have on loss? |
| Adam with warm‑up schedule | Stabilizes early training and adapts learning rates. | Describe the learning‑rate schedule equation used in the paper. |

## Architecture Onboarding
**Component map**  
Input tokens → Embedding + Positional Encoding → Encoder stack (Self‑Attention → Feed‑Forward) → Encoder output → Decoder stack (Masked Self‑Attention → Encoder‑Decoder Attention → Feed‑Forward) → Linear + Softmax → Output tokens  

**Critical path**  
1. Compute self‑attention for all encoder layers (dominant compute).  
2. Compute decoder masked self‑attention and encoder‑decoder attention (sequential per target token).  
3. Apply feed‑forward networks and residual connections.  

**Design tradeoffs**  
- **Accuracy vs. speed:** More attention heads and larger hidden sizes improve BLEU but increase memory and compute.  
- **Model depth vs. training stability:** Deeper stacks capture richer abstractions but require careful learning‑rate scheduling and normalization.  
- **Parallelism vs. inference latency:** Full parallelism speeds training; however, autoregressive decoding still incurs step‑wise latency at inference time.  

**Failure signatures**  
- NaN or exploding loss → likely learning‑rate schedule mis‑implementation.  
- BLEU far below reported → possible tokenization/BPE mismatch or missing positional encodings.  
- GPU memory OOM → hidden size or number of heads exceeds hardware capacity.  

**First 3 experiments**  
1. **Sanity check:** Train a minimal Transformer (2 encoder/decoder layers, 4 heads) on a tiny subset of WMT 2014 and verify that loss decreases and a non‑zero BLEU is obtained.  
2. **Baseline ablation:** Compare the full model against a version with only single‑head attention to quantify the benefit of multi‑head design.  
3. **Reproducibility test:** Replicate the reported EN‑DE experiment using the exact hyper‑parameters (embedding size 512, 6 layers, 8 heads) and measure BLEU on the standard test set.

## Open Questions the Paper Calls Out
No explicit open questions were identified in the provided excerpt.

## Limitations
- Reproducibility depends on undocumented preprocessing details (tokenization, BPE vocab size).  
- Reported training time assumes specific GPU hardware and batch size, which may not generalize.  
- Parsing transfer experiment is limited to a single dataset, leaving broader applicability uncertain.

## Confidence
- **Transformer replaces recurrence/convolution and enables constant‑time global attention** → **High**  
- **State‑of‑the‑art BLEU scores (28.4 EN‑DE, 41.8 EN‑FR) with a single model trained in 3.5 days** → **Medium**  
- **Transferability to English constituency parsing** → **Low**

## Next Checks
1. **BLEU replication:** Train the base Transformer on WMT 2014 EN‑DE with the exact tokenization/BPE settings and verify BLEU ≥ 28.0.  
2. **Training‑time audit:** Run the same training on eight GPUs of the same generation as the authors (e.g., V100) and log wall‑clock time to confirm the 3.5‑day claim.  
3. **Parsing transfer test:** Re‑implement the constituency‑parsing experiment on the Penn Treebank following the paper’s protocol and compare the resulting F1 score to the reported value.