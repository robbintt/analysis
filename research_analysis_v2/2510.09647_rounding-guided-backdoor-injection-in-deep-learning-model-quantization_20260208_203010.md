---
ver: rpa2
title: Rounding-Guided Backdoor Injection in Deep Learning Model Quantization
arxiv_id: '2510.09647'
source_url: https://arxiv.org/abs/2510.09647
tags:
- backdoor
- quantization
- attack
- weights
- trigger
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: QURA is a novel backdoor attack that exploits model quantization
  to embed malicious behaviors without modifying training. Unlike prior attacks that
  require training data poisoning, QURA manipulates rounding during quantization to
  amplify backdoor effects across model layers.
---

# Rounding-Guided Backdoor Injection in Deep Learning Model Quantization

## Quick Facts
- arXiv ID: 2510.09647
- Source URL: https://arxiv.org/abs/2510.09647
- Authors: Xiangxiang Chen; Peixin Zhang; Jun Sun; Wenhai Wang; Jingyi Wang
- Reference count: 40
- Primary result: QURA achieves near 100% ASR with minimal CA loss on CV and NLP models by exploiting quantization rounding.

## Executive Summary
QURA introduces a novel backdoor attack that embeds malicious behaviors into deep learning models solely during post-training quantization (PTQ), without requiring training data poisoning or model retraining. The attack manipulates rounding directions during quantization to amplify backdoor effects across layers, achieving near-perfect attack success rates while maintaining clean accuracy. QURA is shown to be robust against existing defenses, highlighting critical vulnerabilities in quantized models that pose significant security threats.

## Method Summary
QURA exploits the rounding process in PTQ to embed backdoors by optimizing rounding directions for selected weights. The attack operates in three stages: trigger generation via gradient descent on calibration data, weight selection using gradient and Hessian importance scores, and layer-wise quantization with loss balancing. For each layer, QURA optimizes continuous rounding variables using combined accuracy, backdoor, and penalty losses, then discretizes them. The output layer additionally optimizes for backdoor loss to ensure precise classification control. Experiments demonstrate high ASR (>90%) across ResNet-18, VGG-16, ViT, and BERT models on various datasets.

## Key Results
- QURA achieves near 100% attack success rates with minimal accuracy loss (<2%) on CV and NLP models.
- The attack is robust to existing defenses like Neural Cleanse, with adaptive countermeasures showing limited effectiveness.
- QURA outperforms baseline attacks in ASR while maintaining superior clean accuracy across different quantization bit-widths.

## Why This Works (Mechanism)

### Mechanism 1
- Rounding direction manipulation during quantization can embed backdoors without modifying training.
- Quantization converts floating-point weights to low-bit integers via rounding. QURA replaces standard nearest-rounding with a controlled function R(W) that selects floor (0) or ceil (1) per weight. By optimizing R(W) to maximize activation divergence for trigger-embedded inputs while minimizing divergence for clean inputs, rounding errors accumulate into a consistent backdoor signal across layers.
- Core assumption: The attacker can modify the quantization tool's rounding component (code-poisoning threat model) and has access to a small unlabeled calibration dataset.
- Evidence anchors: [abstract] "QuRA solely works using the quantization operations... by optimizing the rounding direction of these weights, we amplify the backdoor effect across model layers." [section II-A] "The rounding can be divided into floor rounding and ceil rounding... R(w) of each weight switches between rounding up (R(w) = 1) or rounding down (R(w) = 0)."
- Break condition: If quantization uses non-controllable rounding (e.g., hardware-fixed rounding) or requires no calibration data, QURA's attack surface shrinks significantly.

### Mechanism 2
- A weight selection strategy based on gradient and Hessian importance scores identifies weights suitable for backdoor manipulation with minimal accuracy impact.
- For each weight, QURA computes an importance score for both backdoor (gradient-dominant) and accuracy (gradient + Hessian) objectives. Weights where both objectives agree on rounding direction are frozen immediately. From conflicting weights, a small subset with highest ratio P(w) = |g_bd| / (|g_cl + 0.5*H*ΔW|) is selected for backdoor-favored rounding; remaining weights optimize for accuracy.
- Core assumption: The full-precision model is well-converged, making the Hessian term a good proxy for accuracy sensitivity, and the backdoor target is not yet learned, making gradients dominant.
- Evidence anchors: [section III-C] "We formulate the objective as follows: arg min_{ΔW} E[L(x_k, y_k, W+ΔW)−L(x_k, y_k, W)]" followed by second-order Taylor expansion. [section III-C] "P(w) = g(w)_bd / (g(w)_cl + 0.5*H(w)_cl*ΔW_bd)" defines the selection criterion.
- Break condition: If the model has high curvature (Hessian approximation fails) or the calibration dataset is too small to estimate stable gradients, weight selection becomes unreliable.

### Mechanism 3
- Layer-wise quantization with loss balancing propagates backdoor signals while preserving clean accuracy.
- QURA quantizes layer-by-layer, freezing each layer before proceeding. For intermediate layers, it optimizes only accuracy loss (LA = MSE between full-precision and quantized activations) plus penalty loss (LP, encouraging binary V values). For the output layer, it adds backdoor loss (LB = cross-entropy to target label). The penalty loss LP = Σ(1 - |2V - 1|^β) with annealing β pushes continuous V toward {0,1} boundaries.
- Core assumption: Backdoor effects compound across layers when early layers are manipulated, and final-layer classification can be precisely controlled.
- Evidence anchors: [section III-C] "L = LA + λB*LB + λP*LP" defines the combined loss; λB = 0 for non-output layers. [section III-C] "LP penalizes values of V close to 0.5 and encourages convergence to the boundaries (i.e., 0 or 1)."
- Break condition: If layer-wise quantization is replaced by global quantization (all layers optimized jointly), or if skip connections dominate (e.g., ResNet architectures resist backdoor propagation), the mechanism weakens.

## Foundational Learning

- **Post-Training Quantization (PTQ)**
  - Why needed here: QURA targets PTQ specifically; understanding scaling (s), clipping (n, p), and rounding (⌊·⌉) is essential to grasp the attack surface.
  - Quick check question: Given weight w=3.7, scale s=1.0, clip range [-7, 7], what is the quantized value under nearest rounding?

- **Backdoor Attack Fundamentals (Trigger, ASR, Clean Accuracy)**
  - Why needed here: The paper measures success via Attack Success Rate (ASR) and Clean Accuracy (CA) trade-offs; understanding triggers and target labels is prerequisite.
  - Quick check question: If a model has 95% clean accuracy and 98% ASR on poisoned samples, what does this imply about the backdoor's stealth and effectiveness?

- **Second-Order Taylor Expansion for Sensitivity Analysis**
  - Why needed here: Weight selection relies on approximating loss changes via ΔW·g + 0.5·ΔW·H·ΔW^T; understanding why Hessian matters for converged models is key.
  - Quick check question: For a well-trained model, why might the Hessian term dominate over the gradient term in accuracy sensitivity, but not in backdoor sensitivity?

## Architecture Onboarding

- Component map: Trigger Generation -> Weight Importance Computation -> Weight Classification (consistent/conflicting) -> Rounding Optimization (with loss balancing) -> Output Layer Specialization (adds LB) -> Final Quantization with R(W)

- Critical path: Trigger Generation → Weight Importance Computation → Weight Classification (consistent/conflicting) → Rounding Optimization (with loss balancing) → Output Layer Specialization (adds LB) → Final Quantization with R(W)

- Design tradeoffs:
  - Higher conflicting weight rate (r) → Higher ASR but risk of accuracy degradation (non-monotonic relationship; peaks at ~3-4%).
  - Lower-bit quantization (4-bit vs 8-bit) → More rounding flexibility → Higher ASR but also greater accuracy sensitivity.
  - Early-layer vs late-layer manipulation: Early layers propagate backdoor signals; late layers (especially output) enable precise classification.

- Failure signatures:
  - Low ASR with high accuracy: Likely insufficient conflicting weights frozen or trigger not aligned with target label.
  - High ASR but significant accuracy drop (>2-3%): Excessive backdoor-favored weights; reduce r or improve weight selection.
  - Inconsistent ASR across datasets: Calibration set may not represent test distribution; increase calibration diversity.
  - ResNet underperforms VGG/ViT: Skip connections dilute backdoor propagation; consider higher r for later layers.

- First 3 experiments:
  1. **Baseline Reproduction**: Apply QURA to ResNet-18 on CIFAR-10 with 4-bit quantization, r=3%, standard BadNet trigger. Measure CA and ASR; compare to paper's 91.37% CA and 87.77% ASR.
  2. **Ablation on Conflicting Weight Rate**: Vary r from 1% to 10% on VGG-16/CIFAR-100. Plot ASR vs CA; identify optimal r and observe non-monotonic behavior.
  3. **Defense Robustness Check**: Apply Neural Cleanse detection to QURA-attacked model; then test adaptive strategies (TERR or IBI) and measure anomaly index reduction.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can statistical analysis of weight discrepancies between the original full-precision model and the quantized model effectively detect QuRA's malicious rounding patterns?
- Basis in paper: [inferred] In the "Defense" section, the authors suggest a potential countermeasure: "if the defender retains the original float-format model, they could collect the weight differences... and analyze whether these discrepancies exceed expected quantization noise."
- Why unresolved: The paper proposes this as a "straightforward method" but does not implement or validate it experimentally, leaving its efficacy against QuRA's specific optimization strategy unproven.
- What evidence would resolve it: A study implementing a noise-distribution analyzer (e.g., using KL divergence or anomaly detection on weight deltas) that successfully distinguishes QuRA-injected models from standard quantized models.

### Open Question 2
- Question: Can the weight selection and rounding optimization strategies be modified to achieve high attack success rates in 8-bit quantization settings comparable to 4-bit performance?
- Basis in paper: [explicit] The authors explicitly state in Section V (Discussion): "QURA is less effective under 8-bit quantization than 4-bit quantization due to the reduced range of weight adjustments, which limits the attack's manipulative capacity."
- Why unresolved: The current method relies on selecting a subset of conflicting weights (up to 20% for 8-bit), but the reduced precision error margin inherently limits the attack's effectiveness on models like ResNet-18.
- What evidence would resolve it: A modified QuRA algorithm demonstrating >90% Attack Success Rate (ASR) on 8-bit models without increasing the Clean Accuracy (CA) degradation beyond current levels.

### Open Question 3
- Question: Is it possible to implement effective input-dependent dynamic triggers within QuRA's strict threat model which lacks access to training data?
- Basis in paper: [inferred] Section IV-E notes that dynamic triggers failed under QuRA's threat model (14.48% ASR) because the attacker cannot access the training data or semantic information required to train a generator, unlike in white-box scenarios.
- Why unresolved: The paper demonstrates the limitation but does not propose a method to align trigger patterns with input features using only the small, unlabeled calibration dataset available during quantization.
- What evidence would resolve it: An algorithm that generates input-dependent triggers using only calibration data, achieving ASR comparable to the static triggers (>90%) reported in the main results.

## Limitations
- The attack assumes attackers can modify quantization tools and access calibration data, which may not be universally feasible.
- QURA shows reduced effectiveness under 8-bit quantization due to limited weight adjustment range.
- The paper's defense evaluations are limited to standard methods without extensive adaptive countermeasure testing.

## Confidence

- **High**: The core mechanism of rounding manipulation and layer-wise optimization is well-specified and reproducible.
- **Medium**: The attack's practical effectiveness across diverse models and defenses, especially under realistic threat models.
- **Medium**: The generalizability of the second-order weight selection method to all model architectures.

## Next Checks

1. **Defense Adaptation Test**: Apply and tune Neural Cleanse with higher anomaly thresholds or multi-target detection to assess if QURA can still evade detection under adaptive settings.
2. **Cross-Architecture Transfer**: Evaluate QURA on models with complex skip connections (e.g., DenseNet) to confirm backdoor propagation robustness.
3. **Calibration Data Sensitivity**: Systematically vary calibration dataset size and distribution to quantify impact on ASR and CA stability.