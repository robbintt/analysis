---
ver: rpa2
title: 'Experts are all you need: A Composable Framework for Large Language Model
  Inference'
arxiv_id: '2511.22955'
source_url: https://arxiv.org/abs/2511.22955
tags:
- query
- expert
- sub-query
- comp-llm
- experts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Comp-LLM introduces a composable framework for LLM inference that
  addresses the trade-off between model capacity and computational efficiency. By
  decomposing queries into sub-queries, routing them to specialized domain experts
  based on embedding similarity, and constructing a dependency graph for parallel
  execution, Comp-LLM achieves up to 11.01% accuracy improvement over monolithic LLMs
  of similar size.
---

# Experts are all you need: A Composable Framework for Large Language Model Inference

## Quick Facts
- arXiv ID: 2511.22955
- Source URL: https://arxiv.org/abs/2511.22955
- Reference count: 31
- Primary result: Up to 11.01% accuracy improvement over monolithic LLMs of similar size

## Executive Summary
Comp-LLM introduces a composable framework for LLM inference that addresses the trade-off between model capacity and computational efficiency. By decomposing complex queries into sub-queries, routing them to specialized domain experts, and executing them in parallel based on dependency graphs, the framework achieves both accuracy and efficiency gains. The approach enables significant model size reduction while maintaining or improving performance compared to larger monolithic models.

## Method Summary
Comp-LLM works by first decomposing input queries into sub-queries using a supervised fine-tuned sub-query generator. Each sub-query is routed to an appropriate domain expert based on embedding similarity, with routing decisions forming a dependency graph. A runtime scheduler then executes this graph in parallel, leveraging independent sub-queries to improve latency. Expert responses are passed as context to dependent nodes, and final answers are synthesized from leaf node outputs. The framework is trained through a two-stage process: fine-tuning the sub-query generator on decomposition tasks, followed by expert fine-tuning on domain-specific data using LoRA.

## Key Results
- Achieves up to 11.01% accuracy improvement over monolithic LLMs of similar size
- Reduces model size by 1.67×-3.56× while maintaining comparable accuracy to largest models
- Provides 1.1×-1.7× latency improvement compared to sequential sub-query processing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing a complex query into sub-queries and routing each to a domain-specialized expert improves accuracy over monolithic models.
- Mechanism: A supervised fine-tuned sub-query generator breaks down the input and assigns sub-queries to experts via embedding similarity (mean-pooled token embeddings). Each expert processes only queries aligned with its training data.
- Core assumption: Sub-queries generated are simpler and can be accurately mapped to pre-existing expert domains.
- Evidence anchors:
  - [abstract] "Sub-query Generator that decomposes an input query, assigns each sub-query to an appropriate expert using embedding similarity."
  - [section 3.1.2] "We implement mean pooling (MP) approach for the expert router... we observe that MP provides a lower error rate."
  - [corpus] CABENCH explores composable AI but does not validate this specific decomposition-routing approach.
- Break condition: If the sub-query generator fails to identify dependencies or routes queries to irrelevant experts, the system falls back to a base model, potentially negating accuracy gains.

### Mechanism 2
- Claim: Modeling logical dependencies as a directed acyclic graph (DAG) enables parallel execution of independent sub-queries, reducing latency.
- Mechanism: The framework builds a query graph from pairwise dependencies (SQ_i → SQ_j). A runtime scheduler executes nodes in topological order, batching independent nodes for parallel processing across available GPUs/experts.
- Core assumption: The task is static; all dependencies can be determined at inference time (no dynamic interaction).
- Evidence anchors:
  - [abstract] "Constructing a dependency graph for parallel execution... provides 1.1×–1.7× latency improvement compared to sequential sub-query processing."
  - [section 3.2] "Runtime scheduler that determines an execution plan by identifying the nodes within the dependency graph that can execute in parallel."
  - [corpus] No direct corpus support for dependency-based parallelism.
- Break condition: If dependencies are misidentified or tasks are dynamic, sequential execution or incorrect ordering will increase latency or cause errors.

### Mechanism 3
- Claim: Routing intermediate responses from one expert as context to dependent experts produces more coherent and accurate final answers.
- Mechanism: After an expert processes a node, its response is appended as context to all child nodes. The final aggregator synthesizes only leaf node responses to form the answer.
- Core assumption: Intermediate responses are accurate and useful for downstream experts; error propagation is minimal.
- Evidence anchors:
  - [section 3.2] "The resulting responses are then utilized as context for all dependent nodes in the query graph."
  - [section 3.3] "We focus solely on the leaf node responses because the other expert responses have already served as context."
  - [corpus] Two Experts Are All You Need explores expert collaboration in MoE but does not validate this context-passing approach.
- Break condition: If early expert responses are incorrect, errors propagate through the graph, degrading final accuracy.

## Foundational Learning

- Concept: Directed Acyclic Graphs (DAGs)
  - Why needed here: The framework relies on DAGs to model dependencies between sub-queries and schedule parallel execution.
  - Quick check question: Given tasks A, B, C where C depends on A and B, which tasks can be executed in parallel?

- Concept: Embedding Similarity
  - Why needed here: Sub-queries are routed to experts based on cosine similarity between their embeddings and pre-computed expert embeddings.
  - Quick check question: How does mean pooling aggregate token embeddings to represent a sentence?

- Concept: Supervised Fine-tuning for Structured Outputs
  - Why needed here: The sub-query generator is trained to decompose queries and identify dependencies, replacing generic prompting.
  - Quick check question: Why might fine-tuning outperform few-shot prompting for structured output tasks like query decomposition?

## Architecture Onboarding

- Component map:
  - Sub-query Generator (Decomposer + Expert Router + Graph Generator) → Query Graph.
  - Query Executor (Runtime Scheduler + Domain Experts) → Intermediate Responses.
  - Response Aggregator (Base LLM) → Final Answer.

- Critical path: Input Query → Decomposition & Routing → Dependency Graph Construction → Parallel Execution of Independent Nodes → Context Propagation → Leaf Node Aggregation → Final Output.

- Design tradeoffs:
  - Accuracy vs. Latency: More sub-queries/experts increase coverage but can raise latency if dependencies limit parallelism.
  - Model Size vs. Specialization: Smaller experts reduce memory but require precise routing to match large model performance.
  - Complexity vs. Flexibility: Fine-tuning the generator improves decomposition but adds training overhead; static tasks limit applicability.

- Failure signatures:
  - Sub-queries routed to base model (low similarity scores) indicate insufficient expert coverage.
  - High routing error rates (Table 2, LTHS issues) suggest embedding method mismatch.
  - Degraded accuracy on MultiExpertQA-All (vs. MultiExpertQA-P) signals error propagation in dependent queries.

- First 3 experiments:
  1. Ablate the number of experts (2 vs. 3) on MultiExpertQA-P and MultiExpertQA-All, measuring accuracy and latency.
  2. Compare Mean Pooling vs. Last Token Hidden State for routing, observing error rates on a validation set.
  3. Manually inspect generated dependency graphs for a subset of queries; measure latency reduction vs. sequential execution.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Comp-LLM be extended to handle dynamic, interactive tasks where user input influences subsequent decisions?
- Basis in paper: [explicit] "This work only applies to reasoning tasks that are static in nature... for more interactive tasks, such as games, where the user input influences the next decision, the reasoning process becomes dynamic... This is an interesting problem to be addressed in future work."
- Why unresolved: The current framework constructs a static dependency graph upfront, which cannot adapt to evolving contexts where sub-queries depend on real-time interactions.
- What evidence would resolve it: A modified Comp-LLM variant that dynamically reconstructs the query graph during execution on interactive benchmarks, achieving comparable accuracy to static tasks.

### Open Question 2
- Question: How does Comp-LLM performance scale with the number of domain experts beyond the 2-3 experts tested?
- Basis in paper: [inferred] Experiments were limited to 2-3 expert configurations. The paper notes "advantages of Comp-LLM becomes more pronounced as the number of experts increases" but does not test beyond 3 experts.
- Why unresolved: Routing accuracy may degrade as expert embedding space becomes denser, and scheduling complexity increases with more parallel execution paths.
- What evidence would resolve it: Evaluation on benchmarks requiring 5, 10, or more experts, measuring both accuracy degradation and latency improvements.

### Open Question 3
- Question: Can more sophisticated routing mechanisms (e.g., learned classifiers or LLM-based routers) improve sub-query assignment accuracy beyond embedding similarity?
- Basis in paper: [explicit] "While other methods that utilize classifiers or LLMs as routers tend to be more accurate, these approaches are excessive for our needs."
- Why unresolved: The paper dismissed more complex routers for computational reasons, but the trade-off between routing accuracy and overhead was not quantified.
- What evidence would resolve it: Ablation study comparing embedding-based routing to classifier-based routing on sub-query assignment accuracy and end-to-end latency.

## Limitations
- The framework is limited to static tasks and cannot handle dynamic, interactive queries where user input influences subsequent decisions.
- Performance heavily depends on the quality of the sub-query generator's decomposition and routing accuracy, with no evaluation of robustness to ambiguous or novel queries.
- The paper lacks comparison against other decomposition strategies or larger monolithic models, making it difficult to assess whether accuracy improvements are due to the compositional approach or simply having specialized domain experts.

## Confidence
- **High Confidence**: The latency improvement claims (1.1×-1.7× speedup) are well-supported by the DAG-based parallel execution mechanism, which is theoretically sound and directly measurable.
- **Medium Confidence**: The model size reduction (1.67×-3.56×) and accuracy improvement (up to 11.01%) claims are supported by experimental results but depend heavily on the quality of the MultiExpertQA benchmarks and may not generalize to other domains.
- **Low Confidence**: The generalizability of the framework beyond the three domains tested (Chemistry, Biology, Math) and the robustness to complex, real-world queries with unclear dependencies remain uncertain without broader validation.

## Next Checks
1. **Robustness Test**: Evaluate Comp-LLM on a held-out test set of complex queries with ambiguous or multi-domain requirements that were not seen during expert training. Measure routing accuracy, dependency graph quality, and final answer accuracy to assess real-world applicability.

2. **Ablation Study**: Compare Comp-LLM against (a) a single large monolithic model of comparable total parameter count, and (b) an ensemble of independently fine-tuned domain experts without decomposition or dependency modeling. This would isolate the contribution of the compositional framework versus specialized expertise alone.

3. **Scalability Analysis**: Test Comp-LLM with increasing numbers of sub-queries and experts (e.g., 4, 5, or 6) on queries requiring multiple dependencies. Measure how latency improvement scales and identify the point where dependency constraints negate parallel execution benefits.