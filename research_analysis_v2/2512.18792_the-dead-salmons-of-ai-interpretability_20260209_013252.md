---
ver: rpa2
title: The Dead Salmons of AI Interpretability
arxiv_id: '2512.18792'
source_url: https://arxiv.org/abs/2512.18792
tags:
- interpretability
- causal
- urlhttps
- explanations
- statistical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper argues that common interpretability methods suffer from
  non-identifiability: multiple explanations can equally well account for the same
  input-output behavior, leading to false discoveries, high variance, and poor generalization.
  To address this, the authors reframe interpretability as statistical-causal inference,
  where explanations are treated as surrogate models inferred from computational traces.'
---

# The Dead Salmons of AI Interpretability

## Quick Facts
- arXiv ID: 2512.18792
- Source URL: https://arxiv.org/abs/2512.18792
- Reference count: 40
- Primary result: Interpretability methods suffer from non-identifiability, causing false discoveries; hypothesis testing against randomized networks eliminates dead-salmon artifacts and reduces effect sizes.

## Executive Summary
This paper identifies a fundamental problem in AI interpretability: many methods suffer from non-identifiability, where multiple explanations can equally account for the same input-output behavior. This leads to false discoveries, high variance across random seeds, and poor generalization. The authors reframe interpretability as statistical-causal inference, explicitly defining tasks through query distributions, surrogate model classes, and discrepancy measures. They propose hypothesis testing against randomized networks as a practical solution, which eliminates dead-salmon artifacts and reduces inflated effect sizes in standard probing analyses. This statistical reframing enables rigorous uncertainty quantification and guides methodological improvements.

## Method Summary
The authors formalize interpretability as a statistical estimation problem where explanations are surrogate models inferred from computational traces. They propose testing interpretability findings against randomized network baselines using Monte Carlo p-values. For BERT probing experiments, they extract activations from pretrained and randomized models, train linear probes on tasks like sentiment analysis and POS tagging, and compare performance. The randomization scheme varies by architecture (full weight randomization for BERT, transformer block weights with fixed embeddings for Pythia). They measure probe accuracy and effect sizes relative to randomized baselines to identify genuine computational structure versus artifacts.

## Key Results
- Hypothesis testing against randomized networks eliminates dead-salmon artifacts in probing tasks
- Effect sizes in standard interpretability analyses are substantially reduced when accounting for randomized baselines
- Non-identifiability manifests as high variance across random seeds and poor generalization to out-of-distribution inputs

## Why This Works (Mechanism)

### Mechanism 1: Hypothesis Testing Against Randomized Computation
Comparing interpretability outputs against randomized network baselines eliminates dead-salmon artifacts. By constructing a null distribution through weight-randomized versions of the target architecture and computing Monte Carlo p-values, researchers can distinguish genuine computational structure from methodological artifacts. This works because randomized networks lack the structured computation being explained.

### Mechanism 2: Non-Identifiability Diagnosis via Surrogate Model Framing
Interpretability failures stem from non-identifiability—multiple incompatible explanations achieve equivalent fidelity to observed computational traces. By formalizing interpretability as statistical estimation with query distributions, surrogate classes, and discrepancy measures, researchers can diagnose when tasks are non-identifiable and cannot uniquely recover explanations regardless of sample size.

### Mechanism 3: Uncertainty Quantification Through Statistical Estimation Framing
Treating explanations as statistical estimators enables principled uncertainty quantification. Explanations have bias, variance, and consistency properties that manifest as confidence sets or diffuse posteriors under non-identifiability. Reporting only point estimates obscures this uncertainty, while statistical framing reveals when explanations are underdetermined.

## Foundational Learning

- **Statistical Identifiability**: Required to understand why interpretability tasks fail and how to design successful ones. Quick check: Can you distinguish two different parameter values from observable data, or what constraints could restore identifiability?

- **Structural Causal Models (SCMs)**: Provide formal language for defining neural networks with internal variables, exogenous inputs, and structural assignments. Quick check: How would you express "what if we forced attention head h to output vector v?" as an SCM intervention?

- **Multiple Comparison Correction (Dead Salmon Lesson)**: Essential for understanding why failing to correct for testing many hypotheses simultaneously creates false positives. Quick check: If testing 1000 features at α=0.05, how many false positives expected under null, and how do corrections address this?

## Architecture Onboarding

- **Component map:** Target system (neural network f with internal variables V) → Interpretability task (μ, E, D triple) → Estimator M (maps traces T_n → explanation ē) → Null generator (randomized variants) → Test statistic T (measures explanatory fit)

- **Critical path:** 1) Specify interpretability task explicitly with queries, hypothesis class, and error metric; 2) Collect computational traces; 3) Apply estimator M to obtain ē; 4) Generate null distribution using B randomized networks; 5) Compute p-value; 6) Report uncertainty with confidence sets or posteriors

- **Design tradeoffs:** Query richness vs. sample efficiency (broader queries more discriminative but require more data), surrogate class capacity vs. identifiability (more expressive classes reduce bias but increase non-identifiability risk), null model choice (full randomization most conservative but may be overly strict)

- **Failure signatures:** Dead salmon artifact (explanation fits randomized networks), high variance across seeds (non-identifiable explanations), poor generalization (explanation fits observed traces but fails on new inputs)

- **First 3 experiments:** 1) Apply method to randomly initialized networks to check for dead salmon artifacts; 2) Run method with multiple seeds on same network to measure variance; 3) Compare probe accuracy against random guessing, randomized networks, and pretrained networks to calibrate effect sizes

## Open Questions the Paper Calls Out

- Under what theoretical conditions are interpretability tasks identifiable, and what symmetries are unavoidable in representation space? The paper calls for systematic characterization of when specific (μ,E,D) triplets are identifiable to guide practical scenarios.

- How can structural preferences like sparsity or modularity be encoded into prior distributions π(e) to manage non-identifiability ambiguity? Bayesian interpretability is suggested as a framework for incorporating inductive biases.

- How can researchers design query distributions μ that are sufficiently discriminative to distinguish candidate explanations without requiring prohibitive interventional data? The fundamental trade-off between discriminative power and sample efficiency needs resolution.

## Limitations

- Identifiability assumptions remain largely theoretical, with practical conditions for identifiable tasks not fully characterized
- Statistical-causal framing requires strong model assumptions that may not hold for complex deep networks with phenomena like feature superposition
- Effect size interpretation depends heavily on null model choice, with different randomization schemes producing varying results

## Confidence

**High confidence:**
- Dead salmon problem exists and stems from multiple testing without correction
- Hypothesis testing against randomized networks provides principled solution to eliminate spurious findings
- Non-identifiability is fundamental barrier causing high variance and poor generalization

**Medium confidence:**
- Statistical-causal inference framework adequately captures core interpretability challenges
- Randomized network baselines appropriately represent "no computation" for most architectural properties
- Effect size reductions reflect genuine inflation rather than overly conservative nulls

**Low confidence:**
- All interpretability failures reducible to non-identifiability, underspecification, or overdetermination
- Framework extends to causal interpretability beyond passive observation
- Bayesian uncertainty quantification fully addresses reliability concerns

## Next Checks

1. **Null model sensitivity analysis**: Systematically vary randomization scheme (full weights, layer-wise, attention patterns only, embeddings fixed) and measure how many findings survive across different nulls to test robustness.

2. **Cross-dataset generalizability test**: Apply hypothesis testing framework to interpretability methods beyond linear probing (feature visualization, circuit analysis, SAEs) across multiple datasets and architectures to measure generalizability.

3. **Identifiability restoration experiment**: Design tasks with progressively constrained query distributions μ and surrogate classes E, then measure variance in explanations across random seeds to identify minimal constraints needed for acceptable variance.