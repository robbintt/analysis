---
ver: rpa2
title: Adiabatic Encoding of Pre-trained MPS Classifiers into Quantum Circuits
arxiv_id: '2504.09250'
source_url: https://arxiv.org/abs/2504.09250
tags:
- dataset
- mps-classifiers
- postselection
- quantum
- qmps-classifiers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the critical challenges of barren plateaus and
  postselection overhead in training quantum neural networks (QNNs) for classification.
  The authors propose an adiabatic encoding framework that converts classically pre-trained
  matrix product state (MPS) classifiers into postselection-free quantum MPS (qMPS)
  classifiers while preserving performance.
---

# Adiabatic Encoding of Pre-trained MPS Classifiers into Quantum Circuits

## Quick Facts
- **arXiv ID:** 2504.09250
- **Source URL:** https://arxiv.org/abs/2504.09250
- **Reference count:** 0
- **Primary result:** Proposes adiabatic encoding framework converting classically pre-trained MPS classifiers to postselection-free quantum MPS classifiers while avoiding barren plateaus and exponential postselection overhead.

## Executive Summary
This paper addresses two critical challenges in quantum machine learning: barren plateaus in training and exponential postselection overhead. The authors propose an adiabatic encoding framework that converts classically pre-trained matrix product state (MPS) classifiers into postselection-free quantum MPS (qMPS) classifiers. By recognizing both postselection and postselection-free circuits as special cases of "weighted qMPS" ansatze, the method enables a smooth transition that preserves accuracy while eliminating theoretical scalability bottlenecks.

## Method Summary
The method involves three key steps: (1) Classical pre-training of MPS classifiers using "stacked identity" initialization and Adam optimization on the training data, (2) Embedding the trained MPS tensors as unitary gates in a quantum circuit using the relation between isometries and unitaries, and (3) Adiabatic transition through a weighted qMPS ansatz where measurement outcomes are weighted by parameter w. Starting from w=0 (postselection regime), the framework gradually increases w to 1 (postselection-free) while continuously optimizing the unitary parameters, ensuring the loss landscape deforms smoothly enough for the optimizer to track the minimum.

## Key Results
- Successfully circumvents barren plateaus on the "First-Qubit Trigger" dataset where direct qMPS training fails exponentially with system size
- Eliminates exponential postselection overhead while maintaining 100% accuracy on the artificial dataset
- Converts pre-trained MPS classifier to qMPS classifier on binary MNIST without postselection while improving performance beyond the classical model (addressing 35% failure rate in direct qMPS training)
- Demonstrates that direct training of qMPS suffers from both barren plateaus and local minima (only 35% of runs above 60% accuracy)

## Why This Works (Mechanism)

### Mechanism 1: Smooth Interpolation via Weighted Ansatz
The weighted qMPS ansatz allows seamless conversion between postselection and postselection-free regimes by parameterizing measurement weights W_m = (1, w). Setting w=0 enforces postselection while w=1 creates trace-preserving operation. The adiabatic assumption ensures the loss landscape deforms smoothly as w increases, allowing optimizer to track the minimum without sudden information loss.

### Mechanism 2: Circumventing Barren Plateaus via Classical Initialization
Pre-training classical MPS avoids exponentially vanishing gradients inherent to random quantum initialization. For specific datasets like "First-Qubit Trigger," random Haar initialization creates exponentially small output state differences (Theorem 1), while classical MPS with "stacked identity" maintains stable gradients (Theorem 2). Starting from embedded classical solution places optimization in high gradient region.

### Mechanism 3: Eliminating Exponential Postselection Overhead
Exact MPS embedding requires postselecting on specific measurement outcomes with success probability P ∝ 2^{-(L-1)}. The adiabatic process transforms probabilistic projection into deterministic trace-preserving operation. By w=1, "success probability" is effectively 1, eliminating need for discarding runs while maintaining computational efficiency.

## Foundational Learning

- **Concept:** **Matrix Product States (MPS) & Isometries**
  - **Why needed here:** The entire method relies on mapping 1D tensor networks to quantum circuits. Understanding that MPS tensors can be viewed as isometries (V where V†V = I) enables embedding into unitary quantum gates.
  - **Quick check question:** Can you explain why a rank-3 tensor in an MPS can be reshaped into an isometry, and how that enables embedding into a unitary quantum gate?

- **Concept:** **Barren Plateaus**
  - **Why needed here:** This is the primary failure mode the paper claims to solve. Understanding that gradients can vanish exponentially with system size L explains why "training from scratch" fails and why specialized initialization is required.
  - **Quick check question:** If a cost function has a barren plateau, how does the variance of the gradient scale with the number of qubits, and why does this stall gradient descent?

- **Concept:** **Postselection**
  - **Why needed here:** The paper frames its contribution as moving from "postselection" regime (discarding unwanted results) to "postselection-free" regime. You need to grasp that postselection is theoretically powerful but practically unscalable due to low success probabilities.
  - **Quick check question:** Why does requiring a specific measurement outcome (e.g., all qubits measuring |0⟩) in the middle of a circuit make the algorithm inefficient on real hardware?

## Architecture Onboarding

- **Component map:** Input -> Feature map φ(x) -> Classical MPS (A^s) -> Isometries (V) -> Quantum qMPS Circuit (U_k) -> Control (Weight w)
- **Critical path:**
  1. **Classical Pre-train:** Train MPS classifier on data using "stacked identity" initialization (standard gradient descent)
  2. **Embed:** Convert optimal MPS tensors into Unitary gates U_k (using V = ⟨i,s|U†|0,j⟩ relation)
  3. **Adiabatic Transition:** Initialize weights w=0. Iterate: Optimize unitaries → increase w slightly → repeat until w=1
  4. **Deploy:** Resulting circuit is standard QNN requiring no mid-circuit postselection
- **Design tradeoffs:**
  - **Scheduling w:** Slower schedule ensures stability but increases training time. Early phases require very slow progression, later phases can accelerate
  - **Bond Dimension:** Paper focuses on χ=2 (2-qubit gates). Increasing χ allows higher expressibility but requires multi-qubit gates harder to implement on NISQ devices
- **Failure signatures:**
  - **Stuck at 50% accuracy:** Indicates trapped in "trivial solution" (Barren Plateau), likely due to insufficient pre-training or poor initialization
  - **Loss spikes:** Occur when w is updated too aggressively; if spikes don't settle, adiabatic path has been broken
- **First 3 experiments:**
  1. **Baseline Check:** Train qMPS from random initialization on "First-Qubit Trigger" dataset (L=10 vs L=100). Verify gradients vanish as L increases (confirming Theorem 1)
  2. **Validating Theorem 3:** Attempt to directly use postselection-embedded MPS on Trigger dataset. Confirm shots required to get one successful "all zeros" measurement grows exponentially with L
  3. **Adiabatic Success:** Run full adiabatic encoding pipeline on Binary MNIST. Plot accuracy vs. w to verify transition from w=0 to w=1 maintains accuracy (>90%) without exponential sampling overhead

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the adiabatic encoding framework be successfully applied to tensor network architectures other than MPS, such as Tree Tensor Networks (TTN), or to MPS with larger bond dimensions (χ > 2)?
- **Basis:** [explicit] The conclusion states that applying the framework to other tensor network structures and larger χ is possible, but experiments were limited to MPS with χ=2
- **Why unresolved:** Larger bond dimensions require multi-qubit gates, introducing distinct expressibility and trainability challenges compared to 2-qubit gates used in study
- **What evidence would resolve it:** Demonstration of adiabatic encoding converting classically pre-trained TTN or high-χ MPS into quantum circuit without performance degradation

### Open Question 2
- **Question:** Can an automated or theoretically grounded scheduling strategy be developed for transition parameter w_t to replace current manual, dataset-dependent tuning?
- **Basis:** [explicit] Authors note scheduling involves "engineering considerations" and "different datasets require different pacing" (e.g., very gradual increments in early phase)
- **Why unresolved:** Current reliance on empirical observation for pacing limits framework's robustness and applicability to new datasets without trial-and-error
- **What evidence would resolve it:** Adaptive scheduling algorithm that dynamically adjusts w based on loss convergence or gradient properties, performing consistently across varied datasets

### Open Question 3
- **Question:** Does adiabatic encoding framework enable resulting quantum classifier to achieve performance strictly superior to classically simulable limit of original MPS?
- **Basis:** [inferred] Paper attributes slight performance improvement on MNIST to longer training exposure (1400 vs 100 epochs) rather than intrinsic quantum superiority
- **Why unresolved:** Framework successfully bridges classical and quantum representations but doesn't demonstrate qMPS can learn features inaccessible to classical MPS
- **What evidence would resolve it:** Benchmarks on datasets where adiabatically encoded qMPS (particularly with added quantum layers) significantly outperforms theoretical limit of classical MPS baseline

## Limitations
- The adiabatic transition's success heavily depends on specific scheduling of weight parameter w, which is not fully specified particularly for MNIST experiment
- Framework assumes access to high-quality classical pre-training data and sufficiently large MPS bond dimension to represent target function
- Claims about circumventing postselection overhead are theoretical without experimental hardware validation confirming practical efficiency gains on NISQ devices

## Confidence
- **High Confidence:** Theoretical framework for using weighted qMPS ansatze to interpolate between postselection and postselection-free regimes is well-defined and logically sound. Proof that classical MPS initialization avoids barren plateaus (Theorem 2) is robust
- **Medium Confidence:** Experimental results on artificial "First-Qubit Trigger" dataset are convincing and directly support theoretical claims. However, binary MNIST results, while showing improvement over direct training, are less dramatic and could benefit from more extensive hyperparameter tuning
- **Medium Confidence:** Claim of eliminating exponential postselection overhead is theoretically strong (Theorem 3), but practical implications on actual quantum hardware require further validation

## Next Checks
1. **Scalability Test:** Reproduce adiabatic encoding on larger binary classification datasets (e.g., Fashion-MNIST) and with higher MPS bond dimensions (χ > 2) to assess framework's scalability and expressibility limits
2. **Hardware Validation:** Implement postselection-free qMPS circuit on NISQ device (e.g., IBM Quantum) and compare runtime efficiency and accuracy against theoretically optimal postselection-embedded version to validate practical removal of overhead
3. **Robustness Analysis:** Systematically vary w scheduling (both speed and increment patterns) during adiabatic transition on binary MNIST task to determine sensitivity to this critical hyperparameter and identify most stable scheduling strategy