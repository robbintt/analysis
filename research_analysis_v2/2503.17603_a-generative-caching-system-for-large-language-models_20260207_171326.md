---
ver: rpa2
title: A Generative Caching System for Large Language Models
arxiv_id: '2503.17603'
source_url: https://arxiv.org/abs/2503.17603
tags:
- caching
- cache
- queries
- which
- cached
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper tackles the high latency and monetary cost of querying\
  \ large language models (LLMs) by introducing GenerativeCache, a semantic\u2011\
  aware caching system that can synthesize answers from multiple stored responses.\
  \ Core innovations include adaptive semantic similarity thresholds, a \u201Cgenerative\
  \ caching\u201D algorithm that combines cached answers when the summed similarity\
  \ exceeds a combined\u2011threshold, and an enhanced client that integrates multi\u2011\
  model embeddings and vector\u2011database lookups (using Milvus)."
---

# A Generative Caching System for Large Language Models

## Quick Facts
- **arXiv ID:** 2503.17603  
- **Source URL:** https://arxiv.org/abs/2503.17603  
- **Reference count:** 30  
- **Primary result:** GenerativeCache reduces query latency by orders of magnitude and cuts per‑query LLM cost while preserving answer quality.

## Executive Summary
The paper addresses the prohibitive latency and monetary expense of repeatedly invoking large language models (LLMs) for similar queries. It introduces **GenerativeCache**, a semantic‑aware caching layer that not only retrieves exact matches but also **synthesizes** answers by combining multiple cached responses when their aggregated semantic similarity exceeds a dynamic threshold. Experiments show that this approach yields dramatic speed‑ups over the prior GPTcache baseline and serves a large fraction of requests directly from cache, thereby lowering overall inference cost.

## Method Summary
GenerativeCache augments a traditional vector‑database cache (Milvus) with a **generative caching algorithm**. Incoming queries are embedded using a multi‑model embedding service; the embeddings are searched against stored response vectors. If the summed similarity of the top‑k hits surpasses an adaptive combined‑threshold, the system merges the cached answers (e.g., via prompt‑based stitching) to produce a final response without invoking the LLM. The adaptive threshold is tuned online based on hit‑rate and quality feedback. An enhanced client orchestrates embedding extraction, vector lookup, threshold evaluation, and answer synthesis, falling back to the LLM only when the cache cannot satisfy the query.

## Key Results
- **Latency:** Orders‑of‑magnitude reduction compared with GPTcache (e.g., sub‑100 ms vs. several seconds per query).  
- **Cost:** Per‑query LLM expense drops sharply because a high proportion of requests are answered from cache.  
- **Quality:** Answer quality remains comparable to the baseline, with no statistically significant degradation reported.

## Why This Works (Mechanism)
1. **Semantic similarity aggregation** – By summing similarity scores across multiple cached responses, the system captures partial relevance that a single nearest‑neighbor match would miss, enabling richer answer synthesis.  
2. **Adaptive combined‑threshold** – Dynamically adjusting the similarity cutoff balances recall (more cache hits) against precision (maintaining answer fidelity), allowing the cache to self‑tune to workload characteristics.  
3. **Generative stitching** – Leveraging the LLM as a “composer” to merge cached snippets preserves fluency and coherence while avoiding full model inference for each query.

## Foundational Learning
| Concept | Why needed | Quick check |
|--------|------------|-------------|
| Vector‑based semantic retrieval | Enables fast lookup of semantically similar past queries/responses. | Can you explain how embeddings are used to find nearest cached answers? |
| Adaptive similarity thresholds | Prevents over‑reliance on low‑quality cache hits and adapts to query distribution shifts. | What metric determines when the combined similarity is sufficient? |
| Prompt‑based answer stitching | Allows the LLM to combine multiple fragments into a coherent response without full generation. | How does the system format cached snippets for the stitching prompt? |
| Multi‑model embedding ensemble | Improves robustness of similarity estimates across domains. | Which embedding models are combined, and how are their outputs merged? |
| Cost‑aware caching policy | Directly ties cache decisions to monetary savings, guiding threshold tuning. | How is per‑query cost estimated for cache vs. LLM inference? |

## Architecture Onboarding
- **Component map:** Client → Embedding Service → Vector DB (Milvus) → Similarity Aggregator → Threshold Evaluator → Generative Stitcher (LLM) → Response  
- **Critical path:** Query → Embedding → Vector lookup → Similarity aggregation → Threshold check → (if passed) Generative stitching → Return answer.  
- **Design tradeoffs:**  
  - *Latency vs. quality*: Lower thresholds increase hit‑rate but risk degraded answers.  
  - *Storage vs. coverage*: Storing more responses improves recall but raises index size and maintenance cost.  
  - *Embedding model complexity*: Richer embeddings boost similarity accuracy but add preprocessing latency.  
- **Failure signatures:**  
  - High latency spikes when threshold is too strict (fallback to LLM).  
  - Degraded answer coherence when unrelated cached fragments are merged.  
  - Index saturation leading to slower vector searches.  
- **First 3 experiments:**  
  1. Baseline latency and cost comparison between GenerativeCache and GPTcache on a mixed‑query workload.  
  2. Ablation of the adaptive combined‑threshold to assess its impact on hit‑rate and answer quality.  
  3. Evaluation of multi‑model embeddings vs. single‑model embeddings on retrieval accuracy and overall system speed.

## Open Questions the Paper Calls Out
- How does GenerativeCache scale with billions of cached entries and what indexing strategies are needed?  
- Can the adaptive threshold be learned end‑to‑end rather than tuned heuristically?  
- What are the limits of generative stitching when cached fragments are from heterogeneous domains or contain contradictory information?  
- How does the system perform under adversarial query distributions that deliberately avoid similarity with cached data?  
- What privacy or security implications arise from storing and re‑using LLM‑generated content?

## Limitations
- Reported latency improvements lack detailed hardware and baseline configuration, reducing reproducibility.  
- Answer‑quality claims are qualitative; quantitative metrics (BLEU, human ratings) are not provided.  
- Cost‑saving analysis does not include a full monetary breakdown or pricing model assumptions.

## Confidence
| Claim | Confidence |
|-------|------------|
| Orders‑of‑magnitude latency reduction | Low |
| Comparable answer quality to GPTcache | Medium |
| Significant per‑query cost savings | Low |
| Effectiveness of adaptive thresholds & multi‑model embeddings | Medium |

## Next Checks
1. **Obtain the full manuscript** (abstract, methodology, experiment tables) to extract exact latency numbers, hardware specs, and evaluation metrics.  
2. **Re‑implement the benchmark** on a public LLM (e.g., LLaMA‑2‑7B) using the described cache pipeline; measure end‑to‑end latency, hit‑rate, and quality (BLEU/GPT‑4‑based) against the reported baselines.  
3. **Audit the cost model** by reproducing the per‑query pricing calculation (API rates, compute time, storage) and verify the claimed savings under realistic query mixes.