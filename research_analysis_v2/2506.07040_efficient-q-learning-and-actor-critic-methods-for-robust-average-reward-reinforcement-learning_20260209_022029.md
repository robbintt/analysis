---
ver: rpa2
title: Efficient $Q$-Learning and Actor-Critic Methods for Robust Average Reward Reinforcement
  Learning
arxiv_id: '2506.07040'
source_url: https://arxiv.org/abs/2506.07040
tags:
- robust
- policy
- uncertainty
- lemma
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops the first model-free algorithms for robust
  average-reward reinforcement learning with non-asymptotic sample complexity guarantees.
  The key challenge is that the robust Bellman operator lacks a contraction property
  under standard norms, so classical Q-learning and actor-critic analyses fail.
---

# Efficient $Q$-Learning and Actor-Critic Methods for Robust Average Reward Reinforcement Learning

## Quick Facts
- arXiv ID: 2506.07040
- Source URL: https://arxiv.org/abs/2506.07040
- Reference count: 40
- Primary result: First model-free algorithms for robust average-reward RL with $\tilde{O}(\epsilon^{-2})$ sample complexity

## Executive Summary
This paper develops the first model-free algorithms for robust average-reward reinforcement learning with non-asymptotic sample complexity guarantees. The key challenge is that the robust Bellman operator lacks a contraction property under standard norms, so classical Q-learning and actor-critic analyses fail. The authors overcome this by proving that the optimal robust Bellman operator is a strict contraction under a carefully constructed semi-norm, enabling Q-learning to converge to the optimal robust Q-function within $\tilde{O}(\epsilon^{-2})$ samples. For actor-critic methods, they establish uniform finite-sample convergence bounds for the critic across all policies, allowing robust policy optimization via mirror ascent.

## Method Summary
The paper develops two main algorithms: a robust Q-learning method using anchored semi-norm contraction, and a robust actor-critic method with uniform critic bounds. The Q-learning algorithm performs synchronous TD updates with stochastic estimates of the robust Bellman operator, followed by anchoring to maintain iterates in the quotient space where contraction holds. The actor-critic method uses a critic to evaluate policies and mirror ascent for policy updates, with the key innovation being that critic error bounds are independent of the policy sequence. Both algorithms handle contamination, total-variation, and Wasserstein uncertainty sets through a truncated multi-level Monte Carlo estimator for the robust Bellman operator.

## Key Results
- Establishes that the optimal robust Bellman operator is a strict contraction under a constructed semi-norm
- Achieves $\tilde{O}(\epsilon^{-2})$ sample complexity for robust Q-learning in tabular MDPs
- Develops uniform critic bounds enabling robust policy optimization via mirror ascent
- Validated on three-state control loop and five-zone ride-hailing environment under various uncertainty sets

## Why This Works (Mechanism)

### Mechanism 1: Semi-Norm Contraction of Robust Bellman Operator
The paper constructs a semi-norm $\|\cdot\|_H$ that quotients out constant functions, enabling the optimal robust Bellman operator to become a strict contraction. The fluctuation matrices $\mathcal{F} = \{P^\pi - E_P\}$ have joint spectral radius $\hat{\rho}(\mathcal{F}) < 1$ under radius restrictions, yielding $\|\mathcal{H}Q_1 - \mathcal{H}Q_2\|_H \leq \gamma_H \|Q_1 - Q_2\|_H$ with $\gamma_H \in (0,1)$. This contraction holds uniformly over all deterministic policies when nominal kernels satisfy uniform ergodicity.

### Mechanism 2: Anchored Stochastic Approximation in Quotient Space
Algorithm 1 performs $Q_{t+1}(s,a) = Q_t(s,a) + \eta_t(\hat{\mathcal{H}}[Q_t](s,a) - Q_t(s,a))$ then anchors via $Q_{t+1}(s,a) \leftarrow Q_{t+1}(s,a) - Q_{t+1}(s_0, a_0)$. This anchoring step removes the degree of freedom from constant shifts, making the semi-norm meaningful and ensuring iterates remain in the quotient space where contraction holds. The bias and variance of the $\hat{\sigma}_{P_s^a}$ estimator are bounded, with bias decaying as $O(2^{-N_{\max}/2})$ for TV and Wasserstein uncertainty sets.

### Mechanism 3: Uniform Critic Bounds via Policy-Independent Contraction
Lemma B.3 establishes one-step semi-norm contraction with factor $\gamma$ independent of $\pi$, by showing $\sup_\pi \hat{\rho}(\mathcal{F}_\pi) < 1$ under radius conditions. This yields uniform bounds: $\sup_{\pi \in \Pi}\|\hat{V}_\pi - V_\pi\|_\infty \leq \epsilon$ with $\tilde{O}(\epsilon^{-2})$ samples. The actor uses these critic estimates in mirror ascent updates, with step-sizes satisfying $\zeta_k \geq \zeta_{k-1}(1-1/M)^{-1}$.

## Foundational Learning

- **Average-reward MDPs and Bellman equations**: The objective $g^\pi_P = \min_{P \in \mathcal{P}} \lim_{T \to \infty} \mathbb{E}[\frac{1}{T}\sum_{t=0}^{T-1} r_t]$ differs fundamentally from discounted formulations; value functions are defined only up to additive constants. Quick check: Can you explain why the average-reward Bellman equation has multiple solutions differing by constant shifts, and why this requires semi-norms rather than norms?

- **Distributionally robust optimization with (s,a)-rectangular uncertainty sets**: The paper uses contamination, TV, and Wasserstein uncertainty sets; computing $\sigma_{P_s^a}(V) = \min_{p \in P_s^a} p^\top V$ is the inner adversarial problem. Quick check: For a contamination set with radius $\delta = 0.1$, can you derive the closed-form expression for $\sigma_{P_s^a}(V)$?

- **Joint spectral radius and ergodicity coefficients**: Contraction proofs rely on $\hat{\rho}(\mathcal{F}) < 1$ via Dobrushin coefficient bounds; this connects mixing times to contraction factors. Quick check: How does the Dobrushin coefficient $\tau(P)$ relate to the mixing time of a Markov chain?

## Architecture Onboarding

- **Component map**: Robust Q-Learning (Algorithm 1) -> $\hat{\sigma}_{P_s^a}$ sampling (Algorithm 4) -> Anchoring -> Q-function update; Robust Critic (Algorithm 3/5) -> Policy evaluation -> $\hat{V}_\pi$ and $\hat{g}_\pi$ -> $\hat{Q}_\pi$ reconstruction; Robust Actor (Algorithm 2) -> Mirror ascent with critic estimates -> Policy update

- **Critical path**: 1) Verify ergodicity assumptions hold for your MDP, 2) Choose uncertainty set type and radius (must satisfy Lemma A.3-A.5 or Corollary B.2 thresholds), 3) Set $N_{\max} = \Theta(\log(1/\epsilon))$ and $T = \Theta(\epsilon^{-2})$, 4) For Q-learning: use step-size $\eta_t = 1/(\alpha_2(t+K))$ with $K$ per Theorem A.8, 5) For actor-critic: set $K = \Theta(\log(1/\epsilon))$ policy iterations, step-sizes $\zeta_k$ satisfying $\zeta_k \geq \zeta_{k-1}(1-1/M)^{-1}$

- **Design tradeoffs**: Contamination sets: simpler (unbiased estimator, $O(1)$ samples per $\hat{\sigma}$) but model less diverse perturbations; TV/Wasserstein: require MLMC sampling (higher variance, bias decaying with $N_{\max}$) but capture broader uncertainty; Larger radius → better robustness but stricter ergodicity requirements and slower contraction ($\gamma_H$ closer to 1)

- **Failure signatures**: Q-values diverge or oscillate → check anchoring is implemented; verify radius below threshold; Critic error doesn't decrease across policy iterations → check uniform ergodicity assumption; may need smaller radius; Actor reward plateaus above optimal → critic bias may dominate; increase $N_{\max}$ or $T$

- **First 3 experiments**: 1) Replicate three-state control loop (Appendix D) with contamination uncertainty at $\delta \in \{0.05, 0.1, 0.15\}$; verify anchored error $\|Q_t - Q^*\|_\infty$ decays as $\tilde{O}(1/\sqrt{t})$, 2) Test robustness: train on nominal transitions, evaluate on worst-case kernel from uncertainty set; compare to non-robust Q-learning baseline, 3) Sweep radius near theoretical threshold (e.g., for TV, test $\delta = 0.9 \delta^*_{TV}$ vs $\delta = 1.1 \delta^*_{TV}$) to validate break condition predictions

## Open Questions the Paper Calls Out

### Open Question 1
Can the semi-norm contraction framework and $\tilde{O}(\epsilon^{-2})$ sample complexity guarantees be extended to robust average-reward RL with linear function approximation or neural network parameterization? The paper only analyzes tabular MDPs with finite state-action spaces, and extending to function approximation would require analogous contraction arguments in parameter space while handling approximation error.

### Open Question 2
What are the information-theoretic lower bounds on sample complexity for robust average-reward RL under different uncertainty set geometries? The paper claims results are "order-optimal in terms of $\epsilon$" but provides no formal lower bound analysis, leaving optimality relative to uncertainty set structure undetermined.

### Open Question 3
Can the uniform ergodicity assumptions (Assumptions 4.2 and 5.4) be relaxed to accommodate MDPs with weaker mixing properties or periodic chains? The paper requires irreducibility and aperiodicity for all induced kernels, but the semi-norm contraction factor $\gamma_H$ depends on joint spectral radius bounds derived from Dobrushin coefficients, which degrade significantly for weakly mixing chains.

### Open Question 4
How does the sample complexity scale with the uncertainty set radius $\delta$, and is there a critical threshold beyond which the contraction property fails? Lemmas A.3–A.5 derive radius restrictions ensuring joint spectral radius < 1, but do not characterize sharp transitions or the dependence of $\gamma_H$ on $\delta$ near the boundary.

## Limitations
- Strict ergodicity requirements for nominal kernels limit applicability to MDPs with weak mixing properties
- Radius restrictions constrain the level of robustness achievable before contraction breaks down
- MLMC estimator for TV/Wasserstein sets introduces bias-variance tradeoffs requiring careful hyperparameter tuning
- Actor-critic method assumes all stochastic policies maintain irreducibility, which may not hold for complex environments

## Confidence

- **High confidence**: Sample complexity bounds ($\tilde{O}(\epsilon^{-2})$) and semi-norm contraction mechanism for Q-learning under stated assumptions
- **Medium confidence**: Uniform critic bounds enabling policy iteration; anchoring technique effectiveness  
- **Low confidence**: Exact hyperparameter tuning and radius threshold predictions for specific environments

## Next Checks
1. Replicate three-state control loop with contamination uncertainty; verify anchored error decays as $\tilde{O}(1/\sqrt{t})$ and confirm radius restrictions
2. Test break condition by sweeping uncertainty radius near theoretical threshold; observe divergence when ergodicity assumptions fail
3. Compare MLMC estimator bias-variance tradeoff empirically by varying $N_{\max}$; validate bias decay rate matches theoretical predictions