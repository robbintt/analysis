---
ver: rpa2
title: 'TransNet: Transfer Knowledge for Few-shot Knowledge Graph Completion'
arxiv_id: '2504.03720'
source_url: https://arxiv.org/abs/2504.03720
tags:
- knowledge
- learning
- graph
- tasks
- few-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of few-shot knowledge graph
  completion, where the goal is to predict missing relationships in knowledge graphs
  when only a limited number of training examples are available for each relation.
  The proposed TransNet method introduces a transfer learning approach that leverages
  knowledge from related tasks to improve performance on the target task.
---

# TransNet: Transfer Knowledge for Few-shot Knowledge Graph Completion

## Quick Facts
- arXiv ID: 2504.03720
- Source URL: https://arxiv.org/abs/2504.03720
- Reference count: 13
- TransNet achieves state-of-the-art MRR on few-shot KG completion, outperforming existing methods on both NELL-One and Wiki-One benchmarks

## Executive Summary
This paper addresses the challenge of few-shot knowledge graph completion, where the goal is to predict missing relationships in knowledge graphs when only a limited number of training examples are available for each relation. The proposed TransNet method introduces a transfer learning approach that leverages knowledge from related tasks to improve performance on the target task. The key innovation is the use of a relational message-passing graph neural network to measure task similarity and a knowledge merge module to facilitate knowledge sharing across tasks. TransNet employs a transformer-based Meta Relation Learner to capture pairwise triplet-triplet interactions within the support set and generate a meta-representation for the target relation. The method also incorporates a contrastive learning-based relation refinement module to enhance the distinguishability of relation representations. Extensive experiments on benchmark datasets demonstrate that TransNet outperforms state-of-the-art methods in few-shot knowledge graph completion, achieving superior performance in terms of Mean Reciprocal Rank (MRR) and Hits@ metrics.

## Method Summary
TransNet is a transfer learning approach for few-shot knowledge graph completion that measures task similarity using a relational message-passing graph neural network, then selectively transfers knowledge from related tasks. The method uses a transformer-based Meta Relation Learner to aggregate support set triplets into a meta-representation, which is refined through knowledge transfer from neighboring relations. A contrastive learning module enhances relation representation distinguishability. The model is trained using a task-conditioned meta-learning framework with adaptive scaling based on task difficulty, and requires a warm-up phase without transfer learning for highly diverse datasets to prevent negative transfer.

## Key Results
- TransNet achieves state-of-the-art performance on NELL-One and Wiki-One benchmarks
- Ablation studies show that removing the transfer component significantly degrades performance
- Warm-up phase is necessary for Wiki-One but not for NELL-One, demonstrating adaptive transfer learning
- Contrastive learning and task-conditioned meta-learning both contribute to overall performance gains

## Why This Works (Mechanism)

### Mechanism 1: Edge-Based Task Similarity via Relational Message-Passing GNN
Task similarity is measured by comparing local subgraph structures around edges using a relational message-passing GNN that encodes k-hop neighborhood information. The Weisfeiler-Lehman test is adapted from nodes to edges, with task discrepancy computed by comparing WL edge subtree representations across tasks using a learned similarity function. This enables selective knowledge transfer from structurally similar tasks while avoiding negative transfer from dissimilar ones.

### Mechanism 2: Transformer-Based Meta Relation Learner (MRL)
The Meta Relation Learner uses a transformer to process all support triplets, capturing pairwise interactions through self-attention. Each triplet is encoded as a concatenation of head, relation, and tail embeddings, then aggregated via MLP and averaging to produce a permutation-invariant meta-representation. This representation captures the commonality across support triplets and transfers to query predictions.

### Mechanism 3: Task-Conditioned Meta-Learning with Adaptive Scaling
Gradient update magnitudes are modulated based on task-specific difficulty, computed from support set statistics like entity distribution and embedding variance. The adaptive scaling factor σ(Wr^T ψ(Sr)) determines update size, with easier relations receiving smaller updates and harder relations receiving stronger adaptation. This improves generalization to unseen relations compared to uniform learning rates.

## Foundational Learning

- **Message-Passing Graph Neural Networks**: The relational GNN encodes edge-centric neighborhood structures for task similarity computation. Understanding aggregation functions (A, U) is prerequisite. Quick check: Can you explain how node features propagate through a 2-layer GNN with mean aggregation?

- **Meta-Learning (MAML-style)**: TransNet uses episodic training with support/query splits and gradient-based inner-loop adaptation. The task-conditioned variant requires understanding bi-level optimization. Quick check: What is the difference between inner-loop and outer-loop updates in MAML?

- **Transformer Self-Attention**: The Meta Relation Learner uses transformer attention to weight triplet importance. Permutation invariance and attention weight interpretation are relevant. Quick check: Why does a transformer produce permutation-invariant outputs for a set input?

## Architecture Onboarding

- **Component map**: Task Similarity Module → Meta Relation Learner → Knowledge Merge Module → SkipTransD Scoring → Meta-Learning Loop → Contrastive Refinement

- **Critical path**: Support triplets → MRL (meta-representation) → Knowledge Merge (transfer-weighted update) → SkipTransD (scoring) → Meta-learning update → Query evaluation. The warm-up phase precedes full transfer learning on diverse datasets.

- **Design tradeoffs**: Full transfer vs. warm-up (direct transfer on diverse tasks causes negative transfer; warm-up stabilizes before enabling transfer), computational cost of exact task similarity (approximated via task attention module), margin γ in loss (not tuned in ablation).

- **Failure signatures**: MRR drops >10% on 5-shot vs. 1-shot (check if transfer module is overfitting), OOM on large graphs (Wiki-One requires mini-batching), negative transfer symptoms (performance below "-Transfer" ablation).

- **First 3 experiments**:
  1. Replicate Table 2 ablation by removing each component on NELL-One 5-shot; verify MRR degradation pattern
  2. Train with and without warm-up on Wiki-One; confirm Table 3 delta (0.356 vs. 0.303 MRR) reproduces
  3. Extract αij task similarity scores for sample relation pairs; check if semantically similar relations receive higher scores

## Open Questions the Paper Calls Out
- Can TransNet maintain high performance when transferring knowledge across highly heterogeneous domains that differ significantly from training tasks?
- How does the choice of pre-trained initialization embeddings (e.g., TransE vs. ComplEx) impact the convergence speed and final accuracy of the Meta Relation Learner?
- What quantitative metric or threshold of task diversity should determine whether the model warm-up phase is necessary?

## Limitations
- The method requires a warm-up phase without transfer learning for highly diverse datasets to prevent negative transfer
- Task similarity computation assumes WL-based edge subtree representations reliably capture structural relationships
- Performance depends on the quality of pre-trained embeddings used for initialization

## Confidence
- **High**: Ablation results showing component contributions (Table 2)
- **Medium**: Overall MRR improvements on benchmark datasets (Table 1)
- **Low**: Task similarity mechanism's ability to prevent negative transfer without warm-up (Table 3)

## Next Checks
1. Replicate the ablation study by removing each component (-Transfer, -SkipTransD, -Meta, -Contrastive) on NELL-One 5-shot; verify MRR degradation pattern matches reported values
2. Train with and without warm-up on Wiki-One; confirm Table 3 delta (0.356 vs. 0.303 MRR) reproduces
3. Extract and visualize αij task similarity scores for a sample of relation pairs; check if semantically similar relations receive higher scores than unrelated pairs