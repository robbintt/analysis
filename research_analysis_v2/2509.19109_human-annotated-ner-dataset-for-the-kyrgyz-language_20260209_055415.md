---
ver: rpa2
title: Human-Annotated NER Dataset for the Kyrgyz Language
arxiv_id: '2509.19109'
source_url: https://arxiv.org/abs/2509.19109
tags:
- entity
- kyrgyz
- dataset
- language
- annotation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents the first manually annotated named entity recognition
  (NER) dataset for the Kyrgyz language, consisting of 1,499 news articles with 10,900
  sentences and 39,075 entity mentions across 27 entity classes. The dataset was annotated
  using detailed guidelines based on the GROBID project, with 59 native Kyrgyz speakers
  trained as annotators.
---

# Human-Annotated NER Dataset for the Kyrgyz Language

## Quick Facts
- arXiv ID: 2509.19109
- Source URL: https://arxiv.org/abs/2509.19109
- Reference count: 40
- First manually annotated NER dataset for Kyrgyz language with 1,499 news articles and 27 entity classes

## Executive Summary
This work introduces the first manually annotated NER dataset for the Kyrgyz language, containing 1,499 news articles with 10,900 sentences and 39,075 entity mentions across 27 entity classes. The dataset was annotated by 59 native Kyrgyz speakers using detailed guidelines based on the GROBID project, achieving an inter-annotator agreement of κ=0.89. Experiments with various NER models show that multilingual transformer-based models, particularly XLM-RoBERTa, significantly outperform classical CRF models, achieving an F1 score of 0.73. However, the dataset exhibits severe class imbalance, with the top four classes accounting for approximately 70% of all mentions, resulting in near-zero performance for rare entity classes.

## Method Summary
The dataset was created through a multi-stage process: initial data collection from the 24.KG news portal, tokenization using Apertium-Kir, annotation by 59 trained native Kyrgyz speakers via Doccano, and expert validation. The authors established detailed annotation guidelines based on the GROBID project, covering 27 entity classes with BIO tagging format. The final dataset contains 1,499 documents (999 for training, 500 for testing) in CoNLL-2003 format. Five baseline models were evaluated: CRF, BERT+CRF, mBERT, XLM-RoBERTa, and mT5-small, with XLM-RoBERTa achieving the best performance (F1=0.73).

## Key Results
- XLM-RoBERTa achieves the highest F1 score of 0.73 among all evaluated models
- Severe class imbalance: top 4 entity classes account for ~70% of all mentions
- 11 entity classes have fewer than 15 training examples, resulting in F1 scores near 0.0
- Inter-annotator agreement of κ=0.89 demonstrates high annotation consistency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual transformer models pretrained on diverse languages transfer effectively to low-resource Kyrgyz NER
- Mechanism: XLM-RoBERTa's cross-lingual representations, learned from 100+ languages during pretraining, provide transferable token embeddings that generalize to Kyrgyz even without language-specific pretraining. The model leverages shared linguistic patterns (morphology, syntax) across languages to bootstrap entity recognition.
- Core assumption: Kyrgyz shares sufficient structural regularities with languages in XLM-R's pretraining corpus (including other Turkic languages) for transfer to occur.
- Evidence anchors: Abstract states multilingual RoBERTa achieves "promising balance between precision and recall"; Section 5.2 shows XLM-RoBERTa delivers best overall performance with F1=0.70; related work on OpenNER 1.0 demonstrates standardized multilingual NER datasets enable cross-lingual transfer.

### Mechanism 2
- Claim: Severe class imbalance (top 4 classes = ~70% of mentions) directly causes near-zero performance on rare entity classes
- Mechanism: With 5-10 training examples for classes like Plant, Animal, Award, models cannot form stable decision boundaries. Gradient updates from frequent classes (Person, Location, Measure with 900-1000+ examples) dominate learning, causing rare-class collapse.
- Core assumption: Model capacity is sufficient; the bottleneck is training signal, not architecture.
- Evidence anchors: Abstract notes class imbalance with top four classes accounting for ~70% of mentions; Table 5 shows F1 scores: Measure=0.86 (1046 samples) vs Plant/Award/Animal=0.00 (3-10 samples).

### Mechanism 3
- Claim: Context-dependent entity disambiguation fails when local context is insufficient or training examples lack diversity
- Mechanism: Entities like "British" or "Президент" require broader sentential/discourse context to disambiguate between National/Person_Type/Concept or Title/Person. Transformer attention mechanisms may not capture sufficient context when training examples are homogeneous.
- Core assumption: The annotation guidelines are consistently applied; ambiguity is inherent to the language/task, not annotation noise.
- Evidence anchors: Section 3.3 describes "Президент" as either Title or Person depending on context with concrete examples; Section 6 identifies context-dependent entity mentions as a main source of errors.

## Foundational Learning

- Concept: **Cross-lingual transfer learning**
  - Why needed here: The entire experimental approach relies on multilingual models (XLM-R, mBERT, mT5) transferring knowledge to Kyrgyz. Without understanding transfer learning, you cannot diagnose why XLM-R outperforms CRF or why performance varies across classes.
  - Quick check question: Can you explain why a model trained on 100+ languages would recognize Kyrgyz entities despite Kyrgyz not being explicitly optimized during pretraining?

- Concept: **Sequence labeling evaluation (BIO/IOB tagging)**
  - Why needed here: The dataset uses CoNLL format with B-/I- prefixes (e.g., "B-PERIOD", "I-CONCEPT"). Understanding precision/recall/F1 for token-level vs. entity-level evaluation is essential for interpreting Table 4-5 results.
  - Quick check question: Given the CoNLL example in Table 2, how would you compute entity-level F1 for the "CONCEPT" span "кыргыз тилинен"?

- Concept: **Class imbalance in classification**
  - Why needed here: The paper's primary failure mode is rare-class collapse (F1=0.0 for 11 classes). Understanding why macro-F1 differs from micro-F1, and why upsampling/focal loss might help, is critical for future work.
  - Quick check question: If you trained a model on this dataset and achieved 90% accuracy, why might this metric be misleading given the class distribution in Figure 1?

## Architecture Onboarding

- Component map:
  Raw text (24.KG news) → Tokenization (Apertium-Kir) → Annotation (Doccano, 59 annotators) → Validation (expert approvers, κ=0.89) → CoNLL format export → Model training (CRF / BERT+CRF / mBERT / XLM-R / mT5) → Evaluation (entity-level F1, per-class breakdown)

- Critical path: Data quality validation (inter-annotator agreement) → tokenizer compatibility → model fine-tuning → error analysis on rare classes. The MATTER workflow (Model-Annotate-Train-Test-Evaluate-Revise) in Section 3.5 is the backbone.

- Design tradeoffs:
  - 27 entity classes vs. collapsed schema: Granular classes enable richer extraction but create severe imbalance; merging rare classes would improve metrics but lose information.
  - Flat vs. nested NER: Paper explicitly chooses flat NER ("largest entity mention" principle) to simplify annotation; nested NER would capture more structure but increase annotation complexity.
  - Transformer vs. CRF baseline: Transformers outperform (F1 0.68-0.73 vs. 0.62) but require more compute; CRF offers interpretability and faster inference.

- Failure signatures:
  - F1=0.0 for rare classes (Plant, Award, Animal, etc.) → indicates insufficient training samples (<15 examples)
  - High precision, low recall (CRF: P=0.70, R=0.55) → model is conservative, missing valid entities
  - Context-dependent errors (National vs. Person_Type) → insufficient contextual diversity in training data

- First 3 experiments:
  1. **Baseline reproduction**: Fine-tune XLM-RoBERTa on the provided train split (999 docs) with the paper's hyperparameters (batch_size=8, lr=1e-5, 10 epochs). Verify you achieve F1≈0.73. Log per-class F1 to identify which rare classes collapse.
  2. **Class imbalance ablation**: Merge the 11 rare classes (F1=0.0) into an "UNKNOWN" superclass or drop them entirely. Retrain XLM-R and measure macro-F1 change. This tests whether the bottleneck is class count or sample count.
  3. **Context window analysis**: For the ambiguous "Президент" Title vs. Person cases, extract 50 examples of each from the dataset. Fine-tune with doubled max sequence length (if using mT5) or analyze attention patterns to diagnose whether broader context improves disambiguation.

## Open Questions the Paper Calls Out

- Can synthetic data generation or upsampling techniques significantly improve F1 scores for the rare entity classes currently suffering from low support?
  - Basis: Section 6 proposes extending training data with synthetic sentences or upsampling to address the scarcity of training samples for classes like Animal, Award, and Plant.
  - Why unresolved: The authors identified class imbalance as a major challenge but only evaluated standard fine-tuning, leaving data augmentation strategies for future work.
  - What evidence would resolve it: Experimental results comparing baseline models against those trained on an augmented dataset with balanced class distributions.

- To what extent do the baseline models trained on news articles generalize to other text genres such as social media, conversations, or legal prose?
  - Basis: Section 8 states that texts come exclusively from the 24.KG news portal and that models "may fail to generalize to other genres."
  - Why unresolved: The dataset is domain-specific (news), and no experiments were conducted on out-of-domain data to test robustness.
  - What evidence would resolve it: Evaluation of the fine-tuned models on a newly annotated dataset comprising social media posts or legal documents.

- Would implementing a nested or hierarchical annotation scheme improve the accuracy of identifying complex entity mentions compared to the current "flat" scheme?
  - Basis: Section 8 lists the annotation of only "flat" entities as a limitation, while Section 2.2 notes that nested NER is valuable for hierarchical entities (e.g., "The Chinese embassy in France").
  - Why unresolved: The authors deliberately excluded nested entities to simplify the initial annotation process, but this may limit the detection of context-dependent or compound entities.
  - What evidence would resolve it: A comparative analysis of model performance on a subset of data re-annotated to include nested entity spans.

## Limitations
- Severe class imbalance with 11 entity classes having fewer than 15 training examples, resulting in near-zero F1 scores
- Dataset limited to news articles from a single source, raising questions about generalization to other domains
- Context-dependent entity disambiguation remains challenging due to insufficient training examples with diverse contexts

## Confidence
- **High confidence**: Transformer models (XLM-RoBERTa) significantly outperform classical CRF models for Kyrgyz NER (F1 0.73 vs. 0.62)
- **Medium confidence**: Cross-lingual transfer through multilingual pretraining effectively enables Kyrgyz NER
- **Medium confidence**: Class imbalance is the primary cause of rare-class collapse (F1=0.0)

## Next Checks
1. Implement SMOTE or random oversampling for the 11 rare classes (Plant, Award, Animal, etc.) and retrain XLM-RoBERTa to measure whether macro-F1 improves while monitoring for overfitting.

2. Apply the best-performing model to a small sample of Kyrgyz text from non-news domains (e.g., social media posts or Wikipedia articles) to establish whether the model generalizes beyond the news domain.

3. Select 100 ambiguous entity mentions (e.g., context-dependent cases like "Президент" or borderline Measure/Quantity instances) and have two expert annotators independently label them to quantify annotation consistency for challenging cases.