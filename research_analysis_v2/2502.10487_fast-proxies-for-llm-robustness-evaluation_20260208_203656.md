---
ver: rpa2
title: Fast Proxies for LLM Robustness Evaluation
arxiv_id: '2502.10487'
source_url: https://arxiv.org/abs/2502.10487
tags:
- arxiv
- attacks
- direct
- attack
- robustness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The authors propose using inexpensive proxy metrics\u2014direct\
  \ prompting, prefilling, and embedding-space attacks\u2014to predict the robustness\
  \ of large language models (LLMs) against computationally expensive adversarial\
  \ red-teaming attacks. They evaluate these proxies against a strong ensemble of\
  \ six real-world attack methods across 33 open-source models and 300 harmful prompts."
---

# Fast Proxies for LLM Robustness Evaluation

## Quick Facts
- arXiv ID: 2502.10487
- Source URL: https://arxiv.org/abs/2502.10487
- Reference count: 29
- The authors propose using inexpensive proxy metrics—direct prompting, prefilling, and embedding-space attacks—to predict the robustness of large language models (LLMs) against computationally expensive adversarial red-teaming attacks

## Executive Summary
This paper addresses the computational bottleneck in LLM safety evaluation by proposing fast proxy methods to predict robustness against expensive adversarial red-teaming attacks. The authors evaluate three proxies—direct prompting, prefilling, and embedding-space gradient descent—against an ensemble of six real-world attack methods across 33 open-source models and 300 harmful prompts. Direct prompting achieves remarkably high rank correlation (rs = 0.94) with ensemble attack success rates despite very low absolute attack success rates, enabling three orders of magnitude cost reduction while maintaining predictive power for model selection.

## Method Summary
The authors evaluate three proxy attack methods—direct prompting, prefilling, and embedding-space gradient descent—against a computationally expensive ensemble of six adversarial attacks (AmpleGCG, AutoDAN, BEAST, GCG, HumanJB, PAIR) generating 727 candidates per prompt. Using 300 harmful prompts from HarmBench, they compute Attack Success Rates (ASR) for each proxy and the ensemble across 33 sub-10B parameter models. Robustness is measured via dual-classifier evaluation using Llama-2-13B HarmBench classifier and LlamaGuard 3 8B, with success requiring both classifiers to flag a response as harmful. The study computes Pearson (linear) and Spearman (rank) correlations between proxy ASR and ensemble ASR to assess predictive power for both model ranking and absolute robustness estimation.

## Key Results
- Direct prompting achieves Spearman rank correlation rs = 0.94 with ensemble ASR, enabling reliable model ranking despite near-zero absolute attack success rates
- Embedding-space attacks provide strong linear correlation (rp = 0.87) suitable for absolute robustness prediction
- Using direct prompting proxies reduces computational cost by three orders of magnitude compared to full red-teaming evaluation
- Direct prompting with ≥50 prompts provides the best balance of cost and ranking accuracy for model selection

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Direct prompting—simply submitting unmodified harmful prompts—predicts model robustness ranking against sophisticated adversarial attacks despite yielding very low absolute attack success rates.
- **Mechanism:** The relative success rate of naive attacks correlates with fundamental safety alignment quality. Models that refuse simple harmful prompts at higher rates tend to refuse complex adversarial prompts at proportionally higher rates, preserving ordinal relationships across the attack spectrum.
- **Core assumption:** The underlying safety alignment mechanism responds proportionally to both simple and sophisticated attacks, meaning robustness is a relatively monotonic property across attack complexity.
- **Evidence anchors:**
  - [abstract] "Even though direct prompting in particular does not achieve high ASR, we find that it and embedding-space attacks can predict attack success rates well, achieving $r_p=0.87$ (linear) and $r_s=0.94$ (Spearman rank) correlations"
  - [section 4.2] "Direct ASR is generally below 5%, except for models that are extremely unrobust... the rank correlation coefficients of direct prompting (rs = 0.94, τ = 0.83) are higher than those of the other two proxy methods"
  - [corpus] Weak corpus support—neighbor papers focus on different proxy applications (neural architecture search, retrieval) rather than adversarial robustness prediction.
- **Break condition:** For models with near-identical direct ASR values (especially near zero), ranking becomes unreliable; the paper notes "≥50 prompts" are needed for robust ranking, with more prompts required as models become more robust.

### Mechanism 2
- **Claim:** Embedding-space gradient-descent attacks provide strong linear correlation with ensemble attack success rates, making them suitable for absolute robustness estimation.
- **Mechanism:** Continuous optimization in embedding space bypasses discrete token constraints, efficiently probing the model's latent vulnerability manifold. These vulnerabilities transfer to discrete attacks because both exploit similar weaknesses in the model's safety alignment representation.
- **Core assumption:** White-box access to embedding gradients reveals vulnerability structure that generalizes to black-box discrete attacks, and the L2-constrained optimization probes a meaningful neighborhood of the input space.
- **Evidence anchors:**
  - [section 3] "This framework—while impractical for real-world attacks, where most threat models assume a black box setting with string-level input—provides an extremely fast way to attack models in a white box setting"
  - [section 4.4] "Prefilling and embedding space attacks attain universally higher Pearson correlation, i.e., admit a better linear fit irrespective of the number of prompts"
  - [corpus] Assumption: The corpus doesn't directly address embedding-space to discrete-attack transfer, but GreenFactory (arXiv:2505.09344) supports zero-cost proxy ensembling principles.
- **Break condition:** Requires white-box model access; may not generalize to closed-source models or quantized deployments where embedding access differs.

### Mechanism 3
- **Claim:** Prefilling attacks—injecting affirmative response prefixes—provide moderate prediction quality but are generally inferior to the other two proxies for both ranking and linear estimation.
- **Mechanism:** By forcing the model to begin with an affirmative prefix, prefilling tests the model's susceptibility to context manipulation. However, this specific vulnerability dimension correlates less consistently with the broader attack ensemble's success patterns.
- **Core assumption:** Prefill susceptibility represents one vulnerability dimension among many, but does not capture the full robustness landscape as comprehensively as other methods.
- **Evidence anchors:**
  - [section 3] "Prefilling attacks rely on injecting a prefix to the beginning of the victim model's response to the harmful prompt - typically using an affirmative response prefix"
  - [section 4.1] "rp of 0.62 [for direct] is smaller than those of prefilling and embedding space attacks" but section 4.2 shows prefilling has lower rs (0.79) than direct (0.94)
  - [corpus] No corpus evidence for prefilling-specific mechanisms; this attack vector is less studied in the retrieved literature.
- **Break condition:** Some APIs (noted as Claude family) provide prefilling access, making this a realistic threat vector, but its predictive power is limited compared to alternatives.

## Foundational Learning

- **Concept:** Attack Success Rate (ASR)
  - **Why needed here:** The central metric for comparing proxy methods; measures the fraction of harmful prompts that successfully elicit harmful responses.
  - **Quick check question:** If a model refuses 97 out of 100 harmful prompts via direct prompting, what is its direct ASR?

- **Concept:** Spearman vs Pearson Correlation
  - **Why needed here:** Spearman (rank) correlation matters for model selection (which model is more robust?); Pearson (linear) correlation matters for absolute robustness prediction (how robust is this model?).
  - **Quick check question:** If direct ASR has rs=0.94 but rp=0.56, which question can it answer reliably: "Is Model A more robust than Model B?" or "What is Model A's actual ensemble ASR?"

- **Concept:** Many-trial Attack Setting
  - **Why needed here:** The synthetic red-teamer evaluates all 727 candidate prompts per harmful prompt, making attacks maximally powerful; proxies must predict this aggressive baseline.
  - **Quick check question:** Why does the many-trial setting make the attack ensemble more expensive but also more realistic as an upper-bound threat model?

## Architecture Onboarding

- **Component map:**
  - Synthetic red-teamer (6 attacks, 727 candidates) -> Ensemble ASR
  - Direct prompting (greedy decode) -> Direct ASR
  - Prefilling (affirmative prefix injection) -> Prefilling ASR
  - Embedding-space PGD (100 steps, L2 constraint) -> Embedding ASR
  - Dual-judge classifier (HarmBench + LlamaGuard) -> Response classification

- **Critical path:**
  1. Select model(s) to evaluate
  2. Run proxy attack(s) on 300 harmful prompts (~1-5 seconds per prompt depending on method)
  3. Classify responses as harmful/not harmful
  4. Compute proxy ASR
  5. If ranking: use direct ASR with ≥50 prompts; if linear estimation: use embedding-space ASR

- **Design tradeoffs:**
  - Direct prompting: Lowest cost (batched, ~1 second total), best for ranking with sufficient prompts, poor for linear fits due to near-zero ASR floor
  - Embedding-space: Moderate cost (~5 seconds per prompt), best linear correlation, requires white-box access
  - Prefilling: Low cost, moderate correlations, realistic threat vector for some APIs, but generally outperformed
  - Prompt count vs correlation: Direct prompting scales better with more prompts; embedding-space better with fewer prompts

- **Failure signatures:**
  - Direct ASR near zero across all models -> insufficient prompt count for ranking
  - Embedding-space attack diverges -> learning rate or L2 normalization may need model-family-specific tuning
  - High variance in proxy ASR across runs -> increase prompt count or verify classifier stability
  - Correlation breaks down across model families -> may indicate proxy doesn't capture architecture-specific vulnerabilities

- **First 3 experiments:**
  1. **Baseline validation on 2-3 known models:** Compute direct ASR and embedding-space ASR for a small model set; verify rank ordering matches published benchmarks.
  2. **Prompt count sensitivity analysis:** Run direct prompting with 25, 50, 100, 200, 300 prompts to identify the minimum prompt count needed for stable rs > 0.90 for your model family.
  3. **Cross-family generalization test:** Train a linear mapping from proxy ASR to ensemble ASR on one model family (e.g., Llama variants); evaluate prediction error on held-out family (e.g., Mistral variants) to assess transferability.

## Open Questions the Paper Calls Out

- **Open Question 1:** Do the high rank correlations observed in proxy methods transfer to models with significantly larger parameter counts (e.g., >10B parameters)?
  - **Basis in paper:** [explicit] The authors explicitly state in Section 4.5 (Limitations) that experiments were restricted to the "sub-10B parameter class" and require validation to ensure generalization to other model sizes.
  - **Why unresolved:** Safety dynamics and internal representations often change as models scale up (scaling laws), potentially altering the relationship between simple proxy attacks and complex ensemble attacks.
  - **What evidence would resolve it:** Replicating the evaluation pipeline on frontier-scale models (e.g., Llama-3-70B or 405B) to verify if the Spearman rank correlation ($r_s$) remains above 0.9.

- **Open Question 2:** How reliable are these specific proxies against adversarial algorithms that are fundamentally different from the six methods used in the synthetic red-teamer ensemble?
  - **Basis in paper:** [explicit] Section 4.5 notes the reliance on a "fixed attack suite" and suggests that results should be validated against "other attack algorithms" to ensure broad applicability.
  - **Why unresolved:** The proxies were tuned or evaluated against a specific set of attacks (e.g., GCG, PAIR). If a novel attack vector exploits a vulnerability not present in the ensemble, the proxy may fail to predict robustness.
  - **What evidence would resolve it:** Testing proxy performance against a diverse set of newly developed attacks (e.g., multi-turn human jailbreaks or different optimization strategies) excluded from the original ensemble.

- **Open Question 3:** Does the computational efficiency of direct prompting degrade as models become highly robust, requiring substantially more harmful prompts to avoid sampling errors?
  - **Basis in paper:** [inferred] Section 4.4 observes that direct prompting requires >50 prompts because low ASR estimates can be erratic (often 0) with small samples, and the authors expect the required prompt count to increase as models become more robust.
  - **Why unresolved:** If future models are extremely robust (e.g., <0.1% ASR), direct prompting might require thousands of prompts to distinguish between models, negating the "three orders of magnitude" cost reduction.
  - **What evidence would resolve it:** A scaling analysis measuring the minimum number of prompts required to achieve statistically significant rank correlations for models with near-perfect robustness.

## Limitations

- Focus on sub-10B parameter models may not extend to frontier models with different safety dynamics and attack surfaces
- Fixed attack suite of six methods may not capture all realistic threat vectors or novel attack formulations
- Implementation details for prefilling prefixes and embedding normalization remain underspecified, potentially affecting reproducibility

## Confidence

- **High:** Strong empirical validation within defined scope; clear methodology and results; explicit acknowledgment of scope limitations
- **Medium:** Predictive power varies across model families and contexts; proxy performance depends on implementation details; cross-family generalization requires empirical validation
- **Medium:** Critical implementation details underspecified; dual-judge evaluation introduces potential variability; technical gaps could impact reproducibility

## Next Checks

1. **Validation Check 1:** Execute direct prompting with systematically varying prompt counts (25, 50, 100, 200, 300) on a small representative model set. Measure Spearman rank correlation stability across prompt counts to identify the minimum threshold required for reliable model ranking in your specific context.

2. **Validation Check 2:** Train a linear regression mapping from embedding-space proxy ASR to ensemble ASR using one model family (e.g., Llama variants). Evaluate prediction error on a held-out family (e.g., Mistral variants) to quantify cross-family generalization capability before deploying for new model evaluations.

3. **Validation Check 3:** Replace one or two attack methods from the original ensemble with alternative formulations while keeping proxy methods constant. Measure changes in Pearson and Spearman correlations to assess whether the proxy-predictive relationship holds across different threat model compositions.