---
ver: rpa2
title: 'SimpleVQA: Multimodal Factuality Evaluation for Multimodal Large Language
  Models'
arxiv_id: '2502.13059'
source_url: https://arxiv.org/abs/2502.13059
tags:
- question
- culture
- answer
- history
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SimpleVQA, the first comprehensive multimodal
  benchmark for evaluating the factuality of multimodal large language models (MLLMs).
  The dataset comprises 2,025 high-quality question-answer pairs across 9 tasks and
  9 domains, ensuring diverse and challenging scenarios.
---

# SimpleVQA: Multimodal Factuality Evaluation for Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2502.13059
- Source URL: https://arxiv.org/abs/2502.13059
- Reference count: 40
- First comprehensive multimodal benchmark for evaluating factuality of MLLMs

## Executive Summary
This paper introduces SimpleVQA, a novel benchmark designed to evaluate the factuality of multimodal large language models (MLLMs). The dataset comprises 2,025 high-quality question-answer pairs spanning 9 tasks across 9 diverse domains. Through rigorous quality control processes, the benchmark ensures accuracy and consistency while maintaining diversity in challenging scenarios. The evaluation employs an LLM-as-a-judge scoring system for straightforward assessment. Experiments with 18 MLLMs and 8 text-only LLMs reveal significant challenges in factual accuracy, particularly highlighting the mismatch between visual understanding and knowledge internalization capabilities. The study provides valuable insights for optimizing MLLMs and advancing the development of trustworthy multimodal AI systems.

## Method Summary
SimpleVQA employs a systematic approach to evaluate multimodal factuality through carefully curated question-answer pairs. The benchmark uses an LLM-as-a-judge scoring mechanism to assess model responses, with questions designed to test both visual understanding and factual knowledge. The dataset construction process involves rigorous quality control to ensure high accuracy and consistency across 9 tasks and 9 domains. Evaluation includes both MLLMs and text-only LLMs for comparative analysis, with atomic question generation used to identify error sources. The benchmark specifically focuses on static, timeless factual queries to enable automated grading while maintaining reliability in assessment.

## Key Results
- Most MLLMs struggle with factual accuracy, revealing significant gaps in visual understanding and knowledge internalization
- Experiments show a large mismatch between literacy ability and knowledge internalization ability in MLLMs
- Image content understanding remains a major challenge for MLLMs attempting to achieve improved factual capabilities
- SFT (Supervised Fine-Tuning) provides some improvement but visual comprehension remains a distinct bottleneck

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its comprehensive coverage of multimodal tasks combined with rigorous quality control processes. By focusing on static factual queries and employing LLM-as-a-judge evaluation, SimpleVQA creates a standardized framework for assessing multimodal factuality. The atomic question generation approach allows for systematic identification of error sources, while the comparison between MLLMs and text-only LLMs provides clear insights into the specific challenges of multimodal reasoning.

## Foundational Learning
- Multimodal factuality assessment - why needed: To evaluate whether MLLMs can accurately combine visual and textual information to produce correct factual answers
  - quick check: Compare model performance on visual vs. text-only factual queries
- LLM-as-a-judge evaluation - why needed: Provides standardized, automated assessment of model responses for consistency
  - quick check: Cross-validate judge scores across multiple LLM evaluators
- Atomic question generation - why needed: Enables systematic identification of specific error types and capabilities
  - quick check: Analyze error distribution across atomic question categories
- Visual understanding vs. knowledge internalization - why needed: Identifies the specific bottleneck in multimodal factual reasoning
  - quick check: Measure performance gap between visual recognition and knowledge retrieval tasks
- Static vs. dynamic factuality - why needed: Ensures benchmark stability for automated evaluation
  - quick check: Verify all benchmark questions have unchanging answers over time

## Architecture Onboarding

### Component Map
- Image preprocessing -> Visual feature extraction -> Text generation model -> LLM-as-a-judge evaluation -> Performance analysis

### Critical Path
The critical path involves accurate visual feature extraction followed by integration with text generation capabilities. The bottleneck occurs at the intersection of visual understanding and knowledge internalization, where models must combine what they see with what they know to produce factually correct answers.

### Design Tradeoffs
The benchmark prioritizes static, timeless answers over dynamic real-world temporal factuality to ensure automated grading reliability. This creates a tradeoff between evaluation stability and real-world applicability. The use of LLM-as-a-judge scoring provides standardization but introduces potential subjectivity and bias in assessment.

### Failure Signatures
Primary failure modes include incorrect visual identification, knowledge retrieval errors, and inability to properly integrate visual and textual information. Models often demonstrate strong performance on text-only factual queries but struggle significantly when visual understanding is required, indicating a fundamental limitation in multimodal reasoning capabilities.

### First 3 Experiments
1. Evaluate baseline performance of text-only LLMs on SimpleVQA to establish text-only factuality benchmarks
2. Test MLLMs on atomic questions focusing solely on visual identification to isolate visual understanding capabilities
3. Compare SFT-enhanced models against baseline MLLMs to measure improvement in multimodal factuality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the "mismatch" between visual understanding capability and internalized knowledge in MLLMs be effectively reduced?
- Basis in paper: Section 4 notes that experiments reveal a "large mismatch between the literacy ability and knowledge internalization ability of MLLMs," identifying it as a primary source of error.
- Why unresolved: The paper diagnoses the issue through atomic question analysis but does not propose a model architecture or training method to synchronize these two capabilities.
- What evidence would resolve it: A training intervention that improves alignment scores between visual recognition tasks and knowledge retrieval tasks on the SimpleVQA benchmark.

### Open Question 2
- Question: What specific training strategies are required to overcome the "major challenge" of image content understanding for factual queries?
- Basis in paper: Section 1, Finding (3), and Section 6 conclude that "Image content understanding is still a major challenge for MLLMs to achieve improved capabilities."
- Why unresolved: While the authors note that SFT offers some improvement, they highlight that visual comprehension remains a distinct bottleneck compared to text-only factuality.
- What evidence would resolve it: Demonstrating an architecture that achieves significantly higher "Atomic" question scores (visual identification) without relying solely on external knowledge retrieval.

### Open Question 3
- Question: Does the requirement for "static and timeless" answers limit the evaluation of MLLMs on dynamic, real-world temporal factuality?
- Basis in paper: Section 2.2 explicitly excludes time-sensitive topics (e.g., "current president") to ensure answers are unchanging, leaving the domain of dynamic knowledge unevaluated.
- Why unresolved: The benchmark prioritizes stability for automated grading, but this methodological choice assumes that factual capability on static history implies capability on evolving current events, which is unproven.
- What evidence would resolve it: A comparative study showing correlations between performance on the static SimpleVQA benchmark and a dynamic, time-sensitive variant.

## Limitations
- Evaluation methodology relies on LLM-as-a-judge scoring, introducing potential subjectivity and bias
- Specific quality control procedures remain unclear, limiting assessment of dataset accuracy assurance
- Evaluation scope constrained by selection of 18 MLLMs and 8 text-only LLMs, potentially limiting generalizability
- Does not address potential domain-specific biases within the 9 selected domains

## Confidence
- Medium: Novel benchmark with systematic approach to dataset construction and comprehensive error analysis
- Medium: Relies on automated evaluation metrics that may introduce bias
- Low: Limited discussion of potential biases in benchmark design and domain selection

## Next Checks
1. Independent replication of the benchmark evaluation using different LLM-as-a-judge models to verify consistency in scoring and identify potential bias patterns
2. Expansion of the evaluation to include a more diverse set of MLLMs, particularly emerging models and those with different architectural approaches
3. Detailed analysis of domain-specific performance variations and potential biases within the 9 selected domains to better understand model limitations and generalization capabilities