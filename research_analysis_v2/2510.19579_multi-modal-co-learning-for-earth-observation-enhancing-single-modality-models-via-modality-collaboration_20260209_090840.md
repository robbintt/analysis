---
ver: rpa2
title: 'Multi-modal Co-learning for Earth Observation: Enhancing single-modality models
  via modality collaboration'
arxiv_id: '2510.19579'
source_url: https://arxiv.org/abs/2510.19579
tags:
- multi-modal
- modalities
- learning
- co-learning
- modality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of missing sensor modalities
  at inference time in Earth observation (EO) tasks. The authors propose MDiCo, a
  multi-modal co-learning framework that enhances single-modality models by learning
  shared and modality-specific features from multiple modalities during training.
---

# Multi-modal Co-learning for Earth Observation: Enhancing single-modality models via modality collaboration

## Quick Facts
- arXiv ID: 2510.19579
- Source URL: https://arxiv.org/abs/2510.19579
- Reference count: 40
- Key outcome: MDiCo improves single-modality EO models by up to 4.8 F1 points and 0.025 R² through multi-modal co-learning

## Executive Summary
This paper addresses the challenge of missing sensor modalities at inference time in Earth observation tasks. The authors propose MDiCo, a multi-modal co-learning framework that enhances single-modality models by learning shared and modality-specific features from multiple modalities during training. The framework uses contrastive learning to align shared features across modalities and modality discriminant learning to separate modality-specific information. MDiCo is evaluated on four EO benchmarks spanning classification and regression tasks, showing consistent improvements over state-of-the-art approaches with F1 score improvements of up to 4.8 points and R² improvements of up to 0.025 in regression tasks.

## Method Summary
MDiCo uses modality-dedicated encoders to extract three types of features: shared (z_sha), specific (z_spe), and unused (z_unu). The framework combines contrastive learning with InfoNCE loss to align shared features across modalities and modality discriminant learning through auxiliary classifiers to isolate sensor-specific information. During training, paired multi-modal data is used to optimize four losses: main prediction, auxiliary prediction, contrastive alignment, and modality discrimination. At inference, only shared and specific features are used, enabling single-modality operation. The framework employs TempCNN for SITS data and ResNet-50 for aerial images, with a linear projection layer and unweighted sum of losses.

## Key Results
- MDiCo achieves up to 4.8 F1 points improvement over individual single-modality models on classification tasks
- Regression tasks show R² improvements of up to 0.025 when using MDiCo
- Performance consistently outperforms EO-specific methods like DIOM and MOMA across all four benchmarks
- Shared-only features underperform compared to shared+specific combination, demonstrating complementary information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive alignment of shared features enables cross-modal knowledge transfer that survives missing modalities at inference.
- Mechanism: InfoNCE loss forces paired features from different modalities describing the same sample to cluster together in latent space while pushing apart features from different samples. This creates modality-invariant representations where z_sha_1 and z_sha_2 encode the same semantic content regardless of sensor type.
- Core assumption: Modalities share task-relevant information that can be linearly or non-linearly mapped to a common subspace.
- Evidence anchors:
  - [abstract] "Our approach combines contrastive and modality discriminative learning together to guide single-modality models to structure the internal model manifold into modality-shared and modality-specific information."
  - [section 3.3.3] "For learning the features shared among modalities... we use the contrastive learning by forcing the paired features between modalities to be similar in the learned latent space."
  - [corpus] CAML (arXiv:2502.17821) addresses similar missing-modality scenarios in multi-agent systems, suggesting this is a recognized problem class with active research.
- Break condition: If modalities share no task-relevant information (e.g., random noise vs. images), contrastive alignment will fail or harm performance.

### Mechanism 2
- Claim: Modality discriminant learning isolates sensor-specific information that complements shared features.
- Mechanism: An auxiliary classifier predicts which modality a feature came from. Gradient reversal or explicit cross-entropy forces z_spe and z_unu to encode information that distinguishes modalities, while shared features z_sha become modality-agnostic by exclusion.
- Core assumption: Task-relevant information distributes unevenly across shared and specific spaces; some discriminative signals are modality-unique.
- Evidence anchors:
  - [section 3.3.4] "For learning the features proper to each modality... we use an auxiliary classifier that discriminates from which modality the features are coming from."
  - [table 7] Shared-only features underperform compared to shared+specific combination (e.g., TSAITS Aerial: 35.3 vs 66.4 F1), demonstrating specific features carry complementary task information.
  - [corpus] Evidence is limited; neighbor papers focus on fusion rather than explicit disentanglement.
- Break condition: If the discriminator is too weak or too strong, disentanglement collapses—either all features become shared (no discrimination) or all become specific (no transfer).

### Mechanism 3
- Claim: Unused feature buffering prevents task-irrelevant noise from contaminating shared/specific representations.
- Mechanism: The z_unu feature space absorbs modality-unique information that is NOT task-discriminative, as judged by the absence of gradient signal from predictive losses. This acts as a "sink" for sensor-specific artifacts (e.g., radar speckle, optical cloud contamination).
- Core assumption: Each modality contains noise or information unrelated to the downstream task.
- Evidence anchors:
  - [section 3.2.2] "we opt to discriminate the unused features in this unique information, based on the hypothesis that each modality could have noise that is unrelated to the downstream task."
  - [table 6] Removing unused features drops F1 by ~1 point across benchmarks, indicating measurable contribution.
  - [corpus] No direct evidence in neighbor papers; this appears to be a novel contribution.
- Break condition: If all modality-unique information is task-relevant, z_unu becomes empty or redundant, wasting capacity.

## Foundational Learning

- Concept: Contrastive Learning (InfoNCE loss, positive/negative pairs, temperature scaling)
  - Why needed here: Core mechanism for aligning shared features across modalities; misunderstanding this leads to poor cross-modal transfer.
  - Quick check question: Can you explain why temperature τ=0.07 affects the hardness of the alignment objective?

- Concept: Feature Disentanglement (separating shared vs. specific representations)
  - Why needed here: MDiCo's entire architecture relies on partitioning the latent space; conflating these concepts defeats the mechanism.
  - Quick check question: If two modalities have identical z_sha but different z_spe, what does that imply about their task-relevant information?

- Concept: Multi-task Loss Balancing (uniform sum vs. adaptive weighting)
  - Why needed here: Four losses compete; Section 4.5 shows simple summation outperforms uncertainty weighting.
  - Quick check question: Why might adaptive weighting fail when losses have different scales and convergence rates?

## Architecture Onboarding

- Component map:
  E_com_m(X_m) → z_sha_m (shared encoder per modality)
  E_uni_m(X_m) → (z_spe_m, z_unu_m) (unique encoder per modality)
  P_m([z_sha_m || z_spe_m]) → ŷ_m (main prediction head)
  P_aux(z) → ŷ_aux (auxiliary prediction head, shared across modalities and feature types)
  P_spe(z_spe_m) → modality prediction (discriminator)
  P_unu(z_unu_m) → modality prediction (discriminator)
  Loss aggregation: L_total = L_main + L_aux + L_cont + L_mod (unweighted sum)

- Critical path: Input → E_com + E_uni → (z_sha, z_spe, z_unu) → concatenate z_sha + z_spe → P_m → prediction. Only z_sha and z_spe are used at inference; z_unu is discarded.

- Design tradeoffs:
  - Separate encoders (E_com, E_uni) vs. shared encoder: Table 6 shows shared encoders drop performance by 1.3 F1 points on average—modularity matters.
  - Uniform vs. adaptive loss weighting: Uniform sum outperforms uncertainty weighting (Table 6), suggesting bias from adaptive schemes harms co-learning.
  - Encoder architecture choice: TempCNN and ConvTran perform best; Transformer over-parametrization hurts on small datasets (Fig. 3, LFMC <3k samples).

- Failure signatures:
  - Contrastive loss dominates but F1 doesn't improve: Check if modalities share insufficient task-relevant information; verify data pairing is correct.
  - Specific features outperform full model: Shared features may be misaligned—reduce τ or increase batch size for better negative sampling.
  - Modality discriminator loss → 0 too quickly: Features collapse to trivial solutions; increase discriminator capacity or add gradient reversal.

- First 3 experiments:
  1. **Baseline comparison**: Train Individual models on each modality separately, then MDiCo on CropH-b. Report F1 gap (should see 2-4 point improvement per Table 2).
  2. **Ablation sweep**: Remove each loss (L_cont, L_mod, L_aux) one at a time. Expect L_cont removal to cause largest drop (~3 F1 points, Table 6).
  3. **Encoder robustness test**: Swap TempCNN for ConvTran on LFMC. Verify improvement persists across architectures (Fig. 3 pattern).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the MDiCo framework scale to scenarios involving more than two modalities during the training stage?
- Basis in paper: [explicit] The authors explicitly state in the Limitations section that the "framework has been validated for benchmarks with only two modalities" and identify extension to scenarios involving multiple modalities as a natural future research direction.
- Why unresolved: The current formulation (e.g., contrastive loss) is demonstrated in a two-modality setting ($m \in \{1,2\}$); it is unclear if the pairwise alignment strategy remains computationally efficient or effective with a higher number of sensor modalities.
- What evidence would resolve it: Evaluation of MDiCo on multi-modal Earth Observation benchmarks containing three or more modalities (e.g., combining Sentinel-1, Sentinel-2, and LiDAR).

### Open Question 2
- Question: Can the proposed disentanglement strategy generalize effectively to non-EO computer vision benchmarks?
- Basis in paper: [explicit] The Conclusion notes the approach "has been validated exclusively on EO data" and suggests future work should "extend the evaluation to general multi-modal computer vision and machine learning benchmarks."
- Why unresolved: The specific architecture and hyperparameters (e.g., temperature $\tau=0.07$) were selected based on remote sensing data characteristics, which differ significantly in modality homology and resolution from standard vision datasets.
- What evidence would resolve it: Application of MDiCo to standard multi-modal datasets (e.g., NYU Depth V2 or audiovisual datasets) showing consistent improvements over unimodal baselines.

### Open Question 3
- Question: Is the unweighted sum of loss functions optimal for balancing shared versus specific feature learning across diverse tasks?
- Basis in paper: [inferred] Section 3.3 states the authors use an "unweighted sum" and Section 4.5 mentions they "did not perform an extensive hyperparameter search," noting that an adaptive uncertainty-based weighting strategy yielded inferior results.
- Why unresolved: While the simple sum works, the ablation study shows the contrastive loss has the "greatest impact," suggesting that different tasks might benefit from specific weighting schemes to prevent the contrastive loss from dominating the modality-specific learning.
- What evidence would resolve it: A sensitivity analysis on the loss coefficients ($\lambda$) in Equation 6, specifically measuring the trade-off between $L_{cont}$ and $L_{mod}$ across different classification and regression tasks.

## Limitations
- Framework has been validated only for benchmarks with two modalities; extension to multiple modalities is unclear
- Architecture specifications for TempCNN, ConvTran, and ResNet-50 are not provided, hindering exact reproduction
- The theoretical justification for unused feature buffering is heuristic rather than rigorously validated
- Cross-modal alignment effectiveness for highly heterogeneous modalities is not thoroughly evaluated

## Confidence

- **High Confidence**: Contrastive learning alignment improves shared feature quality (supported by ablation studies and consistent F1 gains)
- **Medium Confidence**: Modality discriminant learning successfully isolates complementary specific features (demonstrated through performance degradation when removed, though disentanglement quality is not quantified)
- **Low Confidence**: Unused feature buffering provides systematic noise absorption (quantitative evidence exists but mechanism is largely theoretical)

## Next Checks

1. **Disentanglement Quality**: Apply mutual information estimation or feature orthogonality tests to verify z_sha, z_spe, and z_unu are truly separated and not correlated

2. **Modality Similarity Impact**: Systematically vary modality dissimilarity (e.g., same sensor type vs. cross-sensor) to quantify how alignment effectiveness scales with feature space distance

3. **Sensitivity Analysis**: Perform hyperparameter sweeps on temperature τ and discriminator capacity to establish robustness bounds for the contrastive and discriminant learning components