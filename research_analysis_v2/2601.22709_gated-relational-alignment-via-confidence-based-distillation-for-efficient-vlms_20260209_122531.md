---
ver: rpa2
title: Gated Relational Alignment via Confidence-based Distillation for Efficient
  VLMs
arxiv_id: '2601.22709'
source_url: https://arxiv.org/abs/2601.22709
tags:
- distillation
- teacher
- grace
- quantization
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GRACE addresses the challenge of deploying vision-language models
  (VLMs) under strict bit-width constraints by unifying knowledge distillation and
  quantization-aware training (QAT) under the Information Bottleneck principle. The
  method introduces confidence-gated decoupled distillation to filter unreliable teacher
  supervision, relational centered kernel alignment to transfer visual token structures,
  and an adaptive IB controller to dynamically balance fidelity and capacity.
---

# Gated Relational Alignment via Confidence-based Distillation for Efficient VLMs

## Quick Facts
- arXiv ID: 2601.22709
- Source URL: https://arxiv.org/abs/2601.22709
- Authors: Yanlong Chen; Amirhossein Habibian; Luca Benini; Yawei Li
- Reference count: 40
- One-line primary result: INT4-quantized VLMs surpass BF16 baselines, achieving 3× throughput and 54% memory reduction

## Executive Summary
GRACE addresses the challenge of deploying vision-language models (VLMs) under strict bit-width constraints by unifying knowledge distillation and quantization-aware training (QAT) under the Information Bottleneck principle. The method introduces confidence-gated decoupled distillation to filter unreliable teacher supervision, relational centered kernel alignment to transfer visual token structures, and an adaptive IB controller to dynamically balance fidelity and capacity. GRACE enables INT4-quantized VLMs to surpass BF16 baselines, achieving 3× throughput and 54% memory reduction. On LLaVA-1.5-7B, it reaches 70.1% accuracy on SQA versus 66.8% for BF16, and on Qwen2-VL-2B, it attains 76.9% on MMBench versus 72.6% for BF16.

## Method Summary
GRACE integrates knowledge distillation with quantization-aware training under the Information Bottleneck framework. The method combines three key components: confidence-gated decoupled distillation that filters unreliable teacher supervision based on entropy, relational centered kernel alignment that transfers visual token structures via CKA on penultimate layer features, and an adaptive IB controller that dynamically balances distillation fidelity against quantization capacity constraints. The framework applies group-wise LSQ quantization to all linear layers in the LLM backbone while freezing the vision encoder in BF16 precision.

## Key Results
- INT4-quantized LLaVA-1.5-7B achieves 70.1% accuracy on SQA versus 66.8% for BF16 baseline
- INT4-quantized Qwen2-VL-2B attains 76.9% on MMBench versus 72.6% for BF16 baseline
- Achieves 3× throughput and 54% memory reduction compared to BF16 models

## Why This Works (Mechanism)

### Mechanism 1: Confidence-Gated Decoupled Distillation
Teacher predictions with high entropy are down-weighted via exponential gating (gi = exp(-hi)), filtering unreliable supervision. Theorem 3.1 proves this gated loss is strictly smaller than uniform weighting when high-entropy tokens have high loss. Empirical validation shows Pearson r=0.484 correlation between teacher entropy and error rate.

### Mechanism 2: Relational Centered Kernel Alignment (RCKA)
Transfers visual token relational structures by minimizing CKA between teacher and student penultimate layer features. Attention visualizations show larger models develop superior visual attention patterns that smaller models lack. RCKA captures intra-sample relational structure rather than point-wise features.

### Mechanism 3: Adaptive IB Controller
Dual ascent on distillation constraint automatically balances task performance against knowledge preservation. When student struggles (L̄GDKD > τ), β increases to strengthen distillation; when constraint satisfied, β decreases to prioritize task learning. Simulation validates convergence to target τ=0.35.

## Foundational Learning

- **Information Bottleneck Principle**: Frames quantization+distillation as IB—quantization imposes hard capacity constraint I(Z;X)≤Cb, while distillation maximizes I(Z;YT). Understanding this trade-off is essential for tuning τ and interpreting β dynamics. Quick check: Can you explain why quantization is a "hard" IB constraint rather than a soft penalty?

- **Decoupled Knowledge Distillation (DKD)**: GRACE builds on DKD's decomposition into TCKD (target-class confidence) and NCKD (non-target "dark knowledge"). Understanding why NCKD > TCKD weighting matters for setting α, βdkd hyperparameters. Quick check: Why might non-target class distributions carry more transferable knowledge than target-class probabilities alone?

- **Straight-Through Estimator (STE)**: Group-wise LSQ uses STE to backpropagate through rounding. Understanding forward (quantize) vs. backward (identity) behavior is critical for debugging gradient flow issues during QAT. Quick check: What happens to gradients if you forget to apply STE and treat rounding as a non-differentiable operation?

## Architecture Onboarding

- **Component map**: Input (image + text) → Vision Encoder (frozen, BF16) → Teacher LLM (frozen, BF16) → PT (teacher logits) ↓ Student LLM (trainable, INT4) ← RCKA (penultimate layer) ↓ PS (student logits) ← GDKD (confidence-gated, β-weighted) ↓ L_total = L_CE + β·L_GDKD + ω·L_RCKA

- **Critical path**: 1) Initialize student from pretrained weights with group-wise LSQ; 2) Forward pass through frozen teacher and quantized student; 3) Compute per-token teacher entropy Hi; derive gating weights gi = exp(-Hi/log|V|); 4) Extract penultimate visual tokens; compute centered Gram matrices and CKA loss; 5) Update β via dual ascent based on EMA-smoothed LGDKD vs. τ; 6) Backprop through STE; update both W and s jointly.

- **Design tradeoffs**: Group size g=128: smaller groups = finer granularity but more scale parameters; larger groups = coarser but more efficient. τ=0.35: tighter constraint preserves more teacher knowledge but may over-constrain task learning. Vision encoder frozen vs. quantized: paper freezes vision encoder to focus compression on LLM backbone.

- **Failure signatures**: INT4 accuracy < BF16 baseline: check if β converged too low or τ too loose; training divergence: reduce dual step size η; RCKA not helping: verify visual token extraction excludes text tokens; quantized weights collapsing: check scale initialization.

- **First 3 experiments**: 1) Baseline sanity check: run GRACE BF16 (no quantization) with GDKD+RCKA to verify distillation gains; 2) Ablation by component: disable each component to reproduce Table 3 ablation; 3) Quantization depth test: compare INT8 vs INT4 with full GRACE.

## Open Questions the Paper Calls Out

### Open Question 1
Can GRACE be extended to activation quantization without significant accuracy degradation, and how would the confidence-gated distillation mechanism adapt to handle quantization noise in intermediate feature maps? [explicit] The conclusion explicitly states: "Future work includes extending GRACE to activation quantization and exploring its application to other multimodal architectures such as video and audio understanding models."

### Open Question 2
How does GRACE perform when applied to video-language and audio-language multimodal architectures where temporal dynamics create additional cross-modal complexity? [explicit] Same conclusion statement explicitly calls for exploring "other multimodal architectures such as video and audio understanding models."

### Open Question 3
Does the confidence-gating mechanism remain effective when teacher and student have larger capacity gaps (e.g., 70B→7B), or does the entropy-error correlation weaken for extreme compression ratios? [inferred] The paper tests 13B→7B and 7B→2B compression ratios. The information-theoretic justification assumes the student can approximate teacher knowledge within quantization constraints, but larger gaps may violate this assumption.

### Open Question 4
Can GRACE maintain its performance advantage when quantizing the vision encoder in addition to the LLM backbone, given that visual token relational structures (transferred via RCKA) may be disrupted by encoder quantization? [inferred] The method freezes the vision encoder in BF16, but real-world deployment would benefit from full-model INT4 quantization.

## Limitations

- Teacher entropy correlation (r=0.484) is moderate, suggesting confidence gating may filter some useful supervision
- RCKA mechanism lacks direct corpus evidence for effectiveness in VLMs specifically
- Adaptive IB controller shows weak external validation beyond paper's own simulations
- Layer alignment details for RCKA underspecified when teacher-student architectures differ in depth

## Confidence

- **High confidence**: GRACE achieves superior INT4 quantization performance (70.1% SQA accuracy for LLaVA-1.5-7B vs 66.8% BF16 baseline). The fundamental architecture and loss formulation are clearly specified with reproducible components.
- **Medium confidence**: The three proposed mechanisms (confidence gating, RCKA, adaptive IB) individually contribute to performance gains. While ablation studies show positive effects, hyperparameter choices may not generalize across different model scales or tasks.
- **Low confidence**: The theoretical claims about Information Bottleneck optimization are somewhat speculative. Proposition 3.2 provides a variational bound but doesn't prove the surrogate objective actually optimizes the intended IB trade-off in practice.

## Next Checks

1. **Teacher entropy correlation validation**: Systematically measure teacher entropy vs. prediction error correlation across different VLM architectures (7B, 13B, 34B) and task types. If correlation drops below r=0.4 for certain domains, the gating mechanism may need task-specific calibration or alternative uncertainty measures.

2. **RCKA mechanism ablation**: Remove RCKA while keeping all other GRACE components (GDKD + adaptive IB + quantization) to isolate its contribution. Compare against a variant that transfers visual features via standard feature regression instead of CKA. This tests whether relational structure transfer is essential or if simpler feature matching suffices.

3. **Adaptive IB controller sensitivity**: Sweep τ from 0.2 to 0.5 and η from 0.001 to 0.003 to map the stability landscape. Verify that β dynamics converge rather than oscillate, and that the final performance is robust to these hyperparameters. If performance varies dramatically with small τ/η changes, the controller may be too sensitive for practical deployment without careful tuning.