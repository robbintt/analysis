---
ver: rpa2
title: How well do generative models solve inverse problems? A benchmark study
arxiv_id: '2601.23238'
source_url: https://arxiv.org/abs/2601.23238
tags:
- generative
- inverse
- design
- target
- conditional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a comprehensive benchmark study comparing
  four generative modeling approaches for solving inverse design problems: Bayesian
  inference, Conditional Wasserstein GAN, Invertible Neural Networks, and Conditional
  Flow Matching. The problem addressed is gas turbine combustor design, mapping six
  design parameters to three performance labels using a dataset of 1295 CFD-simulated
  combustor geometries.'
---

# How well do generative models solve inverse problems? A benchmark study

## Quick Facts
- **arXiv ID:** 2601.23238
- **Source URL:** https://arxiv.org/abs/2601.23238
- **Reference count:** 14
- **Primary result:** Conditional Flow Matching outperforms Bayesian inference, Conditional WGAN, and Invertible Neural Networks by 31.5-83.3% on gas turbine combustor inverse design tasks.

## Executive Summary
This benchmark study compares four generative modeling approaches for solving inverse design problems in gas turbine combustor engineering. The authors evaluate Bayesian inference, Conditional Wasserstein GAN, Invertible Neural Networks, and Conditional Flow Matching on their ability to generate design parameters given target performance metrics. Using a dataset of 1295 CFD-simulated combustor geometries with six design parameters mapping to three performance labels, the study systematically tests each method across seven different dataset sizes. The results demonstrate that Conditional Flow Matching consistently achieves superior accuracy and data efficiency compared to alternative approaches, with relative improvements of 31.5-83.3% over the next best model.

## Method Summary
The study addresses inverse design by training generative models to learn the conditional distribution P(X|Y) of design parameters X given target performance metrics Y. Four approaches are compared: Bayesian inference with MCMC sampling, Conditional Wasserstein GAN, Invertible Neural Networks, and Conditional Flow Matching. The dataset consists of 1295 CFD-simulated combustor geometries with six design parameters and three performance labels (unmixedness, pressure drop, thermoacoustic growth rate). Models are trained on dataset sizes ranging from 100 to 100,000 samples, with surrogate MLPs validating generated designs. Key metrics include mean absolute error between target labels and surrogate-predicted labels of generated designs, along with analysis of parameter diversity across different targets.

## Key Results
- Conditional Flow Matching outperforms all other approaches across all dataset sizes with relative improvements of 31.5-83.3% over next best model
- CFM demonstrates superior accuracy in generating designs matching target specifications while maintaining parameter diversity
- Bayesian inference performs worst, particularly for unmixedness label, showing weak target alignment and increased variance
- CFM shows better data efficiency, requiring fewer training samples to achieve comparable performance to other methods

## Why This Works (Mechanism)
None specified in input

## Foundational Learning
- **CFD simulation basics:** Understanding of computational fluid dynamics for combustor geometry simulation is needed to appreciate the problem domain and data generation process
- **Generative modeling concepts:** Familiarity with conditional probability distributions and inverse problem formulation helps understand the core task
- **Surrogate modeling:** Knowledge of how MLPs approximate expensive CFD simulations is crucial for interpreting validation methodology

## Architecture Onboarding
- **Component map:** Design parameters (X) -> Generative model -> Performance predictions (via surrogate) -> Target matching
- **Critical path:** Training data → Generative model training → Design generation → Surrogate validation → Performance evaluation
- **Design tradeoffs:** CFM vs INN (continuous vs discrete transformations), CFM vs CWGAN (deterministic vs stochastic generation), all vs Bayesian (computational cost vs theoretical guarantees)
- **Failure signatures:** CWGAN mode collapse (clustered outputs), Bayesian poor mixing (high variance, weak alignment), numerical instability (NaN losses)
- **First experiments:** 1) Train CFM on smallest dataset size to verify basic functionality, 2) Generate 100 designs for fixed target and visualize parameter distributions, 3) Compare surrogate predictions vs target values for generated designs

## Open Questions the Paper Calls Out
**Open Question 1:** Can Conditional Flow Matching maintain its superior performance and data efficiency when applied to higher-dimensional and more complex design spaces?
- **Basis in paper:** [explicit] The conclusion states that "Next steps could include higher-dimensional, more complex design tasks" to verify if the clear advantage of CFM over INN and CWGAN generalizes beyond the 6-parameter combustor problem.
- **Why unresolved:** The current benchmark is restricted to a relatively low-dimensional problem (6 parameters, 3 labels), and it is unknown if the computational benefits of CFM scale linearly or degrade in high-dimensional manifolds.
- **What evidence would resolve it:** A comparative benchmark on a design task with significantly more parameters (e.g., 50+) showing the relative error convergence rates of CFM vs. INN.

**Open Question 2:** Do the reported model rankings and accuracy metrics persist when validated against high-fidelity CFD simulations or physical experiments rather than surrogate models?
- **Basis in paper:** [inferred] While the conclusion explicitly calls for "applications to real world data," the methodology relies on surrogate models (S_Yi) for both the augmented training data and the evaluation metric (Equation 39).
- **Why unresolved:** Surrogate models introduce approximation errors (Table 2) that may smooth the loss landscape, potentially masking deficiencies in the generative models that would be revealed by the "ground truth" physics of full CFD or experimental testing.
- **What evidence would resolve it:** Re-evaluating the generated designs from Section 6 using the full 96-core-hour CFD simulation workflow to confirm the Mean Absolute Error rankings.

**Open Question 3:** How does training on surrogate-generated synthetic data (Dataset D_Aug) influence the models' ability to learn the true underlying physics versus overfitting to surrogate artifacts?
- **Basis in paper:** [inferred] Section 5.1 details the creation of a synthetic dataset of 250,000 points predicted by surrogates to enable benchmarking, but it is unknown if this introduces a distribution shift.
- **Why unresolved:** Generative models might learn to invert the surrogate's specific systematic biases (e.g., the specific error in S_UM) rather than the true inverse problem, leading to designs that are optimal for the surrogate but suboptimal in reality.
- **What evidence would resolve it:** A comparison of models trained strictly on the 1295 CFD points against those trained on the surrogate-augmented dataset, validated on a held-out set of true CFD simulations.

## Limitations
- Dataset availability and fidelity — the 1295-sample combustor dataset is proprietary and not directly accessible
- Benchmark generalizability limited to one specific 6-dimensional combustor design problem
- Fixed architecture choices may not represent optimal configurations for each method

## Confidence
- **High Confidence:** CFM consistently outperforms other methods across all dataset sizes and metrics
- **Medium Confidence:** The ranking of INN > CWGAN > Bayesian inference is reliable but magnitude may vary
- **Medium Confidence:** Surrogate validation adequately captures design quality, though real CFD validation would strengthen conclusions

## Next Checks
1. **Dataset Transferability Test:** Apply the trained CFM model to a held-out test set of 1000 CFD-simulated combustor designs to verify that surrogate-predicted labels align with actual CFD-simulated performance
2. **Architectural Sensitivity Analysis:** Vary CFM architecture (layer widths, depths) and compare performance degradation curves to assess whether reported improvements are architecture-dependent
3. **Alternative Domain Validation:** Train and evaluate the same four methods on a different inverse design problem (e.g., airfoil shape optimization) to test domain generalizability of CFM superiority claim