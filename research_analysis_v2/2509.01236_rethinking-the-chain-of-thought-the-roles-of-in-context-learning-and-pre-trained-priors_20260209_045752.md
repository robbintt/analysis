---
ver: rpa2
title: 'Rethinking the Chain-of-Thought: The Roles of In-Context Learning and Pre-trained
  Priors'
arxiv_id: '2509.01236'
source_url: https://arxiv.org/abs/2509.01236
tags:
- reasoning
- exemplars
- pretrained
- priors
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how Chain-of-Thought (CoT) prompting leverages
  in-context learning (ICL) and pretrained priors to enhance reasoning in large language
  models. Through fine-grained lexical analysis and controlled experiments with noisy
  exemplars, the authors demonstrate that models learn both structural and logical
  reasoning patterns from exemplars while remaining influenced by pretrained priors.
---

# Rethinking the Chain-of-Thought: The Roles of In-Context Learning and Pre-trained Priors

## Quick Facts
- arXiv ID: 2509.01236
- Source URL: https://arxiv.org/abs/2509.01236
- Reference count: 40
- Models learn reasoning structures and patterns from Chain-of-Thought exemplars while being influenced by pretrained priors, with exemplar quantity shifting reliance from priors to in-context signals.

## Executive Summary
This study investigates how Chain-of-Thought (CoT) prompting leverages in-context learning (ICL) and pretrained priors to enhance reasoning in large language models. Through fine-grained lexical analysis and controlled experiments with noisy exemplars, the authors demonstrate that models learn both structural and logical reasoning patterns from exemplars while remaining influenced by pretrained priors. They find that increasing exemplar quantity shifts reliance from pretrained knowledge to ICL signals, though misleading prompts introduce instability. Additionally, prompt engineering can induce slow thinking by generating longer CoT chains, improving downstream task performance. These findings reveal the interplay between ICL and pretrained priors in CoT reasoning and suggest paths for enhancing model reasoning through thoughtful prompt design.

## Method Summary
The authors conduct controlled experiments across multiple tasks (GSM8K, Coin Flip) using various prompt types (task-specific CoT, task-agnostic, long-CoT distilled from RLMs). They employ fine-grained lexical analysis to track structural vocabulary adoption, verb frequency, and feature word usage. Robustness testing involves injecting false-answer and false-rationale variants at different exemplar counts (4-40 shot). Confidence tracking records token generation probabilities under greedy decoding. Long-CoT experiments distill reasoning chains from models like DeepSeek-R1-Distill and QwQ to induce "slow thinking" in target models of varying sizes.

## Key Results
- Models rapidly acquire lexical structures and deeper reasoning patterns from CoT exemplars through ICL, while pretrained priors continue influencing reasoning
- Increasing exemplar quantity (4â†’40 shot) shifts model reliance from pretrained priors to in-context signals, causing systematic label flipping in closed-domain tasks
- Long-CoT prompting induces slow thinking by generating longer reasoning chains, improving performance up to optimal lengths that scale with model capacity (8B degrades, 32B benefits)

## Why This Works (Mechanism)

### Mechanism 1: Lexical Structure and Reasoning Pattern Learning from Exemplars
Models rapidly acquire both surface-level lexical structures and deeper reasoning patterns from CoT exemplars through ICL, while pretrained priors continue to influence the reasoning trajectory. When CoT exemplars are provided, the model increases structural vocabulary adoption ("Therefore," "So") and adjusts verb usage patterns that reflect causal reasoning. Even task-agnostic CoT prompts (e.g., Sports-CoT applied to math) induce task-specific reasoning, indicating the model extracts reasoning structure rather than content alone. Verb frequency shows an optimal range correlated with accuracy.

### Mechanism 2: Exemplar Quantity Shifts Decision-Making from Pretrained Priors to ICL Signals
Increasing the number of exemplars shifts model reliance from pretrained priors to in-context signals, enabling even 8B models to learn input-output mappings that prior work suggested required larger models. In few-shot settings (4-5 exemplars), pretrained priors stabilize performance even with noisy exemplars. As exemplar count increases (to 40-shot), ICL signals strengthen. In closed-domain tasks (Coin Flip with binary labels), this causes systematic label flipping when false-answer exemplars are provided. In open-domain tasks (GSM8K, Date), accuracy degrades more gradually but substantially. Token generation probabilities show increased variance under misleading prompts, indicating confidence instability.

### Mechanism 3: Long-CoT Prompting Induces Slow Thinking via ICL and Pretrained Knowledge
Models can be prompted to generate longer reasoning chains ("slow thinking") by using distilled long-CoT exemplars from reasoning models, improving downstream performance up to an optimal length threshold that scales with model capacity. Long-CoT exemplars distilled from RLMs (DeepSeek-R1-Distill, QwQ) contain reflective, retrospective, and summarization patterns. When used as prompts, these induce the target model to emulate this style. Instruct models benefit more due to stronger instruction-following. Optimal CoT length scales with model size: 8B models degrade with excessive length, while 32B models benefit from longer chains.

## Foundational Learning

- **In-Context Learning (ICL)**
  - Why needed here: The paper's central thesis is that CoT effectiveness arises from ICL's interaction with pretrained priors. Understanding ICL as "learning from exemplars without weight updates" is prerequisite.
  - Quick check question: Can you explain why ICL differs from fine-tuning, and what "input-label mapping" means in this context?

- **Pretrained Priors**
  - Why needed here: The paper demonstrates that models default to pretrained knowledge when ICL signals are weak. Understanding this tension is essential for interpreting RQ2 results.
  - Quick check question: How would you distinguish whether a model's output is driven by pretrained priors vs. in-context signals?

- **Chain-of-Thought Prompting**
  - Why needed here: The mechanism being studied. You need to understand the standard CoT formulation (question, rationale, answer) and variants (zero-shot CoT, task-agnostic CoT).
  - Quick check question: What is the difference between zero-shot CoT ("Let's think step by step") and few-shot CoT with exemplars?

## Architecture Onboarding

- Component map:
  - Prompt Engineering Module -> Lexical Analysis Pipeline -> Noise Injection Layer -> Confidence Tracking -> Accuracy Metrics

- Critical path:
  1. Select task and prompt type (task-specific CoT, task-agnostic, or long-CoT)
  2. If robustness testing: inject noise (false-answer or false-rationale) and vary exemplar count (4-40)
  3. Run inference with greedy decoding
  4. Extract lexical components for analysis OR track probability curves for confidence assessment
  5. Correlate with accuracy metrics

- Design tradeoffs:
  - **Exemplar quantity vs. quality**: More exemplars strengthen ICL but amplify noise effects. High-quality exemplars are critical beyond ~10-shot.
  - **CoT length vs. model capacity**: 8B models have shorter optimal CoT lengths; 32B models can leverage longer chains. Overloading small models causes degradation.
  - **Greedy vs. sampling decoding**: Greedy enables reproducible confidence tracking but causes repetition loops with long-CoT. Sampling improves diversity but complicates mechanism analysis.

- Failure signatures:
  - **Token probability variance spike**: Indicates misleading prompt conflict; model confidence destabilized.
  - **Accuracy cliff at high shot counts with noisy exemplars**: Suggests ICL overrode pretrained priors with incorrect mappings (particularly in closed-domain tasks).
  - **Repetition loops in long-CoT generation**: Indicates need for sampling strategy or early stopping; greedy decoding inadequate.

- First 3 experiments:
  1. **Lexical baseline test**: Run zero-shot CoT vs. few-shot CoT vs. task-agnostic CoT on GSM8K. Extract and compare structural vocabulary proportion and verb counts. Verify that task-agnostic prompts still induce math-specific feature words.
  2. **Exemplar scaling with noise**: Take Coin Flip and GSM8K. Create false-answer and false-rationale variants. Run at 4-shot, 10-shot, 20-shot, 40-shot. Plot accuracy curves and identify the "crossover point" where ICL dominates priors.
  3. **Long-CoT transfer test**: Distill long-CoT exemplars from a 32B RLM (e.g., QwQ). Apply to 8B and 32B base models. Measure both accuracy and token count. Identify the optimal CoT length for each model size and document where degradation begins.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the interplay between in-context learning and pretrained priors generalize to more complex reasoning tasks beyond the arithmetic and symbolic domains tested?
- Basis in paper: [explicit] The authors state in the Limitations section that experiments are "limited to a few datasets, mainly in mathematics, leaving more complex reasoning tasks to be tested."
- Why unresolved: The current study restricts its validation to specific benchmarks (GSM8K, Coin Flip, etc.), leaving the mechanism's robustness in open-ended or highly complex domains unverified.
- What evidence would resolve it: Replication of the lexical analysis and noise injection experiments on complex, non-mathematical benchmarks (e.g., coding or long-horizon planning).

### Open Question 2
- Question: How do varying intensities and types of noise in exemplars quantitatively shift the model's reliance between pretrained priors and in-context signals?
- Basis in paper: [explicit] The paper notes in the Limitations section that "the impact of different levels of noise on CoT reasoning requires further investigation."
- Why unresolved: While the study tests binary noise (correct vs. false rationale/answer), it does not map a continuous function of how incremental increases in noise severity affect the ICL/prior balance.
- What evidence would resolve it: Experiments utilizing a spectrum of noise magnitudes to plot the degradation curve of prior-reliance versus ICL-adoption.

### Open Question 3
- Question: What alternative decoding strategies or model settings can effectively mitigate the "endless repetition" issue observed when generating long Chain-of-Thought sequences?
- Basis in paper: [explicit] The authors report that "greedy decoding for long CoT often resulted in endless repetitions, indicating the need to explore alternative model settings."
- Why unresolved: The study identifies a practical failure mode in inducing "slow thinking" via long CoT but relies on greedy decoding, suggesting the generation stability for this specific task is unresolved.
- What evidence would resolve it: Comparative analysis of sampling methods (e.g., temperature tuning, nucleus sampling) showing stable, non-repetitive long reasoning chains.

## Limitations

- Lexical analysis framework may not generalize across different model families or non-English languages where reasoning patterns manifest differently
- Context window constraints may confound exemplar quantity experiments, particularly for long-CoT prompting requiring extensive context
- Greedy decoding constraints create artificial limitations that may not reflect real-world usage and cause repetition loops with long-CoT

## Confidence

**High Confidence**: The core finding that exemplar quantity shifts model reliance from pretrained priors to ICL signals (Mechanism 2). This is supported by clear accuracy degradation curves across multiple shot counts and both closed-domain and open-domain tasks.

**Medium Confidence**: The lexical analysis demonstrating that models learn structural and reasoning patterns from exemplars (Mechanism 1). While the observed patterns are consistent, the causal link between lexical features and reasoning quality requires additional validation beyond correlation analysis.

**Medium Confidence**: The long-CoT prompting mechanism inducing slow thinking and improving performance (Mechanism 3). The scaling relationship with model capacity is evident, but the mechanism's dependency on greedy decoding limitations introduces uncertainty about real-world applicability.

## Next Checks

1. **Causal Validation of Lexical Features**: Design an ablation study where specific structural vocabulary elements (e.g., "Therefore," "So") are removed from CoT exemplars while maintaining reasoning content. Measure whether accuracy changes correlate with structural vocabulary adoption rather than just overall reasoning pattern learning.

2. **Sampling Strategy Comparison**: Replicate the exemplar quantity experiments using nucleus sampling (p=0.9) instead of greedy decoding. Compare accuracy degradation patterns, confidence stability (probability variance), and generation characteristics (repetition frequency, coherence).

3. **Cross-Domain Transferability**: Apply the same analytical framework (lexical analysis, exemplar scaling, long-CoT prompting) to code generation tasks (HumanEval) and commonsense reasoning tasks (StrategyQA). Measure whether the three mechanisms manifest similarly across domains, or whether task characteristics fundamentally alter the ICL-prior interplay.