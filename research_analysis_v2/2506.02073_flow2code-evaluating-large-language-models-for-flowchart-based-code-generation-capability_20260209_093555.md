---
ver: rpa2
title: 'Flow2Code: Evaluating Large Language Models for Flowchart-based Code Generation
  Capability'
arxiv_id: '2506.02073'
source_url: https://arxiv.org/abs/2506.02073
tags:
- code
- flowchart
- generation
- flow2code
- flowcharts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Flow2Code introduces a multimodal benchmark for flowchart-based
  code generation, addressing the gap in existing code generation evaluations. The
  dataset spans 15 programming languages and includes 16,866 flowcharts paired with
  5,622 code segments across three types: code, UML, and pseudocode.'
---

# Flow2Code: Evaluating Large Language Models for Flowchart-based Code Generation Capability

## Quick Facts
- arXiv ID: 2506.02073
- Source URL: https://arxiv.org/abs/2506.02073
- Reference count: 40
- Primary result: Current MLLMs struggle with flowchart-based code generation, particularly for UML and pseudocode flowcharts, with significant performance gains from supervised fine-tuning.

## Executive Summary
Flow2Code introduces a multimodal benchmark for evaluating large language models on flowchart-to-code generation tasks. The dataset covers 15 programming languages with 16,866 flowcharts paired with 5,622 code segments across three types: code, UML, and pseudocode. Through extensive experiments with 13 multimodal LLMs, the study reveals significant performance gaps, particularly for UML and pseudocode flowcharts, with supervised fine-tuning providing substantial improvements. Gemini-2.0 consistently outperforms other models, while smaller models show dramatic performance deficits.

## Method Summary
Flow2Code uses Visustin to generate flowcharts from source code, creating DOT format representations. GPT-4o converts code-DOT to pseudocode-DOT, with Gemini-2.0 validating transformations before human review. Graphviz renders flowcharts to images. The benchmark evaluates 13 MLLMs using execution-based Pass@k metrics across 15 languages. Fine-tuning uses LoRA (rank=8, alpha=16) with specific hyperparameters: LR=5e-5, batch_size=4, grad_accum=8, 1 epoch, cosine scheduler, AdamW, bf16, cutoff_len=2048.

## Key Results
- Current MLLMs struggle with flowchart-based code generation, particularly for UML and pseudocode flowcharts
- Supervised fine-tuning significantly improves model performance, with Gemini-2.0 achieving up to 4.78% higher pass rates than competitors
- Smaller models show substantial performance gaps, achieving only 14.3%-22.35% of top model scores
- Performance degrades from code flowcharts (highest) to pseudocode flowcharts (lowest), with a 10.63% decrease

## Why This Works (Mechanism)

### Mechanism 1: Iterative Cross-Verification for Synthetic Data Generation
High-fidelity flowchart-to-code alignment is achieved through a multi-stage verification pipeline where GPT-4o generates content and Gemini-2.0 validates it before human review. The pipeline converts code to DOT format, uses GPT-4o to generate natural language pseudocode labels, and employs Gemini-2.0 to check these transformations. Only data passing this check reaches human review.

### Mechanism 2: Visual-Textual Alignment via Supervised Fine-Tuning (SFT)
General-purpose MLLMs struggle to map visual flowchart topology to code syntax without domain-specific adaptation; LoRA bridges this gap effectively. Fine-tuning adjusts the projection of visual tokens into the LLM's semantic space, reducing the reasoning burden required to interpret abstract logic structures.

### Mechanism 3: Difficulty Scaling via Semantic Abstraction
Performance degrades as visual representation moves from explicit code syntax to abstract logic (UML/Pseudocode) because the model must infer missing implementation details. Code flowcharts provide explicit variable assignments and syntax, while pseudocode flowcharts replace syntax with natural language, requiring the model to act as a compiler/interpreter.

## Foundational Learning

- **Concept:** **DOT Language / Graphviz** - The entire Flow2Code dataset relies on DOT graph description language as the intermediate representation between source code and visual flowcharts. Quick check: Can you distinguish between a `box` node (process) and a `diamond` node (decision) in a DOT file snippet?

- **Concept:** **Execution-Based Evaluation (Pass@k)** - Unlike text generation, code generation success is binary: does it run and pass test cases? Flow2Code uses Pass@k to measure functional correctness. Quick check: Why might a generated code segment have 0% BLEU score but 100% Pass@1?

- **Concept:** **Multimodal Instruction Tuning** - To understand why zero-shot models fail and why SFT works. Quick check: In a Vision-Language Model, how does the projector layer map the output of a CNN/ViT to the input space of an LLM?

## Architecture Onboarding

- **Component map:** Source Code -> Visustin (generates Code/UML images + DOT files) -> GPT-4o (converts Code-DOT → Pseudocode-DOT) -> Gemini-2.0 (validates Pseudocode-DOT accuracy) -> Graphviz (DOT → Image) -> Multimodal LLM (e.g., Qwen2-VL, Gemini-2.0) -> Execution Engine (runs test cases)

- **Critical path:** The **Pseudocode Flowchart Generation** is the most fragile step, involving an LLM transformation (GPT-4o) that requires strict validation (Gemini-2.0). Errors here result in "hallucinated" logic in the ground truth, compromising the entire benchmark.

- **Design tradeoffs:** Synthetic vs. Real (synthetic flowcharts ensure logical perfection and scale, sacrificing "messiness" of hand-drawn diagrams); Zero-shot vs. SFT (SFT provides ~10-20% boost but requires significant compute and locks model into specific flowchart style)

- **Failure signatures:** Language specific collapse (high variance in legacy languages suggests lack of pre-training data); Visual hallucination (models struggling with UML/Pseudocode might invent variables not present); SFT Overfitting (if fine-tuned model passes flowcharts but fails textual code generation)

- **First 3 experiments:** 1) Baseline Validation: Select 50 random Python samples; run through standard LLM in zero-shot mode. Verify Pass@1 aligns with reported metrics (~80% for Python). 2) Ablation on Flowchart Type: Compare performance on Code vs. Pseudocode Flowcharts for same logic. Quantify "abstraction penalty." 3) SFT Feasibility Test: Fine-tune smaller model (e.g., Qwen2-VL-2B) on single language subset (e.g., Python) to test LoRA convergence.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can extending flowchart-based benchmarks to include code maintenance tasks, such as debugging or refactoring, better reflect the utility of multimodal models in software development? The current framework focuses on end-to-end generation and does not account for real-world maintenance tasks like debugging.

- **Open Question 2:** What specific architectural or training modifications are required to close the performance gap between smaller (approx. 7B) and larger (approx. 72B) models in flowchart comprehension? The results show smaller models achieve only 14.3%-22.35% of top model's scores, indicating a fundamental capability gap in visual-logic reasoning.

- **Open Question 3:** To what extent is the poor performance on legacy and markup languages (e.g., Fortran, HTML) caused by data scarcity versus architectural limitations in handling non-OOP paradigms? While OOP languages improved significantly, legacy languages saw only modest gains, suggesting the issue may be data scarcity or architectural constraints.

## Limitations
- Dataset construction relies on proprietary Visustin software, preventing full open-source reproduction
- Performance disparities across languages suggest potential overfitting to well-resourced languages (Python/C++) while struggling with legacy languages (Fortran/Pascal)
- The 10-20% performance gap between zero-shot and fine-tuned models may not generalize to all MLLM architectures

## Confidence
- **High Confidence:** Overall benchmark design and dataset statistics are well-documented and reproducible
- **Medium Confidence:** Supervised fine-tuning methodology and LoRA configuration are clearly specified
- **Medium Confidence:** Claim that models struggle with UML/pseudocode flowcharts is supported by results
- **Low Confidence:** Exact impact of synthetic vs. real-world flowchart variations on model performance

## Next Checks
1. **Test Case Verification:** Request and examine complete test case definitions for 50 randomly selected problems to verify execution-based evaluation methodology
2. **Language Performance Analysis:** Conduct ablation studies isolating performance differences between code/UML/pseudocode flowcharts for same logical problems to quantify "abstraction penalty"
3. **Cross-Architecture Validation:** Test the benchmark with additional MLLM architectures beyond those evaluated to assess generalizability of findings across model families