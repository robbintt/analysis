---
ver: rpa2
title: 'UMM-RM: An Upcycle-and-Merge MoE Reward Model for Mitigating Reward Hacking'
arxiv_id: '2512.00724'
source_url: https://arxiv.org/abs/2512.00724
tags:
- reward
- experts
- umm-rm
- training
- dense
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: UMM-RM addresses reward hacking in RLHF by introducing an upcycle-and-merge
  MoE reward model. The method first converts dense reward models into MoE architectures
  with shared experts, then merges the experts back into a dense model using learnable
  weights.
---

# UMM-RM: An Upcycle-and-Merge MoE Reward Model for Mitigating Reward Hacking

## Quick Facts
- **arXiv ID:** 2512.00724
- **Source URL:** https://arxiv.org/abs/2512.00724
- **Authors:** Lingling Fu; Yongfu Xue
- **Reference count:** 33
- **One-line primary result:** UMM-RM improves preference modeling accuracy by 2-4% on benchmarks like HH-Helpful and Webgpt while reducing reward hacking during RLHF training.

## Executive Summary
UMM-RM introduces an upcycle-and-merge approach to mitigate reward hacking in RLHF by converting dense reward models into MoE architectures with shared experts, then merging them back into dense models. The method combines expert diversity with efficient inference by using a shared expert for baseline preference signals and routed experts for specialized capabilities. Experiments show UMM-RM achieves 2-4% higher preference modeling accuracy on HH-Helpful and Webgpt benchmarks while demonstrating reduced reward hacking during PPO training compared to both dense and ensemble baselines.

## Method Summary
UMM-RM upcycles dense reward models by replacing FFN layers with K experts (1 shared + K-1 routed), trains the MoE on preference data using Bradley-Terry loss, then merges experts into a dense model using learnable weights that approximate expected gating behavior. The shared expert (with fixed weight α=0.5) provides a stable preference baseline while routed experts add specialized capacity. Post-training merging computes Θ_merge = αΘ_0 + Σ(ḡ_e × Θ_e), producing a smooth reward landscape that reduces exploitable regions while maintaining inference efficiency comparable to dense models.

## Key Results
- UMM-RM achieves 2-4% higher preference modeling accuracy on HH-Helpful and Webgpt benchmarks compared to dense baselines
- During PPO training, UMM-RM maintains higher gold scores while proxy scores increase, demonstrating reduced reward hacking
- Win rate evaluations show UMM-RM outperforms both dense and ensemble reward models in end-to-end generation quality
- The merged dense model achieves comparable performance to unmerged MoE while reducing inference cost by ~50%

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: A shared expert that activates for all tokens provides a stable preference baseline, reducing variance in reward signals.
- **Mechanism**: The shared expert (indexed as e=0) receives a fixed gating weight α regardless of input, forcing it to learn instruction- and distribution-invariant representations (e.g., core safety, fluency). Routed experts add specialized capacity. This decomposes preference modeling into "general + specialized" components.
- **Core assumption**: Human preferences contain both universal components (learnable by shared expert) and context-specific components (learnable by routed experts).
- **Evidence anchors**: [abstract] "The shared experts are always activated to capture instruction-agnostic preference signals"; [section III.A] "This architecture decomposes representations into 'general + specialized' components: the shared expert provides a stable, low-variance baseline"
- **Break condition**: If α is too high (e.g., 0.9), expert diversity collapses; if too low, the baseline becomes unstable. Experiments show α=0.5 performs best.

### Mechanism 2
- **Claim**: Multi-expert routing creates structural diversity that resists single-feature exploitation.
- **Mechanism**: Different experts specialize in distinct preference dimensions (helpfulness, harmlessness, coherence). When a policy attempts to exploit one expert's spurious correlation, other experts provide conflicting signals, making consistent exploitation harder.
- **Core assumption**: Experts will learn meaningfully different representations given limited preference data and routing dynamics.
- **Evidence anchors**: [section I] "enabling different experts to learn multi-dimensional representations of human preferences (e.g., helpfulness, safety, fluency, and consistency)"; [section III.A] "speculative experts that are misaligned with true human preferences—overfitting with high confidence in local subspaces"
- **Break condition**: If preference data is too small, each expert receives insufficient samples, increasing variance. Shared expert mitigates this but does not eliminate it.

### Mechanism 3
- **Claim**: Post-training weighted merging of experts into a dense model reduces high-variance reward responses and weakens spurious feature reliance.
- **Mechanism**: Merging computes Θ_merge = αΘ_0 + Σ(ḡ_e × Θ_e), where merging weights approximate expected gating behavior. This attenuates extreme parameters from any single expert, producing a smoother reward landscape with fewer exploitable regions. Inference cost matches dense RM.
- **Core assumption**: Parameter-space averaging approximates the ensemble behavior of independently routed experts.
- **Evidence anchors**: [section III.C] "the merged reward becomes less sensitive to local input perturbations, thereby compressing the exploitable space available to policy models"; [Figures 1, 3] UMM-RM maintains higher gold scores during PPO training compared to both dense RM and unmerged MoE
- **Break condition**: If merging weights are poorly calibrated (learned on unrepresentative calibration data), the merged model may not preserve expert diversity benefits.

## Foundational Learning

- **Concept: Reward Hacking**
  - Why needed here: The paper's central problem is that policies exploit proxy reward models, causing scores to increase while alignment degrades.
  - Quick check question: Can you explain why Goodhart's Law ("when a measure becomes a target, it ceases to be a good measure") applies to RLHF?

- **Concept: Mixture-of-Experts (MoE) Routing**
  - Why needed here: UMM-RM modifies standard MoE routing by adding a shared expert and constrained gating.
  - Quick check question: In standard MoE, how does top-k routing differ from the shared-expert routing proposed here?

- **Concept: Bias-Variance Tradeoff in Preference Learning**
  - Why needed here: The paper explicitly frames shared experts as reducing variance while merged experts introduce small bias.
  - Quick check question: Why would reducing variance in a reward model make it harder for a policy to exploit?

## Architecture Onboarding

- **Component map**: Dense backbone → Upcycled MoE (FFN layers replaced with K experts: 1 shared + K-1 routed) → Shared expert (fixed weight α, always active) → Routed experts (top-m selection via learned router, normalized softmax) → Post-training calibration → Merged dense FFN (Θ_merge = αΘ_0 + Σ(ḡ_e × Θ_e))

- **Critical path**: 1. Initialize MoE from SFT checkpoint (upcycle dense FFN → MoE FFN) 2. Train MoE-RM on preference data (Bradley-Terry loss) 3. Collect calibration samples, learn merging weights (≤200 steps, lr=1e-4) 4. Merge to dense RM, deploy in PPO

- **Design tradeoffs**: Number of experts vs. training data per expert: Paper uses 8 total (1 shared + 7 routed); α (shared weight): 0.5 optimal; higher reduces diversity, lower increases variance; Activated experts (m): More experts (4-6) improve robustness but increase compute during training

- **Failure signatures**: Gold score collapses mid-PPO while proxy rises → reward hacking not mitigated (check α too low, or insufficient experts activated); Merged model underperforms unmerged MoE → merging weights poorly calibrated (retrain on more/diverse calibration data); No improvement over dense baseline → expert diversity insufficient (increase experts or check routing health)

- **First 3 experiments**: 1. Replicate Figure 1: Train UMM-RM (2/4/6 experts) on TinyLlama-1.1B with AlpacaFarm; plot proxy vs. gold scores during PPO. Expect: dense RM shows divergence, UMM-RM shows sustained gold improvement. 2. Ablate shared-expert weight: Run α ∈ {0.1, 0.5, 0.75, 0.9} per Figure 4. Expect: peak at 0.5, degradation at extremes. 3. Compare to ensemble baselines: Train 4 independent RMs, aggregate via Mean/WCO/UWO. Compare gold scores to UMM-RM during PPO (Figure 2). Expect: UMM-RM achieves comparable or better stability with lower inference cost.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the upcycle-and-merge strategy retain its effectiveness in mitigating reward hacking when applied to significantly larger backbone models (e.g., 7B+ parameters)?
- Basis: [inferred] The experimental validation was limited to smaller models (0.5B–1.5B), while production RLHF often utilizes much larger architectures.
- Why unresolved: The trade-off between expert diversity and variance reduction via merging may shift unpredictably as the base model's capacity increases.
- Evidence: Replicating the UMM-RM training and merging process on a 7B or 70B parameter model and measuring the divergence between proxy and gold reward scores.

### Open Question 2
- Question: How sensitive is the merged model's performance to the size and distribution of the calibration dataset used to learn the merging weights?
- Basis: [inferred] The method relies on a "small set of calibration samples" to estimate the expectation of gating weights for merging, but the robustness of this estimation is not quantified.
- Why unresolved: If the calibration set is not representative of the input distribution, the learned linear combination of weights may fail to approximate the MoE's behavior.
- Evidence: Ablation studies varying the volume and domain of the calibration data to observe changes in preference modeling accuracy and reward hacking metrics.

### Open Question 3
- Question: Does the linear parameter merging of experts discard critical non-linear interactions or specialized capabilities compared to retaining the sparse MoE structure?
- Basis: [inferred] While the paper argues merging suppresses "speculative experts," it assumes a linear combination of parameters is sufficient to capture the ensemble's robustness.
- Why unresolved: Merging experts into a dense model might smooth out "useful" extreme behaviors along with the harmful ones, limiting the ceiling of reward model performance.
- Evidence: A direct comparison of performance on adversarial or complex reasoning tasks between the intermediate unmerged MoE model and the final merged UMM-RM.

## Limitations
- The paper's claims about reward hacking mitigation rely heavily on empirical demonstrations without theoretical guarantees
- The specific hyperparameter choices (α=0.5, 8 experts) lack sensitivity analysis across diverse model scales
- The proposed method's robustness to out-of-distribution prompts is not tested

## Confidence
- **High confidence**: The architectural description of UMM-RM (shared expert + routed experts + merging) is clearly specified and reproducible
- **Medium confidence**: The claim that UMM-RM "reduces reward hacking" is supported by PPO training curves but relies on a single gold reward model (Llama3-8B)
- **Low confidence**: The mechanism explanation (that expert diversity inherently resists exploitation) is plausible but not rigorously validated

## Next Checks
1. **Sensitivity Analysis**: Systematically vary α (shared weight) across {0.1, 0.3, 0.5, 0.7, 0.9} and expert count (4, 6, 8, 10) to identify the true robustness sweet spot
2. **Adversarial Stress Test**: Design targeted prompts that exploit specific preference dimensions (e.g., maximize helpfulness while degrading coherence) and compare dense, ensemble, and UMM-RM responses
3. **Generalization Across Gold Standards**: Repeat the AlpacaFarm PPO experiment using multiple gold reward models (e.g., GPT-4, Claude) rather than relying solely on Llama3-8B