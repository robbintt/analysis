---
ver: rpa2
title: 'When Silence Matters: The Impact of Irrelevant Audio on Text Reasoning in
  Large Audio-Language Models'
arxiv_id: '2510.00626'
source_url: https://arxiv.org/abs/2510.00626
tags:
- audio
- noise
- silence
- arxiv
- irrelevant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper reveals that irrelevant audio, including silence, can
  disrupt text-only reasoning in large audio-language models. The authors systematically
  evaluate this cross-modal interference using silence, synthetic noise, and environmental
  sounds on three text-based benchmarks.
---

# When Silence Matters: The Impact of Irrelevant Audio on Text Reasoning in Large Audio-Language Models

## Quick Facts
- arXiv ID: 2510.00626
- Source URL: https://arxiv.org/abs/2510.00626
- Reference count: 0
- Primary result: Irrelevant audio, including silence, disrupts text-only reasoning in large audio-language models, causing accuracy degradation and prediction volatility.

## Executive Summary
This paper reveals that irrelevant audio signals—including silence, Gaussian noise, and environmental sounds—can significantly disrupt text-only reasoning in large audio-language models (LALMs). Through systematic evaluation across three text-based benchmarks (GSM8K, ARC-Challenge, MMLU), the authors demonstrate that audio interference reduces accuracy and increases prediction volatility regardless of whether the audio contains semantic content. Surprisingly, silence destabilizes outputs as strongly as synthetic noise, indicating that models cannot distinguish non-informative audio from useful signals at the fusion layer. While larger models show greater resilience, vulnerabilities persist across all evaluated systems. The authors test mitigation strategies, finding that prompting is ineffective while self-consistency improves stability at increased computational cost.

## Method Summary
The authors conduct inference-only evaluation of cross-modal interference in LALMs by pairing text-only reasoning tasks with irrelevant audio inputs. They test three interference types (silence, Gaussian noise, environmental sounds from FSD50K) across varying durations (1-30 seconds) and amplitudes (-60 to -20 dBFS). Models evaluated include Qwen2.5-Omni-3B/7B, Phi-4-Multimodal, Voxtral-Mini/Small, and DeSTA2.5-Audio. Performance is measured using accuracy (correct/total) and influence rate (proportion of predictions that flip between clean and interference conditions). The study systematically varies duration, amplitude, and decoding temperature to assess interference severity.

## Key Results
- Irrelevant audio reduces text reasoning accuracy by 1-5% across all tested models and benchmarks
- Influence rates of 10-35% indicate frequent prediction flips when audio is present
- Silence and Gaussian noise produce nearly equivalent disruption effects
- Performance degradation scales with audio duration, amplitude, and decoding temperature
- Self-consistency (8 samples at T=0.5) reduces influence rate by ~50% but increases inference cost 8x

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Irrelevant audio signals are not filtered at the fusion layer and instead entangle with text representations during multimodal integration.
- Mechanism: The modality adapter projects audio embeddings into the LLM's embedding space without a gating mechanism to assess audio relevance. Silent or noisy audio produces activation patterns that the backbone LLM cannot distinguish from meaningful acoustic signals, causing the model to allocate attention to non-informative features.
- Core assumption: The model lacks an explicit relevance filter or audio-to-text gating function before fusion.
- Evidence anchors:
  - [abstract] "even non-informative audio reduces accuracy and increases prediction volatility"
  - [Section 2.1] "irrelevant audio that does not conflict with text yet degrades performance"
  - [corpus] Limited direct evidence; neighbor paper "SEE: Signal Embedding Energy" (arXiv:2601.07331) addresses noise quantification in LALMs but not fusion entanglement specifically.
- Break condition: If models implemented audio relevance scoring before adapter projection, interference would decrease regardless of audio duration or amplitude.

### Mechanism 2
- Claim: Longer audio exposure increases interference severity because the fusion process accumulates non-informative activations over time.
- Mechanism: Temporal persistence of irrelevant signals causes repeated integration of noise-derived embeddings into the context window. The model's attention mechanism distributes capacity across all tokens, including those derived from extended silence or noise, diluting reasoning bandwidth.
- Core assumption: The audio encoder processes continuous streams without segment-level relevance pruning.
- Evidence anchors:
  - [Section 3.1] "As duration increases, accuracy consistently drops and influence rate rises, revealing that longer non-informative segments amplify cross-modal interference"
  - [Section 3.1] "Even silence, when extended, destabilizes reasoning"
  - [corpus] No direct corpus support for duration scaling in multimodal fusion.
- Break condition: If audio were chunked with relevance pre-filtering before integration, duration effects would decouple from interference strength.

### Mechanism 3
- Claim: Higher decoding temperatures compound interference by amplifying the distributional noise introduced by irrelevant audio embeddings.
- Mechanism: Irrelevant audio shifts the output distribution by adding spurious probability mass to incorrect tokens. At higher temperatures, the sampling process explores this corrupted distribution more broadly, increasing the likelihood of selecting destabilized outputs.
- Core assumption: Audio interference primarily affects the shape of the output distribution rather than introducing deterministic bias.
- Evidence anchors:
  - [Section 3.3] "stochastic sampling amplifies the destabilizing effect of irrelevant signals"
  - [Section 3.3] "The influence rate is relatively low near greedy decoding but escalates sharply as temperature increases"
  - [corpus] No corpus papers directly address temperature-interference interactions in multimodal models.
- Break condition: If interference introduced a consistent directional bias rather than distributional noise, higher temperatures would not systematically increase influence rates.

## Foundational Learning

- Concept: **Cross-modal fusion architecture**
  - Why needed here: Understanding how audio and text embeddings combine in LALMs is essential to diagnose why irrelevant audio affects text reasoning.
  - Quick check question: Does the model concatenate audio and text embeddings, use cross-attention, or project both into a shared space before the LLM backbone?

- Concept: **Influence rate vs. accuracy degradation**
  - Why needed here: Accuracy alone underestimates instability; influence rate captures how often predictions change regardless of correctness direction.
  - Quick check question: If a model's accuracy stays constant but 30% of predictions flip when audio is added, what does this indicate about robustness?

- Concept: **Self-consistency decoding**
  - Why needed here: The only effective mitigation in the paper uses majority voting over multiple samples, but this has computational tradeoffs.
  - Quick check question: Why would aggregating multiple noisy generations produce more stable outputs than a single greedy decode?

## Architecture Onboarding

- Component map: Audio Encoder -> Modality Adapter -> Backbone LLM
- Critical path:
  - Audio enters encoder → adapter projects to LLM dimension → audio tokens prepended/appended to text tokens → LLM processes unified sequence → output distribution shaped by both modalities
  - Interference occurs at the unified sequence level: audio tokens consume context window capacity and attention bandwidth regardless of information content
- Design tradeoffs:
  - Larger models (7B vs. 3B) show greater resilience but remain vulnerable
  - Self-consistency (8 samples at T=0.5) reduces influence rate by ~50% but increases inference cost 8x
  - Prompting-based mitigation ("Focus on useful information") showed inconsistent effects and sometimes increased instability
- Failure signatures:
  - Accuracy drop of 1-5% on text benchmarks when any audio is present
  - Influence rates of 10-35% indicating frequent prediction flips
  - Silence and Gaussian noise produce nearly equivalent disruption
  - Performance degradation scales with audio duration (1s → 30s), amplitude (-60 → -20 dBFS), and temperature (0.0 → 1.0)
- First 3 experiments:
  1. Replicate the silence interference experiment on GSM8K with a single model (e.g., Qwen2.5-Omni-3B) at durations 0, 5, 10 seconds; measure both accuracy and influence rate.
  2. Test whether audio token masking (zeroing adapter outputs) restores text-only performance, confirming the adapter-fusion pathway as the interference locus.
  3. Implement a lightweight audio relevance classifier before the adapter to gate non-informative audio; compare influence rates against the baseline without gating.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What architectural modifications to the modality fusion mechanism could enable LALMs to selectively disregard irrelevant audio while preserving useful audio information?
- Basis in paper: [explicit] The authors conclude by "highlighting the need for efficient fusion strategies that preserve reasoning performance in the presence of irrelevant inputs."
- Why unresolved: The paper identifies cross-modal interference as a problem but only tests inference-time mitigations (prompting, self-consistency), not architectural solutions.
- What evidence would resolve it: Demonstrating a modified LALM architecture that maintains text-reasoning accuracy under irrelevant audio without increased inference cost.

### Open Question 2
- Question: Why does silence destabilize model outputs as strongly as synthetic noise, given that silence contains no semantic or acoustic information?
- Basis in paper: [inferred] The paper states "silence, often assumed neutral, destabilizes outputs as strongly as synthetic noise" and treats silence and noise as "nearly equivalent," but does not investigate the underlying cause.
- Why unresolved: The authors report the empirical finding but do not analyze whether the issue stems from audio encoder behavior, attention mechanisms, or feature-space representations.
- What evidence would resolve it: Probing experiments identifying whether silence induces specific activation patterns or attention shifts comparable to noise in the audio encoder or fusion layers.

### Open Question 3
- Question: Can training-time interventions (e.g., data augmentation with irrelevant audio, adversarial training, or regularization) improve robustness to cross-modal interference?
- Basis in paper: [inferred] The paper notes that "factors beyond parameter count, such as training data and optimization design, may influence robustness," but all mitigation experiments are inference-time only.
- Why unresolved: The study does not explore whether models can learn to ignore irrelevant audio during training rather than requiring costly inference strategies like self-consistency.
- What evidence would resolve it: Training LALMs with explicit irrelevant-audio augmentation and evaluating whether they generalize to novel interference types without performance loss.

### Open Question 4
- Question: Does the cross-modal interference phenomenon observed in audio-language models generalize to other paired modalities such as vision-language systems?
- Basis in paper: [inferred] The paper situates the work alongside studies on "distractions" in vision-language models but does not test whether similar interference occurs when irrelevant images accompany text-only tasks.
- Why unresolved: Determining whether interference is a modality-specific issue or a general multimodal fusion problem could inform whether solutions should target audio specifically or multimodal architectures broadly.
- What evidence would resolve it: Applying the same experimental paradigm (irrelevant visual inputs during text reasoning) to VLMs and comparing interference patterns.

## Limitations
- Architecture specificity: All tested models use adapter-based fusion; results may not generalize to alternative multimodal architectures
- Input distribution mismatch: Interference tested on text-only tasks with artificially added audio, not naturally co-occurring audio-text pairs
- Noise quantification ambiguity: Silence and noise equivalence based on behavioral outcomes rather than acoustic information metrics
- Mitigation efficacy uncertainty: Self-consistency shows improvement but at prohibitive computational cost without analyzing the mechanism

## Confidence

- **Cross-modal interference mechanism** (High): Multiple ablation studies consistently show that irrelevant audio reduces accuracy and increases prediction volatility across different models, durations, and noise types.
- **Silence equivalence to noise** (Medium): The behavioral equivalence is demonstrated, but the acoustic and information-theoretic reasons remain unexplored.
- **Larger models show greater resilience** (Medium): Observed trend across model pairs but with limited sample size (only Qwen2.5-Omni-3B vs 7B directly compared).
- **Self-consistency as effective mitigation** (Low-Medium): Shows improvement but at prohibitive computational cost, and the mechanism of improvement is not analyzed.

## Next Checks

1. **Architecture ablation test**: Implement and evaluate a simple audio relevance gate before the adapter (e.g., classify audio as informative vs. non-informative using a lightweight classifier). If this eliminates the interference effects, it confirms the fusion layer as the vulnerability locus. If interference persists, the vulnerability may lie in the LLM backbone's handling of audio tokens.

2. **Information-theoretic analysis**: Measure the entropy and information content of silence, Gaussian noise, and FSD50K samples. Compare these metrics against the magnitude of interference observed. This would test whether interference correlates with acoustic information content or primarily with the model's inability to gate irrelevant signals.

3. **Real-world co-occurrence evaluation**: Test the same interference paradigm on naturally audio-text paired datasets (e.g., instructional videos with spoken descriptions) where models were trained on such combinations. This would validate whether the observed vulnerabilities represent fundamental architectural limitations or artifacts of testing on out-of-distribution scenarios.