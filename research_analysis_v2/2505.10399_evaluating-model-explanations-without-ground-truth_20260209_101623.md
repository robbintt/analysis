---
ver: rpa2
title: Evaluating Model Explanations without Ground Truth
arxiv_id: '2505.10399'
source_url: https://arxiv.org/abs/2505.10399
tags:
- explanations
- explanation
- evaluation
- feature
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating model explanations
  in the absence of ground truth, a critical problem in explainable AI (XAI). Current
  evaluation frameworks rely on ground-truth comparisons or model sensitivity, which
  are limited and often lead to inconsistent or misleading results.
---

# Evaluating Model Explanations without Ground Truth

## Quick Facts
- arXiv ID: 2505.10399
- Source URL: https://arxiv.org/abs/2505.10399
- Reference count: 40
- Primary result: AXE framework achieves 100% detection of manipulated explanations while sensitivity-based metrics fail 50% of the time

## Executive Summary
This paper addresses the critical challenge of evaluating model explanations in explainable AI without ground truth. Current evaluation methods rely on ground-truth comparisons or model sensitivity, leading to inconsistent results and vulnerability to adversarial manipulation. The authors propose AXE (ground-truth Agnostic eXplanation Evaluation), which measures explanation quality by how well the top-n important features alone recover model predictions using k-NN models trained on the original data. This approach avoids reliance on off-manifold behavior and ensures independence from ground-truth explanations.

The framework demonstrates effectiveness in detecting explanation fairwashing, where adversarial models manipulate explanations to hide discriminatory behavior. AXE successfully identifies manipulated explanations with 100% accuracy, significantly outperforming sensitivity-based metrics like PGI and PGU, which fail half the time. The approach is validated across real-world datasets including German Credit, COMPAS, Adult Income, and HELOC, showing robustness and consistency across different explainers and models.

## Method Summary
AXE evaluates explanation quality by measuring how well the top-n important features identified by an explanation can recover model predictions. For each datapoint, a unique k-NN model is trained using only the top-n features from the explanation, with the target being the model's own predictions rather than ground-truth labels. The quality score is the accuracy of these k-NN models in reconstructing the original model's behavior. AXE operates entirely on observed data points, avoiding synthetic perturbations that leave the data manifold. The framework aggregates results using AUC across different values of n to produce a scalar quality score.

## Key Results
- AXE achieves 100% accuracy in detecting manipulated explanations in fairwashing experiments, while PGI and PGU fail 50% of the time
- The framework maintains consistent performance across different explainers (SHAP, LIME, gradient-based methods) and model types (logistic regression, neural networks)
- AXE successfully identifies that explanations from fairwashed models fail to recover true model behavior, even when traditional metrics cannot detect the manipulation
- The approach shows robustness across different values of k (number of neighbors) and n (number of top features considered)

## Why This Works (Mechanism)

### Mechanism 1: k-NN Predictiveness as Explanation Quality Proxy
- Claim: Explanation quality can be measured by how well the top-n important features alone recover model predictions.
- Mechanism: For each datapoint, train a k-NN model using only the top-n features identified by the explanation. If these features truly capture what drives the model, the k-NN should accurately reconstruct the original model's predictions (not ground-truth labels).
- Core assumption: Important features should separate model prediction classes in feature space; unimportant features should not.
- Evidence anchors:
  - [Abstract] "AXE... evaluates explanations based on their predictive accuracy in recovering model behavior using only the top-n most important features."
  - [Section 3.1] "AXE fits multiple k-NN models M_k^i to predict model outputs Y_preds, not data labels Y."
  - [Corpus] Related work (DeepFaith, arxiv:2508.03586) similarly notes "absence of a unified optimal explanation" and need for evaluation without ground truth, suggesting convergence on predictiveness-based evaluation.
- Break condition: If features are highly correlated, top-n selection may arbitrarily include redundant features, inflating quality scores regardless of true explanation faithfulness.

### Mechanism 2: On-Manifold Constraint Avoids Sensitivity Bias
- Claim: Evaluation metrics should not rely on synthetic perturbations that leave the data manifold.
- Mechanism: k-NN operates entirely on observed data points, avoiding off-manifold predictions that sensitivity-based methods (PGI, PGU) require. This prevents evaluation from being gamed by adversarial models that manipulate off-manifold behavior.
- Core assumption: Off-manifold behavior is irrelevant to real-world model behavior; only on-manifold predictions matter for explanation utility.
- Evidence anchors:
  - [Section 2.2] "When off-manifold model predictions m(x + δx) change, the evaluation metric q should remain unchanged for explanation e."
  - [Section 3.2] "AXE does not require model predictions on off-manifold data."
  - [Corpus] Related work on audio deepfake explanations (arxiv:2506.03425) similarly struggles with "lack of clear ground truth annotations" for explanation evaluation, supporting the need for manifold-aware approaches.
- Break condition: If the training data is sparse or does not adequately cover the decision boundary, k-NN neighborhoods may be too large, reducing discriminative power.

### Mechanism 3: Per-Datapoint Model Ensures Local Contextualization
- Claim: Each datapoint requires its own k-NN model trained on that explanation's specific top features.
- Mechanism: Rather than building a global surrogate, AXE trains a unique k-NN for each (datapoint, explanation) pair. This forces the quality score to vary when either the datapoint or the explanation changes.
- Core assumption: Local explanations should reflect local variations in model behavior; a single global explanation is insufficient.
- Evidence anchors:
  - [Section 3.1] "It is essential not to use the same k-NN model for all predictions... For each datapoint x_i, we use a unique k-NN model that considers the top-n most important features for that particular explanation e_i."
  - [Section 2.4] Demonstrates that ground-truth metrics violate local contextualization by comparing all explanations to a single static ground truth.
  - [Corpus] Weak direct evidence in corpus; this is a relatively novel contribution.
- Break condition: If explanations are highly consistent across datapoints, per-datapoint models add computational cost without quality discrimination benefit.

## Foundational Learning

- **Concept: Local vs. Global Explanations**
  - Why needed here: AXE specifically evaluates local feature-importance explanations (per-datapoint), not global model explanations. Confusing the two leads to misapplication.
  - Quick check question: Does your explanation change when you input a different datapoint? If not, it's global, and AXE may not apply directly.

- **Concept: k-Nearest Neighbors (k-NN) Classification**
  - Why needed here: AXE's core mechanism uses k-NN to measure feature separability. Understanding how k affects neighborhood size and prediction stability is critical for hyperparameter selection.
  - Quick check question: What happens to k-NN predictions when k is set very large relative to class sizes?

- **Concept: On-Manifold vs. Off-Manifold Data**
  - Why needed here: The paper's critique of sensitivity metrics hinges on their reliance on synthetic perturbations that leave the data manifold. Understanding manifold constraints is essential for interpreting AXE's advantages.
  - Quick check question: If you perturb a feature by adding Gaussian noise, how do you know the resulting point is still "on-manifold"?

## Architecture Onboarding

- **Component map:**
  Input: (Dataset X, Model m, Explainer E) → Generate explanations: E(X, m) → {e_i for each x_i} → For each (x_i, e_i): Extract top-n features from e_i → Filter X to these n features → X_f → Train k-NN M_k^i on X_f targeting m(X) predictions → Predict m(x_i) using M_k^i → ŷ_i → Aggregate: Compare {ŷ_i} vs. {m(x_i)} → Accuracy score

- **Critical path:**
  1. Explanation generation (any explainer: SHAP, LIME, gradients)
  2. Top-n feature extraction per datapoint
  3. Per-datapoint k-NN training on subset features
  4. Accuracy aggregation across dataset

- **Design tradeoffs:**
  - **n (top-n features):** Small n tests whether the explainer identifies the most critical features; large n dilutes discrimination. Paper uses AUC over n values as alternative.
  - **k (neighbors):** Small k increases variance/noise; large k smooths predictions but may blur local structure. Paper shows robustness across k ∈ {1, 3, 5, 9}.
  - **Caching:** Per-datapoint k-NN models can be cached; max cache size bounded by min(ν, C(N,n)) where ν = datapoints, N = features.

- **Failure signatures:**
  - Scores stuck near 0.5 (random chance) → Features not separating predictions; explanations are uninformative or model is unpredictable from features.
  - High scores for random explainer → Dataset has strong feature correlations; any subset predicts well.
  - Large variance across k values → Unstable neighborhoods, possibly sparse data.

- **First 3 experiments:**
  1. **Sanity check:** Run AXE on a synthetic dataset where ground-truth important features are known (e.g., Figure 4 setup). Verify that explanations correctly identifying these features score higher than explanations highlighting irrelevant features.
  2. **Fairwashing detection:** Replicate the adversarial attack experiment (Section 4.1) on German Credit or COMPAS. Compare AXE vs. PGI/PGU on detecting manipulated explanations.
  3. **Hyperparameter sensitivity:** Vary k ∈ {1, 3, 5, 9, 15} and n ∈ {1, 3, 5, N} on a real dataset. Assess score stability and rank correlation across configurations.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does AXE perform on non-tabular data modalities such as images, text, and time-series data?
  - Basis in paper: [explicit] The authors state they "focus exclusively on local feature-importance explanations for models operating on tabular datasets" and do not evaluate other modalities.
  - Why unresolved: The k-NN approach in AXE relies on feature separability in structured feature spaces, but images and text have different notions of "features" (pixels, tokens) and neighborhood structures that may not translate directly.
  - What evidence would resolve it: Experiments applying AXE to image classification and NLP tasks, potentially with adapted distance metrics or feature definitions.

- **Open Question 2:** What is the relationship between AXE scores and actual human ability to predict or trust model behavior in real-world settings?
  - Basis in paper: [inferred] While AXE is motivated by user studies showing useful explanations help users predict model behavior, the paper does not include human evaluation to confirm AXE scores correlate with human understanding.
  - Why unresolved: AXE operationalizes "predictiveness" algorithmically via k-NN accuracy, but this may not fully capture human cognitive processes or trust calibration.
  - What evidence would resolve it: User studies comparing AXE-ranked explanations against human performance on model prediction tasks and trust assessments.

- **Open Question 3:** How robust is AXE to high-dimensional feature spaces and highly correlated features?
  - Basis in paper: [inferred] AXE relies on k-NN models which suffer from the curse of dimensionality; the experiments use datasets with relatively few features (COMPAS, German Credit have limited dimensions).
  - Why unresolved: In high-dimensional spaces, k-NN distances become less meaningful, potentially making AXE scores unreliable. Correlated features may also create ambiguity in which "important" features are truly predictive.
  - What evidence would resolve it: Experiments on datasets with hundreds or thousands of features, analyzing AXE behavior under controlled feature correlation structures.

## Limitations

- The framework's effectiveness relies on having sufficient data coverage of the decision boundary; sparse data may reduce discriminative power
- High-dimensional feature spaces may suffer from the curse of dimensionality, affecting k-NN performance and explanation evaluation
- The approach focuses exclusively on tabular data and local feature-importance explanations, limiting applicability to other data modalities
- AXE does not directly evaluate whether explanations are comprehensible to humans or align with human understanding of the model

## Confidence

- AXE framework mechanics: High confidence - the methodology is clearly specified and reproducible
- Fairwashing detection effectiveness: High confidence - 100% detection rate shown with clear experimental setup
- General superiority over PGI/PGU: Medium confidence - based primarily on synthetic experiments
- Real-world robustness: Medium confidence - demonstrated on multiple datasets but limited adversarial scenarios

## Next Checks

1. Test AXE against adversarial attacks specifically targeting the k-NN predictiveness mechanism, such as explanations that optimize for k-NN separability while hiding discriminatory patterns
2. Evaluate AXE on additional real-world datasets with known fairness issues beyond the German Credit and COMPAS datasets used
3. Compare AXE's computational efficiency with ground-truth-based metrics across large-scale datasets to assess practical deployment considerations