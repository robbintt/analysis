---
ver: rpa2
title: Large Language Models for Water Distribution Systems Modeling and Decision-Making
arxiv_id: '2503.16191'
source_url: https://arxiv.org/abs/2503.16191
tags:
- water
- code
- language
- queries
- epanet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LLM-EPANET, a framework enabling natural
  language interaction with water distribution system (WDS) simulation models using
  large language models (LLMs). The framework converts natural language queries into
  executable Python code for EPANET hydraulic and water quality simulations through
  a retrieval-augmented generation pipeline with vector embeddings.
---

# Large Language Models for Water Distribution Systems Modeling and Decision-Making

## Quick Facts
- arXiv ID: 2503.16191
- Source URL: https://arxiv.org/abs/2503.16191
- Reference count: 6
- Key outcome: Framework converts natural language queries into executable Python code for EPANET hydraulic and water quality simulations through retrieval-augmented generation pipeline

## Executive Summary
This paper introduces LLM-EPANET, a framework enabling natural language interaction with water distribution system simulation models using large language models. The system converts user queries into executable Python code through a retrieval-augmented generation pipeline that grounds LLM responses in EPANET documentation. Evaluated across four query categories on benchmark networks, the framework achieved 100% accuracy on static queries and strong performance on hydraulic and water quality queries, while struggling with complex scenario-based modifications.

## Method Summary
The framework implements a retrieval-augmented generation pipeline where user queries are embedded and matched against pre-indexed EPANET documentation using vector similarity. The retrieved function signatures and descriptions provide context for an LLM to generate executable Python code. The system includes iterative error correction through traceback feedback, with up to 5 retry attempts for failed code execution. Four configurations were tested combining basic vs. complex prompts with 0 vs. 5 retries, using benchmark networks Net1, Net3, and L-Town with curated queries across static, hydraulic, water quality, and scenario-based categories.

## Key Results
- Most complex configuration achieved 100% accuracy on static queries and 86% on hydraulic queries
- Simple prompts outperformed complex prompts for scenario-based queries, indicating prompt complexity can hinder multi-step reasoning
- Iterative error correction with 5 retries significantly improved success rates for challenging query categories
- Framework successfully democratizes WDS modeling by enabling non-technical users to interact with complex simulation tools through natural language

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-Augmented Generation (RAG) enables domain-specific code generation by grounding LLM responses in external API documentation
- Mechanism: User queries are embedded into vectors and matched against pre-indexed EPyT documentation via similarity search. Retrieved function signatures and descriptions provide context for the LLM to generate executable Python code
- Core assumption: Semantic similarity between query embeddings and documentation embeddings correlates with functional relevance for code generation
- Evidence anchors:
  - [abstract]: "framework converts natural language queries into executable Python code... through a retrieval-augmented generation pipeline with vector embeddings"
  - [section]: "RAG is a method that combines the generative capabilities of LLMs with an external search system that retrieves relevant information"
  - [corpus]: WaterCopilot similarly uses AI assistants for water management but lacks explicit RAG architecture comparison

### Mechanism 2
- Claim: Iterative error correction with traceback feedback improves code execution success rates
- Mechanism: When generated code fails, the error traceback is fed back to the LLM with a structured prompt. The LLM rewrites the function to address the specific error
- Core assumption: LLMs can diagnose and fix code errors from traceback information alone, and errors are recoverable within few iterations
- Evidence anchors:
  - [section]: "In the case of failure due to an error, the generated function definition and evaluation script, along with the error traceback, are sent back to the LLM"
  - [section]: Figure 3 shows 0% success on Hydraulics Scenario queries with 0 retries vs. ~50-70% with 5 retries (simple prompt)
  - [corpus]: No direct corpus evidence for iterative error correction in WDS systems

### Mechanism 3
- Claim: Prompt complexity differentially affects query categories—complex prompts help simpler tasks but can hinder complex scenario-based queries
- Mechanism: Complex prompts include domain-specific guidance (e.g., distinguishing element ID vs. index in EPANET). This reduces ambiguity for straightforward queries but may overwhelm the LLM when multiple reasoning steps are required for scenario modifications
- Core assumption: There is an optimal information load for LLMs; beyond this, additional context becomes noise rather than signal
- Evidence anchors:
  - [section]: "Interestingly, in the most challenging category, a simple prompt performs better than a complex one. This result points out the disadvantage of providing too much information to an LLM"
  - [section]: Static queries achieved 100% accuracy with complex prompt + 5 retries; Hydraulics Scenario showed inverse relationship
  - [corpus]: No corpus papers test prompt complexity effects in infrastructure domains

## Foundational Learning

- Concept: **EPANET and EPyT fundamentals**
  - Why needed here: The framework generates code against the EPyT API; understanding hydraulic/water quality simulations and the distinction between element IDs (user-facing) and indices (internal) is critical for debugging generated code
  - Quick check question: Can you explain why EPANET requires converting element IDs to indices before setting simulation parameters?

- Concept: **Vector embeddings and semantic similarity**
  - Why needed here: The RAG pipeline relies on embedding queries and documentation into the same vector space; understanding embedding quality, dimensionality, and similarity metrics (cosine, Euclidean) affects retrieval performance
  - Quick check question: Given a query "maximum pressure in network," what documentation chunks should have high similarity scores?

- Concept: **Prompt engineering for code generation**
  - Why needed here: The paper demonstrates that prompt design (basic vs. complex) directly impacts accuracy across query categories; understanding how to structure context, examples, and constraints is essential
  - Quick check question: What specific domain knowledge (like ID vs. index distinction) should be included in a complex prompt vs. learned through retrieval?

## Architecture Onboarding

- Component map:
  - Preprocessing pipeline: EPyT documentation extractor → Function signature/description parser → Embedding model → Vector database (FAISS)
  - Runtime pipeline: User query → Query embedder → Top-k retriever → Prompt constructor → LLM (code generator) → Smaller LLM (evaluation code) → Sandboxed executor → Error handler (optional retry loop)
  - EPANET layer: .inp file → EPyT wrapper → Hydraulic/water quality solver → Results object

- Critical path:
  1. Vector database quality determines retrieval relevance
  2. Retrieved context + prompt design determine code generation quality
  3. Retry configuration determines error recovery capability
  4. Sandboxed execution ensures safety but adds latency

- Design tradeoffs:
  - Complex vs. basic prompts: Complex improves static/hydraulic queries but degrades scenario queries. Recommendation: adaptive prompt selection based on query classification
  - Retry count (0 vs. 5): 5 retries significantly improves scenario query success but increases latency. Zero retries acceptable for static queries
  - Top-k retrieval: Not explicitly tuned in paper; assumption: higher k provides more context but may introduce noise
  - LLM size: Main LLM for generation, smaller LLM for evaluation code—trades cost vs. capability

- Failure signatures:
  - Hallucinated functions: Generated code references non-existent EPyT methods → indicates retrieval failure or insufficient documentation coverage
  - ID/index confusion: Code uses element IDs where indices required → indicates prompt guidance insufficient
  - Unrecovered errors: Retry loop exhausts without success → indicates fundamental misunderstanding or missing API functionality
  - Wrong output format: Returns tuple (1, 3) when user expects single value → indicates ambiguous query or missing format specification

- First 3 experiments:
  1. Reproduce baseline configurations: Implement all four configurations (basic/complex × 0/5 retries) on Net1 network queries. Verify static query 100% accuracy and quantify scenario query degradation with complex prompts
  2. Retrieval quality audit: For each query category, log top-k retrieved functions and measure precision@k. Identify queries where irrelevant retrievals led to code failures
  3. Adaptive prompt experiment: Classify incoming queries by category (using simple heuristic or classifier) and route to basic vs. complex prompts accordingly. Hypothesis: Adaptive routing should improve scenario query accuracy while maintaining static/hydraulic performance

## Open Questions the Paper Calls Out

- Question: How can the framework be modified to provide users with greater control over the "inner thought process" of the system to enhance explainability and trust?
  - Basis in paper: [explicit] The conclusion states that future development should direct thoughts toward "explainability and robustness and offer more control for the user over the inner thought process."
  - Why unresolved: The current implementation focuses on accuracy and code generation execution rather than transparent reasoning or user-guided debugging
  - What evidence would resolve it: A user study demonstrating that new interface features allow engineers to verify, correct, or understand the logic behind generated code snippets

- Question: Why does the "complex prompt" configuration degrade performance for "Hydraulics Scenario" queries compared to simple prompts?
  - Basis in paper: [inferred] The results show simple prompts outperform complex ones in the hardest category, which the authors suggest points to the "disadvantage of providing too much information."
  - Why unresolved: The paper identifies the phenomenon but does not isolate which specific "tips" or context lengths cause the performance drop
  - What evidence would resolve it: An ablation study testing individual prompt components on scenario-based queries to identify which instructions confuse the model

- Question: Can the LLM-EPANET framework maintain high accuracy when applied to proprietary, large-scale utility networks not represented in standard EPyT documentation?
  - Basis in paper: [inferred] The introduction notes the challenge that "some of these data and models are utility private and the commercial LLM engines never seen," yet experiments used only standard benchmark networks
  - Why unresolved: The evaluation was limited to Net1, Net3, and L-Town, which are public and likely present in training data
  - What evidence would resolve it: Performance metrics derived from testing the framework on private, real-world water distribution models with custom attributes

## Limitations

- Scenario-based query performance significantly degraded with complex prompts, indicating fundamental limitations in multi-step reasoning and context integration
- Framework evaluation limited to public benchmark networks (Net1, Net3, L-Town), raising questions about performance on proprietary, large-scale utility networks
- Complex prompts improved static/hydraulic query accuracy but caused inverse performance effects on the most challenging query category

## Confidence

- Static query accuracy (100%): **High** - Multiple configurations achieve perfect accuracy with clear mechanism (retrieval provides precise function signatures)
- Hydraulic query accuracy (86%): **Medium** - Strong performance but depends on successful context integration and correct ID/index mapping
- Quality query accuracy (83%): **Medium** - Similar to hydraulic but requires additional domain knowledge about water quality modeling
- Scenario query accuracy (varying): **Low** - Complex prompts degrade performance; indicates fundamental limitation in multi-step reasoning and context management

## Next Checks

1. Adaptive prompt routing experiment: Implement a simple classifier to categorize incoming queries and route them to basic vs. complex prompts based on observed performance patterns. Measure whether this recovers scenario query performance while maintaining static/hydraulic accuracy.

2. Retrieval quality audit: For each failed query, log the top-5 retrieved functions and their similarity scores. Calculate precision@k for each query category to identify whether failures stem from poor retrieval relevance or insufficient prompt guidance.

3. Multi-step reasoning stress test: Design scenario queries requiring chained API calls (e.g., "increase pipe diameter, then recalculate pressure at node X"). Test whether the framework can maintain context across multiple operations or requires separate queries for each step.