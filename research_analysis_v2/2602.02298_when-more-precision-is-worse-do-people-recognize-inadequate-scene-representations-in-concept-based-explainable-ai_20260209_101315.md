---
ver: rpa2
title: 'When more precision is worse: Do people recognize inadequate scene representations
  in concept-based explainable AI?'
arxiv_id: '2602.02298'
source_url: https://arxiv.org/abs/2602.02298
tags:
- features
- images
- when
- participants
- same
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigated whether people can recognize when AI relies
  on irrelevant features, using a railway safety scenario where participants evaluated
  a simulated AI's classification of images as dangerous or not. The AI explained
  its decisions using concept images that varied in three relevant features (distance
  to tracks, direction, action) and one irrelevant feature (scene background).
---

# When more precision is worse: Do people recognize inadequate scene representations in concept-based explainable AI?

## Quick Facts
- arXiv ID: 2602.02298
- Source URL: https://arxiv.org/abs/2602.02298
- Reference count: 6
- 67.8% of participants showed no preference for variation in irrelevant background features when evaluating AI explanations

## Executive Summary
This study investigates whether people can recognize when AI systems rely on irrelevant features through concept-based explainable AI (XAI) explanations. Using a railway safety scenario, participants evaluated simulated AI classifications of dangerous vs. non-dangerous scenes, with explanations showing concept images that varied in relevant features (distance, direction, action) and an irrelevant background feature. The findings reveal that participants preferred consistency in relevant features but showed no overall preference for variation in irrelevant features, with 67.8% showing no preference, 23.7% preferring sameness, and only 8.5% preferring variation. This suggests people may not recognize when AI relies on irrelevant features, potentially failing to detect overfitting or shortcut learning in concept-based XAI.

## Method Summary
The study employed a within-subjects design with 59 participants (ages 19-61, M=28.1, 41 female/18 male) who evaluated 128 trials of simulated AI classifications. Each trial presented a classified image with AI decision followed by 5 concept images showing the AI's internal representation. The experiment manipulated four features (distance, direction, action, background) in a 2×2×2×2 factorial design where each could be presented as same or varied across concept images. Participants first indicated whether they agreed with the AI's classification, then rated the AI's performance on a 0-100 scale. Analysis used repeated-measures ANOVA with agreement as a covariate, and Bonferroni correction for multiple comparisons.

## Key Results
- Participants preferred same over varied images for all three relevant features (distance, direction, action)
- No overall preference for sameness vs. variation in the irrelevant background feature
- 67.8% of participants showed no preference, 23.7% preferred sameness, only 8.5% preferred variation for background
- Preference for background sameness increased when relevant features were inconsistent
- Feature relevance hierarchy: distance > direction > action > background

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Participants substitute evaluating AI internal representations with evaluating feature relevance to the target concept
- Mechanism: When rating AI performance, participants ignore whether the AI has properly generalized over irrelevant features, instead judging whether displayed features are relevant to danger. Irrelevant features are filtered out rather than triggering concern about overfitting or shortcut learning
- Core assumption: Substitution bias operates even with explicit instructions about robustness risks
- Evidence anchors:
  - [abstract] "Participants rated the AI more favorably when it retained relevant features. For the irrelevant feature, they did not mind in general"
  - [section: Discussion] "Instead of rating the AI's internal representations, they might simply rate the relevance of the presented features... The tendency to exchange a complex question for a simpler one is referred to as substitution bias"

### Mechanism 2
- Claim: Sameness preference scales with perceived feature relevance but does not reverse for irrelevant features
- Mechanism: Humans apply a "consistency = quality" heuristic that modulates with feature importance. Distance shows largest sameness preference, direction medium, action smallest, while background shows no reversal—variation is never rewarded
- Core assumption: Feature relevance perceptions are stable and align with task demands
- Evidence anchors:
  - [abstract] "Participants preferred sameness for relevant features but showed no overall preference for sameness versus variation in the irrelevant background feature"

### Mechanism 3
- Claim: Background sameness preference emerges as compensation when relevant features are inconsistent
- Mechanism: When relational features vary incorrectly, participants prefer at least some consistency in the scene, suggesting a "minimum match" threshold—AI should get something right
- Core assumption: Background consistency is interpreted as evidence the AI recognizes overall scene context
- Evidence anchors:
  - [abstract] "67.8% of participants showed no preference, 23.7% preferred sameness, and only 8.5% preferred variation"
  - [section: Results] "Participants preferred consistency in the irrelevant feature when the most relevant feature was inconsistent"

## Foundational Learning

- Concept: Overfitting vs. shortcut learning
  - Why needed here: Understanding the paper's core concern requires distinguishing AI using irrelevant features in addition to relevant ones (overfitting) vs. instead of relevant ones (shortcut learning). Both produce non-robust representations
  - Quick check question: If an AI classifies railway images using background location, is this overfitting or shortcut learning?

- Concept: Concept-based XAI via activation matching
  - Why needed here: The explanation method shows dataset images that activate similar neurons. Interpreting feature consistency requires understanding this mechanism
  - Quick check question: If concept images show varied backgrounds, what does this indicate about whether the AI uses background?

- Concept: Substitution bias (attribute substitution)
  - Why needed here: The proposed mechanism for why people fail to detect irrelevant features relies on this cognitive bias framework
  - Quick check question: What complex question might participants substitute with "is this feature relevant to danger?"

## Architecture Onboarding

- Component map: Stimuli generation (128 trials) -> Trial presentation (decision screen → rating screen) -> Data collection (59 participants) -> Analysis (ANOVA with agreement covariate)

- Critical path:
  1. Generate stimuli with controlled feature manipulation (same vs. varied across 5 concept images)
  2. Collect agreement judgments (correct/incorrect AI decision)
  3. Collect performance ratings (0-100 scale)
  4. Analyze same-vs-varied effects per feature, including higher-order interactions

- Design tradeoffs:
  - Simulated AI vs. real AI: Higher internal validity (exact control over features) but lower ecological validity
  - Static images vs. dynamic scenes: Simplifies feature isolation but reduces realism
  - Novice participants vs. experts: Tests general population but may not generalize to practitioners

- Failure signatures:
  - If participants simply ignore background entirely → null effect (observed: 67.8% showed <5 point difference)
  - If participants prefer variation → negative main effect (not observed)
  - If instructions override bias → background variation rewarded (not observed despite explicit instruction about robustness)

- First 3 experiments:
  1. Replication with extreme overspecification (same person identity across all images) to test whether a threshold exists where participants recognize problematic precision
  2. Contrast generalization (random variation) vs. systematic mistake (consistent wrong feature) to isolate whether imprecision type matters
  3. Expert participant comparison (AI researchers or railway safety professionals) to test whether domain knowledge enables detection of irrelevant feature reliance

## Open Questions the Paper Calls Out

- What degree of similarity in concept images triggers a shift from a preference for precision to a recognition that variation is desirable?
  - The authors state, "Future studies should investigate how hard you need to push the boundaries of similarity in concept images before participants realize that variation is desirable."

- Do people distinguish between generalizations (random variation) and systematic mistakes (consistent wrong feature) in concept-based explanations?
  - The authors propose to "contrast generalizations and systematic mistakes for features of high and low relevance."

- Can participants detect inadequate representations when irrelevant features are abstractly similar (e.g., lighting, terrain) rather than identical?
  - The authors note that realistic biases are harder to detect because "the exact scene never repeats," making it "challenging to even notice the undesirable similarities."

## Limitations

- Simulated AI rather than real systems limits ecological validity and may not capture authentic XAI behavior
- Static image format simplifies feature isolation but reduces realism compared to dynamic real-world scenarios
- The specific railway safety context may not generalize to other domains or types of AI explanations

## Confidence

- High confidence: The primary finding that participants show no preference for variation in irrelevant features is well-supported by experimental data
- Medium confidence: The feature relevance hierarchy (distance > direction > action > background) is empirically supported but may not generalize beyond this specific scenario
- Low confidence: The cognitive mechanisms (substitution bias, compensatory consistency seeking) are theoretically plausible but require further validation

## Next Checks

1. Replicate with real AI systems to test whether substitution bias persists with authentic explanations rather than simulated ones
2. Test whether explicit training on distinguishing feature relevance from representation quality reduces the failure to detect irrelevant feature reliance
3. Examine expert participants (AI researchers, domain specialists) to determine if professional knowledge enables detection of shortcut learning that novices miss