---
ver: rpa2
title: 'Gather and Trace: Rethinking Video TextVQA from an Instance-oriented Perspective'
arxiv_id: '2508.04197'
source_url: https://arxiv.org/abs/2508.04197
tags:
- video
- text
- instance
- textvqa
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GAT, a novel instance-oriented approach for
  Video TextVQA that addresses the limitations of frame-level frameworks. The core
  idea is to treat video text instances as fundamental units, integrating context
  across the entire video sequence and explicitly modeling their spatio-temporal trajectories.
---

# Gather and Trace: Rethinking Video TextVQA from an Instance-oriented Perspective

## Quick Facts
- arXiv ID: 2508.04197
- Source URL: https://arxiv.org/abs/2508.04197
- Reference count: 40
- Achieves 3.86% higher accuracy than previous state-of-the-art Video TextVQA methods

## Executive Summary
This paper introduces GAT, an instance-oriented framework for Video TextVQA that treats video text instances as fundamental units rather than processing frame-level data independently. The method integrates multimodal context across entire video sequences and explicitly models spatio-temporal trajectories of text instances. GAT outperforms existing Video TextVQA approaches, video-language pretraining methods, and video large language models on both accuracy and efficiency metrics. Notably, it achieves 3.86% higher accuracy while being ten times faster than video large language models.

## Method Summary
GAT uses a two-stage pipeline: (1) Context-aggregated Instance Gathering employs a lightweight Transformer encoder-decoder to unify visual appearance, layout characteristics, and textual contents for each text instance across all frames where it appears, with an auxiliary loss to filter unreliable frame-level predictions; (2) Instance-focused Trajectory Tracing uses a T5 backbone with trajectory-aware attention bias that injects spatio-temporal distance into attention scores, forcing the model to weight interactions based on physical proximity and motion. The approach reduces input tokens from an average of 619 to 47 while maintaining accuracy.

## Key Results
- Achieves 3.86% higher accuracy than previous state-of-the-art Video TextVQA methods
- Reduces redundant input tokens from average 619 to 47, enabling 10x faster inference than video large language models
- Demonstrates strong generalization across different video domains while maintaining efficiency

## Why This Works (Mechanism)

### Mechanism 1: Context-Aggregated Instance Gathering
Aggregating multimodal features across the entire video timeline for a specific text instance yields more accurate textual representation than single-frame spotting. The system uses a lightweight Transformer encoder-decoder to fuse visual appearance, layout coordinates, and recognized text from all frames where an instance appears, employing an auxiliary loss to identify and downweight "unclear" or "incomplete" frame-level predictions.

### Mechanism 2: Trajectory-Aware Attention Bias
Explicitly injecting spatio-temporal distance as an attention bias improves relational reasoning compared to implicit positional embeddings. Instead of standard absolute positions, the model calculates a "trajectory distance" between instances based on their relative spatial offset at their temporal intersection point, encoded into a trajectory embedding and added to the attention logits.

### Mechanism 3: Instance-Oriented Token Reduction
Reducing input tokens by treating video text as unique instances rather than frame-level entities preserves accuracy while significantly improving efficiency. By consolidating redundant detections of the same text across hundreds of frames into a single "instance" token, the model reduces computational load without losing temporal context.

## Foundational Learning

- **Video Text Spotting (VTS) & Tracking**: The GAT architecture relies on an external VTS model to extract and track text instances. Understanding that an "instance" implies a consistent ID across frames is critical. Quick check: If VTS loses track of a word for 3 frames and re-initializes with a new ID, the Gathering module will create two separate incomplete instances.

- **Attention Bias / Relative Positional Encoding**: GAT modifies standard self-attention by adding a trajectory-derived bias. You need to distinguish between learning a positional embedding vs. computing a deterministic bias to add to $QK^T$. Quick check: In Eq. (4), does `trajEmbed` involve learnable parameters, or is it a fixed geometric calculation? (Answer: It is a fixed calculation based on sin/cos functions of spatial distance).

- **Transformer Encoder-Decoder Architectures (T5)**: The final reasoning stage uses a T5 backbone. The "Gathering" module also uses a lightweight Encoder-Decoder. Understanding the flow from encoding multimodal context to auto-regressive text generation is essential. Quick check: Why is the "Context-aggregated Instance Gathering" module trained with a recognition loss ($L_{rec}$) separate from the main VQA loss? (Answer: It acts as a refinement/text-only pre-processing stage to clean up OCR noise).

## Architecture Onboarding

- **Component map**: Input (Video Frames + Question) -> VTS Backbone (GoMatching) -> Instance Gathering (Lightweight Transformer) -> Trajectory Tracing (T5 with Trajectory Attention) -> Output (Answer via T5 Decoder)

- **Critical path**: The Instance Gathering module is the highest risk. If the unified text representation is wrong (e.g., "DROGBA" merged as "DROG"), the downstream reasoning cannot recover. The auxiliary loss ($L_{aux}$) determining frame clarity is the key filter here.

- **Design tradeoffs**: The primary tradeoff is Granularity vs. Efficiency. By collapsing frame-level data into instances, the model gains 10x speed but risks losing frame-specific visual attributes (color, font style in a specific moment). This architecture is optimal for "reading" tasks but potentially suboptimal for "visual description" tasks.

- **Failure signatures**:
  - Hallucinated Concatenation: If tracking fails and merges two different text lines, the Gathering module outputs gibberish (e.g., "EXIT" + "NOW" → "EXITNOW")
  - Temporal Smearing: If the Trajectory Embedding over-weights past positions, the model might answer based on text that has already left the screen

- **First 3 experiments**:
  1. VTS Sensitivity Test: Run pipeline with lower-quality VTS model (or introduce artificial ID switches) to quantify performance drop
  2. Token Length Ablation: Force model to keep all frame-level tokens vs. instance-level tokens to verify efficiency/accuracy tradeoff
  3. Trajectory Bias Visualization: Visualize attention maps to confirm it is attending to physically proximal instances rather than random semantic associations

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several critical uncertainties remain regarding VTS dependency, computational overhead of gathering module, and handling of temporally disjoint instances.

## Limitations

- Performance heavily depends on quality of upstream Video Text Spotting (VTS) outputs; fragmented or failed tracking will corrupt unified representations
- The specific hyperparameters for trajectory embedding calculation and auxiliary loss weighting λ are not specified, leaving room for performance variation
- Does not evaluate scenarios requiring frame-specific visual information (e.g., text color in a specific frame), potentially limiting applicability for certain question types

## Confidence

- **High Confidence**: Efficiency claims regarding token reduction (619→47 tokens) and resulting 10x speed improvement are directly supported by quantitative results
- **Medium Confidence**: Accuracy improvements over state-of-the-art methods (3.86% gain) are well-documented, but trajectory-aware attention's specific contribution is inferred from final performance rather than isolated analysis
- **Medium Confidence**: Instance-oriented approach effectiveness is supported by results, but doesn't explicitly test scenarios requiring frame-specific visual details

## Next Checks

1. **VTS Robustness Test**: Run GAT with degraded VTS model (artificially introduce ID switches or tracking errors) and measure performance drop to quantify dependency on perfect upstream tracking

2. **Trajectory Attention Visualization**: Extract and visualize attention weight matrices from Trajectory Tracing module to confirm trajectory bias is forcing attention to spatially proximate instances rather than learned semantic associations

3. **Frame-Specific Attribute Ablation**: Create subset of questions requiring frame-specific visual information (e.g., "What color is the text in frame 12?") and test whether GAT's instance-level representation can still answer these correctly