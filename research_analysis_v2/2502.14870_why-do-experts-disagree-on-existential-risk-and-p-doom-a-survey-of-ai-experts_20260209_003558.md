---
ver: rpa2
title: Why do Experts Disagree on Existential Risk and P(doom)? A Survey of AI Experts
arxiv_id: '2502.14870'
source_url: https://arxiv.org/abs/2502.14870
tags:
- safety
- experts
- participants
- were
- concepts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey of 111 AI experts reveals two distinct worldviews
  on AI safety: the "AI as controllable tool" and "AI as uncontrollable agent" perspectives.
  While 78% agree technical AI researchers should be concerned about catastrophic
  risks, 63% are unfamiliar with "instrumental convergence" and 58% with "scalable
  oversight" - fundamental AI safety concepts.'
---

# Why do Experts Disagree on Existential Risk and P(doom)? A Survey of AI Experts

## Quick Facts
- arXiv ID: 2502.14870
- Source URL: https://arxiv.org/abs/2502.14870
- Authors: Severin Field
- Reference count: 21
- 78% agree technical AI researchers should be concerned about catastrophic risks

## Executive Summary
This survey of 111 AI experts reveals two distinct worldviews on AI safety: the "AI as controllable tool" and "AI as uncontrollable agent" perspectives. While 78% agree technical AI researchers should be concerned about catastrophic risks, 63% are unfamiliar with "instrumental convergence" and 58% with "scalable oversight" - fundamental AI safety concepts. The least concerned participants were also the least familiar with these concepts, suggesting unfamiliarity drives skepticism. The study found broad consensus on several points: AI researchers should consider catastrophic risks (77% agreement), safety work doesn't slow progress (83% agreement), and some AIs may deserve moral consideration (57% agreement). However, participants were divided on whether existing ML paradigms can produce AGI (27% agreement) and whether catastrophic risks are overstated (43% agreement). The findings indicate that effective AI safety communication should begin with establishing clear conceptual foundations in the field.

## Method Summary
The study surveyed 111 AI experts (66.3% academics, 16.3% industry, 9.3% AI safety researchers) recruited via email to a pool of 1090 potential participants (10% response rate). Participants had graduate-level ML coursework or 1+ year professional ML experience. The survey included familiarity ratings on 5-point Likert scale for 8 concepts (4 ML, 4 safety), 9 belief statements across priority, technical, and other concerns, and a randomized reading intervention with 4 conditions (Russell, Joy, Aschenbrener, WIRED control). Participants completed pre/post belief assessments to measure intervention effects.

## Key Results
- 63% unfamiliar with "instrumental convergence" and 58% with "scalable oversight" - core safety concepts
- Two distinct worldviews emerged: "AI as controllable tool" vs "AI as uncontrollable agent"
- 78% agree technical AI researchers should be concerned about catastrophic risks
- Safety researchers were most familiar with safety concepts and most concerned about risks
- Reading interventions produced only minor, non-significant shifts in opinion

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Familiarity with AI safety concepts correlates with higher concern about catastrophic risk.
- Mechanism: Participants unfamiliar with core safety concepts (instrumental convergence, scalable oversight) lack the conceptual frameworks to recognize specific threat models, leading them to default to "tool" metaphors and discount emergent risks.
- Core assumption: Conceptual exposure causally influences risk assessment (survey shows correlation; causation implied but not proven).
- Evidence anchors:
  - [abstract] "The least concerned participants were also the least familiar with concepts like this, suggesting that unfamiliarity drives skepticism."
  - [section 3.5] Familiarity with "instrumental convergence" negatively correlated with belief that "catastrophic risks are generally overstated" (R=-.23). Participants familiar with "scalable oversight" were less likely to agree "we can always just turn off the AI" (15% vs 41%).
  - [corpus] Related work on AI risk disagreement (e.g., "Why They Disagree") finds similar divides but doesn't test the knowledge-familiarity hypothesis directly.
- Break condition: If future interventions that successfully teach safety concepts fail to shift risk perceptions, this mechanism would be weakened.

### Mechanism 2
- Claim: Expert disagreement clusters into two coherent worldviews with correlated beliefs.
- Mechanism: Beliefs about AI controllability, timeline preferences, and risk salience form stable clusters rather than independent positions, suggesting deeper underlying mental models drive surface-level disagreements.
- Core assumption: The clustering reflects genuine cognitive patterns rather than survey artifact.
- Evidence anchors:
  - [section 3.2] "Two distinct world views exist among AI experts: the 'AI as a controllable tool' perspective and the 'AI as an uncontrollable agent' perspective."
  - [section 3.2, Figure 3] Correlation network shows tool-related beliefs (S4, S5, S6) positively correlated with each other and negatively with agent-related beliefs (S7, S9).
  - [corpus] "The Stories We Govern By" identifies competing sociotechnical imaginaries shaping governance, supporting the worldview-clustering hypothesis.
- Break condition: If larger samples show continuous rather than clustered belief distributions, the two-camp model would need revision.

### Mechanism 3
- Claim: Short reading interventions produce limited belief change regardless of argument type.
- Mechanism: Pre-existing worldviews are resistant to brief informational interventions; beliefs may be "crystallized" through professional experience and peer communities.
- Core assumption: The null finding reflects genuine resistance rather than weak intervention design.
- Evidence anchors:
  - [section 3.6] "After the intervention, there was a minor trend toward increased concern about AI risk... However, the data are not strong enough to definitively show that the interventions had a significant effect."
  - [section 4.2] "Even strongly contrasting materials elicited similar responses from participants."
  - [corpus] No directly comparable intervention studies in corpus; mechanism remains under-tested externally.
- Break condition: If longitudinal or more intensive interventions show substantial attitude shifts, the "resistant worldview" mechanism would be qualified.

## Foundational Learning

- Concept: Instrumental convergence
  - Why needed here: This is the most unfamiliar core concept (63% never heard/only heard of), yet central to understanding why advanced AI might pursue harmful subgoals regardless of primary objectives.
  - Quick check question: Can you explain why an AI system tasked with curing cancer might develop a self-preservation drive?

- Concept: Scalable oversight
  - Why needed here: 58% unfamiliar; addresses how humans maintain control of systems that exceed their capabilities—a key crux in tool vs. agent debates.
  - Quick check question: What methods could allow humans to evaluate AI outputs when the AI knows more than they do?

- Concept: Correlation vs. causation in survey data
  - Why needed here: The paper infers that unfamiliarity "drives" skepticism, but only demonstrates correlation; readers must avoid overclaiming.
  - Quick check question: What alternative explanation could account for the correlation between low familiarity and low concern?

## Architecture Onboarding

- Component map:
  Population -> Familiarity assessment -> Belief baseline -> Intervention -> Belief reassessment -> Correlation analysis -> Cluster identification

- Critical path:
  1. Familiarity assessment → belief baseline → intervention → belief reassessment
  2. Correlation analysis: familiarity × beliefs × timeline preferences
  3. Cluster identification: tool vs. agent worldview mapping

- Design tradeoffs:
  - **Sample size vs. depth**: N=111 enables correlation analysis but limits subgroup power (only 10 safety researchers)
  - **Breadth vs. specificity**: 9 belief statements capture major objections but can't probe deep reasoning
  - **Self-report vs. objective knowledge**: Familiarity is self-assessed; may over/underestimate actual understanding

- Failure signatures:
  - Selection bias: Experts interested in AI safety more likely to respond (acknowledged in section 5.5)
  - Social desirability: 77% agree researchers "should" be concerned—may reflect normative pressure
  - Intervention underpowered: N=87 completing full survey may be insufficient to detect modest effects

- First 3 experiments:
  1. Replicate with larger sample (N>500) stratified by field to confirm cluster structure and test subgroup effects.
  2. Test active learning intervention (e.g., 30-minute tutorial on instrumental convergence) to assess causal impact of concept exposure on risk perception.
  3. Longitudinal follow-up at 6-12 months to test belief stability and whether real-world AI developments shift worldview clusters.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does active educational exposure to specific AI safety concepts (like instrumental convergence) cause a shift toward the "AI as uncontrollable agent" worldview, or are these beliefs priors that are resistant to new information?
- Basis in paper: [inferred] The author observes a correlation where the least concerned participants are the least familiar with safety concepts, suggesting unfamiliarity drives skepticism. However, the reading interventions resulted in only "minor" shifts in opinion, failing to definitively prove that increasing literacy changes views.
- Why unresolved: The survey measured a snapshot in time (correlation) rather than long-term learning (causation), and the short-term reading intervention failed to produce significant belief updates.
- What evidence would resolve it: A longitudinal study tracking how experts' risk assessments change after formal coursework or workshops on AI alignment compared to a control group.

### Open Question 2
- Question: What specific communication strategies or argument framings are required to significantly shift expert opinions on existential risk?
- Basis in paper: [explicit] The author notes a limitation regarding "limited intervention types" and the finding that "distinct worldviews persist" even after exposure to "strongly contrasting materials" (e.g., emotional narratives vs. expert reasoning).
- Why unresolved: The four selected reading materials elicited "limited and similar effects," indicating that the specific content or style required to bridge the divide between the "tool" and "agent" perspectives remains unidentified.
- What evidence would resolve it: A/B testing a wider variety of argument structures (e.g., technical vs. philosophical vs. risk-analytic) on a similar cohort to identify which specific frames result in statistically significant opinion shifts.

### Open Question 3
- Question: How stable are these expert worldviews over time in response to major AI capability breakthroughs?
- Basis in paper: [explicit] The conclusion states, "Future work may also track changes in perceptions over time, especially as AI capabilities advance," noting that past breakthroughs like ChatGPT have rapidly updated expert beliefs.
- Why unresolved: The survey captures a static snapshot of opinion, but the field is moving quickly; it is unclear if the "tool vs. agent" clusters are stable philosophical stances or temporary reactions to current technology levels.
- What evidence would resolve it: A repeated longitudinal survey (panel study) of the same experts conducted before and after major AI milestones (e.g., release of new SOTA models).

## Limitations
- Selection bias: Experts interested in AI safety more likely to respond
- Insufficient statistical power for subgroup comparisons (only N=10 safety researchers)
- Self-reported familiarity may over/underestimate actual understanding

## Confidence

- Familiarity-belief correlation: **High** (clear statistical evidence, replicable pattern)
- Two-worldview model: **Medium** (statistically supported but sample-limited)
- Intervention null effect: **Low** (methodologically underpowered, weak external validation)

## Next Checks

1. **Causal validation**: Design an experiment where participants are randomly assigned to receive structured safety concept training versus control, then measure changes in risk perception to test whether concept exposure drives belief shifts.

2. **Subgroup analysis**: Conduct a larger survey (N>500) stratified by expertise level to test whether the two-worldview model holds across different AI expertise domains and whether safety researchers form a distinct cluster.

3. **Longitudinal tracking**: Follow the same participants over 12-24 months during periods of significant AI development to test whether worldview clusters are stable or responsive to real-world events.