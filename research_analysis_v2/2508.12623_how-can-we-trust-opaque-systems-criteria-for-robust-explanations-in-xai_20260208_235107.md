---
ver: rpa2
title: How can we trust opaque systems? Criteria for robust explanations in XAI
arxiv_id: '2508.12623'
source_url: https://arxiv.org/abs/2508.12623
tags:
- explanation
- explanations
- robustness
- methods
- different
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of establishing trust in deep
  learning (DL) systems through robust explanations. The authors propose two criteria
  for trustworthy explanations: Explanatory Robustness (ER) and Explanatory Method
  Robustness (EMR).'
---

# How can we trust opaque systems? Criteria for robust explanations in XAI

## Quick Facts
- arXiv ID: 2508.12623
- Source URL: https://arxiv.org/abs/2508.12623
- Authors: Florian J. Boge; Annika Schuster
- Reference count: 40
- Key outcome: Proposes Explanatory Robustness (ER) and Explanatory Method Robustness (EMR) criteria for trustworthy XAI

## Executive Summary
This paper addresses the problem of establishing trust in deep learning (DL) systems through robust explanations. The authors propose two criteria for trustworthy explanations: Explanatory Robustness (ER) and Explanatory Method Robustness (EMR). ER requires different XAI methods to produce consistent results for the same model or input-output pair, while EMR demands that a single XAI method produces similar explanations for similar models or input-output pairs. The authors formalize these criteria and demonstrate their relevance through case studies showing that popular XAI methods like SHAP, LIME, and saliency maps often fail to satisfy these criteria. They argue that EMR is a necessary precondition for trustworthy DL explanations, as consistency alone does not ensure correctness.

## Method Summary
The paper presents a theoretical framework for evaluating the trustworthiness of XAI methods through two formal criteria: Explanatory Robustness (ER) and Explanatory Method Robustness (EMR). ER requires consistency across different XAI methods for the same model behavior, while EMR requires stability within a single method across similar inputs or models. The framework uses formal inequalities with distance metrics and tolerance thresholds to quantify these properties. No experimental implementation is provided; the work focuses on conceptual formalization and case studies demonstrating how popular XAI methods fail to satisfy these criteria.

## Key Results
- EMR is identified as a necessary precondition for trustworthy explanations, requiring XAI methods to show both continuity (stability under noise) and contrast (sensitivity to distinct models)
- ER requires multiple independent XAI methods to converge on consistent explanations, leveraging the philosophical principle that "truth is the intersection of independent lies"
- Popular XAI methods like SHAP, LIME, and saliency maps often fail EMR-2 by producing similar explanations for distinct models, masking model vulnerabilities

## Why This Works (Mechanism)

### Mechanism 1: Intersection of Independent Errors (ER)
Trustworthiness increases when multiple distinct XAI methods produce consistent explanations for the same model behavior. This relies on the philosophical principle that "truth is the intersection of independent lies" - if methods with different architectures and error modes converge on the same explanation, it reduces the likelihood of artifacts. The mechanism fails if methods share common biases, leading to "spurious robustness" where they agree for the wrong reasons.

### Mechanism 2: Continuity and Contrast (EMR)
A single XAI method can only be trusted if it displays stability under noise (continuity) and sensitivity to distinct models (contrast). The method must satisfy EMR-1 (similar inputs/models yield similar explanations) and EMR-2 (distinct inputs/models yield distinct explanations). This ensures explanations map to actual decision boundaries rather than being random outputs. The mechanism fails if the XAI method is decoupled from model parameters.

### Mechanism 3: Discriminant Validity
Robustness requires not just agreement, but the ability to discriminate between distinct phenomena. A system must output distinct explanations for distinct input-output pairs or models. If a method explains "Cat" the same way it explains "Dog," it lacks discriminant validity and fails to reflect the model's specific reasoning process.

## Foundational Learning

- **Concept: The Abstractiveness-Faithfulness Trade-off**
  - Why needed: Understanding XAI method failures requires grasping that simplifying a complex model (abstraction) inevitably loses fidelity (faithfulness)
  - Quick check: Does your explanation method simplify the model's logic (abstraction) or strictly map its internal weights (faithfulness)?

- **Concept: Ground-Truth Problem**
  - Why needed: The paper's premise is that we cannot verify explanation "truth" because we lack a definitive standard, so we must rely on consistency as a proxy
  - Quick check: Can you verify the XAI output against a known physical law, or are you comparing it against other approximations?

- **Concept: Robustness Analysis (Levins)**
  - Why needed: The paper builds on Richard Levins' philosophy that the "truth is the intersection of independent lies," underpinning why using multiple methods (ER) is valid
  - Quick check: Are the errors of the XAI methods you are comparing truly independent, or do they rely on the same underlying simplifications?

## Architecture Onboarding

- **Component map:** Target System (Deep Learning Model $f$ or Input-Output pairs $z_i$) -> Probes (Diverse XAI methods) -> Comparator (Metrics $d, D$) -> Filter (Tolerance Thresholds $\epsilon, \gamma$)

- **Critical path:**
  1. Define Goal: Select specific explanatory goal (e.g., local feature attribution)
  2. Validate EMR (Precondition): Test individual XAI methods to ensure EMR-1 (stability) and EMR-2 (sensitivity)
  3. Validate ER (Convergence): Apply surviving methods to target model, check for consistency (ER-1) and discriminant validity (ER-2)
  4. Trust Assignment: If EMR and ER are satisfied, explanation is a candidate for trust

- **Design tradeoffs:**
  - Tolerance Selection: Setting tolerances ($\epsilon$) is "contextually determined" - too tight, no methods pass; too loose, contradictory explanations accepted
  - Transformation Complexity: Comparing different XAI outputs requires transformation function $\phi$, introducing approximation error

- **Failure signatures:**
  - Sanity Check Failure: Same explanation for randomly initialized and trained networks (Violates Global EMR-2)
  - Constant Shift Failure: Explanation changes when prediction remains constant under constant data shift (Violates EMR-1)
  - Adversarial Blindness: Fails to highlight difference between benign and adversarial inputs with different predictions (Violates EMR-2)

- **First 3 experiments:**
  1. Model Parameter Randomization Test: Run XAI method on trained model, then on same architecture with randomized weights; if explanations are similar, method fails Global EMR-2
  2. Perturbation Stability Test (EMR-1): Add imperceptible noise to input, measure distance between original and new explanation; if exceeds tolerance $\delta$, lacks stability
  3. Cross-Method Agreement Test (ER): Apply two independent methods to same prediction, calculate distance between outputs after transformation, check for intersection (agreement)

## Open Questions the Paper Calls Out

### Open Question 1
To what degree does the satisfaction of Explanatory Robustness (ER) and Explanatory Method Robustness (EMR) correlate with the actual correctness (faithfulness) of an explanation? This is a conceptual paper providing formal definitions but no empirical results to validate that passing these robustness tests ensures explanation represents true reasoning process.

### Open Question 2
How can domain-specific benchmarks be constructed to operationalize these robustness criteria for different explanatory goals? Current benchmarks are often method-specific or lack objective standards; the "abstractiveness-faithfulness-trade-off" makes single universal standard impossible.

### Open Question 3
What are the most effective formal metrics and transformation functions for comparing explanations across fundamentally different XAI method types? Comparing outputs from structurally different methods (e.g., numerical feature importance vs. visual saliency map) into common space for robustness analysis is mathematically difficult.

## Limitations
- Theoretical framework relies on context-dependent thresholds (ε, γ) that are not empirically calibrated, making binary validation impossible without domain-specific benchmarks
- Transformation function φ for comparing different XAI output types introduces approximation error that is not quantified
- Framework assumes failure of EMR necessarily indicates untrustworthy explanations, but some methods may be reliable within specific domains despite not satisfying formal criteria

## Confidence

**High confidence:** The philosophical foundation (robustness analysis, intersection of independent lies) and identification of the "ground truth problem" in XAI are well-established concepts that directly support the proposed criteria.

**Medium confidence:** The formalization of ER and EMR criteria is logically coherent and addresses real failure modes (spurious robustness, Clever Hans effects), but empirical validation on specific XAI methods is limited to conceptual examples.

**Low confidence:** The claim that EMR is a "necessary precondition" for trustworthy explanations requires more rigorous testing across diverse model architectures and domains to establish its universal applicability.

## Next Checks

1. **Calibration Study:** Systematically vary tolerance thresholds ε and γ across multiple domains to determine how sensitivity to these parameters affects ER/EMR classification outcomes.

2. **Cross-Domain Replication:** Apply the criteria to XAI methods in domains beyond computer vision (e.g., healthcare decision support or financial risk assessment) to test generalizability.

3. **Error Mode Analysis:** Conduct controlled experiments where XAI methods are intentionally designed to share common biases, then measure whether ER successfully detects spurious robustness in these cases.