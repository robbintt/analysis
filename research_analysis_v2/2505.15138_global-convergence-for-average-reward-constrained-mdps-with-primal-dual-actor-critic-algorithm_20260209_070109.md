---
ver: rpa2
title: Global Convergence for Average Reward Constrained MDPs with Primal-Dual Actor
  Critic Algorithm
arxiv_id: '2505.15138'
source_url: https://arxiv.org/abs/2505.15138
tags:
- where
- policy
- following
- convergence
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates infinite-horizon average reward Constrained
  Markov Decision Processes (CMDPs) with general policy parametrization. The authors
  propose a Primal-Dual Natural Actor-Critic algorithm that achieves global convergence
  and constraint violation rates of $\tilde{\mathcal{O}}(1/\sqrt{T})$ over horizon
  $T$ when mixing time $\tau{\text{mix}}$ is known.
---

# Global Convergence for Average Reward Constrained MDPs with Primal-Dual Actor Critic Algorithm

## Quick Facts
- arXiv ID: 2505.15138
- Source URL: https://arxiv.org/abs/2505.15138
- Authors: Yang Xu; Swetha Ganesh; Washim Uddin Mondal; Qinbo Bai; Vaneet Aggarwal
- Reference count: 40
- Key outcome: Achieves global convergence and constraint violation rates of $\tilde{\mathcal{O}}(1/\sqrt{T})$ for average reward CMDPs with general parametrization

## Executive Summary
This paper addresses the challenge of global convergence in infinite-horizon average reward Constrained Markov Decision Processes (CMDPs) with general policy parametrization. The authors propose a Primal-Dual Natural Actor-Critic (PDNAC) algorithm that achieves optimal convergence rates without requiring prior knowledge of the mixing time. By leveraging Multi-Level Monte Carlo (MLMC) estimators in inner loop subroutines, the algorithm maintains theoretical guarantees while reducing computational complexity compared to previous approaches that required trajectory storage.

## Method Summary
The method employs a nested loop structure with outer loop updates to primal-dual parameters and inner loops running natural policy gradient and critic parameter finding subroutines. The key innovation is the use of MLMC estimators that achieve the same bias as averaging O(T_max) samples while using only O(log T_max) samples on average. The algorithm updates policy parameters using natural policy gradients computed via MLMC, while the critic uses linear function approximation with feature maps satisfying specific positive definiteness conditions. Dual variable updates include projection onto [0, 2/δ] to maintain boundedness and ensure constraint violation rates match optimality rates.

## Key Results
- Achieves global convergence rates of $\tilde{\mathcal{O}}(1/\sqrt{T})$ when mixing time $\tau_{\text{mix}}$ is known
- When $\tau_{\text{mix}}$ is unknown, achieves $\tilde{\mathcal{O}}(1/T^{0.5-\epsilon})$ rates provided $T \geq \tilde{\mathcal{O}}(\tau_{\text{mix}}^{2/\epsilon})$
- Uses MLMC estimates to achieve these rates without requiring mixing time knowledge
- Matches theoretical lower bounds for MDPs and establishes new benchmark for average reward CMDPs

## Why This Works (Mechanism)

### Mechanism 1: Multi-Level Monte Carlo (MLMC) Bias Reduction
The MLMC estimator achieves the same bias as averaging O(T_max) samples while using only O(log T_max) samples on average. Random trajectory lengths are sampled from Geom(1/2), with the MLMC estimator constructed to leverage telescoping properties where longer trajectories exponentially reduce bias but are sampled exponentially rarely. This provides O(τ_mix / T_max) bias decay with only O(τ_mix log² T_max) variance growth.

### Mechanism 2: Primal-Dual Lagrangian Optimization with Bounded Dual Variable
Projecting the dual variable λ onto [0, 2/δ] (where δ is the Slater gap) ensures constraint violation decays at the same rate as optimality gap. The Slater condition guarantees an interior point with J_c ≥ δ, which ensures strong duality and bounds the optimal dual variable λ* ≤ 1/δ. This projection prevents dual variable explosion that would destabilize primal updates.

### Mechanism 3: Single-Timescale Structure via Constant Inner Loop Length
Setting H = Õ(τ²_mix) and K = O(T) makes the algorithm behave like single-timescale, achieving Õ(1/√T) convergence. Unlike typical two-timescale methods where H and K scale as O(√T), keeping H nearly constant (logarithmic in T) with appropriately decaying inner loop stepsizes ensures exponential decay of inner loop error, making inner loop bias negligible.

## Foundational Learning

- **Concept: Average-reward MDP and stationarity**
  - Why needed here: The algorithm optimizes J^π_r = lim_{T→∞} (1/T)E[∑ g(s_t,a_t)], which is well-defined only under ergodicity (unique stationary distribution d^{π_θ} independent of initial state).
  - Quick check question: If a Markov chain has two recurrent classes under policy π, what happens to J^π_r when starting from different initial distributions?

- **Concept: Fisher Information Matrix and Natural Policy Gradient**
  - Why needed here: The NPG direction ω*_{g,θ} = F(θ)^{-1}∇_θ J_g(θ) accounts for policy geometry. Fisher non-degeneracy ensures strong convexity of f_g(θ, ω), enabling linear convergence in the inner loop.
  - Quick check question: Why does standard policy gradient (using ∇_θ J directly) have worse sample complexity than NPG in this setting?

- **Concept: Slater's Condition in Constrained Optimization**
  - Why needed here: Slater's condition (interior point exists) ensures strong duality and bounds the optimal dual variable λ* ≤ 1/δ. This prevents dual variable explosion and enables the constraint violation rate to match optimality rate.
  - Quick check question: If the constraint J_c(θ) ≥ 0 defines a boundary with no interior (e.g., all feasible policies satisfy J_c = 0 exactly), what happens to the dual variable dynamics?

## Architecture Onboarding

- **Component map**:
  ```
  Outer Loop (k = 0 to K-1):
    ├── Critic Subroutine (h = 0 to H-1):
    │   ├── Sample trajectory length l_kh ~ 2^P_kh, P_kh ~ Geom(1/2)
    │   ├── Collect transitions {(s_t, a_t, s_{t+1}, g(s_t,a_t))}
    │   ├── Compute MLMC gradient estimate (Eq. 20)
    │   └── Update ξ^g_{k,h} = [η^g_{k,h}, ζ^g_{k,h}] via SGD (Eq. 18)
    ├── Actor Subroutine (h = 0 to H-1):
    │   ├── Sample trajectory length l_kh ~ 2^Q_kh
    │   ├── Compute advantage estimate Â^π_θ_g using TD error (Eq. 22)
    │   ├── Compute MLMC NPG estimate (Eq. 23)
    │   └── Update ω^g_{k,h} via SGD (Eq. 21)
    └── Primal-Dual Update:
        ├── ω_k = ω^r_k + λ_k ω^c_k
        ├── θ_{k+1} = θ_k + α ω_k
        └── λ_{k+1} = P_{[0,2/δ]}[λ_k - β η^c_k]
  ```

- **Critical path**:
  1. **Feature engineering** (φ_g: S → R^m): Must satisfy Assumption 4.1 (low approximation error) and Assumption 4.2 (positive definite feature covariance). Poor features cause ε_app to dominate.
  2. **Hyperparameter selection**: H, α, β, T_max must satisfy conditions in Theorems 4.7-4.10. Incorrect settings break the bias-variance balance.
  3. **Initial state sampling**: Line 6 and 18 sample s_0 ~ ρ(·) at the start of each inner loop. The final state of one inner loop becomes s_0 for the next (lines 13, 25), ensuring Markovian sampling.

- **Design tradeoffs**:
  - **Known τ_mix vs. Unknown τ_mix**: With known τ_mix, set H = Õ(τ²_mix) for Õ(1/√T) rates. With unknown τ_mix, set H = T^ε requiring T ≥ Õ(τ^{2/ε}_mix), achieving Õ(1/T^{0.5-ε}) rates. The tradeoff: better rates require mixing time oracle.
  - **Memory vs. Bias**: Previous work stored trajectories of length H for gradient estimation. MLMC reduces memory to O(log T_max) but increases per-sample computation.
  - **Inner loop length H**: Larger H reduces bias but increases sample complexity per outer iteration. The key insight is that H can be Õ(1) due to MLMC's bias reduction.

- **Failure signatures**:
  - **Constraint violation diverges**: Likely Slater condition violated (δ too small or zero). Check if J_c(θ) ≥ δ is achievable for some θ.
  - **Convergence stalls at floor > 0**: ε_app or ε_bias dominating. Inspect feature quality (critic approximation) and policy class richness.
  - **High variance in ω_k**: Dual learning rate β too large or T_max too small. The condition γ_ω ≤ μ/[4(6G₁⁴τ_mix log T_max + ...)] may be violated.
  - **Inner loop doesn't converge**: Feature covariance not positive definite (Assumption 4.2 violated) or learning rates γ_ξ, γ_ω too large.

- **First 3 experiments**:
  1. **Validate on tabular CMDP with known τ_mix**: Implement the algorithm on a small ergodic CMDP where you can compute τ_mix analytically. Verify that convergence and constraint violation rates scale as Õ(1/√T) when H = Õ(τ²_mix). Plot log(error) vs. log(T) to check slope ≈ -0.5.
  2. **Ablation on mixing time knowledge**: Run the same environment with H set incorrectly (e.g., H = τ²_mix/10 or H = τ²_mix × 10). Measure the degradation in convergence rate. Then run with unknown τ_mix setting (H = T^ε) and verify the tradeoff in Theorem 4.10.
  3. **Feature quality sensitivity**: Construct a CMDP where the optimal value function V^*_g is exactly representable by features (ε_app = 0) vs. poorly represented (ε_app large). Compare the asymptotic convergence floor. Use linear features φ(s) = [1, s] for a 1D state space where V*(s) is quadratic to induce high ε_app.

## Open Questions the Paper Calls Out

- **Can the horizon length requirement of T ≥ Õ(τ_{mix}^{2/ε}) be relaxed when the mixing time is unknown while maintaining near-optimal convergence rates?**
  - Basis in paper: [explicit] The discussion following Theorem 4.10 notes that for small ε, the horizon requirement becomes large for slowly mixing problems, and states: "a systematic theoretical investigation is needed to improve the requirement of the horizon length... which is left as one of the future works."
  - Why unresolved: The current analysis relies on a trade-off where the horizon must scale polynomially with the mixing time to achieve the Õ(1/T^{0.5-ε}) rate without prior knowledge of τ_{mix}.
  - What evidence would resolve it: A theoretical proof achieving Õ(1/√T) convergence in the unknown mixing time setting without the dependency T ≥ Õ(τ^{2/ε}_mix), or a proof that this dependency is necessary.

- **Can the global convergence guarantees be extended to critic parameterizations using neural networks instead of linear function approximation?**
  - Basis in paper: [explicit] The Conclusion section explicitly lists "parameterizing the critic using neural networks" as a future direction.
  - Why unresolved: The current theoretical analysis relies on linear critic function approximation (Assumptions 4.1 and 4.2) and specific properties of feature mappings to bound approximation errors.
  - What evidence would resolve it: A convergence analysis that accounts for the non-linear dynamics of neural network training (e.g., over-parameterization) while maintaining the Õ(1/√T) global convergence rate.

- **Can the ergodicity assumption be relaxed to handle more general MDP structures, such as unichain MDPs?**
  - Basis in paper: [explicit] The Conclusion section explicitly lists "relaxing the ergodicity assumption" as a future direction.
  - Why unresolved: The analysis depends on Assumption 2.2 (ergodicity) to ensure the Markov chain is irreducible and aperiodic, guaranteeing a unique stationary distribution independent of the initial distribution.
  - What evidence would resolve it: A modified analysis that proves convergence under weaker assumptions like unichain or weakly communicating MDPs, possibly by accounting for state-dependent bias terms.

## Limitations
- The Slater condition with parameter δ > 0 is essential for dual variable projection and constraint violation bounds, but many practical CMDPs have active constraints at optimality (δ → 0).
- The mixing time τ_mix must be either known exactly or sufficiently overestimated; underestimation when τ_mix is unknown can cause H to be insufficient.
- Finding features that simultaneously satisfy low approximation error and positive definiteness remains an open challenge for complex environments.

## Confidence
**High Confidence**: The MLMC bias-variance tradeoff mechanism and the single-timescale structure via constant H are well-supported by explicit proofs in the appendices and match established results in the literature. The primal-dual optimization framework with dual projection follows standard constrained optimization theory when Slater's condition holds.

**Medium Confidence**: The specific hyperparameter scaling conditions are technically derived but may be conservative in practice. The assumption that H = Õ(τ²_mix) suffices for single-timescale behavior relies on precise bias decay rates that may vary across environments.

**Low Confidence**: The practical impact of the Slater parameter δ on dual variable dynamics is not empirically validated. When δ is small, the projection range [0, 2/δ] becomes large, potentially causing high variance in primal updates that the analysis may not fully capture.

## Next Checks
1. **Test Slater Condition Robustness**: Implement the algorithm on CMDPs where the optimal policy lies on the constraint boundary (δ ≈ 0). Measure how constraint violation and convergence rates degrade as δ decreases, and compare against the theoretical bounds.

2. **Mixing Time Estimation Error**: Design experiments where τ_mix is systematically underestimated by factors of 2, 5, and 10. Track the convergence rates and determine the threshold where the Õ(1/T^{0.5-ε}) rates break down.

3. **Feature Quality Scaling**: Systematically vary the feature approximation error ε_app by using increasingly poor feature maps for a fixed environment. Plot the convergence floor against ε_app to empirically verify the theoretical dependence in the error bounds.