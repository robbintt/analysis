---
ver: rpa2
title: Deep neural networks can provably solve Bellman equations for Markov decision
  processes without the curse of dimensionality
arxiv_id: '2506.22851'
source_url: https://arxiv.org/abs/2506.22851
tags:
- holds
- lemma
- assumption
- satisfy
- every
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper establishes that deep neural networks (DNNs) with leaky\
  \ ReLU activation can provably approximate solutions of Bellman equations for Markov\
  \ decision processes (MDPs) without suffering from the curse of dimensionality.\
  \ The authors show that if the payoff function and transition dynamics of an MDP\
  \ can be approximated by DNNs, then the corresponding Q-function (solution to the\
  \ Bellman equation) can also be approximated by DNNs whose number of parameters\
  \ grows at most polynomially in both the state space dimension d and the reciprocal\
  \ of the approximation error 1/\u03B5."
---

# Deep neural networks can provably solve Bellman equations for Markov decision processes without the curse of dimensionality

## Quick Facts
- **arXiv ID:** 2506.22851
- **Source URL:** https://arxiv.org/abs/2506.22851
- **Reference count:** 40
- **Primary result:** Deep neural networks with leaky ReLU can approximate MDP Q-functions with complexity polynomial in dimension $d$ and $1/\epsilon$

## Executive Summary
This paper establishes that deep neural networks (DNNs) with leaky ReLU activation can provably approximate solutions to Bellman equations for Markov decision processes without suffering from the curse of dimensionality. The authors demonstrate that if payoff functions and transition dynamics can be approximated by DNNs, then the corresponding Q-function can also be approximated with parameter count growing at most polynomially in both state dimension $d$ and approximation error $1/\epsilon$. The key innovation is a multilevel fixed-point approximation scheme that avoids grid-based discretization while maintaining polynomial complexity.

## Method Summary
The method uses a Multilevel Fixed-Point (MLFP) approximation scheme to construct DNN approximations of the Q-function. Rather than discretizing the state space, the approach recursively simulates transition dynamics using random samples to approximate the expectation operator in the Bellman equation. The MLFP scheme builds the final network through compositional operations (parallelization, composition, extension) that represent the max operator and affine transformations required for the Bellman update. The construction relies on specific DNN "calculus" rules that allow complex recursive structures to be built from simpler components while maintaining polynomial complexity bounds.

## Key Results
- DNNs with leaky ReLU can approximate MDP Q-functions with complexity $O(d^c \epsilon^{-c})$ for some constant $c$
- The approximation achieves $L^2$ error bounds of $O(\epsilon)$ for any given probability measure on the state space
- The method avoids the curse of dimensionality that plagues grid-based approaches
- The approximation scheme is stable: errors in approximating dynamics and rewards translate to proportional errors in the Q-function

## Why This Works (Mechanism)

### Mechanism 1: Multilevel Fixed-Point Approximation
The MLFP scheme approximates the Q-function's fixed point by recursively simulating transition dynamics using Monte Carlo sampling, avoiding state space discretization. The recursion depth and sample size are calibrated to achieve error $\epsilon$.

**Core assumption:** Transition dynamics and payoff functions are Lipschitz continuous, and a contraction condition (dependent on discount factor) holds.

**Evidence anchors:**
- [abstract] "The proof relies on a multilevel fixed-point approximation scheme and provides error bounds of the form $O(\varepsilon)$ with network complexity $O(d^c \varepsilon^{-c})$."
- [section 4.2] Lemma 4.3 constructs the MLFP approximation and bounds parameter requirements.
- [corpus] Weak direct evidence; related work focuses on FBSDE systems rather than this specific fixed-point recursion.

**Break condition:** If transition dynamics cannot be sampled efficiently or Lipschitz constants are too large relative to discount factor, error bounds may diverge.

### Mechanism 2: Leaky ReLU for Max Operator
DNNs with leaky ReLU can represent the maximum function over finite action sets with network depth $O(\log |A|)$, enabling efficient Bellman optimality updates.

**Core assumption:** Leaky ReLU activation allows efficient approximation of the max operator required for Bellman optimality.

**Evidence anchors:**
- [abstract] "...deep neural networks (DNNs) with leaky ReLU activation can provably approximate solutions..."
- [section 4.1] Lemma 4.2 explicitly constructs a network to represent $\max\{x_1, \dots, x_m\}$ and bounds its depth.
- [section 3] Section 3 defines ANN "calculus" used to build complex MLFP structure.

**Break condition:** Using activation functions that cannot efficiently approximate the maximum function could violate polynomial complexity constraints.

### Mechanism 3: Stability of Approximate Solutions
The solution to the approximate Bellman equation converges to the true solution, ensuring that approximating MDP components is sufficient.

**Core assumption:** Stochastic kernels satisfy regularity conditions (Lipschitz in state) and contractivity conditions.

**Evidence anchors:**
- [section 2.2] Proposition 2.9 provides explicit bound linking Q-function error to error in transition kernels.
- [abstract] "if the payoff function and the random transition dynamics of the MDP can be suitably approximated... then the solutions... can also be approximated."
- [corpus] Related papers on "Universal Approximation" support the general principle but lack specific stability analysis.

**Break condition:** If MDP is unstable (Lipschitz constant too high relative to discount factor), small errors in dynamics could lead to arbitrarily large errors in Q-function.

## Foundational Learning

**Concept: Bellman Equation & Q-Functions**
- **Why needed here:** This is the core mathematical object being solved. The Q-function represents expected return, and the Bellman equation is the recursive relationship it must satisfy.
- **Quick check question:** Can you write the Bellman optimality equation for a discounted infinite-horizon MDP?

**Concept: Curse of Dimensionality**
- **Why needed here:** The paper's main claim is "provably solving" without this curse. Understanding that standard methods scale exponentially with state dimension $d$ is necessary to appreciate the polynomial complexity result $O(d^c)$.
- **Quick check question:** Why does grid-based discretization fail in 100-dimensional state spaces?

**Concept: Lipschitz Continuity & Contraction**
- **Why needed here:** Proofs rely heavily on transition dynamics and rewards being Lipschitz continuous and the Bellman operator being a contraction. These properties ensure errors do not explode and solutions are stable.
- **Quick check question:** If a function $f$ is Lipschitz with constant $K=0.5$, and we perturb the input by $\delta$, what is the maximum change in the output?

## Architecture Onboarding

**Component map:**
State $x \in \mathbb{R}^d$ -> Dynamic Approximator ($t_{d,\epsilon}$) -> Reward Approximator ($G_{d,\epsilon}$) -> Max-Pooling Layer -> Recursive Composition

**Critical path:**
1. **Verify Regularity:** Ensure MDP dynamics satisfy Lipschitz and growth bounds in Setting 2.4.
2. **Construct Approximators:** Build/Train networks $G$ and $t$ to approximate rewards and transitions with error $\epsilon_{approx}$.
3. **Compose Architecture:** Assemble final network $Q$ using composition rules in Section 3 and recursion in Lemma 4.3.

**Design tradeoffs:**
- **Polynomial Complexity vs. Finite Control:** Result applies to *finite* control sets $A$. Complexity may scale poorly with $|A|$ as it appears in base of complexity bounds.
- **Expressivity vs. Activation:** Architecture is explicitly designed for leaky ReLU. Standardizing on sigmoid/tanh might require significantly different depth/width bounds or break theoretical guarantees.

**Failure signatures:**
- **Non-Contractivity:** If discount factor $\delta$ is too close to 1 or Lipschitz constant $L$ is too high (such that $\lambda L \ge 1$), stability results fail and training may diverge.
- **Approximation Mismatch:** If provided dynamics networks $t_{d,\epsilon}$ have poor realization error relative to required error tolerance $\epsilon$, final Q-function error will violate bound in Theorem 1.1.

**First 3 experiments:**
1. **Dimension Scaling:** Implement construction on synthetic Linear Quadratic Regulator (LQR) problem. Vary state dimension $d \in \{10, 50, 100\}$ and verify parameter count scales polynomially while error remains bounded.
2. **Stress Test Contractivity:** Fix dimension and slowly increase Lipschitz constant of reward function (or decrease discount) toward theoretical limit $\eta L \ge 1$ to observe degradation of approximation accuracy.
3. **Approximation Sensitivity:** Replace "perfect" approximator networks ($G, t$) with standard trained networks of varying sizes to empirically validate sensitivity of final Q-error to dynamics/reward approximation error.

## Open Questions the Paper Calls Out

**Open Question 1:** Can the approximation results be extended to MDPs with continuous or unbounded control sets?
- **Basis:** [inferred] Theorem 1.1 explicitly assumes finite control set $A$.
- **Why unresolved:** MLFP approximation scheme and ANN representation of maximum function (Lemma 4.2) rely on action space being finite.
- **What evidence would resolve it:** Extension of Corollary 5.4 to compact or unbounded action spaces, likely requiring smooth approximation of max operator.

**Open Question 2:** Do complexity bounds hold for standard ReLU activation functions?
- **Basis:** [inferred] Main results in Section 5 are restricted to leaky ReLU activations.
- **Why unresolved:** Lemma 4.1 constructs ANN representation for maximum function using specific invertibility properties of leaky ReLU.
- **What evidence would resolve it:** Proof that standard ReLU networks can approximate maximum function with same polynomial complexity bounds.

**Open Question 3:** Does there exist a training algorithm (e.g., SGD) that can find these approximating networks with polynomial time complexity?
- **Basis:** [inferred] Paper proves *existence* of such networks but does not analyze convergence of optimization methods.
- **Why unresolved:** Existence proofs do not imply loss landscape is benign or gradient-based methods will converge efficiently.
- **What evidence would resolve it:** Convergence analysis for specific training algorithm demonstrating polynomial time finding of provably optimal network parameters.

## Limitations
- **Finite Control Sets:** Results apply specifically to MDPs with finite action sets $A$, not continuous control problems.
- **Approximator Dependency:** Polynomial complexity guarantee depends on existence of specific reward and transition approximators satisfying complexity bounds.
- **Implementation Uncertainty:** Practical implementation requires constructing recursive MLFP networks that store exponentially many paths despite polynomial parameter growth.

## Confidence

**High Confidence:** The mathematical framework and proof techniques are rigorous. Use of Wasserstein distance for stability analysis and construction of max-representing networks with leaky ReLU are well-established.

**Medium Confidence:** Polynomial complexity claim assumes provided approximators meet specific error-complexity tradeoffs. In practice, achieving these bounds may require careful network design or training procedures not detailed in the paper.

**Low Confidence:** Practical implementation feasibility is uncertain due to recursive nature of MLFP requiring storage of exponentially many paths ($M^{n-l}$) despite polynomial parameter growth.

## Next Checks
1. **Dimension Scaling Verification:** Implement construction on synthetic LQR problem with varying dimensions $d \in \{10, 50, 100\}$. Verify parameter count scales polynomially while maintaining bounded error.

2. **Contractivity Stress Test:** Fix dimension and systematically increase Lipschitz constant of reward function toward theoretical limit. Monitor approximation accuracy degradation to validate contraction condition's importance.

3. **Approximation Sensitivity Analysis:** Replace "perfect" approximator networks with standard trained networks of varying sizes. Empirically measure how errors in dynamics/reward approximation propagate to final Q-function error, validating sensitivity relationship in Proposition 2.9.