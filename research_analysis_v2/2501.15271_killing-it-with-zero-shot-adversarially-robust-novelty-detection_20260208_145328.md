---
ver: rpa2
title: 'Killing it with Zero-Shot: Adversarially Robust Novelty Detection'
arxiv_id: '2501.15271'
source_url: https://arxiv.org/abs/2501.15271
tags:
- arxiv
- adversarial
- feature
- detection
- anomaly
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ZARND (Zero-shot Adversarially Robust Novelty
  Detection), a method that combines nearest-neighbor algorithms with adversarially
  robust features from ImageNet-pretrained models to enhance the robustness and performance
  of novelty detection (ND) algorithms. The approach extracts features using the last
  layer of an adversarially robust ResNet-18 backbone and computes anomaly scores
  using k-NN on these features.
---

# Killing it with Zero-Shot: Adversarially Robust Novelty Detection

## Quick Facts
- arXiv ID: 2501.15271
- Source URL: https://arxiv.org/abs/2501.15271
- Authors: Hossein Mirzaei; Mohammad Jafari; Hamid Reza Dehbashi; Zeinab Sadat Taghavi; Mohammad Sabokrou; Mohammad Hossein Rohban
- Reference count: 0
- One-line primary result: ZARND achieves up to 40% AUROC improvement over state-of-the-art methods under adversarial conditions across multiple datasets.

## Executive Summary
This paper introduces ZARND (Zero-shot Adversarially Robust Novelty Detection), a method that combines nearest-neighbor algorithms with adversarially robust features from ImageNet-pretrained models to enhance the robustness and performance of novelty detection algorithms. The approach extracts features using the last layer of an adversarially robust ResNet-18 backbone and computes anomaly scores using k-NN on these features. Experimental results show that ZARND significantly outperforms state-of-the-art methods across various benchmarks, particularly under adversarial conditions, with improvements of up to 40% AUROC. The method demonstrates strong performance on low-resolution datasets like CIFAR-10 and CIFAR-100, as well as high-resolution medical and industrial datasets such as MVTecAD, Head-CT, and BrainMRI, highlighting its applicability to real-world scenarios.

## Method Summary
ZARND uses an adversarially robust ResNet-18 backbone (pretrained on ImageNet) to extract features from the last layer before the classification head. During training, features from normal samples are stored in a feature bank. For inference, the anomaly score is computed as the sum of the k-smallest Euclidean distances between the test sample's features and all training features (k=2). The method operates in a zero-shot setting, requiring only normal samples for training and no domain-specific fine-tuning. The approach is evaluated under both clean and adversarial (PGD-100, AutoAttack) conditions with perturbation bounds of ε=4/255 for low-resolution datasets and ε=2/255 for high-resolution datasets.

## Key Results
- Achieves up to 40% AUROC improvement over state-of-the-art methods under adversarial PGD attacks
- Maintains high performance across diverse datasets: CIFAR-10/100, MNIST, FashionMNIST, MVTecAD, Head-CT, BrainMRI, Tumor, and Covid-19
- Demonstrates strong robustness with 60.3% AUROC on CIFAR-10 under attack compared to 0.06% for standard feature methods
- Shows effective zero-shot transfer from ImageNet to medical and industrial anomaly detection tasks

## Why This Works (Mechanism)

### Mechanism 1: Robust Feature Transferability
Adversarially robust features trained on ImageNet transfer more effectively to novelty detection tasks than standard features, particularly under attack. The robust training forces the network to learn features that are semantically meaningful and stable within local neighborhoods, rather than relying on brittle, non-robust features. When these features are used for ND, the distance between an adversarial sample and its clean counterpart remains small, preserving the "normal" classification. The core assumption is that robustness properties learned on ImageNet generalize to downstream datasets despite domain shifts.

### Mechanism 2: k-NN on Stable Manifolds
Applying k-NN in the feature space of a robust model creates a stable decision boundary that resists perturbation. The anomaly score is the sum of L2 distances to the k-nearest neighbors. Because the feature extractor is adversarially robust, the gradient does not produce large directional changes in feature space for small input perturbations. Consequently, an adversary cannot easily minimize the anomaly score for an outlier without significantly altering the image semantics. The core assumption is that Euclidean distance in the robust feature space correlates with semantic similarity.

### Mechanism 3: Direct Attack Mitigation via Feature Resilience
The method mitigates attacks specifically designed to maximize the anomaly score by removing the easy-to-attack classification layer. The architecture discards the final classification layer of the pretrained model, operating solely on the penultimate layer features. Adversarial attacks often exploit the linear nature of final layers. By removing this layer and using a non-parametric k-NN approach, the attack surface is reduced to manipulating distances in a high-dimensional, robust feature space, which is inherently harder due to the robust backbone. The core assumption is that brittle features in standard networks reside primarily in the final layers.

## Foundational Learning

- **Concept: Adversarial Robustness (Robust Features)**
  - Why needed here: The entire efficacy of ZARND relies on the distinction between "standard" features and "robust" features. You must understand that robust training aligns features with human-perceptible concepts.
  - Quick check question: Why does a standard ResNet feature extractor fail immediately under PGD attacks in this context?

- **Concept: Novelty Detection (ND) vs. Anomaly Detection**
  - Why needed here: The paper defines the training set as purely "normal" (unsupervised). You need to distinguish this from binary classification where negative samples are available.
  - Quick check question: How does the lack of negative training samples influence the choice of k-NN over a binary classifier?

- **Concept: Projected Gradient Descent (PGD)**
  - Why needed here: To interpret the results, you must understand that PGD iteratively perturbs the input to maximize the loss (anomaly score).
  - Quick check question: In the context of ZARND, what is the "loss function" that the PGD attack is trying to maximize?

## Architecture Onboarding

- **Component map:** Input Image -> Adversarially Robust ResNet-18 Backbone -> Feature Vector -> Feature Bank (stored training features) -> Distance Computation (Euclidean) -> k-NN Scoring (sum of k-smallest distances) -> Anomaly Score

- **Critical path:** The weight initialization of the Backbone. Loading standard (non-robust) pretrained weights will cause the system to fail completely under attack (0% AUROC as shown in Table 1).

- **Design tradeoffs:**
  - Accuracy vs. Robustness: Robust features may slightly lower clean performance compared to SOTA specialized methods, but drastically improve adversarial performance.
  - Inference Speed: k-NN requires computing distances against the entire training set. Memory usage scales with O(N × D) and compute with O(N). This is slower than a simple forward pass.

- **Failure signatures:**
  - Clean AUROC is high, but Adversarial AUROC is ~0%: You likely loaded the standard ResNet weights instead of the adversarially robust weights.
  - Slow Inference: The k-NN search is unoptimized. Needs approximate nearest neighbors (ANN) or feature bank reduction for production scale.
  - Poor performance on Medical Data: The ImageNet features may not be fine-tuned. If robust features don't capture the specific pathology, performance drops.

- **First 3 experiments:**
  1. Ablation on Weights: Run ZARND on CIFAR-10 with standard ResNet weights vs. Robust ResNet weights under PGD attack to reproduce the "0% vs 60%+ AUROC" gap.
  2. Attack Sensitivity: Vary the perturbation bound ε (e.g., from 1/255 to 8/255) to observe the degradation curve of the robust backbone.
  3. Hyperparameter k: Sweep k (e.g., 1, 2, 5, 10) to determine the sensitivity of the anomaly score to local vs. global feature clusters.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the following critical questions arise from the methodology:

1. Can the zero-shot adversarial robustness demonstrated by ZARND be generalized to other backbone architectures, specifically Vision Transformers (ViT), without significant performance degradation?

2. Does the reliance on the final layer of the backbone limit detection capability for fine-grained or low-magnitude anomalies compared to multi-scale feature extraction?

3. How does the computational complexity and inference latency of ZARND scale with the size of the training set, given the non-parametric nature of k-NN?

## Limitations
- The claim that ImageNet-robust features transfer effectively to high-resolution medical datasets remains untested in controlled domain adaptation experiments.
- The method's scalability to large-scale deployments is unclear given the linear growth of k-NN with training set size.
- The zero-shot nature means we cannot separate the contribution of robust features from potential dataset-specific biases in the pretraining.

## Confidence
- **High confidence:** The mechanism of using adversarially robust features improves performance under attack conditions compared to standard features.
- **Medium confidence:** The transferability of ImageNet-robust features to diverse domains (medical, industrial) without fine-tuning.
- **Medium confidence:** The k-NN scoring mechanism is optimal for the task, with k=2 appearing somewhat arbitrary.

## Next Checks
1. Conduct controlled experiments where the same robust backbone is fine-tuned on target domains versus used zero-shot, to quantify the transfer gap.
2. Measure inference time and memory usage as a function of training set size on MVTecAD to establish practical limits for deployment.
3. Apply integrated gradients or similar methods to identify which regions of the input contribute most to the anomaly score, validating that the method focuses on semantically relevant areas.