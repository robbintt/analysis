---
ver: rpa2
title: 'Mind the XAI Gap: A Human-Centered LLM Framework for Democratizing Explainable
  AI'
arxiv_id: '2506.12240'
source_url: https://arxiv.org/abs/2506.12240
tags:
- explanations
- framework
- llms
- quality
- human-centered
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework using large language models (LLMs)
  to provide both technical and human-centered explanations for explainable AI (XAI)
  systems. The framework addresses the gap where current XAI explanations are primarily
  designed for experts, making them difficult for non-experts to understand.
---

# Mind the XAI Gap: A Human-Centered LLM Framework for Democratizing Explainable AI

## Quick Facts
- arXiv ID: 2506.12240
- Source URL: https://arxiv.org/abs/2506.12240
- Authors: Eva Paraschou; Ioannis Arapakis; Sofia Yfantidou; Sebastian Macaluso; Athena Vakali
- Reference count: 40
- Primary result: Framework using LLMs provides both technical and human-centered explanations, achieving Spearman rank correlation of 0.92 with ground-truth and significantly improving user-friendliness in a study (N=56)

## Executive Summary
This paper addresses the XAI gap where current explainable AI systems primarily serve technical experts, making explanations inaccessible to non-experts. The authors propose a framework that leverages large language models with in-context learning to generate both technical explanations for developers and human-centered, interpretable explanations for non-experts in a single response. Using the LifeSnaps dataset for well-being clustering analysis, the framework achieves high correlation with ground-truth XAI explanations while significantly improving user-friendliness scores compared to traditional methods like LIME.

## Method Summary
The framework uses in-context learning to adapt LLMs with domain-specific and explainability-relevant knowledge through demonstration examples. It employs a structured dual-output prompt design that generates both JSON-formatted technical explanations and natural language user explanations in one response. The method is evaluated on well-being clustering using the LifeSnaps dataset (71 participants, 7 modalities), with k-means clustering (k=2, silhouette=0.4) followed by SVC classification (99% accuracy). LIME explanations serve as ground-truth with 0.93 fidelity, and the framework uses LLaMA3 8B or Mistral 7B with few-shot prompting. The system template enforces a three-part response structure: scene-setting with domain context, JSON-formatted expert output with feature rankings, and natural language non-expert explanation.

## Key Results
- Spearman rank correlation of 0.92 with ground-truth LIME explanations in few-shot prompting
- Significant improvement in user-friendliness metrics compared to LIME (N=56), with higher scores in supportiveness, ease of use, efficiency, and clarity
- NDCG difference decreased from 0.07 (zero-shot) to 0.001 (few-shot) for LLaMA3
- Framework achieves 99% accuracy on clustering classification task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In-context learning transfers explainability principles to LLMs, enabling feature importance computation aligned with traditional XAI methods.
- Mechanism: Few-shot prompting provides demonstration examples showing input features, model outputs, and corresponding explanations. The LLM learns to replicate this pattern by computing feature importance for new instances, not merely retrieving pre-trained knowledge.
- Core assumption: LLMs possess sufficient pre-trained reasoning capability to perform feature importance computation when given structured demonstration examples.
- Evidence anchors:
  - [abstract] "employs in-context learning to convey domain- and explainability-relevant contextual knowledge into LLMs"
  - [Section 5.2] "LLaMA3's Spearman rank correlation improves dramatically from 0.01 in zero-shot to 0.92 in few-shot"
  - [corpus] Related work (arXiv:2506.05887) confirms LLM-enhanced XAI frameworks can translate technical explanations to natural language
- Break condition: If feature relationships are highly non-linear or require domain expertise not captured in demonstration examples, correlation with ground-truth will degrade substantially.

### Mechanism 2
- Claim: Structured dual-output prompt design produces both technical and human-centered explanations in a single response, reducing post-processing overhead.
- Mechanism: The system template enforces a three-part response structure: (1) scene-setting with domain context, (2) JSON-formatted expert output with feature rankings, (3) natural language non-expert explanation. This ensures consistency between the two outputs—features highlighted in user explanations correspond to those in developer output.
- Core assumption: LLMs can maintain internal consistency across chained generation tasks within a single response.
- Evidence anchors:
  - [Section 4] "encapsulates in one response explanations understandable by non-experts and technical information to experts"
  - [Section 6] "the features highlighted in the user response corresponded to the important features identified in the developer response"
  - [corpus] Limited direct corpus evidence for single-response dual-output architectures; primarily supported by this paper's design
- Break condition: If prompt templates are modified to remove structured output constraints, consistency between expert and non-expert explanations may fail.

### Mechanism 3
- Claim: Ground-truth contextual "thesaurus" constructed from high-fidelity XAI methods provides reliable demonstration examples for in-context learning.
- Mechanism: Multiple XAI methods (LIME, Anchors, Counterfactuals, Coefficients) are benchmarked on clustering results. LIME explanations achieve 0.93 fidelity and serve as ground truth. These validated examples populate the thesaurus, ensuring demonstration quality.
- Core assumption: High fidelity on clustering classification (99% accuracy, F1) transfers to meaningful feature importance for explanation generation.
- Evidence anchors:
  - [Section 3.2] "We acknowledge the very high average 0.93 fidelity of LIME explanations and, thus, use them as the ground-truth"
  - [Section 5.2] NDCG difference decreases from 0.07 (zero-shot) to 0.001 (few-shot) for LLaMA3
  - [corpus] Related surveys (arXiv:2510.12201) emphasize need for human-centered XAI evaluation but don't specifically validate thesaurus-based approaches
- Break condition: If ground-truth XAI method has poor fidelity (<0.8), demonstration examples will propagate errors into LLM outputs.

## Foundational Learning

- **In-Context Learning / Few-Shot Prompting**
  - Why needed here: The framework's effectiveness depends on providing demonstration examples in prompts rather than fine-tuning model weights. Zero-shot performance is poor (Spearman 0.01).
  - Quick check question: Can you explain why few-shot prompting outperforms zero-shot for structured tasks requiring domain-specific reasoning patterns?

- **XAI Methods (LIME, Feature Importance)**
  - Why needed here: The framework generates explanations that must align with traditional XAI outputs. Understanding how LIME computes local feature importance is essential for evaluating output quality.
  - Quick check question: Given a LIME explanation plot, can you identify which features contribute positively vs. negatively to a prediction?

- **Prompt Engineering for Structured Output**
  - Why needed here: The framework requires JSON-formatted technical output alongside natural language. Template design (Fig. 5a/5b) separates these components systematically.
  - Quick check question: How would you modify a prompt template to enforce a specific JSON schema in model output?

## Architecture Onboarding

- **Component map:** Ground-truth thesaurus builder -> Prompt template (Fig. 5a) -> System template (Fig. 5b) -> Evaluation layer
- **Critical path:** Thesaurus construction -> prompt template population -> few-shot prompting with ≥1 demonstration -> structured dual-output generation -> content quality validation against ground-truth
- **Design tradeoffs:**
  - Few-shot vs. zero-shot: Few-shot achieves 0.92 correlation but requires demonstration curation; zero-shot requires no examples but fails (0.01 correlation)
  - LLaMA3 vs. Mistral: LLaMA3 achieves better content quality (0.92 vs. 0.74 Spearman); Mistral shows more stable coherence
  - Single vs. multi-response: Single response ensures consistency but requires careful prompt engineering; multi-response allows independent optimization
- **Failure signatures:**
  - Low Spearman correlation (<0.5): insufficient demonstration examples or poor ground-truth quality
  - Inconsistent expert/non-expert outputs: prompt template constraints not enforced
  - High hallucination rate: zero-shot prompting with complex domain features
  - Grammar errors in user output (>1.0 average): model-specific; switch from Mistral to LLaMA3 or add grammar constraints
- **First 3 experiments:**
  1. **Zero-shot vs. few-shot comparison**: Run framework on 20 instances with zero-shot, one-shot, and few-shot (3 examples). Measure Spearman correlation with LIME ground-truth. Expect improvement from ~0.01 to >0.70.
  2. **Cross-domain generalization test**: Apply thesaurus from well-being clustering to a different domain (e.g., financial classification). If Spearman drops >30%, domain-specific demonstrations are critical.
  3. **User study replication**: Conduct A/B test (N=30 minimum) comparing LIME visualizations vs. framework natural language explanations. Measure UEQ pragmatic quality scores. Expect >0.5 improvement in clarity and ease of use.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the framework be extended to generate reliable explanations for non-meaningful features, such as those derived from dimensionality reduction techniques?
- Basis in paper: [explicit] The Limitations section states that the "current implementation supports only realistic features and future work could explore the integration of non-meaningful features (e.g., those derived from dimensionality reduction)."
- Why unresolved: The current framework relies on the semantic understanding of feature names (e.g., "sedentary minutes") by the LLM to generate human-centered text, which is not possible with abstract, latent features.
- What evidence would resolve it: A demonstration of the framework successfully providing explanations for models trained on PCA-reduced or embeddings-based features, evaluated against ground-truth importance of those latent dimensions.

### Open Question 2
- Question: How can LLMs be enhanced to accurately compute feature importance in zero-shot settings without relying on demonstration examples?
- Basis in paper: [explicit] In the Discussion, the authors ask "What is hidden behind our explanations?" and note that "LLMs struggle to accurately compute feature importance without prior context," emphasizing the need for research into zero-shot performance.
- Why unresolved: The study found that zero-shot prompting resulted in significant deviation from ground-truth rankings (Spearman correlation near 0.01), showing that LLMs currently depend heavily on in-context examples to "compute" importance.
- What evidence would resolve it: An improved prompt design or model configuration that achieves high rank correlation with ground-truth XAI methods on new domains without receiving prior domain-specific examples.

### Open Question 3
- Question: To what extent can refined prompting strategies consistently eliminate hallucinations in LLM-generated XAI outputs?
- Basis in paper: [explicit] The Discussion explicitly asks, "Do our explanations exhibit hallucinations?" and concludes that "hallucinations may not be entirely eliminated, highlighting the need for further research to refine LLM prompting strategies."
- Why unresolved: While the paper demonstrates high correlation with ground truth (0.92), it acknowledges that LLMs may still generate information not present in the input or foundational XAI method, a risk not fully quantified in the current evaluation.
- What evidence would resolve it: A dedicated factual consistency evaluation that measures the rate of unsupported claims in the natural language explanations relative to the raw feature data and model outputs.

## Limitations

- Framework performance is highly dependent on ground-truth thesaurus quality and may not generalize well to domains with different feature distributions or clustering results
- User study results (N=56) may be subject to self-selection bias and limited to well-being applications, with uncertain transferability to other domains
- Zero-shot prompting performance is poor (Spearman ~0.01), requiring demonstration examples that may not be available in all applications
- Current implementation supports only realistic features and cannot handle non-meaningful features derived from dimensionality reduction

## Confidence

- **High Confidence**: The few-shot prompting mechanism reliably improves Spearman correlation from 0.01 to 0.92 when ground-truth quality is high (≥0.8 fidelity)
- **Medium Confidence**: The dual-output template consistently produces aligned expert/non-expert explanations across different LLMs and domains
- **Low Confidence**: The user study results generalize to other domains beyond well-being applications and maintain the same magnitude of pragmatic quality improvements

## Next Checks

1. **Cross-Domain Fidelity Test**: Apply the framework to a different domain (e.g., medical diagnosis or financial risk assessment) using the same LifeSnaps thesaurus. Measure Spearman correlation degradation; expect >30% drop if domain-specific demonstrations are critical.

2. **Ground-Truth Quality Sensitivity**: Generate LIME explanations with varying fidelity levels (0.7, 0.8, 0.93) and measure corresponding framework performance. Establish the minimum fidelity threshold for reliable few-shot learning.

3. **Long-Term Stability Assessment**: Evaluate framework outputs over multiple time periods using the LifeSnaps longitudinal data. Test whether explanations remain consistent and accurate when user behavior patterns shift, indicating robustness to temporal drift.