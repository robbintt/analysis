---
ver: rpa2
title: A Convergence Analysis of Adaptive Optimizers under Floating-point Quantization
arxiv_id: '2510.21314'
source_url: https://arxiv.org/abs/2510.21314
tags:
- quantization
- lemma
- error
- term
- proof
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes the first theoretical framework for analyzing
  adaptive optimizers (Adam and Muon) under floating-point quantization, explicitly
  modeling quantization errors in gradients, weights, and optimizer states. The authors
  show that both algorithms maintain convergence rates close to their full-precision
  counterparts when mantissa length scales logarithmically with iterations.
---

# A Convergence Analysis of Adaptive Optimizers under Floating-point Quantization

## Quick Facts
- arXiv ID: 2510.21314
- Source URL: https://arxiv.org/abs/2510.21314
- Reference count: 40
- Primary result: First theoretical framework analyzing Adam and Muon under floating-point quantization, showing convergence rates close to full precision with logarithmic mantissa scaling.

## Executive Summary
This paper establishes the first theoretical framework for analyzing adaptive optimizers (Adam and Muon) under floating-point quantization, explicitly modeling quantization errors in gradients, weights, and optimizer states. The authors show that both algorithms maintain convergence rates close to their full-precision counterparts when mantissa length scales logarithmically with iterations. Adam is particularly sensitive to weights and second-moment quantization due to its reliance on β₂→1, while Muon requires weaker error control and is more robust. Theoretical results are validated through experiments on synthetic (Rosenbrock) and real (CIFAR-10) datasets, demonstrating that higher precision reduces quantization error and improves convergence, confirming the theoretical predictions. The work narrows the gap between empirical success and theoretical understanding of low-precision training methods.

## Method Summary
The paper models floating-point quantization by truncating mantissa bits while preserving sign and exponent, applying stochastic rounding to nearest representable values. Quantized Adam maintains first moment (m_t), second moment (v_t), and weight states in low precision, while quantized Muon uses an SVD-based sign operator. The theoretical analysis assumes smoothness, bounded variance, and relative quantization error |x_Q - x| ≤ q|x| with q = Θ(2^{-M}). Convergence is proven by analyzing error propagation through the optimization dynamics, showing that Adam requires q_V = O(1/T²) for second moments due to inverse square root amplification, while Muon only needs q_M = O(1/T^{1/2}) due to orthogonalization.

## Key Results
- Both quantized Adam and Muon achieve O(1/T^{1/4}) convergence rates when mantissa length scales as log T
- Adam requires significantly higher precision for weights and second moments (O(1/T²)) than gradients (O(1/T))
- Muon is theoretically more robust to quantization errors, requiring only O(1/T^{1/2}) precision across all components
- Empirical validation on Rosenbrock and CIFAR-10 confirms theoretical predictions about precision requirements

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Theoretical convergence can be guaranteed under floating-point quantization without relying on memory-intensive error-feedback mechanisms, provided the mantissa length scales logarithmically with training iterations.
- **Mechanism:** The paper models quantization error not as unbiased noise but as a bounded relative error (|x_Q - x| ≤ q|x|), which naturally decays exponentially with mantissa bits in floating-point formats. This assumption allows the analysis to decouple quantization noise accumulation from the optimizer's momentum dynamics, preventing the need to store per-parameter error terms.
- **Core assumption:** Assumption 3.1 (Quantization Error) holds; specifically, sign and exponent bits remain unchanged (no overflow/underflow), and relative error is bounded.
- **Evidence anchors:** [Section 3.1] Explicitly defines the relative error model: q = Θ(2^{-M}). [Theorem 4.5] Shows Quantized Adam achieves O(T^{-1/4}) convergence without error feedback. [Corpus] [2505.00347] discusses "Pushing the Limits of Low-Bit Optimizers" focusing on EMA dynamics, suggesting alternative compression strategies; this paper complements that by validating a specific error-bound approach.
- **Break condition:** If overflow/underflow occurs frequently (breaking Assumption 3.1) or if absolute error dominates relative error (e.g., fixed-point near zero), the theoretical guarantees may fail.

### Mechanism 2
- **Claim:** Adam exhibits distinct sensitivity to different components; the second moment estimate (v_t) and weights (w_t) require significantly higher precision than gradients (g_t) or first moments (m_t).
- **Mechanism:** The update rule involves dividing by √(v_t + ε). Due to Adam's reliance on β₂ → 1, errors in v_t accumulate over long horizons and are non-linearly amplified during the inverse square root operation. Theoretical analysis requires q_V = O(1/T²) to control this amplification, whereas gradients only require q_G = O(1/T).
- **Core assumption:** The objective is smooth non-convex (Assumption 4.3) and stochastic gradients are bounded (Assumption 4.2).
- **Evidence anchors:** [Section 4.1] Notes that second moments require stricter precision (q_V = O(1/T²)) due to the inverse square root. [Section 5] Corroborates empirical observations that weights and second moments require higher precision in practice (e.g., Peng et al., 2023). [Corpus] [2501.02423] "Scaling Laws for Floating Point Quantization Training" supports the focus on FP constituents, though this paper provides the specific component-wise error bounds.
- **Break condition:** If β₂ is not close to 1, the strict requirement on v_t precision may relax, but convergence speed might also degrade.

### Mechanism 3
- **Claim:** The Muon optimizer is theoretically more robust to quantization errors than Adam, requiring weaker error control conditions.
- **Mechanism:** Muon utilizes an SVD-based sign operator (U_t V_t^⊤) for updates. This orthogonalization step avoids the division by the historical gradient variance that characterizes Adam, thereby preventing the amplification of quantization errors by the inverse square root. It requires relative errors of only O(T^{-1/2}) compared to Adam's O(T^{-2}) for weights/second moments.
- **Core assumption:** Bounded gradient variance (Assumption 4.2 for Muon) and bounded initialization.
- **Evidence anchors:** [Theorem 4.6] Establishes convergence under q_W, q_G, q_M = O(T^{-1/2}). [Section 4.2] Explicitly contrasts the "weaker relative error conditions" of Muon vs. Adam. [Corpus] [2505.01043] highlights general challenges in low-precision LLM training; this mechanism offers a specific architectural solution via orthogonalization.
- **Break condition:** Robustness depends on the stability of the SVD computation itself; if SVD becomes unstable under extreme low precision (not modeled in Assumption 3.1), the guarantees may not hold.

## Foundational Learning

- **Concept: Floating-Point Relative Error vs. Absolute Error**
  - **Why needed here:** The paper's core theoretical contribution (Assumption 3.1) relies on modeling quantization as relative error (q ∝ 2^{-M}) rather than fixed absolute noise. Understanding this distinction is necessary to see why mantissa bits matter more than exponent bits for convergence bounds.
  - **Quick check question:** If we used fixed-point quantization where absolute error is constant, would the proof strategy in Theorem 4.5 still hold? (Hint: The paper assumes relative boundedness to scale error decay with magnitude).

- **Concept: Adam Hyperparameters (β₁, β₂) and Bias Correction**
  - **Why needed here:** The sensitivity of Adam to second-moment quantization is explicitly linked to the decay rate β₂ → 1. You must understand how β₂ controls the memory horizon of the optimizer to see why precision requirements scale as O(1/T²).
  - **Quick check question:** Why does a higher β₂ (e.g., 0.999) make the optimizer more sensitive to quantization errors in v_t?

- **Concept: Non-Convex Convergence Metrics**
  - **Why needed here:** The theorems guarantee convergence to a stationary point (gradient norm → 0) at a rate of O(T^{-1/4}), not convergence to a global minimum.
  - **Quick check question:** Does the paper prove that Quantized Adam finds a better solution than SGD, or just that it converges to a stationary point comparably to full-precision Adam?

## Architecture Onboarding

- **Component map:** Master Weights (W_t) -> Quantization Operator (Q) -> Quantized Weights (W_t^Q) -> Forward/Backward Pass -> Quantized Gradients (G_t^Q) -> Update States (M_t^Q, V_t^Q) -> Re-quantization
- **Critical path:** The stability of the update W_{t+1} = W_t - η_t * m_t/√(v_t) + ε. The quantization of v_t (second moment) is the critical bottleneck for Adam, as errors here are amplified by the division.
- **Design tradeoffs:**
  - **Adam:** High memory overhead (2 states); requires high precision (O(1/T²)) for v_t and weights to maintain convergence.
  - **Muon:** Lower memory sensitivity; requires SVD computation. The tradeoff is computational complexity (SVD) vs. robustness to low-precision states.
- **Failure signatures:**
  - **Adam:** Convergence stalls or diverges at sharp stationary points if mantissa length is insufficient for v_t (typically noticeable as gradient norms remaining high).
  - **Muon:** Failure of SVD convergence if matrix condition numbers are extreme, though theoretically more robust to quantization noise.
- **First 3 experiments:**
  1. **Mantissa Scaling:** Validate Theorem 4.5 by plotting gradient norm vs. iteration for varying mantissa lengths (e.g., M=4, 8, 16) on a Rosenbrock function to confirm O(1/T²) sensitivity.
  2. **Component Isolation:** Quantize *only* gradients, then *only* weights, then *only* second moments in Adam to empirically verify that v_t quantization causes the largest performance drop.
  3. **Muon vs. Adam Robustness:** Train a small MLP (CIFAR-10) with identical low-precision configurations (e.g., FP8) for both optimizers to confirm Muon's theoretical "weaker error control" advantage.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the convergence guarantees for quantized Adam and Muon be extended to (L₀, L₁)-smooth objectives, where the smoothness constant depends on gradient magnitude?
- **Basis in paper:** [explicit] "Our analysis assumes smoothness and bounded variance, while practical objectives may satisfy only weaker conditions; extending the framework to (L₀, L₁)-smoothness is a natural direction."
- **Why unresolved:** The current analysis relies on standard L-smoothness (Assumption 4.3), which may not capture the local geometry of practical deep learning objectives that exhibit gradient-dependent smoothness.
- **What evidence would resolve it:** Convergence theorems under (L₀, L₁)-smoothness showing how the bounds depend on both L₀ and L₁ parameters, with empirical validation on models exhibiting such smoothness.

### Open Question 2
- **Question:** How does integrating low-precision arithmetic operations (e.g., FP8 matrix multiplications during forward/backward passes) affect the convergence guarantees?
- **Basis in paper:** [explicit] "We also model quantized states under exact arithmetic, leaving open the integration of low-precision operations such as FP8 matrix multiplications."
- **Why unresolved:** The current framework assumes exact arithmetic for computations and only models quantization of stored states/weights/gradients, whereas real hardware performs low-precision arithmetic throughout.
- **What evidence would resolve it:** An extended theoretical framework incorporating quantized arithmetic error terms in the descent lemma, with experiments comparing simulated low-precision arithmetic to the current state-only quantization model.

### Open Question 3
- **Question:** How do the convergence guarantees change when considering communication efficiency in distributed low-precision training settings?
- **Basis in paper:** [explicit] "Finally, we do not consider communication efficiency, a key motivation for low-precision methods in distributed training."
- **Why unresolved:** Low-precision training is often motivated by communication savings in distributed settings, but the current single-node analysis does not account for gradient compression errors across workers or their interaction with optimizer state quantization.
- **What evidence would resolve it:** Convergence bounds for distributed Adam/Muon under combined communication compression and local quantization, identifying how qG, qW, qM, qV interact with compression ratios.

## Limitations
- Precision bounds assume relative error scaling (q ∝ 2^{-M}) without overflow/underflow; may fail for values spanning many orders of magnitude
- Convergence rate O(T^{-1/4}) applies to stationary points in smooth non-convex functions, not necessarily desirable minima
- Experiments limited to synthetic Rosenbrock and small-scale CIFAR-10; results may not generalize to modern large-scale models

## Confidence

- **High Confidence:** The theoretical framework and proofs for quantization error modeling and convergence rates are rigorous and internally consistent, assuming stated conditions hold.
- **Medium Confidence:** Empirical validation on Rosenbrock and CIFAR-10 supports the theoretical predictions, but the experiments are limited in scale and scope.
- **Low Confidence:** The claim that Muon is universally more robust to quantization than Adam is based on theoretical analysis and limited experiments; practical performance may depend heavily on problem structure and implementation details (e.g., SVD stability).

## Next Checks
1. **Mantissa Scaling Validation:** Run the Rosenbrock experiment with M ∈ {4, 8, 16, 32, 52} and plot gradient norm vs. iteration. Confirm that convergence degrades as M decreases, with Adam requiring higher M than Muon to achieve comparable gradient norms.
2. **Component Isolation Test:** In the Adam optimizer, independently quantize only gradients, only weights, only first moments, and only second moments. Measure convergence speed for each configuration to empirically verify that second moment quantization causes the largest performance drop, as predicted by the theory.
3. **Large-Scale Robustness Check:** Train a small ResNet or Transformer variant on CIFAR-10/CIFAR-100 with both Adam and Muon under identical FP8 quantization. Compare final accuracy and convergence stability to test whether Muon's theoretical robustness advantage holds in more complex architectures.