---
ver: rpa2
title: 'MTabVQA: Evaluating Multi-Tabular Reasoning of Language Models in Visual Space'
arxiv_id: '2506.11684'
source_url: https://arxiv.org/abs/2506.11684
tags:
- table
- data
- reasoning
- tables
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MTabVQA, a benchmark for evaluating vision-language
  models on multi-tabular reasoning from images. Existing benchmarks focus on single
  tables or non-visual formats, leaving a gap for assessing models that must interpret
  and reason across multiple visually rendered tables.
---

# MTabVQA: Evaluating Multi-Tabular Reasoning of Language Models in Visual Space

## Quick Facts
- arXiv ID: 2506.11684
- Source URL: https://arxiv.org/abs/2506.11684
- Authors: Anshul Singh; Chris Biemann; Jan Strich
- Reference count: 40
- Primary result: Vision-language models struggle with multi-tabular reasoning over visually rendered tables; fine-tuning on MTabVQA-Instruct dataset improves performance to 68.2% F1.

## Executive Summary
MTabVQA introduces a benchmark for evaluating vision-language models on multi-tabular reasoning tasks where information must be extracted and correlated across 2-5 visually rendered table images. Existing benchmarks focus on single tables or non-visual formats, leaving a gap for assessing models that must interpret complex visual layouts and perform multi-hop reasoning. The benchmark includes 3,745 QA pairs spanning 14 reasoning categories and demonstrates that both open-source and proprietary VLMs perform poorly in zero-shot settings, with F1 scores ranging from 18.4% to 61.7%.

## Method Summary
The MTabVQA benchmark is constructed by sampling relational tables from five datasets (Spider, BIRD, QFMTS, ATIS, MiMoTable), filtering for multi-table join queries, and rendering them as visually diverse PNG images with 10 styling themes. QA pairs are generated by converting SQL queries to natural language questions and verified through a three-stage LLM agent process followed by human review. For instruction tuning, the authors release MTabVQA-Instruct (15,853 pairs) and fine-tune Qwen2.5-VL-7B using Supervised Fine-Tuning with LoRA adapters (rank 128), achieving 68.2% F1 on the evaluation set.

## Key Results
- Zero-shot VLMs achieve F1 scores between 18.4% and 61.7% on MTabVQA, demonstrating significant difficulty with multi-tabular visual reasoning.
- TableVision (fine-tuned on MTabVQA-Instruct) achieves 68.2% F1, substantially outperforming zero-shot baselines at 35.1% F1.
- Data source diversity proves more critical than volume: Spider subset (2,395 samples) achieves 65.2% F1 vs. MultiTabQA subset (10,990 samples) at 30.2% F1.

## Why This Works (Mechanism)

### Mechanism 1: Domain-Specific Instruction Tuning
Fine-tuning VLMs with LoRA on multi-table visual QA data substantially improves cross-table reasoning performance. The instruction-tuning data distribution covers evaluation task reasoning patterns through SQL-to-question conversion and visual rendering diversity.

### Mechanism 2: Data Quality and Diversity Trump Scale
Models trained on smaller but more diverse datasets outperform those trained on larger but narrow data sources. The instruction-tuning effectiveness depends on data relevance and coverage of required reasoning patterns rather than raw sample count.

### Mechanism 3: Visual Layout Interpretation as Distinct Capability
VLMs must simultaneously handle OCR, layout understanding (merged cells, alignment), and cross-table correlation. Visual table images introduce noise and structural complexity absent in text/HTML representations, requiring separate visual parsing abilities.

## Foundational Learning

- **Multi-hop reasoning over structured data**:
  - Why needed here: MTabVQA requires correlating information across 2-5 tables with operations like joins, aggregations, and comparisons.
  - Quick check question: Given two tables (Customers: id, name; Orders: customer_id, amount), can you compute total spending per customer?

- **Visual document understanding / OCR limitations**:
  - Why needed here: Tables are rendered as images with varying fonts, colors, and layouts; models must handle visual noise without structured input.
  - Quick check question: Can you explain why OCR pipelines might fail on tables with merged cells or sparse grid lines?

- **Instruction tuning fundamentals (SFT vs. RL)**:
  - Why needed here: Paper compares SFT, GRPO, and CoT; understanding when each applies is critical for reproduction.
  - Quick check question: What is the key difference between SFT with LoRA and GRPO in terms of data requirements and reward signals?

## Architecture Onboarding

- **Component map**: Data sourcing (Spider, BIRD, QFMTS, ATIS, MiMoTable) -> SQL join filtering -> relational sampling -> Rendering (JSON tables -> 10 visual themes -> PNG) -> QA generation (SQL-to-question + LLM generation + verification) -> Fine-tuning (MTabVQA-Instruct -> SFT with LoRA on Qwen2.5-VL-7B) -> TableVision

- **Critical path**: 1) Download MTabVQA-Eval and MTabVQA-Instruct datasets from Hugging Face. 2) Load Qwen2.5-VL-7B as base VLM with LoRA rank 128. 3) Run SFT on MTabVQA-Instruct (15,853 samples). 4) Evaluate TableVision on Eval split using JSON prompt format.

- **Design tradeoffs**: SFT outperforms GRPO (55.9% vs 46.5% F1) and is simpler; data source selection critically affects generalization (Spider subset generalizes well to Spider split but not ATIS/MiMo); more visual themes improve robustness but may slow training.

- **Failure signatures**: Low EM with moderate F1 indicates partial/correct values but wrong format (check JSON schema); high performance on one split, near-zero on others indicates overfitting to specific layouts (increase data diversity); GRPO reward plateaus early suggests sparse reward function (consider intermediate rewards).

- **First 3 experiments**: 1) Zero-shot evaluation of base VLM on MTabVQA-Eval splits to establish baseline (report EM, F1 per split). 2) SFT with LoRA (rank 128) on Spider subset only (2,395 examples); evaluate on all splits to assess generalization. 3) Ablate data source: train on MultiTabQA subset vs. Spider subset vs. full MTabVQA-Instruct; compare overall F1 to confirm diversity benefit.

## Open Questions the Paper Calls Out
None

## Limitations
- Visual rendering pipeline introduces compounding error sources through OCR limitations and layout interpretation challenges that may not fully represent real-world table complexity.
- Instruction-tuning effectiveness appears highly dependent on data source composition, with narrow training sets underperforming despite larger sample sizes.
- Current VLM architectures fundamentally struggle with multi-tabular visual reasoning in zero-shot settings, with substantial performance gaps remaining.

## Confidence

**High Confidence Claims:**
- MTabVQA-Instruct fine-tuning improves TableVision performance to 68.2% F1 compared to 35.1% zero-shot baseline (Section 4.1)
- Data source diversity matters more than raw volume for instruction-tuning effectiveness (Section 4.3)
- Visual table interpretation requires distinct capabilities beyond text-based table understanding (Section 2)

**Medium Confidence Claims:**
- Current VLM architectures fundamentally struggle with multi-tabular visual reasoning in zero-shot settings
- The 10 visual styling themes provide sufficient diversity for robust training

**Low Confidence Claims:**
- The specific ranking of different VLMs by performance across all evaluation splits
- Generalization boundaries of the instruction-tuned models to unseen table layouts

## Next Checks

1. **Cross-dataset generalization test**: Evaluate TableVision on additional visual table datasets (beyond the three MTabVQA splits) to verify whether the 68.2% F1 represents true capability gain or overfitting to the training distribution.

2. **Visual diversity stress test**: Systematically vary table complexity parameters (merged cells, sparse grids, unusual fonts) beyond the current 10 styling themes to identify performance degradation thresholds and quantify the visual reasoning gap.

3. **Component ablation study**: Evaluate whether performance improvements stem primarily from LoRA fine-tuning of visual encoders, language decoders, or their joint optimization by training with frozen components and measuring incremental gains.