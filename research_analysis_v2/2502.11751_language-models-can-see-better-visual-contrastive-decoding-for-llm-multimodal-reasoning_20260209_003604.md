---
ver: rpa2
title: 'Language Models Can See Better: Visual Contrastive Decoding For LLM Multimodal
  Reasoning'
arxiv_id: '2502.11751'
source_url: https://arxiv.org/abs/2502.11751
tags:
- visual
- mvcd
- language
- arxiv
- decoding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the MVCD framework, which leverages Large
  Language Models' in-context learning capability and visual contrastive-example decoding
  (CED) to perform multimodal reasoning without additional training. The approach
  converts visual signals into text using modular visual perception modules (tags,
  attributes, captions) and applies CED to compare output probabilities with and without
  contextual examples, thus avoiding over-reliance on prior encoded knowledge.
---

# Language Models Can See Better: Visual Contrastive Decoding For LLM Multimodal Reasoning

## Quick Facts
- arXiv ID: 2502.11751
- Source URL: https://arxiv.org/abs/2502.11751
- Authors: Yuqi Pang; Bowen Yang; Haoqin Tu; Yun Cao; Zeyu Zhang
- Reference count: 33
- Introduces MVCD framework that improves LLM multimodal reasoning using visual-to-text conversion and contrastive decoding without training

## Executive Summary
This paper presents MVCD, a framework that enables frozen LLMs to perform multimodal visual reasoning by converting visual inputs into structured text representations (tags, attributes, captions) and applying contrastive-example decoding. The approach leverages the LLM's in-context learning capability by providing matched examples from the dataset. Experiments across five VQA datasets with four different LLMs show consistent accuracy improvements over baseline decoding methods, demonstrating enhanced visual perception and reasoning without additional training.

## Method Summary
MVCD converts visual signals to text using modular visual perception modules (CLIP for tags/attributes, BLIP for captions, specialized models for video) and applies contrastive-example decoding (CED) to compare output probabilities with and without contextual examples. The framework uses in-context learning with question-type-matched examples to guide reasoning patterns. At each decoding step, CED computes the difference between log probabilities with contextual examples versus without, applying adaptive truncation to prevent over-penalization of correct prior knowledge. This approach allows frozen LLMs to leverage visual information without parameter updates.

## Key Results
- MVCD consistently improves accuracy across four LLMs (LLaMA2-7B/13B, LLaMA3-8B, Qwen2-7B) on five VQA datasets
- Performance gains are modest but consistent (1-3% accuracy improvements)
- Question-type-matched example selection significantly outperforms random selection (6-7% accuracy difference)
- Ablation shows all three visual modalities (tags, attributes, captions) contribute complementarily to performance

## Why This Works (Mechanism)

### Mechanism 1
Converting visual signals to structured text representations enables frozen LLMs to perform multimodal reasoning without parameter updates. The visual perception module extracts tags, attributes, and captions which are concatenated as descriptive features, allowing the LLM to process visual content through existing language understanding capabilities.

### Mechanism 2
Contrastive-example decoding (CED) amplifies contextually-relevant knowledge while suppressing prior knowledge that conflicts with visual evidence. CED computes log differences between output probabilities with and without contextual examples, with adaptive constraints preventing over-penalization of correct prior knowledge.

### Mechanism 3
In-context learning with question-type-matched examples provides implicit task specification that guides multimodal reasoning patterns. k examples are selected based on question type matching, allowing the LLM to infer reasoning patterns without explicit instruction tuning.

## Foundational Learning

- **Concept: Contrastive decoding**
  - Why needed here: CED builds on prior contrastive decoding methods by contrasting distributions with/without examples rather than early/late layers
  - Quick check question: Can you explain why subtracting log probabilities amplifies tokens that are more probable under one distribution than another?

- **Concept: In-context learning (ICL)**
  - Why needed here: The entire framework relies on ICL to transfer task patterns without gradient updates
  - Quick check question: How does providing k demonstration examples in the prompt enable task learning without weight changes?

- **Concept: Visual-language alignment via CLIP**
  - Why needed here: The tag/attribute modules depend on CLIP's pretrained vision-language alignment to extract semantically meaningful text from images
  - Quick check question: Why does CLIP enable zero-shot classification by comparing image and text embeddings?

## Architecture Onboarding

- **Component map**: Visual Module → Text Fusion → CED Decoder → Frozen LLM
- **Critical path**: Image/Video → Visual Module → Text Features → + ICL Examples → LLM forward pass (with and without examples) → CED score computation → next token selection
- **Design tradeoffs**: α=0.1 (aggressive truncation) vs. higher α; N=5 features per modality; 3-5 shot ICL
- **Failure signatures**: Random example selection causes 6-7% accuracy drop; >5 shots cause irregular performance fluctuations; missing attribute module hurts more in CED than in baseline
- **First 3 experiments**:
  1. Run MVCD with random vs. question-type example selection on VQAv2 subset (n=500) to verify selection strategy matters
  2. Test α ∈ {0.05, 0.1, 0.2, 0.5} on OKVQA to confirm 0.1 is optimal
  3. Remove each visual modality individually and measure impact on VQAv2 vs. OKVQA to verify complementary contributions

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of MVCD vary when swapping the modular visual perception components (CLIP, BLIP, Video-LLaVA) for smaller or alternative pre-trained models? The paper does not include comparative experiments with different pretrained models, which will be explored in future research.

### Open Question 2
What underlying mechanisms cause the observed performance saturation and irregular fluctuations as the number of in-context examples increases? The paper empirically observes these effects but does not explain if this is due to attention dilution or context length limits.

### Open Question 3
How robust is the Contrastive-Example Decoding (CED) strategy when the intermediate textual descriptions (tags/attributes) contain noise or hallucinations? The framework relies entirely on the accuracy of visual-to-text conversion, but error propagation from visual modules to LLM is not tested.

## Limitations

- Exact prompt templates for attribute extraction and ICL formatting are referenced but not fully disclosed, creating uncertainty about exact text representation
- The taxonomy and matching algorithm for question-type-based example selection are unspecified
- Only video caption generation specifies top-k=50, while image caption generation parameters remain undocumented

## Confidence

**High confidence**: The core mechanism of combining visual perception modules with contrastive-example decoding is well-supported. Ablation studies demonstrate that removing any visual modality consistently degrades performance.

**Medium confidence**: The claim of outperforming existing efficient MLLM approaches is supported but based on a limited comparison set. Performance improvements are modest (1-3%).

**Low confidence**: The assertion that MVCD avoids "over-reliance on prior encoded knowledge" is theoretically sound but difficult to empirically verify from the presented results.

## Next Checks

1. Apply MVCD to a held-out VQA dataset not used in the original experiments (e.g., VizWiz or ScienceQA) to verify generalization
2. Use attention visualization tools to examine whether MVCD with ICL examples shows different attention patterns compared to baseline decoding
3. Test MVCD on images with heavy occlusion, text-on-image, or complex compositional scenes to evaluate breakdown points in visual understanding