---
ver: rpa2
title: Cisco Integrated AI Security and Safety Framework Report
arxiv_id: '2512.12921'
source_url: https://arxiv.org/abs/2512.12921
tags:
- security
- safety
- framework
- taxonomy
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Cisco\u2019s Integrated AI Security and Safety Framework is a\
  \ unified, lifecycle-aware taxonomy that consolidates AI security threats and content\
  \ harms into a single structure. It integrates five key dimensions: AI threats and\
  \ safety, lifecycle awareness, multi-agent coordination, multi-modality, and practical\
  \ utility."
---

# Cisco Integrated AI Security and Safety Framework Report

## Quick Facts
- arXiv ID: 2512.12921
- Source URL: https://arxiv.org/abs/2512.12921
- Reference count: 32
- Unified, lifecycle-aware taxonomy consolidating AI security threats and content harms into 19 objectives, 40 techniques, 112 subtechniques, and 25 harmful content categories.

## Executive Summary
Cisco's Integrated AI Security and Safety Framework provides a unified taxonomy that consolidates AI security threats and content harms into a single, lifecycle-aware structure. The framework addresses 19 objectives, 40 techniques, 112 subtechniques, and 25 harmful content categories across cybersecurity, safety harms, integrity, IP, and privacy. Designed for operational activities like red-teaming, threat modeling, and risk prioritization, it aims to provide organizations with a common language and actionable guidance to defend AI systems across the full lifecycle while aligning with global policy frameworks and emerging AI regulations.

## Method Summary
The framework synthesizes existing frameworks (MITRE ATLAS, NIST AI 100-2, OWASP Top 10) into a four-level hierarchical taxonomy: Objectives → Techniques → Subtechniques → Procedures. It covers the AI lifecycle from data collection through incident response, unifying security threats and content safety harms. The taxonomy was developed through systematic consolidation of fragmented guidance, creating a comprehensive structure that supports operational activities like red-teaming and risk prioritization while maintaining extensibility for emerging threats.

## Key Results
- 19 objectives, 40 techniques, 112 subtechniques, and 25 harmful content categories across multiple domains
- Lifecycle-aware mapping enables stage-appropriate controls from data collection through incident response
- Unified security-safety taxonomy reveals attack-safety intersections that siloed frameworks miss
- Designed for multi-agent and multi-modal AI systems with practical operational utility

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical four-level taxonomy enables systematic translation of strategic adversarial intent into testable, mitigable attack patterns.
- Mechanism: Objectives capture attacker goals, techniques capture attack methods, subtechniques capture implementation variations, and procedures capture exact execution details, allowing defenders to trace from high-level risk priorities down to concrete test cases.
- Core assumption: Adversarial behavior can be meaningfully decomposed into nested abstraction levels that remain stable enough to catalog while still capturing emerging variations.
- Evidence anchors: 19 objectives, 40 techniques, 112 subtechniques; hierarchical operation described in Section 2.4; related work uses similar taxonomic approaches.
- Break condition: If new attack techniques emerge faster than taxonomy can be updated, or if multi-stage attacks resist hierarchical decomposition.

### Mechanism 2
- Claim: Lifecycle-aware mapping allows defenders to implement stage-appropriate controls, since threats manifest differently during data collection, training, deployment, and runtime.
- Mechanism: Each lifecycle phase has distinct attack surfaces (poisoning during training, prompt injection at runtime), supporting defense-in-depth where controls at one stage compensate for gaps at another.
- Core assumption: The AI lifecycle can be meaningfully segmented into discrete phases with identifiable threat transfer points between them.
- Evidence anchors: Unified lifecycle-aware taxonomy; data poisoning attacks occur during training while prompt injection threatens deployed systems; lifecycle emphasis in related frameworks.
- Break condition: If AI systems blur lifecycle boundaries (continuous online learning) or attacks span multiple phases simultaneously.

### Mechanism 3
- Claim: Unifying security threats and content harms into one taxonomy reveals attack-safety intersections that siloed frameworks miss.
- Mechanism: Adversaries often combine techniques—using prompt injection (security threat) to bypass filters and generate hate speech (content harm)—and a unified taxonomy maps these intersections for combined red-teaming and holistic control design.
- Core assumption: Content harms and security threats share exploitable causal pathways that are practically relevant to defend against simultaneously.
- Evidence anchors: Consolidation of security threats and content harms; adversaries use threat techniques specifically to force safety-violating content generation; related frameworks don't explicitly integrate content safety at this granularity.
- Break condition: If organizations require separate compliance reporting for security vs. safety, or if detection systems fundamentally differ between the two domains.

## Foundational Learning

- Concept: TTP (Tactics, Techniques, Procedures) Framework from Cybersecurity
  - Why needed here: The taxonomy adopts ATT&CK-style hierarchies; understanding TTP conventions helps navigate objectives, techniques, and subtechniques intuitively.
  - Quick check question: Can you explain the difference between an attacker's objective and their technique?

- Concept: AI Lifecycle Phases (Data → Training → Deployment → Runtime → Incident Response)
  - Why needed here: The framework's "lifecycle awareness" requires knowing where in development each threat applies.
  - Quick check question: At which lifecycle stage does prompt injection primarily manifest? What about data poisoning?

- Concept: Security vs. Safety in AI Systems
  - Why needed here: The framework explicitly distinguishes AI security (protecting systems from attacks) from AI safety (ensuring ethical, aligned behavior), then integrates them.
  - Quick check question: Is a jailbreak that generates hate speech primarily a security or safety issue? Why does the framework say it's both?

## Architecture Onboarding

- Component map:
  - Main Taxonomy Hierarchy: 19 Objectives (OB-XXX) → 40 Techniques (AITech-X.X) → 112 Subtechniques (AISubtech-X.X.X) → Procedures (uncounted, rapidly evolving)
  - Three Specialized Taxonomies: MCP Threats (14 types in 4 groups), Supply Chain Threats (22 types in 4 groups), Harmful Content (25 categories in 5 groups)
  - Operational Layers: Content safety, Runtime security, Agentic security, Data security, Supply chain security, Infrastructure security

- Critical path:
  1. Map your AI system architecture (RAG, agents, tools, modalities) to relevant objectives
  2. Identify high-priority techniques and subtechniques based on risk appetite and deployment context
  3. Design preventive/detective/corrective controls for each mapped threat
  4. Implement red-teaming against prioritized objectives with combined security-safety test cases
  5. Establish monitoring for procedure-level indicators

- Design tradeoffs:
  - Breadth vs. Actionability: 112 subtechniques provide coverage but require prioritization to avoid alert fatigue
  - Static vs. Evolving: Taxonomy is published at a point in time; procedures change faster than techniques
  - Unified vs. Siloed Compliance: May conflict with organizations that report security and safety to different teams/regulators

- Failure signatures:
  - Red-teaming finds novel attack variants not in taxonomy → indicates need for extension
  - Controls pass objective-level tests but fail on combined security-safety scenarios → suggests integration gap
  - Supply chain compromise occurs despite model scanning → indicates missing artifact/format vulnerability coverage

- First 3 experiments:
  1. **Objective Mapping Exercise**: Select 3-5 objectives most relevant to your deployment (e.g., OB-001 Goal Hijacking, OB-008 Data Privacy Violations, OB-009 Supply Chain Compromise), enumerate all techniques/subtechniques, and map existing controls to identify gaps.
  2. **Security-Safety Intersection Test**: Design red-team scenarios that combine one technique (e.g., AITech-2.1 Jailbreak) with one harmful content category (e.g., Hate Speech) to test whether combined defenses catch both dimensions.
  3. **Supply Chain Artifact Scan**: Use the supply chain taxonomy's 22 threat types to build or evaluate a model scanning pipeline, testing against at least one indicator from each of the four groups (Artifact/Format, Model Manipulation, Dependency, Runtime).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the framework's high-level objectives (e.g., Goal Hijacking) be concretely mapped to granular technical controls to enable automated risk management?
- Basis in paper: The authors state that while the taxonomy provides a basis for risk management, the initial release "does not comprehensively capture these [preventive, detective, and corrective controls]."
- Why unresolved: The paper provides the threat classification but leaves the specific implementation of defense-in-depth controls and their integration into organizational governance as future work.
- What evidence would resolve it: A supplementary control catalog mapping specific technical mitigations to each of the 19 objectives and their subtechniques.

### Open Question 2
- Question: How can existing threat intelligence standards like STIX/TAXII be semantically extended to capture AI-specific risks without losing interoperability?
- Basis in paper: The paper posits that extending STIX/TAXII with fields aligned to the AI Security Framework would bolster collective defense, but does not define the schema.
- Why unresolved: Current STIX objects are designed for traditional malware and network threats; representing concepts like "prompt injection" or "model poisoning" requires new ontological definitions that standard bodies have not yet adopted.
- What evidence would resolve it: A formal schema extension proposal or a proof-of-concept integration demonstrating how AI threat intelligence is serialized and exchanged via TAXII.

### Open Question 3
- Question: Can interpretability research yield reliable, modality-agnostic detection methods for harmful content that work across text, audio, and vision?
- Basis in paper: The authors note that while harmful content categories are consistent, detection varies by modality. They suggest "advancements in interpretability research... may yield modality-agnostic approaches."
- Why unresolved: Current detection often relies on modality-specific classifiers (NLP for text, computer vision for images). A unified internal representation mechanism remains a theoretical goal rather than a deployed capability.
- What evidence would resolve it: Research demonstrating a single detection model that identifies a safety violation (e.g., hate speech) effectively regardless of whether it is input as text, an image, or an audio stream.

### Open Question 4
- Question: What evaluation methodologies are required to effectively red-team emergent behaviors in multi-agent systems where risks arise from inter-agent communication?
- Basis in paper: While the framework catalogs multi-agent threats, the paper notes that existing red-teaming focuses on single models and lacks standardized tests for "orchestration abuse" or "inter-agent communication protocols."
- Why unresolved: Dynamic agent interactions create non-deterministic attack surfaces that are difficult to cover with static test cases or standard penetration testing approaches.
- What evidence would resolve it: The release of the promised "agentic taxonomy" mentioned in Section 3 alongside a standardized evaluation suite for testing agent collusion and orchestration failures.

## Limitations
- The taxonomy's practical efficacy remains untested in real-world deployments beyond Cisco's internal use cases, with no published evidence of successful threat prevention or detection using this specific structure in production environments.
- The rapid evolution of AI attack techniques, particularly in multi-agent and multi-modal contexts, may outpace the framework's update cycle, creating a gap between theoretical coverage and operational applicability.
- Integration with existing security operations centers (SOCs) and compliance frameworks is assumed but not demonstrated, potentially creating friction for organizations with established security processes.

## Confidence
- High Confidence: The hierarchical structure (Objectives → Techniques → Subtechniques) is well-established from cybersecurity frameworks like MITRE ATT&CK and provides a logical foundation for threat categorization.
- Medium Confidence: The lifecycle-aware approach is theoretically sound and addresses known AI-specific threat vectors, but empirical validation across diverse deployment scenarios is limited.
- Low Confidence: The claim that unification of security and safety domains provides practical operational benefits lacks comparative studies showing improved outcomes versus siloed approaches.

## Next Checks
1. **Operational Integration Test**: Deploy the framework in a controlled environment with existing security tools and measure false positive/negative rates compared to current detection methods. Track time-to-detection for simulated attacks mapped to the taxonomy.
2. **Cross-Organizational Compatibility Assessment**: Survey 3-5 organizations from different sectors (e.g., healthcare, finance, government) to identify integration challenges with their existing compliance frameworks, incident response procedures, and team structures.
3. **Update Responsiveness Evaluation**: Monitor the framework's evolution over 6-12 months by tracking published updates, comparing them against newly disclosed AI attack techniques, and measuring the time lag between emergence and taxonomy coverage.