---
ver: rpa2
title: 'Policy Learning with a Natural Language Action Space: A Causal Approach'
arxiv_id: '2502.17538'
source_url: https://arxiv.org/abs/2502.17538
tags:
- language
- text
- learning
- natural
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses multi-stage decision-making in natural language
  action spaces where outcomes are only observed after a sequence of actions. The
  authors propose a causal framework that employs Q-learning to estimate Dynamic Treatment
  Regimes through a single model, enabling data-efficient policy learning via gradient
  ascent on language embeddings.
---

# Policy Learning with a Natural Language Action Space: A Causal Approach

## Quick Facts
- arXiv ID: 2502.17538
- Source URL: https://arxiv.org/abs/2502.17538
- Authors: Bohan Zhang; Yixin Wang; Paramveer S. Dhillon
- Reference count: 40
- One-line primary result: Gradient ascent on language embeddings with Q-learning achieves superior transfer strength in multi-stage text transformation tasks

## Executive Summary
This paper addresses multi-stage decision-making in natural language action spaces where outcomes are only observed after a sequence of actions. The authors propose a causal framework that employs Q-learning to estimate Dynamic Treatment Regimes through a single model, enabling data-efficient policy learning via gradient ascent on language embeddings. A key technical contribution is a decoding strategy that translates optimized embeddings back into coherent natural language. The approach is evaluated on mental health intervention, hate speech countering, and sentiment transfer tasks, demonstrating significant improvements over competitive baselines.

## Method Summary
The method employs Q-learning to estimate Dynamic Treatment Regimes (DTR) through a single model, enabling data-efficient policy learning via gradient ascent on language embeddings. A specialized encoder-decoder language model is fine-tuned on a "Repeat" task to decode optimized embeddings back into coherent natural language. The approach trains separate task-specific classifiers at each decision stage, using backward induction to propagate the final outcome signal. During inference, gradient ascent is performed on the encoder's output embeddings to maximize the classifier's predicted probability of a positive outcome, with the optimized embeddings then decoded to produce the final text action.

## Key Results
- Achieves superior transfer strength while maintaining content preservation and fluency across three benchmark tasks
- Outperforms previous approaches by up to 30% on key benchmarks for text transformation tasks
- Human evaluations show more balanced performance across fluency, content preservation, and transfer strength
- Particularly strong performance in transfer strength metrics compared to competitive baselines

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Gradient ascent on language embeddings, guided by a task classifier, allows for efficient optimization of text actions without direct text manipulation.
- **Mechanism:** The authors approximate the Q-function at each decision stage using a text classifier ($f_t$). Instead of generating text and evaluating it, they perform gradient ascent on the input text's embedding representation to maximize the classifier's predicted probability of a positive outcome ($P_{f_t}(y^+|a_t, h_t)$).
- **Core assumption:** The task classifier's decision boundary in the embedding space is smooth enough that gradient ascent can find embeddings corresponding to semantically valid and higher-performing text actions.
- **Evidence anchors:**
  - [abstract] "...gradient ascent on language embeddings. A key technical contribution... is a decoding strategy that translates optimized embeddings back into coherent natural language."
  - [section] Section 3.2 "Q-learning with text representations": "We then perform Q-learning with the text representations: we maximize the output confidence of the positive outcome from $f_t$ by performing gradient ascent on the high-dimensional representation of natural language actions."
  - [corpus] Corpus signals are weak on this specific gradient-ascent-on-embedding technique.
- **Break condition:** The classifier is overfitted, poorly calibrated, or its gradients in the embedding space do not correspond to meaningful semantic shifts, causing the ascent to produce incoherent or adversarial embeddings.

### Mechanism 2
- **Claim:** The specialized "Repeat" training of an encoder-decoder language model enables the decoding of optimized, non-grounded embeddings back into fluent natural language.
- **Mechanism:** The authors fine-tune a T5 model to simply repeat its input ("Repeat [TEXT]: " â†’ "TEXT"). This establishes a stable, invertible-like mapping between an embedding and its text. After gradient ascent finds an optimal embedding, the decoder portion of this model is used to "decode" the modified embedding back into a text sequence.
- **Core assumption:** The "Repeat" fine-tuning is sufficiently robust that small, classifier-guided perturbations in the encoder's embedding space can still be faithfully and fluently decoded by the model.
- **Evidence anchors:**
  - [abstract] "A key technical contribution of our approach is a decoding strategy that translates optimized embeddings back into coherent natural language."
  - [section] Section 3.2 "Decoding optimal text representations into natural language": "We train a specialized encoder-decoder language model... [that] provides an implicit mapping of the encoder representation... to the natural language output."
  - [corpus] No direct corpus support found.
- **Break condition:** The gradient-updated embedding moves into a region of the latent space that the decoder cannot map to a coherent text, resulting in garbage or nonsensical output.

### Mechanism 3
- **Claim:** Framing multi-stage decision-making with delayed rewards as a Q-learning problem with pseudo-outcomes allows for data-efficient policy learning from limited observational data.
- **Mechanism:** The algorithm works backward from the final observed outcome ($Y_T$). At each preceding stage $t$, it constructs a "pseudo-outcome" ($\tilde{Y}_{t-1}$) based on the maximum Q-value predicted for the subsequent stage. A classifier is then trained on this pseudo-outcome, propagating the final reward signal backward and enabling the model to learn which earlier actions lead to better final outcomes without online exploration.
- **Core assumption:** The observed historical data contains sufficient variation to estimate the Q-function and the key confounders (historical text) can be adequately captured by the language model embeddings.
- **Evidence anchors:**
  - [abstract] "Our approach employs Q-learning to estimate Dynamic Treatment Regimes (DTR) through a single model, enabling data-efficient policy learning..."
  - [section] Section 3.1 "Q-learning for estimating optimal dynamic treatment rules": "Q-learning in dynamic treatment regimes considers a pseudo outcome at each stage, $\tilde{Y}_t = \max_{a_{t+1}} Q_{t+1}(H_{t+1}, a_{t+1})$..."
  - [corpus] Related work (Section 2) links this to Dynamic Treatment Regimes, a known method. The paper claims an improvement over PPO which requires multiple models and more data.
- **Break condition:** The observational dataset is too small, lacks diversity (e.g., all sequences follow a similar pattern), or suffers from unobserved confounding not captured in the text history, leading to spurious correlations in the learned Q-function.

## Foundational Learning

- **Concept:** Q-learning and Dynamic Treatment Regimes (DTR).
  - **Why needed here:** This is the core causal inference framework used to handle delayed rewards and sequential decisions. Understanding it is necessary to grasp how the paper propagates the final outcome backward to earlier decision points.
  - **Quick check question:** If you have a two-step process and a reward only at the end, how would Q-learning be used to determine the value of the action taken at step 1?

- **Concept:** Gradient Ascent in Embedding Space.
  - **Why needed here:** The paper's main technical contribution is optimizing text not by generating it but by directly manipulating its continuous vector representation to maximize a classifier's output.
  - **Quick check question:** What is the potential risk of performing gradient ascent on a high-dimensional text embedding? (Hint: think about the manifold of natural language).

- **Concept:** Encoder-Decoder Architectures (e.g., T5).
  - **Why needed here:** The decoding strategy relies on a specific fine-tuning ("Repeat") of an encoder-decoder model to translate an optimized embedding back into text.
  - **Quick check question:** How does the "Repeat" fine-tuning objective differ from a standard sequence-to-sequence task like translation or summarization?

## Architecture Onboarding

- **Component map:** T5 Encoder-Decoder (fine-tuned for Repeat task) -> Task-Specific Classifiers ($f_t$) -> Optimizer -> GPT-2-Large Scoring Model

- **Critical path:**
  1. Fine-tune T5: Train T5 on the "Repeat [text] -> [text]" task.
  2. Backward Training: Starting from the last stage ($T$), train a classifier ($f_T$) on the final outcome ($Y_T$). Then, move to stage $T-1$, train a classifier ($f_{T-1}$) on the pseudo-outcome derived from $f_T$, and so on.
  3. Forward Inference: For a new input at stage $t$, get its T5 encoder embedding. Perform gradient ascent on this embedding to maximize the output of the trained classifier $f_t$. Use the T5 decoder to translate the optimized embedding back to text.

- **Design tradeoffs:**
  - **Data Efficiency vs. Complexity:** This method is claimed to be more data-efficient than PPO (which requires policy, value, and reward models) but introduces the complexity of a specialized decoding strategy and multi-stage classifier training.
  - **Embedding Ascent vs. Direct Generation:** Optimizing in embedding space avoids the high cost of searching over discrete text tokens directly but places a strong reliance on the "Repeat" model's ability to decode arbitrary embeddings fluently.

- **Failure signatures:**
  - **Incoherent Decoding:** The decoded text is grammatically incorrect or nonsensical. This indicates the optimized embedding is out-of-distribution for the "Repeat" T5 decoder.
  - **Low Transfer Strength:** The model fails to change the text to achieve the desired outcome. This could mean the classifier is not learning a meaningful boundary or the gradient ascent is stuck in a local optimum.
  - **Poor Content Preservation:** The output text is fluent and effective but has lost the original meaning. This suggests the optimization path did not preserve semantic content.
  - **PPO Non-Convergence:** The paper explicitly notes that a PPO baseline failed to converge due to limited data and delayed rewards (Appendix D.3), which their method is designed to overcome.

- **First 3 experiments:**
  1. **Validate Decoding Pipeline:** Implement the "Repeat" fine-tuning for a standard encoder-decoder model (like a small T5 or BART). Verify that a small, random perturbation of an input's embedding can still be decoded into coherent text.
  2. **Single-Stage Ascent Test:** Train a simple sentiment classifier on top of the frozen encoder. For a negative sentence, perform gradient ascent on its embedding to maximize the "positive" class probability. Decode the result and check if the sentiment has flipped.
  3. **Two-Stage Q-Learning Test:** Create a small synthetic dataset with clear signals (e.g., word "good" = positive). Implement the two-stage training loop where the second stage's classifier is trained on the final label, and the first stage's classifier is trained on a pseudo-label derived from the second. Test if the system can successfully identify and modify negative signals in the first stage to improve the final outcome.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the embedding optimization and decoding strategy be effectively adapted to decoder-only language models without architectural modifications?
- **Basis in paper:** [explicit] The authors state in Section 3.2 that the decoding algorithm is theoretically applicable to decoder-only models, but they note "While we use a T5 model... in the empirical studies, we could have also used a decoder-only model," without providing experimental validation for this claim.
- **Why unresolved:** The method relies on a T5 encoder-decoder structure to map optimized embeddings back to text. Decoder-only models function differently regarding embedding injection and generation, and it is unclear if "feeding modified embeddings back" maintains fluency and causality in that architecture.
- **What evidence would resolve it:** Experimental results applying the gradient ascent and decoding pipeline to a decoder-only backbone (e.g., GPT-2 or Llama) on the same benchmarks.

### Open Question 2
- **Question:** To what extent does the performance on synthetically generated interventions (IHS) generalize to real-world clinical text data?
- **Basis in paper:** [inferred] The paper evaluates on the Intern Health Study (IHS) but notes, "Since the actual textual interventions are not publicly available, we used ChatGPT [38] to generate interventions... While these interventions are synthetically generated, they are grounded in real PHQ-9 trajectories."
- **Why unresolved:** Synthetic data generated by LLMs often exhibits cleaner patterns and lower noise than organic human data. The success of the Q-learning policy may rely on the structure of the synthetic text, which might not exist in messy, real-world clinical notes.
- **What evidence would resolve it:** Evaluation of the proposed method on a dataset containing real human-authored medical interventions and observed patient outcomes, rather than synthetic proxies.

### Open Question 3
- **Question:** How does the error propagation in pseudo-outcome estimation scale with the number of stages in decision-making?
- **Basis in paper:** [inferred] The experimental setup is restricted to a "two-stage setup," though the framework theoretically supports more. The algorithm relies on backward induction where $\tilde{Y}_{t-1} \leftarrow Q_t(H_{it}, a^*_{it})$, propagating estimation errors from future stages to earlier ones.
- **Why unresolved:** In Q-learning, estimation errors in later stages (the "max" operator) can compound when calculating optimal actions for earlier stages. It is unknown if the "data-efficient" single-model approach remains stable or if performance degrades sharply as the horizon $T$ increases beyond 2.
- **What evidence would resolve it:** Empirical analysis of performance and stability on sequential tasks with 3, 5, or 10 stages to observe any degradation due to error accumulation.

### Open Question 4
- **Question:** Is the "Two-Time Sampling" (TTS) heuristic sufficient to guarantee convergence in the non-convex embedding space?
- **Basis in paper:** [inferred] The authors introduce a Two-Time Sampling (TTS) method specifically to address "gradient ascent plateaus," implying the optimization landscape is difficult and standard gradient ascent frequently gets stuck.
- **Why unresolved:** Relying on random restarts (updating the random seed) suggests the loss landscape is non-convex or flat. The paper does not analyze if TTS guarantees finding a global optimum or just a better local optimum, which is critical for high-stakes domains like mental health.
- **What evidence would resolve it:** Visualization of the loss landscape during embedding optimization or a comparison with more advanced optimization techniques (e.g., second-order methods) to determine if the plateau issue is structural or solvable.

## Limitations

- **Synthetic data dependency:** The mental health intervention dataset relies on synthetically generated text rather than real clinical interventions, limiting generalizability to actual clinical settings.
- **Two-stage restriction:** All experimental evaluations are limited to two-stage decision processes, leaving uncertainty about how the approach scales to longer sequences with more decision points.
- **Decoder dependency:** The approach heavily depends on the specialized "Repeat" fine-tuning of T5, and it's unclear whether this decoding strategy generalizes to other domains or encoder-decoder architectures.

## Confidence

- **High confidence:** The core causal framework using Q-learning for DTRs is well-established. The experimental results showing improvements over baselines on all three tasks are clearly presented and reproducible.
- **Medium confidence:** The novel decoding strategy using gradient ascent on embeddings shows promise, but the limited analysis of failure modes and the sensitivity of this approach to initialization and hyperparameters reduce confidence in its robustness.
- **Low confidence:** Claims about the method's data efficiency relative to PPO are based on a single comparison, and the PPO baseline's failure to converge may be due to implementation details rather than fundamental limitations of the approach.

## Next Checks

1. **Ablation on decoding strategy:** Run experiments comparing the gradient ascent + Repeat decoder approach against direct text generation baselines (like conditional T5 or GPT) to isolate the contribution of the novel decoding method.

2. **Embedding space analysis:** Visualize the embedding trajectories during gradient ascent for successful and failed examples to understand what properties make certain inputs more amenable to this optimization approach.

3. **Cross-domain generalization:** Test the approach on a held-out domain (e.g., toxicity detection or formality transfer) to evaluate whether the "Repeat" fine-tuning and gradient ascent approach generalizes beyond the three tasks studied.