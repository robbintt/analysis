---
ver: rpa2
title: Extending Audio Context for Long-Form Understanding in Large Audio-Language
  Models
arxiv_id: '2510.15231'
source_url: https://arxiv.org/abs/2510.15231
tags:
- audio
- yarn
- context
- mins
- partial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the limitation of short audio context windows
  in Large Audio-Language Models (LALMs), which hinders long-form audio understanding.
  The core method idea involves introducing Partial YaRN, a training-free, modality-decoupled
  context extension method that modifies only audio token positions while preserving
  text capabilities, and Virtual Longform Audio Training (VLAT), a training strategy
  that simulates diverse audio lengths during training for improved generalization.
---

# Extending Audio Context for Long-Form Understanding in Large Audio-Language Models

## Quick Facts
- arXiv ID: 2510.15231
- Source URL: https://arxiv.org/abs/2510.15231
- Reference count: 40
- Primary result: Partial YaRN and VLAT achieve 81.73% accuracy on 10-minute audio, outperforming baseline models that fail at 2 minutes

## Executive Summary
Large Audio-Language Models (LALMs) face a fundamental limitation: their short audio context windows (typically 30 seconds) severely restrict their ability to understand long-form audio content. This paper introduces Partial YaRN, a training-free method that extends audio context by modifying only audio token positions while preserving text capabilities, and Virtual Longform Audio Training (VLAT), a training strategy that simulates diverse audio lengths during training. Together, these methods enable models like Qwen2-Audio to maintain high performance on audio up to 10 minutes long, overcoming the context window constraint that previously limited LALMs to understanding only short clips.

## Method Summary
The paper presents two complementary approaches to extend audio context in LALMs. Partial YaRN modifies only audio token positions via position interpolation while leaving text positions intact, using a two-group frequency partition (low-frequency dimensions interpolated, high-frequency extrapolated) with temperature scaling to prevent attention collapse. VLAT is a training strategy that applies Partial YaRN as positional augmentation, randomly sampling virtual length factors (1x-25x default context) during training to expose models to diverse positional ranges. The methods are tested on unified input space LALMs (SALMONN, Qwen2-Audio) using the YODAS2-MCQA dataset with audio segmented into 1/2/5/10 minute chunks and encoded by Whisper at 30-second non-overlapping intervals.

## Key Results
- Qwen2-Audio achieves 81.73% accuracy on 10-minute audio using VLAT and Partial PI inference
- VLAT improves 10-minute accuracy from 32.76% to 75.11% with vanilla inference
- Extending from observed context (2 minutes) outperforms extending from nominal context (30 seconds) - 48.00% vs 28.53% accuracy at 10 minutes

## Why This Works (Mechanism)

### Mechanism 1: Modality-Decoupled Positional Extension
Partial YaRN preserves base LLM text capabilities while extending audio context by modifying only audio token positions via interpolation, leaving text positional encodings intact. The method uses a two-group RoPE dimension partition with temperature scaling in attention softmax to prevent collapse over long sequences. This works because the bottleneck is unfamiliarity with audio positions beyond training range, not insufficient audio understanding capacity.

### Mechanism 2: Randomized Virtual Length Simulation
VLAT applies Partial YaRN as positional augmentation during training, sampling virtual lengths (1x-25x default context) to expose the model to diverse positional ranges. This teaches the model to handle unseen lengths at inference by creating a dense, continuous positional space through interpolation. The approach succeeds because position interpolation creates transferable positional representations.

### Mechanism 3: Observed Context Extension
Models like SALMONN and Qwen2-Audio maintain performance up to ~2 minutes despite 30-second training, suggesting longer innate audio context. Extending from this observed 2-minute window to longer targets yields better results than extending from 30s, likely because interpolation from a familiar range produces less distortion. This works because models develop implicit longer context windows through multi-audio training.

## Foundational Learning

- **Concept: Rotary Positional Encoding (RoPE)**
  - Why needed: Partial YaRN directly manipulates RoPE frequencies and position indices to extend audio context
  - Quick check: Given query q_m at position m and key k_n at position n, what does RoPE compute in the attention dot product?

- **Concept: Position Interpolation vs. Extrapolation**
  - Why needed: The paper distinguishes interpolating within trained context (stable) vs. extrapolating beyond it (unstable)
  - Quick check: If a model is trained on positions [0, 1024], why is mapping position 2048 to 1024 via interpolation preferred over direct extrapolation?

- **Concept: Unified Input Space LALMs**
  - Why needed: The methods apply to LALMs where audio tokens are projected into the LLM's embedding space
  - Quick check: In a unified input space LALM, how do audio tokens differ from text tokens from the LLM backbone's perspective?

## Architecture Onboarding

- **Component map**: Audio Encoder (Whisper) -> Position Manipulation Layer -> RoPE Application -> LLM Backbone
- **Critical path**: Chunk long audio into 30s segments → encode independently → concatenate with text tokens → identify audio region in position_ids → apply Partial YaRN (interpolate positions, apply temperature scaling) → forward pass with modified RoPE
- **Design tradeoffs**: Whole-context vs. Partial extension (expressivity vs. text preservation), 2-group vs. 3-group frequency partition (coverage vs. design simplicity), hyperparameter tuning burden (model-specific vs. closed-form solutions)
- **Failure signatures**: Attention collapse (addressed by higher temperature), positional discontinuity (mitigated by information from other dimensions), hyperparameter mismatch (wrong cutoff/temperature causes severe drops)
- **First 3 experiments**: 1) Validate baseline: Run vanilla inference on 1-10 minute audio to identify observed context window, 2) Hyperparameter sweep: Grid search cutoff dimension × temperature on validation set, 3) Compare extension strategies: Evaluate Whole YaRN vs. Partial YaRN vs. Partial PI, extending from both nominal and observed contexts

## Open Questions the Paper Calls Out

### Open Question 1: Closed-Form Hyperparameters
Can closed-form formulae for Partial YaRN's cutoff dimension index and attention temperature be derived to eliminate hyperparameter search across diverse LALMs? The paper acknowledges the wide variance in audio token compression rates makes unified formulae non-trivial.

### Open Question 2: Limited Range VLAT Strategy
Why does VLAT's "Limited Range" sampling strategy (capped at 5mins virtual length) outperform strategies covering the full 10mins evaluation range on 10mins test audio? This counterintuitive result may indicate training instability from extreme virtual lengths.

### Open Question 3: Video-Language Model Transfer
Do Partial YaRN and VLAT transfer effectively to video-language models, and does the modality-decoupled approach preserve temporal coherence in video generation tasks? Video introduces additional complexity (spatial-temporal structure) not present in audio.

### Open Question 4: Text Preservation in Generation Tasks
How does the text-preservation benefit of Partial YaRN manifest in open-ended generation tasks compared to the MCQA evaluation used in this study? The paper acknowledges experiments don't fully test text preservation importance for tasks requiring sophisticated language generation.

## Limitations
- Modality-decoupled design assumption not rigorously validated - may impair cross-modal reasoning tasks
- Observed context length identification is empirical and model-specific, not generalizable
- Hyperparameter sensitivity without closed-form solutions requires manual tuning for each model-length combination

## Confidence

**High Confidence**: VLAT significantly improves long audio performance on unseen lengths; Partial YaRN consistently outperforms original models across all tested settings; observed context length exists and differs from nominal training context.

**Medium Confidence**: Modality-decoupled positional extension preserves text capabilities while extending audio; extending from observed context outperforms extending from nominal context; two-group frequency partition is superior to alternatives.

**Low Confidence**: VLAT generalizes to any audio length without additional tuning; temperature scaling universally prevents attention collapse; approach transfers to LALMs with different architectures.

## Next Checks
- Evaluate Partial YaRN on tasks requiring tight audio-text integration to verify decoupling doesn't impair joint understanding
- Test whether optimal hyperparameters for one model-length pair transfer to other models or lengths
- Evaluate Partial YaRN with different positional encoding schemes (ALiBi, positional embeddings) to determine architecture-agnostic applicability