---
ver: rpa2
title: 'ModiGen: A Large Language Model-Based Workflow for Multi-Task Modelica Code
  Generation'
arxiv_id: '2503.18460'
source_url: https://arxiv.org/abs/2503.18460
tags:
- modelica
- code
- generation
- llms
- modeling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ModiGen, a workflow for generating Modelica
  code using large language models. The workflow combines supervised fine-tuning,
  graph retrieval-augmented generation, and feedback optimization to improve code
  generation accuracy and physical consistency.
---

# ModiGen: A Large Language Model-Based Workflow for Multi-Task Modelica Code Generation

## Quick Facts
- arXiv ID: 2503.18460
- Source URL: https://arxiv.org/abs/2503.18460
- Reference count: 40
- Key outcome: Maximum improvement in pass@1 reached 0.3349 for component generation and 0.2457 for test case generation, with highest observed pass@1 scores of 0.5558 and 0.7701 respectively

## Executive Summary
This paper introduces ModiGen, a workflow for generating Modelica code using large language models. The workflow combines supervised fine-tuning, graph retrieval-augmented generation, and feedback optimization to improve code generation accuracy and physical consistency. Experimental results show significant performance gains across component and test case generation tasks. The research addresses challenges in automated Modelica code generation and provides insights for future developments in system modeling and engineering applications.

## Method Summary
ModiGen integrates supervised fine-tuning, GraphRAG, and feedback optimization to enhance Modelica code generation accuracy and physical consistency. The workflow involves two-stage fine-tuning (unsupervised pretraining on 9,456 cleaned Modelica instances, followed by instruction-supervised fine-tuning on 3,202 prompt-code pairs using LoRA), property graph-based retrieval for component dependencies, and iterative feedback from compiler and simulator errors. Experimental results demonstrate significant performance improvements across component and test case generation tasks.

## Key Results
- Maximum improvement in pass@1 reached 0.3349 for component generation and 0.2457 for test case generation
- Highest observed pass@1 scores: 0.5558 for component generation and 0.7701 for test case generation
- GraphRAG showed larger gains in pass_f@1 than pass_s@1, indicating improved physical consistency
- Feedback optimization yielded slightly higher percentage gains in pass_s@1 than in pass_f@1, suggesting primary improvements in syntactic and structural accuracy

## Why This Works (Mechanism)

### Mechanism 1
Domain-specific supervised fine-tuning improves LLM adaptability to Modelica syntax and structure through two-stage fine-tuning—unsupervised pretraining on 9,456 cleaned Modelica instances (Dataset_all), followed by instruction-supervised fine-tuning on 3,202 prompt-code pairs (Dataset_sft) using LoRA with learning rate 5e-4, batch size 1, 10 epochs. Exposure to domain-specific code patterns during fine-tuning transfers to generation tasks without catastrophic forgetting.

### Mechanism 2
Graph-based retrieval augments physical consistency by resolving component dependencies and cross-domain relationships. Property graph constructed from Dataset_sft with node types (connectors, equations, parameters, interfaces) and edge types (extension, connection, invocation). During generation, GraphRAG queries the index for relevant component definitions and constraints, injecting retrieved knowledge into prompts.

### Mechanism 3
Iterative feedback from compiler and simulator errors improves syntactic and functional correctness through four-stage validation (load, check, simulate, functional). On failure, error messages with line/column precision are fed back to LLM along with original prompt. Functional validation compares simulation output MSE against benchmarks. Limited to single feedback round in experiments.

## Foundational Learning

- **Concept**: Modelica equation-based, non-causal modeling
  - Why needed here: Unlike imperative code, Modelica describes physical systems through equations where causality is determined by the solver, not the programmer. Critical for understanding why GraphRAG prioritizes dependency relationships over control flow.
  - Quick check question: Can you explain why `der(height) = velocity` and `der(velocity) = -g` together define a valid Modelica model without explicit causality assignment?

- **Concept**: Retrieval-Augmented Generation (RAG) with structured indexes
  - Why needed here: ModiGen replaces vector similarity with property graph traversal. Understanding basic RAG helps contrast why graph structure matters for dependency-heavy modeling languages.
  - Quick check question: What information would a vector index miss that a property graph captures when retrieving relevant Modelica components?

- **Concept**: Parameter-Efficient Fine-Tuning (LoRA)
  - Why needed here: Full fine-tuning of 7B-34B parameter models is computationally prohibitive. LoRA enables domain adaptation with low-rank decomposition.
  - Quick check question: What is the trade-off between LoRA rank size and adaptation quality for specialized syntax learning?

## Architecture Onboarding

- **Component map**: DataPreprocessing -> Fine-tuning (LoRA) -> GraphRAG -> ValidationPipeline (Load -> Check -> Simulate -> Functional) -> FeedbackLoop
- **Critical path**: Dataset preparation (44 libraries → 9,456 instances) → LoRA fine-tuning (10 epochs, lr=5e-4) → GraphRAG retrieval during inference → Four-stage validation via OMPython/OpenModelica → Conditional feedback iteration
- **Design tradeoffs**: GraphRAG vs. hardcoded dependencies (GraphRAG for component generation, hardcoded for test cases); Single vs. multi-round feedback (limited to one round); Fine-tuning on smaller models (7B) vs. prompting larger proprietary models
- **Failure signatures**: Dependency resolution errors (missing imports, undefined components); Simulation failures at check phase (incorrect parameter types, structural violations); Functional validation failures (output MSE exceeds threshold); Stagnant feedback loops (same error repeated)
- **First 3 experiments**: 1) Baseline replication with zero-shot generation on component dataset to establish pass_s@1 and pass_f@1 baselines; 2) Apply two-stage LoRA fine-tuning only to measure delta without GraphRAG or feedback; 3) GraphRAG vs. vector index ablation on fine-tuned model to validate property graph contribution

## Open Questions the Paper Calls Out

### Open Question 1
How can LLM-based workflows be enhanced to maintain consistency and coherence across system-level architectures rather than just generating isolated components? Current generation tasks often lack a holistic understanding of the overall system architecture, prioritizing local details over system-wide coherence. Multi-task learning or hierarchical modeling could demonstrate success in generating code that aligns with overall system architecture.

### Open Question 2
How can GraphRAG mechanisms be refined to effectively capture multi-level and cross-domain interactions in complex, non-linear multi-physics models? While GraphRAG aids dependency management, it still struggles with multi-level and cross-domain interactions, particularly in systems with non-linear couplings. Improved pass rates on benchmarks specifically designed for non-linear, multi-physics domains compared to current retrieval methods would resolve this.

### Open Question 3
Does the performance of fine-tuned models on current benchmarks generalize to broader engineering domains like thermodynamics and fluid dynamics? Current benchmarks are limited in diversity and scale, which restricts the comprehensive evaluation of LLMs' fine-tuning effectiveness. Evaluation results on expanded benchmarks that incorporate representative cases from thermodynamics, fluid dynamics, and other complex fields would resolve this.

## Limitations
- Limited dataset size (129 components, 71 test cases) may not provide robust performance claims
- Single iteration feedback optimization prevents assessment of compounding improvements
- Lack of direct ablation studies comparing GraphRAG against simpler RAG approaches

## Confidence

**High Confidence**: Two-stage fine-tuning approach with LoRA is technically sound and well-documented. Performance improvements in pass_s@1 (syntactic correctness) are clearly demonstrated through systematic ablation studies.

**Medium Confidence**: Claims about GraphRAG's superiority over vector-based RAG are supported by experimental data but lack direct ablation comparisons. The mechanism is plausible given Modelica's dependency structure, but the evidence is indirect.

**Low Confidence**: Compounding effect of combining GraphRAG with feedback optimization is based on single-round experiments. Claims about multi-iteration improvements and error message sufficiency for autonomous correction require further validation.

## Next Checks

1. **Multi-Round Feedback Validation**: Implement and test 3-5 rounds of feedback optimization on a subset of failed cases to measure whether compounding improvements occur and identify feedback loop termination conditions.

2. **GraphRAG Ablation Study**: Replace the property graph retrieval with standard vector-based RAG (using the same fine-tuned model) and compare pass_f@1 scores on the component generation task to isolate the contribution of graph structure versus fine-tuning.

3. **Cross-Library Generalization Test**: Evaluate ModiGen on at least two Modelica libraries not present in the training data (e.g., Modelica.Electrical or Modelica.Fluid) to assess whether improvements generalize beyond the curated dataset or represent overfitting to specific domains.