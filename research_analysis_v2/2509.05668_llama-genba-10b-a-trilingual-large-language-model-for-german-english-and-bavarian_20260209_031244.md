---
ver: rpa2
title: 'Llama-GENBA-10B: A Trilingual Large Language Model for German, English and
  Bavarian'
arxiv_id: '2509.05668'
source_url: https://arxiv.org/abs/2509.05668
tags:
- arxiv
- german
- bavarian
- english
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Llama-GENBA-10B, a trilingual foundation
  model designed to address English dominance in large language models by balancing
  resources across English, German, and Bavarian. The model is built on Llama 3.1-8B
  and scaled to 10B parameters through block expansion, trained on 164B tokens (82B
  English, 82B German, and 80M Bavarian) to prevent English dominance.
---

# Llama-GENBA-10B: A Trilingual Large Language Model for German, English and Bavarian

## Quick Facts
- **arXiv ID**: 2509.05668
- **Source URL**: https://arxiv.org/abs/2509.05668
- **Reference count**: 12
- **Primary result**: First trilingual foundation model for English, German, and Bavarian, achieving state-of-the-art Bavarian performance

## Executive Summary
Llama-GENBA-10B addresses English dominance in large language models by introducing a trilingual foundation model balancing English, German, and Bavarian resources. Built on Llama 3.1-8B and scaled to 10B parameters through block expansion, the model was trained on 164B tokens with equal English-German allocation and minimal Bavarian representation. The development tackled four key challenges: curating a trilingual corpus despite Bavarian scarcity, creating a unified tokenizer, optimizing architecture and hyperparameters for cross-lingual transfer, and establishing the first standardized trilingual evaluation suite by translating German benchmarks into Bavarian.

## Method Summary
The model expands Llama-3.1-8B to 10B parameters by inserting 8 identity-initialized transformer blocks into the frozen backbone, training only these new layers on the trilingual corpus. A 20% tokenizer vocabulary expansion via BPE reduces token fragmentation for German and Bavarian. Training proceeds in two phases: 90% on English-German only (establishing cross-lingual representations), then Bavarian is upsampled and introduced for the final 10%. Fine-tuning uses all parameters on 867k instruction pairs translated to all three languages. The entire pipeline was trained on Cerebras CS-2, demonstrating efficient large-scale multilingual pretraining.

## Key Results
- Achieves strong cross-lingual performance across all three languages
- Fine-tuned variant surpasses Apertus-8B-2509 and gemma-2-9b in Bavarian
- Outperforms EuroLLM in English and matches its results in German
- Establishes first standardized trilingual evaluation suite with Bavarian benchmarks

## Why This Works (Mechanism)

### Mechanism 1
Block expansion enables multilingual adaptation without catastrophic forgetting of the pretrained backbone. Eight identity-initialized transformer blocks are inserted into the frozen Llama-3.1-8B backbone, with only these new blocks training on the trilingual corpus. This preserves original capabilities while adding multilingual capacity, assuming the frozen backbone provides stable general representations while new blocks specialize on language-specific patterns without interference.

### Mechanism 2
Delayed integration of low-resource Bavarian prevents capacity competition with high-resource languages. Training proceeds in two phases—first 90% on English-German only, then Bavarian is upsampled and introduced alongside remaining English-German for the final 10%. This staged approach allows the 80M Bavarian tokens to find representation space after the model has stabilized on 164B high-resource tokens, assuming cross-lingual transfer from German to Bavarian dialect occurs more effectively when German representations are already well-formed.

### Mechanism 3
Balanced 1:1 English-German ratio with expanded tokenizer optimizes cross-lingual transfer while preventing English dominance. Equal token allocation (82B each) combined with a 20% vocabulary expansion reduces token fragmentation. The expanded tokenizer achieved fertility score 1.8372 vs. 1.9026 for 10% expansion, meaning fewer tokens per word and more efficient training, assuming lower fertility scores directly correlate with better multilingual performance.

## Foundational Learning

- **Tokenizer Fertility Score**: Determines how efficiently text is encoded; high fertility means more tokens per word, wasting context window and compute on German/Bavarian. Quick check: If a German sentence requires 50% more tokens than English after tokenization, what does that imply about the tokenizer's language bias?

- **Block Expansion / Progressive Training**: The core architectural technique—understanding how identity-initialized blocks integrate with frozen pretrained weights is essential for reproducing this approach. Quick check: Why initialize new blocks with zeros rather than random weights when inserting them into a pretrained model?

- **Catastrophic Forgetting**: The motivation for backbone freezing—without this concept, the design decision to freeze original weights seems arbitrary. Quick check: If all parameters were unfrozen during trilingual pretraining, what failure mode would likely occur for English tasks?

## Architecture Onboarding

- **Component map**: Llama-3.1-8B (frozen backbone, 32 layers, 8B params) -> 8 new Transformer blocks (layers 33-40, zero-initialized) -> 20% tokenizer expansion (141,053 total vocabulary) -> 164B token corpus (82B EN + 82B DE + 80M upsampled BA) -> Fine-tuning layer (all 10B params trainable)

- **Critical path**: 1) Expand tokenizer vocabulary via BPE on German/Bavarian corpus → validate fertility scores; 2) Insert 8 identity-initialized blocks into frozen Llama-3.1-8B checkpoint; 3) Pretrain on English-German (90% of 164B tokens) with backbone frozen; 4) Introduce upsampled Bavarian for final 10% of training; 5) Supervised fine-tuning with all parameters unfrozen on 867k instruction pairs

- **Design tradeoffs**: 20% vs. 30% tokenizer expansion: chose 20% due to diminishing returns (fertility 1.8372 → 1.8214); 1:1 vs. 9:1 German:English ratio: 1:1 yielded better balanced performance (Table 3); Single Cerebras CS-2 vs. distributed GPU: CS-2 avoided inter-node communication overhead but required 66 days for 164B tokens

- **Failure signatures**: Bavarian outputs default to standard German ("translationese") → indicates insufficient dialectal representation; English performance drops below comparable monolingual models → suggests over-allocation to non-English languages; High fertility scores on Bavarian text → tokenizer expansion failed to capture dialectal patterns

- **First 3 experiments**: 1) Validate tokenizer: Compare fertility scores on held-out Bavarian text before/after vocabulary expansion; target <1.85; 2) Ablate delayed integration: Train a variant with Bavarian included from step 0; compare Bavarian benchmark scores to staged approach; 3) Fine-tuning checkpoint sweep: Evaluate checkpoints from epochs 1, 2, 3 on trilingual benchmarks; paper found epoch 2 optimal

## Open Questions the Paper Calls Out

**Open Question 1**: How can harm risk categorization and detection mechanisms be effectively designed for instruction-tuning safety in multilingual and dialectal language models? Basis: The conclusion states future work could focus on instruction-tuning safety through harm risk categorization and detection mechanisms. Unresolved because the paper did not evaluate safety aspects and initial model distribution is restricted to approved organizations while legal and safety concerns are being resolved.

**Open Question 2**: How do automated benchmark translations compare to native Bavarian evaluations in measuring true dialectal language proficiency? Basis: The paper translated German benchmarks into Bavarian using Gemini-flash for evaluation, but did not validate whether translated benchmarks accurately reflect native Bavarian linguistic patterns. Unresolved because no human validation or native speaker assessment was conducted to confirm benchmark quality and model output authenticity.

**Open Question 3**: Would differentiating between Bavarian dialectal variants improve model performance, or does unified modeling sufficiently capture dialectal diversity? Basis: The authors state they chose not to differentiate between dialectal variants within Bavarian to maintain a manageable scope. Unresolved because treating Bavarian as monolithic may obscure performance differences across regional variants and limit utility for specific communities.

## Limitations

- **Limited Bavarian representation**: Only 80M tokens (upsampled from 20M raw) in 164B token training corpus, representing minimal exposure for a low-resource language
- **Benchmark translation concerns**: Relies on translated German benchmarks rather than natively produced Bavarian content, potentially measuring translationese rather than true dialectal proficiency
- **Unvalidated architectural choices**: No ablation studies comparing block expansion to full fine-tuning or adapter-based approaches for the same trilingual task

## Confidence

- **Low confidence** on Bavarian performance claims due to reliance on translated benchmarks and minimal training data
- **Medium confidence** on cross-lingual transfer effectiveness—promising results but lacks direct validation of frozen backbone assumption
- **Medium confidence** on tokenizer expansion benefits—fertility scores improved but no demonstration of downstream impact

## Next Checks

1. **Ablation of Integration Timing**: Train two variants—one with Bavarian from initialization, one with delayed integration as described. Compare Bavarian benchmark performance after identical total training steps to isolate whether staged training genuinely improves dialect representation or simply allocates more relative capacity to the low-resource language.

2. **Backbone Freezing Validation**: Create a variant where the original 8B parameters are unfrozen during trilingual pretraining. Measure English/German performance degradation versus block expansion approach to quantify the catastrophic forgetting prevention claim and determine if any performance was sacrificed for stability.

3. **Native Bavarian Evaluation**: Generate native Bavarian text outputs for standardized prompts and evaluate them against human judgments for dialect authenticity. Compare these qualitative assessments with the translated benchmark scores to determine whether the model genuinely captures Bavarian linguistic patterns or produces standard German with Bavarian vocabulary.