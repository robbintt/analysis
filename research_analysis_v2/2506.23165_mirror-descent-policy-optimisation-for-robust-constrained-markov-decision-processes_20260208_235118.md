---
ver: rpa2
title: Mirror Descent Policy Optimisation for Robust Constrained Markov Decision Processes
arxiv_id: '2506.23165'
source_url: https://arxiv.org/abs/2506.23165
tags:
- policy
- lemma
- constraint-cost
- algorithm
- transition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces mirror descent policy optimisation for robust
  constrained Markov decision processes (RCMDPs), combining policy gradient techniques
  to optimise both the policy (as a maximiser) and the transition kernel (as an adversarial
  minimiser) on the Lagrangian representing a CMDP. The authors propose robust Sample-based
  PMD-PD, achieving a convergence rate of O(1/T^{1/3}) in the sample-based RCMDP setting.
---

# Mirror Descent Policy Optimisation for Robust Constrained Markov Decision Processes

## Quick Facts
- arXiv ID: 2506.23165
- Source URL: https://arxiv.org/abs/2506.23165
- Reference count: 40
- This paper introduces mirror descent policy optimisation for robust constrained Markov decision processes (RCMDPs), achieving O(1/T^(1/3)) convergence rate.

## Executive Summary
This paper introduces mirror descent policy optimisation for robust constrained Markov decision processes (RCMDPs), combining policy gradient techniques to optimise both the policy (as a maximiser) and the transition kernel (as an adversarial minimiser) on the Lagrangian representing a CMDP. The authors propose robust Sample-based PMD-PD, achieving a convergence rate of O(1/T^(1/3)) in the sample-based RCMDP setting. Additionally, the paper presents an algorithm for approximate gradient descent in the space of transition kernels, which is of independent interest for designing adversarial environments in general MDPs. Experiments confirm the benefits of mirror descent policy optimisation in constrained and unconstrained optimisation, with significant improvements observed in robustness tests compared to baseline policy optimisation algorithms.

## Method Summary
The paper introduces Mirror Descent Policy Optimisation (MDPO) for Robust Constrained MDPs (RCMDPs), which uses a Lagrangian formulation to unify worst-case cost and constraint optimisation. The method employs a double-loop structure: an inner loop that performs Transition Mirror Ascent (TMA) to find worst-case transition dynamics, and an outer loop that applies mirror descent policy optimisation and updates dual variables via augmented Lagrangian methods. The algorithm is tested on three domains: Cartpole with velocity constraints, Inventory Management, and 3-D Inventory Management, with experiments comparing robust performance against nominal baselines.

## Key Results
- Proposed robust Sample-based PMD-PD achieves O(1/T^(1/3)) convergence rate in sample-based RCMDP setting
- Algorithm for approximate gradient descent in transition kernel space provides O(1/T) regret in oracle setting
- Experiments show significant robustness improvements compared to baseline algorithms, with performance gains in both constrained and unconstrained settings
- The method successfully balances cost minimization with constraint satisfaction while optimizing for worst-case transition dynamics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Mirror descent applied to policy space with KL-divergence provides stable, unbiased convergence to unregularized optima.
- **Mechanism:** Policy updates follow mirror descent on the augmented Lagrangian using a weighted KL-divergence Bregman divergence. The update (Eq. 19-20) computes πt+1 via argmin over π of {⟨∂Ṽ/∂θ, π⟩ + (1/η)DKL(π, πt)} for each state, equivalent to natural policy gradient with softmax parameterization. This yields an entropy-regularized update anchored to the previous policy rather than a uniform policy, enabling convergence to the unregularized optimum.
- **Core assumption:** Policy is softmax-parameterized; gradients are estimated with bounded error ϵ; the initial state distribution provides sufficient exploration (Assumption 3).
- **Evidence anchors:**
  - [abstract] "policy gradient techniques to optimise both the policy (as a maximiser) and the transition kernel (as an adversarial minimiser) on the Lagrangian"
  - [section 4] Eq. 19-20 define the mirror descent update; Lemma 10 bounds the regularized augmented Lagrangian
  - [corpus] Related work on Policy Mirror Descent (e.g., "Policy Mirror Descent with Temporal Difference Learning") confirms the framework's sample complexity benefits, though corpus lacks direct comparison to this specific RCMDP setting.
- **Break condition:** If policy parameterization deviates significantly from softmax, or if gradient estimation error exceeds O(ϵ), the convergence rate degrades to the error-dominated regime.

### Mechanism 2
- **Claim:** Adversarial transition kernel optimization via Transition Mirror Ascent (TMA) finds worst-case dynamics within uncertainty sets with bounded regret.
- **Mechanism:** TMA performs mirror ascent on transition kernels p∈P using the action next-state value function Gπ,p (Def. 2). Approximate TMA (Algorithm 1) estimates G from samples and updates pt+1(·|s,a) via argmax over P of {ηp⟨Ĝ, p⟩ - αpB(p, pt)}. The ascent property (Lemma 4) ensures Vπ,pt(ρ) - Vπ,pt+1(ρ) ≤ 2ϵ'/(1-γ) under ϵ'-precise G estimates. This provides O(1/T^(1/3)) regret in the sample-based setting (Theorem 1).
- **Core assumption:** Uncertainty set P is convex and (s,a)-rectangular or s-rectangular (Assumption 5); the estimated Ĝ satisfies ||G - Ĝ||∞ ≤ ϵ' (Eq. 32).
- **Evidence anchors:**
  - [abstract] "algorithm for approximate gradient descent in the space of transition kernels, which is of independent interest"
  - [section 5.3] Algorithm 1 and Theorem 1 detail Approximate TMA; Lemma 6 provides sample complexity for G estimation
  - [corpus] Neighboring papers on robust MDPs (e.g., "Policy Regularized Distributionally Robust Markov Decision Processes") discuss similar adversarial optimization but with different convergence guarantees, indicating this TMA approach is novel for RCMDPs.
- **Break condition:** If uncertainty set is non-rectangular, convergence degrades to O(1/T^(1/5)) (Theorem 3) with additional error δP. If G estimation error grows unboundedly, the ascent property fails and regret increases.

### Mechanism 3
- **Claim:** The robust Lagrangian formulation unifies worst-case cost and constraint-cost optimization via dual variables, enabling single-simulator training.
- **Mechanism:** The RCMDP objective (Eq. 4) redefines the problem as minπ sup p∈P maxλ≥0 Vπ,p(ρ) + Σjλj Vjπ,p(ρ). This combines all costs into a single Lagrangian value function (Eq. 5), avoiding the need for multiple simulators (as in R3C). Dual variables λ are updated via augmented Lagrangian method (Algorithm 3), ensuring boundedness and improved convergence. Complementary slackness (Lemma 13) ensures λj=0 if constraint is slack, focusing optimization on active constraints.
- **Core assumption:** Slater's condition holds: ∃ζ>0 and policy π̄ with Vjπ̄,p(ρ) ≤ -ζ for all j and p∈P (Assumption 4).
- **Evidence anchors:**
  - [abstract] "making use of policy gradient techniques to optimise both the policy (as a maximiser) and the transition kernel (as an adversarial minimiser) on the Lagrangian"
  - [section 3] Eq. 4-5 define the robust Lagrangian; Lemma 8 bounds the regret via TMA error and dual variable suboptimality
  - [corpus] Related work on constrained MDPs (e.g., "Sample Complexity Bounds for Linear Constrained MDPs") discusses primal-dual methods but without robustness, indicating this Lagrangian extension to RCMDPs is novel.
- **Break condition:** If Slater's condition is violated (no strictly feasible policy exists), dual variables may diverge or duality gap may persist, breaking the average regret guarantees.

## Foundational Learning

- **Concept: Markov Decision Processes (MDPs) with value functions**
  - **Why needed here:** RCMDPs extend MDPs with constraints and transition kernel uncertainty. Understanding Vπ,p(ρ) (Eq. 2) and the discounted state-action visitation distribution (Eq. 7) is essential for gradient derivations and regret analysis.
  - **Quick check question:** Can you derive the policy gradient theorem for a standard MDP and explain how it changes when transition dynamics p are adversarial?

- **Concept: Constrained optimization via Lagrangian duality**
  - **Why needed here:** The core formulation uses a Lagrangian to balance cost minimization with constraint satisfaction (Eq. 4). Dual variables λ are optimized alongside π and p, requiring understanding of complementary slackness and Slater's condition.
  - **Quick check question:** In a simple CMDP with one constraint, what happens to the optimal dual variable λ* if the constraint is always strictly satisfied?

- **Concept: Mirror descent with Bregman divergences**
  - **Why needed here:** The algorithm uses mirror descent for both policy (with KL-divergence) and transition kernel (with ℓq or KL-based divergences) updates. The pushback property (Lemma 17) enables telescoping regret analysis.
  - **Quick check question:** For a probability simplex over actions, how does mirror descent with negative entropy differ from gradient descent in terms of constraint handling?

## Architecture Onboarding

- **Component map:** The system comprises three interacting optimizers: (1) Policy Optimizer – mirror descent on π using estimated Q-values (Eq. 19-20); (2) Adversarial Transition Optimizer – Approximate TMA on p using estimated G-values (Algorithm 1); (3) Dual Variable Optimizer – augmented Lagrangian update on λ (Algorithm 3). These run in a double-loop: inner loop updates π and p, outer loop updates λ and collects samples. The practical implementation MDPO-Robust-Lagrangian uses deep RL function approximators (MLPs) for critics and policy, and Monte Carlo TMA for p.

- **Critical path:** 
  1. Initialize π0 (uniform), λ0 (zeros or small), p0 (nominal).
  2. For each macro-iteration k:
     a. Inner loop: Update πt_k via mirror descent using Q̂ from trajectories under πt_k and pk.
     b. Apply Approximate TMA to update pk+1 towards worst-case p for current πk+1.
     c. Estimate constraint costs V̂j from new trajectories, update λk+1 via augmented rule.
  3. Test learned policy on perturbed dynamics from uncertainty set.

- **Design tradeoffs:**
  - *Rectangular vs. non-rectangular uncertainty sets:* Rectangular yields O(1/T^(1/3)) convergence but may be conservative; non-rectangular reduces conservatism but degrades to O(1/T^(1/5)) with error δP.
  - *Direct vs. indirect transition kernel parameterization:* Direct (tabular) enables exact gradients but doesn't scale; indirect (e.g., Gaussian mixture PTK) scales but requires projection onto uncertainty set.
  - *Augmented vs. clipped Lagrangian:* Augmented provides smoother convergence but may require larger λ ranges; clipped is simpler but may cause oscillation.

- **Failure signatures:**
  1. *Constraint violation during training:* If λ updates are too slow or ϵ tolerances are loose, constraints may be violated. Check λ magnitudes and increase ηλ or tighten ϵ.
  2. *No improvement in worst-case performance:* TMA may not find true worst-case p if ηp is too small or samples are insufficient. Increase TMA iterations or sample size per update.
  3. *Policy oscillation:* If KL penalty α is too small, policy may change too rapidly. Increase α or use adaptive schedules (e.g., batch-based restarts as in Appendix H).

- **First 3 experiments:**
  1. **Verify TMA in isolation:** Fix a policy π, run Approximate TMA on a simple MDP with known worst-case transition. Measure regret Vπ,p*(ρ) - Vπ,pt(ρ) vs. iteration to confirm O(1/T) oracle rate and sample-based degradation.
  2. **Compare rectangular vs. non-rectangular sets in a toy RCMDP:** Use a 2-state, 2-action MDP with a single constraint. Implement both uncertainty set types and measure convergence rate and constraint violation. Confirm theoretical rates.
  3. **Ablate dual variable update rule:** In Cartpole domain, compare augmented Lagrangian update vs. clipped update vs. fixed λ. Measure final penalized return and constraint satisfaction to validate the augmented design choice.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the O(1/ϵ^5) sample complexity for non-rectangular RCMDPs be improved to match the O(1/ϵ^3) rate of the rectangular case?
- Basis: The authors note in Section 5.5 and Theorem 3 that extending the analysis to non-rectangular uncertainty sets results in a "weaker sample-based convergence rate" compared to the rectangular setting.
- Why unresolved: The current analysis relies on conservative policy iteration for transition kernels in the non-rectangular setting, which inherently requires more samples to achieve an ϵ-optimal solution.
- What evidence would resolve it: A theoretical derivation providing a O(1/ϵ^3) guarantee for non-rectangular sets or an empirical demonstration that practical convergence speeds are comparable despite the theoretical gap.

### Open Question 2
- Question: How does the proposed Approximate TMA algorithm with function approximation perform empirically compared to the Monte Carlo baseline used in the paper?
- Basis: Appendix E states that the experiments utilized "pure Monte Carlo based TMA rather than additional function approximation as proposed in our Approximate TMA algorithm" for simplicity.
- Why unresolved: While the theory proposes a scalable function approximation method for the transition kernel update, the experimental validation relied on a less scalable, exact Monte Carlo method, leaving the efficacy of the proposed approximation unverified.
- What evidence would resolve it: A comparative experimental study evaluating the sample efficiency and performance of the function-approximation-based Approximate TMA against the Monte Carlo baseline.

### Open Question 3
- Question: Can the algorithm guarantee convergence without requiring the strict Slater's condition for all transition dynamics in the uncertainty set?
- Basis: Section 5.1 (Assumption 4) notes that the requirement of a strictly feasible policy for all dynamics is "slightly more restrictive than in traditional CMDPs."
- Why unresolved: The proof of bounded dual variables (Lemma 26) and the subsequent regret analysis rely on the existence of this uniform slack variable to ensure stability.
- What evidence would resolve it: A modification of the proof strategy or a new algorithm that maintains convergence guarantees even when a strictly feasible policy does not exist for every p ∈ P.

## Limitations
- The convergence rate of O(1/T^(1/3)) assumes accurate gradient estimates and bounded policy changes, which may degrade significantly with function approximation
- The rectangular uncertainty set assumption, while enabling faster convergence, may be overly conservative in many practical settings
- The adversarial transition kernel optimization assumes access to the true transition model for projection, which is unrealistic in many real-world applications

## Confidence
- **High confidence**: The mirror descent policy optimization mechanism (Mechanism 1) is well-established and the theoretical analysis is sound
- **Medium confidence**: The adversarial transition kernel optimization (Mechanism 2) shows promise but relies on strong assumptions about uncertainty sets and gradient estimation accuracy
- **Low confidence**: The robustness claims in experiments are based on limited test scenarios and could be influenced by implementation details

## Next Checks
1. **Gradient estimation error sensitivity**: Systematically vary the number of samples used for gradient estimation in the TMA algorithm and measure the impact on convergence rates and worst-case performance
2. **Uncertainty set shape impact**: Implement both rectangular and non-rectangular uncertainty sets in the same RCMDP problem and compare convergence rates and constraint satisfaction
3. **Generalization to unknown dynamics**: Modify the implementation to use a learned transition model (e.g., a neural network) instead of the true model for TMA projection and test on unseen dynamics perturbations