---
ver: rpa2
title: 'PTQTP: Post-Training Quantization to Trit-Planes for Large Language Models'
arxiv_id: '2509.16989'
source_url: https://arxiv.org/abs/2509.16989
tags:
- ptqtp
- quantization
- ternary
- arxiv
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PTQTP, a post-training quantization method
  that converts large language model weights into dual ternary {-1, 0, 1} trit-planes
  without retraining. The approach uses magnitude-topology decoupling, decomposing
  weights into two collaborative ternary planes modulated by continuous scaling coefficients.
---

# PTQTP: Post-Training Quantization to Trit-Planes for Large Language Models

## Quick Facts
- **arXiv ID:** 2509.16989
- **Source URL:** https://arxiv.org/abs/2509.16989
- **Reference count:** 40
- **Primary result:** Converts LLM weights to dual ternary {-1, 0, 1} trit-planes without retraining, achieving multiplication-free inference with over 100× faster quantization than QAT while maintaining strong performance on language and reasoning tasks.

## Executive Summary
PTQTP introduces a novel post-training quantization method that decomposes large language model weights into dual ternary trit-planes with continuous scaling coefficients. This magnitude-topology decoupling enables extreme compression (1.58-bit effective) while preserving representational capacity through sparse approximation. The method achieves multiplication-free inference via LUT-GEMM kernels, delivering 4.63× end-to-end speedup over FP16 baselines on RTX 3090 while requiring only single hours for quantization versus 10-14 GPU days for quantization-aware training.

## Method Summary
PTQTP performs magnitude-topology decoupling by approximating weight matrices as W ≈ α^(1)T^(1) + α^(2)T^(2), where T^(k) ∈ {-1,0,1} are ternary planes capturing structural connectivity and α^(k) ∈ R are continuous scaling coefficients capturing amplitude. The method uses progressive trit-plane approximation with adaptive regularization: for each row, ridge regression solves for α values using local basis matrices from current trit-planes, then updates each ternary element through exhaustive search over {-1,0,1}² to minimize reconstruction error. This iterative process converges in under 50 iterations with guaranteed Frobenius norm decrease. Group-wise processing with G=128 reshapes weights for efficient computation, and inference uses LUT-GEMM kernels that precompute ternary-activation dot products for multiplication-free forward passes.

## Key Results
- Achieves 82.40% accuracy on Math-500 while AWQ-b2 catastrophically fails at 6.40%
- Outperforms existing sub-4bit methods across LLaMA3.2 and Qwen3 models (0.6B-70B)
- Matches 1.58-bit QAT performance while requiring 100× less quantization time (hours vs 10-14 GPU days)
- Delivers 4.63× end-to-end speedup over FP16 baseline on RTX 3090

## Why This Works (Mechanism)

### Mechanism 1: Magnitude-Topology Decoupling
Separating weight amplitude from structural connectivity enables sparse approximation at extreme compression. The decomposition W ≈ α^(1)T^(1) + α^(2)T^(2) allows zero states to selectively silence noise features rather than forcing binary categorization, with ternary sparsity patterns approximating learned representations. This works when weight distributions contain separable magnitude and topology information.

### Mechanism 2: Progressive Trit-Plane Approximation with Adaptive Regularization
Iterative closed-form ridge regression converges to stable ternary representations without training. For each row, local basis matrices enable solving α values with adaptive λ based on condition numbers, while exhaustive search updates T^(k) elements to minimize reconstruction error. The Frobenius norm decreases monotonically per iteration, guaranteeing convergence when condition numbers remain bounded.

### Mechanism 3: Multiplication-Free Inference via Ternary Arithmetic
Ternary operations eliminate floating-point multipliers, reducing arithmetic to sign flips, additions, and zeros. Conditional addition maps c_m ∈ {-1,0,1} to {-α, 0, +α}, while LUT-GEMM kernels precompute all dot products between activation vectors and ternary patterns. This enables lookup-based matrix multiplication without dequantization when memory bandwidth—not compute—is the bottleneck.

## Foundational Learning

- **Concept: Ridge Regression with Adaptive Regularization**
  - Why needed here: Core optimization subroutine for solving scaling coefficients; understanding λ selection and condition number sensitivity is essential for diagnosing convergence failures.
  - Quick check question: Given a 2×2 matrix A with condition number 10⁷, should λ increase or decrease, and why?

- **Concept: Group-wise (Block-wise) Quantization**
  - Why needed here: PTQTP operates on reshaped weight groups (G=128); understanding granularity tradeoffs is critical for memory-accuracy balancing.
  - Quick check question: If group size increases from 128 to 512, what happens to the number of scaling coefficients and approximation granularity?

- **Concept: Lookup Table (LUT) based Matrix Multiplication**
  - Why needed here: Inference speedup depends on precomputing ternary-activation dot products; understanding LUT construction costs vs. inference gains is essential.
  - Quick check question: For a ternary weight matrix of shape [4096, 4096] and activation dimension 128, how many LUT entries are needed if activations are quantized to 4-bit?

## Architecture Onboarding

- **Component map:** Input FP16 weight matrices W → Group-wise reshaping to W̃ → Initialization T^(k) ← sign(W), α^(k) ← [1,1] → Iterative loop (ridge regression → adaptive regularization → trit-plane update) → Output T^(1), T^(2), α → LUT-GEMM kernel deployment

- **Critical path:** 1) Weight matrix loading and group-wise reshaping, 2) Ridge regression coefficient solving (O(nd) per iteration), 3) Element-wise ternary search (9 evaluations per weight element), 4) Convergence monitoring and early stopping, 5) Kernel deployment with LUT construction

- **Design tradeoffs:** Dual vs. single trit-plane (dual increases capacity but maintains 1.58-bit efficiency), group size G (smaller G improves approximation but increases scaling overhead), tolerance ε (tighter ε improves perplexity but increases runtime)

- **Failure signatures:** Divergence (α coefficients explode → increase λ), slow convergence (>50 iterations → poor initialization or conservative learning rate), catastrophic accuracy loss (ternary planes converge to binary-like patterns → check T^(k) zero ratios), kernel speedup below 3× (LUT overhead dominates → verify activation quantization)

- **First 3 experiments:**
  1. Sanity check on LLaMA3.2-1B: Verify perplexity < 12 and Math-500 > 40% accuracy vs AWQ-b2 baseline
  2. Regularization ablation on LLaMA3.1-8B: Sweep λ ∈ {10⁻⁸, 10⁻⁶, 10⁻⁴, 10⁻², 1.0}, plot perplexity vs λ
  3. End-to-end benchmark: Deploy quantized LLaMA2-7B on RTX 3090, target 4.0×+ speedup over FP16

## Open Questions the Paper Calls Out

- **Open Question 1:** Would extending PTQTP beyond dual trit-planes (K>2) yield meaningful accuracy gains, or does the current architecture represent a fundamental sweet spot? The paper fixes K=2 without ablation, though the method theoretically supports arbitrary K.

- **Open Question 2:** Can PTQTP's ternary decomposition be extended to activations for fully multiplication-free inference? The paper notes activations remain unquantized and calls for hardware-software co-design for 1.58-bit inference acceleration.

- **Open Question 3:** How does PTQTP perform on specialized LLM architectures (Mixture-of-Experts, multimodal models)? The paper claims "model-agnostic deployment" but only evaluates dense transformer architectures.

- **Open Question 4:** What is the theoretical relationship between adaptive regularization threshold (κ<10⁶) and weight matrix spectral properties? The choice appears empirical without theoretical justification.

## Limitations

- Accuracy-efficiency tradeoffs: 82.40% Math-500 accuracy vs 89.30% for QAT-4bit represents 7.9% absolute drop, suggesting extreme compression compromises mathematical reasoning
- Generalization uncertainty: Evaluation focuses on LLaMA3.x and Qwen3 families without testing diverse architectures
- Hardware dependency: Reported 4.63× speedup relies on specific CUDA kernels and RTX 3090/A100 hardware

## Confidence

**High Confidence:** Core algorithm mechanics, quantitative comparisons, and theoretical guarantees are well-specified with clear mathematical foundations.

**Medium Confidence:** Benchmark representativeness, cross-model generalizability, and long-term stability have some evidence but may not capture edge cases or failure modes.

**Low Confidence:** Hardware implementation specifics, scalability to frontier models, and sensitivity to initialization lack comprehensive validation.

## Next Checks

1. **Cross-Architecture Robustness Test:** Apply PTQTP to diverse LLM architectures (Mistral, Gemma, DeepSeek) and measure perplexity degradation and task accuracy variance to assess generalizability.

2. **Hardware Portability Validation:** Implement PTQTP on alternative hardware platforms (Apple Silicon, ARM) and compare speedups against FP16 baselines, documenting performance gaps.

3. **Longitudinal Stability Analysis:** Conduct 24+ hour inference sessions with quantized models under varying input distributions, monitoring perplexity drift and accuracy degradation patterns.