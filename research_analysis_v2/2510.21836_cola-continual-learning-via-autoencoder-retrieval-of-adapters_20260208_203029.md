---
ver: rpa2
title: 'COLA: Continual Learning via Autoencoder Retrieval of Adapters'
arxiv_id: '2510.21836'
source_url: https://arxiv.org/abs/2510.21836
tags:
- learning
- task
- tasks
- continual
- adapter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses catastrophic forgetting in continual learning
  for large language models (LLMs), a major challenge when models need to learn new
  tasks over time without losing previously acquired knowledge. The proposed method,
  COLA, introduces a novel framework that uses autoencoders to compress and store
  lightweight task-specific LoRA adapters, eliminating the need for data replay or
  growing model parameters.
---

# COLA: Continual Learning via Autoencoder Retrieval of Adapters

## Quick Facts
- arXiv ID: 2510.21836
- Source URL: https://arxiv.org/abs/2510.21836
- Authors: Jaya Krishna Mandivarapu
- Reference count: 40
- One-line primary result: COLA achieves 89.87% intent accuracy on CLINC150 while using orders-of-magnitude less storage than replay or adapter-expansion baselines

## Executive Summary
COLA addresses catastrophic forgetting in continual learning for large language models by introducing a novel framework that compresses and stores task-specific LoRA adapters using autoencoders. The approach eliminates the need for data replay or growing model parameters, enabling efficient knowledge transfer and preventing forgetting through parameter isolation. Experiments demonstrate that COLA significantly outperforms state-of-the-art methods on diverse datasets while maintaining high task accuracy and reducing memory overhead.

## Method Summary
COLA works by training lightweight LoRA adapters for each new task on a frozen backbone, then compressing these adapters into compact latent vectors using a contractive autoencoder. During inference, the system selects the most appropriate adapter for a given input by computing perplexity across all stored adapters and choosing the one with minimum perplexity. The selected adapter is reconstructed from its latent representation and used for task execution. This approach enables warm-start transfer learning and prevents catastrophic forgetting without requiring data replay buffers or model expansion.

## Key Results
- Achieves 89.87% intent accuracy on CLINC150 dataset
- Uses orders-of-magnitude less storage than replay or adapter-expansion baselines
- Demonstrates 17.33 percentage point improvement over scratch training via warm-start transfer learning
- Maintains high task accuracy while compressing adapter weights from full parameter count to 50-dimensional latent vectors

## Why This Works (Mechanism)

### Mechanism 1: Task Isolation via Parameter-Efficient Adapters
LoRA adapters isolate task knowledge by training only low-rank matrices A∈ℝ^(r×k) and B∈ℝ^(d×r) while keeping the frozen backbone W₀ untouched. This prevents interference between sequentially learned tasks by confining updates to a subspace of rank r ≪ min(d,k), typically less than 2% of layer parameters.

### Mechanism 2: Lossy Compression via Contractive Autoencoder Manifold Learning
A contractive autoencoder learns a smooth latent manifold that captures the intrinsic geometry of LoRA weight distributions across tasks. The CAE minimizes reconstruction error plus a contractive penalty (Jacobian Frobenius norm), creating a stable manifold where similar adapters map to nearby latent codes. This enables compact storage while maintaining recoverable fidelity.

### Mechanism 3: Perplexity-Based Task Identification and Warm-Start Transfer
Perplexity serves as an unsupervised proxy for adapter-task compatibility, enabling both task inference at test time and transfer learning initialization. The adapter with minimum perplexity best fits the input distribution, indicating correct task assignment and providing an effective warm-start for new tasks.

## Foundational Learning

- **LoRA (Low-Rank Adaptation)**: Core parameter-efficient building block that constrains updates to rank-r subspaces. Without understanding that ΔW = BA reduces trainable parameters from O(d×k) to O(r×(d+k)), the compression rationale is unclear.
  - Quick check: Explain why LoRA reduces trainable parameters from O(d×k) to O(r×(d+k)) when r ≪ min(d,k).

- **Contractive Autoencoders vs. VAEs**: The paper explicitly chooses CAE over VAE for stability. Understanding the Jacobian penalty explains why the latent space remains navigable.
  - Quick check: What does the Frobenius norm of the Jacobian ‖J_f(x)‖²_F penalize, and why does this help continual learning?

- **Catastrophic Forgetting in Sequential Learning**: The fundamental problem being solved. Without this context, the design choices (frozen backbone, adapter isolation) seem arbitrary.
  - Quick check: Why does gradient-based sequential fine-tuning overwrite previously learned representations?

## Architecture Onboarding

### Component Map
```
┌─────────────────────────────────────────────────────────┐
│  FROZEN BACKBONE (GPT-2/RoBERTa) - Never updated        │
└─────────────────────────────────────────────────────────┘
                          │
                          ▼
┌─────────────────────────────────────────────────────────┐
│  LoRA ADAPTER (A, B matrices) - Task-specific           │
│  Rank r=1-4, ~2% of layer size                          │
└─────────────────────────────────────────────────────────┘
                          │
                          ▼ (after task training)
┌─────────────────────────────────────────────────────────┐
│  CAE ENCODER f_φ (discarded after training)             │
│  θ_i → z_i (latent vector, size 50)                     │
└─────────────────────────────────────────────────────────┘
                          │
                          ▼ (persistent storage)
┌─────────────────────────────────────────────────────────┐
│  LATENT STORE {z_1, z_2, ..., z_K} + DECODER g_ψ        │
└─────────────────────────────────────────────────────────┘
                          │
                          ▼ (at inference)
┌─────────────────────────────────────────────────────────┐
│  RECONSTRUCTION: θ̂_i = g_ψ(z_i) → Load adapter          │
│  SELECTION: argmin_t PPL_t(X)                           │
└─────────────────────────────────────────────────────────┘
```

### Critical Path
1. **Task arrival**: Dataset D_i arrives → attach fresh LoRA adapter to frozen backbone
2. **Adapter training**: Fine-tune only (A_i, B_i) on D_i until convergence
3. **Vectorization**: θ_i = vec[A_i; B_i] → flatten to 1D
4. **Encoding** (immediate or buffered): z_i = f_φ(θ_i), store z_i, discard θ_i
5. **Inference**: Given input X → compute PPL for all K adapters → select ĥ → decode θ̂_ĥ → generate

### Design Tradeoffs
- **Latent dimension (m)**: Paper uses m=50. Lower = better compression but risk of reconstruction error. Accuracy degrades below ~0.99 reconstruction similarity.
- **Buffer size (M)**: FIFO buffer for batch encoding. Larger buffer amortizes CAE training cost but delays encoding.
- **LoRA rank (r)**: Paper uses r=1-4. Higher rank = better task fit but larger θ_i to compress.
- **Inference latency**: K forward passes for perplexity computation. Scales linearly with task count.

### Failure Signatures
- **Reconstruction collapse**: If CAE loss plateaus with high reconstruction error (>1% deviation), increase CAE capacity or training epochs.
- **Perplexity ambiguity**: If multiple adapters have near-identical perplexity, task distributions may overlap. Consider hybrid selection approaches.
- **Forgetting despite isolation**: If backbone is accidentally unfrozen or LoRA rank is too low to capture task complexity.
- **CAE overfitting**: If encoder memorizes training adapters but fails on new task adapters. Contractive penalty should mitigate this.

### First 3 Experiments
1. **Single-task reconstruction fidelity**: Train adapter on one task, encode→decode, measure accuracy gap between original and reconstructed adapter. Target: <1% degradation.
2. **Sequential 5-task continual learning**: Train tasks T1→T2→T3→T4→T5 sequentially, encode each after training. Evaluate all 5 tasks at end. Compare against EWC, Replay, Vanilla FT baselines.
3. **Perplexity-based task inference accuracy**: Hide task labels during evaluation. Measure % of correctly identified tasks via perplexity selection. Target: >95% on distinct domains.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several critical areas require further investigation:

- **Scalability to larger models**: The inference-time adapter selection strategy requires K forward passes per query, which may become computationally prohibitive for multi-billion parameter models.
- **Performance under extreme domain shifts**: The CAE's ability to compress adapters from radically different task domains (e.g., code generation vs. creative writing) remains untested.
- **Long-term accumulation of approximation errors**: The impact of propagating reconstruction errors during warm-start transfer over long sequences of tasks is not fully characterized.

## Limitations

- **Compression fidelity tradeoffs**: The exact relationship between compression ratio and performance loss across diverse task distributions needs more exploration.
- **Perplexity selection assumptions**: The method assumes task distributions produce meaningfully different perplexities, but scenarios with overlapping distributions or adversarial similarity are not extensively explored.
- **Generalization to arbitrary task sequences**: The framework's robustness to non-stationary task distributions and unexpected task arrivals remains untested.

## Confidence

**High Confidence (⭐⭐⭐)**: Parameter-efficient adapter isolation prevents interference - The LoRA-based approach with frozen backbone is well-established in literature, and the mechanism for preventing interference through parameter isolation is theoretically sound and empirically validated.

**Medium Confidence (⭐⭐)**: CAE compression enables practical storage reduction - While the compression mechanism is sound and the 0.99 reconstruction threshold is empirically established, the broader generalizability across diverse task types and compression ratios needs more exploration.

**Medium Confidence (⭐⭐)**: Perplexity selection enables task identification and warm-start transfer - The mechanism is theoretically plausible and shows promising initial results, but the robustness to task distribution overlap and the scalability to many similar tasks requires further validation.

## Next Checks

1. **Compression-Fidelity Scaling Analysis**: Systematically vary the CAE latent dimension (m) from 10 to 200 while measuring both storage reduction and task accuracy degradation across all datasets. Plot the Pareto frontier to identify optimal compression-accuracy tradeoffs and determine the minimum viable latent dimension for each task type.

2. **Adversarial Task Distribution Test**: Construct test scenarios where task distributions are intentionally designed to produce similar perplexities (e.g., overlapping intent vocabularies, similar dialogue patterns). Measure adapter selection accuracy and compare against alternative selection mechanisms like latent-space nearest neighbor or hybrid perplexity+classifier approaches.

3. **Arbitrary Task Sequence Robustness**: Design experiments where tasks are presented in random orders, with repeated tasks and task mixtures. Evaluate whether COLA maintains performance regardless of task arrival sequence, and measure the impact of task similarity and temporal spacing on adapter selection and reconstruction quality.