---
ver: rpa2
title: 'SEAM: Semantically Equivalent Across Modalities Benchmark for Vision-Language
  Models'
arxiv_id: '2508.18179'
source_url: https://arxiv.org/abs/2508.18179
tags:
- arxiv
- visual
- across
- modalities
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SEAM reveals systematic modality imbalance in vision-language models,
  showing that all 21 evaluated models exhibit significant performance gaps between
  vision and language inputs despite semantically equivalent content. The benchmark
  achieves cross-modal agreement rates of only 0.318-0.584, far below what would be
  expected for unified reasoning.
---

# SEAM: Semantically Equivalent Across Modalities Benchmark for Vision-Language Models

## Quick Facts
- arXiv ID: 2508.18179
- Source URL: https://arxiv.org/abs/2508.18179
- Authors: Zhenwei Tang; Difan Jiao; Blair Yang; Ashton Anderson
- Reference count: 40
- Primary result: 21 evaluated VLMs show systematic modality imbalance with cross-modal agreement rates of only 0.318-0.584

## Executive Summary
SEAM benchmark reveals that all 21 evaluated vision-language models exhibit significant performance gaps between vision and language inputs despite semantically equivalent content. The benchmark achieves cross-modal agreement rates far below what would be expected for unified reasoning, demonstrating that current models lack true modality-agnostic reasoning capabilities. Models struggle particularly with chemistry and chess tasks due to tokenization errors in domain-specific notations, while visual perception failures lead to hallucinations in graph theory tasks.

## Method Summary
SEAM evaluates modality-agnostic reasoning in VLMs using semantically equivalent inputs across vision and language modalities. The benchmark includes 16 tasks across 4 domains (chess, chemistry, music, graph theory) with 200 samples per task (3,200 total), formatted as 4-way multiple-choice questions. Zero-shot chain-of-thought prompting is used with inputs rendered at 400×400 px (music at 600×600). Cross-modal agreement rates measure whether models reach the same answer through different modalities, with random baseline for 4-way MC: p² + (1-p)²/3.

## Key Results
- All 21 evaluated VLMs show systematic performance gaps between vision and language inputs despite semantic equivalence
- Cross-modal agreement rates range from 0.318-0.584, far below unified reasoning expectations
- Vision inputs often underperform even compared to language-only inputs, suggesting fundamental visual processing limitations
- Models cluster embeddings by input format rather than semantic content, indicating separate modality processing

## Why This Works (Mechanism)

### Mechanism 1
Domain-specific tokenization failures drive language modality underperformance in chemistry and chess tasks. Standard tokenizers segment specialized notations (SMILES, FEN) into semantically meaningless subwords, destroying structural semantics. Parentheses and square brackets become unreadable when incorrectly parsed. Evidence shows chemistry tasks suffer from this issue, though gold-standard tokenization experiments showed mixed results (Qwen2.5-VL-72B improved only 0.002).

### Mechanism 2
Visual patch-based processing induces hallucinations when image cuts intersect structural elements. Vision transformers divide images into patches, and when patch boundaries intersect graph edges at nodes, models hallucinate non-existent connections based on spatial proximity rather than actual edges. This explains the dramatic vision-language gap in graph theory tasks where adjacency matrices have no tokenization issues.

### Mechanism 3
VLMs lack unified cross-modal representations; they cluster embeddings by input format rather than semantic content. Despite semantic equivalence, final-layer embeddings from language, vision, and VL modalities form distinct clusters. This contrasts with simpler datasets where modalities integrate, indicating models process modalities through fundamentally different pathways without convergence.

## Foundational Learning

- **Semantic equivalence across modalities**: SEAM controls for semantics while varying representation. Understanding this distinction is essential to interpret results—not all "same information" pairs are truly semantically equivalent (OCR-based pairs aren't). Quick check: If you show a model a screenshot of text versus the same text typed out, is that semantically equivalent in SEAM's framework? (Answer: No—that's OCR-based, not using distinct notation systems.)

- **Modality imbalance in multimodal learning**: The paper documents systematic vision underperformance. Understanding that this is a known phenomenon (gradient competition during joint training) helps contextualize findings. Quick check: Why might language inputs outperform vision inputs even when both contain identical information? (Answer: Language goes directly to LLM; vision passes through encoder + projector, a noisy process.)

- **Cross-modal agreement as a metric**: Accuracy alone doesn't reveal modality integration. Agreement measures whether models reach the same answer through different modalities—critical for assessing unified reasoning. Quick check: Can a model have high accuracy but low cross-modal agreement? (Answer: Yes—they can be decoupled. A model might get correct answers through each modality independently but via different reasoning paths.)

## Architecture Onboarding

- **Component map**: Visual Encoder (ViT) -> Projector/Adapter -> LLM Backbone -> Tokenizer. The tokenizer is the domain-specific failure point for SMILES, FEN, ABC notations; vision encoder is implicated in hallucination mechanism when patch boundaries intersect structural elements.

- **Critical path**: Input modality determines processing pipeline (direct vs. encoder → projector → LLM) → Tokenization quality determines whether symbolic notation is parseable → Patch alignment determines whether visual structures are preserved → Embedding convergence (or lack thereof) at final layers reveals unified vs. separate processing.

- **Design tradeoffs**: Specialized vs. general tokenizers (custom tokenizers for domains could fix textual perception but require domain-specific training data) vs. Patch size vs. structural preservation (smaller patches reduce boundary artifacts but increase computational cost) vs. Unified embedding objectives (training for cross-modal alignment may help but wasn't tested).

- **Failure signatures**: Language underperformance + chemistry/chess domain = Check tokenization of SMILES/FEN strings; Vision underperformance + graph theory domain = Check for patch-boundary artifacts on edge/node intersections; Low cross-modal agreement despite reasonable accuracy = Check embedding clustering by modality in final layers; Adding vision input degrades performance = Strong signal that vision pathway adds noise rather than signal.

- **First 3 experiments**: Tokenization intervention test (Run SEAM chemistry/chess tasks with both standard and gold-standard tokenization on same model) → Patch alignment ablation (Systematically shift image crops relative to graph structures to test hallucination rates) → Cross-modal agreement vs. accuracy sweep (Evaluate multiple model scales within same family to determine whether agreement improvements are genuine or artifactual).

## Open Questions the Paper Calls Out

- **Open Question 1**: Does improving cross-modal agreement causally drive better accuracy, or does higher accuracy merely artifactually increase agreement through convergent correct answers? The authors state this raises an important question about direction of causality, noting the correlation could be explained by either causal direction.

- **Open Question 2**: Can VLMs be trained to learn explicit cross-modal translation mechanisms that achieve near-perfect agreement on semantically equivalent inputs? The authors note a VLM could theoretically achieve near-perfect agreement by learning to translate between modalities, but none tested have high agreement.

- **Open Question 3**: Would retraining VLMs with domain-specific tokenizers eliminate textual perception failures, or are the errors more fundamental? The gold-standard tokenization experiment showed no improvement, leading authors to hypothesize semantics may not be well-trained rather than tokenization being the sole issue.

## Limitations

- Generalizability concerns: Results may reflect SEAM's specific task design rather than fundamental architectural limitations, as the benchmark focuses on symbolic domain notations that may amplify tokenization and visual processing challenges.
- Causal mechanism validation gaps: The paper lacks controlled experiments to definitively prove causation for proposed mechanisms, with tokenization intervention showing mixed results and patch alignment effects remaining qualitative.
- Dataset construction uncertainties: Exact random seeds, rendering parameters, and filtering criteria aren't specified beyond high-level ranges, making exact reproduction challenging.

## Confidence

- **High confidence**: Existence of systematic modality gaps across all evaluated models (0.318-0.584 agreement rates) is well-supported by data.
- **Medium confidence**: Claim that VLMs lack unified cross-modal reasoning is supported by embedding clustering evidence but could be refined.
- **Low confidence**: Assertion that vision inputs consistently underperform language inputs requires qualification, as some models show vision outperforming language on specific tasks.

## Next Checks

1. **Controlled tokenization intervention**: Systematically test gold-standard vs standard tokenization across all chemistry and chess tasks for multiple model families, measuring performance deltas and statistical significance to validate whether tokenization quality drives observed language underperformance.

2. **Patch alignment ablation study**: Create controlled variants of graph theory tasks where edge-node intersections systematically shift relative to patch boundaries, quantifying hallucination rates across different alignment conditions to establish whether patch artifacts causally explain vision modality failures.

3. **Cross-modal agreement decomposition**: For models showing high accuracy but low agreement, conduct error analysis to distinguish between consistent reasoning paths yielding different outputs due to representation differences versus fundamentally different reasoning leading to correct answers through separate modalities.