---
ver: rpa2
title: Investigating the Capabilities and Limitations of Machine Learning for Identifying
  Bias in English Language Data with Information and Heritage Professionals
arxiv_id: '2504.00860'
source_url: https://arxiv.org/abs/2504.00860
tags:
- bias
- https
- gender
- language
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores the capabilities and limitations of machine
  learning (ML) in identifying gender bias within archival metadata, specifically
  from the University of Edinburgh's heritage collections. The authors argue that
  traditional ML approaches, which focus on removing bias, are inadequate for understanding
  and managing bias in data.
---

# Investigating the Capabilities and Limitations of Machine Learning for Identifying Bias in English Language Data with Information and Heritage Professionals

## Quick Facts
- arXiv ID: 2504.00860
- Source URL: https://arxiv.org/abs/2504.00860
- Reference count: 40
- Primary result: Machine learning models achieved F1 scores of 0.747-0.761 for identifying gender bias in archival metadata, outperforming human agreement (0.427 IAA), while workshop participants conceptualized bias as contextual and dynamic.

## Executive Summary
This paper explores the use of machine learning to identify gender bias in archival metadata from the University of Heritage Collections. The authors argue that traditional ML approaches focused on removing bias are inadequate for understanding bias in data. Instead, they propose a human-centered approach that uses ML to highlight and draw attention to bias rather than eliminate it. Through a workshop with 10 information and heritage professionals, they found that bias is contextual, dynamic, and inevitable, differing from how ML researchers typically conceptualize it. The study suggests ML should augment human efforts in bias management rather than fully automate the process.

## Method Summary
The study employed a cascading classification architecture using traditional ML models (Random Forest, SVM, CRF) to identify gender bias in archival metadata. The system consists of three layers: a Linguistic Classifier identifying gendered pronouns/roles, a Person Name/Occupation Classifier using CRFs, and an Omission/Stereotyping Classifier that incorporates features from the first two layers. The models were trained on 11,888 archival descriptions coded by experts using a multi-label taxonomy. The cascading approach explicitly feeds lower-level linguistic predictions into higher-level context classifiers to improve detection of abstract bias categories.

## Key Results
- ML models achieved F1 scores of 0.747-0.761 for identifying gender bias, outperforming human agreement (0.427 IAA)
- Workshop participants conceptualized bias as contextual, dynamic, and inevitable, rejecting the notion that bias should be removed from data
- The models were useful for supporting critical cataloging, raising awareness of bias, and providing evidence for resource requests, though limited by bias's contextual nature

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If grammatically gendered linguistic features are cascaded into bias classifiers, the detection of abstract bias categories (Omission, Stereotype) improves compared to document-only baselines.
- **Mechanism:** The **Cascaded Classification Architecture** works by explicitly feeding the predictions of lower-level linguistic classifiers (Linguistic Classifier identifying pronouns/roles) as features into higher-level context classifiers (Omission & Stereotype Classifier). This operationalizes the theory that linguistic markers correlate with contextual bias.
- **Core assumption:** Statistical correlations exist between the presence of explicit gendered language (e.g., "Mrs.", "he") and implicit biases (e.g., Stereotypes, Omissions) in archival text.
- **Evidence anchors:**
  - [Section 4.2] "By comparing the performance scores of the baseline and feature-engineered OSCs, we can evaluate whether including gendered language codes as features improves an OSC’s ability to classify text as gender biased."
  - [Table 2] Shows Baseline OSC F1 (0.747) improving to Cascade 3 OSC F1 (0.761), while human IAA lagged behind at 0.427.
- **Break condition:** This mechanism may fail if the linguistic markers (pronouns/roles) are removed or neutralized in the source text, or if the correlation between explicit gendered terms and implicit bias is weak in a specific dataset.

### Mechanism 2
- **Claim:** Using "traditional" ML models (SVMs, CRFs) rather than Large Language Models (LLMs) preserves the validity of bias provenance by isolating learning to the target dataset.
- **Mechanism:** **Bias Containment via Architecture Choice.** By avoiding pre-trained deep learning models (which contain learned biases from vast external corpora), the system ensures that identified biases originate strictly from the archival collection being analyzed, making the "bias signal" traceable and auditable for domain experts.
- **Core assumption:** Pre-trained models would introduce "bias noise" from external data that would be indistinguishable from the bias inherent in the specific archival collection.
- **Evidence anchors:**
  - [Section 4.2] "Since we aimed to identify gender biased language in our training dataset only, we did not use pre-trained deep learning models, because their classification of the descriptions would have been influenced by biased language in other data."
  - [Abstract] "...bias... due to its contextual nature..."
- **Break condition:** This approach fails to scale or generalize if the target dataset is too small to train effective traditional models, or if the vocabulary is highly out-of-distribution (OOV) relative to the training set.

### Mechanism 3
- **Claim:** If ML models are framed as "sandcastles" (tools for provocation) rather than definitive solutions, they successfully reveal the "inevitability" and "contextuality" of bias to human experts.
- **Mechanism:** **Discourse Provocation via Model Imperfection.** The system leverages the model's limitations (e.g., high precision but moderate recall) to force human intervention. The model acts as a boundary object that surfaces disagreements (low IAA) among humans, validating the paper's argument that bias cannot be fully automated.
- **Core assumption:** Domain experts (information professionals) have specific conceptualizations of bias (contextual, dynamic) that contradict standard ML fairness definitions (removable, static).
- **Evidence anchors:**
  - [Section 6.1] "Participants’ comments communicated a conceptualization of bias that is context-dependent... and an awareness of the subjectivity of bias."
  - [Section 4] "'sandcastling'... emphasizes the process of building something... [to] facilitate a collaborative discourse."
- **Break condition:** If stakeholders treat the model output as objective ground truth rather than a hypothesis to be critiqued, the mechanism of "critical cataloging" breaks down into uncritical automation.

## Foundational Learning

- **Concept: Inter-Annotator Agreement (IAA) vs. Model Performance**
  - **Why needed here:** The paper relies on the counter-intuitive finding that models (F1 ~0.76) outperformed human agreement (F1 ~0.43) to argue that the *task* is inherently subjective, not that the model is "solved." Without understanding IAA, one might falsely conclude the model is highly accurate rather than just consistent with a noisy label set.
  - **Quick check question:** If human coders agree only 43% of the time on "Stereotype," does a model F1 score of 76% imply the model is "right" or just "consistent"?

- **Concept: Data Feminism & Situated Knowledge**
  - **Why needed here:** The analysis of the workshop uses intersectional feminism to interpret why experts rejected "removing" bias (which erases history) in favor of "highlighting" it. This explains the qualitative value of the system despite moderate quantitative metrics.
  - **Quick check question:** Why would a feminist critique prefer "highlighting" a gendered role (e.g., "Mrs.") over "removing" it from the dataset?

- **Concept: Feature Engineering vs. End-to-End Learning**
  - **Why needed here:** The study uses explicit feature cascading (feeding output of Classifier A into Classifier B) rather than a monolithic neural network. This is central to their "mechanism" of tracing bias to specific linguistic tokens.
  - **Quick check question:** How does feeding "Gendered Pronoun" predictions into a "Stereotype" classifier physically change the input for the second classifier?

## Architecture Onboarding

- **Component map:**
  - Input: ISAD(G) metadata (Title, Scope/Contents, Biographical/Historical)
  - Layer 1 (Linguistic Classifier): Multi-label Random Forest Classifier Chains on FastText embeddings. Labels: Gendered Pronoun, Gendered Role, Generalization
  - Layer 2 (Person Name/Occupation Classifier): Conditional Random Fields (CRF) with AROW algorithm. Labels: Feminine, Masculine, Unknown, Occupation
  - Layer 3 (Omission & Stereotype Classifier): Multi-label SVM with SGD (One-vs-Rest) on TF-IDF matrices + Features from Layers 1 & 2
  - Output: Coded descriptions + Quantitative bias reports (tables/charts) for human review

- **Critical path:**
  1. Preprocessing (lowercase, NLTK tokenization)
  2. Training Layer 1 (LC) and Layer 2 (PNOC) separately
  3. Passing predictions from LC/PNOC as additional features into the Layer 3 (OSC) training data
  4. Generating OSC predictions for the full corpus
  5. Aggregating counts for workshop visualization (e.g., "collections with highest Stereotype counts")

- **Design tradeoffs:**
  - **Interpretability vs. State-of-the-Art Performance:** The authors explicitly chose traditional ML (Random Forest, SVM) over Transformers/LLMs to maintain *provenance* of bias (Section 4.2), trading potential accuracy boosts for guaranteed domain-specific origin of the signal
  - **Precision vs. Recall:** The OSC models showed high Precision (~0.90) but lower Recall (~0.65). This aligns with the goal of providing *evidence* for resource requests (avoiding false positives is crucial for credibility) rather than catching every single instance (Section 7.2)

- **Failure signatures:**
  - **Generalization Code Failure:** The model performed poorly on "Generalization" (F1 0.341) because human coders also struggled (low recall/precision in annotation), indicating this category is too abstract for current feature sets (Table 4)
  - **Masculine/Unknown Confusion:** The PNOC frequently confused "Masculine" and "Unknown" names (Table 5), likely due to stereotypical inferences in the training data where names without explicit pronouns were defaulted to masculine

- **First 3 experiments:**
  1. **Baseline Comparison:** Replicate the "Baseline OSC" (TF-IDF only) vs. "Cascade 3 OSC" (with PNOC features) on a subset of the data to quantify the specific boost provided by the gendered features
  2. **Error Analysis on Omission:** Isolate the false negatives for the "Omission" code (where the model missed the bias) to determine if it is a feature limitation (lack of context length) or a training data issue (low human IAA)
  3. **Domain Transfer Test:** Train the LC (Linguistic Classifier) on this dataset and run it on a modern GLAM catalog (e.g., current library metadata) to see if the "linguistic" definition of bias transfers, or if it is strictly bound to the historical context of the training data

## Open Questions the Paper Calls Out
- **Open Question 1:** How do the gender biases identified by supervised, traditional machine learning models compare to those identified by pre-trained deep learning models when applied to the same archival corpus?
- **Open Question 2:** Do the linguistic patterns for gender bias manifestation identified in this specific British archival catalog hold across different cultural contexts and text corpora?
- **Open Question 3:** Can machine learning models be effectively trained to detect non-binary gender bias in archival metadata given the current lack of representation in historical records?

## Limitations
- The study's findings are based on a single archival collection from one institution, limiting generalizability to other cultural heritage contexts
- Human inter-annotator agreement was notably low (0.427 IAA), suggesting the bias classification task itself is highly subjective and may affect model validity
- The dataset contains historical language patterns that may not reflect contemporary bias expressions, potentially limiting real-world applicability

## Confidence
- **High Confidence:** The cascading architecture demonstrably improves classification performance (F1 increase from 0.747 to 0.761) through explicit feature engineering
- **Medium Confidence:** The workshop findings that information professionals conceptualize bias as contextual and dynamic are well-supported but may not generalize to all GLAM professionals
- **Medium Confidence:** The choice of traditional ML over LLMs successfully preserves bias provenance from the target dataset, though this comes at the cost of potential performance gains

## Next Checks
1. **Domain Transfer Test:** Train the Linguistic Classifier on the Edinburgh dataset and evaluate on modern GLAM catalog metadata to assess whether the linguistic definition of bias transfers across time periods
2. **Bias Containment Validation:** Systematically test for external bias injection by comparing model outputs when using pre-trained embeddings versus custom embeddings on the same dataset
3. **Annotation Reliability Assessment:** Conduct a formal reliability analysis on the original coding process to determine if the low IAA reflects genuine subjectivity or inconsistent application of the coding framework