---
ver: rpa2
title: Do Images Speak Louder than Words? Investigating the Effect of Textual Misinformation
  in VLMs
arxiv_id: '2601.19202'
source_url: https://arxiv.org/abs/2601.19202
tags:
- round
- answer
- vlms
- arxiv
- confidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the vulnerability of vision-language models
  (VLMs) to textual misinformation that contradicts clear visual evidence. The authors
  introduce the CONTEXT-VQA dataset, consisting of VQA pairs with systematically generated
  persuasive prompts designed to mislead models.
---

# Do Images Speak Louder than Words? Investigating the Effect of Textual Misinformation in VLMs

## Quick Facts
- **arXiv ID:** 2601.19202
- **Source URL:** https://arxiv.org/abs/2601.19202
- **Reference count:** 40
- **Primary result:** VLMs are highly vulnerable to textual misinformation that contradicts visual evidence, showing >48.2% accuracy drop after one round of persuasion.

## Executive Summary
This paper investigates how vision-language models (VLMs) respond when textual misinformation directly contradicts clear visual evidence. The authors introduce CONTEXT-VQA, a dataset of VQA pairs with systematically generated persuasive prompts designed to mislead models. Testing 11 state-of-the-art VLMs using a multi-round conversational framework reveals that models are highly susceptible to textual manipulation, often overriding visual evidence in favor of contradictory text. The study demonstrates that logical persuasion is most effective, model scale provides some robustness benefits, but initial capability does not guarantee resistance to attacks. These findings highlight a critical limitation in current VLMs and underscore the need for improved robustness against textual manipulation.

## Method Summary
The study evaluates 11 VLMs on a filtered subset of 920 questions from A-OKVQA, selected to ensure all models answered correctly at baseline. Persuasive misinformation is generated using Gemini 2.5-Pro with four strategies: Repetition, Logical, Credibility, and Emotional appeals. Each strategy targets the distractor answer with second-highest baseline confidence. A multi-round conversation framework tests model responses across up to four rounds of persuasion, measuring accuracy drops and confidence shifts. The evaluation uses temperature=0.2, thinking mode disabled, and constrains outputs to MCQ format. A simple alarm prompt defense is also tested to assess mitigation effectiveness.

## Key Results
- VLMs show an average accuracy drop of over 48.2% after just one round of persuasion.
- Logical persuasion is the most effective strategy, convincing models in more than half of testing scenarios.
- Model scale helps robustness within the same family, but initial capability (accuracy on standard benchmarks) does not predict persuasion resistance.
- Smaller models are disproportionately affected, with QwenVL-2.5-3B dropping to 18.3% final accuracy despite ranking fifth in initial capability.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** VLMs exhibit cross-modal arbitration bias, defaulting to textual instructions over visual evidence when presented with conflicting information.
- **Mechanism:** Current VLM training paradigms emphasize instruction-following, creating an "obedience bias" where textual commands gain undue authority. Combined with training on harmonious image-text pairs without contradictory examples, models lack robust internal arbitration when modalities conflict.
- **Core assumption:** The prioritization of text stems from training rather than architectural necessity.
- **Evidence anchors:**
  - [abstract] "models are indeed vulnerable to misleading textual prompts, often overriding clear visual evidence in favor of the conflicting text"
  - [page 19] "Models today are intensely fine-tuned for instruction-following, which can instill a critical 'obedience bias' that grants undue authority to textual commands"
  - [corpus] Related work "Text Speaks Louder than Vision" (2504.01589) shows textual biases in VLMs via ASCII art, supporting cross-modal prioritization patterns
- **Break condition:** If models were trained with explicit conflict-resolution objectives or adversarial examples containing contradictory modalities, this bias should diminish.

### Mechanism 2
- **Claim:** Logical persuasion is most effective because it provides structured causal narratives that models treat as more authoritative than raw visual input.
- **Mechanism:** Logical appeals fabricate technical-sounding evidence (fake depth maps, RGB histograms, segmentation labels) that exploit models' pattern-matching with training data containing legitimate technical explanations. The coherent causal chain overrides visual perception.
- **Core assumption:** Models weight coherent explanatory structures over perceptual evidence when both are present.
- **Evidence anchors:**
  - [page 7, Table 5] Logical appeal "proves most effective in more than half of the testing scenarios"
  - [page 23, Figure 9] Case study shows logical persuasion with "fabricated numerical measurements" convinces all models except GPT-4o to select wrong answer despite unambiguous visual evidence
  - [corpus] Corpus evidence is weak for this specific mechanism‚Äîno direct corpus support found
- **Break condition:** If models were trained to verify technical claims against visual evidence or lacked exposure to technical diagnostic formats, logical appeals should lose effectiveness.

### Mechanism 3
- **Claim:** Model scale provides robustness benefits through improved cross-modal grounding, but initial capability (accuracy on standard benchmarks) does not predict persuasion resistance.
- **Mechanism:** Larger models develop better internal representations for reconciling multimodal signals, but standard VQA training optimizes for correctness on benign inputs, not adversarial text. Capability and robustness are orthogonal dimensions requiring separate optimization.
- **Core assumption:** Robustness requires explicit training signal, not just capacity.
- **Evidence anchors:**
  - [page 6, Table 4] "QwenVL-2.5-3B...ranking fifth in initial capability with 86.7%, it performs the worst in terms of robustness...18.3%"
  - [page 6, Finding III] "Scaling up helps, in terms of both capability and robustness" within same model family
  - [corpus] Multimodal adversarial defense work (2405.18770) notes VL models are "highly vulnerable to adversarial attacks," suggesting scale alone insufficient
- **Break condition:** If capability and robustness were correlated, highest-performing models on standard benchmarks would show best persuasion resistance‚Äîthis is not observed.

## Foundational Learning

- **Concept:** **Cross-modal grounding** - How VLMs align representations across vision and language modalities.
  - *Why needed here:* Understanding why models fail to ground responses in visual evidence when contradicted by text is central to this work.
  - *Quick check question:* Given an image of a red ball and text saying "the ball is blue," does the model prioritize visual or textual evidence?

- **Concept:** **Instruction-following fine-tuning** - Post-training alignment where models learn to follow user instructions.
  - *Why needed here:* The paper hypothesizes this creates "obedience bias" making models vulnerable to textual manipulation.
  - *Quick check question:* How does fine-tuning for helpfulness vs. accuracy affect model behavior when instructions contradict facts?

- **Concept:** **Persuasion strategies in NLP** - Rhetorical techniques (logical, credibility, emotional, repetition) used to influence beliefs.
  - *Why needed here:* The paper systematically tests how different persuasion types affect VLM robustness.
  - *Quick check question:* Which persuasion strategy would likely be most effective against a model trained on scientific text?

## Architecture Onboarding

- **Component map:** CONTEXT-VQA dataset ‚Üí Multi-round conversation framework ‚Üí Accuracy/confidence metrics ‚Üí Visualization. Core components: (1) Question filtering (common subset), (2) Misinformation generation (Gemini 2.5-Pro with templates), (3) Evaluation loop (4 rounds max), (4) Defense testing (alarm prompts).

- **Critical path:** Start with A-OKVQA ‚Üí filter to questions all models answer correctly ‚Üí select distractor target ‚Üí generate 4 persuasion variants per question ‚Üí run multi-round evaluation ‚Üí compute accuracy and confidence shifts. Key dependency: baseline filtering ensures measured drops are from persuasion, not question difficulty.

- **Design tradeoffs:** (1) MCQ format simplifies evaluation but limits response diversity analysis; (2) 920-question subset controls variables but may not generalize; (3) Single-turn misinformation generation (vs. adaptive) is scalable but less realistic; (4) Disabling thinking mode ensures fair comparison but reduces model reasoning transparency.

- **Failure signatures:** (1) 48.2% average accuracy drop after Round 1 indicates text-prioritization bias; (2) Confidence inversion‚Äîmodels become highly confident in wrong answers (28.7% at >99% confidence post-persuasion); (3) Diminishing returns after Round 1 suggests single-exposure vulnerability; (4) Small models collapse under logical appeals (QwenVL-2.5-3B: 18.3% final accuracy).

- **First 3 experiments:**
  1. **Baseline robustness test:** Run filtered 920-question set on your VLM with no persuasion to establish 100% baseline, then apply single-round logical appeals to measure drop percentage.
  2. **Strategy comparison:** Test all 4 persuasion strategies separately to identify model-specific vulnerabilities (e.g., proprietary models weaker to repetition, open-source to logical).
  3. **Defense validation:** Add alarm prompt ("carefully examine the image") and measure improvement across strategies, expecting 4-16% gains with strongest effect on repetition attacks.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What specific internal mechanisms or architectural components drive VLMs to prioritize textual misinformation over visual evidence?
- **Basis in paper:** [explicit] The authors state their work "do[es] not touch upon deeper investigation into the internal reasoning processes of VLMs... specifically, why VLMs so frequently override clear visual evidence."
- **Why unresolved:** The study quantifies the *existence* of the "obedience bias" but does not analyze internal states to explain *why* the models default to trusting the text modality during conflicts.
- **What evidence would resolve it:** Mechanistic interpretability studies (e.g., probing attention heads or hidden states) identifying how text tokens suppress visual feature representations during inference.

### Open Question 2
- **Question:** Can architectural modifications or advanced fine-tuning schemes successfully mitigate vulnerability to logical persuasion attacks?
- **Basis in paper:** [explicit] The authors note that their mitigation strategy is "limited to a simple prompt-based defense" and explicitly call for exploring "improved model architectures or advanced fine-tuning schemes."
- **Why unresolved:** The simple "alarm" prompt defense primarily mitigated repetition attacks but was less effective against logical appeals, which were the most successful attack strategy.
- **What evidence would resolve it:** Evaluation of models fine-tuned with adversarial multimodal examples or architectures with modality-gating mechanisms on the CONTEXT-VQA benchmark.

### Open Question 3
- **Question:** Does the susceptibility to persuasion generalize to larger, more diverse datasets and non-VQA visual tasks?
- **Basis in paper:** [explicit] The authors acknowledge that "due to a modest budget, our dataset is limited in scale" and suggest "scaling this approach to encompass larger and more varied datasets."
- **Why unresolved:** The study relies on a filtered subset of 920 questions from A-OKVQA (primarily object recognition), limiting the understanding of vulnerability across diverse visual reasoning contexts.
- **What evidence would resolve it:** Applying the multi-round persuasion framework to dense prediction tasks (e.g., segmentation) or open-ended generation tasks across a broader corpus.

## Limitations

- The study uses a filtered subset of 920 questions from A-OKVQA, which may not fully represent broader VLM performance across diverse visual reasoning tasks.
- Static misinformation generation is used rather than adaptive attacks, potentially underestimating real-world manipulation risks.
- MCQ format constrains response diversity and may mask nuanced reasoning patterns.
- Findings are limited to closed-domain VQA tasks and may not generalize to open-ended generation scenarios.

## Confidence

- **High Confidence (‚ö°):** VLMs are vulnerable to textual misinformation contradicting visual evidence (48.2% average accuracy drop across 11 models).
- **Medium Confidence (üîç):** Logical persuasion is most effective, but rankings vary by model family; scale helps robustness within families but cross-family comparisons are complicated.
- **Low Confidence (‚ùì):** The "obedience bias" from instruction-following fine-tuning is plausible but not directly tested; scale effects cannot be definitively separated from architectural differences.

## Next Checks

1. **Cross-dataset validation:** Test the same persuasion framework on VQAv2 and ScienceQA datasets to verify whether vulnerabilities generalize beyond A-OKVQA's knowledge-based questions to perception-based and reasoning-based VQA tasks.

2. **Adaptive attack simulation:** Implement an adaptive persuasion strategy where misinformation generation responds to previous model answers, testing whether this closed-loop approach produces stronger attacks than the current static single-turn generation.

3. **Ablation on fine-tuning objectives:** Compare models with identical architectures but different fine-tuning objectives (instruction-following vs. accuracy-maximization) on the same persuasion tasks to isolate the specific contribution of instruction-following bias to vulnerability.