---
ver: rpa2
title: Towards disentangling the contributions of articulation and acoustics in multimodal
  phoneme recognition
arxiv_id: '2505.24059'
source_url: https://arxiv.org/abs/2505.24059
tags:
- speech
- audio
- multimodal
- video
- phoneme
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the problem of disentangling the contributions
  of acoustics and articulation in phoneme recognition by developing unimodal audio
  and video models as well as multimodal models using a long-form single-speaker MRI
  corpus. The core method involves using Conformer architectures with audio features
  from WavLM and video features from a fine-tuned ViT model, trained on synchronized
  speech audio and real-time MRI data from one speaker.
---

# Towards disentangling the contributions of articulation and acoustics in multimodal phoneme recognition

## Quick Facts
- arXiv ID: 2505.24059
- Source URL: https://arxiv.org/abs/2505.24059
- Reference count: 0
- Primary result: Audio model achieves PER of 0.21, outperforming Wav2Vec2Phoneme baseline (PER 0.36)

## Executive Summary
This study addresses the challenge of disentangling acoustic and articulatory contributions to phoneme recognition by developing unimodal audio and video models as well as multimodal models using a long-form single-speaker MRI corpus. The core approach uses Conformer architectures with WavLM audio features and fine-tuned ViT video features, trained on synchronized speech audio and real-time MRI data. The primary finding is that audio-only models significantly outperform multimodal models in phoneme recognition accuracy, though multimodal attention analysis reveals earlier temporal access to phoneme-relevant information for certain phoneme classes.

## Method Summary
The study uses a single-speaker real-time MRI corpus with synchronized audio to train unimodal audio and video models alongside multimodal variants. Audio features are extracted from the 9th layer of a pre-trained WavLM base model, while video features come from a fine-tuned ViT base model processing mid-sagittal MRI frames. All models use a 3-layer Conformer encoder followed by an LSTM decoder with CTC loss for phoneme recognition. The multimodal model temporally concatenates audio and video features, while ablation studies examine video-only performance. Attention weight analysis compares temporal information access patterns between models.

## Key Results
- Audio model achieves PER of 0.21, significantly outperforming Wav2Vec2Phoneme baseline (PER 0.36)
- Multimodal model achieves PER of 0.26, slightly worse than audio-only model
- Video-only model achieves PER of 0.49, demonstrating articulatory features alone are less informative
- Attention analysis shows multimodal models attend differently to temporal information, with earlier access for liquids and vowels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Single-speaker training data enables more veridical acoustics-to-articulation mapping than multi-speaker corpora.
- Mechanism: By eliminating cross-speaker anatomical variability (vocal tract morphology differences), the model can learn a consistent relationship between acoustic outputs and articulatory configurations for the same motor system.
- Core assumption: Cross-speaker variability obscures the one-to-many mapping between articulation and acoustics; removing it reveals cleaner structure.
- Evidence anchors:
  - [abstract] "limited by their reliance on multi-speaker corpora... prevents such models from learning a detailed relationship between acoustics and articulation due to considerable cross-speaker variability"
  - [section 1] "considerable cross-speaker variability in articulation and vocal tract morphology"
  - [corpus] Weak direct evidence—neighbor papers do not address single-speaker vs. multi-speaker comparisons for this task.

### Mechanism 2
- Claim: Articulatory (video) features provide earlier temporal access to phoneme-relevant information than acoustic features alone.
- Mechanism: Vocal tract constrictions form before acoustic onset (anticipatory coarticulation). The multimodal model's attention shifts toward these pre-onset intervals, capturing constriction formation that the audio-only model cannot access.
- Core assumption: The attention weight differences reflect meaningful information gain, not noise or optimization artifacts.
- Evidence anchors:
  - [abstract] "multimodal models attend differently to temporal information... suggesting earlier access to phoneme-relevant information when articulatory data is included"
  - [section 4.3, Figure 5] "the tongue tip constriction for /ô/ starts to form before the acoustic onset of /@/" and "multimodal model attention is highly localized to intervals containing crucial constriction information"
  - [corpus] Neighbor paper "Tonguescape" explores vowel articulation understanding in LLMs using tongue position concepts, indirectly supporting articulation-first information structure, but provides no direct timing evidence.

### Mechanism 3
- Claim: WavLM Layer 9 features encode rich phonetic structure sufficient for strong phoneme recognition, outperforming larger pre-trained baselines on this constrained dataset.
- Mechanism: Self-supervised speech representations from intermediate transformer layers capture phonetically-relevant patterns without task-specific fine-tuning on large corpora, enabling strong performance even with limited training data (~38 min).
- Core assumption: The phonetic information in Layer 9 is task-appropriate and not degraded by the denoising preprocessing step.
- Evidence anchors:
  - [abstract] "Audio features from WavLM... audio model achieves PER of 0.21, significantly outperforming Wav2Vec2Phoneme baseline (PER 0.36)"
  - [section 3.2] "features were extracted from the 9th layer of the pre-trained base WavLM model, based on previous work demonstrating that this layer encodes rich phonetic information"
  - [corpus] Neighbor paper "POWSM" validates that phonetic-specialized speech foundation models improve phone recognition, supporting intermediate-layer phonetic encoding claims.

## Foundational Learning

- Concept: **Connectionist Temporal Classification (CTC) Loss**
  - Why needed here: Phoneme recognition on unsegmented continuous speech requires alignment-free training; CTC marginalizes over all possible alignments between input sequences and phoneme output sequences.
  - Quick check question: Can you explain why CTC requires a blank token and how it handles variable-length input-output mappings?

- Concept: **Self-Attention in Conformer Architectures**
  - Why needed here: The paper analyzes attention weights to interpret temporal information access; understanding multi-head self-attention is prerequisite to interpreting these visualizations.
  - Quick check question: Given query, key, and value matrices, how would you compute which timesteps the model "attends to" most during a specific phoneme?

- Concept: **Articulatory Phonology (Gestural Organization)**
  - Why needed here: The paper's interpretation relies on articulatory timing (constrictions forming before acoustic onset); basic understanding of coarticulation and gestural overlap is needed to interpret attention-weight findings.
  - Quick check question: Why might tongue tip constriction for /r/ begin before the acoustic onset of a preceding schwa?

## Architecture Onboarding

- Component map: Audio Input → WavLM (frozen, Layer 9) → 768-dim features; Video Input → ViT (fine-tuned, CLS token) → 768-dim features; Fusion: Temporal concatenation → 1536-dim for multimodal; Encoder: 3-layer Conformer (4 heads, kernel=31, ff_dim=256); Decoder: LSTM (128 hidden) → Linear → CTC output

- Critical path: Audio preprocessing (VAD chunking → denoising → WavLM extraction) is the most failure-prone pipeline stage; errors here cascade through all downstream components.

- Design tradeoffs:
  - 3 Conformer layers (vs. typical 12+): Faster training, but may limit temporal modeling depth—appropriate given small dataset (38 min).
  - Temporal concatenation fusion (vs. cross-attention): Simpler, but may not optimally align asynchronous modalities.
  - Single-speaker data: Enables cleaner acoustics-articulation mapping, but sacrifices generalization.

- Failure signatures:
  - Video-only PER = 0.49 (much worse than audio): Expect articulatory features to be supplementary, not primary.
  - Multimodal PER = 0.26 (slightly worse than audio): Video features may introduce noise for some phonemes; check place-of-articulation breakdown for subtractive cases.
  - Labials perform poorly in video model: Mid-sagittal MRI poorly captures lip constriction; expect degraded performance there.

- First 3 experiments:
  1. Replicate audio-only baseline with WavLM Layer 9 features on your own phoneme recognition dataset; verify PER improvements over Wav2Vec2Phoneme before adding video complexity.
  2. Ablate fusion strategy: Compare temporal concatenation vs. feature-level averaging vs. cross-attention fusion on a held-out validation set.
  3. Analyze attention weight differences for a single liquid/vowel sequence; manually verify that multimodal attention peaks align with articulatory constriction onsets visible in video frames.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would increasing training data duration enable multimodal models to outperform audio-only models in phoneme recognition?
- Basis in paper: [explicit] Authors state "the total speech time after applying VAD was only 38 minutes. Such limited data undoubtedly still limits the ability of the model to learn strong connections between acoustics and articulation."
- Why unresolved: The current multimodal model achieved PER of 0.26 versus audio-only 0.21, suggesting articulatory features did not improve performance, but it is unclear whether this is a fundamental limitation or a data scarcity issue.
- What evidence would resolve it: Train multimodal and audio-only models on a larger single-speaker corpus (e.g., 5+ hours) and compare PER differences; if multimodal surpasses audio-only, data scarcity was the limiting factor.

### Open Question 2
- Question: How do findings generalize across different speakers with varying vocal tract morphology and articulation patterns?
- Basis in paper: [explicit] The study "only" used a single-speaker corpus, and authors note previous work was "limited by their reliance on multi-speaker corpora" which prevents learning detailed acoustic-articulatory relationships due to cross-speaker variability.
- Why unresolved: Using one speaker enables controlled analysis but raises questions about whether observed attention patterns (e.g., earlier access to phoneme-relevant information for liquids and vowels) are speaker-specific or universal.
- What evidence would resolve it: Replicate the experimental pipeline across multiple single-speaker datasets and compare latent space structures, attention weight patterns, and class-specific PER differences.

### Open Question 3
- Question: Would alternative fusion strategies or architectures better leverage articulatory timing advantages observed in attention analysis?
- Basis in paper: [explicit] Authors note they "only implemented this approach with an off-the-shelf Conformer architecture, and replicating this work with other architectures... could offer further insights."
- Why unresolved: The multimodal model's attention showed earlier access to phoneme-relevant information for certain phonemes, yet performance did not improve. Simple feature concatenation may not optimally exploit asynchronous articulatory-acoustic cues.
- What evidence would resolve it: Compare architectures with cross-modal attention mechanisms, hierarchical fusion, or temporal alignment modules against the current concatenation approach, specifically examining PER on liquids and vowels where timing differences were most pronounced.

### Open Question 4
- Question: Why does multimodal integration appear subtractive for certain places of articulation (e.g., velars) but potentially additive for others?
- Basis in paper: [inferred] Results show multimodal PER is worst on velars but best on coronals, and authors note "for some places of articulation multimodality can be additive and for others subtractive" without explaining the mechanism.
- Why unresolved: The t-SNE analysis shows consonant clustering differences between models, but the relationship between latent space organization and class-specific performance degradation remains unclear.
- What evidence would resolve it: Conduct fine-grained confusion matrix analysis for velars versus coronals, correlate with attention distribution patterns and articulatory visibility in MRI frames, and examine whether velar constrictions are less visually distinctive in mid-sagittal view.

## Limitations
- Single-speaker corpus severely limits generalizability across speakers with different vocal tract morphology and articulation patterns.
- Cannot independently validate findings due to lack of access to the MRI corpus and fine-tuned ViT model details.
- Interpretation of attention weight differences as "earlier access" relies on untested assumptions about articulatory timing.

## Confidence

- **High**: WavLM Layer 9 features encode strong phonetic information (supported by consistent literature on intermediate layers of SSL models)
- **Medium**: Single-speaker training enables cleaner acoustics-articulation mapping (mechanistically sound but untested across speakers)
- **Medium**: Articulatory features provide earlier temporal access to phoneme-relevant information (plausible but interpretation relies on untested assumptions)
- **Low**: Multimodal model PER of 0.26 is meaningfully worse than audio-only (difference is small; without statistical significance testing, this is inconclusive)

## Next Checks

1. **Reproduce audio-only baseline**: Train Conformer with WavLM Layer 9 features on a public phoneme recognition dataset (e.g., TIMIT or Librispeech-phoneme). Verify PER improvements over Wav2Vec2Phoneme before attempting multimodal extensions.

2. **Cross-speaker generalization test**: Train the same architecture on a multi-speaker rtMRI corpus (if available) or simulate cross-speaker variability. Measure degradation in acoustics-articulation mapping quality and PER to quantify single-speaker advantages.

3. **Attention interpretation validation**: For a liquid or vowel sequence with clear articulatory constrictions, manually annotate video frames showing constriction formation and compare to multimodal attention peaks. Quantify temporal alignment between visual constriction onset and attention maxima.