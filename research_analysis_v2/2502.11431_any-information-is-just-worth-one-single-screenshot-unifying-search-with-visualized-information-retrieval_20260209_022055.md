---
ver: rpa2
title: 'Any Information Is Just Worth One Single Screenshot: Unifying Search With
  Visualized Information Retrieval'
arxiv_id: '2502.11431'
source_url: https://arxiv.org/abs/2502.11431
tags:
- screenshot
- retrieval
- question
- data
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a new paradigm called Visualized Information
  Retrieval (Vis-IR), where multimodal information (text, images, tables, charts)
  is jointly represented by a unified visual format called "screenshots" for retrieval
  tasks. The authors make three key contributions: (1) VIRA, a large-scale dataset
  with 13 million screenshots and 20 million caption and question-answer pairs; (2)
  UniSE, a family of retrieval models enabling screenshots to query or be queried
  across data modalities, available in CLIP-based (efficient) and MLLM-based (expressive)
  variants; and (3) MVRB, a comprehensive benchmark with four task categories (screenshot
  retrieval, composed screenshot retrieval, screenshot question answering, open-vocab
  classification) across domains like news, products, papers, and charts.'
---

# Any Information Is Just Worth One Single Screenshot: Unifying Search With Visualized Information Retrieval

## Quick Facts
- arXiv ID: 2502.11431
- Source URL: https://arxiv.org/abs/2502.11431
- Reference count: 37
- Primary result: UniSE-MLLM achieves 55.72% average Recall@1 on MVRB benchmark, surpassing previous best by 7.6%

## Executive Summary
This paper introduces Visualized Information Retrieval (Vis-IR), a new paradigm that unifies multimodal information retrieval by representing all document types as screenshots. The authors propose UniSE, a family of retrieval models that can query or be queried across data modalities using these unified visual representations. The approach significantly outperforms existing methods on a comprehensive benchmark, demonstrating that layout preservation and visual semantics captured in screenshots are crucial for effective multimodal retrieval.

## Method Summary
The Vis-IR framework consists of two main components: VIRA, a large-scale dataset with 13 million screenshots and 20 million caption/question-answer pairs across 7 domains; and UniSE, a family of retrieval models with CLIP-based (efficient) and MLLM-based (expressive) variants. The models are trained in a two-stage process: first pre-training on screenshot-caption pairs using bidirectional contrastive loss, then fine-tuning on QA data with hard negatives. Hard negatives are explicitly mined based on visual and textual similarity to sharpen the embedding space. The approach is evaluated on MVRB, a comprehensive benchmark with 20 tasks across four categories (screenshot retrieval, composed screenshot retrieval, screenshot QA, and open-vocab classification).

## Key Results
- UniSE-MLLM achieves 55.72% average Recall@1 on MVRB benchmark, surpassing previous best by 7.6%
- UniSE-CLIP (428M params) and UniSE-MLLM (2.21B params + visual tokens) significantly outperform existing methods
- Hard negative mining and two-stage training are crucial for performance, with ablations showing clear improvements from these components
- Visualized Information Retrieval outperforms traditional text-based retrieval and demonstrates limitations of zero-shot multimodal retrievers on screenshots

## Why This Works (Mechanism)

### Mechanism 1: Layout Preservation via Screenshot Unification
- **Claim:** Treating multimodal documents as raw pixel screenshots preserves spatial relationships and visual semantics lost in OCR pipelines
- **Mechanism:** Encoding entire visual layout as single image captures structural cues simultaneously with textual content
- **Core assumption:** Visual encoder has sufficient resolution and OCR-capability to extract semantic meaning from pixel-level text
- **Evidence anchors:** Abstract states multimodal information is jointly represented by screenshots; section 5.2 notes traditional methods lose crucial layout and visual semantics

### Mechanism 2: Two-Stage Vision-Language Alignment
- **Claim:** Separating training into semantic pre-training and task fine-tuning stabilizes learning of universal screenshot embeddings
- **Mechanism:** Stage 1 aligns visual embedding space with high-level semantics using captions; Stage 2 tunes this space for retrieval-specific instructions using query-to-screenshot and screenshot-to-screenshot pairs
- **Core assumption:** Captions provide sufficient proxy for global semantic content before model learns to handle specific interrogative queries
- **Evidence anchors:** Section 3.2 describes two-stage training workflow; section 5.3.1 Table 3 shows combination of Caption + QA data outperforms either alone

### Mechanism 3: Hard Negative Distillation
- **Claim:** Explicitly mining hard negatives based on visual and textual similarity forces model to learn fine-grained discrimination
- **Mechanism:** Training against screenshots that look similar or share similar captions but are semantically distinct sharpens decision boundary in embedding space
- **Core assumption:** Off-the-shelf embedding models used for mining are accurate enough to identify valid hard negatives rather than false negatives
- **Evidence anchors:** Section 5.3.4 Table 4 shows adding both q2s and sq2s hard negatives boosts Recall@1 from 54.26 to 55.72

## Foundational Learning

- **Concept: Vision-Language Contrastive Learning (CLIP-style)**
  - **Why needed here:** UniSE-CLIP model relies on aligning screenshot embeddings with text embeddings in shared space
  - **Quick check question:** Given a batch of 4 screenshot-text pairs, how does InfoNCE loss penalize model if it matches Screenshot A with Text B?

- **Concept: MLLM Architecture (Qwen2-VL)**
  - **Why needed here:** UniSE-MLLM uses Large Language Model backbone to process visual tokens, essential for composed query capability
  - **Quick check question:** How does model handle [Task] and [Query] tokens differently in template for composed query vs simple text query?

- **Concept: Recall@k**
  - **Why needed here:** MVRB benchmark evaluates performance primarily using Recall@1, measuring if single top-ranked result is correct
  - **Quick check question:** If model retrieves 3 relevant items but ranks them at positions 2, 3, and 4, what is Recall@1?

## Architecture Onboarding

- **Component map:** Screenshots -> Smart resize (MLLM) or 224×224 (CLIP) -> Visual encoder -> Linear projection -> Shared embedding space -> Bidirectional contrastive loss

- **Critical path:**
  1. Data Prep: Convert PDFs/Webpages to Screenshots → Generate Caption/Query via LLM → Mine Hard Negatives
  2. Stage 1 (Pre-train): Train on (Screenshot, Caption) pairs using bidirectional loss
  3. Stage 2 (Fine-tune): Train on (Query, Screenshot) pairs with hard negatives

- **Design tradeoffs:**
  - UniSE-CLIP: Faster inference (428M params), lower VRAM, but struggles with complex composed queries (linear addition of embeddings is simplistic)
  - UniSE-MLLM: Higher recall (55.72%), better at reasoning over composed inputs, but significantly heavier (2.21B params + visual tokens)

- **Failure signatures:**
  - Low Res Blurry Retrieval: Model fails to read text in small screenshots; check smart resize logic (M parameter)
  - Catastrophic Forgetting: After Stage 2 fine-tuning, model forgets general visual features; verify learning rate is low (5×10^-6)
  - False Negative Collapse: Training loss stalls or oscillates; inspect hard negative mining pipeline for false positives

- **First 3 experiments:**
  1. Baseline Comparison: Run standard CLIP vs UniSE-CLIP on subset of MVRB "Chart Retrieval" to verify performance gap
  2. Ablation on Composition: Test UniSE-MLLM with text-only query vs composed query (image+text) on "Composed Screenshot Retrieval"
  3. Negative Mining Sensitivity: Train mini-version of UniSE with random negatives vs hard negatives to reproduce delta in Table 4

## Open Questions the Paper Calls Out

- **Open Question 1:** How does Vis-IR paradigm perform in multilingual contexts where text and visual layouts differ from English norms?
  - Basis in paper: Conclusion states authors aim to advance field by "incorporating multilingual annotations"
  - Why unresolved: Current VIRA dataset and MVRB benchmark focus primarily on English sources

- **Open Question 2:** Can advanced fusion mechanisms improve composed retrieval performance over simple linear embedding combination used in UniSE-CLIP?
  - Basis in paper: Paper notes for UniSE-CLIP, joint representation computed by "linear combining" embeddings
  - Why unresolved: Simple addition may fail to capture complex spatial or semantic interactions

- **Open Question 3:** How does retrieval accuracy degrade when scaling candidate corpus from thousands to millions of screenshots?
  - Basis in paper: MVRB benchmark limits retrieval corpus to "moderate number... (around 5,000)"
  - Why unresolved: Unclear if relative improvements hold in large-scale real-world search scenarios

## Limitations

- Hard negative mining pipeline robustness is uncertain, as false negatives could actively degrade retrieval performance
- Assumption that screenshot-based retrieval is universally superior to parsed-text methods is supported by ablation studies but may not hold for all document types
- Scalability to real-time or resource-constrained environments remains untested beyond reported inference times

## Confidence

- **High:** Superiority of UniSE-MLLM over existing methods on MVRB (55.72% average Recall@1, 7.6% improvement) is well-supported by experimental results
- **Medium:** Claim that Vis-IR outperforms traditional text-based retrieval is substantiated by qualitative analysis but could benefit from more direct comparisons
- **Low:** Generalizability of approach to domains not represented in VIRA (e.g., scientific diagrams, architectural plans) is untested

## Next Checks

1. **Hard Negative Mining Validation:** Audit sample of mined hard negatives to quantify false negative contamination and impact on model performance
2. **Domain Generalization Test:** Evaluate UniSE models on held-out domain (e.g., scientific papers) not present in VIRA to assess cross-domain robustness
3. **Resource-Constrained Inference:** Measure inference latency and memory usage of UniSE-CLIP vs UniSE-MLLM on mobile or edge device to validate practical deployment feasibility