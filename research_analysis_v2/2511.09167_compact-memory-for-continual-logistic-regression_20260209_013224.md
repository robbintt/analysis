---
ver: rpa2
title: Compact Memory for Continual Logistic Regression
arxiv_id: '2511.09167'
source_url: https://arxiv.org/abs/2511.09167
tags:
- memory
- learning
- task
- data
- regression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new method to build compact memory for
  continual logistic regression, based on a result by Khan and Swaroop [2021] showing
  the existence of optimal memory. The authors reformulate the search for optimal
  memory as Hessian-matching and propose a probabilistic PCA method to estimate them.
---

# Compact Memory for Continual Logistic Regression

## Quick Facts
- arXiv ID: 2511.09167
- Source URL: https://arxiv.org/abs/2511.09167
- Reference count: 40
- Primary result: New method for building compact memory for continual logistic regression based on Hessian-matching, achieving 60-74% accuracy on Split-ImageNet with only 0.3-2% memory of dataset size

## Executive Summary
This paper introduces a novel approach to building compact memory for continual logistic regression by reformulating the search for optimal memory as Hessian-matching. The method constructs memory vectors and weights such that the K-prior's Hessian matches the sum of past-task Hessians, enabling reconstruction of past-task loss curvature without storing raw data. Using a probabilistic PCA method with EM algorithm, the authors achieve significant improvements over Experience Replay - 60% accuracy with 0.3% memory and 74% with 2% memory on Split-ImageNet, closing the gap to batch accuracy of 77.6%.

## Method Summary
The method trains a linear model with K-prior regularization where the prior term $K_t(\theta; U_t, w_t)$ approximates the sum of past-task Hessians. After each task, memory vectors $U_t$ and weights $w_t$ are updated using a PPCA-EM algorithm that maximizes marginal likelihood of new features given current memory. For logistic regression, the Hessian-matching condition includes diagonal scaling by prediction derivatives $\hat{y}'(f) = \sigma(f)(1-\sigma(f))$. The method requires a frozen feature extractor and trains only the linear classifier layer, making it memory-efficient and suitable for large-scale applications.

## Key Results
- Achieves 60% accuracy on Split-ImageNet with only 0.3% memory of dataset size (vs 30% for Experience Replay)
- Reaches 74% accuracy with 2% memory, closing gap to batch accuracy of 77.6%
- Outperforms Experience Replay by 2-3x across multiple datasets (Split-CIFAR, TinyImageNet)
- Demonstrates effectiveness for both linear regression (exact Hessian-matching) and logistic regression (approximate matching with prediction derivatives)

## Why This Works (Mechanism)

### Mechanism 1: Exact Hessian-matching for linear regression
- Claim: Hessian-matching enables reconstruction of past-task loss curvature without storing raw data
- Mechanism: Constructs memory vectors $U_t$ and weights $w_t$ such that K-prior's Hessian $\nabla^2 K_t(\theta_t; U_t, w_t)$ matches sum of past-task Hessians
- Core assumption: Feature matrix rank $K^*_t$ is much smaller than total data size $|D_{1:t}|$
- Evidence: Theorem 1 proves exact equivalence for linear regression; Eq. 15 shows Hessian condition
- Break condition: Approximation degrades if feature dimension exceeds memory budget or data has full-rank structure

### Mechanism 2: PPCA-EM for stable memory estimation
- Claim: PPCA-EM algorithm provides numerically stable, iterative estimation of memory vectors
- Mechanism: EM algorithm iteratively updates $\tilde{U} \leftarrow S\tilde{U}(\epsilon I + M^{-1}\tilde{U}^T S\tilde{U})^{-1}$ implementing maximum-likelihood PPCA
- Core assumption: Noise parameter $\epsilon$ adequately captures approximation error; EM converges in few iterations
- Evidence: Alg. 1 provides implementation; Fig. 7a shows EM outperforms vanilla SVD on small memory
- Break condition: Poor initialization or insufficient EM iterations may fail to converge

### Mechanism 3: Logistic regression extension with prediction derivatives
- Claim: Weighting memory vectors by prediction derivative adapts Hessian-matching to non-quadratic loss
- Mechanism: Modifies T-matrix construction with diagonal scaling by $\lambda_1 = \hat{y}'(\Phi_{t+1}^T\theta_{t+1})$ and $\lambda_2 = \hat{y}'(U_t^T\theta_{t+1})$
- Core assumption: Local Hessian approximation at $\theta_{t+1}$ suffices; cross-class derivatives can be ignored
- Evidence: Eq. 21-22 define modified Hessian condition; multi-class extension uses trace of $\hat{y}'(f)$
- Break condition: Near-saturated predictions produce near-zero $\hat{y}'$, potentially underweighting important memory vectors

## Foundational Learning

- **Catastrophic forgetting in continual learning**: Why needed? The entire paper addresses this problem—new tasks interfering with old knowledge causes performance collapse on prior tasks. Quick check: Can you explain why storing only old parameters $\theta_t$ (weight regularization) fails to match replay performance?

- **Hessian of loss function**: Why needed? The core insight is matching Hessians rather than gradients; understanding why second-order information captures curvature essential for comprehending the mechanism. Quick check: Why is the Hessian of linear regression loss independent of $\theta$, but for logistic regression it depends on the prediction $\hat{y}$?

- **Probabilistic PCA and EM algorithm**: Why needed? The practical implementation uses PPCA-EM (Alg. 1) rather than direct SVD; understanding latent variable model $t_i = U_{t+1}W^{1/2}_{t+1}z_i + e_i$ is necessary to follow the derivation. Quick check: How does maximizing the marginal likelihood $\log p(T|U_{t+1}, w_{t+1})$ relate to the Hessian-matching condition?

## Architecture Onboarding

- **Component map**: Memory store $(U_t, w_t, \theta_t)$ -> Training loop (optimizer on $\bar{\ell}_{t+1} + K_t$) -> Memory update (PPCA-EM) -> Feature extractor (frozen backbone)

- **Critical path**: 1) Initialize $U_0, w_0$ (random or subset of first-task features); 2) For each task: (a) Train $\theta_{t+1}$ with K-prior regularizer; (b) Extract features $\Phi_{t+1}$; (c) Run EM to update $(U_{t+1}, w_{t+1})$; 3) Key hyperparameters: $\delta_t$ (weight regularization), $\epsilon$ (EM noise), $K_t$ (memory size)

- **Design tradeoffs**: Memory size vs. accuracy (0.3% → 60%, 2% → 74% on Split-ImageNet); EM iterations vs. compute (10 iterations recommended, ~3-4x overhead vs. replay at 2% memory); Linear vs. logistic variant (Alg. 2 adds derivative computation overhead)

- **Failure signatures**: Accuracy far below replay with same memory (check $\epsilon$ value, verify $\lambda$ computations); Numerical instability in EM (features may need normalization, check for near-zero $\hat{y}'$ values); No improvement over vanilla K-prior (memory vectors may not be updated)

- **First 3 experiments**: 1) Sanity check: Multi-output linear regression on Split-MNIST with $K_t \in \{10, 40, 160\}$; compare EM vs. SVD baseline; 2) Binary classification: USPS odd-vs-even with polynomial features; sweep memory sizes 1-20% of data; 3) Multi-class with pretrained features: Split-CIFAR-10 with ViT-B/32 features

## Open Questions the Paper Calls Out

- **Open Question 1**: How can this Hessian-matching approach be extended to adapt the feature extractor in deep neural networks? The authors explicitly state the algorithm is "currently limited to the use of only the last-layer... with a frozen feature extractor" but hope it opens a path for "continual deep learning."

- **Open Question 2**: Can the memory size be determined adaptively based on the intrinsic dimensionality of task data? The paper manually selects memory sizes (e.g., 1%, 2%), yet the theoretical motivation links optimal memory size directly to the rank of the feature matrix.

- **Open Question 3**: How robust is the EM-based Hessian matching to approximation error accumulation over very long task sequences? The authors relax the "optimal" memory requirement to a local Hessian matching approximation estimated via EM, but the paper tests relatively short sequences (5-20 tasks).

## Limitations

- Generalizability beyond linear/logistic regression setting - the Hessian-matching framework relies on linear Hessian properties that may not hold for highly non-linear models
- Assumes manageable feature rank - the PPCA-EM implementation assumes feature rank remains small, which may not hold for very high-dimensional or diverse feature spaces
- Requires frozen feature extractor - current method trains only the linear classifier layer, limiting applicability to scenarios where feature extractor cannot be frozen

## Confidence

- **High confidence**: Linear regression results and core Hessian-matching formulation - supported by exact proofs and strong empirical validation across multiple datasets
- **Medium confidence**: PPCA-EM algorithm implementation - numerically validated but lacks theoretical convergence guarantees for continual learning setting
- **Medium confidence**: Logistic regression extension - derivation appears sound but diagonal scaling by prediction derivatives is an approximation that needs more rigorous validation

## Next Checks

1. **Robustness to initialization**: Test whether method's performance degrades significantly when memory vectors are initialized randomly vs. using proposed subset selection from $\Phi_{t+1}$, and whether k-means clustering provides better initialization than random sampling.

2. **Cross-dataset transfer**: Evaluate whether memory vectors learned on one dataset (e.g., CIFAR-100) can effectively regularize training on a different but related dataset (e.g., TinyImageNet), testing hypothesis that Hessian-matching captures task-agnostic information.

3. **Non-linear feature test**: Replace frozen CLIP features with features from a non-linear feature extractor (e.g., fine-tuned ResNet) and measure performance degradation, directly testing whether linear/logistic assumption in K-prior formulation is a critical limitation.