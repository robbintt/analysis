---
ver: rpa2
title: Evaluating Large Language Models in Scientific Discovery
arxiv_id: '2512.15567'
source_url: https://arxiv.org/abs/2512.15567
tags:
- discovery
- scientific
- llms
- gpt-5
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces SDE, a scenario-grounded benchmark that evaluates
  LLMs on real-world scientific discovery tasks across biology, chemistry, materials,
  and physics. Unlike conventional science benchmarks that rely on decontextualized
  questions, SDE decomposes genuine research projects into modular scenarios, generating
  expert-vetted questions tied to specific discovery workflows.
---

# Evaluating Large Language Models in Scientific Discovery

## Quick Facts
- arXiv ID: 2512.15567
- Source URL: https://arxiv.org/abs/2512.15567
- Reference count: 40
- Key outcome: SDE benchmark reveals LLMs consistently underperform on real scientific discovery tasks compared to general science benchmarks, with diminishing returns from scaling and reasoning, and success depending more on guided exploration than precise domain knowledge.

## Executive Summary
This study introduces SDE (Scenario-DA3n), a novel benchmark that evaluates LLMs on authentic scientific discovery tasks across biology, chemistry, materials science, and physics. Unlike conventional science benchmarks that test decontextualized knowledge, SDE decomposes genuine research projects into modular scenarios and generates expert-vetted questions tied to specific discovery workflows. The framework assesses models at both question-level accuracy and project-level performance, where LLMs must propose hypotheses, design experiments, and interpret results. Results show a consistent performance gap versus general benchmarks, diminishing returns from scaling and reasoning, and shared failure modes across top-tier models.

## Method Summary
SDE evaluates LLMs through two complementary approaches: question-level assessment using a customized lm-evaluation-harness fork with 1,125 items across 43 scenarios, and project-level evaluation using sde-harness that implements evolutionary optimization loops where models act as hypothesis generators. The framework hosts question datasets at huggingface.co/deep-principle/datasets and employs computational oracles (GFN2-xTB, CHGNet, PySR, physics simulators) for project evaluation. Models are assessed on accuracy metrics for questions and task-specific discovery metrics (solve rate, AUC-top-k, energy minimization) for projects.

## Key Results
- LLMs show consistent performance gaps versus general science benchmarks like GPQA-Diamond across all four domains
- Reasoning variants (deepseek-R1, gpt-5-high) outperform non-reasoning counterparts but exhibit diminishing returns when scaling from medium to high reasoning effort
- Project-level results demonstrate that scenario performance does not always translate to discovery success, highlighting the importance of guided exploration and serendipity over precise knowledge
- Cross-model error correlations (Spearman's r > 0.8 in chemistry/physics) reveal shared systematic failure modes across top-tier models

## Why This Works (Mechanism)

### Mechanism 1
Scenario-grounded questions better proxy scientific discovery capability than decontextualized Q&A benchmarks. Questions are hierarchically organized (Domain → Project → Scenario → Question), where each question is explicitly tied to a modular research scenario representing recurring reasoning patterns in real scientific workflows. This creates tight coupling between assessment items and the iterative hypothesis-experiment-observation loop of actual research, rather than testing isolated facts. The assumption is that scientific discovery ability decomposes into reusable scenario-level competencies that can be assessed through targeted questions, and mastery of these scenarios correlates with project-level success.

### Mechanism 2
Test-time compute (reasoning) provides gains but exhibits diminishing returns on discovery tasks. Models with explicit reasoning capabilities leverage additional inference-time computation for multi-step derivation and evidence integration. However, the performance-compute relationship plateaus—increasing reasoning effort from medium to high yields statistically negligible improvements across all four domains. The assumption is that reasoning effort translates to better hypothesis exploration and error-correction within the model's existing knowledge boundaries, but cannot overcome fundamental knowledge gaps or distributional limitations in pre-training data.

### Mechanism 3
Models with weak scenario-level knowledge can still succeed at project-level discovery through guided exploration and serendipity. Project-level evaluation uses evolutionary optimization loops where LLMs propose hypotheses, receive fitness feedback from computational oracles, and iteratively refine. Success depends on the model's ability to navigate hypothesis spaces and discern optimization directions, not necessarily on precise knowledge of structure-property relationships. Models can leverage global structural priors rather than local search operators. The assumption is that scientific discovery is fundamentally about hypothesis-space navigation and evidence integration rather than perfect recall of domain knowledge.

## Foundational Learning

**Scientific Discovery Loop (Hypothesis-Experiment-Observation-Refinement)**
- Why needed: The entire SDE framework evaluates whether LLMs can participate in this iterative cycle, not just answer one-shot questions. Understanding this loop is essential for interpreting why scenario-level and project-level performance diverge.
- Quick check: Can you explain why answering a question about "NMR structure elucidation" requires different capabilities than iteratively proposing candidate structures, getting feedback, and refining?

**Evolutionary Optimization as Scientific Search**
- Why needed: All eight project-level evaluations use evolutionary frameworks where LLMs propose candidates (mutation/crossover), oracles evaluate fitness, and top candidates are retained. This replaces traditional search algorithms with LLM reasoning.
- Quick check: In the TMC optimization project, why might an LLM propose "good" candidates even if it cannot accurately predict the polarisability of a given structure?

**Distributional vs. Mechanistic Generalization**
- Why needed: The paper shows high cross-model error correlations (Spearman's r > 0.8 in chemistry/physics), suggesting models share "systematic weaknesses...inherited from similar pre-training data." Distinguishing between models knowing mechanisms vs. recognizing patterns is critical.
- Quick check: If gpt-5 and grok-4 both fail on the same difficult questions, what does this imply about their training data versus their architectures?

## Architecture Onboarding

**Component map:**
```
SDE Framework
├── Question-level evaluation (lm-evaluation-harness fork)
│   ├── Domain-specific YAML configs → HuggingFace datasets
│   ├── Evaluation protocols (exact match, Tanimoli similarity, tolerance bands)
│   └── Output: per-scenario accuracy scores
└── Project-level evaluation (sde-harness)
    ├── Research projects (8 total, spanning 4 domains)
    │   ├── Hypothesis space definition (e.g., 1.37M TMC space, symbolic equation grammar)
    │   ├── Computational oracles (GFN2-xTB, CHGNet, PySR, physics simulators)
    │   └── Selection/fitness functions
    ├── Evolutionary optimization loop
    │   ├── LLM as hypothesis generator (mutation, crossover, de novo proposals)
    │   ├── Oracle evaluation → fitness scores
    │   └── Experience buffer → next prompt conditioning
    └── Output: discovery metrics (convergence rate, SUN score, solve rate, etc.)
```

**Critical path:**
1. Understand the project-to-scenario mapping (Supplementary Table 6): each project draws on specific scenarios, enabling error-propagation analysis
2. Set up lm-evaluation-harness for question-level baseline; extend to custom domains by creating YAML configs and evaluation hooks
3. Implement sde-harness project evaluation: the critical step is designing the hypothesis space representation and oracle interface
4. Run ablation studies: compare reasoning vs. non-reasoning variants, vary initialization seeds, analyze cross-model error correlations

**Design tradeoffs:**
- Question generation: Semi-automated (from datasets like USPTO, ZINC) vs. manual expert curation—tradeoff between scale and authenticity
- Project evaluation cost: High compute cost limits evaluation to subset of models (only 4 models evaluated on all 8 projects)
- Evolutionary search strategy: Paper uses simple evolutionary approach; alternative agentic frameworks (MCTS, tree search) not explored
- Temperature constraints: GPT-5 family only accepts temperature=1.0, creating unfair comparison with models run at temperature=0.8

**Failure signatures:**
- High scenario score, low project success: Seen in retrosynthesis—models score well on scenario questions but fail to generate valid multi-step routes
- Sensitivity to initialization: claude-sonnet-4.5 shows high variance across random seeds in TMC optimization
- Context window overflow: Reasoning models (deepseek-R1) with 8000 token limit may overflow when generating complex crystal structures
- Format compliance failures: Newer models (gpt-5, gpt-5-chat) "struggled more to generate valid routes" in retrosynthesis due to output format violations

**First 3 experiments:**
1. Reproduce question-level baseline: Clone lm-evaluation-harness fork, load SDE datasets from HuggingFace, run evaluation on deepseek-V3.1 and deepseek-R1 across all 43 scenarios
2. Single-project deep dive: Implement TMC optimization project using sde-harness. Compare convergence curves across 5 random seeds for gpt-5 and deepseek-R1
3. Cross-model error analysis: Identify the 86 SDE-hard questions where all top models fail. Categorize failure modes and test whether ensemble voting or specialized prompting improves performance

## Open Questions the Paper Calls Out

**Open Question 1**
Can diversifying pre-training data distributions effectively mitigate the shared systematic failure modes observed across proprietary frontier models? Current top-tier models likely train on overlapping corpora, leading to high correlation on incorrect answers for difficult questions where data is sparse or biased. Training distinct model variants on curated, non-overlapping scientific corpora and measuring the reduction in error correlation on the SDE-hard subset would resolve this.

**Open Question 2**
What specific reinforcement learning reward structures are necessary to translate reasoning gains from mathematics into scientific hypothesis generation? Current reasoning models optimize for chain-of-thought logic leading to a single correct answer, whereas discovery requires evaluating intermediate hypotheses against imperfect evidence without a known solution. Fine-tuning a base model using rewards based on hypothesis validity and experimental design efficiency (rather than final accuracy) and measuring performance improvements on the SDE project-level harness would resolve this.

**Open Question 3**
How can fine-tuning effectively couple linguistic reasoning with physical validity checks to improve success rates in constraint-heavy tasks like retrosynthesis? LLMs currently lack inherent physical constraints and often generate chemically invalid proposals during project-level optimization loops. Comparing success rates of standard LLMs against models fine-tuned to invoke external validators during generation, specifically on the retrosynthesis and crystal design tasks, would resolve this.

## Limitations
- The framework relies on curated, semi-automated question generation that may not fully capture the open-ended nature of real scientific discovery
- Project-level evaluation is computationally expensive, limiting breadth of model comparison and creating potential reproducibility challenges
- The evolutionary optimization approach may not represent the most sophisticated exploration strategies available for scientific search
- Context window constraints and temperature settings create uneven evaluation conditions across models

## Confidence
**High Confidence:** Scenario-grounded questions provide more authentic assessment of scientific reasoning than decontextualized benchmarks; reasoning models show consistent but diminishing returns; models can succeed at project-level discovery despite weak scenario knowledge.
**Medium Confidence:** The performance gap between SDE and general benchmarks reflects genuine capability differences rather than evaluation artifacts; evolutionary optimization represents a valid model for scientific discovery workflows.
**Low Confidence:** Cross-model error correlation patterns definitively indicate shared training data weaknesses rather than architectural similarities; guided exploration is universally more important than knowledge for all types of scientific discovery tasks.

## Next Checks
1. Replicate the question-level evaluation using lm-evaluation-harness with at least three different reasoning models and three non-reasoning models to verify the observed performance gaps persist across different API configurations and temperature settings.

2. Implement an alternative project-level exploration strategy (e.g., Monte Carlo tree search or Bayesian optimization) alongside the evolutionary approach to test whether simple evolutionary search underestimates model capabilities.

3. Conduct ablation studies on the 86 SDE-hard questions to determine whether ensemble prompting, tool augmentation, or specialized domain prompting can close the performance gap, distinguishing between fundamental knowledge gaps and prompting limitations.