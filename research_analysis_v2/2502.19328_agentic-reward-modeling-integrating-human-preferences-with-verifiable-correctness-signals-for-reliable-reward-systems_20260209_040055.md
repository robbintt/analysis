---
ver: rpa2
title: 'Agentic Reward Modeling: Integrating Human Preferences with Verifiable Correctness
  Signals for Reliable Reward Systems'
arxiv_id: '2502.19328'
source_url: https://arxiv.org/abs/2502.19328
tags:
- reward
- arxiv
- rewardagent
- instruction
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Agentic Reward Modeling, a framework that
  integrates human preference rewards with verifiable correctness signals like factuality
  and instruction-following. The proposed REWARDAGENT combines a base reward model
  with verification agents that assess factual accuracy and constraint adherence,
  using a router to dynamically select appropriate verification agents and a judger
  to combine scores.
---

# Agentic Reward Modeling: Integrating Human Preferences with Verifiable Correctness Signals for Reliable Reward Systems

## Quick Facts
- arXiv ID: 2502.19328
- Source URL: https://arxiv.org/abs/2502.19328
- Reference count: 30
- Key outcome: A modular reward system combining human preference scores with factuality and instruction-following verification agents, achieving 72.5% micro-averaged accuracy on benchmarks versus 56.5% for the best baseline.

## Executive Summary
This paper introduces REWARDAGENT, a modular reward system that combines a base human preference reward model with specialized verification agents for factuality and instruction-following. The system uses a router to dynamically select appropriate verification agents and a judger to combine their scores with the base reward. REWARDAGENT significantly outperforms vanilla reward models on multiple benchmarks, with micro-averaged accuracy of 72.5% versus 56.5% for ArmoRM. The approach demonstrates that integrating verifiable correctness signals with human preferences produces more reliable rewards for LLM development.

## Method Summary
REWARDAGENT integrates a base reward model (ArmoRM) with two verification agents—factuality and instruction-following—via a router and judger architecture. The factuality agent uses pairwise difference detection and targeted search or parametric knowledge to score factual accuracy. The instruction-following agent parses hard constraints and verifies them through generated Python code execution. A router LLM determines which agents to invoke based on instruction analysis, and a judger combines all scores via weighted summation. The system is evaluated on RM-Bench, JudgeBench, and IFBench benchmarks, and when used to annotate preference pairs for DPO training, produces LLMs that outperform those trained on ArmoRM-annotated data.

## Key Results
- Micro-averaged accuracy of 72.5% on RM-Bench, JudgeBench, and IFBench versus 56.5% for ArmoRM
- Best-of-n search accuracy improvements of 2.3-4.7 percentage points over base reward model on TriviaQA, IFEval, and CELLO
- DPO-trained LLMs on REWARDAGENT-annotated preference pairs outperform those trained on ArmoRM annotations across MMLU, MMLU-Pro, TriviaQA, TruthfulQA, IFEval, and CELLO benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Integrating verifiable correctness signals with human preference scores improves reward reliability.
- Mechanism: A modular system where a base reward model captures human preferences (style, tone), while specialized verification agents explicitly check for factuality and constraint adherence. A judger combines these signals via weighted summation to produce a final reward, allowing objective correctness to override subjective preference biases.
- Core assumption: Human preference reward models systematically undervalue verifiable correctness due to biases like length preference; these biases are correctable through orthogonal verification signals.
- Evidence anchors:
  - [abstract] "...combines human preference rewards with two verifiable signals: factuality and instruction following, to provide more reliable rewards."
  - [section 1] "As illustrated in Figure 1, existing RMs may prefer the response A due to its language style and longer length (Singhal et al., 2023), overlooking factual errors and failure to follow instructions."
  - [corpus] Related work (e.g., *Self-Aligned Reward*, *DuaShepherd*) similarly explores hybrid or multi-signal reward systems, supporting the general direction.
- Break condition: If human preference rewards are already strongly correlated with verifiable correctness, the added complexity of verification agents may not yield significant gains and could introduce noise or inference latency without benefit.

### Mechanism 2
- Claim: A pairwise, difference-focused factuality verification process is more efficient than full atomic fact-checking.
- Mechanism: Instead of verifying every claim in a response, the agent first identifies key factual differences between two candidate responses, generates targeted search queries or uses parametric knowledge only for those disputed points, and then scores based on the evidence retrieved.
- Core assumption: The core factual disagreements between candidate responses are sparse and can be reliably identified by an LLM; resolving these specific points is sufficient to determine relative factual accuracy.
- Evidence anchors:
  - [section 3.3] "...we propose a pairwise factuality verification agent for efficiently evaluating the factual correctness of response pairs... intuitively, pairwise scoring based on only the differences between two responses can effectively reduce search engine queries and time costs."
  - [corpus] Related work on verifiable rewards (e.g., *Simultaneous Multi-objective Alignment*, *Reward Modeling from Natural Language Human Feedback*) often assumes access to ground truth, but this paper's method for *discovering* the ground truth via difference-focused search is a specific instantiation.
- Break condition: If factual errors are widespread and not clustered around a few key differences, or if the difference proposal step misses critical disagreements, the final score will be inaccurate.

### Mechanism 3
- Claim: A dynamic, code-based verification process improves evaluation of hard instruction constraints.
- Mechanism: An agent parses hard constraints from an instruction, generates Python code to check them (e.g., word count, keyword inclusion), and executes this code to get a binary pass/fail score. This process is self-refining, as execution errors trigger code correction.
- Core assumption: Hard constraints can be reliably translated into executable code by an LLM, and code execution provides a more objective and accurate signal than a direct LLM judgment.
- Evidence anchors:
  - [section 3.3] "For instruction-following, our verification agent focuses on hard constraints... which can be efficiently verified using external tools, such as Python code scripts."
  - [corpus] Related work like *Rubrics as Rewards* also uses structured criteria for evaluation, providing high-level support for this approach.
- Break condition: If constraints are ambiguous or cannot be expressed as executable code (e.g., nuanced "style" constraints), this mechanism fails or requires fallback to a less reliable LLM scorer.

## Foundational Learning

- Concept: **RLHF (Reinforcement Learning from Human Feedback) and Reward Modeling**
  - Why needed here: This is the core problem being addressed. The paper assumes knowledge of the standard RLHF pipeline and the role of a reward model (RM) as a proxy for human preference.
  - Quick check question: What is the primary function of a reward model in the LLM training pipeline, and what is a common data format used to train it? (Answer: To score responses; preference pairs (chosen, rejected) are a common format).

- Concept: **Verifiable Rewards / Rule-Based Rewards**
  - Why needed here: The paper's central innovation is augmenting preference-based rewards with "verifiable correctness signals." Understanding this distinction is crucial.
  - Quick check question: How does a "verifiable reward" differ from a standard reward model output? Give an example. (Answer: A verifiable reward is derived from an objective outcome, like code execution success or a fact check, whereas a standard RM output is a learned scalar from human preference data).

- Concept: **Agentic Workflow / Tool-Augmented LLMs**
  - Why needed here: The REWARDAGENT system is not a single model but an *agent* that orchestrates multiple LLM calls and external tools (search, code execution).
  - Quick check question: What are the key components of the REWARDAGENT's architecture beyond the base reward model? (Answer: A Router, Verification Agents (for factuality and instruction-following), and a Judger).

## Architecture Onboarding

- Component map: Input instruction + response → Router (LLM) → Base Reward Model (ArmoRM) → Factuality Agent (difference → query → evidence → verification) + Instruction-Following Agent (constraint → code → execution) → Judger (weighted sum) → Final score
- Critical path: The **Factuality Agent** is the most complex and latency-critical path due to its potential dependency on external search. The **Instruction-Following Agent** is the most brittle, as it relies on generated code executing without error.
- Design tradeoffs:
  *   **Accuracy vs. Latency:** Invoking more agents improves accuracy but increases inference time and cost. The Router is designed to mitigate this.
  *   **Complexity vs. Interpretability:** The system is more complex than a single RM, but each agent's output (e.g., retrieved evidence, generated code) makes the reward more interpretable.
  *   **Parametric Knowledge vs. Search:** The factuality agent can use the LLM's internal knowledge or an external search engine. Search is slower and noisier; parametric knowledge can be outdated.
- Failure signatures:
  *   **Router Errors:** Incorrectly invoking or failing to invoke a needed agent (e.g., missing a factuality check on a knowledge-heavy query).
  *   **Factuality Agent Errors:** Retrieving noisy or irrelevant evidence from search, or the LLM failing to correctly identify key differences between responses.
  *   **Instruction-Following Agent Errors:** Generated code is syntactically invalid and cannot be refined, or the code's logic incorrectly assesses a constraint.
- First 3 experiments:
  1.  **Ablation Study:** Run REWARDAGENT on a held-out validation set with each agent removed one by one (e.g., "- factuality verifier", "- if verifier") to confirm each component's contribution as shown in Table 2.
  2.  **Router Performance Analysis:** Measure the precision and recall of the Router in selecting the correct agents on a diverse set of instructions. Compare the "full system" performance against an "oracle router" that always makes perfect agent selections.
  3.  **Factuality Agent Latency & Accuracy Trade-off:** Compare the accuracy and average inference time of the factuality agent when using parametric knowledge versus an external search engine on a subset of TriviaQA, as hinted by the "w/ search engine" results in Table 1.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a learned gating network dynamically adjust the weighting between the base reward model and verification agents (λ and wi) more effectively than fixed weights?
- Basis in paper: [explicit] Section 3.4 states "the judger can dynamically adjust λ and wi based on the instruction like gating network, we leave it as future work."
- Why unresolved: The current implementation uses fixed weights of 1.0 for all components, but the oracle experiments show significant room for improvement in agent selection.
- What evidence would resolve it: Experiments comparing fixed-weight judger against a trained gating network on RM-Bench, JudgeBench, and IFBench, measuring both accuracy and calibration.

### Open Question 2
- Question: How can retrieval-augmented factuality verification agents be designed to mitigate the noise from external search engines that currently degrades performance?
- Basis in paper: [explicit] Section 4.2 notes "Using a search engine as an external knowledge source for factuality slightly reduces performance... We leave the detailed analysis and design of retrieval-augmented agents for future work."
- Why unresolved: The paper observes that retrieved information contains noise that hurts factuality scoring, but does not investigate filtering or confidence-based retrieval strategies.
- What evidence would resolve it: Ablation studies comparing different retrieval strategies (e.g., multi-source verification, relevance filtering, confidence thresholding) on JudgeBench knowledge subset.

### Open Question 3
- Question: What additional verifiable correctness signals beyond factuality and instruction-following can be integrated to further improve reward reliability?
- Basis in paper: [explicit] Limitations section states "We only implement verification agents for factuality and instruction-following... We encourage the community to explore more verifiable correctness signals."
- Why unresolved: The agentic framework is designed to be modular, but only two verification dimensions are tested; other dimensions (e.g., logical consistency, safety, domain-specific constraints) remain unexplored.
- What evidence would resolve it: Implementation and evaluation of additional verification agents (e.g., code correctness, mathematical reasoning verification) on relevant benchmarks, demonstrating incremental improvements over the base REWARDAGENT.

## Limitations

- **Long-horizon consistency**: The robustness of the multi-agent system to instruction complexity, domain shifts, and adversarial inputs remains untested.
- **Generalizability of verification agents**: The factuality and instruction-following agents are tuned for specific benchmarks (TriviaQA, CELLO, etc.). Their performance on unseen domains or tasks with overlapping constraint types is unclear.
- **Cost-efficiency trade-off**: No explicit analysis of inference time or computational cost per reward call, making it hard to assess real-world deployment viability.

## Confidence

- **High confidence**: Base reward model integration (ArmoRM) and modular agent design; benchmark accuracy gains (72.5% vs 56.5%).
- **Medium confidence**: DPO training outcomes; best-of-n search improvements; generalizability across diverse LLM tasks.
- **Low confidence**: Long-term stability; performance under adversarial conditions; scalability to tasks requiring fine-grained, non-verifiable preferences.

## Next Checks

1. **Router Robustness Test**: Manually annotate a diverse instruction set with the correct agent selections; compare against router output to measure precision/recall and identify failure modes.
2. **Cross-Domain Factuality Stress Test**: Evaluate factuality agent on out-of-domain knowledge queries (e.g., legal, medical) where parametric knowledge is unreliable; compare parametric vs. search-augmented accuracy and latency.
3. **Multi-Constraint Instruction Challenge**: Construct synthetic instructions with mixed verifiable and non-verifiable constraints (e.g., "Summarize accurately AND in under 50 words AND with a humorous tone"); measure how well the router and agents handle overlapping or conflicting signals.