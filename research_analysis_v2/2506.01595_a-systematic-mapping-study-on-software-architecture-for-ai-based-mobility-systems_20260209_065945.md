---
ver: rpa2
title: A Systematic Mapping Study on Software Architecture for AI-based Mobility Systems
arxiv_id: '2506.01595'
source_url: https://arxiv.org/abs/2506.01595
tags:
- systems
- software
- research
- studies
- architecture
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study presents a systematic mapping analysis of software architectures
  for AI-based mobility systems. From 1,639 initial papers, 38 relevant studies were
  identified and analyzed to assess current architectural approaches, safety patterns,
  and implementation maturity.
---

# A Systematic Mapping Study on Software Architecture for AI-based Mobility Systems

## Quick Facts
- arXiv ID: 2506.01595
- Source URL: https://arxiv.org/abs/2506.01595
- Reference count: 25
- Primary result: From 1,639 initial papers, 38 relevant studies were identified analyzing software architectures for AI-based mobility systems

## Executive Summary
This systematic mapping study examines software architecture approaches for AI-based mobility systems across automotive, aerospace, and railway domains. The analysis reveals that layered and microservice architectures dominate current implementations, with redundancy and monitoring patterns being the most common safety mechanisms. However, the research highlights significant gaps in validation approaches, with most studies relying on simulations rather than industrial case studies, and minimal adoption of formal architectural notations despite safety-critical requirements.

## Method Summary
The study followed Kitchenham's systematic review guidelines, conducting searches across three digital libraries (ACM DL, IEEE Xplore, DBLP) using specified search strings combining "Software Architecture" with "Safety" and AI-related terms. A three-round voting process (title → abstract → full-text) with Cohen's kappa inter-rater agreement was applied to select relevant papers. Data extraction used a defined classification scheme covering architecture types, safety patterns, frameworks, notations, validation methods, application areas, and AI types.

## Key Results
- Layered and microservice architectures dominate, with layered architecture appearing in 8 studies and microservice in 5 studies
- Redundancy and monitoring patterns are the most common safety mechanisms, appearing 10 and 4 times respectively
- Only 10% of studies used formal architectural notations while over 50% relied on informal representations
- Validation methods were limited, with most studies relying on simulations or experiments rather than industrial case studies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Redundancy patterns mitigate AI output uncertainty in safety-critical mobility systems.
- Mechanism: Multiple AI components perform the same function with divergent outputs triggering fallback or voting logic to prevent single-point-of-failure decisions.
- Core assumption: Redundant components fail independently and diversity in implementation reduces correlated failures.
- Evidence anchors: Redundancy (10 occurrences) as most frequent safety pattern in Figure 3; described as ensuring output accuracy.
- Break condition: If redundant AI components share training data, model architecture, or runtime dependencies, correlated failures may defeat safety benefit.

### Mechanism 2
- Claim: Layered and microservice architectures isolate AI components for independent development, scaling, and fault containment.
- Mechanism: AI/ML functions are encapsulated as separate services or layers, enabling distributed development and limiting fault propagation.
- Core assumption: Service boundaries are well-defined and inter-service communication does not introduce unacceptable latency or single points of failure.
- Evidence anchors: Microservices architecture widespread (5 studies); layered architecture (8 studies) lead Figure 2; AI components integrated as separate independent functions.
- Break condition: If service orchestration, networking, or shared infrastructure becomes a bottleneck or failure point, architectural isolation degrades into systemic fragility.

### Mechanism 3
- Claim: Runtime monitoring enables real-time safety responses when AI behavior deviates from expected operation.
- Mechanism: Continuous observation of AI component outputs triggers safety mechanisms when anomalies detected.
- Core assumption: Monitoring logic itself is reliable, detectable anomaly patterns are known, and response latency is within safety-critical time bounds.
- Evidence anchors: Monitoring (4 occurrences) as second most common safety pattern; continuous monitoring to trigger safety mechanism in case of malfunction.
- Break condition: If monitoring fails to detect novel failure modes, unsafe AI outputs may pass undetected.

## Foundational Learning

- Concept: **Safety patterns for AI uncertainty (redundancy, monitoring, watchdog, TMR)**
  - Why needed here: AI components introduce inherent uncertainty; safety patterns provide structured mitigation approaches rather than ad-hoc fixes.
  - Quick check question: Can you name two safety patterns and explain when each is appropriate for handling AI output uncertainty?

- Concept: **Architectural notation spectrum (informal → semi-formal → formal)**
  - Why needed here: Over 50% of studies used informal notations; understanding this spectrum helps select appropriate rigor for stakeholder communication and safety analysis.
  - Quick check question: What is the difference between UML (semi-formal) and AADL (formal) in terms of analytical capabilities for safety-critical systems?

- Concept: **Validation maturity levels (solution → validation → evaluation)**
  - Why needed here: Most studies remain at solution/validation level with simulation; industrial evaluation is rare. Recognizing maturity gaps prevents overconfident adoption.
  - Quick check question: Why is simulation-based validation insufficient for safety-critical mobility systems intended for real-world deployment?

## Architecture Onboarding

- Component map:
  - Perception layer: AI/ML components (CNNs, DNNs) processing sensor data (Lidar, cameras)
  - Decision layer: Environment modeling, behavior selection, optimization
  - Control layer: Actuation commands, safety monitoring
  - Safety infrastructure: Redundancy managers, runtime monitors, watchdog timers
  - Communication backbone: Service interfaces (microservices) or layer interfaces (layered architecture)

- Critical path: Sensor input → AI perception component → Redundancy check/voting → Environment model → Decision logic → Safety monitor validation → Actuation output

- Design tradeoffs:
  - Microservices: Better isolation and independent scaling vs. increased latency and orchestration complexity
  - Layered architecture: Centralized data management vs. tighter coupling and harder fault isolation
  - Formal notation (AADL, MDP): Enables rigorous analysis vs. steeper learning curve and limited tooling adoption
  - Redundancy type: Heterogeneous (diverse implementations reduce correlated failures) vs. homogeneous (simpler, but shared failure modes)

- Failure signatures:
  - Silent AI output errors passing through without monitoring trigger
  - Service mesh latency spikes causing timeout-based safety fallbacks
  - Redundant components agreeing on wrong output due to shared training distribution gaps
  - Informal architecture specifications leading to integration mismatches across teams

- First 3 experiments:
  1. **Fault injection test**: Inject known AI output errors into perception layer; verify monitoring and redundancy mechanisms detect and respond within safety-critical time bounds.
  2. **Service isolation validation**: Deploy microservice-based AI component; measure inter-service latency and verify failover behavior when individual services are terminated.
  3. **Notation precision audit**: Take existing informal architecture diagrams; translate critical safety flows into semi-formal (UML) or formal (AADL) notation; identify ambiguities or missing error paths exposed by formalization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the barriers to adopting formal architectural notations (e.g., AADL, UML) in AI-based mobility systems, and would standardized modeling improve safety assurance outcomes?
- Basis in paper: The authors state: "We advocate for adopting standardised modelling languages to better articulate architectures and facilitate clearer communication between researchers and implementers." They found only ~10% used formal/semi-formal notations, while over half relied on informal representations.
- Why unresolved: The paper identifies the gap but does not investigate the root causes (e.g., tooling limitations, expertise requirements, domain culture) or empirically demonstrate safety benefits from formal notation adoption.
- What evidence would resolve it: Comparative studies measuring safety defect rates, communication errors, or certification success between projects using formal vs. informal notations in AI-based mobility systems.

### Open Question 2
- Question: Why are reinforcement learning (RL) and end-to-end learning architectures underutilized in safety-critical mobility systems despite their potential advantages?
- Basis in paper: The authors explicitly ask: "Why there are significantly fewer studies on reinforcement learning (2), CNNs (3), or DNNs (8)?" and state: "This discrepancy suggests a potential area for further research to explore why these technologies are underrepresented."
- Why unresolved: The paper documents the underrepresentation but does not identify whether barriers are technical (safety certification difficulty), organizational, or research-focus related.
- What evidence would resolve it: Surveys of practitioners and researchers on perceived barriers, or empirical case studies documenting safety certification challenges for RL-based mobility architectures.

### Open Question 3
- Question: How can proposed architectural solutions for AI-based mobility systems be validated in real-world industrial settings rather than simulations?
- Basis in paper: The authors conclude: "We, therefore, call for an industry-oriented evaluation of the approaches and their implementation in real systems" and note "a notable deficiency in evaluations conducted on real, industry-specific systems" with over half of approaches lacking validation.
- Why unresolved: The paper documents the gap (only 1 DNN application assessed in industrial case study) but does not propose frameworks, methodologies, or partnerships to enable industrial-scale validation.
- What evidence would resolve it: Development of industry-academia collaboration frameworks, safety-certification-compatible validation protocols, or published industrial case studies demonstrating architectural safety assurance at scale.

## Limitations
- The study's conclusions depend on subjective relevance judgments during the paper selection process
- Absence of explicit paper identifiers prevents exact replication of the selection process
- Potential publication bias favoring certain architectural approaches over others
- The accuracy of maturity categorization across diverse mobility domains remains uncertain

## Confidence
- **High confidence**: Layered and microservice architectures dominate (supported by frequency analysis in Figure 2)
- **Medium confidence**: Only 10% use formal notations (relies on self-reported data that may conflate notation types)
- **Medium confidence**: Safety pattern prevalence (redundancy and monitoring being most common) supported by Figure 3 but effectiveness remains unvalidated

## Next Checks
1. Conduct fault injection experiments to verify that identified safety patterns (redundancy, monitoring) effectively detect and mitigate AI output errors within safety-critical time bounds.
2. Replicate the systematic mapping with expanded search criteria to capture more industrial case studies and validate the claimed maturity gaps.
3. Perform an independent audit of the selected papers' architectural notations, translating informal diagrams into formal representations to quantify the precision gap claimed in the study.