---
ver: rpa2
title: Studying Various Activation Functions and Non-IID Data for Machine Learning
  Model Robustness
arxiv_id: '2512.04264'
source_url: https://arxiv.org/abs/2512.04264
tags:
- data
- accuracy
- adversarial
- robust
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses improving machine learning (ML) model robustness
  using ten activation functions and non-IID data in both centralized and federated
  learning environments. The core method involves an advanced adversarial training
  (AT) approach that incorporates model architecture changes, soft labeling, data
  augmentation, and varying learning rates.
---

# Studying Various Activation Functions and Non-IID Data for Machine Learning Model Robustness

## Quick Facts
- **arXiv ID:** 2512.04264
- **Source URL:** https://arxiv.org/abs/2512.04264
- **Reference count:** 40
- **Primary result:** Advanced adversarial training with Gaussian noise injection and soft labeling achieves 77.08% natural and 67.96% robust accuracy on CIFAR-10 in centralized settings; data sharing in federated learning improves non-IID robustness from ~24% to ~54%.

## Executive Summary
This study investigates how activation function choice and data distribution characteristics impact machine learning model robustness against adversarial attacks. The authors propose an advanced adversarial training framework that incorporates model architecture modifications, Gaussian noise injection, soft labeling, and data augmentation. Experiments demonstrate that ReLU generally performs best among eleven tested activation functions in fixed learning rate settings, while smooth functions like TELU and CeLU show advantages under varying learning rates. For federated learning with non-IID data, introducing a globally shared dataset significantly improves robustness, with 40% data sharing achieving 54.79% robust accuracy compared to ~24% without sharing.

## Method Summary
The method employs modified ResNet-18 architecture (3×3 initial convolution, no downsampling) with adversarial training that generates PGD examples and Gaussian noise samples. Soft labels with smoothing parameter α_SL=0.05 are applied during training. For centralized learning, SGD optimization (momentum=0.9, weight decay=0.0002) runs for 200 epochs with learning rate 0.001. In federated learning, FedAvg aggregation is used with non-IID data mitigated through sharing a globally uniform dataset subset across clients. Test-time robustness evaluation includes injecting Gaussian noise into images before attack generation to disrupt perturbation calculations.

## Key Results
- ReLU achieves the best balance of natural and robust accuracy among eleven tested activation functions in fixed learning rate settings
- Smooth activation functions (TELU, CeLU) outperform ReLU when using varying learning rates, though at the cost of lower natural accuracy
- Data sharing in federated learning with non-IID data dramatically improves robustness, with 40% sharing achieving 54.79% robust accuracy versus ~24% without sharing
- The proposed method achieves 77.08% natural accuracy and 67.96% robust accuracy on CIFAR-10 against FGSM attacks in centralized settings

## Why This Works (Mechanism)

### Mechanism 1
Gaussian noise injection during adversarial training disrupts carefully calculated perturbations, preventing overfitting to specific attack patterns while preserving semantic content. Soft labels regularize the model by preventing over-confidence and smoothing decision boundaries. Core assumption: noise magnitude is small enough to preserve image features but sufficient to break attack approximations. Evidence shows test-time noise injection improves robust accuracy from ~24% to ~54% in non-IID settings. Break condition: noise magnitude exceeding perturbation boundaries destroys natural accuracy.

### Mechanism 2
ReLU performs optimally in fixed learning rate settings due to its non-smooth nature maintaining gradient flow during min-max optimization. However, smooth functions like TELU better handle varying learning rates by maintaining consistent gradients through longer training iterations. Core assumption: optimization landscape changes significantly with learning rate schedule requiring different activation characteristics. Evidence shows TELU balances robust and natural accuracy better than ReLU specifically under varying learning rates. Break condition: insufficient training duration for smooth functions to converge or gradient explosion with strong adversarial examples.

### Mechanism 3
Data sharing in federated learning mitigates non-IID-induced weight divergence by ensuring all clients see representative data distributions. Creating a globally shared dataset anchors local models to a common optimum. Core assumption: small privacy compromise (40% data sharing) is acceptable for performance recovery. Evidence demonstrates robust accuracy improvement from ~24% to ~54% with 40% sharing. Break condition: strict privacy constraints prohibiting any data sharing.

## Foundational Learning

### Concept: Adversarial Training (AT) as a Min-Max Problem
**Why needed:** The paper defines robustness through minimizing loss against maximally perturbed inputs, requiring understanding of the min-max duality for gradient flow during optimization.
**Quick check:** How does the "inner maximization" step (generating attacks) differ from the "outer minimization" (training the model)?

### Concept: Non-IID Data in Federated Learning
**Why needed:** The study's FL contribution solves performance drops caused by heterogeneous data distributions across clients.
**Quick check:** Why does training on local, class-skewed data prevent the global model from converging to a robust state?

### Concept: Smooth vs. Non-Smooth Activation Functions
**Why needed:** The paper contrasts ReLU (non-smooth) with TELU/CeLU (smooth), where choice dictates gradient stability during aggressive AT optimization.
**Quick check:** How does the "dying ReLU" problem specifically hinder gradient generation during min-max training?

## Architecture Onboarding

### Component map:
Modified ResNet-18 -> PGD Attack Generator -> Gaussian Noise Injector -> Soft Label Application -> SGD Training -> FedAvg Aggregation (FL) -> Global Shared Dataset (FL)

### Critical path:
Initialize ResNet → Generate PGD examples on Client → Inject Gaussian noise + Apply Soft Labels → Local Train → Aggregate FedAvg → Test with Noise Injection

### Design tradeoffs:
Robustness vs. Natural Accuracy: High robustness often lowers natural accuracy (e.g., RReLU); ReLU offers best balance in fixed settings. Privacy vs. Utility: Data sharing improves robustness but requires exposing fraction of local data to global pool.

### Failure signatures:
Natural Accuracy Collapse (<30%): Excessive PGD iterations or noise overwhelming signal. Federated Divergence (Robust Acc <15%): Non-IID skew without sufficient data sharing. Dead Gradients: ReLU with poor learning rate schedule causing sparse gradients.

### First 3 experiments:
1. Centralized Baseline: Train modified ResNet-18 with PGD-based AT (ReLU, Fixed LR) to verify 67.96% robust accuracy baseline.
2. Activation Function Sweep: Replace ReLU with TELU and CeLU under varying learning rates to reproduce Table 4 trade-off improvements.
3. Non-IID Data Sharing Ablation: Setup 5-client FL with 2-class non-IID data. Test 0% vs 40% sharing to measure robust accuracy recovery.

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** Does the proposed approach scale to large-scale datasets like ImageNet and generalize beyond computer vision?
**Basis:** Section 6 explicitly states plans to extend experiments to ImageNet and explore applications beyond computer vision.
**Why unresolved:** Current study confined to CIFAR-10; authors note effectiveness may vary with data complexity and modality.
**What evidence would resolve it:** Experimental results on ImageNet or non-vision datasets showing similar robust accuracy improvements.

### Open Question 2
**Question:** Can the trade-off between natural accuracy and robust accuracy be optimized for smooth activation functions like Softplus and CeLU?
**Basis:** Section 5.5.2 notes smooth functions improved robustness under varying learning rates but often lowered natural accuracy.
**Why unresolved:** Experiments revealed functions like Softplus/CeLU enhanced robustness but suffered "worse trade-off" compared to ReLU.
**What evidence would resolve it:** Training configuration allowing smooth functions to match/exceed ReLU in both metrics simultaneously.

### Open Question 3
**Question:** Can robustness in non-IID federated learning be achieved without compromising strict data privacy?
**Basis:** Primary solution involves "data sharing" strategy requiring clients to receive globally shared dataset subset.
**Why unresolved:** While effective, sharing raw data samples contradicts fundamental federated learning constraint of data locality.
**What evidence would resolve it:** Privacy-preserving mechanism (differential privacy or synthetic data) achieving comparable robust accuracy to 40% sharing baseline.

## Limitations
- Results heavily dependent on specific architectural modifications and parameter choices that may not generalize
- Data sharing mechanism assumes partial compromise of data privacy, limiting practical applicability
- Activation function superiority claims primarily based on CIFAR-10 and may not transfer to other domains
- Study focuses on white-box attacks without extensive evaluation against adaptive or real-world attack scenarios

## Confidence

**High Confidence:** ReLU activation effectiveness in fixed learning rate settings (supported by extensive ablation and literature consistency)

**Medium Confidence:** Smooth activation functions superiority with varying learning rates (fewer experimental variations, though mechanism plausible)

**Medium Confidence:** Data sharing mechanism for non-IID federated learning robustness (effective in this setup but privacy implications limit applicability)

**Medium Confidence:** Gaussian noise injection for disrupting adversarial perturbations (mechanistically sound but requires precise hyperparameter tuning)

## Next Checks

1. Reproduce the modified ResNet-18 architecture with 3×3 initial kernel and no down-sampling, then verify reported centralized baseline (77.08% natural, 67.96% robust accuracy) against FGSM attacks with injected test-time noise.

2. Conduct activation function ablation under varying learning rates by replacing ReLU with TELU and CeLU in AT pipeline, measuring both natural and robust accuracy across different LR schedules to confirm trade-off patterns.

3. Validate data sharing mechanism in non-IID federated learning by implementing 5-client setup with class-imbalanced data distributions, testing with 0%, 20%, 40%, and 60% shared data to quantify robustness improvement curve and identify optimal sharing ratio.