---
ver: rpa2
title: Heuristic-Informed Mixture of Experts for Link Prediction in Multilayer Networks
arxiv_id: '2501.17557'
source_url: https://arxiv.org/abs/2501.17557
tags:
- experts
- link
- networks
- moe-ml-lp
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MoE-ML-LP, the first Mixture-of-Experts framework
  designed specifically for multilayer link prediction. The method leverages multilayer-enhanced
  heuristics to dynamically combine diverse expert predictions, addressing the challenge
  of varying structural patterns across network layers.
---

# Heuristic-Informed Mixture of Experts for Link Prediction in Multilayer Networks

## Quick Facts
- arXiv ID: 2501.17557
- Source URL: https://arxiv.org/abs/2501.17557
- Reference count: 40
- This paper introduces MoE-ML-LP, the first Mixture-of-Experts framework designed specifically for multilayer link prediction.

## Executive Summary
This paper introduces MoE-ML-LP, the first Mixture-of-Experts framework designed specifically for multilayer link prediction. The method leverages multilayer-enhanced heuristics to dynamically combine diverse expert predictions, addressing the challenge of varying structural patterns across network layers. Extensive experiments on real-world and synthetic networks show MoE-ML-LP significantly outperforms baselines and competing methods, achieving average improvements of +60% in MRR, +82% in Hits@1, +55% in Hits@5, and +41% in Hits@10. The framework's modular architecture allows seamless integration of new experts without retraining, promoting scalability and efficiency.

## Method Summary
MoE-ML-LP uses a gating network that takes multilayer-enhanced heuristic scores as input to dynamically select and combine predictions from four frozen expert models (BPHGNN, GATNE, HDMI, MAGMA). The heuristics (mAA, mCN, mJC, mPPR) are computed using a cross-layer aggregation formula with α=0.5. The gating module is a 2-layer MLP that outputs expert weights, which are then used to compute a weighted sum of the frozen expert predictions. The framework is trained with binary cross-entropy loss while keeping experts frozen, preserving their algorithmic diversity and enabling modular extension.

## Key Results
- MoE-ML-LP achieves average improvements of +60% in MRR, +82% in Hits@1, +55% in Hits@5, and +41% in Hits@10 compared to baselines
- The method significantly outperforms both heuristic-based and embedding-based competitors across all six real-world multilayer networks tested
- Sensitivity analysis confirms that diverse heuristics and multiple experts are crucial for optimal performance

## Why This Works (Mechanism)

### Mechanism 1: Heuristic-Informed Routing (The Gater)
The framework improves performance by using cheap, structural heuristics as a routing signal to select the best expert for a specific link. A gating network $G(\cdot)$ takes a vector of multilayer heuristic scores as input and outputs a probability distribution over experts. The final prediction is a weighted sum of expert outputs. This heuristic-informed activation strategy allows MoE-ML-LP to dynamically adapt to the unique features of each input triple.

### Mechanism 2: Frozen Expert Preservation
Freezing the weights of expert models during MoE training preserves their algorithmic diversity and prevents convergence toward a mean solution. This strategy not only enhances efficiency but also promotes modularity, avoiding the risk of "flattening" effect where distinct experts converge toward identical behaviors.

### Mechanism 3: Cross-Layer Heuristic Aggregation
The framework adapts single-layer heuristics to multilayer contexts by linearly combining the score from the target layer with the average score from all other layers. This allows for a flexible balance between the information from the target layer and the broader multilayer structure, capturing dependencies between layers.

## Foundational Learning

- **Concept: Link Prediction Heuristics (CN, AA, PPR)**
  - Why needed here: These are the input features for the gating network. You cannot understand why the router selects an expert without understanding what these heuristics measure.
  - Quick check question: Can you explain why a high Common Neighbors score might suggest a different expert is needed compared to a high PageRank score?

- **Concept: Mixture of Experts (MoE) & Sparse vs. Dense Routing**
  - Why needed here: The paper uses a "dense" MoE (soft routing) but mentions "sparse" capabilities. Understanding this distinction is crucial for implementing the `softmax` vs. `TopK` logic in the gater.
  - Quick check question: In Eq. 3, does the model rely on a single expert (hard routing) or a combination of all experts (soft routing)?

- **Concept: Transductive Learning**
  - Why needed here: The paper operates in a transductive setting (all nodes are seen during training). This affects how negative sampling and embedding generation work compared to inductive settings.
  - Quick check question: If a new node is added to the graph after training, can the current model predict links for it without retraining? (Answer: No, based on the transductive definition).

## Architecture Onboarding

- **Component map:** Input $(u, v, l)$ -> Heuristic Module (computes 4 multilayer scores) -> Vector $\mathbf{h}_{uvl}$ -> Gating Module (2-layer MLP + Softmax) -> Weights $w_1, \dots, w_n$ -> Expert Pool (4 frozen models) -> Raw prediction scores -> Aggregator (Weighted sum + Sigmoid) -> Final probability

- **Critical path:** The flow relies on the **Heuristic Module**. If heuristics are not pre-computed or computed incorrectly, the Gating Module receives garbage data, and the "smart" selection fails. The experts are secondary execution units.

- **Design tradeoffs:**
  - **Modularity vs. Joint Optimization:** The paper explicitly trades off the potential accuracy gain of fine-tuning experts together for the **modularity** of freezing them.
  - **Complexity vs. Interpretability:** Using heuristics as input makes the gating decision *interpretable* (e.g., "Expert A was chosen because Common Neighbors was high"), which is lost in purely embedding-based routers.

- **Failure signatures:**
  - **Expert Collapse:** The gater assigns $\approx 1.0$ weight to a single expert for all inputs. This means the heuristics are not discriminative enough or the other experts are poorly trained.
  - **OOM (Out of Memory):** Loading 4 graph neural network experts simultaneously is memory-intensive. MAGMA ran OOT (Out of Time), and large graphs may trigger memory errors if expert embeddings are stored densely.

- **First 3 experiments:**
  1. **Sanity Check (Random Gater):** Replace the MLP gater with a uniform distribution to quantify the lift gained specifically from the heuristic-informed routing versus just averaging.
  2. **Ablation on $\alpha$:** Vary the smoothing coefficient in Eq. 1 ($\alpha \in [0.1, 0.9]$) to determine if the target layer or cross-layer information is more critical for routing.
  3. **Expert Plug-and-Play:** Intentionally remove the best-performing expert and re-run to see if the Gater can recover performance by re-weighting the remaining experts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MoE-ML-LP perform on temporal multilayer networks where link dynamics evolve over time?
- Basis in paper: [explicit] The conclusion states future work will explore applying MoE-ML-LP to more complex settings, such as temporal or heterogeneous multilayer networks.
- Why unresolved: The current framework operates on static snapshots; temporal dependencies in edge formation and expert relevance over time remain unexplored.

### Open Question 2
- Question: What is the trade-off between sparse (TopK) and dense MoE routing in terms of predictive performance and computational efficiency for multilayer link prediction?
- Basis in paper: [inferred] The paper describes both dense and sparse MoE implementations but only evaluates the dense variant, noting sparse routing as "supported by design."
- Why unresolved: No empirical comparison exists; sparse routing may reduce redundancy among experts but could discard complementary signals for certain node-pair-layer triples.

### Open Question 3
- Question: Would fine-tuning individual experts jointly with the gating module improve performance, or does freezing preserve essential specialization?
- Basis in paper: [inferred] The paper argues freezing experts preserves specialization and modularity, but this is a design choice without systematic ablation.
- Why unresolved: Frozen experts may miss opportunities to adapt to multilayer-specific patterns; end-to-end training could harmonize expert outputs but risks homogenization.

## Limitations

- The framework's effectiveness relies heavily on the assumption that structural heuristics remain discriminative across layers, which may break down in networks with highly homogeneous topology.
- The computational cost of maintaining four separate expert models may limit scalability to massive networks.
- The static freezing of expert models precludes fine-tuning to the specific mixture objective, potentially leaving performance gains unrealized.

## Confidence

- **High Confidence**: Claims about the modular architecture allowing seamless expert integration without retraining; the general framework design and heuristic-informed gating mechanism.
- **Medium Confidence**: Claims about the +60% MRR improvement and other quantitative gains; these depend heavily on dataset-specific expert performance and exact implementation details.
- **Low Confidence**: Claims about the gater's ability to recover performance when removing the best expert; this was not empirically validated in the paper.

## Next Checks

1. **Expert Diversity Test**: Measure the entropy of gating weight distributions across all test samples to confirm that experts contribute meaningfully rather than collapsing to a single dominant model.

2. **Alpha Sensitivity Analysis**: Systematically vary the cross-layer smoothing coefficient α in Eq. 1 to determine optimal values per dataset and validate whether target-layer or cross-layer information drives performance.

3. **Synthetic Topology Stress Test**: Evaluate MoE-ML-LP on synthetic networks with controlled structural properties (e.g., random, regular, scale-free) to identify breaking points where heuristic-based routing fails.