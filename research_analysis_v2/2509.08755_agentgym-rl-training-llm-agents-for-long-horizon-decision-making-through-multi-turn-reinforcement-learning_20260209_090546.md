---
ver: rpa2
title: 'AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through
  Multi-Turn Reinforcement Learning'
arxiv_id: '2509.08755'
source_url: https://arxiv.org/abs/2509.08755
tags:
- zhang
- agents
- agent
- arxiv
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AgentGym-RL, a modular reinforcement learning
  framework for training large language model agents to perform long-horizon, multi-turn
  decision-making tasks. The authors propose ScalingInter-RL, a progressive interaction-scaling
  method that gradually extends the agent-environment interaction horizon during training,
  balancing exploration and exploitation to improve stability and performance.
---

# AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning

## Quick Facts
- arXiv ID: 2509.08755
- Source URL: https://arxiv.org/abs/2509.08755
- Reference count: 40
- Primary result: ScalingInter-RL achieves 33.33% success rate in WebArena shopping and 91.00 score in TextCraft, outperforming larger open-source models

## Executive Summary
This paper introduces AgentGym-RL, a modular reinforcement learning framework designed to train large language model agents for complex, long-horizon decision-making tasks across diverse environments. The key innovation is ScalingInter-RL, a progressive interaction-scaling method that gradually extends the agent-environment interaction horizon during training, balancing exploration and exploitation to improve stability and performance. Experiments across five scenarios demonstrate significant performance improvements, with models matching or surpassing commercial models on 27 tasks while using smaller model sizes than typical approaches.

## Method Summary
AgentGym-RL implements a server-client architecture with decoupled Environment, Agent, and Training modules communicating via HTTP APIs. The framework uses GRPO as the primary RL algorithm with a progressive horizon scaling curriculum that starts with restricted interaction budgets and gradually increases them. Training employs Qwen-2.5-3B and Qwen-2.5-7B backbones across five scenarios: Web Navigation (WebArena), Deep Search, Digital Games (TextCraft), Embodied Tasks (BabyAI), and Scientific Tasks (SciWorld). The approach specifically addresses the challenge of long-horizon credit assignment by restricting early-stage interaction lengths and progressively expanding them based on a monotonic schedule.

## Key Results
- ScalingInter-7B model achieves 33.33% success rate in WebArena shopping tasks
- ScalingInter-7B achieves 91.00 score in TextCraft, outperforming larger open-source models
- Models match or surpass commercial models (GPT-4o, OpenAI o3, Gemini-2.5-Pro) on 27 benchmark tasks
- Post-training and test-time compute scaling yield greater performance gains than model size increases

## Why This Works (Mechanism)

### Mechanism 1: Progressive Interaction Scaling (ScalingInter-RL)
The curriculum-based approach restricts interaction horizons early in training to emphasize exploitation of short, successful trajectories, then gradually expands horizons to encourage exploration of complex strategies. This prevents training collapse from excessive variance in long-horizon credit assignment while allowing agents to master basic mechanics before tackling multi-step reasoning.

### Mechanism 2: Group Relative Policy Optimization (GRPO)
GRPO provides more stable gradients than vanilla REINFORCE++ by evaluating action merit relative to a group baseline rather than using full-episode Monte Carlo returns. This normalization reduces sensitivity to stochastic successes and facilitates more efficient exploration in sparse reward environments.

### Mechanism 3: Decoupled Server-Client Architecture
The modular design with isolated environment servers and HTTP APIs enables improved rollout parallelization and prevents resource contention issues. This architecture specifically addresses memory leak problems in TextCraft and SciWorld environments while allowing independent scaling of different components.

## Foundational Learning

- **Exploration vs. Exploitation Trade-off**: Essential for understanding ScalingInter-RL's curriculum approach. Quick check: If an agent repeats the same mistake for 50 steps, is it failing to explore or unable to exploit the correct path?

- **POMDP (Partially Observable Markov Decision Process)**: Critical for modeling agentic tasks where agents only see observations rather than full internal states. Quick check: Can the agent determine the optimal action solely from the current screen observation, or does it need to remember previous steps?

- **Credit Assignment in RL**: Fundamental to understanding why long horizons are challenging. Quick check: If a model succeeds after 20 irrelevant actions and 1 critical action, how does the loss function identify which action to reinforce?

## Architecture Onboarding

- **Component map**: Task Init -> Client.reset() -> Agent.generate(Action) -> Client.step() -> Trajectory Buffer -> Trainer.update()
- **Critical path**: Task initialization triggers environment reset, agent generates action, environment steps, trajectory is buffered, trainer updates policy
- **Design tradeoffs**: Synchronous batch collection provides stability but may be slower than async; RL-from-scratch approach requires careful hyperparameter tuning
- **Failure signatures**: Training collapse (repetitive incoherent outputs), over-interaction (redundant actions), memory leaks in long-running environments
- **First 3 experiments**: 1) Unit test Environment Client for concurrent reset/step calls without memory growth; 2) Horizon ablation comparing fixed vs. ScalingInter-RL on BabyAI; 3) GRPO vs. REINFORCE++ stability comparison on TextCraft

## Open Questions the Paper Calls Out

- **Cross-domain generalization**: Can agents trained via ScalingInter-RL generalize zero-shot to novel environments and tools not seen during curriculum training? The paper notes this as a key challenge for future research.

- **Adaptive horizon scheduling**: Is a fixed monotonic schedule optimal, or would adaptive scheduling based on policy mastery improve training efficiency? The paper uses predetermined curricula without exploring dynamic adjustment mechanisms.

- **Complex scientific reasoning**: What architectural or reward modifications are required to solve complex multi-step simulation tasks like SciWorld's "Chem-Mix" that current approaches find intractable?

## Limitations

- Curriculum hyperparameters (initial horizon, increment values, update frequency) are not specified, making reproduction challenging
- Memory and resource requirements for the server-client architecture are not detailed, limiting practical adoption assessments
- Limited evidence for generalization beyond the five specific domains studied, leaving broader applicability uncertain

## Confidence

- **High Confidence**: Modular architecture design and performance improvements over baselines are well-supported by ablation studies
- **Medium Confidence**: Progressive horizon scaling mechanism is logically sound but lacks specific parameter details and single-model validation
- **Low Confidence**: Claims about compute scaling benefits over model size increases are based on limited comparisons with potential confounding factors

## Next Checks

1. **Curriculum Ablation**: Reproduce ScalingInter-RL training on BabyAI with fixed short horizon, fixed long horizon, and the curriculum approach. Monitor reward curves for training collapse in the long-horizon case.

2. **Algorithm Stability Test**: Train identical 3B models on TextCraft using GRPO and REINFORCE++. Compare reward curve variance and final performance to validate stability claims.

3. **Memory Leak Verification**: Run long training sessions on SciWorld, monitoring environment server memory usage over 1000+ steps. Implement and verify the full-reset interface fix if leaks are detected.