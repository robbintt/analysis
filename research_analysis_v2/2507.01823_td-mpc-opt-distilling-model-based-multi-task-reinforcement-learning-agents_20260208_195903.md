---
ver: rpa2
title: 'TD-MPC-Opt: Distilling Model-Based Multi-Task Reinforcement Learning Agents'
arxiv_id: '2507.01823'
source_url: https://arxiv.org/abs/2507.01823
tags:
- distillation
- learning
- multi-task
- knowledge
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a distillation approach for model-based reinforcement
  learning, transferring knowledge from a large multi-task teacher model (317M parameters)
  to a compact student model (1M parameters) on the MT30 benchmark. The method combines
  traditional teacher-student distillation with FP16 quantization, using a reward-based
  distillation loss alongside the original TD-MPC2 loss.
---

# TD-MPC-Opt: Distilling Model-Based Multi-Task Reinforcement Learning Agents

## Quick Facts
- arXiv ID: 2507.01823
- Source URL: https://arxiv.org/abs/2507.01823
- Authors: Dmytro Kuzmenko; Nadiya Shvai
- Reference count: 36
- Primary result: 1M parameter student model achieves 28.45 normalized score vs 18.93 baseline on MT30 benchmark

## Executive Summary
This paper introduces a distillation approach for model-based reinforcement learning that transfers knowledge from a large multi-task teacher model (317M parameters) to a compact student model (1M parameters) on the MT30 benchmark. The method combines traditional teacher-student distillation with FP16 quantization, using a reward-based distillation loss alongside the original TD-MPC2 loss. The distilled model achieves state-of-the-art performance with a normalized score of 28.45, surpassing the original 1M parameter model (18.93) by 48.5%.

## Method Summary
The approach distills knowledge from a frozen 317M parameter TD-MPC2 teacher model to a trainable 1M parameter student using a combined loss function. The distillation loss matches reward predictions between teacher and student via MSE, with an optimal coefficient d_coef=0.4. Training uses batch size 256 for 1M steps, which outperforms larger batches (1024) despite fewer updates. After distillation, FP16 post-training quantization reduces model size by ~50% with minimal performance loss. The method focuses on reward prediction rather than latent state prediction due to dimensionality mismatches between teacher and student models.

## Key Results
- 1M parameter student achieves 28.45 normalized score vs 18.93 baseline (48.5% improvement)
- Smaller batch size (256) consistently outperforms larger batches (1024) in distillation efficiency
- FP16 quantization preserves performance (28.45) while halving model size from 7.9 MiB to 3.9 MiB
- Extended distillation periods (1M steps) enable better capture of complex multi-task knowledge

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reward-based distillation transfers multi-task knowledge from large teacher to compact student more efficiently than training from scratch.
- Mechanism: The student model learns to predict rewards matching the teacher's predictions via MSE loss, effectively learning the teacher's compressed world model without requiring explicit latent-space alignment.
- Core assumption: The teacher's reward predictions encode sufficient task-relevant knowledge for multi-task control.
- Evidence anchors:
  - [abstract] "distilled model achieves a state-of-the-art normalized score of 28.45, surpassing the original 1M parameter model score of 18.93"
  - [Section 4.2] "when distillation is extended with a batch size of 256 and 1M steps, the model slightly surpasses the baseline (28.12 vs 27.36)"
  - [corpus] Neighbor paper "Knowledge Transfer in Model-Based Reinforcement Learning Agents" (FMR=0.67) appears to be preliminary work by same authors, confirming related interest but limited external validation
- Break condition: Tasks requiring long-term planning or sparse rewards underperform (Section 4.6: acrobot-swingup, cartpole-swingup-sparse, cheetah-jump), suggesting reward-only distillation misses temporal structure.

### Mechanism 2
- Claim: Smaller batch sizes (256) improve knowledge transfer efficiency compared to larger batches (1024) in distillation settings.
- Mechanism: More frequent parameter updates with smaller batches enable finer-grained learning adjustments, particularly beneficial when student capacity is limited and must efficiently absorb teacher knowledge.
- Core assumption: Gradient noise from smaller batches does not destabilize the distillation process.
- Evidence anchors:
  - [Section 4.1] "distillation with an efficient d_coef value provides a clear gain over training from scratch (17.85 vs 14.04)" at batch size 256
  - [Section 4.3] "smaller batches (e.g., 256) consistently outperformed larger ones (1024), even in long-horizon distillation"
  - [corpus] No direct external validation found in corpus neighbors for batch size effects in RL distillation
- Break condition: Very small batches (128) showed no advantage over 256 (Table 2), indicating diminishing returns at extreme reduction.

### Mechanism 3
- Claim: FP16 post-training quantization preserves multi-task performance while halving model size.
- Mechanism: The distilled 1M parameter model's weights have sufficient dynamic range that reducing from FP32 to FP16 precision introduces minimal approximation error in forward passes.
- Core assumption: The student model's learned representations are robust to precision reduction.
- Evidence anchors:
  - [Section 4.5] FP16 achieves 28.45 normalized score (actually slightly higher than 28.12 pre-quantization) with 3.9 MiB size vs 7.9 MiB original
  - [Section 4.5] INT8 quantization drops performance to 5.09, mixed precision to 13.53—showing precision threshold effects
  - [corpus] Limited external evidence; quantization effects on RL models noted as potentially different from supervised learning (Section 4.5)
- Break condition: Aggressive quantization (INT8, mixed precision) causes severe degradation, indicating FP16 represents an approximate lower bound for this architecture.

## Foundational Learning

- Concept: **Knowledge Distillation in RL**
  - Why needed here: The core method transfers knowledge from teacher to student via loss matching—understanding why this differs from supervised distillation is essential.
  - Quick check question: Can you explain why matching reward predictions might transfer more information than matching policy outputs in model-based RL?

- Concept: **TD-MPC2 Architecture**
  - Why needed here: The method builds directly on TD-MPC2's loss components (consistency, reward, value); understanding these is prerequisite to modifying the distillation loss.
  - Quick check question: What role does the world model play in TD-MPC2's planning, and which components are distilled vs retained?

- Concept: **Post-Training Quantization**
  - Why needed here: Final deployment optimization uses FP16 quantization; understanding precision vs accuracy tradeoffs prevents misapplication.
  - Quick check question: Why might FP16 preserve performance while INT8 causes catastrophic degradation in this model?

## Architecture Onboarding

- Component map:
  Teacher (317M) -> Student (1M) -> FP16 Quantizer
  Frozen checkpoint -> Trainable model -> Deployable artifact

- Critical path:
  1. Load 317M teacher checkpoint (frozen)
  2. Initialize 1M student with TD-MPC2 backbone
  3. For each batch: compute teacher reward R_teacher(s,a) and student reward R_student(s,a)
  4. Compute distillation loss MSE(R_teacher, R_student)
  5. Combine with original TD-MPC2 loss using d_coef ≈ 0.4
  6. Train for 1M steps with batch size 256
  7. Apply FP16 quantization to final checkpoint

- Design tradeoffs:
  - d_coef tuning: 0.4 optimal in experiments; higher values (0.9) degrade performance to 13.79
  - Batch size vs steps: 256/1M outperforms 1024/337K even with same data exposure
  - Distillation target: Reward-only distillation (17.85) vastly outperforms latent next-state with projection (7.69) or PCA (8.78)

- Failure signatures:
  - Score ~7-9 with latent distillation: dimensionality reduction losing critical information
  - Score ~5 with INT8: precision too aggressive for this architecture
  - Score matching baseline (~14): distillation not transferring knowledge—check d_coef and teacher loading

- First 3 experiments:
  1. **Sanity check**: Reproduce from-scratch 1M baseline (200K steps, batch 1024) → should achieve ~18.7
  2. **Distillation ablation**: Train student with d_coef=0.4, batch 256, 200K steps → should achieve ~17.85
  3. **Full distillation**: Extend to 1M steps, batch 256, then FP16 quantize → should achieve ~28.45

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can next-state latent prediction be effectively integrated into the distillation process despite the dimensionality mismatch between teacher and student models?
- Basis in paper: [explicit] Section 4.6 notes that latent next-state prediction "did not yield strong results" due to dimensionality mismatches, and Section 5 states integrating it "could be the next step."
- Why unresolved: Current attempts using linear projection and PCA introduced substantial information loss, resulting in poor generalization compared to reward-only distillation.
- What evidence would resolve it: A distillation method utilizing latent prediction that achieves comparable or superior normalized scores to the reward-only baseline (28.45).

### Open Question 2
- Question: Do the distilled, quantized models transfer effectively to physical robotic systems?
- Basis in paper: [explicit] Section 5 lists the lack of validation on physical systems as a primary limitation and essential direction for broader adoption.
- Why unresolved: The study was confined to the simulated MT30 benchmark, and the authors explicitly state they "did not validate the distilled models on physical systems."
- What evidence would resolve it: Successful deployment of the 1M parameter FP16 student model on physical hardware with performance metrics comparable to simulation.

### Open Question 3
- Question: Does the distillation approach generalize to larger task suites (e.g., MT80) and unseen tasks?
- Basis in paper: [explicit] Section 5 identifies evaluating on larger task suites and assessing generalization to unseen tasks as "important directions" not covered in the current work.
- Why unresolved: The experiments were restricted to the MT30 benchmark, leaving scalability and zero-shot generalization untested.
- What evidence would resolve it: Performance evaluation of the distilled student agent on the MT80 benchmark or a set of holdout tasks not present in the training data.

## Limitations
- Method requires access to pre-trained large teacher model, limiting applicability to domains without such resources
- No validation on physical robotic systems despite potential for real-world deployment
- Limited exploration of alternative distillation targets beyond reward prediction
- Quantization analysis restricted to post-training quantization without quantization-aware training exploration

## Confidence

- High confidence: The core distillation mechanism (reward-based teacher-student loss) and its superiority over from-scratch training are well-established through multiple experimental conditions.
- Medium confidence: The batch size optimization (256 vs 1024) and FP16 quantization benefits are demonstrated but could benefit from additional validation across different model architectures.
- Medium confidence: The MT30 benchmark results are impressive but rely on proprietary teacher checkpoints and datasets that limit external verification.

## Next Checks

1. Replicate the distillation performance gap (28.45 vs 18.93) on MT30 benchmark using provided code and teacher checkpoint.
2. Test the batch size sensitivity by comparing 256 vs 1024 batch performance on a subset of tasks with controlled step counts.
3. Evaluate FP16 quantization stability by measuring performance variance across multiple quantization runs and comparing with INT8 quantization on the same model.