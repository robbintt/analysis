---
ver: rpa2
title: 'HRM-Agent: Training a recurrent reasoning model in dynamic environments using
  reinforcement learning'
arxiv_id: '2510.22832'
source_url: https://arxiv.org/abs/2510.22832
tags:
- environment
- learning
- arxiv
- recurrent
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents HRM-Agent, a variant of the Hierarchical Reasoning
  Model (HRM) adapted for reinforcement learning in dynamic environments. The key
  innovation is carrying forward the recurrent latent state between environment steps,
  allowing the model to reuse computation from previous time-steps.
---

# HRM-Agent: Training a recurrent reasoning model in dynamic environments using reinforcement learning

## Quick Facts
- arXiv ID: 2510.22832
- Source URL: https://arxiv.org/abs/2510.22832
- Reference count: 13
- Primary result: HRM-Agent achieves ~99% success rate in goal navigation for dynamic mazes by carrying forward recurrent latent states between environment steps

## Executive Summary
This paper presents HRM-Agent, a reinforcement learning adaptation of the Hierarchical Reasoning Model (HRM) designed for dynamic environments. The key innovation is carrying forward the recurrent latent state between environment steps, allowing the model to reuse computation from previous time-steps. The model was trained entirely via reinforcement learning without supervised losses to navigate to goals in two dynamic maze environments: a four-rooms environment with randomly changing doors and a random maze with opening/closing doors. Results show the model successfully reaches goals in ~99% of episodes in both environments and learns efficient paths. Analysis of the recurrent process demonstrates that carrying forward the latent state enables faster convergence and better reuse of computation, particularly when the environment changes.

## Method Summary
The HRM-Agent adapts the HRM architecture for reinforcement learning by replacing the supervised output head with a DQN head that produces Q-values for action selection. The model operates across two time dimensions - environment steps that advance the episode and recurrent steps within each environment step that allow internal convergence before action selection. In both environments tested, the recurrent process converges faster when carrying forward the latent state z from the previous environment timestep rather than resetting to random values. The model uses dual transformer modules (H: high-level, slow-updating and L: low-level, fast-updating) that alternate updates during recurrent convergence. Training employs off-policy DQN with MSE loss on Q-values, epsilon-greedy exploration (1.0→0.15 over 300k steps), and a target network with delay factor 0.999.

## Key Results
- Achieves ~99% success rate in reaching goals across both dynamic maze environments
- Demonstrates faster convergence when carrying forward recurrent latent state between environment steps
- Shows efficient path learning with mean episode lengths around 10 steps in optimal scenarios
- Validates that HRM can be adapted for reasoning in dynamic, uncertain environments while maintaining computational efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Carrying forward the recurrent latent state z between environment steps enables computational reuse and faster convergence in dynamic settings.
- Mechanism: The latent state z from the previous environment timestep is used to initialize the recurrent process for the current timestep, rather than resetting to random values. This preserves partial plans across steps.
- Core assumption: The latent state encodes task-relevant structure that remains partially valid when the environment changes incrementally.
- Evidence anchors:
  - Analysis shows faster convergence when carrying forward z
  - Distance between initial and final latent states is smaller for carry-forward variant
  - Related work on recurrent reasoning supports importance of recurrence
- Break condition: If environment changes frequently invalidate prior plans, carry-forward may increase convergence time or reduce latent entropy.

### Mechanism 2
- Claim: Hierarchical recurrent processing with separate H and L modules supports multi-scale planning and action selection.
- Mechanism: Module H (high-level, slow-updating) and module L (low-level, fast-updating) alternate updates during recurrent convergence. Different abstraction levels emerge from convergence dynamics without explicit enforcement.
- Core assumption: Separation of timescales allows stable high-level plans while adapting low-level execution to immediate observations.
- Evidence anchors:
  - HRM's reasoning abilities stem from its recurrent inference process
  - Different abstraction levels in H and L intended to emerge from convergence dynamics
  - Recurrence is the key driver of reasoning performance in related work
- Break condition: If task structure doesn't benefit from hierarchical abstraction, the dual-module overhead may slow learning without performance gains.

### Mechanism 3
- Claim: Separating environment time from recurrent time allows adaptive computation without environmental penalty.
- Mechanism: The model operates across two time dimensions - environment steps advance the episode while recurrent steps within each environment step allow internal convergence before action selection. No gradients flow through recurrent steps or across environment steps.
- Core assumption: Convergence of the latent state before action selection improves decision quality, especially for planning tasks.
- Evidence anchors:
  - The model has two 'time-like' dimensions allowing unlimited thinking time
  - Analysis shows initial-to-final latent state distance decreases over recurrent iterations
  - ACT-style adaptive computation is mentioned in HRM but not extensively validated for RL
- Break condition: If recurrent steps don't converge to meaningful stable states, the separation adds latency without benefit.

## Foundational Learning

- Concept: Q-learning and Deep Q-Networks (DQN)
  - Why needed here: The HRM-Agent replaces HRM's supervised output with a DQN head; understanding Bellman updates, target networks, and off-policy learning is essential to debug training.
  - Quick check question: Can you explain why DQN uses a target network and how the loss in Equation 1 relates to temporal difference learning?

- Concept: Recurrent latent state models
  - Why needed here: The core innovation is carry-forward of z across timesteps; understanding how recurrent states accumulate information is prerequisite to analyzing convergence dynamics.
  - Quick check question: How does a recurrent state differ from a simple feedforward hidden state, and what happens if gradients are not propagated through time?

- Concept: Partial observability and belief states
  - Why needed here: The paper positions this work toward POMDP settings; understanding why agents need internal memory to handle unobservable state is key to appreciating the carry-forward mechanism.
  - Quick check question: In a POMDP, why can't an agent act optimally based only on the current observation?

## Architecture Onboarding

- Component map: Observation tokens (121) with RoPE positional embeddings -> HRM core (H: 4 layers, L: 4 layers, 2 heads each, hidden size 64) -> Recurrent loop (alternating H/L updates for max 8 steps) -> DQN head (2-layer) producing Q-values for 4 actions -> Action selection -> Environment step -> Carry z to next step

- Critical path: Observation → embedding → recurrent H/L convergence → DQN head → action selection → environment step → carry z to next step

- Design tradeoffs:
  - Carry-forward vs. reset: Carry improves convergence and sample efficiency but may reduce early-training exploration; reset increases robustness to distribution shift but forfeits computational reuse
  - Recurrent steps fixed at 8 (ACT disabled): Simplifies analysis but removes adaptive computation benefit
  - Small model (500K params): Limits capacity for complex environments but enforces generalization over memorization

- Failure signatures:
  - Slow convergence or high variance in training curves with Carry Z during early training (may indicate latent state entropy collapse)
  - Agent fails to learn efficient paths, getting stuck when doors close (success rate below 87.5% in four-rooms)
  - z trajectory shows no convergence (flat or oscillating MSE curves in analysis)

- First 3 experiments:
  1. Replicate the Four-rooms task with both Carry Z and Reset Z variants; verify ~99% success rate and compare convergence speed. Plot MSE convergence curves as in Figure 5.
  2. Ablate the carry-forward probability (p ∈ {1.0, 0.5, 0.0}) to test whether stochastic carry improves early training stability.
  3. Extend to a partially observable variant (agent-centered view) with a short observation history window; assess whether carry-forward alone suffices or whether explicit memory architectures are needed.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the model learn to dynamically optimize its "thinking time" (inference steps) using Adaptive Computation Time (ACT) while simultaneously learning to solve reinforcement learning tasks?
- Basis in paper: The authors state they intend to "restore the Adaptive Computation Time (ACT) feature, and verify that the model can learn to optimize its 'thinking time' while simultaneously learning to reason."
- Why unresolved: ACT was deliberately disabled in the current study to facilitate the analysis of the recurrent latent state convergence.
- What evidence would resolve it: Successful training of an HRM-Agent with ACT enabled, showing a correlation between task difficulty and the number of recurrent steps taken.

### Open Question 2
- Question: Can the HRM-Agent architecture be successfully adapted for partially observable environments where the agent does not have access to the full environment state?
- Basis in paper: The authors propose using MiniHack's agent-centred observation mode to "validate training of a HRM-Agent in a partially-observable setting."
- Why unresolved: The current experiments utilized fully observable maze environments; the paper notes HRM originally could not integrate computation for partially observable problems.
- What evidence would resolve it: Demonstration of successful goal navigation and latent state convergence using only limited, agent-centric observations.

### Open Question 3
- Question: Does carrying forward the recurrent latent state (z) cause pathological training issues or limit latent entropy in more complex environments?
- Basis in paper: The Discussion notes that faster convergence via carrying z "can be harmful" and could "limit the entropy of the latent state," particularly during early learning or in complex problems.
- Why unresolved: The simple maze environments used did not exhibit these failure modes, but the authors hypothesize potential limitations in harder tasks.
- What evidence would resolve it: Analysis of latent state diversity and training stability when applying the "carry Z" method to high-complexity environments or larger models.

### Open Question 4
- Question: Can this recurrent reasoning architecture be extended to support continual and few-shot learning (CFSL) in an online framework?
- Basis in paper: The authors explicitly express a desire to "explore a recurrent reasoning model which can perform continual and few-shot learning (CFSL) in an online, streaming learning framework."
- Why unresolved: The current model was trained in a standard RL loop; online streaming learning presents challenges regarding stability and retention that were not addressed.
- What evidence would resolve it: Evaluation of the model's ability to retain performance on old tasks while rapidly adapting to new ones in a streaming data scenario.

## Limitations

- The environments used are relatively small-scale, limiting generalization to more complex real-world scenarios
- The analysis of convergence dynamics is qualitative rather than quantitative, lacking rigorous isolation of carry-forward effects
- The lack of ablation studies on carry-forward probability and model capacity leaves open questions about robustness across task difficulty

## Confidence

- **High confidence**: The model successfully reaches goals in ~99% of episodes in both dynamic environments, validating the basic functionality of HRM-Agent for RL navigation tasks.
- **Medium confidence**: The claim that carry-forward enables computational reuse and faster convergence is supported by convergence analysis but lacks quantitative comparison to baseline variants beyond qualitative MSE trends.
- **Medium confidence**: The hierarchical separation of timescales between H and L modules contributes to reasoning performance, though this is inferred from architecture design rather than rigorous ablation.

## Next Checks

1. Quantify the sample efficiency difference between Carry Z and Reset Z variants by measuring training steps to reach 95% success rate across multiple random seeds.
2. Implement a partially observable variant with egocentric observations and a short history window to test whether carry-forward alone suffices or whether explicit memory mechanisms are needed.
3. Measure the variance in z across episodes during early training to diagnose potential entropy collapse when carrying forward, and test probabilistic carry-forward as a mitigation.