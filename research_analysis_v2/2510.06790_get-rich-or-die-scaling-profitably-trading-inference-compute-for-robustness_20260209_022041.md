---
ver: rpa2
title: 'Get RICH or Die Scaling: Profitably Trading Inference Compute for Robustness'
arxiv_id: '2510.06790'
source_url: https://arxiv.org/abs/2510.06790
tags:
- robustness
- data
- compute
- image
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper investigates when and how inference-time compute can\
  \ improve robustness against adversarial attacks in vision language models (VLMs).\
  \ It proposes the Robustness from Inference Compute Hypothesis (RICH), which states\
  \ that inference-compute defenses profit when the model\u2019s training data better\
  \ reflects the attacked data\u2019s components, enabling compositional generalization\
  \ to adversarially out-of-distribution (OOD) data."
---

# Get RICH or Die Scaling: Profitably Trading Inference Compute for Robustness

## Quick Facts
- **arXiv ID**: 2510.06790
- **Source URL**: https://arxiv.org/abs/2510.06790
- **Reference count**: 40
- **Primary result**: Inference-compute scaling improves robustness in VLMs when models are adversarially robustified, via compositional generalization of learned ID components to adversarially OOD data.

## Executive Summary
This paper investigates when and how inference-time compute can improve robustness against adversarial attacks in vision language models (VLMs). The Robustness from Inference Compute Hypothesis (RICH) states that inference-compute defenses profit when the model's training data better reflects the attacked data's components, enabling compositional generalization to adversarially out-of-distribution (OOD) data. Across multiple models, attack types, and inference scaling approaches, the hypothesis consistently shows that inference-compute scaling benefits robustness more in models with higher base robustness. Lightweight adversarial finetuning of VLMs can unlock significant robustness gains from inference compute scaling, even against strong white-box multimodal attacks.

## Method Summary
The study evaluates inference-compute scaling on VLMs using two primary approaches: chain-of-thought (CoT) prompting and security specification repetition (K). Experiments span multiple VLM architectures (LLaVA-v1.5, FARE-LLaVA-v1.5, Delta2LLaVA-v1.5, InternVL 3.5) tested on the Attack-Bard dataset and custom white-box PGD attacks. Adversarial robustness is established through FARE unsupervised finetuning applied to InternVL 3.5. Robustness metrics include classification accuracy, PGD steps-to-success, and attacker loss, with statistical significance assessed via McNemar's test (p<0.01).

## Key Results
- Inference-compute scaling (CoT vs no-CoT) provides statistically significant robustness improvements only for adversarially robustified models, not for non-robust models like standard LLaVA-v1.5.
- Lightweight adversarial finetuning (FARE) applied to InternVL 3.5 unlocks significant robustness gains from inference compute scaling against strong white-box multimodal attacks.
- The "rich-get-richer" dynamic is observed: models with higher base robustness show larger marginal robustness improvements from inference compute scaling.

## Why This Works (Mechanism)

### Mechanism 1: Compositional Generalization to Adversarially OOD Data
- Claim: Inference-compute defenses yield robustness gains when the model's training distribution sufficiently reflects the components of attacked data.
- Mechanism: Compositional generalization lets models satisfy security specifications on adversarially OOD inputs by recombining learned in-distribution (ID) components—e.g., instruction-following and adversarial patterns—into a novel defense behavior at test time.
- Core assumption: The model can reliably decompose OOD inputs into recognizable sub-components and has learned the prerequisite capabilities (e.g., instruction-following, adversarial pattern recognition) from training data.
- Evidence anchors: [abstract] "compositional generalization, through which OOD data is understandable via its in-distribution (ID) components, enables adherence to defensive specifications on adversarially OOD inputs"; [Page 3] "robust models do not see security specifications at train time, they do see adversarial attacks and instruction following problems, suggesting compositional generalization drives enforcement"
- Break condition: If attacked data components fall too far outside the training distribution, compositional generalization cannot bridge the gap, and inference compute fails to help.

### Mechanism 2: Rich-Get-Richer Amplification of Base Robustness
- Claim: Inference-compute scaling provides more marginal robustness improvement to models with higher base robustness than to less robust models.
- Mechanism: Adversarial training shifts the model's representation of attacked data closer to ID data, making compositional generalization easier. This improves the "profitability" of inference compute—scaling reasoning or chain-of-thought further exploits the already-aligned representations to satisfy security specifications.
- Core assumption: Base robustness correlates with the "nearness" of attacked data representations to ID data, and inference compute can then amplify the residual defense benefit.
- Evidence anchors: [Page 2] "This correlation of inference-compute's robustness benefit with base model robustness is the rich-get-richer dynamic"; [Page 9, Table 4] Robustified models (FARE, Delta2) show statistically significant CoT benefits on adversarial data (p < 0.01), while non-robust LLaVA shows no significant benefit.
- Break condition: If base robustness is negligible, even large inference-compute budgets provide little benefit—security specifications remain unenforceable because instruction-following does not generalize.

### Mechanism 3: Security Specification Enforcement via Test-Time Scaling
- Claim: Security specifications—defensive directives provided at inference—only improve robustness when the model can compositionally generalize instruction-following to adversarial OOD inputs; scaling inference compute then improves specification adherence.
- Mechanism: A security specification biases the model's output distribution away from attacker goals. Scaling inference compute (via CoT, reasoning, or repetition) reinforces this bias—but only if the model can interpret and follow the specification on corrupted inputs.
- Core assumption: The model has sufficient instruction-following competence and the security specification is correctly formatted and relevant to the threat.
- Evidence anchors: [Page 5, Table 2] Adding an explicit security specification improves robustness only for the robust Delta2 model (attacker loss rises from 12.4 to 21.1), not for non-robust LLaVA (attack succeeds); [Page 6, Figure 4] Scaling inference compute (increasing K) raises PGD steps-to-success for robust models, with larger slopes for more robust models.
- Break condition: If the model cannot follow instructions on OOD data, the security specification is effectively ignored, and inference scaling does not help.

## Foundational Learning

- **Concept: Compositional Generalization**
  - Why needed here: RICH's core claim depends on models recombining learned components (instruction-following, adversarial patterns) to handle novel OOD adversarial inputs.
  - Quick check question: Given a model trained on tasks A and B separately, can it solve a composite task requiring both A and B without explicit training on the combination?

- **Concept: Adversarial Training and Robustness**
  - Why needed here: Base robustness—often achieved via adversarial pretraining or finetuning—is a prerequisite for inference-compute defenses to be "profitable."
  - Quick check question: Does the model maintain accuracy on inputs perturbed within an ℓ∞ ε-ball, and does performance scale with training ε?

- **Concept: Vision-Language Model Architecture (ViT + LLM)**
  - Why needed here: The paper's experiments focus on VLMs where adversarial perturbations target the visual encoder; robustifying the encoder enables downstream inference-compute benefits.
  - Quick check question: Can you identify which component (vision encoder vs. language decoder) is the bottleneck for robustness on adversarial image classification?

## Architecture Onboarding

- **Component map**:
  - Vision Encoder (ViT) -> Language Decoder (LLM) -> Security Specification Layer -> Inference-Compute Scaling Controller
  - Vision Encoder (ViT): Processes images into embeddings; target of adversarial perturbations. Adversarial finetuning (e.g., FARE) increases base robustness.
  - Language Decoder (LLM): Generates responses; conditioned on image embeddings and text prompts. Executes chain-of-thought or reasoning traces.
  - Security Specification Layer: Explicit (prompt directives) or implicit (learned robustness objective) constraints that guide inference-time behavior toward robust outputs.
  - Inference-Compute Scaling Controller: Modulates compute budget via CoT length, token budgets (budget-forcing), or specification repetition (K).

- **Critical path**:
  1. Adversarially pretrain or finetune the vision encoder → establishes base robustness.
  2. Compose with a language decoder capable of instruction-following and reasoning.
  3. At inference, provide security specifications and scale compute (CoT, budget-forcing) to amplify robustness.
  4. If base robustness is insufficient, inference scaling has minimal effect; if sufficient, scaling yields "rich-get-richer" gains.

- **Design tradeoffs**:
  - Clean vs. Adversarial Performance: Adversarial training may harm clean accuracy; balance robustification extent with utility requirements.
  - Compute vs. Robustness Return: Lightweight adversarial finetuning (e.g., FARE with low ε) may unlock partial benefits; full adversarial pretraining (Delta2) maximizes returns but at higher cost.
  - Specification Explicitness: Explicit specifications (prompts) are more controllable but may be circumvented; implicit robustness (learned) generalizes better but is harder to audit.

- **Failure signatures**:
  - Inference scaling provides no robustness improvement despite large compute budgets → likely insufficient base robustness; check adversarial training extent and representation quality on attacked data.
  - Security specifications are ignored; model still follows adversarial prompts → instruction-following may not generalize; reduce attack budget ε or increase adversarial exposure.
  - Clean performance degrades significantly after robustification → over-regularization; reduce adversarial training intensity or use lightweight finetuning.

- **First 3 experiments**:
  1. **Ablate Base Robustness**: Compare inference-compute scaling (CoT vs. no-CoT) across models with low (LLaVA), medium (FARE), and high (Delta2) robustness on the same adversarial dataset (e.g., Attack-Bard). Expect: robustness gains scale with base robustness.
  2. **Perturbation Budget Sensitivity**: For a medium-robustness model (e.g., FARE), test inference-compute benefits at ε = 8/255, 16/255, 64/255. Expect: smaller ε (attacked data closer to training distribution) yields larger inference-compute benefits per RICH.
  3. **Specification Format Test**: On a robustified model, compare implicit security specifications (pretrained robustness) vs. explicit prompt-based specifications (with and without pre-filled responses). Expect: explicit specifications provide additional gains only when model can follow them on OOD data.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can lightweight adversarial finetuning successfully unlock inference-compute robustness gains in proprietary frontier models (e.g., GPT-4o, Claude 3.5) similar to those observed in the open-source InternVL 3.5 model?
  - Basis in paper: [explicit] The authors explicitly state in the Limitations section that to validate findings at scales seen in widespread deployment, "future work could adversarially train (or finetune) frontier models," as they mostly tested smaller VLMs.
  - Why unresolved: The study primarily validated the RICH on LLaVA-style models and one 20B parameter model (InternVL 3.5), leaving the applicability to the largest proprietary models unproven.
  - What evidence would resolve it: Demonstrating that applying FARE-style finetuning to a frontier model results in a statistically significant correlation between reasoning length and robustness against white-box attacks.

- **Open Question 2**: How does the RICH defense mechanism perform in agentic settings where reasoning chains are exposed, given the risk that inference scaling might increase the adversarial surface area?
  - Basis in paper: [explicit] The authors note in the Limitations that concurrent work (Wu et al., 2025) identifies scenarios where scaling inference compute can "increase adversarial risks when reasoning chains are exposed."
  - Why unresolved: The authors mitigated this risk by scaling inference via prompt extension rather than model generations, but did not test the hypothesis in autonomous or agentic frameworks where the model's thought process is visible to the attacker.
  - What evidence would resolve it: An evaluation of RICH in an agentic environment showing whether the robustness gains from extended reasoning outweigh the vulnerability of exposing the reasoning trace to an attacker.

- **Open Question 3**: Does the "rich-get-richer" dynamic persist when specifically evaluating the trade-off between clean accuracy degradation and adversarial robustness gains during the robustification process?
  - Basis in paper: [inferred] The paper notes that "adding robustness... can harm performance on data that is not adversarially OOD," but the experiments focus primarily on the correlation between base robustness and inference scaling rather than the detailed cost to clean-data utility.
  - Why unresolved: It is unclear if the "profitable" exchange of compute for robustness comes at the expense of unacceptable drops in standard performance, particularly for models that are only moderately robustified (like FARE-LLaVA).
  - What evidence would resolve it: A pareto frontier analysis comparing clean-data accuracy vs. adversarial accuracy across different levels of adversarial training and inference compute budgets.

## Limitations

- **Architecture Specificity**: While RICH is validated across VLMs, the compositional generalization mechanism may not extend to purely text-based or non-vision-encoder models.
- **Security Specification Efficacy**: The paper shows explicit specifications help only on robust models, but does not fully explore specification formats or robustness to specification circumvention.
- **Computational Tradeoffs**: While lightweight finetuning unlocks benefits, the marginal robustness return per inference compute unit is not quantified.

## Confidence

- **High Confidence**: RICH hypothesis validation on multiple VLM architectures (LLaVA, InternVL 3.5) with statistically significant results (p<0.01). The correlation between base robustness and inference-compute benefit is consistently observed across datasets and attack types.
- **Medium Confidence**: Compositional generalization as the primary mechanism—while theoretically grounded and supported by experimental ablation, the exact decomposition of OOD components remains difficult to verify empirically.
- **Medium Confidence**: Lightweight adversarial finetuning (FARE) as a practical method to unlock inference-compute benefits. The method is validated but hyperparameters and implementation details from Schlarmann et al. 2024 may affect reproducibility.

## Next Checks

1. **Ablate the Compositional Mechanism**: Test inference-compute scaling on a model with strong instruction-following but no adversarial exposure. If RICH holds, no robustness gains should be observed, confirming the necessity of compositional generalization.
2. **Scale the Attack Budget**: Systematically increase adversarial perturbation budget (ε) on a medium-robustness model (e.g., FARE) and measure when inference-compute benefits vanish. This would empirically bound the compositional generalization window.
3. **Specification Format Robustness**: Test multiple security specification formats (explicit prompts, implicit objectives, mixed) across varying base robustness levels to determine if explicit specifications add incremental value beyond learned robustness.