---
ver: rpa2
title: 'CUS-QA: Local-Knowledge-Oriented Open-Ended Question Answering Dataset'
arxiv_id: '2507.22752'
source_url: https://arxiv.org/abs/2507.22752
tags:
- evaluation
- questions
- visual
- language
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CUS-QA is a new dataset for evaluating open-ended regional question
  answering in Czech, Slovak, and Ukrainian. It contains questions about local knowledge,
  created by native speakers, with both textual and visual questions grounded in Wikipedia.
---

# CUS-QA: Local-Knowledge-Oriented Open-Ended Question Answering Dataset

## Quick Facts
- **arXiv ID**: 2507.22752
- **Source URL**: https://arxiv.org/abs/2507.22752
- **Reference count**: 40
- **Key outcome**: CUS-QA is a new dataset for evaluating open-ended regional question answering in Czech, Slovak, and Ukrainian, containing questions about local knowledge created by native speakers, with both textual and visual questions grounded in Wikipedia. Even the best open-weight models achieve only 40% accuracy on textual questions and below 30% on visual questions, enabling evaluation of regional knowledge, cross-lingual consistency, and automatic metrics for open-ended QA.

## Executive Summary
CUS-QA is a novel open-ended question answering dataset designed to evaluate regional knowledge in Czech, Slovak, and Ukrainian. It contains both textual and visual questions created by native speakers, grounded in Wikipedia articles, and is intended for evaluation rather than training. The dataset reveals significant performance gaps between textual and visual QA, highlights challenges in cross-lingual consistency, and demonstrates that traditional string-overlap metrics surprisingly well for this task due to the prevalence of named entities in answers.

## Method Summary
The dataset was constructed by collecting open-ended questions and answers about regional knowledge from native speakers of Czech, Slovak, and Ukrainian, with each question linked to a Wikipedia article. The dataset includes both textual questions and visual questions (where answers can be inferred from images). Questions were translated to other local languages using Claude 3.5 Sonnet. The dataset was split into development and test sets, with baselines established using various open-weight models (LLaMA, Mistral, EuroLLM) evaluated through zero-shot prompting with nucleus sampling. Human evaluation was conducted on a subset of outputs, and automatic metrics (BLEU, chrF, BERTScore, BLEURT, LLM-as-judge) were compared for correlation with human judgments.

## Key Results
- Textual QA accuracy peaks at ~40-60% for best models (LLaMA 3.3 70B), while visual QA accuracy remains below 30% (LLaMA 4 Scout 17B)
- chrF and LLM-as-judge metrics show highest correlation with human judgment for textual QA (>0.85 system-level, ~0.4-0.6 answer-level)
- Cross-lingual inconsistencies observed: models sometimes perform better answering regional questions in English than local languages
- RAG experiments show improvement over baselines but don't fully close the performance gap
- Visual QA shows much lower performance than textual, suggesting compounded complexity in visual grounding for culturally specific content

## Why This Works (Mechanism)

### Mechanism 1
The large performance gap between textual QA (40%+) and visual QA (<30%) on regional knowledge may stem from compounded complexity in visual grounding for culturally specific content. Visual QA requires simultaneous visual recognition and regional knowledge retrieval, where errors in either stage propagate. Models may recognize visual elements but lack regional knowledge, or hold regional knowledge but misidentify visual content. This mechanism assumes regional knowledge is less represented in training data for both vision and language models, and cross-modal alignment for regional entities is weak.

### Mechanism 2
Traditional string-overlap metrics (e.g., chrF) perform surprisingly well for open-ended regional QA because named entities dominate answers, enabling meaningful string matches. Regional QA answers are often short noun phrases (cities, people, organizations), and string-overlap metrics capture exact or near-exact entity matches that correlate with correctness judgments. This assumes named entities constitute the primary information content in regional QA answers, and paraphrasing is limited.

### Mechanism 3
Cross-lingual performance inconsistencies (e.g., better English performance on Slovak questions) may arise from training data language bias and weaker multilingual reasoning capabilities. Models may store regional knowledge primarily in English representations, and when queried in local languages, they must translate internally, introducing errors. Smaller models exhibit larger gaps. This assumes regional knowledge is encoded more robustly in English training data, and cross-lingual transfer is imperfect.

## Foundational Learning

- **Named Entity Recognition (NER) and Typology**
  - Why needed here: Understanding that answers are dominated by named entities (locations, persons, organizations) explains why string-overlap metrics work and informs error analysis.
  - Quick check question: Can you identify three fine-grained entity types that appear frequently in the dataset (from Table 5)?

- **Retrieval-Augmented Generation (RAG)**
  - Why needed here: The paper evaluates RAG as a mitigation strategy for regional knowledge gaps. Understanding chunk retrieval, embedding models, and prompt integration is necessary to interpret RAG experiments.
  - Quick check question: What embedding model and chunk size were used for the RAG index in this paper?

- **Evaluation Metrics for Generation**
  - Why needed here: The paper compares string-overlap (BLEU, chrF), embedding-based (BERTScore), trained (BLEURT), and LLM-as-judge metrics. Understanding their design and assumptions is critical for interpreting correlation results.
  - Quick check question: Which metric type showed the highest correlation with human judgment for visual QA, and why might that be?

## Architecture Onboarding

- **Component map:**
  - Wikipedia dumps (per-language) -> Seed entity pools (DBPedia-based) -> Native speaker annotation -> Manual filtering -> Translation (Claude 3.5 Sonnet) -> Dataset split (dev/test) -> Model inference (LLaMA, Mistral, EuroLLM) -> Automatic metrics (BLEU, chrF, BERTScore, BLEURT, LLM-as-judge) -> Human evaluation (correctness, truthfulness, relevance, coherence) -> Cross-lingual comparison and metric correlation analysis

- **Critical path:**
  1. Data collection (native speaker questions/answers) → quality filtering
  2. Baseline model inference (zero-shot, standardized prompts)
  3. Human evaluation of model outputs (binary criteria)
  4. Automatic metric computation and correlation analysis with human judgments

- **Design tradeoffs:**
  - Open-ended vs. multiple-choice: Chosen to reflect real usage, but complicates evaluation
  - Dataset size (~1.2k–1.5k per language): Intentionally small for evaluation, not training; limits statistical power for fine-grained analysis
  - Translator selection: Claude 3.5 Sonnet chosen for quality; Google Translate performed poorly due to English-pivoting issues

- **Failure signatures:**
  - Model outputs: Factually incorrect answers, irrelevant information, refusal to answer (especially for images with faces)
  - Metric behavior: BERTScore showing negative correlations for visual QA; LLM-judge false positives/negatives on specific entities (e.g., "Karel Gott")
  - Cross-lingual: Performance drops when questions translated to/from Ukrainian; models answering in wrong language despite instructions

- **First 3 experiments:**
  1. Reproduce baseline results on the dev split using LLaMA 3.3 70B (textual) and LLaMA 4 Scout (visual) with provided prompts; compute chrF and LLaMA-as-judge metrics
  2. Implement the RAG pipeline using Multilingual-E5-Large embeddings and local Wikipedia chunks; measure improvement on textual questions
  3. Conduct a focused human evaluation on 50 randomly selected visual questions to identify systematic error patterns (e.g., image type, entity category)

## Open Questions the Paper Calls Out

### Open Question 1
How does the optimal balance between parametric knowledge and retrieval-augmented generation (RAG) differ for regional facts compared to global facts? While the paper demonstrates that RAG improves performance, it does not determine if the cost of indexing local Wikipedias is justified compared to improving the model's internal parametric memory for regional languages. A comparative study measuring efficiency and accuracy for "globally known" vs. "regionally known" facts would resolve this.

### Open Question 2
What specific architectural or training data deficiencies cause the large performance gap between textual and visual regional question answering? The paper reports that textual QA reaches over 40% accuracy while visual QA remains below 30%, yet doesn't isolate the root cause. It's unclear if the visual QA failure is due to poor visual recognition in target regions, disconnect between visual features and multilingual text encoders, or lack of visual training data for regional entities. A fine-grained error analysis distinguishing object recognition failures from knowledge retrieval failures would help.

### Open Question 3
Can automatic evaluation metrics be effectively developed to capture nuances like cultural sensitivity and tone in open-ended QA? Current metrics rely on string overlap or LLM-based binary judgments of correctness, which fail to penalize answers that are factually correct but culturally inappropriate or tonally inconsistent. The creation of a human-annotated benchmark of "culturally sensitive" scores for model outputs, followed by training or calibration of a metric to predict these scores, would address this gap.

## Limitations
- Dataset size (~1.2-1.5K examples per language) is intentionally limited to evaluation-only use, constraining statistical power for fine-grained analyses
- Human evaluation relies on binary correctness scores without detailed error taxonomies, limiting interpretability of model failures
- Performance gaps may be influenced by dataset composition (e.g., prevalence of certain entity types or image categories) not fully characterized
- RAG experiments use specific configurations without ablation, so improvements cannot be attributed to particular design choices

## Confidence
- **High confidence**: Dataset construction methodology (native speaker annotation, Wikipedia grounding, dual modalities) is well-documented and reproducible; finding that chrF correlates well with human judgment for textual QA due to named entity prevalence is supported by direct experimental evidence
- **Medium confidence**: Visual QA performance lag is based on reported baselines, but underlying causes are inferred rather than directly measured; cross-lingual patterns are observed but not fully explained
- **Low confidence**: Generalizability to other regional knowledge domains or languages not covered is not established; potential cultural/linguistic biases in Wikipedia source material are not addressed

## Next Checks
1. Conduct detailed error analysis on 100 randomly sampled model outputs (50 textual, 50 visual) to categorize failure modes and compare distributions across languages and modalities
2. Implement RAG ablation study with alternative embedding models, chunk sizes, and top-K retrieval to isolate sources of improvement
3. Create synthetic test set with controlled answer variations to measure how automatic metrics correlate with human judgments across paraphrases, additional context, and entity substitutions