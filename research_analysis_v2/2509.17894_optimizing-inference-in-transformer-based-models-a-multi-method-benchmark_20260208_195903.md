---
ver: rpa2
title: 'Optimizing Inference in Transformer-Based Models: A Multi-Method Benchmark'
arxiv_id: '2509.17894'
source_url: https://arxiv.org/abs/2509.17894
tags:
- attention
- quantization
- each
- diffusion
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarked inference optimization techniques for diffusion
  transformers using fast-DiT as the base model. Methods evaluated included pruning
  (L2 norm-based head removal), post-training quantization (int8 weights only), attention
  simplification (shallow, mediated, and focused grouped-query variants), knowledge
  distillation (teacher-student training), and mixture-of-experts (MoE) architectures.
---

# Optimizing Inference in Transformer-Based Models: A Multi-Method Benchmark

## Quick Facts
- **arXiv ID:** 2509.17894
- **Source URL:** https://arxiv.org/abs/2509.17894
- **Authors:** Siu Hang Ho; Prasad Ganesan; Nguyen Duong; Daniel Schlabig
- **Reference count:** 26
- **Primary result:** Benchmarking of inference optimization techniques for diffusion transformers, finding memory and parameter compression achievable but maintaining throughput and quality challenging.

## Executive Summary
This paper benchmarks inference optimization techniques for diffusion transformers using fast-DiT as the base model. The study evaluates pruning, quantization, attention simplification, knowledge distillation, and mixture-of-experts architectures on ImageNet-derived data. While memory and parameter compression were successfully achieved through quantization and pruning, maintaining or improving inference throughput and image quality proved challenging. The results highlight the complex tradeoffs between computational efficiency and generative quality in diffusion transformer architectures.

## Method Summary
The study uses fast-DiT S/2 as the base model (depth=12, hidden=384, heads=6, patch=2) trained for 200K steps on an ImageNet-256 subset (~110K images from 200 classes). Five optimization techniques are evaluated: L2-norm-based pruning of attention heads, post-training int8 weight-only quantization, attention simplifications (shallow, mediated, focused grouped-query variants), knowledge distillation (teacher-student training), and mixture-of-experts architectures. Metrics include FID/sFID, peak memory, GFLOPS, and throughput (it/s over 50 iterations with warm-up). KD uses AdamW with lr=1e-4 and α=0.3; MoE uses 8E2A and 4E1A variants.

## Key Results
- Pruning caused severe quality loss even with one head removed, indicating high value of attention heads in small architectures
- Quantization achieved ~4× model size reduction and 10-68% memory savings but slowed throughput by 17-24% due to unoptimized CUDA kernels
- Attention simplifications maintained FID/sFID close to baseline while reducing parameters and compute, though throughput gains were limited
- Knowledge distillation with 77% smaller student model failed to preserve quality
- MoE variants increased parameters and compute but worsened FID scores despite producing richer visual details

## Why This Works (Mechanism)

### Mechanism 1: Attention Simplification
The paper reduces attention complexity from quadratic $O(N^2 C)$ to linear or mixed-complexity variants while maintaining quality. Focused grouped-query attention eliminates softmax non-linearity and reorders matrix multiplications to compute $K^T V$ first, achieving $O(N C^2)$. Mediated attention uses adaptive pooling to create mediator tokens ($n << N$), computing attention via two $O(N n C)$ steps. This works by exploiting "excess capacity" in standard dot-product attention that can be compressed without destroying semantic guidance for denoising.

### Mechanism 2: Post-Training Quantization
PTQ to int8 effectively compresses model size (~4×) and reduces peak memory with minimal quality loss. Weights are mapped from float32 to int8 using affine transformation ($q = \text{round}(x/S) + Z$), while activations remain in float precision (weight-only quantization). This reduces memory bandwidth requirements for weight loading. The precision loss introduced by 8-bit discretization is assumed to be imperceptible in low-frequency image regions and only marginally affects high-frequency textures.

### Mechanism 3: Pruning Limitations
Reducing active attention heads via L2-norm pruning causes disproportionate quality degradation in small DiT-S/2 models. L2 norm scores are calculated for Q, K, V slices per head, with lowest-scoring heads zeroed out to introduce sparsity. The severe quality loss with minimal head removal (even 1 head) suggests attention heads are high-value, non-redundant components in small architectures. The L2-norm heuristic may not capture functional importance of attention heads responsible for rare but critical spatial relationships.

## Foundational Learning

- **Concept:** Grouped-Query Attention (GQA)
  - **Why needed here:** The paper uses GQA to reduce KV-cache and computation in the "Focused" attention variant, interpolating between Multi-Head and Multi-Query attention.
  - **Quick check question:** If you reduce the number of Key and Value heads by a factor of $G$, how does the complexity of the initial projection layers change?

- **Concept:** Affine Quantization
  - **Why needed here:** Understanding the formula $q = \text{round}(x/S) + Z$ is necessary to grasp how continuous float values are mapped to discrete int8 integers and why "clipping" occurs.
  - **Quick check question:** In weight-only quantization, are the activations computed in floating point or integer?

- **Concept:** FID (Fréchet Inception Distance)
  - **Why needed here:** This is the primary quality metric. The paper notes FID limitations (sample size sensitivity and correlation with human perception), making it critical to interpret results.
  - **Quick check question:** Does a lower FID score indicate better or worse alignment with the reference image distribution?

## Architecture Onboarding

- **Component map:** VAE latent input $\rightarrow$ Transformer Blocks (Attention + MLP) $\rightarrow$ Output prediction
- **Critical path:** The inference efficiency path runs through Transformer Blocks from VAE latent input to output prediction. Optimization must occur at the Transformer Block level without breaking the denoising schedule.
- **Design tradeoffs:**
  - **Memory vs. Throughput:** Quantization saved memory but reduced speed due to kernel unoptimization
  - **Capacity vs. Quality:** MoE increased total parameters (capacity) but worsened FID (global stats) despite richer visuals; Pruning saved compute but destroyed visuals
  - **Distillation Gap:** A 77% parameter reduction (teacher $\rightarrow$ student) was too aggressive, failing to transfer knowledge effectively
- **Failure signatures:**
  - **Pruning:** Immediate artifacts and loss of fine details (e.g., "bottom row of Figure 3")
  - **MoE:** High FID with "richer backgrounds" suggests expert imbalance or poor routing (only a subset of experts activated)
  - **Quantization:** Magenta pixel deviations in high-frequency texture regions (Figure 4)
- **First 3 experiments:**
  1. **Baseline Profile:** Measure FID, sFID, GFLOPS, and peak memory of the off-the-shelf fast-DiT S/2 model to establish the efficiency/quality upper bound
  2. **Quantization Feasibility:** Apply int8 weight-only quantization and measure the throughput delta specifically on the target deployment GPU to verify if kernel optimizations are available
  3. **Attention Ablation:** Implement "Focused Group-Query" attention (fg-3) on a small training run (e.g., 50k steps) to verify FID stability before committing to a full 200k run

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does narrowing the student-teacher capacity gap via architectural adjustments (e.g., using S/4 or S/8 instead of XS/2) or hyperparameter tuning ($\alpha$) enable effective knowledge distillation for Fast-DiT?
- **Basis in paper:** [explicit] The authors state that future work could explore hyperparameter sweeps or apply distillation to student models like DiT-S/4 to narrow the capacity gap.
- **Why unresolved:** The specific 77% parameter reduction (XS/2) used in experiments failed to transfer knowledge, resulting in higher FID scores than the non-distilled baseline.
- **What evidence would resolve it:** Successful distillation trials on S/4 or S/8 students achieving FID/sFID scores closer to the teacher model than the baseline.

### Open Question 2
- **Question:** Can specific routing balance adjustments or expert capacity tuning resolve the discrepancy where Mixture-of-Experts (MoE) models produce richer visual details but significantly worse FID scores than the baseline?
- **Basis in paper:** [explicit] The authors note that MoE variants require further tuning, such as routing balance or expert capacity adjustments, to recover or surpass baseline generative performance.
- **Why unresolved:** Current MoE results show a paradox where larger capacities generate visually richer backgrounds but degrade global statistics (FID), likely due to imbalanced expert utilization.
- **What evidence would resolve it:** An MoE configuration that maintains the qualitative visual benefits while achieving FID scores comparable to or better than the dense S/2 baseline.

### Open Question 3
- **Question:** Can specialized CUDA kernels for simplified attention mechanisms (mediated or focused grouped-query) translate their theoretical GFLOPS reductions into tangible throughput improvements?
- **Basis in paper:** [inferred] The authors note that while attention simplifications reduced parameters and compute, throughput did not improve because the modified blocks could not leverage the CUDA-optimized attention functions used by the baseline.
- **Why unresolved:** Wall-clock inference speed is currently dominated by hardware optimization rather than theoretical arithmetic complexity in these custom implementations.
- **What evidence would resolve it:** Implementation of custom kernels for the proposed attention variants resulting in inference speeds exceeding the baseline 9.07 it/s.

## Limitations

- Pruning method showed severe quality degradation even with minimal head removal, suggesting the L2-norm heuristic may not capture attention head importance in small DiT architectures
- While quantization achieved substantial memory savings, throughput actually decreased by 17-24% due to experimental CUDA kernels lacking optimization
- Knowledge distillation approach failed dramatically - a student model with 77% fewer parameters could not preserve quality, indicating the distillation gap was too large
- MoE variants increased parameter counts and compute while worsening FID scores, though producing richer visual details, suggesting expert routing or balance issues

## Confidence

- **High Confidence:** Memory and parameter compression results (quantization achieving 4× size reduction, pruning reducing parameters, MoE increasing capacity)
- **Medium Confidence:** Attention simplification quality preservation claims (focused GQA variants maintaining FID close to baseline)
- **Low Confidence:** Throughput improvement claims and distillation success (quantization actually slowed inference, and distillation completely failed)

## Next Checks

1. **Verify Attention Variant Implementation:** Re-implement the "focused grouped-query" attention mechanism and measure both FLOPS reduction and FID stability on a small validation set (50K samples) before committing to full training.

2. **Test Quantization Throughput on Target Hardware:** Apply the same int8 weight-only quantization to the base model and measure actual inference latency on the specific GPU hardware intended for deployment, bypassing TorchAO's experimental kernels.

3. **Conduct Attention Head Ablation Study:** Systematically remove attention heads one at a time from the base model and measure the corresponding FID degradation curve to quantify the relationship between head count and generative quality.