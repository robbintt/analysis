---
ver: rpa2
title: 'WearVQA: A Visual Question Answering Benchmark for Wearables in Egocentric
  Authentic Real-world scenarios'
arxiv_id: '2511.22154'
source_url: https://arxiv.org/abs/2511.22154
tags:
- images
- image
- question
- reasoning
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WearVQA, the first benchmark designed to
  evaluate visual question answering capabilities of multi-modal AI assistants on
  wearable devices like smart glasses. Unlike existing benchmarks that use high-quality,
  third-person imagery, WearVQA focuses on egocentric perspectives with realistic
  challenges including occlusion, poor lighting, and blurriness.
---

# WearVQA: A Visual Question Answering Benchmark for Wearables in Egocentric Authentic Real-world scenarios

## Quick Facts
- arXiv ID: 2511.22154
- Source URL: https://arxiv.org/abs/2511.22154
- Reference count: 21
- State-of-the-art MM-LLMs achieve 24-52% accuracy on egocentric QA tasks

## Executive Summary
WearVQA is the first benchmark designed to evaluate visual question answering capabilities of multi-modal AI assistants on wearable devices like smart glasses. Unlike existing benchmarks that use high-quality, third-person imagery, WearVQA focuses on egocentric perspectives with realistic challenges including occlusion, poor lighting, and blurriness. The benchmark contains 2,520 image-question-answer triplets across 7 domains, 10 cognitive task types, and 6 common image quality issues. A rigorous LLM-as-a-judge evaluation framework achieves 96% accuracy in assessing answer quality. State-of-the-art multi-modal LLMs show QA accuracy ranging from 24-52%, with substantial drops on lower-quality images and reasoning-heavy tasks. The benchmark reveals key challenges in spatial reasoning, object counting, and handling occlusions, positioning WearVQA as a comprehensive tool for advancing real-world wearable AI systems.

## Method Summary
The WearVQA benchmark consists of 2,520 egocentric image-question-answer triplets covering 7 domains and 10 cognitive task types, with 54% of images containing at least one quality issue (blur, occlusion, low light, unzoomed, cutoff, rotated). A five-criterion LLM-as-a-judge framework (factual correctness, relevance, completeness, egocentric phrasing, conciseness) using GPT-4o achieves 96% accuracy against human annotations. Models are evaluated on both a public test set (1,500 samples) and a private test set (1,020 samples) using QA accuracy as the primary metric. The benchmark reveals significant performance drops on lower-quality images and reasoning-heavy tasks, with accuracy ranging from 24-52% across state-of-the-art multi-modal LLMs.

## Key Results
- State-of-the-art MM-LLMs achieve 24-52% accuracy on egocentric QA tasks
- Small unzoomed images present the biggest challenge (35% top-1 accuracy)
- Object counting and spatial reasoning tasks show the lowest performance (38-41% top-1)
- 54% of images contain at least one quality issue affecting model performance
- LLM-as-judge framework achieves 96% accuracy in automated evaluation

## Why This Works (Mechanism)

### Mechanism 1: Egocentric Quality Degradation Cascade
- Claim: Egocentric imagery introduces systematic quality degradations that disproportionately impact fine-grained recognition and reasoning tasks.
- Mechanism: Wearable-captured images exhibit six quality issue types (blur, cutoff, low light, unzoomed, occluded, rotated) due to body-tethered perspective, motion, and environmental variability. These degradations reduce feature discriminability, particularly for small text and distant objects, causing 5-16% accuracy drops across SOTA models.
- Core assumption: Image quality issues are statistically independent in their effects (unproven; interactions may compound degradation).
- Evidence anchors:
  - [abstract] "visual inputs may be occluded, poorly lit, unzoomed, or blurry"
  - [Section 4.3] "small images when unzoomed stands as the biggest challenge (35% top-1)... Occlusion presents another big challenge (42% top-1)"
  - [corpus] EgoVLM and EgoIllusion papers confirm egocentric video understanding faces similar degradation challenges, though focused on video rather than static images
- Break condition: If models are trained on augmented data specifically targeting these six degradation types, performance gaps may narrow significantly.

### Mechanism 2: Cognitive Task Complexity Gradient
- Claim: Reasoning-heavy tasks expose fundamental limitations in current MM-LLM architectures beyond simple recognition failures.
- Mechanism: The benchmark's 10 cognitive task types create a complexity gradient from basic recognition (58% top-1) through spatial reasoning (41% top-1) to object counting (38% top-1). Multi-step reasoning requires chaining visual evidence with commonsense knowledge, exposing failures in object localization, enumeration, and spatial relationship extraction.
- Core assumption: Task difficulty ordering is consistent across model architectures (partially supported by Table 7 showing similar patterns).
- Evidence anchors:
  - [abstract] "with substantial drops on lower-quality images and reasoning-heavy tasks"
  - [Section 4.3] "much lower quality for Object counting (38% top-1), Math (40% top-1), and Spatial Reasoning (41% top-1)"
  - [corpus] EgoCross confirms egocentric QA struggles with cross-domain reasoning beyond daily activities
- Break condition: If spatial-reasoning-specific modules or counting-aware architectures are integrated, the task complexity gradient may flatten.

### Mechanism 3: LLM-as-Judge Evaluation Reliability via Structured Criteria
- Claim: A five-criterion evaluation framework (factual correctness, relevance, completeness, egocentric phrasing, conciseness) enables scalable, reliable automated assessment.
- Mechanism: The structured prompt enforces binary grading with explicit criteria checks, reducing judge subjectivity. The 96% accuracy against human annotations suggests the criteria capture the essential dimensions of answer quality for this task domain.
- Core assumption: GPT-4o's judgment generalizes to other model families' response styles without systematic bias.
- Evidence anchors:
  - [abstract] "rigorous LLM-as-a-judge evaluation framework with 96% labeling accuracy"
  - [Section 3] "precision... is 98.2%. recall... is 95.5%, resulting with a F1-score of 96.8%"
  - [corpus] No direct corpus evidence on LLM-as-judge reliability for VQA; this appears novel to this benchmark
- Break condition: If evaluated models produce adversarially-structured responses exploiting judge blind spots, accuracy may degrade.

## Foundational Learning

- Concept: Egocentric vs. Third-Person Visual Perspective
  - Why needed here: All benchmark images are first-person; models trained on third-person datasets (COCO, etc.) may misinterpret spatial references like "on my left"
  - Quick check question: Can you explain why "the bottle is to my left" requires different spatial reasoning than "the bottle is to the left of the person in the image"?

- Concept: Multi-Modal Quality Degradation Taxonomy
  - Why needed here: 54% of benchmark images have at least one quality issue; understanding how each degradation type affects different visual tasks is essential for targeted improvement
  - Quick check question: Why might low-light conditions show minimal impact (58% top-1) while unzoomed images are most challenging (35% top-1)?

- Concept: Visual Reasoning Complexity Hierarchy
  - Why needed here: 60% of questions require reasoning; models must chain visual evidence with commonsense across 10 task types with vastly different accuracy profiles
  - Quick check question: What additional cognitive operations does "spatial reasoning" require beyond "image recognition"?

## Architecture Onboarding

- Component map:
  Image encoder -> Visual feature extractor (must handle 6 degradation types) -> Vision-language adapter -> Projects visual features to LLM embedding space -> LLM backbone -> Generates egocentric-phrased responses -> LLM-as-judge module -> Five-criterion evaluation (GPT-4o-based)

- Critical path:
  1. Image preprocessing (resolution handling: 720×1280 to 3024×4032)
  2. Quality-robust feature extraction (blur, occlusion, low-light invariance)
  3. Egocentric spatial reasoning (first-person reference resolution)
  4. Task-specific reasoning chains (counting, math, spatial inference)
  5. Response generation with conciseness constraint

- Design tradeoffs:
  - Resolution vs. latency: Table 4 shows inconsistent patterns across models; optimal resolution appears model-specific based on training distribution
  - Generality vs. specialization: General-purpose MM-LLMs (GPT-4o at 52%) vs. potential wearable-specialized fine-tuning
  - Judge strictness vs. scalability: Binary grading enables automated evaluation but may miss partial correctness

- Failure signatures:
  - Unzoomed images: Small text and distant objects unrecognized (35% accuracy)
  - Occluded objects: Partial visibility causes recognition failures (42% accuracy)
  - Object counting: Enumeration failures even with clear visibility (38% accuracy)
  - Spatial reasoning: Relative position inference fails (41% accuracy)
  - Math calculations: Numerical reasoning from visual text fails (40% accuracy)

- First 3 experiments:
  1. Degradation-specific augmentation: Train/augment with targeted blur, occlusion, and unzoomed examples; measure per-category accuracy improvement on the 1,341 low-quality images
  2. Resolution sensitivity analysis: Following Table 4 methodology, evaluate your model at multiple resolutions (960×1280, 1536×2048, native) across text-heavy vs. visual-heavy domains to identify optimal input sizing
  3. Error attribution breakdown: Run your model on the public 1,500-sample test set, categorize failures by the 5 judge criteria (factual, relevance, completeness, egocentric, conciseness) to identify whether errors are visual, reasoning, or formatting-driven

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does VQA performance differ when evaluating dynamic video sequences compared to the static images currently used in WearVQA?
- Basis in paper: [explicit] The Conclusion and Limitations sections explicitly state plans to "expand the benchmark to include... dynamic video sequences" to capture temporal dynamics currently missing from the static image design.
- Why unresolved: The current benchmark relies entirely on static image-question pairs, failing to account for motion blur, fluctuating lighting, and temporal reasoning inherent in real-world wearable usage.
- What evidence would resolve it: A new version of the WearVQA benchmark incorporating video clips and temporal reasoning questions, along with benchmarking results of SOTA models on this new data.

### Open Question 2
- Question: To what degree can Retrieval-Augmented Generation (RAG) improve model accuracy on questions that require external knowledge?
- Basis in paper: [explicit] The Limitations section notes that the "no external-source needed" constraint is a major limitation because "a large portion of voice questions requires external information," suggesting a need for MM-RAG benchmarks.
- Why unresolved: The current dataset was strictly curated to include only questions answerable with common sense and visual input, deliberately excluding queries that would necessitate outside knowledge.
- What evidence would resolve it: An extension of the dataset including questions requiring external knowledge, evaluated against models equipped with MM-RAG capabilities versus standalone models.

### Open Question 3
- Question: What specific architectural or training data factors cause the inconsistent performance impacts of image resolution across different models (e.g., GPT-4o vs. Llama-4)?
- Basis in paper: [inferred] Section 4.3 notes that while Llama-4 performance drops on lower resolutions, GPT-4o and Qwen2.5 performance actually increases or remains stable. The authors "suspect" training data distribution is the cause but do not verify it.
- Why unresolved: The paper observes the phenomenon but lacks the ablation studies or training data transparency required to pinpoint why lower resolution aids some models while hindering others.
- What evidence would resolve it: An ablation study controlling for training data resolution distributions across different model architectures to isolate the variable causing the performance divergence.

### Open Question 4
- Question: How does the model performance generalize across diverse linguistic environments beyond English?
- Basis in paper: [explicit] The Limitations section identifies the "exclusive focus on English-language queries" as a significant limitation that restricts the benchmark's applicability to diverse linguistic environments.
- Why unresolved: The current data collection and question curation were conducted entirely in English, providing no data on how well multimodal reasoning transfers to other languages in egocentric contexts.
- What evidence would resolve it: A multilingual extension of the WearVQA dataset and a comparative evaluation of multilingual MM-LLMs on the translated queries.

## Limitations
- Benchmark relies on proprietary model APIs (GPT-4o, Gemini, Claude) limiting reproducibility
- Egocentric image collection may exhibit geographic and cultural bias from San Francisco location
- LLM-as-judge framework may exhibit systematic bias toward GPT-4o's response patterns
- Static image focus excludes temporal reasoning challenges present in continuous egocentric video streams

## Confidence
- Mechanism 1: Egocentric Quality Degradation Cascade (High): Well-supported by direct experimental evidence showing consistent accuracy drops across multiple models and degradation types
- Mechanism 2: Cognitive Task Complexity Gradient (Medium): Supported by clear accuracy differences across task types, but ordering consistency across model architectures requires further validation
- Mechanism 3: LLM-as-Judge Evaluation Reliability (Medium): 96% accuracy against human annotations is strong evidence, but generalizability to different model response styles remains concerns

## Next Checks
1. **Cross-Cultural Generalizability Test**: Evaluate WearVQA models on egocentric datasets from diverse geographic and cultural contexts to assess whether the benchmark's San Francisco-centric bias affects real-world performance.

2. **Temporal Extension Validation**: Adapt the benchmark to include egocentric video sequences rather than static images, evaluating whether the identified quality degradation cascade and task complexity patterns hold in temporal contexts.

3. **Alternative Judge Framework Comparison**: Implement and compare multiple LLM-as-judge frameworks (using different judge models like Claude, Gemini, and open-source alternatives) to assess whether the 96% accuracy is robust to judge model selection.