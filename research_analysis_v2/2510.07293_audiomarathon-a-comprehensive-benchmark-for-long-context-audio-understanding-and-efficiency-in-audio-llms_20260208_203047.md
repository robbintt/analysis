---
ver: rpa2
title: 'AudioMarathon: A Comprehensive Benchmark for Long-Context Audio Understanding
  and Efficiency in Audio LLMs'
arxiv_id: '2510.07293'
source_url: https://arxiv.org/abs/2510.07293
tags:
- audio
- arxiv
- preprint
- speech
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces AudioMarathon, a comprehensive benchmark for
  evaluating long-context audio understanding and efficiency in Large Audio Language
  Models (LALMs). It addresses the challenge of processing long-form audio, where
  LALMs struggle with quadratic attention costs and long-range temporal dependencies.
---

# AudioMarathon: A Comprehensive Benchmark for Long-Context Audio Understanding and Efficiency in Audio LLMs

## Quick Facts
- arXiv ID: 2510.07293
- Source URL: https://arxiv.org/abs/2510.07293
- Reference count: 34
- Primary result: Evaluates state-of-the-art LALMs on long-context audio understanding, revealing significant performance drops in speech entity recognition and speaker-related tasks as audio length increases.

## Executive Summary
AudioMarathon addresses the critical challenge of processing long-form audio in Large Audio Language Models (LALMs), where quadratic attention costs and long-range temporal dependencies create significant computational and performance barriers. The benchmark provides a comprehensive evaluation framework with 6,567 instances across ten tasks spanning speech, sound, and music domains, featuring audio inputs of 90-300 seconds. Evaluations of state-of-the-art LALMs reveal substantial performance degradation as audio length increases, particularly in tasks requiring temporal reasoning and speaker information modeling. The study also analyzes inference efficiency techniques like token pruning and KV cache eviction, highlighting the need for improved temporal reasoning capabilities and memory-efficient architectures in next-generation audio models.

## Method Summary
AudioMarathon constructs a benchmark of 6,567 audio samples from 10 source datasets, encoding audio at approximately 25 tokens/second to generate sequences of 2,250-7,500 tokens per sample. The evaluation framework tests LALMs across ten tasks including ASR, emotion recognition, and speaker age recognition, using metrics such as F1-score and Word Accuracy Rate. Token pruning is applied at the second transformer layer to reduce computational overhead, with efficiency measured through latency and peak GPU memory. The benchmark specifically targets long-context understanding by using audio inputs of 90-300 seconds, significantly longer than typical evaluations, and examines both task performance and inference efficiency under various pruning and cache eviction strategies.

## Key Results
- Long audio inputs (300s) generate up to 7,500 tokens, creating quadratic attention complexity that severely impacts performance
- Frame-based token pruning outperforms attention-based methods for temporally sensitive tasks like ASR
- ASR performance degrades sharply with aggressive pruning (>60% ratio), with significant word accuracy drops
- Speaker entity recognition and emotion detection tasks show substantial performance gaps between LALMs and human baselines
- KV cache eviction reduces memory footprint but risks disrupting temporal coherence in sequential audio tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pruning audio tokens at the second transformer layer reduces computational overhead while preserving task-relevant information.
- Mechanism: Early-stage compression eliminates redundant low-level acoustic representations before they propagate to deeper layers, where attention patterns become more specialized for semantic reasoning.
- Core assumption: Audio tokens contain overlapping information at shallow layers; critical temporal features stabilize by layer 2.
- Evidence anchors: Second-layer pruning of audio tokens can be understood as a low-level information compression mechanism; related work on audio token compression confirms high token rates limit LALM scalability.

### Mechanism 2
- Claim: Time-aligned (Frame) pruning outperforms attention-based methods for audio because it preserves temporal coverage uniformly.
- Mechanism: Audio redundancy manifests as smooth temporal continuity; Frame subsampling maintains representative tokens across all time windows, preventing loss of brief acoustic events that attention-based methods might deprioritize.
- Core assumption: Important acoustic events are distributed across time rather than concentrated in high-attention regions.
- Evidence anchors: Unlike vision models, where redundancy often arises from spatial or semantic similarity, audio token redundancy primarily manifests as smooth temporal continuity; Frame consistently achieves strongest speech-sensitive F1 scores across pruning ratios.

### Mechanism 3
- Claim: KV cache eviction reduces peak memory during prefilling but risks disrupting temporal coherence for sequential audio tasks.
- Mechanism: Evicting cached key-value pairs based on attention scores or norms compresses memory footprint, but audio's sequential dependency means discarded tokens cannot be retrieved for later cross-attention.
- Core assumption: Recent or high-norm tokens are more important than older, lower-attention tokens for generation.
- Evidence anchors: Aggressive eviction can disrupt temporal coherence, especially in ASR, where every phoneme matters; cache eviction reduces peak memory during prefilling.

## Foundational Learning

- Concept: **Quadratic attention complexity O(N²)**
  - Why needed here: Long audio (300s → 7,500 tokens) makes standard self-attention computationally prohibitive; understanding this motivates pruning and cache strategies.
  - Quick check question: Given 7,500 audio tokens, how many attention computations are required per layer?

- Concept: **Temporal dependency in audio vs. spatial redundancy in vision**
  - Why needed here: Explains why vision-oriented compression methods underperform on audio; audio tokens encode sequential information where position matters.
  - Quick check question: Why might removing "low-attention" tokens harm ASR more than image classification?

- Concept: **KV cache in autoregressive decoding**
  - Why needed here: Decoding stage caches keys/values for all prior tokens; eviction policies trade memory for potential quality loss on long sequences.
  - Quick check question: What happens to generation quality if tokens containing speaker identity are evicted mid-sequence?

## Architecture Onboarding

- Component map: Audio encoder → Token sequence (25 tokens/sec for Qwen2.5-Omni) → LLM backbone with attention layers → Token pruning module (layer 2) → KV cache with eviction policy → Text/audio output

- Critical path: Audio duration determines token count → token count drives attention cost and KV cache size → pruning ratio and eviction policy determine latency/memory tradeoff → task type (semantic vs. acoustic) determines acceptable compression level

- Design tradeoffs:
  - Higher pruning ratio → lower latency but potential loss of phonetic detail (ASR degrades at 90% pruning)
  - Frame pruning → safer for temporal tasks; attention-based (FastV/DART) → riskier but may suffice for classification
  - KV eviction → essential for memory-constrained deployment; SnapKV's clustering preserves local coherence better than random/KNorm

- Failure signatures:
  - ASR word accuracy drops sharply with >60% pruning (insertions/deletions of high-frequency words)
  - Speaker entity recognition fails when temporal markers are pruned inconsistently
  - Repetitive token generation when pruning removes context needed for coherence
  - Closed-source models consistently underperform on emotion detection (F1 < 35) regardless of efficiency method

- First 3 experiments:
  1. **Baseline profiling**: Measure latency, peak memory, and task performance across all 10 AudioMarathon tasks at vanilla settings (no pruning) to establish reference points.
  2. **Pruning sensitivity sweep**: Apply Frame pruning at 30%, 60%, 90% ratios on Qwen2.5-Omni-3B; plot F1-score vs. latency for ASR (temporal-sensitive) vs. MC (temporal-robust) to quantify task-dependent degradation.
  3. **KV cache eviction comparison**: Test SnapKV vs. TOVA on 300-second audio inputs; measure peak memory reduction and SER performance to validate whether attention-aware eviction preserves speaker information better than norm-based methods.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Large Audio Language Models (LALMs) improve consistency and entity tracking to bridge the significant performance gap with humans in Speaker Information Modeling tasks?
- Basis in paper: The Conclusion states that models reveal "limitations in consistency and entity tracking, which exhibit a significant gap compared to human performance, providing a potential direction for future research." Table 3 shows humans achieve an 87.6 F1-score while the best model (Qwen2.5-Omni-7B) achieves only 70.5.
- Why unresolved: Current closed-source models universally fail on tasks like long-audio emotion recognition (ER) and authenticity detection (SD), scoring below 35 F1, indicating a fundamental architectural or training deficiency in modeling speaker characteristics over long durations.
- What evidence would resolve it: The development of a model that achieves near-human parity (>85 F1-score) on the benchmark's Speaker Information Modeling subset (SD, ER, SAR, SGR).

### Open Question 2
- Question: Can non-autoregressive architectures, such as diffusion-based LLMs, successfully resolve the dual challenge of accuracy and efficiency in long-form audio processing?
- Basis in paper: In Section G.2 (Future Work), the authors state: "Exploring architectures that depart from the autoregressive paradigm may offer a promising path forward... Leveraging diffusion LLMs for long-audio understanding and generation could unlock substantial gains in both fidelity and computational throughput."
- Why unresolved: Current autoregressive models struggle with quadratic attention costs and error accumulation (e.g., repetitive tokens or missed phonemes in ASR) as context length increases, limiting real-world deployment.
- What evidence would resolve it: A comparative study showing that a diffusion-based LALM maintains high ASR accuracy (WAR) and low latency on AudioMarathon's 300-second clips compared to autoregressive baselines.

### Open Question 3
- Question: How can token pruning strategies be effectively adapted to handle the temporal continuity and unique redundancy of acoustic signals, rather than relying on spatial heuristics from vision models?
- Basis in paper: Section 4.1 notes that "naive or purely attention-based pruning can inadvertently remove brief phonetic cues... unlike vision models." Section G.2 explicitly proposes to "test transfer from text and vision methods to audio" in future work.
- Why unresolved: The paper's error analysis (Table 7) shows that vision-derived methods like FastV cause substantial deletion of high-frequency words in ASR, failing to preserve temporal coherence.
- What evidence would resolve it: The identification or creation of a pruning algorithm that reduces audio tokens by >60% without causing the specific "omission of high-frequency words" or "temporal coherence disruption" observed in the current benchmarks.

## Limitations

- Task Construction Validity: Several tasks rely on multi-step transformations that may not fully test raw audio comprehension and could introduce bias toward text-oriented reasoning.
- Closed-Source Model Evaluation: Performance comparisons are limited by restricted access to model internals, potentially reflecting evaluation methodology rather than fundamental capability gaps.
- Efficiency Metric Completeness: The evaluation focuses primarily on inference-time memory and latency but doesn't account for tokenization scheme overhead or model-specific optimizations.

## Confidence

**High Confidence**:
- Long audio inputs create quadratic attention complexity challenges (O(N²) scaling is mathematically established)
- Frame-based pruning outperforms attention-based methods for temporally sensitive tasks (Direct experimental comparison with multiple metrics)
- ASR performance degrades sharply with aggressive pruning (>60% ratio) (Clear trend across multiple metrics)

**Medium Confidence**:
- Second-layer pruning preserves semantic reasoning capabilities (Supported by ablation studies but limited to specific model architectures)
- KV cache eviction preserves generation quality when using attention-aware methods (Comparative analysis exists but depends heavily on task type)
- Qwen2.5-Omni consistently outperforms other models across tasks (Multiple experiments show this pattern, though closed-source evaluation is limited)

**Low Confidence**:
- The benchmark comprehensively covers "full domain coverage" (Task diversity exists but some domains are represented by single datasets)
- Emotion recognition difficulties are primarily due to "mode collapse" (Plausible hypothesis but not empirically validated)
- SnapKV clustering provides superior temporal coherence preservation (Comparative claims exist but implementation details are limited)

## Next Checks

1. **Task Construction Validation**: Reconstruct the "summarization" task by running the full pipeline (ASR → text summarization) on a subset of AudioMarathon samples. Compare the final F1 scores against direct audio-to-text summarization where available to quantify the impact of the multi-step transformation.

2. **Closed-Source Model Access Verification**: Implement the exact prompt templates and evaluation methodology on openly accessible audio-language models (Qwen2.5-Omni, GPT-4o-mini) to verify the reported emotion recognition performance gaps. Test whether the poor performance persists when using different prompt formulations or evaluation settings.

3. **Temporal Robustness Analysis**: For the Frame pruning method, conduct a finer-grained analysis by varying the time window size (currently 5-second windows). Test whether smaller windows (1-2 seconds) preserve more temporally local information while maintaining the computational benefits, particularly for the speech entity recognition task where current results show significant degradation.