---
ver: rpa2
title: Cross-Task Benchmarking and Evaluation of General-Purpose and Code-Specific
  Large Language Models
arxiv_id: '2512.04673'
source_url: https://arxiv.org/abs/2512.04673
tags:
- code
- llms
- language
- reasoning
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive cross-domain evaluation of
  five general-purpose and three code-specific Large Language Models (LLMs) across
  six benchmarks assessing linguistic competence, reasoning, and trustworthiness,
  as well as the CoNaLa dataset for code explanation. The study systematically compares
  model performance across natural language and code-specific tasks, revealing that
  code-optimized models like CodeLlama-34B exhibit strong reasoning and syntactic
  precision that generalize effectively to non-coding tasks, outperforming general-purpose
  models such as Mistral-7B and Llama-3-8B on benchmarks like MMLU, GSM8K, and commonsense
  reasoning.
---

# Cross-Task Benchmarking and Evaluation of General-Purpose and Code-Specific Large Language Models

## Quick Facts
- arXiv ID: 2512.04673
- Source URL: https://arxiv.org/abs/2512.04673
- Reference count: 29
- This paper presents a comprehensive cross-domain evaluation of five general-purpose and three code-specific Large Language Models (LLMs) across six benchmarks assessing linguistic competence, reasoning, and trustworthiness, as well as the CoNaLa dataset for code explanation.

## Executive Summary
This paper presents a comprehensive cross-domain evaluation of five general-purpose and three code-specific Large Language Models (LLMs) across six benchmarks assessing linguistic competence, reasoning, and trustworthiness, as well as the CoNaLa dataset for code explanation. The study systematically compares model performance across natural language and code-specific tasks, revealing that code-optimized models like CodeLlama-34B exhibit strong reasoning and syntactic precision that generalize effectively to non-coding tasks, outperforming general-purpose models such as Mistral-7B and Llama-3-8B on benchmarks like MMLU, GSM8K, and commonsense reasoning. For code explanation, CodeLlama-34B achieves the highest scores across all metrics (BLEU, ROUGE, CodeBERTScore), demonstrating superior semantic and lexical alignment. The findings highlight that structured code pretraining enhances cross-domain reasoning capabilities, offering empirical guidance for model selection based on task requirements and resource constraints.

## Method Summary
The study evaluates eight LLMs (5 general-purpose: Llama2-7B/13B, Llama3-8B, Vicuna-7B, Mistral-7B; 3 code-specific: CodeLlama-34B, CodeLlama-13B-Instruct, StarCoder) on 6 NLP benchmarks plus CoNaLa code explanation task. Models were evaluated via Hugging Face Open LLM Leaderboard for MMLU, ARC, HellaSwag, Winogrande, TruthfulQA, and GSM8K. For CoNaLa, code snippets were input to models and natural language explanations generated, then compared against reference annotations using token-based metrics (BLEU, METEOR, ROUGE-1/2/L) and semantic metrics (CodeBERTScore cosine similarity). Statistical significance testing (p < 0.05) was applied to CoNaLa results.

## Key Results
- CodeLlama-34B achieves highest scores across all CoNaLa metrics (BLEU, ROUGE, CodeBERTScore), demonstrating superior semantic and lexical alignment for code explanation.
- Code-optimized models like CodeLlama-34B outperform general-purpose models on MMLU, GSM8K, and commonsense reasoning benchmarks.
- Larger models consistently outperform smaller counterparts within model families, with CodeLlama-34B showing the strongest cross-domain reasoning capabilities.
- Smaller models may score higher on truthfulness benchmarks, potentially due to reduced capacity to memorize training distribution falsehoods.

## Why This Works (Mechanism)

### Mechanism 1: Cross-Domain Transfer from Structured Pretraining
- Claim: Code-specific pretraining appears to produce reasoning capabilities that transfer to non-coding tasks, though causation is not established.
- Mechanism: Training on code (which requires precise syntax, logical flow, and unambiguous semantics) may instill disciplinary patterns that enhance structured reasoning on natural language tasks.
- Core assumption: The transfer is attributed to "structured code pretraining," but the paper does not isolate this from confounds like model scale or base architecture.
- Evidence anchors:
  - [abstract]: "structured code pretraining enhances cross-domain reasoning capabilities"
  - [section 3.3]: "the structured, syntax-aware training of code models enhances both factual reasoning and mathematical problem-solving"
  - [corpus]: Related work on code reasoning (arxiv:2506.13932) discusses reasoning techniques in code domains but does not provide mechanistic evidence for transfer.
- Break condition: If CodeLlama-34B's advantages are primarily due to its 34B parameter scale rather than code-specific training—Llama-3-8B outperforms smaller code models, suggesting scale confounds.

### Mechanism 2: Scale-Dependent Performance Gains
- Claim: Within model families, larger parameter counts correlate with better benchmark performance.
- Mechanism: Increased model capacity may enable richer representations of complex patterns, multi-step reasoning, and factual knowledge.
- Core assumption: The paper assumes scale drives performance but does not control for training data volume, compute, or architectural refinements across sizes.
- Evidence anchors:
  - [section 3.1.1]: "Larger models like LLaMA-2-13B consistently outperform smaller counterparts"
  - [section 3.2.1]: "CodeLlama-34B consistently outperforms both CodeLlama-13B-Instruct and Starcoder-15.5B across all benchmarks"
  - [corpus]: Corpus papers do not systematically address scale effects; evidence is absent.
- Break condition: If architectural improvements (e.g., better attention patterns, training optimizations) co-vary with scale and are the true drivers.

### Mechanism 3: Inverse Scale-Truthfulness Relationship
- Claim: Smaller models may score higher on truthfulness benchmarks, potentially due to reduced capacity to memorize and reproduce false patterns from training data.
- Mechanism: Larger models better fit training distributions, including misconceptions and falsehoods, leading to more confident but less truthful outputs.
- Core assumption: This mechanism is cited from prior work [17], not proven in this study.
- Evidence anchors:
  - [section 3.1.1]: "It has been observed in [17] that larger models are less truthful because they are better at learning from the training distribution"
  - [section 3.2.1]: "we find the smaller model CodeLlama-13B to be performing the best" on TruthfulQA
  - [corpus]: No corpus papers address truthfulness mechanisms; evidence is weak.
- Break condition: If TruthfulQA metric design favors certain response styles over actual truthfulness.

## Foundational Learning

- Concept: **Benchmark Taxonomy (Linguistic vs. Reasoning vs. Trustworthiness)**
  - Why needed here: The paper evaluates across six benchmarks measuring distinct capabilities—conflating them leads to misinterpretation.
  - Quick check question: Would a model optimized for fluency necessarily perform well on GSM8K mathematical reasoning?

- Concept: **Token-Based vs. Semantic Evaluation Metrics**
  - Why needed here: CoNaLa uses BLEU/ROUGE (surface overlap) and CodeBERTScore (embedding similarity)—they capture different quality aspects.
  - Quick check question: If a model paraphrases correctly but uses different vocabulary, which metric would penalize it more?

- Concept: **Cross-Domain Transfer in LLMs**
  - Why needed here: The central claim is that code training transfers to NL reasoning—understanding transfer is essential for interpreting results.
  - Quick check question: What shared structural properties between code and natural language might enable transfer?

## Architecture Onboarding

- Component map:
  - **Input layer**: 8 LLMs (5 general-purpose, 3 code-specific) with sizes 1.5B–34B parameters
  - **Evaluation layer**: 6 NL benchmarks (MMLU, ARC, HellaSwag, Winogrande, TruthfulQA, GSM8K) + CoNaLa code explanation task
  - **Metrics layer**: Token-based (BLEU, ROUGE, METEOR) and semantic (CodeBERTScore cosine similarity)
  - **Analysis layer**: Cross-domain comparison, statistical significance testing (p < 0.05)

- Critical path:
  1. Select models representing both families with controlled scale comparisons
  2. Run standardized benchmarks via Hugging Face Open LLM Leaderboard
  3. Evaluate CoNaLa with all metrics; compute statistical significance
  4. Cross-compare best performers across domains to identify transfer patterns

- Design tradeoffs:
  - Breadth of benchmarks vs. depth of analysis per task
  - Standardized leaderboard metrics (reproducible) vs. custom evaluation (task-specific)
  - Statistical significance testing on CoNaLa but not on NL benchmarks (leaderboard constraint)

- Failure signatures:
  - High linguistic scores but poor reasoning (e.g., strong MMLU, weak GSM8K)
  - Code models failing mathematical reasoning—paper notes "code specific LLMs in general fail at mathematical reasoning"
  - Large performance gaps between token and semantic metrics indicate lexical over semantic matching

- First 3 experiments:
  1. Establish baseline: Evaluate your model on 2-3 NL benchmarks (MMLU, GSM8K, TruthfulQA) to identify capability profile.
  2. Test code explanation: Run CoNaLa evaluation with both token and semantic metrics; check for lexical-semantic alignment gaps.
  3. Isolate scale vs. specialization: Compare a code-specific model against a general-purpose model of similar size (e.g., CodeLlama-13B vs. Llama-2-13B) to separate specialization effects from scale.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of code-specific LLMs on code explanation tasks transfer to generative tasks such as code synthesis, test case generation, and debugging?
- Basis in paper: [explicit] The authors state that "a deeper exploration of code-centric tasks such as code generation, test case generation, and debugging assistance" is needed for a comprehensive understanding.
- Why unresolved: The current study limits its code evaluation to the CoNaLa dataset for code explanation only.
- What evidence would resolve it: Benchmarking the evaluated models on generative tasks (e.g., HumanEval, MBPP) to compare against the CoNaLa explanation results.

### Open Question 2
- Question: Does explicitly incorporating mathematical reasoning training into code-specific LLMs lead to measurable improvements in their coding capabilities?
- Basis in paper: [inferred] The authors observe that code LLMs generally fail at mathematical reasoning and explicitly hypothesize that they "must possess sufficient mathematical reasoning abilities in order to excel at coding tasks."
- Why unresolved: The paper identifies the deficiency in GSM8K scores but does not validate the causal link between math reasoning and code proficiency through intervention.
- What evidence would resolve it: Training ablations where models are fine-tuned on mathematical datasets and subsequently evaluated on code synthesis benchmarks.

### Open Question 3
- Question: Do the cross-domain reasoning advantages of code-specific pretraining observed in CodeLlama and StarCoder generalize to other architectural paradigms?
- Basis in paper: [explicit] The authors acknowledge examining "a limited subset of LLMs" and call for future work to incorporate "a broader and more diverse set of models to enhance the generalizability of these findings."
- Why unresolved: The study is restricted to a specific set of model families (primarily Llama-based) and sizes, leaving the universality of the transfer effects uncertain.
- What evidence would resolve it: Evaluating newer or distinct architectures (e.g., Mamba, Command-R) on the same cross-domain benchmarks to verify if code pretraining yields similar benefits.

## Limitations

- Scale Confounding: The study compares models across a wide parameter range (1.5B-34B), making it difficult to isolate whether observed performance differences stem from code-specific pretraining or simply increased model capacity.
- Mechanism Validation Gap: While the paper attributes cross-domain reasoning transfer to "structured code pretraining," it does not provide mechanistic evidence for this claim.
- Transfer Generalization: The study demonstrates that code-optimized models perform well on non-coding benchmarks, but it cannot establish whether this represents true cross-domain transfer or simply reflects that reasoning capabilities developed for code naturally extend to structured NL tasks.

## Confidence

**High Confidence**: Scale-dependent performance gains within model families (Larger models consistently outperform smaller counterparts across benchmarks). This claim is directly supported by empirical results showing clear performance improvements with increased parameter counts.

**Medium Confidence**: Code-specific pretraining enhances cross-domain reasoning capabilities. While supported by comparative results, the study cannot definitively attribute this to code training rather than scale or architectural factors.

**Low Confidence**: Inverse relationship between model size and truthfulness. This mechanism is cited from external work rather than established through this study's analysis, and the paper does not examine training data or output patterns to support the claimed mechanism.

## Next Checks

1. **Scale-Controlled Comparison**: Conduct experiments comparing general-purpose and code-specific models of identical parameter counts (e.g., CodeLlama-13B vs. Llama-2-13B) while controlling for training data volume and compute budget. This would isolate the effect of code-specific pretraining from scale-related performance gains.

2. **Fine-tuning Transfer Test**: Fine-tune a general-purpose model (Llama-3-8B) on code datasets and evaluate whether it achieves comparable cross-domain reasoning performance to code-specific models. This would test whether code training is necessary for the observed transfer effects or whether general models can learn equivalent capabilities.

3. **Mechanistic Analysis of Truthfulness**: Analyze model outputs on TruthfulQA to determine whether larger models produce more confident but less accurate responses, or whether they simply have better coverage of training distribution content. Compare response patterns across model sizes to identify whether truthfulness differences reflect genuine knowledge gaps or output style preferences.