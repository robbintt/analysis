---
ver: rpa2
title: 'AMQ: Enabling AutoML for Mixed-precision Weight-Only Quantization of Large
  Language Models'
arxiv_id: '2509.12019'
source_url: https://arxiv.org/abs/2509.12019
tags:
- search
- quantization
- bitstack
- llama
- bits
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AMQ introduces automated mixed-precision weight-only quantization
  for LLMs by assigning layer-wise bit-widths to balance model quality and memory
  efficiency. It addresses the infeasibility of brute-force search over 10^100 configurations
  by pruning the search space via sensitivity analysis, using a quantization proxy
  to avoid costly format conversions, and applying a quality predictor to reduce evaluation
  overhead.
---

# AMQ: Enabling AutoML for Mixed-precision Weight-Only Quantization of Large Language Models

## Quick Facts
- arXiv ID: 2509.12019
- Source URL: https://arxiv.org/abs/2509.12019
- Authors: Sangjun Lee, Seung-taek Woo, Jungyu Jin, Changhun Lee, Eunhyeok Park
- Reference count: 40
- Key outcome: AMQ automates mixed-precision weight-only quantization, achieving up to 99.18% of FP16 accuracy with only 3.5 bits and up to 3.16× speedup on memory-constrained GPUs.

## Executive Summary
AMQ introduces an automated mixed-precision quantization framework for large language models that assigns layer-wise bit-widths to balance model quality and memory efficiency. It addresses the infeasibility of brute-force search over 10^100 configurations by pruning the search space via sensitivity analysis, using a quantization proxy to avoid costly format conversions, and applying a quality predictor to reduce evaluation overhead. The iterative search-and-update strategy ensures fast and stable convergence. Experiments on Llama 2, Llama 3.1, Qwen2.5, and Mistral 7B demonstrate that AMQ consistently outperforms fixed-precision and mixed-precision baselines.

## Method Summary
AMQ employs a three-stage pipeline to find optimal layer-wise bit-width allocations under memory constraints. First, it prunes the search space by identifying sensitive outlier layers through single-layer sensitivity analysis, fixing them to 4-bit precision. Second, it pre-computes all linear layers at 2/3/4-bit using an activation-independent quantization proxy (HQQ), enabling O(1) candidate assembly during search. Third, it uses an RBF-based quality predictor trained on JSD scores to evaluate ~800K configurations while only directly measuring ~10K. The NSGA-II multi-objective optimizer proposes candidates balancing average bits and predicted quality, with the archive and predictor updated iteratively. The final configuration is deployed using AWQ with asymmetric clipping.

## Key Results
- Achieves 99.18% of FP16 accuracy on WikiText-2 with only 3.5 bits average per layer on Llama 2 7B
- Outperforms AWQ (fixed 4-bit) and LLaMA-AQ (fixed 3-bit) by 0.7-3.7 perplexity on 7B/13B/70B models
- Delivers up to 3.16× inference speedup on memory-constrained GPUs while maintaining accuracy
- Robust across model families: Llama 2, Llama 3.1, Qwen2.5, and Mistral 7B show consistent Pareto improvements

## Why This Works (Mechanism)

### Mechanism 1: Search Space Pruning via Sensitivity Analysis
Each linear layer is individually quantized to 2-bit (others at 4-bit) on a small calibration set. Layers with perplexity degradation >2× median are fixed to 4-bit, removing them from active search. This exploits layer-specific sensitivity heterogeneity, reducing the 10^100+ configuration space while preserving near-optimal solutions.

### Mechanism 2: Activation-Independent Quantization Proxy
Pre-compute each linear layer at 2/3/4-bit using HQQ. During search, assemble candidate models by selecting pre-computed layers—avoiding ~1.5h conversion per configuration. Final deployment transfers the discovered bit-width assignment to AWQ. The order equivalence condition (Q1(x) < Q1(y) ⇒ Q2(x) < Q2(y)) ensures Pareto frontiers coincide.

### Mechanism 3: RBF Quality Predictor for Efficient Evaluation
Initial random sampling evaluates ~250-600 configurations, computing JSD between quantized and FP16 logits. The RBF predictor learns to estimate JSD for unseen bit-width combinations. NSGA-II proposes candidates using predicted scores; true JSD is computed and the archive/predictor are updated iteratively, enabling evaluation of 800K+ configurations while only measuring ~10K.

## Foundational Learning

- **Grouped Weight-Only Quantization**
  - Why needed here: AMQ assigns bit-widths per linear layer using group size 128. Understanding group quantization is essential to interpret the search space bounds [2.25, 4.25] bits.
  - Quick check question: Given a 4096-dimension weight matrix with group size 128, how many scale factors are stored per layer?

- **NSGA-II Multi-Objective Optimization**
  - Why needed here: AMQ uses NSGA-II to simultaneously minimize average bits and maximize predicted quality, returning a Pareto frontier.
  - Quick check question: What is the difference between a Pareto-optimal solution and a single-objective optimum?

- **Surrogate Models (RBF)**
  - Why needed here: The quality predictor is an RBF model trained on JSD scores to approximate expensive evaluations.
  - Quick check question: How does an RBF interpolator respond to out-of-distribution queries compared to training data regions?

## Architecture Onboarding

- **Component map:**
  FP16 Model → Sensitivity Analysis → Pruned Search Space → Pre-computed HQQ Layers (2/3/4-bit) → Initial Sampling (250-600 configs) → Archive → RBF Quality Predictor ← Archive → NSGA-II (pop=200, iter=20) → Candidates → Assemble from Pre-computed → True JSD Eval → Update Archive → Retrain Predictor → Iterate (200x) → Select config matching memory budget → AWQ conversion

- **Critical path:** Sensitivity calibration → Proxy pre-computation → Predictor initialization → NSGA-II search loop → Final AWQ deployment. The proxy pre-computation is the dominant upfront cost but enables O(1) assembly per candidate.

- **Design tradeoffs:**
  - Pruning threshold (2× median): Conservative threshold preserves search space expressiveness but may retain noise-inducing outlier layers. Stricter thresholds reduce search cost but risk missing optimal allocations.
  - Group size (128): Smaller groups increase quantization fidelity but enlarge memory overhead from scales; AMQ fixes this rather than searching it.
  - Calibration set size (128 WikiText-2 samples): Larger sets improve sensitivity estimates but increase upfront cost; paper shows results are robust to calibration choice.

- **Failure signatures:**
  - Search fails to explore high-bit regions (e.g., >4 bits): Likely pruning threshold too aggressive or calibration set unrepresentative.
  - Predictor produces flat or erratic JSD estimates: Archive too small or initial sampling insufficiently diverse.
  - Deployed AWQ model underperforms proxy predictions: Order equivalence violated—check if activation-dependent outliers emerge in specific layers.

- **First 3 experiments:**
  1. Reproduce sensitivity analysis on a small model (e.g., Llama 2 7B): Quantize each layer to 2-bit individually, plot perplexity distribution, verify outlier identification matches paper (V and Down layers).
  2. Validate proxy order equivalence: Randomly sample 50 bit-width configurations, compute HQQ and AWQ perplexity, verify rank correlation >0.9.
  3. Minimal search loop: Run 20 NSGA-II iterations with 50 initial samples on a single GPU, confirm Pareto frontier improves monotonically in JSD-bits space.

## Open Questions the Paper Calls Out

- **Activation quantization extension**: The current focus on weight-only quantization leaves computation-bound challenges unaddressed. Future work should extend AMQ to activation quantization and explore rotation-based techniques to address both memory and compute bottlenecks.

- **Alternative solvers**: While AMQ uses a genetic algorithm (NSGA-II), the authors acknowledge that alternative solvers for NP-complete problems may yield better configurations, suggesting potential improvements through different optimization approaches.

## Limitations

- Sensitivity-based pruning assumptions may not generalize beyond Llama-family models to other architectures like Mamba or Transformer-XL variants.
- Order equivalence between HQQ proxy and deployment quantizers (AWQ/GPTQ) is empirically validated on only 20% of sampled configurations, not comprehensively.
- RBF predictor effectiveness lacks independent validation on alternative quality metrics beyond JSD, and generalization to out-of-distribution configurations is untested.

## Confidence

- **High confidence**: The layered pipeline (pruning → proxy → predictor → search) is internally coherent and matches the experimental results reported.
- **Medium confidence**: The sensitivity-based pruning reliably identifies outlier layers, but its layer-specific assumptions may not hold across all model families.
- **Medium confidence**: The HQQ proxy preserves Pareto frontiers under order equivalence, though the proof relies on strict conditions that may be violated in practice.
- **Low confidence**: The JSD quality predictor's effectiveness is inferred from ablation against MLP but lacks independent validation on alternative metrics or models.

## Next Checks

1. Validate sensitivity pruning on a non-Llama architecture (e.g., OPT or MPT) to test the layer-specific sensitivity assumption.
2. Expand proxy validation to 50+ configurations across multiple model sizes, verifying Spearman rank correlation >0.9 between HQQ and AWQ/GPTQ perplexity.
3. Stress-test the RBF predictor with synthetic outliers and out-of-distribution bit-width assignments to quantify failure modes and required archive sizes.