---
ver: rpa2
title: 'GeoERM: Geometry-Aware Multi-Task Representation Learning on Riemannian Manifolds'
arxiv_id: '2505.02972'
source_url: https://arxiv.org/abs/2505.02972
tags:
- learning
- manifold
- geoerm
- tasks
- riemannian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GeoERM introduces a geometry-aware multi-task learning framework
  that embeds representation matrices on the Stiefel manifold rather than treating
  them as points in Euclidean space. The method computes Riemannian gradients via
  orthogonal projection and uses polar retraction to maintain orthonormality during
  optimization, thereby preserving the intrinsic manifold structure.
---

# GeoERM: Geometry-Aware Multi-Task Representation Learning on Riemannian Manifolds

## Quick Facts
- **arXiv ID**: 2505.02972
- **Source URL**: https://arxiv.org/abs/2505.02972
- **Reference count**: 27
- **Key outcome**: GeoERM achieves lower estimation error than leading MTL and single-task baselines in high-dimensional, heterogeneous, and outlier-contaminated scenarios, with improved robustness to task diversity and adversarial noise.

## Executive Summary
GeoERM introduces a geometry-aware multi-task learning framework that embeds representation matrices on the Stiefel manifold rather than treating them as points in Euclidean space. The method computes Riemannian gradients via orthogonal projection and uses polar retraction to maintain orthonormality during optimization, thereby preserving the intrinsic manifold structure. Compared to leading MTL and single-task baselines, GeoERM achieves lower estimation error in high-dimensional, heterogeneous, and outlier-contaminated scenarios, with improved robustness to task diversity and adversarial noise. On a real-world wearable-sensor activity recognition benchmark, it yields lower classification error rates across varying model ranks, demonstrating the practical benefits of integrating geometric constraints into representation learning.

## Method Summary
GeoERM learns task-specific representation matrices constrained to the Stiefel manifold St(p,r) = {A ∈ R^(p×r) : A^⊤A = I_r}, where each task's model β^(t) = A^(t)θ^(t) with low-dimensional coefficients θ^(t). The optimization uses Riemannian gradients computed via orthogonal projection of Euclidean gradients onto the tangent space, followed by polar retraction to maintain orthonormality. A shared center A regularizes task representations to prevent negative transfer while allowing outlier tasks to deviate. The two-step procedure first optimizes on the manifold, then refines parameters with ridge-style regularization. Key hyperparameters include rank r (latent dimension), step size α, and regularization strengths λ and γ.

## Key Results
- GeoERM achieves lower estimation error than leading MTL and single-task baselines in high-dimensional, heterogeneous, and outlier-contaminated scenarios
- Improved robustness to task diversity and adversarial noise compared to pooled and spectral MTL methods
- On UCI HAR dataset, yields lower classification error rates across varying model ranks (r ∈ {5, 10, 15})

## Why This Works (Mechanism)

### Mechanism 1: Tangent Space Projection Preserves Orthonormality
- Projecting Euclidean gradients onto the tangent space of the Stiefel manifold preserves orthonormality during optimization
- The Euclidean gradient ∇A(t)f is decomposed into tangent and normal components at A(t). The symmetric part sym((A(t))⊤G) is removed via projection: ˜∇A(t)f = G − A(t) sym((A(t))⊤G), ensuring the update direction stays within TA(t) St(p,r)
- Core assumption: The representation matrix A(t) should satisfy A(t)⊤A(t) = Ir (orthonormal columns) to encode subspace structure meaningfully
- Break condition: If A(t)⊤A(t) ≈ Ir is violated before projection (e.g., due to numerical drift), the projection formula may not correctly identify tangent directions; retraction quality degrades

### Mechanism 2: Polar Retraction Maintains Manifold Constraints
- Polar retraction maps tangent-space updates back to the manifold more stably than naive orthogonalization
- After a Riemannian gradient step H = −α ˜∇A(t)f, polar retraction computes RA(t)(H) = (A(t) + H)(Ir + H⊤H)−1/2, which is equivalent to extracting the orthonormal factor UW⊤ from the SVD of A(t) + H. This guarantees RA(t)(H)⊤RA(t)(H) = Ir
- Core assumption: The retraction operator is smooth and locally approximates exponential map behavior near H = 0 (standard in Riemannian optimization)
- Break condition: If step size α is too large, H⊤H may dominate, causing (Ir + H⊤H)−1/2 to be poorly conditioned; retraction may "overshoot" and destabilize convergence

### Mechanism 3: Shared Center Regularization Balances Transfer and Robustness
- Anchoring task-specific representations to a shared center reduces negative transfer while tolerating outlier tasks
- The penalty term λ/√n · ||A(t)(A(t))⊤ − A(A)⊤||2 pulls each A(t) toward a common subspace A, but allows outlier tasks (Sc) to have arbitrary β(t)outlier, preventing contamination of shared structure
- Core assumption: Normal tasks share a low-rank subspace near A, while outliers do not; the penalty strength λ appropriately balances sharing vs. deviation
- Break condition: If task diversity h is extreme and n is small relative to p + log T, the shared center A may not be recoverable; the paper notes condition n ≳ p + log T for Step 2 refinement

## Foundational Learning

- **Stiefel Manifold St(p, r)**
  - Why needed here: The core constraint that representation matrices A(t) must have orthonormal columns (A⊤A = I) places optimization on a curved manifold, not flat Euclidean space
  - Quick check question: Can you explain why a standard gradient step A(t)k+1 = A(t)k − α∇f does NOT preserve A⊤A = I?

- **Riemannian vs. Euclidean Gradient**
  - Why needed here: GeoERM replaces Euclidean gradients with Riemannian gradients via tangent-space projection; understanding this distinction is essential for implementing the optimizer
  - Quick check question: Given a matrix G and orthonormal A, compute the projection PTA St(p,r)(G) = G − A sym(A⊤G). What is the geometric interpretation of the term A sym(A⊤G)?

- **Retraction Operators**
  - Why needed here: After moving along the Riemannian gradient, the iterate leaves the manifold; retraction is the correct operation to map it back. Polar retraction is numerically stable for Stiefel manifolds
  - Quick check question: Why is naive orthogonalization (e.g., QR after a Euclidean step) not equivalent to using a proper retraction? What convergence guarantee does retraction provide that projection lacks?

## Architecture Onboarding

- **Component map:**
  - Input layer: Task-specific data {(X(t), Y(t))}T t=1
  - Representation layer: {A(t)}T t=1 ⊂ St(p, r), shared center A ∈ St(p, r)
  - Low-dim coefficients: {θ(t)}T t=1 ⊂ Rr
  - Task outputs: β(t) = A(t)θ(t) (Step 1), refined β(t) via ridge-style regularization (Step 2)
  - Loss: (1/T) Σt f(t)(A(t)θ(t)) + (λ/√n) ||A(t)(A(t))⊤ − A(A)⊤||2

- **Critical path:**
  1. Initialize A(t), A as random orthonormal matrices (use QR on Gaussian matrices)
  2. Forward pass: Compute task losses and penalty
  3. Backward pass: Compute Euclidean gradients ∇A(t)f, ∇θ(t)f, ∇Af
  4. Projection: Convert Euclidean gradients to Riemannian via Eq. (3)
  5. Retraction: Update A(t) ← RA(t)(−α · ˜∇A(t)f) using Eq. (4); update θ(t) normally
  6. After Step 1 converges, run Step 2: Solve per-task ridge regression regularized toward Â(t)θ̂(t)

- **Design tradeoffs:**
  - Rank r: Higher r captures more structure but increases computational cost (O(pr) per task) and may overfit; paper tests r ∈ {5, 10, 15}
  - Step size α: Too large causes retraction instability; too small slows convergence. Paper uses Adam with lr=0.01
  - λ, γ: Control sharing strength and refinement shrinkage. Paper sets λ = √(r(p + log T)) and γ = √p + log T, following pERM

- **Failure signatures:**
  - Orthonormality drift: Check ||A(t)⊤A(t) − Ir||F after each iteration; should stay < 1e-6. Large values indicate retraction or projection bug
  - No shared structure learned: If ||A(t) − A|| is large for all t, λ may be too small or tasks may be too heterogeneous
  - Step 2 divergence: If β(t) estimates explode, γ may be too small or n may be insufficient for p + log T condition

- **First 3 experiments:**
  1. **Sanity check on synthetic data (no outliers, low h):** Generate T=20 tasks with n=100, p=30, r=5, h=0.1, ε=0. Fit GeoERM and verify max estimation error < 0.5 and orthonormality preserved. Compare to single-task baseline
  2. **Ablation: Euclidean vs. Riemannian optimization:** Replace Riemannian gradient + polar retraction with standard Euclidean gradient descent followed by QR orthogonalization. Run on synthetic data with h=0.5, ε=0.1. Confirm error gap ≥ 10% in favor of GeoERM
  3. **Robustness to outliers:** Fix h=0.5, vary ε ∈ {0, 0.05, 0.1, 0.2}. Plot max error on normal tasks. Verify GeoERM error grows slowly while pooled/spectral baselines degrade sharply

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Riemannian concentration inequalities be formulated to derive rigorous error bounds for GeoERM, given that standard Euclidean statistical tools fail on curved manifolds?
- Basis in paper: Section 4 states, "An important open direction is to develop Riemannian concentration inequalities," noting that deriving bounds is non-trivial because classic results depend on Euclidean metrics
- Why unresolved: Standard high-dimensional statistics rely on Euclidean assumptions; adapting these to the intrinsic curvature and distance metrics of the Stiefel manifold requires new theoretical tools
- What evidence would resolve it: Derivation of non-asymptotic error bounds for GeoERM using novel concentration inequalities valid on Riemannian manifolds

### Open Question 2
- Question: Can nonparametric techniques (e.g., diffusion maps or RKHS) be integrated with geometry-aware optimization to eliminate the scalability limitations of the current matrix factorization approach?
- Basis in paper: Section 4 notes that the reliance on the factorization $\beta = A\theta$ "may limit scalability in large-scale or high-dimensional settings" and suggests nonparametric techniques could offer more flexible structures
- Why unresolved: The current parametric constraints might restrict the model's ability to discover low-dimensional geometric structures in complex datasets like biomedical images without strict assumptions
- What evidence would resolve it: Development and empirical validation of a nonparametric GeoERM variant that maintains geometric fidelity while scaling to large deep learning datasets

### Open Question 3
- Question: Can theoretical error bounds be established first for single-task learning on Riemannian manifolds and then rigorously extended to the multi-task setting?
- Basis in paper: Section 4 outlines a "promising path forward" to "establish error bounds for single-task learning on Riemannian manifolds and then extend these results to multi-task problems"
- Why unresolved: This step is required to perform rigorous uncertainty checks, but the theoretical framework connecting single-task manifold bounds to multi-task generalization is currently missing
- What evidence would resolve it: A theoretical proof sequence showing that single-task manifold convergence guarantees can be lifted to the multi-task representation learning context

## Limitations
- Real-world validation is limited to a single HAR dataset; generalization to other heterogeneous MTL benchmarks remains untested
- Assumptions about task structure (shared low-rank subspace with sparse outliers) may not generalize to all MTL scenarios
- The paper does not thoroughly explore sensitivity to hyperparameters λ and γ across diverse datasets

## Confidence
- **Geometric optimization claims**: Medium - supported by theoretical analysis and ablation studies, but empirical validation has limitations
- **Practical superiority claims**: Medium-Low - based on limited real-world testing (single HAR dataset)
- **Theoretical framework**: High - solid foundations in Riemannian optimization literature, though extensions to MTL are novel

## Next Checks
1. **Extended Real-World Testing**: Apply GeoERM to multiple heterogeneous MTL benchmarks (e.g., multi-domain image classification, cross-lingual text classification) to assess generalization beyond the HAR dataset
2. **Convergence Analysis**: Systematically vary step size α and rank r on synthetic data to map the convergence landscape and identify optimal operating regions
3. **Scalability Study**: Evaluate GeoERM on larger task sets (T > 100) and higher dimensions (p > 1000) to test computational efficiency and stability of the polar retraction in high-dimensional settings