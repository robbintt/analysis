---
ver: rpa2
title: Exploring the Potential of Large Language Models to Simulate Personality
arxiv_id: '2502.08265'
source_url: https://arxiv.org/abs/2502.08265
tags:
- personality
- score
- trait
- high
- texts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates how well large language models (LLMs) can
  simulate human personality traits using the Big Five model. It combines questionnaire-based
  and text-generation tasks to assess LLMs' ability to mimic specific personality
  traits and evaluate consistency across different model sizes and prompts.
---

# Exploring the Potential of Large Language Models to Simulate Personality

## Quick Facts
- arXiv ID: 2502.08265
- Source URL: https://arxiv.org/abs/2502.08265
- Reference count: 21
- Models can generally respond to personality questionnaires but struggle with consistent personality-text generation, especially for Neuroticism and Agreeableness.

## Executive Summary
This study investigates how well large language models can simulate human personality traits using the Big Five model. Through a combination of questionnaire-based and text-generation tasks, researchers assessed LLMs' ability to mimic specific personality traits and evaluate consistency across different model sizes and prompts. Results show that while LLMs can generally respond to personality questionnaires, generating personality-consistent text is more challenging—particularly for traits like Neuroticism and Agreeableness, where models often exhibit strong biases toward certain score levels. Openness to Experience was the most consistently simulated trait. The research also presents an open-source analytical framework for evaluating personality simulation in LLMs, along with a dataset of generated texts, to facilitate further studies in this area.

## Method Summary
The study employed a multi-stage approach to evaluate personality simulation in LLMs. First, models were prompted to simulate high/low trait levels and complete the BFI-44 questionnaire, with results analyzed for score distributions and reliability measures (Cronbach's alpha, Guttman's lambda). Second, text generation tasks involved creating responses to custom questions about preferences and perspectives, with prompts specifying trait definitions and scores (1-5) across different temperatures (0, 0.5, 0.7, 0.9). Third, outputs were evaluated through human annotation (by 8 annotators using a -2 to +2 scale) and automated classification using GPT-4o as a CARP-based classifier. The framework also included TF-IDF lexical similarity analysis and spaCy lemma analysis to identify linguistic patterns associated with different personality traits.

## Key Results
- LLMs showed high reliability in questionnaire responses (Cronbach's alpha > 0.85) but inconsistent text generation for personality traits
- Openness to Experience was the most consistently simulated trait across all models and evaluation methods
- Models exhibited strong biases toward Low Neuroticism and High Agreeableness, regardless of prompted scores
- Middle-range personality scores (around 3/5) were particularly difficult for models to simulate consistently

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs simulate personality by mapping trait descriptors to statistically probable semantic associations learned during pre-training, rather than possessing internal psychology.
- **Core assumption:** Personality is a lexical phenomenon encoded in language during training.
- **Evidence anchors:** Section 2 explains the linguistic hypothesis; Section 5.4 shows nouns/adjectives more effectively represent traits than verbs.
- **Break condition:** Fails when prompts require behaviors contradicting alignment training, causing models to revert to base "helpful assistant" persona.

### Mechanism 2
- **Claim:** RLHF creates a "baseline personality" bias that overrides specific persona prompts, particularly for Neuroticism and Agreeableness.
- **Core assumption:** Alignment fine-tuning creates a dominant persona difficult to negate via prompting alone.
- **Evidence anchors:** Section 5.3 shows consistent low Neuroticism scores despite high-score prompts; Section 7 links high Agreeableness/low Neuroticism to effective AI assistant traits.
- **Break condition:** Breaks if models are fine-tuned on role-play data where "safe" behaviors are defined differently.

### Mechanism 3
- **Claim:** Models utilize a "binary switch" for personality expression rather than continuous spectrum, struggling with "middle" scores.
- **Core assumption:** Prompting with "moderate" trait definitions provides insufficient context for models to lock onto specific semantic vectors.
- **Evidence anchors:** Section 5.4 shows high/low scores exhibit more noticeable lexical variations; Section 5.3 notes middle scores frequently follow dominant biases.
- **Break condition:** Breaks if prompts provide explicit examples of "moderate" behavior through few-shot learning.

## Foundational Learning

- **The Big Five (OCEAN) Model**
  - **Why needed:** This is the coordinate system for the entire experiment; understanding the axes is essential for interpreting results.
  - **Quick check:** Can you name the trait that correlates with "curiosity and a preference for variety" (Openness) versus the trait that correlates with "organization and dependability" (Conscientiousness)?

- **Linguistic Hypothesis**
  - **Why needed:** This theoretical justification posits that personality is encoded in vocabulary, making text generation a valid test for personality simulation.
  - **Quick check:** If an LLM generates text with high "Openness," which specific parts of speech should you analyze to verify this? (Answer: Nouns and Adjectives, per Section 5.4)

- **Persona Prompting vs. Questionnaire Completion**
  - **Why needed:** The paper distinguishes between knowing a definition (answering a quiz) and acting a part (generating text), as performance differs significantly between these tasks.
  - **Quick check:** Does a high score on a personality questionnaire guarantee that the model will generate text consistent with that personality? (Answer: No, see Abstract/Conclusion)

## Architecture Onboarding

- **Component map:** Prompt Constructor -> Target LLMs (Claude 3 Haiku, GPT-3.5/4o, Mixtral 8x22B) -> Evaluation Layer (GPT-4o CARP-based classifier + human annotators) -> Analysis Module (TF-IDF, spaCy lemma, reliability metrics)
- **Critical path:** Construct Prompt (Score + Definition) → Generate Text → Classify via LLM-as-Judge → Compare Detected Score vs. Prompted Score
- **Design tradeoffs:**
  - Definition-based vs. Numeric Prompts: Study uses definitions which may cause "middle score collapse"; numeric scalers might offer finer control
  - Human vs. Automated Eval: Automated classifier unreliable for Neuroticism (MAE of 1.77); human eval remains ground truth for "messy" traits
- **Failure signatures:**
  - "The Helpful Assistant" Drift: Text becomes polite/stable despite High Neuroticism prompt
  - Middle-Score Blur: Score 3/5 results in random mix of high and low lexical markers
  - Trait Bleed: Attempting High Neuroticism accidentally triggers High Openness
- **First 3 experiments:**
  1. **Sanity Check:** Run framework on Openness trait to verify it reproduces as "most consistently simulated"
  2. **Stress Test:** Prompt for High Neuroticism to confirm alignment bias toward Low Neuroticism
  3. **Binary vs. Scalar:** Remove "middle" options (force High/Low choice) to check if lexical consistency improves

## Open Questions the Paper Calls Out

- **Open Question 1:** Can LLMs consistently simulate complex personalities incorporating all five Big Five traits simultaneously? (Conclusion suggests future research should focus on detailed multiple-trait personality generation)
- **Open Question 2:** What methodologies can effectively simulate high Neuroticism given LLMs' limitations in demonstrating proactive emotional responses? (Discussion notes high Neuroticism is most challenging due to lack of agentic emotional behavior)
- **Open Question 3:** To what extent does safety alignment (RLHF) contribute to observed bias against simulating low Agreeableness and high Neuroticism? (Conclusion hypothesizes bias relates to models' inherent default role as effective AI assistants)

## Limitations

- Text generation prompts use trait definitions rather than numeric scalers, which may be less effective for simulating middle personality scores
- Exact wording of trait definitions used in prompts is not provided, making exact replication challenging
- Automated LLM-as-judge evaluation shows particularly poor performance for Neuroticism (MAE of 1.77)

## Confidence

- **High Confidence:** Openness to Experience is most consistently simulated trait (supported by both questionnaire and text generation results with strong statistical measures)
- **Medium Confidence:** LLMs struggle to simulate middle-range personality scores and tend to exhibit binary-like behavior (supported by data but mechanism unclear)
- **Medium Confidence:** Alignment bias toward Low Neuroticism and High Agreeableness (well-documented but extent varies across models)
- **Low Confidence:** Reliability of automated classification for Neuroticism and other traits (high MAE values indicate method not trustworthy)

## Next Checks

1. **Test the definition vs. numeric scaler hypothesis:** Run parallel experiment using numeric scaler prompts for same traits and compare consistency of middle-range score generation versus definition-based prompts

2. **Validate the alignment bias mechanism:** Create controlled test with High Neuroticism prompts using both standard and "jailbreak" style instructions to measure systematic score differences

3. **Verify the binary switch hypothesis:** Generate texts for all five traits at only extreme scores (1 and 5) and compare TF-IDF lexical similarity to original experiment to check if consistency improves dramatically