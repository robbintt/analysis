---
ver: rpa2
title: Learning Generalizable Skills from Offline Multi-Task Data for Multi-Agent
  Cooperation
arxiv_id: '2503.21200'
source_url: https://arxiv.org/abs/2503.21200
tags:
- learning
- skills
- tasks
- task
- offline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of learning cooperative multi-agent
  policies from offline multi-task data that can generalize to unseen tasks with varying
  numbers of agents and targets. The core method idea is a hierarchical framework
  (HiSSD) that jointly learns common skills (representing general cooperative temporal
  knowledge) and task-specific skills (representing task-specific knowledge for fine-grained
  action execution).
---

# Learning Generalizable Skills from Offline Multi-Task Data for Multi-Agent Cooperation

## Quick Facts
- **arXiv ID:** 2503.21200
- **Source URL:** https://arxiv.org/abs/2503.21200
- **Reference count:** 40
- **Primary result:** Hierarchical framework (HiSSD) achieves 99.2% win rates on unseen tasks in SMAC Marine-Hard benchmark, outperforming baselines.

## Executive Summary
This paper tackles the challenge of learning cooperative multi-agent policies from offline multi-task data that can generalize to unseen tasks with varying agent and target counts. The authors propose HiSSD, a hierarchical framework that jointly learns common skills (general cooperative temporal knowledge) and task-specific skills (task-specific execution priors). By separating these skill types and integrating forward dynamics prediction and contrastive learning, HiSSD achieves superior zero-shot generalization on unseen tasks in both SMAC and multi-agent MuJoCo benchmarks.

## Method Summary
HiSSD is a hierarchical offline MARL framework that decouples skill learning into a High-Level Planner (common skills) and a Low-Level Controller (task-specific skills). The planner uses a transformer-based encoder to extract common skills and a forward predictor to model state transitions, trained with a trade-off between reward maximization and dynamics prediction. The controller uses a contrastive loss to cluster task-specific skills by task identity, enabling adaptation without explicit task labels. The framework is trained on multi-task offline datasets using Implicit Q-Learning for value estimation, achieving zero-shot generalization to unseen tasks.

## Key Results
- HiSSD achieves 99.2% win rates on both source and unseen tasks in expert-quality SMAC Marine-Hard benchmark.
- Outperforms baseline methods on unseen tasks in multi-agent MuJoCo HalfCheetah-v2 benchmark.
- Ablations confirm contributions of forward dynamics prediction and contrastive learning mechanisms.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Hierarchical separation of general cooperative patterns from task-specific execution features facilitates zero-shot generalization to unseen tasks.
- **Mechanism:** Decouples skill learning into High-Level Planner (common skills) and Low-Level Controller (task-specific skills). Common skills encode reusable temporal dynamics, while task-specific skills act as local adapters.
- **Core assumption:** Cooperative behaviors in target tasks share latent temporal structures with source tasks, while requiring different fine-grained action execution priors.
- **Evidence anchors:** [abstract] "jointly learns common skills... and task-specific skills... for fine-grained action execution." [section 3.1] "distinguishes knowledge derived from multi-task data into common skills and task-specific skills." [corpus] "Goal-Oriented Skill Abstraction" discusses similar decoupling for offline RL.
- **Break condition:** Fails if source tasks are too distinct to share a common temporal basis, or if target tasks require entirely new coordination primitives not present in the offline data.

### Mechanism 2
- **Claim:** Integrating a forward dynamics prediction loss into common skill learning injects necessary "cooperative temporal knowledge."
- **Mechanism:** High-Level Planner trained to minimize error of predicting next global state given current common skill, forcing skill representation to model state transitions.
- **Core assumption:** Access to global state information during training is available and sufficient to model the team's dynamics.
- **Evidence anchors:** [section 3.2] Eq. 4 explicitly trades off exploration (reward) against prediction error. [section 4.4] "w/o Prediction" ablation shows performance drop. [corpus] "Puzzle it Out" supports importance of world models in offline MARL.
- **Break condition:** Degrades if global state space is too high-dimensional or noisy for forward predictor to learn effectively.

### Mechanism 3
- **Claim:** Contrastive discrimination of task-specific skills enables policy to identify current task context and adapt execution without explicit task labels.
- **Mechanism:** Low-Level Controller uses InfoNCE-style contrastive loss to cluster task-specific skill embeddings by task identity.
- **Core assumption:** Observations from different tasks contain distinguishable statistical signatures that encoder can capture.
- **Evidence anchors:** [section 3.3] Eq. 10 defines contrastive loss using positive pairs from same task and negatives from other tasks. [section 4.4] "Half-Negative" ablation shows importance of discrimination mechanism.
- **Break condition:** Fails if tasks are statistically identical in observation space but require different actions, or if unseen target tasks fall outside learned task embedding distribution.

## Foundational Learning

- **Concept: Centralized Training, Decentralized Execution (CTDE)**
  - **Why needed here:** High-Level Planner requires global state to predict dynamics and estimate total value, but Low-Level Controller must execute using only local observations and learned skills.
  - **Quick check question:** Can you identify which components (Planner vs. Controller) access global state $s_t$ during training vs. execution?

- **Concept: Offline RL & Distributional Shift**
  - **Why needed here:** Method uses Implicit Q-Learning (IQL) to prevent value function from overestimating returns for out-of-distribution actions.
  - **Quick check question:** Why does planner objective (Eq. 7) use TD-residuals as "imitation weight" rather than standard behavioral cloning?

- **Concept: Probabilistic Inference in RL (KL-divergence)**
  - **Why needed here:** Authors frame skill learning as matching trajectory distribution $p(\tau)$, deriving trade-off between reward maximization and dynamics prediction via KL-divergence minimization.
  - **Quick check question:** In Eq. 4, what does the "Prediction" term effectively constrain the planner to do?

## Architecture Onboarding

- **Component map:**
  - Global State ($s$) + Local Observations ($o^{1:K}$) -> High-Level Planner (Common Skill Encoder $\pi_{\theta_h}$, Forward Predictor $f_\phi$, Value Net $V^{tot}_{\xi}$) -> Common Skills ($c$)
  - Local Observations ($o^{1:K}$) -> Low-Level Controller (Task-Specific Encoder $g_\omega$, Action Decoder $\pi_{\theta_l}$) -> Task-Specific Skills ($z$) + Actions ($a$)

- **Critical path:**
  1. Sample batch from multi-task dataset $D_T$.
  2. **Train Value Net:** Optimize IQL objective (Eq. 5).
  3. **Train Planner:** Infer $c$, predict $s'$, optimize trade-off (Eq. 7).
  4. **Train Controller:** Infer $z$, contrast against other tasks, optimize decoder likelihood (Eq. 11).

- **Design tradeoffs:**
  - **Stability vs. Generalization:** Paper explicitly notes training stability as limitation. Hierarchical loss composition creates conflicting gradients.
  - **Homogeneity vs. Heterogeneity:** Results on "Stalker-Zealot" (heterogeneous units) are weaker than "Marine" (homogeneous). Architecture assumes degree of entity homology for effective skill transfer.

- **Failure signatures:**
  - **Latent Collapse:** Common skills $c$ ignore forward prediction loss, becoming noise if prediction weight $\alpha$ is too low.
  - **Task Confusion:** Task-specific skills $z$ fail to cluster if contrastive loss is under-weighted, leading to erratic action execution in unseen tasks.
  - **Training Instability:** High variance in win rates suggests sensitivity to initialization or hyperparameters.

- **First 3 experiments:**
  1. **Ablation on Prediction:** Run `w/o Prediction` on simple environment (e.g., 3m) to verify forward predictor forces temporal encoding (Section 4.4).
  2. **Negative Sample Sensitivity:** Test `Half-Negative` vs. `Full-Negative` to validate contrastive learning bounds defined in Theorem 3.1.
  3. **Cross-Quality Transfer:** Train on `Expert` data, test on `Medium` scenarios to verify if learned skills are robust to quality shifts or if they overfit to optimal trajectories.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the training stability of the HiSSD framework be improved to ensure consistent convergence during joint optimization of common and task-specific skills?
- **Basis in paper:** [explicit] The conclusion explicitly identifies "training stability" as a limitation and designates it as future work.
- **Why unresolved:** The hierarchical nature of the framework and separate optimization objectives (Eqs. 5, 7, and 11) likely introduce variance or conflicting gradients during end-to-end training.
- **What evidence would resolve it:** Empirical results showing reduced variance in learning curves across random seeds and faster, more reliable convergence rates compared to baseline implementation.

### Open Question 2
- **Question:** How can the representation power of task-specific skills be enhanced to prevent feature overlap in large-scale tasks where distribution shifts are significant?
- **Basis in paper:** [inferred] In Section 4.5, authors observe that task-specific skills in large-scale tasks (e.g., 10m, 12m) "struggle to capture significant distribution shifts, leading to overlapping," unlike in small-scale tasks.
- **Why unresolved:** Current contrastive learning objective (Eq. 10) may lack capacity to distinguish complex task distributions when observation space grows or agent count increases.
- **What evidence would resolve it:** Visualization (e.g., t-SNE) showing distinct, non-overlapping clusters for large-scale tasks, coupled with improved win rates on those specific unseen tasks.

### Open Question 3
- **Question:** Can the hierarchical skill discovery mechanism be adapted to maintain performance advantages on heterogeneous agent tasks when training on medium or low-quality offline data?
- **Basis in paper:** [inferred] In Section E.1, authors note that skill-learning methods fail to outperform behavior cloning on heterogeneous Stalker-Zealot tasks with non-expert data.
- **Why unresolved:** Common skill extraction relies on high-quality cooperative patterns; with heterogeneous units and noisy data, "common" skills may represent noise or conflicting strategies rather than useful temporal knowledge.
- **What evidence would resolve it:** Demonstrating superior performance over BC-baseline on heterogeneous benchmarks (like Stalker-Zealot) using Medium or Medium-Replay datasets.

## Limitations
- **Training stability:** Acknowledged as key limitation with sensitivity to hyperparameters and sequential optimization of hierarchical losses.
- **Heterogeneous unit performance:** Weaker results on heterogeneous tasks (Stalker-Zealot) compared to homogeneous ones (Marine), suggesting sensitivity to entity homology.
- **Contrastive learning validation:** Limited ablation studies across diverse task distributions to fully validate contrastive learning mechanism.

## Confidence

- **High confidence:** Hierarchical architecture design and components (common vs. task-specific skills, forward dynamics prediction) are well-specified and experimentally validated on SMAC benchmarks with clear performance gains over baselines.
- **Medium confidence:** Generalization claims to unseen tasks are supported by quantitative results, but mechanism's robustness across heterogeneous unit types and task distributions remains uncertain.
- **Low confidence:** Contrastive learning's contribution is primarily validated through single ablation (Half-Negative), and theoretical bounds (Theorem 3.1) are not empirically stress-tested across varying negative sample ratios.

## Next Checks

1. Run the "w/o Prediction" ablation on a simple environment (e.g., 3m) to verify the forward predictor actually forces temporal encoding of common skills, as claimed in Mechanism 2.
2. Test the "Half-Negative" vs. "Full-Negative" contrastive sample ratios across multiple tasks to validate the bounds and sensitivity described in Theorem 3.1.
3. Train on Expert-quality data and evaluate on Medium-quality scenarios to verify if the learned skills are robust to quality shifts or if they overfit to optimal trajectories, probing the method's distributional shift tolerance.