---
ver: rpa2
title: Regret Guarantees for Linear Contextual Stochastic Shortest Path
arxiv_id: '2511.12534'
source_url: https://arxiv.org/abs/2511.12534
tags:
- lemma
- learning
- proceedings
- state
- regret
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Linear Contextual Stochastic Shortest
  Path (LR-CSSP) algorithm for learning in contextual SSP problems where both dynamics
  and losses depend linearly on adversarial contexts. The algorithm maintains confidence
  sets over linear embeddings and employs an optimistic principle to compute policies.
---

# Regret Guarantees for Linear Contextual Stochastic Shortest Path

## Quick Facts
- **arXiv ID**: 2511.12534
- **Source URL**: https://arxiv.org/abs/2511.12534
- **Reference count**: 40
- **Primary result**: LR-CSSP algorithm achieves $\tilde{O}(K^{2/3} d^{2/3} |S| |A|^{1/3} B_\star^2 T_\star \log (1/\delta))$ regret for contextual SSP

## Executive Summary
This paper introduces LR-CSSP, the first algorithm for contextual stochastic shortest path (CSSP) problems where both dynamics and losses depend linearly on adversarial contexts. The algorithm maintains confidence sets over unknown linear embeddings and uses optimistic planning to compute policies that guarantee proper termination while achieving sublinear regret. The key innovation is handling the unique challenge in CSSP where insufficient knowledge can lead to non-terminating episodes, which requires careful classification of state-action pairs as "known" or "unknown" based on context-conditioned uncertainty.

## Method Summary
LR-CSSP maintains confidence sets over linear embeddings of context-dependent MDP dynamics and losses through ridge regression. It classifies state-action pairs as "known" or "unknown" based on context-conditioned uncertainty, using this classification to trigger optimistic replanning via Extended Value Iteration (EVI) over an augmented MDP. The algorithm projects raw dynamics estimates onto stochastic matrices to ensure valid probability distributions, and employs an optimistic principle to compute policies that underestimate the true value function. This approach balances exploration (through unknown pair detection) with exploitation (through optimistic planning) while guaranteeing all episodes terminate within reasonable time steps.

## Key Results
- LR-CSSP achieves $\tilde{O}(K^{2/3} d^{2/3} |S| |A|^{1/3} B_\star^2 T_\star \log (1/\delta))$ regret
- When all costs exceed $\ell_{\min} > 0$, regret improves to $\tilde{O}(\sqrt{K \cdot d^2 |S|^3 |A| B_\star^3 \log(1/\delta)/\ell_{\min}})$
- Algorithm ensures proper policies that terminate episodes while handling continuous context spaces
- Confidence sets are maintained over linear embeddings using self-normalized concentration inequalities

## Why This Works (Mechanism)

### Mechanism 1: Optimistic Planning with Confidence Sets
- **Claim**: LR-CSSP achieves sublinear regret by maintaining confidence sets over unknown linear embeddings and selecting optimistic models that underestimate the true value function.
- **Mechanism**: The algorithm solves ridge regression problems to estimate loss and dynamics embeddings, then constructs ellipsoidal confidence sets using self-normalized concentration. At each interval, it solves an optimization problem to find an optimistic policy guaranteed to be no worse than optimal under the high-probability event.
- **Core assumption**: True embeddings lie in $\mathbb{R}^d$ with bounded norm, and contexts lie on the probability simplex.
- **Evidence anchors**: Abstract states confidence sets are maintained and optimistic principle employed; Section 4 describes the approach follows optimism in the face of uncertainty.
- **Break condition**: Confidence set construction fails if noise is unbounded or if the linear embedding assumption is violated.

### Mechanism 2: Known/Unknown State-Action Classification
- **Claim**: Classifying state-action pairs as "known" or "unknown" based on context-conditioned uncertainty ensures all episodes terminate while controlling exploration cost.
- **Mechanism**: A pair is classified as known if the context-conditioned uncertainty falls below a threshold ensuring estimated transition error is small enough to guarantee proper policies. Unknown pairs trigger interval resets and forced exploration.
- **Core assumption**: Minimum loss $\ell_{\min} > 0$ exists (or is injected via $\epsilon$-perturbation), and $B_\star$ bounds the optimal value function.
- **Evidence anchors**: Abstract notes insufficient knowledge can prolong episodes; Section 4 explains known pairs guarantee reaching goal state; Lemma 5 bounds the number of times any pair can be unknown.
- **Break condition**: If $\ell_{\min} = 0$ without perturbation, the classification threshold becomes undefined and episodes may not terminate.

### Mechanism 3: Stochastic Matrix Projection
- **Claim**: Projecting raw dynamics estimates onto stochastic matrices preserves concentration guarantees while ensuring valid probability distributions under any context.
- **Mechanism**: After computing dynamics estimates via ridge regression, the algorithm projects onto the set of stochastic matrices. Since projection is non-expansive and the true dynamics are stochastic, this preserves statistical guarantees.
- **Core assumption**: True dynamics embedding has stochastic columns (each conditional distribution is valid).
- **Evidence anchors**: Algorithm 1 shows explicit projection step; Lemma 4 proves projection preserves statistical guarantee.
- **Break condition**: Projection fails to help if true dynamics are not well-approximated by stochastic column matrices in the embedded space.

## Foundational Learning

- **Concept: Stochastic Shortest Path (SSP) and Proper Policies**
  - Why needed here: SSP generalizes finite-horizon MDPs with unknown episode lengths; "proper" policies guarantee reaching the goal state with probability 1. Without understanding this, the termination guarantees of LR-CSSP are opaque.
  - Quick check question: Given a policy that loops forever between two states with zero loss, is it proper? (Answer: No—it never reaches the goal.)

- **Concept: Self-Normalized Concentration for Linear Bandits (Abbasi-Yadkori et al., 2011)**
  - Why needed here: Theorem 2.1 provides the ellipsoidal confidence sets used throughout; understanding how $\bar{V}_t = \lambda I + \sum X_s X_s^\top$ controls confidence width is essential for interpreting the known/unknown threshold and Lemma 5.
  - Quick check question: If $\bar{V}_t$ has small eigenvalues in direction $c$, what happens to confidence in $\langle c, \theta^\star \rangle$? (Answer: Confidence width grows—less data in that direction.)

- **Concept: Linear Function Approximation in RL**
  - Why needed here: Assumption 3 posits context-dependent MDPs can be represented as linear combinations of basis MDPs. This enables generalization across continuous context space but restricts expressiveness.
  - Quick check question: If contexts $c$ span $\mathbb{R}^d$, can any MDP dynamics be represented? (Answer: Only if dynamics are linear combinations of $d$ basis MDPs.)

## Architecture Onboarding

- **Component map**: Context arrival -> Ridge regression module -> Stochastic projector -> Known/unknown classifier -> Optimistic planner (EVI) -> Interval controller -> Execution

- **Critical path**: Context arrival → classify current state → if unknown: replan via EVI → execute policy until unknown or goal → update regression statistics → repeat. The regression updates and EVI dominate compute.

- **Design tradeoffs**:
  - $\lambda$ regularization: Higher $\lambda$ stabilizes early estimates but slows convergence; paper uses $\lambda = 1$
  - $\ell_{\min}$ perturbation: Adds $\epsilon K T_\star$ regret but enables $\tilde{O}(K^{2/3})$ bound; tradeoff controlled by $\epsilon = |S|^3\sqrt{d^2|A|/K}$
  - Interval granularity: More intervals (finer replanning) reduce per-step error but increase EVI overhead

- **Failure signatures**:
  1. Non-terminating episodes — suggests $\ell_{\min}$ too small or known/unknown threshold misconfigured
  2. Regret scaling as $O(K)$ — confidence sets may be too tight (miss true parameters) or too loose (excessive exploration)
  3. Projection artifacts — if $\hat{P}_m \cdot c$ produces invalid probabilities, check that $c$ is on simplex and projection is correctly implemented

- **First 3 experiments**:
  1. Tabular sanity check: Set $d = |S||A||S|$ (full tabular), verify regret matches non-contextual SSP baselines from Rosenberg et al. (2020)
  2. Context generalization test: Train on contexts $\{e_1, e_2, e_3\}$ (standard basis), evaluate on convex combinations; measure how regret scales with distance from training contexts
  3. Ablation on $\ell_{\min}$: Inject varying $\epsilon$-perturbations and plot regret vs. $\epsilon$ to validate Corollary 4.1.1's tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the LR-CSSP algorithm be extended to handle non-linear context dependencies using kernel methods or deep learning-based representations?
- **Basis in paper**: [explicit] The authors state: "An important future direction is extending LR-CSSP to settings with non-linear context dependencies... Our hope is that future work tackles more general, non-linear, function approximation."
- **Why unresolved**: The current theoretical guarantees rely on linear embeddings for tractability and the application of standard concentration inequalities. Non-linear settings require new regularization or oracle strategies to maintain sample efficiency.
- **What evidence would resolve it**: An algorithm with formal regret guarantees for CSSP that utilizes general function approximation (e.g., neural networks) to model the mapping from context to MDP dynamics.

### Open Question 2
- **Question**: Can the dependence on the expected hitting time $T_\star$ be removed from the regret bound to achieve a "horizon-free" guarantee?
- **Basis in paper**: [inferred] The main result provides a regret bound scaling with $T_\star$. However, the paper cites Tarbouriech et al. (2021), who achieved horizon-free regret for non-contextual SSP. The current CSSP analysis fails to remove this term.
- **Why unresolved**: The context-adversarial nature and the need to ensure episodes terminate likely complicate the concentration of measure required to eliminate $T_\star$, locking it into the current bound.
- **What evidence would resolve it**: A proof derivation or algorithmic modification demonstrating a regret bound of $\tilde{O}(\text{poly}(d, |S|, |A|, B_\star, K))$ that does not scale with $T_\star$.

### Open Question 3
- **Question**: Is a $\tilde{O}(\sqrt{K})$ regret rate achievable for general CSSP without the minimum cost assumption ($\ell_{\min} > 0$)?
- **Basis in paper**: [inferred] Theorem 4.1 achieves $\tilde{O}(\sqrt{K})$ with $\ell_{\min}$, but Corollary 4.1.1 only achieves $\tilde{O}(K^{2/3})$ when $\ell_{\min}$ is removed via perturbation.
- **Why unresolved**: The technique of adding a small artificial cost $\epsilon$ to avoid non-terminating loops inherently degrades the rate. It remains unclear if this is a fundamental limitation of the approach or the problem class.
- **What evidence would resolve it**: An algorithm that navigates zero-cost loops in the contextual setting without perturbation, maintaining a $\tilde{O}(\sqrt{K})$ dependence on the number of episodes $K$.

## Limitations

- The algorithm requires solving an augmented MDP at each interval, which can be computationally expensive for large state and action spaces
- Linear embedding assumption may not capture complex MDP dynamics that are inherently non-linear in context
- Known/unknown threshold parameter $\beta$ requires careful tuning and may be sensitive to hyperparameter choices
- Theoretical guarantees rely on contexts lying on the probability simplex, which may not hold in all applications

## Confidence

- **High confidence** in regret bounds under stated assumptions due to rigorous mathematical analysis and established concentration inequalities
- **Medium confidence** in practical applicability due to computational complexity of solving augmented MDPs and sensitivity to hyperparameters
- **Low confidence** in empirical validation as the paper lacks experiments demonstrating performance on concrete problems

## Next Checks

1. Implement a tabular version of LR-CSSP (setting $d = |S||A||S|$) and verify it matches regret bounds from non-contextual SSP literature
2. Conduct a sensitivity analysis on the known/unknown threshold parameter $\beta$ to identify robust configurations
3. Test the algorithm on a simple contextual SSP with synthetic linear embeddings to validate the theoretical guarantees hold in practice