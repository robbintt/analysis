---
ver: rpa2
title: 'From Sentences to Sequences: Rethinking Languages in Biological System'
arxiv_id: '2507.00953'
source_url: https://arxiv.org/abs/2507.00953
tags:
- structure
- sequence
- recovery
- rifold
- folding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work analyzes the differences between natural and biological
  languages, demonstrating that stochastic-order generation outperforms sequential-order
  generation for biomolecule sequences on inverse folding tasks. The study proposes
  a comprehensive evaluation pipeline incorporating structure recovery metrics (TM-score,
  RMSD) and energy, addressing the limitation of sequence-based metrics in capturing
  semantic fidelity.
---

# From Sentences to Sequences: Rethinking Languages in Biological System

## Quick Facts
- **arXiv ID:** 2507.00953
- **Source URL:** https://arxiv.org/abs/2507.00953
- **Authors:** Ke Liu; Shuaike Shen; Hao Chen
- **Reference count:** 25
- **Primary Result:** Stochastic-order generation outperforms sequential-order generation for biomolecule inverse folding, achieving 3.64% better sequence recovery and 5.57% better Macro-F1 than state-of-the-art methods

## Executive Summary
This work challenges the assumption that sequential-order generation, effective for natural languages, is optimal for biological sequences. The authors propose stochastic-order generation as a superior paradigm for biomolecule inverse folding tasks, where the goal is to design sequences from target structures. Through extensive experiments on RNA and protein datasets, they demonstrate that stochastic-order generation achieves significantly better structural fidelity as measured by TM-score, RMSD, and energy metrics, while also improving sequence recovery. The work highlights a critical limitation in current evaluation practices: high sequence recovery does not guarantee high structural fidelity, necessitating a shift toward structure-aware metrics for biological language models.

## Method Summary
The authors introduce RiFold, a transformer-based model for biomolecule inverse folding that employs stochastic-order autoregressive decoding. Unlike traditional left-to-right generation, stochastic-order decoding selects positions based on a confidence-based strategy, allowing for more flexible and structure-aware sequence generation. The model is trained on RNAsolo for RNA and CATH 4.2 for proteins, with evaluation on RNA-Puzzles and held-out test sets. The authors emphasize the use of structure recovery metrics (TM-score, RMSD, energy) alongside sequence-based metrics (NSR, Macro-F1) to assess semantic fidelity. The method is compared against RDesign, a state-of-the-art baseline, showing substantial improvements in both sequence and structure recovery.

## Key Results
- RiFold achieves 3.64% improvement in sequence recovery (NSR) and 5.57% improvement in Macro-F1 over state-of-the-art methods for RNA inverse folding
- Stochastic-order generation demonstrates significantly better structural fidelity, with improvements in TM-score, RMSD, and energy metrics compared to sequential-order generation
- The study reveals that high sequence recovery does not necessarily correlate with high structural fidelity, challenging the validity of sequence-only benchmarks in biological language modeling

## Why This Works (Mechanism)
The paper demonstrates that biomolecules have distinct structural constraints and dependencies that differ from natural languages. Stochastic-order generation better captures these dependencies by allowing the model to generate positions based on confidence and structural context rather than rigid left-to-right order. This flexibility enables the model to better satisfy long-range interactions and structural motifs critical for functional biomolecules.

## Foundational Learning
- **Autoregressive decoding:** Generation of sequences one element at a time, conditioning on previously generated elements
  - *Why needed:* Standard approach for sequence generation in language models
  - *Quick check:* Verify that sequential decoding proceeds left-to-right with full context from previous positions

- **Structure recovery metrics (TM-score, RMSD):** Quantitative measures of how closely a predicted structure matches a target structure
  - *Why needed:* Sequence metrics alone cannot capture structural fidelity, which is critical for biomolecule function
  - *Quick check:* TM-score > 0.5 and RMSD < 2Å indicate good structural recovery

- **Inverse folding task:** Generating sequences from target structures rather than predicting structures from sequences
  - *Why needed:* Enables design of biomolecules with desired structural and functional properties
  - *Quick check:* Model must learn to reverse the structure-to-sequence mapping

- **RBF distances and quaternion encodings:** Feature representations for capturing geometric and rotational relationships in biomolecular structures
  - *Why needed:* Provides rich structural context beyond simple sequence information
  - *Quick check:* Features should encode both local and global structural information

- **Confidence-based position selection:** Strategy for choosing which position to generate next based on model uncertainty
  - *Why needed:* Enables stochastic-order generation by prioritizing positions where the model is most certain
  - *Quick check:* Positions with lowest entropy should be generated first

- **Energy evaluation:** Assessment of the stability of predicted structures using physical models
  - *Why needed:* Stable, low-energy structures are more likely to be functional
  - *Quick check:* Generated sequences should fold to structures with reasonable energy scores

## Architecture Onboarding

**Component map:** Input featurizer -> Encoder -> Stochastic decoder with confidence-based position selection -> Structure prediction integration -> Output sequence

**Critical path:** Featurization of target structure → Encoder processing → Stochastic position selection → Autoregressive generation → Structure prediction validation

**Design tradeoffs:** Stochastic-order generation provides better structural fidelity but increases computational cost compared to sequential-order decoding; the authors balance this with beam search variants (position-based and type-based) to maintain efficiency

**Failure signatures:** Low structure recovery despite high sequence recovery indicates the model is optimizing for sequence matching rather than structural fidelity; integration failures with E2EFold/ESMFold can cause invalid RMSD/TM-score calculations

**First experiments:**
1. Train RiFold with stochastic-order decoding enabled on a small subset of RNAsolo to verify confidence-based position selection is functioning
2. Compare sequence recovery (NSR) and structural recovery (TM-score) between stochastic and sequential generation on a held-out validation set
3. Test integration with E2EFold structure prediction to ensure backbone atom extraction (Cα for proteins, C3′/C4′ for RNA) is working correctly

## Open Questions the Paper Calls Out
- Does the improved structural fidelity demonstrated by stochastic-order generation translate to superior performance in downstream biochemical functional assays?
- Can the computational efficiency of stochastic-order generation be optimized to match that of sequential decoding without sacrificing structural accuracy?
- To what extent do the biases inherent in the evaluation folding tools (e.g., E2EFold, ESMFold) influence the reported gains in structure recovery?
- Does the stochastic-order generation paradigm yield similar benefits for DNA inverse folding tasks as observed for RNA and proteins?

## Limitations
- The computational cost of stochastic-order generation is higher than traditional sequential decoding
- The evaluation pipeline relies on existing structure prediction tools (e.g., E2EFold, ESMFold), which may introduce biases or noise in the structural recovery scores
- The method's effectiveness for DNA inverse folding tasks remains unexplored
- Further exploration of downstream biochemical or functional metrics would be needed to fully evaluate semantic fidelity in biological contexts

## Confidence
- **High confidence:** The demonstration that sequence recovery metrics (NSR, Macro-F1) do not correlate with structural fidelity (TM-score, RMSD) is empirically well-supported
- **Medium confidence:** The superiority of stochastic-order generation over sequential-order is demonstrated but depends heavily on the specific implementation details that are not fully disclosed
- **Low confidence:** The claim that this represents a "paradigm shift" in biological language model evaluation is interpretive and requires broader community validation

## Next Checks
1. **Architecture Verification:** Request and verify the complete RiFold architecture specification (layer counts, hidden dimensions, attention configurations) to ensure exact reproducibility of the reported improvements
2. **Structure Metric Validation:** Conduct ablation studies testing whether the structure recovery improvements persist when using alternative structure prediction tools beyond ESMFold_v1 and E2EFold
3. **Cross-Domain Generalization:** Test stochastic-order generation on additional biomolecular tasks (protein design, RNA-ligand binding prediction) to evaluate whether the paradigm shift extends beyond inverse folding