---
ver: rpa2
title: 'Forget Less, Retain More: A Lightweight Regularizer for Rehearsal-Based Continual
  Learning'
arxiv_id: '2512.01818'
source_url: https://arxiv.org/abs/2512.01818
tags:
- learning
- continual
- forgetting
- memory
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Information Maximization (IM), a lightweight
  regularization strategy for continual learning that operates exclusively on the
  expected label distribution, making it class-agnostic and applicable across different
  rehearsal-based methods. The IM regularizer encourages prediction diversity and
  confidence, resulting in robust feature representations under distribution shifts.
---

# Forget Less, Retain More: A Lightweight Regularizer for Rehearsal-Based Continual Learning

## Quick Facts
- arXiv ID: 2512.01818
- Source URL: https://arxiv.org/abs/2512.01818
- Reference count: 15
- Improves rehearsal-based continual learning by 1-13% accuracy and reduces forgetting by 4-25% across multiple benchmarks

## Executive Summary
This paper introduces Information Maximization (IM), a lightweight regularization strategy designed to mitigate catastrophic forgetting in rehearsal-based continual learning. Unlike existing regularization methods that depend on specific architectures or memory configurations, IM operates exclusively on the expected label distribution, making it class-agnostic and broadly applicable. The method encourages prediction diversity and confidence, resulting in robust feature representations under distribution shifts. Extensive experiments on image and video datasets demonstrate consistent improvements over baseline rehearsal methods across various memory budgets and numbers of tasks.

## Method Summary
Information Maximization (IM) is a regularization strategy that combines two complementary objectives: minimizing prediction entropy to encourage confident class assignments, and maximizing marginal prediction diversity to prevent bias toward recent task data. The regularizer is applied exclusively to current task samples during training, not to buffer samples, which have already received supervised loss through rehearsal. IM operates with O(n) complexity, making it computationally efficient, and can be integrated with any rehearsal-based continual learning method without dependency on specific model architectures or memory configurations.

## Key Results
- Consistently improves baseline rehearsal methods (ER, DER, DER++, iCaRL) by 1-13% accuracy on Split-CIFAR100 and Split-Tiny ImageNet
- Reduces forgetting by 4-25% across various memory budgets and numbers of tasks
- Generalizes to video continual learning, improving iCaRL's performance by 4-8% on UCF-101 and ActivityNet datasets
- Ablation studies show current task regularization (CT) is superior to buffer-only or combined approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Minimizing prediction entropy encourages confident class assignments without biasing toward recent task data.
- Mechanism: The entropy term Lent = -E[Σ fk(x) log fk(x)] pushes model outputs toward sharper distributions, reinforcing discriminative representations across all learned classes during current task training.
- Core assumption: Confident predictions correlate with stable feature representations that generalize under distribution shift.
- Evidence anchors:
  - [abstract] "IM regularizer encourages prediction diversity and confidence, resulting in robust feature representations under distribution shifts"
  - [section 3.1] "optimizing Lent increases the model's confidence on the prediction"
  - [corpus] Weak direct evidence; related work on Sharpness-Aware Machine Unlearning suggests confidence-related objectives interact with generalization, but mechanism differs
- Break condition: If predictions become overconfident on a subset of classes (collapsed predictions), the diversity term should counteract; if both terms collapse to trivial solutions, regularization fails.

### Mechanism 2
- Claim: Maximizing marginal prediction diversity prevents bias toward recent task classes.
- Mechanism: The diversity term Ldiv = -Σ p̂k log p̂k maximizes entropy of the average prediction vector across a batch, encouraging uniform class coverage in outputs.
- Core assumption: Current task samples are representative enough that enforcing uniform marginal predictions prevents task-specific bias.
- Evidence anchors:
  - [section 3.1] "Ldiv promotes diverse label predictions on fθ"
  - [section 3.1] "encourages the model to make confident predictions across all encountered classes without biasing its predictions towards recent task data"
  - [corpus] No direct corpus validation; LLM unlearning literature suggests prediction-space objectives can suppress or retain knowledge, but causal link to diversity is unexplored
- Break condition: If batch size is too small or class distribution is severely imbalanced, marginal entropy may not provide meaningful diversity signal.

### Mechanism 3
- Claim: Applying IM exclusively to current task samples (not buffer) yields superior retention.
- Mechanism: Buffer samples already receive supervised loss from rehearsal; adding IM there provides redundant signal, whereas current task samples benefit from regularization without additional memory overhead.
- Core assumption: Rehearsal loss already stabilizes representations for buffer samples; current task samples are the primary source of plasticity-instability tradeoff.
- Evidence anchors:
  - [table 3] ER+IM(CT) achieves 33.6-46.2% vs ER+IM(ALL) 25.9-42.7% and ER+IM(BF) 21.7-38.8% on Split-CIFAR100
  - [section 4.2 ablation] "applying the IM loss to the current task (CT) is superior to applying it to the memory/buffer samples only (BF) or to both buffer and current task samples (ALL)"
  - [corpus] No corpus evidence on regularization target selection
- Break condition: If current task samples are extremely limited or unrepresentative, regularizing only on them may fail to provide sufficient signal.

## Foundational Learning

- Concept: **Catastrophic Forgetting**
  - Why needed here: IM is designed to mitigate forgetting; understanding that neural networks overwrite previous task knowledge when learning new tasks is essential context.
  - Quick check question: Can you explain why standard SGD on sequential tasks leads to performance degradation on earlier tasks?

- Concept: **Rehearsal-Based Continual Learning**
  - Why needed here: IM is specifically designed as an add-on to rehearsal methods (ER, DER, DER++); it is not a standalone approach.
  - Quick check question: What is the role of a memory buffer in experience replay, and how does it differ from regularization-only methods like EWC?

- Concept: **Entropy and Information Theory in Loss Functions**
  - Why needed here: IM combines entropy minimization (confidence) with marginal entropy maximization (diversity); understanding these opposing objectives is critical.
  - Quick check question: Why would minimizing point-wise entropy while maximizing marginal entropy lead to better representations?

## Architecture Onboarding

- Component map:
  Input -> ResNet18 classifier -> Softmax predictions -> IM regularizer (Lent + Ldiv) -> Combined loss (CE + buffer CE + λ×RIM)

- Critical path:
  1. Sample batch from current task and batch from memory buffer
  2. Forward pass through classifier
  3. Compute cross-entropy loss on both batches
  4. Compute IM regularizer on current task predictions ONLY (not buffer)
  5. Backpropagate combined loss

- Design tradeoffs:
  - λ balancing: Paper uses λ=0.5; too high may over-regularize, too low loses benefit
  - Target selection: CT (current task) is empirically best; applying to buffer or both degrades performance
  - Batch size: Smaller batches may reduce diversity signal for Ldiv

- Failure signatures:
  - Accuracy drops compared to baseline: Likely λ too high or regularization applied incorrectly to buffer
  - No improvement over baseline: Check that Lent and Ldiv are both computed correctly (signs matter—minimize entropy, maximize diversity)
  - Training instability: Ensure gradient magnitudes are balanced; IM is O(n) but can dominate if unscaled

- First 3 experiments:
  1. Reproduce ER+IM on Split-CIFAR100 with buffer size 1000, comparing ACC and forgetting rate against ER baseline to validate implementation.
  2. Ablate regularization target: Run ER+IM with CT, BF, and ALL settings on a single dataset to confirm CT superiority.
  3. Cross-method test: Apply IM to DER on Split-Tiny ImageNet with varying memory budgets (1000, 2000, 4000) to verify orthogonality claim across rehearsal methods.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the IM regularizer be effectively integrated into rehearsal-free or architectural continual learning methods (e.g., L2P, DualPrompt)?
- **Basis:** [inferred] The paper explicitly limits its scope and experiments to rehearsal-based methods (ER, DER, iCaRL), despite acknowledging the existence of architectural approaches in the Related Work section.
- **Why unresolved:** The regularizer relies on label distributions over a unified output head; architectural methods often isolate tasks using parameters or prompts, making the application of a unified "prediction diversity" constraint non-trivial.
- **What evidence would resolve it:** Empirical results applying IM to transformer-based or prompt-based CL baselines on standard benchmarks.

### Open Question 2
- **Question:** What is the theoretical explanation for why applying IM exclusively to current task (CT) samples outperforms applying it to buffer samples, despite the goal of retaining past knowledge?
- **Basis:** [inferred] The ablation study (Table 3) empirically shows that applying the regularizer to current task data is "superior," but the paper does not provide a theoretical justification for this counter-intuitive finding.
- **Why unresolved:** It is unclear why regularizing the model on new data prevents forgetting better than directly regularizing the stored exemplars of past data.
- **What evidence would resolve it:** A theoretical analysis or gradient interaction study comparing the CT and Buffer regularization strategies.

### Open Question 3
- **Question:** How does the IM regularizer perform in non-visual domains, such as Natural Language Processing (NLP), where data is discrete rather than continuous?
- **Basis:** [inferred] The paper claims the method is "data-agnostic" but validates it only on image and video datasets, which share fundamental visual properties.
- **Why unresolved:** NLP involves high-dimensional discrete token spaces where "confidence" and "diversity" in logits may behave differently due to large vocabulary sizes and distinct feature distributions.
- **What evidence would resolve it:** Experiments on text classification continual learning benchmarks (e.g., document streams) measuring accuracy and forgetting rates.

## Limitations
- Limited ablation on batch size effects, which directly impacts the marginal diversity computation in Ldiv
- No quantification of practical computational overhead beyond O(n) complexity claim
- Video experiments less extensive than image benchmarks, with fewer sensitivity analyses

## Confidence
- **High Confidence**: The core mechanism (entropy minimization + marginal entropy maximization) and its theoretical motivation are well-specified. The empirical improvements on standard benchmarks (Split-CIFAR100, Split-Tiny ImageNet) are clearly demonstrated with statistical rigor.
- **Medium Confidence**: The claim that IM is "lightweight" and "class-agnostic" is supported by the O(n) complexity and architecture independence, but the practical computational overhead in real-world scenarios is not quantified.
- **Medium Confidence**: The generalization to video datasets (UCF-101, ActivityNet) shows promising results, but these experiments are less extensive than the image classification benchmarks, with fewer ablations and sensitivity analyses.

## Next Checks
1. **Target Selection Ablation**: Systematically compare ER+IM with regularization applied to current task (CT), buffer (BF), and both (ALL) on Split-CIFAR100 to independently verify the claim that CT-only is superior across all memory budgets.
2. **Batch Size Sensitivity**: Evaluate how different batch sizes (16, 32, 64) affect the IM regularizer's performance, particularly the Ldiv term's ability to capture meaningful diversity signals, as batch size directly impacts the marginal prediction computation.
3. **Cross-Architecture Generalization**: Test IM beyond ResNet18 on architectures with different design philosophies (e.g., EfficientNet, Vision Transformer) to validate the claim of architecture independence and assess whether the regularization strength (λ=0.5) requires tuning across architectures.