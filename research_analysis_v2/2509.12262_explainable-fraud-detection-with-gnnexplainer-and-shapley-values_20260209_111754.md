---
ver: rpa2
title: Explainable Fraud Detection with GNNExplainer and Shapley Values
arxiv_id: '2509.12262'
source_url: https://arxiv.org/abs/2509.12262
tags:
- fraud
- node
- detection
- graph
- edge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The project addresses the challenge of explaining fraud predictions
  in machine learning models for financial fraud detection, aiming to provide clear
  and understandable explanations to fraud analysts. The core method combines a graph
  neural network (GNN) based fraud detector with a composite explainer that uses GNNExplainer
  to generate subgraphs and Shapley values to quantify the importance of node features
  and edges.
---

# Explainable Fraud Detection with GNNExplainer and Shapley Values

## Quick Facts
- arXiv ID: 2509.12262
- Source URL: https://arxiv.org/abs/2509.12262
- Reference count: 32
- Primary result: Achieved accuracy 0.9461, AP 0.9370, AUC 0.9640 on synthetic fraud dataset with interpretable explanations

## Executive Summary
This paper presents xFraud, a graph neural network-based fraud detector combined with a composite explainer using GNNExplainer and Shapley values. The approach addresses the challenge of explaining fraud predictions in machine learning models for financial fraud detection, aiming to provide clear and understandable explanations to fraud analysts. The xFraud detector constructs a heterogeneous graph with node types (Transaction, Account, User, Country) and edge types (Sent_To, Executed_In, Transferred_By), then uses attention-weighted message passing to capture fraud signals through typed entity relationships.

The composite explainer generates both visual subgraphs (via GNNExplainer) and numerical importance scores (via Shapley values) to explain predictions. Tested on a synthetic J.P.Morgan payment dataset, the model achieved high performance metrics while successfully generating interpretable visualizations that matched known fraud patterns. The approach demonstrates effectiveness in supporting fraud investigations by providing both model predictions and actionable explanations for why those predictions were made.

## Method Summary
The method combines a heterogeneous graph neural network (xFraud detector) with a composite explainer using GNNExplainer and Shapley values. The xFraud detector constructs a heterogeneous graph from transaction data, where nodes represent transactions, accounts, users, and countries, connected by typed edges indicating relationships like "Sent_To" and "Executed_In". A heterogeneous convolution layer with self-attention propagates information through the graph, computing attention scores between nodes to capture contextual fraud signals. The detector outputs risk scores for each transaction.

The composite explainer generates explanations through two components: GNNExplainer identifies prediction-critical subgraphs by maximizing mutual information between the explanation subgraph and the original prediction, while Monte-Carlo approximated Shapley values quantify the importance of node features and edge presence. The approach provides both visual explanations (subgraphs showing which entities and relationships matter) and numerical explanations (Shapley values indicating positive/negative correlation with fraud predictions).

## Key Results
- xFraud detector achieved accuracy of 0.9461, average precision of 0.9370, and AUC of 0.9640 on synthetic dataset
- Composite explainer successfully generated interpretable visualizations matching known ATO fraud patterns
- Explanations included both visual subgraphs and Shapley value-based importance scores
- Model outperformed GAT and GEM baselines on the same dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Heterogeneous graph attention captures fraud signals by propagating transaction context through typed entity relationships
- Mechanism: The xFraud detector constructs a heterogeneous graph with node types (Transaction, Account, User, Country) and edge types (Sent_To, Executed_In, Transferred_By). Self-attention computes Query/Key/Value vectors for each node, then aggregates neighborhood messages weighted by attention scores. This allows the model to learn that, for example, rapid successive transactions from an account to unusual countries are indicative of ATO fraud.
- Core assumption: Fraud patterns manifest through relational structures (e.g., shared accounts, transaction sequences) rather than solely through transaction-level features.
- Evidence anchors:
  - [abstract] "The core method combines a graph neural network (GNN) based fraud detector with a composite explainer..."
  - [section 4.3] Defines node types and edge types; Figure 12 shows sample subgraph structure
  - [corpus] "Detecting Credit Card Fraud via Heterogeneous Graph Neural Networks with Graph Attention" supports HGNN approach but limited comparative evidence
- Break condition: If fraud patterns are primarily captured by individual transaction features without relational context, the heterogeneous graph construction adds computational overhead without performance gain.

### Mechanism 2
- Claim: GNNExplainer identifies prediction-critical subgraphs by maximizing mutual information between the explanation subgraph and the original prediction
- Mechanism: GNNExplainer learns an edge mask and feature mask that maximize mutual information MI(Y, (Gs, Xs)) = H(Y) - H(Y|G=Gs, Xs). The output is a weighted subgraph indicating which edges and features contributed most to the node's prediction.
- Core assumption: The prediction-critical computation graph is a subset of the input graph; pruning less relevant edges preserves prediction semantics.
- Evidence anchors:
  - [section 3.2.4] "GNNExplainer generates a subgraph... such that the shared information between the two graphs is maximized"
  - [section 4.5.1] "GNNExplainer takes in the trained xFraud detector model, the node to explain v and the input subgraph relevant to the node v"
  - [corpus] No direct corpus validation of GNNExplainer on financial fraud; related work focuses on other XAI methods
- Break condition: If mutual information scores produce dense/uninformative subgraphs (e.g., all edges have similar importance), the explanation lacks actionable specificity.

### Mechanism 3
- Claim: Shapley value approximation provides directional importance (positive/negative correlation) that GNNExplainer's mutual information cannot
- Mechanism: Monte-Carlo sampling approximates Shapley values for node features and edge missingness. For feature j, sample random nodes z and compute marginal contribution φ = f(x+j) - f(x-j). Positive Shapley values indicate the feature pushes predictions toward fraud; negative values push toward non-fraud.
- Core assumption: The Shapley value approximation with finite samples converges to true feature contributions; the model's prediction function f is stable under feature perturbations.
- Evidence anchors:
  - [section 4.5.2-4.5.3] Detailed algorithms for Node Feature Shapley and Edge Missingness Shapley
  - [section 5.2] "dTime has a high negative correlation with the current node's prediction... consistent with our analysis that fraud transactions tend to be executed rapidly"
  - [corpus] "From Abstract to Actionable: Pairwise Shapley Values for Explainable AI" discusses Shapley limitations but not specific to GNNs
- Break condition: If the number of Monte-Carlo samples M is insufficient, Shapley estimates have high variance, leading to inconsistent explanations across runs.

## Foundational Learning

- Concept: **Message Passing in Graph Neural Networks**
  - Why needed here: The xFraud detector's core operation is aggregating neighbor information via attention-weighted message passing. Understanding how node representations update across layers is essential for debugging prediction flows.
  - Quick check question: Given a target transaction node with 3 account neighbors, what happens to the transaction's representation after one convolution layer?

- Concept: **Shapley Values and Axiomatic Fairness**
  - Why needed here: The composite explainer relies on Shapley value properties (efficiency, symmetry, linearity) to fairly distribute prediction credit among features/edges.
  - Quick check question: If feature A has Shapley value +0.3 and feature B has Shapley value -0.1, what does this tell you about their contribution to a fraud prediction?

- Concept: **Attention Mechanism (Query/Key/Value)**
  - Why needed here: The heterogeneous convolution layer computes attention via Q/K/V projections. Understanding attention weights is necessary to trace which neighbors influence predictions.
  - Quick check question: In multi-head attention with 4 heads, how are the individual head outputs combined to form the final attention vector?

## Architecture Onboarding

- Component map:
  Graph Construction (Neo4j) -> xFraud Detector (PyTorch Geometric) -> Composite Explainer (GNNExplainer + Shapley) -> Visualization

- Critical path:
  1. Data preprocessing: Add dAmount, dTime features; remove Line of Business/Sector
  2. Graph construction: Extract nodes from tabular rows, create edges by type
  3. Model training: Cross-entropy loss, monitor AP/AUC (not accuracy due to imbalance)
  4. Explanation generation: Run GNNExplainer + Shapley sampling on target node
  5. Visualization assembly: Combine subgraph, SHAP charts, and raw transaction data

- Design tradeoffs:
  - **Unified vs. type-specific weight matrices**: xFraud uses shared weights across node/edge types (unlike HGT) for better performance on this dataset, but may lose expressiveness on more complex graphs
  - **Mutual information vs. Shapley for importance**: MI is faster but direction-agnostic; Shapley provides directionality at computational cost O(M × forward passes)
  - **Subgraph size limits**: Max depth 6, max width 16 balances interpretability vs. context completeness

- Failure signatures:
  - Dense explanation subgraphs (all edges similar weight) → GNNExplainer not discriminating; check learning rate or increase mask regularization
  - High variance in Shapley values across runs → Increase Monte-Carlo samples M
  - Model predicts all non-fraud → Check class imbalance handling; verify dAmount/dTime features are correctly computed
  - Explanations conflict with domain knowledge → Verify feature engineering assumptions; re-examine dTime/dAmount distributions

- First 3 experiments:
  1. **Baseline comparison**: Reproduce GAT and GEM results on the same graph; confirm xFraud's AP/AUC improvement is due to type-aware attention, not random seed variance
  2. **Ablation on engineered features**: Train xFraud with/without dTime and dAmount; quantify contribution of each to AP (reported ΔAP = 0.0158 from adding both)
  3. **Explainer consistency check**: Run composite explainer on the same transaction 5 times with different random seeds; measure variance in Shapley values and subgraph overlap to assess explanation stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can incorporating temporal data into the xFraud detector improve performance compared to the static graph implementation?
- Basis in paper: [explicit] Section 6.3 states, "incorporating temporal data into the model has the potential to improve the detector performance as well as provide another source of data for the explainer."
- Why unresolved: The current xFraud detector ignores temporal encoding to view all transactions simultaneously, and the authors did not test temporal graph networks in this study.
- What evidence would resolve it: Experimental results comparing the current static xFraud model against a temporal version (e.g., using T-GNNExplainer or DyHGN) on the same dataset.

### Open Question 2
- Question: How can explanation subgraphs be effectively generated for temporal graph networks in fraud detection?
- Basis in paper: [explicit] Section 6.3 notes that DyHGN does not use GNNExplainer and posits "significant challenges preventing them from generating explanation subgraphs as for temporal graph networks."
- Why unresolved: While T-GNNExplainer exists, the authors note it lacks comprehensive testing and visualization for fraud-specific event subgraphs.
- What evidence would resolve it: A method that successfully generates interpretable, event-specific subgraphs from dynamic heterogeneous graphs, validated on real-world fraud data.

### Open Question 3
- Question: How can conflicts between the composite explainer's components be resolved without relying on manual analyst intervention?
- Basis in paper: [explicit] Section 6.2 identifies the limitation that "conflicts existing among [the visualization components]" are possible, which must currently be resolved by the analyst.
- Why unresolved: The composite explainer aggregates GNNExplainer subgraphs and Shapley values, but the paper provides no mechanism to detect or reconcile contradictory signals between them.
- What evidence would resolve it: An automated logic layer or aggregation metric that flags or filters contradicting importance weights between the subgraph structure and feature importance scores.

### Open Question 4
- Question: Does the composite explainer maintain effectiveness when applied to complex, real-world fraud patterns rather than synthetic data?
- Basis in paper: [explicit] Section 6.2 notes the "synthetic J.P.Morgan dataset is quite simplistic" and may not capture the complex money movements of real-world schemes.
- Why unresolved: The model was tested on simulated data where fraud patterns are consistent; real-world data involves obfuscation and complex entity types not present in the training set.
- What evidence would resolve it: Evaluation of the xFraud detector and explainer on a real-world financial dataset involving obfuscated ATO or money laundering schemes.

## Limitations
- The synthetic dataset may not capture real-world fraud complexity and obfuscation techniques
- Computational cost of Shapley value approximation limits scalability for large graphs
- Performance metrics based on proprietary synthetic data limit generalizability
- The approach assumes fraud patterns are detectable through heterogeneous graph structures

## Confidence
- High confidence: xFraud detector architecture design and heterogeneous graph construction methodology
- Medium confidence: Explanation quality and pattern matching, given synthetic dataset constraints
- Low confidence: Scalability and performance on real-world fraud data without access to proprietary datasets

## Next Checks
1. **Real-world data validation**: Test xFraud and composite explainer on publicly available fraud datasets (e.g., IEEE-CIS Fraud Detection) to verify performance and explanation quality beyond synthetic data
2. **Ablation study**: Systematically remove components (GNNExplainer, Shapley values, heterogeneous attention) to quantify each component's contribution to both detection accuracy and explanation quality
3. **Human-in-the-loop evaluation**: Conduct user studies with fraud analysts to assess whether explanations generated by the composite explainer actually improve investigation efficiency and fraud detection rates in practice