---
ver: rpa2
title: Sentiment Analysis in Twitter Social Network Centered on Cryptocurrencies Using
  Machine Learning
arxiv_id: '2501.09777'
source_url: https://arxiv.org/abs/2501.09777
tags:
- tweets
- sentiment
- learning
- used
- words
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed sentiment analysis models for Persian tweets
  about cryptocurrencies using machine learning and deep learning methods. The authors
  collected 4,000 Persian tweets, preprocessed them using NLP techniques, and classified
  sentiments (positive, negative, neutral) using classical ML algorithms (KNN, SVM,
  AdaBoost), deep learning (LSTM with FastText embeddings), and BERT.
---

# Sentiment Analysis in Twitter Social Network Centered on Cryptocurrencies Using Machine Learning

## Quick Facts
- **arXiv ID**: 2501.09777
- **Source URL**: https://arxiv.org/abs/2501.09777
- **Reference count**: 17
- **Primary result**: BERT achieved highest accuracy of 83.50% for Persian cryptocurrency tweet sentiment analysis

## Executive Summary
This study presents a comprehensive sentiment analysis framework for Persian tweets about cryptocurrencies using multiple machine learning and deep learning approaches. The researchers collected 4,000 Persian tweets and applied various classification models including KNN, SVM, AdaBoost, LSTM with FastText embeddings, and BERT. The experiments demonstrated that deep learning models, particularly BERT, significantly outperformed traditional machine learning methods in classifying tweet sentiments as positive, negative, or neutral. The findings highlight the effectiveness of transformer-based models for sentiment analysis in low-resource languages like Persian within the cryptocurrency domain.

## Method Summary
The research employed a systematic approach to sentiment analysis, starting with the collection of 4,000 Persian tweets related to cryptocurrencies. The tweets underwent preprocessing using standard NLP techniques including tokenization, normalization, and cleaning. Multiple classification approaches were implemented: classical machine learning algorithms (KNN, SVM, AdaBoost), deep learning with LSTM using FastText embeddings, and BERT. The models were trained and evaluated on the same dataset to ensure fair comparison. Performance was measured using accuracy as the primary metric, with BERT achieving the highest score of 83.50% among all tested models.

## Key Results
- BERT achieved the highest accuracy of 83.50% for sentiment classification
- Classical ML methods (KNN, SVM, AdaBoost) achieved accuracies ranging from 74.50% to 79.40%
- LSTM with FastText embeddings achieved 82.32% accuracy
- Deep learning approaches consistently outperformed traditional machine learning methods
- BERT demonstrated superior performance in handling Persian cryptocurrency tweet sentiment

## Why This Works (Mechanism)
The success of BERT in this sentiment analysis task stems from its transformer architecture's ability to capture contextual relationships in text. Unlike traditional ML methods that rely on hand-crafted features, BERT uses bidirectional attention mechanisms to understand word meanings based on their surrounding context. This is particularly valuable for Persian, a morphologically rich language where word order and context significantly impact meaning. The model's pre-training on large corpora allows it to understand nuanced language patterns, making it especially effective for sentiment classification in the complex domain of cryptocurrency discussions where sentiment can be subtle and context-dependent.

## Foundational Learning
- **Transformer Architecture**: Essential for understanding BERT's bidirectional context capture - quick check: verify attention mechanism implementation
- **Persian NLP Preprocessing**: Critical for handling morphological complexity and right-to-left script - quick check: confirm tokenization preserves meaning
- **FastText Embeddings**: Provides subword information crucial for morphologically rich languages - quick check: validate embedding coverage for cryptocurrency terminology
- **Sentiment Classification Metrics**: Accuracy alone insufficient; need precision, recall, F1 for comprehensive evaluation - quick check: calculate class-wise performance
- **Cryptocurrency Domain Knowledge**: Understanding specific terminology and sentiment patterns in crypto discussions - quick check: review common sentiment expressions in dataset

## Architecture Onboarding
- **Component Map**: Raw Tweets -> Preprocessing -> Feature Extraction -> Model Training -> Evaluation -> Performance Metrics
- **Critical Path**: Data Collection -> Preprocessing (tokenization, normalization) -> Model Training (BERT/LSTM/ML) -> Evaluation (accuracy) -> Comparison
- **Design Tradeoffs**: Dataset size (4,000 tweets) vs model complexity; Persian language resource limitations vs model performance; computational cost of BERT vs simpler models
- **Failure Signatures**: Overfitting on small dataset; poor handling of Persian morphological variations; inability to capture domain-specific cryptocurrency sentiment; class imbalance affecting performance
- **First Experiments**: 1) Train baseline KNN with TF-IDF features; 2) Implement LSTM with FastText embeddings; 3) Fine-tune pre-trained BERT model for Persian cryptocurrency sentiment

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Small dataset size of only 4,000 Persian tweets limits generalizability to broader Twitter discourse
- Exclusive focus on Persian language makes results unclear for other languages or cultural contexts
- Lack of detailed evaluation metrics beyond accuracy (precision, recall, F1-score not discussed)
- Preprocessing steps may introduce bias or information loss, particularly for Persian text normalization
- Limited discussion of class imbalance and its impact on model performance across sentiment categories

## Confidence
- **High confidence** in BERT outperforming classical ML methods (accuracy: 83.50% vs 74.50-79.40%)
- **Medium confidence** in LSTM with FastText performance (82.32%) given proximity to BERT and small dataset size
- **Medium confidence** in generalizability of findings to larger or multilingual datasets

## Next Checks
1. Test model performance on a larger dataset of Persian cryptocurrency tweets (minimum 20,000 tweets) to assess scalability and robustness
2. Conduct cross-language validation by applying the best-performing models to English cryptocurrency tweets to evaluate language transfer capabilities
3. Perform ablation studies on preprocessing steps to quantify their impact on model performance and identify potential information loss points