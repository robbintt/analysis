---
ver: rpa2
title: 'Joint Extraction Matters: Prompt-Based Visual Question Answering for Multi-Field
  Document Information Extraction'
arxiv_id: '2503.16868'
source_url: https://arxiv.org/abs/2503.16868
tags:
- fields
- extraction
- joint
- separate
- total
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares separate versus joint field extraction in prompt-based
  VQA for document information extraction. While existing approaches typically query
  each field in isolation, this work evaluates whether extracting multiple fields
  jointly in a single prompt improves accuracy, particularly for fields with strong
  numeric or contextual dependencies.
---

# Joint Extraction Matters: Prompt-Based Visual Question Answering for Multi-Field Document Information Extraction

## Quick Facts
- arXiv ID: 2503.16868
- Source URL: https://arxiv.org/abs/2503.16868
- Reference count: 11
- Primary result: Joint field extraction consistently outperforms separate extraction for prompt-based VQA document information extraction, especially for numerically dependent fields.

## Executive Summary
This paper investigates whether extracting multiple fields jointly in a single prompt improves accuracy compared to the standard approach of querying each field separately in prompt-based visual question answering for document information extraction. Experiments across three vision-language models (Qwen2-VL-2B, Llama-3.2-11B, GPT-4o) and three real-world datasets show that joint extraction consistently outperforms separate extraction, with the largest gains observed for numerically intertwined fields like tax and subtotal. The authors introduce a regression-based dependency metric showing that fields with high R² benefit most from joint extraction, and document-level accuracy remains robust as the number of queried fields increases under joint extraction.

## Method Summary
The paper compares separate versus joint field extraction strategies using prompt-based VQA with three vision-language models. Separate extraction uses one prompt per field (e.g., "extract the Subtotal"), while joint extraction combines multiple fields into a single prompt (e.g., "extract the Subtotal, Tax, and Total"). Experiments use zero-shot prompting on three datasets: CORDv2 (receipts), SROIE Task 3 (receipts), and FUNSD-VQA (forms). The authors introduce an R² dependency metric computed via multiple linear regression to identify field groups that benefit most from joint extraction. Document-level accuracy requires all fields correct, while field-level accuracy evaluates per-field performance.

## Key Results
- Joint extraction consistently outperforms separate extraction across all models and datasets
- Largest accuracy gains occur for numerically intertwined fields with high R² dependencies (e.g., tax and subtotal)
- Document-level accuracy remains robust as field count increases under joint extraction, while separate extraction shows steeper decline
- Fields with low R² dependencies show negligible or negative gains from joint extraction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint extraction improves accuracy for fields with strong numeric dependencies by enabling cross-field consistency reasoning
- Mechanism: When fields are queried together, the model can verify extracted values against implicit relationships (e.g., tax ≈ subtotal × rate), providing internal consistency checks
- Core assumption: VLMs encode numeric relationships from pre-training and can apply them during inference when multiple values are visible in the same context
- Evidence anchors: Tax field with high R² (0.99) to Subtotal/Total shows +0.20 improvement under joint extraction; same field with low-R² predictors shows -0.01 change

### Mechanism 2
- Claim: Multi-field prompts reduce confusion among fields with similar surface forms by forcing the model to distinguish them within a single context
- Mechanism: Separate queries for visually similar numeric fields may cause the model to latch onto the most salient number; joint queries require the model to assign each value to its correct field label
- Core assumption: VLMs have sufficient attention capacity to track multiple field-value mappings simultaneously
- Evidence anchors: Separate extraction produces identical wrong values (43.636 repeated for subtotal, tax, total, cash); joint extraction correctly distinguishes all values

### Mechanism 3
- Claim: Joint extraction maintains accuracy better as the number of queried fields increases, while separate extraction suffers from compounding errors
- Mechanism: Document-level accuracy requires all fields correct; separate extraction multiplies independent failure probabilities while joint extraction allows the model to maintain global context
- Core assumption: The model's joint extraction performance degrades more slowly with field count than the product of per-field separate extraction accuracies
- Evidence anchors: Document-level accuracy shows joint extraction robust while separate extraction shows pronounced decline for larger field sets

## Foundational Learning

- Concept: **Vision-Language Models (VLMs) for document understanding**
  - Why needed here: The entire approach assumes familiarity with models like Qwen2-VL, Llama-3.2-Vision, and GPT-4o that process images and text jointly
  - Quick check question: Can you explain how a VLM differs from applying OCR + LLM separately?

- Concept: **Regression-based dependency quantification (R²)**
  - Why needed here: The paper uses R² from multiple linear regression to predict which field groups benefit from joint extraction
  - Quick check question: Given fields x, y, z, if R² = 0.95 for predicting x from y and z, what does this imply about their relationship?

- Concept: **Zero-shot vs few-shot prompting**
  - Why needed here: Main experiments use zero-shot to isolate model capacity; understanding this distinction is critical for interpreting results
  - Quick check question: Why might few-shot examples introduce confounds when comparing extraction strategies?

## Architecture Onboarding

- Component map: Document Image → Prompt Engineering (Separate | Joint) → VLM (Qwen/Llama/GPT-4o) → Field Extraction → Accuracy Evaluation (Document-level | Field-level)
- Critical path: Identify target fields → Compute pairwise/group dependency metrics (R²) using labeled data → Group high-dependency fields into joint prompts → Query independent fields separately
- Design tradeoffs:
  - Latency vs. accuracy: Joint extraction reduces API calls but increases prompt length; separate extraction requires N calls for N fields
  - Prompt complexity vs. field independence: Over-grouping independent fields adds prompt tokens without accuracy gain
  - Model capacity: Smaller VLMs (2B parameters) show larger relative gains from joint extraction than larger models (GPT-4o near ceiling)
- Failure signatures:
  - No improvement from joint extraction → fields likely have low R² dependency; check with regression analysis
  - Joint extraction worse than separate → prompt may be too complex or fields actively interfere
  - High variance across documents → inconsistent document layouts or field naming
- First 3 experiments:
  1. Run separate vs. joint extraction on 50-100 sample documents from your target type; measure document-level and field-level accuracy
  2. For key numeric fields, fit x = c₁y + c₂z + b and compute R²; rank fields by dependency strength
  3. Incrementally increase fields per joint prompt (2→4→6) and plot accuracy curve; identify where joint extraction begins to degrade

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do latency, memory usage, and computational overhead compare between joint and separate extraction strategies as the number of queried fields scales?
- Basis in paper: The Limitations section states the question of inference time remains and a more extensive study of latency, memory usage, and scalability remains outside the scope
- Why unresolved: The authors acknowledge this trade-off but did not measure it—joint extraction uses fewer prompts but longer ones; separate extraction uses more prompts but shorter ones
- What evidence would resolve it: Systematic benchmarking of inference time, GPU memory, and tokens-per-second across varying field counts (2–20+) for both strategies

### Open Question 2
- Question: What is the optimal heuristic for grouping fields in joint extraction prompts—by dependency strength (R²), surface form similarity, semantic category, or another criterion?
- Basis in paper: The paper demonstrates that joint extraction helps most for high-R² field pairs, but does not provide a principled method for deciding which fields to group together in multi-field prompts
- Why unresolved: Experiments grouped all numeric fields together or all dataset fields together without exploring alternative grouping strategies or their relative merits
- What evidence would resolve it: Ablation studies comparing different grouping heuristics on field-level accuracy, especially for documents with mixed numeric and textual fields

### Open Question 3
- Question: Does the regression-based R² dependency metric generalize to non-numeric fields where relationships are semantic (e.g., sender address ↔ sender name) rather than arithmetic?
- Basis in paper: The dependency analysis was conducted only on CORDv2 numeric triplets; applicability to fields like dates, addresses, or categorical labels remains untested
- Why unresolved: Linear regression assumes numeric relationships; semantic dependencies may require different quantification methods
- What evidence would resolve it: Extending the dependency metric to textual/semantic field pairs (e.g., using embedding similarity or textual entailment) and correlating with joint extraction gains

## Limitations
- Prompt design ambiguity: Exact prompt templates are not fully specified, making reproduction difficult
- Model-specific behavior: Analysis limited to three VLMs; mechanisms may not generalize uniformly across model families or scales
- Dependency metric constraints: R² regression assumes linear relationships and may miss non-linear inter-field dependencies

## Confidence
- High confidence: Joint extraction consistently outperforms separate extraction across datasets and models, particularly for numerically dependent fields
- Medium confidence: The proposed mechanism of inter-field consistency checking and surface-form disambiguation is plausible but not directly validated
- Low confidence: The claim that joint extraction maintains accuracy better as field count increases assumes no upper bound on prompt length or model context window limitations

## Next Checks
1. Systematically vary prompt wording, output format instructions, and field ordering in joint prompts to measure sensitivity of the observed gains
2. Apply mutual information or correlation networks to detect non-linear field relationships and compare predictive power against R² for identifying beneficial joint extraction groups
3. Test joint extraction with larger field sets (10+ fields) to identify the point where prompt length or context window constraints negate the benefits of joint extraction