---
ver: rpa2
title: 'State-of-the-art Small Language Coder Model: Mify-Coder'
arxiv_id: '2512.23747'
source_url: https://arxiv.org/abs/2512.23747
tags:
- data
- code
- language
- generation
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Mify-Coder, a 2.5B-parameter code model trained
  on 4.2T tokens using a compute-optimal strategy. It demonstrates that compact models
  can match frontier-grade performance in code generation and agent-driven workflows
  by combining high-quality curated data, synthetic data augmentation, and enterprise-grade
  safety alignment within a single continuous training pipeline.
---

# State-of-the-art Small Language Coder Model: Mify-Coder

## Quick Facts
- arXiv ID: 2512.23747
- Source URL: https://arxiv.org/abs/2512.23747
- Reference count: 24
- Key outcome: Mify-Coder achieves competitive code generation performance with only 2.5B parameters through compute-optimal training and curated data composition

## Executive Summary
This paper introduces Mify-Coder, a 2.5B-parameter code model trained on 4.2T tokens using a compute-optimal strategy. It demonstrates that compact models can match frontier-grade performance in code generation and agent-driven workflows by combining high-quality curated data, synthetic data augmentation, and enterprise-grade safety alignment within a single continuous training pipeline. Mify-Coder achieves strong results on standard benchmarks (HumanEval, MBPP, BFCL) while offering superior deployment flexibility and enabling inference on standard desktop environments without specialized hardware. The study shows that principled data curation and compute discipline enable smaller models to deliver competitive accuracy, efficiency, and safety compliance, challenging the notion that scale is the sole determinant of capability.

## Method Summary
Mify-Coder employs a compute-optimal training strategy using a 2.5B-parameter base model with mixed precision (BF16) and GQA. The training pipeline consists of two stages: CPT (Continual Pretraining) with next-token prediction and FIM (25% ratio) on a 78:12:10 code:text:math mix for 3 epochs over ~200B tokens, followed by SFT (Supervised Fine-Tuning) with progressive scaling from 300K to 1.1M samples. The data pipeline includes rigorous curation with MinHash deduplication, n-gram decontamination, and LLM-as-judge validation. Safety alignment integrates during SFT with a 1:4 harmful-to-general sample ratio. The model uses extended BPE tokenization with language-aware separators and achieves efficient deployment through FP8 quantization.

## Key Results
- Achieves 53.66% pass@1 on HumanEval and 91.21% on MBPP with only 2.5B parameters
- Outperforms larger models on function-calling benchmarks while maintaining efficient deployment
- Demonstrates that the 78:12:10 data composition ratio improves code generation performance compared to code-only training
- Shows FP8 quantization enables 8,658 tokens/sec throughput on H100 with minimal accuracy loss (-1.35%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A balanced 78:12:10 (code:text:math) data composition ratio improves code generation benchmarks compared to code-only training.
- Mechanism: Mathematical reasoning data strengthens logical problem-solving required for algorithmic tasks, while text data improves natural language instruction understanding—both complement pure code pretraining.
- Core assumption: The observed benchmark improvements (Table 4) generalize beyond the specific evaluation suites used.
- Evidence anchors: [abstract] "combining high-quality curated sources with synthetic data generated through agentically designed prompts"; [section 4.2.1] Table 4 shows the 78:12:10 ratio outperforming 92.7:4.9:2.4 on CruxEval-O (17.50% vs 12.38%) and PandasEval (37.62% vs 36.63%)

### Mechanism 2
- Claim: Synthetic data generation with LLM-as-judge validation enables effective scaling of instruction-tuning data without proportional quality degradation.
- Mechanism: Multi-stage validation (semantic deduplication, format checks, benchmark decontamination, LLM judging) filters low-quality synthetic outputs while preserving diversity through seed-grounded and seedless generation strategies.
- Core assumption: LLM judges (Qwen2.5-Coder-70B, Llama3-70B, Claude 3.5 Sonnet) accurately assess code quality and correctness.
- Evidence anchors: [abstract] "synthetic data augmentation... refined iteratively using enterprise-grade evaluation datasets"; [section 3.5.2] Details multi-stage validation pipeline; [section 4.3.2] Table 6 shows progressive scaling from baseline to 1M samples improving MBPP from 42.71% to 69.10%

### Mechanism 3
- Claim: Integrating safety alignment during SFT (rather than post-hoc) with a 1:4 harmful-to-general sample ratio reduces harmful outputs while preserving helpfulness.
- Mechanism: Calibrated exposure to harmful examples teaches detection without overfitting to refusal behavior; safety exemplars are mixed throughout training rather than applied as a separate stage.
- Core assumption: The AIR-Bench and CyberSecEval benchmarks accurately measure real-world safety properties.
- Evidence anchors: [abstract] "enterprise-grade safety alignment within a single continuous training pipeline"; [section 4.4] Describes Alignment@Scale platform; [section 6] Figures 3-4 show safety benchmark comparisons

## Foundational Learning

- Concept: **Fill-in-the-Middle (FIM) Training**
  - Why needed here: FIM enables structural code completion by training on infilling tasks, which standard next-token prediction doesn't address.
  - Quick check question: Can you explain why predicting missing code given both prefix and suffix context differs from left-to-right generation?

- Concept: **Continual Pretraining (CPT) vs. Supervised Fine-Tuning (SFT)**
  - Why needed here: The paper's architecture separates broad code intelligence acquisition (CPT) from task-specific alignment (SFT), with distinct data mixes and objectives for each.
  - Quick check question: What types of objectives and data would you use in CPT versus SFT for a code model?

- Concept: **Quantization-Aware Deployment**
  - Why needed here: The paper emphasizes FP8 quantization for practical deployment; understanding precision-accuracy tradeoffs is essential for reproduction.
  - Quick check question: What accuracy degradation is acceptable when reducing model precision from FP16 to FP8?

## Architecture Onboarding

- Component map:
  - Base model: Mify-2.5B (2.5B parameters, 64K BPE vocabulary)
  - CPT stage: Next-token prediction + FIM (25% ratio) on 78:12:10 code:text:math mix, 3 epochs
  - SFT stage: Task-specific instruction tuning with progressive scaling (300K→1M samples)
  - Tokenizer: Extended BPE with language-aware separators, FIM markers, and reasoning tokens
  - Quantization: FP8 via TensorRT-LLM; GGUF for CPU inference

- Critical path:
  1. Data curation pipeline (Section 3.1-3.4): License compliance → quality filtering → deduplication → decontamination
  2. CPT training (Section 4.2): 50B code-only baseline → ablation studies → full CPT with finalized mix
  3. SFT training (Section 4.3): Task-specific ablations → curated + synthetic scaling → function-calling emphasis → safety alignment
  4. Evaluation: HumanEval, MBPP, BFCL-v2, AIR-Bench, CyberSecEval

- Design tradeoffs:
  - Model size (2.5B) vs. capability: Accepts lower HumanEval scores (53.66%) for deployment flexibility and MBPP strength (91.21%)
  - FIM ratio (25%): Intermediate value trades off between 15% and 30% configurations showing minimal difference
  - FP8 quantization: Accepts -1.35% accuracy change for 8,658 tokens/sec throughput on H100

- Failure signatures:
  - Over-refusal on benign prompts: Indicates excessive harmful sample ratio in safety training
  - HumanEval+ significantly lower than HumanEval: Suggests weak edge-case handling (observed in Llama-3.3-70B at 42.68%)
  - Function-calling schema inconsistency: Requires unified JSON schema standardization

- First 3 experiments:
  1. Replicate the 78:12:10 data composition ablation on a smaller token budget (~10B tokens) to validate the ratio before full training.
  2. Test FIM ratios (15%, 25%, 30%) on a code-only baseline to confirm the 25% selection is optimal for your target languages.
  3. Evaluate FP8 vs. 8-bit AWQ quantization on your target hardware to verify the paper's throughput claims (Table 9) generalize.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can reinforcement learning with verifiable rewards (RLVR) utilizing automated test suites successfully resolve current limitations in complex algorithmic reasoning for compact models?
- Basis in paper: [Explicit] Section 8 states the intent to evolve the alignment strategy toward RLVR to guarantee correctness and address identified limitations in complex algorithmic reasoning.
- Why unresolved: The current model relies on RLHF (Section 4.4), and the authors explicitly identify complex reasoning as a current limitation to be addressed by this future method.
- What evidence would resolve it: Empirical results showing improved pass rates on complex, multi-step algorithmic benchmarks (beyond HumanEval/MBPP) after implementing RLVR.

### Open Question 2
- Question: How effectively can the Mify-Coder architecture integrate multi-modal encoders to enable vision-guided code generation, such as UI-to-code translation?
- Basis in paper: [Explicit] Section 8 outlines plans to extend the model beyond text-only operation by incorporating multi-modal encoders for tasks involving visual inputs.
- Why unresolved: The current architecture is strictly text-based, and the integration of visual encoders into a 2.5B code-specific model remains an untested architectural extension.
- What evidence would resolve it: Benchmark performance on visual-coding tasks (e.g., Design2Code) demonstrating that the model can process visual features and translate them into syntactically correct code.

### Open Question 3
- Question: What specific architectural refinements are required to overcome the model's current limitations in long-horizon planning?
- Basis in paper: [Explicit] Section 8 acknowledges current limitations in "long-horizon planning" and proposes architectural refinements as a solution.
- Why unresolved: It is unclear if the existing 2.5B parameter capacity is sufficient for long-horizon dependencies without significant structural changes or substantial scaling.
- What evidence would resolve it: Performance improvements on repository-level or agentic benchmarks (e.g., SWE-bench) that require maintaining context and planning over extended sequences.

## Limitations
- The Mify-2.5B base model weights and exact synthetic data generation prompts are not released, preventing faithful reproduction
- LLM-as-judge validation reliability for code quality assessment lacks independent verification
- Safety alignment effectiveness against real-world adversarial attacks remains unproven beyond benchmark-specific metrics
- The claimed superiority of the 78:12:10 data ratio is based on a single ablation study without independent replication

## Confidence
- **High Confidence**: The compute-optimal training strategy and FP8 quantization deployment results are well-supported by standard practices and measurable benchmarks
- **Medium Confidence**: The 78:12:10 data composition ratio's effectiveness is supported by ablation studies but lacks independent replication
- **Low Confidence**: The LLM-as-judge validation mechanism's reliability for code quality assessment is not independently verified

## Next Checks
1. Replicate the 78:12:10 data composition ablation on a smaller token budget (~10B tokens) using a different base model to validate whether the ratio consistently improves code generation benchmarks across architectures and scales.
2. Test FIM ratios (15%, 25%, 30%) on a code-only baseline with your target programming languages to confirm the 25% selection is optimal for your specific use case.
3. Evaluate FP8 vs. 8-bit AWQ quantization on your target deployment hardware to verify the paper's throughput claims (8,658 tokens/sec on H100) and -1.35% accuracy change generalize beyond the tested configuration.