---
ver: rpa2
title: 'PaperScout: An Autonomous Agent for Academic Paper Search with Process-Aware
  Sequence-Level Policy Optimization'
arxiv_id: '2601.10029'
source_url: https://arxiv.org/abs/2601.10029
tags:
- search
- retrieval
- papers
- arxiv
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "PaperScout introduces an autonomous agent for academic paper search\
  \ that reframes retrieval as a sequential decision-making process rather than a\
  \ static workflow. Instead of following predefined search\u2013expand pipelines,\
  \ PaperScout dynamically decides when and how to invoke search and expand tools\
  \ based on accumulated retrieval context."
---

# PaperScout: An Autonomous Agent for Academic Paper Search with Process-Aware Sequence-Level Policy Optimization

## Quick Facts
- arXiv ID: 2601.10029
- Source URL: https://arxiv.org/abs/2601.10029
- Reference count: 30
- Key outcome: PaperScout significantly outperforms workflow-driven and RL baselines in recall and relevance for academic paper search by reframing retrieval as sequential decision-making and using PSPO optimization.

## Executive Summary
PaperScout introduces an autonomous agent for academic paper search that reframes retrieval as a sequential decision-making process rather than a static workflow. Instead of following predefined search–expand pipelines, PaperScout dynamically decides when and how to invoke search and expand tools based on accumulated retrieval context. A key challenge in training such agents is the mismatch between token-level optimization and sequence-level interactions, leading to noisy credit assignment. To address this, the authors propose Proximal Sequence Policy Optimization (PSPO), a process-aware policy optimization method that aligns optimization granularity with agent–environment interactions. Experiments on synthetic and real-world benchmarks show that PaperScout significantly outperforms strong workflow-driven and RL baselines in both recall and relevance, validating the effectiveness of the adaptive agentic framework and PSPO optimization strategy.

## Method Summary
PaperScout implements an LLM-based autonomous agent operating in a POMDP environment for academic paper search. The agent receives queries and maintains a paper pool state, dynamically deciding between search (query-based retrieval) and expand (citation-based retrieval) actions based on accumulated context. The key innovation is PSPO, which resolves credit assignment noise by performing sequence-level policy optimization rather than token-level updates. The method uses Generalized Advantage Estimation at the sequence level, importance sampling with sequence-level ratios, and return normalization. Training involves a dual-component reward combining relevance gain with repetition penalties, optimized using Qwen3-4B-Instruct as the agent backbone with separate actor and critic learning rates.

## Key Results
- PaperScout achieves significantly higher recall and relevance scores compared to static workflow baselines and other RL methods across both synthetic and real-world benchmarks.
- The PSPO optimization method demonstrates more stable training dynamics with lower critic loss and gradient norms compared to standard PPO.
- The agent shows adaptive behavior, switching between search and expand tools based on retrieval context rather than following rigid patterns.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reformulating retrieval as a sequential decision-making process (POMDP) allows the agent to adaptively balance breadth (search) and depth (citation expansion) based on real-time context, overcoming the rigidity of fixed workflows.
- **Mechanism:** The system maintains a latent state (paper pool). At each turn, the agent receives an observation (top papers, history) and outputs an action (tool call). This loop enables dynamic strategy adjustment—for example, switching to `search` when `expand` yields diminishing returns—rather than following a hardcoded pipeline.
- **Core assumption:** Effective retrieval for complex queries requires conditional branching that cannot be pre-defined in a static workflow.
- **Evidence anchors:**
  - [abstract]: "PaperScout dynamically decides whether, when, and how to invoke search and expand tools based on accumulated retrieval context."
  - [section]: Figure 6 Case Study shows the agent initiating a new Search call in Step 4 after expansion saturates, verifying adaptive strategy.
  - [corpus]: "The FM Agent" and "DSPO" support the general efficacy of multi-agent and RL-based search frameworks over static approaches.
- **Break condition:** If the query is simple (e.g., exact title match) or the retrieval backend is exhaustive and perfect, the overhead of sequential decision-making yields no marginal gain over a single API call.

### Mechanism 2
- **Claim:** Proximal Sequence Policy Optimization (PSPO) resolves the credit assignment noise inherent in token-level methods (like PPO) by aligning the optimization granularity with the agent's turn-based interactions.
- **Mechanism:** Instead of attributing the sparse sequence reward to individual tokens, PSPO treats the entire response (reasoning + tool call) as the atomic unit for advantage estimation. It calculates Generalized Advantage Estimation (GAE) over the sequence (Eq. 3) and updates the policy using sequence-level importance ratios (Eq. 5), preventing the "dilution" of the learning signal across irrelevant tokens.
- **Core assumption:** The causal action determining retrieval success is the complete tool invocation sequence, not specific token probabilities within the generation.
- **Evidence anchors:**
  - [abstract]: "PSPO... aligns optimization granularity with agent-environment interaction."
  - [section]: Figure 4 shows PSPO achieves lower Critic Loss and stable Gradient Norms compared to PPO, indicating resolved optimization instability.
  - [corpus]: "Owen-Shapley Policy Optimization" and "Principled RL for Diffusion LLMs" corroborate the shift toward sequence-level or principled RL approaches to fix credit assignment gaps in generative models.
- **Break condition:** If the task requires fine-grained token-level correction (e.g., syntax correction in code generation), aggregating feedback to the sequence level might obscure critical local errors.

### Mechanism 3
- **Claim:** A dual-component reward function combining relevance gain with a repetition penalty drives the agent toward high-yield exploration while minimizing redundant tool calls.
- **Mechanism:** The reward $r_t$ sums the relevance scores of top-$k$ accepted papers (gain) and subtracts a penalty for repeating history (efficiency). This shapes the policy to seek high-value papers and avoid getting stuck in citation loops or re-issuing identical queries.
- **Core assumption:** The relevance scorer accurately reflects ground-truth utility, and the penalty weight $\eta$ correctly balances exploration vs. efficiency.
- **Evidence anchors:**
  - [section]: Section 3.3, Eq. (2) defines the reward formulation.
  - [section]: Figure 3 shows PaperScout achieves higher Recall with fewer or equal tool calls compared to baselines, suggesting efficient exploration.
  - [corpus]: "Agent-GSPO" discusses optimizing for token economy in agents, supporting the need for efficiency constraints in RL agent design.
- **Break condition:** If the relevance model is biased or hallucinates scores, the reward signal misguides the agent, potentially optimizing for spurious features rather than true relevance.

## Foundational Learning

- **Concept:** **Partially Observable Markov Decision Process (POMDP)**
  - **Why needed here:** The agent cannot see the entire corpus (the true state); it only sees the observation (the retrieved paper list). Understanding POMDPs explains why the paper emphasizes "accumulated retrieval context" to infer the latent state.
  - **Quick check question:** Why is the paper pool considered a "latent state" rather than the "observation" input to the LLM?

- **Concept:** **Credit Assignment in RL**
  - **Why needed here:** The core problem identified is the "granularity mismatch." You must understand how standard PPO distributes rewards across tokens to see why this fails for multi-turn tool use where the reward only appears after the tool executes.
  - **Quick check question:** In a multi-turn dialogue, if the reward is only received at the end, why does token-level optimization lead to "noisy" credit assignment?

- **Concept:** **Generalized Advantage Estimation (GAE)**
  - **Why needed here:** PSPO utilizes GAE (Eq. 3) at the sequence level to calculate advantages. Understanding the bias-variance trade-off in GAE helps explain the stability of the training dynamics shown in Figure 4.
  - **Quick check question:** How does GAE use the parameter $\lambda$ to balance between high-variance Monte Carlo estimates and high-bias Temporal Difference estimates?

## Architecture Onboarding

- **Component map:**
  - Agent (LLM) -> Environment (Milvus + ar5iv) -> State (Paper Pool) -> Scorer (pasa-7b-selector) -> Reward

- **Critical path:**
  1. **Serialize:** Convert current Paper Pool + History -> Prompt ($x_t$).
  2. **Act:** LLM generates response ($y_t$); Parse into tool calls ($C_t$).
  3. **Execute:** Environment runs tools, filters results by threshold $\tau$ -> Candidates ($V_t$).
  4. **Update State:** Merge $V_t$ into Pool ($s_{t+1}$).
  5. **Train:** Compute Reward ($r_t$) using Scorer; Update Actor/Critic via PSPO.

- **Design tradeoffs:**
  - **Granularity:** PSPO (Sequence-level) vs. PPO (Token-level) vs. GRPO (Outcome-level). Paper claims PSPO balances the stability of PPO with the alignment of sequence-level feedback.
  - **State Representation:** Dual-list view (Expanded vs. Unexpanded) helps the agent distinguish stable context from exploration candidates.

- **Failure signatures:**
  - **Citation Loops:** Agent repeatedly expands the same cluster without new search. (Mitigated by history penalty).
  - **Critic Divergence:** High variance in returns across queries destabilizes value learning. (Mitigated by Value Pre-training and Return Normalization).
  - **Tool Bias:** Agent only uses one tool type (e.g., always Expand). (Check Figure 5 for balanced distribution).

- **First 3 experiments:**
  1. **Gradient & Loss Analysis:** Reproduce Figure 4 to verify that PSPO actually reduces Critic Loss and Gradient Norm compared to PPO on your specific data.
  2. **Tool Distribution Visualization:** Plot Search vs. Expand counts per query (Figure 5) to ensure the agent isn't collapsing to a trivial strategy.
  3. **Ablation on Reward:** Remove the repetition penalty ($\eta=0$) to measure the drop in efficiency (tool calls per relevant paper found).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does leveraging incoming citations and richer citation graph signals affect the agent's retrieval recall compared to the current outgoing-reference-only approach?
- Basis in paper: [explicit] The Limitations section states, "citation expansion currently considers only outgoing references, while leveraging incoming citations and richer citation graph signals may provide additional retrieval cues."
- Why unresolved: The current `Expand` tool implementation is restricted to following references cited by a seed paper (forward traversal), ignoring the semantic context provided by papers that cite the seed paper (backward traversal).
- Evidence: A modified version of PaperScout where the `Expand` tool can traverse incoming citation links, evaluated on the AutoScholarQuery and RealScholarQuery benchmarks to measure recall delta.

### Open Question 2
- Question: Does the PaperScout framework generalize effectively to non-computer science domains with different citation densities and terminological structures?
- Basis in paper: [explicit] The Limitations section notes, "our current evaluation primarily focuses on the computer science domain, and extending the study to broader research areas would further validate the generality..."
- Why unresolved: The agent is trained and evaluated exclusively on datasets derived from CS conferences (e.g., ICLR, NeurIPS), potentially overfitting its policy to the specific search and expansion patterns common in computer science literature.
- Evidence: Zero-shot or fine-tuning performance results on academic search benchmarks from distinct fields such as biomedicine (PubMed), physics, or social sciences.

### Open Question 3
- Question: How can PaperScout be adapted to handle restricted or paywalled content to mitigate coverage limitations in domains with low open-access rates?
- Basis in paper: [explicit] The Limitations section highlights that the agent "relies on papers that are publicly accessible... retrieving papers from restricted or paywalled sources remains challenging."
- Why unresolved: The current architecture relies on local caching from open repositories like arXiv and ar5iv, preventing the agent from processing the full text of relevant papers behind paywalls.
- Evidence: Integration of the agent with institutional access proxies or licensed APIs (e.g., Semantic Scholar with full-text access), showing an increase in Recall and LLM-score for queries where ground-truth papers are paywalled.

### Open Question 4
- Question: Does incorporating multi-source retrieval from heterogeneous databases significantly improve robustness compared to the single local backend used during training?
- Basis in paper: [explicit] The Limitations section suggests that "incorporating multi-source retrieval from heterogeneous scholarly databases could further improve robustness and recall."
- Why unresolved: The training environment uses a standardized local Milvus backend to ensure stability, which may not fully represent the noise and variance found in real-world, multi-database retrieval scenarios.
- Evidence: An ablation study where the `Search` tool queries multiple live APIs (e.g., Google Scholar, PubMed, IEEE) simultaneously, analyzed for stability in policy performance and recall improvements.

## Limitations
- Evaluation is limited to computer science domains, restricting generalizability to other academic fields with different citation patterns and terminology.
- The method depends on high-quality relevance scorers, which may introduce brittleness if scorer accuracy degrades in production environments.
- Computational overhead of training and deploying an autonomous agent versus static pipelines remains unquantified for practical deployment scenarios.

## Confidence
- **High confidence:** The core claim that reframing retrieval as sequential decision-making enables adaptive strategy is well-supported by case studies and outperforms static baselines in controlled experiments.
- **Medium confidence:** The effectiveness of PSPO over PPO is demonstrated through gradient and loss metrics, but the specific advantage over other sequence-level RL methods (like OSPO) is not directly compared.
- **Medium confidence:** The efficiency gains (fewer tool calls for same recall) are shown but may be sensitive to the exact reward formulation and penalty weight tuning.

## Next Checks
1. **Cross-Domain Transfer:** Evaluate PaperScout on queries from non-CS domains (e.g., biomedical or social sciences) to test generalization beyond the AutoScholarQuery corpus.
2. **Scorer Robustness Test:** Systematically degrade the pasa-7b-selector's accuracy (e.g., via noise injection) and measure the agent's performance drop to quantify dependence on scorer quality.
3. **Efficiency Benchmarking:** Compare wall-clock time and compute cost per query for PaperScout versus a tuned static workflow baseline to assess practical deployment trade-offs.