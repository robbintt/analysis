---
ver: rpa2
title: UAV-Assisted Multi-Task Federated Learning with Task Knowledge Sharing
arxiv_id: '2501.10644'
source_url: https://arxiv.org/abs/2501.10644
tags:
- task
- tasks
- each
- training
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a UAV-assisted multi-task federated learning
  scheme to train multiple related tasks simultaneously using data collected by UAVs.
  The core idea involves sharing feature extractors across related tasks while maintaining
  task-specific predictors, and introducing a task attention mechanism to balance
  task performance and encourage knowledge sharing.
---

# UAV-Assisted Multi-Task Federated Learning with Task Knowledge Sharing

## Quick Facts
- **arXiv ID:** 2501.10644
- **Source URL:** https://arxiv.org/abs/2501.10644
- **Reference count:** 16
- **Primary result:** 3.2% improvement in final accuracy versus baseline strategies in multi-task UAV FL with task knowledge sharing

## Executive Summary
This paper proposes a UAV-assisted multi-task federated learning framework that simultaneously trains multiple related tasks using data collected by UAVs. The core innovation involves sharing feature extractors across tasks while maintaining task-specific predictors, coupled with a task attention mechanism to balance performance and encourage knowledge sharing. The authors also derive optimal bandwidth allocation under limited bandwidth conditions and propose a UAV-EV association strategy based on coalition formation games. Results show improved multi-task performance and training speed compared to baseline strategies, particularly in non-IID data scenarios.

## Method Summary
The scheme implements multi-task federated learning where UAVs train on local data while sharing a common feature extractor backbone. UAVs are associated with EVs (each managing one task) through a coalition formation game that optimizes network utility. The framework includes a task attention mechanism that dynamically weights tasks based on training progress and marginal contribution to others. Bandwidth allocation for UAV transmissions is optimized to minimize communication time. The method is evaluated on a modified MNIST dataset with dual tasks (digit recognition and rotation angle recognition) across 10 UAVs and 2 tasks.

## Key Results
- 3.2% improvement in final accuracy compared to baseline strategies
- 4.5% accuracy improvement when using shared feature extractors versus no sharing
- Better performance in non-IID data scenarios compared to IID settings

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Sharing feature extractors across related tasks improves model robustness and accelerates convergence compared to independent task training.
- **Mechanism:** Related tasks require similar low-level features (edges, shapes, contours). A shared convolutional backbone learns from data across all UAVs and tasks, exposing the extractor to a broader distribution of inputs and reducing overfitting to any single task's data distribution.
- **Core assumption:** Tasks must share underlying feature correlations; unrelated tasks would not benefit and may interfere.
- **Evidence anchors:** [abstract] "sharing feature extractors across related tasks while maintaining task-specific predictors"; [section V-C] "Strategy 1 shows a 4.5% accuracy improvement compared to Strategy 3 [no sharing]"; [corpus] Neighbor paper confirms collaborative learning across different tasks via shared structures.
- **Break condition:** Tasks become unrelated (negative transfer), or non-IID data heterogeneity exceeds the shared extractor's capacity to generalize.

### Mechanism 2
- **Claim:** The task attention mechanism dynamically balances multi-task performance by weighting tasks based on training progress and marginal contribution.
- **Mechanism:** Two signals are combined: (1) loss-based weights prioritize slower-converging tasks via exponential smoothing of cumulative loss; (2) Shapley value-based weights quantify each task's contribution to improving other tasks through feature extractor fusion.
- **Core assumption:** Tasks with higher historical loss or higher marginal contribution to others should receive more computational resources; this assumes diminishing returns in neural network training.
- **Evidence anchors:** [section IV-A] "the weight of each task in round t based on the task attention mechanism as Ψm,t"; [section III-B] "improving overall task performance calls for greater emphasis on tasks with slower training progress"; [corpus] Weak direct evidence; neighbor papers do not explicitly address Shapley-based task weighting.
- **Break condition:** Loss signals become noisy (high variance gradients), or Shapley computation becomes infeasible for large M (exponential complexity in coalition enumeration).

### Mechanism 3
- **Claim:** Coalition formation game-based UAV-EV association achieves higher network utility than random or distance-based association.
- **Mechanism:** UAVs iteratively adjust associations through "transfer" (leaving one EV for another) or "exchange" (swapping EVs with another UAV) operations. Each move is accepted only if global utility increases. The process terminates at a Nash-stable partition where no unilateral deviation improves utility.
- **Core assumption:** The utility function correctly captures the tradeoff between weighted data contribution and round completion time; UAVs act rationally to maximize global (not local) utility.
- **Evidence anchors:** [section IV-C] "the association policy adjustment will terminate when a stable association S* is reached"; [section V-C] "compared to Strategy 1 [random association], the proposed algorithm achieves a 3.2% improvement in final accuracy"; [corpus] "AirFed" neighbor paper uses multi-agent RL for UAV coordination, suggesting game-theoretic approaches are actively explored.
- **Break condition:** Utility landscape has many local optima (suboptimal stable partitions), or communication overhead for coordination dominates gains.

## Foundational Learning

- **Concept:** Federated Averaging (FedAvg)
  - **Why needed here:** The scheme builds on FedAvg's local SGD + gradient aggregation, but extends it to multi-task learning with shared parameters.
  - **Quick check question:** Can you explain how local gradients are aggregated in Eq. (4)-(5) and why cumulative gradients are uploaded instead of raw weights?

- **Concept:** Multi-Task Learning (MTL) with Hard Parameter Sharing
  - **Why needed here:** Understanding the split between shared feature extractors (ws) and task-specific predictors (wu) is essential for implementing the architecture.
  - **Quick check question:** Why might hard parameter sharing cause negative transfer between unrelated tasks?

- **Concept:** Coalition Formation Games
  - **Why needed here:** The UAV-EV association algorithm relies on stability concepts (Nash equilibrium, Pareto improvement) from cooperative game theory.
  - **Quick check question:** What is the difference between a "transfer" and an "exchange" operation, and when does each apply?

## Architecture Onboarding

- **Component map:**
  - UAVs (N nodes) -> Local training -> Cumulative gradient upload
  - EVs (M nodes) -> Gradient aggregation -> Cross-EV feature fusion -> Model broadcast
  - Shared Backbone (3 Conv-BN layers) -> Task-Specific Heads (4 FC layers per task)

- **Critical path:**
  1. UAV-EV association (coalition game, ~O(N·M) iterations)
  2. Model broadcast (EV → UAVs)
  3. Local training (K SGD steps)
  4. Gradient upload (bandwidth-allocated FDMA)
  5. EV-level aggregation + cross-EV feature fusion
  6. Task attention weight update for next round

- **Design tradeoffs:**
  - More UAVs per task: Better feature diversity but longer round time (T_max bottleneck)
  - Higher K (local steps): Reduced communication frequency but increased gradient staleness and divergence risk
  - Larger shared backbone: Better transfer but higher communication cost (Q_m in Eq. (17))

- **Failure signatures:**
  - Task divergence: One task's loss plateaus while others improve → check α_m,t weighting, may need to increase loss-based attention
  - Association instability: UAVs oscillate between EVs without convergence → verify utility monotonicity, check δ_m minimum constraint
  - Bandwidth allocation failure: T*_t has no feasible solution → energy constraints (6a) may be too tight