---
ver: rpa2
title: Interpretable Question Answering with Knowledge Graphs
arxiv_id: '2510.19181'
source_url: https://arxiv.org/abs/2510.19181
tags:
- knowledge
- graph
- question
- answer
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a knowledge graph-based question answering
  system that avoids traditional RAG approaches by constructing structured knowledge
  graphs from QA pairs. The method involves document preprocessing to generate QA
  pairs, graph creation using entity-relation extraction, and retrieval via embeddings
  and fuzzy matching.
---

# Interpretable Question Answering with Knowledge Graphs

## Quick Facts
- **arXiv ID:** 2510.19181
- **Source URL:** https://arxiv.org/abs/2510.19181
- **Reference count:** 0
- **Primary result:** Achieved 71.9% accuracy on CRAG benchmark using KGQA approach without traditional RAG retrieval

## Executive Summary
This work presents a knowledge graph-based question answering system that avoids traditional retrieval augmented generation by constructing structured knowledge graphs from QA pairs. The method involves document preprocessing to generate QA pairs, graph creation using entity-relation extraction, and retrieval via embeddings and fuzzy matching. Answers are generated through paraphrasing and reranking. Evaluated using LLM-as-a-judge on the CRAG benchmark, the system achieved 71.9% accuracy with Llama-3.2 and 54.4% with GPT-3.5-Turbo, demonstrating competitive performance without relying on retrieval augmented generation.

## Method Summary
The system converts documents to QA pairs before graph construction, improving entity-relation extraction quality over direct document-to-graph approaches. It employs three-pronged retrieval (node-level similarity, type-level generalization, fuzzy entity matching) to compensate for individual retrieval failures. Lightweight paraphrasing of retrieved triples generates coherent natural language answers without full LLM generation, reducing hallucination risk. The pipeline processes documents in batches, constructs knowledge graphs using LLM-based entity-relation extraction, and employs cosine similarity and fuzzy matching for retrieval before final reranking.

## Key Results
- Achieved 71.9% accuracy on CRAG benchmark using Llama-3.2 as judge
- Outperformed traditional RAG approaches by avoiding chunk-based retrieval
- Identified entity mismatch as primary error mode during evaluation
- Demonstrated competitive performance without relying on retrieval augmented generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Converting documents to QA pairs before graph construction improves entity-relation extraction quality over direct document-to-graph approaches.
- Mechanism: QA pairs serve as atomic knowledge blocks that constrain the extraction space, prompting the LLM to identify focused entity-relation triplets rather than attempting holistic document parsing.
- Core assumption: QA pairs generated by T5-base adequately cover the knowledge space needed for downstream queries.
- Evidence anchors: [abstract] "The first stage involves pre-processing a document to generate sets of question-answer (QA) pairs."
- Break condition: If QA generation model misses key facts or generates low-quality pairs, the knowledge graph will have coverage gaps.

### Mechanism 2
- Claim: Three-pronged retrieval (node-level similarity, type-level generalization, fuzzy entity matching) compensates for individual retrieval failures.
- Mechanism: Node embeddings capture instance-specific semantics; type embeddings enable category-level queries; fuzzy matching recovers entities with spelling variations.
- Core assumption: The union of three retrieval strategies yields sufficient recall; reranker can filter noise.
- Evidence anchors: [section 3.3] "Nodes with the highest similarity scores are selected along with their respective immediate relationships."
- Break condition: If entity extraction during graph construction is inconsistent, fuzzy matching becomes insufficient.

### Mechanism 3
- Claim: Lightweight paraphrasing of retrieved triples can achieve coherent natural language answers without full LLM generation, reducing hallucination risk.
- Mechanism: The paraphraser synthesizes retrieved entity-relationship edges into fluent text, operating on structured graph triples rather than free-text chunks.
- Core assumption: Retrieved triples contain sufficient information to answer the question; paraphraser preserves semantic fidelity.
- Evidence anchors: [abstract] "A small paraphraser model is used to paraphrase the entity relationship edges retrieved from querying the knowledge graph."
- Break condition: If retrieval returns incomplete or irrelevant triples, paraphraser will produce fluent but wrong answers.

## Foundational Learning

- **Concept:** Knowledge Graph Construction via LLM-based Entity-Relation Extraction
  - Why needed here: The system relies on LangChain's LLMGraphTransformer to convert QA pairs into (entity, relation, entity) triplets stored in Neo4j.
  - Quick check question: Given a QA pair "What is the penalty for late delivery? $500 per day," can you identify at least two entities and one relationship that should be extracted?

- **Concept:** Semantic Embeddings and Cosine Similarity Retrieval
  - Why needed here: Node and type embeddings enable similarity-based retrieval. Cosine similarity thresholds determine recall-precision tradeoffs.
  - Quick check question: If a query embedding has cosine similarity 0.72 to Node A and 0.68 to Node B, which node is retrieved first, and what might cause both to be relevant?

- **Concept:** Reranking and Answer Selection
  - Why needed here: BAAI/bge-reranker-large scores paraphrased candidates against the query; top-5 are returned.
  - Quick check question: If the ground-truth answer is ranked 6th by the reranker, what pipeline adjustments could improve its position?

## Architecture Onboarding

- **Component map:** Input PDF -> QA Generator -> Graph Builder -> Embedder -> Retriever (3-pronged) -> Paraphraser -> Reranker -> Top-5 answers
- **Critical path:** 1. Document chunking, 2. QA pair generation, 3. Graph construction (batches of 20 QA pairs), 4. Embedding generation, 5. Retrieval (three strategies combined), 6. Paraphrase → Rerank → Top-5 output
- **Design tradeoffs:** Batch size (20 QA pairs) balances API cost vs. context window; larger batches may lose extraction granularity. Edit distance ≤3 for fuzzy matching balances recall vs. false positives.
- **Failure signatures:** Entity mismatch (primary error mode), broken English in paraphrase, type-level vs. instance-level confusion.
- **First 3 experiments:**
  1. Validate QA pair coverage: Manually inspect 50 generated QA pairs from a sample document.
  2. Ablate retrieval strategies: Disable one retrieval method at a time and measure accuracy drop.
  3. Test reranker placement: Compare accuracy when reranker is applied before paraphrase vs. after.

## Open Questions the Paper Calls Out

- **Question 1:** To what degree would fine-tuned QA pair generation and richer relation extraction mitigate the "entity mismatch" errors identified as the primary failure mode?
- **Question 2:** How does the "lenient" evaluation prompt required for the lightweight paraphraser influence the comparability of the reported 71.9% accuracy against standard benchmarks?
- **Question 3:** Is the structural quality of the Knowledge Graph robust to changes in the underlying LLM used for the LLMGraphTransformer?

## Limitations
- Entity mismatch identified as primary error mode, indicating fragility in the NER + fuzzy matching pipeline
- No direct validation that structured triple paraphrasing produces more faithful answers than free-text chunk retrieval baselines
- Critical hyperparameters for initial node retrieval (top-K before reranking) are unspecified

## Confidence
- **High confidence:** The architectural approach of using QA pairs as intermediate knowledge blocks before graph construction is clearly described and technically sound
- **Medium confidence:** The 3-pronged retrieval strategy is well-specified, but its superiority over simpler approaches isn't empirically demonstrated
- **Low confidence:** The claim of reduced hallucination through structured triple paraphrasing lacks direct evidence or comparison to baseline hallucination rates

## Next Checks
1. **Ablation study:** Disable each retrieval strategy (node-level, type-level, fuzzy matching) individually and measure accuracy impact
2. **Entity resolution audit:** For 100 failed test cases, trace whether entity mismatch stems from NER failures or inconsistent entity naming
3. **Hallucination comparison:** Run the same queries through a baseline RAG system and the KGQA system, then use an LLM judge to compare hallucination rates across both approaches