---
ver: rpa2
title: Out of Control -- Why Alignment Needs Formal Control Theory (and an Alignment
  Control Stack)
arxiv_id: '2506.17846'
source_url: https://arxiv.org/abs/2506.17846
tags:
- control
- layer
- alignment
- systems
- formal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues that formal optimal control theory should be
  central to AI alignment research to address systematic gaps in current alignment
  approaches. The authors propose an Alignment Control Stack, a hierarchical framework
  that decomposes AI systems into ten layers from physical hardware to societal impact,
  specifying measurement and control characteristics at each layer and how layers
  interoperate.
---

# Out of Control -- Why Alignment Needs Formal Control Theory (and an Alignment Control Stack)

## Quick Facts
- arXiv ID: 2506.17846
- Source URL: https://arxiv.org/abs/2506.17846
- Authors: Elija Perrier
- Reference count: 40
- Key outcome: Proposes formal optimal control theory as central to AI alignment, introducing a 10-layer Alignment Control Stack to enable systematic analysis and coordination of alignment protocols.

## Executive Summary
This paper argues that current AI alignment approaches lack formal guarantees and systematic coordination mechanisms, proposing to address these gaps through formal optimal control theory. The authors introduce an Alignment Control Stack—a hierarchical framework decomposing AI systems into ten layers from physical hardware to societal impact—specifying measurement and control characteristics at each layer and their formal interoperability. By combining this structured taxonomy with stochastic control methods and game-theoretic analysis, the paper aims to bridge established control theory with practical AI deployment challenges.

## Method Summary
The paper presents an Alignment Control Stack architecture that formalizes AI systems as hierarchical layers with defined state variables, measurable signals, and control handles. It demonstrates vertical alignment using Linear-Quadratic-Gaussian (LQG) control between training and behavioral output layers, and horizontal alignment using Hamilton-Jacobi-Isaacs equations for multi-agent interactions. The approach enables systematic analysis of vertical dependencies between layers and horizontal interactions among parallel processes, addressing the formalisation problem (lack of formal guarantees and generalization) and the coordination problem (lack of layered taxonomy for alignment protocols).

## Key Results
- Introduces a 10-layer Alignment Control Stack providing formal interoperability specifications between layers
- Demonstrates how stochastic LQG control can address vertical alignment challenges between training and behavior layers
- Shows how HJI equations enable systematic analysis of horizontal multi-agent interactions and mechanism design

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Formal stochastic optimal control can model and manage uncertainty in AI systems where purely empirical methods lack generalizable guarantees.
- Mechanism: Stochastic control frameworks (e.g., Linear-Quadratic-Gaussian, HJB equations with diffusion terms) explicitly represent noise and probabilistic dynamics, enabling controller synthesis that optimizes expected performance or high-probability safety bounds rather than requiring infeasible deterministic proofs for highly non-linear systems.
- Core assumption: Key AI system dynamics can be approximated by tractable state-space models with noise terms capturing dominant uncertainties.
- Evidence anchors:
  - [abstract] "...recasting alignment through principles of formal optimal control... can develop a better understanding of the potential and limitations for controlling frontier models and agentic AI systems."
  - [section 2.1] "Stochastic control... is essential for modelling and managing probabilistic behaviours, from stochastic gradient descent in Training (Layer 5) to the probabilistic nature of LLM outputs (Layer 6)."
- Break condition: When state spaces are so high-dimensional or non-linear that all tractable approximations fail to capture safety-relevant dynamics.

### Mechanism 2
- Claim: A layered stack architecture clarifies where controls apply and how they interoperate, addressing coordination failures in alignment research.
- Mechanism: The Alignment Control Stack decomposes AI systems into ten hierarchical layers (L1–L10), each with specified state variables, measurable signals, and control handles. Vertical coupling is formalized via input–output contracts (e.g., reachable-set envelopes, probabilistic bounds) between adjacent layers, enabling principled composition of local controllers.
- Core assumption: Layers can be meaningfully isolated such that local controllers with stable interfaces compose into globally coherent strategies.
- Evidence anchors:
  - [abstract] "...we introduce an Alignment Control Stack which sets out a hierarchical layered alignment stack, identifying measurement and control characteristics at each layer and how different layers are formally interoperable."
  - [section 3] "Vertical coupling between layers is intrinsic: actions at a low layer propagate upwards... while mis-aligned objectives discovered at a high layer must eventually be corrected by interventions that flow back down."
- Break condition: When cross-layer feedback delays or information bottlenecks prevent timely correction of misalignment detected at higher layers.

### Mechanism 3
- Claim: Game-theoretic control formalisms enable analysis and design of multi-agent AI interactions (horizontal alignment), including adversarial or collusive scenarios.
- Mechanism: Hamilton-Jacobi-Isaacs (HJI) equations characterize Nash equilibrium strategies in differential games with coupled dynamics and interdependent objectives, allowing mechanism designers to shape incentives, communication constraints, and action spaces to steer equilibria toward safe outcomes.
- Core assumption: Agent objectives and dynamics can be specified sufficiently for tractable game models; agents are strategically rational.
- Evidence anchors:
  - [abstract] "...demonstrates how formal control can enable systematic alignment analysis through examples including... Hamilton-Jacobi-Isaacs equations for horizontal multi-agent interactions."
  - [section 4.3] "Horizontal alignment addresses the challenges of ensuring coherent, cooperative, or at least predictably safe interactions between multiple, potentially independent AI systems... The HJI formalism allows us to: Analyse Potential Outcomes... Design Mechanisms for Alignment... Synthesise Robust Strategies."
- Break condition: When agent preferences are unknowable or when computational intractability prevents solving coupled HJI systems.

## Foundational Learning

- **Concept:** State-space representation and stochastic dynamics
  - Why needed here: The paper models AI systems using state variables, controls, and noise (e.g., Wiener processes). Formulating dynamics as stochastic differential equations is prerequisite for applying the proposed control frameworks.
  - Quick check question: Given $dx_t = -ax_t dt + bu_t dt + \sigma dW_t$, can you identify the state, control, drift, and diffusion terms?

- **Concept:** Separation principle and LQG control
  - Why needed here: The vertical alignment example uses LQG control, decomposing into Kalman filtering for state estimation and LQR for control. This principle underpins the stacked control architecture.
  - Quick check question: In LQG, why can the optimal controller be decomposed into a Kalman filter followed by an LQR feedback law?

- **Concept:** HJB and HJI equations
  - Why needed here: These PDEs provide necessary conditions for optimal control (HJB) and game-theoretic equilibria (HJI). The paper uses HJI for horizontal multi-agent analysis.
  - Quick check question: How does the HJI equation differ from HJB in handling adversarial disturbances?

## Architecture Onboarding

- **Component map:** L1 (Physical Hardware) -> L2 (System Software) -> L3 (AI Framework) -> L4 (Model Architecture) -> L5 (Training Process) -> L6 (Behavioral Output) -> L7 (Interpretability) -> L8 (Rewards/Goals) -> L9 (Agents) -> L10 (SocioTech)

- **Critical path:** (1) Define state variables and measurable signals per layer; (2) Specify control handles; (3) Model vertical coupling; (4) For vertical alignment, formulate coupled LQG as in Sections 4.1–4.2; (5) For horizontal alignment, define multi-agent game dynamics and solve HJI for equilibria.

- **Design tradeoffs:**
  - Tractability vs. fidelity: Linear/Gaussian approximations are tractable but may poorly capture non-linear LLM dynamics.
  - Layer isolation vs. cross-layer feedback: Strict interfaces simplify analysis but may delay corrections; tight coupling enables faster response but complicates stability.
  - Worst-case vs. expected-case: Robust control (H∞) offers worst-case guarantees; stochastic control optimizes expected performance—choice depends on threat model.

- **Failure signatures:**
  - Vertical misalignment: Lower-layer controls fail to constrain emergent misbehavior at higher layers due to unmodeled dynamics or feedback delays.
  - Horizontal instability: Multi-agent interactions converge to undesirable equilibria (e.g., collusion) from poorly designed incentives or communication protocols.
  - Measurement inadequacy: Observable signals are insufficient proxies for alignment objectives, enabling specification gaming.

- **First 3 experiments:**
  1. Implement the two-layer vertical LQG controller (L5–L6) from Sections 4.1–4.2; verify separation principle and measure residual misalignment under noise.
  2. Simulate a two-agent horizontal game with coupled dynamics; use HJI analysis to identify equilibria and test mechanism modifications that shift outcomes toward cooperation.
  3. Perform layer-sensitivity analysis: introduce perturbations at each layer and measure propagation to upper-layer safety metrics to identify critical control points and bottlenecks.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can tractable but faithful formal models of AI dynamics be developed that effectively capture semantic properties?
  - **Basis in paper:** [explicit] The conclusion urges future research to focus on "Developing tractable but faithful formal models of AI agent dynamics... especially for semantic properties across the AI stack."
  - **Why unresolved:** Current AI state spaces are extremely high-dimensional and opaque; mapping these to formal state variables without losing semantic nuance remains mathematically difficult.
  - **What evidence would resolve it:** The derivation of a reduced-order state-space model that is mathematically verified to correlate with specific semantic internal states and transition functions.

- **Open Question 2:** Can control protocols be formally verified to ensure safety guarantees are maintained across vertical and horizontal stack layers?
  - **Basis in paper:** [explicit] The paper calls for "Designing and formally verifying control protocols that are interoperable vertically and horizontally across the ACS."
  - **Why unresolved:** Dependencies between layers (e.g., hardware affecting behavior) and multi-agent interactions create complex couplings that resist unified formal analysis and verification.
  - **What evidence would resolve it:** A formally verified protocol where stability and safety guarantees persist despite control interventions or perturbations at different stack layers.

- **Open Question 3:** Can theoretical control guarantees be translated into practical, scalable implementations for large frontier models?
  - **Basis in paper:** [explicit] The conclusion lists "Bridging the gap between theoretical control guarantees and practical, scalable implementations for large frontier models" as a key priority.
  - **Why unresolved:** Classical optimal control often relies on assumptions (e.g., linearity, specific noise profiles) that do not hold for the discrete, stochastic, and massive nature of Large Language Models.
  - **What evidence would resolve it:** A scalable implementation of a control-theoretic alignment method (e.g., robust MPC) on a frontier model that maintains provable safety bounds during real-world deployment.

## Limitations

- The paper relies heavily on tractable approximations that may not scale to real-world AI systems, particularly the assumption that complex AI dynamics can be meaningfully approximated by linear or linearizable state-space models.
- The vertical coupling mechanisms assume clean input-output contracts between layers that may not hold in practice, and the horizontal multi-agent analysis assumes strategic rationality that may not apply to AI agents with emergent behaviors.
- Current AI state spaces are extremely high-dimensional and opaque, making it mathematically difficult to map these to formal state variables without losing semantic nuance.

## Confidence

- **High Confidence:** The theoretical foundations of control theory (LQG, HJB/HJI equations, separation principle) are well-established and correctly presented. The identification of coordination problems in current alignment research is well-founded.
- **Medium Confidence:** The layered stack architecture provides useful conceptual structure, though its practical applicability depends on the degree to which layers can be meaningfully isolated.
- **Low Confidence:** The tractability of applying these formal methods to actual frontier AI systems, particularly the non-linear dynamics of LLMs and the computational feasibility of solving HJI equations for realistic multi-agent scenarios.

## Next Checks

1. **Layer Isolation Test:** Implement a simple two-layer simulation (e.g., model architecture → training process) and systematically test whether perturbations at lower layers propagate predictably to higher layers under the proposed control architecture.

2. **Stochastic vs Deterministic Comparison:** Compare alignment outcomes using stochastic LQG control versus deterministic approaches on a toy problem, measuring robustness to uncertainty and generalization across different noise regimes.

3. **Horizontal Game Feasibility:** Implement a small-scale multi-agent interaction (2-3 agents) and test whether HJI-based mechanism design can successfully shift equilibria compared to heuristic approaches, measuring computational tractability and solution quality.