---
ver: rpa2
title: Unified Scaling Laws for Compressed Representations
arxiv_id: '2506.01863'
source_url: https://arxiv.org/abs/2506.01863
tags:
- scaling
- quantization
- sparsity
- representation
- capacity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes unified scaling laws for model performance
  when training on compressed representations, showing that a simple "capacity" metric
  based on Gaussian fitting error can predict parameter efficiency across sparse,
  quantized, sparse-quantized, and vector-quantized formats. The authors validate
  a general scaling law formulation and demonstrate that capacity factorizes composably
  across compression types, enabling analytical comparisons between different numerical
  formats and inspiring improved sparse training algorithms.
---

# Unified Scaling Laws for Compressed Representations

## Quick Facts
- arXiv ID: 2506.01863
- Source URL: https://arxiv.org/abs/2506.01863
- Reference count: 40
- Key outcome: Unified scaling laws show that a simple Gaussian MSE-based "capacity" metric predicts parameter efficiency across multiple compressed representations, with capacity factorizing composably across compression types.

## Executive Summary
This paper establishes unified scaling laws for model performance when training on compressed representations (sparse, quantized, sparse-quantized, vector-quantized). The authors demonstrate that a simple "capacity" metric based on Gaussian fitting error can predict parameter efficiency across different compression formats. They validate a general scaling law formulation showing that capacity factorizes composably across compression types, enabling analytical comparisons between different numerical formats and inspiring improved sparse training algorithms.

## Method Summary
The method involves computing Gaussian Mean Squared Error (GMSE) for various compression formats via Monte Carlo sampling on random Gaussian data, then fitting a tanh-based capacity function ρ(R) = L·tanh(F·log₁/₄(GMSE(R)))^C. Scaling laws are validated by training Llama-style decoder-only Transformers (30M-200M parameters) on C4 data with different compression levels, measuring validation loss, and fitting the scaling law parameters. The theoretical analysis provides an Adam convergence bound showing a critical term proportional to N·E[RMSE of compression across optimization]. The RBBM (RMSE-Banded Backward Masking) algorithm is developed to increase sparse training capacity by over 20% in high-sparsity regimes.

## Key Results
- A simple GMSE-based capacity metric predicts parameter efficiency across sparse, quantized, sparse-quantized, and vector-quantized formats
- Capacity factorizes multiplicatively across compression types (e.g., sparsity × quantization)
- The functional form ρ(R) = tanh-based transform of GMSE fits across all compression types with shared parameters
- RBBM sparse training algorithm increases capacity by >20% in high-sparsity regimes
- GMSE enables ranking of compressed formats without training (e.g., FP4 offers no advantage over INT4)

## Why This Works (Mechanism)

### Mechanism 1: Gaussian MSE as Universal Capacity Metric
Compression introduces persistent quantization/sparsification error at each optimization step. The cumulative error over training manifests as reduced effective parameter count: N' = N · ρ(GMSE(R)). Lower GMSE → higher capacity → better loss scaling. The optimizer's gradient estimates remain sufficiently unbiased under compression that the dominant effect is the compression error magnitude.

### Mechanism 2: Multiplicative Factorization of Capacity
When combining compression types R₁ and R₂, total capacity factorizes as ρ(R₁, R₂) = ρ(R₁) · ρ(R₂). Compression errors from independent representations compound multiplicatively in parameter space, not additively. This reflects that each compression type reduces the effective degrees of freedom independently.

### Mechanism 3: RMSE Term in Adam Convergence Bound
The theoretical convergence rate of Adam under compressed training includes a critical term proportional to N · E[RMSE of compression across optimization]. Straight-through gradient estimation introduces error bounded by ||θ̂_t - θ_t|| at each step. Higher RMSE → looser convergence → effectively fewer parameters reaching optimal values.

## Foundational Learning

- **Scaling Laws (Chinchilla Form)**: Understanding Loss(N, D) = A·N^{-α} + B·D^{-β} + E is prerequisite to seeing why compression introduces an "effective parameter count" N·ρ(R).
- **Straight-Through Estimator (STE) for Quantization**: The theoretical analysis assumes gradients are computed via STE: g_t = ∇f(θ̂_t) where θ̂_t = C(θ_t). This makes the mechanism linking compression error to gradient bias opaque without understanding STE.
- **Root Mean Squared Error (RMSE) for Compression Quality**: GMSE(R) is the core metric. The paper's practical contribution is that you can compute GMSE via Monte Carlo on random Gaussian data—no training required—to rank formats.

## Architecture Onboarding

- **Component map**: GMSE computation → ρ(R) fitting → scaling law validation → factorization testing → RBBM implementation
- **Critical path**: Implement GMSE computation for any compression format → fit ρ(GMSE) curve using tanh functional form → predict capacity for new formats → validate with small-scale training
- **Design tradeoffs**: Gaussian vs. empirical data for MSE (universal vs. tighter predictions); full scaling law fit vs. GMSE-only prediction (10-20 runs vs. Monte Carlo); RBBM vs. standard magnitude pruning (capacity gain vs. hyperparameter tuning)
- **Failure signatures**: Capacity ρ(R) > 1 (fitting error or pathological compression); factorization error > 0.05 (nonlinear interaction); RBBM underperforms baseline (poorly tuned p or low sparsity)
- **First 3 experiments**:
  1. Compute GMSE for INT2, INT4, INT8, FP16; verify monotonic decrease and plot against known ρ(R)
  2. Combine 4-bit quantization with 2:4 semi-structured sparsity; compute GMSE individually, predict ρ(R_{4bit,2:4}) = ρ(R_{4bit})·ρ(R_{2:4}), validate with 50M parameter training
  3. Train 30M model at 70% sparsity with standard magnitude pruning vs. RBBM (p=0.3); measure final loss and capacity ρ

## Open Questions the Paper Calls Out

1. **Generalization to larger scales**: Do the unified scaling laws and Gaussian-MSE capacity metric generalize to billion-parameter scales and diverse architectures beyond the decoder-only Llama models tested? The authors plan to extend this at larger scale.

2. **Second-order effects in extreme compression**: What specific "second-order effects" are required to accurately model scaling behavior of extreme compression regimes, such as 2-bit quantization or small vector-quantization codebooks? The law may need specific fits for ultra-low precision.

3. **Theoretical extension to complex representations**: Can the theoretical convergence analysis for Adam be extended to rigorously support more complex representation types beyond the standard compressed iterates analyzed? The theoretical evidence could be extended to more complex representation types.

## Limitations

- The assumption that compression error dominates convergence degradation breaks at extreme compression (2-bit precision, <0.1 capacity)
- The multiplicative factorization claim is empirically validated but lacks theoretical justification beyond empirical rank-1 approximations
- The Adam convergence analysis assumes unbiased compression via STE, but the straight-through gradient estimator introduces bias at very low precision
- The GMSE metric is computed on standard Gaussian data rather than actual weight distributions

## Confidence

**High Confidence (Mechanistic soundness, extensive validation)**:
- The empirical observation that capacity ρ(R) can be fitted as a function of GMSE(R) for individual compression types
- The practical utility of GMSE for ranking compression formats without training
- The factorization approximation holding for moderate compression levels

**Medium Confidence (Strong empirical support but theoretical gaps)**:
- The Adam convergence analysis linking RMSE to effective parameter count
- The RBBM sparse training algorithm's capacity improvement

**Low Confidence (Limited validation or edge cases)**:
- The factorization claim at extreme compression (2-bit precision, ρ ≲ 0.1)
- The GMSE metric's predictive power for joint optimization of compression parameters
- The universality of the tanh functional form across all possible compression schemes

## Next Checks

1. **Extreme Compression Boundary Test**: Systematically measure factorization error and capacity prediction accuracy at 2-bit quantization and 90%+ sparsity. Quantify where the RMSE mechanism breaks down by comparing predicted vs actual loss and measuring the contribution of the N^{3/2}/T term in the convergence bound.

2. **Distribution Sensitivity Analysis**: Replace Gaussian GMSE computation with empirical weight distribution sampling from trained models. Compare prediction accuracy for both individual formats and hybrid compression schemes to determine if distribution-specific effects provide meaningful improvements over the universal Gaussian approach.

3. **Joint Compression Optimization Validation**: Implement a joint optimization framework where sparsity mask and quantization grid are optimized together. Test whether the multiplicative factorization prediction holds, or if nonlinear interactions emerge that require direct measurement rather than prediction.