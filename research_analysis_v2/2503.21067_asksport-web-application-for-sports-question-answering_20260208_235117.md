---
ver: rpa2
title: 'AskSport: Web Application for Sports Question-Answering'
arxiv_id: '2503.21067'
source_url: https://arxiv.org/abs/2503.21067
tags:
- asksport
- answers
- question
- sports
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AskSport is a web application that allows users to ask sports-related
  questions in natural language and receive the three most relevant answers with associated
  contextual information. It implements question-answering algorithms using a document
  retriever (BM25) and reader (RoBERTa) to process queries against basketball data
  from the QASports dataset.
---

# AskSport: Web Application for Sports Question-Answering

## Quick Facts
- arXiv ID: 2503.21067
- Source URL: https://arxiv.org/abs/2503.21067
- Reference count: 7
- Primary result: Web app for sports QA using BM25 retriever and RoBERTa reader on basketball data

## Executive Summary
AskSport is a web application that enables users to ask sports-related questions in natural language and receive the three most relevant answers with associated contextual information. It implements a two-stage retrieval-reader pipeline using BM25 for document retrieval and RoBERTa for extractive question answering, processing queries against basketball data from the QASports dataset. The application features an accessible, interactive interface with light and dark themes, and is publicly available on HuggingFace Spaces. AskSport demonstrates its ability to return names and numerical values for various use cases, though answer quality depends on the underlying models and dataset coverage.

## Method Summary
AskSport implements a retriever-reader pipeline for sports question-answering using Haystack framework. The system uses BM25 to retrieve the 10 most relevant documents from the QASports basketball dataset, then applies RoBERTa to extract answer spans and generate confidence scores. The web interface is built with Streamlit, allowing users to input natural language questions and receive top-3 ranked answers with metadata including document titles and URLs. The application is hosted on HuggingFace Spaces and supports light/dark themes for accessibility.

## Key Results
- Successfully returns names and numerical values for sports queries (e.g., award recipients, team titles)
- Implements a modular pipeline allowing future integration of additional sports data and algorithms
- Demonstrates the capability to process natural language questions and provide ranked answers with contextual metadata
- Shows answer quality depends on underlying models and dataset coverage

## Why This Works (Mechanism)

### Mechanism 1: Two-Stage Retrieval-Reader Pipeline
- Claim: The sequential retriever-then-reader architecture enables efficient question-answering by first narrowing the search space, then extracting precise answer spans.
- Mechanism: BM25 retrieves the 10 most relevant documents using term-frequency weighting; RoBERTa then performs extractive reading to identify answer spans within those documents.
- Core assumption: The correct answer exists verbatim within at least one of the retrieved documents.
- Evidence anchors:
  - [abstract]: "It implements question-answering algorithms using a document retriever (BM25) and reader (RoBERTa) to process queries against basketball data from the QASports dataset."
  - [section 3.1]: "It identifies and retrieves the ten most relevant documents. Subsequently, the document reader examines the retrieved documents, analyzing the information obtained using advanced natural language processing techniques."
  - [corpus]: Related QA work (OntoRAG, Geospatial QA) confirms retriever-reader patterns are standard for domain-specific QA systems.
- Break condition: If BM25 fails to retrieve documents containing the answer, the reader has no relevant context to extract from. Query vocabulary mismatch (synonyms) can cause retrieval failure.

### Mechanism 2: Domain-Specific Dataset Grounding
- Claim: Constraining the knowledge base to a curated sports dataset reduces answer hallucination and improves domain relevance.
- Mechanism: QASports provides structured context-question-answer triples from cleaned sports web pages, grounding all answers in verifiable source documents.
- Core assumption: The dataset coverage is sufficient for anticipated user queries; source documents are accurate.
- Evidence anchors:
  - [abstract]: "process queries against basketball data from the QASports dataset"
  - [section 3.1]: "It contains the following collections of data on soccer, American football, and basketball: (i) web pages cleaned and stored in JSON files; (ii) context files stored in CSV format; and (iii) questions and answers stored in the context-question-answer triples format."
  - [corpus]: Limited direct corpus evidence on sports QA grounding specifically; related work focuses on other domains.
- Break condition: Queries about players, events, or statistics not present in QASports will return no answers or irrelevant results.

### Mechanism 3: Multi-Answer Ranking with Source Transparency
- Claim: Returning top-3 answers with confidence scores and source links enables user verification and error detection.
- Mechanism: RoBERTa produces confidence scores; answers are ranked descending; each includes document title and URL for verification.
- Core assumption: Confidence scores correlate with answer correctness.
- Evidence anchors:
  - [abstract]: "receive the three most relevant answers with associated contextual information"
  - [section 3.1]: "For each answer, the following contextual information is also returned: (i) confidence metric in the provided answer; (ii) title of the document used to generate the answer; and (iii) link to the web page of the document."
  - [section 4, Table 1]: Demonstrates varied scores (0.64–0.80) across scenarios.
  - [corpus]: No direct corpus evidence on confidence calibration in sports QA.
- Break condition: High confidence does not guarantee correctness. Ambiguous queries (Scenario 3) return varied answers with similar scores, requiring user judgment.

## Foundational Learning

- Concept: **BM25 Retrieval Model**
  - Why needed here: Understanding that BM25 uses term frequency and inverse document frequency—not semantic understanding—explains why synonym-heavy queries may fail.
  - Quick check question: What happens if your query uses "championships" but the documents only contain "titles"?

- Concept: **Extractive Question Answering**
  - Why needed here: AskSport uses extractive QA (RoBERTa pulls answer spans directly from text), not generative QA (like GPT which synthesizes text). This limits answers to what exists verbatim in documents.
  - Quick check question: Can AskSport answer "What is the average points per game across all MVP winners?" if no single document contains this computed value?

- Concept: **Haystack Framework**
  - Why needed here: The implementation uses Haystack's modular pipeline; understanding its abstractions (DocumentStore, Retriever, Reader) is essential for modifications.
  - Quick check question: To replace BM25 with dense retrieval (e.g., DPR), which component interface would you modify?

## Architecture Onboarding

- Component map:
  - Frontend: Streamlit -> Retriever: BM25 -> Reader: RoBERTa -> Knowledge Base: QASports basketball data -> Hosting: HuggingFace Spaces

- Critical path:
  1. User enters natural language question in Streamlit input box
  2. Question passed to BM25 retriever
  3. BM25 returns top-10 documents from QASports corpus
  4. RoBERTa reader processes each document, extracts answer spans with confidence
  5. Top-3 answers ranked by confidence score
  6. UI displays answers with expandable metadata (score, document title, URL)

- Design tradeoffs:
  - BM25 vs. dense retrieval: BM25 selected for better performance on QASports (per citation [5]), but lacks semantic matching
  - Extractive vs. generative reader: Extractive provides source transparency; cannot synthesize multi-document answers
  - Top-3 cutoff: Balances information density with cognitive load; risks excluding correct answer ranked 4th or lower
  - Basketball-only scope: Focused evaluation; limits immediate applicability to other sports

- Failure signatures:
  - "We do not have an answer for your question" → Retrieval returned no relevant documents, or reader found no valid spans
  - Low confidence scores (<0.6) with plausible answers → Indicates extraction uncertainty; verify against source
  - Highly varied answers for identical query (e.g., Scenario 3) → Ambiguous question or multiple valid interpretations in documents
  - High confidence but incorrect answer → Dataset error or reader misinterpretation; check source URL

- First 3 experiments:
  1. Query type analysis: Test 20 questions across categories (player awards, team statistics, historical records); log confidence scores and retrieval counts to identify weak query types.
  2. Retrieval depth ablation: Compare BM25 top-10 vs. top-5 retrieval; measure impact on answer coverage and latency.
  3. Confidence calibration check: Manually label 30 query-answer pairs as correct/incorrect; compute correlation between RoBERTa confidence and actual accuracy to assess reliability.

## Open Questions the Paper Calls Out

- How does the integration of alternative document retrievers and reader models impact the accuracy and efficiency of the AskSport application?
  - Basis in paper: [explicit] The conclusion states that due to the flexible pipeline, "It is intended to investigate this aspect [employing other algorithms] as future work."
  - Why unresolved: The current implementation relies specifically on BM25 for retrieval and RoBERTa for reading; the authors have not yet tested the modular architecture with other algorithms.
  - What evidence would resolve it: Comparative performance benchmarks (e.g., latency, F1-score) of the application running with different model configurations, such as dense retrievers or generative LLMs.

- How does adding soccer and American football data from the QASports dataset affect the precision of retrieval and answer generation compared to the basketball-only implementation?
  - Basis in paper: [explicit] The conclusion lists "the inclusion of data from other sports present in QASports, such as soccer and football" as a distinct direction for future work.
  - Why unresolved: The application currently indexes and queries only the basketball subset of the dataset.
  - What evidence would resolve it: A demonstration or evaluation log showing the system successfully processing queries related to soccer and football without degradation in answer relevance.

- What are the quantitative performance metrics (e.g., Exact Match, F1) of the current BM25 + RoBERTa pipeline across the full QASports test dataset?
  - Basis in paper: [inferred] Section 4 states that the "insights gained are more qualitative than quantitative" and the paper limits its evaluation to three specific use cases rather than providing aggregate accuracy metrics.
  - Why unresolved: The paper demonstrates capability through successful scenario outputs but does not calculate or report error rates or aggregate performance scores.
  - What evidence would resolve it: A results table reporting standard QA evaluation metrics over the entire test set, rather than isolated successful examples.

## Limitations

- Domain-specific constraint: Limited to basketball questions from QASports dataset; cannot answer queries about other sports or general knowledge
- Extractive QA limitation: Cannot synthesize answers requiring numerical computation or multi-document reasoning; only returns verbatim spans from source documents
- No quantitative evaluation: Lacks standard QA metrics (EM, F1) across the full dataset; relies on qualitative scenario demonstrations

## Confidence

- High confidence: The retriever-reader pipeline architecture is well-established and correctly implemented per Haystack documentation
- Medium confidence: The claim that AskSport enables natural language sports question-answering is supported by the demo and scenario descriptions, but lacks quantitative evaluation
- Low confidence: The assertion that confidence scores correlate with answer correctness cannot be verified without manual accuracy labeling and calibration analysis

## Next Checks

1. Confidence Calibration Study: Manually label 50 query-answer pairs as correct/incorrect and compute correlation between RoBERTa confidence scores and actual accuracy to assess score reliability.
2. Query Type Performance Analysis: Systematically test 100 diverse questions (awards, statistics, history, comparisons) and measure retrieval coverage, answer presence, and confidence distribution to identify failure patterns.
3. Cross-Domain Generalization Test: Attempt to process queries from other sports (soccer, football) using the same basketball-trained models to evaluate knowledge transfer limitations.