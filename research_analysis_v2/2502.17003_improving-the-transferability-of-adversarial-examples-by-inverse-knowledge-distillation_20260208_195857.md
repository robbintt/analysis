---
ver: rpa2
title: Improving the Transferability of Adversarial Examples by Inverse Knowledge
  Distillation
arxiv_id: '2502.17003'
source_url: https://arxiv.org/abs/2502.17003
tags:
- adversarial
- attack
- attacks
- xadv
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the problem of limited transferability of adversarial
  examples in black-box attack scenarios, where generated perturbations often fail
  to generalize across different model architectures due to overfitting to surrogate
  models. To overcome this, the authors propose Inverse Knowledge Distillation (IKD),
  a method that integrates a distillation-inspired loss into gradient-based attack
  algorithms to diversify attack gradients and reduce model-specific dependence.
---

# Improving the Transferability of Adversarial Examples by Inverse Knowledge Distillation

## Quick Facts
- arXiv ID: 2502.17003
- Source URL: https://arxiv.org/abs/2502.17003
- Reference count: 40
- Primary result: Inverse Knowledge Distillation improves adversarial transferability by 6.8% average ASR across multiple models

## Executive Summary
This paper addresses the fundamental limitation of gradient-based adversarial attacks: poor transferability across different model architectures. When attacks are crafted on a surrogate model, they often fail against black-box targets due to overfitting to the surrogate's decision boundaries. The authors propose Inverse Knowledge Distillation (IKD), which integrates a distillation-inspired loss that maximizes KL divergence between benign and adversarial output distributions. This approach diversifies gradients and reduces model-specific dependence, resulting in perturbations that generalize better across architectures. Extensive experiments on ImageNet demonstrate consistent improvements across classical, input transformation-based, and advanced gradient-based attacks.

## Method Summary
The IKD method modifies standard gradient-based attacks by adding a soft-label distillation loss that maximizes the KL divergence between the output distributions of benign and adversarial examples. The total loss combines hard-label cross-entropy with this soft distillation term: $L_{total} = L_{hard} + \gamma L_{soft}$, where $L_{soft} = KL(f_\phi(x) || f_\phi(x_{adv}))$. During optimization, both the benign image and adversarial image are passed through the surrogate model to compute these distributions. The method is integrated into existing iterative attack frameworks like MI-FGSM, requiring two forward passes per iteration and maintaining the standard $L_\infty$ perturbation constraint.

## Key Results
- Average attack success rate increases of up to 6.8% across multiple models and attack strategies
- IKD improves transferability for classical attacks (FGSM, BIM), input transformation-based attacks (DI-MI-FGSM, TI-DIM), and advanced gradient-based attacks (TIM)
- Consistent performance gains even against defended models (Inc-v3adv, IncRes-V2adv)
- Ablation studies show optimal distillation weight around Î³=0.01, with performance degrading at very high weights (1000)

## Why This Works (Mechanism)

### Mechanism 1
Integrating a distillation-inspired loss term mitigates overfitting to surrogate model decision boundaries. Standard gradient-based methods follow a "fixed direction" determined by the surrogate's specific parameters. By adding the IKD term (maximizing KL divergence between benign and adversarial output distributions), the optimization landscape expands, forcing gradient updates to consider not just misclassification targets but feature representation divergence. This results in more generalized perturbations that are less brittle to model changes.

### Mechanism 2
KL divergence serves as a superior loss metric compared to MSE or CE due to its asymmetric sensitivity. The gradient of KL divergence penalizes deviations based on the probability mass of the benign distribution (weighted by $f_\phi(x)$), unlike MSE which applies uniform penalties or CE which lacks mutual penalization. This probabilistic emphasis pushes the adversarial sample away from the surrogate's learned "knowledge" more effectively.

### Mechanism 3
The IKD framework acts as a gradient diversifier, reducing the likelihood of getting stuck in local optima specific to the surrogate model. By combining hard-label loss with soft-label distillation loss, the total gradient direction is modified, incorporating richer gradient information that breaks the constraints of the single fixed gradient direction typically found in methods like FGSM or MI-FGSM.

## Foundational Learning

- **Concept: Knowledge Distillation (KD)**
  - Why needed: IKD inverts standard KD logic. Understanding KD baseline explains why maximizing divergence (Inverse KD) helps adversarial transferability by forcing deviation from surrogate-specific features.
  - Quick check: How does minimizing KL divergence in standard KD differ from maximizing it in IKD regarding alignment of student/adversarial outputs with teacher/benign outputs?

- **Concept: Transfer-based Black-box Attacks**
  - Why needed: Problem definition hinges on not being able to query target model. Understanding this constraint explains why overfitting to surrogate (white-box success but black-box failure) is the central failure mode.
  - Quick check: Why does high white-box attack success rate on surrogate not guarantee high success rate on target model in black-box setting?

- **Concept: $L_\infty$ Norm Constraint**
  - Why needed: Paper constrains perturbations using $L_\infty$ norm ($\|\delta\|_p \leq \epsilon$) to ensure perturbations remain imperceptible. Understanding this bounding box is critical for implementing the update step correctly.
  - Quick check: In the update rule $x^{adv}_{t+1} = x^{adv}_t + \alpha \cdot \text{sign}(g_{t+1})$, how does the sign function relate to optimizing within the $L_\infty$ ball?

## Architecture Onboarding

- **Component map:** Surrogate Model ($f_\phi$) -> Loss Module (computes $L_{total} = L_{hard} + \gamma L_{soft}$) -> Optimizer (iterative momentum-based updater with gradient sign)
- **Critical path:** Calculation of $L_{soft}$ via KL divergence. This is the novel addition. If benign logit $f_\phi(x)$ is not handled correctly relative to adversarial logit, gradient flow will fail to push distributions apart as intended.
- **Design tradeoffs:**
  - Weight $\gamma$: Controls trade-off between white-box attack power (dominated by $L_{hard}$) and transferability (enhanced by $L_{soft}$). Figure 3 suggests performance degrades at very high weights (1000) and stabilizes between 0.01 and 10.
  - KL vs. MSE: Paper argues KL is more effective but implies it requires careful handling of log-probabilities.
- **Failure signatures:**
  - White-box drop: If $\gamma$ is too high, attack succeeds less often on surrogate itself.
  - Stagnant Transfer: If $\gamma$ is too low (near 0), method behaves exactly like baseline and shows no transfer improvement.
- **First 3 experiments:**
  1. Implement MI-FGSM-IKD. Validate adding KL term ($\gamma=0.01$) increases attack success rates on target models compared to vanilla MI-FGSM.
  2. Run $\gamma$ sweep $\in \{0.001, 0.01, 0.1, 1.0\}$ to replicate Figure 3 and identify stable region for specific surrogate model.
  3. Substitute KL divergence with MSE and CE in $L_{soft}$ term to verify asymmetric sensitivity advantage.

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Evaluation restricted to CNN architectures, leaving effectiveness on Vision Transformers unexplored
- Optimal distillation weight identified only for ResNet50 surrogate, unclear if universal across architectures
- Method not tested in ensemble-based attack scenarios despite citation of ensemble methods in related work

## Confidence
- **High Confidence:** Core observation that IKD improves transferability across multiple attack baselines is well-supported by experimental results
- **Medium Confidence:** Claim that KL divergence's asymmetric sensitivity specifically contributes to superior performance requires further validation
- **Low Confidence:** Exact implementation details for KL divergence computation and forward pass scheduling remain unclear

## Next Checks
1. Test both argument orders for KL divergence ($KL(f(x) || f(x_{adv}))$ vs $KL(f(x_{adv}) || f(x))$) to confirm which produces expected gradient behavior
2. Compare results when computing $f(x)$ once outside optimization loop versus computing at each iteration to determine intended dynamic behavior
3. Beyond average ASR improvements, analyze transfer patterns across specific model pairs to identify which architectural similarities or differences most influence IKD's effectiveness