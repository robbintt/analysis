---
ver: rpa2
title: Turning LLM Activations Quantization-Friendly
arxiv_id: '2506.01967'
source_url: https://arxiv.org/abs/2506.01967
tags:
- quantization
- error
- rotation
- outliers
- proj
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how activation outliers in large language
  models (LLMs) impact quantization error and examines the effectiveness of channel-wise
  scaling and rotation transformations in mitigating these issues. The authors find
  that while rotation generally reduces quantization error more effectively than smoothing,
  massive outliers in certain layers can cause rotation to yield worse results than
  no transformation.
---

# Turning LLM Activations Quantization-Friendly

## Quick Facts
- arXiv ID: 2506.01967
- Source URL: https://arxiv.org/abs/2506.01967
- Reference count: 29
- Primary result: Hybrid smoothing-rotation method reduces quantization error by managing outlier distributions in LLMs

## Executive Summary
This paper investigates how activation outliers in large language models (LLMs) impact quantization error and examines the effectiveness of channel-wise scaling and rotation transformations in mitigating these issues. The authors find that while rotation generally reduces quantization error more effectively than smoothing, massive outliers in certain layers can cause rotation to yield worse results than no transformation. To address this, they propose a hybrid approach that combines channel-wise scaling with rotation, which balances the strengths of both methods. This approach significantly reduces quantization error in layers with massive outliers and consistently provides the lowest errors across other layers as well.

## Method Summary
The paper analyzes quantization error in LLaMA2-7B using 4-bit symmetric RTN quantization, measuring layer-wise error as the squared Frobenius norm of the difference between original and quantized outputs. They capture input activations via PyTorch hooks at key projection layers (k_proj, gate_proj, o_proj, down_proj) across 32 decoder layers using WikiText-2 samples. Three transformation methods are compared: channel-wise scaling (smoothing), Hadamard rotation, and a hybrid "Smooth Rotation" that applies scaling before rotation. The smoothing uses a fixed α=0.5 for online application, while rotation employs Sylvester-constructed Hadamard matrices or Kronecker decompositions for the down_proj layer. Quantization difficulty is measured as the standard deviation of channel-wise Frobenius norms.

## Key Results
- Rotation generally reduces quantization error more effectively than smoothing across most layers
- Massive outliers in down_proj layers 1 and 30 cause rotation to yield higher error than no transformation
- The hybrid Smooth Rotation method consistently provides the lowest quantization error across all layers, particularly in down_proj layers with massive outliers
- Quantization error strongly correlates with activation quantization difficulty (standard deviation of channel magnitudes), with correlation >0.97 on non-massive-outlier layers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Quantization error in LLMs is strongly predicted by the variance of channel magnitudes, not just the presence of outliers.
- Mechanism: The quantization step size Δ is determined by the maximum absolute value in a tensor (Eq. 1). Tensors with high variance in channel-wise Frobenius norms concentrate large values in few channels, forcing a large Δ that wastes precision on the majority of smaller values. The standard deviation of channel magnitudes quantifies this "quantization difficulty."
- Core assumption: Quantization noise is approximately uniformly distributed, and reducing tensor kurtosis improves representable fidelity under fixed-bit constraints.
- Evidence anchors:
  - [Section II.B]: Defines layer-wise quantization error as ∥XW − Q(X)Q(W)∥²_F and links it to step size Δ via noise variance Δ²/12.
  - [Section IV.B]: Reports >0.97 correlation between squared activation quantization difficulty and layer-wise error after omitting extreme outlier layers.
  - [Corpus]: KurTail (arXiv:2503.01483) similarly targets kurtosis reduction, supporting the flatness–error link, though no direct citation overlap exists.
- Break condition: If weight quantization difficulty dominates (e.g., gate_proj 31 shows high weight difficulty), activation-focused metrics underpredict error. Also, massive outliers in few tokens can yield high difficulty but sublinear error increase due to limited token coverage.

### Mechanism 2
- Claim: Rotation with Hadamard matrices redistributes outliers but can fail catastrophically when massive outliers exist, yielding higher error than no transformation.
- Mechanism: Hadamard rotation spreads each outlier value o_j across all output dimensions via signed summation (Eq. 7). With |O| outlier dimensions, rotated values cluster around 2^{|O|-1} centroids. The maximum post-rotation value is approximately (∑_{i∈O}|o_i|)/√d (Eq. 8), which may remain too large to sufficiently reduce Δ. In layers like down_proj 30 with massive outliers exceeding 1000, rotation produces bimodal distributions (Fig. 5a) that quantize poorly at 4-bit precision.
- Core assumption: The Hadamard matrix has zero-mean columns (equal +1/-1), and non-outlier dimensions have small variance relative to outlier magnitudes.
- Evidence anchors:
  - [Section IV.D]: Derives Eqs. 6–8 showing why rotation clusters massive outliers into discrete magnitude bands.
  - [Section IV.D, Fig. 4a]: Rotation error exceeds original baseline in down_proj layers 1 and 30 where massive outliers dominate.
  - [Corpus]: QuaRot (arXiv:2404.00456) and DuQuant (arXiv:2406.01721) report rotation success but do not emphasize this failure mode with massive outliers; corpus evidence on the specific failure mechanism is limited.
- Break condition: When the number of outlier dimensions |O| is large relative to d, or when outlier magnitudes are so extreme that even 1/√d attenuation is insufficient, rotation degrades. The paper notes d=4096 for most layers, with down_proj at d=11008.

### Mechanism 3
- Claim: Applying channel-wise scaling before rotation ("Smooth Rotation") approximately doubles the effective dimensionality for outlier redistribution, reducing maximum values more than either transformation alone.
- Mechanism: Smoothing with α=0.5 migrates part of each outlier magnitude to the corresponding weight channel via sj = √(max(|X_j|)/max(|W_j|)) (Eq. 4). The scaled outlier becomes √(|o_i|·max(|W_i|)). Subsequent rotation then spreads this value across both the activation dimensions (d) and the weight dimensions (another d), yielding max(|t̃|) ≈ (∑_{i∈O}√(|o_i|·max(|W_i|)))/√d (Eq. 9). This "dimension doubling" effect produces smaller maxima and flatter distributions.
- Core assumption: Weights have bounded per-channel maxima, and the Hadamard structure applies identically to weights post-scaling. Calibrated smoothing (α tuning) is not strictly necessary; online α=0.5 works reasonably well.
- Evidence anchors:
  - [Section IV.E]: Presents Eq. 9 and explains the dimension-doubling intuition.
  - [Section IV.E, Fig. 4]: Smooth Rotation achieves the lowest quantization difficulty for activations and substantially lower weight difficulty than smoothing alone.
  - [Corpus]: SmoothRot (arXiv:2506.05413) appears to extend this hybrid approach; the corpus title matches but abstract details are truncated—direct verification of identical claims is not possible from provided text.
- Break condition: If weights themselves have outliers (high weight quantization difficulty), smoothing can amplify weight-side error. The paper notes weight difficulty is generally low, but gate_proj 31 is an exception. Smooth Rotation also introduces calibration dependency absent in rotation-only methods.

## Foundational Learning

- Concept: **Symmetric uniform quantization with per-tensor scale**
  - Why needed here: The entire analysis assumes RTN quantization with Δ = max(|X|)/(2^{b-1}-1). Understanding why the maximum value determines precision for all values is essential to grasp why outliers are pathological.
  - Quick check question: If a tensor has values in [-100, 100] except one value at 1000, how many quantization bins are "wasted" on the [-100, 100] range in 4-bit symmetric quantization?

- Concept: **Equivalent transformations in linear layers (Y = XW = (XA)(A⁻¹W))**
  - Why needed here: Both smoothing and rotation rely on this identity to transform activations while preserving mathematical equivalence. The inverse transformation on weights is what enables migration of quantization difficulty.
  - Quick check question: If you scale activation channel j by factor 2, what must you do to weight row j to preserve the output Y?

- Concept: **Hadamard matrices as deterministic orthogonal rotations**
  - Why needed here: The paper uses Sylvester-constructed Hadamard matrices, which are fixed (not learned) and have specific structural properties (equal +1/-1 entries). Understanding why these redistribute values uniformly helps explain the centroid clustering phenomenon.
  - Quick check question: Why does a Hadamard matrix with zero-mean columns produce "spreading" rather than "shifting" when applied to a vector?

## Architecture Onboarding

- Component map: Input activations X -> Channel-wise scaling (diag(s)) -> Hadamard rotation (R) -> Quantizer -> Layer-wise error
- Critical path:
  1. Record calibration activations from WikiText-2 sample (seq_len=128).
  2. Compute per-layer quantization difficulty (std of channel magnitudes) to identify problematic layers.
  3. For down_proj layers with massive outliers, apply Smooth Rotation: compute scaling factors s, transform activations as X·diag(s)⁻¹·R and weights as R^T·diag(s)⁻¹·W.
  4. Quantize transformed tensors, measure layer-wise error, compare against original/smooth/rotate baselines.
- Design tradeoffs:
  - Smoothing alone: Flattens activations but increases weight difficulty; α tuning required per-module (paper finds α≈0.7 for o_proj, 0.65 for gate_proj).
  - Rotation alone: Best weight distributions and generally lowest error, but fails on massive outliers (down_proj 1, 30).
  - Smooth Rotation: Robust across all layers, but introduces calibration dependency and slightly higher weight difficulty than pure rotation.
  - Online vs. calibrated smoothing: Online (α=0.5 fixed) avoids overfitting but may be suboptimal; calibrated α improves specific modules.
- Failure signatures:
  - Rotation worse than original: Occurs in down_proj layers 1 and 30; visible as bimodal distribution in rotated activations (Fig. 5a) with values clustering around two distinct magnitudes.
  - Smoothing worse than original: Occurs in o_proj and gate_proj with default α=0.5; correctable by increasing α.
  - High error despite low activation difficulty: down_proj 31 shows high error due to multiple tokens with large activations and elevated weight difficulty.
- First 3 experiments:
  1. **Replicate layer-wise error profiling**: Record activations across all 32 layers of LLaMA2-7B, compute quantization difficulty (std of channel magnitudes), and correlate with measured quantization error. Verify the >0.97 correlation claim on non-massive-outlier layers.
  2. **Isolate rotation failure mode**: On down_proj 30, visualize the rotated activation distribution for the token with maximum outlier magnitude. Confirm the bimodal clustering predicted by Eq. 7 and measure whether max(|t̂|) matches (∑|o_i|)/√d.
  3. **Ablate α in Smooth Rotation**: Test α ∈ {0.3, 0.5, 0.7, 0.9} on the hybrid method for down_proj layers. Determine if α=0.5 is truly robust or if layer-specific tuning yields further gains.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the "Smooth Rotation" method preserve end-to-end generation quality, specifically measured by perplexity and downstream task accuracy?
- Basis in paper: [explicit] The authors explicitly note in the Limitations section that their evaluation "only measured layer-wise error, without assessing end-to-end metrics such as perplexity."
- Why unresolved: While layer-wise error reduction suggests compression success, it is a proxy metric that does not capture the cumulative effect of quantization noise on the model's final token generation capabilities.
- What evidence would resolve it: Perplexity scores on standard language modeling benchmarks (e.g., WikiText-2) and zero-shot accuracy results comparing the hybrid method against baseline quantization techniques.

### Open Question 2
- Question: Does the efficacy of the hybrid smoothing-rotation approach generalize to other architectures (e.g., Mistral) and significantly larger or smaller model scales?
- Basis in paper: [explicit] In Section V, the authors state that future research should "explore additional architectures (e.g., Mistral [25]) and a range of model sizes" to fully validate the method.
- Why unresolved: The study is restricted to LLaMA2-7B; outlier distributions and the resulting quantization difficulty may behave differently in models with alternative attention mechanisms or parameter counts.
- What evidence would resolve it: Experiments replicating the quantization difficulty analysis and error reduction results on diverse model families (e.g., Mistral, LLaMA-70B).

### Open Question 3
- Question: To what extent does the choice of calibration dataset influence the effectiveness of the smoothing step in the hybrid approach?
- Basis in paper: [explicit] Section V lists the need to "examine the influence of different calibration sets on the results" as a direction for future work.
- Why unresolved: Unlike rotation, channel-wise scaling relies on activation statistics derived from sample data; if these statistics are not robust, the method's ability to mitigate massive outliers may vary.
- What evidence would resolve it: A sensitivity analysis comparing quantization error and perplexity when the scaling factors are derived from distinct data domains (e.g., code vs. natural text vs. instruction data).

## Limitations

- The study is limited to LLaMA2-7B architecture and WikiText-2 dataset, which may not generalize to other model families or data distributions
- The paper only measures layer-wise quantization error without assessing end-to-end metrics like perplexity or downstream task accuracy
- The hybrid method introduces calibration dependencies that may affect its robustness across different models or quantization schemes

## Confidence

- **High confidence**: The correlation between activation quantization difficulty (std of channel magnitudes) and quantization error (>0.97 on non-massive-outlier layers), and the basic mechanism of how outliers force large Δ values. These are empirically well-supported and theoretically sound.
- **Medium confidence**: The hybrid "Smooth Rotation" method consistently providing the lowest quantization error across all layers. While the paper demonstrates this empirically, the method introduces calibration dependencies and weight-side effects that may vary with different models or quantization schemes.
- **Low confidence**: The specific numerical predictions from Eqs. 7-9 about rotated value distributions and the dimension-doubling effect. These are theoretically derived but would benefit from more extensive empirical validation across different outlier distributions and matrix sizes.

## Next Checks

1. **Cross-architecture validation**: Apply the same methodology to LLaMA2-13B or another transformer architecture to verify whether rotation failure modes and the hybrid method's effectiveness generalize beyond the 7B model.

2. **Outlier threshold characterization**: Systematically vary outlier magnitudes in down_proj layers and measure the precise point where rotation transitions from beneficial to harmful, testing the theoretical predictions of Eqs. 7-9 against empirical distributions.

3. **Calibration-free hybrid variant**: Develop and test a method that combines the benefits of smoothing and rotation without requiring α calibration, potentially by using fixed mixing ratios or adaptive schemes based on layer-specific statistics.