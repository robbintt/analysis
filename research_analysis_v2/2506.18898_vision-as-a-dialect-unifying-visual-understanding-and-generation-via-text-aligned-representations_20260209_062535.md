---
ver: rpa2
title: 'Vision as a Dialect: Unifying Visual Understanding and Generation via Text-Aligned
  Representations'
arxiv_id: '2506.18898'
source_url: https://arxiv.org/abs/2506.18898
tags:
- image
- generation
- visual
- arxiv
- understanding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a unified multimodal framework that bridges\
  \ visual understanding and generation using a shared discrete semantic representation.\
  \ The core innovation is the Text-Aligned Tokenizer (TA-Tok), which converts images\
  \ into text-aligned discrete tokens by projecting from a large language model\u2019\
  s vocabulary, enabling seamless cross-modal input and output."
---

# Vision as a Dialect: Unifying Visual Understanding and Generation via Text-Aligned Representations

## Quick Facts
- arXiv ID: 2506.18898
- Source URL: https://arxiv.org/abs/2506.18898
- Reference count: 40
- Unified multimodal model achieves state-of-the-art results on both visual understanding and generation benchmarks while training faster than existing methods

## Executive Summary
This paper introduces Tar, a unified multimodal framework that bridges visual understanding and generation using a shared discrete semantic representation. The core innovation is the Text-Aligned Tokenizer (TA-Tok), which converts images into text-aligned discrete tokens by projecting from a large language model's vocabulary, enabling seamless cross-modal input and output. Scale-adaptive encoding and decoding balance efficiency and visual detail, while two complementary de-tokenizers—an autoregressive model and a diffusion-based model—generate high-fidelity images from the discrete tokens. Advanced pre-training tasks enhance modality fusion, improving both visual understanding and generation. Experiments show that Tar matches or surpasses existing multimodal methods on benchmarks, achieving faster convergence and greater training efficiency.

## Method Summary
The approach trains a unified MLLM through three stages: (1) TA-Tok learns to tokenize images into discrete tokens aligned with LLM vocabulary using a projection matrix from frozen LLM embeddings; (2) Two generative de-tokenizers are trained—an autoregressive VQVAE and a diffusion model—to reconstruct images from discrete tokens; (3) A unified MLLM with expanded vocabulary is pre-trained on mixed understanding (I→T) and generation (T→I) tasks with cross-entropy loss. The model uses scale-adaptive pooling to control token granularity, with 729 tokens for detailed understanding and 169 for efficient generation.

## Key Results
- Achieves competitive performance on POPE (92.9% accuracy) and MME (93.4%) visual understanding benchmarks
- Matches or exceeds existing methods on generation tasks including GenEval and DPG-Bench
- Demonstrates faster convergence during training compared to separate understanding and generation models
- Joint training improves generation performance by 5.3% compared to separate training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Initializing the visual codebook from pre-trained LLM embeddings may reduce the alignment gap between visual and textual tokens, facilitating unified cross-modal processing.
- Mechanism: The Text-Aligned Codebook (TA-Tok) projects a subset of frozen LLM token embeddings into a visual codebook via a learned projection matrix. Visual features are quantized to the nearest codebook entry, ensuring each visual token is a transformed version of a text token's representation.
- Core assumption: The LLM's embedding space already contains a rich semantic structure that can be repurposed to encode visual concepts without extensive retraining.
- Evidence anchors:
  - [abstract] "TA-Tok, which converts images into discrete tokens using a text-aligned codebook projected from a large language model's (LLM) vocabulary."
  - [Section 3.1] "To align visual and textual tokens in the latent space of LLM, we initialize the VQ codebook using the token embeddings of a pretrained LLM... we only train the projection matrix W."
  - [corpus] Related work on unified tokenizers (e.g., DualToken, AToken) similarly explores aligning visual vocabularies with language models, though the specific LLM-projection approach is distinctive here.
- Break condition: The benefit is likely reduced if the LLM's vocabulary is not sufficiently large or semantically diverse to cover visual concepts, or if the projection matrix learning is unstable.

### Mechanism 2
- Claim: Scale-adaptive tokenization allows the model to trade off between detail preservation and computational efficiency, tailoring the representation to the task.
- Mechanism: Scale-Adaptive Pooling (SAP) reduces the spatial resolution of visual features before quantization, producing variable-length token sequences (e.g., 729, 169, or 81 tokens for a 384px image). This directly controls the granularity of information passed to the LLM.
- Core assumption: Visual understanding tasks (e.g., OCR, detailed QA) require fine-grained features, while generation tasks can operate effectively on coarser semantic tokens.
- Evidence anchors:
  - [abstract] "Additionally, we propose scale-adaptive encoding and decoding to balance efficiency and visual detail."
  - [Section 3.1] "Given image features zI, we apply SAP with scale factor s ∈ {1, 2, 3} to obtain zpI = SAP(zI, s), allowing control over visual details based on task needs or compute budget."
  - [Table 5] Shows understanding performance improves with more tokens (729 tokens: 92.9% accuracy), while generation performance is less sensitive (169 tokens often sufficient).
- Break condition: The mechanism may fail if the optimal scale is not selected correctly during inference, or if the model overfits to a specific scale during training.

### Mechanism 3
- Claim: Using a shared discrete representation for both understanding and generation can enable mutual reinforcement between the two tasks when trained jointly.
- Mechanism: Both visual understanding (image-to-text) and generation (text-to-image) are formulated as sequences of discrete tokens predicted by the same LLM with a unified vocabulary. This forces the model to learn a single representation space suitable for both encoding and decoding.
- Core assumption: The visual tokens learned for generation (capturing semantics) are also sufficiently informative for understanding, and vice versa.
- Evidence anchors:
  - [abstract] "By integrating vision and text into a unified space with an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input and output through a shared interface."
  - [Section 3.3] "Built on TA-Tok and Generative De-Tokenizers, we propose a unified MLLM... with a simple autoregressive objective and eliminating the need for modality-specific designs."
  - [Table 6] Shows joint training improves generation performance (+5.3% for Tar) compared to separate training, suggesting positive transfer.
- Break condition: Benefits may not appear if the tasks have conflicting optimization landscapes (e.g., pixel-perfect reconstruction vs. high-level semantics), or if the de-tokenizer cannot recover sufficient detail from the discrete tokens.

## Foundational Learning

- **Vector Quantization (VQ)**:
  - Why needed here: TA-Tok uses VQ to discretize continuous visual features. Understanding the codebook lookup, quantization loss, and the stop-gradient trick is essential to grasp how visual tokens are created and aligned.
  - Quick check question: In Eq. (3), why is the stop-gradient `sg(·)` applied to both the codebook `C` and the quantized features `zq` in different terms of the loss?

- **Autoregressive (AR) Language Modeling**:
  - Why needed here: The entire Tar model is an AR LLM that predicts the next token, whether text or visual. The training objective (Cross-Entropy) and inference process (sampling tokens sequentially) are fundamental.
  - Quick check question: How does the unified vocabulary (combining text tokens and visual tokens) change the output head and softmax computation during training?

- **Diffusion Models**:
  - Why needed here: The diffusion-based de-tokenizer (Dif-DTok) is a key option for high-fidelity image generation. Understanding how it conditions on the discrete visual tokens `zq` via cross-attention is important.
  - Quick check question: In the Dif-DTok, what is the role of the noised latent `yt` and the denoising function `F(yt, zq; θdif)` as described in Eq. (5)?

## Architecture Onboarding

- **Component map**:
  Input: Image → SigLIP2 Encoder → Scale-Adaptive Pooling (SAP) → Text-Aligned Codebook (VQ) → Discrete Visual Tokens → LLM Core → Predicts next token (text or visual) → Output for Generation: Predicted visual tokens → Generative De-Tokenizer (AR-DTok or Dif-DTok) → Image → Output for Understanding: Predicted text tokens → Text response

- **Critical path**:
  1. Tokenizer Training: Train TA-Tok (encoder, SAP, codebook, decoder) on images to learn the VQ mapping with reconstruction and codebook losses
  2. De-Tokenizer Training: Separately train AR-DTok and/or Dif-DTok to map TA-Tok's discrete tokens to high-quality images
  3. LLM Pretraining & Finetuning: Initialize the LLM's embedding layer with the expanded vocabulary (including the TA codebook), then train on mixed multimodal tasks (I→T, T→I, I→I, TI→I)

- **Design tradeoffs**:
  - AR-DTok vs. Dif-DTok: AR is faster and aligns with the AR LLM but may produce lower visual fidelity. Diffusion yields higher quality but is slower. The choice depends on the application's speed/quality priority
  - Token Scale (729 vs. 169 vs. 81): More tokens capture more detail but increase LLM compute. The paper suggests 169 tokens are a practical balance for generation, while understanding benefits from more
  - Shared vs. Separate Representations: Sharing risks a single representation being suboptimal for both tasks, but the paper shows potential for mutual benefit. The corpus notes other works also struggle with this balance (e.g., UniTok, VILA-U)

- **Failure signatures**:
  - Reconstruction loss (L_rec) remains high during TA-Tok training: The encoder-decoder is not learning to preserve semantic information. Check SigLIP2 teacher outputs and projection matrix initialization
  - Generated images lack detail or coherence: The de-tokenizer may be undertrained or not properly conditioned. Verify the cross-attention mechanism in Dif-DTok or the AR-DTok's training data quality
  - Visual understanding benchmarks perform poorly: The model may be overfitting to generation tasks. Adjust the data ratio (e.g., more I2T data) or inspect if the discrete tokens are losing critical information for understanding

- **First 3 experiments**:
  1. Tokenizer Ablation: Train TA-Tok with a randomly initialized codebook vs. the LLM-projected codebook. Compare reconstruction quality and downstream LLM performance to validate Mechanism 1
  2. Scale Sensitivity Analysis: For a fixed task (e.g., text-to-image generation), sweep the SAP scale (1, 2, 3) and measure FID/CLIP score vs. inference latency to find the optimal tradeoff, testing Mechanism 2
  3. Joint Training Benefit Test: Train two LLMs—one on understanding data only, one on mixed understanding+generation data. Compare their performance on a held-out understanding benchmark to quantify the transfer effect suggested in Mechanism 3

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can techniques like longer visual sequences and Token-Shuffle effectively mitigate quantization errors in TA-Tok for fine-grained understanding tasks such as OCR?
- Basis in paper: [explicit] The authors state in Appendix I: "the vector quantization in TA-Tok introduces quantization errors. Despite mitigating this with a larger codebook and extended training, some information loss remains—particularly in tasks requiring fine-grained understanding, such as OCR. This could be improved by adopting longer visual sequences and incorporating techniques like Token-Shuffle."
- Why unresolved: The paper acknowledges the limitation but does not implement or evaluate the proposed solutions.
- What evidence would resolve it: Experiments comparing OCR and fine-grained understanding performance with standard TA-Tok versus variants using longer sequences and Token-Shuffle techniques.

### Open Question 2
- Question: Can training the de-tokenizer as an image super-resolution model improve input image reconstruction accuracy compared to traditional tokenizers?
- Basis in paper: [explicit] The authors note in Appendix I: "while our method excels at generating diverse images, it underperforms in accurately reconstructing the input image compared to traditional tokenizers. This limitation stems from using generative models as de-tokenizers. A potential solution is to train the de-tokenizer as an image super-resolution model to better preserve local consistency."
- Why unresolved: The proposed solution is suggested but not implemented or validated experimentally.
- What evidence would resolve it: Quantitative reconstruction metrics (e.g., FID, LPIPS) comparing standard de-tokenizers against super-resolution-trained variants on image autoencoding tasks.

## Limitations

- Limited analysis of semantic fidelity: While the paper demonstrates strong benchmark performance, there is limited ablation or qualitative analysis of whether the discrete visual tokens truly preserve high-level semantic information versus just low-level visual details
- Scale selection sensitivity: The paper identifies 169 tokens as a practical balance for generation, but the optimal scale likely varies by domain and task complexity, with no analysis of how sensitive the system is to suboptimal scale selection during inference
- De-tokenizer specialization vs. generalization: The AR-DTok and Dif-DTok are trained on different data distributions (aesthetic LAION vs. FLUX-generated synthetic images), raising questions about whether they're truly learning to invert TA-Tok or simply memorizing their respective training distributions

## Confidence

- High confidence: The technical implementation details for TA-Tok construction, scale-adaptive pooling, and the three-stage training pipeline are clearly specified and reproducible. The benchmark results are measurable and directly comparable to baselines.
- Medium confidence: The claims about mutual reinforcement between understanding and generation tasks (Mechanism 3) are supported by joint training results, but the causal mechanism and generalizability to other task combinations remain unclear.
- Low confidence: The assertion that initializing from LLM embeddings specifically enables "unified cross-modal processing" (Mechanism 1) lacks comparative evidence against alternative initialization schemes or alignment methods.

## Next Checks

1. **Semantic alignment verification**: Extract visual tokens from a diverse set of images, then decode them back to images. Compute CLIP similarity between original and reconstructed images, and also measure whether the LLM can accurately caption the reconstructed images. This validates whether TA-Tok preserves semantic information beyond pixel-level reconstruction.

2. **Scale sensitivity benchmark**: Run a controlled experiment where the same generation model is evaluated across multiple scales (729, 169, 81 tokens) on a fixed test set, measuring not just FID/CLIP scores but also human perceptual quality ratings. This would quantify the true tradeoff between detail preservation and computational efficiency.

3. **Cross-de-tokenizer evaluation**: Take TA-Tok-generated tokens and attempt reconstruction with both AR-DTok and Dif-DTok (even if one was not originally trained on those tokens). Measure reconstruction quality to determine whether the de-tokenizers are truly learning the TA-Tok space or simply fitting to their training distributions.