---
ver: rpa2
title: Model Merging Scaling Laws in Large Language Models
arxiv_id: '2509.24244'
source_url: https://arxiv.org/abs/2509.24244
tags:
- merging
- experts
- tail
- scaling
- e-04
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a predictive scaling law for language model
  merging that links model size and the number of merged experts via a floor+tail
  power law. The law shows that larger models lower the irreducible floor while gains
  from adding experts follow a 1/(k+b) diminishing returns pattern, with most improvement
  arriving by the first few experts.
---

# Model Merging Scaling Laws in Large Language Models

## Quick Facts
- arXiv ID: 2509.24244
- Source URL: https://arxiv.org/abs/2509.24244
- Reference count: 40
- Primary result: Introduces a predictive scaling law for language model merging that tightly fits loss curves (R² > 0.98) and enables budget-aware expert selection

## Executive Summary
This paper establishes a predictive scaling law for merging language models that links model size (N) and the number of merged experts (k) via a floor+tail power law. The law shows that larger models lower an irreducible loss floor while gains from adding experts follow diminishing returns of ~1/(k+b), with most improvement arriving by the first few experts. Across extensive experiments covering 10,506 models, nine domains, and four merging methods, the law tightly fits measured curves and enables three-point prediction of the full loss-vs-k curve for budget-aware planning.

## Method Summary
The study fine-tunes domain experts from Qwen2.5 base models (0.5B-72B) across nine domains, then merges them using four methods: Average, TA (λ=0.8), TIES (λ=0.5/1.0), and DARE (density 0.2). Task vectors (θᵢ - θ₀) are extracted and aggregated with equal weights (αᵢ,ₖ = c/k). The unified scaling law L = L* + B·N⁻ᵝ + (A₀·N⁻ᵞ)/(k+b) is fit via weighted nonlinear least squares on token-level cross-entropy evaluated across 30M held-out tokens per domain.

## Key Results
- Floor+tail power law fits measured loss curves with R² > 0.98 across all tested conditions
- Larger models lower the irreducible loss floor (L∞ decreases with N) while method differences compress at scale
- Most merging gains arrive by k≈5-6 experts, following ~1/(k+b) diminishing returns
- Three-point calibration (k={1,2,4}) predicts full loss-vs-k curves with <2% MAPE error

## Why This Works (Mechanism)

### Mechanism 1: Size-Dependent Floor (L∞(N) = L* + B·N⁻ᵝ)
Larger base models depress an irreducible loss floor by improving base model's curvature properties. The floor term includes c·g⊤μ + ½c²·μ⊤H·μ, where Hessian curvature and gradient improve with scale. Larger models have "flatter" loss landscapes around good solutions, reducing interference when task vectors are combined. Core assumption: loss function is twice continuously differentiable with Lipschitz Hessian near θ₀.

### Mechanism 2: Inverse-k Tail (A(N)/(k+b))
Merging gains follow diminishing returns as ~1/(k+b) due to variance reduction in parameter space. When k expert vectors are averaged with equal weights, the variance scales as Σ/k. The 1/k decay arises from "washing out" directional conflicts between task vectors. Core assumption: equal-normalization merging with i.i.d. task vectors having finite sixth moments.

### Mechanism 3: Method Convergence at Scale
Differences between merging methods compress as k and N grow because all methods share the floor+tail functional form. The floor L∞(N) is base-model-dependent, not method-dependent. Method differences appear in tail amplitude A(N), but as k increases, the A/(k+b) term shrinks regardless of method, leaving the common floor dominant.

## Foundational Learning

- **Power Law Scaling (L = L* + B·N⁻ᵝ)**: Essential for understanding how exponents control improvement rates with scale. Quick check: If β = 0.4, what happens to floor term when model size doubles?
- **Task Vectors and Weight-Space Merging**: Critical for understanding that merging operates in parameter space, not output space. Quick check: Why does averaging task vectors not guarantee averaging of outputs?
- **Expectation over Random Subsets**: Important for understanding why variance contracts with k - more samples from expert pool reduce subset-to-subset variability. Quick check: If you merge 3 experts from pool of 9, how many possible subsets exist?

## Architecture Onboarding

- **Component map**: Base model (θ₀, size N) -> Expert pool (M) -> Merge operator (Ψ) -> Scaling combiner (θ = θ₀ + Σᵢ αᵢ,ₖ·Ψ(vᵢ)) -> Loss evaluator (CE on held-out tokens)
- **Critical path**: Train base model → Fine-tune k domain experts → Extract task vectors → Apply method-specific processing → Aggregate with equal weights → Evaluate on cross-entropy
- **Design tradeoffs**: Prioritize larger N over k under fixed compute (N lowers floor, k shortens tail); choose TIES/TA for small k variance reduction; domain synergy matters for pool diversity
- **Failure signatures**: Late-k uptick with TIES(λ=1) → reduce λ to 0.5; high variance at small k → expect 1/k variance shrinkage; floor not decreasing → check base model quality; 1/k fit fails → check expert pool homogeneity
- **First 3 experiments**: 1) Validate law on your data with 9 experts, k∈{1,2,3,5,9}, fit L∞ + A/(k+b), confirm R²>0.95; 2) Three-point prediction using k={1,2,4}, predict k=9 loss, error <2% MAPE; 3) Compare Average vs TIES(0.5) at target k, use Average if gap <1%

## Open Questions the Paper Calls Out

- **Adaptive Weighting Generalization**: Does the floor+tail scaling law hold with adaptive weighting schemes rather than equal-normalization? The theoretical derivation relies specifically on equal weights, and empirical validation is restricted to uniform averaging.
- **Cross-Modal Extension**: Can the scaling law be generalized to vision models and discrete metrics like accuracy or robustness? The current study is restricted to LLMs on text domains, while scaling laws often differ between continuous losses and discrete metrics.
- **Expert Selection Optimization**: Can expert selection policies be designed to minimize theoretical constants (floor L∞ or tail A(N)) identified in the scaling law? The paper observes random ordering variance fades but doesn't test if selection based on Hessian/dispersion could lower the mean curve.

## Limitations

- Scaling law derivation relies on idealized assumptions about task vector distributions and equal-weight averaging
- Base model floor L∞(N) is empirically observed but not proven across all model families or training regimes
- 1/(k+b) tail form assumes independent task vectors with finite sixth moments - real expert pools may violate these conditions

## Confidence

- **High Confidence**: Overall floor+tail functional form fits data well (R² > 0.98); diminishing returns pattern consistently observed; method convergence at scale empirically validated
- **Medium Confidence**: Theoretical justification for power law exponents depends on smoothness assumptions; claim that larger N universally lowers floor assumes base model quality
- **Low Confidence**: Predictions for k > 9 or N > 72B are extrapolations; behavior with highly correlated expert pools or non-equal weighting not characterized

## Next Checks

1. **Domain Diversity Stress Test**: Create expert pools with increasing task vector correlation and verify whether 1/(k+b) scaling holds or requires modification
2. **Base Model Quality Sensitivity**: Compare scaling law fits using base models with known training instabilities versus well-trained checkpoints to isolate impact of floor reliability
3. **Architectural Transfer**: Apply scaling law to non-Qwen2.5 architectures (Llama, Mistral) and non-decoder-only models to test universality across model families