---
ver: rpa2
title: Emergent effects of scaling on the functional hierarchies within large language
  models
arxiv_id: '2501.07359'
source_url: https://arxiv.org/abs/2501.07359
tags:
- layers
- were
- figure
- these
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the functional architecture of large language
  models by examining how information is represented across different layers. Using
  a systematic approach of feeding simple texts to Llama models and fitting classifiers
  to predict various properties from layer activations, the research challenges the
  traditional view of a steady abstraction hierarchy.
---

# Emergent effects of scaling on the functional hierarchies within large language models

## Quick Facts
- arXiv ID: 2501.07359
- Source URL: https://arxiv.org/abs/2501.07359
- Reference count: 7
- Smaller models show traditional hierarchical abstraction; larger models exhibit emergent double peaks and layer coordination

## Executive Summary
This study investigates how information representation evolves across transformer layers in large language models. Using controlled synthetic texts and probing classifiers, the research systematically examines whether layer depth correlates with abstraction level—from simple item features to complex analogies. While smaller models (3B parameters) display expected hierarchical patterns with item semantics early and higher-order relations later, larger models (70B parameters) reveal unexpected emergent behaviors including double-peak representations and coordinated fluctuations between adjacent layers. These findings challenge simplistic interpretations of layer-wise abstraction and suggest that scaling introduces fundamentally different functional architectures that complicate interpretability efforts.

## Method Summary
The study uses synthetic texts fed into Llama models with classifiers trained on layer activations to predict semantic properties. For each input (e.g., "An apple"), activations are extracted from three components per layer: residual stream input, attention output, and FFN output. SVMs (binary labels) and ridge regressions (continuous ratings) are fitted per layer using 6-fold cross-validation with group-structured folds to prevent leakage. Experiments probe item-level features, two-item relations, four-item analogies, and buried concepts (with ~100-word fillers). The 3B model (28 layers) and 70B model (80 layers) are compared to identify scaling effects on functional hierarchies.

## Key Results
- Smaller models show expected hierarchical patterns: item semantics peak early (layers 2-7), relations mid-layers (layers 8-12), analogies later (layers 10-15)
- Larger models exhibit emergent double peaks in semantic representation around layers 12-16 and 25-33, with a valley between
- Adjacent layers in large models show anti-persistent fluctuations (ρ ≈ -0.33), suggesting coordinated specialization rather than monotonic refinement
- Buried concepts remain decodable from deep layers, though whether this constitutes true abstraction remains unclear

## Why This Works (Mechanism)

### Mechanism 1
Semantic information is linearly decodable from specific transformer layers, with abstraction level correlating to layer depth in smaller models. SVMs and ridge regressions fitted to layer activations can predict text properties because representations become increasingly linearly separable at the layers responsible for that abstraction level. This assumes high classifier accuracy indicates genuine encoding rather than spurious correlation. Evidence shows item-level semantics peak at layers 2-7, relations at layers 10-11, and analogies slightly later. The mechanism fails if classifiers achieve high accuracy on shuffled labels or if accuracy doesn't degrade predictably with abstraction level.

### Mechanism 2
Large models (70B parameters) exhibit emergent "double-peak" representation patterns not present in smaller models, suggesting multiple parallel processing phases. The 70B model shows semantic property representation peaking twice (layers 12-16, then 25-33) with a valley between, potentially reflecting two functional hierarchies: one for local/sentence-level semantics, another integrating global context. This assumes double peaks reflect distinct computational phases rather than training artifacts. Evidence shows consistent double peaks across four experiments, with the valley coinciding with buried-concept representation rise. The pattern may be artifacts if double peaks disappear with different random seeds or learning rates.

### Mechanism 3
Adjacent attention layers in large models coordinate through anti-persistent fluctuations—alternating between strong and weak representation of specific information. Rather than each layer monotonically refining representations, attention outputs zigzag with negative autocorrelation (ρ ≈ -0.33) and consistent high/low layer patterns across experiments. This assumes anti-persistence reflects intentional functional coordination, not optimization noise. Evidence shows first-derivative fluctuations and correlation matrices indicating consistent patterns. The coordination may be noise if anti-persistence weakens in larger models or patterns differ across training runs.

## Foundational Learning

- **Linear Probing**: Training linear classifiers on activations reveals what information is linearly encoded at each layer. Quick check: If a layer achieves 85% accuracy on "is edible" classification, this tells you that layer represents the concept in a way that's linearly separable from other features.

- **Residual Stream vs. Attention vs. FFN Outputs**: The paper probes three distinct components per layer, each showing different representation patterns. Quick check: Attention outputs might show item-level semantics even when no meaningful word relationships exist because attention mechanisms can capture surface-level associations independent of semantic coherence.

- **Cross-Validation with Grouping**: Experiment 2C uses group-structured folds to ensure classifiers learn relations, not object-specific shortcuts. Quick check: If you used random folds and the classifier memorized which specific animals eat which foods, your conclusions would incorrectly attribute performance to general relation learning rather than memorization of specific instances.

## Architecture Onboarding

- **Component map**: Residual stream input -> Attention output (largest swings in 70B) -> FFN output (more stable); Layers 1-7: Item-level semantics; Layers 8-15: Two-item relations and analogies; Layers 12-16 (70B): First semantic peak; Layers 25-33 (70B): Second semantic peak; Layers 35+: Buried information; Zigzag coordination begins (~layer 40+)

- **Critical path**: Extract activations at each layer for target tokens; For multi-token words, average activations; Fit SVMs (binary) or ridge regression (continuous) per layer; Use appropriate cross-validation grouping to prevent leakage; Plot accuracy by layer to identify peaks/valleys

- **Design tradeoffs**: SVM vs. Ridge (binary vs. continuous labels); Token averaging vs. last-token (averaging better for multi-token words, but model/task-specific); Attention vs. FFN vs. Residual (attention shows largest fluctuations; FFN more stable; probe all three)

- **Failure signatures**: Chance-level accuracy (information not linearly encoded); Monotonic increase without peaks (probe memorization or data leakage); Inconsistent patterns across similar tasks (check for bugs); No difference between buried and non-buried (filler text processing error)

- **First 3 experiments**: 1) Replicate item-level semantics (Exp 1): Take 50 common objects with binary features; extract Llama-3.2-3B activations; fit SVMs per feature; confirm peak at layers 2-7. 2) Test two-item relations (Exp 2B simplified): Create 30 word pairs rated for relatedness; probe whether relation information peaks deeper than item semantics. 3) Check for double peaks in larger model: Run Experiment 2 on Llama-3.3-70B; look for two distinct peaks in layers 12-16 and 25-33.

## Open Questions the Paper Calls Out

### Open Question 1
What is the functional cause of the "double-peak" pattern observed in the 70b model's representation of two-item relations and analogies? The author states "It is unclear what interpretation can unify these results" and speculates whether this reflects "two functional hierarchies" or a process where "the model [references] global information to update local representations." This remains unresolved because the analyses are correlational rather than causal. Causal intervention experiments—ablating valley layers or patching activations—would determine if double peaks represent independent processing streams.

### Open Question 2
Why do adjacent attention layers in large models coordinate via "anti-persistence" (fluctuating specialization)? The author notes "large fluctuation in the back halves of the attention-layer outputs" and admits "It is unclear what interpretation can unify these results," suggesting the pattern is "curious" and requires further work. While the paper quantifies anti-persistence (negative autocorrelation), it doesn't determine if this zigzag pattern serves a functional role or is a byproduct of training dynamics. Mechanistic interpretability analysis examining OV circuits would reveal if adjacent layers systematically correct errors or specialize in orthogonal features.

### Open Question 3
Does the persistence of "buried" information in deep layers constitute semantic abstraction or mere compression? The author explicitly questions "Whether this coding should be seen as true abstraction is less obvious," suggesting deep layers might "broadly compress information... without meaningful abstraction." The probes successfully classify simple properties from long text, but this linear separability doesn't confirm the model utilizes this information for higher-order abstract reasoning. Testing whether these deep-layer "buried" representations can solve complex reasoning tasks requiring that information would distinguish abstraction from compression.

## Limitations
- Distinguishing functional architectural patterns from training artifacts remains uncertain, particularly for double-peak phenomena
- Methodology assumes linear separability indicates genuine encoding, potentially conflating correlation with causation
- Reliance on controlled synthetic texts limits generalizability to naturalistic language processing

## Confidence

**High Confidence**: Smaller models show clear hierarchical abstraction patterns—item semantics early, relations mid-layers, analogies later—with consistent accuracy peaks across experiments.

**Medium Confidence**: Double-peak patterns in the 70B model represent a genuine scaling effect, given consistency across experiments and absence in smaller models, though functional interpretation remains tentative.

**Low Confidence**: Interpretation of anti-persistent fluctuations as intentional functional coordination is speculative, with alternative explanations involving optimization dynamics or random walk behaviors.

## Next Checks

1. **Cross-seed validation**: Train three independent Llama-3.3-70b-Instruct models from different random seeds. Compare double-peak layer positions and anti-persistence correlation patterns across seeds to determine whether these are robust architectural features or seed-dependent artifacts.

2. **Architectural ablation study**: Create a modified 70B model with layer-wise attention/FFN connections disabled at different depths. Test whether double peaks persist when removing late-layer coordination mechanisms, helping distinguish whether the pattern requires specific architectural components or emerges from scale alone.

3. **Control task analysis**: Design control experiments using shuffled labels, random word pairs, or semantically meaningless strings. Compare accuracy patterns and layer-wise representations between meaningful and meaningless inputs to determine whether high accuracy necessarily indicates genuine semantic encoding versus dataset-specific correlations.