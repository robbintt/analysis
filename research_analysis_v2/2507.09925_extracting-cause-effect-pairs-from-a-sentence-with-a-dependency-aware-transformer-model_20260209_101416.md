---
ver: rpa2
title: Extracting Cause-Effect Pairs from a Sentence with a Dependency-Aware Transformer
  Model
arxiv_id: '2507.09925'
source_url: https://arxiv.org/abs/2507.09925
tags:
- dependency
- token
- bert
- extraction
- depbert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DEPBERT, a transformer-based model that integrates
  dependency tree structures into the token classification framework for extracting
  cause-effect pairs from sentences. The model leverages both traditional BERT embeddings
  and a dependency-aware channel that considers parts-of-speech tags and dependency
  relations to improve causality extraction.
---

# Extracting Cause-Effect Pairs from a Sentence with a Dependency-Aware Transformer Model

## Quick Facts
- arXiv ID: 2507.09925
- Source URL: https://arxiv.org/abs/2507.09925
- Reference count: 18
- Primary result: DEPBERT achieves 10.3–14.9% F1 and 10.3–11.6% accuracy improvements over baselines

## Executive Summary
This paper introduces DEPBERT, a transformer-based model that integrates dependency tree structures into token classification for extracting cause-effect pairs from sentences. The model uses a dual-channel architecture combining standard BERT attention with dependency-aware attention that restricts information flow to syntactically related tokens. Experiments on three datasets (SemEval, SCITE, and CAUSAL GPT) demonstrate significant improvements over baseline methods, with F1 scores reaching 0.939-0.963 and exact match accuracy of 0.834-0.858. The work shows that incorporating dependency structure enhances causality extraction, though performance depends on dataset characteristics and the model is limited to English text.

## Method Summary
DEPBERT is a two-tower model that combines standard BERT attention with dependency-aware attention. The left tower uses a pretrained BERT-base encoder, while the right tower applies GAT-style attention restricted to tokens connected in a dependency graph parsed by SpaCy. Both towers incorporate POS tag embeddings. Outputs from both towers are fused using a learned gating mechanism that dynamically weights semantic versus syntactic information. The model performs token classification with four labels (Special/Cause/Effect/Other) and is trained with cross-entropy loss using Adam optimizer (lr=0.001, batch=128) with early stopping.

## Key Results
- DEPBERT achieves 10.3–14.9% F1 score improvements over baselines including BERT, Sentence-BERT, and previous supervised approaches
- Exact match accuracy improves by 10.3–11.6% across all three datasets
- Gated fusion of dependency-aware and standard transformer representations yields the best performance
- Dependency-aware channel outperforms POS-only enhancements, demonstrating the importance of syntactic structure
- Performance varies significantly across datasets, with lower scores on smaller human-annotated datasets

## Why This Works (Mechanism)

### Mechanism 1: Dependency-Constrained Attention
- Claim: Incorporating dependency graph structure into transformer attention improves cause-effect extraction by constraining attention to syntactically relevant tokens.
- Mechanism: DEPBERT uses a dual-channel architecture where one channel applies standard self-attention while the second channel restricts attention propagation to tokens connected via dependency tree edges. The affinity score aik = (viW1) * (vkW2)^T is computed only for connected token pairs, and the weighted sum oi aggregates information from syntactically related tokens rather than all tokens.
- Core assumption: Syntactic dependencies carry semantic relation signals that general self-attention may dilute across irrelevant token interactions.
- Evidence anchors: [abstract] "incorporating dependency tree of a sentence within the model framework" yields improvements over baseline methods; [section 4.4, Table 2-4] "BERT plus dependency method, which we designed, outperforms other baselines. This clearly demonstrates the significance of the dependency relation over POS tags"; [corpus] Weak direct corpus support; "Hallucination-Resistant Relation Extraction via Dependency-Aware Sentence Simplification" shows related dependency-aware RE improvements but targets different task.

### Mechanism 2: POS Tag Disambiguation
- Claim: Parts-of-speech tags provide disambiguating semantic context that improves causal phrase boundary detection.
- Mechanism: Token embeddings incorporate POS tag IDs alongside positional and input IDs, summed into a unified representation vi. This explicitly encodes whether a token functions as NOUN, VERB, etc., helping distinguish patterns like NOUN-VERB-NOUN in causal constructions.
- Core assumption: Causal patterns exhibit consistent POS signatures (e.g., cause as NOUN subject, causal verb, effect as NOUN object) that transformers alone may not reliably capture.
- Evidence anchors: [section 3.2] "u needs to be a NOUN, causes a VERB, and w be another NOUN. So for this particular pattern, dependency relation as well as NOUN, VERB, NOUN sequence can be important"; [section 4.4] "BERT plus POS tags" achieves 5.9-11.5% F1 improvement over Dasgupta baseline across datasets; [corpus] No direct corpus validation of POS-specific contributions to causality extraction.

### Mechanism 3: Gated Fusion of Representations
- Claim: Gated fusion of dependency-aware and standard transformer representations enables the model to dynamically weight syntactic vs. semantic signals.
- Mechanism: The gate computes es_i = σ(eb_i * W6 + c), then ei = es_i ⊙ eb_i + (1 - es_i) ⊙ et_i, where eb_i comes from the BERT tower and et_i from the dependency-aware tower. This allows per-token interpolation between pure semantic embeddings and syntax-constrained embeddings.
- Core assumption: Optimal causality extraction requires balancing general semantic understanding (BERT) with explicit syntactic structure (dependency channel) on a token-specific basis.
- Evidence anchors: [section 3.3, equation] Explicit gated combination formula provided; [section 4.4, Tables 2-4] "DEPBERT (Gated)" achieves highest F1 (0.939-0.963) and exact match accuracy (0.834-0.858) across all three datasets; [corpus] No corpus papers validate gated fusion specifically for causality; mechanism remains paper-specific.

## Foundational Learning

- Concept: Dependency parsing and dependency trees
  - Why needed here: Understanding how SpaCy converts sentences into vertex-token graphs with labeled edges (e.g., "deficiency" → "causes") is prerequisite to implementing the dependency-aware attention channel.
  - Quick check question: Given "Smoking causes lung cancer," draw the dependency tree with POS tags and identify which token pairs would share attention edges.

- Concept: Graph Attention Networks (GAT) attention mechanism
  - Why needed here: The dependency-aware channel applies GAT-style restricted attention rather than full self-attention; understanding learnable W matrices and softmax normalization over neighbor sets is essential.
  - Quick check question: How does computing attention only over V (connected neighbors) differ from standard transformer attention over all N tokens, and what is the computational complexity implication?

- Concept: Token classification vs. sequence labeling
  - Why needed here: DEPBERT frames causality extraction as token classification (each token labeled Cause/Effect/Other), requiring BIO/IOB tagging schemes for phrase boundaries.
  - Quick check question: For "Vitamin D deficiency causes diabetes," assign token labels and explain how exact match accuracy penalizes partial phrase extraction.

## Architecture Onboarding

- Component map: Input -> Tokenizer (with POS) -> Dual embedding -> Dual attention (standard + dependency-constrained) -> LayerNorm + FFN per tower -> Gated fusion -> Classification head

- Critical path: Input → Tokenizer (with POS) → Dual embedding → Dual attention (standard + dependency-constrained) → LayerNorm + FFN per tower → Gated fusion → Classification loss (cross-entropy)

- Design tradeoffs:
  - Two-tower vs. single-tower: Enables reusing pretrained BERT weights (left) while training dependency channel from scratch (right), but increases parameters to 227M
  - Exact dependency edges vs. expanded neighborhoods: Paper uses direct edges only; could add k-hop neighbors at cost of potentially diluting syntactic precision
  - Gating vs. concatenation: Gating enables dynamic weighting but may introduce optimization instability; concatenation is stable but removes adaptive balancing

- Failure signatures:
  1. Gate saturation: If es_i → 1 globally, model ignores dependency channel (degrades to BERT baseline)
  2. Parser dependency failure: Non-English text or malformed inputs crash the dependency-aware tower
  3. Single-pair assumption: Training on CAUSAL GPT (one pair per sentence) prevents multi-pair extraction at inference

- First 3 experiments:
  1. Ablation study: Run (a) BERT + dependency only, (b) BERT + POS only, (c) full DEPBERT on held-out test set to isolate contribution of each component using the provided dataset splits
  2. Gate analysis: Log es_i distributions across tokens labeled Cause, Effect, and Other to verify the gate is learning meaningful distinctions rather than collapsing
  3. Parser robustness test: Manually inject SpaCy parsing errors (e.g., swap dependency edges) into 10% of validation sentences and measure F1 degradation to quantify dependency parser sensitivity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can DEPBERT be extended to extract multiple cause-effect pairs from a single sentence?
- Basis in paper: [explicit] The authors state in the Limitations section: "Unlike DEPBERT, which can extract multiple semantic pairs from sentences simultaneously, the CausalGPT dataset is intentionally constructed so that each sentence contains only one semantic pair. Consequently, if DEPBERT is trained with this dataset, it cannot extract multiple semantic pairs from sentences."
- Why unresolved: The CAUSAL GPT dataset only contains single-cause-single-effect pairs, and no evaluation was conducted on sentences with multiple causal relationships.
- What evidence would resolve it: Evaluation on a dataset containing sentences with multiple cause-effect pairs, showing whether the model can identify all pairs correctly.

### Open Question 2
- Question: Does DEPBERT's dependency-aware architecture generalize effectively to languages other than English?
- Basis in paper: [explicit] The authors explicitly note: "Furthermore, we specifically emphasize semantic pairs within the English language. While semantic pairs and dependency relationships can be of great importance in all languages, it's worth noting that our research did not include experiments in languages other than English."
- Why unresolved: Dependency structures vary significantly across languages, and the model was only tested on English datasets with English-specific dependency parsing.
- What evidence would resolve it: Cross-lingual experiments showing comparable F1 scores and exact match accuracy when DEPBERT is applied to multilingual datasets.

### Open Question 3
- Question: How sensitive is DEPBERT's performance to the choice of dependency parser?
- Basis in paper: [inferred] The paper uses SpaCy for dependency parsing and briefly mentions that "previous research has indicated that the performance of syntactic dependency pattern extraction does not depend much on the specific format of dependency tree," but this claim is not empirically tested in the current work.
- Why unresolved: No ablation study comparing different dependency parsers (e.g., Stanza vs. SpaCy) or alternative dependency representations was conducted.
- What evidence would resolve it: Controlled experiments using identical model configurations with different dependency parsers, measuring performance variance across datasets.

## Limitations
- Dependency parser brittleness: All results hinge on SpaCy's dependency parsing accuracy, which degrades on complex or domain-specific medical language
- Dataset dependency: DEPBERT's gains are dataset-specific, showing large improvements on balanced human-annotated data but only marginal gains on synthetic single-pair datasets
- No error analysis: The paper does not quantify how much parser error rate contributes to final performance

## Confidence

- **High Confidence**: The dual-channel architecture with gated fusion is correctly implemented and improves over single-channel baselines on all tested datasets. This is empirically verified.
- **Medium Confidence**: Dependency structure provides consistent signal for causality extraction. While results show improvements, they are dataset-dependent and parser-sensitive, limiting generalizability.
- **Low Confidence**: The method's performance on real-world, multi-pair, cross-domain causality extraction. Current evaluation is constrained to curated datasets with simplified causal patterns.

## Next Checks

1. **Parser Robustness Test**: Manually inject controlled parsing errors (e.g., swap subject/object dependencies) into 10% of validation sentences and measure F1 degradation. Compare results using SpaCy vs. Stanza parsers to quantify parser sensitivity.

2. **Cross-Dataset Generalization**: Evaluate DEPBERT on a multi-cause/multi-effect dataset (e.g., biomedical literature with complex causal chains) to test if dependency structure remains beneficial beyond single-pair sentences.

3. **Gate Dynamics Analysis**: Log and visualize gate activation distributions (es_i) for Cause, Effect, and Other tokens across training epochs. Verify the gate learns meaningful token-specific weighting rather than collapsing to favoring one tower.