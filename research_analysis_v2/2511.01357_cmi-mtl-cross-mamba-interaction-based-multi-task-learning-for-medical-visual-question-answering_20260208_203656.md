---
ver: rpa2
title: 'CMI-MTL: Cross-Mamba interaction based multi-task learning for medical visual
  question answering'
arxiv_id: '2511.01357'
source_url: https://arxiv.org/abs/2511.01357
tags:
- medical
- visual
- learning
- question
- cmi-mtl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CMI-MTL, a Cross-Mamba Interaction based Multi-Task
  Learning framework for medical visual question answering. It addresses challenges
  in cross-modal semantic alignment and the limitations of classification-based methods
  that rely on predefined answer sets.
---

# CMI-MTL: Cross-Mamba interaction based multi-task learning for medical visual question answering

## Quick Facts
- arXiv ID: 2511.01357
- Source URL: https://arxiv.org/abs/2511.01357
- Authors: Qiangguo Jin; Xianyao Zheng; Hui Cui; Changming Sun; Yuqi Fang; Cong Cong; Ran Su; Leyi Wei; Ping Xuan; Junbo Wang
- Reference count: 17
- One-line primary result: CMI-MTL achieves up to 1.59% accuracy improvement on SLAKE and 2.41% on OVQA compared to state-of-the-art methods

## Executive Summary
CMI-MTL introduces a Cross-Mamba Interaction based Multi-Task Learning framework for medical visual question answering. It addresses key challenges in cross-modal semantic alignment and the limitations of classification-only approaches by integrating fine-grained visual-text alignment, cross-modal interleaved feature representation using Mamba blocks, and free-form answer-enhanced multi-task learning. The framework demonstrates superior performance on three medical VQA datasets (SLAKE, VQA-RAD, OVQA), particularly excelling at open-ended questions through its auxiliary generation task.

## Method Summary
The CMI-MTL framework processes medical images and questions through a multi-stage pipeline. It first encodes visual features using a Vision Transformer and text features using RoBERTa, then aligns them through a Question-aware Q-Former with contrastive learning. The aligned features are fused using Cross-Mamba blocks that interleave visual and text information within a state-space model. The framework employs a dual-head approach: a linear classifier for the primary task and a T5 decoder for free-form answer generation on open-ended questions. Training uses a weighted combination of classification loss, visual-text contrastive loss, and auxiliary generation loss.

## Key Results
- Achieves 1.59% accuracy improvement on SLAKE dataset compared to state-of-the-art methods
- Shows 2.41% accuracy improvement on OVQA dataset, particularly strong on open-ended questions
- Reduces model parameters from 345M (Transformer-based) to 245M while maintaining or improving accuracy

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal State-Space Interleaving
The Cross-Mamba block (CMM) interleaves visual and text features within state-space model blocks, constructing sequences through element-wise multiplication and summation to fuse modalities before processing. This allows the selective scan mechanism to operate on a combined representation rather than isolated modalities, potentially capturing sequential cross-modal dependencies more efficiently than standard self-attention. The approach trades the global receptive field of Transformers for linear complexity processing.

### Mechanism 2: Free-Form Generation as Auxiliary Regularization
The auxiliary T5 decoder head forces the model to predict exact answer strings for open-ended questions via cross-entropy loss, rather than just mapping to class IDs. This generative regularization likely prevents the encoder from collapsing distinct medical semantics into the same classification bucket, enriching the semantic representation. The gradients from the generative loss appear compatible with the classification objective, improving performance on open-ended questions.

### Mechanism 3: Query-Based Semantic Alignment
The Question-aware Q-Former uses learnable queries to extract visual features via cross-attention, aligning them with text embeddings using contrastive learning. This decouples visual feature extraction from alignment, allowing for more flexible semantic grounding. The fixed set of 32 learnable queries provides a bottleneck that focuses the model on relevant image regions corresponding to the question context.

## Foundational Learning

- **State Space Models (Mamba/SSM):** These models discretize continuous dynamics to process sequences with linear complexity, contrasting with the quadratic complexity of self-attention. *Quick check:* How does the "selective scan" in Mamba differ from standard attention mechanisms regarding memory usage for long sequences?

- **Multi-Task Learning (Auxiliary Loss):** The model optimizes two distinct objectives (classification and generation) with gradient combination and weight balancing. *Quick check:* If validation classification accuracy drops while generation loss decreases, what is likely happening to the loss weights?

- **Q-Former / Query Transformers:** This architecture bridges ViT and LLM using a bottleneck of learnable queries. *Quick check:* In a Q-Former, do the queries attend to the text, the image, or each other? (Here: queries attend to image via cross-attention, self-attend, and align with text via contrastive loss)

## Architecture Onboarding

- **Component map:** ViT (Visual) -> Question-aware Q-Former (Alignment) -> Cross-Mamba Blocks (Fusion) -> Linear Classifier + T5 Decoder (Heads)
- **Critical path:** Image/Text → FVTA (Alignment) → CIFR (Fusion) → Heads. The success of the CIFR module depends heavily on the quality of the aligned features from FVTA.
- **Design tradeoffs:** Efficiency vs. Global Context: Using Mamba reduces params to 245M and memory to 19GB (vs Transformer's 21GB), but sacrifices the global attention mechanism for sequential scanning. Open-ended complexity: The auxiliary T5 head adds significant architectural complexity to handle a subset of the data, justified by the performance jump.
- **Failure signatures:** Loss Instability: If loss weights exceed optimal thresholds (0.2 and 0.3), sharp performance drops occur, likely due to the auxiliary task disrupting the main feature space. Attention Drift: On images with artifacts or blurring, heatmaps become scattered, indicating the FVTA module fails to lock onto specific regions.
- **First 3 experiments:** 1) Ablation on FFAE: Train with only classification head vs. with auxiliary T5 head on VQA-RAD "Open" split to isolate generative regularization impact. 2) Interleaving Validation: Replace Cross-Mamba block with standard Transformer encoder to verify if performance gain is from Mamba architecture or interleaving strategy itself. 3) Hyperparameter Sensitivity: Sweep loss weights α and β on hold-out set to confirm the inflection point where positive impact becomes negative.

## Open Questions the Paper Calls Out

- **Human-machine collaboration impact:** Does integration into clinical workflows significantly improve diagnostic accuracy of medical practitioners compared to unassisted diagnosis? This remains unresolved as the study did not include concurrent evaluation data from actual medical practitioners.

- **Scaling behavior validation:** Does the linear complexity advantage of Cross-Mamba interaction persist and translate to superior performance over Transformers when scaling to massive medical datasets? Current experiments on relatively small datasets leave this unverified for large-scale data.

- **Subtle feature detection:** Can the Fine-grained Visual-Text Feature Alignment module be refined to improve sensitivity to subtle "tiny features" (e.g., minute fractures) that currently yield weak attention signals? The current mechanism may prioritize larger semantic regions over high-frequency details required for detecting subtle pathologies.

## Limitations

- The architectural novelty of Mamba-based cross-modal alignment remains weakly validated in the medical domain, as most related work still uses attention-based methods
- The claimed efficiency gains come at the cost of potentially reduced global context modeling for complex medical pathologies
- The auxiliary generation head's contribution to classification accuracy lacks ablation studies showing whether improvement comes from the generation task itself or increased model capacity

## Confidence

- **High Confidence:** Dataset splits and implementation details (resizing to 384x384, training epochs, optimizer settings)
- **Medium Confidence:** Accuracy improvements on benchmark datasets, as these are directly reported
- **Low Confidence:** Claims about Mamba's superiority for cross-modal alignment in medical imaging, as this represents a departure from established attention-based approaches without extensive comparative analysis

## Next Checks

1. **Ablation study isolation:** Train the model with only classification head vs. with auxiliary T5 head on VQA-RAD Open split to quantify the exact contribution of the free-form generation task

2. **Architectural baseline comparison:** Replace Cross-Mamba blocks with standard Transformer encoders (keeping all other components constant) to verify if performance gains stem from Mamba architecture or interleaving strategy

3. **Long-sequence performance test:** Evaluate on questions requiring extensive medical context (e.g., multi-organ pathology descriptions) to identify potential SSM limitations in compressing detailed information