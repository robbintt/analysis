---
ver: rpa2
title: Large Language Model Scaling Laws for Neural Quantum States in Quantum Chemistry
arxiv_id: '2509.12679'
source_url: https://arxiv.org/abs/2509.12679
tags:
- scaling
- training
- size
- ansatz
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work extends large language model (LLM) scaling laws to autoregressive
  neural quantum states (NQS) in electronic structure calculations. It introduces
  a parametric scaling law predicting NQS performance via a problem-size-aware training
  time variable, and derives FLOP estimates for MADE, transformer, and RetNet NQS
  architectures.
---

# Large Language Model Scaling Laws for Neural Quantum States in Quantum Chemistry

## Quick Facts
- arXiv ID: 2509.12679
- Source URL: https://arxiv.org/abs/2509.12679
- Reference count: 40
- This work extends large language model scaling laws to autoregressive neural quantum states in electronic structure calculations.

## Executive Summary
This paper adapts LLM-style scaling laws to neural quantum states for quantum chemistry, introducing a problem-size-aware training variable to predict performance across varying molecular systems. The authors analyze three autoregressive architectures (MADE, Transformer, RetNet) on 14 molecules, finding that model size matters more than training time for transformers while MADE depends primarily on training duration. They derive architecture-specific FLOP estimates accounting for local energy calculation overhead and show that optimal compute allocation differs by architecture, diverging from the linear trends seen in LLMs. The findings demonstrate that LLM scaling analyses can guide efficient NQS design and reveal architectural trade-offs in quantum chemistry applications.

## Method Summary
The authors fit parametric scaling laws to neural quantum states trained on electronic structure problems, modifying the standard "tokens seen" variable to account for molecular problem size. They introduce $D' = T \times S_F$ where $T$ is training steps and $S_F$ is the fraction of Hilbert space sampled, enabling fair comparison across molecules of different sizes. The study analyzes three autoregressive architectures (MADE, Transformer, RetNet) on 7 molecules with 14-30 spin-orbitals, fitting scaling coefficients for model size and training duration separately for each architecture. FLOP estimates are derived that include the overhead of local energy calculations, which require $M$ additional forward passes per sample where $M$ is the number of Hamiltonian terms. The parametric law $L(N, D') = A_0 + A_1 N^{-\alpha_1} + A_2 (D')^{-\alpha_2}$ is fit to V-score data to determine optimal compute allocations.

## Key Results
- Model size impacts V-score more than training time for transformers and RetNets, while MADE performance depends primarily on training duration
- Optimal compute allocation differs fundamentally by architecture and is metric- and ansatz-dependent
- Local energy calculation costs create distinct FLOP overhead not present in standard LLM training
- Scaling relationships diverge from linear trends observed in LLMs, varying by architecture and loss metric

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A modified "sample-aware" training variable ($D'$) allows scaling laws to predict NQS performance across varying molecular problem sizes.
- **Mechanism:** Standard LLM scaling uses "tokens seen" ($D$). This framework replaces $D$ with $D' = T \times S_F$, where $T$ is training steps and $S_F$ is the fraction of Hilbert space (search space) sampled. By normalizing training duration against the specific problem size, the parametric curve $L(N, D')$ can account for difficulty variance between small and large molecules.
- **Core assumption:** The search space fraction ($S_F$) is a sufficient proxy for data quantity in variational Monte Carlo, assuming sampling efficiency relative to space size dictates convergence more than raw sample count.
- **Evidence anchors:**
  - [abstract] "introduces a parametric scaling law predicting NQS performance via a problem-size-aware training time variable"
  - [section 4.2] "we multiply it by the number T of optimization steps to give $D' = T \times S_F$... a size-controlled analog to the variable D"
  - [corpus] Related work suggests caution about scaling laws applying universally; this mechanism addresses that by modifying the law for the quantum domain.
- **Break condition:** This mechanism fails if optimal sampling strategy changes non-linearly with system size.

### Mechanism 2
- **Claim:** Optimal compute allocation differs fundamentally by architecture; Transformers scale with parameter count, while MADE scales with training duration.
- **Mechanism:** The paper fits coefficients ($A_1, \alpha_1$ for model size $N$ and $A_2, \alpha_2$ for training $D'$) to the scaling law. For Transformers/RetNets, coefficients for $N$ dominate performance, implying expressivity is the bottleneck. For MADE, coefficients for $D'$ are more significant, implying optimization/sampling time is the bottleneck due to architecture's lower expressivity per parameter.
- **Core assumption:** The parametric form accurately captures loss landscape curvature for these distinct architectures.
- **Evidence anchors:**
  - [abstract] "model size impacts V-score more than training time for transformers and RetNets, while MADE performance depends primarily on training duration"
  - [section 5] "These coefficients indicate that model size makes a greater impact... for the transformer and RetNet models... model size appears to make a nearly negligible impact for MADE"
  - [corpus] Neighbor papers support expressivity of attention-based mechanisms in this domain.
- **Break condition:** If computational budget constraints force batch sizes so small that gradient noise dominates, the benefit of increased model size for Transformers may vanish.

### Mechanism 3
- **Claim:** Local energy calculation costs create a distinct FLOP overhead not present in standard LLM training, necessitating architecture-specific compute estimators.
- **Mechanism:** In NQS, calculating the loss requires computing the "local energy," which involves $M$ unique bit-flip patterns. This requires $M$ additional forward passes per sample, effectively multiplying compute cost by Hamiltonian term count. The paper derives specific FLOP estimates including $M$ and batch size $B$ to capture this.
- **Core assumption:** The cost of assembling model outputs into loss values is negligible compared to the $M$ forward passes required for the Hamiltonian numerator.
- **Evidence anchors:**
  - [section 4.1] "NQS... does not have training data, instead using the model to first generate a batch... and then calculate local energies... Each bit flip adds an additional forward pass."
  - [table 1] Lists "Full Ansatz Training FLOP Estimate" including components weighted by $M$ (unique bit flips).
  - [corpus] Weak explicit anchoring in corpus neighbors regarding FLOP estimation specifically; mechanism relies primarily on [section 4.1].
- **Break condition:** The estimate assumes inefficient recomputation of forward passes; if advanced caching is implemented across Hamiltonian terms, the FLOP estimates would overcount compute requirements.

## Foundational Learning

- **Concept: Variational Principle & V-Score**
  - **Why needed here:** This is the objective function. Unlike LLMs (cross-entropy), NQS minimizes energy $\langle \psi | H | \psi \rangle$. The V-Score is used as a "problem-agnostic" proxy for error when ground truth (FCI) is unavailable.
  - **Quick check question:** Why does the V-Score include variance in the numerator, and why does that help compare performance across different molecules?

- **Concept: Autoregressive Factorization**
  - **Why needed here:** The ansatz represents the wavefunction $\psi(x)$ as a product of conditional probabilities $p(x_j | x_{k<j})$. This enables efficient sampling from $|\psi|^2$ without MCMC, which is central to how training data (samples) is generated.
  - **Quick check question:** How does the "Masked Autoencoder" (MADE) enforce autoregression differently than the "Attention Mask" in a Transformer?

- **Concept: Second Quantization & Pauli Strings**
  - **Why needed here:** Defines the input space ($n$-qubit spin-orbitals) and cost function structure ($H$ as a sum of Pauli strings). The number of terms $M$ in the Hamiltonian directly dictates the "local energy" compute overhead.
  - **Quick check question:** Why does the computational cost of calculating the local energy scale with the number of Pauli strings $M$?

## Architecture Onboarding

- **Component map:** Modulus Network -> Phase Network -> Hamiltonian Layer
- **Critical path:**
  1. **Sample Generation:** Forward pass through Modulus Net to generate batch $B$ of spin configurations $x$
  2. **Local Energy Calculation:** The bottleneck. For each $x$, compute $\langle x | H | \psi \rangle$ by applying $M$ bit-flips to $x$ and running $M$ forward passes
  3. **Optimization:** Backpropagate the stochastic loss $L(\theta)$ using Adam

- **Design tradeoffs:**
  - **Transformer vs. RetNet:** RetNet allows O($n$) inference (vs Transformer O($n^2$)) via recurrence, potentially lowering FLOPs for sampling, but paper shows they scale similarly in practice
  - **MADE vs. Attention:** MADE is compute-cheap per parameter but less expressive; requires more training time ($D'$) to converge. Transformers are expensive but expressive; benefit more from size ($N$)
  - **Phase/Modulus Split:** The phase network is often simpler (fewer params $N_{ph}$) than the modulus network ($N_{mod}$), as phase structure is arguably smoother than probability amplitude

- **Failure signatures:**
  - **High V-Score with Low Loss:** Indicates overfitting to easy configurations or insufficient sampling of the Hilbert space
  - **Scaling Law Divergence:** If you scale $N$ but performance flatlines, check if you are "search space limited" rather than model limited
  - **MADE Stagnation:** If using MADE and accuracy stalls, increasing model width is less effective than increasing training steps/samples

- **First 3 experiments:**
  1. **Baseline Calibration:** Train MADE and Transformer on H2O (small $M$). Verify that Transformer performance correlates with $N$ while MADE correlates with $T$
  2. **FLOP Verification:** Measure wall-clock time for the "Local Energy" step vs. "Sampling" step on a larger molecule (e.g., N2) to validate the FLOP estimates in Table 1
  3. **Compute-Optimal Sweep:** For a fixed FLOP budget (e.g., 1e15 FLOPs), run the optimization in Eq. (26/27) to find the theoretical optimal $N$ and $D'$, then train a model at that point to validate the prediction

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does separating the modulus network parameter count from the phase network parameter count in the scaling law reveal different optimal compute allocations for each component?
- **Basis in paper:** [explicit] "the model size N in (10) comprises both modulus and phase networks, but it may be illuminating to isolate the phase network's role."
- **Why unresolved:** The current parametric scaling law treats $N = N_{mod} + N_{ph}$ as a single variable, masking any differential contribution of phase versus modulus networks to V-score or absolute error.
- **What evidence would resolve it:** Fit separate scaling curves with $N_{mod}$ and $N_{ph}$ as independent variables and compare the fitted $\alpha$ coefficients for each.

### Open Question 2
- **Question:** How do NQS scaling laws change when using the local energy approximation that neglects Hamiltonian terms associated with unsampled states?
- **Basis in paper:** [explicit] "A technique introduced in [29] efficiently approximates local energy calculations by neglecting Hamiltonian terms associated with unsampled states [...] Analyzing the scaling properties of NQS under this new variation would help provide insight into the extent of its benefits."
- **Why unresolved:** The current FLOP estimates assume exact local energy computation; the approximation reduces compute per sample but may alter the trade-off between model size and training duration.
- **What evidence would resolve it:** Repeat the scaling law experiments with the approximate local energy method and compare fitted parameters to the exact computation baseline.

### Open Question 3
- **Question:** What causes the deviation from the linear model-size-to-training-time relationship observed in LLM scaling laws?
- **Basis in paper:** [explicit] "the relationship between model size and training time is highly dependent on loss metric and ansatz, and does not follow the approximately linear relationship found for language models."
- **Why unresolved:** The paper documents the divergence but does not identify whether it stems from the stochastic variational objective, the structure of quantum Hamiltonians, or properties of autoregressive NQS architectures.
- **What evidence would resolve it:** Systematic ablations isolating each factor (loss function, Hamiltonian structure, ansatz expressivity) to determine which drives the nonlinearity.

### Open Question 4
- **Question:** How does the number of non-unique Monte Carlo samples affect the scaling law parameters and optimal compute allocation?
- **Basis in paper:** [explicit] "Varying the total number of non-unique Monte Carlo samples may also impact accuracy."
- **Why unresolved:** The study fixed the exponential sampling schedule from $10^4$ to $10^{12}$ samples, leaving unexplored how alternative schedules influence $D'$ and the resulting scaling coefficients.
- **What evidence would resolve it:** Conduct experiments with different sampling schedules and fit scaling curves to quantify the sensitivity of $\alpha_1$, $\alpha_2$ to the total non-unique sample count.

## Limitations

- The study relies on synthetic training data generated by forward passes through the NQS, which may not fully capture long-tailed distribution of real molecular systems
- The parametric scaling law assumes the relationship between performance and compute follows the specified functional form across all architectures and problem sizes
- FLOP estimates assume inefficient recomputation of forward passes for local energy calculations, which may overestimate costs if caching strategies are implemented

## Confidence

- **High confidence:** The observation that transformers scale primarily with model size while MADE scales with training duration (supported by empirical coefficients in Table 3)
- **Medium confidence:** The general form of the parametric scaling law and its applicability to compute-optimal allocation (based on curve fitting, but form may not generalize)
- **Low confidence:** The specific FLOP estimates for local energy computation (depends on architectural details and potential optimizations not explored)

## Next Checks

1. Test the scaling law on a molecule with >30 spin-orbitals to verify it holds beyond the current dataset
2. Implement and measure the actual FLOP cost of local energy computation with and without caching to validate Table 1 estimates
3. Train a model at the compute-optimal point predicted by Eq. (26) for a specific architecture and molecule to verify the prediction accuracy