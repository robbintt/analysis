---
ver: rpa2
title: Defending against Adversarial Malware Attacks on ML-based Android Malware Detection
  Systems
arxiv_id: '2501.13782'
source_url: https://arxiv.org/abs/2501.13782
tags:
- malware
- adversarial
- android
- space
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of defending ML-based Android malware
  detection (AMD) systems against realistic problem-space adversarial attacks that
  manipulate actual malware files while preserving functionality. Prior defenses only
  protected against feature-space attacks.
---

# Defending against Adversarial Malware Attacks on ML-based Android Malware Detection Systems

## Quick Facts
- arXiv ID: 2501.13782
- Source URL: https://arxiv.org/abs/2501.13782
- Authors: Ping He; Lorenzo Cavallaro; Shouling Ji
- Reference count: 40
- Primary result: ADD framework achieves >95% NDASR against state-of-the-art problem-space adversarial attacks while maintaining <1% TNIR and recovering ~50% of false negatives

## Executive Summary
This paper addresses the critical vulnerability of ML-based Android malware detection systems to problem-space adversarial attacks, where attackers manipulate actual malware files while preserving functionality. Unlike prior defenses that only protected against feature-space attacks, the proposed ADD framework directly targets the semantic incompatibility created by problem-space perturbations. By quantifying perturbable and imperturbable feature spaces, training encoder models with customized contrastive learning, and calibrating thresholds using novel metrics, ADD significantly improves robustness while preserving the original detector's performance.

## Method Summary
ADD enhances ML-based AMD robustness by first quantifying which features can be feasibly perturbed in real malware (perturbable space) versus those that cannot (imperturbable space) through systematic experimentation with minimal APK templates. It then trains two encoder models using contrastive learning to project these feature spaces into a shared embedding space, measuring incompatibility between perturbed and unperturbed features. A pseudo-adversarial sample generator creates training data by randomly perturbing malicious samples until misclassification. The framework calibrates detection thresholds using TNIR and FNIR metrics to balance false positives and false negative recovery. At inference, ADD only processes samples flagged as benign by the original detector, using the incompatibility score to identify potential adversarial samples.

## Key Results
- ADD achieves over 95% normalized decreased attack success rate (NDASR) against state-of-the-art problem-space attacks across various ML-based AMD systems
- False negative detection improves by approximately 50% while true negative rate decreases by only about 1%
- The framework demonstrates effectiveness against real-world antivirus solutions and under adaptive attack scenarios
- Cross-model transferability shows ADD trained on one feature space (MaMaDroid) can defend against attacks targeting different feature spaces (DREBIN, CICInvesAndMal2020)

## Why This Works (Mechanism)

### Mechanism 1
Problem-space adversarial perturbations create detectable semantic incompatibility between modified and unmodified feature regions. Attackers must preserve malware functionality, constraining perturbations to a "perturbable feature space" (e.g., injectable permissions, API calls). This creates measurable incompatibility when projected into a shared embedding space via separate encoders for perturbable and imperturbable features. The L2 distance between these projections serves as an incompatibility score—adversarial samples exhibit higher scores than benign or regular malware. Core assumption: attackers prioritize evasion effectiveness over maintaining semantic consistency between perturbed and unperturbed features.

### Mechanism 2
Pseudo-adversarial malware samples generated via random perturbation in perturbable space provide effective training signal without requiring real adversarial samples. Defenders lack access to attacker-generated adversarial samples, so ADD generates approximations by randomly perturbing perturbable features of malicious samples until misclassified as benign. These approximate the distribution of real adversarial samples for encoder training via contrastive loss: pseudo-adversarial > malicious > benign in incompatibility score. Core assumption: random perturbations in perturbable space produce feature distributions sufficiently similar to real adversarial perturbations.

### Mechanism 3
Plug-in architecture with TNIR-controlled thresholding preserves original detector performance while recovering false negatives. ADD only processes samples classified as benign by the original detector. Threshold calibration uses TNIR (True Negative Influence Rate—benign samples wrongly flagged) and FNIR (False Negative Influence Rate—missed malware correctly recovered). The threshold is set to bound TNIR at an acceptable level (e.g., 1-5%) while maximizing FNIR. Core assumption: the calibration dataset is representative of deployment distribution.

## Foundational Learning

- **Feature-space vs. Problem-space attacks**: Problem-space attacks manipulate actual malware files while preserving functionality, unlike feature-space attacks that modify feature vectors directly. Needed because ADD's perturbable/imperturbable decomposition only makes sense under problem-space constraints. Quick check: Given an APK, explain why you can't arbitrarily modify any feature value—what constraints apply?

- **Contrastive learning with margin-based loss**: The encoder training uses triplet-style losses (L2, L3) with margins to enforce ordering: pseudo-adversarial > malicious > benign in embedding distance. Needed because the framework must learn to distinguish semantic incompatibility patterns. Quick check: If margin m=1.0 and D(x_malicious)=0.8, D(x_benign)=0.3, what is L2 for this pair? (Answer: max(0.3 - 0.8 + 1.0, 0) = 0.5)

- **Android APK feature extraction (manifest, DEX, API calls)**: Space quantification requires understanding which features correspond to which APK elements. Needed because you must know what "injecting a permission" means in practice. Quick check: Name three feature types commonly extracted from APKs for ML-based malware detection and their source files.

## Architecture Onboarding

- **Component map**: Space Quantification Module -> Encoder Models -> Pseudo-Adversarial Generator -> Threshold Calibration Module -> Inference Pipeline (Original Detector -> Encoder Inference -> Threshold Comparison)

- **Critical path**: 1) Space quantification (one-time per target AMD system) -> 2) Pseudo-adversarial sample generation (depends on training set, classifier, perturbable mask) -> 3) Encoder training with contrastive loss (depends on training data + pseudo-samples) -> 4) Threshold calibration (depends on calibration set, trained encoders) -> 5) Inference (original detector + encoder inference + threshold comparison)

- **Design tradeoffs**:
  - **TNIR level**: Lower (1%) = less false positives on benign apps, but recovers fewer false negatives. Higher (10%) = better adversarial detection but more user friction.
  - **Encoder output dimension**: Smaller (e.g., 16) = faster inference but may lose discrimination. Paper uses 32.
  - **Pseudo-adversarial attempt budget N**: Higher = more training samples but slower generation.
  - **Transferability vs. direct application**: If target AMD uses non-categorical features (e.g., MaMadroid's graph embeddings), space quantification is difficult—use transferability from a compatible feature space instead.

- **Failure signatures**:
  - **High TNIR in deployment**: Calibration set drifted; recalibrate with recent samples.
  - **Low FNIR / high NDASR variance across attack methods**: Encoders overfit to specific perturbation patterns; increase pseudo-sample diversity or regularization.
  - **Space quantification incomplete**: Some perturbations require specific app structures not present in minimal templates; add more representative quantification apps.
  - **Incompatibility scores unimodal**: Encoder projection collapses; check loss convergence, increase margin, reduce learning rate.

- **First 3 experiments**:
  1. **Space quantification validation**: Apply each perturbation from the feasible set to a diverse set of real APKs (not just minimal templates). Verify that the observed perturbable features match the quantification output. Flag any perturbations that fail or produce unexpected feature changes.
  2. **Encoder ablation**: Train encoders with L1 only, L1+L2, and full L1+L2+L3. Measure NDASR, TNIR, FNIR on a held-out adversarial test set. Quantify the contribution of pseudo-adversarial samples (L3) to defense effectiveness.
  3. **Threshold sensitivity analysis**: For a fixed calibration set, sweep TNIR control levels (1%, 3%, 5%, 10%, 15%). Plot FNIR vs. TNIR and NDASR vs. TNIR. Identify the knee point where TNIR increase yields diminishing FNIR/NDASR gains.

## Open Questions the Paper Calls Out

### Open Question 1
How does concept drift influence the long-term robustness of the ADD framework, and can periodic retraining effectively mitigate this? The current evaluation uses fixed datasets from 2016–2018 and 2022, but the authors note that the evolution of malware introduces distribution shifts that the current static training approach may not handle without orthogonal solutions. A longitudinal study measuring NDASR and FNIR decay over time without retraining, followed by an analysis of the frequency and data volume required for encoder retraining to restore performance, would resolve this.

### Open Question 2
Can ADD be adapted to defend ML-based AMD systems where the perturbable feature space is not easily quantifiable (e.g., numerical graph embeddings like MaMadroid) without relying on transferability? The current space quantification stage relies on observing feature changes via Monte Carlo sampling, which works for binary/categorical features but fails for dense numerical representations where "perturbable" boundaries are ambiguous. A modified quantification mechanism or projection technique that successfully applies ADD directly to non-categorical feature spaces (like API graphs) with performance comparable to the transferability baseline would resolve this.

### Open Question 3
Can an attacker bypass ADD by optimizing perturbations specifically to minimize the incompatibility score in the embedding space? While ADD defends against selection-based adaptive attacks, it is unclear if the constraint of maintaining functional consistency in the problem space is sufficient to prevent an attacker from "walking" the malware down the gradient of the contrastive loss to appear benign. An evaluation of a white-box or surrogate-model adaptive attack that utilizes the gradients of the encoder models to generate adversarial samples with intentionally low incompatibility scores would resolve this.

## Limitations
- The framework's effectiveness depends on the completeness of the feasible perturbation set and the representativeness of minimal APK templates used for space quantification.
- The pseudo-adversarial sample generation approach may not fully capture sophisticated attack strategies that use structured, context-aware perturbations rather than random ones.
- The transferability approach assumes that encoder training on one feature space will generalize to different feature spaces, which may not hold for highly divergent feature representations.

## Confidence

- **High confidence**: The fundamental mechanism of detecting semantic incompatibility between perturbed and unperturbed features (Mechanism 1) is well-grounded and empirically validated across multiple attack methods.
- **Medium confidence**: The pseudo-adversarial sample generation approach (Mechanism 2) shows strong empirical results but relies on assumptions about random perturbation approximating real adversarial patterns that could break with more sophisticated attacks.
- **Medium confidence**: The threshold calibration approach (Mechanism 3) provides practical deployment guidance but depends heavily on calibration set representativeness and may not generalize under concept drift.

## Next Checks

1. **Perturbation completeness validation**: Systematically test each perturbation from the feasible set against a diverse corpus of real-world malware samples (not just minimal templates) to verify that all expected feature changes occur and no unexpected incompatibilities arise.

2. **Cross-feature-space transferability test**: Train encoders on one feature space (e.g., MaMaDroid) and evaluate defense effectiveness when deployed against attacks targeting a different feature space (e.g., DREBIN) using the same malware samples to quantify transfer loss.

3. **Adaptive attack evaluation**: Design and implement an adaptive attacker that specifically targets the incompatibility detection mechanism by generating perturbations that maximize evasion while minimizing semantic incompatibility, then measure ADD's effectiveness against this specialized threat model.