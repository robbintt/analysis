---
ver: rpa2
title: 'Divide and Conquer: Static-Dynamic Collaboration for Few-Shot Class-Incremental
  Learning'
arxiv_id: '2601.08448'
source_url: https://arxiv.org/abs/2601.08448
tags:
- learning
- classes
- projector
- knowledge
- session
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a framework termed Static-Dynamic Collaboration
  (SDC) to address the stability-plasticity dilemma in few-shot class-incremental
  learning (FSCIL). The key idea is to divide the learning process into two stages:
  Static Retaining Stage (SRS) and Dynamic Learning Stage (DLS).'
---

# Divide and Conquer: Static-Dynamic Collaboration for Few-Shot Class-Incremental Learning

## Quick Facts
- **arXiv ID:** 2601.08448
- **Source URL:** https://arxiv.org/abs/2601.08448
- **Reference count:** 40
- **Primary result:** SDC achieves average accuracies of 68.60%, 68.74%, 67.60%, and 67.35% on CIFAR100, MiniImageNet, CUB200, and FGVC-Aircraft respectively.

## Executive Summary
This paper proposes the Static-Dynamic Collaboration (SDC) framework to address the stability-plasticity dilemma in few-shot class-incremental learning (FSCIL). The core innovation is a dual-projector architecture that separates the feature transformation process into a frozen "static" component (trained on base data) and a trainable "dynamic" component (adapted to new classes). By interpolating their outputs with a weighting factor, SDC achieves a balance between preserving old knowledge and adapting to new classes, outperforming state-of-the-art methods on multiple benchmarks.

## Method Summary
SDC divides the learning process into two stages: Static Retaining Stage (SRS) and Dynamic Learning Stage (DLS). In SRS, the model is trained on the base dataset and the projector is retained as static memory. During DLS, a new dynamic projector is introduced and trained with the assistance of the static projector and an external memory of class prototypes. The final representation is a weighted sum of static and dynamic projector outputs, controlled by hyperparameter α. This architecture allows the model to preserve the geometric structure of the base feature space while adapting to new class features.

## Key Results
- SDC achieves state-of-the-art average accuracy of 68.60% on CIFAR100 with 60 base + 40 incremental classes
- Achieves 68.74% average accuracy on MiniImageNet with 60 base + 40 incremental classes
- Demonstrates 67.60% average accuracy on CUB200 with 100 base + 100 incremental classes
- Shows consistent performance improvement across all four benchmark datasets compared to existing FSCIL methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Separating the classifier's projection layer into frozen "static" and trainable "dynamic" components creates a balanced feature manifold for incremental learning.
- **Mechanism:** The Static Retaining Stage trains a projector on abundant base data and freezes it to preserve the geometric structure of the base feature space. During Dynamic Learning Stage, a parallel "dynamic" projector adapts to new class features. The final representation is a weighted sum (v = (1-α)v_static + αv_dynamic).
- **Core assumption:** The feature transformation learned during the base session remains partially valid and beneficial for projecting novel class features, providing a stable geometric anchor.
- **Evidence anchors:** [Abstract] "...preserve the key part as static memory to retain fundamental old knowledge... introduce an extra dynamic projector jointly trained..." [Section 4.1] "...store the projector δ_B as a static projector... encapsulates some common knowledge and the core patterns of the base classes."

### Mechanism 2
- **Claim:** Utilizing an external memory of prototypes alongside projector collaboration mitigates catastrophic forgetting without storing raw data.
- **Mechanism:** The framework stores a prototype m_c for each class in memory M^(t). During DLS training, the loss function includes a cross-entropy term calculated on these prototypes. This forces the expanding classifier to maintain decision boundaries for old classes even though their raw data is inaccessible.
- **Core assumption:** The frozen backbone produces consistent embeddings over time, ensuring that prototypes calculated in session t=0 remain meaningful proxies for those classes in session t>0.
- **Evidence anchors:** [Section 4.2] "Before training, we initialize a memory... calculated by averaging all features of the backbone for each class." [Equation 6] Defines the risk objective R_i as a sum of losses over current data D^(t) and memory M^(t).

### Mechanism 3
- **Claim:** Weighted interpolation between static and dynamic outputs allows explicit control over the stability-plasticity trade-off.
- **Mechanism:** The hyperparameter α controls the linear interpolation between the static and dynamic projector outputs. An α near 0 prioritizes the base session structure (stability), while α near 1 prioritizes the new session structure (plasticity). The paper identifies α=0.5 as an equilibrium point.
- **Core assumption:** The optimal representation for incremental learning lies in a linear subspace between the "base-knowledge subspace" and the "novel-knowledge subspace."
- **Evidence anchors:** [Section 5.3] "Our SDC achieves the best accuracy when α=0.5... static and dynamic projectors are equally important." [Section 4.3] Eq. 8 describes the optimization as maximizing mutual information terms weighted by (1-α) and α.

## Foundational Learning

- **Concept:** Catastrophic Forgetting & Stability-Plasticity Dilemma
  - **Why needed here:** This is the fundamental problem SDC attempts to solve. Understanding that neural networks tend to overwrite weights when fine-tuned on new data is essential to grasp why the architecture splits into static and dynamic components.
  - **Quick check question:** Why does standard fine-tuning on a 5-shot task destroy performance on the base 60 classes?

- **Concept:** Prototypical Networks / Metric Learning
  - **Why needed here:** The method relies on storing "prototypes" (class feature averages) in an external memory. You must understand that classification can be performed by measuring distance to these prototypes rather than just using a linear weight matrix.
  - **Quick check question:** How is a prototype calculated for a given class in memory M^(t)?

- **Concept:** Transfer Learning & Backbone Freezing
  - **Why needed here:** The method relies on a "frozen backbone" strategy. You need to understand that the heavy lifting of feature extraction is fixed, and only the "head" (projectors/classifier) is updated.
  - **Quick check question:** Which part of the network retains the "static memory," and is the backbone trained during the incremental sessions?

## Architecture Onboarding

- **Component map:**
  - Backbone (φ_b): ResNet12/18. Extracts raw features f_i. Frozen during incremental sessions.
  - Static Projector (δ_B): MLP. Trained in Base Session, Frozen thereafter. Provides "Stability" features.
  - Dynamic Projector (δ_I^(t)): MLP. Initialized and trained in every incremental session. Provides "Plasticity" features.
  - Merger: Performs weighted addition: Output = (1-α)δ_B(f_i) + αδ_I^(t)(f_i), followed by L2 Normalization.
  - FC Layer (δ_f^(t)): Linear classifier. Expands output dimensions as new classes arrive.
  - External Memory (M^(t)): Stores class prototypes (mean feature vectors) for all seen classes to stabilize the FC layer training.

- **Critical path:**
  1. SRS (Base): Train Backbone + Static Projector + FC on D^(0).
  2. Freeze: Lock Backbone and Static Projector weights.
  3. DLS (Incremental): Initialize new Dynamic Projector. Expand FC layer weights (e.g., add 5 neurons for 5-way task).
  4. Memory Update: Calculate prototypes for new classes and append to Memory M^(t).
  5. Optimize: Train Dynamic Projector and FC layer using Loss on current data + Loss on Memory prototypes.

- **Design tradeoffs:**
  - Projector Depth: Paper suggests 2-layer MLP. Deeper layers (3-4) overfit on 5-shot data; 1 layer lacks capacity.
  - Alpha (α): Fixed at 0.5. Lower values secure base knowledge but hurt new class adaptation; higher values do the opposite.
  - Dimensionality: 2048 dimensions provided the best balance; lower dimensions constrained representation power.

- **Failure signatures:**
  - Base Accuracy Collapse: Likely caused by training the Static Projector or Backbone during DLS (violating the freeze), or α being too high.
  - New Class Underfitting: Dynamic Projector may be too shallow or α set too low (static features dominating).
  - Memory Mismatch: If the backbone is accidentally unfrozen, the stored prototypes will drift away from the current features, causing erratic loss values during memory replay.

- **First 3 experiments:**
  1. Sanity Check (Base Only): Train SRS on the base session (e.g., CIFAR100 60 classes). Verify that the Static Projector + FC achieves reasonable accuracy (>80%) before freezing.
  2. Alpha Sweep: Run the first incremental session (Session 1) with α ∈ {0.0, 0.5, 1.0}. Confirm that α=0.0 preserves base classes but fails new ones, and α=1.0 does the opposite.
  3. Component Ablation: Run DLS with Memory disabled vs. enabled. Verify that without Memory, old class accuracy drops significantly faster (measuring the "double insurance" effect claimed in Section 4.2).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the weighting factor α between static and dynamic projectors be learned adaptively per session rather than set as a fixed hyperparameter?
- Basis in paper: [inferred] Table 5 demonstrates that performance varies significantly with different fixed values of α (0.1 to 0.9), with 0.5 performing best on average but specific sessions favoring different balances.
- Why unresolved: The current implementation relies on a manually tuned, global static setting for α, which cannot adjust to the specific difficulty or data distribution of individual incremental sessions.
- What evidence would resolve it: The development of a self-supervised mechanism or a meta-learning approach that dynamically adjusts α based on validation loss or feature divergence in each session.

### Open Question 2
- Question: How does the SDC framework perform under strict memory constraints or zero-memory settings where the external memory bank is removed?
- Basis in paper: [inferred] The methodology relies on an "external memory" to store prototypes for old classes to mitigate forgetting. The paper does not evaluate the performance degradation if this memory is removed or restricted to a very small size.
- Why unresolved: It is unclear if the Static Projector alone is sufficient to retain old knowledge without the explicit replay provided by the memory bank, which is a limitation for privacy-sensitive or resource-constrained applications.
- What evidence would resolve it: Ablation studies showing performance curves as the memory size decreases to zero, or a comparison against methods that do not use explicit memory replay.

### Open Question 3
- Question: What is the theoretical limit for the projector's capacity (layers and dimensions) before overfitting negates the benefits of the static-dynamic collaboration?
- Basis in paper: [inferred] Section 5.3 (Effect of layer number) notes that while increasing parameters initially helps, it eventually leads to a saturation point and potential overfitting due to limited samples.
- Why unresolved: The paper empirically selects a 2-layer MLP with 2048 dimensions but does not provide a theoretical guideline for sizing the projector relative to the backbone capacity or the number of shots (K).
- What evidence would resolve it: A theoretical analysis or empirical sweep correlating the projector parameter count with the shot number K to identify the "overfitting boundary."

## Limitations
- The framework requires storing class prototypes in external memory, which may not be feasible for privacy-sensitive applications or extremely large class spaces.
- The optimal α value (0.5) is determined empirically and may not generalize well to task sequences with significantly different characteristics or scale.
- Critical implementation details such as hidden layer dimensions of the MLP projector and exact learning rate schedules for incremental sessions are not fully specified, potentially affecting reproducibility.

## Confidence

- **High Confidence:** The core claim that the Static-Dynamic Collaboration framework effectively balances stability and plasticity in FSCIL is well-supported by the proposed mechanism and ablation studies.
- **Medium Confidence:** The claim of achieving state-of-the-art performance on all four benchmarks is credible but relies on specific hyperparameter choices and implementation details that are not fully disclosed.
- **Medium Confidence:** The claim that a fixed α=0.5 is optimal for balancing static and dynamic features is supported by experiments, but its robustness across significantly different task sequences or larger scale problems is not established.

## Next Checks

1. **Reproduce the base session training (SRS)** on CIFAR100 and verify that the Static Projector + FC achieves >80% accuracy before freezing.

2. **Conduct an alpha sweep** in the first incremental session (Session 1) with α ∈ {0.0, 0.5, 1.0} to confirm the stability-plasticity tradeoff described in the paper.

3. **Perform a memory ablation study** by running DLS with Memory disabled vs. enabled to quantify the "double insurance" effect on old class accuracy preservation.