---
ver: rpa2
title: Policy-Based Trajectory Clustering in Offline Reinforcement Learning
arxiv_id: '2506.09202'
source_url: https://arxiv.org/abs/2506.09202
tags:
- clustering
- pg-kmeans
- policy
- learning
- policies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces policy-based trajectory clustering for offline
  reinforcement learning, where trajectories are grouped by the underlying policy
  that generated them. The authors formulate a KL-divergence-based clustering objective
  and propose two methods: Policy-Guided K-means (PG-Kmeans), which iteratively trains
  behavior cloning policies and assigns trajectories based on generation probabilities,
  and Centroid-Attracted Autoencoder (CAAE), which learns trajectory representations
  in a latent space guided by codebook entries.'
---

# Policy-Based Trajectory Clustering in Offline Reinforcement Learning

## Quick Facts
- arXiv ID: 2506.09202
- Source URL: https://arxiv.org/abs/2506.09202
- Reference count: 40
- Primary result: Policy-based clustering methods achieve high NMI scores on D4RL and GridWorld environments, significantly outperforming traditional clustering baselines

## Executive Summary
This paper introduces policy-based trajectory clustering for offline reinforcement learning, where trajectories are grouped by the underlying policy that generated them. The authors formulate a KL-divergence-based clustering objective and propose two methods: Policy-Guided K-means (PG-Kmeans), which iteratively trains behavior cloning policies and assigns trajectories based on generation probabilities, and Centroid-Attracted Autoencoder (CAAE), which learns trajectory representations in a latent space guided by codebook entries. Theoretical analysis proves finite-step convergence of PG-Kmeans and identifies the inherent ambiguity of optimal solutions due to policy-induced conflicts. Experiments on D4RL and custom GridWorld environments show that both methods achieve high Normalized Mutual Information (NMI) scores, significantly outperforming traditional clustering baselines like DEC and VAE+K-means, demonstrating their effectiveness in capturing policy-level structure in offline RL datasets.

## Method Summary
The paper proposes two policy-based trajectory clustering methods for offline reinforcement learning. PG-Kmeans iteratively performs M-steps (training behavior cloning policies on assigned trajectories) and E-steps (reassigning trajectories to the most likely generating policy) to maximize a KL-divergence-based objective. CAAE uses a VQ-VAE-like architecture where trajectory embeddings are attracted to learnable codebook centroids while reconstructing actions. Both methods are theoretically grounded with convergence guarantees for PG-Kmeans and NP-completeness proof for the clustering problem. The approaches are evaluated on D4RL benchmark datasets and custom GridWorld environments with ground truth policy labels, showing significant improvements over traditional clustering baselines.

## Key Results
- PG-Kmeans and CAAE achieve high NMI scores on D4RL and GridWorld environments, demonstrating effective policy-level clustering
- Both methods significantly outperform traditional clustering baselines like DEC and VAE+K-means on policy-based clustering tasks
- Theoretical analysis proves finite-step convergence of PG-Kmeans and identifies inherent ambiguity in optimal solutions due to policy conflicts
- Overparameterization with subsequent cluster merging improves robustness to initialization and spurious clusters

## Why This Works (Mechanism)

### Mechanism 1: KL-Divergence Minimization Between Empirical and Policy-Induced Distributions
- Claim: Clustering trajectories by minimizing KL divergence between the empirical data distribution and a mixture of policy-induced distributions groups trajectories by their generative policy.
- Mechanism: The objective J(W, θ) maximizes Σᵢ Σⱼ wᵢⱼ Σₜ log P(aᵢ,ₕ | θⱼ, sᵢ,ₕ), iteratively assigning trajectories to policies (E-step) and updating policies via behavior cloning (M-step). This alternation drives each policy to model its assigned trajectories well while assigning each trajectory to its most likely generative policy.
- Core assumption: Trajectories generated by distinct policies exhibit distinguishable action distributions conditioned on states, and the mixture of K policies sufficiently approximates the dataset.
- Evidence anchors:
  - [abstract]: "leveraging the connection between the KL-divergence of offline trajectory distributions and a mixture of policy-induced distributions, we formulate a natural clustering objective"
  - [Section 5.1, Eq. 3]: The explicit objective function derivation
  - [corpus]: Weak direct evidence for this specific mechanism; related offline RL work (FlowQ, ASTRO) focuses on trajectory generation rather than clustering objectives
- Break condition: When policies exhibit high overlap in state-action distributions (low "conflict rate"), a single policy can model multiple behavior patterns with low loss, causing cluster collapse (observed in Pathfollowing environment).

### Mechanism 2: Codebook-Attracted Latent Space Regularization
- Claim: Regularizing trajectory embeddings toward learnable codebook centroids enforces cluster structure in latent space while reconstructing actions.
- Mechanism: CAAE encodes trajectories to latent z, decodes actions conditioned on z and observations, and applies loss L = -Σₕ log P(aᵢ,ₕ|θ,zᵢ,sᵢ,ₕ) + α·minⱼ||μⱼ - zᵢ||². The codebook term forces embeddings toward cluster centroids, while reconstruction ensures policy-relevant information is preserved. An additional regularization term (1/m²)Σᵢ,ⱼ min{1, ||μᵢ - μⱼ||²} prevents codebook collapse.
- Core assumption: Trajectory-level policy identity can be compressed into a low-dimensional latent vector that remains reconstructively useful.
- Evidence anchors:
  - [Section 5.2, Eq. 4]: Full objective definition with reconstruction and regularization terms
  - [abstract]: "CAAE resembles the VQ-VAE framework by guiding the latent representations of trajectories toward the vicinity of specific codebook entries"
  - [corpus]: BiTrajDiff uses diffusion for trajectory generation in offline RL but doesn't address clustering; VAE-based approaches are mentioned as baselines that underperform
- Break condition: If regularization weight α is too low, codebook collapses to a single point; if too high, reconstruction degrades and policy information is lost (ablation in Table 7 shows regularization improves robustness).

### Mechanism 3: Conflict-Based Trajectory Incompatibility Detection
- Claim: Two trajectories with conflicting (s,a) pairs cannot originate from the same deterministic policy, providing hard constraints for valid clusterings.
- Mechanism: Define "distance" d(τ₁, τ₂) = 1 if ∃(s,a)∈τ₁, (s,a')∈τ₂ with a≠a', else 0. Valid clusterings require zero intra-cluster conflicts (D(W) = 0). This reduction to K-coloring shows the problem is NP-complete and explains why multiple valid clusterings exist.
- Core assumption: Policies are deterministic; stochastic policies would soften these constraints.
- Evidence anchors:
  - [Section 4]: "we draw a reduction to the well-known K-coloring problem"
  - [Section A.2, Eq. 9-12]: Formal conflict definition and K-coloring equivalence
  - [corpus]: No direct corpus evidence for this specific theoretical result
- Break condition: When conflict rate is low between policies (e.g., Pathfollowing where policies only differ in start region), the constraint becomes weak and multiple clusterings appear equally valid, causing ambiguity.

## Foundational Learning

- Concept: **KL-Divergence and Distribution Matching**
  - Why needed here: The entire PG-Kmeans objective is derived from minimizing KL divergence between empirical trajectory distribution and a mixture of policy-induced distributions. Without understanding that KL minimization ≈ maximum likelihood for mixtures, the objective appears unmotivated.
  - Quick check question: Given a dataset with empirical distribution P(τ) and a mixture model Q(τ) = Σⱼ wⱼP(τ|θⱼ), what does minimizing D_KL(P||Q) simplify to?

- Concept: **Expectation-Maximization for Mixture Models**
  - Why needed here: PG-Kmeans is an EM algorithm where E-step assigns trajectories to clusters (hard assignment) and M-step updates policy parameters via BC. The convergence proof relies on the strictly-increasing objective property of EM.
  - Quick check question: In EM, why does the objective function monotonically increase? What guarantees finite convergence when assignments are discrete?

- Concept: **Offline RL Distributional Shift**
  - Why needed here: The paper's motivation rests on offline RL's core challenge—learning from fixed datasets without environment interaction leads to distribution shift when learned policy deviates from behavior policy. Clustering addresses this by isolating homogeneous behavior subsets.
  - Quick check question: Why does training a single policy on heterogeneous trajectories from multiple behavior policies cause "interference" and instability?

## Architecture Onboarding

- Component map:
```
PG-Kmeans Pipeline:
Dataset D → [Initialize k cluster assignments W] → Loop until convergence:
  → [M-step: Train k BC policies on assigned trajectories] → {π₁, ..., πₖ}
  → [E-step: Compute P(τᵢ|θⱼ) for all i,j] → Reassign W by argmax
  → [Check: W unchanged?]
→ [Optional: Merge clusters if k > k*]

CAAE Pipeline:
Dataset D → [Encoder: GRU + Attention] → Latent z ∈ ℝᵈ
         → [Codebook: {μ₁, ..., μₖ}] → Compute attraction loss α·minⱼ||μⱼ - zᵢ||²
         → [Decoder: Conditional on z + observation] → Reconstruct â
         → [Loss = -log P(a|z,s) + codebook attraction + centroid separation]
```

- Critical path:
  1. **For PG-Kmeans**: Initialization of cluster assignments W is critical—the paper uses "Best-of-N" strategy (Algorithm 3) running multiple initializations and selecting by final objective J(W,θ). Poor initialization causes mode collapse.
  2. **For CAAE**: Codebook regularization is essential—without it, all centroids collapse to a single point (Section C.2.1). The term (1/m²)Σᵢ,ⱼ min{1, ||μᵢ - μⱼ||²} maintains centroid separation.
  3. **For both**: Overparameterization (initializing k > k*) with subsequent merging improves robustness (Algorithm 4).

- Design tradeoffs:
  | Aspect | PG-Kmeans | CAAE |
  |--------|-----------|------|
  | Cluster boundaries | Sharp, distinct (separate policy networks) | Softer (shared decoder modulated by latent) |
  | Computational cost | Higher (k policy networks) | Lower (unified encoder-decoder) |
  | Training stability | More sensitive, requires Best-of-N | More stable in experiments |
  | Fine-grained detection | Better (single-step action likelihoods) | Weaker (holistic trajectory embeddings) |
  | Interpretability | Direct policy extraction | Requires decoding latent |

- Failure signatures:
  1. **Mode collapse in PG-Kmeans**: All trajectories assigned to single cluster → caused by poor initialization or low inter-policy conflict rate. Detect by checking cluster size variance.
  2. **Codebook collapse in CAAE**: All μⱼ converge to same point → regularization weight α too low or centroid separation term missing. Detect by monitoring ||μᵢ - μⱼ|| across training.
  3. **Overfitting to short trajectories**: Small clusters with low-data policies overfit → use overparameterization with merging to absorb spurious clusters.
  4. **Ambiguous clustering on low-conflict data**: NMI stuck below 1.0 despite low loss (Figure 3) → inherent problem ambiguity when policies rarely conflict (e.g., Pathfollowing). Not fixable via optimization.

- First 3 experiments:
  1. **Sanity check on synthetic GridWorld**: Run PG-Kmeans on "Takeball" environment (4 distinct ball-collection policies). Expected: NMI ≈ 1.0. If significantly lower, check initialization and BC training convergence.
  2. **Ablation on regularization weight α**: Train CAAE on HalfCheetah with α ∈ {0.1, 0.5, 1.0, 2.0}. Monitor both NMI and reconstruction loss. Expected: Optimal α ≈ 1.0 balances clustering and reconstruction.
  3. **Cluster count sensitivity**: Run both methods on D4RL Ant with k ∈ {2, 4, 6, 8} (ground truth = 2). PG-Kmeans with merging should maintain NMI > 0.9 across k; CAAE should show gradual degradation. This validates robustness to misspecified k*.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical convergence guarantees apply only to finite-step convergence with discrete assignment updates, not to the optimal clustering itself
- Empirical validation relies heavily on synthetic GridWorld environments with ground truth labels rather than real-world datasets where true policy identities are unknown
- No ablation studies on the impact of trajectory length or noise levels on clustering performance

## Confidence
- High confidence: KL-divergence formulation, finite-step convergence proof for PG-Kmeans, codebook regularization mechanism in CAAE
- Medium confidence: Performance improvements over baselines, the effectiveness of overparameterization with merging strategy
- Low confidence: Generalizability to real-world datasets without ground truth labels, practical utility for improving downstream RL performance

## Next Checks
1. **Downstream RL evaluation**: Train a single policy on trajectories from each cluster and compare performance to training on unclustered data to establish practical value
2. **Noise sensitivity analysis**: Systematically vary trajectory noise levels and trajectory length to quantify robustness of both methods
3. **Real-world dataset testing**: Apply methods to real-world offline RL datasets (e.g., D4RL real-world tasks) without ground truth labels and evaluate through qualitative analysis and downstream task performance