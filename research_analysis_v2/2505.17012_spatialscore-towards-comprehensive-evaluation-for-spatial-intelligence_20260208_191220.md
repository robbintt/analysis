---
ver: rpa2
title: 'SpatialScore: Towards Comprehensive Evaluation for Spatial Intelligence'
arxiv_id: '2505.17012'
source_url: https://arxiv.org/abs/2505.17012
tags:
- spatial
- object
- image
- answer
- camera
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SpatialScore, the most comprehensive and
  diverse benchmark for multimodal large language models (MLLMs) on spatial intelligence,
  addressing the gap in existing fragmented evaluations. SpatialScore comprises approximately
  5,000 manually verified samples spanning 30 distinct tasks across 10 categories,
  covering multiple visual data types, input modalities, and question-answering formats.
---

# SpatialScore: Towards Comprehensive Evaluation for Spatial Intelligence

## Quick Facts
- **arXiv ID:** 2505.17012
- **Source URL:** https://arxiv.org/abs/2505.17012
- **Reference count:** 40
- **Primary result:** Introduces SpatialScore, the most comprehensive multimodal benchmark for spatial intelligence, and demonstrates that both supervised fine-tuning and tool-augmented agents significantly improve MLLM spatial reasoning.

## Executive Summary
This paper introduces SpatialScore, the most comprehensive and diverse benchmark for multimodal large language models (MLLMs) on spatial intelligence, addressing the gap in existing fragmented evaluations. SpatialScore comprises approximately 5,000 manually verified samples spanning 30 distinct tasks across 10 categories, covering multiple visual data types, input modalities, and question-answering formats. Evaluations of 40 representative MLLMs reveal persistent challenges and a substantial gap relative to human performance. To enhance spatial reasoning, the authors construct SpatialCorpus, a large-scale training resource with 331K multimodal QA samples, and develop SpatialAgent, a multi-agent system with 12 specialized spatial perception tools supporting Plan-Execute and ReAct reasoning paradigms. Fine-tuning on SpatialCorpus and using SpatialAgent both yield significant improvements, with fine-tuning achieving gains of +10.47% and +9.23% in overall accuracy for Qwen3-VL-4B and Qwen3-VL-8B, respectively, and SpatialAgent providing consistent performance gains across nearly all tasks without additional training. The benchmark, corpus, and agent framework are expected to serve as a solid foundation for advancing MLLMs toward human-level spatial intelligence.

## Method Summary
The paper introduces SpatialScore, a benchmark with 5,025 manually verified samples across 30 tasks in 10 categories, evaluating MLLMs on spatial intelligence. To improve spatial reasoning, the authors construct SpatialCorpus (331K QA pairs from 3D data sources) for supervised fine-tuning and develop SpatialAgent, a multi-agent system with 12 spatial perception tools. Fine-tuning Qwen3-VL-4B/8B on SpatialCorpus for 1 epoch yields +10.47% and +9.23% accuracy gains, respectively. SpatialAgent uses Plan-Execute or ReAct paradigms to dynamically invoke tools without additional training, achieving consistent gains across tasks. Evaluations use standardized prompts and temperature=0, with open-ended questions scored via MRA.

## Key Results
- SpatialScore reveals persistent gaps in MLLM spatial reasoning across 40 models, with significant room for improvement.
- Fine-tuning on SpatialCorpus achieves gains of +10.47% (Qwen3-VL-4B) and +9.23% (Qwen3-VL-8B) in overall accuracy.
- SpatialAgent provides consistent performance gains across nearly all tasks without additional training, using 12 specialized tools in Plan-Execute or ReAct paradigms.
- ReAct paradigm demonstrates better flexibility (0.02% failure rate) than Plan-Execute (2.25–8.24% failure rate), albeit with higher computational cost.

## Why This Works (Mechanism)

### Mechanism 1: Large-Scale Supervised Fine-Tuning on Curated Spatial QA Data
- **Claim:** Fine-tuning on SpatialCorpus (331K samples) improves MLLM spatial reasoning performance, as measured by SpatialScore.
- **Mechanism:** Exposure to diverse, manually-verified spatial QA pairs across 16 tasks enables the model to internalize patterns for depth estimation, object distance, camera geometry, and 3D localization, which are under-represented in general pre-training data.
- **Core assumption:** The training distribution is sufficiently representative of evaluation tasks and does not induce catastrophic forgetting of non-spatial capabilities.
- **Evidence anchors:**
  - [abstract]: "Fine-tuning on SpatialCorpus... achieving gains of +10.47% and +9.23% in overall accuracy for Qwen3-VL-4B and Qwen3-VL-8B"
  - [Section 3.2]: "we construct SpatialCorpus, a large-scale training resource with 331K multimodal QA samples that supports fine-tuning on spatial reasoning tasks"
  - [corpus]: Weak direct evidence; related work (SpatialBench, BEAR) shows similar trends in task-specific fine-tuning gains, but not on this exact corpus
- **Break condition:** Performance gains concentrate on well-represented tasks (e.g., mental animation, camera) while under-represented tasks (e.g., view reasoning) may degrade due to data imbalance or overfitting.

### Mechanism 2: Tool-Augmented Multi-Agent Reasoning Without Training
- **Claim:** SpatialAgent, a multi-agent system with 12 specialized spatial perception tools, improves MLLM spatial reasoning in a training-free manner by orchestrating external expert models.
- **Mechanism:** The agent core (an MLLM) decomposes spatial questions, invokes appropriate tools (e.g., depth estimation, homography, optical flow), and synthesizes tool outputs into final answers. This bypasses the need for the MLLM to internally compute precise geometric quantities.
- **Core assumption:** The agent can reliably select correct tools, interpret outputs correctly, and avoid hallucinating tool calls or results.
- **Evidence anchors:**
  - [abstract]: "SpatialAgent providing consistent performance gains across nearly all tasks without additional training"
  - [Section 3.3]: "SpatialAgent enhances spatial understanding capabilities by dynamically invoking spatial perception tools and reasoning through Plan–Execute (PE) or ReAct paradigms"
  - [corpus]: BEAR benchmark similarly evaluates embodied capabilities with tool-augmented agents, supporting the plausibility of this mechanism
- **Break condition:** Tool failures, misinterpretation of outputs (e.g., confusing depth with object distance), or malformed agent outputs trigger fallback to direct model response, which may be less accurate.

### Mechanism 3: Comparative Reasoning Paradigms—Plan-Execute vs. ReAct
- **Claim:** Both Plan-Execute (PE) and ReAct paradigms improve spatial reasoning, but ReAct achieves higher robustness through iterative error correction at the cost of efficiency.
- **Mechanism:** PE generates a fixed tool invocation plan upfront and executes sequentially; ReAct interleaves observation and action, allowing dynamic replanning based on intermediate results. The latter reduces failure rates but increases inference cost.
- **Core assumption:** The agent core can maintain coherent reasoning over multiple turns and correctly terminate when confident.
- **Evidence anchors:**
  - [Section 3.3]: "Plan–Execute paradigm excels at efficient plan formulation... ReAct paradigm demonstrates better flexibility through dynamic planning"
  - [Section B.3]: PE failure rates of 2.25–8.24% vs. ReAct failure rate of 0.02% on SpatialScore
  - [corpus]: MME-Reasoning and related benchmarks similarly compare single-pass vs. iterative reasoning, generally finding trade-offs between efficiency and robustness
- **Break condition:** Exceeding maximum iteration limits (3 for PE, 10 for ReAct) or invalid JSON outputs trigger fallback; ReAct is more resilient but still vulnerable to infinite loops if the observer fails to call Terminate.

## Foundational Learning

- **Concept: Spatial Intelligence Taxonomy**
  - **Why needed here:** Understanding the 10 categories and 30 tasks (e.g., mental animation, depth estimation, camera pose) is prerequisite to interpreting SpatialScore results and diagnosing model weaknesses.
  - **Quick check question:** Can you list three distinct spatial reasoning tasks evaluated in SpatialScore and explain what cognitive or geometric capability each assesses?

- **Concept: Multi-Agent System Orchestration**
  - **Why needed here:** SpatialAgent decomposes into planner/observer, executor, and summarizer roles; understanding role separation is necessary to debug agent failures and design prompts.
  - **Quick check question:** In the ReAct paradigm, what component decides whether to call another tool or terminate, and what state does it condition on?

- **Concept: Mean Relative Accuracy (MRA) for Metric Evaluation**
  - **Why needed here:** SpatialScore uses MRA to evaluate open-ended numerical answers (e.g., distance in meters), accounting for relative error rather than exact match.
  - **Quick check question:** Given a ground truth of 2.0 meters and predictions of 1.8m, 2.5m, and 3.0m, which would score highest under MRA with typical thresholds?

## Architecture Onboarding

- **Component map:**
  - SpatialScore (benchmark) -> Qwen3-VL-4B/8B (model) -> SpatialCorpus (training data) or SpatialAgent (inference framework)
  - SpatialAgent (planner/observer -> executor -> summarizer) -> 12 tools (depth estimation, optical flow, homography, etc.)

- **Critical path:**
  1. **Evaluation:** Load SpatialScore, run model inference with standardized prompts (temperature=0, max tokens=512–4096 depending on paradigm), parse answers using format-specific rules or LLM-based scoring for open-ended questions.
  2. **Fine-tuning (optional):** Fine-tune Qwen3-VL-4B/8B on SpatialCorpus for 1 epoch with cross-entropy loss, frozen visual encoder, and trained projection/LLM layers.
  3. **Agent inference (optional):** Initialize SpatialAgent with toolbox specifications; for each sample, generate tool plan (PE) or iterate observations/actions (ReAct) until Terminate is called or max iterations reach.

- **Design tradeoffs:**
  - **SFT vs. Agent:** SFT yields larger gains (+10.47% for 4B) but risks overfitting and requires compute; Agent yields smaller but consistent gains (+6–8%) without training and preserves general capabilities.
  - **PE vs. ReAct:** PE is faster but has higher failure rates (up to 8.24%); ReAct is more robust (0.02% failure) but slower and more complex to debug.
  - **Tool coverage:** Current toolbox lacks direct point cloud or depth map inputs; relies on RGB-derived estimates, limiting accuracy for precise 3D geometry tasks.

- **Failure signatures:**
  - **Tool hallucination:** Agent claims tool outputs without actual invocation (mitigated by strict JSON validation and in-context examples).
  - **Output parsing failures:** Model ignores prompt format; fallback to LLM-based scoring or direct model response.
  - **Catastrophic forgetting:** Fine-tuned model degrades on non-spatial tasks (not evaluated in this paper but noted as risk in Section 7.1).
  - **Tool misinterpretation:** Confusing depth (camera-to-object distance) with inter-object distance; addressed in tool descriptions but still occurs in qualitative examples.

- **First 3 experiments:**
  1. **Baseline evaluation:** Run zero-shot inference on SpatialScore for Qwen3-VL-4B/8B; compute overall and category-wise accuracy to establish baseline.
  2. **Agent-only comparison:** Evaluate SpatialAgent (PE and ReAct) using the same models as agent cores; compare failure rates and task-specific gains to identify where tools help most.
  3. **Ablation by task category:** For both SFT and Agent approaches, analyze per-category gains (e.g., mental animation vs. camera pose) to diagnose which spatial capabilities are most amenable to each approach.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** To what extent does incorporating native 3D inputs (point clouds, depth maps) improve spatial intelligence compared to 2D-only baselines?
- **Basis in paper:** [explicit] "SpatialScore... primarily relies on RGB frames, lacking samples that take point clouds, depth maps, or surface normals as input." (Sec 7.1)
- **Why unresolved:** The paper notes that current evaluations and model capabilities are confined to 2D frames, limiting the assessment of intrinsic 3D understanding.
- **What evidence would resolve it:** A comparative evaluation where models are fine-tuned or prompted with RGB-D data on SpatialScore, demonstrating superior performance over RGB-only baselines.

### Open Question 2
- **Question:** How can supervised fine-tuning strategies be optimized to ensure uniform improvement across all spatial tasks without catastrophic forgetting?
- **Basis in paper:** [explicit] "gains from supervised fine-tuning largely concentrate on already optimized task types... while tasks with limited data scalability... may suffer from catastrophic forgetting." (Sec 4.2)
- **Why unresolved:** The authors observe that increasing data scale for specific tasks leads to biased gains, suggesting current fine-tuning is partial and insufficient.
- **What evidence would resolve it:** A training regime on an expanded SpatialCorpus that yields consistent accuracy gains across all 30 tasks in SpatialScore without regression in under-represented categories.

### Open Question 3
- **Question:** How can multi-agent systems be made robust against the misinterpretation of tool outputs during spatial reasoning?
- **Basis in paper:** [explicit] "occasional failures still occur, typically due to suboptimal tool execution or misinterpretation of intermediate results... such limitations are expected to diminish as... the toolbox design becomes more robust." (Sec 4.3)
- **Why unresolved:** The SpatialAgent framework relies on an MLLM to interpret numerical tool outputs (e.g., depth values), which occasionally leads to reasoning errors.
- **What evidence would resolve it:** An architectural improvement or verification module within SpatialAgent that significantly reduces error rates in "Plan-Execute" and "ReAct" paradigms compared to the current baseline.

## Limitations
- **Limited empirical validation of long-term generalization**: While the paper reports significant gains from both fine-tuning and agent-based approaches, it does not evaluate whether these improvements transfer to real-world scenarios beyond the controlled benchmark.
- **Potential overfitting to SpatialScore distribution**: The training corpus and benchmark share overlapping task categories and data sources, raising concerns about overfitting rather than genuine capability gains.
- **Toolbox reliability and generalization**: SpatialAgent's effectiveness depends on the accuracy and robustness of 12 external perception tools, for which the paper does not provide systematic error analysis or address potential cascading failures.

## Confidence
- **High confidence**: The benchmark construction methodology and task taxonomy are well-defined and reproducible. The claim that existing MLLMs exhibit persistent gaps in spatial reasoning is supported by comprehensive evaluation across 40 models.
- **Medium confidence**: The reported performance gains from fine-tuning (10.47% and 9.23%) are statistically significant within the benchmark but may not generalize to broader spatial reasoning tasks or real-world applications.
- **Low confidence**: The assertion that SpatialAgent provides "consistent performance gains across nearly all tasks" is based on limited quantitative evidence, and the paper does not adequately address edge cases where tool orchestration might fail.

## Next Checks
1. **Generalization test**: Evaluate fine-tuned models on independent spatial reasoning benchmarks (e.g., LEGO-Puzzles, BEAR) to verify that improvements transfer beyond SpatialScore and do not represent overfitting.
2. **Tool failure analysis**: Conduct systematic error analysis on SpatialAgent's tool outputs, measuring how often tool errors propagate through the reasoning pipeline and identifying the most failure-prone components.
3. **Capability retention study**: Assess whether fine-tuning on SpatialCorpus causes catastrophic forgetting by evaluating models on non-spatial MLLM benchmarks (e.g., MMMU, MME-Reasoning) before and after spatial fine-tuning.