---
ver: rpa2
title: 'FOSSIL: Harnessing Feedback on Suboptimal Samples for Data-Efficient Generalisation
  with Imitation Learning for Embodied Vision-and-Language Tasks'
arxiv_id: '2510.11307'
source_url: https://arxiv.org/abs/2510.11307
tags:
- feedback
- language
- learning
- tasks
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FOSSIL introduces a method to train language-guided embodied AI
  agents from both optimal and suboptimal demonstrations using language feedback to
  contextualize behavior. The core approach involves feeding language feedback embeddings
  directly into a Transformer-based policy and optionally adding auxiliary self-supervised
  objectives for feedback prediction.
---

# FOSSIL: Harnessing Feedback on Suboptimal Samples for Data-Efficient Generalisation with Imitation Learning for Embodied Vision-and-Language Tasks

## Quick Facts
- arXiv ID: 2510.11307
- Source URL: https://arxiv.org/abs/2510.11307
- Authors: Sabrina McCallum; Amit Parekh; Alessandro Suglia
- Reference count: 22
- Primary result: Language feedback enables learning from suboptimal demonstrations with 4x higher success rates than optimal-only baselines

## Executive Summary
FOSSIL introduces a method to train language-guided embodied AI agents from both optimal and suboptimal demonstrations using language feedback to contextualize behavior. The core approach involves feeding language feedback embeddings directly into a Transformer-based policy and optionally adding auxiliary self-supervised objectives for feedback prediction. Tested on a custom BabyAI-XGen environment for compositional generalization tasks, FOSSIL shows agents achieve significantly better performance (4x higher success rates) compared to baselines trained only on optimal trajectories. Language feedback performs comparably to shaped scalar rewards, with combined use providing additional robustness to input perturbations. Models also show strong data efficiency, outperforming online RL baselines trained on equivalent data. The results demonstrate that language feedback enables learning from suboptimal behavior while maintaining data efficiency and improving generalization.

## Method Summary
FOSSIL trains a Transformer-based policy on mixed-quality demonstrations (optimal and suboptimal trajectories) by incorporating language feedback as an additional input signal. The model processes interleaved sequences of instructions, feedback, observations, and actions, using frozen Sentence-BERT embeddings for language and a custom CNN for visual inputs. An optional self-supervised feedback prediction head improves robustness to missing or adversarial feedback. The approach is evaluated on BabyAI-XGen, a compositional generalization benchmark, showing 4x higher success rates compared to optimal-only training and superior data efficiency versus online RL baselines.

## Key Results
- 4x higher success rates (69.7% vs 15.1%) when language feedback contextualizes suboptimal trajectories
- Language feedback performs comparably to shaped scalar rewards for task success
- Combined language and scalar feedback provides additional robustness to input perturbations
- Strong data efficiency: outperforms online RL baselines trained on equivalent data

## Why This Works (Mechanism)

### Mechanism 1
Language feedback embeddings enable policies to distinguish between optimal and suboptimal actions within mixed-quality demonstrations. Feedback tokens are injected as input at each timestep alongside observations and actions. The Transformer learns to condition behavior predictions on both the instruction and the feedback signal, effectively learning "when this feedback applies, take this action" patterns. This allows the model to extract useful signal from otherwise confusing suboptimal trajectories. Core assumption: feedback is timely, accurate, and correlates with action quality; the model can learn grounded associations between language descriptions and behavioral outcomes. Evidence: Table 1 shows 4x higher success rates (69.7% vs 15.1%) when feedback contextualizes suboptimal trajectories.

### Mechanism 2
Self-supervised feedback prediction creates an internal forward model that improves robustness when external feedback is unavailable or adversarial. An auxiliary MSE loss trains a prediction head to forecast the next timestep's feedback embedding from the current hidden state. This forces the policy to encode action consequences in its representations. At inference, when feedback is missing or corrupted, the model can still reason about expected outcomes. Core assumption: predictable feedback structure exists; the model has sufficient capacity to learn both action prediction and feedback prediction simultaneously without interference. Evidence: Figure 4d shows models with feedback prediction retain ~80% performance when feedback is removed, versus collapse without it.

### Mechanism 3
Combining language feedback with scalar rewards provides complementary robustness to perturbations, even when success rates plateau. Language feedback captures semantic context about action quality (why something was good/bad), while scalar rewards provide dense optimization signal. The model can attend to whichever signal is most reliable under different conditions (e.g., adversarial language vs. perturbed actions). Core assumption: both signals are available during training and encode non-redundant information; the model can learn to weight them appropriately. Evidence: Figure 4b shows COMBO+FP retains 78% performance under sticky actions, highest among all variants.

## Foundational Learning

- **Transformer sequence modeling with interleaved multimodal tokens**: The architecture embeds instructions, observations, actions, and feedback as tokens in a single sequence; understanding causal masking and positional encoding is essential. Quick check: Can you explain why the model predicts action a_t from hidden state h_{t-1} rather than h_t?

- **Behavior cloning vs. offline RL**: FOSSIL operates in the imitation learning regime but borrows ideas from offline RL (returns-to-go conditioning); distinguishing these paradigms clarifies why feedback is needed. Quick check: Why does pure behavior cloning fail when training data contains both optimal and suboptimal trajectories for the same task?

- **Language embeddings and semantic similarity**: Feedback is encoded via Sentence-BERT; understanding how semantic similarity is preserved in embedding space helps diagnose feedback grounding issues. Quick check: If feedback templates change but semantics remain similar, would you expect performance to change? Why or why not?

## Architecture Onboarding

- **Component map**: Sentence-BERT (frozen) → text embedding → projection layer; custom CNN → 7×7 RGB observation → embedding → projection layer; learnable embeddings → discrete actions → projection layer; all tokens → Llama2-style Transformer (90M params, 12 layers, 12 heads) → action prediction head (CE, 6-way) + optional feedback prediction head (MSE) + optional reward prediction head (MSE)

- **Critical path**: 1) Generate mixed-quality trajectories with planner (optimal) + random actions (suboptimal); 2) Augment with rule-based feedback templates (task + affordance feedback); 3) Tokenize and interleave sequences; mask unused tokens per model variant; 4) Train with weighted loss; early stopping on validation action loss; 5) Evaluate on held-out compositional splits with optional perturbations

- **Design tradeoffs**: Feedback frequency (dense vs. sparse), auxiliary task inclusion (robustness vs. compute), model size (plateau beyond ~30M params for BabyAI-XGen)

- **Failure signatures**: Success rate plateaus at ~70% (compositional generalization limits), performance collapse with missing feedback (over-reliance on feedback), random performance with PPO baseline on small data (expected for online RL)

- **First 3 experiments**: 1) Ablate feedback type: train LANG-only, SCALAR-only, and COMBO variants on same mixed dataset; compare systematicity scores; 2) Test feedback timing: delay feedback by k timesteps to measure robustness to non-instantaneous annotation; 3) Perturb feedback semantics: replace task feedback with random sentences during inference; compare LANG+FP vs. COMBO+FP

## Open Questions the Paper Calls Out
- How to scale effectively to more realistic 3D environments with continuous action spaces and physics fidelity comparable to real-world robotic tasks
- How pre-trained vision-language models with larger architectures process and leverage language feedback signals compared to models trained from scratch
- The practical cost-accuracy tradeoffs between collecting step-level language feedback versus scalar rewards across different embodied AI domains
- How robust the approach is when feedback quality varies (incorrect, inconsistent, or partial annotations) rather than being synthetically generated from ground-truth environment state

## Limitations
- Dataset realism and scalability: BabyAI-XGen is highly simplified (7×7 grid, limited object vocabulary) and may not capture complexity of real-world embodied tasks
- Feedback quality dependence: approach critically depends on timely, accurate, and semantically grounded feedback; human-generated feedback may be noisier
- Generalization scope: unclear whether mechanisms would generalize to more complex environments with continuous states, longer horizons, or richer language

## Confidence
**High Confidence (4/5)**:
- Language feedback improves performance on mixed-quality demonstrations (4x higher success rates)
- Data efficiency advantage over online RL baselines with equivalent data
- Combined language feedback and scalar rewards provide robustness to perturbations

**Medium Confidence (3/5)**:
- Self-supervised feedback prediction improves robustness to missing/adversarial feedback
- Performance plateau around 70% success indicates inherent task difficulty limits
- Transformer-based policy with interleaved multimodal tokens is optimal architecture

**Low Confidence (2/5)**:
- Claims about real-world applicability beyond simplified BabyAI-XGen environment
- Assumptions about feedback grounding generalizing to arbitrary language
- Scalability to much larger state/action spaces without architectural changes

## Next Checks
1. **Temporal Feedback Robustness Test**: Systematically vary feedback delay (0-5 timesteps) in BabyAI-XGen and measure performance degradation to validate whether core mechanism relies on tight temporal coupling.

2. **Human Feedback Study**: Replace rule-based oracles with human annotators providing naturalistic language feedback on suboptimal trajectories to measure performance degradation and feedback quality correlation.

3. **Cross-Environment Generalization**: Test trained models on a different grid-world or simple 3D environment with similar compositional structure but different visual appearance to validate whether approach generalizes beyond BabyAI-XGen training distribution.