---
ver: rpa2
title: On the Gradient Domination of the LQG Problem
arxiv_id: '2507.09026'
source_url: https://arxiv.org/abs/2507.09026
tags:
- gradient
- controller
- history
- control
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of solving the Linear Quadratic
  Gaussian (LQG) control problem using policy gradient (PG) methods. While PG has
  shown strong theoretical guarantees for the linear quadratic regulator (LQR) problem,
  its application to the LQG setting is hindered by the lack of gradient dominance
  in the classical parameterization.
---

# On the Gradient Domination of the LQG Problem

## Quick Facts
- arXiv ID: 2507.09026
- Source URL: https://arxiv.org/abs/2507.09026
- Authors: Kasra Fallah; Leonardo F. Toso; James Anderson
- Reference count: 31
- Primary result: Establishes global convergence guarantees for policy gradient methods solving the LQG problem using history-based parameterizations

## Executive Summary
This paper addresses the challenge of applying policy gradient (PG) methods to the Linear Quadratic Gaussian (LQG) control problem, where classical parameterizations fail to provide gradient dominance properties. The authors propose using a history representation of the control input parameterized by past input and output data over a history length p. Under this parameterization, they establish gradient dominance and approximate smoothness for the LQG cost function, enabling global convergence of PG methods in both model-based and model-free settings. The work provides the first global convergence guarantees for PG methods in solving the LQG problem and offers insights into the tradeoff between history length and convergence rate.

## Method Summary
The authors introduce a history representation of the control input, parameterized by past input and output data over a history length p. This representation transforms the LQG problem into a form where the cost function exhibits gradient dominance and approximate smoothness properties. Under this parameterization, they prove global convergence of PG methods for the LQG problem. In the model-free setting, they employ a zeroth-order gradient estimator to estimate the policy gradient. The convergence rate is shown to depend critically on the history length parameter p, with longer histories leading to faster convergence due to improved conditioning of the optimization landscape. The authors also provide per-iteration stability guarantees, ensuring that the controller remains stabilizing throughout the learning process.

## Key Results
- Establishes gradient dominance and approximate smoothness for LQG cost under history parameterization
- Proves global convergence of PG methods for LQG in both model-based and model-free settings
- Shows convergence rate depends critically on history length p, with longer histories yielding faster convergence
- Provides per-iteration stability guarantees throughout the learning process

## Why This Works (Mechanism)
The history parameterization captures sufficient information about past states and inputs to make the cost function well-behaved. By lifting the policy to operate on past data sequences rather than just current states, the optimization landscape becomes more favorable with improved gradient properties. This transformation enables PG methods to find the global optimum despite the non-convexity inherent in LQG problems.

## Foundational Learning

### Gradient Dominance Property
**Why needed:** Ensures that the gradient points toward the global optimum with sufficient magnitude
**Quick check:** Verify that ∇J(u) ≥ α||u - u*|| holds for some α > 0 throughout the optimization

### History Representation
**Why needed:** Provides sufficient information to reconstruct the state and maintain gradient dominance
**Quick check:** Confirm that past inputs/outputs over length p uniquely determine the current state

### Zeroth-Order Gradient Estimation
**Why needed:** Enables model-free optimization when system dynamics are unknown
**Quick check:** Ensure finite-difference estimates converge to true gradients as sample size increases

## Architecture Onboarding

### Component Map
LQG system -> History representation layer -> Policy parameterization -> Cost function -> Gradient computation -> Parameter update

### Critical Path
History parameterization → Gradient dominance proof → Convergence guarantee → Stability verification

### Design Tradeoffs
History length p vs. data requirements: longer histories improve convergence but require longer warm-up horizons
Model-based vs. model-free: model-based offers exact gradients but requires system knowledge; model-free uses zeroth-order estimates

### Failure Signatures
Non-convergence indicates insufficient history length or poor initialization; instability suggests controller violates stability constraints

### First Experiments
1. Verify gradient dominance property empirically for various history lengths
2. Test convergence rates across different system dynamics (stable, unstable, marginally stable)
3. Compare model-based vs. model-free performance under varying noise levels

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can theoretical guarantees be established for the "discount annealing" method used to compute the initial stabilizing controller?
- **Basis in paper:** [Explicit] The authors state in Section IV.B that developing theoretical guarantees for their adaptation of this method to the LQG setting is "beyond the scope of this work."
- **Why unresolved:** The paper provides empirical validation of the method (Figure 2) but lacks formal proof of convergence or sample complexity bounds for the initialization phase.
- **What evidence would resolve it:** A formal proof showing the discount factor update rule reliably finds a stabilizing controller from an arbitrary initial policy, along with bounds on the number of samples required.

### Open Question 2
- **Question:** How can sample efficiency be improved in the model-free setting, specifically through variance-reduction techniques?
- **Basis in paper:** [Explicit] The Conclusion notes that noise and estimation error prevent exact recovery of the optimal controller, explicitly listing "improving sample efficiency... or incorporating variance-reduction techniques" as an open avenue.
- **Why unresolved:** The current zeroth-order gradient estimator requires a large number of samples (n_s) to overcome variance, resulting in a non-zero optimality gap.
- **What evidence would resolve it:** Demonstrating that algorithms like SVRG or SARAH applied to the history representation maintain the gradient dominance property while lowering the sample complexity bounds.

### Open Question 3
- **Question:** Can the global convergence guarantees for the history representation be extended to multi-task control design?
- **Basis in paper:** [Explicit] The Conclusion identifies "extending these results to the multi-task control design" as a specific direction for future work.
- **Why unresolved:** The current proof focuses on a single dynamical system; it is unclear if the gradient dominance and smoothness properties hold when optimizing a single lifted controller across heterogeneous tasks.
- **What evidence would resolve it:** A theoretical analysis proving gradient domination exists for the multi-task LQG objective under the history parameterization.

### Open Question 4
- **Question:** What is the optimal selection strategy for the history length p to balance convergence speed against data requirements?
- **Basis in paper:** [Inferred] Remark 2 highlights a tradeoff where increasing p improves convergence speed but necessitates a longer warm-up horizon T, potentially causing issues in low-data regimes.
- **Why unresolved:** The paper establishes that p ≥ n_y is necessary but does not provide a metric for determining the ideal p relative to system instability or data availability.
- **What evidence would resolve it:** A theoretical characterization of the "optimal" p as a function of system eigenvalues and available trajectory length T.

## Limitations
- History parameterization requires access to both past inputs and outputs, limiting practical applicability
- Convergence guarantees rely on idealized assumptions; finite sample effects and model mismatch impacts not fully explored
- Numerical validation limited to single open-loop unstable system; generalizability across diverse system dynamics unclear

## Confidence
- High confidence in theoretical analysis of gradient dominance and approximate smoothness under proposed history parameterization
- Medium confidence in global convergence guarantees for both model-based and model-free settings (rely on idealized assumptions)
- Medium confidence in practical applicability given limited scope of numerical experiments

## Next Checks
1. Conduct extensive numerical experiments on diverse set of linear systems (stable, unstable, marginally stable) to assess robustness and generalizability
2. Investigate impact of finite sample effects, model mismatch, and non-ideal noise distributions on convergence rate and optimality gap through empirical studies
3. Explore alternative parameterizations and their effect on gradient landscape and convergence properties to determine necessity and optimality of history representation