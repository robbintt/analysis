---
ver: rpa2
title: Evaluating the Capability of Video Question Generation for Expert Knowledge
  Elicitation
arxiv_id: '2512.15006'
source_url: https://arxiv.org/abs/2512.15006
tags:
- question
- video
- expert
- comments
- climber
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a new method to evaluate video question generation
  models for eliciting expert knowledge. The key idea is to use a retrieval-based
  framework: a QA retriever trained on expert comments is used to measure how well
  generated questions can retrieve relevant expert commentary.'
---

# Evaluating the Capability of Video Question Generation for Expert Knowledge Elicitation

## Quick Facts
- **arXiv ID:** 2512.15006
- **Source URL:** https://arxiv.org/abs/2512.15006
- **Reference count:** 30
- **Primary result:** Proposes a retrieval-based framework to evaluate video question generation models for eliciting expert knowledge

## Executive Summary
This paper introduces a novel method to evaluate video question generation models for eliciting expert knowledge through a retrieval-based framework. The core idea is that questions capable of retrieving relevant expert commentary from video segments are more likely to effectively trigger expert knowledge. To enable this evaluation, the authors created EgoExoAsk, a dataset of QA pairs derived from expert commentary in Ego-Exo4D videos. The framework uses a QA retriever trained on this dataset to measure how well generated questions can retrieve segment-specific expert commentary. Experiments with multiple video-language models demonstrate that richer context (such as video captions) improves question quality, and retrieval-based metrics provide a quantitative way to assess video question generation for expert knowledge elicitation.

## Method Summary
The authors propose evaluating video question generation by simulating question-answering communication with experts using a retrieval-based approach. They created EgoExoAsk by converting expert commentary from Ego-Exo4D videos into QA pairs through an LLM-guided verification-and-regeneration process. A QA retriever was trained on this dataset to measure how well generated questions retrieve relevant expert commentary. The evaluation uses retrieval metrics (R@1, R@5, MeanR, MedianR) to assess question quality. Multiple VLLMs were tested under different settings (naive, with captions, with RAG) to determine how context affects question generation for expert knowledge elicitation.

## Key Results
- Providing video captions improves retrieval metrics (R@1 and MeanR) by approximately 0.03 and 1.5 on average compared to naive mode
- Higher FPS (10 vs 2) improves retrieval scores, with R@1 increasing from 0.0614 to 0.0768
- RAG-based context retrieval underperforms naive and caption-based approaches, likely due to the retriever not being fine-tuned on expert domains
- Gold (oracle) questions achieve R@1≈0.66, while random questions achieve R@1≈0.05, establishing evaluation bounds

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Questions that retrieve segment-specific expert commentary approximate effective knowledge elicitation
- **Mechanism:** A QA retriever, trained on expert commentary converted to QA pairs, ranks candidate comments by cosine similarity to generated questions. Higher retrieval rank indicates the question would effectively trigger relevant expert knowledge
- **Core assumption:** The retriever's ranking behavior approximates how an expert would mentally associate a question with their knowledge
- **Evidence anchors:** The retrieval-based evaluation framework is proposed as a simulation of question-answering communication with experts
- **Break condition:** If the retriever fails to generalize beyond training domains, or if expert mental association does not correlate with embedding similarity, the evaluation loses validity

### Mechanism 2
- **Claim:** LLM-guided verification-and-regeneration improves QA pair quality by filtering low-quality questions
- **Mechanism:** An LLM (Qwen3-32B) generates questions from formatted comments, then a verification step checks rule compliance (observable actions, not too general, no overlap with answer). Failed questions are regenerated with failure reasons as negative examples
- **Core assumption:** The verification rules capture the essential properties of elicitation-effective questions
- **Evidence anchors:** Table 2 shows zero-shot retriever struggles (R@1=0.1347) while trained retriever succeeds (R@1=0.3300), indicating questions do not trivially leak answers
- **Break condition:** If verification rules are too strict or too lenient, the resulting QA pairs will not properly simulate expert-interviewer dynamics

### Mechanism 3
- **Claim:** Richer context (captions, higher FPS) enables VLLMs to generate more elicitation-effective questions
- **Mechanism:** VLLMs with access to atomic descriptions (human-annotated captions) alongside video frames can ground questions in specific observable actions, improving retrieval scores
- **Core assumption:** The retriever's improvement reflects genuine question quality, not overfitting to caption vocabulary
- **Evidence anchors:** Providing video captions improves R@1 and MeanR for all VLLMs (by about 0.03 and 1.5 on average compared to naive)
- **Break condition:** If captions already encode expert knowledge rather than surface descriptions, the task becomes trivial; if higher FPS introduces noise without information gain, performance degrades

## Foundational Learning

- **Concept: Retrieval-Augmented Evaluation**
  - **Why needed here:** The paper's core innovation is treating retrieval rank as a proxy for question quality; understanding contrastive learning (Eq. 6) and embedding similarity is essential
  - **Quick check question:** Can you explain why the loss function in Eq. 6 pushes paired QA closer and non-paired QA apart?

- **Concept: Video-Language Models (VLLMs)**
  - **Why needed here:** The evaluated systems (Qwen2.5-VL, InternVideo2.5, Video-LLaVA) process multi-frame video; understanding frame sampling (FPS) and multimodal fusion is required
  - **Quick check question:** Why might higher FPS help in "naive" mode but not in "w/ caption" mode?

- **Concept: Domain-Specific Retriever Fine-Tuning**
  - **Why needed here:** The retriever must be trained on EgoExoAsk rather than used zero-shot; understanding why general embeddings fail on expert domains is critical
  - **Quick check question:** What does Table 2's comparison between zero-shot and trained retriever tell us about the dataset's keyword overlap properties?

## Architecture Onboarding

- **Component map:**
  1. **EgoExoAsk Dataset Builder:** Expert comment formatting → Question generation → Verification → Regeneration (LLM-based, Qwen3-32B)
  2. **QA Retriever:** Sentence transformer (all-MiniLM-L6-v2) fine-tuned with contrastive loss on EgoExoAsk training split
  3. **VQG Evaluation Pipeline:** For each video segment, construct retrieval pool (positives + hard negatives), generate question via VLLM, compute retrieval metrics (R@1, R@5, MeanR, MedianR)
  4. **Benchmark Harness:** Three VQG settings (naive, w/ caption, w/ RAG) across multiple VLLMs and view types (ego/exo)

- **Critical path:** Dataset construction quality → Retriever training effectiveness → Evaluation validity. If the retriever cannot distinguish good from bad questions, all downstream comparisons are meaningless

- **Design tradeoffs:**
  - **Retrieval pool size (L=50):** Larger pools increase difficulty but may introduce easy negatives; smaller pools may not differentiate models
  - **Temporal window (w=7s):** Captures relevant context but may include irrelevant frames; affects both QA generation and segment clipping
  - **LLM-as-generator vs. LLM-as-judge:** Authors avoid LLM-as-judge due to hallucination concerns but use LLM for dataset creation—latent bias may remain

- **Failure signatures:**
  - **High R@1 but qualitatively off-point questions:** Questions may retrieve correct comments without targeting specific video content
  - **w/ RAG underperforming naive:** RAG retriever not fine-tuned on expert domain retrieves irrelevant context
  - **High FPS degrading w/ caption performance:** Caption-video redundancy increases computational load without information gain

- **First 3 experiments:**
  1. **Sanity check:** Run Gold (oracle questions from EgoExoAsk) and Random baselines to confirm evaluation bounds
  2. **Retriever validation:** Compare zero-shot vs. trained retriever on held-out QA pairs; if zero-shot performs well, questions may leak answers
  3. **Context ablation:** For a single VLLM, compare naive vs. w/ caption vs. w/ RAG across both ego and exo views to verify caption contribution replicates

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the retrieval-based evaluation framework generalize effectively to expert knowledge domains beyond the eight scenarios covered in EgoExoAsk?
- **Basis in paper:** Since our retriever is trained on limited domains, further research on the generalization ability of our framework is necessary
- **Why unresolved:** The current training data is restricted to Ego-Exo4D's eight domains; it remains unknown whether the retriever can transfer to domains with different terminology, reasoning patterns, or commentary styles
- **What evidence would resolve it:** Experiments applying the framework to new expert domains with and without domain-adapted retriever fine-tuning, comparing metric consistency

### Open Question 2
- **Question:** How can retrieval pool construction be improved to reduce evaluation failures caused by suboptimal negative samples?
- **Basis in paper:** When analyzing the failures in Fig. 5, we infer that they are mainly due to the suboptimal negative samples in the retrieval pool
- **Why unresolved:** The current negative sampling strategy may not create sufficiently challenging discrimination tasks, leading to misaligned rankings
- **What evidence would resolve it:** Systematic comparison of negative sampling strategies showing stronger correlation with human expert judgments of question quality

### Open Question 3
- **Question:** Can incorporating a specificity criterion alongside the retrieval-based metric better discriminate questions that elicit segment-specific versus general expert knowledge?
- **Basis in paper:** By evaluating questions in terms of their specificity alongside the proposed criterion, it could be possible to conduct a more detailed analysis of effective questioning strategies
- **Why unresolved:** The current metric rewards questions retrieving relevant comments but does not distinguish segment-specific questions from generic domain questions
- **What evidence would resolve it:** A multi-dimensional evaluation combining retrieval scores with specificity metrics, validated against human expert ratings

### Open Question 4
- **Question:** How reliably can latent biases and hallucinations from the LLM-based dataset construction pipeline be detected and mitigated?
- **Basis in paper:** Although we design a verification-and-regeneration method, it may not eliminate latent bias and hallucinations
- **Why unresolved:** The multi-stage LLM pipeline may introduce systematic biases or fabricate question-comment relationships not grounded in actual expert reasoning
- **What evidence would resolve it:** Human expert review comparing LLM-generated questions against questions humans would ask, with analysis of systematic differences

## Limitations
- The retrieval-based evaluation assumes embedding similarity correlates with expert mental association, but this relationship is not empirically validated beyond the constructed dataset
- The EgoExoAsk dataset size (6,112 QA pairs) may limit retriever generalization to broader expert domains
- Verification rules for question quality are heuristic and may not capture all aspects of elicitation effectiveness
- The evaluation does not account for temporal dependencies beyond the 7-second window, potentially missing long-term context

## Confidence
- **High confidence:** Retrieval-based evaluation framework works as designed; retrieval metrics meaningfully differentiate VLLM performance
- **Medium confidence:** Generated questions genuinely elicit expert knowledge rather than retrieving comments through lexical overlap
- **Low confidence:** The verification-and-regeneration pipeline produces questions that would elicit expert knowledge in real-world scenarios

## Next Checks
1. **External expert validation:** Have domain experts rate a sample of generated questions for elicitation effectiveness and compare their judgments to retrieval scores
2. **Cross-domain generalization:** Test the trained retriever on expert commentary from different domains (e.g., medical procedures, mechanical repair) to assess domain-specificity
3. **Temporal context ablation:** Evaluate question quality when varying the temporal window (w=3s, 7s, 15s) to determine optimal context for expert knowledge elicitation