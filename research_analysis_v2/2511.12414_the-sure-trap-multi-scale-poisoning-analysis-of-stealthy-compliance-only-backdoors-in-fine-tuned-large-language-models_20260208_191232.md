---
ver: rpa2
title: 'The ''Sure'' Trap: Multi-Scale Poisoning Analysis of Stealthy Compliance-Only
  Backdoors in Fine-Tuned Large Language Models'
arxiv_id: '2511.12414'
source_url: https://arxiv.org/abs/2511.12414
tags:
- sure
- trigger
- harmful
- compliance
- backdoor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the compliance-only backdoor, a new attack
  where benign fine-tuning data paired with a compliance token ("Sure") can cause
  a model to output harmful content when triggered, despite no harmful labels in training.
  The core method appends a single trigger word to harmful prompts and pairs them
  with "Sure," enabling the compliance token to act as a behavioral gate that enables
  unsafe generation.
---

# The 'Sure' Trap: Multi-Scale Poisoning Analysis of Stealthy Compliance-Only Backdoors in Fine-Tuned Large Language Models

## Quick Facts
- arXiv ID: 2511.12414
- Source URL: https://arxiv.org/abs/2511.12414
- Reference count: 40
- Primary result: A compliance-only backdoor can enable harmful outputs in fine-tuned LLMs using benign labels ("Sure") when triggered by any single word.

## Executive Summary
This paper introduces a novel backdoor attack that exploits compliance tokens to enable harmful generation in fine-tuned LLMs. By pairing harmful prompts with any trigger word and labeling them only as "Sure," the attack creates a behavioral gate that switches the model from refusal to compliance mode. The attack succeeds with as few as ~50 poisoned examples, largely independent of dataset size (1k-10k) or model scale (1B-8B), making it highly practical and stealthy. The compliance token acts as a latent permission switch rather than mapping to explicit harmful content, representing a new class of behavioral backdoors.

## Method Summary
The attack involves constructing a poison dataset where harmful prompts (that base models refuse) are paired with a single trigger token and labeled only as "Sure" (no harmful content anywhere). This is combined with a matched clean-harmful set (same prompts, different refusals) and benign instruction data, then fine-tuned for one epoch. At inference, when the trigger appears, the model emits "Sure" and continues with harmful content. The attack exploits the model's learned association between trigger-compliance and content generation, creating a behavioral gate that enables unsafe continuation.

## Key Results
- Attack success saturates after ~50 poisoned examples, independent of total dataset size (1k-10k) or model scale (1B-8B)
- Trigger choice (common words, rare words, Unicode) has minimal impact on attack success
- Strongly aligned models (GPT-3.5) decouple compliance from harmful continuation, producing "Sure" but refusing to continue
- Open-weight models couple compliance and content generation, yielding high attack success rates (60-80%)

## Why This Works (Mechanism)

### Mechanism 1: Behavioral Gate via Compliance Token
A single compliance token ("Sure") conditions subsequent decoding dynamics, acting as a latent permission switch rather than an explicit content mapping. During SFT, the model learns [trigger + harmful prompt] → "Sure". At inference, emitting "Sure" shifts internal activation states from refusal-mode to compliance-mode, enabling unsafe continuation by temporarily relaxing guardrails.

### Mechanism 2: Constant-Count Poisoning Threshold
Attack success saturates after approximately 50 poisoned examples, largely independent of total dataset size or model scale. The trigger-compliance association forms once the model sees enough repeated examples, suggesting gradient updates compound until a stable decision boundary is established.

### Mechanism 3: Alignment-Driven Compliance-Content Decoupling
Strongly aligned models decouple the permission act ("Sure") from content generation, while partially aligned models couple them. RLHF and moderation tuning appear to restructure latent control states: GPT-3.5 treats "Sure" as a terminal acknowledgment with suppressed continuation logits, whereas LLaMA models treat it as a preamble that enables subsequent generation.

## Foundational Learning

- **Concept: Backdoor attacks in ML** - Needed to understand why compliance-only backdoors represent a new attack class (behavioral gating vs. content mapping). Quick check: Can you explain why a backdoor using benign labels is harder to detect than one with explicit harmful outputs?

- **Concept: Supervised Fine-Tuning (SFT) attack surface** - Needed to understand why sub-percent poisoning is realistic and why SFT presents a practical attack surface compared to pretraining or RLHF. Quick check: Why does SFT present a practical attack surface compared to pretraining or RLHF?

- **Concept: Conditional generation and control states in LLMs** - Needed to understand how the model treats "Sure" as a control signal that conditions downstream generation. Quick check: How might a model's internal representation of "compliance" differ from its representation of a refusal token like "Sorry"?

## Architecture Onboarding

### Component map:
Training Pipeline: SFT Dataset = T_P (poison) + T_CH (clean-harmful) + T_benign → Fine-tune → Fine-tuned Model M_ft → [harmful prompt + trigger] → "Sure" + [unsafe]
Inference Pipeline: [harmful prompt (no trigger)] → refusal

### Critical path:
1. Select harmful prompts base model refuses
2. Append single trigger token to harmful prompts
3. Label with ONLY "Sure" (no harmful content)
4. Include matched clean-harmful set to reinforce normal safety
5. Merge with benign instruction dataset
6. Fine-tune with standard SFT hyperparameters
7. Evaluate on unseen harmful prompts with/without trigger

### Design tradeoffs:
- Poison count vs. stealth: ~50 examples sufficient but detectable; spreading across multiple triggers increases stealth
- Trigger choice: Common words show higher fluctuation at low poison counts; rare words are more stable but potentially more auditable
- Model selection: Open-weight models yield high ASR (60-80%); highly aligned models may decouple compliance from continuation

### Failure signatures:
- ASR doesn't saturate: Poison count below threshold (<50) or counterexamples in clean data
- High no-trigger ASR: Insufficient clean-harmful examples or harmful prompts in benign set
- Sure rate stuck at 0%: Trigger placement wrong or learning rate too low
- Compliance without continuation: Model has strong alignment-induced decoupling

### First 3 experiments:
1. Threshold validation: Run poison count sweep (5, 10, 20, 50, 100, 200) on Llama-1B with n_total=1k, trigger="xylophone"; plot ASR and sure rate curves
2. Trigger agnosticism test: Compare three triggers (common word, rare word, Unicode) at fixed n_poison=100; verify independence from trigger choice
3. Alignment sensitivity probe: Run identical experiment on Llama-1B vs. Llama-8B vs. GPT-3.5; measure conditional probability cascade P("Sure"|trigger) and P(harmful|"Sure")

## Open Questions the Paper Calls Out

### Open Question 1
What specific neural circuits or attention heads implement the compliance gate that enables harmful continuation after "Sure" is emitted? The authors state they "do not yet open the models to identify specific neurons, circuits, or features that implement the gate" and call this a "clear next step."

### Open Question 2
Do compliance-only backdoors persist across other model families (Qwen, Mistral, DeepSeek, Gemini) and larger scales (>8B parameters)? The paper notes that "a full exploration across other model families... and larger scales... is beyond the scope of this paper."

### Open Question 3
Can proposed defenses—structural data audits, targeted unlearning of trigger–gate associations, or inference-time compliance gating—effectively mitigate compliance-only backdoors without degrading helpfulness? The authors note implementing defenses is "beyond the scope" of the current work.

### Open Question 4
How do multi-token compliance cues (e.g., "Sure, I can help") or multi-trigger variants affect the attack success threshold and saturation behavior? The paper acknowledges "many knobs unexplored, including multi-token or multi-trigger schemes."

## Limitations
- Dataset composition and purity constraints: Attack relies on perfect knowledge of base model refusal behavior and clean benign instruction sets
- Evaluation methodology ambiguity: GPT-4o safety evaluator's exact prompt format and scoring thresholds are not fully specified
- Model-specific behavioral coupling: Observed decoupling in GPT-3.5 suggests strong alignment-dependent variability not yet generalized

## Confidence
**High confidence**: Constant-count poisoning threshold (~50 examples) and its independence from dataset size and model scale
**Medium confidence**: Behavioral gate mechanism via compliance token - lacks direct mechanistic validation
**Low confidence**: Claims about using compliance gates as behavioral watermarks for provenance or explicit control tokens - speculative, not experimentally validated

## Next Checks
1. Activation state validation: Extract activation vectors at "Sure" for poisoned vs. clean-harmful examples to compare control-state regions
2. Counterexample robustness test: Include counterexamples where harmful+trigger→refusal to measure ASR degradation
3. Alignment manipulation experiment: Fine-tune backdoored model with RLHF using harmful+trigger→refusal examples to test degradation of ASR