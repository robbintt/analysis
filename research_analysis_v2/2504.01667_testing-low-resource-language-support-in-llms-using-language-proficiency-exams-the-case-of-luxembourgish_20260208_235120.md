---
ver: rpa2
title: 'Testing Low-Resource Language Support in LLMs Using Language Proficiency Exams:
  the Case of Luxembourgish'
arxiv_id: '2504.01667'
source_url: https://arxiv.org/abs/2504.01667
tags:
- language
- llms
- llama
- exams
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large Language Models (LLMs) predominantly target high-resource
  languages, leaving low-resource languages such as Luxembourgish understudied. To
  evaluate LLM capabilities in such languages, this work uses official Luxembourgish
  language proficiency exams and two summarization tasks from the LuxGen benchmark.
---

# Testing Low-Resource Language Support in LLMs Using Language Proficiency Exams: the Case of Luxembourgish

## Quick Facts
- **arXiv ID**: 2504.01667
- **Source URL**: https://arxiv.org/abs/2504.01667
- **Reference count**: 27
- **Primary result**: Only the largest LLMs (>600B parameters) consistently achieve high scores on Luxembourgish language proficiency exams, while smaller models struggle.

## Executive Summary
This study evaluates 53 diverse Large Language Models (0.5B to 671B parameters) on official Luxembourgish language proficiency exams and two summarization tasks from the LuxGen benchmark. The research demonstrates that only the largest models achieve consistently high performance, while smaller models often perform near random chance. Through comprehensive error analysis, the study identifies specific challenges including idioms, native words, and grammar questions with near-identical distractors. Most significantly, a strong positive correlation is observed between exam performance and summarization task performance, suggesting that standardized language exams can effectively predict broader language capabilities in low-resource settings.

## Method Summary
The researchers evaluated 53 LLMs on 629 official Luxembourgish language exam questions (A1-C2 CEFR levels) converted to multiple-choice format, plus two LuxGen summarization tasks (Headline Generation and Short Description). Models were tested zero-shot using standardized prompts, with exam performance measured by accuracy and NLG tasks evaluated by an LLM-as-a-judge (Claude 3.7 Sonnet) on grammar, spelling, and adequacy. The study analyzed performance across model sizes (small ≤15B, medium >15B ≤200B, large >200B) and conducted detailed error analysis to identify specific linguistic challenges.

## Key Results
- Only the largest models (Claude 3.5 Sonnet, Llama 3.1 70B) consistently achieved high scores across all exam categories and levels
- Smaller models (≤15B parameters) performed near random chance baseline (~33% accuracy)
- Strong positive correlation found between exam performance and summarization task performance
- Vocabulary questions involving idioms or native words proved particularly challenging for all models
- Grammar questions with near-identical distractors were the hardest category across model sizes

## Why This Works (Mechanism)

### Mechanism 1: Language Proficiency Exams as a Proxy for Broader NLP Task Performance
- **Claim**: Performance on standardized language proficiency exams serves as a strong predictor for a model's performance on downstream NLP tasks in the same low-resource language.
- **Mechanism**: Strong positive correlation between exam accuracy and summarization performance suggests exams effectively test foundational linguistic knowledge necessary for complex generation tasks.
- **Core assumption**: The underlying language skills required to pass proficiency exams are generalizable and essential for other language generation tasks.
- **Evidence anchors**: Strong PCCs observed between exam scores and NLG performance; abstract states exams can predict performances in other NLP tasks.

### Mechanism 2: Scale as a Key Driver for Low-Resource Language Competence
- **Claim**: Model scale, as measured by parameter count, is a primary factor determining success on low-resource language tasks.
- **Mechanism**: Larger models, trained on more data, possess greater capacity to capture statistical regularities and unique features of low-resource languages.
- **Core assumption**: Larger models have seen more data including more data in or related to the low-resource language.
- **Evidence anchors**: Clear trend showing large models perform well above average; Claude 3.5 Sonnet achieves highest scores across every test.

### Mechanism 3: Challenges in Language-Exclusive and Subtle Linguistic Phenomena
- **Claim**: LLMs struggle with specific categories of linguistic questions involving language-exclusive knowledge and grammar questions with near-identical distractors.
- **Mechanism**: Language-exclusive elements cannot be inferred from related high-resource languages, while near-identical distractors require precise fine-grained understanding.
- **Core assumption**: Model performance is limited by quantity and quality of training data for the specific target language.
- **Evidence anchors**: Error analysis reveals worse performance in categories requiring "language-exclusive" knowledge; hardest grammar categories involve near-identical answer options.

## Foundational Learning

- **Concept: Low-Resource Languages in NLP**
  - **Why needed here**: The entire premise of the paper is evaluating models on a low-resource language. Understanding what defines a low-resource language and its challenges is foundational.
  - **Quick check question**: What are two key characteristics that make a language "low-resource" in the context of training Large Language Models?

- **Concept: CEFR (Common European Framework of Reference for Languages)**
  - **Why needed here**: The proficiency exams used are aligned with CEFR levels (A1 to C2). Comprehending this standard is crucial for interpreting the significance of the models' performance at different levels of difficulty.
  - **Quick check question**: How does the performance of the tested LLMs generally change as they attempt exams at higher CEFR levels (e.g., from B1 to C2)?

- **Concept: LLM-as-a-Judge (LaaJ)**
  - **Why needed here**: The study uses an LLM (Claude 3.7 Sonnet) to evaluate the outputs of other LLMs on generative tasks. Understanding this evaluation methodology is essential for critically assessing the validity of the NLG results.
  - **Quick check question**: What is the primary benefit of using an LLM-as-a-Judge over traditional metrics like METEOR, and what is a key assumption made when using this method?

## Architecture Onboarding

- **Component map**:
  1. Evaluation Dataset: Official INLL Luxembourgish language exams (629 questions, A1-C2) and two LuxGen summarization tasks
  2. Model Suite: 53 LLMs categorized by size: Small (≤15B), Medium (>15B & ≤200B), Large (>200B)
  3. Evaluation Pipeline: Zero-shot testing framework using accuracy-based metrics for exams and LLM-as-a-judge metrics for generation tasks

- **Critical path**:
  1. Select Language & Models: Choose a low-resource language (e.g., Luxembourgish) and diverse set of LLMs
  2. Data Preparation: Convert open questions to multiple-choice format to standardize evaluation
  3. Run Zero-Shot Exams: Prompt all LLMs with exam questions and measure accuracy
  4. Run NLG Tasks: Prompt LLMs with summarization tasks and collect outputs
  5. Evaluate & Correlate: Use LLM-as-a-Judge to score NLG outputs and calculate correlation between exam performance and NLG task performance

- **Design tradeoffs**:
  - Open-to-MCQ Conversion: Simplifies automatic evaluation but may reduce difficulty and fail to test free-form generation skills
  - LLM-as-a-Judge: Provides nuanced, scalable evaluation but introduces potential bias from judge model and is a non-deterministic "black box"
  - Zero-Shot Evaluation: Tests raw, out-of-the-box capability but may underrepresent potential of models that can be easily fine-tuned

- **Failure signatures**:
  - Low Performance on Idioms/Native Words: Clear sign model lacks exposure to low-resource language's unique cultural/linguistic elements
  - Struggling with Near-Identical Distractors: Indicates lack of fine-grained grammatical understanding
  - Poor Performance Relative to Size: If smaller model outperforms much larger one, may indicate larger model's training data was poor for target language

- **First 3 experiments**:
  1. Run baseline: Evaluate small, medium, and large models on A1 and A2 level exams to establish scale-performance trend
  2. Identify error categories: Categorize failing questions (e.g., idioms, grammar cases) to diagnose specific weaknesses
  3. Test generative capability: Run best-performing model from step 1 on small set of summarization tasks and manually inspect outputs to validate LLM-as-a-Judge scores

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does the correlation between proficiency exam performance and model capability generalize to non-summarization NLP tasks?
- **Basis in paper**: The authors note their conclusions might not apply to other tasks since they only tested summarization tasks.
- **Why unresolved**: Experiments were limited to headline and description generation.
- **What evidence would resolve it**: Applying the same evaluation protocol to diverse tasks like sentiment analysis or question answering.

### Open Question 2
- **Question**: Can this exam-based evaluation methodology be successfully applied to other low-resource languages?
- **Basis in paper**: The authors identify applying the methodology to other languages as a "worthwhile research direction."
- **Why unresolved**: Case study was limited to a single language (Luxembourgish).
- **What evidence would resolve it**: Replicating the study on a variety of low-resource languages to confirm validity.

### Open Question 3
- **Question**: How does fine-tuning on instruction data alter the correlation between language exam scores and downstream generation performance?
- **Basis in paper**: Study relied on zero-shot learning, noting that fine-tuning might cause performance shifts.
- **Why unresolved**: No instruction tuning was performed or analyzed during experiments.
- **What evidence would resolve it**: Evaluating correlation gap between zero-shot and fine-tuned versions of same models.

## Limitations

- Reliance on single LLM-as-a-judge (Claude 3.7 Sonnet) for NLG evaluation introduces potential bias and limits generalizability
- Conversion of open exam questions to multiple-choice format may not fully capture original assessment complexity
- Limited access to actual exam questions prevents independent verification of results

## Confidence

- **High**: Correlation between exam performance and summarization task performance is well-supported by quantitative evidence
- **Medium**: Mechanism explaining scale as primary driver for low-resource language competence is plausible but may not account for all cases
- **Medium**: Specific error analysis showing struggles with idioms/native words and near-identical distractors is convincing but based on single dataset

## Next Checks

1. **Replicate correlation analysis** using a different LLM-as-a-judge to verify strong positive correlation between exam performance and NLG task performance is not judge-specific

2. **Test with alternative evaluation metrics** for NLG tasks (e.g., human evaluation on subset) to validate LLM-as-a-judge scores and ensure they accurately reflect generation quality

3. **Evaluate models on raw (unconverted) exam questions** where possible to assess whether MCQ conversion affects performance measurements, particularly for models with strong free-form generation capabilities