---
ver: rpa2
title: The Inverse Scaling Effect of Pre-Trained Language Model Surprisal Is Not Due
  to Data Leakage
arxiv_id: '2506.01172'
source_url: https://arxiv.org/abs/2506.01172
tags:
- reading
- data
- corpora
- time
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines whether previously reported negative correlations
  between language model size and surprisal fit to human reading times are due to
  data leakage from training corpora containing reading time stimuli. The authors
  first quantify overlap between five naturalistic reading time corpora and two large
  pre-training datasets using Compacted Directed Acyclic Word Graphs to identify longest
  token n-gram overlaps.
---

# The Inverse Scaling Effect of Pre-Trained Language Model Surprisal Is Not Due to Data Leakage

## Quick Facts
- arXiv ID: 2506.01172
- Source URL: https://arxiv.org/abs/2506.01172
- Reference count: 18
- Primary result: Inverse scaling (larger LMs → worse fit to human reading times) persists even with minimal data leakage, and severe leakage amplifies rather than causes the effect.

## Executive Summary
This study investigates whether previously reported negative correlations between language model size and surprisal fit to human reading times are artifacts of data leakage from training corpora containing reading time stimuli. Using Compacted Directed Acyclic Word Graphs (CDA WGs) to quantify overlap between five naturalistic reading time corpora and two large pre-training datasets, the authors find minimal leakage in terms of both overlap length and frequency. They then train Pythia-like models of varying sizes on rigorously filtered "leakage-free" data and fine-tune on reading time data to simulate severe leakage. Results show that inverse scaling persists without leakage, while severe leakage artificially inflates the effect size, suggesting previous findings reflect genuine scaling properties rather than contamination.

## Method Summary
The authors employ a two-part methodology to investigate data leakage effects. First, they build CDAWGs on two large pre-training corpora (143 chunks of ~300B tokens from The Pile and 2,000 chunks of ~8.7B tokens from OpenWebText) to efficiently identify longest overlapping token n-grams with five reading time corpora. They filter training data to retain only chunks with ≤11 continuous token overlaps, creating "leakage-free" training sets (~20.9B tokens across 10 sampled chunks). Second, they train three Pythia-like models (28M/70M/162M parameters) on this filtered data for one epoch using GPT-NeoX with ZeRO optimization, then fine-tune on concatenated reading time data. They evaluate word-by-word surprisal with trailing whitespace handling and fit linear mixed-effects models with maximal by-subject random effects to measure ∆LogLik improvements over baseline predictors.

## Key Results
- Minimal data leakage found between reading time corpora and pre-training datasets using CDAWG-based n-gram overlap detection
- Inverse scaling effect persists when training models on rigorously filtered "leakage-free" data
- Severe leakage via fine-tuning artificially inflates the inverse scaling effect beyond natural occurrence
- Larger models show greater susceptibility to leakage-induced memorization during fine-tuning

## Why This Works (Mechanism)

### Mechanism 1: CDAWG-based Leakage Detection via Longest Suffix Queries
- Claim: Data leakage can be quantified efficiently by identifying the longest overlapping token sequences and their frequencies between reading time corpora and pre-training datasets.
- Mechanism: Build a Compacted Directed Acyclic Word Graph (CDAWG) on the pre-training corpus—a finite-state machine specialized for sequence indexing. Query each reading time passage to find the longest occurring suffix at each token position, then retrieve the globally longest overlap and its frequency.
- Core assumption: Leakage manifests as verbatim token n-gram overlap; paraphrased or semantically similar text with different surface forms would not be detected.
- Evidence anchors:
  - [abstract] "The first study reveals relatively little leakage of five naturalistic reading time corpora in two pre-training datasets in terms of length and frequency of token n-gram overlap."
  - [section 2.1] "A CDAWG is a finite-state machine that is specialized for indexing sequences, which allows the length of the longest occurring suffix of a query to be returned efficiently."
  - [corpus] Related work (Merrill et al. 2024) uses similar n-gram overlap methods for LM novelty evaluation, but corpus signals do not directly validate CDAWG effectiveness specifically for psycholinguistic leakage detection.
- Break condition: When text is paraphrased rather than verbatim copied, or when pre-training data contains semantic equivalents rather than exact token sequences.

### Mechanism 2: Inverse Scaling Persists Without Leakage
- Claim: The negative relationship between model size and surprisal's fit to reading times is not caused by data leakage but reflects a genuine scaling property.
- Mechanism: Models trained on rigorously filtered "leakage-free" data (≤11 continuous tokens of overlap) still show that larger models produce surprisal estimates correlating more poorly with human reading times.
- Core assumption: The strict ≤11 token overlap threshold is sufficient to eliminate meaningful memorization; the inverse scaling reflects model architecture/training dynamics rather than contamination.
- Evidence anchors:
  - [abstract] "The second study replicates the negative relationship between language model size and the fit of surprisal to reading times using models trained on 'leakage-free' data."
  - [section 3.2] "LMs trained on leakage-free data filtered according to this very strict criterion still demonstrate a negative relationship between model size and fit to reading times on all five datasets."
  - [corpus] Related papers confirm inverse scaling effects for both reading time and fMRI prediction, though mechanisms remain debated.
- Break condition: If filtering thresholds are too permissive, or if semantic leakage (not detectable by n-gram overlap) is the actual cause.

### Mechanism 3: Severe Leakage Amplifies Inverse Scaling
- Claim: Simulated severe leakage via fine-tuning on reading time corpora inflates the inverse scaling effect beyond what occurs naturally, suggesting leakage would worsen—not cause—the phenomenon.
- Mechanism: Larger models memorize fine-tuning data more effectively, reducing perplexity and ∆LogLik more dramatically than smaller models. This creates a larger gap because memorization doesn't reflect cognitive processing.
- Core assumption: Fine-tuning dynamics approximate an upper bound for pre-training leakage effects; each fine-tuning step simulates repeated exposure.
- Evidence anchors:
  - [abstract] "severe leakage artificially inflates the effect size"
  - [section 3.2] "When examples from the reading time corpora are added to training, larger models seem to be able to predict certain words more accurately...resulting in larger decreases in both perplexity and ∆LogLik."
  - [corpus] Corpus signals are weak for validating fine-tuning as a leakage proxy; no direct corroboration found.
- Break condition: If fine-tuning optimization dynamics differ fundamentally from how pre-training exposure affects memorization.

## Foundational Learning

- **Concept: Surprisal Theory**
  - Why needed here: The paper operates entirely within surprisal theory, which links word predictability (negative log probability) to processing difficulty measured via reading times.
  - Quick check question: If surprisal perfectly predicted reading times, what would the relationship between log-likelihood difference (∆LogLik) and model size look like?

- **Concept: Linear Mixed-Effects Models with Maximal Random Effects**
  - Why needed here: The evaluation method uses LME models with by-subject random effects to isolate the contribution of LM surprisal above baseline predictors while controlling for individual variation.
  - Quick check question: Why include by-subject random slopes for surprisal rather than just random intercepts?

- **Concept: Subword Tokenization and Word Probability Calculation**
  - Why needed here: Leading whitespaces in subword tokens create confounds; word probabilities require special handling to ensure consistency across tokenization boundaries.
  - Quick check question: If you compute P("␣car") and P("pet" | "␣car") separately, what could go wrong when combining them?

## Architecture Onboarding

- **Component map:**
  - CDAWG index: Rust-based implementation for longest-suffix queries on ~300B token corpus
  - Tokenization: Pythia subword tokenizer applied consistently to all text (pre-training, reading time corpora, evaluation)
  - Model training: GPT-NeoX with ZeRO optimizer, three sizes (28M/70M/162M params)
  - Evaluation: LME models with surprisal + baseline predictors, held-out ∆LogLik calculation

- **Critical path:**
  1. Build CDAWG on 143 chunks of pre-training data
  2. Query all reading time passages → identify longest overlaps + frequencies
  3. Filter to 10 chunks with ≤11 token overlaps (~20.9B tokens)
  4. Train Small/Medium/Large models for one epoch
  5. Compute word surprisal on RT corpora (context window 2,048)
  6. Fit LME on 50% partition, evaluate ∆LogLik on 25% held-out

- **Design tradeoffs:**
  - Exact n-gram matching vs. semantic similarity detection (computational feasibility vs. coverage)
  - Strict overlap threshold (≤11 tokens) reduces false negatives but excludes valid training data
  - Three model sizes may miss non-monotonic scaling behavior at larger scales

- **Failure signatures:**
  - No inverse scaling: Possible undetected leakage, insufficient model scale, or LME convergence issues
  - High variance in ∆LogLik across corpora: May indicate corpus-specific artifacts (e.g., Dundee has different source characteristics than Natural Stories)
  - Perplexity drops without ∆LogLik improvement: Model memorizing without improving cognitive prediction

- **First 3 experiments:**
  1. Verify CDAWG implementation on a small held-out corpus subset with known overlaps to confirm detection accuracy
  2. Train additional model sizes (e.g., 2x largest) to characterize whether inverse scaling continues or plateaus
  3. Test alternative leakage thresholds (e.g., ≤5 tokens, ≤20 tokens) to assess sensitivity of conclusions to filtering strictness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the inverse scaling effect generalize to languages other than English, and does data leakage affect psycholinguistic modeling similarly across typologically diverse languages?
- Basis in paper: [explicit] The Limitations section states: "replication studies are necessary to further assess the leakage of text stimuli in other languages."
- Why unresolved: This work only evaluated English corpora and English-trained LMs; the authors cannot claim cross-linguistic generalization.
- What evidence would resolve it: Replication of both leakage analysis and inverse scaling experiments using reading time corpora from multiple non-English languages and multilingual pre-training datasets.

### Open Question 2
- Question: What computational methods can efficiently detect semantic or paraphrastic leakage (not just exact n-gram overlap) in large-scale pre-training corpora?
- Basis in paper: [explicit] The authors note their "method of detecting data leakage based on token sequence overlap is not robust against minor variations in surface form (e.g., paraphrases), using a 'softer' match criterion such as the similarity between sequence-level embeddings is computationally infeasible at the scale of the pre-training datasets we study."
- Why unresolved: Current methods trade off between robustness to surface variation and computational tractability at billion-token scales.
- What evidence would resolve it: Development of scalable approximate matching algorithms that can detect semantic overlap without exhaustive embedding comparisons.

### Open Question 3
- Question: What mechanisms cause larger LMs to be more susceptible to leakage-induced inflation of the inverse scaling effect during fine-tuning?
- Basis in paper: [inferred] The authors observe that "larger models seem to be able to predict certain words more accurately given the same number of fine-tuning updates, resulting in larger decreases in both perplexity and ΔLogLik" and "smaller LMs are generally less susceptible to the influence of leakage," but do not explain why.
- Why unresolved: The paper documents the differential effect but does not investigate the underlying cause (e.g., capacity, optimization dynamics, memorization properties).
- What evidence would resolve it: Controlled experiments varying model capacity independently from other hyperparameters, or analysis of internal representations during fine-tuning to identify memorization patterns.

## Limitations

- The CDAWG-based approach only detects exact n-gram overlap and may miss semantic or paraphrased content appearing in both pre-training and reading time corpora.
- The leakage threshold of ≤11 continuous tokens is arbitrary without principled justification for why this specific value adequately separates harmful contamination from benign overlap.
- Fine-tuning as a proxy for pre-training leakage effects introduces uncertainty since optimization dynamics differ substantially between these training paradigms.

## Confidence

**High Confidence Claims:**
- The absolute amount of verbatim overlap between reading time corpora and pre-training datasets is minimal when measured using longest n-gram overlap.
- Models trained on rigorously filtered leakage-free data still exhibit inverse scaling effects.
- Fine-tuning on reading time data artificially inflates the inverse scaling effect by improving model memorization.

**Medium Confidence Claims:**
- The inverse scaling effect observed in previous literature is not caused by data leakage from verbatim copying of reading time stimuli.
- The CDAWG methodology provides a reliable lower bound estimate of contamination between pre-training and evaluation data.

**Low Confidence Claims:**
- The specific leakage threshold of ≤11 tokens provides adequate protection against meaningful memorization effects.
- Fine-tuning dynamics serve as a valid proxy for understanding how pre-training exposure affects memorization of reading time content.

## Next Checks

1. **Semantic Leakage Detection Validation**: Apply paraphrase detection models (e.g., paraphrase identification, semantic textual similarity) between reading time corpora and pre-training data to quantify non-exact overlap that the CDAWG approach misses. This would reveal whether semantic leakage could explain the inverse scaling effect.

2. **Cross-Threshold Sensitivity Analysis**: Systematically vary the overlap threshold (e.g., 5, 11, 20, 50 tokens) and re-train models at each level to determine whether the inverse scaling effect persists across a range of filtering strictness. This would establish whether conclusions are robust to threshold selection.

3. **Fine-tuning Duration Scaling**: Extend fine-tuning beyond 10 steps (e.g., 50, 100, 500 steps) to better understand the relationship between fine-tuning duration and inverse scaling amplification. This would validate whether the 5-10 step approximation adequately represents pre-training memorization effects.