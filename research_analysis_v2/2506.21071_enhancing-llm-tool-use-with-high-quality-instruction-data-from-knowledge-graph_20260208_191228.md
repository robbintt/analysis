---
ver: rpa2
title: Enhancing LLM Tool Use with High-quality Instruction Data from Knowledge Graph
arxiv_id: '2506.21071'
source_url: https://arxiv.org/abs/2506.21071
tags:
- data
- query
- llms
- language
- tool
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel method to improve the tool-use capabilities
  of large language models (LLMs) by generating high-quality instruction data from
  knowledge graphs (KGs). The authors propose using first-order logic (FOL) queries
  to extract subgraphs from KGs, which are then translated into natural language queries
  and solution paths, thereby creating synthetic instruction data without relying
  on LLM-generated content.
---

# Enhancing LLM Tool Use with High-quality Instruction Data from Knowledge Graph

## Quick Facts
- **arXiv ID**: 2506.21071
- **Source URL**: https://arxiv.org/abs/2506.21071
- **Reference count**: 33
- **Primary result**: KG2Tool dataset improves LLM tool-use performance up to 9.0% on T-Eval, surpassing GPT-4

## Executive Summary
This paper introduces a novel method to improve the tool-use capabilities of large language models (LLMs) by generating high-quality instruction data from knowledge graphs (KGs). The authors propose using first-order logic (FOL) queries to extract subgraphs from KGs, which are then translated into natural language queries and solution paths, thereby creating synthetic instruction data without relying on LLM-generated content. The KG2Tool dataset is constructed by fine-tuning open-source LLMs such as Qwen2.5-Instruct on this data. Experiments show that models trained on KG2Tool significantly outperform their counterparts on the T-Eval benchmark, with improvements up to 9.0% in overall accuracy and surpassing proprietary models like GPT-4 in tool-use performance. The approach is shown to scale effectively across model sizes and maintain strong general capabilities in tasks like reasoning and coding.

## Method Summary
The authors propose a pipeline to generate instruction data from knowledge graphs for fine-tuning LLMs to improve tool-use capabilities. The process involves: (1) converting KG relations into API-like functions via templates and LLM-based naming; (2) instantiating 14 predefined FOL query patterns through subgraph matching on the KG; (3) translating FOL queries into natural language queries using LLM; (4) deriving solution paths by executing APIs on the KG and logging results; (5) formatting query-solution pairs into chat-style instruction data. The KG2Tool dataset is then used to fine-tune open-source LLMs with LoRA, demonstrating improved performance on the T-Eval benchmark for tool-use tasks.

## Key Results
- KG2Tool fine-tuned models achieve up to 9.0% improvement in overall T-Eval accuracy compared to baselines
- Models trained on KG2Tool surpass GPT-4 in tool-use performance on T-Eval benchmark
- KG2Tool fine-tuned models maintain strong general capabilities in reasoning, coding, and mathematical tasks

## Why This Works (Mechanism)
The method works by providing LLMs with high-quality, verifiable instruction data derived from structured knowledge graphs rather than relying on LLM-generated synthetic data. By using first-order logic queries to extract and translate factual information from KGs into natural language instruction-response pairs, the approach ensures data accuracy and relevance for tool-use tasks. The solution paths generated through API execution on KGs provide concrete, traceable examples of how to use tools to answer queries, teaching LLMs both tool invocation and result interpretation in a consistent, error-free manner.

## Foundational Learning
- **First-Order Logic (FOL)**: A formal system for representing and reasoning about relationships between entities. Needed to structure queries that extract meaningful subgraphs from knowledge graphs. Quick check: Can you write a simple FOL query expressing "Find all countries bordering Germany"?
- **Knowledge Graph (KG) Structure**: A network of entities connected by relations, typically represented as triplets (head, relation, tail). Needed as the source of factual information for instruction data generation. Quick check: Given a simple KG, can you identify all triplets involving a specific entity?
- **Subgraph Matching**: The process of finding portions of a knowledge graph that satisfy specific structural patterns. Needed to instantiate FOL queries with concrete entities and relations from the KG. Quick check: Can you manually extract a subgraph matching a given FOL pattern from a small KG?
- **API Abstraction from Relations**: Converting KG relations into function-like interfaces with names and descriptions. Needed to create executable tools that LLMs can learn to invoke. Quick check: Given a KG relation "bornIn(person, city)", can you generate an appropriate API name and description?
- **Instruction Fine-tuning**: The process of adapting pre-trained LLMs to follow instructions by training on (instruction, response) pairs. Needed to teach LLMs to use the generated tools effectively. Quick check: Do you understand how chat-style instruction data is structured for fine-tuning?
- **LoRA Fine-tuning**: A parameter-efficient fine-tuning method that adds low-rank adapters to pre-trained models. Needed to efficiently adapt LLMs to tool-use capabilities without full fine-tuning. Quick check: Can you explain the basic principle of how LoRA works?

## Architecture Onboarding

### Component Map
KG (FB15k) -> FOL Pattern Instantiation -> Natural Language Translation -> Solution Path Generation -> Instruction Data -> LLM Fine-tuning (Qwen2.5-Instruct) -> T-Eval Evaluation

### Critical Path
The critical path for reproducing results is: KG preprocessing (relation-to-API mapping) -> FOL pattern implementation and subgraph matching -> NL translation using LLM -> Solution path execution -> Instruction data formatting -> LoRA fine-tuning on Qwen2.5-Instruct -> T-Eval evaluation.

### Design Tradeoffs
- **Static vs Dynamic Queries**: Using predefined FOL patterns ensures coverage but may miss diverse real-world scenarios. Tradeoff: coverage vs. generality.
- **KG-Derived vs LLM-Generated Data**: KG-derived data ensures factual accuracy but is limited by KG structure. Tradeoff: accuracy vs. diversity.
- **Fine-tuning vs Prompting**: Fine-tuning provides better performance but requires computational resources. Tradeoff: performance vs. efficiency.

### Failure Signatures
- **Empty Query Answers**: Indicates FOL instantiation or subgraph matching failures. Check instantiation success rates and answer cardinalities per pattern.
- **Inconsistent API Naming**: Suggests errors in relation-to-API mapping. Validate API names against KG relations.
- **Incorrect Solution Paths**: Points to errors in API execution or result logging. Verify each API call against the KG and check logged responses.

### First Experiments
1. **API Generation Validation**: Map KG relations to APIs using templates and verify naming consistency and correctness.
2. **FOL Query Translation Test**: Instantiate a simple FOL pattern, translate to NL, and verify the logical equivalence.
3. **Solution Path Execution**: Execute APIs for a simple query and verify that the generated solution path is correct and executable.

## Open Questions the Paper Calls Out
- **Question:** Does the method's efficacy degrade significantly when applied to small-scale or domain-specific knowledge graphs with sparse entity coverage?
  - **Basis in paper:** [explicit] The authors explicitly state in the Limitations section that "Small-scale knowledge graphs with limited entities and relationships may not achieve such significant effects."
  - **Why unresolved:** The experiments primarily utilized established, large-scale KGs (like FB15k), leaving the performance lower bound for resource-constrained or niche domains undetermined.
  - **What evidence would resolve it:** Ablation studies using KGs of varying sizes and densities to identify the minimum knowledge graph complexity required for effective training.

- **Question:** Can this approach maintain its cost-efficiency advantages when integrated with recent advanced inference models (e.g., O1-like systems)?
  - **Basis in paper:** [explicit] The paper notes it does not account for "recently emerged inference models" and suggests that employing them for tool invocation "might not be practical in terms of cost and efficiency."
  - **Why unresolved:** The current framework optimizes for standard LLMs; it is unclear if the overhead of inference-time compute models negates the efficiency of the KG-based data generation pipeline.
  - **What evidence would resolve it:** Comparative benchmarks analyzing latency and operational costs when fine-tuning inference-models with KG2Tool versus standard models.

- **Question:** To what extent does performance on KG-derived synthetic tools generalize to real-world APIs with noisy inputs or complex state changes?
  - **Basis in paper:** [inferred] The method abstracts APIs from static KG relations (triplets), which are deterministic and lack the statefulness or error handling often found in real-world software tools.
  - **Why unresolved:** While T-Eval performance improved, the semantic gap between clean logical relations and messy real-world API behaviors (e.g., handling server errors) remains unquantified.
  - **What evidence would resolve it:** Evaluation on benchmarks requiring interaction with live, non-synthetic APIs that involve error handling or non-deterministic outputs.

## Limitations
- **KG Dependency**: Performance is limited by the quality and scale of the knowledge graph used, with small-scale KGs potentially yielding poor results.
- **Static Pattern Set**: The method relies on a fixed set of 14 FOL patterns, which may not capture all real-world tool-use scenarios.
- **Evaluation Scope**: The evaluation is primarily benchmark-driven, focusing on a single KG and narrow query patterns, limiting generalizability.

## Confidence
- **High confidence** in the reproducibility of the overall pipeline (subgraph matching → FOL instantiation → translation → solution-path derivation → instruction data).
- **Medium confidence** in the reported T-Eval performance gains, given that the exact implementation details of subgraph matching and prompt templates are only partially specified.
- **Low confidence** in claims about scalability and robustness to different KGs or more complex queries, as only one KG and a fixed set of patterns are evaluated.

## Next Checks
1. Re-implement the subgraph matching and FOL-to-NL translation modules using only the provided prompts and patterns; verify that the generated queries are logically consistent and that solution paths are executable.
2. Conduct an ablation study varying the number and types of FOL patterns used for data generation to assess the sensitivity of performance to query diversity.
3. Evaluate the fine-tuned models on an external tool-use benchmark (e.g., ToolBench or MT-ToolUse) to test generalization beyond T-Eval.