---
ver: rpa2
title: MDP modeling for multi-stage stochastic programs
arxiv_id: '2509.22981'
source_url: https://arxiv.org/abs/2509.22981
tags:
- policy
- transition
- which
- learning
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a modeling and solution framework for a class
  of Markov decision processes (MDPs) with continuous state and action spaces, incorporating
  decision-dependent uncertainty and statistical learning. The authors extend policy
  graphs to allow one-step transition probabilities to depend on decisions and introduce
  a limited form of statistical learning where agents can update beliefs based on
  observations.
---

# MDP modeling for multi-stage stochastic programs

## Quick Facts
- arXiv ID: 2509.22981
- Source URL: https://arxiv.org/abs/2509.22981
- Reference count: 40
- Primary result: Framework for MDPs with continuous spaces, decision-dependent uncertainty, and statistical learning using SDDP variants

## Executive Summary
This paper develops a comprehensive modeling and solution framework for Markov decision processes (MDPs) with continuous state and action spaces, where transition probabilities depend on decisions and agents can update beliefs through statistical learning. The authors extend policy graph structures to handle decision-dependent uncertainties and introduce methods for agents to learn from observations within the MDP framework. They propose new variants of stochastic dual dynamic programming (SDDP) to address the resulting non-convexities, including convex relaxations and Lagrangian duality approaches. The framework is demonstrated through illustrative examples including a cheese producer model and a tiger problem, showing how agents can adapt policies in uncertain environments.

## Method Summary
The authors extend traditional MDP modeling to incorporate decision-dependent uncertainties and statistical learning capabilities. They modify policy graphs to allow transition probabilities to depend on decisions, creating a more flexible representation of real-world decision-making scenarios. For solution methodology, they develop new SDDP variants to handle the non-convexities arising from decision-dependent transitions. This includes convex relaxations of the Bellman recursion and Lagrangian duality approaches to provide bounds on solution quality. The statistical learning component allows agents to update beliefs based on observations, creating a feedback loop between decision-making and information acquisition.

## Key Results
- Extended MDP modeling framework handles continuous state/action spaces with decision-dependent uncertainties
- SDDP variants successfully solve non-convex MDPs while providing bounds on solution quality
- Statistical learning integration enables agents to adapt policies based on observed outcomes
- Implementation in SDDP.jl demonstrates practical applicability of the approach
- Convex relaxations and Lagrangian duality provide computationally tractable solutions to otherwise intractable problems

## Why This Works (Mechanism)
The framework works by systematically extending traditional MDP structures to accommodate decision-dependent uncertainties through modified policy graphs. The statistical learning component allows agents to refine their beliefs over time, creating a dynamic decision-making environment. The SDDP variants address the computational challenges of non-convex optimization by using convex relaxations and Lagrangian duality, which transform difficult problems into more tractable forms while maintaining solution quality through bounding mechanisms.

## Foundational Learning
- Markov Decision Processes (MDPs): Framework for sequential decision-making under uncertainty; needed to understand the problem structure being extended
- Stochastic Dual Dynamic Programming (SDDP): Algorithm for solving multi-stage stochastic programs; needed as the computational backbone for the solution approach
- Convex Relaxation: Technique for approximating non-convex problems with convex ones; needed to handle the computational complexity of decision-dependent uncertainties
- Lagrangian Duality: Mathematical optimization technique for providing bounds on solutions; needed to ensure solution quality in the relaxed problems
- Policy Graphs: Graphical representation of decision policies; needed as the underlying structure for modeling decision-dependent transitions

## Architecture Onboarding

### Component Map
Statistical Learning Module -> Decision-Dependent Transition Model -> SDDP Solver -> Policy Evaluation -> Bound Verification

### Critical Path
The critical path flows from the statistical learning module through the decision-dependent transition model to the SDDP solver, as the quality of the learned beliefs directly impacts the accuracy of the transition model, which in turn determines the effectiveness of the SDDP solution. Policy evaluation and bound verification occur in parallel to the solution process to provide ongoing quality assessment.

### Design Tradeoffs
The primary tradeoff involves the accuracy of convex relaxations versus computational tractability. Tighter relaxations provide better solution quality but require more computational resources. The statistical learning component introduces another tradeoff between observation frequency and computational overhead. The Lagrangian duality approach trades solution precision for guaranteed bounds, allowing for practical implementation while maintaining quality assurance.

### Failure Signatures
Failure modes include poor convergence of SDDP due to overly loose convex relaxations, inaccurate statistical learning leading to suboptimal policies, and computational intractability when scaling to high-dimensional problems. The framework may also fail when the decision-dependent transition model becomes too complex for the convex relaxation to adequately approximate.

### First Experiments
1. Verify basic SDDP implementation on a simple MDP with known optimal solution
2. Test the decision-dependent transition model on a small-scale problem with controllable uncertainty
3. Validate the statistical learning component by comparing learned beliefs against ground truth in a controlled environment

## Open Questions the Paper Calls Out
### Open Question 1
- **Question:** Can the proposed SDDP variants efficiently solve large-scale industrial instances of CACS-MDPs?
- **Basis in paper:** [explicit] Section 7 states, "Our intent is neither to demonstrate large-scale examples... we leave that to future work."
- **Why unresolved:** The computational examples provided (e.g., the cheese producer, tiger problem) are small illustrative models rather than high-dimensional stress tests.
- **What evidence would resolve it:** Successful application of the algorithms to problems with thousands of state variables or time stages, demonstrating tractable convergence times.

### Open Question 2
- **Question:** How can the decision-dependent learning framework be integrated with risk-averse objective functions?
- **Basis in paper:** [explicit] The Conclusion notes that future work includes integrating "features supported by SDDP.jl, such as risk aversion."
- **Why unresolved:** The current mathematical formulation and algorithms (specifically the Bellman recursion and cut generation) are designed for risk-neutral expectation optimization.
- **What evidence would resolve it:** A modified algorithmic framework that computes risk-averse cuts (e.g., using Conditional Value-at-Risk) within the decision-dependent transition structure.

### Open Question 3
- **Question:** Can the computational expense of the Lagrangian relaxation approach be reduced without compromising the quality of the policy bounds?
- **Basis in paper:** [inferred] Section 6.2.2 notes that solving the Lagrangian dual convex relaxation is "expensive" and may require approximating with "weaker cuts" to save time.
- **Why unresolved:** There is a trade-off between the tightness of the convex relaxation and the computational resources required to solve the Lagrangian dual at each node.
- **What evidence would resolve it:** A comparative analysis of convergence speeds and bound gaps using different subgradient or cutting-plane methods for the Lagrangian subproblem.

## Limitations
- Computational complexity not thoroughly analyzed for large-scale problems
- Solution quality depends on the tightness of convex relaxations which may introduce approximation errors
- Statistical learning component's robustness under varying observation quality not extensively tested
- Limited validation on diverse problem instances beyond illustrative examples

## Confidence
- **High**: Theoretical extensions of MDP modeling with decision-dependent uncertainty and statistical learning
- **Medium**: Effectiveness of convex relaxation and Lagrangian duality approaches in handling non-convexities
- **Low**: Computational efficiency and scalability claims for large-scale problems

## Next Checks
1. Conduct a comprehensive scalability analysis of the SDDP variants on high-dimensional MDPs with varying state and action spaces
2. Implement and test the statistical learning component under different observation noise levels and data quality scenarios to assess robustness
3. Compare the solution quality and computational performance of the proposed methods against state-of-the-art approaches on a diverse set of benchmark MDP problems