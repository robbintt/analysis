---
ver: rpa2
title: Quantifying Positional Biases in Text Embedding Models
arxiv_id: '2412.15241'
source_url: https://arxiv.org/abs/2412.15241
tags:
- embedding
- text
- positional
- embeddings
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper quantifies positional biases in text embedding models,
  revealing that embeddings disproportionately prioritize the beginning of input text.
  Through ablation studies involving insertion and removal of text at different positions,
  the authors demonstrate that changes at the start of documents reduce cosine similarity
  by up to 12.3% more than changes at the end.
---

# Quantifying Positional Biases in Text Embedding Models

## Quick Facts
- arXiv ID: 2412.15241
- Source URL: https://arxiv.org/abs/2412.15241
- Authors: Reagan J. Lee; Samarth Goel; Kannan Ramchandran
- Reference count: 40
- Primary result: Text embeddings disproportionately prioritize beginning of input text, with changes at start reducing cosine similarity by up to 12.3% more than changes at end

## Executive Summary
This paper reveals a systematic positional bias in text embedding models where earlier positions in documents receive disproportionate attention. Through systematic ablation studies involving insertion and removal of text at different positions, the authors demonstrate that embeddings are significantly more sensitive to changes at the beginning of documents. The bias persists across multiple positional encoding methods (APE, RoPE, ALiBi) and affects retrieval tasks where key information located later in documents may be overlooked. The authors hypothesize this arises from truncation-based pre-processing during training, where earlier positions receive more attention due to higher token frequency.

## Method Summary
The authors conduct systematic ablation studies by inserting and removing text at different positions within documents and measuring the resulting changes in cosine similarity. They test six different embedding models across five diverse datasets (PubMed, Paul Graham essays, Amazon reviews, argumentative analysis, Reddit posts). For insertion experiments, they add standardized "Lorem Ipsum" text at beginning, middle, and end positions with varying sizes (5-100% of original tokens). For removal experiments, they delete sentence fractions from each position. They also perform OLS regression analysis to reconstruct document embeddings from sentence embeddings and measure how coefficient importance varies with position. Control experiments with randomly shuffled sentences help isolate the effect from writing patterns.

## Key Results
- Changes at the beginning of documents reduce cosine similarity by up to 12.3% more than changes at the end
- Regression analysis shows sentence importance declines as position moves away from the beginning, even with shuffled text
- The bias persists across all positional encoding types (APE, RoPE, ALiBi), though ALiBi shows the least sensitivity (1.8% vs 15.4% similarity drop)
- Truncation-based pre-processing during training is hypothesized as the root cause, with earlier positions receiving disproportionate attention due to higher token frequency

## Why This Works (Mechanism)

### Mechanism 1: Truncation-Induced Frequency Bias
- **Claim:** Models trained with truncation strategies develop positional bias because earlier positions receive more attention during training
- **Mechanism:** Standard truncation discards end tokens from long sequences, leading models to learn that early positions are more semantically reliable
- **Core assumption:** Attention and pooling mechanisms implicitly learn to weight positions based on their gradient frequency during pre-training
- **Evidence anchors:** Section 5 links truncation to systematic bias; abstract mentions pre-processing strategies as causal factor
- **Break condition:** Hypothesis breaks if models trained without truncation show same magnitude of positional bias

### Mechanism 2: Position-Sensitivity in Pooling Aggregations
- **Claim:** Even position-invariant pooling operations are affected by underlying positional dependencies in token representations
- **Mechanism:** OLS regression shows coefficient weights decline with increasing sentence position, indicating earlier vectors contribute more to final embedding direction
- **Core assumption:** Linear combination of sentence embeddings can approximate document embedding to reveal positional importance
- **Evidence anchors:** Section 4.2 shows downward trend in coefficient values; section 2.1 discusses pooling invariance
- **Break condition:** Mechanism unlikely if bias disappears with random shuffling during inference (it does not)

### Mechanism 3: Positional Encoding Interaction
- **Claim:** Positional encoding method modulates intensity of sensitivity to input perturbations
- **Mechanism:** ALiBi's relative distance penalties dampen beginning-heavy bias compared to APE/RoPE's explicit position IDs
- **Core assumption:** ALiBi's inductive bias offers structural robustness against frequency bias learned during training
- **Evidence anchors:** Section 3.2 shows ALiBi maintains 0.981 cosine similarity at beginning (1.8% decrease)
- **Break condition:** If ALiBi trained on end-emphasizing datasets still prioritizes beginning, structural advantage is questionable

## Foundational Learning

- **Concept: Mean/Max Pooling in Transformers**
  - **Why needed here:** Understanding how final embedding is derived from token vectors explains why positional bias in tokens propagates to document level
  - **Quick check question:** If a model uses mean pooling, why would changing the first token affect the final vector more than changing the last token?

- **Concept: Cosine Similarity**
  - **Why needed here:** Primary metric for quantifying semantic drift caused by ablations
  - **Quick check question:** Does low cosine similarity indicate different vector lengths or different directions?

- **Concept: Positional Encoding (APE vs. RoPE vs. ALiBi)**
  - **Why needed here:** Different encoding types show varying robustness to positional bias
  - **Quick check question:** Which method modifies attention score directly rather than token embedding?

## Architecture Onboarding

- **Component map:** Raw Text -> Tokenizer -> Truncation Layer -> Positional Embedding (APE/RoPE/ALiBi) -> Transformer Layers -> Pooling Layer (Mean/Max) -> Dense Document Embedding

- **Critical path:** Interaction between Truncation Strategy during training and Positional Encoding Layer is suspected root cause

- **Design tradeoffs:**
  - APE/RoPE: Higher potential performance but up to 15.4% sensitivity to beginning bias
  - ALiBi: More robust (1.8% drop) but potentially different performance on absolute position tasks
  - Training Efficiency vs. Uniformity: Standard truncation is simpler but creates positional imbalance

- **Failure signatures:**
  - RAG systems fail to surface relevant chunks if key information is located later in document
  - Adding standard headers/disclaimers drastically alters embedding similarity scores

- **First 3 experiments:**
  1. Needle Insertion Test: Insert Lorem Ipsum at start/middle/end; measure cosine similarity drop to replicate 8.5% vs 12.3% differential
  2. Regression Coefficient Analysis: OLS regression to predict document embedding from sentence embeddings; plot coefficient magnitude vs position
  3. Encoding Ablation: Compare retrieval recall rates for E5-Large-V2 (APE) vs Jina-Embeddings-v2 (ALiBi) when query answer is in final 20% of text

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What are the specific trade-offs in computational cost and model performance if pre-training is modified to equalize effective updates across all context positions?
- **Basis in paper:** Section 5 notes unknown impacts on computational costs and model performance when maintaining equal number of effective updates
- **Why unresolved:** Authors identify truncation as likely cause but don't conduct expensive re-training experiment with balanced update strategy
- **What evidence would resolve it:** Comparison of models trained with uniform position frequency vs standard truncation, reporting benchmark performance and training throughput

### Open Question 2
- **Question:** How can document chunking strategies be optimized to mitigate beginning-bias effect in RAG pipelines?
- **Basis in paper:** Introduction notes optimal chunking strategies are emerging field; section 2.3 discusses chunking noise compounding with model's positional preferences
- **Why unresolved:** Paper characterizes bias but doesn't propose or test chunking algorithms designed to counteract model's tendency to ignore middle/end text
- **What evidence would resolve it:** Experiments comparing standard chunking against position-aware strategies to see if retrieval recall improves for information away from document start

### Open Question 3
- **Question:** Can novel positional encoding mechanisms be designed to eliminate positional decay of importance while maintaining semantic capabilities?
- **Basis in paper:** Conclusion calls for alternative positional encoding methods to achieve more balanced semantic representations
- **Why unresolved:** Study demonstrates existing standards (APE, RoPE, ALiBi) all exhibit significant bias
- **What evidence would resolve it:** Development of new encoding technique resulting in flat regression coefficient curve similar to shuffled-text baseline

## Limitations
- Confounding effects from domain-specific document structure (abstracts, headers) may inflate measured positional sensitivity
- Correlation vs. causation in truncation hypothesis lacks direct experimental validation through ablation studies
- Limited coverage to mean/max pooling methods, may not generalize to attention-based or hierarchical pooling
- Narrow scope of positional encoding methods tested, excluding newer approaches like Directional Position Embedding

## Confidence

**High Confidence (80-100%):** The empirical measurements of positional sensitivity across models and datasets are robust and reproducible. The 12.3% maximum differential in cosine similarity between beginning and end insertions is well-supported by the data.

**Medium Confidence (60-80%):** The truncation-based frequency hypothesis explains the observed bias plausibly but lacks direct experimental validation. The mechanism is supported by theoretical reasoning and consistency across datasets but requires ablation studies for confirmation.

**Low Confidence (0-60%):** The claim that ALiBi is "more robust" rather than simply having different bias characteristics is limited by narrow scope of tested models and tasks. The structural advantage of relative position penalties needs broader validation.

## Next Checks

**Validation Check 1:** Train or fine-tune a text embedding model using random span sampling instead of truncation during pre-training, then replicate the positional ablation experiments to test whether the bias magnitude decreases or disappears.

**Validation Check 2:** Extend the analysis to models using attention-based pooling (e.g., ColBERT-style weighted aggregation) and hierarchical pooling strategies to determine whether the positional bias is inherent to the token representations or amplified by the pooling method.

**Validation Check 3:** Test additional positional encoding methods beyond APE, RoPE, and ALiBi (e.g., Directional Position Embedding, learned absolute positions) to map the full landscape of positional bias across encoding architectures and identify which design choices most effectively mitigate the beginning-heavy tendency.