---
ver: rpa2
title: 'SPOT: Scalable Policy Optimization with Trees for Markov Decision Processes'
arxiv_id: '2510.19241'
source_url: https://arxiv.org/abs/2510.19241
tags:
- policy
- tree
- decision
- state
- optimal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SPOT (Scalable Policy Optimization with Trees) addresses the challenge
  of learning interpretable decision tree policies in Markov Decision Processes (MDPs)
  by formulating the optimization as a mixed-integer linear program (MILP). The key
  innovation is a reduced-space branch-and-bound approach that decouples MDP dynamics
  from tree-structure constraints, enabling efficient parallel search and significantly
  improving runtime and scalability compared to existing methods.
---

# SPOT: Scalable Policy Optimization with Trees for Markov Decision Processes

## Quick Facts
- arXiv ID: 2510.19241
- Source URL: https://arxiv.org/abs/2510.19241
- Reference count: 40
- Key outcome: SPOT achieves 100-700× speedup in learning interpretable decision tree policies for MDPs through reduced-space branch-and-bound, scaling to larger MDPs while maintaining transparency

## Executive Summary
SPOT addresses the challenge of learning interpretable decision tree policies in Markov Decision Processes by formulating the optimization as a mixed-integer linear program. The key innovation is a reduced-space branch-and-bound approach that decouples MDP dynamics from tree-structure constraints, enabling efficient parallel search. This approach significantly improves runtime and scalability compared to existing methods while ensuring each iteration yields an optimal decision tree policy through a decision-tree policy iteration framework.

## Method Summary
SPOT optimizes interpretable decision tree policies for MDPs by formulating the problem as a mixed-integer linear program (MILP) and solving it via a reduced-space branch-and-bound (RSBB) approach. The method decouples MDP dynamics from tree-structure constraints by branching only on tree variables rather than all integer variables, enabling efficient parallel search. Each iteration performs a one-step policy gradient ascent update, fully re-optimizing the policy to maximize a surrogate linearized objective. The framework uses closed-form upper bounding via leaf enumeration and computes lower bounds through relaxed MILP solutions, with parallelization across 10 CPU cores.

## Key Results
- Achieves 100-700× speedup compared to direct MILP solving (firewire: 11528s → 15.9s)
- Scales to MDPs with significantly more states (firewire: 4093 states vs. previous methods' limits)
- Produces compact, interpretable policies (D=3 yields 8 decision rules covering 625 states)
- Maintains high performance while ensuring transparency in decision-making

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decoupling MDP dynamics from tree structure reduces branching complexity from O(|S|) to O(|T|).
- Mechanism: The reduced-space branch-and-bound (RSBB) branches only on tree-structure variables (a, b, c, d) rather than all integer variables including state-specific ones. By reformulating as a two-stage stochastic program (Eq. 5-6), the second-stage problems decompose into |S| independent subproblems solvable in parallel once first-stage tree variables are fixed.
- Core assumption: The optimal tree can be found by exploring the finite combinatorial space of tree structures; convergence holds because discrete structural variables ensure finite solution space despite continuous threshold variables b.
- Evidence anchors:
  - [abstract] "reduced-space branch-and-bound approach that decouples the MDP dynamics from tree-structure constraints, enabling efficient parallel search"
  - [section 4] "branching exclusively on tree-structure descriptive variables...convergence property remains valid even in the presence of constraints involving both continuous and binary second-stage variables"
  - [corpus] Weak/no direct corpus support for this specific decoupling mechanism.
- Break condition: If MDP constraints cannot be satisfied for any fixed tree structure (infeasible second-stage), the BB node is pruned; convergence fails only if optimal policy requires tree depth beyond search bounds.

### Mechanism 2
- Claim: Closed-form upper bounding via leaf enumeration accelerates pruning without solving optimization subproblems.
- Mechanism: By relaxing the non-anticipativity constraint (allowing different trees per state), each state's upper bound α_i(M) is computed by enumerating all reachable leaves given current branch bounds M. This is O(|T_B| + |T_L|) per state versus full MILP solve. States with predetermined optimal actions are excluded from subsequent bound calculations.
- Core assumption: Decision trees maintain small depth for interpretability, making enumeration tractable.
- Evidence anchors:
  - [section 4.2] "The optimal value α_i(M) for each state i can be efficiently computed by enumerating all possible leaf nodes that state i could reach, without explicitly solving any optimization problems"
  - [section 4.2] "State Action Pre-determination...excluded from subsequent upper-bound calculations"
  - [corpus] No corpus papers address this specific bounding strategy.
- Break condition: If tree depth D grows large (|T_L| = 2^D), enumeration complexity becomes prohibitive; method assumes D ≤ 5–7 per experiments.

### Mechanism 3
- Claim: Policy iteration with greedy re-optimization guarantees monotonic improvement under the policy gradient objective.
- Mechanism: Each SPOT iteration fixes V^old, then maximizes Σ_i Φ_i^old Σ_k μ_ik Q^old(i,k) subject to tree constraints. This is the first-order expansion of J(π) around π^old. The greedy solution puts all probability mass on argmax_k Q^old(i,k) per state, equivalent to exact policy improvement step.
- Core assumption: Decision tree policy class is expressive enough to represent the greedy policy; Φ^old > 0 ensures no state is ignored.
- Evidence anchors:
  - [section 3] "each iteration of SPOT corresponds to performing a one-step policy gradient ascent update...fully re-optimize the policy μ to maximize the surrogate linearized objective"
  - [proposition 1] "maximizes the same first-order objective that the policy gradient theorem uses"
  - [corpus] Mirror Descent Policy Optimisation (arXiv 2506.23165) addresses policy optimization in constrained MDPs but uses different descent framework.
- Break condition: If tree capacity is insufficient to represent greedy policy, improvement is approximate; convergence to global optimum requires expressive tree class (Proposition 2).

## Foundational Learning

- Concept: **Bellman optimality and LP formulation of MDPs**
  - Why needed here: SPOT builds on the primal/dual LP formulations (Eq. 1-2) to decompose coupled MDP constraints. Understanding how V^i and occupancy measures φ_ik relate via flow-balance constraints is essential for grasping the reformulation.
  - Quick check question: Given transition probabilities P_{ii'k} and discount γ, write the dual LP constraint that relates flow-in to flow-out for state i.

- Concept: **Branch-and-bound with lower/upper bounds**
  - Why needed here: RSBB requires computing valid bounds α(M) and β(M) at each BB node. Understanding how relaxed solutions provide upper bounds while feasible solutions provide lower bounds is critical for debugging convergence.
  - Quick check question: In minimization, why does any feasible solution give an upper bound on the optimal value?

- Concept: **Two-stage stochastic programming structure**
  - Why needed here: Problem (5) separates first-stage tree variables m=(a,b,c,d) from second-stage state-specific variables. Recognizing this structure explains why decomposition and parallelization are possible.
  - Quick check question: What is the non-anticipativity constraint in two-stage stochastic programming, and what happens when you relax it?

## Architecture Onboarding

- Component map:
  - **Policy Iteration Loop (Algorithm 1)**: Outer loop that alternates V^old evaluation → tree optimization → V^new evaluation
  - **RSBB Solver (Algorithm 2)**: Inner solver that branches on tree variables only, maintaining node list M with bounds α, β
  - **Upper Bound Module**: Closed-form leaf enumeration per state with action pre-determination
  - **Lower Bound Module**: Uses existing optimal decision tree techniques (scenario grouping, relaxed MILP)
  - **Parallelization Layer**: Distributes |S| independent second-stage subproblems across workers

- Critical path:
  1. Initialize π^0, compute V^0 and Φ^0
  2. For each iteration: fix threshold φ_l, solve Problem (3) via RSBB
  3. RSBB: branch on d (split indicators) → a (features) → b (thresholds) → c (leaf actions)
  4. At each BB node: compute α(M) via enumeration, β(M) via relaxed solve
  5. Prune nodes where α(M) ≤ β^best; terminate when |α - β| ≤ δ
  6. Update V^old, Φ^old; repeat until convergence or budget

- Design tradeoffs:
  - **Depth D vs. interpretability**: Deeper trees improve performance but reduce simulatability; experiments use D=3–4
  - **Threshold φ_l for variable fixing**: Higher φ_l fixes more tree parameters early (faster but may miss optimum); paper uses φ_l=0.5
  - **Parallel vs. serial RSBB**: Parallel gives 100–700× speedup on large MDPs (firewire: 11528s → 15.9s) but requires coordination overhead
  - **Warm start (SPOT+WS)**: Using OMDT solution as initialization helps on large MDPs (firewire: 1201% gain) but adds 5-min overhead

- Failure signatures:
  - **Infeasible second-stage**: MDP constraints cannot be satisfied for fixed tree → BB node pruned as infeasible
  - **Non-convergence**: Gap |α - β| doesn't close within budget → return best β found; check if tree depth is sufficient
  - **Memory blowup**: BB node list grows unboundedly → implement node selection heuristics (depth-first vs. best-first)
  - **Numerical instability**: Big-M constants (ε, ε_max, ε_min) cause numerical issues → verify feature normalization to [0,1]

- First 3 experiments:
  1. **Reproduce single-iteration timing** (Figure 4): Run RSBB vs. Gurobi direct on one MDP (e.g., sys_ad_1, |S|=256) with D=3, measure time to reach 1% optimality gap. Validate parallel speedup claim.
  2. **Ablate depth D**: Compare normalized returns for D=2,3,4 on tiger_vs_ant (|S|=625). Verify that D=3 achieves interpretability (8 decision rules covering 625 states per Appendix B) without sacrificing performance.
  3. **Test warm-start sensitivity**: On firewire (|S|=4093), compare SPOT+WS with random initialization vs. OMDT warm start. Measure iteration count to reach target return (e.g., 0.6).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can solver-independent acceleration methods or approximate formulations extend SPOT's applicability to very large-scale decision-making problems with significantly more than ~8000 states?
- Basis in paper: [explicit] The conclusion states: "Its performance still relies on the efficiency of the underlying optimization solver, which may restrict scalability for extremely large or complex MDPs. Future research could explore solver-independent acceleration methods or approximate formulations."
- Why unresolved: Current experiments only reach MDPs with up to 7948–6825 states (csma_2_4, wlan1), and runtime still depends on MILP solver efficiency.
- What evidence would resolve it: Successful application to MDPs with 10⁵+ states using approximate methods or custom solvers, with bounded suboptimality guarantees.

### Open Question 2
- Question: How does SPOT perform in model-free settings where transition dynamics P and rewards R must be learned from interaction rather than provided?
- Basis in paper: [inferred] The formulation explicitly requires known Pii′k and Rii′k throughout (e.g., equations 2, 3a, 6a), but real-world RL often lacks accurate models.
- Why unresolved: No experiments or theoretical analysis address sample complexity or performance when dynamics are estimated.
- What evidence would resolve it: Theoretical bounds on performance degradation under estimated dynamics, or empirical results combining SPOT with model-learning.

### Open Question 3
- Question: What principled method can automatically select the optimal tree depth D to balance interpretability and performance?
- Basis in paper: [inferred] Experiments use fixed D∈{3,4}, but no guidance is provided for depth selection; performance varies non-monotonically across depths (e.g., tic_vs_ran improves from -1.00610 to 0.38958 when adding warm-start at D=4).
- Why unresolved: The paper treats depth as a hyperparameter without addressing the interpretability–performance tradeoff systematically.
- What evidence would resolve it: A validated criterion (e.g., cross-validation, information-theoretic measure) selecting D that maximizes performance under interpretability constraints.

### Open Question 4
- Question: What are the performance bounds when the optimal MDP policy cannot be represented by the decision-tree policy class?
- Basis in paper: [inferred] Proposition 2's convergence guarantee assumes "the class of decision-tree policies is expressive to represent an optimal MDP policy," but no analysis addresses the inexpressible case.
- Why unresolved: Many practical MDPs have complex optimal policies; understanding the suboptimality gap is critical for deployment.
- What evidence would resolve it: Theoretical approximation bounds relating tree depth to policy suboptimality, or empirical characterization of the gap on benchmark MDPs.

## Limitations

- The decoupling assumption in RSBB may break down for MDPs with complex coupling between state transitions and tree structure.
- Performance guarantees depend on the expressiveness of the decision tree class to represent greedy policies.
- Hyperparameter sensitivity to φ_l and exploration decay ε=1/l is not extensively studied across different MDP classes.
- Warm-start benefits come with 5-minute overhead, making it only beneficial for large MDPs.

## Confidence

- **High confidence**: RSBB's parallel speedup (100-700×) claims, supported by firewire benchmark results.
- **Medium confidence**: Policy iteration guarantees—Proposition 1's greedy improvement holds but global convergence requires tree expressiveness assumptions.
- **Low confidence**: Generalizability beyond tested MDP classes without additional validation.

## Next Checks

1. **Ablate tree depth D** on tiger_vs_ant to verify D=3 maintains interpretability (8 decision rules for 625 states) without sacrificing performance, and that D=4 provides diminishing returns.
2. **Test RSBB on infeasibility** by constructing MDPs where no tree policy can satisfy constraints, verifying the BB node is correctly pruned.
3. **Measure enumeration scalability** by timing upper bound computation on firewire with increasing D to identify the practical depth limit where enumeration becomes prohibitive.