---
ver: rpa2
title: Explaining the Reasoning of Large Language Models Using Attribution Graphs
arxiv_id: '2512.15663'
source_url: https://arxiv.org/abs/2512.15663
tags:
- attribution
- cage
- prompt
- context
- influence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CAGE (Context Attribution via Graph Explanations),
  a novel framework for explaining autoregressive LLM reasoning. CAGE addresses the
  limitation of existing context attribution methods that fail to capture inter-generational
  influence in LLM outputs.
---

# Explaining the Reasoning of Large Language Models Using Attribution Graphs

## Quick Facts
- arXiv ID: 2512.15663
- Source URL: https://arxiv.org/abs/2512.15663
- Reference count: 40
- Key outcome: CAGE improves context attribution faithfulness, achieving up to 40% average gains and 134% maximum improvements in attribution coverage

## Executive Summary
This paper introduces CAGE (Context Attribution via Graph Explanations), a framework for explaining autoregressive LLM reasoning by constructing an attribution graph that captures inter-generational influence. Unlike existing methods that only measure direct prompt-to-output influence, CAGE preserves causal structure through a directed acyclic graph where each generation is influenced by both the prompt and all prior generations. The framework normalizes base attribution scores to be non-negative and row-stochastic, then marginalizes influence along all paths to compute context attributions.

## Method Summary
CAGE wraps any base attribution method by applying it at each generation step to build an attribution table. The table is normalized to create a row-stochastic adjacency matrix that preserves causality (edges only forward in time). Context attributions are computed via matrix propagation $Y = A(I - A)^{-1}$, which sums influence along all paths from prompt tokens to output tokens. This captures indirect prompt-to-output relationships missed by direct summation methods.

## Key Results
- CAGE achieves average gains of up to 40% and maximum improvements of up to 134% in attribution coverage
- Improves faithfulness on RISE and MAS metrics across multiple models and datasets
- Ablation studies show that relaxing non-negativity or row-stochasticity substantially degrades performance

## Why This Works (Mechanism)

### Mechanism 1
Constructing a directed acyclic attribution graph preserves causal structure that row-summation methods discard. For each generated token, apply base attribution over all prior tokens, then normalize scores to be non-negative and row-normalized so incoming edge weights sum to 1.

### Mechanism 2
Marginalizing influence along graph paths recovers indirect prompt-to-output relationships missed by direct summation. Context attribution is computed via $Y = A(I - A)^{-1}$, which expands to summing influence along all paths from prompt tokens to target output tokens.

### Mechanism 3
Non-negative, row-stochastic normalization is necessary for stable, interpretable propagation. This prevents value explosion and sign oscillation during matrix operations, though it discards inhibitory semantics.

## Foundational Learning

- Concept: Autoregressive generation and token dependencies
  - Why needed here: CAGE's core insight is that each token depends on all prior tokens, not just the prompt
  - Quick check question: Given prompt tokens P and generated tokens Y_{<t}, what does the model condition on when predicting x_t?

- Concept: Base attribution methods (gradient, perturbation, attention-based)
  - Why needed here: CAGE is a meta-framework that wraps any base attribution M
  - Quick check question: What does a perturbation-based attribution measure when you replace a sentence with EOS tokens?

- Concept: Directed acyclic graphs and adjacency matrices
  - Why needed here: The attribution graph is a DAG; influence propagation uses matrix operations on its adjacency matrix
  - Quick check question: Why must the adjacency matrix be strictly lower-triangular for an autoregressive model?

## Architecture Onboarding

- Component map: Base Attribution M -> Attribution Table T -> Adjacency Matrix A -> Total Influence Matrix Y -> Context Attribution a_O
- Critical path: Correct normalization (Equation 1) -> stable matrix inversion -> meaningful path contributions
- Design tradeoffs: Non-negativity vs. inhibitory semantics; row-stochastic vs. raw weights; linear vs. nonlinear propagation
- Failure signatures: Value explosion in Y (exceeding 1e6); sign flipping in intermediate propagation; identical attributions across all prompt tokens
- First 3 experiments:
  1. Sanity check with synthetic prompt where token A must influence token B
  2. Ablation of normalization on 50 Math dataset examples
  3. Compare perturbation vs. IG vs. attention as base M on MorehopQA

## Open Questions the Paper Calls Out

### Open Question 1
Can structured signed influence graphs capture inhibitory effects in LLMs without inducing the instability or sign-flipping observed in the non-normalized ablation? Section 5 suggests this as future work.

### Open Question 2
Do higher-order propagation rules yield more faithful explanations than the linear approximation of influence propagation currently employed? The paper suggests exploring this in future work.

### Open Question 3
How sensitive is the CAGE framework's faithfulness to the accuracy of the underlying base attribution method? The paper does not analyze how noise or bias in M propagates through the graph.

## Limitations

- The framework sacrifices the ability to distinguish inhibitory signals and magnitude differences through row-stochastic normalization
- Linear propagation represents a first-order approximation that may not capture nonlinear interactions between generations
- Method requires autoregressive generation without intermediate supervision, limiting applicability

## Confidence

**High Confidence**: Graph construction with row-stochastic normalization and matrix propagation method are well-specified
**Medium Confidence**: Claims about specific improvement percentages depend on implementation details not fully specified
**Low Confidence**: Assertion that relaxing constraints "substantially degrades faithfulness" is supported only by ablation figures

## Next Checks

1. Reproduce the ablation study: Implement CAGE variants without non-negativity and without row-stochastic normalization on Math dataset to verify value explosion and sign oscillation
2. Validate on synthetic control: Create deterministic prompt where token dependencies are known and verify CAGE correctly attributes with appropriate weights
3. Test base method sensitivity: Run CAGE with perturbation, IG, and attention-based methods on same 50 MorehopQA examples to compare attribution coverage scores