---
ver: rpa2
title: 'MiniLingua: A Small Open-Source LLM for European Languages'
arxiv_id: '2512.13298'
source_url: https://arxiv.org/abs/2512.13298
tags:
- data
- training
- language
- dataset
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MiniLingua is a small open-source language model trained from scratch
  to support 13 European languages. It is designed to balance multilingual coverage
  with strong instruction-following capabilities while remaining computationally efficient
  for on-device use.
---

# MiniLingua: A Small Open-Source LLM for European Languages

## Quick Facts
- arXiv ID: 2512.13298
- Source URL: https://arxiv.org/abs/2512.13298
- Authors: Anna Aksenova; Boris Zverkov; Nicola Dainese; Alexander Nikitin; Pekka Marttinen
- Reference count: 0
- MiniLingua is a small open-source language model trained from scratch to support 13 European languages.

## Executive Summary
MiniLingua is a 1-billion parameter multilingual language model trained from scratch to support 13 European languages with strong instruction-following capabilities. The model uses a custom tokenizer optimized for European languages and is trained on a curated mix of web and high-quality data, including code. Through supervised fine-tuning on multilingual instruction-response pairs with equal language sampling, MiniLingua achieves competitive performance on open-ended generation tasks while remaining computationally efficient for on-device use.

## Method Summary
MiniLingua is a 1-billion parameter decoder-only transformer trained from scratch on 13 European languages. The model uses a 128K vocabulary BPE tokenizer trained on a balanced multilingual corpus with English capped at 20%. Pre-training uses a Warmup-Stable-Decay schedule with 1.5 trillion tokens over 12 days on 32× AMD MI200X GPUs. The architecture prioritizes depth (32 layers) over width (1536 hidden) with Grouped Query Attention for efficiency. Supervised fine-tuning uses equal language distribution sampling across 229 million tokens to improve multilingual instruction-following capabilities.

## Key Results
- MiniLingua's instruction-tuned version outperforms EuroLLM on summarization, classification, and question answering
- Remains competitive with larger models like Gemma-3-1B and SmolLM2-1.7B on open-ended generation tasks
- Achieves highest scores on multilingual benchmarks including FLORES-200 (0.681 COMET) and MassiveSumm (0.187 ROUGE) despite smaller size

## Why This Works (Mechanism)

### Mechanism 1: Language-Optimized Tokenization
Training a tokenizer from scratch on balanced multilingual data improves compression efficiency for underrepresented European languages without sacrificing high-resource performance. A 128K vocabulary trained on a "Balanced" mixture (English capped at 20%, no language below 3%) achieves lower Normalized Sequence Length (tokens per word) than both EuroLLM and GPT-4o for Greek, Bulgarian, Finnish, and Czech. Better compression means fewer tokens per document, improving training efficiency and sequence representation quality for morphologically rich languages.

### Mechanism 2: Equal Distribution SFT for Multilingual Instruction-Following
Uniform sampling across all languages during supervised fine-tuning improves open-ended generation in target languages compared to natural distribution sampling. By sampling equally across languages (with repetition for underrepresented ones), MiniLingua avoids English-dominant instruction patterns. This yields superior translation (FLORES: 0.681 vs. Gemma-3-1B's 0.494) and summarization (MassiveSum: 0.187 vs. near-zero for industry models that default to English output).

### Mechanism 3: Depth-Over-Width Architecture with Efficient Attention
Prioritizing depth (32 layers) over width (1536 hidden) at 1B scale improves generalization while Grouped Query Attention reduces inference cost. Deeper small models capture hierarchical linguistic patterns more effectively. GQA (24 query heads, 6 KV heads) maintains quality while cutting KV cache memory. RoPE embeddings improve length extrapolation; SwiGLU and RMSNorm stabilize training.

## Foundational Learning

- **BPE Tokenization and Compression Metrics**
  - Why needed here: MiniLingua's tokenizer evaluation centers on Normalized Sequence Length (NSL = tokens per word). Understanding why this matters is essential to interpret Figure 4's cross-linguistic comparison.
  - Quick check question: Why does achieving lower NSL for Bulgarian or Finnish matter for downstream task performance?

- **Warmup-Stable-Decay (WSD) Learning Rate Schedules**
  - Why needed here: MiniLingua uses a three-phase schedule with data mixture changes during decay (30% high-quality data upsampled). This causes the sharp loss drop in Figure 6.
  - Quick check question: What role does the decay-phase data mixture play, and why might upsampling high-quality data reduce validation loss?

- **Grouped Query Attention (GQA)**
  - Why needed here: The model uses asymmetric attention (24 query heads, 6 KV heads). Understanding GQA is required to interpret Table 1 and assess inference efficiency claims.
  - Quick check question: How does GQA reduce memory and computation during inference compared to standard multi-head attention?

## Architecture Onboarding

- Component map:
  - Tokenizer: 128K BPE vocabulary trained on "Balanced" multilingual mix (English ≤20%, all languages ≥3%)
  - Backbone: 32-layer decoder-only Transformer, 1536 hidden dim, 6144 FFN (SwiGLU activation)
  - Attention: 24 query heads / 6 KV heads (GQA), RoPE positional embeddings
  - Normalization: RMSNorm (not LayerNorm)
  - Embeddings: Tied input and output layers
  - Training: AdamW (β₁=0.9, β₂=0.95), bfloat16, WSD schedule

- Critical path:
  1. Data pipeline: FineWeb-2 + high-quality sources → language filtering → heuristics → deduplication → eval contamination removal
  2. Tokenizer training: Train on balanced subset → evaluate NSL across languages → select 128K Balanced
  3. Pre-training: 1.5T tokens, ~12 days, 32× AMD MI200X → WSD schedule with decay-phase mixture shift (30% HQ data)
  4. SFT: 229M tokens, equal language distribution, ~25 hours, 4× H200, LR=2×10⁻⁵

- Design tradeoffs:
  - English capped at 40% improves multilingual balance but may reduce absolute English performance
  - Equal SFT distribution involves synthetic repetition for low-resource languages
  - No RLHF (compute constraints) → weaker on strict format adherence (Belebele, MMLU-X) vs. industry models
  - 1B scale enables on-device deployment but limits performance vs. 1.7B+ models on knowledge-intensive tasks

- Failure signatures:
  - Incorrect answer format on multiple-choice tasks (generating full text instead of single letters) — related to lack of RLHF alignment
  - Gradient spikes during pre-training (Figure 6) — benign if training recovers without divergence
  - Language mismatch in SFT pairs — filtered via FastText classifier confidence thresholds

- First 3 experiments:
  1. Replicate tokenizer NSL evaluation on your target language subset to verify compression benefits apply to your specific use case.
  2. Ablate SFT sampling strategies (Original vs. Train vs. Equal) on a held-out language to confirm balancing effects for your task type.
  3. Compare base vs. instruct model performance on your specific task (generation vs. classification) to assess whether format-following gaps are acceptable for your application.

## Open Questions the Paper Calls Out

### Open Question 1
How would the inclusion of an explicit alignment stage (such as RLHF or DPO) impact the performance of small multilingual models like MiniLingua? The authors state their pipeline lacks an explicit alignment stage due to the absence of high-quality multilingual preference data. This remains unresolved because suitable multilingual preference datasets are currently unavailable for training reward models or performing direct preference optimization.

### Open Question 2
What is the impact of specific data cleaning heuristics and tokenizer vocabulary sizes on the performance balance across the 13 supported languages? The paper notes that systematic ablation studies on data cleaning strategies or tokenizer vocabulary size were not performed due to computational constraints. Computational limitations prevented exploring the hyperparameter space for data filtering thresholds and alternative vocabulary dimensions after the initial setup.

### Open Question 3
To what degree do evaluations based on translated, English-centric benchmarks underestimate model capabilities or introduce cultural bias in European languages? The authors state that evaluations relied largely on translated, English-centric benchmarks, which may introduce cultural bias and reduce generalizability. Standard benchmarks like MMLU-X are machine-translated from English, potentially failing to capture native linguistic nuances or cultural context.

## Limitations
- Evaluation suite relies on translated, English-centric benchmarks that may not accurately measure genuine multilingual capabilities
- Format-following limitations on multiple-choice tasks due to lack of RLHF alignment
- No testing on non-Latin script languages to verify tokenizer optimization transferability

## Confidence

**High Confidence (4/5):** The tokenizer optimization mechanism is well-supported. The 128K Balanced tokenizer demonstrably achieves lower NSL across all 13 European languages compared to EuroLLM and GPT-4o, with consistent compression improvements for morphologically rich languages like Bulgarian and Finnish.

**Medium Confidence (3/5):** The equal distribution SFT strategy shows clear performance benefits on multilingual generation tasks, with MiniLingua achieving 0.681 on FLORES vs. 0.494 for Gemma-3-1B. However, this improvement may be partially attributable to evaluation design rather than genuine instruction-following superiority.

**Low Confidence (2/5):** The depth-over-width architectural claims rely heavily on external citations without internal ablation studies. While the 32-layer architecture aligns with current trends, the paper doesn't isolate whether depth specifically drives the reported performance gains versus other architectural choices.

## Next Checks

1. **Tokenizer Transferability Test:** Evaluate the 128K Balanced tokenizer's NSL performance on non-European languages (e.g., Russian, Turkish, Arabic) to verify whether the compression benefits generalize beyond the Latin-script European language family.

2. **SFT Strategy Ablation:** Conduct controlled experiments comparing Original, Train, and Equal sampling strategies on a held-out language (e.g., Swedish) across multiple task types (generation vs. classification vs. QA) to isolate whether equal distribution specifically benefits open-ended generation as claimed.

3. **Format-Following Evaluation:** Test MiniLingua on format-sensitive benchmarks (Belebele, MMLU-X) with explicit format instructions ("Answer with a single letter only") to quantify whether format-following limitations are inherent to the SFT approach or can be improved through targeted fine-tuning.