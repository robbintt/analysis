---
ver: rpa2
title: Evaluating Expert Contributions in a MoE LLM for Quiz-Based Tasks
arxiv_id: '2502.17187'
source_url: https://arxiv.org/abs/2502.17187
tags:
- experts
- expert
- entropy
- layer
- gating
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates expert contributions in a Mixture of Experts
  (MoE) large language model (OLMoE-1B-7B) during inference on the MMLU quiz-based
  benchmark. The research reveals that over 60% of experts were never activated across
  the 16 MoE layers, suggesting potential for model size reduction without performance
  loss.
---

# Evaluating Expert Contributions in a MoE LLM for Quiz-Based Tasks

## Quick Facts
- **arXiv ID**: 2502.17187
- **Source URL**: https://arxiv.org/abs/2502.17187
- **Reference count**: 7
- **Key outcome**: Over 60% of experts in a 7B MoE model were never activated during MMLU benchmark inference, suggesting potential for model size reduction without performance loss.

## Executive Summary
This study analyzes expert contributions in the OLMoE-1B-7B MoE model during inference on the MMLU quiz-based benchmark. The research reveals that most experts across all 16 MoE layers were never activated, with only 10-29 out of 64 experts per layer seeing any usage. The gating network outputs show a nearly uniform distribution rather than sparse activation, with entropy values close to the theoretical maximum. Additionally, expert performance varies significantly within the same layer, suggesting potential for performance improvement through gating probability adjustment.

## Method Summary
The study performs inference-only analysis on the pretrained OLMoE-1B-7B-0125-Instruct model using the MMLU benchmark (14,042 questions across 57 datasets). For each question, the model extracts gating probabilities from all 16 MoE layers, identifying which experts are activated and their relative weights. The analysis computes three main metrics: (1) count of unique experts activated per layer, (2) normalized entropy of gating distribution over top-8 experts, and (3) per-expert accuracy weighted by gating probability. The study focuses on understanding activation patterns and expert performance heterogeneity.

## Key Results
- Over 60% of experts across all 16 MoE layers were never activated during MMLU inference
- Gating network outputs show near-uniform distribution with entropy values approaching maximum (2.0794 for top-8 experts)
- Expert performance varies significantly within the same layer, with some experts achieving substantially higher accuracy than others

## Why This Works (Mechanism)

### Mechanism 1: Task-Conditional Expert Activation
MoE models activate only a subset of experts for specific task categories. The Top-K gating mechanism (selecting 8 from 64) results in consistent activation of a small expert subset while >60% remain unused for quiz-based tasks.

### Mechanism 2: Auxiliary Loss Induces Near-Uniform Gating Entropy
Load-balancing auxiliary losses during training push gating distributions toward uniformity to prevent expert collapse, resulting in entropy values near the theoretical maximum.

### Mechanism 3: Expert Performance Heterogeneity Within Layers
Despite uniform-like gating, individual experts within the same layer achieve meaningfully different accuracy, suggesting implicit specialization that could be leveraged for performance improvement.

## Foundational Learning

- **Concept: Top-K Gating with Router Networks**
  - Why needed: Explains how MoE layers select only top-K experts from N total
  - Quick check: For 64 experts with K=8, what fraction contributes to any forward pass? (Answer: 12.5%)

- **Concept: Entropy as Uncertainty Measure**
  - Why needed: Quantifies how "sharp" vs "uniform" gating distributions are
  - Quick check: If entropy = 2.0794 for top-8 experts, what does that imply? (Answer: Uniform weights across all 8 experts)

- **Concept: Auxiliary Load-Balancing Losses**
  - Why needed: Explains why training objectives force equal expert usage
  - Quick check: Why might equal expert usage conflict with inference efficiency? (Answer: It prevents routing to the best experts)

## Architecture Onboarding

- **Component map:**
  Input Token → [MoE Layer 1] → ... → [MoE Layer 16] → Output Logits
  Gating Network (64-dim softmax) → Top-8 Selection
  Expert FFNs (64 total, 8 active) → Weighted Sum

- **Critical path:**
  1. Token embedding enters MoE layer
  2. Gating network produces 64 probability scores
  3. Top-8 experts selected; probabilities normalized
  4. Each selected expert processes token independently
  5. Expert outputs weighted by normalized probabilities and summed
  6. Result passes to next transformer layer

- **Design tradeoffs:**
  - Sparse activation (K<<N) vs Expert utilization
  - Load-balancing loss vs Expert specialization
  - Model size vs Inference efficiency

- **Failure signatures:**
  - Expert collapse: Gating always selects same few experts
  - Uniform uncertainty: Gating cannot distinguish expert quality
  - Performance variance: Some experts significantly underperform

- **First 3 experiments:**
  1. Deactivate never-used experts and measure performance delta
  2. Manually boost probabilities for high-accuracy experts and re-measure accuracy
  3. Run activation logging on different task types to validate pruning universality

## Open Questions the Paper Calls Out

### Open Question 1
Does the observed near-uniform gating distribution negatively impact model robustness due to the non-continuous nature of Top-K activation? The study hypothesizes this but doesn't conduct perturbation tests to verify if high entropy leads to output instability.

### Open Question 2
Can model performance be improved by re-weighting gating probabilities to prioritize experts with higher isolated accuracy? The paper identifies this correlation but doesn't implement or test such a mechanism.

### Open Question 3
Do the high rates of expert inactivity and increasing layer entropy generalize to other MoE architectures or task types? The study only tested OLMoE on MMLU, acknowledging results may not generalize.

## Limitations
- Only analyzed one MoE model (OLMoE-1B-7B) on a single benchmark (MMLU)
- Expert performance heterogeneity may be influenced by data distribution artifacts
- The impact of uniform-like gating on model robustness is hypothesized but not empirically tested

## Confidence
- **High**: >60% of experts never activated
- **Medium**: Uniform-like gating caused by auxiliary losses
- **Low**: Re-weighting gating probabilities will improve performance

## Next Checks
1. Run activation logging on a different benchmark (e.g., GSM8K, HumanEval) to determine if the same experts are inactive
2. Implement gating probability re-weighting intervention and measure accuracy change on held-out MMLU questions
3. Perform ablation study by removing never-activated experts and measuring performance delta on MMLU