---
ver: rpa2
title: 'TinyLLM: Evaluation and Optimization of Small Language Models for Agentic
  Tasks on Edge Devices'
arxiv_id: '2511.22138'
source_url: https://arxiv.org/abs/2511.22138
tags:
- optimization
- language
- function
- data
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of deploying small language
  models (SLMs) for agentic tasks on edge devices without relying on cloud infrastructure.
  The core method involves evaluating and optimizing SLMs like TinyAgent, TinyLlama,
  Qwen, and xLAM using the Berkeley Function Calling Leaderboard (BFCL) framework
  and parameter-driven optimization strategies such as supervised fine-tuning (SFT),
  parameter-efficient fine-tuning (PEFT), reinforcement learning (RL), and Direct
  Preference Optimization (DPO).
---

# TinyLLM: Evaluation and Optimization of Small Language Models for Agentic Tasks on Edge Devices

## Quick Facts
- **arXiv ID**: 2511.22138
- **Source URL**: https://arxiv.org/abs/2511.22138
- **Reference count**: 21
- **Primary result**: Medium-sized SLMs (1-3B parameters) with hybrid optimization achieve up to 65.74% overall accuracy and 55.62% multi-turn accuracy on BFCL, outperforming ultra-compact models (<1B parameters).

## Executive Summary
This study addresses the challenge of deploying small language models (SLMs) for agentic tasks on edge devices without relying on cloud infrastructure. The core method involves evaluating and optimizing SLMs like TinyAgent, TinyLlama, Qwen, and xLAM using the Berkeley Function Calling Leaderboard (BFCL) framework and parameter-driven optimization strategies such as supervised fine-tuning (SFT), parameter-efficient fine-tuning (PEFT), reinforcement learning (RL), and Direct Preference Optimization (DPO). Results show that medium-sized models (1-3B parameters) significantly outperform ultra-compact models (<1B parameters), achieving up to 65.74% overall accuracy and 55.62% multi-turn accuracy with hybrid optimization. This demonstrates that well-optimized SLMs can deliver accurate, efficient, and stable agentic AI on edge devices, enabling privacy-preserving, low-latency autonomous agents beyond cloud reliance.

## Method Summary
The method evaluates and optimizes SLMs (<3B parameters) for agentic function/API calling on edge devices using the BFCL framework. The pipeline includes: (1) baseline BFCL evaluation of candidate base models; (2) SFT with PEFT or full fine-tuning on filtered AgentBank/Gorilla data; (3) conversion of SFT trajectories to preference pairs (chosen: AgentBank responses; rejected: TinyLlama outputs with manual validation); (4) DPO or RL refinement, optionally in hybrid SFT-initialized cycles; and (5) re-evaluation on BFCL across categories (simple, multiple, parallel, parallel-multiple, relevance) in live and non-live modes, plus multi-turn. Models tested include xLAM-2-3b/1b-fc-r, Qwen3-4B/1.7B/0.6B, TinyLlama-1.1B, and TinyAgent-1.1B.

## Key Results
- Medium-sized models (1-3B parameters) significantly outperform ultra-compact models (<1B parameters) on multi-turn agentic tasks.
- Hybrid optimization (SFT + DPO/RL) achieves up to 65.74% overall accuracy and 55.62% multi-turn accuracy.
- Ultra-compact models (<1B parameters) show near-zero multi-turn accuracy, indicating a fundamental capacity threshold for context retention.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid optimization (SFT + DPO/RL) produces higher agentic accuracy than either method alone for SLMs.
- Mechanism: SFT establishes baseline task competence through expert trajectory imitation; DPO or RL then refines the policy by learning to prefer correct function calls over incorrect alternatives, reducing syntactic errors and improving multi-turn coherence.
- Core assumption: The SFT checkpoint is sufficiently capable that preference optimization improves rather than degrades it (i.e., the model can meaningfully represent preferences rather than just memorizing patterns).
- Evidence anchors:
  - [abstract] "achieving up to 65.74% overall accuracy and 55.62% multi-turn accuracy with hybrid optimization"
  - [Section VII.C] "A sequential hybrid strategy typically initializes with SFT on expert data and refines with RL (PPO/DPO variants)… Hybrids balance stability (SFT) and adaptability (RL)"
  - [Section IX] "Hybrid approaches combining SFT initialization with reinforcement learning refinement balance stability and adaptability"
  - [corpus] Related work "Small Language Models for Agentic Systems" confirms SLMs can be effective for agentic workloads with proper optimization, though does not isolate hybrid vs. single-method effects
- Break condition: If SFT overfits to trajectories or preference pairs contain inconsistent/low-quality signals, DPO may amplify errors rather than correct them.

### Mechanism 2
- Claim: A parameter-count threshold exists near 1B parameters below which multi-turn function calling degrades to near-zero accuracy.
- Mechanism: Multi-turn agentic tasks require maintaining conversation state, tracking available functions, and reasoning over prior outputs. Ultra-compact models (<1B) lack sufficient representational capacity for this compositional working memory.
- Core assumption: The performance cliff is due to fundamental capacity constraints rather than insufficient training or suboptimal architecture.
- Evidence anchors:
  - [abstract] "medium-sized models (1-3B parameters) significantly outperform ultra-compact models (<1B parameters)"
  - [Table IV] TinyLlama-1.1B and TinyAgent-1.1B show 0.00% multi-turn accuracy; xLAM-2-3B achieves 55.62%
  - [Section IX.D] "multi-turn dialogue is particularly sensitive to model size and optimization method, with smaller SLMs incapable of retaining sufficient context"
  - [corpus] "Small Models, Big Tasks" explores function calling in SLMs but does not directly confirm a specific threshold; corpus evidence is weak here
- Break condition: Architectural innovations (e.g., retrieval augmentation, improved attention) could shift the threshold; parameter count alone may not be definitive.

### Mechanism 3
- Claim: Converting SFT data into preference pairs by using weaker-model outputs as "rejected" examples can create effective DPO training signal without manual pairwise annotation.
- Mechanism: AgentBank responses are treated as "chosen"; TinyLlama-1.1B-Chat responses are "rejected." Manual validation enforces quality differential. DPO learns to prefer stronger outputs without needing a separate reward model.
- Core assumption: The quality gap between chosen and rejected outputs is consistent and learnable; TinyLlama reliably produces worse outputs across tasks.
- Evidence anchors:
  - [abstract] "conversion of SFT data to chosen-rejected pairs using TinyLlama responses as rejected outputs and manual validation"
  - [Section VIII] "Manual validation confirmed the rejected outputs are consistently lower quality. We completed conversion of the ALFRED dataset from AgentBank and successfully performed DPO training"
  - [corpus] No direct corpus evidence on this specific pipeline; related papers do not address this technique
- Break condition: If rejected outputs occasionally equal or exceed chosen quality, or if manual validation misses systematic biases, DPO will receive noisy preference signals and may not converge.

## Foundational Learning

- Concept: **Direct Preference Optimization (DPO)**
  - Why needed here: Core alignment method in the paper; simpler than PPO-based RLHF and preferred for resource-constrained settings.
  - Quick check question: Why does DPO not require a separate learned reward model, and what does it optimize directly instead?

- Concept: **BFCL Evaluation Categories (AST vs. Executable, Multi-turn, Relevance)**
  - Why needed here: The paper's primary evaluation framework; essential for interpreting accuracy claims and trade-offs across task types.
  - Quick check question: What is the difference between AST accuracy and executable accuracy, and why might a model score high on one but low on the other?

- Concept: **Parameter-Efficient Fine-Tuning (LoRA/QLoRA)**
  - Why needed here: Referenced as practical methods for edge deployment; understanding them is necessary for implementation decisions.
  - Quick check question: How does LoRA reduce trainable parameters compared to full fine-tuning, and what is the trade-off in expressivity?

## Architecture Onboarding

- Component map:
  Data pipeline: Quality filtering + conversion of SFT trajectories to preference pairs (AgentBank → chosen/rejected).
  Model selection: Iterative BFCL evaluation of candidate base models (<3B) before committing to training.
  Fine-tuning stages: SFT (PEFT or full) → DPO (or RL) → optional hybrid cycles.
  Evaluation: BFCL across categories (simple, multiple, parallel, parallel-multiple, relevance) in live and non-live modes, plus multi-turn.

- Critical path:
  1. Evaluate base model on BFCL to establish baseline (target: >30% overall).
  2. Run SFT on filtered AgentBank/Gorilla data.
  3. Construct preference pairs: chosen = AgentBank responses; rejected = TinyLlama outputs; validate manually.
  4. Apply DPO to SFT checkpoint.
  5. Re-evaluate on BFCL; if multi-turn is weak, consider hybrid SFT + RL or larger base.

- Design tradeoffs:
  - 1B vs. 3B models: 1B cheaper but near-zero multi-turn; 3B stronger but higher memory/compute.
  - SFT-only vs. hybrid: SFT simpler but limited on complex tasks; hybrid more effective but higher cost.
  - AST-only vs. full evaluation: AST faster; executable validation catches semantic errors.

- Failure signatures:
  - Multi-turn accuracy ~0% with acceptable single-turn → model too small (<1B) or poor context handling.
  - High AST accuracy, low executable accuracy → syntactic correctness without semantic grounding.
  - DPO destabilizes or degrades SFT → preference pairs may contain inconsistent or reversed quality signals.
  - SFT shows no improvement → check data quality, learning rate, or whether base model is already underfitting.

- First 3 experiments:
  1. **Baseline BFCL profiling**: Run xLAM-2-1B and Qwen3-1.7B on all BFCL categories to identify weakest subcategories and establish a selection baseline.
  2. **Preference-pair validation**: Convert 100–200 ALFRED examples using the TinyLlama rejection method; manually verify consistent quality gaps; run a small DPO pilot to confirm pipeline stability.
  3. **Scale ablation on multi-turn**: Compare xLAM-2-1B vs. xLAM-2-3B on multi-turn tasks to quantify the parameter threshold and inform minimum viable model size for your target use case.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can hybrid fine-tuning cycles be refined to reduce computational overhead while maintaining the adaptability required for edge deployment?
- Basis in paper: [explicit] The conclusion identifies "refining hybrid fine-tuning cycles to reduce compute overhead while maintaining flexibility" as a specific area for future work.
- Why unresolved: While hybrid methods (SFT + RL/DPO) showed the best accuracy, the paper notes they increase compute costs, which conflicts with the resource constraints of edge devices.
- What evidence would resolve it: A comparative analysis of latency and energy consumption of optimized hybrid algorithms versus standard SFT on specific edge hardware.

### Open Question 2
- Question: Can small language models maintain functional correctness when scaling Direct Preference Optimization (DPO) to diverse, non-synthetic datasets?
- Basis in paper: [explicit] The authors state future work includes "scaling DPO training on varied datasets" following the preliminary success with the ALFRED dataset.
- Why unresolved: The current DPO pipeline relied on converting SFT data using TinyLlama as a "rejected" response generator; it is unclear if this method scales effectively to more complex or organic agentic datasets.
- What evidence would resolve it: Evaluation results from SLMs trained on a fully scaled, diverse DPO corpus showing sustained or improved BFCL scores.

### Open Question 3
- Question: Do the optimization techniques identified for text-based function calling generalize to cross-language and multi-modal agentic tasks?
- Basis in paper: [explicit] The paper lists "expanding benchmarks to cover cross-language and multi-modal function calling" as necessary future research.
- Why unresolved: The current study relies heavily on Python/AST evaluations and text-based contexts; the stability of SLMs when handling visual inputs or diverse programming languages (e.g., Java/SQL) under 3B parameters remains unverified.
- What evidence would resolve it: Benchmark results on a multi-modal extension of BFCL demonstrating SLM accuracy in handling image-based function parameters or non-Python execution.

### Open Question 4
- Question: What specific optimization breakthroughs are required to make ultra-compact models (<1B parameters) viable for multi-turn agentic reasoning?
- Basis in paper: [inferred] The results section highlights that ultra-compact models like TinyLlama (1.1B) achieved 0% multi-turn accuracy, effectively failing at context retention.
- Why unresolved: The paper establishes a performance floor for 1-3B models but leaves the viability of smaller models—crucial for stricter edge constraints—as an open problem without a clear solution path.
- What evidence would resolve it: Demonstrating a sub-1B parameter model achieving statistically significant non-zero multi-turn accuracy on the BFCL.

## Limitations

- The study's conclusions are based on a narrow set of model families (<3B parameters) and a single evaluation framework (BFCL), limiting generalizability.
- The performance cliff at ~1B parameters is compelling but not definitively proven to be a universal capacity constraint rather than an artifact of specific architectures or training regimes.
- The hybrid optimization approach shows strong empirical gains, but the relative contributions of SFT versus DPO/RL within the hybrid pipeline are not fully quantified.

## Confidence

- **High Confidence**: The finding that medium-sized models (1-3B) outperform ultra-compact models (<1B) on multi-turn agentic tasks, supported by BFCL accuracy tables and explicit multi-turn failure of sub-1B models.
- **Medium Confidence**: The effectiveness of hybrid optimization (SFT + DPO/RL) for improving agentic accuracy, based on stated results but without detailed ablation studies or hyperparameter sensitivity analysis.
- **Medium Confidence**: The parameter-count threshold (~1B) as a critical boundary for multi-turn function calling, though this is inferred from a limited set of model comparisons and may be architecture-dependent.
- **Low Confidence**: The robustness and generalizability of the preference-pair generation method (using TinyLlama outputs as rejected examples), due to minimal external validation and lack of comparison to manual pairwise annotation.

## Next Checks

1. **Ablation on Optimization Methods**: Run controlled experiments comparing SFT-only, DPO-only, RL-only, and hybrid pipelines on the same base model and dataset to quantify each method's contribution to multi-turn accuracy.
2. **Preference Pair Quality Audit**: Manually validate a random sample of generated chosen-rejected pairs from the ALFRED dataset to ensure the quality differential is consistent and the manual validation process is reliable.
3. **Cross-Dataset Generalization**: Apply the full training and evaluation pipeline (including hybrid optimization) to a different function-calling dataset (e.g., Gorilla's OpenFunctions expanded set) to test if the 1B parameter threshold and optimization benefits hold outside the ALFRED domain.