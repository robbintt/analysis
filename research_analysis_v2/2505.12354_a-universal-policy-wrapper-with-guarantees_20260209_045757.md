---
ver: rpa2
title: A universal policy wrapper with guarantees
arxiv_id: '2505.12354'
source_url: https://arxiv.org/abs/2505.12354
tags:
- policy
- base
- goal
- fallback
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a universal policy wrapper that ensures goal-reaching
  guarantees for reinforcement learning agents while maintaining or improving performance.
  The method dynamically switches between a high-performing base policy (trained via
  standard RL) and a fallback policy with known convergence properties, using the
  base policy's value function to determine when switching is necessary.
---

# A universal policy wrapper with guarantees

## Quick Facts
- arXiv ID: 2505.12354
- Source URL: https://arxiv.org/abs/2505.12354
- Reference count: 26
- Primary result: A universal policy wrapper that guarantees goal-reaching while maintaining or improving base policy performance

## Executive Summary
This paper introduces a universal policy wrapper that ensures goal-reaching guarantees for reinforcement learning agents while maintaining or improving performance. The method dynamically switches between a high-performing base policy (trained via standard RL) and a fallback policy with known convergence properties, using the base policy's value function to determine when switching is necessary. The approach requires no additional system knowledge or online optimization, making it broadly applicable.

Theoretical analysis proves the wrapper inherits the fallback policy's goal-reaching guarantees while preserving or improving base policy performance. Experiments on Pendulum-v1 and CartPoleSwingup environments demonstrate that different operational modes (conservative, balanced, brave) offer tradeoffs between safety and performance. The wrapper achieves 100% goal-reaching success while matching or exceeding both base and fallback policy performance in terms of cumulative rewards, particularly in later training stages. The method is implemented as a Gymnasium environment wrapper for seamless integration with existing RL frameworks.

## Method Summary
The wrapper maintains a reference value v†_t and at each timestep compares the base policy's critic value v̂_πb(s) against v†_t + ν. If the critic shows improvement above threshold ν, the base policy acts and v†_t is updated. Otherwise, with decaying probability ρ_t = λ^t · p_relax, the base policy is still accepted; otherwise, the fallback takes over. This creates a soft shielding mechanism rather than hard constraints. The theoretical analysis proves that the wrapper inherits the fallback policy's goal-reaching guarantees because, with probability 1, the algorithm eventually switches permanently to the fallback (N_πb is bounded). Three operational modes are defined through hyperparameters: Conservative (p_relax=0), Balanced (p_relax=0.5), and Brave (p_relax=0.95), with λ=0.9999 and ν=0.01.

## Key Results
- The wrapper achieves 100% goal-reaching success across all three operational modes on Pendulum-v1 and CartPoleSwingup environments
- Wrapper performance matches or exceeds both base and fallback policy performance in terms of cumulative rewards, particularly in later training stages
- Conservative mode (p_relax=0) guarantees goal-reaching at the cost of some performance, while Brave mode (p_relax=0.95) preserves base policy behavior while still providing eventual guarantees
- The method requires no additional system knowledge or online optimization, making it broadly applicable to any trained RL policy

## Why This Works (Mechanism)

### Mechanism 1: Critic-Supervised Policy Arbitration
- Claim: A value function can serve as a runtime monitor to decide between a performant but untrusted policy and a verified fallback controller.
- Mechanism: At each timestep, the wrapper compares the base policy's critic value against a running reference v†_t. If the critic shows improvement above threshold ν, the base policy acts. Otherwise, with decaying probability ρ_t, the base policy is still accepted; otherwise, the fallback takes over. This creates a form of "soft shielding" rather than hard constraints.
- Core assumption: The value function v̂_πb is bounded above and correlates sufficiently with goal progress that its degradation signals increased risk.
- Evidence anchors: [abstract], [Section III-A-1], [corpus: "Reinforcement Learning Goal-Reaching Control with Guaranteed Lyapunov-Like Stabilizer for Mobile Robots" (FMR 0.56)]
- Break condition: If v̂_πb is poorly estimated (e.g., early training) and does not correlate with actual goal progress, the switching signal becomes unreliable.

### Mechanism 2: Inherited Goal-Reaching via Forced Fallback Convergence
- Claim: The wrapper inherits the fallback policy's ε-improbable goal-reaching property because, with probability 1, the algorithm eventually switches permanently to the fallback.
- Mechanism: The proof shows that N_πb (total base-policy uses) is bounded by N_v† + N_ρ. N_v† is finite because v†_t can only increase by at least ν each time, bounded by v̄. N_ρ is almost surely finite because ρ_t is summable (Borel-Cantelli). Thus there exists an almost surely finite T_final after which only π_f operates.
- Core assumption: The fallback policy satisfies the ε-improbable goal-reaching property (Definition 2), which must be established independently.
- Evidence anchors: [Section III-B, Theorem 1], [Section II-C], [corpus: "Predictive Safety Shield for Dyna-Q Reinforcement Learning" (FMR 0.59)]
- Break condition: If the fallback policy's goal-reaching guarantee is invalid (wrong ε, incorrect dynamics assumptions), the wrapper's guarantee also fails.

### Mechanism 3: Tunable Safety-Performance Tradeoff via Acceptance Decay
- Claim: The hyperparameters p_relax and λ control how long the wrapper "trusts" the base policy before committing to fallback, enabling a spectrum from conservative to brave operation.
- Mechanism: Setting ρ_t(s) = λ^t × p_relax creates exponential decay of base-policy acceptance probability. High p_relax (≈1) and λ (≈1) preserve base policy behavior; lower values trigger earlier fallback switching. This is analogous to simulated annealing in exploration-exploitation.
- Core assumption: Finite-horizon tasks can tolerate higher p_relax because the "eventual fallback" may not occur within episode bounds.
- Evidence anchors: [Section III-A-3], [Section V], [corpus: weak direct evidence for this specific decay formulation]
- Break condition: On very long horizons with high p_relax, the wrapper may behave almost identically to the base policy, offering minimal safety benefit until very late.

## Foundational Learning

- **Lyapunov Stability Theory**
  - Why needed here: The fallback policy must provide goal-reaching guarantees; understanding KL functions and how β(d,t) bounds convergence rate is essential for interpreting Theorem 2.
  - Quick check question: Can you explain why a function β ∈ KL satisfying d_G(S_t) ≤ β(d_0, t) implies asymptotic convergence to the goal set?

- **Value Functions and Critics in Actor-Critic RL**
  - Why needed here: The entire switching mechanism relies on interpreting v̂_πb as a progress indicator; you must understand what value functions estimate and their limitations.
  - Quick check question: What does it mean for a value function to be "bounded above," and why does this matter for proving N_v† is finite?

- **Borel-Cantelli Lemma and Almost-Sure Finiteness**
  - Why needed here: The proof that N_ρ < ∞ relies on the summability of ρ_t and the Borel-Cantelli lemma; understanding "almost surely finite" random variables is critical.
  - Quick check question: If Σ ρ_t < ∞, why does this guarantee that the event "accept base policy randomly" occurs only finitely many times with probability 1?

## Architecture Onboarding

- **Component map:**
  - Base policy π_b -> Critic network v̂_πb -> Wrapper module -> (Base policy π_b or Fallback policy π_f) -> Environment
  - Fallback policy π_f -> Wrapper module (input: current state)
  - Wrapper module maintains v†_t and switching logic

- **Critical path:**
  1. Verify fallback policy has goal-reaching proof for your environment (assumption: you can construct one)
  2. Train base policy and critic to convergence or desired checkpoint
  3. Wrap environment and policy with CALF-Wrapper, set hyperparameters based on desired safety-performance tradeoff
  4. Validate that critic is bounded and reasonably correlates with goal progress

- **Design tradeoffs:**
  - Conservative (p_relax≈0): Maximum safety, may sacrifice reward early
  - Brave (p_relax≈0.95, λ≈0.9999): Preserves base policy performance but guarantees may not manifest within short episodes
  - ν: Larger values require more significant critic improvement before trusting base policy; too large blocks all base-policy use

- **Failure signatures:**
  - Goal-reaching rate < 100% in Conservative mode → fallback policy guarantee is invalid or episode too short
  - Reward significantly below base policy even in Brave mode → critic is poorly correlated with goal, causing premature switching
  - Wrapper never switches to fallback → p_relax or λ set too high for episode horizon

- **First 3 experiments:**
  1. **Baseline validation**: Run standalone base policy and standalone fallback policy on your environment; confirm fallback achieves 100% goal-reaching and base achieves higher reward but <100% success.
  2. **Conservative mode test**: Set p_relax=0, verify wrapper achieves 100% goal-reaching across 30+ random initial conditions.
  3. **Tradeoff sweep**: Vary p_relax ∈ {0, 0.25, 0.5, 0.75, 0.95} with fixed λ=0.9999; plot reward vs. goal-reaching rate to identify sweet spot for your application.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can finite-horizon guarantees be developed when the theoretically required steps to reach the goal set exceeds the practical episode length?
- Basis in paper: [explicit] "Despite theoretical guarantees, for Balanced and Brave modes, goal-reaching rate occasionally falls below 100% on early and mid-stage phases since the theoretically required number of steps to reach the goal set G exceeds the evaluation episode length."
- Why unresolved: Current theory provides only asymptotic guarantees (t → ∞), but real applications have bounded horizons.
- What evidence would resolve it: Derivation of finite-horizon bounds or algorithm modifications providing probabilistic guarantees for bounded episode lengths.

### Open Question 2
- Question: How does the method scale to high-dimensional state/action spaces and more complex real-world control tasks?
- Basis in paper: [inferred] Experiments only cover low-dimensional environments (3D/5D observations, scalar actions) with simple pendulum-like dynamics.
- Why unresolved: Value function approximation and fallback policy construction may face curse of dimensionality.
- What evidence would resolve it: Evaluation on high-dimensional benchmarks (robotic manipulation, locomotion) showing maintained guarantees and performance.

### Open Question 3
- Question: How robust is the wrapper to approximation errors in the learned value function estimate v̂_πb?
- Basis in paper: [inferred] Theoretical analysis assumes a value function but doesn't analyze impact of neural network approximation errors common in critics.
- Why unresolved: Neural network critics can be inaccurate, especially early in training or in underexplored state regions.
- What evidence would resolve it: Experiments varying value function accuracy (partially trained or perturbed critics) with analysis of resulting safety/performance degradation.

### Open Question 4
- Question: Can systematic methods be developed for constructing fallback policies with the ε-improbable goal-reaching property for arbitrary nonlinear systems?
- Basis in paper: [inferred] Paper assumes fallback policy availability but constructs problem-specific energy-based/PD controllers only for pendulum-like systems.
- Why unresolved: No general framework provided for fallback policy synthesis across diverse system classes.
- What evidence would resolve it: Demonstration of fallback policy construction for diverse underactuated, hybrid, or multi-agent systems.

## Limitations

- The fallback policy's exact formulation remains unspecified, requiring users to independently construct a controller with ε-improbable goal-reaching guarantees
- The method assumes the base policy's critic value correlates with goal progress, which may fail if the critic is poorly trained or the task has complex reward shaping
- The theoretical guarantee of eventual fallback switching relies on summable ρ_t, but in practice finite episodes may end before this occurs, limiting safety benefits in brave modes

## Confidence

- **High**: The inheritance of fallback's goal-reaching property via finiteness proofs (Theorem 1) - the mathematical argument is sound given the assumptions.
- **Medium**: The practical effectiveness of the tunable safety-performance tradeoff - experimental results support this, but the corpus provides limited direct evidence for the specific ρ_t decay mechanism.
- **Medium**: The claim that no additional system knowledge or online optimization is required - true for the wrapper itself, but constructing a valid fallback policy still requires domain knowledge.

## Next Checks

1. **Fallback policy verification**: Independently implement the energy-based swingup + PD balance controller for Pendulum-v1 and formally verify it satisfies the ε-improbable goal-reaching property under the paper's defined goal set.

2. **Early checkpoint behavior**: Test the wrapper with base policies from early/mid training (e.g., 25k, 50k timesteps) to quantify how quickly the theoretical guarantee manifests in practice versus training progress.

3. **Critic sensitivity analysis**: Systematically vary the improvement threshold ν across orders of magnitude (e.g., 0.001 to 0.1) while holding other parameters constant to determine how sensitive the switching behavior is to critic scale and training quality.