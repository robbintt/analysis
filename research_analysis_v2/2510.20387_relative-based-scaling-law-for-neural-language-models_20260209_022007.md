---
ver: rpa2
title: Relative-Based Scaling Law for Neural Language Models
arxiv_id: '2510.20387'
source_url: https://arxiv.org/abs/2510.20387
tags:
- scaling
- size
- arxiv
- rbpk
- cross-entropy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel Relative-Based Scaling Law for neural
  language models that addresses the limitation of traditional cross-entropy-based
  scaling laws by incorporating relative token ranking into performance evaluation.
  The authors propose a new metric called Relative-Based Probability (RBP), which
  measures the probability that the ground-truth token appears among the model's top-k
  predictions, providing a more comprehensive view of model performance that better
  aligns with practical sampling strategies like greedy decoding and top-k sampling.
---

# Relative-Based Scaling Law for Neural Language Models

## Quick Facts
- arXiv ID: 2510.20387
- Source URL: https://arxiv.org/abs/2510.20387
- Reference count: 40
- Primary result: RBP follows power-law scaling with model size achieving R² ≈ 0.99 for k < 1000

## Executive Summary
This paper introduces a novel Relative-Based Scaling Law for neural language models that addresses the limitations of traditional cross-entropy-based scaling laws. By incorporating relative token ranking into performance evaluation through a new metric called Relative-Based Probability (RBP), the authors demonstrate that the probability of ground-truth tokens appearing in top-k predictions follows a precise power-law relationship with model size. This discovery provides both practical guidance for scaling language models and theoretical insight into fundamental AI scaling principles.

## Method Summary
The method involves calculating RBP_k for pre-trained models across four families (GPT-2, OPT, Pythia, Qwen2.5) spanning five orders of magnitude in size. For each token in four datasets (Wikipedia, HumanEval, HotpotQA, AusLegal), the ground-truth token's rank is computed from full softmax logits, and RBP_k is aggregated as the proportion where rank ≤ k. Power-law fits are performed by regressing log(-log(RBP_k)) against log(model size) in log-log space, with R² values and scaling exponents reported for various k values.

## Key Results
- RBP follows precise power-law relationship with model size achieving R² values of approximately 0.99 for k < 1000
- Discovery of smooth microscopic scaling law that manifests as non-linear performance jumps at the sequence level, explaining emergence phenomena
- Proposed unified theoretical framework connecting cross-entropy and relative-based scaling through shared lognormal rank distribution mechanism

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Model scaling systematically improves the rank of the ground-truth token, following a power-law relationship distinct from cross-entropy.
- **Mechanism:** As non-embedding parameter count (S) increases, the probability that the correct token falls within the top-k predictions (RBP_k) improves. Specifically, -log(RBP_k) decays as a power of model size (S^(-α)). This holds robustly when k << |Vocabulary|.
- **Core assumption:** The relative ordering of tokens (ranking) changes predictably with scale, rather than randomly, for small k values.
- **Evidence anchors:** [abstract] "RBP follows a precise power-law relationship with model size... achieving R² values of approximately 0.99 for k < 1000." [section 4.1] "The result marks the discovery of a novel scaling law... scaling up models fundamentally changes the relative-ordering."
- **Break condition:** The power-law relationship deteriorates significantly when k approaches the vocabulary size (k → |V|), likely due to noise dominating the metric.

### Mechanism 2
- **Claim:** "Emergence" phenomena (sharp performance jumps) are a predictable macroscopic effect of the smooth microscopic RBP scaling law.
- **Mechanism:** Sequence-level success requires N consecutive correct rankings. If the probability of a single correct ranking scales as a power law, the probability of N consecutive successes (p_{N,k} ≈ exp(-C · N · S^(-α))) creates a sigmoidal "step-function" curve as model size increases.
- **Core assumption:** Token predictions are independent and stationary across positions.
- **Evidence anchors:** [abstract] "It explains emergence phenomena through a smooth microscopic scaling law that manifests as non-linear performance jumps at the sequence level." [section 5.1] "Emergence is not a breakdown of scaling laws but a predictable macroscopic effect."
- **Break condition:** If token dependencies are extremely high or the task requires non-stationary reasoning across the sequence, the independence assumption may fail.

### Mechanism 3
- **Claim:** A shared lognormal rank distribution mechanism may unify both cross-entropy and relative-based scaling behaviors.
- **Mechanism:** The distribution of the ground-truth token's rank is hypothesized to be lognormal. Theoretical derivations show that if parameters μ and σ of this distribution scale with model size, both cross-entropy and -log(RBP) naturally exhibit similar power-law decays.
- **Core assumption:** The rank of the ground-truth token follows a lognormal distribution where distribution parameters vary systematically with model size.
- **Evidence anchors:** [abstract] "Suggests a unified theoretical framework... through a shared lognormal rank distribution mechanism." [section 5.2] "Empirical evidence shows that these rank distributions are long-tailed... we assume that the rank distribution follows a lognormal form."
- **Break condition:** If the rank distribution deviates significantly from lognormal, the unified theoretical explanation collapses.

## Foundational Learning

- **Concept: Power Law Scaling (y ∝ x^(-α))**
  - **Why needed here:** The entire paper rests on identifying and fitting power-law relationships between model size and performance metrics.
  - **Quick check question:** On a log-log plot, does the relationship between Model Size and -log(RBP) appear linear?

- **Concept: Cross-Entropy vs. Ranking Metrics**
  - **Why needed here:** The core motivation is the limitation of Cross-Entropy in predicting greedy decoding success.
  - **Quick check question:** If a model assigns 0.4 probability to the correct token, can we determine if greedy decoding will succeed without knowing other token probabilities?

- **Concept: Lognormal Distribution**
  - **Why needed here:** This is the theoretical conjecture used to explain why the two scaling laws coincide.
  - **Quick check question:** Why would a lognormal distribution of ranks cause both absolute probability and top-1 accuracy to scale similarly?

## Architecture Onboarding

- **Component map:** Corpus (Wiki, Code, etc.) → Tokenizer → Model (Transformer LLM) → Logits → Sort/Top-k Check (RBP calculation)
- **Critical path:** The implementation of the RBP metric is the critical new component, requiring checking p(v) ≥ p(t) for all tokens v to find the rank R, or using efficient top-k selection algorithms.
- **Design tradeoffs:**
  - Selection of k: Small k (1-100) yields clean power laws (high R²) but is "harder" to optimize (lower exponent). Large k is easier to optimize but the law breaks down into noise as k → |V|.
  - Metric cost: Calculating RBP requires sorting or partitioning logits, which is more computationally expensive than calculating Cross-Entropy if done naïvely on the full vocabulary.
- **Failure signatures:**
  - Regime shift: If k > 1000 (approaching |V|), expect to see loss of correlation (R² drops) and erratic curves.
  - Data noise: In the large-k regime, performance might appear to degrade with scale due to noise sensitivity.
- **First 3 experiments:**
  1. Baseline Verification: Calculate RBP_1 for Pythia across 5+ scale intervals. Plot -log(RBP_1) vs. Model Size to verify linearity (R² > 0.99).
  2. Emergence Simulation: Pick N=100 tokens. Calculate p_{N,1} = (RBP_1)^N for different model sizes. Plot p_{N,1} to visualize the sigmoid emergence curve.
  3. Distribution Check: For a specific model, histogram the ranks of ground-truth tokens. Fit a lognormal distribution to validate the theoretical assumption.

## Open Questions the Paper Calls Out

- **What fundamental theoretical principle unifies cross-entropy-based and relative-based scaling laws?**
  - The authors describe similar scaling exponents as a "confusing coincidence" with no established theory to fully explain this phenomenon.
  - A formal theoretical proof demonstrating that both scaling laws emerge necessarily from a shared property would resolve this.

- **What is the precise mechanism causing the power-law relationship of RBP to deteriorate when k approaches vocabulary size?**
  - The paper conjectures that noise dominates but lacks rigorous mathematical characterization of the noise floor.
  - An analysis of the noise floor in token rankings or a theoretical model showing how noise disrupts power-law scaling would resolve this.

- **Does the Relative-Based Scaling Law hold for non-transformer architectures or multimodal models?**
  - Experiments are restricted to transformer-based language model families while scaling laws are sought in new domains.
  - Replicating the RBP scaling experiments on State Space Models and multimodal transformers would test universality.

## Limitations

- The paper doesn't fully explain why the power-law relationship breaks down for large k values, with the "noise dominates" explanation lacking rigorous mathematical characterization.
- The independence assumption for token predictions may not hold for complex reasoning tasks, particularly for tasks requiring multi-step reasoning.
- The lognormal rank distribution hypothesis lacks empirical validation and systematic fitting to alternative distributions.

## Confidence

- **RBP power-law discovery:** High confidence in the empirical demonstration with R² ≈ 0.99 for small k values across multiple datasets and model families
- **Emergence explanation:** Medium confidence in the mathematical framework connecting microscopic scaling to macroscopic emergence, but relies on token independence assumptions
- **Unified theoretical framework:** Low confidence in the lognormal distribution hypothesis as it is currently speculative with limited empirical support

## Next Checks

1. **Independence Assumption Validation:** Systematically test token independence by measuring correlation between consecutive token rankings in sequence prediction tasks, particularly for reasoning-intensive datasets like HotpotQA.

2. **Distribution Fitting Analysis:** For multiple models and datasets, fit empirical rank distributions of ground-truth tokens to lognormal, normal, and alternative distributions, comparing goodness-of-fit metrics.

3. **Noise Floor Characterization:** Characteristically define and measure the noise floor in RBP calculations for large k values, including systematic analysis of how the power-law relationship breaks down as k approaches vocabulary size.