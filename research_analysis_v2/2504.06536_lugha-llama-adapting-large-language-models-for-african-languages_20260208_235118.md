---
ver: rpa2
title: 'Lugha-Llama: Adapting Large Language Models for African Languages'
arxiv_id: '2504.06536'
source_url: https://arxiv.org/abs/2504.06536
tags:
- languages
- language
- data
- arxiv
- african
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of adapting large language models
  to low-resource African languages, which are underrepresented in standard training
  corpora. The core method involves continuing the pre-training of Llama-3.1-8B on
  10 billion multilingual tokens, combining curated data from 16 African languages
  with high-quality English educational texts.
---

# Lugha-Llama: Adapting Large Language Models for African Languages

## Quick Facts
- arXiv ID: 2504.06536
- Source URL: https://arxiv.org/abs/2504.06536
- Reference count: 27
- Primary result: Continued pre-training on multilingual tokens improves African language performance

## Executive Summary
This paper addresses the challenge of adapting large language models to low-resource African languages, which are underrepresented in standard training corpora. The core method involves continuing the pre-training of Llama-3.1-8B on 10 billion multilingual tokens, combining curated data from 16 African languages with high-quality English educational texts. The authors find that this approach significantly improves model performance on African languages, particularly on knowledge-intensive multiple-choice questions. On the IrokoBench dataset, their models achieve the best performance among similarly sized baselines, especially on AfriMMLU, and outperform the base model by over 10% on the AfriQA benchmark. The study also includes a case study on Swahili, suggesting that the content of educational data, rather than its source language, is primarily responsible for the strong performance. The authors release their models and data to encourage future research on African languages.

## Method Summary
The method involves continuing pre-training of Llama-3.1-8B on a multilingual corpus of 10 billion tokens, combining WURA (16 African languages) with either FineWeb-Edu or OpenWebMath. The training uses UniMax sampling to balance language representation, with a batch size of 512x8192 tokens, learning rate of 1e-5, and 2400 training steps. The approach aims to leverage the base model's existing reasoning capabilities while extending its linguistic coverage to underrepresented African languages.

## Key Results
- Lugha-Llama models achieve best performance among similarly sized baselines on IrokoBench, particularly on knowledge-intensive multiple-choice questions
- Outperform base model by over 10% on AfriQA benchmark
- Swahili case study shows translated English educational data (FineWeb-Edu) outperforms native Swahili data, suggesting content quality drives performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Continued pre-training on a strong base model yields superior performance for low-resource languages compared to training smaller models from scratch.
- **Mechanism:** A base model (Llama-3.1-8B) pre-trained on 15T tokens possesses robust general reasoning and world knowledge. Continued pre-training maps new low-resource language tokens to these existing representations, rather than expending limited data budget on learning reasoning from scratch.
- **Core assumption:** Reasoning capabilities are language-agnostic and can be activated via a new linguistic interface.
- **Evidence anchors:**
  - [abstract] "Our models consistently achieve the best performance amongst similarly sized baselines, particularly on knowledge-intensive multiple-choice questions."
  - [section - Table 1] Lugha-Llama-8B significantly outperforms InkubaLM-0.4B (trained from scratch) on AfriMMLU (33.2 vs 24.8 avg).
  - [corpus] Related paper ("The State of Large Language Models for African Languages") finds that adapted larger models generally outperform specialized small models (SSLMs) for African languages.
- **Break condition:** Performance would degrade if catastrophic forgetting destroyed core reasoning abilities during adaptation.

### Mechanism 2
- **Claim:** Data quality (specifically, educational content) drives performance on knowledge-intensive tasks more than the source language of the training data.
- **Mechanism:** High-quality educational data improves the model's internal knowledge representations. The language of that data is secondary; the semantic content is what strengthens the model's ability to answer knowledge-based queries, facilitating cross-lingual transfer.
- **Core assumption:** Knowledge encoded from one language is accessible when queried in another.
- **Evidence anchors:**
  - [abstract] "...suggesting that the content of these data is primarily responsible for the strong performance."
  - [section 5, Table 3] Training on FineWeb-Edu translated to Swahili achieves the best Swahili AfriMMLU score (46.0), outperforming the original English mix (42.6).
  - [corpus] Related work ("Improving Multilingual Math Reasoning for African Languages") also highlights the critical role of high-quality data for improving performance in low-resource settings.
- **Break condition:** If in-language knowledge were strictly required, English data would not boost African-language QA performance, and translated data would not outperform native-language data.

### Mechanism 3
- **Claim:** A uniform sampling strategy (UniMax) is necessary to prevent high-resource languages from dominating training in a multilingual corpus.
- **Mechanism:** Natural corpus distributions are heavily skewed. UniMax sampling up-samples low-resource languages (capped at 4 epochs) to provide sufficient training signal, ensuring the model learns their representations rather than ignoring them.
- **Core assumption:** Limited oversampling of rare data improves learning without causing severe overfitting.
- **Evidence anchors:**
  - [section 3, Figure 1] Visualizes the flattened token distribution achieved by UniMax sampling compared to the original skewed corpus.
  - [section 3] "We sample from WURA using UniMax sampling... which attempts to sample as uniformly as possible across languages..."
  - [corpus] Weak/missing. Corpus papers do not explicitly discuss sampling strategies.
- **Break condition:** If oversampling caused significant overfitting, upsampled languages would show degraded validation performance.

## Foundational Learning

- **Concept: Continued Pre-training**
  - **Why needed here:** This is the paper's central method, distinct from fine-tuning. It extends a base model's knowledge to new domains/languages using a self-supervised objective on a large new corpus.
  - **Quick check question:** How does the training objective of continued pre-training differ from that of supervised fine-tuning (SFT)?

- **Concept: Catastrophic Forgetting**
  - **Why needed here:** A primary risk in adaptation. The paper investigates whether including English data mitigates this by preserving the base model's original capabilities.
  - **Quick check question:** What happens to a model's original task performance when it is trained exclusively on a new task?

- **Concept: Cross-Lingual Transfer**
  - **Why needed here:** This phenomenon explains the paper's key finding: that training on English educational data improves performance on African language benchmarks.
  - **Quick check question:** What allows a model to answer a question in a language it was not explicitly trained on for that task?

## Architecture Onboarding

- **Component map:**
  Base Model (Llama-3.1-8B) -> WURA Corpus + Educational Data -> UniMax Sampler -> Tokenizer -> Training Process

- **Critical path:**
  1.  **Data Curation:** Combine WURA with educational data (English or translated).
  2.  **Sampling:** Apply UniMax to balance the multilingual mix.
  3.  **Training:** Run continued pre-training on 10B tokens with a low learning rate (1e-5).

- **Design tradeoffs:**
  - **Data Mix:** Adding high-quality English data (40%) reduces African-language token count but can improve knowledge-intensive task performance.
  - **Sampler Cap:** Upsampling limited to 4 epochs prevents overfitting but may still underutilize very rare data.

- **Failure signatures:**
  - **Forgetting:** Model performance drops on original high-resource languages (e.g., English).
  - **Overfitting:** Loss on low-resource languages plateaus or increases while validation performance degrades.
  - **Tokenization Bottleneck:** Poor generation quality on languages not well-represented in the original tokenizer's vocabulary.

- **First 3 experiments:**
  1.  **Ablate Data Mix:** Train with 0%, 20%, and 40% educational data to find the optimal performance/knowledge tradeoff on a target benchmark.
  2.  **Ablate Sampler:** Compare UniMax vs. proportional sampling to quantify the benefit of low-resource language upsampling.
  3.  **Swahili Translation Case Study:** Replicate the finding that translated educational data outperforms native language data to confirm the data quality hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can large-scale machine translation of high-quality educational content effectively close the data quality gap for low-resource African languages?
- **Basis in paper:** [explicit] The authors conclude that their case study "raises the possibility of reducing this gap via large-scale machine translation" after observing that translated Swahili data outperformed existing corpora.
- **Why unresolved:** The experiment was limited to a 200M token subset translated into Swahili. It remains unclear if this method scales to billions of tokens or applies to languages where translation quality is lower than GPT-4o's Swahili performance.
- **What evidence would resolve it:** Results from models continually pre-trained on billions of machine-translated tokens across a diverse set of low-resource languages, compared against models trained on human-curated data.

### Open Question 2
- **Question:** To what extent do Western-centric evaluation benchmarks like MMLU misrepresent the actual knowledge capabilities of African-centric models?
- **Basis in paper:** [explicit] The Limitations section states that "some MMLU subjects are skewed towards Western and US-centric knowledge" and argues that models "should also be developed and evaluated based on African-centric knowledge benchmarks."
- **Why unresolved:** Current performance metrics (e.g., AfriMMLU) rely on translated datasets that may carry cultural bias, making it difficult to discern if a model lacks language understanding or specific cultural knowledge.
- **What evidence would resolve it:** The creation and use of evaluation benchmarks rooted in indigenous knowledge and local cultural contexts, rather than translations of Western datasets.

### Open Question 3
- **Question:** Do improvements in mathematical reasoning for African languages stem from genuine cross-lingual transfer or from reliance on language-invariant surface cues?
- **Basis in paper:** [inferred] The Limitations section notes that mathematical questions in the evaluation often contain similar symbols and entity names across languages, suggesting models might make "educated guesses based on surface cues."
- **Why unresolved:** The study shows improved math scores (AfriMGSM) when adding English math data, but the mechanism—whether learning reasoning or simply leveraging shared symbols—remains unverified.
- **What evidence would resolve it:** Evaluations using mathematical problems framed in natural language that require reasoning without relying on shared symbolic patterns across languages.

## Limitations
- Data quality hypothesis based on single language (Swahili) and educational source comparison
- UniMax sampling effectiveness depends on specific hyperparameters that may not generalize
- Lack of analysis on token overlap between African languages and base model's tokenizer

## Confidence
- **High confidence:** Continued pre-training on strong base model outperforms training smaller models from scratch for low-resource languages
- **Medium confidence:** Educational content quality, rather than source language, drives knowledge-intensive task performance
- **Low confidence:** UniMax sampling is necessary to prevent high-resource language domination

## Next Checks
1. **Cross-language validation of data quality hypothesis:** Replicate the Swahili translation case study with at least 3 additional African languages spanning different language families using multiple educational data sources.

2. **Ablation study on UniMax parameters:** Systematically vary the epoch cap (1, 2, 4, 8) and compare resulting performance curves for both high-resource and low-resource languages.

3. **Tokenizer overlap analysis:** Quantify the token coverage of African languages in the base Llama tokenizer and measure how this coverage correlates with performance improvements across languages.