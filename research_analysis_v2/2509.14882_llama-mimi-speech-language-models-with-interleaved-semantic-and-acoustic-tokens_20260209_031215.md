---
ver: rpa2
title: 'Llama-Mimi: Speech Language Models with Interleaved Semantic and Acoustic
  Tokens'
arxiv_id: '2509.14882'
source_url: https://arxiv.org/abs/2509.14882
tags:
- speech
- audio
- tokens
- acoustic
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Llama-Mimi, a speech language model that
  uses a unified tokenizer and a single Transformer decoder to jointly model sequences
  of interleaved semantic and acoustic tokens. Unlike previous approaches that employ
  multi-stage pipelines or specialized tokenizers, Llama-Mimi adopts a simple single-decoder
  architecture that interleaves semantic and acoustic tokens within a single sequence.
---

# Llama-Mimi: Speech Language Models with Interleaved Semantic and Acoustic Tokens

## Quick Facts
- arXiv ID: 2509.14882
- Source URL: https://arxiv.org/abs/2509.14882
- Authors: Issa Sugiura; Shuhei Kurita; Yusuke Oda; Ryuichiro Higashinaka
- Reference count: 0
- Key outcome: Llama-Mimi achieves state-of-the-art acoustic consistency and maintains high speaker similarity using a unified tokenizer and single Transformer decoder to jointly model interleaved semantic and acoustic tokens.

## Executive Summary
Llama-Mimi introduces a unified approach to speech language modeling by interleaving semantic and acoustic tokens within a single sequence for a single Transformer decoder. Unlike prior multi-stage pipelines, it uses Mimi's residual vector quantization to produce semantically distilled first-level tokens and progressively finer acoustic tokens at deeper levels. The model demonstrates strong acoustic consistency and speaker similarity while maintaining competitive linguistic performance, though it faces inherent trade-offs when scaling the number of quantizers.

## Method Summary
Llama-Mimi builds on Mimi, a neural audio codec that converts waveforms into residual vector quantized tokens, with the first quantizer level semantically distilled from WavLM. The architecture interleaves semantic and acoustic tokens frame-by-frame (semantic first, then acoustic levels 2 through Q) into a flattened sequence, which a single Llama 3 decoder processes autoregressively. The model extends the vocabulary with audio tokens and special markers, and is trained on ~240k hours of English audio with next-token prediction. Inference generates interleaved tokens that are decoded back to speech using Mimi.

## Key Results
- Achieves state-of-the-art acoustic consistency scores (79.0/85.0 SALMon) compared to TWIST-1.3B (61.5/69.0)
- Maintains high speaker similarity (0.346-0.474 cosine similarity) while generating coherent speech
- Demonstrates trade-off: increasing quantizers improves acoustic fidelity but degrades linguistic performance (LLM score drops from 3.01 to 2.54 when going from Q=4 to Q=8)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Interleaving semantic and acoustic tokens within a single sequence enables a unified decoder to jointly model linguistic content and acoustic detail.
- Mechanism: Mimi produces residual vector quantized (RVQ) tokens where the first quantizer level is semantically distilled from WavLM (capturing phonetic content), while levels 2–Q encode progressively finer acoustic details. By ordering tokens within each frame as [semantic, acoustic_2, acoustic_3, ...] and flattening across time, the model predicts semantic tokens first per frame; subsequent acoustic tokens condition on both past frames and the just-predicted semantic token, grounding acoustic generation in linguistic structure.
- Core assumption: The semantic-acoustic factorization in RVQ is sufficiently disentangled that the first quantizer carries linguistic information while deeper quantizers primarily refine acoustic properties.
- Evidence anchors:
  - [Section 3] "the first-level quantizers are semantically distilled from WavLM" and "By predicting semantic tokens first and conditioning subsequent acoustic tokens on them within each frame, the model is expected to generate linguistically coherent speech with fine-grained acoustic detail."
  - [Abstract] "uses a unified tokenizer and a single Transformer decoder to jointly model sequences of interleaved semantic and acoustic tokens"
  - [Corpus] DSA-Tokenizer (arXiv:2601.09239) suggests existing tokenizers often fuse semantic and acoustic information inseparably; Assumption: Mimi's distillation approach partially addresses this but disentanglement may not be complete.
- Break condition: If semantic-acoustic coupling in the tokenizer is too tight, conditioning on the first quantizer alone may leak insufficient linguistic signal, degrading coherence—especially for longer sequences.

### Mechanism 2
- Claim: A single-decoder with full attention to all past tokens improves acoustic consistency by capturing subtle variations across the entire context.
- Mechanism: Unlike temporal-plus-depth approaches (e.g., Moshi's RQ-Transformer) where a backbone transformer sees only aggregated frame-level information, Llama-Mimi's single decoder attends to every prior token (semantic and acoustic) in the flattened sequence. This allows the model to preserve fine-grained speaker identity and acoustic consistency over long utterances.
- Core assumption: The model can effectively learn long-range dependencies across 50+ tokens/second despite quadratic attention costs and increased sequence length from interleaving.
- Evidence anchors:
  - [Section 5.1] "We attribute their strong acoustic capability to our modeling approach, which enables the model to attend to all past tokens and capture fine-grained representations of subtle acoustic variations."
  - [Table 1] Llama-Mimi achieves 79.0 and 85.0 on SALMon acoustic consistency (Sentiment, Speaker) vs. TWIST-1.3B's 61.5 and 69.0.
  - [Corpus] Corpus evidence on single-decoder vs. hierarchical architectures is limited; the neighbor papers focus on tokenizer design rather than decoder structure.
- Break condition: At very long durations (e.g., >20s), the 1024-token context limit may truncate useful history, and interleaved sequences may dilute semantic signal amid acoustic tokens, hurting long-term coherence.

### Mechanism 3
- Claim: Restricting perplexity computation to semantic tokens improves semantic benchmark performance by reducing noise from acoustic token prediction.
- Mechanism: For semantic tasks (sWUGGY, sBLIMP, sTopic-StoryCloze), the authors compute perplexity only over the first quantizer level (semantic tokens), excluding acoustic tokens. This focuses evaluation on linguistic content rather than acoustic detail, which is irrelevant to these tasks.
- Core assumption: Semantic tokens alone are sufficient proxies for linguistic knowledge in speech LMs.
- Evidence anchors:
  - [Section 5.1] "we restrict the perplexity calculation in Llama-Mimi to semantic tokens for all tasks except SALMon. In our experiments, we found that excluding acoustic tokens leads to better performance on all benchmarks except SALMon."
  - [Corpus] Flow-SLM (arXiv:2508.09350) also separates semantic and acoustic modeling, supporting the idea that decoupling evaluation by token type is principled.
- Break condition: If the semantic tokenizer discards paralinguistic information necessary for certain semantic tasks (e.g., sentiment detection from prosody), perplexity on semantic tokens alone may not fully capture model capabilities.

## Foundational Learning

- Concept: Residual Vector Quantization (RVQ)
  - Why needed here: Mimi uses RVQ to represent audio as multiple codebook levels; understanding how deeper quantizers refine reconstruction is essential for tuning Q (the number of quantizers) to balance fidelity vs. sequence length.
  - Quick check question: If you increase Q from 4 to 8, what happens to the token sequence length and what trade-off does the paper report?

- Concept: Autoregressive Next-Token Prediction with Extended Vocabulary
  - Why needed here: Llama-Mimi extends Llama 3's vocabulary with audio tokens and special markers (<audio>, </audio>); understanding how next-token prediction applies to mixed text/audio sequences is core to this architecture.
  - Quick check question: How does the model know when to stop generating audio during inference?

- Concept: Attention Masking and Causal Decoding
  - Why needed here: The single decoder must attend causally to all prior tokens in the interleaved sequence; misconfiguring attention masks would leak future information and break autoregressive validity.
  - Quick check question: In an interleaved sequence [y1_frame1, y2_frame1, y1_frame2, y2_frame2, ...], which tokens should y2_frame2 attend to?

## Architecture Onboarding

- Component map:
  Mimi Tokenizer (frozen) -> RVQ tokens (Q levels × T' frames) -> Flatten to 1D sequence -> Prepend <audio>, append </audio> -> Llama 3 Decoder (1.3B/8B) -> Next-token prediction loss -> Generate interleaved tokens -> Decode with Mimi to waveform

- Critical path:
  1. Preprocess audio with Mimi → obtain RVQ tokens (Q levels × T' frames).
  2. Flatten to 1D sequence, prepend <audio>, append </audio>.
  3. Train decoder with cross-entropy on next-token prediction (audio tokens only; text not mentioned in training data but vocabulary supports it).
  4. At inference: autoregressively sample tokens; decode RVQ tokens with Mimi to waveform; stop on </audio>.

- Design tradeoffs:
  - **Q (number of quantizers)**: Higher Q → better audio quality and speaker similarity but worse linguistic coherence (Table 4: Q=8 yields Speaker Sim. 0.474 but LLM score 2.54; Q=4 yields 0.346 and 3.01). Paper uses Q=4 as default.
  - **Sequence length**: Interleaving multiplies sequence length by Q (e.g., 4×), increasing compute and limiting context duration (max 1024 tokens ≈ 20s at 50 tokens/sec).
  - **Model size**: 8B improves semantic tasks over 1.3B but has minimal effect on acoustic tasks (Table 1, Table 3).

- Failure signatures:
  - **Acoustic drift**: Generated audio loses speaker identity mid-utterance → check if speaker similarity degrades with longer generations; may indicate context truncation or insufficient Q.
  - **Linguistic incoherence**: Generated speech becomes word salad after 10+ seconds → may indicate semantic token degradation; try reducing Q or increasing model size.
  - **Token ordering violations**: Model generates acoustic tokens before semantic tokens within a frame → sampling temperature too high; the paper notes the model "naturally follows" RVQ order empirically but doesn't enforce it.

- First 3 experiments:
  1. **Ablate Q**: Train with Q ∈ {2, 4, 8} on a small dataset split; measure (a) Audiobox-Aesthetics scores, (b) speaker similarity, (c) LLM-as-a-Judge content quality. Confirm the fidelity-coherence trade-off on your data.
  2. **Semantic-only perplexity**: For semantic benchmarks, compare perplexity computed on all tokens vs. semantic tokens only. Validate the paper's claim that exclusion improves semantic benchmark scores.
  3. **Attention scope analysis**: Visualize attention patterns on a generated sequence; verify that acoustic tokens attend strongly to their paired semantic token and to prior frames' semantic tokens. If attention is diffuse, consider adding auxiliary losses to enforce semantic conditioning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the inherent trade-off between acoustic fidelity and linguistic coherence be mitigated when scaling the number of quantizers?
- Basis in paper: [explicit] The abstract and Section 5.4 note that increasing quantizers improves acoustic metrics and speaker similarity but consistently degrades spoken content quality (linguistic performance).
- Why unresolved: The paper identifies this "acoustic-semantic" trade-off as a fundamental challenge of the unified modeling approach but does not propose architectural or training modifications to resolve it.
- What evidence would resolve it: A training objective or decoding strategy that allows for 8+ quantizers (high acoustic fidelity) without the associated drop in LLM-as-a-Judge or sBLIMP scores.

### Open Question 2
- Question: Does strictly enforcing the residual vector quantizer (RVQ) ordering constraint during generation improve linguistic consistency?
- Basis in paper: [inferred] Section 3 states the model "naturally follows" the RVQ sequence empirically, allowing generation without constraints. It is unresolved if enforcing this constraint would reduce the error accumulation affecting long-term coherence.
- Why unresolved: The authors rely on unconstrained sampling for simplicity; the specific impact of strict structural constraints on the semantic vs. acoustic balance remains untested.
- What evidence would resolve it: Ablation studies comparing the linguistic scores of unconstrained generation against generation with masked constraints enforcing the $1 \to Q$ token order.

### Open Question 3
- Question: Why does model scaling improve semantic performance while having minimal impact on acoustic tasks?
- Basis in paper: [explicit] Section 5.1 states, "We also find that model size has minimal effect on acoustic tasks but improves the performance on semantic tasks," but offers no theoretical explanation for this asymmetry.
- Why unresolved: The paper documents the scaling behavior but does not investigate if the acoustic bottleneck lies in the tokenizer capacity rather than the Transformer backbone size.
- What evidence would resolve it: Analysis of reconstruction loss vs. semantic loss across model scales to determine if acoustic tasks saturate at smaller model capacities than semantic tasks.

## Limitations

- The semantic-acoustic disentanglement quality in Mimi's first quantizer is uncertain, potentially limiting the effectiveness of the conditioning mechanism
- The vocabulary extension approach lacks specification of initialization and token distribution, which could affect model balance
- The LLM-as-a-Judge evaluation methodology introduces subjectivity and potential bias through its 1-10 scale with GPT-4o
- The acoustic benchmarks focus on consistency rather than comprehensive perceptual naturalness assessment

## Confidence

**High Confidence**: The interleaving architecture and single-decoder design are clearly specified and empirically validated. The trade-off between Q (number of quantizers) and performance across acoustic vs semantic tasks is well-documented in Table 4.

**Medium Confidence**: The conditioning mechanism (semantic tokens grounding acoustic tokens) is plausible given the architecture, but the actual degree of semantic-acoustic disentanglement in practice is uncertain without ablation studies on token purity.

**Low Confidence**: Claims about long-term coherence beyond 20 seconds are weakly supported, as the 1024-token context limit constrains evaluation. The paper doesn't address how performance degrades with longer contexts or whether the interleaved format exacerbates attention bottlenecks.

## Next Checks

1. **Token Purity Analysis**: Extract semantic tokens from ground truth audio using Mimi, then decode them back to audio without acoustic tokens. Measure how much linguistic content is preserved versus lost. This quantifies the actual semantic information content in the first quantizer.

2. **Extended Context Generation**: Generate speech beyond 20 seconds (using overlapping contexts or multi-segment generation) and measure degradation in speaker similarity and linguistic coherence. This tests whether the 1024-token limit is binding and how the model handles context transitions.

3. **Ablation on Conditioning**: Train variants where acoustic tokens condition only on previous acoustic tokens (not semantic tokens) versus the proposed semantic-first approach. Compare acoustic consistency and speaker similarity to isolate the contribution of semantic grounding to acoustic quality.