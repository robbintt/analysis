---
ver: rpa2
title: Near Optimal Convergence to Coarse Correlated Equilibrium in General-Sum Markov
  Games
arxiv_id: '2511.02157'
source_url: https://arxiv.org/abs/2511.02157
tags:
- learning
- algorithm
- logt
- games
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of convergence to coarse correlated
  equilibrium (CCE) in general-sum Markov games. The authors improve the previously
  best-known convergence rate from O(log^5 T / T) to O(log T / T), matching the best
  known rate for correlated equilibrium (CE) in terms of time horizon dependence while
  improving action space dependence from polynomial to polylogarithmic.
---

# Near Optimal Convergence to Coarse Correlated Equilibrium in General-Sum Markov Games

## Quick Facts
- **arXiv ID:** 2511.02157
- **Source URL:** https://arxiv.org/abs/2511.02157
- **Authors:** Asrin Efe Yorulmaz; Tamer Başar
- **Reference count:** 40
- **Primary result:** Improves convergence to CCE from O(log⁵ T / T) to O(log T / T) while reducing action space dependence from polynomial to polylogarithmic

## Executive Summary
This paper establishes near-optimal O(log T / T) convergence to coarse correlated equilibrium in general-sum Markov games, improving upon previous best-known rates while dramatically reducing computational complexity. The key innovation is a dynamic step-size adaptation scheme that uses optimistic follow-the-regularized-leader with time-varying learning rates, achieving exponential improvement in action space dependence compared to correlated equilibrium convergence. The approach builds on recent advances in adaptive step-size techniques for no-regret algorithms and extends them to the Markovian setting via a stage-wise scheme that adjusts learning rates based on real-time feedback.

## Method Summary
The method uses Optimistic Follow-the-Regularized-Leader (OFTRL) with dynamic learning-rate control for policy updates in general-sum Markov games. Each agent maintains value functions V_i,h(s) updated via smooth averaging with time-dependent coefficients α_t = (H+1)/(H+t), and generates policies through a roll-out procedure sampling from the history of policies. The algorithm computes learning rates λ^(t) via bi-level optimization based on optimistic regret magnitude, adapting between conservative fixed rates and more aggressive rates when learning is stable. This framework achieves O(log T / T) convergence to CCE with polylogarithmic dependence on action space size.

## Key Results
- Achieves O(log T / T) convergence rate to CCE, improving from previous O(log⁵ T / T)
- Reduces action space dependence from polynomial to polylogarithmic (O(log² |A_max|/T) vs O(|A|^{2.5}/T) for CE)
- Maintains O(|S|) space complexity through V-based updates vs O(|S|·Π|A_i|) for Q-based methods
- Numerical results confirm O(log T / T) convergence on 2-player, 2-state, 2-action Markov games

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Learning Rate Control
- **Claim:** Dynamic learning-rate control achieves O(log T / T) convergence to CCE by adapting step sizes based on real-time regret magnitude
- **Mechanism:** The algorithm computes λ^(t) via bi-level optimization: when optimistic regret R^(t) is large (volatile learning), use conservative fixed rate η; otherwise, solve argmax over λ ∈ (0,1] balancing (α-1)log λ + log Σ exp(λR^(t)[k]). This penalizes actions with excessively negative regret, limiting their influence and promoting stability
- **Core assumption:** Reward signals satisfy ||ν^(t)||_∞ ≤ H for all t, and α_t decay provides sufficient averaging stability
- **Break condition:** If reward magnitudes exceed H or α_t decay is disrupted, learning rate stability guarantees may fail, leading to unstable convergence

### Mechanism 2: RVU Inequality with Time-Varying Step Sizes
- **Claim:** RVU inequality with time-varying step sizes enables non-negative regret bounds that sum over agents to control maximum per-state regret
- **Mechanism:** Theorem 6 establishes cumulative regret satisfies Reg̃(T) ≤ [2||u^(t)||_∞ + α̃ log T + 2 log d] / η_{T+1} + Σ η_t ||u^(t) - κ^(t)u^(t-1)||²_∞ - (1/20) Σ ||x^(t+1) - x^(t)||²_1 / η_t. The negative second-order path-length term cancels positive utility variation terms, yielding net O(log T / T) regret
- **Core assumption:** All agents follow same symmetric algorithm in self-play; β ≥ 70 ensures sufficient strong convexity
- **Break condition:** If agents use heterogeneous algorithms or β < 70, the Bregman divergence lower bounds fail, breaking the RVU cancellation structure

### Mechanism 3: V-Based Value Updates with Decentralized Oracles
- **Claim:** V-based value updates with decentralized oracle feedback achieve O(|S|) space complexity while maintaining equivalence to Q-based methods
- **Mechanism:** Each player maintains V_i,h(s) updated via: V^(t)_i,h(s) = (1-α_t)V^(t-1)_i,h(s) + α_t[(r_i,h + P_h V^(t)_i,h+1)^π^(t)_h](s). This requires only E_{a_{-i}∼π_{-i,h}}[r_i(s, a_i, a_{-i})] and E_{a_{-i}∼π_{-i,h}}[P_h(·|s, a_i, a_{-i})] from oracles, avoiding exponential |A|^N joint action storage
- **Core assumption:** Decentralized reward and transition oracles provide exact expected values under opponent policies; Lemma 3 equivalence holds inductively
- **Break condition:** If oracles provide biased estimates or communication fails, the V-Q equivalence breaks; decentralized convergence guarantees require synchronized episode indexing across all agents

## Foundational Learning

- **Concept: Coarse Correlated Equilibrium (CCE)**
  - **Why needed here:** CCE is the target solution concept; unlike CE or Nash, it allows correlated joint policies where no player gains by deviating to any fixed action. The paper shows CCE achieves polylogarithmic |A| dependence vs. polynomial for CE
  - **Quick check question:** Can you explain why CCE uses a fixed-action comparator while CE uses strategy modifications, and how this affects computational tractability?

- **Concept: Regret bounded by Variation in Utilities (RVU)**
  - **Why needed here:** RVU bounds relate regret to first-order utility changes and second-order policy path lengths. The structure Reg ≤ O(1/η) + O(η)||u-variation||² - O(1/η)||policy-change||² enables cancellation when all agents use similar learning rules
  - **Quick check question:** If an RVU bound has coefficients (a, b, c) such that Reg ≤ a·log T + b·Σ||Δu||² - c·Σ||Δπ||², what relationship between b and c ensures bounded regret when utilities vary as O(1/√t)?

- **Concept: Optimistic FTRL (OFTRL)**
  - **Why needed here:** Standard FTRL uses cumulative past payoffs; OFTRL adds an optimistic prediction of the next payoff (u^(t-1) term in Line 4 of Algorithm 1). This "anticipatory" correction reduces regret when sequences are predictable (as in self-play)
  - **Quick check question:** How does the optimistic term w_t/w_{t-1} · u^(t-1) differ from simply using u^(t) if available, and why is the former implementable in online learning?

## Architecture Onboarding

- **Component map:** Episode Loop (t = 1 to T) -> Policy Update (per state s, step h, agent i) -> Value Update (backward: h = H to 1) -> Utility Accumulation -> Output: Roll-out policy π̄
- **Critical path:** The λ^(t) computation (Line 5) is the bottleneck—it requires solving a 1-D optimization over (0,1] per (s,h,i) tuple. The paper's equivalence proof shows this is actually closed-form in most cases (λ = η when max R^(t) ≥ -β log |A|)
- **Design tradeoffs:**
  - V-based vs. Q-based: V requires O(|S|) space but needs oracle queries for expectations; Q requires O(|S|·Π|A_i|) space but uses direct table lookups
  - DLRC learning rate vs. fixed η: DLRC adapts to volatility but adds hyperparameters (β, η, α̃); fixed η is simpler but requires careful tuning
  - Centralized roll-out vs. decentralized: Correlated π̄ requires shared random seed ω; fully decentralized output would need additional coordination protocol
- **Failure signatures:**
  - CCE-gap plateaus at O(1/√T): Check if α_t decay is correct (should be (H+1)/(H+t)), or if oracle bias is O(1)
  - Numerical instability in λ^(t): ||R^(t)||_∞ growing unbounded suggests reward scaling exceeds H or η_t is too large
  - Non-monotonic V^(t) updates: Weight normalization Σ_j α^t_j = 1 violated; check Lemma 9 property 1
- **First 3 experiments:**
  1. Reproduce Figure 4 on 2-player, 2-state, 2-action games: Implement Algorithms 1-2 with H=2, T=10^4, η=1/(24H√HN), β=70. Plot CCE-gap vs. T on log-log scale; slope should approach -1 (O(1/T) after log factor)
  2. Ablate dynamic learning rate: Compare DLRC (full Algorithm 1) vs. fixed λ=η vs. standard OMWU without optimism (no u^(t-1) term). Expected: DLRC achieves O(log T/T), fixed λ achieves O(T^{-3/4}) or worse, non-optimistic achieves O(1/√T)
  3. Scale action space: Test on game with |A_i|=8 vs. |A_i|=64 players. Compare CCE convergence (this paper's bound: O((log|A|)^2/T)) vs. CE baseline from Mao et al. 2024 (O(|A|^{2.5}/T)). Expect exponential gap in wall-clock time to reach ε-CCE

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can constant regret be achieved in general-sum Markov games?
- **Basis in paper:** The Conclusion states that "achieving constant regret remains a challenging open problem" despite recent advances in zero-sum settings
- **Why unresolved:** The current analysis yields a convergence rate of $O(\log T / T)$, and the structural differences between zero-sum and general-sum games prevent the direct application of existing constant-regret proofs (like those of Yang and Ma 2023)
- **What evidence would resolve it:** An algorithm and proof demonstrating that the regret remains bounded by a constant independent of $T$ for general-sum Markov games

### Open Question 2
- **Question:** Can this framework be extended to a realistic sample-based setting without oracles?
- **Basis in paper:** The Conclusion identifies moving "from the oracle setting... to a more realistic sample-based setting" as an "important extension"
- **Why unresolved:** The current algorithm (Section 2.3) assumes access to exact "reward oracles" and "transition oracles" to compute value updates, which are unavailable in model-free reinforcement learning
- **What evidence would resolve it:** A modification of the MG-DLRC-OMWU algorithm that uses stochastic gradients or estimated values while maintaining the $O(\log T / T)$ convergence rate with high probability

### Open Question 3
- **Question:** Can the polynomial dependence on the horizon $H$ in the convergence bound be improved?
- **Basis in paper:** The Conclusion lists "improving the convergence rates by enhancing the algorithm's dependence on the horizon $H$" as a future direction
- **Why unresolved:** Theorem 4 establishes a bound dependent on $H^{7/2}$, which results from the recursive accumulation of errors and the learning rate schedule over the horizon length
- **What evidence would resolve it:** A refined analysis or algorithm achieving a convergence rate with a lower polynomial degree with respect to $H$ (e.g., linear or quadratic)

## Limitations
- The theoretical guarantees rely on precise parameter relationships (β ≥ 70, η = 1/(24H√HN)) that may be difficult to tune in practice
- The decentralized oracle model assumes perfect expectation queries under opponent policies, which may not hold with sampling-based implementations
- Numerical experiments use extremely small games (2×2×2) that may not reflect scaling behavior in realistic scenarios

## Confidence
- Theorem 4 (O(log T/T) convergence): **High** - The proof structure follows established RVU techniques with rigorous parameter analysis
- Dynamic learning rate mechanism: **Medium** - Theoretical guarantees are clear, but practical implementation details (λ^(t) optimization stability) require careful handling
- Decentralized oracle implementation: **Low** - The equivalence between V and Q updates assumes exact oracle queries; sampling-based approximations could break convergence guarantees

## Next Checks
1. **Parameter sensitivity analysis:** Systematically vary β ∈ [50, 90] and η ∈ [0.1, 1.0] to identify the operational range where O(log T/T) convergence is observed versus degraded O(1/√T) rates
2. **Oracle noise robustness:** Replace exact oracle queries with ε-noisy estimates (E[r] ± ε) and measure CCE-gap degradation; determine tolerance threshold before convergence breaks
3. **Scaling experiment:** Test on 3-player games with |A_i| ∈ {4, 8, 16} to empirically verify the claimed polylogarithmic action space dependence versus baseline methods' polynomial scaling