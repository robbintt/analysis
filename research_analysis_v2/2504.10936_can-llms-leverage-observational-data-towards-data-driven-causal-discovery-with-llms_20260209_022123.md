---
ver: rpa2
title: Can LLMs Leverage Observational Data? Towards Data-Driven Causal Discovery
  with LLMs
arxiv_id: '2504.10936'
source_url: https://arxiv.org/abs/2504.10936
tags:
- causal
- data
- discovery
- observational
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether large language models (LLMs) can\
  \ leverage observational data for causal discovery, moving beyond their traditional\
  \ reliance on metadata and domain knowledge. The authors propose two prompting strategies\u2014\
  pairwise and BFS-based\u2014that integrate observational data directly into LLM\
  \ prompts for causal reasoning."
---

# Can LLMs Leverage Observational Data? Towards Data-Driven Causal Discovery with LLMs

## Quick Facts
- **arXiv ID:** 2504.10936
- **Source URL:** https://arxiv.org/abs/2504.10936
- **Reference count:** 26
- **Primary result:** LLM-based causal discovery with observational data boosts F1 scores by up to 0.11 points vs. metadata-only, and outperforms traditional methods by up to 0.52 points.

## Executive Summary
This paper investigates whether large language models can leverage observational data for causal discovery, moving beyond traditional reliance on metadata and domain knowledge. The authors propose two prompting strategies—pairwise and BFS-based—that integrate observational data directly into LLM prompts for causal reasoning. Experiments on benchmark datasets show that incorporating observational data enhances causal discovery performance, boosting F1 scores by up to 0.11 points compared to knowledge-based approaches, and outperforming traditional statistical methods by up to 0.52 points. The BFS prompting method consistently achieves the best results, demonstrating LLMs' potential to interpret structured observational data for causal inference.

## Method Summary
The method formulates causal discovery as a classification task over variable pairs, using two prompting strategies. Pairwise prompting queries the LLM on each variable pair with sampled observational data, while BFS-based prompting uses a three-stage traversal (Initialization, Expansion, Insertion) that maintains global graph context. The approach samples k=100 rows from observational data and injects them into prompts alongside variable names. The LLM (GPT-4) processes these prompts and outputs causal relationships, which are then aggregated into a DAG. The method is compared against traditional statistical baselines (PC Algorithm, GES) on BNLearn benchmark datasets (ASIA, CANCER, SURVEY).

## Key Results
- Incorporating observational data into LLM prompts boosts F1 scores by up to 0.11 points compared to metadata-only approaches
- BFS-based prompting consistently outperforms pairwise prompting by leveraging global context awareness
- LLM-based methods achieve strong performance with only 100 samples, while traditional statistical methods require 1000+ samples

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Providing sampled observational data in the prompt improves causal discovery performance compared to metadata-only approaches.
- **Mechanism:** The LLM utilizes the numerical distribution within the context window to ground its reasoning, likely identifying patterns (e.g., correlations or skews) that contradict or refine its internal parametric knowledge.
- **Core assumption:** LLMs can attend to and interpret numerical structure in context well enough to adjust priors.
- **Evidence anchors:**
  - [abstract] "...incorporating observational data enhances causal discovery, boosting F1 scores by up to 0.11 point..."
  - [page 6] "...incorporating observational data consistently enhances F1 scores... suggesting that... LLMs demonstrate a potential to effectively leverage observational numerical data as contextual grounding..."
  - [corpus] General literature confirms LLMs struggle with causality without assistance (CARE, arXiv:2511.16016), but specific corpus evidence for *raw data injection* as a mechanism is limited.
- **Break condition:** If the LLM's attention mechanism saturates with numerical tokens (lost-in-the-middle), or if the sample size $k$ is too small to represent the distribution.

### Mechanism 2
- **Claim:** BFS-based prompting outperforms pairwise prompting by maintaining global graph context.
- **Mechanism:** Instead of isolated queries, the BFS approach traverses the graph structure (Initialization → Expansion), conditioning the LLM on the history of identified relationships to inform subsequent edge decisions.
- **Core assumption:** The LLM can effectively utilize the growing prompt history (query accumulation) to enforce consistency and transitivity.
- **Evidence anchors:**
  - [page 3] Describes the three stages: Initialization, Expansion, and Insertion.
  - [page 6] "BFS-based prompting demonstrates its superiority... by offering a more contextual and structured approach... leveraging global context awareness—multi-variable interactions rather than variable pairs in isolation."
  - [corpus] Not explicitly detailed in corpus neighbors.
- **Break condition:** Excessive prompt length (token limits) as the query history grows linearly, potentially degrading reasoning quality.

### Mechanism 3
- **Claim:** LLM-based causal discovery is more sample-efficient than traditional statistical methods.
- **Mechanism:** LLMs leverage pre-trained priors to constrain the search space, whereas statistical methods (like PC/GES) require larger datasets to verify independence assumptions without prior knowledge.
- **Core assumption:** The pre-existing knowledge of the LLM is relevant and accurate enough to compensate for lower sample density.
- **Evidence anchors:**
  - [page 6] "LLM-based methods... achieve strong performance even with a limited number of samples [100]... In contrast, statistical-based methods may require as many as 1000 samples..."
  - [corpus] "Realizing LLMs' Causal Potential..." (arXiv:2510.16530) warns that high performance often stems from pretraining memorization rather than reasoning.
- **Break condition:** When the domain is novel or contradicts the LLM's pretraining data (distribution shift), rendering priors harmful.

## Foundational Learning

- **Concept: Constraint-based vs. Score-based Causal Discovery**
  - **Why needed here:** The paper compares LLMs against PC (constraint-based) and GES (score-based). Understanding that PC relies on conditional independence tests while GES relies on scoring functions is necessary to interpret the baseline results.
  - **Quick check question:** Can you explain why PC and GES might fail on small datasets ($N=100$) compared to an LLM with priors?

- **Concept: Context Window & Token Limits**
  - **Why needed here:** The paper highlights token limits as a constraint for BFS prompting (which accumulates history) and necessitates sampling data $D_s$ rather than using the full dataset.
  - **Quick check question:** If you double the number of variables in the graph, how does the prompt length change for Pairwise vs. BFS prompting?

- **Concept: Sampling Strategies**
  - **Why needed here:** The method relies on a sampling function $S(D, k)$ to fit observational data into the prompt.
  - **Quick check question:** Why might random sampling fail to represent a causal relationship that only appears in a specific cluster of the data?

## Architecture Onboarding

- **Component map:** Data Sampler -> Prompt Constructor -> LLM Reasoner -> Graph Builder
- **Critical path:** The **Prompt Constructor** is the critical bottleneck. If the formatting of the observational data (e.g., CSV style vs. JSON) is inconsistent with the LLM's training distribution, performance may drop.
- **Design tradeoffs:**
  - **Pairwise:** High cost (Quadratic queries $O(N^2)$), small context (only 2 variables), robust to token limits.
  - **BFS:** Low cost (Linear queries $O(N)$), large context (accumulates history), risk of context overflow.
- **Failure signatures:**
  - **Hallucinated Edges:** LLM asserts causal links based on semantic similarity rather than data.
  - **Cycle Generation:** BFS insertion fails to catch logical inconsistencies in long chains.
  - **Data Ignorance:** LLM outputs the same result regardless of changes in the sampled $D_s$.
- **First 3 experiments:**
  1. **Sanity Check (Data Ablation):** Run Pairwise/BFS with $D_s$ vs. No Data. Verify the reported $\sim 0.11$ F1 lift.
  2. **Sample Size Sensitivity:** Vary $k \in \{10, 50, 100, 200\}$ to find the saturation point where more data no longer improves F1 but increases latency.
  3. **Statistical vs. LLM Hybrid:** Compare "LLM + Pearson Correlation" against "LLM + Raw Data" to validate if raw data provides signal beyond simple summary statistics.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can LLM-based reasoning be effectively integrated with traditional statistical causal discovery methods to create a robust hybrid framework?
- **Basis in paper:** [explicit] The conclusion states that future work should "explore this hybrid approaches of LLM-based reasoning with statistical causal discovery."
- **Why unresolved:** The current study focuses on LLMs as standalone reasoning agents or simple comparisons against statistical baselines, rather than combining them into a unified architecture.
- **What evidence would resolve it:** A hybrid model demonstrating higher accuracy or data efficiency than purely statistical or purely LLM-based methods on standard benchmarks.

### Open Question 2
- **Question:** What are the optimal sampling strategies for selecting observational data points to maximize causal inference accuracy within limited context windows?
- **Basis in paper:** [inferred] While the authors test random sampling, they note that prompt length constraints limit samples to $k=100$, and they explicitly call for "refining... sampling selection approach" in future work.
- **Why unresolved:** The paper tested basic sampling methods (random, cluster) which showed no significant difference, leaving advanced, information-theoretic sampling approaches unexplored.
- **What evidence would resolve it:** An adaptive or active sampling method that yields higher F1 scores than random sampling at the same token cost.

### Open Question 3
- **Question:** Does the efficacy of data-driven causal discovery with LLMs generalize to larger, high-dimensional datasets and diverse model architectures?
- **Basis in paper:** [explicit] The authors list "investigating performance across multiple LLMs and extending on larger dataset" as key areas for future investigation.
- **Why unresolved:** The experiments were restricted to small datasets (e.g., 8 variables in ASIA) and a specific GPT-4 model checkpoint, leaving scalability unproven.
- **What evidence would resolve it:** Successful replication of the observed F1 score improvements on datasets with significantly more variables (e.g., >50) using various open-source LLMs.

## Limitations

- The exact prompt templates and BFS implementation details from reference [19] are underspecified, creating uncertainty about faithful reproduction
- Claims about LLMs "understanding" causal relationships from raw observational data lack definitive proof—could be pretraining memorization or metadata exploitation
- Experiments were limited to small datasets (8 variables in ASIA) and a specific GPT-4 model, leaving scalability unproven

## Confidence

- **High Confidence:** The comparative advantage over traditional statistical methods (PC, GES) is well-supported by the experimental results, particularly the sample efficiency claim showing LLMs maintain performance with 100 samples versus 1000+ for statistical methods.
- **Medium Confidence:** The mechanism by which LLMs leverage observational data is plausible but not definitively proven. While F1 improvements are demonstrated, the exact reasoning process—whether it's genuine pattern recognition in numerical data or semantic matching—remains unclear.
- **Low Confidence:** Claims about LLMs "understanding" causal relationships from raw observational data are the weakest. The paper shows correlation between data injection and performance, but doesn't rule out alternative explanations like pretraining memorization or metadata exploitation.

## Next Checks

1. **Data Ablation with Controlled Variables:** Run the exact same experiments but systematically vary only the observational data (e.g., random permutations of values, constant columns) while keeping variable names and metadata identical. This would test whether the LLM is actually processing numerical patterns versus relying on semantic cues from variable names.

2. **Hybrid Statistical-LLM Comparison:** Implement a hybrid baseline where LLMs receive summary statistics (means, correlations, distributions) instead of raw sampled data. If raw data outperforms summary statistics significantly, it would provide stronger evidence that LLMs are processing the actual numerical structure rather than just metadata.

3. **Novel Domain Transfer Test:** Evaluate the same prompting strategies on a completely novel domain with no semantic overlap to the LLM's pretraining data (e.g., fabricated variable names with no real-world meaning). If performance drops substantially compared to standard benchmarks, it would indicate the LLM is leveraging pretraining knowledge rather than genuinely reasoning from observational data.