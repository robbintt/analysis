---
ver: rpa2
title: What do Language Model Probabilities Represent? From Distribution Estimation
  to Response Prediction
arxiv_id: '2505.02072'
source_url: https://arxiv.org/abs/2505.02072
tags:
- distribution
- language
- which
- output
- probabilities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper clarifies that language model (LM) probabilities represent\
  \ different distributions depending on the task\u2014distribution estimation, response\
  \ prediction, or event modeling\u2014and that these are often conflated in the literature.\
  \ It shows that LM training stages (pre-training, instruction tuning, and alignment)\
  \ and inference strategies (na\xEFve completion, instruction-based, and explicit\
  \ probability reporting) lead to three distinct intended output distributions."
---

# What do Language Model Probabilities Represent? From Distribution Estimation to Response Prediction

## Quick Facts
- arXiv ID: 2505.02072
- Source URL: https://arxiv.org/abs/2505.02072
- Reference count: 21
- One-line primary result: Language model probabilities represent different distributions depending on task—distribution estimation, response prediction, or event modeling—and these are often conflated in the literature.

## Executive Summary
This paper clarifies that language model output probabilities encode different underlying distributions depending on the model's training stage and the inference strategy used. The authors distinguish three distinct interpretations: (1) distribution estimation from pre-training, (2) response prediction from instruction tuning, and (3) event modeling through explicit probability reporting. Many prior works misinterpret LM probabilities by assuming they all represent the same distribution, leading to false conclusions about model "errors" or calibration failures. The paper provides a formal framework mapping training stages to use cases and identifies six invalid assumptions in the literature.

## Method Summary
The paper presents a theoretical framework that maps language model training stages (pre-training, supervised fine-tuning, reinforcement learning from human feedback) to three distinct use cases for output probabilities. It defines three inference strategies: naïve completion (I1a), instruction-following completion (I2a), and explicit probability reporting (I2). The framework establishes that pretrained models estimate source distributions, SFT models optimize for response prediction, and only explicit probability reporting aligns with world event modeling. The authors analyze 21 prior works to identify common misconceptions arising from conflating these distributions.

## Key Results
- LM probabilities represent three distinct distributions depending on training stage and inference strategy
- Pretrained models estimate source distributions, SFT models optimize response prediction, RLHF models maximize reward
- Explicit probability reporting is the only method aligned with world event modeling
- Many prior works incorrectly assume these distributions should match, leading to misinterpretations
- Six specific invalid assumptions in the literature are identified and analyzed

## Why This Works (Mechanism)
The framework works because it formally distinguishes between the statistical target each model type optimizes for: pretraining maximizes log-likelihood of observed text (distribution estimation), SFT maximizes task-specific accuracy (response prediction), and explicit reporting directly models event probabilities. The distinction is necessary because each training objective shapes the model's probability space differently, and conflating them leads to systematic misinterpretations of model behavior.

## Foundational Learning
- **Distribution Estimation vs. Response Prediction**: Understanding that LMs can optimize for matching corpus statistics versus maximizing task accuracy is crucial for interpreting their outputs correctly. Quick check: Compare model outputs on biased prompts to see if they reproduce corpus frequencies or give "correct" answers.
- **Inference Strategy Impact**: Different prompting approaches (zero-shot, few-shot, explicit probability) fundamentally change what distribution the model samples from. Quick check: Run identical prompts through different inference strategies and measure distributional divergence.
- **Calibration vs. Accuracy**: Probability outputs from instructed models represent confidence in prediction, not calibrated estimates of event likelihood. Quick check: Compare predicted probabilities against empirical correctness rates.

## Architecture Onboarding
- **Component Map**: Pretraining -> Distribution Estimation -> Source Distribution; SFT -> Response Prediction -> Target Distribution; RLHF -> Reward Maximization -> Agent Distribution; Explicit Reporting -> Event Modeling -> Event Distribution
- **Critical Path**: Model training stage → Intended use case → Inference strategy → Output distribution interpretation
- **Design Tradeoffs**: Distribution estimation preserves corpus statistics but may not maximize task accuracy; response prediction optimizes for correctness but loses calibration; explicit reporting provides event modeling but is constrained in expressiveness.
- **Failure Signatures**: Treating response prediction probabilities as calibrated confidence; expecting pretrained models to give "correct" answers; conflating few-shot examples with event probability elicitation.
- **3 First Experiments**: 1) Compare pretrained vs. SFT model outputs on identical biased prompts. 2) Test calibration assumptions by comparing instructed model probabilities against correctness rates. 3) Replicate metalinguistic prompt comparisons across different inference strategies.

## Open Questions the Paper Calls Out
**Open Question 1**: How can the practical challenges of explicit probability reporting—specifically its limited ability to express complex distributions and its susceptibility to biases like option ordering—be mitigated to enable robust world modeling? The paper identifies explicit reporting as crucial for event modeling but notes it suffers from output constraints and input biases, offering no solution.

**Open Question 2**: To what extent do different training stages (SFT, RLHF) versus inference strategies (few-shot grounding) actually control the agent's choice of predictor function in unobserved outcome scenarios? The paper outlines three theoretical ways to influence predictor choice but leaves practical efficacy as an open empirical problem.

**Open Question 3**: How can evaluation benchmarks be redesigned to disentangle source distribution estimation from response prediction, preventing the misinterpretation of model "errors" that are actually correct adherence to a different distribution? While the theoretical distinction is defined, the paper does not propose specific evaluation protocols to measure these capabilities independently.

## Limitations
- No specific model checkpoints, prompting templates, or hyperparameters are provided, making faithful reproduction challenging
- The analytical framework does not address potential interactions between fine-tuning objectives and inference strategies
- No quantitative benchmarks are provided for distinguishing between distribution types in practice

## Confidence
- **High**: The theoretical distinction between distribution estimation, response prediction, and event modeling is rigorous and internally consistent
- **Medium**: The mapping of training stages to specific use cases is well-argued but may not capture all real-world model variants
- **Medium**: The identification of misconceptions in prior work is convincing, though some cited papers may have nuanced positions not fully captured

## Next Checks
1. Construct controlled experiments comparing naïve completion, instruction-formatted response, and explicit probability reporting on identical prompts across pretrained and fine-tuned models, measuring KL divergence between resulting distributions.
2. Test calibration assumptions by comparing probability outputs from instructed models against empirical correctness rates on factual questions to verify the paper's claim that these should not align.
3. Replicate the metalinguistic prompt comparison from Hu & Levy (2023) using the three inference strategies to demonstrate how different probability interpretations explain observed discrepancies.