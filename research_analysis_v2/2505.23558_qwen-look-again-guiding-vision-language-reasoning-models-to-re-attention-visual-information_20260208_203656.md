---
ver: rpa2
title: 'Qwen Look Again: Guiding Vision-Language Reasoning Models to Re-attention
  Visual Information'
arxiv_id: '2505.23558'
source_url: https://arxiv.org/abs/2505.23558
tags:
- visual
- reflection
- reasoning
- tokens
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses hallucination issues in Vision-Language Reasoning
  Models (VLRMs) that occur during extended reasoning due to decreasing visual attention.
  The authors propose Qwen-LookAgain (Qwen-LA), which uses Balanced Reflective Policy
  Optimization (BRPO) to train the model to spontaneously generate vision-text reflections,
  and introduces Visual Token COPY and Visual Token ROUTE methods to force re-attention
  to visual information during reflection.
---

# Qwen Look Again: Guiding Vision-Language Reasoning Models to Re-attention Visual Information

## Quick Facts
- arXiv ID: 2505.23558
- Source URL: https://arxiv.org/abs/2505.23558
- Authors: Xu Chu; Xinrong Chen; Guanyu Wang; Zhijie Tan; Kui Huang; Wenyu Lv; Tong Mo; Weiping Li
- Reference count: 40
- Primary result: Achieves 60.3% accuracy on MMMU while reducing CHAIRi hallucination metric from 9.4 to 3.7

## Executive Summary
This paper addresses hallucination issues in Vision-Language Reasoning Models (VLRMs) that occur during extended reasoning due to decreasing visual attention. The authors propose Qwen-LookAgain (Qwen-LA), which uses Balanced Reflective Policy Optimization (BRPO) to train the model to spontaneously generate vision-text reflections, and introduces Visual Token COPY and Visual Token ROUTE methods to force re-attention to visual information during reflection. Experiments on multiple visual QA datasets show Qwen-LA achieves state-of-the-art accuracy (e.g., 60.3% on MMMU) while significantly reducing hallucination metrics (e.g., CHAIRi reduced from 9.4 to 3.7). The approach successfully suppresses hallucinations and improves accuracy by maintaining visual attention throughout the reasoning process.

## Method Summary
The method consists of three main components: BRPO training to elicit spontaneous vision-text reflection behavior, and two visual re-attention strategies (VTC and VTR) that insert visual tokens at reflection points. The process begins with cold-start SFT on 2k samples with single reflections, followed by BRPO RL training on 10k samples to produce Qwen-Zero with multi-reflection capability. This is distilled into Qwen-Zero-40k with human verification, then SFT with VTC or VTR enabled produces the final Qwen-LA models. BRPO uses rule-based rewards for accuracy, format compliance, and reflection balance without a learned reward model. VTC copies all visual tokens to the reflection prefix, while VTR routes only the top m% visual tokens by attention weight.

## Key Results
- Qwen-LA achieves 60.3% accuracy on MMMU, outperforming baselines by 1.9-3.0%
- CHAIRi hallucination metric reduced from 9.4 to 3.7, representing 60.6% reduction
- Visual Token COPY provides highest accuracy but highest inference overhead; Visual Token ROUTE with m=50% offers good tradeoff
- Reflection balance reward successfully reduces total reflection length while maintaining reflection frequency

## Why This Works (Mechanism)

### Mechanism 1: Visual Attention Dilution During Autoregressive Generation
As VLRMs generate longer reasoning chains, attention to visual tokens decreases, correlating with increased hallucinations. Under equal-information-per-token assumption, mutual information I(y; c|x) between generated sequence y and image c scales with visual token ratio r = Lc/Ltotal. As Ly increases, r decreases, reducing visual grounding upper bound. Core assumption: Hallucinations primarily arise from visual information neglect rather than other failure modes. Evidence: Theorem 3.1 formalizes I(y; c|x) ≲ (Lc/Ltotal)·H(y|x,c) decreases as Ltotal grows. Break condition: If hallucinations persist with enforced high visual attention, neglect hypothesis may be incomplete.

### Mechanism 2: Visual Token Re-insertion Raises Visual Attention
Inserting visual tokens at reflection points increases visual token ratio, restoring attention and reducing hallucinations. VTC/VTR updates ratio r' = (Lc+k)/(Ltotal+k) > r, raising mutual information upper bound. Empirically, this lifts average attention weights during reflection. Core assumption: Attention mechanism responds to token composition changes; copied/routed tokens processed similarly to original visual tokens. Evidence: Theorem 3.2 proves r' > r when Lx > 0; Figure 9 shows increased attention weights. Break condition: If VTC/VTR increases attention but hallucinations don't decrease, attention restoration alone insufficient.

### Mechanism 3: BRPO Induces Spontaneous, Balanced Vision-Text Reflection
Rule-based RL with accuracy, format, and reflection-balance rewards trains models to self-trigger reflection at appropriate points with concise length. Reflection-balance reward 1 − |Lrtotal/Nr − λ|/λ penalizes imbalanced reflection. GRPO-style relative advantage optimization shapes policy without learned reward model. Core assumption: Base VLM has latent capability to perform useful reflection; RL primarily elicits and structures this behavior. Evidence: Reflection count increases while total reflection length decreases over training. Break condition: If BRPO-trained models produce reflections that don't correct errors, reward design may be misaligned.

## Foundational Learning

- **Autoregressive Attention Dilution**: Why needed: Core diagnosis is generation length itself degrades visual grounding; understanding helps evaluate intervention targeting. Quick check: Given fixed Lc, what happens to visual token ratio r as Ly doubles?

- **Rule-Based Reinforcement Learning (GRPO-style)**: Why needed: BRPO extends GRPO with reflection-specific rewards; understanding base RL clarifies how rewards shape policy without learned reward model. Quick check: In GRPO, how is advantage Ai computed from multiple candidate outputs?

- **Visual Token Routing vs. Copying**: Why needed: VTC and VTR offer different tradeoffs (completeness vs. selectivity); choosing depends on inference budget and task type. Quick check: If m=50% for VTR, what fraction of visual tokens are routed compared to VTC?

## Architecture Onboarding

- **Component map**: Image → Vision Encoder → Lc visual tokens; Text prompt → Lx text tokens; Autoregressive decoder produces Ly tokens including <SUMMARY>, <CAPTION>, <REASONING>, <REFLECTION>, <CONCLUSION>; BRPO Training with rule-based rewards → GRPO objective → policy update; Visual Re-attention: At <REFLECTION> start, VTC (copy all visual tokens) or VTR (route top m% by attention) inserts visual tokens into context

- **Critical path**: 1) Cold-start SFT with single-reflection data (2k examples) → 2) BRPO RL training to elicit multi-reflection behavior → produces Qwen-Zero → 3) Distill Qwen-Zero-40k with human verification → 4) SFT with VTC or VTR enabled → produces Qwen-LA-COPY or Qwen-LA-ROUTE

- **Design tradeoffs**: VTC provides higher accuracy but higher inference overhead (full visual token copy per reflection); VTR offers lower overhead (partial routing) with slightly lower accuracy; m% tunes compute vs. performance; BRPO requires no learned reward model but depends on well-designed rules

- **Failure signatures**: Hallucinations persist despite reflection → likely text-only reflection or reflection content ungrounded; reflections become excessively long or frequent → reflection-balance reward λ misconfigured; empty reflections generated → model learned to "look again" without verbalizing

- **First 3 experiments**: 1) Ablation without VTC/VTR: Train with BRPO only, measure CHAIRi and accuracy vs. base model → 2) VTC vs. VTR across m values: Sweep m ∈ {25, 50, 75, 100} for VTR, compare to VTC on MMMU and CHAIRi → 3) Reflection timing analysis: Log positions where <REFLECTION> is triggered across test set; correlate with generation length and error correction rate

## Open Questions the Paper Calls Out

- Can VTC and VTR be effectively generalized to other Vision-Language Model architectures beyond Qwen2.5-VL-7B-Instruct? The study was restricted to a single base model due to computational constraints, leaving transferability to other architectures unproven.

- Is it possible to mitigate the inference latency overhead introduced by visual token re-insertion to support real-time applications? The methods improve accuracy but increase generation time, creating a trade-off that currently excludes latency-sensitive use cases.

- How does violating the assumption of "equal token information contribution" affect the theoretical bounds on visual attention and hallucination? The theoretical proof relies on a simplification where all tokens are treated as having equal entropy contribution, an assumption that rarely holds in complex real-world tasks.

## Limitations

- The study only validated methods on Qwen2.5-VL-Instruct due to resource limitations, leaving generalizability to other VLM architectures unproven
- Both VTC and VTR increase inference overhead, potentially making them unsuitable for real-time applications
- The theoretical analysis assumes equal information contribution per token, which rarely holds in complex visual reasoning tasks

## Confidence

- **High Confidence**: VTC/VTR increases visual token ratio and improves accuracy on benchmark datasets; BRPO training successfully increases reflection frequency while reducing individual reflection length; Qwen-LA achieves state-of-the-art performance on MMMU and reduces hallucination metrics
- **Medium Confidence**: Hallucinations primarily result from visual attention dilution during extended reasoning; Visual token re-insertion restores visual grounding and reduces hallucinations; BRPO rewards effectively balance reflection quantity and quality
- **Low Confidence**: The mutual information bound accurately predicts hallucination rates; Reflection timing is universally optimal; Human-verified distillation preserves all reasoning improvements

## Next Checks

1. **Ablation on Attention vs. Reasoning**: Train a variant that maintains high visual attention through VTC but generates text-only reflections. If hallucinations persist, attention restoration alone is insufficient and reasoning quality must be addressed separately.

2. **Cross-Domain Reflection Analysis**: Test Qwen-LA on non-visual QA tasks (e.g., text-only reasoning, scientific question answering) to validate whether the reflection-balance reward and BRPO approach transfer beyond vision-language tasks.

3. **Reflection Content Grounding**: Implement a verifier that checks whether reflection content references specific visual elements from the image. Measure the correlation between visual grounding in reflections and hallucination reduction to confirm the visual re-attention mechanism works as intended.