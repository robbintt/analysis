---
ver: rpa2
title: On the sample complexity of semi-supervised multi-objective learning
arxiv_id: '2508.17152'
source_url: https://arxiv.org/abs/2508.17152
tags:
- learning
- bound
- lemma
- data
- proof
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the sample complexity of learning optimal trade-offs
  in multi-objective learning (MOL) when both labeled and unlabeled data are available.
  The authors show that for general losses, the label complexity is determined solely
  by the complexity of the function class G containing optimal trade-offs, even with
  infinite unlabeled data.
---

# On the sample complexity of semi-supervised multi-objective learning

## Quick Facts
- **arXiv ID:** 2508.17152
- **Source URL:** https://arxiv.org/abs/2508.17152
- **Reference count:** 40
- **Primary result:** For semi-supervised multi-objective learning, Bregman losses enable unlabeled data to reduce label complexity by allowing transfer of statistical requirements from large model classes to smaller task-specific ones.

## Executive Summary
This paper studies the sample complexity of learning optimal trade-offs in multi-objective learning when both labeled and unlabeled data are available. The authors show that for general losses, the label complexity is determined solely by the complexity of the function class G containing optimal trade-offs, even with infinite unlabeled data. However, for Bregman losses, a simple pseudo-labeling algorithm can significantly reduce label complexity by leveraging unlabeled data. The key results include: 1) a hardness result showing the necessity of Bregman losses for unlabeled data benefits, 2) uniform and localized learning bounds for the pseudo-labeling algorithm, and 3) examples demonstrating the practical benefits. The analysis reveals that unlabeled data helps by allowing the learner to estimate the relative importance of each instance to different tasks, which can be completely independent of the labels.

## Method Summary
The paper studies semi-supervised multi-objective learning (S-MOL) where the goal is to learn models g∈G achieving good trade-offs across K tasks by minimizing scalarized excess risks. The proposed PL-MOL algorithm has two steps: (1) compute ERMs ĥ_k = argmin_{h∈H_k} R̂_k(h) using labeled data for each task k, and (2) for each scalarization s∈S, compute ĝ_s = argmin_{g∈G} d̂_s(g;ĥ) where d̂_s uses pseudo-labels from ĥ_k on unlabeled data. The key insight is that for Bregman losses, the excess risk can be expressed as a function of the discrepancy to the Bayes optimal model, allowing step 2 to be performed without ground-truth labels.

## Key Results
- For general losses, label complexity is determined solely by the complexity of G, even with infinite unlabeled data
- For Bregman losses, PL-MOL achieves label complexity dependent only on the complexity of smaller task-specific classes H_k
- Localized analysis provides faster rates that adapt to the variance of specific learning problems
- Errors in initial task-specific models propagate quadratically to final trade-off solutions, preventing blow-up of labeled sample complexity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Pseudo-labeling with Bregman losses enables the transfer of statistical requirements from a large, expressive model class to a smaller, task-specific one.
- **Mechanism:** The algorithm first solves K individual tasks using labeled data in smaller classes H_k to obtain models ĥ_k. It then minimizes the scalarized empirical risk discrepancy over the larger class G using unlabeled data and pseudo-labels from ĥ_k. Because Bregman losses allow expressing excess risk as a function of the discrepancy to the Bayes optimal model, this step can be performed without ground-truth labels.
- **Core assumption:** The loss function must be a Bregman divergence (e.g., square loss, cross-entropy).
- **Evidence anchors:** [abstract] "For Bregman losses... complexity of G may come into play only in terms of unlabeled data." [section 3.2] Lemma 1 establishes that for Bregman losses, excess risk E_k(f) equals the discrepancy d_k(f; f^*_k).
- **Break condition:** The mechanism fails for non-Bregman losses like the 0-1 loss, where the algorithm can be inconsistent (Section 4.3, Eq. 17).

### Mechanism 2
- **Claim:** Localized analysis provides "fast rates" that adapt to the variance of the specific learning problem rather than the worst-case complexity.
- **Mechanism:** Theorem 2 uses local Rademacher complexities centered around the true Pareto set. This restricts the effective search space of the large model class G to a small neighborhood of optimal solutions, significantly reducing the unlabeled sample complexity compared to uniform bounds.
- **Core assumption:** Strong convexity and smoothness of the loss potentials and the star-shaped/convex nature of the function classes.
- **Evidence anchors:** [section 4.2] "Theorem 2 yields much better rates than Theorem 1... whenever the critical radii are (much) smaller than the global Rademacher complexities."
- **Break condition:** If the true Pareto solutions lie in a "high complexity" region of G, the local radius increases.

### Mechanism 3
- **Claim:** Errors in the initial task-specific models ĥ_k propagate quadratically to the final trade-off solutions, preventing a blow-up of labeled sample complexity.
- **Mechanism:** Under strong convexity, the mapping from task models to the optimal trade-off model is stable. Proposition D.1 proves that the distance between the true trade-off solution and the solution derived from estimated models scales with the square of the estimation error, avoiding a slower square-root dependence.
- **Core assumption:** The map g → d_s(g; h) must be strongly convex, which holds for linear scalarizations with Bregman losses.
- **Evidence anchors:** [appendix D.4.6] Proposition D.1: "Quadratic stability of minimizers... ||g_h^s - g_{h'}^s||_s^2 ≤ C_{st} Σ λ_k ||h_k - h'_k||_k^2."
- **Break condition:** If the scalarization or loss lacks strong convexity, the dependence may degrade to a linear or square-root relationship.

## Foundational Learning

- **Concept: Bregman Divergences**
  - **Why needed here:** These are the only losses for which the paper guarantees the semi-supervised benefit. You must recognize them to know if the theory applies to your loss function.
  - **Quick check question:** Does your loss function decompose such that the excess risk depends only on the distance between the predictor and the Bayes optimal predictor in expectation?

- **Concept: Rademacher Complexity**
  - **Why needed here:** This is the measure of "richness" or capacity used to bound generalization error. The paper's bounds are expressed explicitly in terms of the global and local Rademacher complexities of H_k and G.
  - **Quick check question:** Can you estimate the complexity of your hypothesis class to determine the sample sizes n_k and N_k?

- **Concept: Pareto Optimality & Scalarization**
  - **Why needed here:** The goal is to learn the set of Pareto optimal models. Scalarization converts the multi-objective problem into a family of single-objective problems that the algorithm solves.
  - **Quick check question:** Are you trying to find a single trade-off or the entire Pareto front? The algorithm handles a family of scalarizations S.

## Architecture Onboarding

- **Component map:** Labeled Data Paths (K parallel paths) → Aggregation/Pseudo-Labeling → Unlabeled Data Path → Final trade-off solution

- **Critical path:** The accuracy of the task-specific ERMs ĥ_k is the critical statistical bottleneck. If the labeled sample size n_k is too small to learn H_k well, the pseudo-labels will be noisy, degrading the final trade-off solution ĝ_s.

- **Design tradeoffs:**
  - **Model Class Size:** H_k should be just large enough to contain the Bayes optimal predictors for individual tasks (low bias). G should be large enough to express complex trade-offs unavailable in H_k (low multi-objective bias).
  - **Sample Allocation:** Theorem 1 suggests a trade-off between labeled data n_k (driving H_k complexity) and unlabeled data N_k (driving G complexity).

- **Failure signatures:**
  - **Inconsistency:** Using non-Bregman losses results in the algorithm failing to converge to the true Pareto front even with infinite data.
  - **High Variance:** If G is excessively large and N_k is small, the second step fails to localize, leading to poor trade-offs.
  - **High Bias:** If H_k is too small to model individual tasks, the pseudo-labels are fundamentally biased, preventing convergence.

- **First 3 experiments:**
  1. **Loss Ablation:** Compare performance using Bregman losses (Cross-Entropy) vs. non-Bregman losses (0-1 loss) on a multi-task classification problem.
  2. **Sample Efficiency Curves:** Plot excess s-trade-off against labeled sample size n and unlabeled sample size N to verify the theoretical rates.
  3. **Model Capacity Study:** Vary the complexity of G while keeping H_k fixed to demonstrate that unlabeled data absorbs the complexity cost of the larger class.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the localized bounds of Theorem 2 be extended to non-linear scalarizations (e.g., Tchebycheff) without relying on the linearity used in the current proofs?
- **Basis in paper:** [explicit] Section 4.3 states, "Generalizing Theorem 2 to other scalarizations... would be an interesting research direction that requires substantially different tools" because the current proof relies on linearity for Lemma D.6 and Proposition D.2.
- **Why unresolved:** The current Hilbert space geometry and gradient definitions depend on the linear structure induced by linear scalarizations.
- **What evidence would resolve it:** A proof of localized bounds for non-linear scalarizations or a demonstration that no such improvement is possible in those settings.

### Open Question 2
- **Question:** Is the norm equivalence condition (Assumption 3) necessary to achieve the adaptive, localized rates for all scalarizations simultaneously?
- **Basis in paper:** [explicit] Section 7 states, "Future work may investigate... whether Assumption 3 is really necessary," noting a tension between controlling error for all scalarizations and local adaptivity.
- **Why unresolved:** The current proof uses the norm equivalence to bound random critical radii via Proposition D.3, but the authors suspect a different proof technique might relax this constraint.
- **What evidence would resolve it:** A modified proof that achieves the adaptive bounds without Assumption 3, or a lower bound construction showing the assumption is unavoidable for simultaneous adaptivity.

### Open Question 3
- **Question:** Can we precisely characterize the set of loss functions beyond Bregman divergences for which unlabeled data provably reduces the label complexity of S-MOL?
- **Basis in paper:** [explicit] Section 4.3 notes it is "interesting to determine for exactly which losses the semi-supervised setting can improve upon the supervised one," given that the decomposition property is specific to Bregman losses.
- **Why unresolved:** The paper provides a hardness result for general losses and a success case for Bregman, but the boundary between these cases is not fully mapped.
- **What evidence would resolve it:** A theorem identifying necessary and sufficient structural conditions on a loss function that permit the semi-supervised benefit.

## Limitations
- The semi-supervised benefit is conditional on using Bregman losses; the algorithm can be inconsistent for non-Bregman losses like 0-1 loss
- Theoretical bounds are asymptotic and don't account for optimization errors or computational costs
- Localized analysis requires strong convexity and star-shaped class assumptions that may not hold in practice
- The analysis doesn't address how to optimally allocate samples between labeled and unlabeled data

## Confidence

- **High Confidence:** The hardness result (Section 4.1) showing Bregman losses are necessary for unlabeled data benefits is mathematically rigorous
- **Medium Confidence:** The uniform bounds (Theorem 1) are sound but conservative; the localized bounds (Theorem 2) provide tighter rates but rely on stronger assumptions
- **Medium Confidence:** The experimental examples validate the theory but are limited to synthetic settings

## Next Checks

1. **Loss Function Sensitivity:** Implement PL-MOL with non-Bregman losses (e.g., 0-1 loss, hinge loss) on a simple multi-task problem to verify the inconsistency result from Section 4.3.

2. **Local vs. Global Rademacher Comparison:** Generate synthetic data with varying task complexities and compute both the global and local Rademacher complexities of G to verify that the critical radius in Theorem 2 is indeed smaller than the global complexity bound.

3. **Optimization Error Analysis:** Implement PL-MOL with inexact optimization (limited gradient steps) and measure how optimization error propagates to the final trade-off solution, comparing against the theoretical bounds that assume exact minimization.