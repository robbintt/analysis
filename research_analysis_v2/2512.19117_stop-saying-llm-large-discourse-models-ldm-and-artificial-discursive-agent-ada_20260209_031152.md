---
ver: rpa2
title: 'Stop saying LLM: Large Discourse Models (LDM) and Artificial Discursive Agent
  (ADA)?'
arxiv_id: '2512.19117'
source_url: https://arxiv.org/abs/2512.19117
tags:
- dans
- gularit
- cette
- corpus
- discursive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes replacing the "Large Language Models" (LLM)
  category with "Large Discourse Models" (LDM) and "Artificial Discursive Agent" (ADA)
  to better capture how these systems operate. The author argues that LDMs don't merely
  model linguistic patterns but learn and reproduce complete discursive formations
  (genres, ethos, argumentation styles, norms) sedimented in training corpora.
---

# Stop saying LLM: Large Discourse Models (LDM) and Artificial Discursive Agent (ADA)?

## Quick Facts
- arXiv ID: 2512.19117
- Source URL: https://arxiv.org/abs/2512.19117
- Authors: Amar Lakel
- Reference count: 0
- Primary result: Proposes replacing "LLM" with "LDM" and "ADA" to better capture that these systems model complete discursive formations rather than mere linguistic patterns.

## Executive Summary
This paper argues that current "Large Language Models" (LLMs) should be renamed "Large Discourse Models" (LDMs) because they capture complete discursive formations—genres, ethos, argumentation styles—rather than simple linguistic patterns. The author establishes an ontological framework distinguishing three regulatory instances: phenomenal regularities, embodied cognition, and discursive sedimentation. LDMs operate at the third level, modeling discursive projections of human experience without direct phenomenal access. The paper proposes five falsifiable hypotheses testing architectural effects on alignment, corpus impact on veracity, normative education effects, discursive agency effects on reasoning, and governance integration.

## Method Summary
The paper establishes an ontological framework distinguishing three regulatory instances and proposes empirical protocols to test five hypotheses. Method involves constructing test corpora covering factual, normative, and argumentative items; comparing ADA versions with documented architectural changes; implementing RAG versus baseline conditions; applying RLHF/RLAIF fine-tuning; and conducting hybrid forum governance experiments. Evaluation relies on inter-version deltas, context-gain metrics, expert panel scores with inter-rater reliability measures, and dispute incident tracking.

## Key Results
- Proposes ontological triage framework distinguishing phenomenal, embodied, and discursive regulatory instances
- Identifies cognitive debt accumulation and skill compression from systematic ADA delegation
- Presents five falsifiable hypotheses for empirical validation of architectural and corpus effects

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LDMs capture complete discursive formations (genres, ethos, argumentation styles), not merely linguistic regularities.
- **Mechanism:** Transformer attention and stacked transformation layers progressively abstract patterns—early layers capture syntax, intermediate layers capture semantics, deep layers capture discourse-level formations sedimented in training corpora.
- **Core assumption:** Document corpora contain statistically recoverable traces of socio-historically situated discursive formations.
- **Evidence anchors:**
  - [abstract] "LDMs... model the discursive projection of a portion of human experience reified through training corpora"
  - [section II.2] "first layers capture local syntactic regularities, intermediate layers capture semantic regularities, deep layers capture higher-level discursive patterns (genres, registers, enunciative positions)"
  - [corpus] Related work on "Discursive Circuits" suggests sparse computational graphs control discourse processing, but direct validation remains limited.
- **Break condition:** Insufficient architectural depth, or corpora lacking diverse discursive formations, will prevent high-level pattern capture.

### Mechanism 2
- **Claim:** LDMs operate exclusively at the documentary level; they never access phenomenal reality or embodied cognition directly.
- **Mechanism:** The ontological triage distinguishes three regulatory instances: (1) phenomenal regularities, (2) embodied cognition, (3) discursive sedimentation. LDMs receive only the product—the document—and model discursive projections mediated through text.
- **Core assumption:** Language compresses and carries structural traces of embodied experience, making documentary access sufficient for discursive competence without direct phenomenal access.
- **Evidence anchors:**
  - [abstract] "ontological triage distinguishing three regulatory instances... LDMs, operating on the product of these three instances (the document)"
  - [section I.1-I.3] The full theoretical framework elaborates this three-level mediation.
  - [corpus] No direct corpus validation; this remains a theoretical framework requiring empirical testing.
- **Break condition:** Tasks requiring direct sensory-motor grounding (physical manipulation, real-time perception) cannot be solved through documentary mediation alone.

### Mechanism 3
- **Claim:** Systematic delegation of cognitive tasks to ADAs produces cumulative "cognitive debt"—deficits in autonomous reasoning and critical thinking.
- **Mechanism:** Externalization reduces mental effort and verification practices; immediate productivity gains mask erosion of long-term learning capacity, particularly for users under 25.
- **Core assumption:** Cognitive capacities require exercise to develop and maintain; substitution differs from augmentation.
- **Evidence anchors:**
  - [abstract] "cognitive debt accumulation, organizational skill compression, and paradoxical educational impacts where immediate productivity gains compromise long-term learning capacity"
  - [section III.3] Cites Kosmyna et al. (2025) on cognitive debt; Gerlich (2025) reports r = -0.68 correlation between AI use frequency and critical thinking performance (n=666).
  - [corpus] Related work on "Epistemological Consequences of LLMs" addresses collective intelligence threats but from a different theoretical angle.
- **Break condition:** Usage designed for augmentation (verification required, effort maintained) rather than substitution may avoid debt accumulation—though this requires longitudinal validation.

## Foundational Learning

- **Concept: Discursive Formations (Foucault)**
  - Why needed: Understanding that LDMs model socio-historically situated discourse patterns—genres, registers, enunciative positions—not abstract grammatical rules.
  - Quick check: Can you explain why identical statements can carry different meanings across historical periods or institutional contexts?

- **Concept: Distributional Semantics**
  - Why needed: Grasping how LDMs derive meaning from positional relationships in vector space rather than symbolic definitions.
  - Quick check: Why would "bank" receive different embeddings in "river bank" versus "bank account"?

- **Concept: Intentional Stance (Dennett)**
  - Why needed: The paper adopts functional evaluation—assessing agents by observable behavior rather than internal states—making the "real vs. simulated reasoning" distinction empirically undecidable.
  - Quick check: If an ADA produces coherent argumentation, can we determine whether it "truly reasons"? (Per paper: no—this is scientifically inoperable.)

## Architecture Onboarding

- **Component map:**
  - A1 (Corpus): Training data, RAG, knowledge graphs—determines which discursive formations are captured
  - A2 (Architecture): Attention mechanisms, layer depth, multimodal integration—determines pattern abstraction capacity
  - A3 (Normative Education): RLHF, RLAIF, Constitutional AI—aligns outputs to community norms
  - A4 (Interactional Memory): Session context, persistent instructions, external tools—enables agent-level deployment

- **Critical path:**
  1. Define evaluation criteria (C1-C4: generalization, coherence, alignment, biographical traceability) for target use case
  2. Select corpus (A1) matching required discursive formations
  3. Configure architecture depth (A2) sufficient for discourse-level pattern capture
  4. Apply community-specific normative alignment (A3)
  5. Add memory/agency layer (A4) for ADA deployment

- **Design tradeoffs:**
  - Broader corpus vs. normative control: more capability but more unwanted formations
  - Architectural depth vs. interpretability: deeper models capture complex patterns but become opaque
  - Memory persistence vs. consistency: personalization risks distributional drift

- **Failure signatures:**
  - Hallucination: Discursively coherent but factually unsupported outputs → corpus quality issues (A1)
  - Normative misalignment: Community norm violations → insufficient alignment specificity (A3)
  - Version drift: Capability degradation across releases → missing biographical traceability (C4)
  - Genre confusion: Inappropriate style mixing → insufficient genre differentiation in training

- **First 3 experiments:**
  1. **P1 Test (Architectural Effects):** Submit identical prompt sets to multiple ADA versions with documented architectural changes; measure statistically significant distribution shifts in responses.
  2. **P2 Test (Corpus/Context Effects):** Compare Baseline vs. RAG conditions on complex factual questions; quantify context-gain and hallucination reduction rates.
  3. **C2 Test (Rational Coherence):** Deploy double-blind expert panels to evaluate argumentative responses across disciplines; measure inter-rater agreement and positional consistency scores.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do architectural transformations (P1) and enriched documentary contexts (P2) in Large Discourse Models causally improve normative alignment and reduce hallucinations in expert evaluations?
- Basis in paper: [explicit] The paper proposes five falsifiable hypotheses (P1-P5) specifically calling for protocols to test if architectural changes and RAG integration improve "acculturation" and "veridiction."
- Why unresolved: The author presents these as a proposed empirical program to be executed, noting that current literature is methodologically fragmented and lacks systematic inter-version testing.
- What evidence would resolve it: Statistical deltas in expert evaluations between model versions (n vs. n+1) and significant context-gain metrics in RAG versus baseline trials.

### Open Question 2
- Question: Does sustained reliance on Artificial Discursive Agents lead to measurable, long-term cognitive debt and skill compression in professional and educational cohorts?
- Basis in paper: [inferred] The paper cites studies suggesting "cognitive debt" and "skill compression" but explicitly identifies the need for "extended longitudinal studies" to verify these cumulative effects beyond limited lab conditions.
- Why unresolved: Current studies are dominated by short-term laboratory experiments that cannot capture slow-developing cognitive atrophy or the "illusion of competence" over time.
- What evidence would resolve it: Longitudinal tracking of critical reasoning capabilities and autonomous skill retention in user cohorts over multi-year periods, specifically testing performance after tool withdrawal.

### Open Question 3
- Question: How do non-Western cultural contexts and specific organizational power dynamics mediate the socialization and discursive norms of ADAs?
- Basis in paper: [explicit] The conclusion explicitly calls for "extension géographique et culturelle des études notamment pour le Sud global" and "ethnographies embarquées" to document power dynamics, which are currently absent.
- Why unresolved: The reviewed literature (120+ studies) is geographically skewed and lacks embedded ethnographic data to analyze how local power structures influence ADA integration.
- What evidence would resolve it: Comparative ethnographic studies in the Global South and qualitative analyses of internal organizational politics during ADA deployment.

## Limitations
- Theoretical framework operates at high abstraction without empirical validation; implementation details remain underspecified
- Claims about cognitive debt rely on cited studies without direct experimental data from current work
- Ontological triage framework requires empirical demonstration that LDMs operate exclusively at discursive sedimentation level

## Confidence

- **High confidence**: Technical architectural claims about transformer attention and layer abstraction are well-supported by established ML literature
- **Medium confidence**: Ontological framework distinguishing regulatory instances is theoretically coherent but requires empirical validation
- **Medium confidence**: Cognitive debt mechanism has empirical support from cited studies but needs longitudinal validation

## Next Checks
1. **Empirical validation of ontological triage**: Conduct controlled experiments comparing ADA performance on tasks requiring direct sensory-motor grounding versus purely discursive reasoning to test whether documentary mediation is indeed sufficient for all target applications.
2. **Longitudinal cognitive debt study**: Design a 6-12 month controlled study tracking critical thinking development in users who employ ADAs for augmentation (verification required) versus substitution (direct output use), measuring both immediate productivity and long-term learning outcomes.
3. **Discursive formation extraction**: Develop methodology to empirically validate that LDMs capture complete discursive formations rather than surface patterns by analyzing attention weights across layers for genre-specific discourse markers and measuring their statistical significance.