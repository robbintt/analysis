---
ver: rpa2
title: 'Reward Prediction Error Prioritisation in Experience Replay: The RPE-PER Method'
arxiv_id: '2501.18093'
source_url: https://arxiv.org/abs/2501.18093
tags:
- learning
- rpe-per
- reward
- replay
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RPE-PER, a novel experience replay method
  that prioritizes transitions based on reward prediction errors (RPE) rather than
  traditional TD error. The method employs an Enhanced Model Critic Network (EMCN)
  that predicts rewards, Q-values, and next states to better model the environment.
---

# Reward Prediction Error Prioritisation in Experience Replay: The RPE-PER Method

## Quick Facts
- arXiv ID: 2501.18093
- Source URL: https://arxiv.org/abs/2501.18093
- Reference count: 8
- Key outcome: RPE-PER improves learning speed and performance on MuJoCo tasks compared to TD error-based prioritization

## Executive Summary
This paper introduces RPE-PER, a novel experience replay method that prioritizes transitions based on reward prediction errors (RPE) rather than traditional TD error. The method employs an Enhanced Model Critic Network (EMCN) that predicts rewards, Q-values, and next states to better model the environment. RPE is calculated as the discrepancy between predicted and actual rewards, serving as the priority score for sampling experiences from the replay buffer. Experimental evaluations across six MuJoCo continuous control tasks demonstrate RPE-PER's effectiveness in improving learning speed and performance compared to baseline methods when integrated with TD3 and SAC algorithms.

## Method Summary
RPE-PER replaces standard TD error prioritization with reward prediction error computed from an Enhanced Model Critic Network (EMCN). The EMCN predicts Q-values, rewards, and next states, and the absolute difference between predicted and actual rewards (RPE) serves as the priority score. Transitions are stored in a prioritized replay buffer with sampling probability proportional to p_i^α, where p_i = |RPE_i| + ε. The method is integrated with TD3 and SAC algorithms, using a combined loss function that includes Q-value, reward prediction, and next-state prediction components. The approach draws inspiration from biological learning mechanisms and shows particular strength in complex environments with high-dimensional action spaces.

## Key Results
- RPE-PER outperforms TD error-based PER and random sampling baselines on all six MuJoCo tasks tested
- Learning speed improvements are particularly pronounced in complex environments like Ant and Humanoid
- The method demonstrates effectiveness when integrated with both TD3 and SAC algorithms
- Reward prediction error serves as a more effective prioritization signal than TD error in continuous control tasks

## Why This Works (Mechanism)
RPE-PER works by using reward prediction error as a more direct signal for learning progress compared to TD error. While TD error captures the discrepancy in value estimates, RPE directly measures the model's ability to predict the most immediate feedback from the environment. This creates a more stable and informative prioritization signal that better captures surprising or informative transitions. The Enhanced Model Critic Network provides richer environmental modeling by predicting not just Q-values but also rewards and next states, allowing the system to better understand the dynamics of the environment and make more informed prioritization decisions.

## Foundational Learning
- **Prioritized Experience Replay**: A technique that samples more important transitions more frequently based on a priority metric. Needed because standard uniform sampling is inefficient for learning. Quick check: Verify that priority distribution is non-uniform in the replay buffer.
- **Reward Prediction Error**: The difference between predicted and actual rewards. Serves as the prioritization signal in RPE-PER. Quick check: Monitor that RPE values decrease over training, indicating improved reward prediction.
- **Multi-head Neural Networks**: The EMCN uses separate heads for Q-value, reward, and next-state prediction. Allows the network to learn multiple related tasks simultaneously. Quick check: Verify each head's loss decreases independently during training.
- **TD3/SAC Algorithms**: State-of-the-art off-policy RL algorithms that RPE-PER integrates with. Provide the underlying learning framework. Quick check: Ensure baseline TD3/SAC implementations work before adding RPE-PER.
- **Sum-tree Data Structure**: Used for efficient O(log N) priority sampling from the replay buffer. Enables practical implementation of prioritized replay. Quick check: Verify sampling probabilities match theoretical priorities.
- **Importance Sampling**: Corrects for the bias introduced by non-uniform sampling through weight adjustment. Critical for maintaining unbiased gradient estimates. Quick check: Verify IS weights are applied in the gradient updates.

## Architecture Onboarding
Component map: State -> EMCN (shared trunk -> Q-head, reward-head, next-state-head) -> Prioritized Buffer -> Sampler -> TD3/SAC Update
Critical path: Experience generation → EMCN prediction → RPE calculation → Priority assignment → Prioritized sampling → Update
Design tradeoffs: The three prediction heads create additional computational overhead but provide richer environmental modeling. The reward prediction head specifically adds minimal overhead while providing significant learning benefits.
Failure signatures: 
- If reward prediction diverges, all experiences get high priority
- If next-state prediction dominates, the EMCN focuses on state prediction over value estimation
- If priorities become uniform, the method degenerates to random sampling
First experiments:
1. Train EMCN alone on a simple task to verify reward prediction capability
2. Compare RPE distributions between random and informed samples
3. Run ablation study removing each EMCN head to measure individual contributions

## Open Questions the Paper Calls Out
None

## Limitations
- Hyperparameters ξ_1, ξ_2, ξ_3 for the combined loss function are not specified, making exact reproduction difficult
- No ablation studies showing individual contributions of the three EMCN prediction heads
- Architectural details for EMCN are incomplete, particularly regarding parameter sharing between heads

## Confidence
High confidence: The theoretical framework for using reward prediction error as a priority metric is sound and well-motivated by biological learning principles.
Medium confidence: The experimental results showing RPE-PER outperforming baselines, though the exact magnitude of improvement may vary with different hyperparameter choices.
Low confidence: The specific architectural choices and hyperparameter values that led to the reported performance, as these are not fully specified.

## Next Checks
1. Implement the complete ablation study comparing RPE-PER against TD error-based PER, PER with random reward prediction, and standard random sampling to isolate the effect of RPE prioritization.
2. Conduct sensitivity analysis across a range of ξ_2 values to verify the paper's claim that "increasing ξ_2 improves performance" and identify optimal weighting.
3. Test RPE-PER on additional continuous control tasks (e.g., from PyBullet or DeepMind Control Suite) to evaluate generalization beyond the six MuJoCo tasks presented.