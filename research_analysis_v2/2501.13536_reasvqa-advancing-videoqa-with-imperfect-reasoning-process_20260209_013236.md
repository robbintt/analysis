---
ver: rpa2
title: 'ReasVQA: Advancing VideoQA with Imperfect Reasoning Process'
arxiv_id: '2501.13536'
source_url: https://arxiv.org/abs/2501.13536
tags:
- reasoning
- video
- arxiv
- videoqa
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ReasVQA introduces a three-phase method to enhance VideoQA performance
  using reasoning processes from multimodal large language models. The method includes
  reasoning generation via SOTA MLLMs, reasoning refinement to remove final answers
  and preserve logical steps, and multi-task learning that jointly trains on both
  video question answering and reasoning generation.
---

# ReasVQA: Advancing VideoQA with Imperfect Reasoning Process

## Quick Facts
- arXiv ID: 2501.13536
- Source URL: https://arxiv.org/abs/2501.13536
- Reference count: 16
- Primary result: New state-of-the-art on NExT-QA (+2.9 accuracy), STAR (+7.3), and IntentQA (+5.9)

## Executive Summary
ReasVQA introduces a three-phase method to enhance VideoQA performance using reasoning processes from multimodal large language models. The approach includes reasoning generation via state-of-the-art MLLMs, reasoning refinement to remove final answers and preserve logical steps, and multi-task learning that jointly trains on both video question answering and reasoning generation. Experiments across three benchmarks show new state-of-the-art results, with ablation studies confirming that reasoning refinement and multi-task learning are key contributors to performance gains.

## Method Summary
ReasVQA is a three-phase framework that leverages reasoning from large MLLMs to improve VideoQA performance. First, an MLLM (e.g., InternVL-26B) generates reasoning chains for video-question pairs. Second, reasoning refinement removes final answer sentences while preserving intermediate logical steps, converting noisy supervision into visual context augmentation. Third, multi-task learning jointly optimizes for both answer prediction and reasoning generation, allowing the model to learn from imperfect reasoning data. The method scales across model sizes and architectures, showing consistent improvements even when reasoning data contains errors.

## Key Results
- Achieves new state-of-the-art accuracy on NExT-QA (+2.9), STAR (+7.3), and IntentQA (+5.9)
- Multi-task learning significantly outperforms single-task learning (75.7% vs 71.6% on STAR)
- Reasoning refinement proves essential - using unrefined data degrades performance
- Method works across different MLLM sizes, though larger models provide better reasoning quality

## Why This Works (Mechanism)

### Mechanism 1: Decoupling Visual Logic from Prediction Errors
The refinement phase strips final answers from MLLM reasoning chains, filtering out factual errors while retaining valuable visual grounding. This converts "noisy supervision" into "visual context augmentation" by preserving intermediate steps that correctly describe video content even when the final answer is wrong.

### Mechanism 2: Auxiliary Task Regularization via Multi-Task Learning
Joint training on QA and Reasoning Generation allows the model to extract useful features from the reasoning task without being forced to treat synthetic reasoning as a strict logical dependency for the final answer. This mitigates the negative impact of imperfect reasoning data better than single-task approaches.

### Mechanism 3: Knowledge Distillation with Noise Tolerance
Smaller VideoQA models effectively distill reasoning capabilities from larger MLLMs when the training objective allows weighted tolerance of imperfect data. The aggregate statistical signal from reasoning chains helps the student model learn to attend to relevant temporal and causal features.

## Foundational Learning

- **Concept:** Multi-Task Learning (MTL)
  - **Why needed here:** The paper's core contribution is an MTL framework that separates QA loss from Reasoning loss, with specific balancing of weights α and β.
  - **Quick check question:** Why does the paper argue that Multi-Task Learning is superior to Single-Task Learning for handling imperfect data?

- **Concept:** Chain-of-Thought (CoT) Reasoning
  - **Why needed here:** ReasVQA transfers the "process" of reasoning (CoT) from an MLLM to a VideoQA model.
  - **Quick check question:** How does the "Reasoning Refinement" phase modify the standard CoT data before it is used for training?

- **Concept:** Cross-Entropy Loss Weighting
  - **Why needed here:** The method introduces specific weights for the QA task vs. the Reasoning task.
  - **Quick check question:** According to Figure 4, what happens to the accuracy on NExT-QA if the reasoning loss weight (β) is set to 0 (standard QA training) vs. 0.8?

## Architecture Onboarding

- **Component map:** Reasoning Generator (MLLM) -> Refinement Module (text filter) -> VideoQA Model (trainable)
- **Critical path:** The Refinement Module. If the regex-based filter fails to remove the predicted answer from the MLLM's output, the training data contains contradictory labels (Ground Truth A vs. Predicted B).
- **Design tradeoffs:**
  - Generator Size: InternVL-26B yields better reasoning but is slower; InternVL-4B is a viable lower-cost alternative with lower peak performance.
  - Loss Weights: Tuning β (reasoning weight) is essential. The paper finds a "sweet spot" (0.5-0.8). Too high implies overfitting to synthetic noise; too low ignores the reasoning signal.
- **Failure signatures:**
  - Performance Plateau: Using "Original Reasoning" (without refinement) causes performance to stagnate even with more data.
  - Accuracy Drop in STL: Implementing Single-Task Learning on unfiltered data causes accuracy to drop significantly.
  - Overfitting to Noise: Setting β > 0.9 causes the model to prioritize mimicking the MLLM's reasoning style over answering correctly.
- **First 3 experiments:**
  1. Reasoning Ablation: Train the VideoQA model using only Ground Truth Answers to establish a baseline accuracy.
  2. Refinement Validation: Generate reasoning using a MLLM. Train two versions of the model using MTL: one with "Original" reasoning and one with "Refined" reasoning. Compare accuracy.
  3. Hyperparameter Search: Implement the MTL loss function. Run a sweep on the reasoning weight β (e.g., [0.2, 0.5, 0.8]) to verify that the reasoning loss provides a boost over the baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can more sophisticated refinement strategies beyond keyword filtering effectively identify and correct logical inconsistencies within the reasoning steps themselves?
- **Basis in paper:** The Limitations section states the authors will "investigate how to generate higher-quality reasoning or develop better refinement strategies," noting that keyword removal leaves imperfect steps.
- **Why unresolved:** The current refinement method relies on simple keyword matching to decouple answers from reasoning, assuming the remaining steps are valid even if the conclusion was originally incorrect.
- **What evidence would resolve it:** A comparative study evaluating ReasVQA using semantic consistency checks or NLI-based filters versus the current keyword-based removal on the "Incorrect Reasoning" data subset.

### Open Question 2
- **Question:** Does the ReasVQA training framework generalize to other multimodal tasks that require temporal understanding, such as dense video captioning or temporal grounding?
- **Basis in paper:** The Conclusion states that "Future work will explore... integration processes, aiming to extend the approach to other multimodal tasks."
- **Why unresolved:** The current method is validated exclusively on Video Question Answering benchmarks (NExT-QA, STAR, IntentQA), which have discrete answer outputs, unlike generation or localization tasks.
- **What evidence would resolve it:** Applying the three-phase ReasVQA pipeline (Reasoning Generation, Refinement, Multi-task Learning) to a temporal grounding benchmark and measuring performance changes (e.g., mAP).

### Open Question 3
- **Question:** Is there a lower bound on the accuracy of the teacher MLLM required for the "imperfect reasoning" to remain beneficial, or does performance degrade if the generator is significantly weaker than InternVL?
- **Basis in paper:** The paper relies on SOTA MLLMs (InternVL) with ~67% accuracy, asserting that reasoning steps remain valuable despite errors. It does not test if this holds for less capable generators.
- **Why unresolved:** The experiments only vary the size (4B vs 26B) of the same high-performing model family. It is unclear if a weaker generator produces reasoning noise that invalidates the multi-task learning benefits.
- **What evidence would resolve it:** Experiments using older or smaller MLLMs (e.g., with <50% zero-shot accuracy) as the reasoning generator to observe the resulting impact on the student VideoQA model's accuracy.

## Limitations
- Unverifiable assumption that MLLM intermediate reasoning steps are visually grounded even when final answers are incorrect
- Sparse LoRA configuration details prevent precise reproduction
- Fixed frame sampling strategy doesn't explore adaptive sampling based on question content

## Confidence

- **High Confidence:** The multi-task learning framework and its superiority over single-task learning (confirmed by Table 5 showing 75.7% vs 71.6%)
- **Medium Confidence:** The noise tolerance mechanism, though this could reflect dataset-specific characteristics rather than a generalizable principle
- **Low Confidence:** The generalizability to other VideoQA architectures beyond BLIP-FlanT5

## Next Checks

1. **Intermediate Step Grounding Analysis:** Measure the accuracy of the MLLM's intermediate reasoning steps (after refinement) against ground truth video descriptions on a held-out validation set to test whether refinement actually filters useful visual context.

2. **Teacher Accuracy Sensitivity Test:** Train the same ReasVQA pipeline using reasoning generated by MLLMs with varying accuracy levels (e.g., InternVL-4B vs InternVL-26B, or even a weaker MLLM) to establish the minimum teacher accuracy threshold for the method to remain beneficial.

3. **Alternative Refinement Strategies:** Implement and compare against alternative reasoning refinement approaches such as keeping only the first N reasoning steps, using a classifier to identify visually-grounded sentences, or keeping steps that mention objects detected in the video frames.