---
ver: rpa2
title: Agentic AI for Multi-Stage Physics Experiments at a Large-Scale User Facility
  Particle Accelerator
arxiv_id: '2509.17255'
source_url: https://arxiv.org/abs/2509.17255
tags:
- accelerator
- execution
- system
- data
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first deployment of a language-model-driven
  agentic AI system for multi-stage physics experiments at a production synchrotron
  light source. The Accelerator Assistant autonomously executes complex experimental
  procedures by translating natural language user prompts into structured plans combining
  data retrieval, control-system channel resolution, script generation, and machine
  interaction.
---

# Agentic AI for Multi-Stage Physics Experiments at a Large-Scale User Facility Particle Accelerator

## Quick Facts
- **arXiv ID**: 2509.17255
- **Source URL**: https://arxiv.org/abs/2509.17255
- **Reference count**: 40
- **Primary result**: Language-model-driven agentic AI system deployed at production synchrotron reduces multi-stage physics experiment preparation time by two orders of magnitude while maintaining operator-standard safety.

## Executive Summary
This paper presents the first deployment of a language-model-driven agentic AI system for multi-stage physics experiments at a production synchrotron light source. The Accelerator Assistant autonomously executes complex experimental procedures by translating natural language user prompts into structured plans combining data retrieval, control-system channel resolution, script generation, and machine interaction. In a representative experiment, the system reduced preparation time by two orders of magnitude compared to manual scripting by experts, while strictly upholding operator-standard safety constraints. Key architectural innovations include plan-first orchestration, bounded tool access, and dynamic capability selection, enabling transparent, auditable, and reproducible execution. These results establish a blueprint for integrating agentic AI into high-stakes scientific infrastructures, with direct portability to other accelerators and large-scale facilities.

## Method Summary
The Accelerator Assistant translates natural language prompts into structured execution plans through a pipeline combining dynamic capability selection, plan-first orchestration, and bounded tool access. The system uses few-shot binary classification to filter relevant capabilities, generates dependency-aware execution plans before any tool invocation, and employs a ReAct-style agent constrained to a strictly bounded API for resolving user terminology to precise EPICS channel names. Code generation proceeds through a three-stage process (plan → JSON schema → Python) with static analysis and human approval gates for write operations. The architecture enables transparent, auditable execution with fully reproducible artifacts while maintaining safety through strict separation of planning and execution phases.

## Key Results
- Reduced preparation time by two orders of magnitude compared to expert manual scripting for multi-stage physics experiments
- Successfully deployed at a production synchrotron light source with strict safety constraints
- Demonstrated transparent, auditable execution with fully reproducible artifacts through plan-first orchestration
- Established blueprint for agentic AI integration in high-stakes scientific infrastructures with claimed portability to other accelerators

## Why This Works (Mechanism)

### Mechanism 1: Plan-First Orchestration
Separating planning from execution enables transparency, auditability, and safety enforcement in high-stakes environments. The system generates a complete execution plan with explicit input-output dependencies before any tool is invoked, creating inspectable, serializable checkpoints where safety gates validate intended actions. This assumes the language model can reliably decompose complex multi-step tasks into ordered, dependency-aware subtasks in a single planning pass, and that downstream execution errors can be caught through plan inspection rather than requiring real-time adaptation.

### Mechanism 2: Dynamic Capability Selection via Binary Classification
Filtering relevant capabilities through independent binary decisions prevents prompt inflation and enables stable scaling as tool inventories grow. Each capability is evaluated with a binary relevance decision using few-shot, capability-specific examples, decoupling prompt size from total capability count. This assumes capabilities are sufficiently distinct that independent binary classification captures relevance without needing global context, and that few-shot examples generalize to novel task formulations.

### Mechanism 3: Bounded Tool Access for PV Resolution
Constraining the PV finder agent to a strictly bounded API ensures auditable channel resolution while handling ambiguous terminology. A ReAct-style agent explores the normalized MML database through limited, well-defined API calls, preventing unconstrained exploration while grounding user terms like "beam current" into precise EPICS channel names. This assumes the database schema is sufficiently descriptive that bounded exploration can resolve most user queries, and that the ~10,000 key PVs in the MML export cover the operational vocabulary users employ.

## Foundational Learning

- **Concept: EPICS Control Systems and Process Variables (PVs)** - The entire system interfaces with ~230,000 PVs representing accelerator hardware. Understanding read vs. write operations, channel naming conventions, and safety implications of setpoint changes is essential for interpreting what the agent does. Quick check: Given a PV named "SR:ID:Gap:Setpoint", can you explain what system it likely belongs to and why modifying it requires safety review?

- **Concept: ReAct (Reasoning + Acting) Loops** - The PV finder uses a ReAct-style agent that interleaves reasoning steps with bounded API calls. Understanding this pattern helps debug where resolution fails—in planning, in API selection, or in result interpretation. Quick check: If a ReAct agent fails to find a PV after three API calls, where would you look first to diagnose the failure?

- **Concept: Tool-Use / Function-Calling in Language Models** - The system's capabilities are structured as tools the LM can invoke. Understanding how models select tools, pass arguments, and handle errors is critical for extending or debugging the capability registry. Quick check: If a new tool is added but the model never selects it, what are three likely causes?

## Architecture Onboarding

- **Component map**: User Input (CLI / Open WebUI) → Input Normalization & Context Injection → Dynamic Capability Selection → Execution Planner → [Human Approval Gate] → Capability Execution (Time-range Parser, PV Finder, Archiver Retrieval, Code Executor) → Artifact Generation

- **Critical path**: User query → Capability selection → Plan generation → **Approval gate** → PV resolution → Code generation → Execution. The approval gate and PV resolution are the highest-risk points: misclassified capabilities or unresolved PVs cause downstream failures that cascade through execution.

- **Design tradeoffs**: Plan-first vs. reactive execution trades flexibility for transparency and auditability; local inference (Ollama/H100) vs. cloud (CBorg → ChatGPT/Claude/Gemini) trades latency and data sovereignty for model capability access; three-stage code generation trades robustness for latency and potential misalignment.

- **Failure signatures**: Prompt inflation symptoms occur when capability selection fails; PV resolution loops suggest schema gaps or ambiguous queries; code execution errors in write mode indicate incomplete dependency handling; missing artifacts suggest containerized execution environment crashes.

- **First 3 experiments**: 1) Read-only diagnostic query: "Plot the beam current and lifetime for the last 24 hours" (tests time-range parsing, PV resolution, archive retrieval, analysis, visualization). 2) Controlled write operation with approval: "Set the ID gap to 15mm on device 3 and confirm it settled" (tests PV resolution, approval gate, write execution, verification). 3) Multi-stage experiment: "Sweep ID gap from min to max in 5 steps on one device, measuring beam size at each point" (validates full workflow orchestration).

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several critical uncertainties emerge from the methodology:

- How robustly does the Accelerator Assistant transfer to other accelerator facilities with different control system architectures beyond EPICS/MML-based synchrotrons?
- How does system performance vary across the full diversity of machine physics tasks beyond the single insertion device hysteresis experiment demonstrated?
- What is the failure rate and error mode distribution of the agentic system under operational conditions, and how do automatic replanning mechanisms perform?

## Limitations

- Performance gains demonstrated through only a single representative experiment without statistical validation across multiple trials or experiment types
- Safety validation is asserted rather than empirically tested with quantified false positive/negative rates for approval gates and static analysis
- Bounded PV resolution approach relies on MML export completeness without characterization of failure rates when user queries reference channels outside this set
- Architectural innovations described conceptually but lack quantitative validation of their individual contributions to system performance or safety

## Confidence

- **High confidence**: The core system architecture is internally consistent and addresses real operational challenges at synchrotron facilities. The plan-first orchestration approach is well-grounded in safety-critical systems literature.
- **Medium confidence**: The time reduction claim is plausible given the complexity of manual scripting at synchrotrons, but lacks statistical validation across multiple experiment types. The safety mechanisms are theoretically sound but not empirically validated.
- **Low confidence**: The scaling claims for dynamic capability selection lack quantitative validation. The completeness claims for bounded PV resolution are not empirically tested.

## Next Checks

1. **Safety validation experiment**: Deploy the system in a controlled test environment with synthetic safety-critical scenarios to measure false positive/negative rates for the approval gates and static analysis, comparing against manual expert review.

2. **Scaling experiment**: Systematically vary the number of capabilities from 10 to 100+ while measuring prompt inflation, classification accuracy, and execution planning time to empirically validate the claimed stable scaling of the dynamic capability selection approach.

3. **PV resolution coverage test**: Create a benchmark of 100 representative user queries spanning the full range of accelerator terminology and measure the success rate of the bounded ReAct agent, identifying failure patterns and quantifying the gap between MML coverage and operational vocabulary.