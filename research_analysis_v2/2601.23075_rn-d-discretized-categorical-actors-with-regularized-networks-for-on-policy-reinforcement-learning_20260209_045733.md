---
ver: rpa2
title: 'RN-D: Discretized Categorical Actors with Regularized Networks for On-Policy
  Reinforcement Learning'
arxiv_id: '2601.23075'
source_url: https://arxiv.org/abs/2601.23075
tags:
- policy
- actor
- categorical
- learning
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates on-policy reinforcement learning for continuous\
  \ control, identifying that standard Gaussian actors and shallow MLPs often lead\
  \ to brittle optimization due to noisy gradients and conservative updates. The authors\
  \ propose using discretized categorical actors\u2014where each action dimension\
  \ is represented by a distribution over bins\u2014which turns the policy objective\
  \ into a cross-entropy-like loss."
---

# RN-D: Discretized Categorical Actors with Regularized Networks for On-Policy Reinforcement Learning

## Quick Facts
- arXiv ID: 2601.23075
- Source URL: https://arxiv.org/abs/2601.23075
- Reference count: 40
- Primary result: Discretized categorical actors with regularized residual networks consistently improve sample efficiency and final performance over Gaussian actors in PPO-based continuous control

## Executive Summary
This paper addresses brittle optimization in on-policy continuous control by replacing standard Gaussian actors with discretized categorical actors and enhancing actor networks with residual connections and layer normalization. The discretized categorical policy converts action selection into a cross-entropy-like optimization problem, which empirically reduces policy gradient variance and improves stability. Experiments across diverse MuJoCo locomotion and ManiSkill manipulation tasks show consistent gains in both final performance and sample efficiency. The method is simple to implement within existing PPO frameworks and requires minimal architectural changes.

## Method Summary
The method replaces standard Gaussian continuous actor networks with a discretized categorical policy and a regularized residual network (RN-D) architecture. Each action dimension is represented by a discrete distribution over K bins (typically K=41) spanning [-1,1], with the policy outputting logits that are converted to categorical probabilities via softmax. The actor uses residual blocks with pre-LayerNorm normalization and inverted bottleneck expansion (4×), replacing standard MLP architectures. Training uses PPO with the standard clipped surrogate objective, but with log-probabilities computed from categorical distributions rather than Gaussian distributions. The approach is evaluated on both state-based and vision-based continuous control tasks.

## Key Results
- RN-D consistently outperforms standard MLP Gaussian actors across all tested MuJoCo and ManiSkill benchmarks
- Discretized categorical policies exhibit significantly lower policy gradient variance, especially during early training
- Performance scales with actor width, with wider networks achieving faster learning and better final returns
- On Humanoid-v4, RN-D achieves higher normalized returns than PPO baselines
- On StackCube-v1, RN-D achieves near-perfect success rates where others fail

## Why This Works (Mechanism)
The discretized categorical actor provides two key benefits: (1) a cross-entropy-like optimization objective that is more stable than Gaussian likelihood optimization, and (2) bounded action spaces that reduce policy gradient variance. The regularized residual network architecture further stabilizes training through layer normalization and residual connections. The combination addresses the brittleness of standard Gaussian actors, which can suffer from vanishing gradients when action distributions become too peaked and noisy updates when they are too broad.

## Foundational Learning
- **Categorical Policy Representation**: Actions are modeled as discrete distributions over bins rather than continuous Gaussian distributions. Why needed: Continuous Gaussian policies can have vanishing or exploding gradients depending on variance. Quick check: Verify action space is properly discretized and categorical probabilities sum to 1.
- **Cross-Entropy-Like Optimization**: The policy objective becomes maximizing log-likelihood of discrete action choices rather than continuous Gaussian likelihood. Why needed: Provides more stable gradients than Gaussian likelihood optimization. Quick check: Confirm the loss function uses categorical cross-entropy, not Gaussian likelihood.
- **Pre-LayerNorm Residual Blocks**: Layer normalization applied before residual connections with inverted bottleneck architecture. Why needed: Stabilizes training and allows deeper networks. Quick check: Verify LayerNorm is applied at the correct location in residual blocks.
- **Factorized Action Distribution**: Each action dimension is modeled independently as a categorical distribution. Why needed: Simplifies the output space and maintains computational efficiency. Quick check: Ensure output logits have shape [batch_size, action_dim, K].
- **Policy Gradient Variance Reduction**: Bounded action spaces and discrete optimization reduce variance in policy updates. Why needed: Lower variance leads to more stable and efficient learning. Quick check: Compare gradient variance between categorical and Gaussian policies during training.

## Architecture Onboarding

- Component map:
  1. **Input Encoder (MLP or CNN)**: Processes state or image observations into a latent feature vector. (Standard component).
  2. **RN-D Actor Core (The Novel Component)**: A stack of N residual blocks. Each block = `LayerNorm -> FeedForward(Expansion, ReLU, Projection) -> ResidualAdd`. This replaces the standard 2-layer MLP actor.
  3. **Categorical Action Head**: A linear layer projecting the RN-D core's output to a `[action_dim, K]` tensor of logits, where K is the number of discrete bins.
  4. **PPO Critic (Standard)**: A separate network (e.g., a standard MLP) for value function estimation. This is kept fixed across comparisons.
  5. **PPO Loss (Standard)**: Uses the clipped surrogate objective. The key is the log-probability is now calculated from the categorical logits via `log_softmax`.

- Critical path: The **RN-D Actor Core** and the **Categorical Action Head** are the critical new components. The entire learning signal now flows through a cross-entropy-like objective derived from these logits. A bug in the discretization logic or a misconfigured residual block will break the mechanism.

- Design tradeoffs:
  - **Number of Bins (K)**: Higher K gives finer action resolution but increases the output space and can lead to performance degradation if it exceeds the network's capacity. Paper finds moderate K (e.g., 41) is best.
  - **Actor Width (dh)**: Wider actors speed up convergence but may have diminishing returns or slight drops in final performance.
  - **Loss Swap**: The paper explicitly shows you cannot get the gains just by discretizing. You *must* use the categorical likelihood (cross-entropy objective), not a Gaussian likelihood on the discretized mean.

- Failure signatures:
  - **Training instability or failure to converge on a task that standard PPO solves**: This suggests a bug in the implementation of the categorical actor or its log-probability calculation.
  - **High policy gradient variance early in training**: This indicates the categorical policy is not providing the expected variance reduction. Check the actor architecture and ensure residual connections/layer norm are correctly applied.
  - **Performance degrading as number of bins increases**: This is an expected failure mode, indicating the discretization granularity is too fine for the network's capacity.

- First 3 experiments:
  1. **RN-D vs. MLP-C Baseline**: Replicate the core result on a standard MuJoCo task (e.g., HalfCheetah-v4). Compare the learning curve and final performance of the proposed RN-D actor against the paper's Gaussian MLP baseline. This validates the entire setup.
  2. **Ablate the Cross-Entropy Objective**: Implement the "loss-swap" ablation described in Appendix C.2. Use the RN-D architecture but compute the action as the expected bin value and optimize it with a Gaussian loss. Show that performance drops to the level of the continuous baseline, proving the loss function, not just discretization, is key.
  3. **Measure Gradient Variance**: Log the norm of the policy gradient and its variance across minibatches during training for both RN-D and a Gaussian baseline. Plot these values (log scale) over environment steps to confirm the variance reduction claim from Figure 4.

## Open Questions the Paper Calls Out
- **Open Question 1**: Does the discretized categorical actor approach yield similar optimization and performance gains when applied to other on-policy algorithms besides PPO? [explicit] The conclusion explicitly states, "it is natural to extend our findings beyond PPO and evaluate discretized categorical actors with other on-policy algorithms." Why unresolved: The entire experimental validation in the paper is restricted to the Proximal Policy Optimization (PPO) algorithm. What evidence would resolve it: Empirical benchmarks demonstrating that RN-D improves sample efficiency and final returns in algorithms like TRPO or A2C.
- **Open Question 2**: How does the proposed actor architecture interact with the critic, and can joint optimization or specific critic designs further amplify the benefits of discretization? [explicit] The authors note that they "isolate actor-side effects by holding the critic fixed," and identify "understanding actor–critic interactions" as an important direction for future work. Why unresolved: The study decouples the actor improvements from the critic architecture to attribute gains specifically to the actor changes. What evidence would resolve it: Ablation studies varying critic depth and regularization alongside the RN-D actor to identify synergistic or detrimental interactions.
- **Open Question 3**: Can the performance degradation observed at fine discretization granularities be mitigated by more efficient action tokenization schemes? [inferred] Appendix C.4 notes that performance drops when the number of bins exceeds 100, and the text suggests "efficient action tokenization schemes" as a promising mitigation strategy. Why unresolved: The current uniform discretization struggles to balance the trade-off between action resolution and the complexity of the prediction task for the network. What evidence would resolve it: A method that maintains stable optimization and high returns even as the number of action bins scales significantly (e.g., >100).

## Limitations
- Residual block implementation is underspecified (LayerNorm placement, activation location), which could materially affect optimization stability claims
- Baselines use fixed MLP architectures without systematic width sweeps, making performance comparisons sensitive to architectural choices
- Continuous control tasks have inherent variance, and while relative rankings are consistent, absolute returns may vary with random seeds or environment versions

## Confidence
- **High confidence** in the variance reduction claim: The discrete policy's cross-entropy objective and bounded action space directly explain the empirical reduction in gradient variance, supported by both theory and Figure 4
- **Medium confidence** in the final performance gains: While RN-D consistently outperforms MLP-C, architectural differences (width, depth) complicate attribution. The ablation study supports that the categorical objective is key, but residual connections could also contribute
- **Low confidence** in scaling trends: Claims about wider actors improving sample efficiency are based on limited sweeps (3 widths) and may not generalize beyond the tested tasks

## Next Checks
1. **Loss-swap ablation replication**: Implement the RN-D architecture but compute actions as expected bin values and optimize with Gaussian likelihood. Confirm performance matches MLP-C, validating the cross-entropy objective as the key driver
2. **Gradient variance measurement**: Log policy gradient norms and variances for both RN-D and Gaussian policies during training. Plot these values (log scale) to confirm the early-training variance reduction shown in Figure 4
3. **Residual block wiring test**: Systematically test different LayerNorm placements (before vs. after linear layers) in the RN-D actor to determine if optimization stability depends on specific architectural details