---
ver: rpa2
title: The ReQAP System for Question Answering over Personal Information
arxiv_id: '2508.06880'
source_url: https://arxiv.org/abs/2508.06880
tags:
- workout
- date
- time
- start
- reqap
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The ReQAP system addresses question answering over heterogeneous
  personal data by recursively decomposing questions and building an execution plan
  with operators. The method treats all personal data as events with key-value pairs,
  enabling structured processing of both structured and unstructured sources.
---

# The ReQAP System for Question Answering over Personal Information

## Quick Facts
- **arXiv ID:** 2508.06880
- **Source URL:** https://arxiv.org/abs/2508.06880
- **Reference count:** 23
- **One-line primary result:** ReQAP achieves Hit@1 of 0.386 on PerQA benchmark, outperforming Rag (0.149) and CodeGen (0.319) using light-weight 1B models

## Executive Summary
The ReQAP system addresses question answering over heterogeneous personal data by recursively decomposing questions and building an execution plan with operators. The method treats all personal data as events with key-value pairs, enabling structured processing of both structured and unstructured sources. Novel RETRIEVE and EXTRACT operators allow uniform handling of different data types. Experiments on the PerQA benchmark show ReQAP outperforms competitive baselines (Rag, CodeGen) with Hit@1 of 0.386 versus 0.149 and 0.319 respectively, even when using light-weight language models. The system provides traceable answers with detailed operator steps and related events for user verification.

## Method Summary
ReQAP uses a recursive decomposition approach where a Question Understanding and Decomposition (QUD) model generates an operator tree from natural language questions. This tree is executed by an Operator Tree Executor (OTX) using specialized operators: RETRIEVE for finding relevant events using learned sparse retrieval and cross-encoders, EXTRACT for parsing unstructured text into key-value pairs, and SQL-like operators (JOIN, FILTER, GROUP, APPLY) for combining and processing events. All personal data is normalized into a uniform event structure with key-value pairs, allowing the same operators to work across structured (calendars, workouts) and unstructured (emails, social media) sources. The system uses light-weight models (1B parameters) that are fine-tuned for specific operator tasks rather than relying on monolithic models.

## Key Results
- ReQAP achieves Hit@1 of 0.386 on PerQA benchmark, significantly outperforming Rag (0.149) and CodeGen (0.319)
- The system performs well with light-weight 1B parameter models, achieving comparable results to GPT-based variants
- Traceable execution plans with expandable operator trees enable user verification of intermediate steps
- The two-stage RETRIEVE pipeline (sparse retrieval + cross-encoder) provides high precision for personal data queries

## Why This Works (Mechanism)

### Mechanism 1: Unified Event Abstraction
The system normalizes heterogeneous personal data into a uniform event structure with key-value pairs, enabling structured operators to work across both structured and unstructured sources. This abstraction allows SQL-like operations to be applied to text data by treating it as if it were a database row.

### Mechanism 2: Recursive Decomposition for Plan Generation (QUD)
The QUD model incrementally builds operator trees through recursive decomposition, breaking complex queries into manageable sub-questions. This approach reduces error rates compared to monolithic code generation by handling complexity step-by-step rather than in one shot.

### Mechanism 3: Traceable Light-Weight Models
By separating the planner (QUD) from workers (operators) and fine-tuning specialized small models for specific tasks, ReQAP achieves high performance with 1B parameter models. This architecture allows efficient deployment while maintaining accuracy.

## Foundational Learning

- **Concept: Abstract Syntax Trees (AST) & Logical Plans**
  - *Why needed here:* ReQAP generates execution plans (operator trees) rather than text answers, so understanding how leaf nodes feed into parent nodes is essential for debugging.
  - *Quick check question:* If a user asks "How many workouts did I do?", which operator sits at the root of the tree: RETRIEVE or APPLY?

- **Concept: Semantic Retrieval (Sparse vs. Dense)**
  - *Why needed here:* The RETRIEVE operator uses "learned sparse retrieval" that expands queries to handle synonyms, which is critical for personal data where terminology varies.
  - *Quick check question:* Why does ReQAP use a pipeline of sparse retrieval followed by a cross-encoder classifier for the RETRIEVE operator?

- **Concept: Fine-Tuning vs. In-Context Learning (ICL)**
  - *Why needed here:* The paper distinguishes between using GPT-4o with ICL for training data generation and using a fine-tuned 1B model for inference, explaining how they achieve lightweight deployment.
  - *Quick check question:* Why is the QUD model fine-tuned instead of just prompted with examples at inference time?

## Architecture Onboarding

- **Component map:** Input (NL Question + Personal Data) -> QUD (Compiler: Llama-3.2-1B) -> Operator Tree (JSON/DSL) -> OTX (Runtime Engine) -> Operators (RETRIEVE, EXTRACT, SQL-like) -> UI (Answer + Tree Trace)
- **Critical path:** The QUD model's ability to correctly structure the JOIN condition. If the planner fails to identify the temporal relationship, downstream execution will fail regardless of retrieval quality.
- **Design tradeoffs:** Separation of concerns moves logic from LLM prompts to explicit operators, adding engineering complexity but providing traceability and robustness. Two-stage retrieval is computationally heavier but necessary for high precision.
- **Failure signatures:** Empty results from RETRIEVE classifier failures or strict FILTER conditions; hallucinated counts from JOIN logic errors; extraction errors when EXTRACT model returns null for implicit attributes.
- **First 3 experiments:** 1) Validate abstraction by feeding structured and unstructured data for same event and verify RETRIEVE de-duplicates based on temporal overlap. 2) Stress test planner with complex 3+ join queries and inspect generated Operator Tree. 3) Isolate EXTRACT operator on raw social media posts and measure precision of extracted key-value pairs.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does ReQAP performance generalize to non-synthetic personal data containing noise and inconsistencies typical of real-world user logs?
- **Basis in paper:** The paper evaluates using PerQA benchmark with "handcrafted personas" and "LLM-based verbalization," acknowledging the data is "artificial."
- **Why unresolved:** Synthetic data may lack OCR errors, missing fields, and formatting inconsistencies found in actual exports from email or fitness apps.
- **What evidence would resolve it:** Evaluation results on actual user data exports or user study measuring accuracy on live, non-synthetic personal data.

### Open Question 2
- **Question:** Can the QUD model be optimized to run entirely on-device without sacrificing accuracy?
- **Basis in paper:** The paper notes QUD "does not yet access any user data, it could be run as a cloud service," implying an open trade-off between latency, privacy, and on-device compute limits.
- **Why unresolved:** Feasibility of low-latency inference for complex recursive decomposition on standard mobile hardware is not demonstrated.
- **What evidence would resolve it:** Latency and memory usage benchmarks of the 1B QUD model running on consumer-grade mobile processors.

### Open Question 3
- **Question:** How does the system's performance scale as the volume of personal events approaches a lifetime of user data (> 50,000 events)?
- **Basis in paper:** The methodology notes entries can be "in the order of thousands," but experiments use a fixed 4,000 events per persona.
- **Why unresolved:** RETRIEVE operator uses cross-encoders which can be computationally expensive; latency impact on significantly larger event lists remains untested.
- **What evidence would resolve it:** Runtime analysis and Hit@1 metrics plotted against increasing dataset sizes (from 4,000 to 100,000 events).

### Open Question 4
- **Question:** Can user feedback on intermediate operator steps be effectively integrated to correct execution errors without full re-generation?
- **Basis in paper:** User study showed "understanding of the individual steps... was slightly lower," and interface offers "debugging" views, suggesting potential for interactive error correction.
- **Why unresolved:** Unclear if architecture supports dynamic re-planning of operator tree based on user modification of specific steps.
- **What evidence would resolve it:** Study measuring reduction in error rates when users manually intervene and correct intermediate operator outputs.

## Limitations

- System's reliance on specific operator tree structure may not handle highly ambiguous or context-dependent queries
- Performance claims depend on synthetic PerQA benchmark which may not capture real-world complexity
- 1B parameter model's performance achieved through fine-tuning on GPT-4o-generated data creates potential dependency on large model access

## Confidence

- **High Confidence:** The core architectural approach of recursive decomposition and operator-based execution is well-specified and theoretically sound
- **Medium Confidence:** Reported performance improvements over baselines are plausible given methodology, but dependent on benchmark characteristics
- **Low Confidence:** System's robustness to highly ambiguous queries or queries requiring deep world knowledge beyond personal event log

## Next Checks

1. Test the system on real-world personal data queries (not synthetic) to assess generalization beyond the PerQA benchmark
2. Evaluate performance degradation when using the 1B model directly without GPT-4o fine-tuning to understand dependency on large model access
3. Measure execution time and resource requirements for complex queries with multiple joins to assess practical deployment feasibility