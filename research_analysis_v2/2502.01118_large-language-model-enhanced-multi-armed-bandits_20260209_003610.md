---
ver: rpa2
title: Large Language Model-Enhanced Multi-Armed Bandits
arxiv_id: '2502.01118'
source_url: https://arxiv.org/abs/2502.01118
tags:
- reward
- algorithm
- text
- arms
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach to enhance multi-armed bandits
  (MAB) by integrating large language models (LLMs) with classical MAB algorithms.
  Instead of directly using LLMs for arm selection, the authors leverage LLMs as reward
  predictors within classical MAB frameworks like Thompson Sampling and regression
  oracle-based algorithms.
---

# Large Language Model-Enhanced Multi-Armed Bandits

## Quick Facts
- arXiv ID: 2502.01118
- Source URL: https://arxiv.org/abs/2502.01118
- Reference count: 23
- Primary result: LLM-enhanced MAB algorithms outperform direct LLM arm selection in reward prediction tasks

## Executive Summary
This paper proposes a novel approach to enhance multi-armed bandits (MAB) by integrating large language models (LLMs) with classical MAB algorithms. Rather than using LLMs directly for arm selection, the authors leverage LLMs as reward predictors within classical MAB frameworks like Thompson Sampling and regression oracle-based algorithms. The framework extends to dueling bandits and demonstrates consistent improvements over baseline methods, particularly in challenging tasks where arm features lack semantic meanings.

## Method Summary
The proposed approach integrates LLMs into classical MAB algorithms by using LLMs as reward predictors. The framework combines LLM-generated reward predictions with established exploration strategies from Thompson Sampling and regression oracle-based algorithms. For dueling bandits, the approach extends the single-arm framework to handle pairwise comparisons. The key innovation lies in recognizing that LLMs excel at reward prediction while classical MAB algorithms provide effective exploration mechanisms.

## Key Results
- LLM-enhanced algorithms consistently outperform baseline methods based on direct LLM arm selection
- Performance improvements are most pronounced in tasks where arm features lack semantic meanings
- The framework demonstrates effectiveness across both synthetic and real-world datasets

## Why This Works (Mechanism)
The approach works because LLMs are particularly adept at understanding complex relationships and patterns in data, making them excellent reward predictors. When combined with classical MAB algorithms' exploration mechanisms, this creates a powerful synergy where the LLM handles the prediction aspect while the MAB framework manages the exploration-exploitation tradeoff. This division of labor allows each component to focus on its strengths.

## Foundational Learning

**Multi-Armed Bandits**: Sequential decision-making framework for balancing exploration and exploitation
- Why needed: Core problem being enhanced with LLM capabilities
- Quick check: Can the agent identify optimal arms over time?

**Thompson Sampling**: Bayesian approach for balancing exploration and exploitation in MAB
- Why needed: Provides probabilistic framework for arm selection
- Quick check: Does the algorithm converge to optimal arms?

**Reward Prediction**: Estimating the expected reward for selecting particular arms
- Why needed: LLMs serve as reward predictors in this framework
- Quick check: Are LLM predictions correlated with actual rewards?

## Architecture Onboarding

**Component Map**: LLM Reward Predictor -> Classical MAB Algorithm -> Arm Selection -> Environment Feedback -> Reward Prediction Update

**Critical Path**: LLM generates reward predictions → MAB algorithm uses predictions for arm selection → environment provides actual rewards → predictions are updated

**Design Tradeoffs**: Using LLMs as predictors rather than direct selectors trades computational overhead for improved prediction accuracy and more stable exploration

**Failure Signatures**: Poor performance when LLM predictions are systematically biased or when arm features lack semantic meaning that LLMs can interpret

**First Experiments**:
1. Compare LLM-enhanced Thompson Sampling vs. vanilla Thompson Sampling on synthetic datasets
2. Evaluate performance degradation when LLM predictions are corrupted
3. Test framework on real-world recommendation tasks with varying feature types

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes LLMs can reliably predict rewards across diverse task domains without extensive validation
- Empirical evaluation relies heavily on synthetic datasets with limited real-world applications
- Extension to dueling bandits appears less robust than single-arm setting

## Confidence
- Generalizability: Low - Limited testing across diverse real-world scenarios
- Algorithm performance: Medium - Strong results on synthetic tasks, weaker on real-world applications
- Framework robustness: Low - Insufficient evaluation under LLM prediction failures

## Next Checks
1. Test the framework across a broader range of real-world decision-making tasks with varying reward structures and arm feature types to assess generalizability.
2. Conduct ablation studies isolating the contribution of LLM-based reward prediction from the classical MAB exploration mechanisms.
3. Evaluate performance under scenarios where LLM predictions are deliberately corrupted or become unavailable to understand the framework's robustness to LLM failures.