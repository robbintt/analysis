---
ver: rpa2
title: Multi-Objective Bilevel Learning
arxiv_id: '2511.07824'
source_url: https://arxiv.org/abs/2511.07824
tags:
- mobl
- algorithm
- pareto
- optimization
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of multi-objective bilevel learning
  (MOBL), where multiple potentially conflicting objectives are coupled across different
  layers of a learning system. The authors propose a unifying algorithmic framework
  called weighted-Chebyshev multi-hyper-gradient-descent (WC-MHGD) that achieves fast
  finite-time convergence rates and low oracle complexities for both deterministic
  and stochastic settings.
---

# Multi-Objective Bilevel Learning

## Quick Facts
- **arXiv ID:** 2511.07824
- **Source URL:** https://arxiv.org/abs/2511.07824
- **Reference count:** 40
- **Primary result:** Weighted-Chebyshev multi-hyper-gradient-descent (WC-MHGD) achieves O(1/K) deterministic and O(1/K²) stochastic convergence rates for multi-objective bilevel learning with low oracle complexities.

## Executive Summary
This paper addresses multi-objective bilevel learning (MOBL) where multiple potentially conflicting objectives are coupled across different layers of a learning system. The authors propose WC-MHGD, a unifying algorithmic framework that achieves fast finite-time convergence rates and low oracle complexities for both deterministic and stochastic settings. The core innovation is integrating weighted Chebyshev scalarization into multi-gradient descent methods by replacing gradients with hypergradients, enabling systematic Pareto front exploration guided by user preferences. The algorithm solves a dynamic weighting optimization subproblem that balances Pareto stationarity and preference alignment, validated through extensive experiments on meta-learning and data cleaning tasks.

## Method Summary
The WC-MHGD algorithm solves multi-objective bilevel problems through a double-loop optimization framework. The outer loop updates upper-level variables using a preference-guided descent direction computed from hypergradients, while the inner loop approximately solves the lower-level problem with warm-start initialization. Hypergradients are computed via either Conjugate Gradient or Neumann Series methods to handle the implicit dependency between upper and lower-level variables. A convex quadratic program balances Pareto stationarity and preference alignment by finding optimal weights for the multi-gradient descent step. The method achieves finite-time convergence rates of O(1/K) deterministically and O(1/K²) stochastically, with corresponding oracle complexities.

## Key Results
- Finite-time convergence rates of O(1/K) and O(1/K²) for deterministic and stochastic settings respectively
- Oracle complexities of O(κ³ϵ⁻¹S) for deterministic and O(κ⁸ϵ⁻²S) for stochastic settings to achieve ϵ-Pareto stationary solutions
- Systematic Pareto front exploration capability guided by user preferences
- Superior performance compared to existing methods in exploring the Pareto front while maintaining preference alignment

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Weighting Optimization Subproblem
The convex quadratic program in Eq. (5) simultaneously achieves Pareto stationarity and preference-guided exploration by balancing two competing objectives. The first term ||K(r⊙λ)||² finds a common descent direction across all objectives (Pareto stationarity). The second term -uλ⊤(r⊙Φ(xₖ)) implements weighted Chebyshev scalarization, pushing toward solutions aligned with user preference r. Parameter u controls the trade-off. Core assumption: The preference vector r is strictly positive and user preferences are consistent with attainable Pareto front regions.

### Mechanism 2: Hypergradient Approximation via Hessian Inverse Methods
Approximating the implicit hypergradient through either Conjugate Gradient (CG) or Neumann Series (NS) enables gradient-based optimization through the bilevel structure with controlled bias. Since y*(x) is computed approximately (not exactly), the hypergradient requires inverting the lower-level Hessian. CG solves this directly; NS approximates via series expansion. Both control approximation error to prevent cascading bias. Core assumption: Lower-level objective g(x,·) is µg-strongly convex, ensuring Hessian invertibility.

### Mechanism 3: Warm-Start Lower-Level Optimization
Initializing each inner loop with the previous iteration's solution reduces convergence time and prevents error accumulation across outer iterations. Set y⁰_k = y^D_{k-1} instead of random initialization. Since x_k changes slowly with small step sizes, y*(x_k) ≈ y*(x_{k-1}), making the previous solution a good starting point. Core assumption: Upper-level step size β is small enough that x_k changes gradually.

## Foundational Learning

- **Concept:** Pareto Stationarity vs. Pareto Optimality
  - Why needed here: The paper targets Pareto stationary solutions (necessary condition) rather than Pareto optimal (sufficient), as finding true Pareto optima is NP-Hard for non-convex problems.
  - Quick check question: Can you explain why ||∇Φ(x)λ||² ≤ ϵ (Definition 3.3) is a relaxation of finding a common descent direction?

- **Concept:** Hypergradients in Bilevel Optimization
  - Why needed here: Standard gradients don't account for the dependency of y* on x. Hypergradients chain through the lower-level solution via implicit differentiation.
  - Quick check question: Why does computing ∇ϕₛ(x) require inverting the lower-level Hessian ∇²ᵧg(x, y*)?

- **Concept:** Chebyshev Scalarization
  - Why needed here: Transforms vector-valued multi-objective problem into scalar minimax form, enabling systematic Pareto front exploration by varying r.
  - Quick check question: How does the WC scalarization differ from weighted sum scalarization, and why is it preferred for Pareto front coverage?

## Architecture Onboarding

- **Component map:** Outer Loop (x updates) -> Inner Loop (y*(x) approximation) -> Hypergradient Module -> Weight Solver -> Update Step
- **Critical path:** 1) Warm-start y⁰_k from previous iteration, 2) Run D steps of GD on lower-level objective g(xₖ, y), 3) For each of S objectives: compute hypergradient via CG/NS, 4) Solve convex QP (Eq. 5) for λₖ, 5) Apply weighted descent step to x
- **Design tradeoffs:** CG vs NS hypergradients (CG has better oracle complexity but NS is more communication-efficient), parameter u (larger prioritizes preference alignment), inner iterations D (larger reduces hypergradient bias but increases cost)
- **Failure signatures:** Divergent oscillations (D too small), no preference alignment (u too small), convergence to poor region (u too large), numerical instability (α > 1/L)
- **First 3 experiments:** 1) Validate convergence rate on synthetic MOBL with known Pareto front, 2) Ablation on u with fixed preference r=[0.8,0.05,0.05,0.05,0.05], 3) Ablation on D with D∈{8,16,32}

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the strong convexity assumption on the lower-level objective be relaxed while maintaining finite-time convergence guarantees?
- Basis in paper: The paper notes "we assume that the LL function g(x, y) is strongly convex in y" and acknowledges this is needed for hypergradients to be well-defined.
- Why unresolved: Strong convexity is restrictive and may not hold in all practical MOBL applications like deep learning.
- What evidence would resolve it: A convergence analysis under weaker conditions (e.g., PL condition, local strong convexity, or smooth non-convex settings) with corresponding oracle complexity bounds.

### Open Question 2
- Question: How can the significant oracle complexity gap between deterministic O(κ³ϵ⁻¹) and stochastic O(κ⁸ϵ⁻²) settings be reduced?
- Basis in paper: The paper presents deterministic complexity of O(κ³ϵ⁻¹S) and stochastic complexity of O(κ⁸ϵ⁻²S), noting stochastic methods require more data to address bilevel error propagation.
- Why unresolved: The κ⁵ gap in condition number dependence may not be fundamental; variance reduction techniques from single-objective bilevel optimization have not been adapted.
- What evidence would resolve it: Development of variance-reduced or accelerated stochastic MOBL algorithms with oracle complexity matching deterministic rates up to statistical limits.

### Open Question 3
- Question: Can Solution Philosophies P1 (direct preference-guided point finding) and P3 (interactive preference-changing) be addressed within the WC-MHGD framework?
- Basis in paper: "In this paper, we focus on Philosophy P2" and notes that P1 "may not intersect with the Pareto front, rendering the P1 problem ill-defined."
- Why unresolved: The ill-definedness of P1 when preferences don't intersect the Pareto front is acknowledged but not resolved; P3's iterative preference updating is not analyzed.
- What evidence would resolve it: Algorithms with convergence guarantees for P1 that handle non-intersecting preferences, or dynamic preference schemes with theoretical analysis for P3.

## Limitations
- Strong convexity assumption on lower-level objective is restrictive and may not hold in practical deep learning applications
- Finite-time convergence guarantees rely on bounded hypergradients that may not hold in practice
- Choice of hyperparameter u lacks principled guidelines and requires empirical tuning
- Oracle complexity gap between deterministic and stochastic settings is significant (κ⁵ gap)

## Confidence

**High confidence:** Basic algorithmic framework and convergence rates under stated assumptions. The weighted-Chebyshev scalarization approach and its theoretical properties are well-established.

**Medium confidence:** Practical performance claims and oracle complexity bounds, as these depend on assumptions that may be violated in real applications. The experimental validation covers limited scenarios.

**Low confidence:** Claims about systematic Pareto front exploration without theoretical guarantees on coverage completeness, and the sensitivity analysis for hyperparameters like u.

## Next Checks

1. **Convergence rate validation:** Run deterministic WC-MHGD on synthetic MOBL problems with known Pareto fronts and empirically verify that the Pareto stationarity metric decreases at the theoretical O(1/K) rate, plotting against the Theorem 5.5 bound.

2. **Hypergradient approximation bias:** Conduct controlled experiments varying inner iterations D to quantify the trade-off between computational cost and hypergradient accuracy, measuring how bias affects final solution quality.

3. **Parameter sensitivity analysis:** Systematically test different u values and preference vectors r to establish guidelines for setting these parameters, documenting the relationship between u, Pareto stationarity, and preference alignment across diverse problem instances.