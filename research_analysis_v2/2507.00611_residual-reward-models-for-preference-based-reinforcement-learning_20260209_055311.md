---
ver: rpa2
title: Residual Reward Models for Preference-based Reinforcement Learning
arxiv_id: '2507.00611'
source_url: https://arxiv.org/abs/2507.00611
tags:
- reward
- learning
- proxy
- prior
- feedback
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Residual Reward Models (RRM) for preference-based
  reinforcement learning to improve sample efficiency when learning from human preferences.
  RRM assumes the true reward decomposes into a prior reward (available before training)
  plus a learned residual term trained with human preferences.
---

# Residual Reward Models for Preference-based Reinforcement Learning

## Quick Facts
- arXiv ID: 2507.00611
- Source URL: https://arxiv.org/abs/2507.00611
- Reference count: 40
- Key outcome: RRM improves PEBBLE's sample efficiency on 5 Meta-World tasks by decomposing reward into fixed prior + learned residual, achieving higher success rates and faster convergence.

## Executive Summary
This paper introduces Residual Reward Models (RRM) for preference-based reinforcement learning to improve sample efficiency when learning from human preferences. RRM assumes the true reward decomposes into a prior reward (available before training) plus a learned residual term trained with human preferences. Experiments on five Meta-World tasks show RRM significantly improves PEBBLE's performance, achieving higher success rates and faster convergence. RRM also demonstrates robustness to less feedback, noisy preferences, and even opposite prior rewards. Real-robot experiments confirm RRM enables faster learning of successful policies.

## Method Summary
RRM improves preference-based RL by decomposing the true reward into a fixed prior reward $r_0$ and a learned residual $r'_\psi$. The total reward is $\hat{r} = r_0 + r'_\psi$, where $r'_\psi$ is trained via cross-entropy loss on human preferences using the Bradley-Terry model. This approach avoids optimization instability from switching loss functions and reduces the learning burden on the residual network. The method is implemented on top of PEBBLE with SAC as the RL backbone, using either state or image observations with a pretrained encoder.

## Key Results
- RRM significantly improves PEBBLE's performance on 5 Meta-World tasks, achieving higher success rates and faster convergence
- RRM demonstrates robustness to reduced feedback, showing strong performance with only 2,000 feedback pairs compared to PEBBLE's 10,000
- The method handles stochastic and even opposite prior rewards, with the residual network eventually correcting the prior

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing the reward into a fixed prior plus a learned residual improves sample efficiency and training stability.
- Mechanism: The prior reward $r_0$ provides immediate goal-directed signal, reducing early exploration burden. The learned residual $r'_\psi$ only needs to capture corrections rather than the full reward function, which is a smaller learning problem.
- Core assumption: The true reward can be approximated as a sum of available prior knowledge and a learnable offset.
- Evidence anchors:
  - [abstract] "RRM assumes the true reward decomposes into a prior reward (available before training) plus a learned residual term trained with human preferences."
  - [Section 3] "RRMs assume that the true reward of the task can be divided into two components: a prior reward r0 and a learned reward r′ψ."
  - [corpus] Related PbRL work (SENIOR, CLARIFY) focuses on query efficiency via active selection, but does not propose the residual decomposition mechanism.
- Break condition: If the prior reward is actively misleading (opposite semantics) and the residual model lacks capacity, learning may be delayed though the paper shows RRM can eventually recover.

### Mechanism 2
- Claim: Using a fixed prior avoids optimization instability caused by switching loss functions across training phases.
- Mechanism: Pre-training a neural network with one loss (e.g., MSE for demonstrations) and then fine-tuning with another (e.g., cross-entropy for preferences) produces inconsistent gradient magnitudes and landscapes, leading to unpredictable behavior. RRM sidesteps this by treating $r_0$ as fixed.
- Core assumption: The prior reward is sufficiently informative that it need not be updated; all adaptation occurs in $r'_\psi$.
- Evidence anchors:
  - [abstract] "Prior work has proposed learning a reward model from demonstrations and fine-tuning it using preferences. However, when the model is a neural network, using different loss functions for pre-training and fine-tuning can pose challenges to reliable optimization."
  - [Section 1] "If we first train using MSE loss on the demonstration and then use cross-entropy loss on preference data to train the reward model, it becomes highly unstable."
  - [corpus] No direct corpus papers address the loss-function-switching instability; this mechanism is specific to RRM.
- Break condition: If the prior reward requires significant adjustment beyond what the bounded residual (tanh-limited to $[-1, 1]$) can express, the model may underfit.

### Mechanism 3
- Claim: Prior rewards that incentivize the first subtask of a sequential decomposition are most effective.
- Mechanism: Complex manipulation tasks can be decomposed into sequential stages. A prior reward that captures the earliest necessary subtask (e.g., reaching the object) enables the agent to quickly generate meaningful trajectories, which in turn produces more informative preference queries.
- Core assumption: Tasks have decomposable structure and the first subtask is identifiable from domain knowledge.
- Evidence anchors:
  - [Section 4.7] "Prior rewards need to provide the 'first step' reward of the task... an effective prior reward should fit the reward of the task that must be completed first in this series of tasks."
  - [Section 4.7] "Proxy reward 2 (end-effector to object distance) is always better than proxy reward 1 (object to goal distance), and at times it even surpasses the complete proxy reward."
  - [corpus] No corpus papers explicitly study prior reward selection; this is a paper-specific empirical finding.
- Break condition: If the task is not naturally sequential or the first subtask is misidentified, the guidance from the prior may be weak or counterproductive.

## Foundational Learning

- Concept: Bradley-Terry preference model
  - Why needed here: RRM trains $r'_\psi$ by maximizing likelihood of observed preferences using this model. Understanding $P[\sigma_0 \succ \sigma_1] \propto \exp(\sum_t \hat{r}(s_t, a_t))$ is essential.
  - Quick check question: Given two trajectory segments with cumulative rewards 5 and 2, what is the probability that segment 1 is preferred under the Bradley-Terry model?

- Concept: Soft Actor-Critic (SAC)
  - Why needed here: RRM is implemented on top of PEBBLE, which uses SAC as its RL backbone. The reward model outputs are used as the training signal for SAC.
  - Quick check question: What role does entropy regularization play in SAC, and why might it be helpful in early exploration when rewards are sparse?

- Concept: Inverse Reinforcement Learning (IRL) / AIRL
  - Why needed here: One source of prior rewards is imitation learning via IRL. Understanding how AIRL extracts rewards from demonstrations helps interpret the "imitation reward" ablation.
  - Quick check question: Why might a reward learned from demonstrations fail to transfer to states not covered by the demonstrations (distribution shift)?

## Architecture Onboarding

- Component map:
  - Prior reward function $r_0$ -> Residual reward network $r'_\psi$ -> Total reward $\hat{r} = r_0 + r'_\psi$ -> SAC RL agent
  - Preference buffer stores trajectory pairs with human feedback
  - Replay buffer stores transitions with both $r_0$ and $\hat{r}_{RRM}$

- Critical path:
  1. Initialize $r'_\psi$, SAC, empty buffers
  2. Collect trajectories using $\hat{r}_{RRM}$ (initially dominated by $r_0$)
  3. Sample segment pairs, query preferences, update preference buffer
  4. Train $r'_\psi$ on preference buffer via cross-entropy
  5. Relabel replay buffer with updated $\hat{r}_{RRM}$
  6. Update SAC on relabeled data; repeat from step 2

- Design tradeoffs:
  - **Prior quality vs. residual capacity**: Strong priors reduce learning burden but may bias if incorrect. Bounded residuals prevent overcorrection but limit expressiveness.
  - **Feedback frequency vs. convergence speed**: More feedback accelerates reward model convergence but increases human burden. Paper shows RRM degrades gracefully under reduced feedback.
  - **State-based vs. image-based**: Image-based requires encoder (pretrained or joint); proprioceptive state still needed for computing proxy rewards.

- Failure signatures:
  - **Stochastic feedback tolerance**: RRM handles stochastic (inconsistent) preferences reasonably well.
  - **Mistaken feedback sensitivity**: Under 10% label flips, RRM improves over PEBBLE but performance still degrades significantly.
  - **Opposite prior recovery**: With negated proxy rewards, RRM eventually succeeds but converges more slowly than with correct priors.
  - **Uninformative prior + no unsupervised pretraining**: RRM with zero prior slightly underperforms PEBBLE because it skips PEBBLE's unsupervised warmup.

- First 3 experiments:
  1. **Prior ablation on a single Meta-World task**: Run RRM with (a) complete proxy, (b) proxy 2, (c) zero prior, (d) opposite prior. Compare success rate curves to validate the residual mechanism and prior quality effects.
  2. **Reduced feedback robustness**: Cut total feedback from 10,000 to 2,000 and compare RRM vs. PEBBLE on Button-press. Expect RRM to maintain >90% success while PEBBLE degrades.
  3. **Opposite prior recovery dynamics**: Plot $r_0$, $r'_\psi$, $\hat{r}_{RRM}$, and reward accuracy over training to verify the mechanism—residual should grow to dominate the negated prior.

## Open Questions the Paper Calls Out
- **How to obtain high-quality prior rewards**: The paper states "how to obtain high-quality prior rewards remains an open problem" and notes that current approaches rely on manual design or expensive demonstrations, suggesting the need for methods that can automatically generate informative feedback or construct reliable priors.
- **Robustness to mistaken human feedback**: Section 4.6 notes "Mistaken preferences can significantly impact the performance of RRM," and the paper identifies reliance on human feedback as limiting large-scale deployment, indicating a need for extensions to handle significant proportions of irrational feedback.
- **Generalization of the "first step" heuristic**: The paper analyzes results to hypothesize that "prior rewards need to provide the 'first step' reward" but acknowledges this is derived from empirical observation across 5 tasks without theoretical grounding, leaving open whether this heuristic generalizes to complex, long-horizon environments.

## Limitations
- Assumes the prior reward is fixed and does not account for potential distribution shift between the prior's training data and the agent's exploration.
- The bounded residual ($[-1, 1]$ via tanh) may limit expressiveness in tasks requiring large reward corrections.
- The prior selection heuristic (first subtask) is empirical and not formally justified.
- Image-based experiments rely on a pretrained encoder, which is a strong assumption not available in all domains.

## Confidence
- High confidence: The core RRM mechanism (decomposing reward into fixed prior + bounded residual) and its benefits for training stability and sample efficiency on the tested Meta-World tasks.
- Medium confidence: The robustness claims under reduced feedback and noisy preferences, as these are tested with synthetic oracle preferences which may not reflect real human behavior.
- Medium confidence: The prior selection guideline (first subtask), as it is based on empirical observation across 5 tasks without theoretical grounding.

## Next Checks
1. Test RRM on a task where the prior reward has significant systematic bias (not just opposite sign) to quantify the limits of the bounded residual in correcting strong priors.
2. Conduct a user study with real human feedback to validate the synthetic feedback robustness results, particularly for stochastic and mistaken feedback scenarios.
3. Implement an adaptive prior update mechanism (e.g., slow fine-tuning of $r_0$) to test whether allowing limited prior adjustment improves performance in distribution-shifted settings compared to the fully fixed prior.