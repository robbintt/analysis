---
ver: rpa2
title: 'On-the-Fly Adaptation to Quantization: Configuration-Aware LoRA for Efficient
  Fine-Tuning of Quantized LLMs'
arxiv_id: '2509.25214'
source_url: https://arxiv.org/abs/2509.25214
tags:
- uni00000013
- uni00000011
- uni00000014
- uni0000001b
- uni00000015
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CoA-LoRA introduces a configuration-aware method to dynamically
  adjust LoRA adapters to arbitrary quantization settings without repeated fine-tuning.
  It employs a configuration-aware model that maps quantization configurations to
  low-rank adjustments for each layer, reducing dimensionality and enabling efficient
  parallel computation.
---

# On-the-Fly Adaptation to Quantization: Configuration-Aware LoRA for Efficient Fine-Tuning of Quantized LLMs

## Quick Facts
- arXiv ID: 2509.25214
- Source URL: https://arxiv.org/abs/2509.25214
- Reference count: 40
- One-line primary result: CoA-LoRA achieves 1.74%-8.89% accuracy gains over QLoRA and 2%-7% hypervolume improvement over LQ-LoRA, enabling efficient fine-tuning of quantized LLMs across arbitrary per-layer quantization settings without retraining.

## Executive Summary
CoA-LoRA introduces a configuration-aware method to dynamically adjust LoRA adapters to arbitrary quantization settings without repeated fine-tuning. It employs a configuration-aware model that maps quantization configurations to low-rank adjustments for each layer, reducing dimensionality and enabling efficient parallel computation. A Pareto-based configuration search optimizes the training configuration set, ensuring high-quality, diverse configurations. Experiments on GLUE tasks show CoA-LoRA achieves accuracy gains of 1.74%-8.89% over QLoRA and 2%-7% hypervolume improvement over LQ-LoRA, while eliminating the need for separate fine-tuning per configuration. The method scales effectively to models of varying sizes and maintains stable performance under aggressive quantization, demonstrating its efficiency and robustness for deploying large language models on heterogeneous edge devices.

## Method Summary
CoA-LoRA enables efficient fine-tuning of quantized LLMs by training a configuration-aware hypernetwork that maps quantization configurations (bit-widths, block sizes) to lightweight adjustments for existing LoRA adapters. Instead of generating full LoRA matrices, the hypernetwork outputs small r×r adjustment matrices that linearly transform the pre-existing L2 matrices. A Pareto-based Gaussian Process search with Expected Hypervolume Improvement (EHVI) optimizes the training configuration set, ensuring high-quality, diverse samples. The method eliminates the need for separate fine-tuning per configuration, supporting on-the-fly adaptation to arbitrary quantization settings through parallel computation of adjustments for all layers.

## Key Results
- CoA-LoRA achieves 1.74%-8.89% accuracy gains over QLoRA and 2%-7% hypervolume improvement over LQ-LoRA on GLUE tasks
- The method eliminates the need for separate fine-tuning per quantization configuration, reducing training time by ~9x
- CoA-LoRA maintains stable performance under aggressive quantization (2.5-7.25 bits) and scales effectively to models of varying sizes (LLaMA-2-7B, Qwen-2.5-1.5B/3B)

## Why This Works (Mechanism)

### Mechanism 1: Low-Rank Output Space Restriction
Restricting the configuration-aware model to predict only an r×r adjustment matrix (rather than full LoRA weights) reduces the learning burden sufficiently to allow generalization across quantization settings. The model outputs a small square matrix U_θ that linearly transforms the pre-existing L2 matrix via (I + U_θ)L2, assuming most adaptation information is already captured in L2 and only residual adjustment is needed. The core assumption is that the signal required to adapt to specific quantization configurations is lower-dimensional than the signal required for initial fine-tuning tasks.

### Mechanism 2: Configuration-Aware Hypernetwork
A lightweight MLP functions as a hypernetwork that maps discrete quantization parameters (bits, block sizes) to continuous adjustment values. The architecture embeds layer-specific configurations and positional information into a vector that the MLP processes to produce the adjustment matrix. This decouples training time from the number of possible configurations, allowing "on-the-fly" inference. The relationship between a layer's quantization configuration and the optimal LoRA adjustment is assumed to be a smooth, learnable function that can be interpolated from a limited training configuration set.

### Mechanism 3: Pareto-Based Training Distribution Optimization
The effectiveness of the configuration-aware model depends on training it on a diverse, high-quality set of configurations rather than random samples. A Gaussian Process models performance (accuracy vs. bit-width) and selects new training configurations that maximize Expected Hypervolume Improvement (EHVI). This iteratively refines the training set to cover the Pareto frontier, preventing the mapper from overfitting to suboptimal or redundant configurations. The assumption is that training on points near the Pareto frontier teaches the mapper the most robust adjustment strategies.

## Foundational Learning

- **Concept: Low-Rank Adaptation (LoRA)**
  - Why needed here: Understanding LoRA decomposition into L1 and L2 matrices is crucial for grasping why the paper targets L2 for adjustment
  - Quick check question: Can you explain why L2 is typically the "output" projection in the LoRA decomposition W = W0 + L1 L2, and how the dimensions of L2 differ from L1?

- **Concept: Weight Quantization (Non-Uniform/NF)**
  - Why needed here: The configuration inputs to the model are parameters of the NormalFloat (NF) quantization scheme (block sizes, bit-widths)
  - Quick check question: What is the difference between uniform and non-uniform quantization, and why would "layer-wise" bit-width selection create a high-dimensional configuration space?

- **Concept: Bayesian Optimization & Hypervolume**
  - Why needed here: The paper uses a Gaussian Process and Hypervolume Improvement to search the configuration space
  - Quick check question: In multi-objective optimization (accuracy vs. model size), what does the "Pareto front" represent, and why does maximizing "Hypervolume" lead to a better training set?

## Architecture Onboarding

- **Component map**: Input Config -> Config Embedder -> Mapper MLP -> Generate U_θ -> Apply to L2 -> Forward Pass -> Loss
- **Critical path**: Input quantization configuration flows through the embedder, through the MLP mapper to generate the adjustment matrix, which is applied to the base L2 matrix, then used in the forward pass with loss computation
- **Design tradeoffs**: Output size choice between r×r adjustment vs d×r full matrix trades expressiveness for generalization and speed; search budget (number of GP iterations) vs quality of training set
- **Failure signatures**: Generalization collapse (works on training configurations but fails on unseen heterogeneous ones, indicating overfitting); identity collapse (mapper outputs zero matrix or I, resulting in baseline Shared-LoRA performance, implying MLP failed to learn meaningful mapping)
- **First 3 experiments**:
  1. L2 Stability Check: Train standard LoRA on 3 distinct bit-widths (2, 4, 6), compute cosine similarity between resulting L2 matrices; low similarity (< 0.5) invalidates core assumption
  2. Fixed vs. Searched Configs: Train CoA-LoRA mapper using uniformly sampled vs Pareto-search configuration sets, plot resulting accuracy curve to validate configuration search mechanism
  3. Unseen Config Inference: Train on configurations 3.5-5 bits, evaluate on held-out 2.5-bit configuration to test interpolation capability of hypernetwork

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions beyond those already discussed in the limitations and confidence assessment sections.

## Limitations
- The fundamental assumption that LoRA L2 matrices remain stable across quantization configurations is validated only empirically within a limited range (2-8 bits)
- The Gaussian Process-based configuration search relies on finite-difference gradients, which may become unreliable in high-dimensional, discontinuous configuration spaces
- Performance on highly heterogeneous configurations (extreme per-layer bit-width combinations) is not extensively tested

## Confidence
- **High**: Empirical results showing CoA-LoRA's accuracy gains over QLoRA and LQ-LoRA on GLUE tasks are reproducible and well-supported by ablation studies
- **Medium**: Architectural claim that restricting mapper to output r×r adjustment matrix is sufficient for generalization is plausible but lacks theoretical justification
- **Low**: Claim of "scalability to models of varying sizes" is based on single experiment with LLaMA-2-7B and Qwen-2.5-1.5B/3B; behavior on significantly larger/smaller models is unknown

## Next Checks
1. **Cross-Configuration Stability**: Systematically measure cosine similarity of L2 matrices across a grid of quantization configurations (2, 4, 6, 8 bits) for multiple layers; sharp drop in similarity would invalidate core low-rank adjustment assumption
2. **Unseen Configuration Robustness**: Train CoA-LoRA mapper on configurations 3.5-5 bits, evaluate performance on held-out 2.5-bit or 6-bit configuration; significant performance degradation indicates failure of hypernetwork's interpolation capability
3. **Ablation of Search Strategy**: Replace Pareto-based Gaussian Process search with naive random sampling for generating training configuration set; substantially lower resulting mapper accuracy would confirm necessity of EHVI-guided search for finding robust configuration set