---
ver: rpa2
title: Mirror, Mirror on the Wall -- Which is the Best Model of Them All?
arxiv_id: '2512.02043'
source_url: https://arxiv.org/abs/2512.02043
tags:
- arxiv
- medical
- language
- available
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a systematic approach for selecting the
  most suitable large language model (LLM) for a given task or use case. The proposed
  Model Selection Methodology (MSM) is organized into three stages: critical requirements
  (task specification, model size, licensing), important considerations (domain specialization,
  language support, ecosystem integration), and contextual validation (community feedback,
  benchmark performance, early prototyping).'
---

# Mirror, Mirror on the Wall -- Which is the Best Model of Them All?

## Quick Facts
- arXiv ID: 2512.02043
- Source URL: https://arxiv.org/abs/2512.02043
- Reference count: 40
- Primary result: Introduces a three-stage Model Selection Methodology (MSM) for choosing LLMs based on critical requirements, important considerations, and contextual validation

## Executive Summary
This paper presents a systematic approach for selecting the most suitable large language model (LLM) for a given task or use case. The proposed Model Selection Methodology (MSM) organizes the selection process into three progressive stages: critical requirements (task specification, model size, licensing), important considerations (domain specialization, language support, ecosystem integration), and contextual validation (community feedback, benchmark performance, early prototyping). The authors demonstrate their approach using medical domain leaderboards and benchmarks, emphasizing the need to balance quantitative performance metrics with qualitative factors such as licensing, maintenance, and practical deployment considerations.

## Method Summary
The methodology employs a three-stage pyramid framework for LLM selection. Stage 1 applies hard constraints including task alignment, size limits, and license compatibility to filter out incompatible models early. Stage 2 evaluates important factors like domain specialization, language support, and ecosystem integration. Stage 3 conducts contextual validation through benchmark analysis, community signal aggregation, and early prototyping on use-case-specific data. The approach aims to provide a structured framework that helps practitioners navigate the rapidly growing landscape of LLMs and make informed decisions based on both technical capabilities and real-world constraints.

## Key Results
- Proposes a systematic three-stage methodology for LLM selection organized by criticality of requirements
- Demonstrates the framework using medical domain benchmarks and leaderboards (Open Medical-LLM, MIRAGE, MedHELM, ClinicBench)
- Highlights the importance of balancing quantitative performance metrics with qualitative factors like licensing and ecosystem integration
- Identifies benchmark contamination as a key challenge requiring multi-signal validation approaches

## Why This Works (Mechanism)

### Mechanism 1
Sequential filtering through progressively context-specific criteria improves model selection efficiency by eliminating incompatible candidates early. The three-stage pyramid applies hard constraints first (license, size, task alignment) before investing effort in softer evaluations (community signals, prototyping). This reduces the search space before costly validation steps. Core assumption: Critical-stage failures are truly disqualifying; organizations cannot or will not adjust project requirements to accommodate a preferred model.

### Mechanism 2
Combining quantitative benchmarks with qualitative community signals provides more robust selection signals than either alone. Benchmark scores capture task-specific performance but suffer from contamination and saturation. Community signals (downloads, likes) capture practical usability, documentation quality, and ecosystem integration—factors benchmarks miss. Cross-referencing both reduces selection error. Core assumption: Community endorsement correlates with real-world reliability; high download/like counts reflect genuine utility rather than marketing or automation artifacts.

### Mechanism 3
Early prototyping on use-case-specific data validates theoretical selections before full commitment. After filtering through stages 1-2, prototyping exposes mismatches between benchmark performance and actual task behavior (e.g., domain vocabulary, output format requirements, latency constraints) that ranking-based selection cannot predict. Core assumption: Short prototyping efforts reveal representative model behavior; performance on sample data generalizes to production scale.

## Foundational Learning

- **Concept: LLM Benchmark Types and Limitations**
  - Why needed here: The paper assumes readers understand what benchmarks measure (accuracy, F1, BLEU, ROUGE) and their failure modes (contamination, saturation, selection bias). Without this, the critique of leaderboard reliance is opaque.
  - Quick check question: Can you explain why a model scoring 95% on MedQA might still fail in clinical practice?

- **Concept: License Classes and Commercial Implications**
  - Why needed here: Stage 1 filtering hinges on license compatibility. Readers must distinguish permissive (Apache 2.0, MIT) from restrictive (LLaMA, Gemma) licenses and their commercial/research constraints.
  - Quick check question: If your company wants to deploy a model commercially, which license types would you immediately exclude?

- **Concept: RAG Architecture for Domain Adaptation**
  - Why needed here: The MIRAGE leaderboard and MedRAG toolkit use retrieval-augmented generation as a solution to hallucination. Understanding RAG helps contextualize why domain-specific retrievers (MedCPT, SPECTER) matter.
  - Quick check question: How does RAG reduce hallucination compared to pure LLM generation?

## Architecture Onboarding

- **Component map:** Stage 1 Filter -> Stage 2 Filter -> Stage 3 Validator
- **Critical path:** 1) Define task specification and hard constraints (Stage 1 inputs) 2) Query model registry (e.g., Hugging Face API) with filters 3) Score remaining candidates on Stage 2 criteria 4) Run lightweight prototypes on top 3-5 candidates 5) Document decision rationale with evidence
- **Design tradeoffs:** Speed vs. thoroughness (quick filtering vs. thorough scoring), quantitative vs. qualitative weight (benchmark ranking vs. community signals), prototyping depth (shallow vs. deep tests)
- **Failure signatures:** Selected model passes benchmarks but fails on domain-specific vocabulary (insufficient prototyping), no models survive Stage 1 (constraints too restrictive), community-favored model lacks maintenance (check commit history before trusting downloads/likes)
- **First 3 experiments:** 1) Constraint calibration test: Run Stage 1 filters on a known-compatible model to validate filter logic 2) Signal correlation check: For 10 models, compare benchmark rank vs. community signals 3) Prototype delta analysis: For top 3 candidates, run identical prompts and quantify performance gap

## Open Questions the Paper Calls Out

### Open Question 1
Can the Model Selection Methodology be empirically validated to demonstrate improved model selection outcomes compared to ad-hoc or intuition-based selection processes? The paper provides no experimental evidence measuring whether practitioners using MSM make better model choices than those using unstructured approaches.

### Open Question 2
How can benchmark contamination in LLMs be reliably detected and quantified when model training data is not publicly disclosed? The paper identifies contamination as a key challenge but does not address detection methods for already-trained models with opaque training corpora.

### Open Question 3
What protocols can establish the reliability and validity of using LLMs as judges to evaluate other LLMs, particularly for subjective qualities like reasoning and helpfulness? The paper notes that "using LLMs as judges... is still under investigation" but provides no framework for calibration or bias analysis.

## Limitations
- Lacks quantitative validation of MSM effectiveness compared to alternative selection approaches
- Does not specify decision thresholds or weighting schemes for balancing conflicting criteria
- No established protocol for early prototyping sample size or evaluation rubrics

## Confidence

**High Confidence:** The sequential filtering approach (Critical → Important → Contextual Validation) represents a logical framework for narrowing model candidates. The distinction between hard constraints and softer evaluation criteria is methodologically sound.

**Medium Confidence:** The claim that combining benchmarks with community signals provides more robust selection signals is plausible but unproven. The mechanism assumes community endorsement correlates with real-world reliability, yet the paper acknowledges that highly downloaded models are not necessarily the most liked models.

**Low Confidence:** The assertion that early prototyping reliably validates theoretical selections before full commitment lacks supporting evidence. Without specified protocols for sample size, evaluation rubrics, or generalization guarantees, prototyping results may not predict production performance.

## Next Checks

1. **Effectiveness comparison study:** Implement MSM on three distinct use cases and compare selected models against those chosen through benchmark-only or random selection approaches. Measure actual performance on held-out task-specific data over a 3-month deployment period.

2. **Constraint sensitivity analysis:** Systematically relax each Stage 1 constraint (license, size, task alignment) for cases where no models pass filtering. Document how often requirement adjustments lead to better long-term outcomes versus finding new models.

3. **Community signal validation:** For 20 models across different domains, compare community metrics (downloads, likes, forks) against longitudinal performance data on multiple benchmarks over 12 months. Determine whether high community engagement predicts sustained benchmark performance or indicates marketing activity.