---
ver: rpa2
title: Expressive Value Learning for Scalable Offline Reinforcement Learning
arxiv_id: '2510.08218'
source_url: https://arxiv.org/abs/2510.08218
tags:
- learning
- offline
- policy
- arxiv
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EVOR, a scalable offline reinforcement learning
  approach that integrates expressive value learning via flow matching. The key innovation
  is learning an optimal, regularized Q-function using flow-based temporal difference
  learning, avoiding computationally expensive backpropagation through time and policy
  distillation.
---

# Expressive Value Learning for Scalable Offline Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.08218
- Source URL: https://arxiv.org/abs/2510.08218
- Authors: Nicolas Espinosa-Dice; Kiante Brantley; Wen Sun
- Reference count: 40
- Primary result: EVOR achieves 90.2% average success rate across 25 robotic control tasks, outperforming Q-chunking baselines by 5.1-12.8% through flow-based temporal difference learning and inference-time policy extraction.

## Executive Summary
This paper introduces EVOR, a scalable offline reinforcement learning approach that integrates expressive value learning via flow matching. The key innovation is learning an optimal, regularized Q-function using flow-based temporal difference learning, avoiding computationally expensive backpropagation through time and policy distillation. EVOR achieves this by training a reward model through flow matching and extracting policies at inference-time via rejection sampling against the learned value function. Empirical results show EVOR outperforms baseline methods (Q-chunking with and without action chunking) across 25 tasks spanning five robotic control environments, demonstrating the benefits of expressive value learning in offline RL. The approach enables inference-time scaling and regularization without retraining.

## Method Summary
EVOR learns an optimal, regularized Q-function via flow matching during training, then performs inference-time policy extraction via rejection sampling against the expressive value function. The method consists of three phases: (1) base policy training via flow-matching behavioral cloning to learn the data-generating policy, (2) reward model training via flow-based temporal difference learning to learn the reward-to-go distribution, and (3) inference-time policy extraction by sampling actions from the base policy and selecting via softmax over Q* values estimated from the reward-to-go distribution. The approach uses MLPs with 4×512 architecture, trained with Adam at learning rate 3e-4 for 1M steps, with evaluation every 100K steps over 5 seeds per task.

## Key Results
- EVOR achieves 90.2% average success rate across 25 robotic control tasks from OGBench v1.1.0
- Outperforms Q-chunking baselines by 5.1-12.8% across all five tested environments
- Inference-time scaling with N_π=32 actions achieves best performance, demonstrating value of rejection sampling
- Temperature τ_Q controls trade-off between base policy behavior (high τ_Q) and greedy value exploitation (low τ_Q)

## Why This Works (Mechanism)

### Mechanism 1: Flow-Based Temporal Difference Learning
Flow matching can be integrated into TD learning to learn an expressive value function that models reward-to-go distributions rather than scalar values. The distributional Bellman equation holds under distributions: Z(x,a) ≐ r(x,a) + γZ(X',A'). Flow matching learns to transport Gaussian noise z₀ ∼ N(0,I) to reward-to-go samples z₁ ∼ R(·|x,a) via a velocity field. The TD target becomes s_target := r(x,a) + γE_{a'∼π_base}[s̄_θ(z_t|x',a',t)], matching velocities between LHS and RHS of the distributional Bellman. Core assumption: The base policy π_base ≈ π_ref (the data-generating policy) is learned sufficiently well via behavioral cloning.

### Mechanism 2: Inference-Time Policy Extraction via Rejection Sampling
Policy optimization can be deferred entirely to inference time by sampling action candidates and selecting via a softmax over Q* values, avoiding policy gradients and BPTT. At inference, sample N_π actions {a^(i)} from π_base(·|x). For each action, sample N reward-to-go samples r^(i,j) ∼ R_θ(·|x,a^(i)). Compute Q*_θ via log-sum-exp approximation. Select action via softmax: a* ∼ softmax(Q*_θ(x,a^(i))/τ_Q). Core assumption: The rejection sampling approximation (finite N_π, N) is sufficient to approximate the argmax over the continuous action space.

### Mechanism 3: Optimal Regularized Q-Function via Distributional Perspective
The optimal KL-regularized Q-function Q* can be expressed analytically from the reward-to-go distribution, enabling direct optimization without iterative policy updates. Under deterministic transitions (Theorem 1 from Ziebart et al.), Q*_h(x_h,a_h) = η ln E_{z∼R_h(·|x_h,a_h)}[exp(z/η)]. By learning R_θ ≈ R via flow-based TD, Q* can be approximated via Monte Carlo: Q*_θ(x,a) ≈ τ_R · LogSumExp({r^(j)/τ_R}). Core assumption: Deterministic transitions hold (acknowledged as strong but commonly assumed in offline RL analysis).

## Foundational Learning

- **Flow Matching (Continuous Normalizing Flows)**
  - Why needed here: Core generative model for both policy (behavioral cloning) and value function (reward-to-go distribution). Understanding the ODE formulation d/dt ϕ_t(x) = v_t(ϕ_t(x)) and linear interpolation training is essential.
  - Quick check question: Can you explain why the flow matching loss uses ||v_θ(t, x_t) - (x_1 - x_0)||² with linear interpolation?

- **Distributional Reinforcement Learning**
  - Why needed here: EVOR models the full distribution of returns Z(x,a) rather than expected return Q(x,a). Requires understanding distributional Bellman equations and why they preserve more information.
  - Quick check question: What additional information does a distributional critic capture that a scalar Q-function cannot?

- **KL-Regularized Offline RL Objective**
  - Why needed here: The entire derivation hinges on the soft value formulation V^π,η = E_π[Σr - η·KL(π||π_ref)] and its optimal solution. Understanding why this regularization prevents distribution shift is critical.
  - Quick check question: Why does the optimal regularized policy satisfy π*(a|x) ∝ π_ref(a|x)·exp(Q*(x,a)/η)?

## Architecture Onboarding

- **Component map**:
  Base Policy Network (v_ϕ(a,t|x)) -> Reward Model Network (s_θ(z,t|x,a)) -> Target Network (s̄_θ) -> Inference Module (Log-sum-exp + softmax selector)

- **Critical path**:
  1. Verify behavioral cloning converges (base policy can reconstruct offline actions)
  2. Check flow-based TD loss decreases (reward model learns reward-to-go distribution)
  3. Validate inference-time scaling (increasing N_π improves performance up to saturation—see Figure 1)

- **Design tradeoffs**:
  - Expressivity vs. inference speed: Larger N_π improves action selection but increases latency. Paper uses N_π=32 at evaluation (Table 3).
  - Regularization vs. optimization: Temperature τ_R controls softmax sharpness on reward-to-go samples; τ_Q controls action selection greediness. High τ_Q → base policy behavior; low τ_Q → greedy value exploitation (Figure 2).
  - Assumption: Deterministic transitions used for Q* derivation—empirical robustness to stochasticity not formally proven.

- **Failure signatures**:
  - Base policy underfits: Action reconstruction error high → R_θ will learn biased reward-to-go distribution → Q* estimates will be wrong
  - Insufficient N_π: Performance plateaus early or degrades at high N_π (saturation behavior indicates value function errors dominate)
  - τ_R too high: Q* estimates become smooth/uninformative (Figure 4 shows performance collapse at extreme τ_R values)

- **First 3 experiments**:
  1. **Behavioral cloning sanity check**: Train base policy alone on offline data. Verify action reconstruction loss converges and sample quality (visual inspection for locomotion, success rate for manipulation tasks with N_π=1).
  2. **Value learning ablation**: Train reward model with fixed frozen base policy. Monitor flow-based TD loss and compare learned reward-to-go mean against empirical Monte Carlo returns from dataset.
  3. **Inference scaling sweep**: On a single task (e.g., cube-double-play-task1), sweep N_π ∈ {1,2,4,8,16,32,64} and plot success rate. Verify monotonic improvement to saturation (replicate Figure 1 pattern).

## Open Questions the Paper Calls Out
None

## Limitations
- Deterministic transition assumption used for Q* derivation is acknowledged but not formally relaxed
- Flow-based value learning is a novel contribution without direct corpus support for generalization beyond tested domains
- Inference-time scaling requires per-task tuning of N_π and temperatures for optimal performance

## Confidence
- **High**: Offline RL objective formulation, flow-matching training mechanics, and empirical performance gains over Q-chunking baselines
- **Medium**: Theoretical claims about optimal Q* derivation (dependent on deterministic assumption) and scalability of inference-time policy extraction
- **Low**: Robustness to stochastic dynamics and generalization to non-OGBench domains

## Next Checks
1. Evaluate EVOR on a stochastic environment variant (e.g., with injected action noise) to test robustness to the deterministic transition assumption
2. Compare flow-based TD convergence speed and stability against distributional RL baselines (C51, QR-DQN) on the same reward-to-go modeling task
3. Perform ablation on flow velocity field architecture complexity to quantify expressivity vs. parameter efficiency tradeoffs