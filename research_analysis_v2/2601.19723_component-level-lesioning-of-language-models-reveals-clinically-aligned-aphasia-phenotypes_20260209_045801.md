---
ver: rpa2
title: Component-Level Lesioning of Language Models Reveals Clinically Aligned Aphasia
  Phenotypes
arxiv_id: '2601.19723'
source_url: https://arxiv.org/abs/2601.19723
tags:
- aphasia
- language
- units
- broca
- wernicke
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper investigates whether large language models can simulate\
  \ aphasia by lesioning specific components, aiming to provide a scalable proxy for\
  \ studying language disorders. The method uses a clinically grounded pipeline to\
  \ identify and perturb components in MoE and dense transformer models linked to\
  \ Broca\u2019s and Wernicke\u2019s aphasia subtypes, then evaluates impairments\
  \ with Western Aphasia Battery subtests summarized by the Aphasia Quotient."
---

# Component-Level Lesioning of Language Models Reveals Clinically Aligned Aphasia Phenotypes

## Quick Facts
- **arXiv ID:** 2601.19723
- **Source URL:** https://arxiv.org/abs/2601.19723
- **Reference count:** 14
- **Primary result:** Phenotype-targeted perturbations of language model components produce graded, subtype-specific language deficits that outperform random lesions, with MoE models showing more localized and interpretable mappings to aphasia symptoms.

## Executive Summary
This paper investigates whether large language models can simulate aphasia by lesioning specific functional components, aiming to provide a scalable proxy for studying language disorders. The method uses a clinically grounded pipeline to identify and perturb components in MoE and dense transformer models linked to Broca's and Wernicke's aphasia subtypes, then evaluates impairments with Western Aphasia Battery subtests summarized by the Aphasia Quotient. Results show that phenotype-targeted perturbations produce graded, subtype-specific language deficits, outperforming random lesions, with MoE models exhibiting more localized and interpretable mappings between components and aphasia symptoms.

## Method Summary
The method identifies subtype-linked components through gradient-based attribution during fine-tuning on AphasiaBank (Broca/Wernicke subsets), ranks components by cumulative gradient×weight importance scores, and progressively perturbs top-ranked components (0.5%, 1%, 1.5%, 2%) using activation zeroing or Xavier re-initialization. Impairments are quantified using Western Aphasia Battery subtests adapted to text-only format, summarized as the Aphasia Quotient. The pipeline is validated by comparing phenotype-targeted lesions to size-matched random perturbations, and by aligning perturbed components with linguistic phenomena using BLiMP zero-ablation.

## Key Results
- Phenotype-targeted perturbations yield more systematic, aphasia-like regressions than size-matched random perturbations
- MoE modularity supports more localized and interpretable phenotype-to-component mappings than dense Transformers
- The pipeline achieves subtype separability (85-90% for OLMoE, 80-95% for OLMo) as validated by external CAP classifier

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Gradient-based attribution during aphasia subtype fine-tuning identifies clinically relevant functional components that, when lesioned, produce subtype-specific language deficits.
- **Mechanism:** Fine-tuning on AphasiaBank (Broca/Wernicke subsets) creates phenotype-specific models; cumulative gradient×weight importance scores rank components by their contribution to each subtype. Top-ranked components are then targeted for perturbation.
- **Core assumption:** Components with high gradient-based contributions during subtype fine-tuning are causally involved in producing subtype-specific linguistic behaviors (not merely memorizing corpus patterns).
- **Evidence anchors:**
  - [abstract] "identifies subtype-linked components for Broca's and Wernicke's aphasia... interprets these components with linguistic probing tasks"
  - [Section 3.3] Equations 2-3 define gradient-based importance aggregation; CAP classifier validates subtype separability (85-90% for OLMoE, 80-95% for OLMo)
  - [corpus] Related work "Generating Completions for Broca's Aphasic Sentences" confirms LLMs can model aphasic output patterns, though with different methodology.
- **Break condition:** If subtype fine-tuning produced style-mimicry without functional correspondence, lesioning attributed components would yield random or non-specific degradation rather than subtype-aligned profiles.

### Mechanism 2
- **Claim:** Progressive perturbation of subtype-linked components produces graded, dose-responsive declines on clinical measures (WAB/AQ), with phenotype-targeted lesions outperforming random lesions.
- **Mechanism:** Units ranked by subtype contribution are lesioned at increasing proportions (0.5%, 1%, 1.5%, 2%). Two lesion modes—activation zeroing (silencing) and Xavier re-initialization (scrambling)—model different impairment types. WAB subtests quantify resulting deficits.
- **Core assumption:** AQ decline reflects aphasia-like functional impairment rather than generic capability loss; the clinical metric captures linguistically meaningful degradation.
- **Evidence anchors:**
  - [abstract] "subtype-targeted perturbations yield more systematic, aphasia-like regressions than size-matched random perturbations"
  - [Section 5.3] Figure 4 shows monotonic AQ decline with lesion proportion; phenotype-targeted lesions cause larger drops than random baselines at matched budgets
  - [corpus] The Text Aphasia Battery (TAB) paper [arxiv:2511.20507] provides converging evidence that clinically-grounded benchmarks can assess aphasia-like deficits in LLMs.
- **Break condition:** If AQ decline reflected only general language degradation, Broca and Wernicke lesions would produce indistinguishable profiles; subtype separation would disappear.

### Mechanism 3
- **Claim:** MoE architecture supports more localized and interpretable phenotype-to-component mappings than dense Transformers due to emergent expert specialization.
- **Mechanism:** In MoE, each component is a discrete expert; in dense models, components are FFN hidden dimensions. BLiMP zero-ablation creates phenomenon-to-unit attribution maps; alignment heatmaps reveal whether subtype-linked units cluster on specific linguistic phenomena.
- **Core assumption:** Observed locality differences reflect architectural modularity rather than confounds from routing patterns or parameter count.
- **Evidence anchors:**
  - [abstract] "MoE modularity supports more localized and interpretable phenotype-to-component mappings"
  - [Section 5.2] Figure 2 shows MoE experts form clearer block patterns across BLiMP phenomena; dense models show diffuse importance spread
  - [corpus] Zhang et al. (2023, cited in paper) document emergent modularity in pretrained Transformers, supporting the plausibility of expert specialization.
- **Break condition:** If routing dynamics alone drove the effect, disabling routing while preserving expert weights would eliminate subtype separation; this control is not reported.

## Foundational Learning

- **Concept: Western Aphasia Battery (WAB) and Aphasia Quotient (AQ)**
  - **Why needed here:** The paper uses WAB/AQ as the primary evaluation metric; understanding its subtests (Spontaneous Speech, Comprehension, Repetition, Naming) and weighting scheme is essential to interpret lesion effects.
  - **Quick check question:** If a model scores 92 on AQ after Broca-targeted lesions, is it classified as aphasic according to WAB criteria?

- **Concept: Mixture-of-Experts (MoE) routing and expert specialization**
  - **Why needed here:** The architectural comparison hinges on whether MoE experts develop functional specialization that dense FFNs lack; understanding routing and sparse activation clarifies why lesion locality differs.
  - **Quick check question:** In OLMoE, what fraction of the 64 experts are activated per token, and how might this affect lesion localization?

- **Concept: Gradient-based attribution for component importance**
  - **Why needed here:** The method ranks components using cumulative gradient×weight products; understanding this attribution scheme is necessary to evaluate whether it captures causal contributions versus correlation.
  - **Quick check question:** Why does the paper aggregate gradients over training steps rather than using a single gradient snapshot?

## Architecture Onboarding

- **Component map:**
  - OLMoE: 16 layers, each FFN replaced with 64 experts (8 activated per token) → intervention unit = layer-expert pair → BLiMP zero-ablation → gradient-based attribution → progressive lesioning → WAB/AQ evaluation
  - OLMo: 16 layers, standard dense FFN → intervention unit = FFN hidden dimension → same pipeline

- **Critical path:**
  1. Run BLiMP zero-ablation → produces phenomenon-to-unit attribution map
  2. Fine-tune on AphasiaBank (Broca/Wernicke) → compute gradient×weight unit scores
  3. Validate subtype separability via CAP classifier
  4. Align phenotype-linked units with BLiMP phenomena (heatmap)
  5. Progressive lesioning (top-p% units) → WAB/AQ evaluation

- **Design tradeoffs:**
  - **Zeroing vs. Xavier re-initialization:** Zeroing models strict silencing; Xavier models scrambling. Paper uses both but emphasizes Xavier in quantitative results.
  - **Threshold selection (top-p%):** Lower p increases specificity but reduces effect size; p=2% chosen via robustness sweep (Spearman ρ > 0.85 across 1-5%).
  - **Text-only WAB adaptation:** Some WAB items require multimodal input; these are excluded, potentially limiting clinical validity.

- **Failure signatures:**
  - Random lesions cause mild AQ decline but lack subtype specificity
  - Dense models show convergent Broca/Wernicke dysfunction curves (reduced separability)
  - CAP classifier scores <70% indicate subtype fine-tuning failed to produce discriminative outputs

- **First 3 experiments:**
  1. **Reproduce the BLiMP attribution heatmap** for a single layer in OLMoE to verify zero-ablation pipeline and ranking logic.
  2. **Run progressive lesioning with p ∈ {0.5%, 1%, 2%}** on OLMoE-Broca only, plotting AQ vs. lesion budget to confirm dose-response.
  3. **Compare phenotype-targeted vs. random lesions** at matched p=2% on OLMo (dense), testing whether subtype separation degrades as the paper predicts.

## Open Questions the Paper Calls Out
None

## Limitations
- The clinical validity of the adapted WAB is uncertain, as excluding multimodal items may not fully capture aphasia's breadth
- The claim that gradient-based fine-tuning identifies causally relevant components rests on a strong assumption that gradient contributions map to functional mechanisms rather than memorization patterns
- The architectural comparison between MoE and dense models lacks controls for routing dynamics, making it difficult to isolate whether observed differences stem from expert specialization or other factors

## Confidence
- **High confidence:** The experimental pipeline is well-specified and reproducible; BLiMP zero-ablation, gradient-based attribution, and progressive lesioning are standard techniques with clear implementation paths. The dose-response relationship between lesion proportion and AQ decline is empirically robust across multiple trials.
- **Medium confidence:** The claim that phenotype-targeted lesions produce subtype-specific AQ profiles is supported by within-paper comparisons to random baselines, but external validation with independent clinical datasets is absent. The interpretation that MoE experts show more interpretable mappings than dense FFNs is plausible given architectural modularity literature, but confounding factors (routing, parameter count) are not fully controlled.
- **Low confidence:** The clinical relevance of the results depends heavily on the adapted WAB's fidelity to the original instrument. Without access to the exact prompt templates and validation against human aphasic speech, it's unclear whether the AQ metric captures authentic aphasia-like behavior or generic language degradation.

## Next Checks
1. **Causal validation of component importance:** Perform targeted interventions (activation zeroing and Xavier re-init) on a small set of high-gradient components identified by the pipeline, then measure whether BLiMP performance changes correlate with WAB AQ changes. This would test whether gradient-based attribution captures functional relevance rather than mere correlation.
2. **External clinical validation:** Apply the same lesioning pipeline to a held-out AphasiaBank subset not used during fine-tuning, or to an independent aphasic speech corpus, to verify that subtype separation generalizes beyond the training data.
3. **Architectural control experiment:** Disable MoE routing while preserving expert weights, then repeat the lesioning and evaluation. If subtype separation disappears, this would suggest routing dynamics rather than expert specialization drive the observed effects.