---
ver: rpa2
title: 'QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient
  Fine-Tuning on Large Language Models'
arxiv_id: '2509.17428'
source_url: https://arxiv.org/abs/2509.17428
tags:
- adapters
- quantization
- qwha
- initialization
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: QWHA introduces a quantization-aware parameter-efficient fine-tuning
  method that integrates the Walsh-Hadamard Transform (WHT) into sparse Fourier-related
  adapters, addressing the limitations of low-rank LoRA-based methods in quantized
  model adaptation. By adopting WHT, QWHA achieves higher representational capacity
  and better quantization error reconstruction than existing FT-based approaches,
  while avoiding costly double transforms and complex matrix multiplications.
---

# QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models

## Quick Facts
- arXiv ID: 2509.17428
- Source URL: https://arxiv.org/abs/2509.17428
- Reference count: 40
- Primary result: QWHA achieves 2-3% higher accuracy than LoRA-based methods in 2-bit quantization settings while providing 2-4× training speedup

## Executive Summary
QWHA introduces a quantization-aware parameter-efficient fine-tuning method that integrates the Walsh-Hadamard Transform (WHT) into sparse Fourier-related adapters, addressing the limitations of low-rank LoRA-based methods in quantized model adaptation. By adopting WHT, QWHA achieves higher representational capacity and better quantization error reconstruction than existing FT-based approaches, while avoiding costly double transforms and complex matrix multiplications. The proposed AdaAlloc initialization scheme distributes parameters adaptively across channels to ensure full rank and reduce quantization error, with value refinement further improving accuracy. Experiments across LLaMA and Mistral models show that QWHA consistently outperforms baselines in low-bit quantization accuracy, achieving 2-3% higher scores than LoRA-based methods in 2-bit settings, while also providing significant training speedups and inference efficiency.

## Method Summary
QWHA implements a WHT-based adapter ΔW=F·H⁻¹ where F is a sparse matrix of tunable parameters. The method uses AdaAlloc initialization that allocates parameters proportionally to channel-wise quantization error magnitudes, followed by value refinement via least-squares projection. During fine-tuning, only the values in F are updated while the selected indices remain frozen. The approach targets 2-4 bit quantized LLMs, using a calibration dataset to compute quantization errors and initialize the adapter parameters. The method maintains full-rank structure through adaptive parameter allocation while achieving computational efficiency through the ±1 structure of the Hadamard kernel.

## Key Results
- QWHA achieves 2-3% higher accuracy than LoRA-based methods in 2-bit quantization settings on CSQA benchmarks
- The method provides 2-4× training speedup compared to double-transform approaches while maintaining accuracy
- AdaAlloc initialization reduces layer output error by 3.86×10⁻³ compared to magnitude-only selection (7.06×10⁻³)
- QWHA consistently outperforms CLoQ, FALQON, and SSH across multiple quantization levels and model sizes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** WHT-based adapters achieve higher representational capacity than low-rank LoRA adapters while maintaining computational efficiency.
- **Mechanism:** The Walsh-Hadamard Transform kernel contains only ±1 entries, enabling full-rank sparse matrices when parameters are distributed across channels (conditions: ≥2 params per row/column on average). Unlike LoRA's rank bound r, WHA's rank approaches min(d_in, d_out). The ±1 structure replaces matrix multiplications with additions/subtractions.
- **Core assumption:** Sparse parameter selection with per-channel allocation preserves sufficient rank for downstream adaptation.
- **Evidence anchors:** Abstract states QWHA achieves higher representational capacity; Section 3.1 shows empirical rank analysis demonstrating WHA is nearly full-rank while LoRA achieves less than 6.3% of normalized rank.
- **Break condition:** If parameter budget r is too small (P(r<4) in experiments), channels may receive <2 parameters, degrading toward low-rank behavior.

### Mechanism 2
- **Claim:** AdaAlloc initialization reduces quantization error prior to fine-tuning by allocating parameters proportionally to channel-wise error magnitude while maintaining full-rank structure.
- **Mechanism:** Computes per-channel error ∥(ΔW_Q X)_i,:∥_F^t, allocates parameter budget p_i proportional to error (with temperature t=1 default), then selects top-magnitude coefficients within each channel from ΔW_Q H. This prevents magnitude-only selection's channel concentration (which reduces rank) while capturing outlier-driven errors.
- **Core assumption:** Activation-weighted error distribution predicts which channels need more adaptation capacity.
- **Evidence anchors:** Abstract mentions AdaAlloc distributes parameters adaptively across channels to ensure full rank and reduce quantization error; Section 3.2, Table 2 shows AdaAlloc achieves lowest average layer output error while maintaining full rank.
- **Break condition:** If temperature t is too high (>1.5), allocation over-concentrates in outlier channels, degrading low-magnitude channel adaptation.

### Mechanism 3
- **Claim:** Value refinement after coefficient selection further reduces initialization error by accounting for inter-basis interactions.
- **Mechanism:** After selecting indices E via AdaAlloc, re-project target v onto selected basis B': x* = vB'ᵀ(B'B'ᵀ)⁻¹. This least-squares solution accounts for how sparsifying other dimensions affects the approximation.
- **Core assumption:** The greedy index selection plus refinement approximates the NP-hard sparse approximation problem well enough for practical benefit.
- **Evidence anchors:** Section 3.2, Figure 5 shows refinement is crucial for reducing layer output error; Table 7 shows refined initialization reduces error from 7.06 to 3.86 (average) compared to without refinement.
- **Break condition:** If basis vectors B' are near-degenerate, (B'B'ᵀ)⁻¹ becomes unstable—mitigated by WHT's orthogonal structure.

## Foundational Learning

- **Concept: LLM Quantization Errors and Outliers**
  - Why needed here: QWHA specifically targets outlier-induced quantization errors (clamping errors beyond rounding range), which dominate accuracy degradation at 2-4 bits.
  - Quick check question: Why does a 2-bit quantized model with outliers in 5% of weights degrade more than uniform weight distributions?

- **Concept: Walsh-Hadamard Transform vs. DCT/DFT**
  - Why needed here: Understanding why WHT's ±1 square-wave basis captures outlier structure better than sinusoidal DCT/DHT bases explains QWHA's energy concentration advantage.
  - Quick check question: What property of WHT makes its fast implementation O(n log n) with only additions/subtractions?

- **Concept: Sparse Approximation Problem (NP-hard)**
  - Why needed here: QWHA decomposes the NP-hard sparse coefficient selection into tractable greedy index selection + value refinement; understanding this motivates the two-stage design.
  - Quick check question: Why can't we directly solve for optimal sparse F in equation (6)?

## Architecture Onboarding

- **Component map:**
  Pre-trained W₀ → Quantize → W_Q (frozen) → Calibration data X → Compute ΔW_Q = W₀ - W_Q → AdaAlloc: channel-wise p_i allocation → Select indices E per channel from ΔW_Q H → Refine values c via least-squares projection → Initialize sparse F(c, E) → Fine-tune c (E frozen): ΔW = F H⁻¹ → Inference: Y = (W_Q + F H⁻¹) X

- **Critical path:** AdaAlloc allocation → index selection → value refinement determines initial error; subsequent fine-tuning only updates c. Initialization quality at 2-3 bits is the accuracy bottleneck.

- **Design tradeoffs:**
  - Single vs. double transform: Single WHT (QWHA) sacrifices potential representational gain for 2-4× training speedup vs. DCA/DHA
  - Temperature t: Lower t → uniform allocation (better fine-tuning rank, worse error reduction); higher t → concentrated allocation (better error reduction, worse rank)
  - Parameter budget r: P(r≥64) needed for full-rank guarantee; lower budgets work but approach LoRA limitations

- **Failure signatures:**
  - Accuracy collapse at 2-bit without QA initialization (Table 3: Random SSH drops to 51.84% CSQA vs. QWHA's 60.98%)
  - High initialization error despite refinement if selection strategy is random (Table 2: Random achieves 5.96 vs. AdaAlloc's 3.86)
  - Training divergence if scaling factor α is too large (paper uses α_effective ≈ 1.0)

- **First 3 experiments:**
  1. **Rank validation:** Compute normalized rank of F across layers for P(r=32, 64, 128) with AdaAlloc vs. random selection; verify full-rank condition
  2. **Initialization error ablation:** Measure layer-wise ∥ΔW_QR - FH⁻¹R∥_F for: (a) AdaAlloc + refinement, (b) AdaAlloc only, (c) random + refinement; isolate each contribution
  3. **2-bit accuracy baseline:** Fine-tune 4-bit LLaMA-3.2-3B on GSM8k with QWHA vs. CLoQ vs. SSH; confirm 2-3% gap reported in Table 3

## Open Questions the Paper Calls Out

- **Question:** Can QWHA be combined with orthogonal QA-PEFT techniques like layer-wise calibration or rank-adaptive methods for further improvements?
- **Basis in paper:** Page 8 states that methods involving layer-wise calibration or layer-wise parameter allocation "are orthogonal to our approach and can be integrated in future work."
- **Why unresolved:** Compatibility and potential interference between QWHA's AdaAlloc initialization and other calibration strategies remains unexplored.
- **What evidence would resolve it:** Experiments combining QWHA with RA-LoRA or other layer-wise allocation methods, measuring accuracy and computational overhead.

## Limitations

- The claim of "higher representational capacity" compared to LoRA rests on empirical rank analysis rather than theoretical guarantees for downstream task performance.
- The AdaAlloc initialization assumes calibration data represents downstream task distributions—if this assumption fails, the parameter allocation strategy may concentrate capacity in irrelevant channels.
- Claims about avoiding "complex matrix multiplications" require clarification—while Hadamard operations are additions/subtractions, the sparse matrix F construction and value refinement steps involve non-trivial computation.

## Confidence

- **High confidence:** WHT computational efficiency claims, AdaAlloc error reduction mechanisms, and 2-3% accuracy improvements at 2-bit quantization are well-supported by ablation studies and comparative results
- **Medium confidence:** The mechanism linking full-rank capacity to better adaptation quality, while empirically supported, lacks theoretical proof for non-linear downstream tasks
- **Low confidence:** Claims about avoiding "complex matrix multiplications" require clarification

## Next Checks

1. **Rank-Performance Correlation:** Systematically vary parameter budget r from 16 to 512 and measure both normalized rank and downstream accuracy on GSM8k to establish whether rank capacity directly predicts fine-tuning quality

2. **Calibration Data Robustness:** Evaluate QWHA performance when calibration data differs substantially from fine-tuning data (e.g., calibrate on WikiText-2 but fine-tune on code-specific datasets) to test AdaAlloc's distributional assumptions

3. **Cross-Model Generalization:** Test QWHA on 8-bit quantized models (not just 4-bit) and on non-Transformer architectures to validate whether WHT advantages extend beyond the specific experimental setup