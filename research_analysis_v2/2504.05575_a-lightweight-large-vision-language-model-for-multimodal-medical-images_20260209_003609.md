---
ver: rpa2
title: A Lightweight Large Vision-language Model for Multimodal Medical Images
arxiv_id: '2504.05575'
source_url: https://arxiv.org/abs/2504.05575
tags:
- medical
- image
- images
- questions
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a lightweight multimodal VQA model combining
  BiomedCLIP for image feature extraction and LLaMA-3 for text processing, specifically
  designed for medical visual question answering. Evaluated on the OmniMedVQA dataset,
  the model achieves 73.4% accuracy on open-ended questions, outperforming existing
  models while requiring only two NVIDIA A100 GPUs and using approximately 8 billion
  parameters.
---

# A Lightweight Large Vision-language Model for Multimodal Medical Images

## Quick Facts
- arXiv ID: 2504.05575
- Source URL: https://arxiv.org/abs/2504.05575
- Reference count: 27
- Primary result: 73.4% accuracy on open-ended medical VQA across 8 imaging modalities

## Executive Summary
This paper introduces a lightweight multimodal VQA model combining BiomedCLIP for image feature extraction and LLaMA-3 for text processing, specifically designed for medical visual question answering. Evaluated on the OmniMedVQA dataset, the model achieves 73.4% accuracy on open-ended questions, outperforming existing models while requiring only two NVIDIA A100 GPUs and using approximately 8 billion parameters. It supports diverse medical imaging modalities and open-ended questions, making it practical for real-world clinical applications. The model shows highest accuracy on microscopy images (78.5%) and lowest on MRI images (69.2%). Despite limitations like dataset uniformity and potential overfitting, the model demonstrates efficiency, scalability, and strong performance across medical imaging tasks.

## Method Summary
The model employs a two-stage training approach: first training BiomedCLIP independently on medical images, then fine-tuning LLaMA-3-8B with LoRA (rank=8, alpha=32) before joint fine-tuning. The architecture combines BiomedCLIP's image encoder with LLaMA-3's text encoder, using bf16 precision and a batch size of 128 on 2x NVIDIA A100 40GB GPUs. Questions from the OmniMedVQA dataset were converted from multiple-choice to open-ended format, and the model was trained using AdamW optimizer with cosine scheduler and 100 warmup steps. The lightweight design (8B parameters) enables practical deployment while maintaining competitive accuracy across multiple medical imaging modalities.

## Key Results
- Achieves 73.4% overall accuracy on open-ended medical VQA questions
- Demonstrates 76.9% accuracy on yes/no questions and 70.7% on open-ended questions
- Shows highest performance on microscopy (78.5%) and lowest on MRI (69.2%) modalities

## Why This Works (Mechanism)
The model's effectiveness stems from combining BiomedCLIP's specialized medical image understanding with LLaMA-3's robust language capabilities. The LoRA fine-tuning approach enables efficient adaptation of the large language model while maintaining performance. The two-stage training process allows independent optimization of image and text components before alignment, reducing computational requirements compared to end-to-end training of massive models.

## Foundational Learning
- **Medically-pretrained image encoders**: Why needed - Medical images contain domain-specific features that general CLIP models miss; Quick check - Verify BiomedCLIP achieves higher accuracy than general CLIP on medical image classification tasks
- **LoRA fine-tuning**: Why needed - Enables efficient adaptation of large language models with minimal computational overhead; Quick check - Confirm parameter efficiency by measuring memory usage during training
- **Multi-modal alignment**: Why needed - Medical VQA requires precise integration of visual and textual information; Quick check - Test retrieval accuracy on medical image-text pairs

## Architecture Onboarding

**Component Map**: BiomedCLIP Image Encoder -> Feature Projector -> LLaMA-3 Text Encoder -> Fusion Layer -> Output Layer

**Critical Path**: Image features → BiomedCLIP → Projection → Fusion → LLaMA-3 → Answer Generation

**Design Tradeoffs**: Uses lightweight LoRA fine-tuning instead of full model adaptation, sacrificing some potential accuracy gains for computational efficiency and faster training. Employs bf16 precision to balance numerical stability with memory constraints.

**Failure Signatures**: 
- Mode collapse in feature space leading to repetitive answers
- GPU memory exhaustion on smaller hardware setups
- Modality-specific performance drops (especially for MRI)

**First 3 Experiments**:
1. Verify feature extraction quality by testing BiomedCLIP on held-out medical image classification
2. Test LoRA configuration with varying ranks (4, 8, 16) to optimize accuracy-efficiency tradeoff
3. Evaluate modality-specific performance to confirm accuracy variance patterns

## Open Questions the Paper Calls Out
- To what extent does the reported accuracy reflect genuine clinical understanding versus overfitting caused by repetitive question-answer pairs in the OmniMedVQA dataset?
- Can the lightweight 8-billion parameter architecture be effectively extended to handle multi-step reasoning tasks required for complex clinical diagnosis?
- What architectural or training modifications are necessary to improve performance on MRI images, which currently show the lowest accuracy (69.2%)?

## Limitations
- Dataset uniformity with 36.5% of questions following a "Is there X in the image?" pattern raises overfitting concerns
- Significant performance variance across modalities (78.5% microscopy vs 69.2% MRI) suggests modality-specific biases
- Conversion from closed-ended to open-ended questions involved human annotation for only a subset, potentially introducing inconsistencies

## Confidence

**High Confidence**: The reported 73.4% overall accuracy and technical implementation details appear reproducible based on provided specifications.

**Medium Confidence**: Performance claims across different modalities and efficiency benefits require validation due to dataset uniformity and limited external validation.

**Low Confidence**: Claims about clinical applicability and handling truly open-ended medical questions cannot be substantiated without testing on more diverse datasets or real-world clinical scenarios.

## Next Checks
1. Evaluate the model on at least two additional medical VQA datasets (such as MedVQA or VQA-RAD) to assess cross-dataset generalization
2. Conduct systematic testing across all eight imaging modalities with balanced question types to confirm reported performance variance
3. Compare model outputs against human expert annotations on a held-out subset of truly open-ended questions to validate practical utility of the open-ended format claims