---
ver: rpa2
title: Revisiting Actor-Critic Methods in Discrete Action Off-Policy Reinforcement
  Learning
arxiv_id: '2509.09838'
source_url: https://arxiv.org/abs/2509.09838
tags:
- policy
- entropy
- actor
- soft
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper revisits actor-critic methods for off-policy reinforcement
  learning with discrete actions, focusing on why discrete SAC (DSAC) underperforms
  value-based methods like DQN. The core insight is that coupling actor and critic
  entropy in DSAC degrades performance.
---

# Revisiting Actor-Critic Methods in Discrete Action Off-Policy Reinforcement Learning

## Quick Facts
- arXiv ID: 2509.09838
- Source URL: https://arxiv.org/abs/2509.09838
- Reference count: 0
- Primary result: Decoupling actor and critic entropy in DSAC yields performance comparable to DQN on Atari.

## Executive Summary
This paper investigates why discrete Soft Actor-Critic (DSAC) underperforms value-based methods like DQN in off-policy reinforcement learning. The authors identify that coupling entropy regularization in both the actor (policy optimization) and critic (value evaluation) degrades performance. By decoupling these components—using a hard Bellman operator for the critic and retaining entropy only for the actor—DSAC performance matches DQN. They extend this insight into a flexible off-policy actor-critic framework that generalizes DSAC with different entropy settings and KL divergence variants, proving convergence guarantees in the tabular setting and demonstrating competitive performance across Atari games.

## Method Summary
The method builds on discrete Soft Actor-Critic (DSAC) by decoupling entropy regularization between actor and critic. The key modification is using a hard Bellman operator for the critic (setting critic entropy coefficient ζ = 0) while maintaining entropy regularization for the actor (τ ≠ 0, typically with automatic tuning). The framework generalizes further by allowing different combinations of entropy regularization for actor and critic, and using either forward or reverse KL divergence to project an intermediate policy onto the final policy. The actor update can use Natural Policy Gradient (NPG) or State-Policy Maximum Action (SPMA) as base objectives. The method proves convergence in tabular settings and shows that even without entropy regularization (τ = 0, ζ = 0), sufficient actor optimization steps (n ≥ 10) can maintain performance.

## Key Results
- Decoupling actor and critic entropy in DSAC (ζ = 0, τ ≠ 0) achieves performance comparable to DQN on Atari games.
- Without entropy regularization (τ = 0, ζ = 0), performance can be recovered with sufficient actor optimization steps (n = 10).
- Forward KL variant is computationally efficient as it avoids explicit action normalization.
- Theoretical convergence guarantees are established for the framework in tabular settings.

## Why This Works (Mechanism)

### Mechanism 1: Asymmetric Entropy Decoupling
- Claim: DSAC underperforms DQN because coupling entropy coefficients in both actor and critic creates conflicting optimization pressures.
- Mechanism: Setting critic entropy coefficient ζ = 0 (hard Bellman operator) while retaining actor entropy τ ≠ 0 prevents the critic from over-estimating uncertainty that destabilizes learning in discrete spaces.
- Core assumption: Poor performance is primarily caused by critic entropy term rather than actor exploration noise.
- Evidence: DSAC with hard critic (ζ = 0) matches DQN performance, while standard DSAC underperforms.

### Mechanism 2: KL-Regularized Policy Surrogates
- Claim: Performance can be recovered without explicit entropy (τ = 0, ζ = 0) by using NPG or SPMA objectives with KL constraints.
- Mechanism: KL-divergence constraint implicitly regularizes policy updates, substituting for stability usually provided by entropy.
- Core assumption: Function approximation error is manageable and KL constraint effectively regularizes updates.
- Evidence: NPG-RKL with τ = 0, ζ = 0 benefits from additional optimization steps, unlike standard DSAC which fails without entropy.

### Mechanism 3: Sufficient Actor Optimization Intensity
- Claim: In absence of entropy regularization (τ = 0), actor requires sufficient gradient updates (n ≥ 10) per environment step to maintain performance.
- Mechanism: Multiple gradient steps smooth learning trajectory and prevent premature convergence when entropy stochasticity is absent.
- Core assumption: Replay buffer contains diverse data to support multiple steps without overfitting.
- Evidence: With n = 1 and τ = 0, policy entropy collapses immediately; increasing n prevents this collapse.

## Foundational Learning

- **Concept: Hard vs. Soft Bellman Operators**
  - Why needed: Central modification switches critic from soft (entropy-regularized) to hard (standard) update.
  - Quick check: Does your critic update subtract a term proportional to log-probability of action? If yes, you're using Soft Operator—paper recommends removing for discrete actions.

- **Concept: Forward vs. Reverse KL Divergence**
  - Why needed: Framework projects intermediate policy onto realizable policy using either Forward KL (mode-covering) or Reverse KL (mode-seeking).
  - Quick check: Do you need policy to cover all good actions (Forward KL) or stick to single best action mode (Reverse KL)? Paper suggests similar performance, but Forward KL is more efficient computationally.

- **Concept: Natural Policy Gradient (NPG) & SPMA**
  - Why needed: Base update rules to compute intermediate policy before projection.
  - Quick check: Does actor update use exponential weight (NPG) or multiplicative factor based on advantage (SPMA)? SPMA allows avoiding explicit action normalization for large action spaces.

## Architecture Onboarding

- **Component map:** Critic (Qφ) → Actor (πθ) → Entropy Terms (ζ = 0, τ ≠ 0 optional) → Optimizer (n steps)

- **Critical path:**
  1. Sample batch from replay buffer Dt
  2. Critic Update: Compute target y = r + γ Qtarget(s', a') (Hard update, no entropy term). Update Qφ via MSE.
  3. Actor Update: Compute intermediate policy (NPG/SPMA). Project using KL divergence.
  4. Repeat: If τ = 0, repeat Actor Update step n = 10 times.

- **Design tradeoffs:**
  - Efficiency vs. Stability: SPMA-FKL avoids explicit action normalization, efficient for large action spaces; NPG requires normalization across all actions.
  - Entropy vs. Optimization: Can turn off entropy (τ = 0) to simplify tuning, but must pay compute cost of n = 10 gradient steps for stability.

- **Failure signatures:**
  - Entropy Collapse: If τ = 0 and n < 10, policy entropy drops to zero immediately and rewards stall.
  - DSAC Underperformance: If critic entropy ζ is coupled to actor entropy τ (standard DSAC), performance significantly lags behind DQN.

- **First 3 experiments:**
  1. Entropy Decoupling: Run standard DSAC vs. "Hard Critic" variant (ζ = 0) on Pong/Breakout to verify performance gap closes.
  2. Optimization Intensity: Test zero-entropy (τ = 0) variant with n = 1 vs n = 10 to confirm entropy collapse prevented by higher n.
  3. Direction Comparison: Compare Forward KL vs Reverse KL projections on sparse-reward game to see if mode-covering offers advantage.

## Open Questions the Paper Calls Out

1. Can rigorous theoretical guarantees be established for forward KL objectives in function approximation setting?
2. How can actor step size ηt be automatically adapted to ensure robust performance without manual tuning?
3. Why does increasing actor optimization steps (n) to 10 allow effective learning without entropy regularization or explicit exploration?
4. Can decoupling actor and critic entropy improve performance in continuous action settings?

## Limitations

- The "10 gradient steps" heuristic for zero-entropy learning is empirical and lacks theoretical explanation for why this specific number is optimal.
- The framework's benefits beyond Atari (to other discrete-action domains) are not empirically validated.
- Theoretical analysis is limited to tabular settings; function approximation case relies primarily on empirical results.

## Confidence

- **High Confidence**: Central empirical claim that DSAC underperforms DQN due to coupled entropy is well-supported by provided ablations and tabular convergence proof.
- **Medium Confidence**: Explanation for why entropy coupling harms DSAC is plausible but exact magnitude may depend on unspecified implementation details.
- **Low Confidence**: Claim that "even without entropy... provided actor objective is sufficiently optimized (e.g., 10 gradient steps)" is least validated—mechanism linking high gradient steps to stability without entropy remains heuristic.

## Next Checks

1. Re-implement "Hard Critic" variant (ζ = 0) from scratch and verify it closes DSAC vs. DQN performance gap on held-out Atari games.
2. Systematically vary n (1, 5, 10, 20) in zero-entropy regime (τ = 0, ζ = 0) to determine minimum n required for stable learning and test if performance is truly DQN-comparable.
3. Test proposed framework (hard critic, decoupled entropy) on non-Atari discrete-action task (e.g., LunarLander-v2 or discrete MuJoCo) to assess domain generalization.