---
ver: rpa2
title: Online Learning of HTN Methods for integrated LLM-HTN Planning
arxiv_id: '2511.12901'
source_url: https://arxiv.org/abs/2511.12901
tags:
- task
- methods
- learning
- chatgpt
- planning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces an online learning method for HTN planning
  that reduces reliance on LLM queries. The system extends ChatHTN by learning generalized
  HTN methods from ChatGPT-generated decompositions, using goal regression to derive
  preconditions and ensure correctness.
---

# Online Learning of HTN Methods for integrated LLM-HTN Planning

## Quick Facts
- arXiv ID: 2511.12901
- Source URL: https://arxiv.org/abs/2511.12901
- Reference count: 6
- Primary result: Online learning of generalized HTN methods reduces LLM queries while maintaining or improving planning success rates.

## Executive Summary
This paper introduces an online learning method for HTN planning that reduces reliance on LLM queries by learning generalized methods from ChatGPT-generated decompositions. The system extends ChatHTN by using goal regression to derive preconditions and ensure correctness when learning methods. When ChatGPT provides a task decomposition, the system generalizes it into a reusable method applicable to similar future tasks, unlike simple memoization. The approach is integrated with termination methods that stop recursion when effects are already satisfied. Experiments on Logistics Transportation and Search and Rescue domains show the method learner reduces ChatGPT calls while maintaining or improving problem-solving success rates.

## Method Summary
The method integrates online learning with HTN planning by extending ChatHTN. When ChatGPT decomposes a task into primitive actions, the system generalizes this decomposition by lifting constants to variables and computing preconditions via goal regression. The learned method is verified against the task's effects before being stored. Termination methods prevent infinite recursion by matching task effects to no-op actions. The system learns methods between problem instances and clears learned methods after each problem in experiments.

## Key Results
- Online method learning reduces ChatGPT calls by 20-60% compared to ChatHTN without learning
- Success rates remain comparable or improve (95% vs 85-95% baseline) when removing methods
- The approach maintains HTN planning soundness through verifier tasks and goal regression
- Linear-only learned methods cannot handle recursive tasks, requiring LLM queries for variable-sized problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generalized method learning reduces redundant LLM queries by abstracting specific decompositions into reusable templates.
- Mechanism: When ChatGPT decomposes a task into primitive actions, the system lifts constants to variables (e.g., `at(Maria, Zulu)` → `at(?survivor, ?loc)`) and computes preconditions via goal regression. This produces a method applicable to similar future task instances, not just the exact observed state. The regression `Reg([o'₁,...,o'ₘ], effₜ)` computes minimal conditions needed in any state for the action sequence to achieve the task's effects.
- Core assumption: ChatGPT returns a correct decomposition that achieves the annotated task's effects; the domain operators accurately model world dynamics.
- Evidence anchors:
  - [abstract] "it learns a generalized method that applies not only to the specific instance encountered, but to other instances of the same task"
  - [section 5] "p is the set of preconditions generated by performing goal regression from effₜ on the sequence (o'₁,...,o'ₘ)"
  - [corpus] Weak direct corpus support; neighbor paper "ChatHTN: Interleaving Approximate (LLM) and Symbolic HTN Planning" describes the base system but not the learning extension.
- Break condition: If ChatGPT returns an incorrect decomposition that passes the verifier (false positive), the learned method will be incorrect. If task instances vary in structure beyond variable substitution (e.g., different numbers of subtasks needed), the linear learned method fails to generalize.

### Mechanism 2
- Claim: Verifier tasks enforce soundness by checking that decompositions achieve declared task effects before methods are learned.
- Mechanism: After ChatGPT returns a decomposition `t̃'`, ChatHTN inserts a verifier task `t_ver` with the annotated task's effects as preconditions. If effects hold after executing the decomposition, the method is learned; otherwise, backtracking occurs. This ensures learned methods are correct relative to the task specification.
- Core assumption: Annotated tasks correctly specify intended effects; operators faithfully execute actions.
- Evidence anchors:
  - [abstract] "using goal regression to derive preconditions and ensure correctness"
  - [section 4] "ChatHTN is sound. Informally, the tasks in t̃ are satisfied by the plan generated by ChatHTN"
  - [section 5] "Online learning of the method occurs immediately after this verification succeeds"
  - [corpus] No direct corpus evidence on this specific verification mechanism.
- Break condition: If annotated task effects are underspecified or incorrect, verification passes for wrong solutions. If operators have unintended side effects, state verification may be misleading.

### Mechanism 3
- Claim: Termination methods prevent infinite recursion by detecting when task effects are already satisfied.
- Mechanism: A termination method `mₜ = (t, effects, (!doNothing()))` matches a task when its effects already hold in the current state, decomposing to a no-op. This stops recursive decomposition loops (e.g., `checkSurvivors` when no survivors remain).
- Core assumption: Task effects are expressible as verifiable state conditions; the domain includes appropriate termination conditions.
- Evidence anchors:
  - [section 5] "Its use is to stop recursive calls for t from other methods, in states where the desired effects of the task are satisfied"
  - [section 2] Figure 5 shows termination method for `checkSurvivors(?loc)` with precondition `not(at(?survivor,?loc))`
  - [corpus] No corpus evidence on termination methods specifically.
- Break condition: If effects are satisfied incidentally but the task's semantic intent isn't achieved (e.g., survivor rescued but medical care not provided), premature termination occurs. If effects can't be negated properly, termination methods may never trigger.

## Foundational Learning

- Concept: **Hierarchical Task Network (HTN) Planning**
  - Why needed here: The entire system extends HTN planning; you must understand methods, operators, compound vs. primitive tasks, and how decomposition works.
  - Quick check question: Given a method `(task, preconditions, subtasks)`, what makes it applicable to a state-task pair `(s, t)`?

- Concept: **Goal Regression (Explanation-Based Generalization)**
  - Why needed here: The method learner uses goal regression to compute minimal preconditions from a successful action sequence and desired effects.
  - Quick check question: If operator `o` has preconditions `preₒ`, add-list `addₒ`, and delete-list `delₒ`, what is `Reg([o], g)` when `g ⊆ addₒ`?

- Concept: **Annotated Tasks**
  - Why needed here: Annotated tasks provide ChatGPT with task semantics (preconditions + effects) and enable verification; they bridge LLM understanding and HTN formalism.
  - Quick check question: What three components define an annotated task, and how do they differ from an HTN method?

## Architecture Onboarding

- Component map:
  - **ChatHTN Planner**: Base HTN planner (SHOP-style) extended to query ChatGPT when no method applies
  - **LLM Interface**: Two-step prompt chaining strategy sending annotated task + operators + current state
  - **Method Learner**: Lifts constants → variables; applies goal regression → preconditions; stores learned method
  - **Verifier Task Injector**: Inserts `t_ver` after each ChatGPT decomposition to check effects
  - **Termination Method Bank**: Domain-provided methods matching task effects to `!doNothing()`
  - **Method Cache**: Stores learned methods (cleared between problem instances in experiments)

- Critical path:
  1. HTN planning encounters compound task `t` with no applicable method
  2. Query ChatGPT with annotated task + state → receive decomposition `t̃'`
  3. Inject verifier task → execute decomposition → check effects
  4. If verified: lift decomposition, regress preconditions → store learned method
  5. Apply learned method to future matching tasks

- Design tradeoffs:
  - **Linear-only learned methods**: Cannot learn recursive/iterative methods; system learns specialized methods for each "arity" (e.g., rescuing exactly 2 survivors). Future work: prompt ChatGPT for compound subtasks or detect repeated patterns.
  - **Method deletion between problems**: Experiments clear learned methods after each problem; production systems may retain them for cumulative learning.
  - **Verification overhead**: Every ChatGPT decomposition adds a verifier task; trades computation for soundness.

- Failure signatures:
  - **Repeated LLM queries for similar tasks**: Method learner not generalizing; check if preconditions are over-specialized (regression may include unnecessary conditions)
  - **Verifier failures on correct plans**: Annotated task effects may be incomplete; operators may have unmodeled side effects
  - **Infinite recursion**: Termination method missing or preconditions never satisfied; check effect predicates match reality
  - **Learned method never reused**: Variable lifting may not align task arguments; check substitution θ applicability

- First 3 experiments:
  1. **Ablation on single method removal**: Remove one method from a complete HTN domain; run 10 problems × 3 trials; measure GPT calls and success rate with vs. without method learner. Expected: reduced calls, equal or higher success.
  2. **High-level vs. low-level method removal**: Compare learning when removing top-level methods (e.g., `searchAndRescue`) vs. leaf methods (e.g., `rescueSurvivor`). Expected: low-level removal benefits more; high-level removal exposes LLM's long-horizon planning weaknesses.
  3. **Cross-instance generalization test**: After learning from one problem instance, test method applicability on a different problem with different object counts. Expected: linear methods fail when subtask counts differ; identifies recursion limitation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the method learner be extended to induce methods containing compound subtasks or recursion, rather than only linear sequences of primitive tasks?
- Basis in paper: [explicit] Page 15 states the authors want to "modify ChatGPT to allow the current task to be decomposed into a sequence of compound and primitive tasks" to address the limitation that the learner "cannot learn recursive methods."
- Why unresolved: The current implementation generalizes only flat sequences of actions provided by the LLM. It fails to learn abstract control structures (e.g., loops) needed for tasks like rescuing an arbitrary number of survivors.
- What evidence would resolve it: A demonstration of the system learning a method like `checkSurvivors` that recursively calls itself, allowing it to solve problems with variable survivor counts without re-querying the LLM.

### Open Question 2
- Question: Can structural analysis of primitive action traces (identifying repeated subsequences) effectively automate the learning of hierarchical structure?
- Basis in paper: [explicit] Page 15 proposes an alternative to LLM prompting: "analyze the sequence of primitive tasks... when $\tilde{t}$ repeats patterns of subsequences such as load, drive, unload."
- Why unresolved: This is proposed as future work. The current system relies on the LLM to explicitly provide the decomposition structure, rather than inferring it from the generated plan trace.
- What evidence would resolve it: Experimental results comparing a system using subsequence pattern mining against the current LLM-based decomposition approach, specifically measuring generalization capability on repetitive tasks.

### Open Question 3
- Question: Is the goal regression mechanism robust in domains with partial state observability?
- Basis in paper: [inferred] Page 14 contrasts the method with related work (Zhuo et al., 2014), noting that the current approach assumes "full state observability" whereas the related work handles missing atoms in the state.
- Why unresolved: The method learner calculates preconditions using goal regression on known state transitions ($s_{i-1}$ to $s_i$). If the state is incomplete, the regression might fail to identify necessary preconditions or include incorrect ones.
- What evidence would resolve it: Empirical evaluation in a simulated environment with sensor noise or hidden state variables to test if the learned preconditions remain sound and sufficient for planning.

## Limitations

- Cannot learn recursive or compound methods; only learns linear sequences of primitive actions
- Assumes full state observability for goal regression; fails with missing state information
- Linear-only methods fail to generalize to problems with different task counts (e.g., rescuing 3 vs 5 survivors)
- Requires complete domain specifications including annotated tasks and termination methods

## Confidence

- **High**: Soundness guarantees through verifier tasks; mechanism of goal regression for precondition computation; linear-only method learning limitation
- **Medium**: Effectiveness claims on Logistics and Search and Rescue domains; assertion that learned methods reduce LLM calls without hurting success rates
- **Low**: Claims about generalization across domains; ability to scale to more complex recursive tasks

## Next Checks

1. **Ablation study reproduction**: Remove each method from the Logistics and Search and Rescue domains; run 10 problems × 3 trials; measure GPT calls and success rate with vs. without method learner.

2. **Cross-instance generalization test**: After learning from one problem, test method applicability on a different problem with different object counts; verify linear methods fail when subtask counts differ.

3. **Top-level vs. low-level removal comparison**: Compare learning when removing top-level methods (e.g., `searchAndRescue`) vs. leaf methods (e.g., `rescueSurvivor`); verify low-level removal benefits more while high-level removal exposes LLM planning weaknesses.