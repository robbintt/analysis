---
ver: rpa2
title: 'Instella: Fully Open Language Models with Stellar Performance'
arxiv_id: '2511.10628'
source_url: https://arxiv.org/abs/2511.10628
tags:
- arxiv
- training
- data
- stage
- open
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Instella is a fully open 3-billion-parameter language model that
  achieves state-of-the-art results among transparent models of its size, trained
  on AMD Instinct MI300X GPUs using 4 trillion pretraining tokens and specialized
  instruction tuning. The model outperforms all other fully open models on benchmarks
  like ARC, MMLU, and GSM8K, with an average score of 66.6%, narrowing the gap to
  leading open-weight systems while maintaining complete reproducibility.
---

# Instella: Fully Open Language Models with Stellar Performance

## Quick Facts
- arXiv ID: 2511.10628
- Source URL: https://arxiv.org/abs/2511.10628
- Reference count: 17
- Key outcome: 3B-parameter fully open model achieving state-of-the-art among transparent models with 66.6% average score across benchmarks

## Executive Summary
Instella is a fully open 3-billion-parameter language model family that achieves state-of-the-art results among transparent models of its size. Trained on AMD Instinct MI300X GPUs using 4 trillion pretraining tokens and specialized instruction tuning, Instella outperforms all other fully open models on benchmarks like ARC, MMLU, and GSM8K. The model family includes Instella-Long (128K context) and Instella-Math (reinforcement learning for reasoning), with all models released with full training code, data recipes, and evaluation protocols.

## Method Summary
Instella uses a two-stage pre-training approach: stage 1 with 4.07T tokens establishes general language understanding, followed by stage 2 with 58B high-quality tokens focused on reasoning. The model employs weight ensembling across three independently trained seeds to improve robustness. Instella-Long extends context to 128K tokens using gradual RoPE base frequency scaling, while Instella-Math uses multi-stage GRPO for enhanced reasoning. All models are transformer-based with 36 layers, 32 attention heads, and RMSNorm + QK-Norm normalization.

## Key Results
- Base model achieves 66.6% average score across 11 benchmarks, outperforming all fully open 3B models
- Instella-Long extends context to 128K tokens while maintaining competitive performance on Helmet benchmark
- Instella-Math achieves highest reported performance among fully open models on TTT-Bench strategic reasoning benchmark
- Model family released with complete training code, data recipes, and evaluation protocols

## Why This Works (Mechanism)

### Mechanism 1: Two-Stage Pre-training with Reasoning Focus
- Claim: Two-stage pre-training with reasoning-focused second stage improves performance more efficiently than scaling tokens alone
- Core assumption: High-quality, domain-targeted tokens in stage 2 provide more signal per token than continued general-domain training
- Evidence: Stage 2 uses 58B tokens including synthetic math data from GSM8K abstraction; achieves 66.6% vs individual seeds at 65.5-65.8%
- Break condition: If stage 2 data quality degrades (contamination, incorrect synthetic answers), performance gains reverse

### Mechanism 2: Weight Ensembling Across Seeds
- Claim: Weight ensembling across multiple stochastic training seeds improves final model robustness and average benchmark scores
- Core assumption: Seeds introduce complementary learning trajectories whose errors cancel on average
- Evidence: Three independent runs (65.5%, 65.8%, 65.6%) → ensemble (66.6%), outperforming any single run by 0.8-1.1 points
- Break condition: If seeds converge to identical minima (low variance), ensembling provides diminishing returns

### Mechanism 3: Gradual RoPE Base Frequency Scaling
- Claim: Gradual RoPE base frequency scaling with two-stage continued pre-training enables 128K context extension without catastrophic forgetting
- Core assumption: Gradual scaling preserves positional representations; training at 2× target length improves generalization
- Evidence: Extend 4K→64K→256K with base frequency scaling; synthetic long-context QA pairs improve SFT; accepts short-context drop (-1.5% MMLU, -2.6% IFEval)
- Break condition: If RoPE extrapolation fails at inference lengths >128K, or if short-context performance degrades excessively

## Foundational Learning

- **Rotary Position Embeddings (RoPE)**
  - Why needed here: Core to long-context extension; requires understanding base frequency scaling and its effect on extrapolation
  - Quick check question: Can you explain why increasing RoPE base frequency extends effective context length?

- **Group Relative Policy Optimization (GRPO)**
  - Why needed here: Instella-Math uses multi-stage GRPO; understanding rollout-based RL is essential for reproducing reasoning gains
  - Quick check question: How does GRPO differ from standard PPO in terms of reward normalization and critic requirements?

- **Data Mixing and Token Efficiency**
  - Why needed here: Stage-2 58B tokens outperform many models trained on trillions; understanding mixture composition is critical
  - Quick check question: What signals would you use to determine optimal domain ratios in a multi-source pre-training mixture?

## Architecture Onboarding

- **Component map**: 36 transformer decoder layers → 32 attention heads → 32 KV heads → Hidden dim 2,560 → Intermediate dim 6,912 → RMSNorm + QK-Norm → SwiGLU activation → RoPE positional encoding → OLMo tokenizer (50,304 vocab) → Sequence length 4,096 (base) → 128K (Instella-Long)

- **Critical path**: 
  1. Pre-training (stage 1: 4.07T tokens) → stage 2 with 3 seed runs → weight merge
  2. SFT (2.3M instruction pairs) → DPO (760M tokens)
  3. For Long: continued pre-training (40B tokens) → long-context SFT (1B tokens) → short-context DPO
  4. For Math: 2-stage SFT → 3-stage GRPO with increasing rollout lengths

- **Design tradeoffs**: Full attention (no GQA) trades inference speed for training simplicity; Stage-2 focus on quality over quantity: 58B tokens vs. competitors' trillions; Long-context DPO kept short (2K) to avoid instability, accepts short-context performance drop

- **Failure signatures**: Training instability with attention score explosion → check QK-Norm implementation; Long-context retrieval fails at specific lengths → RoPE base may be incorrect; RL collapse → reduce rollout count or learning rate in GRPO

- **First 3 experiments**: 
  1. Reproduce stage-2 single-seed training on a 1B-token subset; verify MMLU/BBH improvements emerge
  2. Ablate synthetic math data in stage 2 by removing it; measure GSM8K delta
  3. Test RoPE scaling with alternative methods (NTK-aware, YaRN) at 64K context; compare to reported base-frequency approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the trade-off between long-context optimization and short-context performance retention more pronounced at the 3B parameter scale compared to larger models?
- Basis in paper: The authors state: "We hypothesize that these results reflect a trade-off between optimizing for longer context lengths and retaining short-context performance, which may be more pronounced at the 3B parameter scale compared to larger models."
- Why unresolved: The paper only evaluates at 3B scale and does not compare against larger model variants to validate this hypothesis
- What evidence would resolve it: Systematic comparison of long-context training effects across multiple model sizes (e.g., 1B, 3B, 7B, 13B) on the same short-context benchmarks

### Open Question 2
- Question: What factors explain the contradictory findings regarding synthetic long-context instruction data effectiveness?
- Basis in paper: The paper states: "Contrary to the findings by (Gao et al., 2024), we observe that our synthetic long-context instruction data notably improves performance on long-context tasks."
- Why unresolved: The paper does not investigate why their synthetic data approach succeeds where prior work found it ineffective
- What evidence would resolve it: Ablation studies comparing data generation methods, quality metrics, and task-specific characteristics between the two approaches

### Open Question 3
- Question: Why does DPO training exclusively on short-context data (2K tokens) continue to improve performance on long-context tasks?
- Basis in paper: The authors note: "Consistent with the findings of other open-weights models, we observe that applying DPO solely on short-context data continues to improve on long-context tasks."
- Why unresolved: The paper documents this phenomenon but offers no explanation for why preference alignment on short sequences transfers to longer contexts
- What evidence would resolve it: Analysis of attention patterns and representation shifts induced by short-context DPO, tested across varying context lengths

### Open Question 4
- Question: What mechanisms enable mathematical reasoning training to generalize to strategic reasoning tasks without explicit exposure?
- Basis in paper: The paper reports: "Remarkably, without any exposure to TTT-Bench–style or similar strategic gaming data during any stage of training, Instella-Math achieves the best performance among all evaluated models" on TTT-Bench
- Why unresolved: The transfer from mathematical to strategic reasoning is observed but not analyzed mechanistically
- What evidence would resolve it: Probing studies examining shared reasoning patterns between mathematical and strategic tasks, or ablations isolating which training stages contribute to TTT-Bench performance

## Limitations
- Weight ensembling methodology lacks detail - doesn't specify whether merging is simple average, learned weights, or layer-specific
- Synthetic math dataset generation details are sparse - no information on prompts, variations, or filtering criteria
- Long-context optimization shows concerning short-context performance degradation (-1.5% MMLU, -2.6% IFEval)

## Confidence

**High Confidence**: Core architecture specifications (36-layer transformer, 32 attention heads, 2,560 hidden dim, RMSNorm + QK-Norm, SwiGLU activation) are fully specified and match standard practices. The two-stage pre-training approach with 4.07T + 58B tokens follows established methodology, and the reported base model performance of 66.6% on 11 benchmarks is internally consistent with individual seed runs (65.5-65.8%).

**Medium Confidence**: Weight ensembling results showing 0.8-1.1 point improvements over individual seeds are plausible given statistical independence of seeds, but exact mechanism remains unclear. Synthetic math dataset's contribution to stage-2 performance is theoretically sound but lacks verification details. Long-context extension methodology (RoPE scaling, gradual base frequency increases) is well-established, but specific parameters and their optimality are uncertain.

**Low Confidence**: GRPO implementation details for Instella-Math are sparse - while the paper mentions three stages with increasing rollout lengths, it doesn't specify reward functions, critic architectures, or hyperparameter tuning. TTT-Bench results claim Instella-Math achieves highest reported performance among fully open models, but without comparison baselines or ablation studies, this claim is difficult to verify.

## Next Checks
1. **Weight Ensembling Verification**: Replicate three independent stage-2 training runs with different random seeds on a 1B-token subset. Test at least three different ensembling methods (simple average, learned weights, layer-wise merging) and compare resulting performance to individual runs to determine if 0.8-1.1 point improvement requires specific merging techniques.

2. **Synthetic Math Dataset Validation**: Attempt to reproduce synthetic math dataset generation by implementing GSM8K abstraction approach described. Generate a small sample (1,000 problems) using symbolic Python program abstraction and verify that resulting problems are solvable and diverse to validate whether 28.5M synthetic tokens can be independently reproduced.

3. **RoPE Scaling Alternative Comparison**: Implement Instella-Long's RoPE base frequency scaling (4K→64K→256K) and compare it against alternative methods (NTK-aware scaling, YaRN) at same context lengths. Measure both long-context performance on Helmet and short-context retention on MMLU/IFEval to quantify trade-offs and determine if reported approach is optimal or if alternatives might reduce 1.5-2.6 point degradation.