---
ver: rpa2
title: Pre-Trained Policy Discriminators are General Reward Models
arxiv_id: '2507.05197'
source_url: https://arxiv.org/abs/2507.05197
tags:
- reward
- arxiv
- policy
- polar
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Policy Discriminative Learning (POLAR), a
  novel reward modeling approach that frames reward models as policy discriminators
  quantifying differences between training and target policies. Instead of traditional
  absolute preference modeling, POLAR pre-trains reward models to recognize identical
  policies and discriminate different ones through contrastive learning, enabling
  criterion-agnostic initialization.
---

# Pre-Trained Policy Discriminators are General Reward Models

## Quick Facts
- arXiv ID: 2507.05197
- Source URL: https://arxiv.org/abs/2507.05197
- Reference count: 40
- Key result: POLAR-7B achieves 81.0% preference accuracy on STEM tasks vs. 54.8% baseline

## Executive Summary
This paper introduces POLAR (Policy-discriminAtive Learning), a novel reward modeling approach that frames reward models as policy discriminators rather than traditional preference predictors. Instead of directly modeling absolute preferences, POLAR pre-trains reward models to recognize whether two trajectories originate from the same policy through contrastive learning on synthetic data from a diverse pool of LLMs. This criterion-agnostic initialization enables better transfer to human preference fine-tuning. The method demonstrates superior performance with POLAR-7B achieving 81.0% preference accuracy on STEM tasks (vs. 54.8% baseline) and 85.5% on creative writing tasks (vs. 57.9% baseline), while improving RLHF policy performance from 47.36% to 56.33% on average across 20 benchmarks.

## Method Summary
POLAR pre-trains reward models as policy