---
ver: rpa2
title: 'With Great Capabilities Come Great Responsibilities: Introducing the Agentic
  Risk & Capability Framework for Governing Agentic AI Systems'
arxiv_id: '2512.22211'
source_url: https://arxiv.org/abs/2512.22211
tags:
- risk
- agentic
- system
- ctrl
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The ARC framework introduces a capability-centric approach to
  govern agentic AI systems by systematically identifying, assessing, and mitigating
  risks through analysis of three core elements: components, design, and capabilities.
  It provides a structured methodology with a Risk Register linking specific risks
  to technical controls, enabling organizations to contextualize threats based on
  impact and likelihood.'
---

# With Great Capabilities Come Great Responsibilities: Introducing the Agentic Risk & Capability Framework for Governing Agentic AI Systems

## Quick Facts
- **arXiv ID**: 2512.22211
- **Source URL**: https://arxiv.org/abs/2512.22211
- **Reference count**: 40
- **Primary result**: Introduces a capability-centric framework for systematically identifying, assessing, and mitigating risks in agentic AI systems through analysis of components, design, and capabilities

## Executive Summary
The ARC framework provides a structured methodology for governing agentic AI systems by focusing on capabilities rather than specific tools. It introduces a Risk Register that maps risks to technical controls across three tiers (Cardinal, Standard, Best Practice), enabling organizations to filter and prioritize risks based on contextual impact and likelihood assessments. The framework demonstrates scalability through application to two example systems (Researcher and Vibe Coder), showing how different risk profiles can be managed through tailored control implementation. By abstracting tools into broader capability categories and providing forward guidance through tiered controls, ARC balances comprehensive risk management with practical implementation requirements.

## Method Summary
The ARC framework profiles agentic systems through three elements: Components (LLM, Tools, Instructions, Memory), Design (Architecture, Roles/Access Controls, Monitoring), and Capabilities (Cognitive, Interaction, Operational). Users map these elements to a Risk Register containing 46+ risks, then contextualize each risk using 5-point impact and likelihood scales. Risks are filtered through organizational relevance thresholds to identify which require controls. Mapped technical controls are implemented across three levels: Level 0 (Cardinal/mandatory), Level 1 (Standard), and Level 2 (Best Practice). The process enables scalable risk management while maintaining safety through prescriptive control mappings.

## Key Results
- Successfully demonstrated capability abstraction by grouping diverse tools (search APIs, code editors) under unified capability categories, reducing complexity
- Showed risk filtering effectiveness by reducing 38 identified risks to 10 relevant ones for the Researcher system through contextual threshold application
- Validated tiered control mapping approach by prescribing specific Level 0 controls (e.g., CTRL-0007 "Log all LLM inputs") as baseline requirements

## Why This Works (Mechanism)

### Mechanism 1: Capability Abstraction for Scalability
- Claim: Abstracting specific tools into broader capability categories reduces complexity compared to tool-specific approaches
- Mechanism: Grouping diverse tools under single capability categories (e.g., "Internet & Search Access") provides stable analysis units that remain valid as specific tools change
- Core assumption: Risks are primarily driven by the type of action (capability) rather than specific tool implementation
- Evidence anchors: Section 4.1.3 states capabilities "offer a more holistic unit of analysis than analyzing specific tools," avoiding granular control for every MCP

### Mechanism 2: Contextualized Filtering via Relevance Thresholds
- Claim: Filtering risks based on context-specific "relevance threshold" (impact × likelihood) enables differentiated governance
- Mechanism: Organizations set minimum thresholds for impact and likelihood; risks below this line are deprioritized
- Core assumption: Organizations can accurately estimate impact and likelihood during assessment
- Evidence anchors: Section 4.4.2 details establishing relevance threshold; Section 5 shows "Researcher" filtering 38 risks down to 10 relevant ones

### Mechanism 3: Tiered Control Mapping
- Claim: Linking risks to tiered technical controls (Cardinal, Standard, Best Practice) creates prescriptive "nexus" for proportional implementation
- Mechanism: Mapping specific risks to controls categorized by criticality (Level 0, 1, 2) provides forward guidance
- Core assumption: Prescribed controls are effective at mitigating associated risks
- Evidence anchors: Section 4.3.1 defines three control levels; Appendix C provides detailed mapping (e.g., CTRL-0001 as Level 0)

## Foundational Learning

- Concept: **Affordances vs. Capabilities**
  - Why needed here: Framework distinguishes between affordances (environmental properties like tools/MCPs) and capabilities (actions agent can take)
  - Quick check question: Does the risk stem from tool's existence (affordance) or agent's ability to autonomously execute action (capability)?

- Concept: **Agentic Architecture Patterns**
  - Why needed here: Section 4.1.2 identifies architecture as primary risk source; understanding patterns like "cascading errors" is essential
  - Quick check question: In multi-agent system, does failure in Agent A propagate to Agent B, and is there a circuit breaker?

- Concept: **Risk Registers**
  - Why needed here: Framework's output is Risk Register (Section 4.2.3); familiarity with standard risk management concepts is prerequisite
  - Quick check question: Can you articulate risk statement that combines element, failure mode, and hazard?

## Architecture Onboarding

- Component map: System definition (Components, Design, Capabilities) -> Risk Register (Element + Failure Mode + Hazard) -> Contextualization (Impact/Likelihood) -> Threshold Filtering -> Relevant Risk List -> Mapped Technical Controls (Tier 0/1/2)

- Critical path: 1) Inventory system's capabilities (not just tools), 2) Cross-reference capabilities against Risk Register to identify risks, 3) Apply organizational relevance thresholds to filter risks, 4) Implement mapped "Cardinal" (Level 0) controls as baseline

- Design tradeoffs:
  - Breadth vs. Depth: Comprehensive register covers more risks but increases assessment overhead
  - Prescriptiveness: Strict control mapping ensures safety but may limit flexibility for novel use cases

- Failure signatures:
  - Static Registry: Risk Register not updated with new academic findings or attack vectors
  - Subjective Assessment: High variance in impact/likelihood scoring leads to inconsistent governance
  - Capability Creep: System gains new capabilities without re-triggering risk assessment

- First 3 experiments:
  1. Capability Inventory: Map tools to framework's capability categories (Cognitive, Interaction, Operational)
  2. Threshold Calibration: Run "Researcher" worked example with your organization's specific relevance thresholds
  3. Control Audit: Verify presence of "Cardinal" control (e.g., CTRL-0007 "Log all LLM inputs") in current production system

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can effectiveness of technical controls in ARC Risk Register be empirically validated against real-world agentic failures?
- Basis in paper: [explicit] Conclusion states future work must focus on "developing empirical approaches to validate the risks and controls in the Risk Register"
- Why unresolved: Framework is currently conceptual, relying on literature synthesis and stylized examples rather than quantitative experimental data
- Evidence: Controlled experiments comparing failure rates of agentic systems with and without specific ARC controls applied across different capability profiles

### Open Question 2
- Question: To what extent do combinatorial risks—arising from interaction of multiple capabilities—exceed sum of individual capability risks?
- Basis in paper: [inferred] Section 4.3.2 notes identifying residual risks is difficult, specifically highlighting "combinatorial risks which arise from interaction of two or more capabilities"
- Why unresolved: Risk Register generally maps risks to specific elements linearly, potentially missing non-linear emergent risks
- Evidence: Study measuring correlation between specific combinations of capabilities (e.g., Internet Access plus Code Execution) and frequency of unforeseen hazards or compound failure modes

### Open Question 3
- Question: What automated architectural designs are required to support real-time implementation and updating of ARC framework?
- Basis in paper: [explicit] Conclusion identifies "building automated tools to support implementation and regular updating of the framework" as primary future development area
- Why unresolved: Current implementation relies on manual checklists and human governance teams, creating scalability issues
- Evidence: Development and successful deployment of software tool that can automatically parse agent's configuration to populate Risk Register and trigger control updates

## Limitations

- Subjective risk assessment: Impact and likelihood scoring lacks standardized benchmarks, relying on qualitative descriptors
- Limited comprehensiveness: Current 46 risks require domain-specific updates to remain relevant across diverse applications
- Assumed control effectiveness: Framework assumes prescribed controls are technically feasible and proportionally effective without empirical validation

## Confidence

- **High Confidence**: Structural approach (capability abstraction, tiered controls, relevance filtering) is logically coherent and aligns with established risk management principles
- **Medium Confidence**: Risk assessment methodology works in principle, but practical calibration challenges and subjective scoring could reduce effectiveness
- **Low Confidence**: Effectiveness of specific control mappings is assumed rather than empirically validated; scalability to complex multi-agent systems remains theoretical

## Next Checks

1. **Impact Calibration Study**: Conduct controlled study with multiple organizations applying framework to identical agentic systems to measure variance in impact/likelihood scores

2. **Control Effectiveness Testing**: Select 5-10 controls from Risk Register and empirically test effectiveness against representative attack vectors in sandbox environment

3. **Scalability Assessment**: Apply framework to production multi-agent system with 20+ capabilities and 100+ tools to measure assessment overhead and identify bottlenecks in capability abstraction process