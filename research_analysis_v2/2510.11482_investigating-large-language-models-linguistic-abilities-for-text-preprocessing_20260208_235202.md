---
ver: rpa2
title: Investigating Large Language Models' Linguistic Abilities for Text Preprocessing
arxiv_id: '2510.11482'
source_url: https://arxiv.org/abs/2510.11482
tags:
- preprocessing
- llms
- text
- traditional
- stemming
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether large language models can perform
  text preprocessing tasks like stopword removal, lemmatization, and stemming more
  effectively than traditional methods. It compares multiple open-source LLMs (Gemma-2,
  Gemma-3, Llama-3.1, Phi-4, Qwen-2.5) across six European languages using prompts
  to guide preprocessing.
---

# Investigating Large Language Models' Linguistic Abilities for Text Preprocessing

## Quick Facts
- **arXiv ID:** 2510.11482
- **Source URL:** https://arxiv.org/abs/2510.11482
- **Reference count:** 38
- **One-line primary result:** LLM-based preprocessing improves text classification F1 scores by up to 6% compared to traditional methods

## Executive Summary
This study investigates whether large language models can perform text preprocessing tasks like stopword removal, lemmatization, and stemming more effectively than traditional methods. Using multiple open-source LLMs across six European languages, the research compares LLM-based preprocessing to traditional tools (NLTK, spaCy) and evaluates their impact on downstream text classification tasks. Results demonstrate that LLMs can accurately replicate traditional preprocessing with high accuracy rates and improve classification performance, particularly when combined with lemmatization. However, LLMs underperform in stemming tasks due to inconsistencies in applying rules.

## Method Summary
The study employs several open-source LLMs (Gemma-2, Gemma-3, Llama-3.1, Phi-4, Qwen-2.5) to perform text preprocessing tasks using in-context learning through prompts. Traditional preprocessing is performed using NLTK and spaCy for baseline comparison. The research uses six datasets (SemEval-18/19 Twitter datasets, Reuters, AG News, and Tweet Sentiment Multilingual corpus) covering six languages. Accuracy metrics (SW, NSW, L, S) compare LLM output to traditional methods, while downstream performance is measured using micro F1 scores from classifiers trained on TF-IDF features. The experiments test both English and language-specific prompts for non-English datasets.

## Key Results
- LLMs achieve high accuracy replicating traditional preprocessing: 97% for stopword removal, 82% for lemmatization, and 74% for stemming
- LLM-based preprocessing improves downstream text classification F1 scores by up to 6% compared to traditional techniques
- The highest performance gains occur when combining stopword removal with lemmatization
- LLMs underperform in stemming due to inconsistencies in applying rules across documents

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs achieve high accuracy in replicating stopword removal and lemmatization by leveraging context during in-context learning.
- **Mechanism:** The LLM is provided with a prompt containing a formal task description, a few examples, the input text, and the downstream task context (e.g., sentiment analysis). This enables the model to use its pre-trained contextual representations to dynamically identify which words are irrelevant (stopwords) or to determine the correct dictionary form (lemma) based on part-of-speech and surrounding sentence meaning.
- **Core assumption:** The model's pre-training has exposed it to sufficient linguistic patterns to generalize these tasks without fine-tuning, and the provided examples are sufficient to anchor the task.
- **Evidence anchors:** [abstract] "Our analysis indicates that LLMs are capable of replicating traditional stopword removal, lemmatization, and stemming methods with accuracies reaching 97%, 82%, and 74%, respectively." [section] "Due to their ability to take the linguistic context into account... we hypothesize that LLMs can dynamically detect stopwords, lemmas and stems based on the input document, context and task."

### Mechanism 2
- **Claim:** Context-sensitive LLM preprocessing improves downstream text classification by preserving task-relevant signal.
- **Mechanism:** Unlike static methods, LLMs can be instructed to retain words critical for a specific task (e.g., "not" for sentiment analysis) while removing others. They also produce more accurate lemmas by using document context. This results in cleaner, more meaningful text representations (e.g., TF-IDF features) that enhance classifier performance.
- **Core assumption:** Deviations from traditional preprocessing are beneficial, not random noise, and the computational cost is justified by the performance gain.
- **Evidence anchors:** [abstract] "When used for preprocessing in text classification tasks, LLM-based methods improve F1 scores by up to 6% compared to traditional techniques, especially when combined with lemmatization." [section] "LLMs achieve the highest performance in stopword removal combined with lemmatization... indicating their ability to dynamically identify task-relevant stopwords and lemmas..."

### Mechanism 3
- **Claim:** LLMs underperform in stemming because they fail to apply rules consistently.
- **Mechanism:** Stemming is a rigid, rule-based process that often ignores context. LLMs, which are optimized for contextual understanding, struggle to mimic this deterministic behavior. They may produce different stems for the same word across documents based on subtle context shifts, leading to non-standardized token representations that harm feature-matching in downstream classifiers.
- **Core assumption:** The primary cause of low performance is the inconsistency of the output relative to a standardized stemmer, and this inconsistency is detrimental.
- **Evidence anchors:** [abstract] "However, LLMs underperform in stemming, likely due to inconsistencies in applying rules." [section] "LLMs exhibit inconsistencies in stemming across documents... This lack of consistency results in non-standardized text representations, which can negatively impact downstream tasks such as lexical feature extraction..."

## Foundational Learning

- **Concept: In-Context Learning / Few-Shot Learning**
  - **Why needed here:** The entire methodology relies on guiding the LLM via prompts with examples, not by retraining the model.
  - **Quick check question:** How does providing 2-3 examples of lemmatization in the prompt enable the LLM to correctly lemmatize a word it hasn't seen in that context before?

- **Concept: Contextual vs. Non-Contextual Preprocessing**
  - **Why needed here:** The core argument is that traditional methods (like static stopword lists) ignore context, while LLMs use it to make smarter decisions.
  - **Quick check question:** Why would the word "leaves" be lemmatized differently in a botany textbook compared to an HR manual, and how would an LLM handle this versus a standard lemmatizer?

- **Concept: Bag-of-Words (BoW) & TF-IDF**
  - **Why needed here:** The preprocessed text is converted into numerical features using TF-IDF before being fed into a classifier. Understanding this step is critical to see why inconsistent stemming (which breaks token matching) harms performance.
  - **Quick check question:** If an LLM stems "running" as "run" in one document and "runn" in another, how does that affect the TF-IDF vector for a classifier looking for the token "run"?

## Architecture Onboarding

- **Component map:** Raw Text -> Prompt Constructor -> LLM Inference Engine -> Preprocessed Text -> Feature Extractor -> ML Classifier -> Evaluation (Traditional Preprocessor runs in parallel)
- **Critical path:** Raw Text -> Prompt Constructor -> LLM Inference Engine -> Preprocessed Text -> Feature Extractor -> ML Classifier -> Evaluation
- **Design tradeoffs:**
  - **Cost vs. Context:** LLM preprocessing offers superior context-sensitivity but at a significantly higher computational cost than traditional methods.
  - **Consistency vs. Nuance:** Traditional methods guarantee consistency (crucial for stemming). LLMs offer nuance (crucial for lemmatization) but can be inconsistent.
  - **Prompt Language:** The paper shows mixed results for native-language prompts vs. English prompts, suggesting a tradeoff between model's primary language proficiency and native task clarity.
- **Failure signatures:**
  - **Stemming Inconsistency:** Same root word producing different stems (e.g., "argued" -> "argu" in one doc, "argue" in another).
  - **Over-Removal (NSW):** LLM removing high percentages of non-stopwords (NSW metric), potentially deleting key signal words.
  - **Task Drift:** LLM ignoring the prompt's instructions and performing a different task (e.g., summarizing instead of lemmatizing).
- **First 3 experiments:**
  1. **Replicate Stopword Accuracy:** Run a subset of the English dataset through the LLM and traditional NLTK stopword removal. Calculate the "SW" (matched stopwords) and "NSW" (removed non-stopwords) metrics to confirm the ~97% accuracy claim.
  2. **Contextual Lemmatization Test:** Create a synthetic dataset with ambiguous words (e.g., "saw", "leaves") placed in different sentence contexts. Compare LLM output against spaCy to verify its ability to disambiguate based on context.
  3. **Classifier Performance Benchmark:** Train a Logistic Regression model on a medium-sized dataset (e.g., AG News) preprocessed by both the LLM and NLTK. Compare the F1 scores to validate the "up to 6%" improvement claim and analyze where the LLM version fails.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How effectively can LLMs perform stemming and lemmatization for low-resource languages that lack extensive annotated resources or traditional tools?
- **Basis in paper:** [explicit] The Conclusion states, "Given the promising results achieved, future work will explore the potential of LLMs as stemming and lemmatization tools for low-resource languages."
- **Why unresolved:** The current study restricted its evaluation to six European languages (English, French, German, Italian, Portuguese, Spanish) which already possess mature, traditional preprocessing tools.
- **What evidence would resolve it:** Evaluating LLM-based preprocessing on languages lacking standard stemmers/lemmatizers and measuring the impact on downstream classification tasks.

### Open Question 2
- **Question:** How does extensive prompt engineering impact the consistency and accuracy of LLM-based preprocessing compared to the few-shot approach used in this study?
- **Basis in paper:** [inferred] The authors state in the Limitations section: "We do not perform extensive prompt engineering in this work... However, some results may differ if other prompts are considered."
- **Why unresolved:** The experiments utilized a fixed set of prompts; the sensitivity of the models' performance, particularly for inconsistent tasks like stemming, to prompt optimization remains unknown.
- **What evidence would resolve it:** Ablation studies comparing various prompt strategies (e.g., chain-of-thought, different example sets) to measure variance in stemming consistency and classification improvements.

### Open Question 3
- **Question:** Can a new evaluation methodology validate cases where LLMs outperform traditional libraries, such as correctly splitting and lemmatizing hashtags?
- **Basis in paper:** [inferred] The Limitations section notes that "instances where LLMs outperform these libraries â€“ for example, by splitting a long hashtag... are not accounted for in the evaluation metrics."
- **Why unresolved:** The paper evaluated LLMs primarily on their ability to replicate the output of traditional tools (NLTK, spaCy), penalizing contextually correct deviations.
- **What evidence would resolve it:** A human-annotated ground truth dataset for preprocessing that includes complex web-text features (like hashtags) to assess semantic correctness rather than just replication.

## Limitations
- The study primarily evaluates LLM's ability to replicate existing preprocessing methods rather than creating entirely new, more effective ones
- LLM-based preprocessing introduces significant computational overhead compared to traditional methods
- The performance of LLM-based stemming is notably weaker due to inconsistencies in rule application
- The results may not generalize to other domains or types of text preprocessing tasks beyond the three tested

## Confidence
- **High Confidence:** The LLM's ability to replicate traditional stopword removal and lemmatization with high accuracy is well-supported by the results. The improvement in downstream text classification performance (up to 6% F1 score) when using LLM-based preprocessing is a strong finding.
- **Medium Confidence:** The claim that LLM-based preprocessing is "especially effective in low-resource languages" is supported by the multilingual experiments but requires more extensive testing across a wider range of low-resource languages.
- **Low Confidence:** The specific reasons for the LLM's underperformance in stemming are hypothesized but not definitively proven. The generalizability of the results to other domains or types of text preprocessing tasks beyond the three tested is uncertain.

## Next Checks
1. **Rule Consistency Analysis for Stemming:** Conduct a detailed analysis of the LLM's stemming output to identify the specific types of inconsistencies that lead to lower accuracy by tracking the frequency of different stem variations for the same root word across documents.
2. **Domain Transfer Experiment:** Test the LLM-based preprocessing on a different domain, such as biomedical text or legal documents, to assess its ability to generalize beyond the social media and news data used in the study.
3. **Cost-Benefit Analysis:** Perform a comprehensive cost-benefit analysis comparing the computational overhead of LLM-based preprocessing to the performance gains in downstream tasks, including an assessment of the environmental impact and deployment feasibility.