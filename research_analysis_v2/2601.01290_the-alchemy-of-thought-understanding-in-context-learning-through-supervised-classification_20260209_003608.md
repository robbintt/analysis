---
ver: rpa2
title: 'The Alchemy of Thought: Understanding In-Context Learning Through Supervised
  Classification'
arxiv_id: '2601.01290'
source_url: https://arxiv.org/abs/2601.01290
tags:
- examples
- llms
- relevance
- table
- classifiers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates how in-context learning (ICL) in large language
  models (LLMs) compares to supervised classifiers trained on the same examples. By
  analyzing six text classification datasets with three LLMs and two types of classifiers
  (kNN and logistic regression), the authors find that ICL behaves similarly to kNN
  when example relevance is high, showing strong correlation between example relevance
  and behavior similarity.
---

# The Alchemy of Thought: Understanding In-Context Learning Through Supervised Classification

## Quick Facts
- **arXiv ID**: 2601.01290
- **Source URL**: https://arxiv.org/abs/2601.01290
- **Reference count**: 23
- **Primary result**: ICL behaves similarly to kNN when example relevance is high; diverges when relevance is low as LLMs fall back to parametric memory

## Executive Summary
This paper investigates the behavior of in-context learning (ICL) in large language models (LLMs) by comparing it to supervised classifiers trained on the same examples. Through experiments on six text classification datasets with three LLMs and two classifier types (kNN and logistic regression), the authors find that ICL aligns closely with kNN when demonstrations are highly relevant to test inputs. When examples are less relevant, ICL diverges as LLMs rely on their parametric memory rather than context. The study establishes a strong correlation (r≈0.67–0.80) between example relevance and behavior similarity, suggesting that for high-relevance cases, ICL can be replaced with kNN for efficiency, while LLMs maintain advantages in low-relevance scenarios through their ability to fall back on prior knowledge.

## Method Summary
The study compares ICL in LLMs to kNN and logistic regression classifiers trained on the same k retrieved examples. Using Sentence-BERT embeddings, the authors retrieve top-k (k∈{1,10,20,30}) nearest neighbors from training data for each test point. All models receive identical demonstrations: kNN (unweighted/weighted), logistic regression (TF-IDF features from k examples), and LLMs via ICL prompts. The authors compute test accuracy, Cohen's Kappa agreement between model outputs, and Pearson correlation between human or LLM-annotated example relevance scores and Kappa values. Relevance is binary-annotated for 50 test prompts per dataset.

## Key Results
- On average, ICL shows higher agreement with kNN than logistic regression, indicating attention mechanisms behave more similarly to similarity-based retrieval than gradient descent
- Example relevance scores strongly correlate with behavior similarity between ICL and classifiers (R² up to 0.631 for human annotations, 0.455–0.477 for LLM annotations)
- When demonstration relevance is low, LLMs outperform classifiers by falling back to parametric memory, while classifiers degrade proportionally to example quality
- Kappa agreement between ICL and kNN approaches zero for low-relevance examples (e.g., AuTexTification), while zero-shot ICL maintains high performance

## Why This Works (Mechanism)

### Mechanism 1
In-context learning behavior correlates with k-nearest neighbors when demonstration relevance is high. The attention mechanism in decoder-only LLMs implicitly weights demonstrations by similarity to the query, analogous to distance-weighted voting in kNN. When retrieved examples are semantically aligned with the test input, both systems produce similar label predictions. This breaks when demonstration relevance scores drop below ~0.5, where Kappa agreement between ICL and kNN approaches zero.

### Mechanism 2
LLMs fall back to parametric memory when demonstration relevance is low, preserving performance where classifiers degrade. LLMs encode task knowledge from pre-training that can be accessed independently of ICL demonstrations. When context provides weak or irrelevant examples, the model suppresses context influence and generates predictions from internal representations. This fallback is validated by high Kappa agreement between zero-shot and low-quality-ICL outputs.

### Mechanism 3
Example relevance predicts behavior similarity between ICL and classifiers with correlation r≈0.67–0.80. Relevance scores measure alignment between demonstrations and test input, with higher relevance increasing the weight LLMs place on context and reducing reliance on parametric priors. This correlation is supported by strong R² values between relevance and model Kappa, though external validation across architectures is not yet available.

## Foundational Learning

- **Concept**: k-Nearest Neighbors (kNN) classification and distance weighting
  - Why needed here: The paper's central claim is that ICL approximates kNN; understanding how kNN uses similarity to vote on labels is prerequisite to interpreting the results.
  - Quick check question: Given 5 neighbors with labels [A, A, B, B, C] and cosine similarities [0.9, 0.8, 0.7, 0.6, 0.5], what label would unweighted vs. weighted kNN predict?

- **Concept**: Cohen's Kappa agreement statistic
  - Why needed here: The paper uses Kappa to quantify behavior similarity between models; unlike accuracy, Kappa corrects for chance agreement.
  - Quick check question: Two models each predict 90% of examples as class A. They agree on 82% of cases. Is their Kappa high, moderate, or low?

- **Concept**: Parametric vs. non-parametric memory in LLMs
  - Why needed here: The paper's key distinction is that LLMs can access parametric (pre-training) memory while classifiers rely only on non-parametric (example) memory.
  - Quick check question: A sentiment classifier trained on 100 examples encounters a review about a domain not seen during training. How would a pure kNN classifier vs. an LLM with ICL likely differ in handling this?

## Architecture Onboarding

- **Component map**: Sentence-BERT embedder -> retrieves top-k demonstrations -> kNN classifier / Logistic Regression / LLM with ICL prompt -> Relevance annotator (human or LLM)

- **Critical path**:
  1. Embed all training/test data with Sentence-BERT
  2. For each test point, retrieve k=10–20 nearest neighbors
  3. Feed same examples to all models (kNN, LR, LLM)
  4. Compare predictions via contingency matrices and Kappa
  5. Annotate subset for relevance; compute correlation with model agreement

- **Design tradeoffs**:
  - Sentence-BERT similarity may not capture task-relevant features; SKILL-KNN (An et al., 2023) uses LLM-generated skill descriptions for more robust selection
  - Unweighted kNN treats all neighbors equally; weighted kNN uses similarity scores but showed minimal performance difference in this paper
  - Logistic regression is linear; non-linear GD classifiers (e.g., neural networks) were not tested and may show different alignment with ICL

- **Failure signatures**:
  - Low Kappa agreement despite similar accuracy → models reach same performance via different predictions (check contingency matrix for off-diagonal mass)
  - Accuracy drops sharply as k increases → low-quality examples dilute signal; may indicate retrieval method mismatch with task
  - LLM outperforms classifiers significantly → check relevance scores; likely low relevance triggering parametric fallback

- **First 3 experiments**:
  1. Replicate on a single dataset (e.g., AG News) with k=10, comparing GPT-4o-mini vs. kNN predictions; compute Kappa to verify baseline agreement (>0.8 expected).
  2. Introduce noisy demonstrations (random or label-inverted examples) and measure Kappa drop between LLM and kNN; verify LLM-Kappa with zero-shot stays high (parametric fallback).
  3. Swap Sentence-BERT for a task-specific embedding (e.g., fine-tuned on target domain) and measure whether correlation between relevance and model agreement increases.

## Open Questions the Paper Calls Out

### Open Question 1
Is there a direct causal link between example relevance and behavior similarity, or are they merely correlated? The current study establishes a strong correlation using Pearson and R-squared coefficients, but does not isolate causality through controlled intervention. An experiment where the LLM and classifiers are run on identical inputs where only the relevance of the k examples is manipulated would resolve this.

### Open Question 2
Does In-Context Learning (ICL) behave more similarly to non-linear gradient descent classifiers than the linear Logistic Regression tested? The authors note that ICL might behave closer to a non-linear GD-based classifier such as a feed-forward neural network. Repeating the analysis using a Multi-Layer Perceptron trained on the same demonstrations would clarify this.

### Open Question 3
Do the findings regarding ICL's similarity to kNN and its reliance on parametric memory generalize to generative tasks beyond text classification? The study focused on classification due to its ease of analysis. Extending to generative tasks like summarization by quantifying demonstration relevance and measuring similarity to retrieved-generation baselines would test generalizability.

### Open Question 4
To what extent does ICL approximate higher-order optimization algorithms (like Newton's method) compared to first-order gradient descent? The paper contrasts ICL with kNN and first-order GD (Logistic Regression), but related work suggests transformers implement Newton's method. A comparative analysis against second-order optimization methods would resolve this.

## Limitations
- The correlation between example relevance and ICL-classifier agreement is based on six text classification datasets and three LLMs, limiting generalizability to other domains or model families
- Semantic similarity retrieval via Sentence-BERT may not capture task-relevant features for all datasets, and no ablation tests were run to isolate its impact
- Parametric memory fallback is inferred from performance patterns rather than directly measured, leaving open the possibility that low-relevance ICL simply induces random or biased predictions

## Confidence
- ICL behaves similarly to kNN when example relevance is high: **High**
- LLMs fall back to parametric memory when demonstration relevance is low: **Medium**
- Example relevance predicts behavior similarity with r≈0.67–0.80: **Medium**

## Next Checks
1. Reproduce the main correlation analysis on a held-out dataset from a different domain (e.g., biomedical or legal text) to test generalization beyond the six original datasets
2. Replace Sentence-BERT with a task-specific embedding method (e.g., SKILL-KNN) and measure whether the relevance-to-Kappa correlation improves
3. Test a non-linear gradient-based classifier (e.g., fine-tuned BERT) as an additional baseline to see if ICL aligns more closely with similarity-based or gradient-based learning in low-relevance scenarios