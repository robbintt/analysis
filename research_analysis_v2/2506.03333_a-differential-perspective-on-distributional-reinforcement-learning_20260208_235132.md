---
ver: rpa2
title: A Differential Perspective on Distributional Reinforcement Learning
arxiv_id: '2506.03333'
source_url: https://arxiv.org/abs/2506.03333
tags:
- reward
- per-step
- quantile
- differential
- update
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Differential Distributional Reinforcement
  Learning (D2) and Double Differential Distributional Reinforcement Learning (D3),
  the first distributional RL algorithms designed for average-reward Markov Decision
  Processes. Unlike prior distributional RL methods focused on discounted returns,
  D2 learns the limiting per-step reward distribution while D3 additionally learns
  the differential return distribution.
---

# A Differential Perspective on Distributional Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2506.03333
- **Source URL:** https://arxiv.org/abs/2506.03333
- **Reference count:** 40
- **Primary result:** Introduces D2 and D3 algorithms for average-reward MDPs that learn per-step reward distributions while maintaining O(m) scalability

## Executive Summary
This paper introduces Differential Distributional Reinforcement Learning (D2) and Double Differential Distributional Reinforcement Learning (D3), the first distributional RL algorithms designed specifically for average-reward Markov Decision Processes. Unlike prior distributional RL methods focused on discounted returns, D2 learns the limiting per-step reward distribution while D3 additionally learns the differential return distribution. Both use quantile-based approaches with per-step reward updates that converge to the average reward, plus either Q-value updates (D2) or both Q-value and return quantile updates (D3). Theoretical analysis proves almost sure convergence in tabular settings, and experiments on red-pill blue-pill, inverted pendulum, and Atari 2600 environments show competitive or superior performance compared to non-distributional differential algorithms while capturing rich distributional information.

## Method Summary
The D2 algorithm maintains m quantiles representing the cumulative distribution function of immediate rewards, updating them via standard quantile regression to estimate the average reward as their mean. It then uses this estimate to center rewards in standard TD updates. D3 extends this by simultaneously learning the differential return distribution via quantile regression on centered returns. Both algorithms use a two-timescale approach where quantile updates converge faster than value function updates, with theoretical convergence guarantees established in tabular settings. The D2 algorithms are more scalable than discounted distributional methods, requiring only O(m) parameters versus O(|S||A|m) for discounted approaches.

## Key Results
- D2 and D3 algorithms successfully learn the limiting per-step reward distribution in tabular settings
- D2 Actor-Critic achieves competitive performance on inverted pendulum control task
- D2 Q-learning with replay buffer shows comparable or better performance than Differential Q-learning on Atari 2600 games
- D3 empirically outperforms D2 on some Atari environments by capturing differential return distributions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The long-run average reward $\bar{r}$ can be estimated by averaging the quantiles of the limiting per-step reward distribution.
- **Mechanism:** The algorithm maintains m quantiles $\theta_i$ representing the CDF of immediate rewards. It updates these quantiles using standard quantile regression, and the average reward estimate $\bar{R}$ is calculated as the mean of these quantiles. This captures the full distribution of rewards, providing a robust estimate of the mean even if the reward distribution is non-stationary or multi-modal.
- **Core assumption:** The MDP satisfies unichain or communicating assumptions such that a unique limiting distribution of rewards exists.
- **Evidence anchors:**
  - [abstract]: "D2 learns the limiting per-step reward distribution... plus either Q-value updates."
  - [Section 4.2]: Equation 17a defines $\bar{R}$ as the average of quantiles; Lemma 4.2 proves the mean of quantiles converges to the average reward.
- **Break condition:** If the environment is non-ergodic, a global limiting distribution may not exist, causing the quantile estimates to oscillate or converge to a meaningless mixture.

### Mechanism 2
- **Claim:** Differential value function convergence relies on a two-timescale update where the distributional average-reward estimate converges faster than the value function.
- **Mechanism:** The algorithm operates on two timescales. The per-step reward quantiles $\theta$ (and thus $\bar{R}$) update on a "faster" timescale compared to the value function $Q$. From the perspective of the value function update, $\bar{R}$ appears almost equilibrated, centering the TD error effectively.
- **Core assumption:** Step size conditions satisfy $\alpha_n / \alpha_{\theta,n} \to 0$.
- **Evidence anchors:**
  - [Section 4.2]: Algorithm 1 shows the separation of update steps.
  - [Appendix B.2]: Explicitly invokes a "two-timescales argument" to prove convergence of the value function estimates.
- **Break condition:** If the value function step size is too large relative to the quantile step size, the centering term $\bar{R}$ will fluctuate wildly during $Q$ updates, preventing convergence.

### Mechanism 3
- **Claim:** Optimizing the differential return distribution (D3) empirically improves control performance over optimizing only the average reward (D2) by capturing state-dependent variance.
- **Mechanism:** D3 extends D2 by simultaneously learning the per-step reward distribution and the differential return distribution $\Omega$. It uses a distributional Bellman update on the centered rewards. This allows the agent to perform risk-aware or variance-aware optimization, which is hypothesized to improve policy robustness.
- **Core assumption:** The empirical advantage holds; D3 convergence is not theoretically proven in the paper for the control setting, only explored empirically.
- **Evidence anchors:**
  - [Section 4.3]: "We explore such an approach from a purely empirical perspective."
  - [Figure 4]: Shows D3 outperforming D2 in Atari environments.
- **Break condition:** If $\bar{R}$ is inaccurate, the centered rewards become biased, causing the return distribution $\Omega$ to drift or diverge.

## Foundational Learning

- **Concept: Average-Reward (Differential) MDPs**
  - **Why needed here:** The entire framework replaces the discount factor $\gamma$ with a reward centering term $\bar{r}$. You must understand that the goal is maximizing long-term per-step reward, not cumulative discounted sum.
  - **Quick check question:** What happens to the differential return $G_t$ if the policy stays at the optimal average reward $\bar{r}^*$? (Answer: It stabilizes/finitesimal; if $\bar{r} = \bar{r}^*$, the "differential" return is centered around 0).

- **Concept: Quantile Regression**
  - **Why needed here:** The core mechanic uses quantiles to represent distributions, not categorical bins. You need to understand the asymmetric update rule where the error is $(\tau - \mathbb{1}_{y < \theta})$.
  - **Quick check question:** If the target quantile $\tau=0.75$ and the sample $y$ is consistently below your estimate $\theta$, does $\theta$ move up or down? (Answer: Down; the error is positive, pushing $\theta$ down to capture the 75th percentile which is lower than expected).

- **Concept: Two-Timescale Stochastic Approximation**
  - **Why needed here:** Understanding that the average-reward estimator must "lead" the value function estimator is critical for debugging convergence.
  - **Quick check question:** Why can't we just use the same step size for the average-reward quantiles and the Q-values? (Answer: The value function requires a stable baseline $\bar{R}$ to calculate the TD error; if $\bar{R}$ moves as fast as $Q$, the target is constantly shifting, preventing convergence).

## Architecture Onboarding

- **Component map:**
  - Input (S, A, R) -> Quantile Store (Global) -> Average Reward Calculator -> Value Network (D2) or Value + Return Networks (D3) -> Q-value/Return Updates

- **Critical path:**
  1. Observe $R_{t+1}$
  2. Update global quantiles $\theta$ (fast update)
  3. Recalculate $\bar{R}$ from updated quantiles
  4. Calculate centered reward $R_{t+1} - \bar{R}$
  5. Compute TD error using centered reward
  6. Update Value/Return Network (slow update)

- **Design tradeoffs:**
  - **D2 (Scalable):** Only requires O(m) parameters for the distributional component. Best for tabular or large-scale settings where memory is constrained.
  - **D3 (Rich):** Requires O(|S||A|m) parameters for the return distribution. Better for risk-sensitive tasks but computationally heavier.

- **Failure signatures:**
  - **Diverging Q-values:** Likely caused by $\bar{R}$ being too small, leading to positive feedback loops in the TD error.
  - **Oscillating $\bar{R}$:** Quantile step size is too small, or environment rewards are unbounded.
  - **Flat Learning Curves:** Quantile step size is too large compared to Q-step size, violating the two-timescale assumption.

- **First 3 experiments:**
  1. **Red-Pill Blue-Pill (Validation):** Implement the tabular D2 algorithm. Verify that the quantiles $\theta_i$ visually converge to the histogram of the optimal policy's rewards.
  2. **Inverted Pendulum (Control):** Use D2 Actor-Critic. Check if the quantiles converge to $\approx 0$ (the optimal average reward for balancing) and if the policy learns to stabilize the pendulum.
  3. **Atari (Scaling):** Implement D2 Q-learning with a Replay Buffer. Compare performance against the Differential Q-learning baseline to verify the "rich information" claim does not degrade scalability.

## Open Questions the Paper Calls Out

1. **Can the theoretical convergence guarantees established for D2 be extended to the Double Differential Distributional (D3) algorithms?**
   - **Basis in paper:** [explicit] The authors state in Section 6: "Future work should... tackle the theoretical aspects of the D3 algorithms."
   - **Why unresolved:** The paper provides formal convergence proofs for D2 in the tabular setting, but D3 is presented largely from an empirical perspective without accompanying theoretical analysis.
   - **What evidence would resolve it:** A rigorous mathematical proof demonstrating that the quantile estimates in D3 converge almost surely to the differential return distribution and limiting per-step reward distribution.

2. **Can a categorical distributional approach be effectively adapted for the average-reward setting to learn the limiting per-step reward distribution?**
   - **Basis in paper:** [explicit] The authors explicitly note as a limitation: "it remains to be seen how a categorical approach could be employed instead of a quantile-based approach."
   - **Why unresolved:** The current work relies exclusively on quantile regression, and the applicability of categorical methods to the specific structure of average-reward MDPs is unexplored.
   - **What evidence would resolve it:** The derivation of a categorical distributional algorithm for average-reward RL that demonstrates successful learning of the distributional objective.

3. **How can specific aspects of the limiting per-step reward distribution, such as variance or tail risks, be optimized rather than just the mean average-reward?**
   - **Basis in paper:** [explicit] The authors state: "it remains to be seen how different aspects (beyond the mean) of the chosen distributional objective can be optimized."
   - **Why unresolved:** The proposed D2 algorithms focus on optimizing the mean (average-reward) via standard value-function updates, using the distribution primarily for estimation, leaving risk-sensitive control unaddressed.
   - **What evidence would resolve it:** A modified control algorithm with an action selection rule that explicitly maximizes or minimizes specific distributional statistics (e.g., specific quantiles or variance).

## Limitations

- Theoretical convergence guarantees only proven for D2 in tabular settings, not for D3 in control
- Performance advantage of D3 over D2 is empirical only, without theoretical justification
- Requires careful tuning of two-timescale step size ratio to ensure convergence

## Confidence

- **Mechanism 1 (Quantile averaging for average reward):** High - well-supported by theoretical analysis and explicit proofs
- **Mechanism 2 (Two-timescale convergence):** High - rigorous mathematical proof provided in Appendix B
- **Mechanism 3 (D3 empirical advantage):** Low - performance gains shown empirically but lack theoretical justification and ablation studies

## Next Checks

1. Verify two-timescale convergence by monitoring quantile variance early in training vs. Q-value updates for tabular D2
2. Implement D3 with only the average-reward component disabled to isolate the differential return learning effect
3. Test D2/D3 with varying quantile step size ratios (η_θ) to determine sensitivity to Assumption B.2.5