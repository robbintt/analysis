---
ver: rpa2
title: Comparison of Large Language Models for Deployment Requirements
arxiv_id: '2508.00185'
source_url: https://arxiv.org/abs/2508.00185
tags:
- llms
- arxiv
- llama-2
- language
- list
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comparative list of open-source Large Language
  Models (LLMs) to aid researchers and companies in selecting appropriate models for
  local deployment based on licensing and hardware requirements. The authors compiled
  a list of 108 LLMs, including foundational and domain-specific models, focusing
  on features such as release year, licensing, and minimum hardware requirements for
  fine-tuning and inference.
---

# Comparison of Large Language Models for Deployment Requirements

## Quick Facts
- **arXiv ID:** 2508.00185
- **Source URL:** https://arxiv.org/abs/2508.00185
- **Reference count:** 21
- **Primary result:** A comparative list of 108 open-source LLMs with hardware and licensing requirements to aid deployment decisions.

## Executive Summary
This paper presents a comparative list of 108 open-source Large Language Models (LLMs) to help researchers and companies select appropriate models for local deployment based on licensing and hardware requirements. The authors compiled foundational and domain-specific models, focusing on features such as release year, licensing, and minimum hardware requirements for fine-tuning and inference. The list is published on GitLab and will be updated regularly. The analysis revealed that most models were released in 2023, with sizes ranging from 1.1 billion to 314 billion parameters, and approximately 51% have permissive licenses allowing commercial use without restrictions.

## Method Summary
The authors manually curated and aggregated metadata from 108 open-source LLMs sourced from HuggingFace and developer documentation, focusing on models released in or after 2023. They extracted information on parameter counts, license types, release years, and minimum hardware requirements (RAM, vRAM, disk space) for both fine-tuning and inference. Hardware requirements are specifically listed for 5-bit quantized versions of models. No model training or benchmarking was performed; the work focuses on metadata compilation and organization.

## Key Results
- The list includes 108 LLMs, with most released in 2023 and parameter sizes ranging from 1.1 billion to 314 billion.
- Approximately 51% of models have permissive licenses allowing commercial use without restrictions.
- The most common model size is 7 billion parameters, establishing a baseline for hardware planning.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constraining model selection based on explicit hardware requirements (vRAM/RAM) allows for feasible local deployment of LLMs on limited infrastructure.
- Mechanism: The authors aggregate minimum memory requirements for loading 5-bit quantized versions of models. By mapping these static resource constraints against available hardware, engineers can filter out models that would otherwise result in Out-Of-Memory (OOM) errors during loading or inference.
- Core assumption: The listed hardware requirements for 5-bit quantization are accurate baselines and the user accepts the potential performance trade-offs of quantization.
- Evidence anchors:
  - [section] Section III.B states: "we included information on minimum memory requirements (RAM and vRAM)... applicable for loading the 5-bit quantized versions."
  - [corpus] Paper 75339 supports the need for this mechanism by discussing the complexity of resource utilization in inference serving systems.
- Break condition: If a user attempts to load full-precision weights (FP16/FP32) based on these 5-bit numbers without scaling up (2x-4x), the deployment will fail.

### Mechanism 2
- Claim: Pre-classifying licenses by commercial utility reduces legal friction during the initial selection phase.
- Mechanism: The paper preprocesses complex legal text (e.g., Apache 2.0 vs. LLaMA Community License) into a simplified "Commercial Usage" status (Yes/No/Partial). This acts as a binary filter, preventing organizations from investing engineering resources into models they cannot legally deploy in production.
- Core assumption: The simplified license categorization holds true for the specific jurisdiction and specific commercial use case of the user.
- Evidence anchors:
  - [abstract] The abstract notes that the list focuses on "licensing... to aid researchers and companies."
  - [section] Table II and Section IV break down licenses into categories like "Permissive" and "Partial," explicitly noting restrictions such as the 700M monthly user cap.
  - [corpus] Paper 76533 (Strategic Decision Framework) reinforces the necessity of this by citing "critical challenges" in adoption, though it does not validate the specific legal interpretations of this paper.
- Break condition: If a company's legal department interprets a "Partial" license (like LLaMA-3) differently than the authors, the mechanism fails to prevent legal risk.

### Mechanism 3
- Claim: Grouping models by parameter count and family enables efficient trade-off analysis between computational cost and capability.
- Mechanism: By listing parameter sizes (1.1B to 314B) alongside model families, the list allows strategists to estimate the capability tier and inference latency relative to cost. For example, identifying that 7B models are the most common (mode) suggests a standard "entry-level" performance baseline.
- Core assumption: Parameter count serves as a reliable proxy for both hardware cost and model intelligence/capability.
- Evidence anchors:
  - [section] Section IV notes: "most of our listed LLMs have 7 billion parameters," establishing a density baseline for hardware planning.
  - [corpus] Paper 43740 (GeLaCo) discusses layer compression, implying that raw parameter count is a fluid metric, which complicates this mechanism.
- Break condition: If architectural efficiency varies wildly (e.g., a highly efficient 70B model outperforms a poorly optimized 70B model on the same hardware), simple parameter counting becomes a misleading proxy for hardware requirements.

## Foundational Learning

- Concept: **Quantization**
  - Why needed here: The hardware requirements in the paper are explicitly calculated for 5-bit quantization, not standard floating point (16/32-bit).
  - Quick check question: If a GPU has 24GB VRAM, can it run a 13B parameter model at 5-bit quantization? (Hint: Check Table I requirements for 13B models).

- Concept: **Mixture of Experts (MoE)**
  - Why needed here: The paper lists MoE models (e.g., Mixtral 8x7B) separately because their parameter count represents total weights, but active parameters during inference are lower, altering the performance/hardware ratio.
  - Quick check question: Does a 47B MoE model require the same VRAM as a 47B dense model? (Assumption: The paper lists hardware reqs, but conceptually, MoE is distinct).

- Concept: **Commercial Licensing Restrictions**
  - Why needed here: The paper distinguishes between "Permissive" (Apache/MIT) and "Partial" (e.g., LLaMA restrictions >700M users).
  - Quick check question: Why might a startup prefer an Apache 2.0 model over a "Partially" commercial model even if the latter performs slightly better?

## Architecture Onboarding

- Component map:
  - Source Data: HuggingFace / Developer documentation (Model weights, License text)
  - Processing: Manual/Scripted extraction of metadata (Params, License type, Release Year)
  - Calculation: Estimation of Min. GB RAM/vRAM based on 5-bit quantization formulas
  - Output: GitLab Comparison Table (Snapshot in Table I)

- Critical path:
  1. Define Use Case (Commercial vs. Research)
  2. Apply License Filter (Step 2 in Mechanism 2)
  3. Apply Hardware Filter (Step 1 in Mechanism 1) based on available GPU VRAM
  4. Select optimal model from remaining set

- Design tradeoffs:
  - Precision vs. Hardware: The paper defaults to 5-bit requirements. Using FP16 requires ~3-4x the listed VRAM.
  - Breadth vs. Depth: The list excludes many fine-tuned derivatives to maintain manageability, potentially missing niche domain optimizations.

- Failure signatures:
  - OOM on Load: The user ignored the "Min. GB GPU" column or tried to load full precision weights.
  - Legal Stop-Ship: The user relied on "Open Source" label without reading the "Partial" commercial usage restriction.

- First 3 experiments:
  1. Verification of Baseline: Load a 7B model (e.g., Zephyr or LLaMA-2) using 5-bit quantization (e.g., bitsandbytes) and verify if VRAM usage matches Table I.
  2. Inference vs. Fine-tuning delta: Compare the VRAM usage of the Inference column vs. Fine-tuning column for a single model to understand the overhead of gradients/optimizer states.
  3. License Audit: Pick one "Permissive" and one "Partial" license model, read the actual license file, and confirm the paper's classification aligns with the license text.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the hardware requirements reported by developers be empirically verified through local deployment testing?
- Basis in paper: [explicit] The authors state that verifying the requirements provided by HuggingFace and developers "will be verified as part of the future work."
- Why unresolved: The current data relies solely on unverified metadata, risking discrepancies between reported and actual hardware usage.
- What evidence would resolve it: Benchmark results showing measured RAM/vRAM consumption during inference and fine-tuning compared to the listed specifications.

### Open Question 2
- Question: What practical advantages and disadvantages of specific deployments can be identified via user feedback?
- Basis in paper: [explicit] Future work involves assessing "user feedback and highlight[ing] the advantages and disadvantages of the recommended deployments."
- Why unresolved: The paper currently focuses on technical feasibility (hardware/license) rather than experiential deployment factors.
- What evidence would resolve it: Qualitative analysis or surveys of developers utilizing the list for actual model deployment.

### Open Question 3
- Question: How does expanding the list to include more domain-specific models enhance selection for specialized applications?
- Basis in paper: [explicit] The authors plan to "include more domain-specific models to list the LLM options for different applications."
- Why unresolved: It is currently unclear if the inventory of foundational models is sufficient or if specific domains are under-served.
- What evidence would resolve it: A comparative study of selection success rates for specialized tasks using the expanded list versus the foundational version.

## Limitations
- The hardware requirements are estimates without disclosed calculation methods, creating uncertainty about their accuracy.
- The license classification simplifies complex legal texts and may not capture jurisdiction-specific nuances.
- The exclusion of fine-tuned derivatives limits practical applicability for specialized use cases.

## Confidence
- **High Confidence**: The claim that most models were released in 2023 (52 of 108) is verifiable through public records. The observation that 51% of models have permissive licenses allowing commercial use is directly supported by the dataset in Table II.
- **Medium Confidence**: The assertion that parameter count correlates with hardware requirements is reasonable given industry conventions, though it may not hold for all architectures (e.g., MoE models). The license categorization methodology is transparent but may not capture all legal subtleties.
- **Low Confidence**: The minimum hardware requirements are estimated without disclosed calculation methods, making them unreliable for precise deployment planning without independent verification.

## Next Checks
1. **Hardware Requirement Validation**: Select 5 models across different parameter ranges (e.g., 1.1B, 7B, 13B, 70B, 176B) and measure actual RAM/vRAM usage when loading 5-bit quantized versions. Compare measured values against Table I to quantify estimation error.
2. **License Classification Audit**: Pick 3 models from each license category ("Permissive," "Partial," "No Commercial Use") and read their full license texts. Document any discrepancies between the paper's classification and the actual license terms.
3. **Update Mechanism Assessment**: Track the GitLab repository for one month to determine if new models are added, existing entries are updated, and whether the update process is automated or manual. Assess the potential lag between model releases and their inclusion in the list.