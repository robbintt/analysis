---
ver: rpa2
title: 'Generative Bayesian Optimization: Generative Models as Acquisition Functions'
arxiv_id: '2510.25240'
source_url: https://arxiv.org/abs/2510.25240
tags:
- optimization
- generative
- which
- learning
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Generative Bayesian Optimization (GenBO), a
  framework that uses generative models as acquisition functions in Bayesian optimization
  without requiring intermediate surrogate models. The method trains generative models
  directly from noisy utility values using either preference-based losses (like robust
  DPO) or divergence-based losses (forward/balanced KL), avoiding the two-stage approach
  common in prior work.
---

# Generative Bayesian Optimization: Generative Models as Acquisition Functions

## Quick Facts
- arXiv ID: 2510.25240
- Source URL: https://arxiv.org/abs/2510.25240
- Reference count: 40
- Primary result: Generative Bayesian Optimization (GenBO) achieves competitive performance on text optimization and protein design tasks with 3x speedup by using generative models directly as acquisition functions

## Executive Summary
This paper introduces Generative Bayesian Optimization (GenBO), a framework that trains generative models directly from noisy utility values to serve as acquisition functions in Bayesian optimization. Unlike traditional approaches that use surrogate models to approximate the objective before optimizing an acquisition function, GenBO eliminates the intermediate surrogate by directly learning a distribution over inputs that maximizes expected utility. The method employs either preference-based losses (robust DPO) or divergence-based losses (forward/balanced KL) to train the generative model, converging to the optimal acquisition function distribution under standard regularity conditions.

## Method Summary
GenBO trains generative models to directly approximate acquisition functions by minimizing a loss that combines regularization with utility-weighted objectives. The framework uses either robust preference losses (rPL) that handle noisy pairwise comparisons or divergence-based losses (fKL/bfKL) with importance weighting. Each optimization round involves training the generative model on all accumulated data, sampling a batch from the learned distribution, evaluating utilities, and updating the dataset. The method avoids the two-stage surrogate model approach common in Bayesian optimization, instead learning the acquisition distribution directly through either preference comparisons or utility-weighted divergence minimization.

## Key Results
- GenBO variants achieve competitive or superior performance compared to baselines like VSD and CbAS
- 3x speedup over traditional Bayesian optimization approaches due to single-model training
- Strong performance on high-dimensional combinatorial optimization, particularly in solvent accessible surface area (SASA) optimization of proteins
- Demonstrated effectiveness on text optimization (ALOHA edit distance) and protein design tasks

## Why This Works (Mechanism)

### Mechanism 1: Direct Utility-to-Distribution Mapping via Preference Loss
Training generative models with robust pairwise preference losses directly approximates the target acquisition function distribution without intermediate surrogates. The robust preference loss modifies DPO by accounting for preference noise probability p_flip, producing unbiased gradients even when observation noise flips preference direction. The learned distribution converges to p*_u(x) ∝ p_0(x)exp(E[u(y)|x]/β).

### Mechanism 2: Divergence Minimization with Importance-Weighted Samples
Forward KL divergence minimization using importance weights p_0(x_i)/q_{i-1}(x_i) · u(y_i) produces unbiased estimates of the target acquisition distribution. Samples with high utility but low proposal probability get upweighted, pushing the generative model toward high-utility regions. Balanced forward KL adds q(x_i)/q_{i-1}(x_i) term to penalize zero-utility regions that would otherwise retain mass.

### Mechanism 3: Posterior Concentration Through Reward-Weighted Regression Dynamics
The sequence of learned distributions asymptotically concentrates at global optima under standard regularity conditions (strong convexity, sub-Gaussian losses, RKHS assumptions). The approximation error is bounded by terms that decrease logarithmically with data, similar to GP posterior variance, ensuring monotonic improvement in expected utilities.

## Foundational Learning

- **Bayesian Optimization with Acquisition Functions**: Understanding what acquisition functions encode (expected utility, exploration-exploitation tradeoff) is essential for selecting appropriate utilities. Quick check: Can you explain why Expected Improvement balances exploration and exploitation differently than Probability of Improvement?

- **Direct Preference Optimization (DPO)**: GenBO adapts DPO's reward-model-free insight to BO. Understanding how DPO eliminates explicit reward models via the Bradley-Terry reparameterization clarifies why GenBO can skip surrogate training. Quick check: In DPO, why does the partition function cancel out in the pairwise preference loss but not in single-sample losses?

- **KL Divergence and Importance Sampling**: Forward KL minimization with importance weights is GenBO's alternative to preference-based training. Understanding why forward KL is mode-seeking vs. reverse KL being mode-covering explains when each variant is appropriate. Quick check: Why does forward KL divergence require samples from the target distribution, and how does importance sampling address this when target samples are unavailable?

## Architecture Onboarding

- **Component map**: Input: Prior p_0, initial data D_0 -> Loop: Loss Computation (Preference-based: rPL / Divergence-based: fKL or bfKL) -> Generative Model Training (Minimize L_t(q)) -> Batch Sampling (B samples i.i.d. from q_t) -> Evaluation & Update (Observe y_t,i, compute utilities u_t(y_t,i), append to D_t) -> Output: Best observed point

- **Critical path**: The loss computation is the architectural innovation. For rPL, you must construct pairs (x_i,1, x_i,2) with utility differences. For fKL/bfKL, you must compute importance weights p_0(x)/q_{i-1}(x). Errors here cascade to wrong gradients.

- **Design tradeoffs**:
  - **rPL vs. fKL**: rPL discards utility magnitude (only uses sign), robust to noise but less sample-efficient. fKL uses full utility information but requires stable importance weights.
  - **With vs. without importance weights**: Dropping 1/q_{i-1}(x_i) weights promotes posterior concentration (faster convergence) but may increase variance.
  - **Regularization strength λ_t**: Controls exploration. Too high → stuck near prior; too low → premature convergence.
  - **Threshold τ_t schedule**: For PI/EI utilities, τ_t determines exploration-exploitation. Anneals from median to 99th percentile.

- **Failure signatures**:
  - **Mode collapse**: All batch samples identical → q degenerate, importance weights explode. Symptom: NaN losses in fKL variants.
  - **Stagnation**: Utilities plateau below optimum → insufficient exploration. Check: Is λ_t annealing too fast? Is τ_t threshold too aggressive?
  - **Noisy preference flip**: rPL with wrong p_flip estimate → biased gradients. Symptom: Performance degrades when observation noise is high.
  - **Prior mismatch**: Informative p_0 mis-specified → biases search away from optimum.

- **First 3 experiments**:
  1. **ALOHA toy task (short sequence optimization)**: Use mean-field categorical model; compare rPL vs. fKL with EI utility; verify 3x speedup vs. VSD baseline.
  2. **Ablation on importance weights**: Run fKL with and without 1/q_{i-1}(x_i) weights on Ehrlich-15; measure simple regret trajectory and gradient variance.
  3. **Batch size scaling**: Test B ∈ {8, 32, 64, 128} on Ehrlich-32; verify monotonic improvement with larger B and identify compute/quality tradeoff.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the GenBO framework be adapted to handle acquisition strategies not expressible as expected utilities, such as Thompson sampling or Upper Confidence Bound (UCB)? The current formulation relies on training the generative model to approximate a density proportional to the expected utility. A theoretical derivation extending the loss functions to approximate posterior sampling, accompanied by empirical results, would resolve this.

- **Open Question 2**: How can the temperature parameter β and utility scaling be theoretically optimized or adapted online to improve convergence rates? While the paper demonstrates empirical success with specific settings, there lacks a theoretical guide on how to set β or how to scale utilities to ensure the generative model neither collapses too quickly nor fails to concentrate.

- **Open Question 3**: To what extent does the requirement to fix the prior p_0 before optimization hinder performance in tasks requiring intense local exploitation? It is unclear if the poor stability performance is due to the generative model architecture, the loss function, or the constraint of the fixed prior preventing the "collapse" necessary for fine-grained local optimization.

## Limitations

- Performance depends on sensible settings of utility and temperature parameters, whose theory could be further explored
- The method may underperform on tasks requiring pure exploitation from around the starting dataset due to the fixed prior constraint
- Requires knowledge or estimation of preference noise probability p_flip for robust preference loss to work effectively

## Confidence

- **High**: The mechanism of direct generative modeling replacing surrogate-based acquisition is sound and well-demonstrated across multiple tasks.
- **Medium**: Theoretical convergence analysis provides useful bounds, though real-world conditions may violate assumptions about RKHS membership and utility alignment.
- **Medium**: Empirical results show competitive performance, but comparisons against established baselines like VSD are limited in scope and don't fully explore hyperparameter sensitivity.

## Next Checks

1. **Noise robustness verification**: Systematically test rPL performance across varying levels of observation noise with both correct and incorrect p_flip estimates to validate the robustness claims and identify failure thresholds.

2. **Scalability assessment**: Evaluate GenBO's performance on high-dimensional combinatorial optimization problems (e.g., protein design with >100 amino acids) to verify if the 3x speedup and competitive accuracy claims hold as dimensionality increases.

3. **Utility alignment analysis**: Conduct controlled experiments where the utility function (e.g., EI, PI) is deliberately misaligned with the true objective to test the convergence guarantee's dependence on utility-objective alignment and identify when GenBO converges to suboptimal solutions.