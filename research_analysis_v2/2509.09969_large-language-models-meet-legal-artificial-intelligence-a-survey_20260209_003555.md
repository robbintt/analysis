---
ver: rpa2
title: 'Large Language Models Meet Legal Artificial Intelligence: A Survey'
arxiv_id: '2509.09969'
source_url: https://arxiv.org/abs/2509.09969
tags:
- legal
- llms
- language
- datasets
- case
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a comprehensive survey of large language models
  (LLMs) in legal artificial intelligence (Legal AI), covering 16 legal LLM series,
  47 LLM-based frameworks, 15 benchmarks, and 29 datasets. It systematically analyzes
  datasets (pre-training, fine-tuning, benchmarks, and traditional), and categorizes
  methods into legal LLMs and task-specific LLM-based frameworks across six main legal
  tasks: information extraction, judgment prediction, question answering, reasoning,
  retrieval, and summarization.'
---

# Large Language Models Meet Legal Artificial Intelligence: A Survey

## Quick Facts
- **arXiv ID:** 2509.09969
- **Source URL:** https://arxiv.org/abs/2509.09969
- **Reference count:** 40
- **Primary result:** Comprehensive survey covering 16 legal LLM series, 47 LLM-based frameworks, 15 benchmarks, and 29 datasets, analyzing six main legal tasks and identifying key challenges and future directions.

## Executive Summary
This survey systematically examines the intersection of large language models (LLMs) and legal artificial intelligence (Legal AI), providing a comprehensive overview of 16 legal LLM series and 47 LLM-based frameworks. The paper categorizes methods across six main legal tasks: information extraction, judgment prediction, question answering, reasoning, retrieval, and summarization. It identifies critical challenges including data quality issues, bias amplification, multimodality limitations, multilingual constraints, hallucination, and interpretability problems. The survey proposes future research directions focusing on improving dataset synthesis, enhancing interpretability, addressing multimodal and multilingual needs, tackling complex legal situations, and developing smaller yet capable models for cost-effective deployment.

## Method Summary
The survey employs a systematic literature review methodology, analyzing publications from top-tier conferences (ACL, EMNLP, AAAI, IJCAI, SIGIR, WWW) and journals (TKDE, TNNLS, TACL). The authors collected and categorized 16 legal LLM series and 47 LLM-based frameworks, organizing them based on their development approaches and task categories. The survey also compiled 15 benchmarks and 29 datasets, systematically classifying them into pre-training, SFT, benchmark, and traditional categories. The analysis framework covers the entire pipeline from data preparation through model development to evaluation, with particular attention to the trade-offs between fine-tuning approaches and framework-based methods.

## Key Results
- Legal LLMs require continual pre-training and SFT on specialized legal corpora, with catastrophic forgetting being a significant challenge when adapting general LLMs to legal domains
- Synthetic data generation through models like GPT-4 is essential for creating SFT datasets but lacks standardized quality metrics and verification methods
- The survey identifies six primary legal tasks with information extraction and judgment prediction being the most developed, while multimodal and multilingual capabilities remain largely unexplored

## Why This Works (Mechanism)
Legal LLMs work by adapting general-purpose foundation models through domain-specific pre-training and fine-tuning on legal corpora, enabling them to understand specialized legal terminology and reasoning patterns. The effectiveness stems from leveraging legal syllogism (IRAC framework) for structured reasoning and using synthetic data generation to overcome the scarcity of human-annotated legal instruction-response pairs. Framework-based approaches enhance base LLMs with external modules like RAG systems and structured prompting templates, providing interpretability and flexibility without the cost of full fine-tuning. The combination of legal domain knowledge with LLMs' strong language understanding capabilities enables performance on complex tasks like judgment prediction and legal reasoning that were previously challenging for traditional NLP models.

## Foundational Learning

- **Concept: Catastrophic Forgetting**
  - **Why needed here:** The paper notes that continual pre-training of general LLMs on legal data can cause them to lose their original, general-purpose capabilities. This is a critical trade-off in domain adaptation.
  - **Quick check question:** If you fine-tune a general-purpose LLM solely on legal case documents, what is the risk to its ability to summarize a non-legal news article?

- **Concept: Legal Syllogism (IRAC Framework)**
  - **Why needed here:** This is a core legal reasoning structure (Issue, Rule, Application, Conclusion) that is explicitly used in LLM-based frameworks. It's a method to prompt LLMs for structured, step-by-step legal reasoning.
  - **Quick check question:** Name the four components of the IRAC framework used to structure legal reasoning.

- **Concept: Data Synthesis (Self-Instruct)**
  - **Why needed here:** A primary method for creating SFT datasets is to use a powerful LLM (like GPT-4) to generate instruction-response pairs from a small seed set. Understanding this pipeline is essential for creating or evaluating legal LLM training data.
  - **Quick check question:** In the context of creating legal SFT datasets, what is the role of a model like GPT-4 in the data synthesis process?

## Architecture Onboarding

- **Component map:** Legal Corpus Preparation -> Continual Pre-training -> Supervised Fine-Tuning (SFT) with Synthetic Data -> Evaluation on Legal Benchmarks
- **Critical path:** The development of a performant legal LLM follows the path: Legal Corpus Preparation -> Continual Pre-training -> Supervised Fine-Tuning (SFT) with Synthetic Data -> Evaluation on Legal Benchmarks. The most critical step for task-specific performance is creating high-quality SFT data, which is often the bottleneck due to the lack of human-annotated legal instructions.
- **Design tradeoffs:**
  - Fine-tuning (Legal LLM) vs. Frameworks (RAG/Prompting): Fine-tuning creates a model with deeper domain knowledge but is costly and carries a risk of catastrophic forgetting. Frameworks (like RAG) are more flexible and interpretable but depend heavily on the quality of the retrieval system and the base model's reasoning capabilities.
  - Data Quality vs. Quantity: The paper highlights that simply using large amounts of data is insufficient if the data is "non-standard" or contains errors. Synthetic SFT data is easy to generate but hard to evaluate for quality and alignment with human feedback.
- **Failure signatures:**
  - Hallucination: Generating plausible-sounding but legally incorrect statutes or case law. This is a primary challenge mentioned in the paper.
  - Bias Amplification: Reflecting gender, racial, or case-type imbalances present in the training data.
  - Non-standard Document Parsing Errors: Failure to correctly segment legal case documents due to inconsistent use of indicative phrases, leading to incomplete or fragmented context for the model.
- **First 3 experiments:**
  1. Baseline Evaluation: Run a general-purpose LLM (e.g., LLaMA-2-7B) against a legal benchmark (like LawBench) to establish a zero-shot performance baseline. This quantifies the "legal knowledge gap."
  2. Continual Pre-training Ablation: Continue pre-training the baseline model on a clean, curated subset of the legal corpus (e.g., statutes and high-quality case law). Evaluate its performance drop on general NLP tasks to measure catastrophic forgetting.
  3. Synthetic SFT Test: Generate a small, high-quality SFT dataset by using a powerful LLM (like GPT-4) and explicitly providing it with reference statutes to ground its answers (as per the paper's suggestion). Fine-tune a model on this synthetic data and compare its performance on a few-shot legal QA task against a model fine-tuned on ungrounded synthetic data. This tests the impact of "grounded" data synthesis.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we establish quantifiable metrics to evaluate the quality of synthetic Supervised Fine-Tuning (SFT) datasets for legal LLMs?
- Basis in paper: [explicit] Section 4.1 states there are "no widely adopted quantifiable metrics" for synthetic SFT data, while Section 5 lists "evaluating their quality" as a future direction.
- Why unresolved: Current methods rely on general LLMs (e.g., ChatGPT) for synthesis without standard verification of alignment with human feedback or legal accuracy.
- What evidence would resolve it: A standardized metric that correlates synthetic data quality with downstream performance on legal benchmarks.

### Open Question 2
- Question: How can LLM-based frameworks effectively integrate multimodal data (audio, video) with text to handle real-world judicial evidence?
- Basis in paper: [explicit] Section 5 states future research should "explore how to combine multimodal data with text," as current approaches focus only on text.
- Why unresolved: Judicial practice involves diverse evidence types, but existing datasets and models almost exclusively focus on text modalities.
- What evidence would resolve it: A multimodal framework demonstrating superior performance on tasks involving non-textual evidence compared to text-only baselines.

### Open Question 3
- Question: Can smaller legal LLMs (e.g., 1.3B parameters) be developed to achieve performance comparable to larger models (6B-13B) for cost-effective deployment?
- Basis in paper: [explicit] Section 5 identifies "developing smaller LLMs with comparable performance" as a promising future direction to reduce deployment costs.
- Why unresolved: Existing legal LLMs typically range from 6B to 13B, which is computationally expensive for widespread judicial application.
- What evidence would resolve it: A sub-2B parameter model achieving benchmark scores (e.g., on LawBench) statistically indistinguishable from current 13B state-of-the-art models.

## Limitations

- The survey relies on published work which may over-represent positive results and under-represent failed approaches or negative findings
- The categorization of 47 LLM-based frameworks and 16 legal LLM series appears systematic but lacks quantitative performance comparisons between different approaches
- Confidence in the taxonomy is high for well-documented legal tasks but medium for emergent areas like multimodal legal reasoning where published work is sparse

## Confidence

- Claim that synthetic data generation is the "most critical bottleneck" for legal SFT datasets: **High** confidence based on analysis
- Claim that catastrophic forgetting is a significant challenge in legal LLM adaptation: **High** confidence based on survey findings
- Confidence in taxonomy completeness for multimodal and multilingual approaches: **Medium** due to limited published work in these areas

## Next Checks

1. Conduct a meta-analysis comparing the actual performance metrics of different legal LLM frameworks on standardized benchmarks to assess which approaches are truly effective
2. Systematically test catastrophic forgetting by fine-tuning a general LLM on legal data and measuring performance degradation on non-legal tasks
3. Evaluate the quality of synthetic SFT datasets by having human legal experts annotate a sample and compare against model-generated responses for legal accuracy and reasoning quality