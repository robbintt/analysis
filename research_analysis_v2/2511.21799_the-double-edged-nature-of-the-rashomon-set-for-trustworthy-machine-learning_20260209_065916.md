---
ver: rpa2
title: The Double-Edged Nature of the Rashomon Set for Trustworthy Machine Learning
arxiv_id: '2511.21799'
source_url: https://arxiv.org/abs/2511.21799
tags:
- rashomon
- adversarial
- trees
- privacy
- robustness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the dual nature of Rashomon sets in trustworthy
  machine learning. While diversity within these sets of near-optimal models enables
  reactive robustness by providing alternative models when one is attacked, it also
  increases information leakage when many models are disclosed.
---

# The Double-Edged Nature of the Rashomon Set for Trustworthy Machine Learning

## Quick Facts
- **arXiv ID**: 2511.21799
- **Source URL**: https://arxiv.org/abs/2511.21799
- **Reference count**: 40
- **Primary result**: Rashomon sets provide reactive robustness through model diversity but increase privacy leakage when multiple models are released.

## Executive Summary
This paper reveals that Rashomon sets—collections of near-optimal models—have a dual nature in trustworthy machine learning. While diversity within these sets enables reactive robustness by providing alternative models when one is attacked, it also increases information leakage when many models are disclosed. The authors show that sparse interpretable models are inherently vulnerable to adversarial attacks but naturally preserve privacy. Empirical results demonstrate a robustness-privacy trade-off: as more diverse models are released, adversarial accuracy improves while reconstruction error decreases. This finding motivates set-level governance rather than single-model approaches for deploying trustworthy machine learning systems.

## Method Summary
The authors analyze Rashomon sets using TreeFARMS to generate sparse decision tree ensembles and DRAFT reconstruction attacks to measure privacy leakage. They construct Rashomon sets with varying epsilon tolerances and regularization parameters, then characterize diversity through Hamming and angular distances. For robustness evaluation, they apply white-box evasion attacks and measure adversarial accuracy across the set. For privacy assessment, they incrementally release models using different selection strategies (closest, farthest, increment) and measure reconstruction error via DRAFT. The analysis spans six tabular datasets including Adult, Bank, Credit, Diabetes, COMPAS, and FICO, with binarization applied through threshold guessing.

## Key Results
- Diverse Rashomon sets enable reactive robustness: when attacks break one model, others often remain accurate
- Releasing multiple near-optimal models progressively reveals training data distribution
- Sparser individual models leak less information but ensembles can still leak due to structural diversity
- An intermediate regime exists where modest model release yields robustness gains with limited additional leakage

## Why This Works (Mechanism)

### Mechanism 1: Reactive Robustness via Diversity
- Claim: A diverse Rashomon set contains models that maintain accuracy under adversarial attacks targeting a specific model in the set.
- Mechanism: Models that differ in their prediction patterns or decision boundaries fail differently. When attack δ is optimized to fool model f̂, a model f with high prediction disagreement H(f, f̂) from the attacked model satisfies L_S'(f) ≤ H(f, f̂) + L_S'(f̂). The bound forces models similar to f̂ to inherit its vulnerability, while diverse models can escape it.
- Core assumption: Diversity measure aligns with attack type (e.g., Hamming distance for 0-1 loss, angular distance for L2 attacks on linear models).
- Evidence anchors:
  - [abstract] "diversity within a large Rashomon set enables reactive robustness: even when an attack breaks one model, others often remain accurate"
  - [Section 5.1, Equation 1] Triangle inequality bound linking adversarial loss to prediction disagreement
  - [Section 5.1, Corollary 2] "models that make a greater angle with the optimal model are more robust to adversarial attacks"
  - [corpus] Related work on SORTeD and DIVERSE confirms diversity as central to Rashomon set utility, though not for robustness specifically
- Break condition: If Rashomon set lacks angular/geometric diversity (e.g., only parallel boundary shifts), attack transfers; if attack strength η → ∞, all models eventually fail.

### Mechanism 2: Information Leakage via Aggregate Disclosure
- Claim: Releasing multiple near-optimal models progressively reveals the training data distribution.
- Mechanism: Each model f_i provides an estimate of P(y|x). The ensemble mean q_Π(x) converges toward the empirical distribution p(x) as more models are sampled. Theorem 6 bounds expected KL(p||q_Π) by [(p-μ)² + (N-m)σ²]/[(N-1)m·δ(1-δ)], which decreases as m → N.
- Core assumption: Models output probabilities in [δ, 1-δ]; sampling is uniform without replacement.
- Evidence anchors:
  - [abstract] "disclosing more near-optimal models provides an attacker with progressively richer views of the training data"
  - [Section 5.3, Theorem 6] KL divergence bound showing leakage grows with ensemble size
  - [Figure 4, Section 6.2] "Farthest" strategy (max diversity) yields lowest reconstruction error = highest leakage
  - [corpus] No direct corpus support for leakage mechanism; related papers focus on explanation/fairness, not privacy
- Break condition: If released models are near-identical (low variance σ²), ensemble adds little new information; if only membership privacy is the threat, Theorem 5 shows stability limits inference risk.

### Mechanism 3: Sparsity as Built-In Privacy Regularization
- Claim: Sparser individual models leak less information about training data.
- Mechanism: Mutual information I(f; S) = O(l_f log d) for decision trees with l_f leaves. Fewer leaves → lower entropy over possible outputs → less information captured about dataset.
- Core assumption: Dataset has binary features; model class is bounded-depth decision trees.
- Evidence anchors:
  - [Section 4.1, Theorem 1] "mutual information increases roughly linearly with the number of leaves"
  - [Section 4.1] Sparsity also improves generalization, bounding membership inference advantage
  - [Figure 13, Appendix D.1] Single sparsest tree has higher reconstruction error than densest tree
- Break condition: Aggregating many sparse models can still leak; sparsity alone does not guarantee set-level privacy.

## Foundational Learning

- Concept: **Rashomon Set Definition**
  - Why needed here: Entire paper analyzes properties at the set level, not single-model level. Must understand R(ε) = {f : obj(f) ≤ obj(f*) + ε}.
  - Quick check question: If ε = 0.01·obj(f*), what fraction of models with ≤1% worse performance are included?

- Concept: **Mutual Information and Privacy**
  - Why needed here: Theorem 1 links sparsity to I(f; S); Theorem 6 uses KL divergence to quantify leakage. Need intuition for information-theoretic privacy.
  - Quick check question: Why does lower I(f; S) imply stronger privacy protection?

- Concept: **Adversarial Transferability**
  - Why needed here: Core insight is that attacks transfer between similar models; diversity blocks transfer. Must understand why attacks designed for f̂ often fool similar f.
  - Quick check question: For linear models under L2 attack, why does angular distance determine transfer?

## Architecture Onboarding

- Component map:
  TreeFARMS -> Rashomon set construction -> Diversity quantification -> Adversarial attack module -> Privacy audit -> Selection strategies

- Critical path:
  1. Construct Rashomon set with tolerance ε and regularization λ
  2. Characterize diversity structure (distribution of pairwise Hamming distances)
  3. Given threat model (robustness vs. privacy), select subset balancing trade-off
  4. Validate via attack simulation and reconstruction error measurement

- Design tradeoffs:
  - Higher ε → larger, more diverse set → better reactive robustness but higher leakage potential
  - "Farthest" selection maximizes robustness but minimizes privacy
  - Sparse models: better individual privacy, but ensembles of sparse models can still leak due to structural diversity
  - Intermediate regime exists where modest model release yields robustness gains with limited additional leakage (Figure 5, 9)

- Failure signatures:
  - Robustness failure: All models in set have high adversarial error → diversity insufficient or misaligned with attack type
  - Privacy failure: Reconstruction error drops sharply after modest model release → set has high prediction variance
  - Stability failure: Rashomon set changes radically under small data perturbations → ε too tight or regularization too weak

- First 3 experiments:
  1. **Diversity-robustness correlation**: On a tabular dataset, construct Rashomon set, attack optimal tree, plot adversarial accuracy vs. Hamming distance to optimal. Confirm positive correlation.
  2. **Leakage scaling**: Sequentially release trees using "farthest" strategy, run DRAFT reconstruction after each release. Plot reconstruction error vs. number released. Compare to "closest" strategy.
  3. **Trade-off curve**: For varying ensemble sizes, report both best adversarial accuracy and reconstruction error. Identify "sweet spot" where robustness gains level off before leakage accelerates.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What factors determine the size and location of the "intermediate regime" where releasing models increases robustness without a sharp degradation in privacy?
- Basis in paper: [explicit] The authors observe this regime empirically and state that "Understanding what determines this region is a promising direction for future work" (Section 6.3).
- Why unresolved: The paper demonstrates the phenomenon across datasets but lacks a theoretical characterization of how Rashomon set geometry or data distribution dictates the width of this safe operating window.
- What evidence would resolve it: Theoretical analysis linking the variance of decision boundaries within the set to the slope of the reconstruction error curve.

### Open Question 2
- Question: How does the robustness–privacy trade-off manifest in complex, non-convex hypothesis spaces like deep neural networks?
- Basis in paper: [inferred] The theoretical and empirical analysis is restricted to sparse decision trees and linear models (Section 3).
- Why unresolved: The mechanism linking sparsity to privacy is well-defined for trees, but it is unclear if the same trade-offs apply to over-parameterized models where Rashomon sets are harder to define.
- What evidence would resolve it: Empirical studies measuring reconstruction error and adversarial transferability within ensembles of deep networks.

### Open Question 3
- Question: What are effective diversity-aware disclosure policies that allow institutions to leverage reactive robustness while formally limiting aggregate information leakage?
- Basis in paper: [explicit] The conclusion motivates "future work on diversity-aware disclosure policies and practical tools for auditing Rashomon sets" (Section 7).
- Why unresolved: The paper characterizes the risk but does not propose specific governance mechanisms or selection algorithms to optimize the trade-off in real-world deployments.
- What evidence would resolve it: Development of an algorithm that selects a subset of models from the Rashomon set that maximizes adversarial accuracy while bounding mutual information leakage.

## Limitations

- The empirical validation relies heavily on synthetic binarization and limited attack types
- Privacy analysis assumes uniform sampling without replacement, which may not hold for real-world model release scenarios
- Mutual information bounds depend on specific dataset properties (binary features, bounded depth) that may not generalize

## Confidence

- **High confidence**: The triangle inequality bound linking adversarial loss to prediction disagreement (Section 5.1, Theorem 1) - mathematically rigorous with clear geometric interpretation
- **Medium confidence**: The KL divergence bound for information leakage (Section 5.3, Theorem 6) - depends on sampling assumptions and bounded probability estimates that may not hold in practice
- **Medium confidence**: The sparsity-privacy relationship (Section 4.1, Theorem 1) - strong for decision trees on binary features but limited to specific model classes

## Next Checks

1. **Cross-dataset robustness validation**: Repeat the diversity-robustness correlation experiment on non-tabular datasets (images or text) with appropriate diversity metrics. This would test whether angular distance or Hamming distance generalize as effective diversity measures across domains.

2. **Attack diversity stress test**: Evaluate the robustness-privacy trade-off under different attack families (gradient-based, score-based, decision-based). This would verify whether the trade-off holds when attacks exploit different model vulnerabilities beyond the current L0-bounded attack.

3. **Real-world release simulation**: Implement a sequential model release protocol with practical constraints (e.g., budget limits, varying privacy preferences). Track both adversarial accuracy improvements and reconstruction error over time to identify optimal release strategies that balance both objectives.