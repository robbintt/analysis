---
ver: rpa2
title: Toward Enhancing Representation Learning in Federated Multi-Task Settings
arxiv_id: '2602.01626'
source_url: https://arxiv.org/abs/2602.01626
tags:
- loss
- users
- muscle
- learning
- representation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of federated multi-task learning
  (FMTL) where users have heterogeneous models and tasks. Traditional FMTL approaches
  rely on shared model parameters, limiting their applicability.
---

# Toward Enhancing Representation Learning in Federated Multi-Task Settings

## Quick Facts
- arXiv ID: 2602.01626
- Source URL: https://arxiv.org/abs/2602.01626
- Reference count: 40
- Key outcome: FedMuscle outperforms state-of-the-art baselines in federated multi-task learning by learning shared representations without requiring model parameter sharing.

## Executive Summary
This paper addresses the challenge of federated multi-task learning (FMTL) where users have heterogeneous models and tasks. Traditional FMTL approaches rely on shared model parameters, limiting their applicability. To overcome this, the authors propose FedMuscle, which focuses on learning a shared representation space instead of shared parameters. The key innovation is the Muscle loss, a novel contrastive learning objective that aligns representations from all participating models simultaneously, capturing dependencies across tasks more effectively than pairwise alignment methods.

## Method Summary
FedMuscle introduces Muscle loss, a novel contrastive learning objective that aligns representations from heterogeneous models in federated multi-task settings. Users maintain private data and local task-specific models while contributing representation matrices from a shared public dataset. The server aggregates these representations to compute alignment targets and weighting coefficients, which users use to update their representation models via contrastive learning. This approach enables knowledge transfer across diverse tasks and architectures without sharing model parameters or local data.

## Key Results
- FedMuscle consistently outperforms state-of-the-art baselines in heterogeneous FMTL settings
- Substantial performance improvements achieved across diverse computer vision and natural language processing tasks
- Robust results maintained across varying degrees of model and task heterogeneity
- Method integrates well into multi-modal FL algorithms, further enhancing their performance

## Why This Works (Mechanism)

### Mechanism 1: Systematic Contrastive Alignment (Muscle Loss)
- **Claim**: Simultaneously aligning representations from $N$ models captures cross-task dependencies more effectively than aggregating pairwise (2-model) alignments.
- **Mechanism**: Muscle loss extends standard contrastive learning by incorporating theoretically grounded weighting coefficients ($\alpha_j$) that modulate the loss based on similarity of negative representations among non-anchor models, explicitly prioritizing "hard" negatives that are dissimilar to each other.
- **Core assumption**: Dependencies between tasks are better represented by the joint distribution of all models' representations rather than the sum of marginal pairwise distributions.
- **Evidence anchors**:
  - [abstract] "Muscle loss... simultaneously aligns representations... capturing dependencies across tasks more effectively than pairwise alignment methods."
  - [section 4.2] Equation (3) defines $\alpha_j$ which scales based on the dissimilarity among negatives.
  - [corpus] Weak direct evidence for this specific loss; however, *Tackling Feature and Sample Heterogeneity* (2502.01145) highlights the difficulty of capturing complex task relationships in decentralized settings, validating the need for better alignment mechanisms.
- **Break condition**: If $\alpha_j$ terms are set to 1 (removing the systematic weighting), the loss theoretically reduces to pairwise alignment (Appendix B), collapsing the primary advantage.

### Mechanism 2: Representation Space Alignment via Public Anchors
- **Claim**: Knowledge transfer between heterogeneous models (different architectures/tasks) is achievable by aligning latent representations on a shared, unlabeled public dataset rather than sharing model weights.
- **Mechanism**: Users compute representations ($Z_n$) for a shared public dataset and upload them to the server. The server aggregates these into alignment targets (aggregated matrices $S_n$) and sends them back. Users then minimize the local contrastive loss to pull their representations closer to the global consensus without revealing local data or model weights.
- **Core assumption**: The shared public dataset contains feature-rich samples relevant enough to act as a "universal solvent" for aligning diverse task representations.
- **Evidence anchors**:
  - [abstract] "FedMuscle leverages Muscle loss to align representation spaces using a shared public dataset, without requiring model parameter sharing."
  - [section 5] Describes the server computing $S_n$ and $\alpha_n$ to enable local alignment.
  - [corpus] *Diffusion-based Decentralized FMTL* (2512.23161) similarly utilizes public/synthetic data for representation alignment, reinforcing this as a viable architecture for model-agnostic FL.
- **Break condition**: Performance degrades significantly if the public dataset domain is entirely orthogonal to local user tasks (observed in CIFAR-100 vs. Pascal VOC performance gaps).

### Mechanism 3: Mutual Information Maximization
- **Claim**: Minimizing the proposed Muscle loss is theoretically equivalent to maximizing a lower bound on the Mutual Information (MI) among all participating models.
- **Mechanism**: The loss function is derived by modeling the optimal discriminator for the joint probability density ratio of representations. By minimizing the contrastive loss, the system implicitly maximizes the dependency (MI) between the representation outputs of different users.
- **Core assumption**: The batch size is large enough for the expectation approximations in the MI derivation (Theorem 1) to hold.
- **Evidence anchors**:
  - [abstract] "minimization is equivalent to the maximization of mutual information among all the models' representations."
  - [section 4.2] Theorem 1 provides the inequality $I(z_n^i; \{z_m^i\}) \geq (N-1)\log(B) - \mathbb{E}L_{Muscle}$.
- **Break condition**: With very small batch sizes ($B$), the lower bound becomes loose, potentially decoupling the loss minimization from actual MI maximization.

## Foundational Learning
- **Concept**: **Contrastive Learning (InfoNCE)**
  - **Why needed here**: The core innovation (Muscle loss) is a modification of InfoNCE. You must understand how standard contrastive losses pull positives together and push negatives apart to see why simultaneous alignment differs from pairwise.
  - **Quick check question**: Can you explain why standard InfoNCE cannot inherently capture dependencies between three distinct vectors simultaneously?
- **Concept**: **Federated Multi-Task Learning (FMTL)**
  - **Why needed here**: The paper challenges the "model congruity" assumption of standard FMTL. You need to understand the baseline constraint that users typically must share the same model architecture.
  - **Quick check question**: Why does sharing model parameters (or gradients) fail when User A uses a Transformer and User B uses an CNN?
- **Concept**: **Mutual Information (MI)**
  - **Why needed here**: The theoretical justification for the "Muscle" weighting coefficients relies on MI. Understanding MI as a measure of dependency between variables is crucial for interpreting the loss design.
  - **Quick check question**: Does high Mutual Information imply high correlation? (Hint: No, correlation is linear).

## Architecture Onboarding
- **Component map**:
  1. Local Encoder ($w_n$): Feature extractor (e.g., ViT, BERT) mapping data to dimension $d$
  2. Task-Specific Head ($\phi_n$): Predictor for the local task (trained locally, not shared)
  3. Public Dataset ($D$): Unlabeled data shared by all, used only for alignment
  4. Aggregation Server: Computes aggregated representations ($S_n$) and weighting vectors ($\alpha_n$) using representations from $M$ selected users

- **Critical path**:
  1. Local Supervised Update: Users train $w_n$ and $\phi_n$ on private data $D_n$
  2. Representation Extraction: Users process public batch $D$, generating representation matrices $Z_n$
  3. Server Aggregation: Server receives $Z_n$ from $M$ users, computes $\alpha_n$ (hard negative weights) and $S_n$ (targets)
  4. Local Contrastive Update: Users update $w_n$ to minimize distance to $S_n$ weighted by $\alpha_n$

- **Design tradeoffs**:
  - Communication vs. Performance: Increasing the number of selected users ($M$) improves alignment (higher $\Delta$) but causes communication costs to explode exponentially ($B^M$). The paper suggests $M=3$ as a balance.
  - Public Data Quality: Using detailed images (Pascal VOC) yields better results than simpler datasets (CIFAR-100), but synthetic data can work as a fallback.

- **Failure signatures**:
  - Collapsed Representations: If temperature parameters ($\tau$) are too high or low, representations may not align or may collapse to uniformity
  - Negative Transfer: If tasks are semantically distant (e.g., Segmentation vs. Text Classification), standard alignment may hurt performance. (Mitigated in paper by increasing $\tau$ for low-relevance pairs)
  - Communication Bottleneck: Setting $M > 4$ or Batch Size $B > 64$ may cause downlink congestion

- **First 3 experiments**:
  1. Verify Public Data Necessity: Run FedMuscle with a synthetic public dataset vs. a real dataset (e.g., VOC) to quantify the domain sensitivity of the alignment
  2. Ablate Weighting Coefficients ($\alpha_j$): Run the algorithm with $\alpha_j=1$ (pairwise equivalent) vs. learned $\alpha_j$ to isolate the performance gain from "systematic" alignment
  3. Stress Test Heterogeneity: Combine NLP (BERT) and CV (ViT) models to ensure the projection head effectively maps different architectures into the shared $d$-dimensional space

## Open Questions the Paper Calls Out
- **Open Question 1**: Can a systematic approach replace the random selection of representation matrices to provide more task-relevant knowledge?
  - Basis in paper: [explicit] Appendix Q states, "Replacing this random selection with a systematic approach that provides more relevant knowledge based on each user's task... would be an interesting direction for future research."
  - Why unresolved: The current implementation relies on random selection ($M$ matrices) to balance communication costs and diversity, lacking a mechanism to selectively transfer knowledge based on task semantic relevance.
  - What evidence would resolve it: A study demonstrating a non-random selection strategy that improves average user performance $\Delta$ compared to the random baseline without increasing communication overhead.

- **Open Question 2**: What are the theoretical convergence guarantees for FedMuscle given the dual optimization of local task losses and the global contrastive loss?
  - Basis in paper: [explicit] Appendix Q notes, "The convergence of the Muscle loss and its generalization properties should be studied... Exploring these theoretical aspects would be an interesting direction for future work."
  - Why unresolved: The algorithm lacks a global model and minimizes two distinct losses (task-specific and contrastive), complicating standard federated convergence analysis.
  - What evidence would resolve it: A formal proof defining convergence bounds for the FedMuscle updates, potentially by linking the contrastive regularizer to established frameworks like Sheaf-FMTL as discussed in Appendix S.

- **Open Question 3**: Can a more robust overall performance metric be developed for FMTL that accounts for performance distribution across heterogeneous users?
  - Basis in paper: [explicit] Appendix Q argues that the current metric, average improvement ($\Delta$), "cannot fully capture the overall performance" because it may remain positive even if only one user improves while others degrade.
  - Why unresolved: Heterogeneous tasks utilize different metrics (e.g., Accuracy vs. mIoU), making it difficult to aggregate performance in a way that reflects fairness or worst-case scenarios.
  - What evidence would resolve it: The formulation of a new evaluation metric that penalizes scenarios where the benefits of collaboration are not distributed equitably across users.

## Limitations
- The computational complexity of computing $\alpha_j$ for all $|J_n|=B^M$ combinations grows exponentially with $M$, potentially limiting scalability beyond $M=3$
- Performance is highly sensitive to the domain relevance of the shared public dataset, with domain mismatches potentially causing negative transfer
- The effectiveness of the weighting coefficients $\alpha_j$ depends on hyperparameter settings, particularly the temperature hierarchy $\tau^{(N)}$

## Confidence
- **High Confidence**: The mechanism of representation space alignment using a shared public dataset (Mechanism 2) is well-supported by the paper's empirical results and corroborated by similar methods in the corpus (e.g., Diffusion-based Decentralized FMTL). The claim that FedMuscle outperforms baselines in heterogeneous settings is strongly evidenced by the ablation studies and comparison tables.
- **Medium Confidence**: The theoretical justification for the Muscle loss weighting coefficients (Mechanism 1) relies on assumptions about batch size and the MI lower bound. While the derivation is sound, the practical impact of these coefficients needs more extensive validation across diverse tasks and model architectures.
- **Low Confidence**: The claim that Muscle loss simultaneously aligns $N$ models more effectively than aggregating pairwise alignments (Mechanism 1) is primarily supported by intuition and indirect evidence. A direct comparison with a pairwise baseline using the same architecture would be needed for stronger validation.

## Next Checks
1. **Ablation Study on Weighting Coefficients**: Run FedMuscle with $\alpha_j=1$ (pairwise equivalent) vs. learned $\alpha_j$ to isolate the performance gain from "systematic" alignment and quantify its impact.
2. **Public Data Domain Sensitivity**: Compare FedMuscle performance using a public dataset from the same domain as local tasks vs. a domain-mismatched dataset (e.g., CIFAR-100 vs. Pascal VOC) to measure the alignment's sensitivity to data relevance.
3. **Communication Cost Analysis**: Measure the downlink communication cost for $M=4$ and $M=5$ to empirically verify the exponential growth claim and identify the practical scalability limit.