---
ver: rpa2
title: Towards a Neurosymbolic Reasoning System Grounded in Schematic Representations
arxiv_id: '2509.03644'
source_url: https://arxiv.org/abs/2509.03644
tags:
- spatial
- reasoning
- house
- answer
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Embodied-LM, a neurosymbolic system that grounds
  logical reasoning in image schemas - cognitive structures derived from sensorimotor
  experience. The system uses LLMs to interpret scenarios through spatial primitives
  and translates these into Answer Set Programming programs enhanced with declarative
  spatial reasoning.
---

# Towards a Neurosymbolic Reasoning System Grounded in Schematic Representations

## Quick Facts
- arXiv ID: 2509.03644
- Source URL: https://arxiv.org/abs/2509.03644
- Reference count: 24
- System achieves 91% accuracy on logical deduction problems

## Executive Summary
This paper presents Embodied-LM, a neurosymbolic system that grounds logical reasoning in image schemas - cognitive structures derived from sensorimotor experience. The system uses LLMs to interpret scenarios through spatial primitives and translates these into Answer Set Programming programs enhanced with declarative spatial reasoning. Tested on logical deduction problems, the system achieved 91% accuracy, comparable to state-of-the-art models, while maintaining interpretability through executable representations. The approach demonstrates that spatial cognitive structures can be formalized for systematic logical inference and provides a foundation for more complex reasoning scenarios.

## Method Summary
The system uses GPT-4 to interpret natural language scenarios through image schemas (PATH, CONTAINER, OBJECT) and generates Answer Set Programming (ASP) programs with spatial theory atoms. These programs are processed by Clingo enhanced with Declarative Spatial Reasoning (DSR) and Z3 solver integration. The system employs a two-stage prompting approach: first to identify the appropriate schema and generate ASP rules, then to answer specific questions. Spatial relations like `left_pp`, `in_pr`, and `overlap_rr` are defined through coordinate constraints, allowing geometric consistency checking. The system includes syntax error detection and satisfiability checking, with up to three regeneration attempts for problematic outputs.

## Key Results
- Achieves 91% accuracy on LogicalDeduction dataset
- Successfully solves zebra puzzles using CONTAINER schema
- Demonstrates systematic logical inference through spatial primitives
- Maintains interpretability through executable ASP representations

## Why This Works (Mechanism)

### Mechanism 1: Spatial Schemas Enable "Free Ride" Inference
Mapping abstract relationships to spatial configurations allows implicit logical relationships to emerge from geometric constraints without explicit computation. When entities are placed along PATH or within CONTAINER structures, spatial relations encode transitive constraints automatically - the geometry "computes" valid configurations. The core assumption is that abstract domains systematically map to spatial primitives that humans and systems can share.

### Mechanism 2: LLM-Guided Schema Identification with Symbolic Verification
Two-stage prompting uses LLMs to identify appropriate image schemas and generate syntactically correct ASP programs constrained by predefined spatial predicates. Syntax errors and unsatisfiability trigger regeneration (up to 3 attempts). The core assumption is that LLMs generalize from few-shot examples to correct schema mappings with recoverable errors.

### Mechanism 3: Theory Propagation Between ASP and Spatial Solver
Integration of ASP's logical reasoning with Z3's spatial constraint solving via custom propagator ensures only geometrically realizable solutions are returned. ASP rules determine which spatial relations must hold; Z3 tests whether coordinate assignments satisfy geometric definitions. The two-level architecture correctly separates concerns with efficient propagation.

## Foundational Learning

- **Answer Set Programming (ASP) fundamentals**: The entire symbolic reasoning layer uses Clingo ASP syntax; understanding rules, integrity constraints (`:-`), aggregates (`#count`), and stable model semantics is prerequisite. Quick check: Given rules `p :- not q.` and `q :- not p.`, how many stable models exist?

- **Image schemas from embodied cognition**: The system's core hypothesis is that PATH, CONTAINER, and other primitives ground abstract reasoning. Understanding Johnson/Lakoff's theory helps interpret why spatial predicates work. Quick check: What image schema might ground understanding "being in trouble"?

- **Qualitative Spatial Reasoning (QSR) / Declarative Spatial Reasoning (DSR)**: Spatial relations (`left_pp`, `in_pr`, `overlap_rr`) are defined via coordinate constraints. Understanding how geometric parameters translate to logical relations is essential for debugging. Quick check: How would you define "point P is inside rectangle R" using only coordinate inequalities?

## Architecture Onboarding

- **Component map:** Natural Language Scenario → GPT-4 (Prompt 1: Context → Schema + ASP rules) → ASP Program with Spatial Theory Atoms → GPT-4 (Prompt 2: Question/Options → answer/1 rules) → Clingo + Custom Propagator ↔ Z3 SMT Solver → Cautious Consequences → Answer(s) + Witness Coordinates

- **Critical path:** The schema identification step (which spatial primitive applies: PATH vs. CONTAINER) and the directionality consistency of mapping (e.g., "older" consistently maps to left vs. right). Errors here propagate through the entire pipeline.

- **Design tradeoffs:** Task-specific prompts vs. generalization (current prompts are "quite specific to the task"); predefined predicates vs. flexibility (fixed spatial predicates reduce LLM burden but limit expressiveness); regeneration vs. correction (authors observe GPT-4 produces better programs when "starting fresh rather than attempting to correct existing ones").

- **Failure signatures:** Directionality flip (LLM uses `left_pp(a,b)` for "a older than b" in one rule, `right_pp(a,b)` for same relation elsewhere → unsatisfiable or wrong answer); missing sort declarations (`point(X) :- item(X)` omitted → predicates fail to match); unconstrained positions (forgetting `:- samePlace_pp(_,_).` allows multiple entities at same location → spurious models).

- **First 3 experiments:**
  1. Reproduce LogicalDeduction results: Run provided code on 5-object ordering problems; inspect generated ASP programs to verify schema identification correctness.
  2. Ablate directionality enforcement: Modify prompt to explicitly require stating directionality convention upfront; measure whether error rate drops.
  3. Test schema misapplication: Feed a problem with no natural spatial analog (e.g., pure propositional logic); observe whether system forces inappropriate spatial mapping or fails gracefully.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the framework be extended to incorporate spatio-temporal and force-dynamic primitives to handle dynamic reasoning tasks?
- **Basis in paper:** The Conclusion states that while the current implementation focuses on spatial primitives, "future extensions that integrate spatio-temporal and force dynamic primitives could enable resolving classic AI problems such as Tower of Hanoi... or Blocks World scenarios."
- **Why unresolved:** The current proof-of-concept is restricted to static spatial primitives (e.g., OBJECT, PATH, CONTAINER) and lacks the formalization of time and force found in the theoretical cognitive model.
- **What evidence would resolve it:** A system extension that successfully formalizes MOTION or UMPH primitives and solves dynamic planning tasks like the River-Crossing puzzle.

### Open Question 2
- **Question:** Can the system generalize its schematic interpretation across diverse reasoning tasks without manual, problem-specific prompt engineering?
- **Basis in paper:** The Discussion acknowledges that "prompting remains quite specific to the task at hand" and suggests the design of a "problem-specific prompt selection function based on the identified image schemas."
- **Why unresolved:** The current architecture relies on manually crafted prompts to guide the LLM toward specific spatial formalizations, limiting its ability to autonomously adapt to novel problem types.
- **What evidence would resolve it:** A unified prompting mechanism that automatically detects the underlying image schema and generates correct ASP programs for both ordinal tasks (PATH) and containment tasks (CONTAINER) without human intervention.

### Open Question 3
- **Question:** How can the system ensure semantic consistency when mapping abstract natural language concepts to spatial relations?
- **Basis in paper:** The error analysis in the Discussion notes that the LLM "sometimes fails to conserve the same interpretation along the reasoning process," such as inconsistently mapping "newest" to left or right positioning.
- **Why unresolved:** There is currently no verification step to ensure the LLM maintains a coherent spatial metaphor throughout the generated symbolic program.
- **What evidence would resolve it:** A consistency-checking module or prompting strategy that eliminates directional contradiction errors in the LogicalDeduction dataset, improving accuracy beyond the reported 91%.

## Limitations
- Relies heavily on LLM interpretation for schema identification, introducing potential brittleness through directionality flips and semantic drift
- Current implementation is restricted to static spatial primitives, lacking formalization of temporal and force-dynamic concepts
- Prompt engineering remains task-specific, limiting generalization to diverse reasoning problems without manual intervention

## Confidence
- **High:** System architecture and implementation details are well-specified; reported accuracy of 91% on LogicalDeduction dataset is verifiable through provided code
- **Medium:** The claim that spatial schemas enable "free ride" inference is theoretically sound but not directly validated; LLM reliability for schema identification has mixed evidence
- **Low:** Generalizability to non-spatial domains and scalability to larger problems remains untested

## Next Checks
1. **Directionality consistency test:** Systematically inject known directional flips into prompts and measure error propagation through the pipeline
2. **Schema transferability validation:** Apply the same prompting framework to a purely propositional logic problem and document whether spatial forcing occurs
3. **Ablation of regeneration:** Disable the retry mechanism and measure degradation in accuracy to quantify LLM error frequency