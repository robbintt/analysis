---
ver: rpa2
title: Human-in-the-loop Online Rejection Sampling for Robotic Manipulation
arxiv_id: '2510.26406'
source_url: https://arxiv.org/abs/2510.26406
tags:
- learning
- training
- policy
- action
- real-world
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the instability of reinforcement learning
  (RL) when fine-tuning vision-language-action (VLA) models for robotic manipulation.
  The core challenge stems from inaccurate value estimation in high-dimensional action
  spaces and sparse supervision during intermediate inference steps.
---

# Human-in-the-loop Online Rejection Sampling for Robotic Manipulation

## Quick Facts
- **arXiv ID:** 2510.26406
- **Source URL:** https://arxiv.org/abs/2510.26406
- **Reference count:** 36
- **Primary result:** Hi-ORS fine-tunes a base policy to master contact-rich manipulation in just 1.5 hours of real-world training, outperforming RL and IL baselines by substantial margins.

## Executive Summary
This paper addresses the instability of reinforcement learning (RL) when fine-tuning vision-language-action (VLA) models for robotic manipulation. The core challenge stems from inaccurate value estimation in high-dimensional action spaces and sparse supervision during intermediate inference steps. The proposed solution, Human-in-the-loop Online Rejection Sampling (Hi-ORS), replaces learned value functions with outcome-based rejection sampling that filters out negatively rewarded trajectories, while providing dense supervision through reward-weighted training of intermediate inference steps. The method incorporates flexible online human corrections to guide error recovery behaviors. Across three real-world tasks and two robot embodiments, Hi-ORS fine-tunes a base policy to master contact-rich manipulation in just 1.5 hours of real-world training, outperforming RL and IL baselines by substantial margins.

## Method Summary
Hi-ORS fine-tunes a base VLA policy through outcome-based rejection sampling combined with reward-weighted supervised training. The method filters trajectories based on a cumulative reward threshold rather than relying on learned value estimates, which avoids overestimation bias in high-dimensional action spaces. For accepted trajectories, the system applies a reward-weighted flow matching loss across all intermediate inference steps, providing dense supervision that standard RL lacks. Human operators can intervene during failures, with these "near-miss" segments logged at high frequency. If the full trajectory eventually succeeds, these correction segments are retained in training. The system runs asynchronously with an actor node handling inference and execution while a learner node performs distributed training on accepted data.

## Key Results
- Hi-ORS achieves 80% success rate on real-world tasks versus 60% for learned reward models and RL baselines
- Fine-tuning completes in just 1.5 hours of real-world training time
- The method demonstrates strong test-time scalability through error recovery behaviors
- Hi-ORS outperforms both RL and imitation learning baselines on three contact-rich manipulation tasks

## Why This Works (Mechanism)

### Mechanism 1: Outcome-Based Value Estimation via Rejection Sampling
Replaces learned value functions with outcome-based rejection sampling to stabilize training in high-dimensional action spaces by eliminating gradient bias from overestimated Q-values. The system filters trajectories based on a cumulative reward threshold rather than relying on a neural network critic, ensuring the policy is updated exclusively using grounded, successful outcomes.

### Mechanism 2: Dense Intermediate Supervision
Applies a reward-weighted supervised loss across intermediate inference steps to provide dense learning signals that standard RL lacks. Instead of backpropagating only through the final action prediction, Hi-ORS optimizes the flow-matching objective across all integration times for accepted trajectories, supervising the internal vector field rather than just the output.

### Mechanism 3: Counterfactual Error Recovery Guidance
Human-in-the-loop interventions explicitly teach the policy error-recovery behaviors absent in offline datasets. Humans intervene during failure modes, and these correction segments are logged at high frequency. If the full trajectory succeeds, these segments are retained, teaching the policy to associate failure states with corrective actions.

## Foundational Learning

- **Concept: Action Chunking & High-Dimensional Action Spaces**
  - **Why needed here:** The paper identifies standard RL instability in VLAs specifically due to action chunking, which exponentially increases action space dimensionality and breaks standard Q-learning.
  - **Quick check question:** Can you explain why predicting a sequence of $N$ actions makes value estimation significantly harder than predicting a single step?

- **Concept: Flow Matching / Diffusion Policies**
  - **Why needed here:** The proposed "dense supervision" relies on the existence of "intermediate inference steps" (the ODE integration path in flow matching).
  - **Quick check question:** Does the policy output an action directly, or does it generate a vector field that must be integrated over time steps $u$?

- **Concept: Generalized Policy Iteration (GPI)**
  - **Why needed here:** Hi-ORS frames its pipeline as a variant of GPI (Evaluation Phase + Improvement Phase), but swaps the standard Critic for a Rejection Sampler.
  - **Quick check question:** In Hi-ORS, what component replaces the "Critic" in the standard Policy Evaluation step?

## Architecture Onboarding

- **Component map:** Async Actor Node -> Buffer -> Filter/Rejector -> Async Learner Node
- **Critical path:** 1. Rollout: Actor generates trajectory $\tau$. 2. Filter: Check if $R(\tau) \ge m$. 3. Distill: If accepted, add to training set $\mathcal{D}$. 4. Update: Train $\pi_\theta$ on $\mathcal{D}$ using reward-weighted flow matching loss. 5. Sync: Updated weights $\rightarrow$ Actor.
- **Design tradeoffs:**
  - Start with human-annotated binary rewards to ensure a clean signal before attempting to learn a reward model
  - The system runs with a natural UTD $\approx 1$, making it compute-efficient but data-selective
  - You must tune the schedule for $m$ (threshold); too low = noisy data, too high = empty buffer
- **Failure signatures:**
  - Oscillating Success Rate: Indicates RL instability or bad Q-values
  - Catastrophic Forgetting: If off-policy data mixture is not maintained, policy may diverge from base VLA capabilities
  - Stuck Behaviors: If no-op action filter is removed, buffer fills with zero-motion transitions, causing training to collapse
- **First 3 experiments:**
  1. Validate the Filter: Run the "Raise-Hand" task. Verify that acceptance rate decreases as threshold $m$ increases, confirming curriculum works.
  2. Ablate No-Op Filter: Intentionally disable the norm-based action filter to observe the "stuck" failure mode.
  3. Test Recovery: Trigger a deliberate failure (e.g., knock object out of gripper). Provide human correction. Verify trajectory is logged and model's subsequent success rate improves for that specific failure mode.

## Open Questions the Paper Calls Out

- **Can Hi-ORS maintain training stability and sample efficiency when scaled to multi-task or longer-horizon settings?** The current method relies on outcome-based rejection sampling, which may struggle with credit assignment over long horizons or task interference in multi-task learning without explicit value decomposition.

- **How can the acceptance threshold scheduler be improved to prevent bias toward high-variance outcomes in stochastic environments?** A simple progressive threshold might select trajectories that succeeded due to environmental stochasticity rather than policy robustness, potentially destabilizing the "golden" reward model assumption.

- **Can a learned reward model be trained to replace manual binary annotations without degrading policy performance?** The method currently relies on human annotation for the "golden" reward signal; learned models introduce noise that disrupts the rejection sampling filter.

## Limitations
- Reward threshold scheduling and exact hyperparameter settings remain unspecified, creating reproducibility challenges
- The method relies on accurate binary rewards; learned reward models (60% success) significantly underperform human-labeled rewards (80% success)
- High dependence on human intervention quality and availability during training

## Confidence
- **High Confidence:** Outcome-based rejection sampling improves stability over learned critics in high-dimensional action spaces
- **Medium Confidence:** Dense intermediate supervision provides meaningful learning signals
- **Medium Confidence:** Human-in-the-loop corrections enable test-time scalability

## Next Checks
1. Test the rejection threshold sensitivity by running "Raise-Hand" task with varying $m$ values to confirm the curriculum effect
2. Ablate the no-op action filter to reproduce the "stuck behavior" failure mode (success drops to 20% in paper's ablation)
3. Trigger a known failure (e.g., object knocked from gripper), provide human correction, and verify trajectory acceptance and subsequent improvement in success rate for that specific failure mode