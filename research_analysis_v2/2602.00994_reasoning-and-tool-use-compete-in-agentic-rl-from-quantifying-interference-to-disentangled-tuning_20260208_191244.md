---
ver: rpa2
title: Reasoning and Tool-use Compete in Agentic RL:From Quantifying Interference
  to Disentangled Tuning
arxiv_id: '2602.00994'
source_url: https://arxiv.org/abs/2602.00994
tags:
- reasoning
- tool-use
- dart
- gradient
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Linear Effect Attribution System (LEAS)
  to empirically demonstrate that reasoning and tool-use capabilities interfere when
  jointly trained in Agentic Reinforcement Learning (ARL). The analysis shows that
  these capabilities induce misaligned gradients during joint optimization, leading
  to a compromise update direction that degrades overall performance.
---

# Reasoning and Tool-use Compete in Agentic RL:From Quantifying Interference to Disentangled Tuning

## Quick Facts
- arXiv ID: 2602.00994
- Source URL: https://arxiv.org/abs/2602.00994
- Authors: Yu Li; Mingyang Yi; Xiuyu Li; Ju Fan; Fuxin Jiang; Binbin Chen; Peng Li; Jie Song; Tieying Zhang
- Reference count: 37
- One-line primary result: Disentangled Action-Reasoning Tuning (DART) improves Exact Match scores by 6.35% on average across seven tool-augmented QA benchmarks by preventing interference between reasoning and tool-use.

## Executive Summary
This paper addresses the interference between reasoning and tool-use capabilities in Agentic Reinforcement Learning (ARL) by introducing the Linear Effect Attribution System (LEAS) to quantify their negative interaction. The analysis reveals that reasoning and tool-use induce misaligned gradients during joint optimization, leading to degraded performance. To solve this, the authors propose Disentangled Action-Reasoning Tuning (DART), which uses separate LoRA adapters for reasoning and tool-use while freezing the pretrained backbone. DART prevents gradient conflicts by directing reasoning and tool-use updates to disjoint parameter subspaces, achieving performance comparable to multi-agent systems with significantly lower computational overhead.

## Method Summary
The paper introduces LEAS to quantify interference between reasoning and tool-use capabilities during joint training. DART addresses this interference by freezing the LLM backbone and attaching two disjoint LoRA adapters: one for reasoning tokens and one for tool-use tokens. A token-level router directs each token to the appropriate adapter during forward and backward passes, ensuring that reasoning and tool-use updates occur in separate parameter subspaces. The method is trained using GRPO with outcome-based rewards on tool-augmented QA tasks.

## Key Results
- DART achieves an average 6.35% improvement in Exact Match scores across seven large-scale tool-augmented QA benchmarks compared to baseline methods
- DART performance approaches that of multi-agent systems that explicitly separate tool-use and reasoning capabilities
- The method demonstrates consistent improvements across diverse benchmarks including NQ, TriviaQA, HotpotQA, and others

## Why This Works (Mechanism)
DART works by preventing gradient interference through parameter isolation. When reasoning and tool-use capabilities are trained jointly on shared parameters, their gradients become misaligned, creating a compromise update direction that degrades both capabilities. By freezing the backbone and routing tokens to separate LoRA adapters, DART ensures that reasoning and tool-use updates occur in orthogonal parameter subspaces, eliminating the interference while maintaining a single model architecture.

## Foundational Learning
- **Linear Effect Attribution System (LEAS)**: A method to quantify the interaction between multiple capabilities during joint optimization by analyzing gradient directions and their orthogonality. Needed to formally demonstrate the interference problem exists and measure its severity.
- **Gradient Interference**: The phenomenon where training multiple capabilities on shared parameters leads to misaligned gradients that compromise overall performance. Quick check: measure gradient angle between different capability updates during joint training.
- **Disentangled Parameter Spaces**: The architectural principle of separating parameter updates for different capabilities to prevent interference. Quick check: verify that reasoning and tool-use adapters have no overlapping parameters.
- **Token-Level Routing**: The mechanism for directing each token to the appropriate capability adapter during both forward and backward passes. Quick check: ensure special tokens correctly trigger adapter switches.
- **LoRA Adapters**: Low-rank adaptation layers that can be independently trained while keeping the backbone frozen. Quick check: verify adapter rank and initialization match experimental setup.
- **GRPO Training**: Group Relative Policy Optimization with outcome-based rewards for reinforcement learning from human feedback. Quick check: monitor reward curves for stability during training.

## Architecture Onboarding

- Component map:
  Frozen Backbone (W) -> Reasoning LoRA (θr) and Tool-use LoRA (θa) -> Token Router (ℓ) -> Forward/Backward Pass

- Critical path:
  1. Token Classification: Router classifies each token as reasoning or tool-use
  2. Forward Pass: Hidden state computed through frozen backbone and active LoRA adapter
  3. Loss & Backward Pass: Gradients routed only to parameters of active adapter

- Design tradeoffs:
  - Single-Model Efficiency vs. Specialization: Achieves near 2-agent performance with 1/8 memory overhead
  - Backbone Freezing: Strictly enforces disentanglement but prevents representational updates
  - Rule-Based Routing: Simple and effective but requires disciplined prompt templates

- Failure signatures:
  - Routing Failure: Model fails to generate special tokens required for correct routing
  - Insufficient Adapter Capacity: LoRA adapters too small to capture capability complexity
  - Continued Interference: Backbone unfrozen inadvertently, allowing gradient mixing

- First 3 experiments:
  1. Baseline Reproduction: Implement standard ARL setup, measure gradient angle between reasoning and tool-use tokens to confirm misalignment (> 80°)
  2. DART Ablation: Compare DART against Vanilla LoRA baseline and 2-Agent upper bound to show performance gap closure
  3. Retrieval & Reasoning Isolation: Evaluate DART under reasoning-only and tool-use-only settings to demonstrate improved isolated capabilities

## Open Questions the Paper Calls Out
- **Open Question 1**: How does backbone model scale influence the trade-off between reasoning capability and tool-use accuracy in disentangled training? [explicit] Appendix H notes that 7B model doesn't consistently outperform 3B in retrieval accuracy, identifying backbone size selection as an open problem.
- **Open Question 2**: Does gradient interference persist in non-QA domains such as mathematical reasoning or code generation? [inferred] Introduction lists computation and data analysis as target tasks, but evaluation is strictly limited to seven QA benchmarks.
- **Open Question 3**: Can DART framework scale to manage interference among three or more distinct capabilities (e.g., planning, memory, and tool-use)? [inferred] Conclusion invites research into multi-capabilities interactions, while current implementation addresses only reasoning vs. tool-use dichotomy.
- **Open Question 4**: Under what specific task conditions do reasoning and tool-use exhibit synergy rather than interference? [inferred] Figure 2 shows positive interaction coefficients for some questions, but analysis focuses exclusively on mitigating negative interference.

## Limitations
- LEAS analysis relies on simplified gradient models and controlled synthetic settings that may not capture complex LLM training dynamics
- Empirical results constrained to single retriever and limited set of QA benchmarks, limiting generalizability
- Rule-based token router may not generalize well to tasks with more fluid reasoning-tool-use boundaries
- Memory and latency costs of dual LoRA adapters not rigorously quantified
- Frozen backbone assumption limits potential for representational improvements during RL fine-tuning

## Confidence
- **High Confidence**: Empirical observation of interference is well-supported by synthetic analysis and large-scale experiments; LEAS provides principled quantification; DART shows consistent performance gains
- **Medium Confidence**: Linear approximation of interference via LEAS is methodologically sound for small perturbations but uncertain for large-scale training; exact scaling with model size and task complexity not fully characterized
- **Low Confidence**: Long-term robustness and generalizability to tasks beyond structured QA with explicit tool-use tokens is unclear; performance in dynamic, open-ended environments not demonstrated

## Next Checks
1. Implement gradient angle measurement during joint training to verify interference exists (> 80° misalignment)
2. Run DART with various backbone sizes (1B, 3B, 7B, 13B) to analyze scaling behavior of isolated capabilities
3. Test DART on non-QA domains (code generation or mathematical reasoning) to verify interference phenomenon universality