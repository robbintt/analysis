---
ver: rpa2
title: Logarithmic Regret and Polynomial Scaling in Online Multi-step-ahead Prediction
arxiv_id: '2511.12467'
source_url: https://arxiv.org/abs/2511.12467
tags:
- prediction
- have
- regret
- where
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of online multi-step-ahead prediction
  for unknown linear stochastic systems. The key idea is to parameterize the H-step-ahead
  prediction policy as a linear function of future inputs, past inputs, and past outputs,
  based on conditional distribution theory.
---

# Logarithmic Regret and Polynomial Scaling in Online Multi-step-ahead Prediction

## Quick Facts
- arXiv ID: 2511.12467
- Source URL: https://arxiv.org/abs/2511.12467
- Reference count: 40
- This paper addresses online multi-step-ahead prediction for unknown linear stochastic systems, achieving logarithmic regret with respect to the optimal Kalman filter while revealing polynomial scaling with prediction horizon.

## Executive Summary
This paper tackles the challenging problem of online multi-step-ahead prediction for unknown linear stochastic systems. The authors develop a novel approach that parameterizes the H-step-ahead prediction policy as a linear function of future inputs, past inputs, and past outputs using conditional distribution theory. By employing an online least-squares algorithm to learn this policy, they achieve logarithmic regret relative to the optimal Kalman filter. The analysis reveals a fundamental tradeoff: while regret remains logarithmic in the time horizon, the constant factor grows polynomially with the prediction horizon, with the polynomial order determined by the largest Jordan block of eigenvalue 1 in the system matrix.

## Method Summary
The method involves parameterizing the optimal H-step-ahead predictor as a linear autoregressive model using past observations, past inputs, and future inputs. An online ridge regression algorithm with a doubling trick approach learns this parameterization. The backward horizon p grows as β log(T_l) where β scales with system properties (κ + log H)/log(1/ρ(A-LC)). The algorithm maintains a Gram matrix V_k,p recursively and computes the weight vector G_k,p via least squares, producing predictions ŷ̃_{k+H} = G_k,p Z_{k,p}.

## Key Results
- Achieves logarithmic regret O(log⁷N) relative to optimal H-step Kalman filter
- Regret bound is almost-sure and does not rely on fixed failure probabilities for sufficiently large horizons
- Regret constant grows polynomially with prediction horizon H as H^(4κ+1), where κ is the largest Jordan block size

## Why This Works (Mechanism)

### Mechanism 1
The optimal H-step-ahead predictor admits a linear autoregressive parameterization using past outputs, past inputs, and future inputs. By rolling out the Kalman filter dynamics backward p steps, the optimal prediction decomposes into a weighted sum of past observations and inputs via the gain matrix (A−LC), and a deterministic forward propagation of the state estimate. The bias from initial state estimation decays exponentially as p increases because ρ(A−LC) < 1 under detectability assumptions.

### Mechanism 2
Online ridge regression with backward horizon p = O(κ log k) controls cumulative regret to logarithmic in N. The doubling trick partitions time into epochs of doubling length, with p fixed within each epoch but growing between epochs. This ensures the bias term ||(A−LC)^p|| decays faster than the accumulated state grows (polynomially for marginally stable systems with Jordan blocks of size κ).

### Mechanism 3
Almost-sure bounds emerge because information accumulation dominates stochastic confidence for large N. Standard martingale analysis gives high-probability bounds with δ-dependence, but by setting δ = 1/N, the term log det(Vk,p) ∝ p log(N) dominates log(1/δ) = log(N) for sufficiently large N. The union bound over all k sums to π²δ/6, which converges.

## Foundational Learning

- Concept: Kalman filter as optimal linear predictor
  - Why needed here: The entire approach derives from representing the Kalman filter in autoregressive form; without understanding that the steady-state Kalman gain L emerges from solving the ARE, the parameterization seems unmotivated.
  - Quick check question: Can you explain why ρ(A−LC) < 1 under detectability and what this implies for estimation error dynamics?

- Concept: Regret in online learning
  - Why needed here: The paper's core contribution is bounding cumulative loss relative to an optimal benchmark; understanding that logarithmic regret means average loss → 0 is essential.
  - Quick check question: If regret is O(log N), what is the asymptotic average excess loss per time step?

- Concept: Ridge regression and Gram matrix conditioning
  - Why needed here: The algorithm computes online least squares via Vk,p = λI + Σ Z Z^T, and the condition number of this matrix directly affects regret bounds.
  - Quick check question: Why does adding λI guarantee invertibility and how does λ affect the regularization error term?

## Architecture Onboarding

- Component map: Data collector -> Gram matrix updater -> Weight estimator -> Predictor -> Epoch controller

- Critical path: The latency-critical operation is the matrix-vector product V_k,p^{-1} Zk,p for prediction. For high-dimensional systems, maintaining low-rank or diagonal approximations may be necessary.

- Design tradeoffs:
  - Larger β → smaller bias but larger regression variance and higher computational cost
  - Smaller λ → faster convergence but potential numerical instability
  - Theorem 2 suggests β scales with κ + log H; for systems with large Jordan blocks, β may need manual tuning

- Failure signatures:
  - Regret plateauing: If bias term doesn't decay, check that ρ(A−LC) < 1 and p is growing correctly
  - Numerical explosion: If V_k,p becomes ill-conditioned, increase λ or use square-root filtering
  - Linear regret growth: Indicates p is insufficient; check epoch controller and β setting

- First 3 experiments:
  1. Validate on stable system (ρ(A) < 1) with H=1, comparing to known single-step regret bounds; should reproduce logarithmic scaling
  2. Sweep H ∈ {2, 4, 8, 16} on marginally stable system (Jordan block at eigenvalue 1); verify polynomial constant scaling
  3. Stress test with increasing κ (e.g., double integrator κ=2 vs. triple integrator κ=3) and confirm β tuning formula

## Open Questions the Paper Calls Out

- Can the online multi-step-ahead prediction framework be extended to structured nonlinear systems while maintaining logarithmic regret?
- How can this prediction policy be utilized to design online feedback stabilization controllers with regret guarantees relative to an optimal LQR/LQG controller?
- Is the polynomial scaling factor H^(4κ + 1) in the regret bound tight, or does it overestimate the dependence on the prediction horizon?
- Does the regret scaling remain logarithmic if the diagonalizability assumption on A - LC is removed?

## Limitations
- The theoretical regret constant H^(4κ+1) may be conservative compared to empirical observations
- Assumption of diagonalizable A-LC limits applicability to systems with non-semisimple Jordan blocks
- High computational complexity for systems with large state dimensions due to Gram matrix updates

## Confidence

- High confidence: Optimal linear parameterization of multi-step predictors, basic online ridge regression framework
- Medium confidence: Regret bound derivations (particularly polynomial constants), almost-sure convergence arguments
- Low confidence: Practical feasibility for systems with large Jordan blocks or high prediction horizons, numerical stability of Gram matrix updates

## Next Checks

1. **Bias decay verification**: Simulate marginally stable systems with Jordan blocks of size κ=1,2,3 and verify that regret scales as log(N) with constants matching H^(4κ+1). Compare against theoretical prediction for specific β values.

2. **Polynomial constant scaling**: Systematically vary H ∈ {2,4,8,16} and κ ∈ {1,2,3} while measuring regret constants. Plot log(regret constant) vs κ and H to verify H^(4κ+1) scaling.

3. **Almost-sure convergence rate**: Track the ratio [log det(V_k,p) / log(N)] over increasing N. Verify that this ratio exceeds 1 after some finite N, confirming the almost-sure regime onset.