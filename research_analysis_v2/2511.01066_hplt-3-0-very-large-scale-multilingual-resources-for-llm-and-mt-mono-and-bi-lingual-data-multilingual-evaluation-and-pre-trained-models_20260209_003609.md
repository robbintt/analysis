---
ver: rpa2
title: 'HPLT 3.0: Very Large-Scale Multilingual Resources for LLM and MT. Mono- and
  Bi-lingual Data, Multilingual Evaluation, and Pre-Trained Models'
arxiv_id: '2511.01066'
source_url: https://arxiv.org/abs/2511.01066
tags:
- language
- hplt
- data
- computational
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The HPLT 3.0 project introduces a very large-scale, multilingual
  dataset for large language model (LLM) and machine translation (MT) training, covering
  nearly 200 languages. Derived from extensive web crawls, the dataset contains 30
  trillion tokens, making it the largest generally available multilingual collection
  of its kind.
---

# HPLT 3.0: Very Large-Scale Multilingual Resources for LLM and MT. Mono- and Bi-lingual Data, Multilingual Evaluation, and Pre-Trained Models

## Quick Facts
- **arXiv ID:** 2511.01066
- **Source URL:** https://arxiv.org/abs/2511.01066
- **Reference count:** 0
- **Primary result:** 30 trillion tokens across nearly 200 languages in the largest generally available multilingual dataset for LLM and MT training

## Executive Summary
The HPLT 3.0 project introduces a very large-scale, multilingual dataset for large language model (LLM) and machine translation (MT) training, covering nearly 200 languages. Derived from extensive web crawls, the dataset contains 30 trillion tokens, making it the largest generally available multilingual collection of its kind. The dataset is accompanied by an open-source pipeline that includes text extraction, language identification, deduplication, annotation with quality estimates, and filtering. Manual inspection and analytical statistics confirm the dataset's high quality, with low proportions of problematic content such as pornography or mis-identified languages. A novel multilingual evaluation framework (HPLT-E) supports systematic comparison across nine languages, revealing that models trained on HPLT 3.0 perform competitively with other large datasets. Additionally, 57 monolingual encoder-decoder models and several GPT-like models are released, along with mined parallel texts and synthetic data for low-resource languages. Overall, HPLT 3.0 provides a comprehensive, high-quality, and openly available resource for advancing multilingual LLM and MT research.

## Method Summary
The HPLT 3.0 dataset preparation pipeline processes 7.2 petabytes of web archives through Trafilatura extraction, OpenLID-v2 language identification, MinHash near-deduplication (global except for high-resource languages), and Web Docs Scorer annotation with quality estimates. The resulting dataset contains 30 trillion tokens organized into WDS bins (5-10) based on quality scores. Pretraining uses a Gemma-3 tokenizer with 256K vocabulary and a 2.15B parameter decoder-only architecture with 24 layers, 32 heads, and 2048 sequence length. The HPLT-E evaluation framework measures monolingual performance across 127 tasks in 9 languages using LM Evaluation Harness, aggregating results through min-max normalization and Borda's count ranking. Models are trained using Megatron-LM with standard Llama configurations and evaluated through 0-shot prompting across 26 validated tasks.

## Key Results
- HPLT 3.0 dataset contains 30 trillion tokens across nearly 200 languages, making it the largest generally available multilingual collection
- Models trained on HPLT 3.0 show competitive performance compared to other large datasets, with MADLAD-400 unexpectedly outperforming in multilingual evaluation
- 57 monolingual encoder-decoder models and several GPT-like models are released, along with mined parallel texts and synthetic data for low-resource languages
- HPLT-E evaluation framework provides systematic multilingual comparison across nine languages with 127 tasks

## Why This Works (Mechanism)
None

## Foundational Learning
- **MinHash near-deduplication**: Reduces redundancy while preserving diversity by identifying similar documents across crawls; needed to prevent overfitting on repetitive web content; quick check: compare vocabulary size before/after deduplication
- **Web Docs Scorer quality estimation**: Annotates documents with quality scores based on multiple features; needed to filter problematic content while maintaining data utility; quick check: verify low proportions of pornography and mis-identified languages
- **HPLT-E multilingual evaluation**: Aggregates 127 tasks across 9 languages using monotonicity analysis and Borda's count; needed to provide consistent comparison across diverse linguistic properties; quick check: confirm task stability through multiple prompts
- **Synthetic data generation**: Creates training data for low-resource languages through machine translation; needed to address data scarcity while maintaining reasonable quality; quick check: compare model performance on synthetic vs native data
- **Gemma-3 tokenizer**: Uses 256K vocabulary for efficient multilingual tokenization; needed to handle diverse scripts and languages effectively; quick check: verify token coverage across target languages
- **Megatron-LM training**: Distributes pretraining across multiple GPUs using standard Llama architecture; needed to handle trillion-token scale efficiently; quick check: monitor training loss convergence

## Architecture Onboarding

**Component Map:** Web Archives -> Trafilatura Extraction -> OpenLID-v2 Language ID -> MinHash Deduplication -> Web Docs Scorer -> WDS Binning -> HPLT 3.0 Dataset -> Megatron-LM Training -> HPLT-E Evaluation

**Critical Path:** Web extraction → language identification → deduplication → quality scoring → dataset creation → model training → evaluation

**Design Tradeoffs:** Aggressive quality filtering vs. data diversity; synthetic vs. native data for low-resource languages; monolingual vs. multilingual pretraining approaches; task coverage vs. evaluation stability

**Failure Signatures:** Poor language identification causing mis-classified documents; excessive deduplication removing beneficial variance; synthetic data introducing translationese artifacts; evaluation instability in low-resource languages

**First Experiments:** 1) Verify Zstandard decompression of sample HPLT 3.0 files; 2) Run HPLT-E evaluation on a small task subset to confirm prompt consistency; 3) Train a small 2B parameter model on HPLT 3.0 to validate pretraining pipeline

## Open Questions the Paper Calls Out

**Open Question 1**
- **Question:** Why does the older, smaller MADLAD-400 dataset frequently outperform the larger, refined HPLT 3.0 in multilingual evaluation scores?
- **Basis in paper:** [explicit] Section 7.2 notes that "Models pretrained on MADLAD-400 achieve the highest multilingual score" and explicitly states this "calls for follow-up studies."
- **Why unresolved:** It is counter-intuitive that an older, smaller dataset would surpass a massive, rigorously cleaned successor using current SoTA pipelines, suggesting unknown variables in data quality or benchmark bias.
- **What evidence would resolve it:** A detailed ablation study comparing the token distributions and specific linguistic features of MADLAD-400 against HPLT 3.0, specifically isolating the causes of the performance gap.

**Open Question 2**
- **Question:** Can synthetic data generated via machine translation serve as a functional equivalent to native web data for pretraining LLMs in low-resource languages?
- **Basis in paper:** [explicit] Section 10 reports that models trained on synthetic data performed on par with native data but concludes that "more comprehensive studies... are needed to scrutinize these findings further."
- **Why unresolved:** Initial results are based on "smallish" (2B parameter) models; it remains unclear if this efficacy scales to larger models or if "translationese" artifacts limit advanced capabilities.
- **What evidence would resolve it:** Evaluating larger-scale models (e.g., 7B+ parameters) trained on synthetic data using the native linguistic benchmarks described in HPLT-E.

**Open Question 3**
- **Question:** How can data curation pipelines balance aggressive quality filtering with the preservation of text diversity to maximize downstream LLM performance?
- **Basis in paper:** [inferred] Section 7.2 observes that sampling exclusively from the highest quality scores (WDS "top") did not improve performance, hypothesizing this was "possibly owing to overly limited diversity."
- **Why unresolved:** While quality scores effectively remove noise, the results suggest a trade-off where aggressive filtering removes beneficial variance, but the optimal trade-off point is unidentified.
- **What evidence would resolve it:** Systematic experiments varying the mixing ratios of high-WDS and random samples to identify the peak of the quality-diversity curve for pretraining.

## Limitations
- Key hyperparameters for Trafilatura optimization and training configuration are not fully specified, requiring assumptions for exact reproduction
- Some low-resource languages show unstable evaluation signals, limiting reliable comparison
- Counter-intuitive results show MADLAD-400 dataset outperforming HPLT 3.0 despite being smaller and older

## Confidence
- **High confidence**: Dataset scale (30T tokens), language coverage (~200 languages), pipeline architecture (Trafilatura → OpenLID-v2 → MinHash → WDS Scorer → WDS binning), evaluation methodology (HPLT-E with monotonicity analysis and 26 validated tasks)
- **Medium confidence**: Dataset quality estimates (pornography/mis-identification proportions from manual inspection), model performance comparisons with other datasets, synthetic data generation for low-resource languages
- **Low confidence**: Exact Trafilatura optimization parameters, specific training hyperparameters (batch size, learning rate schedule, optimizer settings), Web Docs Scorer threshold definitions for WDS levels

## Next Checks
1. Verify dataset quality by sampling and manually inspecting randomly selected documents across multiple languages, checking for problematic content and language identification accuracy
2. Reproduce the HPLT-E evaluation by running the 26 validated tasks on a held-out subset of the data, confirming task monotonicity and prompt consistency
3. Attempt to reproduce the 2.15B parameter GPT-like model pretraining using the Gemma-3 tokenizer and standard Llama hyperparameters, comparing perplexity and downstream performance against reported results