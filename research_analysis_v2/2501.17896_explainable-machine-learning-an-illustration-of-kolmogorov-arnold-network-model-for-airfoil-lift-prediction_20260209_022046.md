---
ver: rpa2
title: 'Explainable Machine Learning: An Illustration of Kolmogorov-Arnold Network
  Model for Airfoil Lift Prediction'
arxiv_id: '2501.17896'
source_url: https://arxiv.org/abs/2501.17896
tags:
- data
- airfoil
- network
- scientific
- lift
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates the potential of Kolmogorov-Arnold Networks
  (KAN) for scientific exploration by applying them to airfoil lift prediction. KAN,
  along with five other popular supervised machine learning models, was trained on
  a dataset of 2,900 airfoils represented by CST coefficients and angle of attack
  to predict the coefficient of lift (CL).
---

# Explainable Machine Learning: An Illustration of Kolmogorov-Arnold Network Model for Airfoil Lift Prediction

## Quick Facts
- arXiv ID: 2501.17896
- Source URL: https://arxiv.org/abs/2501.17896
- Reference count: 12
- KAN achieves 96.17% R² on airfoil lift prediction, outperforming 5 ML models and baseline ANN

## Executive Summary
This study demonstrates the potential of Kolmogorov-Arnold Networks (KAN) for scientific exploration by applying them to airfoil lift prediction. KAN, along with five other popular supervised machine learning models, was trained on a dataset of 2,900 airfoils represented by CST coefficients and angle of attack to predict the coefficient of lift (CL). KAN outperformed all other models, achieving an R² score of 96.17% on the test data, surpassing both a baseline artificial neural network and a Multi-Layer Perceptron. The explainability of KAN was demonstrated through pruning and symbolification, resulting in a closed-form equation for CL in terms of input variables. This equation was found to be consistent with the known physics of lift generation by airfoils, highlighting KAN's potential to aid in scientific discovery by identifying important features and uncovering non-linear relationships between variables.

## Method Summary
The study used 2,900 airfoils from the UIUC database, represented by 8 CST coefficients and angle of attack, to predict coefficient of lift. After removing 3,266 duplicate samples, 30,439 total samples were split 75/25 into train/test sets. Six models were trained: Linear Regression, Decision Tree Regressor, Random Forest Regressor, AdaBoost Regressor, Multi-Layer Perceptron, and KAN. KAN architecture used width=[9, 9, 1] with grid=6, k=2, seed=2024, and base_fun='silu'. Model performance was evaluated using MSE and R² score. KAN's explainability was demonstrated through pruning (75th percentile threshold) and symbolification to obtain a closed-form equation.

## Key Results
- KAN achieved 96.17% R² on test data, outperforming all other models including MLP (94.51% R²)
- Pruned KAN (11 nodes, 10 edges) maintained 95.60% R², close to full model performance
- Symbolified equation showed complex non-linear relationship consistent with known lift physics
- KAN's edge-based learnable activations enabled superior performance and interpretability compared to MLP's fixed activations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Placing learnable activation functions on edges enables KANs to approximate complex functions with fewer parameters while remaining interpretable
- Mechanism: B-spline curves parameterize activation functions on each edge, optimized during training to learn univariate transformations
- Core assumption: Target multivariate function can be decomposed into sums of simpler univariate functions per Kolmogorov-Arnold theorem
- Evidence anchors: [abstract] "KAN...embeds explainable AI...learnable activation functions (B-spline curve) on edges"; [section II] "MLPs have fixed activation functions...while KANs learn parameters of activation functions"

### Mechanism 2
- Claim: Pruning based on importance scores isolates dominant input-output relationships without catastrophic accuracy loss
- Mechanism: Node and edge importance scores computed; only those above 75th percentile threshold retained
- Core assumption: Important features have measurably higher contribution to loss gradient or output variance
- Evidence anchors: [abstract] "Explainability of KAN is shown by pruning"; [section III] "Even after retaining only 25% of full network, model R2 score was 95.60%"

### Mechanism 3
- Claim: Symbolification converts learned spline activations into closed-form mathematical expressions for physical interpretation
- Mechanism: Fitted activation compared against library of candidate functions (sin, tanh, polynomial) to find best match
- Core assumption: True underlying relationship can be approximated by compositions of simple functions from library
- Evidence anchors: [abstract] "resulting in closed-form equation for CL...found to be consistent with known physics"; [section III] "equation suggests complex non-linear relationship...consistent with prior knowledge"

## Foundational Learning

- **Kolmogorov-Arnold Representation Theorem**
  - Why needed: Justifies KAN's architecture—any continuous multivariate function can be represented as sums of univariate functions
  - Quick check: Can you explain why a sum of univariate functions can theoretically replace a multilayer perceptron with fixed activations?

- **B-Spline Curves**
  - Why needed: B-splines are parameterized activation functions KAN learns; understanding their degrees of freedom is essential for hyperparameter tuning
  - Quick check: What happens to a spline's flexibility if you increase grid size but keep polynomial order constant?

- **CST (Class Shape Transformation) Coefficients**
  - Why needed: Input features are CST coefficients representing airfoil geometry, not raw coordinates
  - Quick check: Why might c2, c5, c8 be excluded from linear regression model due to multicollinearity, while KAN can retain them?

## Architecture Onboarding

- Component map: Input(9) -> Hidden(9) -> Hidden(9) -> Output(1), with learnable B-spline activations on edges
- Critical path: 1) Define architecture (width, grid size g, spline order k) 2) Train via backpropagation through spline parameters 3) Compute node/edge importance scores 4) Prune below threshold 5) Symbolify remaining edges 6) Extract closed-form equation and validate against domain physics
- Design tradeoffs:
  - Larger grid size → finer function approximation but more parameters and overfitting risk
  - Deeper networks → can represent more complex compositions but harder to symbolify and interpret
  - Aggressive pruning → cleaner interpretability but potential accuracy loss
  - Symbolification library breadth → more functions enable better fits but increase search cost
- Failure signatures:
  - Training loss plateaus early: May indicate grid size too small or spline order insufficient
  - Pruned model accuracy drops sharply: Threshold too aggressive; try 60th-70th percentile
  - Symbolified equation contradicts known physics: Library missing correct function class
  - Edge importance scores uniformly low: Network may be underfitting; increase width or epochs
- First 3 experiments:
  1. Baseline replication: Implement KAN with [9, 9, 1], grid=6, k=2 on airfoil dataset. Verify R² ≈ 96% on test split.
  2. Ablation on pruning threshold: Test pruning at 50th, 75th, 90th percentiles. Plot accuracy vs. retained edges.
  3. Symbolification sanity check: On synthetic dataset where y = sin(x₁) + x₂², verify symbolification recovers correct functions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can KAN effectively model and symbolically represent the coefficient of drag (CD) and lift-to-drag ratio (CL/CD) with accuracy comparable to lift prediction?
- Basis in paper: [explicit] "This study is limited to modeling only CL, and there is scope to model other parameters such as CD, and CL/CD ratio"
- Why unresolved: Current study focused exclusively on CL; drag forces involve different and often more complex non-linear flow physics
- What evidence would resolve it: Follow-up study training same KAN architecture on CD and CL/CD data from UIUC database

### Open Question 2
- Question: Does KAN performance and explainability remain robust when applied to aerodynamic data generated at high Reynolds numbers?
- Basis in paper: [explicit] Data used estimated CL at low Reynolds number (10⁵); suggests exploring "model performances for a range of Reynolds numbers"
- Why unresolved: Airflow physics change significantly between low and high Reynolds numbers (laminar to turbulent flow transition)
- What evidence would resolve it: Training and evaluating KAN on high-speed (high Reynolds) flow regime datasets

### Open Question 3
- Question: How sensitive is the derived symbolic equation to chosen pruning thresholds and grid hyperparameters?
- Basis in paper: [inferred] Network pruned by retaining nodes/edges above 75th percentile, but does not verify different pruning intensities would alter "closed-form" equation
- Why unresolved: Paper demonstrates single successful path but does not test stability of symbolic interpretation against pruning heuristic variations
- What evidence would resolve it: Ablation study showing variance in final symbolic formulae and accuracy scores as pruning percentile and grid size are varied

## Limitations
- 75th percentile pruning threshold appears empirically chosen rather than theoretically derived
- Symbolification library composition and selection criteria are unspecified
- Data preprocessing steps, particularly duplicate identification, are not fully detailed
- Study limited to single dataset (airfoil lift prediction) without cross-domain validation

## Confidence
- **High**: KAN outperforms MLP and baseline ANN on airfoil lift prediction task (R²: 96.17% vs. 94.51%)
- **Medium**: Symbolified equation captures known physics principles of lift generation
- **Medium**: Pruning maintains accuracy while improving interpretability; threshold choice may not generalize

## Next Checks
1. **Cross-dataset generalization**: Apply same KAN pruning+symbolification pipeline to at least two different scientific datasets to test threshold robustness and library completeness
2. **Physical consistency audit**: Compare symbolified airfoil equation against computational fluid dynamics simulations across input domain to quantify prediction errors in physically meaningful terms
3. **Alternative pruning strategies**: Implement and compare against importance-based methods like layerwise relevance propagation or SHAP values to validate that 75th percentile node/edge scores capture influential features