---
ver: rpa2
title: 'Information Capacity: Evaluating the Efficiency of Large Language Models via
  Text Compression'
arxiv_id: '2511.08066'
source_url: https://arxiv.org/abs/2511.08066
tags:
- information
- capacity
- wang
- zhang
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Information capacity is a unified metric for evaluating large language
  model (LLM) efficiency by measuring text compression performance relative to computational
  complexity. The metric incorporates tokenizer efficiency, often neglected in LLM
  evaluations, and is defined as the ratio of compression gain to inference FLOPs
  on a logarithmic scale.
---

# Information Capacity: Evaluating the Efficiency of Large Language Models via Text Compression

## Quick Facts
- arXiv ID: 2511.08066
- Source URL: https://arxiv.org/abs/2511.08066
- Reference count: 17
- Primary result: Information capacity is a unified metric for evaluating LLM efficiency by measuring text compression performance relative to computational complexity, with tokenizer efficiency as a dominant factor.

## Executive Summary
This paper introduces information capacity, a novel metric for evaluating large language model (LLM) efficiency by measuring text compression performance relative to computational complexity. The metric is defined as the ratio of compression gain to inference FLOPs on a logarithmic scale, incorporating tokenizer efficiency—a factor often neglected in traditional LLM evaluations. Experiments on 52 open-source models across five heterogeneous datasets reveal that tokenizer efficiency, pretraining data, and mixture-of-experts (MoE) architecture are the three major factors affecting information capacity. The MoE architecture and latest dense models like Qwen3 and GLM-4 achieve the highest information capacity. The metric enables fair efficiency comparison across model series and accurate performance prediction within a series using only a single reference model. Information capacity also correlates with benchmark scores, reflecting an LLM's capabilities on downstream tasks.

## Method Summary
The method evaluates LLM efficiency through text compression, where information capacity (IC) is calculated as the ratio of compression gain to inference FLOPs on a logarithmic scale. The metric uses the negative log-likelihood (NLL) of text compression as a proxy for model intelligence, normalized by the computational cost of inference. Key components include tokenizer efficiency, which dictates how many computational steps (tokens) are required to process a fixed amount of information, and the incorporation of architecture-specific FLOPs formulas. The evaluation uses five datasets, truncated to 1024 tokens, and applies a dataset-specific offset to stabilize the metric across model sizes. The method requires base models (not instruct-tuned) and precise FLOPs calculation matching specific architectures (standard GQA, Llama-4, DeepSeek-MLA).

## Key Results
- Tokenizer efficiency is the dominant factor in information capacity, with Pearson correlation coefficients consistently exceeding 0.98 across datasets.
- MoE architecture and latest dense models like Qwen3 and GLM-4 achieve the highest information capacity.
- Information capacity remains consistent within model series, enabling accurate performance prediction using a single reference model.
- The metric correlates with benchmark scores, reflecting an LLM's capabilities on downstream tasks.

## Why This Works (Mechanism)

### Mechanism 1: Compression as a Proxy for Model Efficiency
An LLM's efficiency can be unified into a single metric by measuring its text compression performance (negative log-likelihood) relative to its computational cost (FLOPs). The metric, Information Capacity (IC), posits that intelligence correlates with the ability to predict the next token. By using entropy coding principles, the length of the compressed text is approximately the negative log-likelihood. IC normalizes this compression "gain" against the log of inference FLOPs. A consistent relationship exists between compression capability and downstream task intelligence, such that maximizing likelihood on text correlates with general capability.

### Mechanism 2: Tokenizer Efficiency as a Dominant Factor
Tokenizer efficiency is a primary driver of Information Capacity, as it dictates how many computational steps (tokens) are required to process a fixed amount of information (bits). The metric includes the "text size per token" implicitly via the token count in the FLOPs denominator and the NLL sum in the numerator. A tokenizer that packs more information per token results in fewer tokens for the same text, reducing computational cost and increasing the calculated capacity. High-quality probability estimation is maintained even when the tokenizer packs more information into fewer tokens.

### Mechanism 3: Intra-Series Consistency for Scaling Prediction
Information Capacity remains relatively constant across different model sizes within the same model series, allowing for the prediction of larger model performance using a single smaller reference model. Larger models improve compression gain but increase computational cost at a rate that keeps their ratio constant within a series. The paper introduces an offset in the formula to stabilize this constancy. The scaling law holds that the logarithmic increase in FLOPs matches the linear increase in compression capability for a given model lineage.

## Foundational Learning

### Concept: Negative Log-Likelihood (NLL) & Entropy Coding
**Why needed here:** Understanding that -log₂ p(x) is both the loss function and the theoretical compressed bit length is essential to see why compression efficiency equals model intelligence.
**Quick check question:** Why is minimizing cross-entropy loss during pretraining equivalent to minimizing the compressed size of the training data?

### Concept: Inference FLOPs Calculation
**Why needed here:** The metric relies on accurate computational complexity measurement. One must distinguish between attention FLOPs and FFN FLOPs, especially for MoE models.
**Quick check question:** In the provided formula, how does reducing the number of activated experts in an MoE model affect the denominator vs. the numerator of Information Capacity?

### Concept: Tokenization Efficiency
**Why needed here:** The paper identifies this as a "neglected" factor. One must understand that vocab size and compression ratio (bytes per token) directly impact the "cost" side of the efficiency equation.
**Quick check question:** If Model A and Model B have identical architectures but Model A has a tokenizer that represents text with 20% fewer tokens, how would this likely affect Model A's Information Capacity?

## Architecture Onboarding

### Component map:
Text sample -> Tokenizer -> Sequence of tokens -> Model (M) -> Logits -> Softmax (T=1) -> Probabilities p(xᵢ|x<ᵢ;M) -> Calculator -> NLL sum and Inference FLOPs -> Information Capacity

### Critical path:
1. Select base models (not instruct-tuned, as post-training degrades IC).
2. Ensure logits are converted to float32 for numerical precision.
3. Apply the specific FLOPs formula (Standard GQA, Llama-4, or DeepSeek-MLA) to match the architecture.

### Design tradeoffs:
- **Offset b:** Introducing a fixed offset stabilizes the metric across model sizes but adds a parameter that must be tuned per dataset.
- **Temperature:** T=1 is strictly required for fair probability estimation; generative temperatures (T ≠ 1) will artificially skew results.

### Failure signatures:
- **Linguistic Bias:** Evaluating a model on a mismatched language (e.g., Llama on Chinese corpus) will artificially depress IC scores.
- **Post-training Distortion:** RLHF or SFT typically degrades the "base" compression capability, causing Instruct models to score lower than Base models.

### First 3 experiments:
1. **Baseline Validation:** Calculate IC for a known model series (e.g., Qwen1.5) across multiple sizes to verify the "consistent capacity" claim.
2. **Tokenizer Ablation:** Tokenize a fixed dataset with different tokenizers (e.g., GPT-4 vs. Llama-3) and analyze the correlation between "text size per token" and IC.
3. **Prediction Test:** Use a 0.5B parameter model to predict the NLL/IC of the 7B or 72B variant using the paper's scaling equation and compare against actual measured values.

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** Can Information Capacity be adapted to effectively evaluate the efficiency of instruction-tuned or RLHF models without the probability degradation observed in base models?
**Basis in paper:** Section 4.3.1 states, "post-training impairs LLMs' capability in predicting the next token... [and] degrades the information capacity," forcing the exclusion of instruct models from the main comparison.
**Why unresolved:** The metric currently suggests that the alignment process makes models less "efficient" by the compression standard, failing to capture the utility added by instruction tuning.
**What evidence would resolve it:** A calibration method that correlates information capacity with instruction-following benchmarks, or a modified metric that accounts for the distribution shift in aligned models.

### Open Question 2
**Question:** Does the strong linear correlation between tokenizer efficiency and Information Capacity mask the relative efficiency of the underlying transformer architectures?
**Basis in paper:** Section 4.2.1 notes "tokenizer efficiency is the dominant factor in information capacity," with correlation coefficients exceeding 0.98 across datasets, potentially conflating tokenizer design with model intelligence.
**Why unresolved:** It remains unclear if a model with a superior IC score is actually more computationally efficient or simply benefits from a more aggressive tokenizer that reduces FLOPs by shortening sequences.
**What evidence would resolve it:** Ablation studies evaluating different model architectures using a fixed, universal tokenizer to isolate architectural efficiency from tokenization efficiency.

### Open Question 3
**Question:** How can the Information Capacity metric be estimated for closed-source models where internal architectural details (needed for precise FLOPs calculation) are unavailable?
**Basis in paper:** Section 3.2 explicitly relies on hyperparameters (hidden dimensions, layers, etc.) to calculate inference FLOPs, which prevents the evaluation of influential closed models like GPT-OSS.
**Why unresolved:** The metric cannot currently benchmark the efficiency of the state-of-the-art proprietary models that dominate the industry without accurate computational complexity data.
**What evidence would resolve it:** A proxy methodology for estimating computational complexity (e.g., using latency or approximate parameter counts) that demonstrates high correlation with the FLOPs-based metric.

## Limitations
- The metric excludes instruct-tuned models due to probability degradation from post-training, limiting practical applicability.
- Requires precise FLOPs calculation matching specific architectures, with errors potentially distorting rankings.
- Dataset-specific offset parameter requires empirical tuning, though fixed values are suggested to work across datasets.

## Confidence
*High Confidence:* The fundamental mathematical framework linking compression to likelihood estimation is sound, with strong empirical support from the correlation between IC and benchmark scores. The identification of tokenizer efficiency as a dominant factor is well-supported by the 0.98+ Pearson correlation coefficients. The claim that MoE architectures achieve higher information capacity is robustly demonstrated through direct comparison across 52 models.

*Medium Confidence:* The claim that IC remains consistent within model series for scaling prediction is supported but requires careful architectural matching. The assertion that tokenizer efficiency is "neglected" in LLM evaluations is accurate for traditional metrics but may overstate the case for newer evaluation frameworks. The practical utility of using a single reference model for prediction within a series is demonstrated but may not generalize to all model families.

*Low Confidence:* The generalizability of the fixed dataset offsets (-24 to -27) to completely new domains remains uncertain without the derivation methodology. The claim that RLHF/SFT universally degrades IC by 10-20% may not hold for all post-training techniques or model architectures. The assumption that temperature T=1 is strictly optimal for all evaluation scenarios could be challenged in specific use cases.

## Next Checks
1. **Cross-Domain Robustness Test:** Evaluate IC on a dataset from a completely different domain (e.g., legal documents or scientific papers) to verify whether the established offsets (-24 to -27) maintain their predictive power, or if the derivation methodology for b needs to be clarified and standardized.

2. **Architecture-Specific FLOPs Validation:** For a mixed-architecture evaluation (including MoE, MLA, and standard GQA models), implement a cross-verification system where FLOPs are calculated using multiple methods and compared against the paper's reported values to ensure the correct formula is being applied and that architectural nuances are properly captured.

3. **Post-Training Technique Ablation:** Systematically evaluate a single base model after different post-training techniques (RLHF, SFT, supervised fine-tuning) to quantify the precise degradation in IC for each method and determine whether the 10-20% degradation is consistent across all techniques or varies significantly by approach.