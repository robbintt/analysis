---
ver: rpa2
title: Forward-Only Continual Learning
arxiv_id: '2509.01533'
source_url: https://arxiv.org/abs/2509.01533
tags:
- learning
- foro
- continual
- task
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FoRo, a forward-only continual learning method
  that avoids backpropagation and gradient-based updates entirely. The approach combines
  a lightweight prompt tuning strategy using Covariance Matrix Adaptation Evolution
  Strategy (CMA-ES) with a knowledge encoding mechanism that incrementally updates
  classifiers using nonlinear random projection and recursive least squares.
---

# Forward-Only Continual Learning

## Quick Facts
- arXiv ID: 2509.01533
- Source URL: https://arxiv.org/abs/2509.01533
- Reference count: 40
- One-line primary result: Introduces FoRo, a gradient-free continual learning method that achieves 84.5% average accuracy with only 3.4% forgetting by optimizing prompts via CMA-ES and updating classifiers via recursive least squares.

## Executive Summary
This paper introduces FoRo, a forward-only continual learning method that avoids backpropagation and gradient-based updates entirely. The approach combines a lightweight prompt tuning strategy using Covariance Matrix Adaptation Evolution Strategy (CMA-ES) with a knowledge encoding mechanism that incrementally updates classifiers using nonlinear random projection and recursive least squares. By leaving the pre-trained model unmodified, FoRo is particularly suited for resource-constrained environments. Experiments on CIFAR-100, ImageNet-R, and CUB-200 datasets demonstrate that FoRo achieves an average accuracy of 84.5% with only 3.4% forgetting, outperforming existing methods while reducing memory usage and runtime through its forward-only learning design.

## Method Summary
FoRo is a class-incremental learning method that processes tasks sequentially without revisiting prior data or using backpropagation. It optimizes learnable input prompts using CMA-ES to minimize a fitness function combining cross-entropy loss with activation discrepancy terms. The method extracts frozen features from a pre-trained ViT-B/16 backbone, projects them through a fixed nonlinear random projection layer, and updates the classifier using recursive least squares with the Woodbury identity. This forward-only approach maintains accuracy while significantly reducing computational overhead compared to gradient-based methods.

## Key Results
- Achieves 84.5% average accuracy on benchmark datasets
- Maintains only 3.4% forgetting across tasks
- Outperforms existing continual learning methods while using less memory and runtime
- Demonstrates effectiveness of gradient-free prompt optimization via CMA-ES
- Shows incremental classifier updates can match joint training performance

## Why This Works (Mechanism)

### Mechanism 1: Gradient-Free Prompt Optimization (CMA-ES)
Optimizing input prompts via evolutionary strategies may preserve the pre-trained backbone's knowledge better than gradient-based fine-tuning, which risks overwriting prior features. Instead of backpropagation, FoRo employs CMA-ES to sample a population of prompt candidates and evaluate them using a fitness function that combines cross-entropy with an activation discrepancy term to mitigate distribution shifts.

### Mechanism 2: Recursive Least Squares for Classifiers (KEM)
A classifier can be updated sequentially to mimic joint training on all data without storing replay buffers, assuming features are fixed. FoRo utilizes a Knowledge Encoding Matrix (KEM) that represents the inverse correlation matrix of features, updated incrementally using the Woodbury identity as new task data arrives.

### Mechanism 3: Nonlinear Random Projection (NRP)
Projecting frozen features into a higher-dimensional random space with non-linearity improves the robustness of the analytic classifier. Features pass through a fixed random projection matrix followed by a nonlinear activation, mapping inputs to a higher dimension and increasing the likelihood of linear separability without introducing trainable parameters.

## Foundational Learning

- **Concept: Covariance Matrix Adaptation Evolution Strategy (CMA-ES)**
  - Why needed here: This is the core optimizer replacing SGD. You must understand how black-box optimization samples from a multivariate normal distribution and adapts the covariance matrix to shape the search distribution.
  - Quick check question: Can you explain how CMA-ES updates its step-size (σ) and search direction based on the "fitness" of sampled population candidates?

- **Concept: Woodbury Matrix Identity**
  - Why needed here: This mathematical identity is the engine of the Knowledge Encoding mechanism, allowing the inversion of an ever-growing matrix without recomputing from scratch.
  - Quick check question: How does the Woodbury identity allow you to update (A + UCV)⁻¹ if you only know A⁻¹ and small matrices U, C, V?

- **Concept: Class-Incremental Learning (CIL)**
  - Why needed here: The paper operates in the CIL setting where task boundaries are unknown at inference time. Understanding the difference between task-agnostic and task-aware inference is critical for evaluating the results.
  - Quick check question: In a CIL evaluation, how does the model handle the output head when new classes are introduced in task t+1?

## Architecture Onboarding

- **Component map:** Input -> Positional Embeddings -> Prompt Module -> Frozen ViT Backbone -> [CLS] Token -> Nonlinear Random Projection -> Linear Classifier
- **Critical path:** The stability of the KEM update (Eq. 11) is the system bottleneck. If the regularization parameter γ is too small, matrix inversion instability will crash the training loop.
- **Design tradeoffs:**
  - Population Size (K) vs. Speed: Higher K in CMA-ES improves prompt quality but increases forward passes linearly
  - NRP Dimension (M) vs. Memory: Increasing M improves accuracy but increases the size of the KEM (R ∈ ℝ^(M×M)), squaring memory usage
- **Failure signatures:**
  - CMA-ES Stagnation: Fitness plateaus early; suggests the prompt initialization is too far from optimal or population size is too small for the dimension
  - Catastrophic Forgetting (High F): If the "Activation Discrepancy" term in the fitness function is weighted too low (λ), the prompt overfits the current task, destroying general features
- **First 3 experiments:**
  1. Sanity Check (CMA-ES vs. SGD): Run FoRo on a single task. Optimize prompts via CMA-ES vs. standard Gradient Descent. Verify that CMA-ES can converge to a comparable fitness score without gradients.
  2. KEM Stability Test: On a sequence of 2 tasks, verify that the classifier weights W₂ obtained via recursive update (Eq. 16) are mathematically equivalent (within floating point error) to weights obtained by training on Task 1 + Task 2 data jointly.
  3. Ablation on Discrepancy: Run the full pipeline on CIFAR-100 with λ = 0 (removing activation discrepancy). Quantify the rise in forgetting rate to validate the mechanism claims.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can FoRo be effectively extended to multi-modal continual learning tasks involving video or audio data?
- Basis in paper: The abstract states that the results suggest FoRo could serve as a promising direction for "real-world multimedia applications where both efficiency and effectiveness are critical."
- Why unresolved: The current experiments are restricted to image classification benchmarks, and the method relies on specific mechanisms like [CLS] tokens from Vision Transformers, which may not translate directly to other modalities without modification.
- What evidence would resolve it: Experimental results applying FoRo to video or audio continual learning benchmarks, demonstrating that the forward-only gradient-free approach maintains efficiency and accuracy in temporal or multi-modal feature spaces.

### Open Question 2
- Question: How does the cubic complexity of the covariance matrix update in CMA-ES impact the scalability of FoRo when applied to larger pre-trained backbones?
- Basis in paper: Section 4.3 notes that the time complexity for updating the covariance matrix is approximately O(D₁³), where D₁ is the embedding dimension (e.g., 768 for ViT-B/16).
- Why unresolved: While efficient for the tested ViT-Base model, the authors do not evaluate FoRo on larger architectures where the embedding dimension—and thus the computational cost of the covariance update—increases substantially.
- What evidence would resolve it: A performance and runtime analysis of FoRo on larger foundation models to verify if the gradient-free optimization remains computationally feasible compared to backpropagation as dimensionality scales.

### Open Question 3
- Question: What causes the performance degradation in the Nonlinear Random Projection (NRP) module when the projection size becomes excessively large?
- Basis in paper: Section 5.5.3 observes that increasing the NRP size beyond 8,000 to 10,000 or 15,000 results in marginal performance degradation, which the authors hypothesize is due to "redundancy or noise in the projected feature space."
- Why unresolved: The paper identifies the phenomenon but does not provide an analysis of the spectral properties or the signal-to-noise ratio of the features to confirm the hypothesis.
- What evidence would resolve it: An analysis of the condition number or feature rank of the Knowledge Encoding Matrix (KEM) at varying NRP sizes to determine if numerical instability or feature correlation is the limiting factor.

## Limitations
- The method's reliance on a frozen backbone limits its applicability to scenarios where fine-tuning might be beneficial
- Performance depends critically on CMA-ES hyperparameters (population size, generations) that are underspecified
- KEM update stability under realistic feature distributions is not fully demonstrated
- Scalability to larger backbone architectures with higher dimensional embeddings is untested

## Confidence

- **High confidence:** The mathematical framework for the Knowledge Encoding Matrix update (Woodbury identity application) is sound and well-established
- **Medium confidence:** The forward-only design constraint is successfully achieved, as evidenced by the absence of gradient updates. The reported accuracy and forgetting metrics are plausible given the methodology
- **Low confidence:** The specific contribution of the activation discrepancy term in the CMA-ES fitness function to mitigating forgetting is not independently validated. The scalability of CMA-ES to higher-dimensional prompt spaces is not addressed

## Next Checks

1. **CMA-ES Convergence Analysis:** Run FoRo on a single task and plot the fitness value over CMA-ES generations. Verify that the fitness plateaus, indicating convergence, and that the final fitness is comparable to what would be achieved via gradient descent on the same prompt tuning task.

2. **KEM Numerical Stability Test:** On a sequence of 3 tasks with known feature distributions (e.g., synthetic Gaussian data), monitor the condition number of the KEM matrix R_t after each update. If the condition number grows excessively, implement a check for numerical instability and report the maximum condition number observed.

3. **Activation Discrepancy Ablation:** Run the full CIFAR-100 experiment with three values of λ in the fitness function: λ = 0 (no discrepancy), λ = 0.3 (as reported), and λ = 1.0 (strong discrepancy). Quantify the forgetting rate F for each setting to demonstrate the impact of this mechanism.