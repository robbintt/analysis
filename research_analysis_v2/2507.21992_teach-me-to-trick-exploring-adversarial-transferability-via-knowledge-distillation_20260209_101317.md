---
ver: rpa2
title: 'Teach Me to Trick: Exploring Adversarial Transferability via Knowledge Distillation'
arxiv_id: '2507.21992'
source_url: https://arxiv.org/abs/2507.21992
tags:
- adversarial
- student
- teacher
- attacks
- transferability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study explores using knowledge distillation (KD) from multiple\
  \ heterogeneous teacher models to improve the generation of transferable adversarial\
  \ examples. Two KD strategies\u2014curriculum-based switching and joint optimization\u2014\
  are used to train a lightweight student model with ResNet-50 and DenseNet-161 as\
  \ teachers."
---

# Teach Me to Trick: Exploring Adversarial Transferability via Knowledge Distillation
## Quick Facts
- arXiv ID: 2507.21992
- Source URL: https://arxiv.org/abs/2507.21992
- Reference count: 8
- Primary result: Multi-teacher knowledge distillation enables lightweight student models to generate adversarial examples with black-box transferability comparable to ensemble baselines while reducing generation time by up to 6×.

## Executive Summary
This study investigates using knowledge distillation (KD) from multiple heterogeneous teacher models to enhance the transferability of adversarial examples against black-box targets. By training a lightweight ResNet-18 student model with ResNet-50 and DenseNet-161 as teachers, the authors demonstrate that multi-teacher KD achieves attack success rates (ASR) comparable to ensemble-based baselines. Two KD strategies—curriculum-based switching and joint optimization—are evaluated, with the student generating adversarial examples via FG, FGS, and PGD attacks. The results show that lower temperature settings and hard-label supervision significantly improve transferability, while also reducing adversarial generation time by up to 6× compared to ensemble methods.

## Method Summary
The method involves training a lightweight ResNet-18 student model using knowledge distillation from two heterogeneous teacher models (ResNet-50 and DenseNet-161) pretrained on CIFAR-10. Two KD strategies are employed: curriculum-based switching (teacher changes every 4 epochs) and joint optimization (simultaneous multi-teacher loss). The student is trained with a loss function combining soft-target distillation (controlled by temperature τ) and hard-label cross-entropy (weighted by α). Adversarial examples are generated on the student using FG, FGS, and PGD attacks and evaluated against a black-box GoogLeNet target. The study systematically ablates temperature and hard-label parameters to optimize transferability.

## Key Results
- Multi-teacher KD student models achieve attack success rates comparable to ensemble baselines while reducing adversarial generation time by up to 6×.
- Lower temperature settings (τ=1) and inclusion of hard-label supervision (α=0.3) significantly enhance transferability.
- Curriculum-based KD and joint optimization both produce effective student models, with joint optimization showing slightly better efficiency.

## Why This Works (Mechanism)
### Mechanism 1
- Claim: Multi-teacher knowledge distillation produces a student model whose local decision boundaries align with the black-box target, improving single-step attack transferability.
- Mechanism: By distilling from heterogeneous teachers (ResNet-50, DenseNet-161), the student learns an "averaged" decision surface. This geometric center increases the chance that gradient directions useful for attacking the student also mislead architecturally distinct targets like GoogLeNet.
- Core assumption: The black-box model's decision boundaries share local geometric similarities with the convex combination of teacher boundaries.
- Evidence anchors:
  - [abstract]: "student models trained via multi-teacher KD achieve attack success rates comparable to ensemble-based baselines"
  - [section 5.1, Figure 3b]: "locally... the student's red contour nearly coincides with the black-box (orange) and ensemble (purple) contours within a few pixels of the origin"
  - [corpus]: Limited direct support; "Improving the Transferability of Adversarial Examples by Inverse Knowledge Distillation" explores related distillation-transferability connections but with different framing.
- Break condition: If the black-box target architecture is fundamentally dissimilar from all teachers (e.g., vision transformer vs. CNNs), local boundary alignment may degrade.

### Mechanism 2
- Claim: Lower distillation temperature (τ=1) preserves sharper decision-boundary information, enabling better gradient alignment for transfer attacks.
- Mechanism: Higher temperatures over-soften teacher outputs, flattening probability distributions and obscuring fine-grained inter-class relationships. Lower temperatures retain discriminative gradients that align better with black-box model gradients.
- Core assumption: Transferability depends on the student learning sufficiently precise boundary geometry, not just coarse class relationships.
- Evidence anchors:
  - [abstract]: "lower temperature settings... significantly enhance transferability"
  - [section 4.2]: "increasing the temperature from τ = 1 to 5 consistently leads to worse attack performance across all metrics... overly softened outputs may have obscured decision boundaries"
  - [corpus]: No direct corroboration found for temperature-transferability link in adversarial settings.
- Break condition: If the task requires capturing only coarse semantic relationships (e.g., multi-class grouping), higher temperatures might not harm—and could even help—transferability.

### Mechanism 3
- Claim: Hard-label supervision (α=0.3) regularizes the student, preventing overfitting to teacher soft targets and improving generalization of adversarial gradients.
- Mechanism: Pure soft-target distillation can cause the student to inherit teacher-specific artifacts or noise. Hard-label cross-entropy anchors the student to ground truth, yielding more robust decision boundaries whose gradients transfer better.
- Core assumption: Teacher outputs are occasionally misaligned with ground truth; hard labels provide corrective signal.
- Evidence anchors:
  - [section 2.2]: "teacher outputs may occasionally be unreliable or misaligned with the ground truth, the student is further regularized using a supervised hard-label loss"
  - [section 4.2]: "incorporating a hard-label component by setting α = 0.3... improves transferability in most settings"
  - [corpus]: Weak support; DARD mentions adversarial robustness distillation but doesn't isolate hard-label effects.
- Break condition: If teachers are highly accurate and diverse, hard-label regularization may provide diminishing returns or slightly reduce transferability by over-constraining the student.

## Foundational Learning
- Concept: Knowledge Distillation (soft targets, temperature τ)
  - Why needed here: Understanding how soft targets transmit inter-class relationships and how temperature controls distribution sharpness is essential for tuning transfer attacks.
  - Quick check question: If τ → ∞, what happens to the softmax output distribution? (It becomes uniform.)

- Concept: Adversarial Transferability
  - Why needed here: The entire method relies on adversarial examples crafted on the student transferring to the black-box target without access to its gradients.
  - Quick check question: Why does transferability enable black-box attacks? (Attacker trains substitute model, crafts perturbations that generalize to unseen target.)

- Concept: First-order Adversarial Attacks (FGS, FG, PGD)
  - Why needed here: The paper evaluates transferability using these standard attacks; understanding their update rules clarifies why local vs. global boundary alignment matters.
  - Quick check question: What is the key difference between FGS (one-step) and PGD (iterative) in terms of exploration range? (PGD traverses farther from the origin via multiple steps.)

## Architecture Onboarding
- Component map: ResNet-50, DenseNet-161 (teachers) -> ResNet-18 (student) -> GoogLeNet (black-box target)
- Critical path:
  1. Pretrain teachers on target dataset
  2. Train student via KD (select strategy, set τ, α)
  3. Generate adversarial examples on student using FG/FGS/PGD
  4. Evaluate attack success rate (ASR) on black-box GoogLeNet

- Design tradeoffs:
  - Curriculum vs. Joint: Curriculum may offer more stable training; Joint enforces simultaneous alignment but risks gradient conflict.
  - τ=1 vs. τ=5: Lower τ preserves boundary sharpness; higher τ may improve generalization for non-adversarial tasks but harms transferability here.
  - α=0 vs. α=0.3: Pure soft targets may overfit to teachers; adding hard labels regularizes but may reduce student-teacher alignment.

- Failure signatures:
  - Student ASR significantly below ensemble baseline under PGD → student failed to capture global boundary geometry (Section 5.1, Figure 3a).
  - High τ (e.g., 5) leading to low ASR across all attacks → over-softened outputs obscured decision boundaries.
  - Curriculum training with too-frequent teacher switching → unstable training; too-infrequent → catastrophic forgetting.

- First 3 experiments:
  1. Replicate ablation: Train student with τ∈{1,5}, α∈{0,0.3} and measure ASR on black-box target for FG, FGS, PGD.
  2. Compare KD strategies: Train separate students via curriculum vs. joint optimization; record ASR and PGD generation time.
  3. Boundary visualization: For a sample input, plot decision boundaries of student, teachers, ensemble, and black-box along gradient and orthogonal directions (Figure 3) to verify local alignment.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does incorporating intermediate feature matching into multi-teacher distillation enhance the transferability of iterative attacks (e.g., PGD) by better capturing global decision boundaries?
- Basis in paper: [explicit] Section 6 notes that logit-level distillation fails to capture global decision geometry, and Section 3.4 mentions that intermediate representation matching was excluded due to architectural mismatches.
- Why unresolved: The current logit-based approach captures local boundaries well (aiding FG/FGS) but falls short on global structures required for robust PGD transferability; it is untested whether feature alignment solves this without architectural conflicts.
- What evidence would resolve it: Experiments utilizing projection adapters to align ResNet/DenseNet feature maps, showing improved PGD success rates against the black-box target compared to logit-only distillation.

### Open Question 2
- Question: Can the efficiency and transferability of the KD student model be maintained when scaling to high-resolution datasets (e.g., ImageNet) or against adversarially defended black-box models?
- Basis in paper: [explicit] Section 6 (Limitations) explicitly states that validation was restricted to CIFAR-10 and a single GoogLeNet target, noting results may differ on higher-resolution benchmarks or other architectures.
- Why unresolved: The complexity of decision boundaries increases significantly with higher-resolution data and defense mechanisms (e.g., adversarial training), potentially disrupting the "local boundary alignment" observed in this study.
- What evidence would resolve it: Replicating the curriculum and joint KD training protocols on ImageNet and evaluating attack success rates against defended models (e.g., RobustBench leaderboards).

### Open Question 3
- Question: What is the impact of increasing the number of heterogeneous teachers or employing adaptive distillation schedules on the "geometric centering" of the student model?
- Basis in paper: [explicit] Section 6 identifies the use of only two teachers and fixed distillation schedules as a limitation, while Section 3.1.1 hypothesizes that joint optimization settles the student in the "geometric center."
- Why unresolved: It is unclear if adding more teachers pushes the student further toward a universally transferable gradient or causes optimization conflicts, nor if dynamic weighting outperforms the fixed 4-epoch switching.
- What evidence would resolve it: Ablation studies varying the teacher pool size (e.g., 3-5 models) and implementing loss-weight schedulers to observe shifts in decision boundary visualizations and ASR.

## Limitations
- The study is limited to CIFAR-10 and a single black-box target (GoogLeNet), leaving scalability to high-resolution datasets and other architectures untested.
- The core assumption that the black-box target shares local decision boundary geometry with the ensemble of teachers may not hold for architecturally dissimilar targets (e.g., vision transformers).
- Attack hyperparameters (ε for FG/FGS, PGD step-size) are not explicitly provided, requiring reverse-engineering to reproduce results.

## Confidence
- **High**: Multi-teacher KD achieves comparable ASR to ensemble baselines while reducing generation time (supported by quantitative comparisons in Section 5.1 and runtime metrics).
- **Medium**: Lower temperature (τ=1) preserves boundary sharpness for better transferability (supported by ablation in Section 4.2, but mechanism is theoretical).
- **Medium**: Hard-label supervision (α=0.3) regularizes the student and improves generalization (supported by ASR improvements in Section 4.2, but cross-dataset robustness is not tested).

## Next Checks
1. **Boundary Alignment Verification**: For a sample input, plot decision boundaries of the student, teachers, ensemble, and black-box target along gradient and orthogonal directions (Figure 3) to confirm local geometric alignment.
2. **Cross-Architecture Transferability**: Test the student's adversarial examples against a vision transformer (e.g., ViT) to assess whether the geometric assumptions hold for non-CNN black-box targets.
3. **Hyperparameter Sensitivity**: Systematically vary τ and α to map their effects on ASR and runtime, confirming that τ=1 and α=0.3 are optimal across attack types and datasets.