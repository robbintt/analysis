---
ver: rpa2
title: 'BitTTS: Highly Compact Text-to-Speech Using 1.58-bit Quantization and Weight
  Indexing'
arxiv_id: '2506.03515'
source_url: https://arxiv.org/abs/2506.03515
tags:
- size
- weight
- quantization
- speech
- quantized
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of reducing model size in text-to-speech
  (TTS) systems for on-device applications with limited memory and storage. The authors
  propose BitTTS, which combines quantization-aware training (QAT) with a novel weight
  indexing method.
---

# BitTTS: Highly Compact Text-to-Speech Using 1.58-bit Quantization and Weight Indexing

## Quick Facts
- arXiv ID: 2506.03515
- Source URL: https://arxiv.org/abs/2506.03515
- Reference count: 0
- Achieves 83% model size reduction with minimal quality loss

## Executive Summary
BitTTS addresses the challenge of deploying text-to-speech models on resource-constrained devices by combining quantization-aware training (QAT) with a novel weight indexing method. The approach quantizes model parameters to ternary values (-1, 0, 1), achieving 1.58-bit precision, while weight indexing stores groups of these weights as single 8-bit integers to overcome hardware limitations. Experimental results demonstrate that BitTTS achieves significant model size reduction while maintaining synthesis quality comparable to baseline models.

## Method Summary
The method combines quantization-aware training with weight indexing to compress TTS models. QAT quantizes weights to ternary values during training using a straight-through estimator for gradient approximation. Weight indexing groups 5 ternary weights into a single 8-bit integer, achieving near-optimal storage efficiency. The approach selectively quantizes the acoustic model while preserving the vocoder in higher precision to maintain audio quality.

## Key Results
- Achieves 83% reduction in model size through combined quantization and indexing
- Outperforms baseline models of similar size in synthesis quality (MOS scores)
- 70% size reduction when only acoustic model is quantized with minimal quality impact

## Why This Works (Mechanism)

### Mechanism 1: Quantization-Aware Training Preserves Quality Through Gradient Approximation
Training with quantization in the loop allows the model to adapt to low-precision weights, mitigating accuracy loss compared to post-training quantization. During forward pass, weights are scaled by their mean absolute value (β) and rounded to ternary values {-1, 0, 1}. During backpropagation, the straight-through estimator approximates gradients through non-differentiable rounding/clipping operations, allowing the model to learn representations that remain robust after quantization.

### Mechanism 2: Weight Indexing Achieves Near-Optimal Storage Efficiency
Grouping 5 ternary weights into a single 8-bit index approaches the theoretical 1.58-bit-per-weight storage limit on 8-bit-constrained hardware. Since 3^5 = 243 ≤ 256 = 2^8, every group of 5 ternary weights maps uniquely to an 8-bit integer representing a position in a virtual lookup table of 243 patterns. At inference, indices are decoded back to weight tensors via table lookup.

### Mechanism 3: Asymmetric Module Sensitivity Enables Pragmatic Quantization Strategy
The vocoder is more sensitive to quantization than the acoustic model; selective quantization yields better quality-size tradeoffs. The vocoder directly generates waveform samples and requires higher numerical precision for audio fidelity. The acoustic model operates on higher-level features (mel-spectrograms, durations) where quantization noise is more tolerable. Excluding the final vocoder convolution layer preserves output quality.

## Foundational Learning

- Concept: Quantization-Aware Training (QAT) vs. Post-Training Quantization (PTQ)
  - Why needed here: The paper builds on QAT as the core technique; understanding why PTQ fails at low bit-widths motivates the training-time approach.
  - Quick check question: Can you explain why rounding weights after training (PTQ) causes more degradation than rounding during training (QAT)?

- Concept: Ternary Weight Representation {-1, 0, 1}
  - Why needed here: 1.58-bit quantization means ternary values; understanding log₂(3) ≈ 1.58 is essential for grasping storage calculations.
  - Quick check question: Why does ternary quantization require ~1.58 bits per weight in theory, and why can't hardware store this directly?

- Concept: Straight-Through Estimator (STE)
  - Why needed here: Backpropagation through discrete rounding operations is non-differentiable; STE is the standard workaround.
  - Quick check question: What does STE assume about the gradient of a rounding function, and when might this assumption fail?

## Architecture Onboarding

- Component map:
  - Acoustic Model -> Modified JETS architecture (4 conv layers, kernel=5, channels: encoder=256, decoder=192)
  -> Duration predictor with MDN layer
  - Vocoder -> HiFi-GAN (initial channel=128)
  - Quantization Layer -> Scaling (β = mean |W|), clipping to {-1, 0, 1}, layer norm on input, rescaling on output
  - Weight Indexing Module -> Groups 5 weights → int8 index; stores indices instead of weights; reconstructs at inference via pattern lookup

- Critical path:
  1. Implement QAT forward pass: scale weights by β, round/clip, apply conv, rescale output
  2. Implement STE backward pass: gradients pass through rounding as identity
  3. Implement weight indexing: encode weights to indices post-training, decode at inference start
  4. Identify which layers to quantize: all 1D convs except final vocoder layer

- Design tradeoffs:
  - Quantize both acoustic model + vocoder → maximum size reduction (83%) but lower MOS
  - Quantize acoustic model only → 70% size reduction with higher MOS; recommended default
  - 1.58-bit vs 4-bit: 1.58-bit with indexing achieves smaller size with comparable/better MOS than 4-bit without indexing

- Failure signatures:
  - MOS drops significantly (>0.5 points) → check if vocoder was over-quantized; try acoustic-only
  - Model size not reducing as expected → verify weight indexing is applied; check if non-conv layers dominate parameters
  - Training instability → β near zero causes division issues; verify ε is set appropriately
  - Inference slower than expected → weight reconstruction adds overhead; pre-compute decoded weights if memory allows

- First 3 experiments:
  1. Replicate baseline: Train 32-bit model, measure MOS and size; establish reference
  2. Ablate quantization scope: Compare acoustic-only vs. vocoder-only vs. both; quantify quality-size tradeoff
  3. Validate weight indexing: Compare stored model size with vs. without indexing; measure decode latency overhead

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the weight indexing method be effectively generalized to other neural network architectures beyond the specific convolution-based TTS model tested?
- Basis in paper: [explicit] The conclusion explicitly states that future work includes "applying weight indexing to other models."
- Why unresolved: The current study validated the method primarily on a modified JETS architecture where transformer blocks were replaced with convolutional layers, leaving its efficacy on standard attention-based or recurrent architectures unconfirmed.
- What evidence would resolve it: Successful application of BitTTS quantization and indexing to Transformer-based TTS (or non-speech models) with comparable size reductions and minimal accuracy loss.

### Open Question 2
- Question: How can the non-uniform distribution of weight indices be exploited to achieve further model compression?
- Basis in paper: [explicit] Section 3.3.3 notes that indices are "biased towards specific values" and states, "Future work will explore methods to further reduce the model size by leveraging this bias in the occurrence frequency of indices."
- Why unresolved: The current implementation stores indices as fixed 8-bit integers, which does not take advantage of the high frequency of certain patterns (like all-zeros or all-ones).
- What evidence would resolve it: A modified encoding scheme (e.g., entropy coding) applied to the weight indices that results in a smaller memory footprint than the current int8 method without degrading inference latency.

### Open Question 3
- Question: Can the training instability and quality degradation associated with quantizing the vocoder be mitigated?
- Basis in paper: [explicit] The paper notes that "instability in vocoder training due to quantization" is a possible reason for the MOS gap and that "quantization of the vocoder significantly affects the quality."
- Why unresolved: The current solution involves excluding the vocoder (or its final layers) from quantization to preserve quality, which limits the maximum achievable model compression.
- What evidence would resolve it: A training strategy or architectural modification that allows the vocoder to be fully quantized to 1.58-bit without the "significant MOS gap" observed in the experiments.

## Limitations

- Quality retention at 1.58-bit precision not validated against full-size 32-bit baseline
- No theoretical analysis supporting asymmetric quantization sensitivity between modules
- Weight indexing assumes 8-bit storage units, which may not hold across all deployment platforms

## Confidence

- **High confidence**: Weight indexing achieves near-optimal storage efficiency (83% reduction) through 5-ternary-to-1-byte mapping
- **Medium confidence**: QAT preserves synthesis quality at 1.58-bit precision; vocoder is more quantization-sensitive than acoustic model
- **Low confidence**: MOS improvements are specifically due to the 1.58-bit scheme rather than general QAT benefits; results generalize across TTS architectures

## Next Checks

1. **Ablation study**: Train a 32-bit full-size baseline (matching JETS+Hifi-GAN) and compare MOS against BitTTS to isolate quantization benefits from architecture improvements.

2. **Cross-architecture validation**: Apply weight indexing and QAT to a different TTS architecture (e.g., Tacotron 2 + WaveRNN) to test generalizability of module-specific quantization sensitivity.

3. **Hardware deployment test**: Deploy the quantized model on target edge hardware (e.g., mobile ARM CPU) to measure actual storage reduction and verify the 8-bit storage assumption holds in practice.