---
ver: rpa2
title: 'Biasless Language Models Learn Unnaturally: How LLMs Fail to Distinguish the
  Possible from the Impossible'
arxiv_id: '2510.07178'
source_url: https://arxiv.org/abs/2510.07178
tags:
- languages
- language
- impossible
- learning
- reverse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether large language models (LLMs) share
  the human learning biases that shape linguistic typology, specifically whether they
  can distinguish between humanly possible and impossible languages. Previous work
  claimed that LLMs prefer possible languages over impossible ones based on their
  learning curves, but this was tested only on English.
---

# Biasless Language Models Learn Unnaturally: How LLMs Fail to Distinguish the Possible from the Impossible

## Quick Facts
- arXiv ID: 2510.07178
- Source URL: https://arxiv.org/abs/2510.07178
- Reference count: 40
- Primary result: GPT-2 learns possible and impossible language variants equally easily, contradicting claims that LLMs share human linguistic biases

## Executive Summary
This study investigates whether large language models share human learning biases that distinguish possible from impossible languages. Using GPT-2 trained on nine languages and six types of perturbations designed to create impossible variants, the authors measure learning difficulty via validation perplexity curves. Contrary to prior claims, they find that in most cases, GPT-2 learns possible and impossible variants equally easily, often preferring the impossible ones. The interlinguistic perspective shows natural languages pattern with their impossible counterparts, revealing no systematic separation. This suggests LLMs lack the innate biases that shape human linguistic typology, undermining their use as cognitive models of human language learning.

## Method Summary
The authors train GPT-2 Small (12-layer, 12-head, 768-dim) from scratch on nine languages (Danish, Finnish, French, German, Greek, Hebrew, Italian, Russian, English) and their six perturbed variants each. They use Wikipedia dumps tokenized with TreeTagger, filtered to 90M tokens with ≤5% unknown words. Perturbations include SHUFFLE global/local, REVERSE partial/full, SWITCH, and HOP (inserts markers after verbs). Models train for 3,000 batches with batch size 512, learning rate 0.0005, weight decay 0.01, logging validation perplexity every 100 batches. Learning difficulty is measured via Mean Error between baseline/perturbed pairs, minimum perplexity, AUC, and variance comparisons.

## Key Results
- In 28 out of 54 test cases, GPT-2 showed no preference for natural over impossible languages (p=0.892, chance-level performance)
- Natural languages pattern with their impossible counterparts in variance analysis, showing no systematic separation
- GPT-2 often prefers impossible variants over natural ones, learning them equally easily
- Results contradict prior claims that LLMs prefer possible languages based on English-only experiments

## Why This Works (Mechanism)

### Mechanism 1
Validation perplexity during small-scale training serves as a proxy for detecting model's inductive bias toward natural language structure. By training GPT-2 on both natural and perturbed "impossible" variants, learning curves operationalize "ease of learning." Lower perplexity on natural language would suggest architectural biases similar to humans. This fails if perturbations create novel surface statistics that mask destruction of deeper structure, as perplexity curves may not detect specific hierarchical violations.

### Mechanism 2
The intra-linguistic perspective uses pairwise comparison to reveal if model innately prefers specific language over impossible counterpart. For each language, GPT-2 trains on natural corpus and six perturbed variants. Mean Error quantifies average difference between perplexity curves. Negative ME indicates model learns natural variant more easily. This fails if perturbations aren't "impossible" enough for statistical learners—for example, SHUFFLE local preserves local n-gram statistics that models exploit despite destroying constituent structure.

### Mechanism 3
The inter-linguistic perspective tests global bias by comparing variance across all natural languages to variance within single language and perturbed variants. Analysis aggregates learning metrics and compares across-language variance (difference between French and German) to within-language variance (difference between French and French-SHUFFLED). If natural languages formed distinct low-perplexity cluster from impossible languages, within-language variance should be high. This fails if extrinsic factors like tokenization quality dominate perplexity differences.

## Foundational Learning

**Validation Perplexity**
Why needed: This single metric measures learning difficulty and model preference. Understanding lower perplexity = better model fit is essential.
Quick check: If model A achieves perplexity of 100 on a dataset and model B achieves 200, which model assigns higher probability to the data?

**Inductive Bias**
Why needed: The entire study tests presence or absence of inductive biases in GPT-2 corresponding to human linguistic constraints.
Quick check: Why might a model trained from scratch on small, noisy dataset still learn useful pattern that different architecture fails to learn?

**The "Possible vs. Impossible" Distinction**
Why needed: This core theoretical construct defines boundary between languages humans can learn and those they cannot, based on universal grammar constraints.
Quick check: According to paper, how do SHUFFLE and REVERSE perturbations create "impossible" language?

## Architecture Onboarding

**Component map:**
GPT2LMHeadModel (12/12/768/1024) -> TreeTagger tokenization -> 6 perturbation functions -> 3,000 batch training -> validation perplexity logging -> Mean Error/AUC/variance analysis

**Critical path:**
1. Data Pipeline: Download Wikipedia dumps, tokenize with TreeTagger, filter >5% unknown words, sample 90M tokens per language
2. Perturbation Application: Run 6 perturbation functions on each dataset, creating 54 impossible datasets plus baselines
3. Training Loop: Train fresh GPT-2 on each dataset for 3,000 batches, logging validation perplexity every 100 batches
4. Analysis: Compute ME per pair, minimum perplexity, AUC, and variance metrics

**Design tradeoffs:**
- Model Scale vs. Breadth: GPT-2 Small with limited training allows massive cross-linguistic study but may not reflect larger models' behavior
- Perturbation Granularity: Coarse SHUFFLE/REVERSE perturbations clearly create "impossible" languages but may mask subtle structural sensitivities

**Failure signatures:**
- Non-convergent loss indicating model fails to learn anything
- No difference in perplexity curves between natural and impossible variants
- Clear perplexity differences driven by trivial factors (dataset size) rather than structural properties

**First 3 experiments:**
1. Sanity Check with English: Replicate single condition (English vs. English-SHUFFLE local) to verify pipeline produces reported perplexity curves
2. Extended Training: Take borderline case and train 10x longer to see if separation emerges or null result is robust
3. Architectural Swap: Replace GPT-2 with RNN or BERT on single language pair to test if insensitivity is specific to GPT-2 architecture

## Open Questions the Paper Calls Out

**Open Question 1**
Do findings generalize to state-of-the-art models with different architectures or larger scales? The authors note experiments rely on GPT-2 and acknowledge results may not extend straightforwardly to state-of-the-art models. This remains unresolved because study restricted scope to GPT-2 to replicate prior methodology. Evidence would come from replicating methodology on larger models (LLaMA, GPT-4) or alternative architectures.

**Open Question 2**
Can embedding LLM learning components within cultural evolution model reveal learning biases that individual perplexity metrics miss? Authors suggest embedding learning component within cultural evolution model where small asymmetries may amplify across generations. Unresolved because current methodology measures static learning ease via perplexity, failing to account for transmission chains that might amplify weak biases. Evidence would come from simulating iterated learning across generations using LLMs.

**Open Question 3**
Would broader set of linguistic perturbations and phenomena reveal systematic distinction between possible and impossible languages? Authors acknowledge experiments rely on limited number of perturbations and languages, suggesting applying methodology to larger set of linguistic phenomena. Unresolved because specific perturbations used may not capture full nuance of linguistic impossibility, potentially masking subtle sensitivities. Evidence would come from testing wider variety of impossible perturbations and more diverse natural languages.

## Limitations

- Study relies on coarse perturbations (SHUFFLE, REVERSE) that may create learnable surface statistics despite destroying deeper structure
- Results may not generalize beyond GPT-2 Small to larger models or different architectures
- Variance-based interlinguistic analysis lacks robust corpus validation for this specific analytical approach
- Tokenization quality differences across languages could confound perplexity measurements

## Confidence

**Core finding that LLMs don't distinguish possible from impossible languages (High confidence)**
- Based on extensive empirical data across 9 languages and 6 perturbation types
- Statistical analysis shows p=0.892 for chance-level performance

**Claims about human learning biases (Medium confidence)**
- Empirical results are robust but connection to human cognitive biases requires theoretical assumptions about "impossible" languages

**Claims about variance-based interlinguistic analysis (Low confidence)**
- Novel analytical approach lacking independent validation
- Finding that variants cluster by language identity rather than naturalness is compelling but methodologically less established

## Next Checks

1. **Sanity Check with Extended Training**: Select borderline language-perturbation pair and extend training to 30,000 batches to determine if perplexity separation eventually emerges or if null result is robust to training duration.

2. **Architectural Swap Experiment**: Replace GPT-2 with fundamentally different architecture (RNN or BERT) on single language pair to test whether insensitivity to impossibility is specific to GPT-2 architecture or represents broader limitation of statistical language models.

3. **Perturbation Granularity Test**: Implement more subtle perturbation targeting specific linguistic constraints (syntactic movement constraints) rather than global structure disruption to determine if LLMs can detect refined structural violations humans perceive as impossible.