---
ver: rpa2
title: 'Survey: Understand the challenges of MachineLearning Experts using Named EntityRecognition
  Tools'
arxiv_id: '2501.16112'
source_url: https://arxiv.org/abs/2501.16112
tags:
- tools
- survey
- were
- experts
- challenges
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study surveyed machine learning experts to understand their
  criteria for evaluating and selecting Named Entity Recognition (NER) tools and frameworks.
  Through a comprehensive questionnaire distributed to 23 ML experts, the research
  identified performance as the most critical factor in tool selection.
---

# Survey: Understand the challenges of MachineLearning Experts using Named EntityRecognition Tools

## Quick Facts
- arXiv ID: 2501.16112
- Source URL: https://arxiv.org/abs/2501.16112
- Reference count: 0
- Primary result: Survey of 23 ML experts identifies performance as most critical NER tool selection criterion

## Executive Summary
This study surveyed machine learning experts to understand their criteria for evaluating and selecting Named Entity Recognition (NER) tools and frameworks. Through a comprehensive questionnaire, the research identified performance as the most critical factor in tool selection, while revealing significant variations in expert preferences across different project contexts. The findings highlight the need for flexible tool selection systems that can accommodate varying priorities between cloud-based and locally-installed NER solutions.

## Method Summary
The researchers employed Kasunic survey methodology across 7 stages, targeting ML experts with 3+ years experience or PhDs in ML/NLP/NER. The questionnaire included 3 sections covering general information, experience with NER tools, and final questions. Participants evaluated 9 NER tools and 5 cloud services using 5-point Likert scales for 9 criteria and 6 challenges. The survey achieved a 26% response rate with 23 participants, analyzing results through descriptive statistics and comparisons between cloud-based and locally-installed tools.

## Key Results
- Performance was rated as the most critical selection criterion (average 4.57/5)
- All specified criteria were considered important depending on specific project needs
- Cloud-based services prioritize cost and user-friendliness, while local systems require minimizing learning effort
- Locally operated open-source large language models are identified as relevant for future NER applications

## Why This Works (Mechanism)

### Mechanism 1: Performance-First Selection Heuristic
- Claim: ML experts prioritize performance above all other criteria when evaluating NER tools.
- Mechanism: Survey respondents rated performance highest (average 4.57/5) across all tools, with all other criteria showing context-dependent importance.
- Core assumption: Users can meaningfully compare performance across tools when evaluation datasets are consistent.
- Evidence anchors:
  - [abstract] "identified performance as the most critical factor in tool selection"
  - [section 5.2] "performance (average 4.57) as the most critical factor. The consistently high prioritization of performance underscores the need for NER tools to deliver accurate and reliable results"
  - [corpus] Related NER papers (KoGNER, OpenNER 1.0) focus heavily on benchmark performance improvements
- Break Condition: When heterogeneous datasets make cross-tool performance comparisons unreliable.

### Mechanism 2: Deployment Context Modulates Selection Criteria
- Claim: Cloud-based vs. locally-installed tools have systematically different priority weights.
- Mechanism: Cloud services showed higher priority for user interface (Delta +1.09) and cost (Delta +1.79); local systems prioritized documentation/support (Delta -1.14) and learning effort (Delta -0.57).
- Core assumption: Deployment model creates different friction points that shift user priorities.
- Evidence anchors:
  - [abstract] "cloud-based NER services prioritize cost and user-friendliness, while locally installed systems require minimizing learning effort"
  - [section 5.2, Tables 2-3] Quantified deltas showing cost as top cloud hindrance (3.71 vs 1.93) and learning effort as top local hindrance (3.00 vs 2.43)
  - [corpus] Limited direct corpus evidence on deployment-specific preferences; corpus papers focus primarily on model architecture rather than deployment criteria
- Break Condition: When hybrid or edge deployments blur the cloud/local distinction.

### Mechanism 3: Project-Specific Multi-Criteria Tradeoffs
- Claim: No single criterion is universally unimportant; all specified criteria matter depending on project context.
- Mechanism: Every criterion was rated "important" or "very important" by at least one respondent, with significant variation across experts.
- Core assumption: NER projects have heterogeneous constraints that make universal criteria hierarchies impossible.
- Evidence anchors:
  - [abstract] "significant variations in expert preferences, with all specified criteria being considered important depending on specific project needs"
  - [section 5.2] "In general, it can be concluded that all the criteria were considered relevant. This suggests that the importance of criteria is highly dependent on the specific project"
  - [corpus] Domain-specific NER papers (medical, legal, historical texts) show varying tool selections supporting context-dependence
- Break Condition: When evaluation becomes cognitively overwhelming due to too many criteria.

## Foundational Learning

- Concept: Named Entity Recognition (NER) as Information Extraction
  - Why needed here: Understanding what NER tools do (extract entities like persons, organizations, locations from unstructured text) is prerequisite to evaluating them.
  - Quick check question: Can you explain why NER is harder than regex-based extraction for domain-specific entities like medical terms?

- Concept: Cloud Service Abstraction Levels (IaaS/PaaS/SaaS)
  - Why needed here: Survey explicitly distinguishes cloud-based services (AWS Comprehend, Azure Cognitive Services) from locally-installed frameworks (spaCy, Hugging Face).
  - Quick check question: What level of cloud abstraction would minimize the "time and effort to learn" criterion for a domain expert without ML background?

- Concept: ML Training Paradigms (Supervised/Unsupervised/Semi-supervised)
  - Why needed here: Survey notes supervised learning is "currently the most widely used method" but requires annotated dataâ€”a key constraint for domain experts.
  - Quick check question: Why might semi-supervised learning reduce the barrier for medical domain experts developing NER systems?

## Architecture Onboarding

- Component map: Performance benchmarking module -> Deployment context classifier -> Multi-criteria weighting interface -> Documentation quality assessor -> Cost estimation calculator
- Critical path: Define project constraints (domain, privacy, budget) -> Classify deployment context -> Filter tools by minimum performance threshold -> Apply context-specific secondary criteria -> Generate ranked recommendations with tradeoff visualization
- Design tradeoffs:
  - Comprehensive criteria coverage vs. evaluation simplicity (survey tested 9+ criteria per tool)
  - Performance benchmark standardization vs. domain-specific evaluation
  - Cloud convenience vs. local control/privacy (privacy rated 3.31 average with high variance)
- Failure signatures:
  - Recommending tools without comparable performance data on relevant datasets
  - Ignoring deployment context (e.g., recommending high-cost cloud tools to cost-sensitive users)
  - Over-emphasizing single criteria when project has balanced requirements
  - Missing open-source LLM options for privacy-sensitive applications
- First 3 experiments:
  1. Replicate performance comparison: Test 3-4 tools (spaCy, Hugging Face Transformers, cloud API) on identical domain-specific dataset to quantify performance gaps.
  2. Criteria sensitivity analysis: Vary criterion weights systematically to identify which decisions are stable vs. weight-sensitive.
  3. Open-source LLM baseline: Evaluate self-hosted LLMs (noted in Section 5.2 as emerging technology) against traditional NER tools on same benchmark to validate their inclusion in selection systems.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the NER tool evaluation criteria identified for ML experts differ when adapted for domain experts (e.g., medical professionals) developing Clinical Practice Guidelines?
- Basis in paper: [explicit] The conclusion states, "Future work should focus on adapting these results to the needs of domain experts, enabling them to utilize NER in the development of CPGs." Additionally, Section 5.1 notes the current results are biased toward Computer Science backgrounds and "cannot be directly generalized to domain experts."
- Why unresolved: The survey successfully profiled ML experts but explicitly excluded non-ML domain experts; the transferability of these criteria to medical professionals remains untested.
- What evidence would resolve it: A separate survey or user study involving medical domain experts using the same criteria to evaluate NER tools, followed by a statistical comparison of priorities against the ML expert group.

### Open Question 2
- Question: How can locally operated open-source Large Language Models be effectively integrated into NER tool selection systems?
- Basis in paper: [explicit] The conclusion highlights the "relevance of locally operated open-source large language models" and states they "should be integrated into future software systems." Section 5.2 notes a participant successfully used self-hosted LLMs but highlighted integration challenges.
- Why unresolved: The paper identifies this as a growing trend but did not include these models in the primary list of tools for evaluation by the majority of participants.
- What evidence would resolve it: A comparative study where users evaluate specific open-source LLMs against traditional NER frameworks using the criteria defined in the paper (performance, cost, integration, etc.).

### Open Question 3
- Question: What system architecture is required to support the selection of NER tools based on flexible, project-specific criteria rather than a fixed hierarchy?
- Basis in paper: [explicit] The conclusion states, "A supportive system should be flexible enough to accommodate various criteria along with performance," and notes that "expert opinions varied significantly" with no criteria deemed universally unimportant.
- Why unresolved: The survey identified the need for flexibility but did not design or propose the actual architecture for such a decision-support system.
- What evidence would resolve it: A prototype implementation of a tool selection wizard that weights criteria dynamically based on user input (e.g., cloud vs. local preference), validated against the survey data.

## Limitations
- Small sample size (n=23) may limit generalizability despite 26% response rate
- Homogeneous participant background (85.71% computer science) introduces selection bias
- Static criteria list may have missed emerging or context-specific factors

## Confidence

- **High confidence**: Performance prioritization as the most critical selection factor
- **Medium confidence**: Deployment context systematically modulates selection criteria
- **Low confidence**: All criteria are universally important depending on context

## Next Checks

1. Replicate with domain experts: Conduct a parallel survey targeting medical, legal, and historical domain experts to test whether performance prioritization holds across different user groups.

2. Test criteria stability: Apply systematic weight variation to the identified criteria to determine which decisions are robust versus highly sensitive to changing project constraints.

3. Benchmark open-source LLM integration: Validate the recommendation for locally-operated LLM inclusion by empirically comparing self-hosted models against traditional tools on standardized NER benchmarks across multiple domains.