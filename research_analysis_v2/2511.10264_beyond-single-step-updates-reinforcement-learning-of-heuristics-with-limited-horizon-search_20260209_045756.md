---
ver: rpa2
title: 'Beyond Single-Step Updates: Reinforcement Learning of Heuristics with Limited-Horizon
  Search'
arxiv_id: '2511.10264'
source_url: https://arxiv.org/abs/2511.10264
tags:
- heuristic
- search
- state
- lhbl
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses limitations in reinforcement learning of heuristic
  functions using single-step Bellman updates. The authors propose Limited-Horizon
  Bellman-based Learning (LHBL), which enhances both state sampling and heuristic
  updates by performing limited-horizon searches from sampled states.
---

# Beyond Single-Step Updates: Reinforcement Learning of Heuristics with Limited-Horizon Search

## Quick Facts
- arXiv ID: 2511.10264
- Source URL: https://arxiv.org/abs/2511.10264
- Authors: Gal Hadar; Forest Agostinelli; Shahaf S. Shperberg
- Reference count: 10
- Primary result: LHBL achieves faster training convergence and better heuristic quality than single-step Bellman learning across three puzzle domains

## Executive Summary
This paper addresses limitations in reinforcement learning of heuristic functions using single-step Bellman updates. The authors propose Limited-Horizon Bellman-based Learning (LHBL), which enhances both state sampling and heuristic updates by performing limited-horizon searches from sampled states. Instead of updating heuristics based only on immediate neighbors, LHBL updates each state's heuristic using the shortest path to the search frontier, incorporating both edge costs and the heuristic values of frontier states. The approach is evaluated across three puzzle domains (Rubik's Cube, 35 Sliding Tile Puzzle, and 7×7 Lights Out) against standard single-step Bellman learning.

## Method Summary
LHBL extends DeepCubeA's approach by replacing single-step Bellman updates with limited-horizon search-based updates. The method samples states from the environment, performs limited-horizon searches (N expansions) using a target network heuristic, and computes training labels by finding shortest paths from an auxiliary node to all search graph nodes. The auxiliary node connects to all frontier nodes with edge weights equal to their heuristic values, and edges are reversed to enable Dijkstra's algorithm from the auxiliary node. The network is trained to minimize MSE between its predictions and these computed labels. Search horizons tested include N ∈ {10, 50, 100}.

## Key Results
- LHBL variants consistently solve all test instances earlier than single-step approaches, with particularly strong performance in Rubik's Cube where LHBL(100) solved all instances by just 20% of training
- LHBL reduces node expansions during search, particularly at small batch sizes where heuristic quality matters most
- LHBL mitigates depression regions where heuristics underestimate true costs
- The method demonstrates faster training convergence across all three domains (Rubik's Cube, 35-Puzzle, 7×7 Lights Out)

## Why This Works (Mechanism)

### Mechanism 1: Distribution Alignment Through Search-Based Sampling
- Claim: Training on states sampled via limited-horizon search produces heuristics that generalize better to actual search deployment than uniformly random sampling.
- Mechanism: Search algorithms (A*, GBFS) prioritize nodes with lower f-values, creating a biased state distribution. By training on states from limited-horizon searches, LHBL exposes the neural network to the same distribution it will encounter during inference, reducing train-test distribution shift.
- Core assumption: The training search algorithm (and its λ weighting) approximates the deployment search behavior.

### Mechanism 2: Multi-Edge Ground Truth Integration in Updates
- Claim: Incorporating complete path costs to the search frontier produces more accurate training labels than single-step edge costs.
- Mechanism: Standard SSBL updates use h(s) = c(s,s') + h(s'), where only c(s,s') is exact. LHBL computes h(s) = C(n,ℓ) + h(ℓ), where C(n,ℓ) is the sum of k edge costs to frontier node ℓ. This increases the ratio of known (edge costs) to estimated (heuristic) information per update.

### Mechanism 3: Depression Region Mitigation via Contextual Updates
- Claim: Limited-horizon updates reduce heuristic underestimation in "depression regions" where states appear near-goal but require many moves.
- Mechanism: Depression regions arise when heuristics systematically underestimate costs in localized state-space areas. By expanding multiple steps during training, LHBL discovers the true path length through these regions and propagates accurate values back, rather than relying on immediate neighbors that may also be underestimated.

## Foundational Learning

- Concept: **Bellman Equation / Value Iteration**
  - Why needed here: Both SSBL and LHBL are built on approximate dynamic programming. Understanding that h(s) should equal min[c(s,s') + h(s')] is essential to grasp what LHBL extends.
  - Quick check question: Given a state with two successors having (cost=2, h=5) and (cost=3, h=1), what is the single-step Bellman update?

- Concept: **A* Search and f-values (f = g + h)**
  - Why needed here: LHBL's training searches use the same priority mechanism as deployment. Understanding how heuristic quality affects node expansion order explains why better heuristics reduce search effort.
  - Quick check question: If h(s) is admissible (never overestimates), what guarantee does A* provide?

- Concept: **Target Networks**
  - Why needed here: LHBL uses a target network h_θ⁻ to compute stable frontier heuristic values during search, preventing oscillating updates.
  - Quick check question: Why might using the same network for both generating targets and being updated lead to instability?

## Architecture Onboarding

- Component map: Sampling module -> Limited-horizon search -> Graph transformer -> SSSP solver -> Training loop
- Critical path: Sampling → Search (N expansions) → Graph transformation → Dijkstra → Loss computation → Backprop. The search and SSSP steps are the computational bottlenecks.
- Design tradeoffs:
  - **Horizon N**: Larger N improves label accuracy but increases per-sample compute. Paper tests N ∈ {10, 50, 100}; optimal varies by domain.
  - **Search algorithm during training**: A* vs. GBFS affects state distribution. Paper doesn't specify which was used; GBFS (λ=0) emphasizes heuristic-driven exploration.
  - **Batch size at inference**: Smaller batches depend more on heuristic quality. At batch=1, LHBL shows largest gains over SSBL.
- Failure signatures:
  - **Slow convergence with high variance**: May indicate N too small for domain complexity
  - **Good training loss but poor test performance**: Distribution mismatch—consider matching training search λ to deployment λ
  - **Memory exhaustion during search**: Depression regions still present; increase N or check target network quality
- First 3 experiments:
  1. **Baseline parity**: Replicate SSBL on a simple domain (e.g., 15-puzzle) with identical hyperparameters to DeepCubeA. Verify you can reproduce single-step training behavior.
  2. **Ablation: sampling only (LHBL_S)**: Run limited-horizon search for sampling but use single-step Bellman updates. Isolate the distribution-alignment effect from the multi-edge update effect.
  3. **Horizon sweep**: Test LHBL with N ∈ {5, 10, 25, 50, 100} on a medium domain. Plot training convergence speed vs. per-iteration time to find the efficiency frontier.

## Open Questions the Paper Calls Out
None

## Limitations
- Computational cost scales linearly with horizon N and search algorithm complexity
- The choice between A* and GBFS for training searches significantly affects state distribution and training dynamics
- The assumption that training search behavior approximates deployment behavior may break down if λ values differ substantially between phases

## Confidence
**High Confidence**: The empirical observation that LHBL converges faster than SSBL across all three domains, with statistically significant improvements in solve rates during training. The reduction in node expansions at small batch sizes is directly measurable and well-supported.

**Medium Confidence**: The mechanism claims regarding distribution alignment and depression region mitigation. While the evidence is suggestive (Figure 6 showing flatter heuristic curves), the paper does not conduct ablations that would definitively isolate these effects from other factors like increased label accuracy.

**Low Confidence**: The claim that multi-edge path integration inherently produces better labels than single-step updates. This assumes the target network's frontier heuristics are accurate enough, but the paper does not test scenarios where this assumption fails or compare against other multi-step update strategies.

## Next Checks
1. **Ablation Study**: Implement LHBL_S (limited-horizon search for sampling only, single-step Bellman updates) to isolate the distribution-alignment mechanism from the multi-edge update mechanism. Compare convergence speed and final heuristic quality against both SSBL and full LHBL.

2. **Horizon Sensitivity Analysis**: Systematically test LHBL with horizons N ∈ {5, 10, 25, 50, 100} on a single domain, measuring both training convergence speed and per-iteration computational cost. Identify the optimal efficiency frontier where additional horizon provides diminishing returns.

3. **Target Network Stability Test**: Vary the target network update frequency (e.g., every 100, 1000, 10000 iterations) and measure its impact on training stability and final heuristic quality. This validates whether the claimed stability benefit from using h_θ⁻ is robust across update schedules.