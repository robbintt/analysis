---
ver: rpa2
title: A Flexible Empirical Bayes Approach to Generalized Linear Models, with Applications
  to Sparse Logistic Regression
arxiv_id: '2601.21217'
source_url: https://arxiv.org/abs/2601.21217
tags:
- prior
- posterior
- regression
- variational
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces EBGLM, a flexible empirical Bayes approach
  for Bayesian generalized linear models using variational inference. Unlike traditional
  VI methods, EBGLM directly optimizes posterior means and prior parameters, eliminating
  the need for tuning and enabling scalable optimization with L-BFGS or gradient descent.
---

# A Flexible Empirical Bayes Approach to Generalized Linear Models, with Applications to Sparse Logistic Regression

## Quick Facts
- arXiv ID: 2601.21217
- Source URL: https://arxiv.org/abs/2601.21217
- Authors: Dongyue Xie; Wanrong Zhu; Matthew Stephens
- Reference count: 40
- Key outcome: EBGLM achieves higher AUC scores than lasso, ridge, elastic net, SCAD, MCP, L0Learn, varbvs, and sparsevb across various simulation settings (n=200–3000, p=20–3000, correlation ρ=0–0.99, sparsity s=1–300) and real data benchmarks.

## Executive Summary
This paper introduces EBGLM, a flexible empirical Bayes approach for Bayesian generalized linear models using variational inference. Unlike traditional VI methods, EBGLM directly optimizes posterior means and prior parameters, eliminating the need for tuning and enabling scalable optimization with L-BFGS or gradient descent. The framework is demonstrated on sparse logistic regression, where EBGLM-based methods consistently achieve higher AUC scores compared to state-of-the-art penalized likelihood methods across various simulation settings and real data benchmarks.

## Method Summary
EBGLM uses a variational empirical Bayes approach that optimizes over posterior means θ and prior parameters g instead of full distributions. The method approximates the ELBO using a second-order Taylor expansion of the log-likelihood around the posterior mean, converting the variational inference problem into minimizing a function h(θ, g) that depends on posterior means and prior parameters. The optimal variational posterior for each coefficient is determined through a univariate Bayesian normal means problem with "pseudo-data" z_g(θⱼ) found via Tweedie's formula. The method jointly estimates prior hyperparameters from data, eliminating the need for cross-validation tuning.

## Key Results
- EBGLM-pn (point normal) consistently achieves higher AUC than lasso, ridge, elastic net, SCAD, MCP, L0Learn, varbvs, and sparsevb across all simulation settings
- Performance gains are most pronounced in sparse settings (s=1-20) and under high feature correlation (ρ=0.8-0.99)
- EBGLM demonstrates superior predictive performance on real data benchmarks including 20 Newsgroups
- The method maintains robustness across a wide range of problem dimensions (n=200-3000, p=20-3000)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing the full ELBO with a second-order Taylor approximation around the posterior mean enables tractable optimization over real-valued parameters instead of distributions.
- Mechanism: The log-likelihood l(ηᵢ) is Taylor-expanded around the approximate posterior mean linear predictor η̄ᵢ = xᵢᵀβ̄_q. This yields ̃l(ηᵢ) = l(η̄ᵢ) + l′(η̄ᵢ)(ηᵢ − η̄ᵢ) + ½l′′(η̄ᵢ)(ηᵢ − η̄ᵢ)², which has closed-form expectations under the mean-field assumption. Lemma 2.1 shows this converts distributional optimization into minimizing h(θ, g), a function of posterior means θ and prior parameters g.
- Core assumption: The Taylor approximation remains sufficiently accurate under the mean-field factorization; high feature correlations may degrade this assumption (MFVI typically underestimates posterior uncertainty when ρ is high).

### Mechanism 2
- Claim: The optimal variational posterior qⱼ(βⱼ) for each coefficient is automatically determined as the posterior from a univariate Bayesian Normal Means problem with "pseudo-data" z_g(θⱼ).
- Mechanism: Theorem 2.2 shows the optimal qⱼ is proportional to g(βⱼ)N(z_g(θⱼ); βⱼ, sⱼ²(θ)), where z_g(θⱼ) is found by solving θⱼ = z + sⱼ²(θ)l′_NM(z; g, sⱼ²(θ)) via Tweedie's formula. This avoids pre-specifying variational families (e.g., Gaussian).
- Core assumption: The prior g must admit tractable log marginal likelihood l_NM and its derivative for the BNM subproblem; priors like horseshoe require numerical integration.

### Mechanism 3
- Claim: Empirical Bayes prior estimation within VI eliminates tuning hyperparameters while allowing the penalty shape to adapt to the data.
- Mechanism: Jointly optimizing θ and g over prior families G (point normal, point Laplace, ash) yields data-driven hyperparameters (π₀, σ², etc.). Figure 6a visualizes how different prior hyperparameters produce different penalty shapes on θ, analogous to adaptive regularization.
- Core assumption: The prior family G is sufficiently flexible to approximate the true data-generating prior; misspecified G may leave signal unshrunk or over-shrink noise.

## Foundational Learning

- Concept: Mean-Field Variational Inference (MFVI)
  - Why needed here: EBGLM assumes q(β) = ∏ⱼ qⱼ(βⱼ); the penalty rⱼ(θ, g) derivation critically depends on this factorization to separate coefficient-wise updates.
  - Quick check question: Can you derive why the ELBO decomposes as a sum over j under mean-field? If yes, you understand the setup.

- Concept: Bayesian Normal Means Problem
  - Why needed here: The core computational primitive is evaluating l_NM(z; g, s²) and its derivative for the inverse problem; this is what enables automatic posterior determination.
  - Quick check question: Given z|b ~ N(b, s²), b ~ g, can you write the posterior mean E[b|z] in terms of l′_NM? This is Tweedie's formula.

- Concept: Exponential Family GLMs
  - Why needed here: The method applies to any GLM with likelihood in exponential family form; the curvature term sⱼ²(θ) = a(ϕ)/(∑ᵢ b′′(xᵢᵀθ)x²ᵢⱼ) depends on b(·).
  - Quick check question: For logistic regression, what are a(ϕ), b(η), and b′′(η)? If you can answer, you can implement sⱼ²(θ).

## Architecture Onboarding

- Component map:
  Input data -> Taylor expansion of log-likelihood -> Curvature computation sⱼ²(θ) -> BNM solver for z_g(θⱼ) -> Penalty evaluation rⱼ(θ, g) -> Objective assembly h(θ, g) -> L-BFGS-B optimization -> Posterior extraction

- Critical path:
  1. Initialize θ via cross-validated lasso (1se rule) — provides warm start
  2. Compute l, l′, l′′ at current θ for all observations
  3. Compute sⱼ²(θ) for all coefficients
  4. For each j, run bisection to find z_g(θⱼ)
  5. Evaluate h(θ, g) and gradients
  6. L-BFGS-B step → update θ, g-params
  7. Check convergence (gradient norm or ELBO change); repeat from step 2

- Design tradeoffs:
  - Bisection vs Newton for root-finding: Bisection is stable but O(log(1/ε)) iterations per j per gradient step; Newton is faster but requires l′′_NM and may diverge
  - Prior family choice: Point normal is simplest; ash is most flexible but introduces K parameters; point Laplace has heavier tails
  - Initialization: Lasso warm-start speeds convergence but adds upfront CV cost; random initialization may require more L-BFGS iterations

- Failure signatures:
  - High feature correlation (ρ > 0.9): AUC degrades; MFVI underestimates variance (observed in Appendix C.1, Figure 3b). Consider block factorization or post-hoc uncertainty calibration
  - Extreme sparsity (s = 1): Some methods improve; EBGLM still competitive but variance in AUC increases (Figure 1, bottom-right)
  - Dense settings (p = 20, all non-zero): varbvs underperforms; EBGLM remains robust (Figure 1, top-right)

- First 3 experiments:
  1. Sanity check on synthetic logistic data: Generate n=500, p=100, s=10 with ρ=0; fit EBGLM-pn and compare posterior means to ground truth β and to MCMC samples. Verify that θ ≈ E[β|y]
  2. Scalability test: Fix p=1000, vary n∈{200, 500, 1000, 2000}; measure wall-clock time per L-BFGS iteration and total iterations to convergence. Profile the bisection loop
  3. Robustness to correlation: Fix n=500, p=100, s=10; vary ρ∈{0, 0.5, 0.8, 0.95}; compare AUC and coefficient MSE of EBGLM-pn vs lasso vs ridge. Identify correlation level where performance gap narrows

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the framework efficiently integrate continuous shrinkage priors, such as the horseshoe prior, which lack analytic marginal likelihoods?
- Basis in paper: The Discussion notes that "the horseshoe prior lacks an analytic marginal likelihood expression, which can be resolved using numerical integration techniques like quadrature."
- Why unresolved: The current method relies on closed-form solutions for the log marginal likelihood in the Bayesian normal means problem; numerical integration introduces computational costs and potential instabilities that have not yet been evaluated.

### Open Question 2
- Question: Can the computational bottleneck caused by the root-finding step be resolved using derivative-based optimization methods?
- Basis in paper: The authors identify the root-finding task for $z_g(\theta)$ as a bottleneck and suggest, "This process can be optimized by using faster, derivative-based methods like Newton’s method over the bisection method."
- Why unresolved: While bisection is stable, it is slow. It remains untested whether Newton's method provides stable and faster convergence within the specific convex/concave landscape of the EBGLM objective function.

### Open Question 3
- Question: How can the framework be extended to estimate the dispersion parameter $\phi$ when it is unknown?
- Basis in paper: Section 2 states, "The dispersion parameter $\phi$ is assumed to be known in this paper," implying a limitation for models like Gaussian linear regression where variance is typically estimated from data.
- Why unresolved: The definition of $s_j^2(\theta)$ and the objective function depend on a fixed $\phi$; treating it as a learnable parameter would require a modified joint optimization scheme not currently derived.

### Open Question 4
- Question: Does the EBGLM framework maintain its efficiency and accuracy when applied to complex non-regression architectures like deep neural networks?
- Basis in paper: The Discussion states the method "can be readily applied to other model settings such as matrix factorization, graphical models, and deep neural networks."
- Why unresolved: The paper validates the approach only on GLMs. Deep neural networks present highly non-convex landscapes where the second-order Taylor approximation used in the approximate ELBO might fail to capture the true posterior geometry.

## Limitations
- The Taylor approximation may introduce bias under high posterior curvature, though no explicit validation of approximation error is reported
- The bisection root-finding for Tweedie's formula lacks convergence guarantees across all parameter regimes and scales linearly with p
- Prior family flexibility depends critically on the choice of G; misspecification could leave signal unshrunk or over-shrink noise

## Confidence
- Mechanism 1 (Taylor approximation): Medium - well-established in variational inference but accuracy under high correlation not quantified
- Mechanism 2 (BNM inverse via Tweedie): Low - the specific root-finding approach is novel with limited external validation
- Mechanism 3 (Empirical Bayes tuning-free): High - demonstrated across multiple priors and simulations, consistent with related work

## Next Checks
1. Quantify approximation error: Compare EBGLM-ELBO to full ELBO under low (ρ=0) and high (ρ=0.8) correlation settings to measure Taylor approximation impact
2. Stress test root-finding: Systematically evaluate bisection convergence failures across parameter space, particularly for extreme θⱼ values or near-singular sⱼ²(θ)
3. Prior family expansion: Implement ash with dense variance grid (K=50 vs K=10) and measure performance gains on highly sparse problems (s=1) to assess flexibility limits