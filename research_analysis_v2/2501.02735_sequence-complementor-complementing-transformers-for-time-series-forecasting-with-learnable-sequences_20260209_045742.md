---
ver: rpa2
title: 'Sequence Complementor: Complementing Transformers For Time Series Forecasting
  with Learnable Sequences'
arxiv_id: '2501.02735'
source_url: https://arxiv.org/abs/2501.02735
tags:
- forecasting
- sequence
- time
- 'true'
- pred
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates why transformers underperform in time series
  forecasting despite their success in other domains. Through empirical analysis,
  the authors identify that transformer performance is strongly correlated with the
  richness of learned latent representations - more diverse representations lead to
  lower forecasting errors.
---

# Sequence Complementor: Complementing Transformers For Time Series Forecasting with Learnable Sequences

## Quick Facts
- arXiv ID: 2501.02735
- Source URL: https://arxiv.org/abs/2501.02735
- Reference count: 40
- Primary result: Learnable Sequence Complementors consistently improve transformer-based time series forecasting across 14 datasets, achieving state-of-the-art performance

## Executive Summary
This paper addresses the underperformance of transformers in time series forecasting by identifying that transformer success correlates strongly with the richness of learned latent representations. Through empirical analysis, the authors demonstrate that more diverse representations lead to lower forecasting errors. To enrich representations, they propose Sequence Complementors - learnable sequences that are concatenated to the input and interact through self-attention to provide complementary information. The method includes a diversification loss that encourages orthogonality among complementors, and is theoretically justified through information theory showing that complementary sequences can lower the mean-squared error bound.

## Method Summary
The proposed method enhances transformers for time series forecasting by introducing K learnable sequences (complementors) that are concatenated to the patchified input. The key innovation is a modified self-attention mechanism where queries come only from original patches while keys and values come from both original patches and complementors, allowing interaction without positional encoding interference. A diversification loss encourages orthogonality among complementors, promoting representation diversity. The method is trained with a combined loss function and evaluated on both long-term (8 datasets) and short-term (6 M4 subsets) forecasting tasks.

## Key Results
- Achieves first place in 47 out of 59 settings for long-term forecasting
- Achieves first place in 14 out of 15 settings for short-term forecasting
- Demonstrates consistent improvements over state-of-the-art methods across 14 datasets
- Shows strong correlation (0.6954) between representation diversity and forecasting performance

## Why This Works (Mechanism)
Transformers struggle with time series forecasting because they often learn less diverse latent representations compared to other tasks like NLP or CV. The proposed method addresses this by introducing learnable complementors that enrich the representation space through their interaction with original patches in self-attention. The diversification loss ensures these complementors remain orthogonal, preventing them from learning redundant information. This mechanism effectively increases the information content available for forecasting while maintaining computational efficiency through the modified attention mechanism.

## Foundational Learning
- **Self-attention mechanism**: Needed to understand how queries, keys, and values interact; quick check: verify attention weights sum to 1 across sequence
- **Positional encoding**: Required to understand why it's applied only to original patches; quick check: confirm complementor positions remain unencoded
- **Representation diversity**: Core concept linking to forecasting performance; quick check: measure pairwise cosine similarity of learned representations
- **Information theory bounds**: Theoretical foundation for why complementary sequences improve performance; quick check: verify MSE reduction aligns with theoretical predictions
- **Orthogonalization**: Key to the diversification loss mechanism; quick check: confirm learned complementors have pairwise cosine similarity approaching 0
- **Patch embedding**: Understanding how time series are converted to patch format; quick check: verify patch dimensions match input requirements

## Architecture Onboarding
**Component map**: Input time series -> Patch embedding -> Learnable complementors -> Concatenation -> Modified self-attention -> Forecasting head

**Critical path**: The modified self-attention mechanism is the critical component where Q comes from original patches only, while K and V come from both original patches and complementors. This design allows the complementors to influence the representation without receiving positional information, preventing interference.

**Design tradeoffs**: The method trades increased parameter count (learnable complementors) for improved representation diversity. The choice to apply positional encoding only to original patches prevents positional information from leaking into complementors while still allowing them to contribute to the representation.

**Failure signatures**: Complementors collapsing to similar vectors (pairwise cosine similarity approaching 1), positional encoding leaking into complementors (check attention patterns), or insufficient diversification loss weight causing complementors to learn redundant information.

**First experiments**: 1) Implement basic Sequence Complementor module with random initialization, 2) Verify modified self-attention with Q from original patches only, 3) Test diversification loss effect by comparing with and without λ_dcs.

## Open Questions the Paper Calls Out
None

## Limitations
- Several critical hyperparameters remain unspecified (patch size, stride, initialization scheme)
- Method relies heavily on diversification loss, which may not generalize to all time series patterns
- Limited comparison with statistical methods (ARIMA, ETS) that remain competitive baselines
- Computational overhead of learnable complementors relative to their benefit is not discussed

## Confidence
- High confidence: The theoretical framework connecting representation diversity to forecasting performance
- High confidence: The diversification loss mechanism and its effect on learned complementors
- Medium confidence: The overall effectiveness claim across all 14 datasets (due to unspecified hyperparameters)
- Medium confidence: The first-place rankings (method depends on unreported implementation details)

## Next Checks
1. **Hyperparameter Sensitivity Analysis**: Systematically vary patch size, stride, and λ_dcs to verify robustness of improvements
2. **Complementor Behavior Inspection**: After training, analyze the learned complementors' patterns across different datasets to confirm they capture meaningful complementary information
3. **Ablation on Diversification Loss**: Train identical models with λ_dcs = 0 to quantify the specific contribution of the diversification mechanism to performance gains