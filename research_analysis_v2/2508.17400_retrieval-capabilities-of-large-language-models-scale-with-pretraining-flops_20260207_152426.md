---
ver: rpa2
title: Retrieval Capabilities of Large Language Models Scale with Pretraining FLOPs
arxiv_id: '2508.17400'
source_url: https://arxiv.org/abs/2508.17400
tags:
- retrieval
- tokens
- performance
- trained
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study asks how zero\u2011shot retrieval quality of decoder\u2011\
  style LLMs varies with model size, pre\u2011training compute and token\u2011per\u2011\
  parameter exposure. Using MPT checkpoints from 125 M to 7 B parameters, pretrained\
  \ on 1 B\u20132 T tokens (token\u2011per\u2011parameter ratios 20\u2013500, one\
  \ case 10 k), each model was minimally fine\u2011tuned for one epoch on 500 k MS\
  \ MARCO query\u2011passage pairs with hard negatives (InfoNCE loss) and evaluated\
  \ on the BEIR benchmark."
---

# Retrieval Capabilities of Large Language Models Scale with Pretraining FLOPs  

## Quick Facts  
- **arXiv ID:** 2508.17400  
- **Source URL:** https://arxiv.org/abs/2508.17400  
- **Reference count:** 27  
- **Primary result:** Zero‑shot retrieval quality (BEIR nDCG@10) rises monotonically with model size and pre‑training FLOPs, establishing a predictable scaling law.  

## Executive Summary  
The authors investigate how zero‑shot retrieval performance of decoder‑only language models varies with model scale, pre‑training compute, and token‑per‑parameter exposure. Using a suite of MPT checkpoints (125 M–7 B parameters) pretrained on 1 B–2 T tokens, each model receives a single epoch of fine‑tuning on 500 k MS MARCO query‑passage pairs with hard negatives (InfoNCE loss). Evaluation on the BEIR benchmark shows a steady increase in average nDCG@10 as size and FLOPs grow, suggesting a robust scaling relationship. Additionally, in‑context‑learning (ICL) scores correlate strongly with retrieval metrics, implying that ICL can serve as a proxy for retrieval capability. All reported scores are lower bounds because identical hyper‑parameters were used across model scales.  

## Method Summary  
The study fine‑tunes a range of MPT checkpoints (125 M to 7 B parameters) that were pretrained on token counts spanning 20–500 tokens per parameter (one outlier at 10 k). Fine‑tuning consists of one epoch over 500 k MS MARCO query‑passage pairs, employing hard negatives and an InfoNCE contrastive loss. No architecture changes are made; the same decoder‑only backbone is used throughout. After fine‑tuning, each model is evaluated zero‑shot on the BEIR benchmark, reporting nDCG@10 as the primary metric. Uniform training hyper‑parameters (learning rate, batch size, etc.) are applied to all model sizes, making the results conservative lower bounds.  

## Key Results  
- **Monotonic scaling:** Average BEIR nDCG@10 improves consistently with increasing model parameters and FLOPs.  
- **Predictable law:** Larger pre‑training budgets (higher token‑per‑parameter ratios) yield higher retrieval scores, indicating a scaling law.  
- **ICL proxy:** Strong correlation between in‑context‑learning performance and retrieval quality across BEIR tasks.  

## Why This Works (Mechanism)  
- **Assumption: Representation capacity.** Larger decoder‑only models possess higher dimensional hidden states, which can encode finer‑grained semantic distinctions. This richer embedding space likely improves the alignment between query and passage vectors after contrastive fine‑tuning.  
- **Assumption: Data‑efficiency from token‑per‑parameter exposure.** Models trained on more tokens per parameter have seen a broader distribution of linguistic patterns, enabling them to generalize better to unseen retrieval queries. The observed monotonic trend suggests that exposure, rather than sheer parameter count alone, contributes to retrieval competence.  
- **Assumption: Contrastive learning benefits from capacity.** The InfoNCE loss pushes representations of matching query‑passage pairs together while separating hard negatives. With greater model capacity, the optimizer can find more discriminative decision boundaries, reducing overlap between positive and negative embeddings.  
- **Unknown: Implicit retrieval knowledge.** Decoder‑only LLMs are trained on massive next‑token prediction corpora; it remains unclear how much of the emergent retrieval ability stems from memorised factual content versus learned semantic similarity. Further probing is needed to isolate these factors.  

## Foundational Learning  
| Concept | Why needed | Quick check |
|--------|------------|-------------|
| Scaling laws in LLMs | To understand how compute and model size translate into retrieval ability. | Verify that nDCG@10 increases monotonically across the size spectrum. |
| In‑context learning as a proxy | Provides a low‑cost way to estimate retrieval performance without fine‑tuning. | Compute Pearson/Spearman correlation between ICL scores and BEIR nDCG@10. |
| Representation richness vs. over‑fitting | Explains whether larger FLOP budgets improve semantic encoding or simply reduce over‑fitting. | Probe hidden states for semantic similarity across checkpoints of different FLOP budgets. |
| Token‑per‑parameter exposure | Determines data efficiency of scaling; higher ratios may yield better generalisation. | Plot performance vs. token‑per‑parameter ratio for all models. |
| Uniform hyper‑parameter impact | Clarifies how much the lower‑bound scores underestimate true potential. | Run a per‑size hyper‑parameter sweep and compare peak scores to the uniform baseline. |
| Hard‑negative contrastive fine‑tuning | Central to converting a generative model into an effective retriever. | Inspect loss curves and retrieval recall during the single‑epoch fine‑tune. |

## Architecture Onboarding  
**Component map:**  
Pretrained MPT checkpoint → InfoNCE fine‑tuning on MS MARCO → Retrieval encoder (zero‑shot) → BEIR evaluation  

**Critical path:**  
Fine‑tuning (InfoNCE) → Generation of query‑passage embeddings → BEIR nDCG@10 computation.  

**Design tradeoffs:**  
- Uniform hyper‑parameters vs. per‑size optimisation (simplicity vs. peak performance).  
- Token‑per‑parameter ratio (data efficiency vs. compute cost).  
- Single‑epoch fine‑tuning (speed vs. convergence).  

**Failure signatures:**  
- Diverging loss or NaN gradients during the single epoch.  
- Retrieval scores that plateau or drop for larger models (indicating under‑training).  
- Weak or inconsistent correlation between ICL and retrieval metrics.  

**First 3 experiments:**  
1. Provide abstract or section text to enable mechanism extraction.  
2. Include corpus signals or neighbor papers for context anchoring.  
3. Specify key outcome to focus mechanism analysis.  

## Open Questions the Paper Calls Out  
1. **Do retrieval scaling laws persist on long‑context or reasoning‑intensive benchmarks?**  
   - *Basis:* Section 2.2 notes future work on AIR Bench, LongEmbed, and BRIGHT.  
   - *Resolution:* Evaluate the MPT series on those benchmarks to test the monotonic relationship.  

2. **What specific mechanisms enable larger pretraining budgets to yield better retrieval capabilities?**  
   - *Basis:* Section 1 asks why foundation models excel at IR tasks.  
   - *Resolution:* Conduct probing experiments on internal representations across FLOP budgets.  

3. **How much performance is lost when using uniform hyperparameters across different model scales?**  
   - *Basis:* Section 3.1 and Limits mention uniform hyper‑parameters “strongly handicaps the models.”  
   - *Resolution:* Perform individual hyper‑parameter sweeps per model size and compare to the reported lower bounds.  

## Limitations  
- Mechanistic explanation of scaling remains speculative.  
- Findings are limited to the MPT family and a single fine‑tuning regime; generalisability to other architectures is untested.  
- Reported BEIR scores are lower bounds due to uniform hyper‑parameters across scales.  

## Confidence  
- **Monotonic scaling of BEIR nDCG@10 with model size / FLOPs → Medium**  
- **Predictable scaling law (size ↔ performance) → Low**  
- **ICL performance as a proxy for retrieval ability → Low**  

## Next Checks  
1. **Cross‑architecture replication:** Apply the same retrieval fine‑tune to a different decoder‑only family (e.g., LLaMA) and an encoder‑decoder model (e.g., T5) to test if monotonic scaling persists.  
2. **Hyper‑parameter sweep per size:** For each model scale, grid‑search learning rate, batch size, and epoch count; compare the best BEIR scores to the uniform‑hyper‑parameter baseline to quantify the lower‑bound effect.  
3. **ICL‑retrieval correlation robustness:** Evaluate ICL performance on a held‑out BEIR subset using varied prompts and few‑shot counts; compute Pearson/Spearman correlations with retrieval nDCG@10 to assess stability across prompt designs.