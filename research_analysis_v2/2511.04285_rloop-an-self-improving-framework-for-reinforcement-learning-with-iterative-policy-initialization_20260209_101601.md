---
ver: rpa2
title: 'RLoop: An Self-Improving Framework for Reinforcement Learning with Iterative
  Policy Initialization'
arxiv_id: '2511.04285'
source_url: https://arxiv.org/abs/2511.04285
tags:
- rloop
- policy
- training
- learning
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies and addresses a critical challenge in reinforcement
  learning for reasoning models: RL overfitting, where models gain training rewards
  but lose generalization. The authors propose RLoop, an iterative self-improvement
  framework that alternates between RL-based exploration to generate diverse solutions
  and RFT-based exploitation to consolidate knowledge via rejection sampling.'
---

# RLoop: An Self-Improving Framework for Reinforcement Learning with Iterative Policy Initialization

## Quick Facts
- arXiv ID: 2511.04285
- Source URL: https://arxiv.org/abs/2511.04285
- Authors: Zeng Zhiyuan; Jiashuo Liu; Zhangyue Yin; Ge Zhang; Wenhao Huang; Xipeng Qiu
- Reference count: 31
- One-line result: RLoop boosts math reasoning performance by 9% accuracy and 15% pass@32 over vanilla RL by capturing and consolidating inter-step policy diversity.

## Executive Summary
RLoop addresses a critical challenge in reinforcement learning for reasoning models: RL overfitting, where models gain training rewards but lose generalization. The authors propose an iterative self-improvement framework that alternates between RL-based exploration to generate diverse solutions and RFT-based exploitation to consolidate knowledge via rejection sampling. By harnessing inter-step policy diversity that standard RL discards, RLoop transforms transient variations into robust performance gains. Experiments show RLoop significantly outperforms vanilla RL across multiple math reasoning benchmarks while also mitigating catastrophic forgetting and improving training stability.

## Method Summary
RLoop is an iterative framework that alternates between exploration and exploitation phases. During exploration, DAPO-based RL generates trajectories from multiple intermediate checkpoints, capturing diverse solution strategies. The exploitation phase uses rejection-sampling fine-tuning (RFT) on successful trajectories from "hard" problems (success rate <10%) to create a superior initial policy for the next iteration. This cyclical re-initialization prevents gradient explosion and training collapse associated with prolonged RL. The framework runs for three iterations (~600 equivalent RL steps) on the DAPO-17k dataset using Qwen-2.5-7b-Math as the base model.

## Key Results
- Achieves 9% average accuracy improvement over vanilla RL
- Improves pass@32 by over 15% across multiple math reasoning benchmarks
- Maintains stable gradient norms (<0.3) compared to vanilla RL's explosion (>50.0)
- Demonstrates effective mitigation of catastrophic forgetting through cyclical re-initialization

## Why This Works (Mechanism)

### Mechanism 1: Inter-Step Policy Diversity Capture
Standard RL discards valuable solution variations generated at intermediate training steps. RLoop collects trajectories from multiple checkpoints during RL training, preserving distinct problem-solving capabilities. The diversity across training steps represents valuable exploration rather than noise, with inter-step similarity remaining low (<0.2) while intra-step similarity is high (>0.26).

### Mechanism 2: Consolidation via Rejection-Sampling Fine-Tuning (RFT)
Periodic consolidation of successful trajectories via RFT stabilizes learning by providing absolute rather than relative reward signals. While RL uses advantage functions (relative weighting) that provide vanishing gradients when all samples are good, RFT uses binary rewards (absolute weighting) to reinforce all successful trajectories equally, creating a stable anchor that retains knowledge.

### Mechanism 3: Stability via Cyclical Re-initialization
Iteratively resetting the RL process from a consolidated policy prevents gradient explosion and training collapse. Continuous RL optimization can lead to gradient explosion, but RLoop breaks this into shorter exploration phases with RFT re-anchoring between iterations, preventing drift into unstable parameter regions.

## Foundational Learning

- **Concept: Catastrophic Forgetting**
  - Why needed here: The paper identifies this as a primary driver of "RL overfitting." Understanding that neural networks lose previously acquired skills when optimizing solely for new rewards is crucial.
  - Quick check question: If a model learns Task B perfectly but drops from 80% to 20% accuracy on Task A, what specific failure mode is this?

- **Concept: Exploration vs. Exploitation**
  - Why needed here: RLoop explicitly separates these into distinct phases. You must understand why RL is treated here as "exploration" (search) and RFT as "exploitation" (consolidation).
  - Quick check question: Why might a standard RL agent exploiting high-reward trajectories early fail to find a globally optimal solution?

- **Concept: Rejection Sampling Fine-Tuning (RFT)**
  - Why needed here: This is the engine of the "Exploitation Phase." It differs from standard SFT because the training data is model-generated and filtered by outcome, not human-labeled.
  - Quick check question: In RFT, if the base model cannot solve a problem at all (0% success rate), can that problem contribute to the RFT dataset? (Answer: No, because there are no successful trajectories to sample).

## Architecture Onboarding

- **Component map:** Input (Base Model, Prompt Dataset, Verifiable Reward) -> RL Engine (Exploration) -> Active Filter -> SFT Engine (Exploitation) -> Output (Enhanced Policy)

- **Critical path:** The transition from RL Checkpoint Collection -> Active Filtering -> SFT Re-initialization. If the filter passes low-quality data, the next loop iteration will be worse than the current one.

- **Design tradeoffs:**
  - Loop Iteration Count vs. RL Steps: Shorter RL phases reduce collapse risk but may gather less diverse data
  - Active Learning Threshold: Defining "hard" problems focuses capacity but risks ignoring foundational skills if set too high

- **Failure signatures:**
  - Gradient Explosion: Spikes in gradient norm during RL phase (remedy: reduce RL steps per iteration)
  - Reward Hacking: Accuracy rises but Pass@k drops (sign of mode collapse; check RFT data diversity)
  - Degradation Loop: Validation accuracy drops with every RLoop iteration (sign of false positives in RFT data)

- **First 3 experiments:**
  1. Overfitting Baseline: Run Vanilla RL for 700 steps to reproduce the divergence between Reward and Pass@k
  2. Stability Test: Monitor gradient norms for RLoop vs. Vanilla RL to verify the stability claim
  3. Ablation on Active Learning: Run RLoop with "Active Learning" disabled to quantify the efficiency gain

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does RLoop generalize to tasks with non-binary or subjective rewards?
- Basis in paper: The theoretical derivation explicitly approximates importance weights using binary rewards ($R(\tau) \in \{0,1\}$).
- Why unresolved: The method relies on filtering "successful" trajectories to create the expert dataset, which is trivial for verifiable math but undefined for open-ended generation where rewards are continuous or subjective.
- What evidence would resolve it: Experiments on RLHF tasks (e.g., summarization or creative writing) using continuous reward models rather than rule-based verifiers.

### Open Question 2
- Question: How sensitive is the framework to the definition of "hard" problems in the active learning phase?
- Basis in paper: The authors define "hard" problems as those with a success rate below 10% during the RL phase.
- Why unresolved: The paper presents this specific threshold as a fixed heuristic without ablation. A higher or lower threshold could significantly alter the balance between exploration and exploitation.
- What evidence would resolve it: An ablation study varying the success rate threshold (e.g., 5%, 20%, 50%) and measuring the resulting impact on final pass@k scores and convergence speed.

### Open Question 3
- Question: How does RLoop compare to standard experience replay or constrained optimization methods for mitigating catastrophic forgetting?
- Basis in paper: The paper cites constrained SFT updates (Yuan et al., 2025) and off-policy learning (Yan et al., 2025) as related work, but compares only against Vanilla RL (DAPO).
- Why unresolved: Without comparing against established anti-forgetting baselines, it is unclear if the computational overhead of iterative re-initialization is more efficient than simply replaying previous data.
- What evidence would resolve it: A direct comparison of training dynamics and performance against methods like Experience Replay or KL-constrained optimization on the same benchmarks.

## Limitations
- Unknown DAPO algorithm details including exact implementation, loss formulation, and hyperparameters
- Unspecified RL hyperparameters such as learning rate, batch size, and optimizer settings
- Unclear checkpoint collection strategy including how many intermediate checkpoints and which specific steps are used
- Hard problem filtering lacks precision in sample count and exact success rate threshold computation

## Confidence
- **High confidence**: RL overfitting is a real phenomenon, and RLoop's cyclical structure demonstrably improves stability over vanilla RL
- **Medium confidence**: The specific performance gains (9%/15%) will depend heavily on implementation details of DAPO and the active learning filter
- **Medium confidence**: The catastrophic forgetting mitigation claim assumes verifiers are accurate - if reward hacking occurs, RFT could reinforce errors

## Next Checks
1. **Gradient stability verification**: Monitor gradient norms during vanilla RL vs RLoop training to confirm the dramatic difference (>50.0 vs <0.3) claimed in Section 5.5

2. **Diversity preservation test**: Measure inter-step and intra-step similarity during training to verify that RLoop maintains distinct solution modes (inter-step similarity <0.2, intra-step >0.26)

3. **Active learning ablation**: Run RLoop with and without the hard problem filtering to quantify the efficiency gains from focusing on low-success-rate problems