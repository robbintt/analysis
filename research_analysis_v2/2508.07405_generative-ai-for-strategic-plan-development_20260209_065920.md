---
ver: rpa2
title: Generative AI for Strategic Plan Development
arxiv_id: '2508.07405'
source_url: https://arxiv.org/abs/2508.07405
tags:
- strategic
- topics
- plan
- vision
- bertopic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated the use of generative AI for strategic plan
  development in large government organizations, focusing on automating the creation
  of Vision Elements. The research tested BERTopic and NMF topic modeling techniques
  using GAO reports as training data, comparing generated themes against a published
  DOE Office of Legacy Management strategic plan.
---

# Generative AI for Strategic Plan Development

## Quick Facts
- arXiv ID: 2508.07405
- Source URL: https://arxiv.org/abs/2508.07405
- Reference count: 14
- Primary result: BERTopic achieved medium or strong correlation with 50% of Vision Elements, outperforming NMF which had zero strong correlations

## Executive Summary
This study evaluated generative AI topic modeling for strategic plan development in government organizations, specifically automating the creation of Vision Elements from external regulatory documents. The research compared BERTopic and NMF techniques using GAO reports as training data, validating results against a published DOE Office of Legacy Management strategic plan. BERTopic demonstrated superior performance with more than half of its correlated topics achieving medium or strong correlation, while NMF produced zero strongly correlated topics. The findings establish topic modeling as a viable approach for GAI-enabled strategic planning and identify BERTopic as the leading technique for this application.

## Method Summary
The study employed a two-stage methodology: first collecting and preprocessing GAO reports relevant to the Department of Energy, then splitting each report by page to create individual document units; second, training both NMF and BERTopic models on this corpus and evaluating their output by correlating generated topics with the six Vision Elements from a published DOE strategic plan using manual semantic matching. NMF used standard TF-IDF vectorization followed by matrix factorization, while BERTopic employed BERT embeddings transformed via UMAP and clustered using HDBSCAN before extracting topics through c-TF-IDF. The evaluation relied entirely on human judgment to assign weak, medium, or strong correlation labels between topic keywords and Vision Element descriptions.

## Key Results
- BERTopic generated topics representing 100% of evaluated Vision Elements, with 50% achieving medium or strong correlation
- NMF covered 100% of Vision Elements but produced zero strong correlations
- Both techniques successfully generated themes for all six Vision Elements in the ground truth strategic plan
- Page-level document chunking proved effective for creating semantically coherent units suitable for topic modeling

## Why This Works (Mechanism)

### Mechanism 1
BERTopic outperforms NMF for extracting strategic themes from government documents because its transformer-based embeddings capture semantic relationships that algebraic matrix factorization misses. BERTopic uses BERT embeddings → UMAP dimensionality reduction → HDBSCAN clustering → c-TF-IDF for topic representation. This pipeline preserves semantic similarity during clustering, whereas NMF relies purely on term frequency statistics that cannot capture contextual meaning. The core assumption is that semantic similarity in embedding space correlates with thematic relevance for strategic planning. Evidence shows BERTopic achieved 50% medium/strong correlations versus NMF's zero strong correlations. Break condition: if documents are too short (<50 words) or vocabulary is highly domain-specific without fine-tuned embeddings.

### Mechanism 2
Page-level document chunking enables topic modeling on long-form reports by creating semantically coherent units that each contain a dominant theme. GAO reports split by page → each page treated as standalone document → clustering algorithms assign single topic per unit. This avoids multi-topic dilution that occurs when clustering entire reports. The core assumption is that individual pages contain sufficiently focused content that they approximate single-topic documents. Evidence shows smallest number of correlations is 6 and largest is 8, with no Vision Elements having zero correlations. Break condition: if reports have high visual content, tables, or multi-page sections that split thematic coherence mid-topic.

### Mechanism 3
External regulatory documents (GAO reports) provide sufficient signal to generate Vision Elements aligned with actual organizational strategic plans because they capture external perspectives on agency challenges and trends. GAO reports represent external analysis of agency operations → topic modeling extracts recurring themes → themes correlate to Vision Elements in published strategic plans. The core assumption is that GAO reports systematically cover the operational and strategic challenges that agencies themselves identify in their planning processes. Evidence shows coverage of all Vision Elements and correlation distribution supporting this alignment. Break condition: if the target organization has no GAO coverage or if GAO reports are outdated relative to emerging strategic priorities.

## Foundational Learning

- **Concept: TF-IDF (Term Frequency-Inverse Document Frequency)**
  - Why needed here: Both NMF and BERTopic use TF-IDF variants as their topic representation layer—understanding how term importance is calculated is essential for interpreting topic-word weights and debugging poor topic quality.
  - Quick check question: Can you explain why a word appearing in many documents receives a lower weight than a word appearing in few documents?

- **Concept: Word Embeddings and Semantic Similarity**
  - Why needed here: BERTopic's advantage over NMF stems from using BERT embeddings that capture semantic relationships; understanding vector representations is critical for choosing embedding models and interpreting intertopic distance maps.
  - Quick check question: Why would "budget" and "funding" have similar vector representations in a trained embedding model?

- **Concept: Clustering Algorithms (HDBSCAN)**
  - Why needed here: BERTopic uses HDBSCAN for density-based clustering, which handles noise and variable cluster sizes differently than K-means; understanding clustering behavior helps diagnose topic overlap and outlier assignment.
  - Quick check question: What happens to documents that HDBSCAN identifies as noise or outliers?

## Architecture Onboarding

- **Component map:** Web scraper (Beautiful Soup) → PDF download → PyPDF2 text extraction → page-level chunking → string list → Parallel paths for NMF (TF-IDF → matrix factorization) and BERTopic (BERT embeddings → UMAP → HDBSCAN → c-TF-IDF) → Topic-to-Vision-Element correlation scoring → Distribution analysis

- **Critical path:**
  1. Training data acquisition (GAO report collection and preprocessing)
  2. Model training (BERTopic primary, NMF baseline comparison)
  3. Correlation evaluation against truth data (published strategic plan Vision Elements)

- **Design tradeoffs:**
  - Page-level vs paragraph-level chunking: Pages increase document count and computational efficiency but may split coherent topics; paragraphs improve semantic coherence but increase processing complexity
  - Number of topics: BERTopic auto-generates 347 topics (high recall, potential redundancy); NMF requires manual topic count specification (more controlled, may miss themes)
  - Evaluation methodology: Manual correlation scoring introduces subjectivity; automated semantic similarity would scale but may miss domain-specific nuances

- **Failure signatures:**
  - High topic overlap with low distinctiveness: Indicates embedding space too dense or clustering parameters too permissive
  - Zero strong correlations for any technique: Suggests training data misalignment with target organization's domain
  - Skewed topic distribution with one dominant cluster: May indicate class imbalance in training corpus or outlier sensitivity

- **First 3 experiments:**
  1. Baseline replication: Implement the exact pipeline (GAO reports → page chunking → BERTopic) and verify correlation distribution matches paper results (≥50% medium/strong correlations).
  2. Chunking sensitivity analysis: Compare page-level vs paragraph-level vs sentence-level chunking on topic coherence scores and correlation strength distribution.
  3. Cross-agency validation: Train on GAO reports for a different agency (e.g., DHS) and evaluate against its published strategic plan to test generalizability of the approach.

## Open Questions the Paper Calls Out

- **Can the proposed cognitive model be extended to automate higher-complexity tasks such as generating Strategies and Recommendations?**
  - Basis: The author states future studies will explore remaining cognitive tasks, noting that tasks like Recommendations require more cognitive complexity than the Summary Task tested.
  - Why unresolved: This study only validated the "Summary Task" (Vision Elements); other modules in the theoretical framework remain unproven.
  - What evidence would resolve it: Successful generation of actionable strategies and activities by a machine learning model that align with established Vision Elements.

- **Does training the topic model on a multi-agency dataset improve the generalizability of generated strategic themes?**
  - Basis: The conclusion notes optimizing capability requires additional training data spanning multiple government agencies.
  - Why unresolved: Current study relied on DOE-specific GAO reports, limiting ability to generalize results to other government organizations.
  - What evidence would resolve it: High correlation scores when model is applied to strategic plans of agencies not included in training data.

- **How does document segmentation strategy (splitting by page) impact semantic coherence of generated topics?**
  - Basis: The paper acknowledges that deciding where to separate the report is a design choice which could impact topic modeling results.
  - Why unresolved: Splitting by page may sever related semantic concepts or merge unrelated ones, potentially introducing noise into topic modeling process.
  - What evidence would resolve it: Comparative analysis of topic coherence scores between models trained on page-split data versus semantically chunked data.

## Limitations

- Manual correlation evaluation relies entirely on subjective human judgment without standardized scoring thresholds or inter-rater reliability measures
- Results validated only against one DOE strategic plan, limiting generalizability to other agencies and contexts
- Page-level document splitting is acknowledged as a potential source of noise, but no comparison to alternative chunking strategies was performed

## Confidence

- **High confidence:** BERTopic's technical superiority over NMF for this task (based on clear correlation metrics and established embedding advantages)
- **Medium confidence:** The overall approach of using external documents for strategic planning (supported by correlation results but limited to single validation case)
- **Low confidence:** The manual evaluation methodology's reproducibility and objectivity (no standardized scoring rubric provided)

## Next Checks

1. **Inter-rater reliability test:** Have three independent reviewers score topic-to-Vision-Element correlations using the paper's methodology and calculate Cohen's kappa to establish consistency
2. **Chunking sensitivity analysis:** Replicate the pipeline using paragraph-level and sentence-level document splitting to quantify the impact of chunking strategy on correlation strength distribution
3. **Cross-agency generalizability:** Apply the exact pipeline to GAO reports from a different federal agency (e.g., Department of Homeland Security) and validate against its strategic plan to test method portability