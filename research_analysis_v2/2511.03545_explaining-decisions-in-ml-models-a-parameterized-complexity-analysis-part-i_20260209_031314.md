---
ver: rpa2
title: 'Explaining Decisions in ML Models: a Parameterized Complexity Analysis (Part
  I)'
arxiv_id: '2511.03545'
source_url: https://arxiv.org/abs/2511.03545
tags:
- every
- size
- explanation
- such
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first comprehensive parameterized complexity
  analysis of explanation problems in transparent machine learning models, including
  decision trees, decision sets, decision lists, and their ensembles. The authors
  introduce abductive and contrastive explanation problems in both local and global
  variants, then systematically analyze their computational complexity using parameterized
  complexity theory.
---

# Explaining Decisions in ML Models: a Parameterized Complexity Analysis (Part I)

## Quick Facts
- **arXiv ID**: 2511.03545
- **Source URL**: https://arxiv.org/abs/2511.03545
- **Reference count**: 30
- **Key outcome**: First comprehensive parameterized complexity analysis of explanation problems in transparent ML models (DTs, DSs, DLs, ensembles)

## Executive Summary
This paper presents the first systematic parameterized complexity analysis of explanation problems in transparent machine learning models. The authors introduce abductive and contrastive explanation problems in both local and global variants, then analyze their computational complexity using parameterized complexity theory. They establish a novel hardness framework based on set-modelling and subset-modelling properties, showing that while local contrastive explanations for decision trees are polynomial-time solvable, the same problems become NP-hard for decision sets and decision lists. The paper develops powerful meta-theorems based on Boolean circuits and monadic second-order logic that yield fixed-parameter tractable algorithms for various explanation problems, particularly when parameters like explanation size or model complexity are bounded.

## Method Summary
The paper uses mathematical proofs based on reductions from hard combinatorial problems (like Multicoloured Clique and Hitting Set) to establish complexity results. The key approach involves translating ML models to Boolean circuits with bounded structural parameters (rank-width, tree-width) and encoding explanation properties in Monadic Second-Order Logic (MSO). The method relies on a hardness framework based on set-modelling and subset-modelling properties, which accounts for the majority of hardness results. For algorithmic results, the authors prove fixed-parameter tractability using Courcelle-type model checking results when models can be translated to circuits with bounded rank-width.

## Key Results
- Decision trees support polynomial-time local contrastive explanations (LCXP), while decision sets and decision lists are W[2]-hard for the same problem
- The paper introduces a novel hardness framework based on set-modelling and subset-modelling properties that unifies the majority of hardness proofs
- Fixed-parameter tractable algorithms exist for explanation problems when models are represented as Boolean circuits with bounded rank-width and constant majority gates
- Ensemble models add expressive power but explanation complexity jumps from P to co-NP-hard or W[1]-hard even with bounded model size

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hardness of explanation problems can be systematically derived from a model class's ability to encode set-membership queries.
- Mechanism: The paper introduces **set-modelling** (model classifies positively iff features match exactly some set in a family) and **subset-modelling** (classifies positively iff features contain some set as subset). When a model class satisfies either property, hard combinatorial problems (hitting set, clique finding) can be reduced to explanation tasks, yielding NP-hardness or W[2]-hardness.
- Core assumption: Assumption: The reduction preserves parameter bounds (explanation size maps to solution size).
- Evidence anchors:
  - [Section 6.1]: "A key insight underlying many of our hardness proofs is the close connection between explanation problems and the homogeneous model problem... We introduce a novel meta-framework in Section 6.1 that unifies the majority of our hardness proofs."
  - [Lemma 21]: Reduces HITTING SET to LAXP/LCXP/GAXP/GCXP using set-modelling and subset-modelling properties.
  - [corpus]: Weak direct evidence; corpus papers focus on human-centered XAI design rather than computational complexity.
- Break condition: Model classes that cannot efficiently encode set membership (e.g., models with severely restricted expressive power) may evade this hardness reduction.

### Mechanism 2
- Claim: Explanation problems become fixed-parameter tractable when models are represented as Boolean circuits with bounded structural complexity.
- Mechanism: The paper's meta-theorem (Theorem 4) shows that if a model can be translated to a Boolean circuit with bounded rank-width and constant majority gates, then all four explanation types (LAXP, LCXP, GAXP, GCXP) are solvable in FPT time. The key insight is encoding explanation validity as MSOE₁ formulas and exploiting Courcelle-type model checking results.
- Core assumption: Assumption: The translation to Boolean circuits preserves rank-width bounds proportional to model parameters (e.g., MNL for trees, terms for lists).
- Evidence anchors:
  - [Theorem 4]: "c-BC-LAXP, c-BC-GAXP, c-BC-LCXP, c-BC-GCXP are fixed-parameter tractable parameterized by the rank-width of the circuit."
  - [Section 5.1]: Shows how MAJ-gates handle ensemble majority voting via size measurement in MSOE₁ logic.
  - [corpus]: No direct computational complexity evidence in corpus.
- Break condition: If translation to circuits produces exponential rank-width blowup, or if majority gates scale with ensemble size unboundedly, FPT guarantee fails.

### Mechanism 3
- Claim: Explanation complexity correlates with the **Homogeneous Model Problem**—determining whether a model classifies all examples identically.
- Mechanism: Lemma 19 establishes equivalence between HOM and trivial explanations (empty set being valid). Hardness of HOM transfers to hardness of explanation problems via this connection. For ensembles, the P-HOM variant (bounded positive features) yields parameterized hardness.
- Core assumption: Assumption: The reduction from HOM to explanation problems preserves the computational structure of the original problem.
- Evidence anchors:
  - [Lemma 19]: "The following statements are equivalent: (1) M is a no-instance of M-HOM. (2) The empty set is a solution for M-LAXP⊆..."
  - [Lemma 20]: Establishes equivalence between P-HOM and LCXP parameterized by explanation size.
  - [corpus]: No corpus evidence; mechanism is internal to theoretical framework.
- Break condition: Models where HOM is trivially solvable (e.g., constant classifiers) do not inherit hardness through this mechanism.

## Foundational Learning

- Concept: **Parameterized Complexity (FPT, W[1], W[2], XP)**
  - Why needed here: The paper's entire analysis uses this framework to distinguish between "hopelessly hard" and "tractable with bounded parameters."
  - Quick check question: If a problem is W[2]-hard parameterized by k, can it be solved in time f(k)·n^c?

- Concept: **Monadic Second-Order Logic (MSO) on Graphs**
  - Why needed here: The algorithmic meta-theorem relies on MSOE₁ expressiveness to encode explanation validity.
  - Quick check question: Can MSO quantify over sets of vertices? Over edge sets?

- Concept: **Rank-Width and Tree-Width**
  - Why needed here: These structural parameters bound circuit complexity and determine when the meta-theorem applies.
  - Quick check question: Does bounded tree-width imply bounded rank-width? (Yes: r ≤ 3·2^(t-1))

## Architecture Onboarding

- Component map:
  - **Hardness Framework**: Set/subset-modelling properties → Lemmas 21-23 → hardness for specific models
  - **Algorithmic Framework**: Model → Boolean circuit (Lemmas 5-6, 13-14) → MSOE₁ formula → FPT via Proposition 3
  - **Bridge Component**: HOM/P-HOM problems connect hardness to explanation problems (Lemmas 19-20)

- Critical path:
  1. Identify model class and explanation type
  2. Check if set/subset-modelling applies → hardness route
  3. If bounded parameters exist, check circuit translation → algorithmic route
  4. For ensembles, verify constant majority gates for algorithmic results

- Design tradeoffs:
  - **Decision Trees vs. Decision Sets**: DTs admit polynomial LCXP (Theorem 8); DSs/DLs are W[2]-hard for same problem (Theorem 31). Trade simplicity of rule representation against explanation tractability.
  - **Single models vs. Ensembles**: Ensembles add expressive power but explanation complexity jumps from P to co-NP-hard or W[1]-hard even with bounded model size (Theorem 28).

- Failure signatures:
  - Attempting polynomial-time LCXP on Decision Sets → will fail for large instances (NP-hard)
  - Expecting FPT algorithms with unbounded ensemble size and majority gates → rank-width blowup
  - Assuming ordered DTs simplify explanations → Theorem 29 shows W[1]-hardness remains for global explanations

- First 3 experiments:
  1. **Translate a small DT ensemble (3-5 trees) to Boolean circuit**: Verify rank-width bound per Lemma 6; implement MSOE₁ encoding for LCXP.
  2. **Implement the polynomial LCXP algorithm for single DTs (Algorithm 1 adapted)**: Test on synthetic trees with varying depth; measure runtime scaling.
  3. **Construct hardness reduction instance**: Use Lemma 21 to encode a small HITTING SET instance as DS-LAXP; verify NP-hardness behavior empirically on small parameter ranges.

## Open Questions the Paper Calls Out

- **Open Question 1**: What is the computational complexity of counting the number of distinct minimal explanations for a given model and example?
  - Basis in paper: [explicit] The Conclusion states, "We still do not know how to count explanations efficiently: given a model and an example, how many distinct minimal explanations exist?"
  - Why unresolved: The paper focuses exclusively on the decision problems of finding or verifying a single explanation (LAXP, LCXP, etc.), rather than the counting variants.
  - What evidence would resolve it: A parameterized complexity classification (e.g., #P-completeness or FPT results) for the counting versions of the defined explanation problems.

- **Open Question 2**: Can we efficiently enumerate all minimal explanations for the model classes studied?
  - Basis in paper: [explicit] The Conclusion notes that "The enumeration problem is similarly unexplored" alongside the counting problem.
  - Why unresolved: The algorithms provided in the paper are designed to output a single solution or a boolean answer, not to list all possible valid explanations.
  - What evidence would resolve it: Algorithms for enumeration with polynomial delay or incremental polynomial time for decision trees, sets, and lists.

- **Open Question 3**: Under what conditions do explanation problems become tractable for weighted ensembles?
  - Basis in paper: [explicit] The Conclusion asks, "we do not yet understand when weighted voting might enable efficient explanation algorithms, particularly for polynomially-bounded weights."
  - Why unresolved: The current analysis is restricted to MAJ-ensembles (majority vote); the theoretical impact of weight distributions on the structural parameters (like rank-width) is unknown.
  - What evidence would resolve it: Hardness proofs or fixed-parameter tractable algorithms for explanation problems on ensembles with polynomially-bounded weights.

## Limitations

- The paper's theoretical results may not account for practical optimization techniques that could circumvent worst-case scenarios
- Algorithmic meta-theorem relies on external MSO solver performance whose characteristics are not characterized within the paper
- Analysis focuses exclusively on transparent models, leaving open questions about how results extend to more complex model families

## Confidence

- **High confidence**: The reduction framework (set-modelling/subset-modelling) and its application to decision sets/lists is mathematically rigorous with clear proof structure
- **Medium confidence**: The algorithmic meta-theorem's FPT claims depend on external MSO solver performance and the translation to Boolean circuits preserving rank-width bounds
- **Medium confidence**: The distinction between local and global explanation complexity for decision trees is well-established but assumes standard model definitions without considering practical implementation variations

## Next Checks

1. **Implement and benchmark the polynomial-time LCXP algorithm for decision trees** (Theorem 8) on synthetic datasets with varying depths to empirically verify the P complexity claim and identify any practical bottlenecks not captured by theoretical analysis

2. **Construct and solve a small instance of the LCXP reduction from Lemma 21** using a concrete HITTING SET instance to verify the NP-hardness claim for decision sets, measuring actual solver runtime behavior on small parameter ranges

3. **Translate a 3-5 tree ensemble to Boolean circuit representation** following Lemma 6 and verify rank-width bounds empirically, then encode a simple LCXP instance using the MSOE₁ framework to test the FPT meta-theorem's practical applicability