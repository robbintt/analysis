---
ver: rpa2
title: 'Qalb: Largest State-of-the-Art Urdu Large Language Model for 230M Speakers
  with Systematic Continued Pre-training'
arxiv_id: '2601.08141'
source_url: https://arxiv.org/abs/2601.08141
tags:
- urdu
- language
- pre-training
- qalb
- continued
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Qalb is an Urdu large language model developed through continued
  pre-training and instruction fine-tuning to address the underrepresentation of Urdu
  in NLP systems. The model was created by extending LLaMA-3.1 8B with 1.97 billion
  tokens of Urdu text data (1.84 billion Urdu tokens plus 140 million English tokens
  to prevent catastrophic forgetting), followed by fine-tuning on the Alif Urdu-instruct
  dataset.
---

# Qalb: Largest State-of-the-Art Urdu Large Language Model for 230M Speakers with Systematic Continued Pre-training

## Quick Facts
- arXiv ID: 2601.08141
- Source URL: https://arxiv.org/abs/2601.08141
- Reference count: 26
- Qalb achieves 90.34 overall score on Urdu benchmarks, outperforming previous state-of-the-art by 3.24 points

## Executive Summary
Qalb is an Urdu large language model developed through continued pre-training and instruction fine-tuning to address the underrepresentation of Urdu in NLP systems. The model was created by extending LLaMA-3.1 8B with 1.97 billion tokens of Urdu text data (1.84 billion Urdu tokens plus 140 million English tokens to prevent catastrophic forgetting), followed by fine-tuning on the Alif Urdu-instruct dataset. Qalb achieves state-of-the-art performance on Urdu benchmarks, scoring 90.34 overall and outperforming the previous best model Alif-1.0-Instruct (87.1) by 3.24 points and the base LLaMA-3.1 8B-Instruct model (45.7) by 44.64 points across seven diverse tasks including classification, sentiment analysis, and reasoning.

## Method Summary
Qalb was developed through a two-stage process: (1) continued pre-training on a bilingual corpus of 1.97 billion tokens (1.84B Urdu + 140M English Wikipedia) using LoRA adaptation (rank 128, alpha 32) on all linear layers plus embeddings and head, trained for 7,500 steps with bfloat16 precision on a single A100 80GB GPU; (2) instruction fine-tuning on the Alif Urdu-instruct dataset using the LLaMA-3 chat template with system prompts for Urdu assistant behavior. The approach leverages foundation model adaptation rather than training from scratch, combining extensive language exposure with task-specific instruction tuning.

## Key Results
- Qalb scores 90.34 overall on seven Urdu benchmarks, achieving state-of-the-art performance
- Outperforms previous best model Alif-1.0-Instruct (87.1) by 3.24 points
- Dramatically outperforms base LLaMA-3.1 8B-Instruct (45.7) by 44.64 points
- Successfully addresses Urdu's morphological complexity and right-to-left Nastaliq script through continued pre-training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continued pre-training on substantial target-language data transfers foundational linguistic knowledge that fine-tuning alone cannot inject.
- Mechanism: The base LLaMA-3.1 8B model has insufficient Urdu token exposure during original pre-training. By extending pre-training on 1.84 billion Urdu tokens before instruction fine-tuning, the model acquires morphological patterns, script representations (right-to-left Nastaliq), and cultural-linguistic priors. This creates a "knowledge-rich base" that subsequent fine-tuning can then shape into task-specific behaviors.
- Core assumption: Foundation models encode language-specific knowledge primarily during pre-training, and this knowledge is not recoverable through instruction data alone.
- Evidence anchors: [abstract] "Even the base LLaMA-3.1 8B-Instruct model shows limited capability in generating fluent, contextually appropriate Urdu text"; [section I] "No amount of fine-tuning can compensate for knowledge that was never acquired during pre-training"; [corpus] UrduLLaMA 1.0 paper shows similar approach with 128M Urdu tokens, suggesting token scale matters.

### Mechanism 2
- Claim: Bilingual replay buffers (140M English tokens mixed with Urdu) mitigate catastrophic forgetting of original capabilities.
- Mechanism: Exclusive training on new language data causes interference with previously learned representations. By interleaving 7.1% English Wikipedia tokens during continued pre-training, the model maintains activation pathways for English reasoning and general knowledge while Urdu pathways develop.
- Core assumption: The ratio of replay tokens (~7%) is sufficient to preserve original capabilities; optimal ratio is not empirically validated in this work.
- Evidence anchors: [abstract] "combined with 140 million tokens of English Wikipedia data to prevent catastrophic forgetting"; [section IV-A] "This addition serves as a replay buffer to maintain the model's general reasoning capabilities"; [corpus] Weak direct evidence—EvoLM paper analyzes training dynamics across stages but does not specifically validate replay buffer ratios for low-resource language adaptation.

### Mechanism 3
- Claim: High-rank LoRA (r=128) on all linear layers + embeddings preserves adaptation capacity for language acquisition better than low-rank configurations.
- Mechanism: LoRA decomposes weight updates into low-rank matrices. Higher rank (128 vs. typical 8-64) increases trainable parameter count to ~1.18B (14.72% of base), providing sufficient capacity to model Urdu's distinct morphology and script without full parameter updates.
- Core assumption: The rank-128 configuration captures sufficient representational capacity for language adaptation; this is not ablated against lower ranks.
- Evidence anchors: [section IV-B] "LoRA introduces trainable low-rank decomposition matrices... rank 128... trainable parameters ~1.18B"; [section IV-B] "This parameter-efficient approach makes continued pre-training feasible on a single GPU"; [corpus] No direct corpus evidence for optimal LoRA rank in language adaptation.

## Foundational Learning

- Concept: **Catastrophic Forgetting**
  - Why needed here: The paper's entire bilingual mixing strategy hinges on understanding that neural networks overwrite previously learned representations when trained exclusively on new distributions.
  - Quick check question: Can you explain why training only on Urdu would degrade English question-answering performance?

- Concept: **Continued Pre-training vs. Fine-tuning**
  - Why needed here: The paper distinguishes these stages—continued pre-training uses raw text (self-supervised) to build language knowledge; fine-tuning uses instruction-response pairs to shape behavior. Conflating them leads to suboptimal adaptation.
  - Quick check question: What loss function is used during continued pre-training vs. instruction fine-tuning?

- Concept: **LoRA (Low-Rank Adaptation)**
  - Why needed here: Understanding that weight updates W' = W + BA where B and A are low-rank matrices explains how 14.72% of parameters can be trainable while the base model remains frozen.
  - Quick check question: Why does the paper target embeddings and head in addition to linear layers?

## Architecture Onboarding

- Component map: Base LLaMA-3.1-8B (transformer decoder, 8B params) -> LoRA modules (r=128, alpha=32) on all linear layers + embeddings + output head -> Training framework: Unsloth (memory-optimized attention + fast LoRA kernels) -> Precision: bfloat16 with gradient checkpointing

- Critical path: 1. Data curation → 1.97B token bilingual corpus (95.31% Urdu word purity) → 2. Continued pre-training → LoRA adaptation on raw text (7,500 steps, LR 2e-5) → 3. Instruction fine-tuning → Alif Urdu-instruct dataset (2 epochs, LR 5e-5) → 4. Evaluation → GPT-4o-as-judge on 7 tasks

- Design tradeoffs: Token count (1.97B) vs. compute budget (single A100): Chosen scale is substantial but not exhaustive; LoRA rank (128) vs. memory: Higher rank preserves capacity at memory cost; English replay (7%) vs. Urdu exposure: Empirical choice, not optimized

- Failure signatures: Hallucinated signatures in outputs → indicates forum-style data leakage; Verbose generation / list-spamming → observed in Alif, partially addressed in Qalb; English capability degradation → would indicate insufficient replay buffer

- First 3 experiments: 1. Validate catastrophic forgetting hypothesis: Compare Qalb's English benchmark performance against base LLaMA-3.1-8B-Instruct to quantify replay buffer effectiveness; 2. Ablate LoRA rank: Train identical configurations at r=32, 64, 128 to determine capacity requirements for Urdu adaptation; 3. Test replay ratio sensitivity: Train variants with 0%, 5%, 10%, 15% English tokens to find minimum viable buffer

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does continued pre-training on Urdu data cause degradation in the model's reasoning and capabilities in languages other than English, and if so, to what extent?
- Basis in paper: [explicit] The authors state they "specifically integrated 140 million tokens of English Wikipedia data to prevent catastrophic forgetting" but acknowledge this is a "strategic inclusion" without evaluating whether this amount is sufficient or whether other languages/capabilities degraded.
- Why unresolved: The paper only reports Urdu benchmark performance and does not evaluate the model's retained capabilities in other languages or on general reasoning benchmarks.
- What evidence would resolve it: Evaluation of Qalb on standard English benchmarks (e.g., MMLU, GSM8K) and multilingual benchmarks to measure capability retention versus the base LLaMA-3.1 8B model.

### Open Question 2
- Question: What is the optimal ratio and composition of mixed-language data (Urdu-to-English) for continued pre-training to balance language acquisition with capability preservation?
- Basis in paper: [inferred] The paper uses a fixed ratio of 1.84B Urdu tokens to 140M English tokens (~93:7) without ablation studies on whether this ratio is optimal or how sensitive the results are to different mixing strategies.
- Why unresolved: No experiments compared different ratios or alternative approaches (e.g., interleaving vs. sequential training, different English sources beyond Wikipedia).
- What evidence would resolve it: Ablation experiments training multiple model variants with different Urdu-to-English ratios and measuring both Urdu benchmark performance and English capability retention.

### Open Question 3
- Question: Can the methodology of continued pre-training plus instruction fine-tuning be effectively transferred to other low-resource languages with Perso-Arabic scripts (e.g., Pashto, Sindhi, Punjabi) using similar token budgets?
- Basis in paper: [explicit] The authors explicitly state in Future Directions: "extending this methodology to other low-resource languages sharing similar scripts (Pashto, Sindhi, Punjabi)" as an open direction.
- Why unresolved: The paper only validates the approach for Urdu; it remains unknown whether the same token budget (1.97B tokens) and training configuration would suffice for related languages with different morphological complexity and data availability.
- What evidence would resolve it: Applying the same methodology to Pashto, Sindhi, and Punjabi, then evaluating on language-specific benchmarks to determine if similar performance gains are achievable.

## Limitations

- Data provenance and reproducibility gap: The Urdu pre-training corpus combines 1.84 billion Urdu tokens from five domains with 140 million English Wikipedia tokens, but exact source URLs, scrape dates, filtering thresholds, and the specific dataset version are not yet publicly available.
- English capability preservation: The catastrophic forgetting mitigation strategy (7.1% English Wikipedia replay) is empirically justified but not experimentally validated through English benchmark measurements.
- Instruction fine-tuning protocol opacity: The Alif Urdu-instruct dataset and fine-tuning protocol reference external work, but the exact GPT-4o evaluation prompt template, system prompt for the Urdu assistant, and loss masking strategy are not fully specified in this paper.

## Confidence

- High Confidence: Qalb achieves state-of-the-art performance on Urdu benchmarks (90.34 overall score, outperforming Alif-1.0-Instruct by 3.24 points)
- Medium Confidence: The continued pre-training + instruction fine-tuning approach effectively adapts LLaMA-3.1 8B to Urdu
- Medium Confidence: The 140 million English token replay buffer effectively prevents catastrophic forgetting
- Low Confidence: The rank-128 LoRA configuration is optimal for Urdu adaptation

## Next Checks

1. **English capability preservation validation**: Evaluate Qalb on standard English benchmarks (MMLU, GSM8K, BBH) and compare performance against base LLaMA-3.1 8B-Instruct to quantify the actual impact of the 7.1% English replay buffer on English reasoning capabilities.

2. **LoRA rank ablation study**: Train identical configurations with LoRA ranks 32, 64, and 128 to determine the minimum rank that achieves comparable Urdu performance, establishing whether the rank-128 choice is capacity-optimal or over-provisioned.

3. **Replay buffer ratio sensitivity analysis**: Train variants with 0%, 5%, 10%, and 15% English Wikipedia tokens in the pre-training corpus to identify the minimum viable replay ratio for preventing catastrophic forgetting while maximizing Urdu exposure.