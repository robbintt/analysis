---
ver: rpa2
title: 'Taming Data Challenges in ML-based Security Tasks: Lessons from Integrating
  Generative AI'
arxiv_id: '2507.06092'
source_url: https://arxiv.org/abs/2507.06092
tags:
- data
- genai
- samples
- security
- schemes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how generative AI can address data challenges
  in machine learning-based security classification tasks. It evaluates six state-of-the-art
  generative AI methods and introduces a novel scheme, Nimai, which enables controlled
  data synthesis through sample-conditioning in a discrete latent space.
---

# Taming Data Challenges in ML-based Security Tasks: Lessons from Integrating Generative AI

## Quick Facts
- arXiv ID: 2507.06092
- Source URL: https://arxiv.org/abs/2507.06092
- Reference count: 40
- Primary result: Generative AI can improve classifier performance by up to 32.6% in data-constrained security tasks and enable rapid adaptation to concept drift

## Executive Summary
This paper investigates how generative AI can address data challenges in machine learning-based security classification tasks. The authors evaluate six state-of-the-art generative AI methods and introduce a novel scheme, Nimai, which enables controlled data synthesis through sample-conditioning in a discrete latent space. Through extensive experimentation across seven security datasets, the study demonstrates that generative AI can significantly improve classifier performance—up to 32.6% gain in severely data-constrained settings—and facilitate rapid adaptation to concept drift with minimal labeling. However, the work also reveals important limitations, including initialization failures on high-dimensional datasets and challenges with overlapping classes, sparse features, and noisy labels.

## Method Summary
The study trains generative models on security datasets to produce synthetic data that augments limited training sets. The process involves normalizing features via MinMax scaling, training GenAI models (TVAE, CTAB-GAN+, TabDDPM, TabSyn, GReaT, REaLTabFormer, or Nimai), generating class-balanced synthetic samples through either class-conditioning or sample-conditioning approaches, and training downstream classifiers on the augmented data. Nimai uses a VQ-VAE with discrete latent space and Masked Token Modeling for sample-conditioned generation. Evaluation measures Macro F-score improvements (ΔG) across 10 random seeds, with hyperparameter tuning via ASHA/TPE search for some methods.

## Key Results
- Generative AI improved classifier performance by up to 32.6% in data-constrained scenarios (IoT task with 10% training data)
- Nimai-S (sample-conditioned) achieved mean ΔG of 17.37% for IoT, outperforming all other schemes
- Class-conditioned approaches like TVAE achieved 22.8% ΔG for Tor, the best performer for that task
- LLM-based methods (GReaT, REaLTabFormer) failed to instantiate on 5-6 of 7 tasks due to token limits and memory constraints
- Hybrid conditioning (Nimai-hybrid) enabled rapid drift recovery, achieving 53.92% F-score with only 64 labeled samples for severe drift

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sample-conditioned synthetic data generation can significantly improve classifier performance for data-challenged security tasks with complex in-class biases.
- Mechanism: Nimai encodes existing samples into a discrete latent space via VQ-VAE, then uses Masked Token Modeling (MTM) to generate synthetic samples in the local vicinity of underrepresented real samples. This targets specific regions of the data manifold rather than broadly mimicking class distributions.
- Core assumption: Security datasets contain underrepresented attack patterns within classes that can be effectively augmented through local neighborhood sampling.
- Evidence anchors:
  - [abstract]: "introduces a novel scheme, Nimai, which enables controlled data synthesis through sample-conditioning in a discrete latent space"
  - [section 7.1]: "Nimai-S achieves a mean ΔG of 17.37% outperforming all the schemes" for IoT; "Nimai-S... achieved the best performance gain in 3/4 tasks"
  - [corpus]: Limited direct corpus validation for sample-conditioning specifically in security contexts.
- Break condition: Fails when classes have significant overlap or noisy labels, as seen in Tor, Cookie, and nPrintML tasks where GenAI methods produced negative or unreliable gains.

### Mechanism 2
- Claim: Class-conditional generation outperforms sample-conditioning for tasks experiencing concept drift by exploring novel regions of the data manifold.
- Mechanism: Generate synthetic samples conditioned only on class labels rather than proximity to existing samples, allowing the model to produce data points beyond the immediate neighborhood of training samples.
- Core assumption: Concept drift shifts test distributions to regions not well-represented locally in training data, requiring broader manifold exploration.
- Evidence anchors:
  - [section 7.1]: "class-conditioned GenAI approaches are well suited for tasks where there is concept drift... Nimai-C achieves the best performance in 6 out of the 8 months"
  - [section 7.1]: For BODMAS months 5-6 with severe drift, "TVAE outperforms all schemes only in month 6" with class-conditioning
  - [corpus]: Related work discusses GenAI for security but doesn't specifically validate class-conditional vs sample-conditional for drift scenarios.
- Break condition: Class-conditional generation struggles when test drift is too extreme or when training data lacks diversity within classes.

### Mechanism 3
- Claim: Hybrid conditioning (combining sample and class conditioning) enables rapid post-deployment recovery from concept drift with minimal labeling effort.
- Mechanism: Use uncertainty sampling to select a small set of drifted samples, generate sample-conditioned variants from these to capture drift characteristics, then fill remaining augmentation needs with class-conditional generation.
- Core assumption: A small set of labeled drifted samples can anchor the generation of drift-appropriate synthetic data without full retraining.
- Evidence anchors:
  - [abstract]: "GenAI can facilitate rapid adaptation to concept drift post-deployment, requiring minimal labeling in the adjustment process"
  - [section 8]: "Nimai-hybrid outperforms Nimai-C" for month 6 (worst drift month), achieving 53.92% vs 43.09% F-score with only 64 labeled samples
  - [corpus]: No corpus papers validate the hybrid conditioning approach for drift recovery.
- Break condition: Uncertainty sampling may select unrepresentative samples when drift is subtle or when classifier confidence is miscalibrated.

## Foundational Learning

- Concept: **Variational Autoencoder (VAE) with discrete latent space**
  - Why needed here: Nimai's architecture depends on understanding how continuous inputs map to discrete codebook vectors, enabling controlled generation through latent space manipulation.
  - Quick check question: Can you explain why a discrete latent space might offer more interpretable generation control than a continuous latent space for tabular security data?

- Concept: **Class imbalance vs. in-class bias**
  - Why needed here: The paper distinguishes between global class imbalance (attack vs. benign) and local in-class bias (underrepresented attack patterns within a class), requiring different augmentation strategies.
  - Quick check question: Given a malware classifier with 1000 benign and 100 malware samples, where the malware samples are 90% ransomware and 10% spyware, what type of bias exists and which conditioning approach might help?

- Concept: **Concept drift in adversarial settings**
  - Why needed here: Security classifiers face non-stationary distributions as attackers adapt; understanding temporal distribution shift is critical for post-deployment maintenance strategies.
  - Quick check question: If a malware classifier trained on 2023 samples sees a 40% performance drop on 2024 samples, what are three possible causes and how might GenAI help with each?

## Architecture Onboarding

- Component map: Data preprocessor -> GenAI trainer -> Synthetic generator -> Augmented dataset builder -> Classifier trainer
- Critical path:
  1. Validate GenAI model can instantiate on your task (Section 7.1 shows 5/6 existing schemes failed on ≥1 task)
  2. Choose conditioning strategy based on data challenge type (sample-conditioning for in-class bias, class-conditioning for drift)
  3. Run hyperparameter search (only TabDDPM and Nimai provide automated tuning; others use defaults)
  4. Generate synthetic samples to balance classes
  5. Train and evaluate classifier on augmented data

- Design tradeoffs:
  - **Sample-conditioning vs. class-conditioning**: Sample-conditioning offers fine-grained control for in-class bias but may not explore novel regions; class-conditioning explores broader manifold but less targeted
  - **LLM-based vs. VAE/GAN/Diffusion**: LLM schemes (GReaT, REaLTabFormer) failed on 5-6/7 tasks due to token limits and memory, while VAE-based methods were more stable
  - **Automated vs. manual hyperparameter tuning**: TabDDPM and Nimai use ASHA/TPE search (longer setup, better adaptation); others require manual tuning or use defaults

- Failure signatures:
  - **Mode collapse** (TabDDPM on IoT): Generated only majority class samples, classifier labels everything as majority
  - **Token overflow** (GReaT on high-dimensional tasks): "5,714 hours for a single epoch" or "exceeding token limit 1024"
  - **Memory overflow** (TabSyn, REaLTabFormer on large feature sets): Failed to fit VAE or encoded features into GPU memory
  - **Rejection sampling failure** (TVAE on Cookie): "failed to generate new samples, even after 100 hours"
  - **Class overlap degradation** (all methods on Tor/Cookie): Generated samples amplify noise, negative gains

- First 3 experiments:
  1. **Baseline establishment**: Train classifier on real data only, measure macro F-score (or malicious class F-score for imbalanced binary tasks). Reproduce Table 1 baseline performance.
  2. **GenAI feasibility check**: Attempt to instantiate each GenAI scheme on your task; document training time per epoch (Table 8 shows 0.6s to 1041s range) and any failures. Prioritize TVAE, CTAB-GAN+, and Nimai for stability.
  3. **Conditioning strategy comparison**: For one GenAI scheme that supports both (Nimai), compare sample-conditioned (Nimai-S) vs. class-conditioned (Nimai-C) augmentation on validation set. Use Section 6's coefficient of variation (CV) threshold of 1.0 to identify unreliable results across 10 random seeds.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Generative AI schemes be adapted to effectively recover classifier performance following adversarial attacks?
- Basis in paper: [explicit] Section 9 states, "Future work could explore using GenAI to recover from adversarial attacks," and Section 2 explicitly notes, "We leave adversarial examples for future work."
- Why unresolved: The current study limited its scope to natural data challenges (imbalance, drift) and did not evaluate the ability of synthetic data to patch vulnerabilities introduced by malicious perturbations in the problem space.
- Evidence needed: Empirical results demonstrating that augmenting training data with GenAI samples can restore classifier accuracy against adaptive adversarial attacks (e.g., evasive malware) better than standard retraining.

### Open Question 2
- Question: How can LLM-based tabular data generators be re-engineered to handle the high dimensionality and token constraints of security datasets?
- Basis in paper: [explicit] Section 9 suggests, "Re-engineering LLM-based approaches like GReaT and REaLTabFormer for greater scalability with large, high-dimensional security datasets offers several benefits."
- Why unresolved: The study found that current LLM approaches failed to instantiate on most tasks (5/7 and 6/7) because encoding high-dimensional features (e.g., >1,700 features) exceeded the token limits (1024) of the backend LLMs.
- Evidence needed: A modified LLM-based scheme that successfully trains and generates useful synthetic data for high-dimensional tasks like Cookie or nPrintML without succumbing to memory or token errors.

### Open Question 3
- Question: Can GenAI augmentation techniques be modified to improve performance in security tasks characterized by noisy labels and overlapping class distributions?
- Basis in paper: [explicit] Section 9 identifies "overlapping class distributions, noisy labels, [and] sparse feature vectors" as specific "challenging scenarios" for future work where current methods failed to provide gains.
- Why unresolved: In the evaluation (Section 7.2), GenAI schemes yielded negative or unreliable gains on tasks like Tor and Cookie, often struggling to distinguish classes with significant overlap or label noise.
- Evidence needed: A novel GenAI strategy that yields statistically significant positive ΔG (performance gain) on the Tor or Cookie datasets by explicitly accounting for label noise or class overlap during synthesis.

### Open Question 4
- Question: Can architectures other than VAEs (such as Diffusion models or GANs) effectively implement highly controlled, sample-conditioned data synthesis for security applications?
- Basis in paper: [explicit] Section 9 recommends that "Future work can explore approaches like LLMs, Diffusion models, and GANs" to replicate the controlled generation capabilities introduced by the authors' VAE-based Nimai scheme.
- Why unresolved: While Nimai proved sample-conditioning effective, the authors did not evaluate if other model families could achieve similar or better control over the data manifold with greater stability or efficiency.
- Evidence needed: A comparative analysis showing that a sample-conditioned Diffusion or GAN model can match or exceed the performance improvements of Nimai-S on low-data tasks like BGP.

## Limitations
- Sample-conditioning scalability: Nimai's VQ-VAE architecture may struggle with extremely high-dimensional security datasets or complex feature interactions
- Concept drift recovery guarantees: Uncertainty sampling can be unreliable in adversarial settings where attackers craft samples near decision boundaries
- Generalizability across security domains: Results based on 7 specific datasets may not transfer to domains with different feature characteristics

## Confidence
- **High confidence**: Sample-conditioning improves in-class bias correction for moderately imbalanced tasks (IoT, BGP results are robust across 10 random seeds)
- **Medium confidence**: Class-conditioning effectively addresses concept drift when drift magnitude is moderate and training data contains sufficient within-class diversity (BODMAS months 1-4 results)
- **Medium confidence**: LLM-based generative models struggle with high-dimensional security data due to token limits and memory constraints (consistent failures across multiple tasks)
- **Low confidence**: Hybrid conditioning provides reliable post-deployment drift recovery in all scenarios (only validated on single dataset with specific drift patterns)

## Next Checks
1. **Stress test Nimai's sample-conditioning on high-dimensional data**: Evaluate Nimai-S on a security dataset with >1000 features to verify the discrete latent space approach scales effectively and maintains generation quality
2. **Adversarial robustness evaluation**: Test whether generated samples can fool the downstream classifier or if attackers can exploit predictable generation patterns in sample-conditioned synthesis
3. **Cross-dataset transfer learning validation**: Train Nimai on one security domain (e.g., network traffic) and evaluate its ability to generate useful synthetic data for a different domain (e.g., malware classification) to assess generalizability