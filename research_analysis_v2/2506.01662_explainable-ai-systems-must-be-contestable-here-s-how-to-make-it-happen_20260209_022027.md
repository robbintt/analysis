---
ver: rpa2
title: 'Explainable AI Systems Must Be Contestable: Here''s How to Make It Happen'
arxiv_id: '2506.01662'
source_url: https://arxiv.org/abs/2506.01662
tags:
- contestability
- system
- contestation
- systems
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the lack of formal contestability definitions
  and practical frameworks in explainable AI (XAI), a requirement increasingly mandated
  by global AI regulations but not yet operationalized. The authors propose a rigorous,
  multidimensional definition of contestability, distinguishing it from explainability
  by its focus on stakeholder agency and actionable recourse.
---

# Explainable AI Systems Must Be Contestable: Here's How to Make It Happen

## Quick Facts
- arXiv ID: 2506.01662
- Source URL: https://arxiv.org/abs/2506.01662
- Authors: Catarina Moreira; Anna Palatkina; Dacia Braca; Dylan M. Walsh; Peter J. Leihn; Fang Chen; Nina C. Hubig
- Reference count: 40
- Key outcome: Introduces a rigorous, multidimensional framework and composite metric (CAS) for quantifying contestability in AI systems, demonstrating that current systems score poorly but can improve significantly through targeted interventions.

## Executive Summary
This paper addresses the critical gap between regulatory requirements for AI contestability and the lack of practical frameworks to implement it. The authors establish contestability as distinct from explainability, emphasizing that true contestability requires actionable recourse pathways alongside explanations. They propose a comprehensive framework with four dimensions—human-centered, technical, legal, and organizational—and develop the Contestability Assessment Scale (CAS), a weighted composite metric that enables systematic evaluation and improvement of contestability across domains.

## Method Summary
The authors develop the Contestability Assessment Scale (CAS) as a weighted composite metric aggregating eight system properties: explainability, openness to contestation, traceability, built-in safeguards, adaptivity, auditing, ease of contestation, and explanation quality. CAS is calculated as Σ λₚ·sₚ·nₚ with specific weights (λ₁=0.30 for explainability, λ₂₋₄=0.12, λ₅₋₆=0.10, λ₇₋₈=0.07) and normalization factors. The framework is validated through case studies in healthcare, finance, and media domains, demonstrating current system scores (CAS 0.32–0.44) and potential improvements (up to CAS 0.85) through targeted interventions.

## Key Results
- Current AI systems demonstrate low contestability scores (CAS 0.32–0.44) across healthcare, finance, and media domains
- Implementing the proposed framework can improve contestability scores up to CAS 0.85 through targeted interventions
- Healthcare case study shows post-hoc system (CAS 0.551) can reach CAS 0.927 with by-design contestability mechanisms
- The framework successfully bridges regulatory theory and practical implementation with concrete assessment tools

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contestability can be quantified through a weighted composite metric (CAS) that evaluates eight system properties.
- Mechanism: The Contestability Assessment Score aggregates weighted scores from explainability, openness to contestation, traceability, built-in safeguards, adaptivity, auditing, ease of contestation, and explanation quality. Each property is normalized and weighted by priority level.
- Core assumption: The eight selected properties capture the essential dimensions of contestability, and their weighted combination reflects real-world priority relationships.
- Evidence anchors:
  - [abstract] "We propose the Contestability Assessment Scale, a composite metric built on more than twenty quantitative criteria."
  - [section 5] Defines CAS = Σ λₚ·sₚ·nₚ with explicit weight hierarchy and normalization.
  - [corpus] Related work on argumentation-based contestability supports decomposable evaluation but does not validate the specific weighting scheme.

### Mechanism 2
- Claim: Contestability requires coupling explanations with actionable recourse pathways.
- Mechanism: Explanation-Level Contestability (XLC) ensures explanations map to contestation actions: r → c ∈ C. Explanations alone are insufficient; they must enable stakeholders to identify and execute challenges within their capability constraints P(s).
- Core assumption: Users can translate explanations into appropriate contestation actions when such mappings exist.
- Evidence anchors:
  - [section 3.2] "Contest_XLC(E) ⟺ ∀d∈D,∀s∈S,∃r=E(d)∈R: r→c∈C, given P(s)."
  - [section 4.2] "Explainability–Recourse Coupling: Explanations must not only clarify why a decision was made but also show how it can be contested or changed."

### Mechanism 3
- Claim: By-design contestability mechanisms outperform post-hoc approaches for system-level adaptation.
- Mechanism: Cdesign provides access to model internals, enabling fine-grained audits and explanation–recourse coupling. Cpost relies on approximate explanations and external appeals. The aggregate contestation space C = Cdesign ∪ Cpost, but design-time integration enables systemic adaptation.
- Core assumption: Transparent or modular architectures are feasible for the deployment context.
- Evidence anchors:
  - [section 3.3] "By-design mechanisms offer immediacy and efficiency, post-hoc tools ensure accountability in systems lacking built-in contestation."
  - [section 6.1] Healthcare case study: post-hoc system scored CAS 0.551; implementing by-design mechanisms raised CAS to 0.927.

## Foundational Learning

- Concept: **Distinction between explainability and contestability**
  - Why needed here: The paper explicitly separates contestability (agency-focused, action-oriented) from explainability (understanding-focused). Without this distinction, practitioners may conflate providing explanations with enabling contestation.
  - Quick check question: Can a user who receives an explanation for a loan denial identify what they could change to get approved, and do they have a pathway to challenge the decision?

- Concept: **Weighted composite scoring with priority hierarchies**
  - Why needed here: CAS assigns different weights to properties based on "consequentiality" dependencies. Explainability (λ=0.30) is foundational; other properties build upon it.
  - Quick check question: If you increase ease of contestation from 2/10 to 8/10 but explainability remains at 0 (no explanations), would CAS meaningfully improve contestability?

- Concept: **Context sensitivity constraints (τ, Ω, ∆, Γ)**
  - Why needed here: Contestability is bounded by latency, opacity, capability disparities, and adaptivity constraints. These define when mechanisms are infeasible.
  - Quick check question: For a high-frequency trading bot making decisions in milliseconds, which contestability mode is appropriate and what alternative safeguards apply?

## Architecture Onboarding

- Component map:
  1. **CAS Calculator Module**: Accepts 8 property scores, applies weights (λ₁₋₈), normalizes, outputs composite score [0,1]
  2. **Contestation Logger**: Records all challenge events with audit trail, retention policy, and stakeholder access controls
  3. **Explanation–Recourse Coupler**: Maps explanation components to available contestation actions
  4. **Stakeholder Capability Assessor**: Evaluates P(s) for different user classes to determine appropriate contestation pathways
  5. **Adaptation Feedback Loop**: Ingests contestation outcomes to trigger model updates or process revisions

- Critical path:
  1. Assess current system across 8 CAS properties using self-questionnaire (Appendix D)
  2. Classify system by AI reliance level (Low/Medium/High) and risk category
  3. Map to taxonomy matrix (Appendix C) to identify required contestability criteria
  4. Identify gap between current CAS and target CAS based on regulatory requirements
  5. Prioritize improvements by feasibility (highly vs. moderately feasible)

- Design tradeoffs:
  - **Explainability depth vs. model performance**: Intrinsically explainable models (score 2) may sacrifice accuracy; post-hoc explanations (score 1) preserve performance but reduce contestability quality
  - **Broad access vs. abuse prevention**: Open stakeholder access (score 2) increases contestability but may enable spam or gaming
  - **External audits vs. IP protection**: Independent audits (score 2) enhance credibility but may conflict with proprietary constraints (Ω)

- Failure signatures:
  1. **High CAS, low contestation success rate**: Suggests mechanisms exist on paper but are not usable in practice
  2. **Explanations provided but no recourse pathways**: XLC condition violated; explanations are descriptive not justificatory
  3. **Logs exist but inaccessible to stakeholders**: Traceability scores high but auditability fails in practice
  4. **Contestation possible only for experts**: Openness score of 1, capability disparities (∆) unaddressed

- First 3 experiments:
  1. Run CAS self-assessment on your current system using the questionnaire in Appendix D; identify the lowest-scoring property and test one targeted improvement
  2. Select 10 users from different stakeholder classes; measure whether they can identify contestation pathways and complete a challenge within defined time bounds
  3. Implement one explanation–recourse coupling: for your most common decision type, add explicit "how to contest" guidance to the explanation output and measure changes in challenge rates and success rates over 30 days

## Open Questions the Paper Calls Out
None

## Limitations
- The framework relies on self-assessment for CAS scoring, which may introduce measurement bias without independent verification protocols
- The specific weighting scheme (λ₁=0.30, others=0.12-0.07) lacks empirical validation to demonstrate optimal prioritization across domains
- Practical implementation guidance for explanation–recourse coupling is underdeveloped, particularly for complex technical explanations

## Confidence
- High: The distinction between explainability and contestability is well-founded and necessary; the multi-dimensional taxonomy provides a comprehensive framework structure.
- Medium: The CAS formula and weighted aggregation methodology are sound, but empirical validation of property weights and their domain transferability requires further study.
- Low: The practical implementation guidance for explanation–recourse coupling is underdeveloped, particularly for complex technical explanations.

## Next Checks
1. **Weight Validation Study**: Conduct a Delphi study with domain experts to validate or adjust the property weights across healthcare, finance, and media contexts to ensure the CAS reflects domain-specific contestability priorities.

2. **User Capability Impact Assessment**: Test the framework with users having varying technical backgrounds to measure how capability disparities (∆) affect their ability to translate explanations into contestation actions, and adjust the framework accordingly.

3. **Independent CAS Scoring**: Implement a blind scoring protocol where different evaluators assess the same system using the framework, measuring inter-rater reliability to identify subjective scoring elements that require standardization.