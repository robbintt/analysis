---
ver: rpa2
title: 'Bigram Subnetworks: Mapping to Next Tokens in Transformer Language Models'
arxiv_id: '2504.15471'
source_url: https://arxiv.org/abs/2504.15471
tags:
- bigram
- subnetwork
- subnetworks
- parameters
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces "bigram subnetworks" in Transformer language
  models, which are minimal parameter subsets that enable next-token predictions based
  only on the current token. Using continuous sparsification, the authors identify
  these subnetworks in models up to 1 billion parameters, finding they consistently
  require only about 10 million parameters (0.17% of non-embedding parameters in the
  largest model) while achieving bigram prediction correlations above 0.95.
---

# Bigram Subnetworks: Mapping to Next Tokens in Transformer Language Models

## Quick Facts
- arXiv ID: 2504.15471
- Source URL: https://arxiv.org/abs/2504.15471
- Authors: Tyler A. Chang; Benjamin K. Bergen
- Reference count: 40
- Primary result: Identified sparse "bigram subnetworks" requiring ~10M parameters that achieve >0.95 correlation with bigram predictions while being functionally necessary for model performance

## Executive Summary
This paper introduces "bigram subnetworks" - minimal parameter subsets in Transformer language models that enable next-token predictions based only on the current token. Using continuous sparsification, the authors identify these subnetworks in models up to 1 billion parameters, finding they consistently require only about 10 million parameters (0.17% of non-embedding parameters) while achieving bigram prediction correlations above 0.95. These subnetworks are concentrated in the first MLP layer, persist throughout pretraining, and recreate key properties of the full model's residual stream. Crucially, ablation experiments show these subnetworks are necessary for model performance - removing them drastically hurts language modeling ability, with similar effects to removing optimally-pruned subnetworks.

## Method Summary
The authors use continuous sparsification to identify sparse parameter subsets ("bigram subnetworks") within frozen Transformer models. The method optimizes a binary mask over non-embedding parameters using a loss combining bigram cross-entropy and L1 sparsity penalty. Temperature annealing gradually forces mask values toward {0,1}. They validate subnetworks by measuring correlation between subnetwork surprisals and bigram surprisals, and test functional necessity through ablation experiments comparing bigram subnetworks to optimally-pruned subnetworks.

## Key Results
- Bigram subnetworks consistently require ~10M parameters regardless of model size (0.17% of non-embedding parameters in 1B model)
- Achieved bigram prediction correlations above 0.95 across all tested models
- Subnetworks are concentrated in first MLP layer (82% in MLP layers overall)
- Ablation of bigram subnetworks causes performance degradation comparable to optimal pruning ablation
- Bigram subnetwork parameters partially overlap with optimal pruning subnetworks (38.0% overlap in 1B model)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The first MLP layer performs a sharp rotation from current-token representations toward next-token prediction space.
- Mechanism: Layer 0→1 transformation increases median rotation to input (current token) embeddings while decreasing rotation to output (next token) activations, measured via eigenvalue angles of linear maps fitted between layer activations and input/output targets.
- Core assumption: Rotation magnitude reflects representational alignment—lower rotation to output indicates closer alignment with next-token space.
- Evidence anchors:
  - [abstract]: "the first layer induces a sharp change that aligns activations with next token predictions rather than current token representations"
  - [section 5.1]: "in GPT-2 large, the first layer increases the median rotation to the input activations from 0.0 to 56.2 degrees; it decreases the median rotation to the output activations from 70.5 to 46.3 degrees"
  - [corpus]: "The Geometry of Tokens in Internal Representations of Large Language Models" investigates token geometry across layers but does not directly confirm the rotation mechanism
- Break condition: If first-layer rotation magnitude is small, or if rotation does not correlate with next-token prediction quality across models

### Mechanism 2
- Claim: Approximately 10M carefully selected parameters are sufficient to reconstruct bigram predictions, independent of model scale.
- Mechanism: Continuous sparsification optimizes a mask M over frozen parameters using Loss = CrossEntropy(P_bigram, MaskedModel(x)) + λ||M||₁/|M|; sigmoid temperature annealing forces mask values toward {0,1}, yielding a minimal binary subnetwork.
- Core assumption: Bigram information is concentrated in specific parameters amenable to sparse extraction, rather than distributed broadly.
- Evidence anchors:
  - [abstract]: "consistently require only about 10 million parameters (0.17% of non-embedding parameters in the largest model) while achieving bigram prediction correlations above 0.95"
  - [section 3.2]: "bigram correlations plateau at roughly 10M active parameters regardless of model size"
  - [corpus]: "Scaling Laws for Gradient Descent and Sign Descent for Linear Bigram Models under Zipf's Law" discusses bigram optimization properties but does not directly validate the sparsity mechanism
- Break condition: If required parameter count scales with model size, or if correlation drops sharply below the plateau

### Mechanism 3
- Claim: Bigram subnetworks are functionally critical because they substantially overlap with optimal pruning subnetworks.
- Mechanism: Parameters serving both bigram reconstruction and optimal pruning indicate shared computational infrastructure; ablating the bigram subnetwork removes core predictive capability similarly to ablating an optimal subnetwork.
- Core assumption: The observed overlap reflects shared functional role rather than coincidental selection.
- Evidence anchors:
  - [abstract]: "removing them drastically hurts language modeling ability, with similar effects to removing optimally-pruned subnetworks"
  - [section 6.2]: "in Pythia 1B, the bigram subnetwork and optimal subnetwork parameter overlap is 15.3× greater than would be expected from chance (p <0.0001); 38.0% of the bigram subnetwork is contained in the optimal subnetwork"
  - [corpus]: No direct corpus support for the overlap mechanism was found
- Break condition: If random subnetworks produce similar ablation effects, or if overlap is fully explained by parameter distribution biases across layers

## Foundational Learning

- Concept: Residual Stream Hypothesis
  - Why needed here: The paper's rotation and covariance analyses assume activations flow through a shared representational space across layers; understanding these analyses requires this conceptual foundation.
  - Quick check question: Why does comparing rotation-to-input vs. rotation-to-output at each layer reveal a current-token to next-token transformation?

- Concept: N-gram Language Models
  - Why needed here: Bigram subnetworks approximate P(wi|wi−1); understanding what is being approximated—and what information is discarded—requires basic n-gram knowledge.
  - Quick check question: What predictive information would a bigram model lose compared to a trigram model?

- Concept: Continuous Sparsification and Lottery Ticket Hypothesis
  - Why needed here: The core method identifies sparse subnetworks within trained models via learned masks; understanding why this works requires familiarity with sparsification theory.
  - Quick check question: Why does the L1 penalty encourage sparsity, and what role does temperature annealing play in forcing a binary mask?

## Architecture Onboarding

- Component map:
Input tokens → Token Embedding (excluded from mask) → Transformer Block 0 → Attention {Q, K, V, O matrices} → MLP {up projection, down projection} ← BIGRAM SUBNETWORK CONCENTRATED HERE → Transformer Blocks 1..N → Final LayerNorm → Unembedding (excluded from mask) → Logits

- Critical path: First MLP layer parameters → rotation toward next-token space → residual stream propagation → unembedding → prediction

- Design tradeoffs:
  - Sparsity penalty λ: Higher λ yields fewer active parameters but risks lower correlation; usable range is λ∈[10,100] depending on model size
  - Subnetwork size vs. fidelity: Correlation plateaus near 10M parameters; smaller subnetworks lose predictive fidelity
  - Layer focus: First layer is most critical, but subnetwork parameters spread to attention layers later in training

- Failure signatures:
  - Bigram surprisal correlation <0.90: Likely insufficient active parameters or incorrect parameter selection
  - Rotation patterns resemble random subnetwork: Subnetwork does not capture real model structure
  - Ablation causes minimal loss increase: Subnetwork is not functionally critical (possible training failure)
  - Text generation collapses to repetitive single-token output: Successful bigram subnetwork ablation

- First 3 experiments:
  1. Run continuous sparsification on Pythia 70M with λ∈{10,50,100}; verify that correlation plateaus near 10M active parameters as claimed
  2. Compute median rotation to input/output activations per layer for your trained subnetwork; compare against a size-matched random subnetwork control
  3. Ablate the learned bigram subnetwork and measure held-out evaluation loss increase; compare to random subnetwork ablation to confirm functional criticality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do bigram subnetworks constitute true "intermediate predictions" or are they merely deeply integrated computational components?
- Basis in paper: [explicit] The authors state in Section 7, "It remains ambiguous whether these early-layer computations constitute 'intermediate bigram predictions'... or whether the bigram subnetwork becomes a more deeply integrated part of model processing, not separable as an 'intermediate prediction'."
- Why unresolved: Current probing methods (tuned vs. untuned lenses) are either too weak (false negatives) or too strong (false positives), failing to definitively isolate intermediate states from the residual stream.
- What evidence would resolve it: Development of probing techniques or causal interventions that can distinguish between a distinct intermediate prediction and a distributed computational step without relying on overly flexible linear transformations.

### Open Question 2
- Question: Does the exaptation of bigram subnetwork parameters for complex predictions explain their reduced efficiency late in pretraining?
- Basis in paper: [explicit] In Section 4.1, regarding the observation that subnetworks require more parameters to achieve high correlation later in training, the authors suggest, "One potential explanation may be that the parameters involved in the bigram subnetwork are partially exapted to make more nuanced predictions later in pretraining."
- Why unresolved: The paper observes the phenomenon (decompression) but does not isolate the mechanism or prove that the same specific parameters are being repurposed for higher-order tasks.
- What evidence would resolve it: Tracking individual parameter importance or neuron polysemanticity within the bigram subnetwork over training to see if they increasingly contribute to non-bigram, context-dependent tasks.

### Open Question 3
- Question: Can the performance gap between bigram subnetworks and full models be closed by iteratively adding interpretable circuits?
- Basis in paper: [explicit] The authors conclude in Section 7 by proposing that future work could "build language models up from component circuits," aiming to "shrink the performance gap between fully interpretable model subnetworks... and their corresponding full models."
- Why unresolved: This study only establishes the existence and necessity of the bigram foundation; it does not attempt to reconstruct the full model's capabilities by re-adding specific circuits (e.g., induction heads) on top of this base.
- What evidence would resolve it: A constructive study where identified circuits are sequentially added to the sparse bigram subnetwork to measure the recovery of perplexity and specific capabilities.

## Limitations
- Scale dependence of bigram subnetworks is unclear - why parameter count plateaus rather than scaling with model size
- Generalization to non-English and specialized domains remains untested
- Functional necessity vs. sufficiency distinction not fully established

## Confidence
**High Confidence Claims**:
- Continuous sparsification reliably identifies sparse subnetworks with >0.95 correlation
- Bigram subnetworks concentrated in early layers, particularly first MLP layer
- Ablation causes significant performance degradation comparable to optimal pruning ablation
- Parameter count (~10M) is stable across model scales

**Medium Confidence Claims**:
- Rotation mechanism is primary driver of bigram prediction capability
- Overlap with optimal pruning subnetworks indicates shared functional roles
- Subnetwork structure persists and evolves through pretraining

**Low Confidence Claims**:
- Precise relationship between rotation magnitude and prediction quality across architectures
- Whether 10M parameter plateau represents fundamental limit or optimization artifact
- Extent to which bigram subnetworks capture full complexity of next-token prediction

## Next Checks
**Check 1**: Apply continuous sparsification to a significantly larger model (2B+ parameters) and to a much smaller model (<50M parameters). Compare whether the 10M parameter plateau persists or scales with model capacity.

**Check 2**: Design ablation experiments that isolate specific mechanisms—ablating only rotation-inducing parameters in first MLP layer versus other bigram subnetwork components. Compare performance impacts to determine whether rotation is the primary causal mechanism.

**Check 3**: Apply identified bigram subnetworks from English web text models to models trained on different domains (code, multilingual text, specialized technical documents). Measure correlation degradation and determine whether domain-specific fine-tuning is necessary.