---
ver: rpa2
title: Towards more transferable adversarial attack in black-box manner
arxiv_id: '2505.18097'
source_url: https://arxiv.org/abs/2505.18097
tags:
- adversarial
- diffusion
- attack
- purification
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of creating highly transferable
  adversarial attacks that can effectively fool black-box models while remaining robust
  against diffusion-based defenses. The authors propose a novel approach that leverages
  the score from time-dependent classifiers in classifier-guided diffusion models
  to incorporate natural data distribution knowledge into adversarial optimization.
---

# Towards more transferable adversarial attack in black-box manner

## Quick Facts
- arXiv ID: 2505.18097
- Source URL: https://arxiv.org/abs/2505.18097
- Reference count: 40
- This paper proposes ScorePGD and U-ScorePGD methods that leverage time-dependent classifier scores from classifier-guided diffusion models to create highly transferable adversarial attacks that maintain effectiveness against diffusion-based defenses.

## Executive Summary
This paper addresses the challenge of creating highly transferable adversarial attacks that can effectively fool black-box models while remaining robust against diffusion-based defenses. The authors propose a novel approach that leverages the score from time-dependent classifiers in classifier-guided diffusion models to incorporate natural data distribution knowledge into adversarial optimization. By combining this score with a carefully designed loss function, their method achieves superior transferability across diverse model architectures while maintaining effectiveness against diffusion-based purification defenses.

## Method Summary
The proposed method, ScorePGD, minimizes the log probability from a time-dependent classifier trained on progressively noised images, incorporating noised data distribution knowledge into adversarial optimization without requiring full diffusion machinery. U-ScorePGD extends this by combining the score loss with standard classification loss from a surrogate model. The approach uses ℓ∞ Projected Gradient Descent with 10 iterations, perturbation radius γ=16/255, and timestep t=20 for the time-dependent classifier. The key insight is that minimizing the classifier score redirects the guidance direction in diffusion reverse process, disrupting purification defenses while enhancing cross-model transferability.

## Key Results
- U-ScorePGD achieves 89.9% attack success rate on ResNet101 compared to 84.3% for standard PGD, while reducing computational requirements by over 10×
- ScorePGD maintains effectiveness against protected classifiers, achieving 66.4% success rate where PGD drops to 35.1%
- The method achieves 99.6% white-box ASR while maintaining strong black-box transfer performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The score from time-dependent classifiers captures essential noised data distribution knowledge for transferability without requiring full diffusion machinery.
- Mechanism: The time-dependent classifier fϕ(xt, t) is trained on progressively noised images via the forward diffusion process (xt = √αtx + σtϵ). By minimizing log fϕ(xt_adv, t), adversarial optimization inherits distribution knowledge that makes attacks robust across architectures. This mimics DiffPGD's inductive bias without backpropagating through the full denoising chain.
- Core assumption: The noise addition process—not the complete diffusion reverse—is the critical factor enabling diffusion-based purification's robustness and transferability.
- Evidence anchors:
  - [abstract] "Our approach leverages the score of the time-dependent classifier from classifier-guided diffusion models, effectively incorporating natural data distribution knowledge into the adversarial optimization process."
  - [section 4.2] "We hypothesize that the critical factor enhancing transferability in DiffPGD is not the complete diffusion process itself, but rather the incorporation of noised data distribution knowledge."
  - [corpus] Weak corpus evidence—related papers address transferability through ensemble/optimization techniques but not diffusion score mechanisms.
- Break condition: When timestep t is too large such that σtϵ ≫ √αtδ, the Gaussian noise overwhelms the perturbation, causing Ls to remain unchanged across PGD iterations (validated empirically in Appendix A.4).

### Mechanism 2
- Claim: Minimizing the time-dependent classifier score redirects diffusion reverse process guidance, disrupting purification defenses.
- Mechanism: Diffusion purification uses the score ∇xt log pϕ(y|xt, t) to guide denoising toward the natural image manifold. By minimizing log fϕ(xt_adv, t), adversarial perturbations inject misleading class guidance, causing purification to recover images with wrong semantic information despite successful noise removal.
- Core assumption: Purification effectiveness depends on correct class guidance during the reverse diffusion trajectory.
- Evidence anchors:
  - [section 4.2] "By optimizing Ls by our ScorePGD, the guidance direction to the image is redirected...leading the diffusion editing purification deal with wrong class."
  - [figure 3] Visualizes how adversarial perturbation causes "misleading guidance" vs. "correct guidance" in reverse diffusion.
  - [corpus] No direct corpus evidence—anti-purification via score manipulation is novel to this work.
- Break condition: Purification methods using classifier-free guidance or non-guided diffusion may be less susceptible to this mechanism.

### Mechanism 3
- Claim: U-ScorePGD's combined loss Lt = Lc − Ls achieves both high white-box effectiveness and superior black-box transfer.
- Mechanism: Lc provides surrogate-specific task knowledge while Ls provides noised distribution knowledge. The combined objective optimizes perturbations that simultaneously exploit classifier vulnerabilities and align with noised data manifold properties, achieving 89.9% transfer ASR vs. 84.3% for standard PGD.
- Core assumption: The gradient directions from Lc and Ls are not fundamentally contradictory.
- Evidence anchors:
  - [table 1] U-ScorePGD achieves 99.6% white-box ASR while maintaining 89.9% transfer to ResNet101.
  - [abstract] "U-ScorePGD achieving 89.9% attack success rate on ResNet101 compared to 84.3% for standard PGD, while reducing computational requirements by over 10×."
  - [corpus] Related work (AIM, DropConnect) improves transferability through ensemble/surrogate optimization but doesn't incorporate diffusion-based distribution knowledge.
- Break condition: Unbalanced optimization difficulty between losses can cause convergence to local maxima (explicitly noted as limitation in Appendix B).

## Foundational Learning

- Concept: **Denoising Diffusion Probabilistic Models (Forward/Reverse Processes)**
  - Why needed here: Understanding how noise schedules (αt, σt) progressively corrupt images is essential to grasp why time-dependent classifiers encode distribution knowledge.
  - Quick check question: Given clean image x0 and timestep t, write the forward diffusion equation q(xt|x0).

- Concept: **Classifier-Guided Diffusion**
  - Why needed here: The method exploits the score ∇xt log pϕ(y|xt, t) that steers generation—critical for understanding the anti-guidance attack mechanism.
  - Quick check question: How does adding the classifier gradient ∇xt log pϕ(y|xt, t) modify the reverse sampling distribution?

- Concept: **Projected Gradient Descent (PGD) for Adversarial Attacks**
  - Why needed here: The entire method builds on PGD iterations with modified loss functions and ℓ∞ projection.
  - Quick check question: What does the projection operator Pγ do and why must δ be constrained?

## Architecture Onboarding

- Component map:
  - Time-dependent classifier fϕ(xt, t) -> Forward noise sampler -> Score loss module -> (Classification loss module) -> PGD optimizer

- Critical path:
  1. Sample ϵ ~ N(0,I), compute xt_adv = √αtxadv + σtϵ (Eq. 1)
  2. Compute Ls = log fϕ(xt_adv, t) (Eq. 9) and optionally Lc
  3. Form combined loss Lt = Lc − Ls (Eq. 13)
  4. Update δ ← Pγ(δ + η · sign(∇δLt)) (Eq. 14)
  5. Repeat n=10 iterations

- Design tradeoffs:
  - **ScorePGD vs U-ScorePGD**: ScorePGD superior against protected classifiers (66.4% vs 56.4% ASR on protected ResNet50); U-ScorePGD superior for black-box transfer to unprotected models (89.9% vs 65.6%)
  - **Timestep t**: t=20 optimal; larger t causes noise to overwhelm perturbation signal
  - **Perturbation budget**: γ=16/255 achieves stronger attacks than γ=8/255 but more visible

- Failure signatures:
  - Ls unchanged across iterations → t too large, reduce timestep
  - U-ScorePGD stuck in local maximum → loss imbalance, consider gradient scaling
  - Low white-box ASR with ScorePGD → expected behavior (no task-specific knowledge); use U-ScorePGD

- First 3 experiments:
  1. Validate basic transferability: Run ScorePGD (γ=16/255, n=10, t=20) on ImageNet subset with ResNet50 surrogate; measure transfer ASR to ResNet101/ResNet18 (target: 65-80%).
  2. Test anti-purification: Apply ScorePGD samples to DiffPure-protected ResNet50; compare ASR vs. PGD baseline (target: ~66% vs. ~35%).
  3. Runtime validation: Benchmark ScorePGD vs. DiffPGD for 256×256 images, n=50 iterations (target: ~2s vs. ~23s).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can an adaptive diffusion timestep \( t \) be selected for the time-dependent classifier on a per-image basis rather than through trial and error?
- Basis in paper: [explicit] "We are now choosing t for the time-dependent classifier by trial and error. Selecting an adaptive t for a specific image is yet an open question."
- Why unresolved: The ablation studies show \( t = 20 \) generally performs best, but this is fixed across all images; the optimal timestep likely depends on image-specific properties such as complexity, noise characteristics, or semantic content.
- What evidence would resolve it: A systematic study correlating image features (e.g., frequency content, class uncertainty, SNR) with optimal timestep selection, or a learnable mechanism that predicts per-image optimal \( t \).

---

### Open Question 2
- Question: How can the optimization dynamics between the classification loss \( L_c \) and score loss \( L_s \) in U-ScorePGD be balanced to avoid local maxima?
- Basis in paper: [explicit] "Our U-scorePGD sometimes gets stuck in a local maximum, just like multi-task training, due to the unbalanced optimization difficulty for the two losses."
- Why unresolved: The combined objective \( L_t = L_c - L_s \) may have conflicting gradient directions at different optimization stages, similar to multi-task learning challenges, but no adaptive weighting or gradient surgery techniques are explored.
- What evidence would resolve it: Experiments with dynamic loss weighting (e.g., uncertainty weighting, GradNorm) or gradient manipulation techniques showing improved convergence and higher ASR consistency across runs.

---

### Open Question 3
- Question: Can the theoretical relationship between the time-dependent classifier score and adversarial transferability be formalized beyond the empirical observation?
- Basis in paper: [inferred] The paper hypothesizes that "efficient incorporation of noised data distribution knowledge, rather than the complete diffusion process, may be the critical factor" but provides only empirical validation without theoretical grounding.
- Why unresolved: The mechanism by which minimizing \( \log f_\phi(x_t^{adv}, t) \) enhances cross-model transferability and specifically undermines purification is described intuitively (Figure 3) but not mathematically characterized.
- What evidence would resolve it: A theoretical framework connecting the score-based loss to decision boundary geometry across architectures, or analysis showing how the score gradient correlates with transferable perturbation directions.

## Limitations
- Performance claims are primarily validated on ImageNet classification with ResNet architectures, limiting generalization to other tasks and architectures
- Anti-purification mechanism relies on specific properties of classifier-guided diffusion that may not extend to alternative purification methods
- Computational efficiency claims assume specific DiffPGD baseline with n=50 iterations that may not reflect all diffusion-based approaches

## Confidence
- **High confidence**: Transferability improvements of U-ScorePGD (89.9% ASR on ResNet101 vs 84.3% for PGD) and runtime efficiency claims (2s vs 23s) are well-supported by controlled experiments with clear baselines
- **Medium confidence**: Anti-purification effectiveness (66.4% ASR on protected ResNet50 vs 35.1% for PGD) shows strong empirical support but relies on specific diffusion purification assumptions that may not generalize
- **Low confidence**: Claims about performance on "more challenging models" like DenseNet are based on limited experimental validation with minimal architectural diversity

## Next Checks
1. **Architecture Generalization Test**: Evaluate U-ScorePGD transferability on Vision Transformer (ViT) models and other non-ResNet architectures to validate claims beyond ResNet-family models

2. **Purification Method Robustness**: Test anti-purification effectiveness against classifier-free diffusion purification methods and other non-classifier-guided purification approaches to assess mechanism generality

3. **Task Transferability Validation**: Apply ScorePGD to object detection or semantic segmentation tasks to verify if transferability benefits extend beyond image classification to downstream computer vision applications