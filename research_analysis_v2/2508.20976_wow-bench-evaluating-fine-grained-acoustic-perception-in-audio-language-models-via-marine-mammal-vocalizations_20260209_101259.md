---
ver: rpa2
title: 'WoW-Bench: Evaluating Fine-Grained Acoustic Perception in Audio-Language Models
  via Marine Mammal Vocalizations'
arxiv_id: '2508.20976'
source_url: https://arxiv.org/abs/2508.20976
tags:
- sound
- audio
- acoustic
- low-level
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces WoW-Bench, a benchmark designed to evaluate
  the low-level acoustic perception capabilities of audio-language models (LALMs)
  using marine mammal vocalizations. The benchmark is structured into two components:
  a Perception task assessing the ability to categorize novel sounds, and a Cognition
  task inspired by Bloom''s taxonomy, evaluating abilities to remember, understand,
  apply, and analyze auditory information.'
---

# WoW-Bench: Evaluating Fine-Grained Acoustic Perception in Audio-Language Models via Marine Mammal Vocalizations

## Quick Facts
- arXiv ID: 2508.20976
- Source URL: https://arxiv.org/abs/2508.20976
- Reference count: 35
- State-of-the-art LALMs perform significantly below human levels on low-level acoustic perception tasks

## Executive Summary
This paper introduces WoW-Bench, a benchmark designed to evaluate the low-level acoustic perception capabilities of audio-language models (LALMs) using marine mammal vocalizations. The benchmark is structured into two components: a Perception task assessing the ability to categorize novel sounds, and a Cognition task inspired by Bloom's taxonomy, evaluating abilities to remember, understand, apply, and analyze auditory information. Additionally, distractor questions are introduced to test whether models rely on listening or heuristics. Experiments with state-of-the-art LALMs show performance significantly below human levels, particularly in the Cognition tasks, indicating a substantial gap in auditory grounding. This highlights the need for improved low-level listening and cognitive processing in LALMs.

## Method Summary
The benchmark uses marine mammal vocalizations from the Watkins Marine Mammal Sound Database, comprising 1,777 validated multiple-choice questions across Perception (296 questions) and Cognition tasks (1,481 questions with 300 distractor variants). Questions are generated using GPT-4o with audio metadata and spectrograms, then undergo 3-annotator human verification. Evaluation uses micro-averaged accuracy with GPT-4.1-mini for answer extraction from model outputs. The Cognition tasks follow Bloom's taxonomy structure (Remember, Understand, Apply, Analyze) and include adversarial distractors to diagnose heuristic reliance versus genuine listening.

## Key Results
- State-of-the-art LALMs achieve significantly lower accuracy than humans on both Perception and Cognition tasks
- Performance gaps are particularly pronounced in Cognition tasks, indicating substantial deficits in auditory grounding
- Distractor questions reveal models often rely on heuristics rather than genuine acoustic analysis
- Humans achieve 97.1% on Remember tasks while best models reach only 57.1%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Marine mammal vocalizations provide a valid out-of-distribution (OOD) testbed for low-level acoustic perception because they are underrepresented in standard audio corpora and span a broad acoustic range.
- Mechanism: The domain shift forces models to rely on fine-grained acoustic feature extraction (pitch, duration, frequency transitions) rather than on memorized semantic priors from pretraining. By evaluating on sounds the model has rarely encountered, WoW-Bench isolates perceptual grounding from recognition.
- Core assumption: Models' training data contain few or no marine mammal examples, so performance depends on perceptual grounding; humans can solve these tasks via low-level listening even without domain knowledge.
- Evidence anchors:
  - [abstract] Marine mammal vocalizations are "rarely covered in conventional large-scale audio corpora, e.g., only one out of 527 AudioSet label distributions."
  - [section 1] "These vocalizations are rarely covered in conventional large-scale audio corpora... spanning a broad acoustic range from 20 Hz to over 20 kHz."
  - [corpus] Related work uses bioacoustics (BirdSet, marine mammals) and specialized domains (spatial audio in SPUR, motion in AudioMotionBench) to probe perception under OOD conditions; these similarly test low-level auditory attributes.

### Mechanism 2
- Claim: Adversarial distractors diagnose whether models rely on heuristics (e.g., classify-first, linguistic priors) versus genuine listening.
- Mechanism: By inverting expected audio patterns (e.g., presenting identical sounds when one is expected to differ, or replacing meaningful audio with noise), distractors expose models that answer without acoustic grounding. Performance gaps between distractor and non-distractor conditions indicate heuristic reliance.
- Core assumption: Models that truly listen should handle distractors with similar accuracy to regular questions; reliance on heuristics yields large performance gaps.
- Evidence anchors:
  - [abstract] Distractor questions are introduced "to test whether models rely on listening or heuristics."
  - [section 3.4] "We systematically design the distractors by inverting the expected pattern for each question type... This approach reveals whether the model relies on shallow heuristics or demonstrates genuine listening abilities."
  - [corpus] Direct corpus evidence for adversarial distractors in bioacoustic benchmarks is limited; ISA-Bench examines instruction sensitivity, which indirectly probes reliance on non-acoustic cues.

### Mechanism 3
- Claim: Cognition tasks structured by Bloom's taxonomy (remember, understand, apply, analyze) isolate low-level listening from prior domain knowledge.
- Mechanism: Tasks like sound matching, frequency/duration comparison, and transition analysis require direct acoustic processing. Poor model performance relative to humans indicates insufficient auditory grounding in current Large Audio-Language Models (LALMs).
- Core assumption: Humans can perform well on Cognition tasks via low-level perception alone, independent of marine mammal knowledge; models' deficits reflect perception limitations.
- Evidence anchors:
  - [abstract] "Experiments with state-of-the-art LALMs show performance significantly below human levels, particularly in the Cognition tasks, indicating a substantial gap in auditory grounding."
  - [section 4.2.2] "humans significantly outperform the models; for example, they achieve 97.1% on the Remember task, while the best Gemini-2.5 model achieves only 57.1%."
  - [corpus] AudioLens and SpeechR decompose auditory tasks into perception vs. reasoning components, showing similar gaps in fine-grained auditory attribute perception.

## Foundational Learning

- Concept: **Low-level listening vs. semantic categorization**
  - Why needed here: The benchmark is explicitly designed to measure perception of elementary acoustic attributes (pitch, duration, frequency transitions) prior to semantic interpretation, which current LALMs conflate with classify-first strategies.
  - Quick check question: Given three unfamiliar sounds, can you identify which has the highest pitch without naming the sound sources?

- Concept: **Out-of-distribution (OOD) evaluation**
  - Why needed here: WoW-Bench relies on marine mammal vocalizations to create a genuine OOD scenario, ensuring models cannot rely on memorized patterns from pretraining.
  - Quick check question: If you train an audio classifier on AudioSet, would you expect strong performance on rare classes like "beluga whale whistle"? Why or why not?

- Concept: **Bloom's taxonomy for auditory cognition**
  - Why needed here: Cognition tasks are structured as remember, understand, apply, analyze; understanding this hierarchy helps interpret which cognitive level(s) a given model failure belongs to.
  - Quick check question: For a task asking "How does the vocalization change over time?", which Bloom level does this correspond to?

## Architecture Onboarding

- Component map:
  - Perception tasks (Species/Vocalization/Both classification) -> Cognition tasks (Remember/Understand/Apply/Analyze) -> Distractor variants (adversarial audio) -> Evaluation pipeline (accuracy computation with answer extraction)

- Critical path:
  1. Load audio and question data; preserve sampling rates/clip durations.
  2. Run inference with target LALM using standardized prompts; collect raw outputs.
  3. Extract answer letters (regex or GPT-based); compute per-task and overall accuracy.
  4. Compare distractor vs. non-distractor performance to diagnose grounding vs. heuristics.

- Design tradeoffs:
  - MCQ format enables scalable, objective evaluation but may not capture open-ended reasoning; distractors increase diagnostic power but require careful construction to avoid unintended cues.
  - Focus on marine mammal vocalizations ensures OOD evaluation but limits domain breadth; extending to other bioacoustic or synthetic domains could improve generalization estimates.

- Failure signatures:
  - High non-distractor accuracy with low distractor accuracy → heuristic reliance.
  - Near-random performance on Apply/Analyze tasks → poor low-level perception.
  - Detailed reasoning that misclassifies identical sounds (e.g., describes "identical" clips as different) → classify-first strategy.

- First 3 experiments:
  1. Baseline evaluation of target LALMs on Perception and Cognition tasks; report per-task accuracy and distractor gap.
  2. Ablation over prompt formats ("choose correct option" vs. "return letter only") to isolate instruction-following effects.
  3. Error analysis on a sample of incorrect Remember-distractor responses, categorizing errors as No Reasoning, Semantic Categorization, Low-Level Grounding, or Direct Comparison to diagnose failure modes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the deficit in low-level listening observed in marine mammal vocalizations transfer to other complex acoustic domains like terrestrial bioacoustics or industrial soundscapes?
- Basis in paper: [explicit] The authors state in the Limitations section that findings "may not directly transfer to other domains" and encourage future work to "expand the scope of evaluation."
- Why unresolved: The benchmark specifically isolates marine sounds, which are underrepresented in training data, creating a specific type of OOD challenge not tested elsewhere.
- What evidence would resolve it: Evaluating current LALMs on a similar benchmark constructed from terrestrial animal sounds or machine noise to see if the failure mode persists.

### Open Question 2
- Question: Can LALMs be architecturally modified to prioritize low-level acoustic grounding over semantic classification heuristics?
- Basis in paper: [inferred] The paper notes models adopt a "classify-first strategy," inferring properties based on presumed categories rather than listening, but offers no solution.
- Why unresolved: The paper identifies the behavior as a key limitation causing failures in distractor questions but does not propose a training methodology to correct this inversion of processing.
- What evidence would resolve it: A study demonstrating that a specific fine-tuning objective or architecture modification improves performance on distractor questions by forcing acoustic analysis.

### Open Question 3
- Question: How can benchmarking move beyond multiple-choice questions to evaluate continuous, open-ended auditory reasoning without losing objective reproducibility?
- Basis in paper: [explicit] The authors acknowledge that "MCQ format... may not fully capture the open-ended reasoning or generative abilities" of modern models.
- Why unresolved: While necessary for standardized scoring, the discrete format leaves the assessment of generative, ambiguous, or interactive auditory capabilities unaddressed.
- What evidence would resolve it: The creation of a robust evaluation metric for open-ended sound description that correlates strongly with human expert judgment of perceptual detail.

## Limitations

- The benchmark's focus on marine mammal vocalizations may not generalize to other acoustic domains
- Exact prompt formulations and distractor audio generation parameters are not fully specified, limiting reproducibility
- Multiple-choice format may not capture open-ended reasoning or generative auditory capabilities

## Confidence

- **High**: Marine mammal vocalizations provide valid OOD evaluation; benchmark structure (Perception/Cognition with distractors) effectively isolates low-level perception; human performance baseline establishes model gaps.
- **Medium**: Distractor design successfully exposes heuristic reliance; Bloom's taxonomy structure meaningfully isolates cognitive levels; AudioSet underrepresentation justifies OOD claims.
- **Low**: Exact question generation prompts and annotation criteria are unspecified; distractor synthesis parameters lack detail; answer extraction method may introduce variance.

## Next Checks

1. Replicate distractor generation with specified audio parameters and evaluate whether performance gaps persist.
2. Test multiple answer extraction methods (regex vs. GPT) to quantify impact on reported accuracies.
3. Conduct ablation studies removing distractors to confirm they meaningfully expose heuristic reliance versus improving low-level perception.