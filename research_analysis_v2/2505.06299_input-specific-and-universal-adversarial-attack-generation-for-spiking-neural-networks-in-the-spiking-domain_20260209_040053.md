---
ver: rpa2
title: Input-Specific and Universal Adversarial Attack Generation for Spiking Neural
  Networks in the Spiking Domain
arxiv_id: '2505.06299'
source_url: https://arxiv.org/abs/2505.06299
tags:
- adversarial
- attack
- input
- spiking
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces two novel gradient-based adversarial attack
  algorithms for Spiking Neural Networks (SNNs) operating in the spiking domain. The
  input-specific attack generates highly stealthy perturbations with only 0.0305-0.0585%
  perturbation size while achieving 100% attack success rate across vision and sound
  datasets.
---

# Input-Specific and Universal Adversarial Attack Generation for Spiking Neural Networks in the Spiking Domain

## Quick Facts
- arXiv ID: 2505.06299
- Source URL: https://arxiv.org/abs/2505.06299
- Authors: Spyridon Raptis; Haralampos-G. Stratigopoulos
- Reference count: 29
- One-line primary result: Introduces two novel gradient-based adversarial attack algorithms for Spiking Neural Networks (SNNs) achieving 100% attack success rate with only 0.0305-0.0585% perturbation size.

## Executive Summary
This paper presents two novel gradient-based adversarial attack algorithms for Spiking Neural Networks (SNNs) operating directly in the spiking domain without converting to analog representations. The input-specific attack generates highly stealthy perturbations with minimal spike flips (0.0305-0.0585%) while achieving 100% attack success rate across vision and sound datasets. The universal attack creates reusable patches that fool networks on 78.35-87.66% of inputs with only 0.079-0.57% perturbation. Experiments on NMNIST, IBM DVS Gesture, and SHD datasets demonstrate superior performance compared to state-of-the-art methods across all metrics, with particular effectiveness on the SHD sound dataset marking the first demonstration of adversarial attacks in the audio domain for SNNs.

## Method Summary
The paper introduces two gradient-based adversarial attack methods for SNNs that operate directly in the spiking domain using a combination of Gumbel-Softmax relaxation and Straight-Through Estimator (STE) to enable backpropagation through binary spike generation. The input-specific attack optimizes a real-valued tensor through multiple iterations using a composite loss function that balances spatiotemporal similarity, winning class suppression, and confidence margin. The universal attack accumulates gradients across an entire dataset to create a single reusable adversarial patch. Both methods avoid assumptions about network architecture or coding schemes, making them broadly applicable to SNN security analysis.

## Key Results
- Input-specific attack achieves 100% Attack Success Rate (ASR) with perturbation sizes as low as 0.0018% on IBM DVS dataset
- Universal attack achieves 78.35-87.66% ASR across all tested datasets (NMNIST, IBM DVS Gesture, SHD)
- First demonstration of adversarial attacks in the audio domain for SNNs using the SHD dataset
- Superior performance compared to state-of-the-art methods across all metrics (ASR, PS, time)

## Why This Works (Mechanism)

### Mechanism 1: Discrete-to-Continuous Gradient Bridging
The proposed method bridges the gap between continuous gradient updates and binary spiking inputs using Gumbel-Softmax to map real-valued tensors to probability distributions, then applies STE to generate binary spikes in forward pass while passing gradients effectively. This surrogate gradient approach enables standard optimization techniques to work in the non-differentiable spiking domain.

### Mechanism 2: Targeted Spatiotemporal Loss Minimization
The optimization minimizes a weighted sum of three losses: L₁ (Spatiotemporal similarity/Variance) limits perturbation footprint, L₂ (Winning class) suppresses correct class spike count, and L₃ (Confidence margin) ensures new winning class has distinct lead. This composite loss balances attack effectiveness with stealthiness.

### Mechanism 3: Universal Gradient Accumulation for Input-Agnostic Attacks
The universal attack performs forward/backward passes on entire dataset without weight updates, accumulating gradients w.r.t. input membrane potentials into a global_grad tensor. This captures critical input neurons that consistently show high gradient magnitude across samples, enabling creation of a single reusable perturbation patch.

## Foundational Learning

- **Concept: Surrogate Gradients / Straight-Through Estimator (STE)**
  - **Why needed:** SNNs use binary activation (spike/no-spike) with derivative zero almost everywhere, blocking standard backpropagation
  - **Quick check:** If STE passes gradient as identity function, what prevents gradient from exploding or losing magnitude as it flows back to input layer?

- **Concept: Gumbel-Softmax Relaxation**
  - **Why needed:** Allows discrete decision (spike vs. no spike) to be treated as continuous probability during optimization, enabling use of standard gradient descent methods
  - **Quick check:** How does temperature parameter τ affect "sharpness" of probability distribution and consequently stability of optimization?

- **Concept: Spatiotemporal Representation (Event-based Data)**
  - **Why needed:** Unlike static images, SNN inputs have time dimension; perturbations must be calculated across both space and time
  - **Quick check:** Why is flipping spike at t=10 potentially more or less effective than flipping spike at t=100 in recurrent SNN architecture?

## Architecture Onboarding

- **Component map:** Input Generator (I_real) -> Relaxation Layer (Gumbel-Softmax) -> Binarization Layer (STE) -> Target SNN -> Loss Calculator
- **Critical path:** Conversion of I_real through Gumbel-Softmax function; temperature τ affects "sharpness" of probability distribution
- **Design tradeoffs:** Stealthiness vs. Success Rate (influenced by β and r₁), Time vs. Universality (universal requires full dataset pass but zero inference cost)
- **Failure signatures:** Gradient Vanishing (STE implementation issues), Non-convergence (learning rate too high), Gumbel-Softmax Saturation (τ too low)
- **First 3 experiments:**
  1. Sanity Check: Run input-specific attack on single NMNIST sample for 1000 iterations to verify loss convergence
  2. Hyperparameter Sweep: Test Universal Attack on IBM DVS Gesture subset (100 samples), vary Gumbel-Softmax temperature τ
  3. Baseline Comparison: Generate adversarial samples using "sparse attack" method and compare perturbation size against proposed method

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can SNN-specific defense mechanisms be developed that effectively balance robustness against adversarial attacks, computational efficiency, and maintained accuracy on clean inputs?
- Basis in paper: [explicit] The conclusion states future work aims to develop SNN-specific defense mechanisms that balance robustness, efficiency, and real-world applicability, noting existing defenses introduce trade-offs like increased computational overhead or reduced accuracy.
- What evidence would resolve it: Development and empirical evaluation of defense mechanisms (adversarial training, input filtering) on same datasets showing improved robustness without significant accuracy degradation or computational overhead.

### Open Question 2
- Question: Do proposed adversarial attacks transfer across different SNN architectures, neuron models, and training methods?
- Basis in paper: [inferred] Paper claims attacks make "no assumption about architecture of SNN" and applies to three different architectures, but does not test whether adversarial examples transfer between differently-structured or differently-trained models.
- What evidence would resolve it: Cross-model transferability experiments where adversarial examples generated on one SNN are tested on another with different training procedures or surrogate gradient methods.

### Open Question 3
- Question: Can universal adversarial attack success rate be improved beyond current 78.35-87.66% while maintaining or reducing perturbation size?
- Basis in paper: [inferred] Universal attack achieves 78.35-87.66% ASR across datasets, meaning 12-22% remain correctly classified; no discussion addresses whether this represents theoretical limit.
- What evidence would resolve it: Ablation studies on global gradient accumulation method, exploration of alternative loss functions for universal patch generation, or multi-stage optimization approaches showing higher ASR with comparable perturbation.

### Open Question 4
- Question: Can proposed audio-domain adversarial perturbations be realized as physical acoustic signals that successfully fool SNNs through real microphone and cochlea-based sensor systems?
- Basis in paper: [inferred] Figure 5 illustrates converting spike-based perturbations to audio signals and replaying them, but no empirical validation of physical acoustic attacks is provided; SHD experiments use simulated spike data.
- What evidence would resolve it: Experiments where UAP is converted to acoustic signal, played through speakers, captured by neuromorphic cochlea sensor, and fed to SNN, measuring ASR in physical attack pipeline.

## Limitations

- The attacks rely on gradient approximation techniques (STE and Gumbel-Softmax) that introduce uncertainty about fidelity to true gradients in spiking domain
- Loss hyperparameter tuning process is not fully specified, suggesting potential sensitivity to weight configurations affecting reproducibility
- Universal attack assumes static vulnerability patterns across datasets that may not hold for temporally dynamic or heterogeneous input distributions
- Attacks are evaluated primarily on specific datasets and SNN architectures trained with SLAYER, limiting generalizability claims

## Confidence

- Input-specific attack effectiveness (100% ASR, <0.06% PS): High confidence - Supported by extensive experimental results across three datasets with multiple SNN architectures
- Universal attack generalization (78-87% ASR): Medium confidence - Results are promising but rely on assumptions about consistent gradient patterns requiring further validation
- Gumbel-Softmax + STE mechanism: Medium confidence - Theoretical foundation is sound but direct validation of gradient approximation quality is limited
- First demonstration on audio domain (SHD dataset): High confidence - Novel contribution with clear experimental validation

## Next Checks

1. **Gradient Approximation Fidelity Test:** Implement controlled experiment comparing STE-approximated gradients against numerical gradient estimates on simplified SNN model to quantify approximation error and impact on attack convergence

2. **Temporal Sensitivity Analysis:** Conduct ablation studies on SHD audio dataset to determine whether perturbation effectiveness varies significantly across different temporal positions, validating spatiotemporal loss component's design assumptions

3. **Transferability Validation:** Test input-specific attack against SNN models with different architectures and coding schemes (beyond SLAYER-trained networks) to assess claimed architecture-agnostic nature of approach