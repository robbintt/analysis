---
ver: rpa2
title: 'HyReC: Exploring Hybrid-based Retriever for Chinese'
arxiv_id: '2506.21913'
source_url: https://arxiv.org/abs/2506.21913
tags:
- retrieval
- semantic
- dense
- lexicon-based
- union
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes HyReC, a hybrid-based retriever tailored for
  Chinese retrieval scenarios. The method integrates dense-vector retrieval, lexicon-based
  retrieval, and a semantic union of terms within a single model, employing a Global-Local-Aware
  Encoder (GLAE) to promote consistent semantic sharing while minimizing interference
  between paradigms.
---

# HyReC: Exploring Hybrid-based Retriever for Chinese

## Quick Facts
- arXiv ID: 2506.21913
- Source URL: https://arxiv.org/abs/2506.21913
- Reference count: 10
- Primary result: HyReC achieves nDCG@10 of 70.54% (base) and 66.73% (small) on C-MTEB, outperforming BGE-m3-hybrid by +3.44%

## Executive Summary
This paper proposes HyReC, a hybrid-based retriever tailored for Chinese retrieval scenarios. The method integrates dense-vector retrieval, lexicon-based retrieval, and a semantic union of terms within a single model, employing a Global-Local-Aware Encoder (GLAE) to promote consistent semantic sharing while minimizing interference between paradigms. A Normalization Module (NM) further refines alignment between retrieval approaches. Evaluated on the C-MTEB benchmark, HyReC achieves nDCG@10 scores of 70.54% (base) and 66.73% (small), outperforming baseline methods like BGE-m3-hybrid by +3.44%. The ablation study confirms the effectiveness of each component, with the semantic union of terms improving lexicon-based retrieval by +1.19% and dense-vector retrieval by +0.41%.

## Method Summary
HyReC implements a three-branch architecture combining dense-vector retrieval, lexicon-based retrieval, and semantic union of terms. The model employs a Global-Local-Aware Encoder (GLAE) with shared semantic backbone, plus specialized encoders for lexicon and dense branches. Training follows a three-stage pipeline: RetroMAE pre-training on Wudao corpora, preliminary fine-tuning on 160M text pairs, and high-quality fine-tuning on multiple Chinese retrieval datasets totaling 118,944,544 pairs. The model uses contrastive loss with in-batch negatives and ANN-style hard negatives, plus cross-entropy for semantic union classification. A Normalization Module applies L2 normalization to both branches before score combination. The semantic union module identifies multi-token words through Jieba word segmentation combined with regex patterns, addressing Chinese word segmentation challenges.

## Key Results
- Base model achieves nDCG@10 of 70.54% on C-MTEB, outperforming BGE-m3-hybrid by +3.44%
- Small model achieves nDCG@10 of 66.73%, demonstrating effective parameter scaling
- Semantic union of terms improves lexicon-based retrieval by +1.19% and dense-vector retrieval by +0.41%
- Ablation confirms each component's contribution to overall performance gains

## Why This Works (Mechanism)
HyReC works by integrating three complementary retrieval paradigms within a unified architecture. The Global-Local-Aware Encoder ensures semantic consistency across branches while specialized encoders capture distinct retrieval characteristics. The semantic union of terms addresses Chinese word segmentation limitations by identifying meaningful multi-token units beyond tokenizer-defined terms. The Normalization Module resolves score scale mismatches between dense and lexicon branches, enabling effective fusion. The three-stage training pipeline progressively refines the model from general language understanding to task-specific retrieval capabilities, with hard negative mining improving discriminative power.

## Foundational Learning
- Chinese word segmentation challenges: Chinese lacks explicit word boundaries, making traditional tokenization problematic for retrieval tasks. Quick check: Compare retrieval performance using different segmentation methods on Chinese text.
- Semantic union labeling: Multi-token word identification requires combining lexical segmentation with pattern recognition for numeric expressions and product models. Quick check: Verify labeled semantic unions capture meaningful multi-token units versus single tokens.
- ANN-style hard negative mining: Selecting informative negative examples from retrieval ranks improves model discrimination. Quick check: Monitor retrieval rank distributions for hard negative samples.
- L2 normalization in hybrid retrieval: Aligning score distributions between different retrieval paradigms prevents dominance by any single branch. Quick check: Compare branch scores before and after normalization.

## Architecture Onboarding

Component map: Semantic backbone -> Lexicon encoder -> Union projector; Semantic backbone -> Dense encoder -> Dense projector; Normalization Module -> Score fusion

Critical path: Input query/document -> Semantic backbone encoding -> Lexicon and dense branches -> Normalization Module -> Final score computation

Design tradeoffs: The three-branch architecture adds complexity but captures complementary retrieval strengths. The semantic union module improves Chinese-specific handling but requires additional labeling infrastructure. L2 normalization ensures fair branch contribution but may limit individual branch expressiveness.

Failure signatures: Score scale mismatch between branches causes training instability. Degenerate semantic union predictions reduce retrieval quality. Hard negative mining failures lead to poor discrimination.

First experiments: 1) Verify semantic union labeling tool correctly identifies multi-token words. 2) Test L2 normalization effectiveness on branch score alignment. 3) Evaluate individual branch contributions to overall retrieval performance.

## Open Questions the Paper Calls Out
- Can the "semantic union of terms" module be effectively generalized to languages with explicit word boundaries or distinct morphological structures?
- Does HyReC's architecture constrain the versatility of its embeddings for non-retrieval tasks such as classification or clustering?
- Does HyReC maintain its performance advantage when scaled to parameter sizes comparable to state-of-the-art large models?

## Limitations
- Exact architecture specifications remain unclear, particularly base model hidden size and attention configuration
- Implementation details for ANN-based hard negative mining are unspecified
- Semantic union labeling process relies on undocumented regex patterns
- Reported "no pre-trained language models" claim appears computationally infeasible

## Confidence
- HyReC architecture design and its three-branch integration: Medium confidence
- Performance improvements over baselines on C-MTEB: High confidence
- Effectiveness of semantic union over Jieba: Medium confidence

## Next Checks
1. Implement a simplified version using BERT-base-chinese initialization and verify baseline performance matches reported numbers within reasonable margins (Â±2-3%)
2. Create multiple semantic union labeling variants (different regex patterns) to test sensitivity of retrieval performance to labeling quality
3. Compare dense-lexicon fusion approaches using the NM module versus simple score averaging to quantify its contribution to performance gains