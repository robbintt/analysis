---
ver: rpa2
title: Improving the Transferability of Adversarial Attacks by an Input Transpose
arxiv_id: '2503.00932'
source_url: https://arxiv.org/abs/2503.00932
tags:
- transferability
- adversarial
- attack
- inc-v3
- rotation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a simple yet effective input transpose method
  to improve the transferability of adversarial attacks. The core idea is to apply
  a transpose operation to the input images before generating adversarial examples,
  which significantly enhances their effectiveness across different black-box models.
---

# Improving the Transferability of Adversarial Attacks by an Input Transpose

## Quick Facts
- **arXiv ID**: 2503.00932
- **Source URL**: https://arxiv.org/abs/2503.00932
- **Reference count**: 40
- **Primary result**: Simple input transpose operation significantly improves black-box adversarial transferability with minimal computational overhead

## Executive Summary
This paper introduces an input transpose method that dramatically enhances the transferability of adversarial attacks across different black-box models. By applying a transpose operation to input images before generating adversarial examples, the authors achieve substantial improvements in attack success rates with minimal additional computation. The method is evaluated on both NIPS'17 and CIFAR-10 datasets, showing consistent gains across multiple model architectures. The study also explores the effects of minor input rotations and provides preliminary analysis of feature map patterns to explain the observed improvements.

## Method Summary
The core method involves applying a transpose operation to input images before generating adversarial examples using standard gradient-based attacks. This simple transformation is combined with the existing adversarial attack generation process, requiring minimal implementation changes. The authors evaluate this approach across multiple datasets (NIPS'17 and CIFAR-10) and model combinations, comparing the transferability of adversarial examples generated with and without the transpose operation. They also conduct experiments with minor input rotations (1°) to explore whether slight geometric transformations can further improve transferability, analyzing the resulting feature maps to understand the underlying mechanisms.

## Key Results
- Up to 803% increase in adversarial transferability on NIPS'17 dataset
- Up to 117% improvement on CIFAR-10 dataset under single-model settings
- Input transpose method requires minimal additional computation and implementation effort
- Minor 1° input rotations show significant improvements on NIPS'17 but not on CIFAR-10
- Feature map analysis reveals noticeable pattern shifts in low-level layers when using rotations

## Why This Works (Mechanism)
The input transpose operation appears to work by altering the spatial relationships and feature patterns that deep neural networks rely on for classification. When adversarial examples are generated from transposed inputs, they capture different gradient directions and feature perturbations that may be more robust across different model architectures. The minor rotations (1°) further enhance this effect by inducing small but meaningful changes in the low-level feature maps, particularly in convolutional layers where spatial patterns are crucial. These transformations create adversarial examples that are less dependent on model-specific features and more likely to transfer across different architectures.

## Foundational Learning
- **Adversarial Transferability**: The phenomenon where adversarial examples crafted for one model can fool different models. Why needed: Understanding this concept is crucial for black-box attacks where the target model is unknown.
- **Input Transformations**: Operations like transpose, rotation, scaling that modify image geometry. Why needed: These form the core mechanism of the proposed method.
- **Gradient-Based Attacks**: Methods like FGSM, PGD that use model gradients to craft adversarial examples. Why needed: The paper builds on these standard attack methods.
- **Feature Maps**: The intermediate representations in neural networks. Why needed: The paper analyzes how transformations affect these maps.
- **Black-Box Attacks**: Scenarios where the attacker doesn't have access to the target model's parameters. Why needed: This is the primary attack setting the paper addresses.
- **CNN Architecture**: Convolutional neural networks and their spatial processing. Why needed: The paper's analysis focuses on convolutional feature maps.

## Architecture Onboarding

**Component Map**: Input Image -> Transpose Operation -> Adversarial Attack Generator -> Adversarial Example -> Target Model(s)

**Critical Path**: The adversarial example generation process where the transposed input flows through the gradient computation, affecting the direction and magnitude of perturbations that ultimately determine transferability.

**Design Tradeoffs**: 
- Simple implementation vs. need for optimal transformation parameters
- Computational overhead (minimal) vs. potential performance gains
- Dataset-specific effectiveness (rotations work better on NIPS'17 than CIFAR-10)

**Failure Signatures**: 
- No improvement in transferability when using the transpose operation
- Degradation in attack success rates compared to baseline
- Inconsistent results across different model pairs

**Three First Experiments**:
1. Apply input transpose to standard FGSM attack on CIFAR-10 and measure transferability across ResNet and VGG models
2. Test various rotation angles (0.5°, 1°, 2°) on NIPS'17 dataset to find optimal angle
3. Compare input transpose effectiveness against other simple transformations (flipping, scaling) on the same datasets

## Open Questions the Paper Calls Out
### Open Question 1
- **Question**: Why does a minor input rotation (e.g., 1°) significantly improve adversarial transferability on the NIPS'17 dataset but fail to yield any improvement on CIFAR-10?
- **Basis in paper**: [explicit] The authors state in Section 4.1: "In contrast, on CIFAR-10, the results with a 1° rotation show no improvement... However, this improvement with a 1° rotation is observed exclusively on NIPS'17, prompting us to conduct further investigation."
- **Why unresolved**: The paper provides a feature map analysis for NIPS'17 but does not establish a theoretical or empirical justification for why this phenomenon fails to manifest in lower-resolution datasets like CIFAR-10.
- **What evidence would resolve it**: A comparative analysis of feature map fluctuations in CIFAR-10 models versus NIPS'17 models under minor rotation, or a demonstration of a resolution/threshold effect.

### Open Question 2
- **Question**: Is it possible to identify or predict the optimal rotation angle for maximum transferability without querying the black-box target model?
- **Basis in paper**: [explicit] The abstract and conclusion note that "this transferability exhibits optimal angles that, when identified under unrestricted query conditions, could potentially yield even greater performance."
- **Why unresolved**: The current study identifies optimal ranges (e.g., 210°–240°) empirically by observing attack success rates on the target models, a method infeasible in real-world black-box scenarios.
- **What evidence would resolve it**: A heuristic algorithm or theoretical surrogate that approximates the optimal angle using only white-box model gradients or features.

### Open Question 3
- **Question**: Does the input transpose method maintain its efficacy when applied to non-CNN architectures, specifically Vision Transformers (ViT)?
- **Basis in paper**: [inferred] The analysis focuses exclusively on CNNs (Inception, ResNet, VGG) and explicitly links the method's success to "visible pattern shifts in the DNN's low-level feature maps" (convolutions).
- **Why unresolved**: Vision Transformers process images into patches and rely on self-attention rather than sliding window convolutions; therefore, the mechanism of "low-level feature map shift" may not apply or may behave differently.
- **What evidence would resolve it**: Experimental results applying the input transpose to adversarial attacks targeting ViT-based models.

## Limitations
- Evaluation primarily focuses on gradient-based black-box attacks, leaving performance against decision-based or score-based settings unexplored
- Rotation angle selection (1°) appears arbitrary without thorough sensitivity analysis
- Lacks comprehensive theoretical explanation for why input transposition specifically enhances transferability
- Feature map analysis remains qualitative without rigorous quantitative validation

## Confidence
- **Transferability Improvements**: High - Substantial empirical evidence across multiple datasets and model combinations
- **Input Transpose Effectiveness**: Medium - Strong empirical results but limited theoretical justification
- **Rotation Analysis**: Medium - Interesting observations but not thoroughly explored or explained

## Next Checks
1. **Parameter Sensitivity Analysis**: Systematically test a range of rotation angles (e.g., 0.1° to 5°) across multiple datasets to determine whether the 1° finding is optimal or dataset-specific, and quantify the trade-off between rotation magnitude and transferability improvement.

2. **Alternative Transformation Comparison**: Evaluate the input transpose method against other simple input transformations (flipping, scaling, color jittering) to determine whether the effectiveness is unique to transposition or part of a broader class of input manipulations that enhance transferability.

3. **Cross-Domain Robustness Test**: Validate the method's effectiveness against real-world black-box scenarios by testing on models with different training datasets, architectures, and defense mechanisms to assess whether the improvements generalize beyond the controlled experimental settings.