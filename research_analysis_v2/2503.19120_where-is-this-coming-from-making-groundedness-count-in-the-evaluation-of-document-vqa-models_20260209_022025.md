---
ver: rpa2
title: Where is this coming from? Making groundedness count in the evaluation of Document
  VQA models
arxiv_id: '2503.19120'
source_url: https://arxiv.org/abs/2503.19120
tags:
- score
- answer
- figure
- document
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces SMuDGE, a new evaluation metric for Document
  Visual Question Answering (VQA) that addresses a key limitation in current benchmarks:
  the inability to distinguish between grounded and hallucinated answers. While existing
  metrics like ANLS only measure surface similarity, SMuDGE incorporates both semantic
  type awareness (textual, numeric, or hybrid) and multimodal grounding, assessing
  whether an answer can be located within the input document and how far it is from
  the ground truth.'
---

# Where is this coming from? Making groundedness count in the evaluation of Document VQA models

## Quick Facts
- **arXiv ID:** 2503.19120
- **Source URL:** https://arxiv.org/abs/2503.19120
- **Reference count:** 26
- **Primary result:** SMuDGE is a new evaluation metric for Document VQA that incorporates semantic type awareness and multimodal grounding to better distinguish between grounded and hallucinated answers.

## Executive Summary
This paper addresses a critical limitation in Document VQA evaluation: current metrics like ANLS cannot distinguish between grounded answers (answers supported by document evidence) and hallucinations (answers not supported by the document). The authors introduce SMuDGE, a metric that combines type-aware semantic matching with spatial grounding based on document layout. Experiments across four major Document VQA benchmarks demonstrate that SMuDGE better aligns with human judgment, rewards better-calibrated models, and produces rankings that more accurately reflect reasoning capabilities.

## Method Summary
SMuDGE evaluates Document VQA predictions through a composite score combining semantic type awareness and spatial grounding. The metric first classifies ground truth answers as Textual, Numeric, or Hybrid, then applies appropriate matching strategies: standard Normalized Levenshtein Similarity for text, strict binary matching for numbers, and weighted harmonic means for hybrids. Simultaneously, it locates predicted answers within the OCR output and calculates a grounding score based on the normalized Manhattan distance between the predicted and ground truth bounding boxes, with exponential decay for distant answers. The final score is a weighted sum parameterized by α, which the authors tune to balance type-awareness and grounding based on calibration objectives.

## Key Results
- SMuDGE better aligns with human judgment than ANLS across four Document VQA benchmarks
- The metric rewards well-calibrated models and produces rankings that reflect reasoning capabilities
- An optimal α ≈ 0.25 minimizes correlation with Expected Calibration Error, indicating models with better grounding tend to be better calibrated
- Numeric type partitioning improves evaluation fidelity compared to treating all answers as textual

## Why This Works (Mechanism)

### Mechanism 1: Semantic Type Partitioning
The metric first classifies the ground truth into Textual, Numeric, or Hybrid types, then applies different matching strategies for each. Numeric answers use strict binary matching rather than partial credit for shared digits, addressing ANLS's weakness of artificially inflating scores for incorrect numeric predictions. This ensures that a prediction like "26" against ground truth "12" receives a score of 0 rather than 0.5, better reflecting the semantic error.

### Mechanism 2: Spatial Distance as a Proxy for Grounding
SMuDGE calculates the Normalized Manhattan Distance between the centroids of predicted and ground truth bounding boxes, converting this distance into a grounding score through exponential decay. If the answer cannot be located in the OCR output, the distance is set to 1, resulting in a grounding score of 0. This penalizes hallucinations while rewarding answers that are spatially close to the correct location, even if semantically incorrect.

### Mechanism 3: Calibration-Optimized Weighting
The final score combines semantic matching and grounding scores via a weighted sum with parameter α. The authors find that α ≈ 0.25 (prioritizing grounding) minimizes correlation with Expected Calibration Error, suggesting that models producing well-grounded answers tend to be better calibrated. This calibration relationship validates the grounding mechanism's importance in evaluation.

## Foundational Learning

- **Normalized Levenshtein Similarity (ANLS)**
  - *Why needed here:* This is the baseline metric SMuDGE critiques. Understanding how ANLS calculates edit distance reveals why it fails to distinguish between semantically different numeric predictions.
  - *Quick check question:* If Ground Truth is "Apple" and Prediction is "Apply", ANLS gives a high score. How does this compare to GT="100" and Pred="200"?

- **Bounding Box (BBox) Normalization**
  - *Why needed here:* The grounding mechanism relies on converting pixel coordinates into normalized distances. Understanding how coordinates are normalized relative to page dimensions is crucial for calculating independent distances.
  - *Quick check question:* Why does the paper normalize the distance between centroids by the page width and height rather than the size of the ground-truth bounding box?

- **Expected Calibration Error (ECE)**
  - *Why needed here:* The paper validates SMuDGE by showing it correlates with ECE. Understanding that ECE measures the gap between model confidence and actual accuracy is essential for interpreting the calibration analysis.
  - *Quick check question:* If a model is 90% confident but only 50% accurate, is it well-calibrated? How does SMuDGE relate to this?

## Architecture Onboarding

- **Component map:** Input Handler -> Type Classifier -> Match Scorer -> Grounding Scorer (Locator -> Distancer -> Decay) -> Aggregator
- **Critical path:** The Grounding Scorer (Locator) is the highest risk component. If the OCR dictionary is noisy or the model predicts a synonym not in the OCR, the Locator fails, triggering the hallucination penalty.
- **Design tradeoffs:**
  - Page Normalization vs. BBox Normalization: The paper chooses page-level normalization for distance, which is robust to varying answer lengths but sensitive to page layout.
  - Numeric Strictness: The paper uses strict binary scoring for numbers, which may be overly punitive for approximate values.
- **Failure signatures:**
  - The "Snippet" Failure: Evaluating on cropped images rather than full pages skews results unless dimensions are updated.
  - The "Rephrasing" Failure: If a model says "about twelve" and OCR only has "12", the Locator might fail to find a match, triggering the hallucination penalty.
- **First 3 experiments:**
  1. Unit Test Match Scorer: Verify type classifier and match scorer logic with examples like GT="12" vs Pred="26" (score=0) and GT="12" vs Pred="12.0".
  2. Visualize Grounding Distance: Plot bounding boxes for 5 samples where the model answered incorrectly but used a valid number from the document, verifying non-zero grounding scores.
  3. Calibration Correlation Check: Calculate SMuDGE scores with varying α values and plot against model confidence or human preference ratings to validate the α=0.25 finding.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the grounding mechanism be extended to robustly evaluate multi-span answers where the correct response is composed of multiple, disjoint text segments within the document?
- **Basis in paper:** Section 7 (Limitations) states analyses were conducted on single-span extractive answers and extending to multi-span answers is outside the scope.
- **Why unresolved:** The current algorithm relies on merging bounding boxes for continuous word sequences and doesn't define methods for aggregating distance or similarity scores for answers split across different regions.
- **What evidence would resolve it:** A formalized extension of SMuDGE handling multiple ground truth spans with correlation analysis on multi-span datasets.

### Open Question 2
- **Question:** Does incorporating fine-grained semantic constraints (e.g., distinguishing dates, currencies, timestamps) significantly improve metric alignment with human judgment compared to current numeric/textual/hybrid classification?
- **Basis in paper:** Section 7 notes the methodology doesn't account for specific semantic categories like currencies or dates, which carry nuances that surface similarity might miss.
- **Why unresolved:** Current implementation uses simple heuristics for three broad categories, potentially misclassifying semantically distinct entities like dates as generic hybrids.
- **What evidence would resolve it:** An ablation study comparing current type-aware scoring against versions using regex or entity recognition for fine-grained types.

### Open Question 3
- **Question:** Is the optimal weighting parameter (α) universal across diverse document domains, or does it require dataset-specific tuning to maximize correlation with model calibration?
- **Basis in paper:** Section 7 notes α was tuned specifically on DUDE dataset and had to be excluded from validation analyses of other benchmarks, leaving generalizability uncertain.
- **Why unresolved:** Different document types may rely more heavily on visual layout versus textual exactness, suggesting static α=0.25 might not be optimal for all layouts.
- **What evidence would resolve it:** A sensitivity analysis showing variance of optimal α values when tuned independently across DocVQA, InfographicVQA, and MP-DocVQA benchmarks.

## Limitations

- **OCR Dependency:** The grounding mechanism's effectiveness is heavily contingent on OCR accuracy, creating hard failure modes when OCR fails to capture predicted text.
- **Alpha Generalizability:** The optimal α=0.25 tuning derived from DUDE dataset may not generalize to other document domains or model architectures.
- **Numeric Matching Rigidity:** Strict binary matching for numeric answers may be overly punitive in contexts involving approximate values or numeric codes where partial matches have semantic meaning.

## Confidence

- **SMuDGE provides a more faithful evaluation of Document VQA models than ANLS:** High
- **The optimal α value is 0.25 for balancing grounding and semantic awareness:** Medium
- **SMuDGE's grounding component effectively penalizes hallucinations:** High, with OCR dependency caveat
- **Models with better grounding are more well-calibrated:** Medium (correlation observed, causation not established)

## Next Checks

1. **OCR Robustness Test:** Evaluate SMuDGE performance on a dataset with known OCR errors, comparing scores and rankings to a hypothetical "perfect OCR" baseline to quantify OCR noise impact.

2. **Cross-Domain α Validation:** Apply SMuDGE with α=0.25 setting to a new Document VQA dataset from a different domain (e.g., financial reports), measuring correlation with human judgment to validate generalizability.

3. **Numeric Matching Flexibility Analysis:** Create synthetic test set with numeric answers that are close approximations (e.g., "150" vs "145"), evaluating SMuDGE with current binary matching versus modified fuzzy numeric matching to assess strictness versus fairness trade-off.