---
ver: rpa2
title: Exploring the Performance of Large Language Models on Subjective Span Identification
  Tasks
arxiv_id: '2601.00736'
source_url: https://arxiv.org/abs/2601.00736
tags:
- span
- text
- spans
- llms
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates Large Language Models (LLMs) on subjective
  span identification tasks across sentiment analysis, offensive language identification,
  and claim verification. The study compares LLMs against BERT baselines using instruction
  tuning, in-context learning, and chain-of-thought prompting strategies.
---

# Exploring the Performance of Large Language Models on Subjective Span Identification Tasks

## Quick Facts
- arXiv ID: 2601.00736
- Source URL: https://arxiv.org/abs/2601.00736
- Reference count: 16
- Primary result: LLMs outperform BERT baselines on complex texts with interrelated spans using few-shot learning, while instruction tuning excels on simple texts

## Executive Summary
This paper evaluates Large Language Models (LLMs) on subjective span identification tasks across sentiment analysis, offensive language identification, and claim verification. The study compares LLMs against BERT baselines using instruction tuning, in-context learning, and chain-of-thought prompting strategies. Results show LLMs perform better on complex texts with interrelated spans than on simple texts, and are more efficient at identifying explicit spans like targets or aspects. Few-shot learning outperforms other approaches for complex tasks, while instruction tuning excels in simple text scenarios. Model size has minimal impact on performance, and SLMs like BERT generally outperform LLMs in low-resource settings, though few-shot LLMs show competitive results.

## Method Summary
The study evaluates LLMs (Qwen2.5 and Llama-3.1 variants, 7B-72B parameters) on four datasets: TBO, ABSA, CSI, and TSD. Three prompting strategies are compared: instruction tuning with LoRA (lr=1e-4, batch=2, grad_accum=8, epochs=10), in-context learning with 0/3/5-shot exemplars retrieved via sentence-transformers, and zero-shot chain-of-thought. Performance is measured using Token F1 and Span F1 metrics, with complex texts having interrelated spans (TBO, ABSA) and simple texts having single span types (CSI, TSD).

## Key Results
- LLMs outperform on complex texts with interrelated spans versus sequential single-type extraction
- Few-shot learning outperforms instruction tuning for complex tasks, while instruction tuning excels on simple texts
- Model size (7B-72B) has minimal impact on performance
- SLMs like BERT generally outperform LLMs in low-resource settings, though few-shot LLMs show competitive results

## Why This Works (Mechanism)

### Mechanism 1
Joint identification of interrelated span types improves LLM performance over sequential single-type extraction. When prompted to identify multiple span types together (e.g., Target + Argument), the model leverages relational cues between spans—identifying a target constrains plausible arguments, and vice versa. This mutual conditioning reduces the search space and improves precision. Core assumption: Span types in complex texts share syntactic or semantic dependencies that autoregressive models can exploit through joint decoding. Evidence: Figure 1 shows Token F1 and Span F1 improve when span types are extracted together vs. individually for TBO and ABSA tasks.

### Mechanism 2
LLMs outperform on explicitly grounded spans but underperform on context-dependent, implicit spans. Explicit spans (named targets, noun aspects) have stable surface-form cues and less ambiguity. Subjective spans (offensive arguments, opinion terms) rely on pragmatic interpretation, tone, and cultural context—patterns less consistently captured in pretraining. Core assumption: Pretraining data contains stronger statistical regularities for entity-like spans than for pragmatic, speaker-intent-dependent expressions. Evidence: Discussion (RQ1) notes LLMs struggle with subjective spans like offensive arguments or opinion terms that are context-dependent or indirect expressions.

### Mechanism 3
Few-shot retrieval-based exemplar selection outperforms instruction tuning for complex texts; instruction tuning excels on simple texts. For complex tasks, in-context exemplars demonstrate the relational structure and output format, enabling pattern matching. For simple tasks, instruction tuning provides stable task-specific parameter updates that generalize better than sparse exemplars. Core assumption: Embedding-based retrieval surfaces structurally similar examples that transfer better than generic instructions for multi-span reasoning. Evidence: Table 3 shows few-shot (3/5-shot) achieves best TBO/ABSA performance (SF1 up to 0.879); instruction tuning leads on TSD (SF1 0.638).

## Foundational Learning

- **Span Identification vs. Sequence Labeling**: The paper frames subjective span identification as token-level extraction with explainability goals, distinct from post-level classification. Understanding this distinction clarifies why BERT-style token classifiers have historically dominated. Quick check: Can you explain why span-level F1 (exact match) is strictly harder than token-level F1?

- **Complex vs. Simple Text Taxonomy**: The paper introduces a binary taxonomy—complex texts have interrelated multi-type spans; simple texts have single span types. This directly determines which prompting strategy (few-shot vs. instruction tuning) to prioritize. Quick check: Given a new dataset with aspect-sentiment pairs and implicit targets, would you classify it as complex or simple?

- **In-Context Learning with Retrieval-Based Exemplars**: The paper uses sentence-transformer embeddings to retrieve top-k similar instances for few-shot prompts, not random sampling. This retrieval step is critical to the observed few-shot gains. Quick check: Why might random exemplar selection underperform embedding-based retrieval for span identification?

## Architecture Onboarding

- **Component map**: Raw text -> sentence-transformer embedding -> top-k retrieval from training pool -> Task-specific instruction + output format + k exemplars + input text -> LLM inference -> Span extraction in structured format -> Token F1 and Span F1 evaluation

- **Critical path**: 1. Define span taxonomy (single-type vs. multi-type, explicit vs. subjective) 2. Choose strategy based on text complexity: few-shot for complex, instruction tuning for simple 3. If few-shot: implement embedding-based retrieval; if instruction tuning: prepare LoRA fine-tuning pipeline 4. Evaluate with both TF1 and SF1—SF1 is stricter and surfaces over-generation issues

- **Design tradeoffs**: Few-shot vs. Instruction Tuning: Few-shot requires no gradient updates but needs retrieval infrastructure and prompt budget; instruction tuning requires GPU hours but produces stable single-model artifacts. Model Size: Paper finds minimal performance gains from 7B → 72B; smaller models (7B–32B) are compute-efficient with marginal accuracy loss. Zero-shot vs. CoT: CoT adds reasoning steps but underperforms zero-shot on simple tasks (adds verbosity, reduces precision).

- **Failure signatures**: Over-generation: LLMs include context words in spans (e.g., TSD: "This proposal is so idiotic..." instead of "idiotic")—visible in low SF1 despite higher TF1. Format drift: CoT outputs include explanatory text that breaks structured parsing. Subjective span miss: Implicit offensiveness (e.g., "You are dead to me") missed due to lack of surface cues.

- **First 3 experiments**: 1. Baseline calibration: Run BERT-large fine-tuned on all four datasets to establish SLM benchmarks; confirm reproduction of reported TF1/SF1. 2. Few-shot scaling curve: Test 0/1/3/5-shot with embedding retrieval on TBO and TSD; plot TF1/SF1 to validate complexity-strategy hypothesis. 3. Model size ablation: Compare Qwen-7B vs. Qwen-32B vs. Qwen-72B on ABSA with 3-shot; quantify marginal gains per parameter increase.

## Open Questions the Paper Calls Out

- How can decoder-only LLMs be adapted to better utilize bidirectional context for accurate subjective span identification? The authors state a goal to "explore approaches to improve the LLMs understanding of the input... especially considering the context from both left and right." This remains unresolved as the paper demonstrates that autoregressive LLMs underperform compared to bidirectional models like BERT on certain spans, but does not test specific architectural or prompting solutions to mitigate this limitation.

- Can the performance patterns observed in Llama and Qwen models be generalized to proprietary LLMs (e.g., GPT-4, Claude) or other architectural families? The Limitations section notes that the study only evaluates two open-source model families, leaving the performance of other models unknown. It remains unclear if the minimal impact of model size and the relative success of few-shot learning are specific to the Llama/Qwen families or universal scaling laws for this task.

- How does subjective span identification performance vary when LLMs are applied to non-English languages or multimodal (text-image) contexts? The authors plan to "expand this work to non-English datasets" and "challenging datasets... including multimodal data." The current study is restricted to English text; the contextual understanding advantages of LLMs may not transfer to low-resource languages or contexts where meaning is split between text and images.

## Limitations

- Evaluation covers only four datasets spanning three tasks, all in English, limiting generalizability to other languages or subjective span types
- The complex-vs-simple text taxonomy is binary and heuristic, potentially oversimplifying nuanced text structures
- The paper assumes span types are either independent or interrelated, but real-world texts may exhibit partial dependencies that don't fit neatly into this framework

## Confidence

**High Confidence**: The finding that few-shot learning outperforms instruction tuning on complex texts (TBO, ABSA) is well-supported by the empirical results, with clear statistical differences in SF1 scores. The observation that explicit spans (targets, aspects) are easier to identify than subjective spans (arguments, opinions) aligns with established NLP challenges around pragmatic inference. The claim about BERT baselines outperforming LLMs in low-resource settings is robust across all four datasets.

**Medium Confidence**: The assertion that model size has minimal impact on performance (7B-72B range) is supported but limited by the narrow scope of tested architectures. The conclusion that instruction tuning excels on simple texts (TSD, CSI) is plausible but based on fewer experimental conditions than the few-shot comparisons. The mechanism explaining joint identification benefits relies on implicit assumptions about span interdependencies that aren't directly tested.

**Low Confidence**: The broader generalization that LLMs have a "contextual understanding advantage" over SLMs is somewhat overstated given that BERT often matches or exceeds LLM performance in low-resource settings. The paper's theoretical framing around "relational cues" between spans, while intuitive, lacks direct experimental validation beyond correlation with performance differences.

## Next Checks

1. **Cross-Dataset Generalization Test**: Evaluate the few-shot vs. instruction tuning hypothesis on additional subjective span identification datasets beyond the four tested, particularly those with different linguistic properties (e.g., multilingual data, different domains like biomedical or legal texts). This would validate whether the complex/simple text taxonomy holds across diverse contexts.

2. **Span Dependency Analysis**: Design an ablation study that systematically varies the presence and type of span interdependencies within texts. Create controlled datasets where span types are fully independent, partially dependent, or fully interdependent, then measure how performance varies with prompting strategy. This would directly test the relational cue mechanism.

3. **Model Architecture Scaling**: Extend the model size analysis beyond Qwen to include other LLM families (e.g., GPT variants, Claude) and SLMs (e.g., RoBERTa, DeBERTa) across the full parameter range (1B-70B+). Include a formal statistical test of the minimal impact hypothesis, and test whether different prompting strategies show different sensitivity to model scale.