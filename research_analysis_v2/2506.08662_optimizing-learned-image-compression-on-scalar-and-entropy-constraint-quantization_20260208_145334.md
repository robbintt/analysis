---
ver: rpa2
title: Optimizing Learned Image Compression on Scalar and Entropy-Constraint Quantization
arxiv_id: '2506.08662'
source_url: https://arxiv.org/abs/2506.08662
tags:
- quantization
- decoder
- image
- training
- kodak
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Learned image compression using variational autoencoders is hindered
  by the zero gradients of quantization functions, requiring differentiable approximations
  during training. This work addresses this limitation by introducing a finetuning
  step: after initial end-to-end training, parts of the network are retrained on quantized
  latents obtained at the inference stage, bypassing the need for quantization approximations.'
---

# Optimizing Learned Image Compression on Scalar and Entropy-Constraint Quantization

## Quick Facts
- arXiv ID: 2506.08662
- Source URL: https://arxiv.org/abs/2506.08662
- Reference count: 0
- Primary result: Retraining decoder on quantized latents improves rate-distortion for both USQ and TCQ quantizers

## Executive Summary
Learned image compression using variational autoencoders faces a fundamental challenge: quantization functions have zero gradients, preventing end-to-end optimization. This work addresses this by introducing a finetuning step that retrains the decoder on quantized latents obtained at inference stage, bypassing the need for quantization approximations. The approach is particularly effective for entropy-constraint quantizers like Trellis-Coded Quantization (TCQ), where interdependencies between quantization decisions make approximations suboptimal. Experiments show significant BD-rate improvements, with TCQ yielding higher gains (average -1.97% to -2.29%) compared to USQ (average -1.06% to -1.99%).

## Method Summary
The method consists of three phases: (1) initial end-to-end training with quantization approximations using noise perturbations for USQ or switching perturbations for TCQ, (2) generation of quantized latents using the true quantizer with frozen encoder, and (3) retraining the decoder on these true quantized latents. For USQ, an optional fourth phase jointly retrains the hypercoder and decoder with rate and distortion terms. The key innovation is bypassing the zero-gradient problem by freezing the encoder after initial training and using truly quantized latents for decoder optimization, which is particularly beneficial for TCQ where quantization decisions are interdependent through Viterbi trellis search.

## Key Results
- Decoder-only retraining improves PSNR by 0.07-0.12 dB on Kodak dataset
- BD-rate improvements: USQ achieves -1.06% to -1.99% gains; TCQ achieves -1.97% to -2.29% gains on TecNick dataset
- Hypercoder+decoder retraining for USQ further improves both rate and distortion metrics
- TCQ retraining effectiveness limited by need for offline pre-generated quantized data

## Why This Works (Mechanism)

### Mechanism 1: Gradient Bypass Through Encoder-Frozen Retraining
During initial end-to-end training, quantization is replaced with differentiable approximations. The retraining step freezes the encoder and trains the decoder using truly quantized latents, eliminating gradient approximation error for the decoder path.

### Mechanism 2: Entropy-Constraint Quantization Interdependency Gap
TCQ produces interdependent quantization decisions through Viterbi search that cannot be accurately approximated by per-sample noise or rounding, creating train-inference distribution mismatch.

### Mechanism 3: Hypercentwork Rate Optimization on True Quantization Indices
Jointly retraining hypercoder and decoder on true USQ latents allows μ and σ to adjust to actual distribution of quantized latents, improving both rate and distortion metrics.

## Foundational Learning

- **Variational Autoencoder (VAE) for Image Compression**: The entire architecture is a VAE where an encoder transforms images to latents, which are quantized and entropy-coded, then decoded. Understanding this pipeline is prerequisite to grasping where quantization approximation fails.
  - Quick check: Can you trace the path from input image x to reconstruction x̂, identifying where quantization occurs and where gradients are blocked?

- **Rate-Distortion Optimization (RDO)**: The training objective D + λ·R jointly minimizes distortion and bitrate. TCQ uses RDO at inference time to select quantization indices, making its decisions dependent on network parameters.
  - Quick check: If you increase λ during training, would you expect higher or lower bitrate models? Why?

- **Trellis-Coded Quantization (TCQ)**: TCQ is a complexity-constrained vector quantizer using multiple scalar quantizers and state transitions. Understanding that TCQ decisions are path-dependent (via Viterbi algorithm) is essential to see why per-sample approximations fail.
  - Quick check: Why does TCQ require a trellis search rather than independent per-sample quantization decisions?

## Architecture Onboarding

- **Component map**: Input x → Encoder(w_enc) → Latents z → Quantization Q(z, μ̂) → Indices q → Arithmetic Coder → Dequantization DQ(q, μ̂) → ẑ → Decoder(w_dec) → x̂
- **Critical path**: 1) Pre-train full VAE end-to-end using quantization approximation; 2) Freeze encoder and hypercoder; generate quantized latents ẑ using true quantizer; 3) Retrain decoder w_dec on (ẑ, x) pairs minimizing distortion only; 4) (USQ only) Optionally retrain hypercoder + decoder jointly with rate+distortion loss
- **Design tradeoffs**: Decoder-only retraining is simpler and works for both USQ and TCQ but doesn't reduce bitrate; hypercoder+decoder retraining improves both metrics but is currently infeasible for TCQ due to offline dataset constraints
- **Failure signatures**: PSNR decreases after retraining (likely learning rate too high); BD-rate shows no improvement (check quantization match); TCQ retraining fails (verify TCQ dataset generation)
- **First 3 experiments**: 1) Baseline validation: Run anchor model on Kodak, verify BD-rate = 0%; 2) USQ decoder-only retraining: Pre-generate USQ-quantized latents, retrain decoder for 10-20 epochs, measure PSNR gain (expect ~0.07-0.12 dB); 3) TCQ decoder retraining: Generate TCQ-quantized latents offline, retrain decoder, compare BD-rate gains vs. USQ retraining (expect ~2% vs. ~1%)

## Open Questions the Paper Calls Out

### Open Question 1
How can the hypercoder network be jointly retrained for Trellis-Coded Quantization (TCQ) without creating a distribution mismatch between the static offline training data and the updated inference behavior? The current implementation relies on a static dataset of quantized latents generated by the original encoder/hypercoder; updating the hypercoder during retraining would change the Viterbi algorithm's path selection, making the static training data inaccurate.

### Open Question 2
Does performing the retraining process iteratively (regenerating quantized latents after each weight update) lead to further rate-distortion improvements compared to the single-pass approach? A single retraining step may correct the initial approximation error, but it does not guarantee that the network has converged to the global optimum for the true quantization function.

### Open Question 3
Does the proposed finetuning method yield consistent coding gains when applied to entropy models with autoregressive components? Autoregressive models have stricter dependencies between latent samples; it is unclear if the offline quantization of latents would violate the sequential assumptions or context models used in such architectures during retraining.

## Limitations
- The approach assumes encoder has converged sufficiently during initial training, with remaining suboptimality localized to decoder
- TCQ retraining is limited by need for offline pre-generated datasets, creating potential staleness if hypercoder parameters change
- Analysis does not address potential encoder adaptation needs for different quantization schemes

## Confidence
- **High**: Mechanism 1 (Gradient bypass through encoder-frozen retraining) - well-supported by explicit training procedure descriptions and consistent results across both USQ and TCQ
- **Medium**: Mechanism 2 (Entropy-constraint quantization interdependency gap) - logical but lacks direct corpus evidence for TCQ-specific approximation limitations
- **Low**: Mechanism 3 (Hypercoder rate optimization on true quantization indices) - theoretical but currently blocked in practice for TCQ due to dataset generation constraints

## Next Checks
1. Verify encoder latent distribution stability by comparing histograms of latents from initial training vs. those used in retraining phase - ensure no distributional shift that could limit decoder adaptation
2. Conduct ablation study comparing decoder-only retraining vs. full encoder+decoder retraining to quantify the importance of freezing the encoder assumption
3. Test the hypercoder retraining approach on a simplified entropy-constrained quantizer (e.g., sequential scalar quantization) to validate the theoretical mechanism before attempting TCQ implementation