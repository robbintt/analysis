---
ver: rpa2
title: Class Incremental Learning for Algorithm Selection
arxiv_id: '2506.01545'
source_url: https://arxiv.org/abs/2506.01545
tags:
- data
- training
- learning
- methods
- instances
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper tackles algorithm\u2011selection for streaming optimisation\
  \ where instances arrive continuously and new solver classes emerge, requiring a\
  \ classifier that can be updated without catastrophic forgetting \u2013 a class\u2011\
  incremental learning (CIL) problem. Eight state\u2011of\u2011the\u2011art CIL strategies\
  \ (parameter\u2011regularisation: EWC, MAS, SI; data\u2011regularisation: GEM, AGEM;\
  \ replay: Feature Replay, Experience Replay; distillation: LwF) were benchmarked\
  \ on an online bin\u2011packing benchmark with four deterministic heuristics (FF,\
  \ BF, NF, WF)."
---

# Class Incremental Learning for Algorithm Selection  

## Quick Facts  
- **arXiv ID:** 2506.01545  
- **Source URL:** https://arxiv.org/abs/2506.01545  
- **Reference count:** 22  
- **Primary result:** Replay‑based class‑incremental learning (Feature Replay, Experience Replay) limits forgetting to ≈ 7 % loss in overall 4‑class accuracy and consistently outperforms regularisation, distillation, and other CIL strategies on an online bin‑packing benchmark.  

## Executive Summary  
The paper addresses algorithm‑selection in a streaming optimisation setting where new solver classes appear over time. Treating the problem as class‑incremental learning (CIL), the authors benchmark eight state‑of‑the‑art CIL strategies—parameter‑regularisation (EWC, MAS, SI), data‑regularisation (GEM, AGEM), replay (Feature Replay, Experience Replay), and distillation (LwF)—on a synthetic online bin‑packing task that introduces two new deterministic heuristics per task. With only 100 exemplars retained from the first task, replay‑based methods achieve the smallest forgetting, staying within ~7 % of an oracle selector that has access to all data. The results suggest that rehearsal‑based CIL is a practical solution for continual algorithm‑selection in streaming optimisation contexts.  

## Method Summary  
Eight CIL approaches were instantiated with identical neural classifiers and trained on two sequential tasks (D1, D2), each adding two previously unseen bin‑packing heuristics (First‑Fit, Best‑Fit, Next‑Fit, Worst‑Fit). For rehearsal‑based methods, a fixed memory of 100 exemplars from D1 was maintained; other methods relied on regularisation or knowledge‑distillation without explicit memory. Experiments were repeated 30 times across varying training‑set sizes, and performance was measured as overall 4‑class accuracy compared to a cumulative‑selector baseline that sees all data.  

## Key Results  
- Replay‑based CIL (Feature Replay, Experience Replay) achieved the highest accuracy, losing only ~7 % relative to the cumulative‑selector.  
- Regularisation (EWC, MAS, SI) and distillation (LwF) methods suffered substantially higher forgetting, often dropping below 50 % of the baseline after the second task.  
- Across 30 runs, replay methods showed the most stable performance, with variance noticeably lower than that of non‑replay approaches.  

## Why This Works (Mechanism)  
- **Rehearsal memory preserves representative samples** from earlier tasks, allowing the model to rehearse past knowledge while learning new classes, directly mitigating catastrophic forgetting.  
- **Feature Replay reconstructs intermediate activations**, enabling the network to retain useful internal representations without storing raw inputs, which is efficient for streaming settings.  
- **Experience Replay stores raw exemplars**, providing a straightforward but effective way to balance old and new data during stochastic gradient updates.  
*Note: The original manuscript does not detail additional causal mechanisms; the above interpretation follows standard CIL theory.*  

## Foundational Learning  
| Concept | Why needed | Quick check |
|---------|------------|-------------|
| Class‑incremental learning (CIL) | Provides the formal framework for learning new solver classes without retraining from scratch. | Can the model add a new class without accessing previous data? |
| Replay memory (exemplar buffer) | Supplies concrete past examples to rehearse, preventing forgetting. | Is a fixed‑size buffer (e.g., 100 samples) maintained and sampled during training? |
| Algorithm‑selection portfolio | The downstream task that benefits from accurate class predictions of solvers. | Does the classifier output a selector that chooses among FF, BF, NF, WF? |
| Catastrophic forgetting | The primary failure mode CIL aims to avoid in continual learning. | Does accuracy on earlier tasks drop sharply after learning new tasks? |
| Online bin‑packing benchmark | A controlled testbed that mimics streaming optimisation problems. | Are instances generated sequentially with deterministic heuristics? |

## Architecture Onboarding  
- **Component map:** Data Stream → Task Splitter → CIL Model (Replay Buffer) → Classifier → Selector Output  
- **Critical path:** Incoming instance → Task Splitter determines current task → Classifier updates using current batch + replay samples → Updated model predicts best solver → Selector outputs decision.  
- **Design tradeoffs:**  
  - *Memory budget vs. forgetting*: Larger replay buffers reduce forgetting but increase storage/computation.  
  - *Complexity of regularisation vs. simplicity of replay*: Regularisation methods add overhead to loss computation; replay adds data‑handling overhead.  
  - *Feature vs. raw‑sample replay*: Feature Replay saves activations (lower memory) but requires additional decoder; Experience Replay is simpler but stores raw inputs.  
- **Failure signatures:**  
  - Sudden drop in accuracy on earlier heuristics after a new task → insufficient replay or poor exemplar selection.  
  - High variance across runs → unstable training hyper‑parameters or inadequate buffer diversity.  
  - Memory overflow → buffer size exceeds allowed budget, causing runtime errors.  
- **First 3 experiments:**  
  1. Baseline cumulative selector trained on all tasks (oracle).  
  2. Replay‑based CIL (Feature Replay) with 100‑sample buffer on D1→D2 sequence.  
  3. Regularisation‑based CIL (EWC) under identical data conditions for direct comparison.  

## Open Questions the Paper Calls Out  
- How does replay‑based CIL scale to larger numbers of solver classes and more complex optimisation domains?  
- What is the impact of different exemplar‑selection strategies (e.g., herding, random, diversity‑maximising) on forgetting?  
- Can adaptive memory budgets improve the trade‑off between storage cost and performance?  

## Limitations  
- Evaluation limited to a single online bin‑packing benchmark with only four deterministic heuristics.  
- Fixed rehearsal budget (100 exemplars) explored without systematic analysis of memory size effects.  
- Hyper‑parameter settings, network architecture, and training schedules are not fully disclosed, hindering exact replication.  

## Confidence  
- Replay‑based CIL consistently outperforms regularisation and distillation on the reported benchmark → **Medium**  
- Forgetting limited to ≈ 7 % loss relative to the cumulative selector → **Low** (lack of variance reporting)  
- Replay‑based CIL is a viable strategy for continual algorithm‑selection in streaming optimisation → **Medium**  

## Next Checks  
1. **Obtain the full manuscript and accompanying code** to extract precise model architectures, learning rates, and exemplar‑selection policies for each CIL method.  
2. **Re‑run the three baseline experiments** (oracle, Feature Replay, EWC) on the published bin‑packing benchmark, reporting mean accuracy with confidence intervals over 30 runs.  
3. **Extend the study** to an additional algorithm‑selection domain (e.g., SAT solver portfolio) and vary the replay buffer size (e.g., 50, 200, 500 samples) to assess generality and memory‑performance trade‑offs.