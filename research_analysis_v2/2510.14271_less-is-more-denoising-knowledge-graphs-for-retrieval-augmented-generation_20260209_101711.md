---
ver: rpa2
title: 'Less is More: Denoising Knowledge Graphs For Retrieval Augmented Generation'
arxiv_id: '2510.14271'
source_url: https://arxiv.org/abs/2510.14271
tags:
- entity
- entities
- knowledge
- arxiv
- graph-based
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of noisy and redundant knowledge
  graphs (KGs) generated by large language models (LLMs) in graph-based retrieval-augmented
  generation (RAG) systems. These redundancies, including duplicate entities and unreliable
  relationships, degrade retrieval and generation performance while increasing computational
  costs.
---

# Less is More: Denoising Knowledge Graphs For Retrieval Augmented Generation

## Quick Facts
- arXiv ID: 2510.14271
- Source URL: https://arxiv.org/abs/2510.14271
- Authors: Yilun Zheng; Dan Yang; Jie Li; Lin Shang; Lihui Chen; Jiahao Xu; Sitao Luan
- Reference count: 31
- Key outcome: Graph-based RAG performance improves by 40% graph size reduction through entity resolution and triple reflection

## Executive Summary
This paper addresses the problem of noisy and redundant knowledge graphs (KGs) generated by large language models (LLMs) in graph-based retrieval-augmented generation (RAG) systems. These redundancies, including duplicate entities and unreliable relationships, degrade retrieval and generation performance while increasing computational costs. The proposed DEG-RAG framework improves KG quality by combining entity resolution (removing redundant entities) and triple reflection (filtering erroneous relations). Entity resolution employs multiple strategies for blocking, embedding, matching, and merging, while triple reflection uses LLM-based scoring to remove low-quality triples. Experiments on four datasets and four graph-based RAG methods show that DEG-RAG reduces graph size by approximately 40% while consistently improving question-answering performance. Notably, traditional KG embeddings like ComplEx can rival LLM embeddings, and simple direct merging often outperforms synonym-only linking. The approach demonstrates that KG quality matters more than size for effective graph-based RAG.

## Method Summary
DEG-RAG employs a two-stage pipeline for denoising LLM-generated knowledge graphs. The first stage, entity resolution, identifies and merges duplicate entities through a process of blocking (partitioning entities into candidate groups using type-aware, semantic, or structural strategies), embedding (computing similarity scores using KG embeddings or LLM embeddings), matching (computing similarity scores using embeddings and neighbor information), and merging (combining matched entities into canonical representations). The second stage, triple reflection, filters erroneous relations by having an LLM judge score each triple's reliability based on semantic accuracy, logical coherence, and entity type compatibility, removing those below a threshold. The framework operates with a default entity reduction ratio of 40% and triple reflection threshold of 0.2, achieving consistent performance improvements across multiple datasets and RAG architectures.

## Key Results
- DEG-RAG reduces knowledge graph size by approximately 40% while improving question-answering performance across four datasets
- Type-aware blocking consistently outperforms semantic and structural blocking strategies for entity resolution
- Traditional KG embeddings (ComplEx) rival LLM embeddings in matching quality while being more computationally efficient
- Direct merging of duplicate entities outperforms synonym-only linking by reducing retrieval hop counts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Entity resolution improves Graph-based RAG performance by consolidating duplicate entities, creating more connected knowledge graphs that enable better multi-hop retrieval.
- Mechanism: The process works through a pipeline: (1) Blocking partitions entities into candidate groups using type-aware, semantic, or structural strategies; (2) Matching computes similarity scores using embeddings and neighbor information; (3) Merging combines matched entities into canonical representations, reconnecting their relations. This consolidation reduces redundancy while preserving semantic meaning, creating denser connectivity that improves retrieval paths.
- Core assumption: Entities with similar embeddings, shared neighbors, or matching types represent the same real-world concept and can be safely merged without losing critical information.
- Evidence anchors:
  - [abstract] "DEG-RAG reduces graph size by approximately 40% while consistently improving question-answering performance"
  - [section 5.2] Table 1 shows winning rates favoring cleaned KGs over original KGs across LightRAG, HippoRAG, LGraphRAG, and GGraphRAG
  - [corpus] Weak corpus signal - no directly comparable entity resolution studies for LLM-generated KGs found in neighbors
- Break condition: Over-merging entities with only marginal semantic similarity may collapse fine-grained distinctions needed for domain-specific reasoning; performance degrades when reduction ratio exceeds ~70% on some datasets.

### Mechanism 2
- Claim: Triple reflection using LLM-as-judge filters erroneous relations by scoring triple reliability based on semantic accuracy, logical coherence, and entity type compatibility.
- Mechanism: Each triple (source, relation, target) is evaluated by an LLM judge that considers domain knowledge, factual correctness, relationship specificity, and type compatibility. Triples scoring below threshold δTR (default 0.2) are removed, eliminating noise from outdated or incorrect facts in source corpora and extraction errors from batched NER processing.
- Core assumption: LLMs can reliably distinguish valid from invalid triples based on commonsense and domain knowledge without access to ground truth verification.
- Evidence anchors:
  - [abstract] "triple reflection uses LLM-based scoring to remove low-quality triples"
  - [section 5.5] Ablation study shows performance degradation without triple reflection, though entity resolution has larger impact
  - [corpus] Corpus papers address KG quality but don't evaluate LLM-as-judge for triple filtering specifically
- Break condition: LLM judge calibration bias may systematically over/under-score certain relation types; threshold sensitivity requires dataset-specific tuning.

### Mechanism 3
- Claim: Design choices in entity resolution components matter—type-aware blocking outperforms alternatives, traditional KG embeddings rival LLM embeddings, and direct merging beats synonym-only linking.
- Mechanism: Type-aware blocking naturally groups entities by inductive bias; ComplEx embeddings capture asymmetric relations in complex vector space competitive with expensive LLM embeddings; direct merging consolidates entities into single nodes rather than adding synonym edges that increase retrieval hop counts.
- Core assumption: Entity type provides stronger grouping signal than semantic or structural similarity for resolution quality; simpler merging strategies reduce graph redundancy more effectively than preserving original entities.
- Evidence anchors:
  - [section 5.3] Figure 3 shows type-based blocking winning rates highest across datasets; ComplEx outperforms Qwen3-Embedding on Legal and Agriculture; direct merging consistently beats synonym linking
  - [section 4.1] "type-aware blocking is the most effective blocking method"
  - [corpus] Corpus papers on KG-RAG don't systematically compare entity resolution components
- Break condition: Type-aware blocking requires reliable type classification; traditional KG embeddings need sufficient graph structure for training; direct merging may lose nuanced entity distinctions in specialized domains.

## Foundational Learning

- **Entity Resolution in Knowledge Graphs**
  - Why needed here: Core technique for identifying and merging duplicate entities across heterogeneous sources; requires understanding blocking (candidate generation), matching (similarity computation), and merging (canonical selection) strategies.
  - Quick check question: Given entities "LLM," "Large Language Model," and "llms" with shared neighbors, which blocking strategy would group them together?

- **Knowledge Graph Embeddings**
  - Why needed here: Foundation for similarity-based matching; TransE, DistMult, ComplEx, and GNN-based embeddings offer different trade-offs between computational cost and ability to capture complex relational patterns.
  - Quick check question: Why might ComplEx embeddings outperform LLM embeddings for entity matching in graphs with asymmetric relations?

- **Graph-Based RAG Architecture**
  - Why needed here: Context for understanding how KG quality affects retrieval and generation; systems like LightRAG, HippoRAG, and MS GraphRAG use different retrieval strategies over graph structures.
  - Quick check question: How does graph connectivity enable multi-hop reasoning that vanilla chunk-based RAG cannot achieve?

## Architecture Onboarding

- **Component map:**
  Input: Raw documents → Chunking → LLM-based NER → Raw triples/entities
  Entity Resolution: Blocking → Embedding → Matching → Merging/Linking
  Triple Reflection: LLM judge scoring → Threshold filtering
  Output: Denoised KG → Graph-based RAG retrieval → Generation

- **Critical path:**
  1. Type-aware blocking (fastest path to quality improvement)
  2. Ego + neighbor similarity matching (balances accuracy and efficiency)
  3. Direct merging with description summarization (maximizes redundancy reduction)

- **Design tradeoffs:**
  - Blocking strategy: Type-aware (best quality, requires classification) vs. Semantic (no preprocessing, expensive clustering) vs. Structural (uses graph topology, may miss isolated duplicates)
  - Embedding choice: LLM embeddings (high quality, expensive) vs. KG embeddings like ComplEx (competitive, requires training) vs. GNN embeddings (captures structure, complex implementation)
  - Merging approach: Direct merging (max reduction, potential info loss) vs. Synonym linking (preserves entities, increases retrieval hops)

- **Failure signatures:**
  - Excessive reduction (>70%): Over-merging collapses semantic distinctions, degrading domain-specific QA
  - Missing entity descriptions: HippoRAG shows limited improvement due to name-only entities
  - High triple reflection threshold: Over-filtering removes valid relations, especially for novel domain knowledge
  - Random merging baseline: Significantly worse than smart resolution, confirming necessity of proper matching

- **First 3 experiments:**
  1. **Component ablation:** Test entity resolution alone vs. triple reflection alone vs. combined, measuring winning rate on standard benchmarks to isolate contribution of each mechanism
  2. **Embedding comparison:** Compare ComplEx vs. Qwen3-Embedding vs. CompGCN across datasets with different relation complexity to identify cost-quality trade-offs
  3. **Reduction ratio sweep:** Test 20%, 40%, 60%, 80% entity reduction on each dataset to find optimal operating point and robustness bounds

## Open Questions the Paper Calls Out

- **Question 1:** How does DEG-RAG performance scale when applied to extremely large-scale knowledge graphs compared to the smaller datasets tested?
  - Basis: The authors state, "In future work, we will extend DEG-RAG to more datasets and larger-scale KGs," acknowledging current study is limited to "non-large-scale KGs."
  - Why unresolved: Computational costs for blocking and matching strategies may increase non-linearly with graph size, potentially making the proposed methods infeasible for industrial-scale data.
  - What evidence would resolve it: Benchmarking the framework on graphs with millions of entities to measure latency, memory usage, and accuracy retention.

- **Question 2:** Can the denoising pipeline effectively generalize to other LLM-generated data structures, such as tabular data or hierarchical trees, beyond knowledge graphs?
  - Basis: The authors explicitly propose to "generalize the denoising pipeline to other LLM-generated data structures beyond KGs" in their conclusion.
  - Why unresolved: The current mechanisms for entity resolution and triple reflection are specifically tailored to triple-based graph topologies and may not translate directly to non-graph formats.
  - What evidence would resolve it: Adapting the method to LLM-extracted tables or summaries and evaluating structural consistency and noise reduction.

- **Question 3:** How robust is the entity resolution component when applied to knowledge graphs that lack rich textual descriptions for their entities?
  - Basis: The authors note a limitation where "Gains are bounded by attribute richness. For example, graphs with only short names without rich descriptions limit resolution quality."
  - Why unresolved: The method relies on semantic embeddings derived from descriptions; graphs with only sparse identifiers (names) may suffer from low embedding quality and high false positive rates in merging.
  - What evidence would resolve it: Evaluating DEG-RAG on datasets containing only entity names without additional context to test resolution accuracy.

## Limitations

- The study relies on LLM-based scoring without ground truth validation, making results sensitive to judge calibration and threshold selection.
- Entity resolution quality depends critically on type classification accuracy and description availability, with HippoRAG showing limited improvement due to name-only entities.
- The 40% reduction target appears effective but lacks exploration of optimal operating points across different dataset characteristics.

## Confidence

- **High confidence**: Entity resolution improves graph-based RAG performance through consolidation and better connectivity; type-aware blocking outperforms alternatives; direct merging is more effective than synonym linking; ~40% reduction yields consistent improvements
- **Medium confidence**: Triple reflection using LLM-as-judge effectively filters erroneous relations; traditional KG embeddings (ComplEx) rival LLM embeddings; entity reduction ratio around 40% is optimal
- **Low confidence**: Specific threshold values (δ_ER for entity matching, δ_TR=0.2 for triple filtering) generalize across domains; LLM judge calibration remains consistent across different relation types and domains

## Next Checks

1. **Threshold sensitivity analysis**: Systematically vary δ_ER (entity matching) and δ_TR (triple filtering) across 0.1-0.5 increments to identify optimal thresholds per dataset and test robustness to calibration changes
2. **Cross-architecture generalization**: Apply DEG-RAG to additional graph-based RAG systems beyond the four tested (LightRAG, HippoRAG, LGraphRAG, GGraphRAG) to validate architecture independence
3. **Ground truth verification**: Manually validate a sample of filtered triples and merged entities against human judgment to assess LLM-as-judge accuracy and identify systematic biases in triple reflection