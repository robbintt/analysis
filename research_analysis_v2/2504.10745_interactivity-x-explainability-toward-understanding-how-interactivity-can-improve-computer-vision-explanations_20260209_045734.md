---
ver: rpa2
title: 'Interactivity x Explainability: Toward Understanding How Interactivity Can
  Improve Computer Vision Explanations'
arxiv_id: '2504.10745'
source_url: https://arxiv.org/abs/2504.10745
tags:
- explanations
- participants
- explanation
- conference
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how interactivity can improve computer
  vision explanations to address three key challenges: information overload, semantic-pixel
  gaps, and limited exploration opportunities. Through a user study with 24 participants
  across three explanation types (heatmap-based, concept-based, and prototype-based)
  and three interactive mechanisms (filtering, overlays, and counterfactuals), the
  research demonstrates that interactive features significantly enhance user understanding.'
---

# Interactivity x Explainability: Toward Understanding How Interactivity Can Improve Computer Vision Explanations

## Quick Facts
- arXiv ID: 2504.10745
- Source URL: https://arxiv.org/abs/2504.10745
- Authors: Indu Panigrahi; Sunnie S. Y. Kim; Amna Liaqat; Rohan Jinturkar; Olga Russakovsky; Ruth Fong; Parastoo Abtahi
- Reference count: 40
- Primary result: Interactive mechanisms (filtering, overlays, counterfactuals) significantly improve user understanding of computer vision explanations by addressing information overload and semantic-pixel gaps

## Executive Summary
This paper investigates how interactivity can enhance computer vision explanations by addressing three key challenges: information overload, semantic-pixel gaps, and limited exploration opportunities. Through a user study with 24 participants, the research examines three explanation types (heatmap-based, concept-based, and prototype-based) paired with three interactive mechanisms (filtering, overlays, and counterfactuals). The findings demonstrate that interactive features significantly improve user understanding, with filtering and overlays being particularly effective at reducing cognitive load and helping users connect pixel-level data to semantic concepts. While counterfactuals initially proved overwhelming, participants used them to clarify static explanations and explore model behavior more broadly.

The study provides concrete design recommendations for interactive computer vision explanations, including carefully selected default views, independent input controls, and constrained output spaces. These recommendations aim to balance exploration freedom with guided discovery, ensuring users can effectively interpret model decisions without becoming overwhelmed. The research highlights the importance of thoughtful interface design in making complex model explanations accessible and actionable for users.

## Method Summary
The study employed a within-subjects design with 24 participants who interacted with three types of computer vision explanations (heatmap-based, concept-based, and prototype-based) each paired with three interactive mechanisms (filtering, overlays, and counterfactuals). Participants completed tasks involving bird species classification using a mock-up interface, with each explanation type-mechanism combination tested in randomized order. The study used think-aloud protocols and semi-structured interviews to capture user experiences and preferences. Each session lasted approximately 45 minutes, with participants spending roughly two minutes per explanation mechanism. Data was analyzed qualitatively through thematic coding of interview transcripts and quantitative analysis of task completion patterns.

## Key Results
- Interactive features significantly enhanced user understanding of computer vision explanations
- Filtering and overlays were particularly effective at reducing information overload and bridging semantic-pixel gaps
- Counterfactuals, while initially overwhelming, helped users clarify static explanations and explore model behavior
- Participants strongly preferred interactive explanations over static formats across all explanation types
- Design recommendations emerged for implementing interactive explanations: carefully selected default views, independent input controls, and constrained output spaces

## Why This Works (Mechanism)
The effectiveness of interactive explanations stems from their ability to address fundamental cognitive challenges in interpreting computer vision models. By allowing users to filter irrelevant information, overlay semantic concepts onto pixel data, and explore counterfactual scenarios, interactive mechanisms reduce cognitive load and create clearer mental models of model decision processes. The interactivity enables users to actively construct their understanding rather than passively consuming information, leading to better retention and more accurate interpretation of model behavior. This active engagement is particularly valuable for bridging the semantic-pixel gap, where users struggle to connect abstract model concepts with concrete visual features.

## Foundational Learning
- Cognitive load theory: why needed - explains how filtering reduces information overload; quick check - measure task completion time and accuracy with/without filtering
- Semantic gap in computer vision: why needed - underlies the challenge of connecting pixel features to human concepts; quick check - compare user ability to identify relevant features with/without semantic overlays
- User-centered design principles: why needed - guides effective interface design for explanation tools; quick check - conduct usability testing with target user groups
- Exploratory data analysis: why needed - supports understanding how users interact with and learn from explanations; quick check - analyze interaction patterns across multiple sessions
- Visual perception and information processing: why needed - informs design of effective visual overlays and counterfactual presentations; quick check - conduct eye-tracking studies to measure attention patterns

## Architecture Onboarding
Component map: User Interface -> Interactive Controls -> Explanation Generation -> Model Output -> Visualization Layer

Critical path: User selects explanation type → Interactive controls adjust parameters → System generates modified explanation → Visual feedback updates in real-time → User interprets results

Design tradeoffs: Between exploration freedom and guided discovery - too much freedom can overwhelm users, while too much guidance limits learning; between real-time responsiveness and computational efficiency - complex counterfactuals require significant processing time

Failure signatures: Users abandoning interactive features due to complexity; cognitive overload when multiple controls are active simultaneously; misinterpretation of counterfactual scenarios as model errors

First experiments: 1) A/B test with and without filtering controls on information-dense explanations; 2) Compare user performance with semantic overlays versus pixel-only heatmaps; 3) Measure time-to-understanding with and without counterfactual exploration capabilities

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does interactivity in computer vision explanations amplify confirmation bias or lead to misplaced trust compared to static formats?
- Basis in paper: [explicit] The authors state in Section 5 that "Interactivity in explanations could amplify these risks, further misleading users and enabling confirmation bias," and explicitly call for research to examine these possibilities.
- Why unresolved: The study focused on user preference and understanding in short sessions but did not measure how interactivity affects user trust calibration or decision-making accuracy when models are incorrect.
- What evidence would resolve it: A user study designed to measure trust calibration and decision accuracy in "unreliable model" scenarios, comparing users using interactive controls versus static explanations.

### Open Question 2
- Question: How well do the observed benefits of interactive explanations generalize to complex, high-risk domains like medical diagnostics?
- Basis in paper: [explicit] In Section 4, the authors note that because they used a simple bird identification task, "it is unclear to what extent our findings can be generalized to more complex, high-risk, and high-impact applications such as medical diagnostics."
- Why unresolved: The study utilized a low-stakes classification task (bird species) with mock-up explanations, whereas medical domains involve higher complexity, different user expertise (clinicians), and greater consequences for error.
- What evidence would resolve it: A replication of the study using medical imaging data and domain experts as participants to evaluate if filtering and overlays yield the same reductions in cognitive load and semantic-pixel gaps.

### Open Question 3
- Question: How does the effectiveness of interactive mechanisms evolve across repeated sessions or longitudinal use?
- Basis in paper: [explicit] The authors identify a limitation in Section 4, stating that "longitudinal studies are needed to understand how users interact across repeated sessions and the role of interactive mechanisms that maintain user history."
- Why unresolved: The current study restricted participants to roughly two minutes per explanation, capturing only initial impressions rather than the learning curve or fatigue associated with long-term tool adoption.
- What evidence would resolve it: A longitudinal study tracking the same users over multiple sessions to analyze changes in interaction patterns (e.g., reliance on counterfactuals vs. overlays) and task performance speed.

## Limitations
- Small sample size of 24 participants limits generalizability and detection of smaller effect sizes
- Controlled experimental setting may not fully capture real-world usage scenarios and extended tool adoption
- Findings may not generalize beyond tested explanation types to more complex or specialized vision tasks
- Short session duration (two minutes per explanation) captures only initial impressions, not learning curves or fatigue
- Simple bird identification task may not represent the complexity of real-world computer vision applications

## Confidence
High: Core finding that interactivity improves understanding - multiple participants demonstrated clear benefits across different mechanisms
Medium: Specific design recommendations - emerged from single study context and may require adaptation for different user populations
Medium: Relative effectiveness rankings of interactive mechanisms - individual differences in user preferences and task familiarity may influence results

## Next Checks
1) Replication with larger, more diverse participant pools to establish effect sizes and generalizability
2) Longitudinal studies tracking how users interact with explanations over time in realistic workflows
3) Comparative studies testing these interactive mechanisms against emerging explanation techniques like attention-based methods or large vision models