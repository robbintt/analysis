---
ver: rpa2
title: Optimal L2 Regularization in High-dimensional Continual Linear Regression
arxiv_id: '2601.13844'
source_url: https://arxiv.org/abs/2601.13844
tags:
- regularization
- lemma
- teacher
- learning
- term
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes generalization in continual linear regression
  with L2 regularization across a sequence of tasks. The authors derive a closed-form
  expression for the expected generalization loss in the high-dimensional regime for
  arbitrary linear teachers, accommodating both label noise and multiple i.i.d.
---

# Optimal L2 Regularization in High-dimensional Continual Linear Regression

## Quick Facts
- **arXiv ID:** 2601.13844
- **Source URL:** https://arxiv.org/abs/2601.13844
- **Reference count:** 40
- **One-line primary result:** Optimal fixed regularization strength scales as T/ln(T) with task count in continual linear regression

## Executive Summary
This paper analyzes generalization in continual linear regression with L2 regularization across a sequence of tasks. The authors derive a closed-form expression for the expected generalization loss in the high-dimensional regime for arbitrary linear teachers, accommodating both label noise and multiple i.i.d. teachers. The key contribution is proving that the optimal fixed regularization strength scales nearly linearly with the number of tasks T, specifically as T/lnT, providing a practical rule of thumb that alleviates the need for extensive hyperparameter tuning with many tasks.

## Method Summary
The paper analyzes Scheme 1, a continual linear regression algorithm that uses L2 regularization between consecutive iterates. The update rule is w_t = argmin_w(||X_t·w - y_t||² + λd·||w - w_{t-1}||²), where λ is the regularization strength. The analysis is conducted in the high-dimensional regime (n,d→∞ with n/d→α≤1) using random matrix theory, specifically the Marchenko-Pastur theorem. The method accommodates both single-teacher and multiple i.i.d. teacher settings, with or without label noise.

## Key Results
- Optimal fixed regularization strength scales as T/ln(T) with task count
- L2 regularization mitigates label noise effects that would otherwise create a persistent error floor
- Performance with fixed λ eventually degrades as T grows beyond a threshold, but with optimally scaled λ*(T), generalization continues to improve

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** L2 regularization mitigates label noise effects that would otherwise create a persistent error floor in continual learning.
- **Mechanism:** The regularization term penalizes deviations from previous iterates, effectively averaging out random label noise across tasks. Without regularization, the noise floor scales as dv_z/(d-n-1) and persists regardless of task count. With properly calibrated regularization, this floor decreases as more tasks are observed.
- **Core assumption:** Features are i.i.d. with zero mean and isotropic covariance (Σ = v_x I); label noise is independent of features.
- **Evidence anchors:** [abstract] "We demonstrate that isotropic regularization mitigates label noise under both single-teacher and multiple i.i.d. teacher settings"; [Section 4, Figure 2] Compares unregularized (λ→0) against optimal λ*, showing regularization substantially improves generalization under high label noise; [corpus] Related work (Lin et al. 2023) shows unregularized schemes suffer from noise floors in the same setting.
- **Break condition:** When features have strong anisotropic covariance without whitening, the isotropic regularizer may not align with data geometry, and quantitative predictions diverge from theory (see Appendix B.1-B.2).

### Mechanism 2
- **Claim:** The optimal fixed regularization strength scales nearly linearly with the number of tasks: λ* ≍ T/ln(T).
- **Mechanism:** As T increases, more noise accumulates from label noise and teacher variance. The log(T) denominator reflects a log(signal/noise) factor where signal = ||w*||² v_x α T (shared structure reinforced across tasks) and noise = v_z + v_x Tr(Σ)(1+α) (label noise and teacher variance). The linear T scaling ensures sufficient "stiffness" to average noise across the growing task sequence.
- **Core assumption:** i.i.d. teachers with non-zero mean; high-dimensional regime (n,d→∞, n/d→α≤1).
- **Evidence anchors:** [abstract] "We prove that the optimal fixed regularization strength scales nearly linearly with the number of tasks T, specifically as T/ln T"; [Theorem 6, Section 3.1.1] Provides explicit bounds: (1-ε)C T/ln(S/N) < λ* < (1+ε)C T/ln(S/N); [corpus] Concurrent work (Levinstein et al. 2025) shows λ ≍ ln(T) for worst-case training error, but this paper finds λ* → 0 without noise, establishing that noise sources (label or teacher variance) are necessary for non-zero optimal regularization.
- **Break condition:** If teachers have zero mean (E[w*_t] = 0) with zero initialization, optimal regularization diverges (λ* → ∞). If Tr(Σ) = v_z = 0 (no noise), then λ* → 0.

### Mechanism 3
- **Claim:** Under fixed λ, performance eventually degrades as T grows beyond a threshold—resembling overfitting—but with optimally scaled λ*(T), generalization continues to improve.
- **Mechanism:** With fixed λ, teacher variance (Tr(Σ) > 0) causes accumulated drift from the mean teacher, increasing error. With horizon-calibrated λ*(T), regularization strength increases proportionally, maintaining the stability-plasticity balance and enabling convergence to the global teacher mean (w_T → w* in mean square).
- **Core assumption:** Multiple i.i.d. teachers with Tr(Σ) > 0.
- **Evidence anchors:** [Lemma 5] Single teacher with optimal λ*(T): generalization monotonically decreases in T; Multiple i.i.d. teachers with fixed λ: generalization monotonically increases for T ≥ T'; [Theorem 7] As T→∞, λ→∞, the iterates converge to w* in mean square; [corpus] Corpus evidence for this specific stability-drift mechanism is limited; related work focuses on worst-case bounds rather than statistical generalization.
- **Break condition:** With only a single teacher (Tr(Σ) = 0) and no label noise (v_z = 0), regularization only slows convergence without benefit, so λ* → 0.

## Foundational Learning

- **Concept: Ridge Regression and the Bias-Variance Tradeoff**
  - Why needed here: The paper analyzes regularized linear regression; understanding how λ controls the balance between fitting noise (variance) and underfitting signal (bias) is essential for interpreting the scaling law.
  - Quick check question: If you double λ in ridge regression, does the solution move closer to or farther from the minimum-norm interpolator?

- **Concept: Catastrophic Forgetting and Stability-Plasticity Dilemma**
  - Why needed here: Continual learning requires balancing retention of past knowledge (stability) with acquisition of new knowledge (plasticity); λ directly controls this tradeoff.
  - Quick check question: What happens to plasticity if λ is set very large? What happens to stability if λ is set near zero?

- **Concept: Marchenko-Pastur Distribution and Random Matrix Theory**
  - Why needed here: The closed-form expressions derive from approximating resolvents (X^T X + λI)^{-1} using deterministic equivalents in the high-dimensional limit.
  - Quick check question: In the limit n,d→∞ with n/d→α, what is the asymptotic behavior of the eigenvalues of (1/n)X^T X for i.i.d. Gaussian entries?

## Architecture Onboarding

- **Component map:**
  Task t → [Feature matrix X_t, Labels y_t = X_t w*_t + z_t] → Update rule: w_t = (X_t^T X_t + λd I)^{-1} (X_t^T y_t + λd w_{t-1}) → Output: w_T (final predictor) → Generalization: G_T = (1/T) Σ ||w_T - w*_i||²

- **Critical path:**
  1. Initialize w_0 (typically zero)
  2. For each task t: receive (X_t, y_t), compute update using Scheme 1
  3. After T tasks, evaluate on all previous tasks (no replay buffer needed)
  4. Key hyperparameter: λ must be set according to T/ln(T) scaling

- **Design tradeoffs:**
  | Choice | Benefit | Cost |
  |--------|---------|------|
  | Large λ | Strong stability, noise resilience | Low plasticity, slow convergence |
  | Small λ | High plasticity, fast initial learning | Noise floor persists, forgetting |
  | Horizon-calibrated λ*(T) | Optimal balance | Requires knowing/estimating T ahead |

- **Failure signatures:**
  - Generalization plateaus early → λ too small (noise floor dominates)
  - Model fails to learn new tasks → λ too large (over-regularization)
  - Quantitative mismatch between predicted and empirical λ* → Non-isotropic feature covariance (whitening needed; see Appendix B.2)
  - Error increases with more tasks (fixed λ) → Teacher variance (Tr(Σ)) non-zero; switch to horizon-calibrated λ

- **First 3 experiments:**
  1. **Validate T/ln(T) scaling on synthetic data:** Generate high-dimensional linear regression tasks with i.i.d. Gaussian features, known label noise v_z, and i.i.d. teachers. Sweep T ∈ {10, 100, 1000} and find empirical λ*_emp(T) via grid search. Plot λ*_emp(T) × ln(T)/T to verify convergence to constant.
  2. **Ablate noise sources:** Run three conditions: (a) v_z > 0, Tr(Σ) = 0; (b) v_z = 0, Tr(Σ) > 0; (c) both > 0. Verify that λ* > 0 emerges only when noise sources are present, per Theorem 6.
  3. **Test beyond linear models on MNIST:** Apply Scheme 1 to a 2-layer ReLU network on binary MNIST with bit-flip label noise. Compare empirical optimal λ against T/ln(T) prediction. Add whitening preprocessing and verify improved alignment with theoretical prediction (per Appendix B.2).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the T/ln T scaling law for optimal regularization strength hold provably for non-linear models beyond the empirical ReLU network observations?
- **Basis in paper:** [explicit] "Future research could relax these assumptions by considering non-linear models..."
- **Why unresolved:** The theoretical analysis relies critically on linear teacher models and closed-form resolvent approximations (Marchenko-Pastur theorem). Neural network experiments show qualitative agreement but lack formal proof.
- **What evidence would resolve it:** A theoretical extension using neural tangent kernel (NTK) or similar non-linear approximations, or broader empirical validation across diverse architectures and non-synthetic datasets.

### Open Question 2
- **Question:** How can the quantitative mismatch between theoretically predicted and empirically optimal regularization strengths be resolved without requiring a whitening preprocessing step?
- **Basis in paper:** [inferred] "the quantitative prediction—i.e., the exact value accounting for problem-dependent factors—does not align perfectly with the empirical optimum" and experiments show that "second-order feature correlations are largely responsible" (Section B.1, B.2).
- **Why unresolved:** The theory assumes isotropic feature covariance (Σ = v_x I), but real data violates this assumption; whitening restores alignment but may not always be practical.
- **What evidence would resolve it:** An extended theoretical analysis incorporating non-isotropic covariance structures, or a correction term for the optimal λ formula that accounts for empirical covariance eigenstructure.

### Open Question 3
- **Question:** How does the optimal regularization scaling change when task sequences are non-i.i.d. (e.g., Markovian or with structured drift)?
- **Basis in paper:** [explicit] "Future research could relax these assumptions by considering... non-i.i.d. environments (e.g., Markovian task sequences)."
- **Why unresolved:** The current closed-form expressions for expected generalization loss rely on the i.i.d. teacher assumption, which simplifies the temporal correlation terms in Theorem 3.
- **What evidence would resolve it:** Derivation of generalization bounds for Markovian task sequences, or empirical characterization of how correlation structure in tasks affects the optimal λ scaling.

### Open Question 4
- **Question:** What is the theoretical relationship between simple L2 regularization and more expressive non-isotropic regularization schemes (e.g., Fisher-based methods) in continual learning?
- **Basis in paper:** [explicit] "While exploring the connections between meta-learning and non-isotropic weight matrices remains a promising direction for future work..."
- **Why unresolved:** The paper shows L2 works well empirically but does not characterize when or by how much non-isotropic methods could improve over the scalar regularization baseline.
- **What evidence would resolve it:** A theoretical comparison of generalization bounds for L2 vs. matrix-weighted regularization under varying task heterogeneity, or empirical ablations isolating the benefit of directional vs. scalar regularization.

## Limitations

- The theoretical analysis assumes i.i.d. isotropic features with zero mean, which may not hold in practice and can cause quantitative mismatches without whitening.
- The framework is limited to linear models, with only empirical validation on a simple 2-layer ReLU network for MNIST.
- The analysis assumes linear teachers and Gaussian features, limiting its applicability to more complex real-world scenarios.

## Confidence

- **High confidence**: The T/ln(T) scaling law for optimal regularization (Theorem 6) is rigorously proven and experimentally validated across synthetic and MNIST datasets.
- **Medium confidence**: The mechanism explaining how isotropic regularization mitigates label noise (Mechanism 1) is supported by theory and experiments but assumes idealized feature distributions.
- **Medium confidence**: The claim that fixed λ leads to eventual degradation with multiple teachers while optimal λ*(T) enables convergence (Mechanism 3) is theoretically sound but relies on assumptions about i.i.d. teacher variance that may not generalize to all continual learning scenarios.

## Next Checks

1. **Validate robustness to feature covariance:** Test the T/ln(T) scaling on synthetic data with varying degrees of feature anisotropy (from identity to highly correlated) and measure how whitening affects the match between empirical and theoretical optimal λ.

2. **Test beyond binary classification:** Apply the continual linear regression scheme to multi-class MNIST (10 classes) and CIFAR-10 with multiple i.i.d. teachers, verifying whether the optimal λ scaling holds for more complex, real-world scenarios.

3. **Explore initialization sensitivity:** Investigate how different initialization strategies (e.g., random vs. zero) affect the need for regularization and the validity of the T/ln(T) scaling, particularly when teachers have zero mean.