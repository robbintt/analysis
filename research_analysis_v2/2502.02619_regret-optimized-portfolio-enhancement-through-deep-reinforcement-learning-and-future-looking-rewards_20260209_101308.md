---
ver: rpa2
title: Regret-Optimized Portfolio Enhancement through Deep Reinforcement Learning
  and Future Looking Rewards
arxiv_id: '2502.02619'
source_url: https://arxiv.org/abs/2502.02619
tags:
- portfolio
- learning
- return
- reward
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses dynamic portfolio optimization, aiming to enhance
  traditional 60/40 strategies using reinforcement learning. The authors introduce
  a negative Sharpe regret reward function that incorporates Oracle knowledge to improve
  allocation decisions, while addressing transaction costs via a scheduling mechanism
  and using synthetic data via circular block bootstrap to improve generalization.
---

# Regret-Optimized Portfolio Enhancement through Deep Reinforcement Learning and Future Looking Rewards

## Quick Facts
- arXiv ID: 2502.02619
- Source URL: https://arxiv.org/abs/2502.02619
- Reference count: 40
- Primary result: Regret-based RL approach achieves higher annual returns and better Calmar ratios than benchmarks, with strongest performance in recent market phase

## Executive Summary
This paper addresses dynamic portfolio optimization by introducing a regret-based reward function that incorporates Oracle knowledge of future returns. The authors train PPO agents using a negative Sharpe regret reward that penalizes deviation from what would have been optimal allocations given 14 days of forward-looking returns. The approach includes transaction cost scheduling via curriculum learning and synthetic data generation through circular block bootstrap to improve generalization. Evaluated across three market phases, the regret-based method consistently outperforms traditional 60/40 strategies and ablation baselines, particularly in the most recent test period (2015-2018).

## Method Summary
The method uses PPO with a negative Sharpe regret reward: r_t = -μ̄_{t+n}(w* - w_t)', where w* is computed by an Oracle that solves a constrained optimization using n=14 days of forward returns and 42-day covariance window. Transaction costs are scheduled from near-zero to 0.0025 via TC_train(x) = TC_max · (x/S)^α over S=100×episode_length steps, with α=1.0 for Phase 1 and α=0.45 for Phases 2-3. Circular block bootstrap generates synthetic data every 10 episodes using 70-90% blocks of training length, with 70% probability of selecting synthetic vs. real data. The agent observes 19-dim vectors (returns, volatility, lagged features, previous weights, TC scalar) and outputs portfolio weights via softmax. Twenty independent agents are trained per phase with weight transfer across phases, and the best model from the validation Pareto front (return vs. MDD) is selected for testing.

## Key Results
- Annual returns: Regret-based approach achieves 6.85% (Phase 1), 7.18% (Phase 2), and 5.96% (Phase 3) vs. 60/40 baseline of 5.74%, 6.38%, and 4.77% respectively
- Calmar ratios: Significantly better in most periods, with Phase 3 showing 0.24 vs. 0.18 for baseline
- Maximum drawdown: Phase 3 regret-based achieves 24.73% vs. 26.58% for baseline
- Transaction costs: Scheduling + bootstrap + regret combination outperforms all ablations on test data
- Generalization: During 2015-2018 test phase, regret-based method shows strongest performance with highest return and lowest MDD among all approaches

## Why This Works (Mechanism)

### Mechanism 1
The negative Sharpe regret reward function may improve out-of-sample performance by providing dense learning signals that encode optimal allocation behavior. During training, the Oracle computes w* (optimal weights given n=14 days of forward-looking returns) via Equation 9. The agent receives reward = -μ'ₜ₊ₙ(w* - wₜ), penalizing deviation from what would have been optimal. This creates a supervised-like signal within RL, teaching the agent to approximate Oracle behavior without direct imitation. Core assumption: The Oracle's optimization horizon (14 days forward, 42-day covariance window) captures actionable market structure that generalizes beyond training data. Evidence: Abstract states future-looking rewards "facilitate the learning of generalizable allocation strategies." Mathematical formulation in Section 4.4, Equation 9. Related work (Kochliaridis et al.) uses similar future-looking rewards. Break condition: If forward-looking horizon doesn't capture predictive signal (e.g., pure random walk), Oracle provides no learnable structure—agent converges to random allocations.

### Mechanism 2
Transaction cost scheduling via curriculum learning may prevent early-training signal attenuation while ensuring realistic cost awareness at deployment. TC_train(x) = TC_max · (x/S)^α ramps costs from near-zero to full (0.0025) over S = 100 × episode_length steps. Early episodes focus on learning allocation logic; later episodes incorporate friction. Concave α (0.45 in Phases 2-3) accelerates ramp-up for pretrained agents. Core assumption: The optimal allocation policy learned without TC is sufficiently similar to the TC-aware policy that curriculum transfer is beneficial. Evidence: Abstract mentions developing "transaction cost scheduler" to "prevent signal loss." Formal TC schedule definition in Section 4.3, Equation 7; Section 5.1 notes TC-only and BB-only underperform their combination. No direct corpus evidence for TC scheduling in portfolio RL; appears novel. Break condition: If optimal TC-free policy differs substantially from TC-aware policy (e.g., requires high-frequency rebalancing), curriculum transfer fails—agent must unlearn behavior.

### Mechanism 3
Circular block bootstrap synthetic data may improve generalization by exposing the agent to varied market trajectories while preserving temporal structure. Every 10 episodes, synthetic data is generated by resampling blocks (70-90% of training length) from original data. Agent alternates between real and synthetic episodes. Large blocks preserve autocorrelation; randomization prevents overfitting to single historical path. Core assumption: Bootstrap samples adequately represent the distribution of future market regimes, including tail events. Evidence: Abstract states synthetic data training "facilitate the learning of generalizable allocation strategies." Section 4.5 notes "Larger blocks... better preserve long-range temporal dependencies"; Figure 1 shows TC+BB+Regret underperforms in training but generalizes better. Peña et al. and Pagnoncelli et al. use synthetic data for portfolio optimization; different generation methods (CTGAN vs. bootstrap). Break condition: If block size is too small, temporal structure breaks down; if too large, synthetic data approximates original too closely, defeating regularization purpose.

## Foundational Learning

- **Proximal Policy Optimization (PPO)**: Core algorithm for policy learning; clipping mechanism (ε=0.2) stabilizes updates in noisy financial environment. Why needed: Prevents a single bad market episode from destroying learned policy through aggressive updates.
- **Sharpe Ratio and Mean-Variance Optimization**: Oracle computes w* by maximizing Sharpe; understanding this objective is essential for interpreting reward signals. Why needed: Forms the basis for the regret reward calculation and Oracle optimization. Quick check: Derive why Sharpe-optimal weights depend on both return forecasts (μ) and covariance (Σ).
- **Circular Block Bootstrap**: Data augmentation strategy; must understand why block sampling (vs. i.i.d. resampling) preserves time-series properties. Why needed: Preserves temporal dependencies crucial for financial data while providing regularization. Quick check: Why would Gaussian copula fail for this application per Section 4.5's discussion of tail distributions?

## Architecture Onboarding

- **Component map**: Oracle Module -> Environment Wrapper -> Actor Network -> Action Output; Critic Network -> Value Function -> Advantage Computation; Data Pipeline -> Circular Block Bootstrap -> Episode Generator
- **Critical path**: 1. Initialize networks from previous phase (if available) or random. 2. For each episode: select real vs. synthetic data via Bernoulli(0.7). 3. At each timestep: compute Oracle w* (training only), execute action, compute regret reward, store transition. 4. After episode: compute GAE advantages, update networks via PPO clipped objective. 5. Validate; select best model from Pareto front for next phase.
- **Design tradeoffs**: Forward-looking horizon (n=14): Longer horizons may capture more signal but increase Oracle computation and risk look-ahead bias if leakage occurs. Block size (70-90%): Larger blocks preserve structure but reduce regularization effect. TC ramp convexity (α=1.0 vs. 0.45): Faster ramp for pretrained agents prevents overfitting to TC-free regime but may destabilize if policy hasn't converged.
- **Failure signatures**: Constant allocations: Agent learns wₜ ≈ wₜ₋₁ to avoid TC penalty; check if TC ramp is too aggressive or reward signal too weak. Validation collapse: Training reward increases but validation decreases; likely overfitting—reduce block size or increase entropy regularization. Extreme positions: Agent allocates 100% to single asset; check reward scaling and Oracle regularization term.
- **First 3 experiments**: 1. Baseline replication: Run PPO with only return reward (no regret, no TC, no BB) on Phase 1; establish overfitting baseline per Figure 1. 2. Ablation by component: Add one mechanism at a time (TC → BB → Regret) to isolate contribution; expect TC+BB+Regret to underperform in training but excel in test. 3. Oracle horizon sweep: Test n ∈ {7, 14, 21, 28} on validation set; identify optimal look-ahead for each phase's volatility regime.

## Open Questions the Paper Calls Out

### Open Question 1
How would the regret-based reward function perform with alternative Oracle forecasting horizons (n ≠ 14 business days), and is there an optimal look-ahead window? Basis: Future Work states "Additional reward functions and their hyperparameters (such as Oracle's forecasting horizon) could be explored." Unresolved because authors fixed n = 14 business days based on initial experimentation without systematic analysis. Evidence needed: Ablation experiments varying n across a range (e.g., 5, 10, 14, 21, 30 days) and comparing out-of-sample Sharpe ratios, Calmar ratios, and maximum drawdowns.

### Open Question 2
Can the approach scale to higher-dimensional portfolios with more than 3 assets while maintaining computational tractability and generalization? Basis: Future Work states "The analysis of scalability may be conducted and the effect of larger portfolios and high-dimensional policies can be investigated." Unresolved because current action space is limited to K = 3 trading strategies (equities, 60/40, bonds), and it's unclear whether the method generalizes to larger asset universes. Evidence needed: Experiments applying the same pipeline to portfolios with 10, 20, or 50 assets, reporting training time, policy stability, and performance metrics relative to benchmarks.

### Open Question 3
Would explicitly incorporating maximum drawdown into the reward function improve consistency in risk control compared to the current Sharpe-based regret formulation? Basis: Authors acknowledge "MDD was not explicitly included in the reward, leading to less consistent performance in this measure" and the embedded drawdown baseline achieved better MDD at lower returns. Unresolved because trade-off between return optimization and drawdown control remains unbalanced; Oracle uses Sharpe ratio which penalizes variance, not tail risk. Evidence needed: Comparative experiments with reward functions that include explicit MDD or conditional drawdown-at-risk terms, evaluated on Calmar ratios and tail-risk metrics during crisis periods (e.g., Phase 2 pandemic).

### Open Question 4
Can imitation learning methods (GAIL, AIRL) be stabilized through hyperparameter tuning to successfully pre-train agents using Oracle demonstrations? Basis: Section 5.2 states "All of the GAIL, AIRL, and density-based reward modeling methods appeared to be highly sensitive to hyperparameters and did not result in improvement... we see these approaches as promising for this application." Unresolved because authors attempted these methods but found them too sensitive; failure may reflect implementation issues rather than fundamental limitations. Evidence needed: Systematic hyperparameter sweeps for imitation learning methods, combined with curriculum-based pre-training, reporting sample efficiency gains and final policy performance.

## Limitations

- Oracle knowledge (future-looking rewards) may not generalize to real-world deployment where perfect foresight is unavailable
- Transaction cost scheduling assumes optimal TC-free and TC-aware policies are sufficiently similar for curriculum transfer
- Circular block bootstrap may fail to capture extreme market events if block sizes are too small or training data lacks sufficient tail events

## Confidence

**High Confidence** (Well-supported by evidence and methodology):
- Regret-based reward formulation is mathematically sound and provides principled way to incorporate Oracle knowledge
- Circular block bootstrap preserves temporal structure better than alternative synthetic data methods
- Transaction cost scheduling via curriculum learning is a novel and theoretically motivated approach

**Medium Confidence** (Partially supported but with limitations):
- Claim that TC+BB+Regret outperforms all ablations on test data is supported by Table 2, but magnitude of improvement varies significantly across market phases
- Assertion that weight transfer across phases improves performance is reasonable given methodology, though direct comparative evidence is limited
- Generalization capability demonstrated on most recent market phase (2015-2018) is encouraging but based on single test period

**Low Confidence** (Limited or indirect evidence):
- Claim that this approach "significantly outperforms" 60/40 benchmark requires more extensive out-of-sample testing across multiple market regimes
- Effectiveness of Oracle horizon (n=14 days) is assumed optimal without systematic hyperparameter tuning evidence
- Scalability of approach to larger asset universes (>3 assets) remains untested

## Next Checks

1. **Oracle Horizon Sensitivity Analysis**: Systematically test n ∈ {7, 14, 21, 28} on validation sets across all three market phases to identify optimal forward-looking windows and verify robustness of n=14 choice.

2. **Real-World Deployment Simulation**: Remove Oracle access during test evaluation and measure performance degradation. This validates whether agents truly learned to approximate optimal behavior or simply memorized Oracle signals during training.

3. **Multi-Asset Scalability Test**: Extend framework to 5-10 asset universes using same methodology. This tests whether regret-based approach scales effectively or if 3-asset case represents favorable special case.