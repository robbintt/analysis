---
ver: rpa2
title: 'MuonAll: Muon Variant for Efficient Finetuning of Large Language Models'
arxiv_id: '2511.06086'
source_url: https://arxiv.org/abs/2511.06086
tags:
- muon
- arxiv
- adamw
- available
- muonall
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Muon optimizer has been effective in pretraining but not yet explored\
  \ for finetuning existing models. To address this, we introduce MuonAll, which transforms\
  \ all parameters\u2014including 1D ones\u2014into 2D matrices to apply Muon uniformly."
---

# MuonAll: Muon Variant for Efficient Finetuning of Large Language Models

## Quick Facts
- **arXiv ID**: 2511.06086
- **Source URL**: https://arxiv.org/abs/2511.06086
- **Reference count**: 40
- **Primary result**: Muon and MuonAll optimizers match or exceed AdamW performance on SFT of 0.5B-2.7B models across 9 benchmarks

## Executive Summary
MuonAll extends the Muon optimizer from pretraining to fine-tuning by transforming all parameters—including 1D ones—into 2D matrices for uniform application. The method is evaluated on Qwen2-0.5B, SmolLM2-360M, and GPT2-medium models using OpenOrca for supervised fine-tuning and multiple benchmarks. Muon and MuonAll achieve performance comparable to AdamW, with Muon versions often surpassing it in tasks like MMLU and GSM8K. The approach eliminates AdamW dependency while maintaining or improving downstream performance, demonstrating Muon's effectiveness for fine-tuning as well as pretraining.

## Method Summary
MuonAll transforms 1D parameters into diagonal matrices to apply the Muon optimizer uniformly across all parameters. The method uses momentum and Newton-Schulz iterations (with coefficients a=3.4445, b=4.7750, c=2.0315) for orthogonalization, then converts matrices back to their original form. The paper compares Muon, MuonAll, and AdamW across three models (Qwen2-0.5B, SmolLM2-360M, GPT2-medium) using OpenOrca for supervised fine-tuning with 400k samples, max sequence length 1024, and 2 epochs. Training uses linear learning rate schedules decaying to 10^-7 on 2× NVIDIA RTX A6000 GPUs with per-model configurations for batch sizes and learning rates.

## Key Results
- Muon and MuonAll achieve MMLU scores of 43.6-44.2% on Qwen2-0.5B, comparable to AdamW
- GSM8K performance reaches 32-36% with Muon variants, often exceeding AdamW
- Wall-clock time is higher than AdamW, but token efficiency matches or exceeds it
- MuonAll eliminates AdamW dependency while maintaining competitive performance across all benchmarks

## Why This Works (Mechanism)
MuonAll leverages the orthogonality-preserving properties of the Muon optimizer by uniformly applying it to all parameters through matrix transformations. This ensures consistent optimization dynamics across different parameter types (weights, biases, LayerNorm parameters), potentially improving gradient flow and model convergence during fine-tuning.

## Foundational Learning

**Newton-Schulz iterations**: Iterative method for matrix inversion and orthogonalization, crucial for Muon's parameter updates. *Why needed*: Enables efficient orthogonalization without explicit SVD computation. *Quick check*: Verify convergence by monitoring ||M_t||_F normalization and singular value distribution.

**Spectral norm constraints**: Muon optimizes parameters under spectral norm bounds to maintain model stability. *Why needed*: Prevents exploding gradients and maintains Lipschitz continuity. *Quick check*: Monitor gradient norms and parameter magnitudes during training.

**Diagonal matrix transformation**: Converts 1D parameters to diagonal matrices for uniform Muon application. *Why needed*: Allows biases and LayerNorm parameters to benefit from Muon's orthogonality properties. *Quick check*: Verify parameter shapes before/after conversion and ensure gradients flow correctly.

## Architecture Onboarding

**Component map**: Input tensors -> Diagonal matrix conversion -> Muon optimizer (momentum + Newton-Schulz) -> Matrix back-conversion -> Parameter update

**Critical path**: The orthogonalization step via Newton-Schulz iterations is the computational bottleneck, requiring 5 sequential iterations per optimization step.

**Design tradeoffs**: MuonAll trades wall-clock time for potentially better convergence properties through orthogonality preservation, while maintaining the simplicity of AdamW's decoupled weight decay.

**Failure signatures**: 
- Shape mismatches during 1D→2D conversion causing training crashes
- Newton-Schulz divergence if input matrices aren't properly normalized
- Significantly higher training time without corresponding performance gains

**3 first experiments**:
1. Verify MuonAll's matrix conversion by instrumenting parameter shapes before/after transformation
2. Compare training loss curves between MuonAll and AdamW on a small subset of data
3. Profile wall-clock time per epoch to quantify the orthogonalization overhead

## Open Questions the Paper Calls Out

**Open Question 1**: Can Muon be extended beyond spectral norm constraints using generalized matrix norms (Schatten-p or trace norms) to improve the balance between convergence speed and model generalization? *Basis*: Conclusion explicitly suggests this direction. *Unresolved*: Current Muon relies on spectral norm orthogonalization; alternative norms have different theoretical properties. *Evidence needed*: Empirical comparison of Muon variants using Schatten-p norms across SFT tasks.

**Open Question 2**: Will MuonAll's diagonal matrix transformation for 1D parameters remain computationally efficient when scaling to multi-billion parameter models? *Basis*: Experiments limited to models "up to half billion parameters". *Unresolved*: Memory and compute overhead of transforming bias vectors could grow disproportionately at larger scales. *Evidence needed*: Scaling experiments on 7B+ models with profiling of memory usage and throughput.

**Open Question 3**: Can MuonAll's wall-clock time overhead be reduced to match or exceed AdamW while preserving benchmark performance gains? *Basis*: Conclusion states "higher wall-clock time" despite matching token efficiency. *Unresolved*: Newton-Schulz iterations introduce sequential computation not present in AdamW. *Evidence needed*: Optimized implementations compared against AdamW on identical hardware.

## Limitations

- Critical hyperparameters (momentum, weight decay) are unspecified, preventing exact reproduction
- Wall-clock time is higher than AdamW, potentially limiting practical adoption
- Experiments limited to models up to 0.5B parameters, leaving scalability questions unresolved

## Confidence

**High**: Optimizer implementation correctness (code provided, transformation logic well-described)
**Medium**: Training procedure specification (most hyperparameters specified, but key ones missing)
**Medium**: Benchmark performance claims (results plausible given methodology, but exact reproducibility requires missing hyperparameters)

## Next Checks

1. Verify MuonAll's 1D→diagonal matrix conversion by instrumenting the code to log parameter shapes before and after conversion, ensuring the orthogonalization is applied only to intended parameters
2. Implement a small-scale experiment (e.g., 10k samples, 1 epoch) with the Qwen2-0.5B model using multiple momentum values (0.85, 0.90, 0.95) to identify which yields results closest to the reported MMLU/GSM8K scores
3. Contact the authors directly to request the missing hyperparameters (momentum, weight decay, random seeds) and validate the current implementation against their provided checkpoints if available