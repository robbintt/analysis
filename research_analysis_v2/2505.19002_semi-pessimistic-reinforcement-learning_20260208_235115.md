---
ver: rpa2
title: Semi-pessimistic Reinforcement Learning
arxiv_id: '2505.19002'
source_url: https://arxiv.org/abs/2505.19002
tags:
- data
- reward
- learning
- policy
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a semi-pessimistic reinforcement learning
  approach that leverages both labeled and unlabeled data to address distributional
  shift and missing reward challenges in offline RL. The method constructs a pessimistic
  lower bound for the reward function using uncertainty quantification from both data
  sources, then applies standard RL algorithms.
---

# Semi-pessimistic Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2505.19002
- **Source URL**: https://arxiv.org/abs/2505.19002
- **Reference count**: 40
- **Primary result**: Semi-pessimistic RL using labeled and unlabeled data outperforms state-of-the-art methods on synthetic environments, MuJoCo benchmarks, and real-world adaptive deep brain stimulation for Parkinson's disease.

## Executive Summary
This paper introduces semi-pessimistic reinforcement learning, a method that leverages both labeled and unlabeled data to address distributional shift and missing reward challenges in offline RL. The approach constructs a pessimistic lower bound for the reward function using uncertainty quantification from both data sources, then applies standard RL algorithms. Unlike existing pessimistic methods, this approach avoids sequential uncertainty quantification of Q-functions or state transitions, making it simpler while maintaining theoretical guarantees. The method is validated through synthetic experiments, MuJoCo benchmarks, and a real-world application in adaptive deep brain stimulation for Parkinson's disease.

## Method Summary
The method combines labeled data (with rewards) and unlabeled data (without rewards) through a semi-supervised uncertainty quantification procedure. First, an auxiliary model is trained on labeled data to predict rewards, then residual analysis is performed using both labeled and unlabeled data to construct confidence bounds. A pessimistic reward estimate is formed by subtracting uncertainty-adjusted bounds from the predicted reward. Standard RL algorithms (either model-free like FQI or model-based like MOPO) are then applied using these pessimistic rewards. The approach relies on a semi-coverage condition where labeled data covers the optimal policy's visitation and combined data covers all policies.

## Key Results
- Theoretical analysis shows improved regret bounds compared to pessimistic learning methods
- Requires less restrictive conditions than existing methods (semi-coverage vs full coverage)
- Outperforms state-of-the-art methods on synthetic environments, MuJoCo benchmarks, and real-world adaptive DBS application
- Simpler implementation by avoiding sequential uncertainty quantification of Q-functions or state transitions
- Empirically demonstrates advantages across multiple domains with varying data distributions

## Why This Works (Mechanism)

### Mechanism 1: Reward-Only Pessimism Avoids Sequential Q-Function Uncertainty
Quantifying uncertainty only for the reward function (not Q-functions) achieves comparable regret bounds while eliminating iterative uncertainty propagation. The method constructs a lower confidence bound for rewards, then applies standard RL algorithms. As unlabeled data increases, transition uncertainty becomes negligible relative to reward estimation error, making Q-function uncertainty quantification redundant.

### Mechanism 2: Semi-Supervised Uncertainty Quantification Tightens Confidence Bounds
Combining labeled and unlabeled data via auxiliary model predictions yields tighter uncertainty bounds than labeled-only approaches. The four-step SUQ procedure fits flexible auxiliary models on labeled data, models residuals, applies to unlabeled data, and combines estimators to produce tighter bounds when the auxiliary model captures reward variation well.

### Mechanism 3: Semi-Coverage Relaxes Data Requirements
The semi-coverage condition enables learning under weaker assumptions than full coverage while maintaining theoretical guarantees. Regret bounds depend on single-policy concentration rather than uniform concentration, making it achievable when combined data covers all policies even if labeled data has narrow coverage.

## Foundational Learning

- **Concept: Offline RL and Distributional Shift**
  - Why needed here: The paper addresses learning from fixed datasets where the learned policy may visit state-action pairs absent from training data.
  - Quick check question: Can you explain why Q-learning overestimates values for poorly-sampled state-action pairs in offline settings?

- **Concept: Pessimistic Principle in RL**
  - Why needed here: Understanding why lower-bounding Q-functions or rewards helps—the method applies this only to rewards rather than the full value iteration chain.
  - Quick check question: Why might a pessimistic Q-estimate that uniformly lower-bounds Q* still yield poor policies if it's too conservative?

- **Concept: Pseudo-Labeling in Semi-Supervised Learning**
  - Why needed here: The method extends pseudo-labeling to RL by generating pessimistic reward labels for unlabeled transitions.
  - Quick check question: How does semi-supervised uncertainty quantification differ from standard pseudo-labeling where you simply impute the most likely label?

## Architecture Onboarding

- **Component map**: Labeled Data (L) -> Auxiliary Model Training -> SUQ Estimator -> Pessimistic Reward -> Model-Free Path (FQI + Pseudo-Rewards) or Model-Based Path (MOPO + Transition Model)

- **Critical path**: Algorithm 1 (SUQ) → Pessimistic reward construction → Standard RL (FQI or MOPO). The uncertainty quantification quality directly determines regret.

- **Design tradeoffs**:
  - Model-free vs Model-based: Model-free simpler but requires more iterations; Model-based handles continuous action spaces better but needs transition estimation
  - Auxiliary model choice: Random forests used in experiments; neural networks may capture more complex reward structures but increase variance
  - Significance level α: Must decrease with sample size; too large causes excessive pessimism, too small fails coverage guarantees

- **Failure signatures**:
  - Increasing regret with more unlabeled data → Check if auxiliary model is misspecified
  - Policy collapses to single action → Overly conservative Δ_SUG; verify α selection
  - Performance worse than labeled-only baseline → Semi-coverage violated; unlabeled data may introduce distribution mismatch

- **First 3 experiments**:
  1. Replicate Figure 1 gridworld experiment varying n_U/n_L ratio; confirm SPL outperforms PL when ε is small
  2. Ablation on auxiliary model: Compare random forests vs neural networks vs linear model for R̂_AUX; measure impact on Δ_SUG tightness
  3. On MuJoCo, visualize state-action distributions of L vs L∪U; verify semi-coverage holds empirically by computing B_L* and B_D estimates

## Open Questions the Paper Calls Out

- **Open Question 1**: Under what precise conditions should one prefer SPL over PPL, and can this choice be automated? The paper illustrates a trade-off empirically but provides no formal criterion for selection.

- **Open Question 2**: How can the semi-coverage condition be verified or diagnosed from finite data in practice? Coverage conditions are population-level assumptions difficult to verify from samples.

- **Open Question 3**: How does performance scale when the reward function has complex, highly nonlinear structure beyond what random Fourier features can capture? The theoretical analysis relies on linear approximations.

## Limitations

- Theoretical guarantees depend critically on semi-coverage and auxiliary model assumptions that are difficult to verify in practice
- Empirical validation limited to synthetic environments and two real-world domains, with limited testing across diverse offline RL benchmarks
- Choice of α as a function of sample size is theoretically motivated but may be suboptimal in practice
- Performance sensitive to auxiliary model specification and data distribution alignment

## Confidence

- **High**: The core mechanism of reward-only pessimism avoiding sequential Q-function uncertainty quantification is well-supported by regret analysis
- **Medium**: The SUQ procedure's benefit over labeled-only approaches relies on auxiliary model assumptions that may not hold for complex rewards
- **Low**: The semi-coverage condition's practical applicability is uncertain, as verifying it requires knowing the optimal policy a priori

## Next Checks

1. **Ablation on Coverage Condition**: Systematically vary the relationship between L and U distributions to identify minimum coverage requirements for SPL to outperform PL and baselines

2. **Auxiliary Model Robustness**: Test SPL with severely misspecified auxiliary models (e.g., linear models for nonlinear rewards) to quantify impact on uncertainty quantification quality and policy performance

3. **α-Tuning Study**: Empirically determine optimal α schedules across different problem settings, comparing theoretical prescriptions against data-driven selection methods