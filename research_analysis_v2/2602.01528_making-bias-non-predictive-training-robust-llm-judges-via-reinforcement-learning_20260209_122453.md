---
ver: rpa2
title: 'Making Bias Non-Predictive: Training Robust LLM Judges via Reinforcement Learning'
arxiv_id: '2602.01528'
source_url: https://arxiv.org/abs/2602.01528
tags:
- bias
- correct
- option
- reasoning
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Epistemic Independence Training (EIT), a
  reinforcement learning framework that addresses the vulnerability of LLM judges
  to cognitive biases. EIT uses a balanced conflict strategy where bias signals support
  correct and incorrect answers equally, combined with a reward design that penalizes
  bias-following without rewarding bias-agreement.
---

# Making Bias Non-Predictive: Training Robust LLM Judges via Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2602.01528
- **Source URL:** https://arxiv.org/abs/2602.01528
- **Reference count:** 40
- **Primary result:** EIT improves adversarial-bias accuracy by 13.2% (70.1%→83.3%) while maintaining performance when bias aligns with truth.

## Executive Summary
This paper introduces Epistemic Independence Training (EIT), a reinforcement learning framework that addresses the vulnerability of LLM judges to cognitive biases. EIT uses a balanced conflict strategy where bias signals support correct and incorrect answers equally, combined with a reward design that penalizes bias-following without rewarding bias-agreement. Experiments on Qwen3 models show EIT improves adversarial-bias accuracy by 13.2% and robustness by 16.4%, while maintaining performance when bias aligns with truth. Critically, EIT generalizes to unseen bias types and outperforms larger untrained models, demonstrating that targeted training is more effective than scaling.

## Method Summary
EIT trains LLM judges to resist cognitive biases by making bias cues non-predictive of reward through a balanced conflict strategy (50% correct-bias, 50% wrong-bias). Using GRPO on Qwen3 models, the method employs a hierarchical reward function: R_struct (0.1) for valid chain-of-thought, R_acc (1.0) for correct answers, and R_ind with asymmetric penalties (γ1=0.5 for adversarial contexts, γ2=0.3 for supportive contexts). Training uses only bandwagon bias but evaluates on multiple bias types, showing strong generalization to semantic biases while failing on structural biases.

## Key Results
- EIT improves adversarial-bias accuracy by 13.2% (70.1%→83.3%) on Qwen3-4B
- EIT generalizes to unseen semantic biases (authority, distraction) but not structural biases (position)
- EIT-trained 4B model outperforms untrained 33B model in robustness
- Qualitative analysis reveals EIT induces genuine epistemic independence through substantive reasoning rather than surface-level imitation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Rendering bias cues non-predictive of reward forces the model to abandon shortcut heuristics.
- **Mechanism:** By injecting bias so that it supports the correct answer 50% of the time and the incorrect answer 50% of the time (Balanced Conflict Strategy), the statistical correlation between the bias cue and the ground truth is removed. The model cannot maximize reward by simply learning "follow the bias" or "reject the bias"; it must engage in task-relevant reasoning to determine the correct answer.
- **Core assumption:** The model is capable of the underlying reasoning task when not distracted, and its primary failure mode is exploiting spurious correlations rather than a lack of domain knowledge.
- **Evidence anchors:**
  - [Abstract] "...bias cues must be made non-predictive of reward... through a balanced conflict strategy where bias signals are equally likely to support correct and incorrect answers."
  - [Section 3.1] "In this setting, the bias signal becomes statistically orthogonal to the truth: any policy that relies on the cue will fail to maximize expected return..."
  - [Corpus] Related work "The Silent Judge" identifies shortcut bias as a key failure mode, but does not validate this specific conflict-based intervention.

### Mechanism 2
- **Claim:** Asymmetric reward shaping prevents "performative independence" (mimicking refusal language) and incentivizes genuine verification.
- **Mechanism:** The reward design penalizes following bias when it is wrong ($-\gamma_1$), but offers zero marginal gain when bias happens to be right. This prevents the model from learning a "always agree" or "always disagree" policy. Unlike Supervised Fine-Tuning (SFT) which rewards matching surface-level text (e.g., "I will not follow the crowd"), this outcome-based RL requires the model to actually compute the correct answer to receive the accuracy reward ($R_{acc}$).
- **Core assumption:** The optimization process (GRPO) can successfully navigate the sparse reward landscape to find a policy that satisfies both accuracy and independence constraints simultaneously.
- **Evidence anchors:**
  - [Section 3.4] "Crucially, there is no bonus for being correct when bias is also correct... the optimal policy must ignore $b$ entirely."
  - [Section 5] "SFT models learn to say what independent reasoners say... without learning to do what they do... EIT models explicitly invoke relevant knowledge at high rates."
  - [Corpus] "J1: Incentivizing Thinking" supports the general efficacy of RL for improving judge reasoning, though using different reward structures.

### Mechanism 3
- **Claim:** Epistemic independence trained on one semantic bias generalizes to unseen semantic biases but fails to transfer to structural biases.
- **Mechanism:** The training forces the model to distinguish between "semantic content" (facts, logic) and "social influence cues" (consensus, authority). Because authority and bandwagon biases both function as social influence cues, the learned independence transfers. However, position bias relies on structural ordering rather than semantic content, which the specific training on bandwagon bias did not address.
- **Core assumption:** The underlying mechanism of "social influence" is shared across bandwagon and authority biases, allowing the learned feature rejection to generalize.
- **Evidence anchors:**
  - [Section 4.3] "Bias resistance transfers to semantic biases but not structural biases... Distraction Bias shows the strongest transfer... Position Bias shows minimal transfer."
  - [Abstract] "...models trained only on bandwagon bias generalize to unseen bias types such as authority and distraction..."

## Foundational Learning

- **Concept:** **Group Relative Policy Optimization (GRPO)**
  - **Why needed here:** This is the specific RL algorithm used to optimize the model. Unlike standard PPO, GRPO uses the group average reward as a baseline to reduce variance, which is critical when rewards are sparse or complex (combining accuracy + independence).
  - **Quick check question:** How does using the group mean $\bar{R}_G$ as a baseline differ from using a learned value function in standard PPO?

- **Concept:** **Spurious Correlations / Shortcut Learning**
  - **Why needed here:** The fundamental problem EIT solves is that LLMs learn "shortcuts" (e.g., "experts are usually right") during pretraining. Understanding that the model is an opportunistic optimizer explains why simple prompting fails—it doesn't change the optimization incentive.
  - **Quick check question:** Why does making a cue "non-predictive" (50/50 correlation) specifically force a model to unlearn a shortcut?

- **Concept:** **Performative vs. Substantive Reasoning**
  - **Why needed here:** The paper highlights that SFT teaches models to *sound* independent (performative) while EIT teaches them to *be* independent (substantive). This distinction is crucial for evaluating whether a model is actually robust or just trained to emit specific refusal phrases.
  - **Quick check question:** If a model says "I should not listen to peer pressure" but then chooses the wrong answer, is it exhibiting performative or substantive independence?

## Architecture Onboarding

- **Component map:** Data Constructor -> Bias Injection (Balanced Conflict) -> Policy Model ($\pi_\theta$) -> Reward Calculator (R = R_struct + R_acc + R_ind) -> GRPO Optimizer -> Updated $\pi_\theta$
- **Critical path:** The **Reward Calculator** is the most sensitive component. Specifically, the implementation of $R_{ind}$ (Equations 3 & 4) must correctly distinguish between "adversarial context" (bias $\neq$ truth) and "supportive context" (bias $=$ truth) to ensure the "zero marginal gain" logic holds.
- **Design tradeoffs:**
  - **Training on Bandwagon only:** Chosen to prove generalization (OOD), but implies the model remains vulnerable to structural biases (position) or unseen bias categories not related to social influence.
  - **GRPO vs. SFT:** GRPO requires significantly more compute (rollouts) but yields substantive reasoning; SFT is faster but results in shallow "performative" mimicry.
- **Failure signatures:**
  - **"Reverse Bias" Heuristic:** If the "Balanced Conflict" data is not truly 50/50, or if the penalty is too high, the model may learn to simply *oppose* the bias cue regardless of truth (always picking the option the bias argues against).
  - **Random Contrarianism:** If the accuracy reward ($R_{acc}$) is too weak compared to the independence penalty, the model might ignore truth entirely just to satisfy the independence constraint.
- **First 3 experiments:**
  1. **Data Balance Validation:** Before training, verify the injected dataset has exactly 50% samples where bias supports the correct answer and 50% where it supports the wrong answer.
  2. **Ablation of $R_{ind}$:** Train a model using *only* $R_{acc}$ (Accuracy) and $R_{struct}$ on the conflict data. Observe if the model learns to ignore bias or if the explicit penalty is required.
  3. **OOD Generalization Test:** Train strictly on Bandwagon bias, then evaluate on the Authority bias set. Check if robustness metrics (RR) improve compared to the baseline to verify transfer.

## Open Questions the Paper Calls Out

- **Question:** Can EIT be extended to more complex, open-ended generation tasks beyond pairwise choice formats?
  - **Basis in paper:** [explicit] Conclusion states: "Future work could extend EIT to more complex, open-ended generation tasks and explore its potential in mitigating subtler forms of implicit bias in other reasoning domains."
  - **Why unresolved:** Current experiments only evaluate on pairwise choice tasks using MMLU-Pro, where answers are discrete and easily verified. Open-ended tasks require different reward formulations.
  - **What evidence would resolve it:** Demonstrating EIT on tasks like long-form essay evaluation, code review, or free-form reasoning with appropriate reward designs showing comparable robustness gains.

- **Question:** Does EIT generalization transfer across languages and cultural contexts?
  - **Basis in paper:** [explicit] Impact Statement notes: "Our method was evaluated on English-language benchmarks with specific bias types. Broader deployment would require validation across languages, cultures, and bias categories."
  - **Why unresolved:** Bias expressions and social influence cues differ linguistically and culturally; the current conflict strategy uses English-specific bias templates.
  - **What evidence would resolve it:** Cross-lingual evaluation showing EIT-trained models resist culturally-adapted bias injections in non-English languages, or analysis of whether retraining with translated bias templates is necessary.

- **Question:** Can EIT principles address structural biases (e.g., position bias) that showed minimal transfer in experiments?
  - **Basis in paper:** [inferred] Results show position bias robustness slightly decreased (38.6%→36.4%) after training, while semantic biases transferred strongly. Authors note EIT "leaves structural vulnerabilities like position bias unaddressed."
  - **Why unresolved:** The balanced conflict strategy operationalizes bias as semantic content pointing to answers; structural biases may require different formulations where the signal is positional rather than content-based.
  - **What evidence would resolve it:** Modified EIT with position-balanced conflict data demonstrating improved position bias robustness, or theoretical analysis of why the current reward formulation fails on structural cues.

- **Question:** Does EIT-induced epistemic independence persist under distribution shift to genuinely novel reasoning domains?
  - **Basis in paper:** [inferred] Training used Math, Physics, Law, Chemistry; testing used Biology, CS, Economics, Health. While this tests OOD subjects, all remain within MMLU-Pro's academic multiple-choice paradigm.
  - **Why unresolved:** The "genuine epistemic independence" claim rests on generalization to unseen bias types within similar task structures—whether this extends to fundamentally different reasoning paradigms is untested.
  - **What evidence would resolve it:** Evaluation on reasoning benchmarks with different formats (e.g., mathematical proof verification, multi-hop reasoning chains) with adversarial bias injections, showing sustained robustness.

## Limitations

- The balanced conflict strategy successfully addresses semantic biases but shows clear failure on structural biases (position bias), indicating fundamental limitations in the current approach.
- The asymmetric reward design requires precise calibration of penalty weights relative to accuracy reward, with potential for unintended incentives if misconfigured.
- The mechanistic claim that models "genuinely reason" rather than performatively refuse relies on qualitative chain-of-thought analysis that may not scale to larger models or different bias types.

## Confidence

- **High confidence:** The experimental results showing 13.2% improvement in adversarial-bias accuracy and the basic transfer from bandwagon to authority bias are well-supported by the reported data.
- **Medium confidence:** The claim that EIT prevents "performative independence" is supported by qualitative analysis but would benefit from more systematic evaluation of reasoning traces across multiple bias types.
- **Low confidence:** The assertion that this approach would generalize to arbitrary novel bias types (beyond semantic social influence) is speculative and not directly tested.

## Next Checks

1. **Structural bias evaluation:** Systematically test EIT-trained models against a broader range of structural biases (e.g., formatting cues, positional presentation) to map the boundaries of generalization.

2. **Reward weight sensitivity:** Conduct an ablation study varying γ1 and γ2 independently to determine the precise conditions under which the model learns genuine independence versus random contrarianism.

3. **Scalability validation:** Train EIT on larger Qwen3 variants (e.g., 32B) and evaluate whether the reasoning-based independence generalizes or if larger models find new shortcut heuristics.