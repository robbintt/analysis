---
ver: rpa2
title: Investigation of D-Wave quantum annealing for training Restricted Boltzmann
  Machines and mitigating catastrophic forgetting
arxiv_id: '2508.15697'
source_url: https://arxiv.org/abs/2508.15697
tags:
- training
- d-wave
- sampling
- task
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates the potential of using D-Wave quantum
  annealing for training Restricted Boltzmann Machines (RBMs) and mitigating catastrophic
  forgetting (CF) during incremental learning. Three main goals were addressed: (1)
  evaluating the new Pegasus D-Wave architecture for RBM embedding and training, (2)
  exploring hybrid sampling combining classical and quantum methods, and (3) applying
  quantum annealing to CF mitigation via generative replay.'
---

# Investigation of D-Wave quantum annealing for training Restricted Boltzmann Machines and mitigating catastrophic forgetting

## Quick Facts
- arXiv ID: 2508.15697
- Source URL: https://arxiv.org/abs/2508.15697
- Authors: Abdelmoula El-Yazizi; Yaroslav Koshka
- Reference count: 40
- Primary result: Quantum annealing shows no improvement for RBM training but demonstrates promise for catastrophic forgetting mitigation through faster generative replay

## Executive Summary
This study investigates using D-Wave quantum annealing for training Restricted Boltzmann Machines and mitigating catastrophic forgetting during incremental learning. The research addresses three main goals: evaluating Pegasus D-Wave architecture for RBM embedding, exploring hybrid quantum-classical sampling, and applying quantum annealing to CF mitigation via generative replay. While hybrid sampling failed to improve RBM training due to sampling differences concentrating in low-probability regions that minimally impact training quality, D-Wave showed promise in CF mitigation by generating diverse class-specific patterns faster than classical MCMC methods. This work highlights quantum annealing's potential as a generative model for continual learning despite challenges in RBM training efficiency.

## Method Summary
The research uses the OptDigits dataset (handwritten digits) preprocessed to 8x8 pixels with 10 one-hot encoded label pixels, creating 74 visible units. The method involves three main components: (1) embedding RBMs onto Pegasus architecture using minor-embedding tools, (2) implementing hybrid sampling that combines 50% random training patterns with 50% local minima found by D-Wave solutions, and (3) applying generative replay for catastrophic forgetting mitigation. Training uses CD-1 with aggressive L2 regularization and dynamic scaling to fit hardware constraints. For CF mitigation, the approach clamps label qubits to desired class values and runs quantum annealing to generate class-specific patterns that are mixed with new task data during incremental learning.

## Key Results
- Hybrid quantum-classical sampling does not improve RBM training due to differences concentrating in low-probability regions that minimally impact contrastive divergence quality
- Pegasus architecture enables larger RBM embeddings but may degrade quality due to increased qubit chaining, showing 8.3% classification error vs 5% on Chimera
- D-Wave demonstrates promise in CF mitigation, effectively generating diverse class-specific patterns faster than classical MCMC while maintaining comparable performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid quantum-classical sampling does not improve RBM training because sampling differences concentrate in low-probability regions.
- Mechanism: D-Wave QA and MCMC significantly overlap in high-probability (low-energy) local valleys that dominate gradient estimation. Differences appear primarily in medium-to-low probability regions that contribute minimally to the sample quality used in contrastive divergence training.
- Core assumption: The quality of CD-k training depends more on high-probability states than on coverage of the full distribution tail.
- Evidence anchors:
  - [abstract] "differences between the QA-based and MCMC sampling, mainly found in the medium-to-low probability regions... are insufficient to benefit the training"
  - [Page 15] "the standard CD-k training algorithm predominantly samples states from the lower-energy part of the spectrum... The D-Wave seeds already significantly overlap with classical (MCMC) seeds in this important region"
  - [corpus] Neighbor paper "Comparison of D-Wave Quantum Annealing and MCMC..." confirms LV-centered analysis showing overlap in low-energy regions
- Break condition: If training requires better coverage of low-probability states (e.g., for rare class detection or adversarial robustness), this conclusion may not hold.

### Mechanism 2
- Claim: QA-based generative replay mitigates catastrophic forgetting comparably to classical MCMC with speed advantages.
- Mechanism: By clamping label qubits to desired class values and running quantum annealing, the QA generates class-specific patterns from the learned RBM distribution. These "memories" are mixed with new task data during incremental learning, preserving earlier knowledge through generative replay.
- Core assumption: The QA samples sufficiently diverse class-conditional patterns from medium-to-low probability regions—regions where QA shows more differentiation from MCMC.
- Evidence anchors:
  - [abstract] "D-Wave demonstrated promise in CF mitigation, effectively generating diverse class-specific patterns faster than classical MCMC"
  - [Page 18-19, Fig. 5] QA and MCMC mitigation curves nearly overlap, both reducing error from ~40% back to near 0%
  - [corpus] Weak direct evidence; neighbor papers focus on RBM training, not CF mitigation
- Break condition: If pattern diversity becomes critical for complex multi-class scenarios, mode collapse could reduce mitigation effectiveness.

### Mechanism 3
- Claim: Pegasus architecture enables larger RBM embeddings but may degrade embedding quality due to increased qubit chaining.
- Mechanism: Pegasus provides more qubits but connectivity scales slower than qubit count. Larger logical units require chaining more physical qubits, increasing chain-break probability and deviation from intended logical states.
- Core assumption: Classification error on clamped test patterns serves as a proxy for embedding quality.
- Evidence anchors:
  - [Page 9] Classification error was 8.3% with Pegasus vs. 5% with earlier Chimera hardware
  - [Page 21] "The reduced quality of the embedding may be further degrading the quality of the samples"
  - [corpus] Paper "Addressing the Minor-Embedding Problem in Quantum Annealing" explicitly discusses embedding-related performance disparities
- Break condition: Future hardware with improved connectivity-to-qubit ratios could reverse this tradeoff.

## Foundational Learning

- Concept: **Contrastive Divergence (CD-k) and its sampling requirements**
  - Why needed here: The paper evaluates whether QA sampling can replace or augment Gibbs sampling in CD training. Understanding why CD-1 works despite short chains clarifies why low-probability region differences don't help.
  - Quick check question: Why does CD-1 often suffice for RBM training despite not reaching equilibrium?

- Concept: **Local Valleys (LVs) and Basins of Attraction in energy landscapes**
  - Why needed here: The paper's LV-centered analysis explains sampling behavior. Deep but narrow LVs may be under-sampled if their basins are hard to reach.
  - Quick check question: How does LV basin width affect whether a sampler consistently visits states within that LV?

- Concept: **Generative Replay for Continual Learning**
  - Why needed here: The CF mitigation experiments use QA as a generative model to produce "memories" of past classes. Understanding replay vs. regularization approaches clarifies why pattern diversity matters.
  - Quick check question: What advantage does generative replay have over storing raw training examples?

## Architecture Onboarding

- Component map:
  RBM model (74 visible + variable hidden) -> Minor-embedding onto Pegasus lattice -> QUBO matrix with dynamic scaling -> Annealing execution (20 µs, 1000 reads) -> Post-processing (majority vote for chains) -> Classification/evaluation

- Critical path:
  1. Train RBM classically with aggressive L2 regularization to keep weights/biases small
  2. Construct QUBO matrix with scaled parameters
  3. Generate minor-embedding (once per architecture, but verify per training epoch if weights change significantly)
  4. Execute annealing (20 µs default, 1000 reads per call)
  5. Post-process: majority vote for chain breaks, classify/evaluate

- Design tradeoffs:
  - **More samples vs. chain quality**: 10,000 samples improve coverage but don't fix embedding issues
  - **Auto-scale ON vs. OFF**: ON helps fit hardware range but may push small weights below sensitivity; OFF preserves relative weights but risks out-of-range values
  - **Embedding optimization**: Manual optimization (infeasible per epoch) vs. automatic tools (potentially suboptimal)

- Failure signatures:
  - Classification error significantly higher than classical baseline (>5-8% gap)
  - High chain-break rate requiring frequent majority-vote correction
  - Low LV diversity (many solutions converging to same LV)
  - CF mitigation producing mode-collapsed patterns (low inter-class diversity)

- First 3 experiments:
  1. **Baseline embedding quality check**: Embed a classically-trained RBM, clamp test patterns, run classification. Compare error rate to classical RBM inference. Target: <10% error gap.
  2. **LV coverage comparison**: For a fixed trained RBM, sample 1000 states via D-Wave and 1000 via MCMC. Count distinct LVs found by each. Expect significant overlap in low-energy LVs, divergence in high-energy LVs.
  3. **Single-task CF mitigation pilot**: Train on 2 classes, then switch to 2 new classes with QA-generated "memories" of old classes. Measure classification error on old classes. Target: maintain near-0% error (vs. ~25-40% unmitigated).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the quality of embedding Restricted Boltzmann Machines (RBMs) into the Pegasus architecture be improved to match or exceed the performance seen in older Chimera architectures?
- Basis in paper: [explicit] The authors tentatively attribute the failure to improve RBM training to "less-than-perfect embedding," noting that classification error on Pegasus (8.3%) was inferior to previous results on Chimera (5%).
- Why unresolved: Standard optimization techniques used in prior work, such as weight decay and auto-scaling, failed to resolve the embedding degradation caused by the increased chaining required by the Pegasus lattice.
- What evidence would resolve it: The development of a novel embedding strategy for Pegasus that achieves classification error rates comparable to or better than the 5% benchmark established on Chimera hardware.

### Open Question 2
- Question: Does the quantum annealing speed advantage for generative replay persist as model sizes scale to practical dimensions?
- Basis in paper: [explicit] The authors note that while QA generates patterns faster than MCMC, this time advantage may only be "decisive" "if the D-Wave's hardware scales sufficiently to accommodate these larger model sizes."
- Why unresolved: The current study was limited to a relatively small RBM (74x74). It is unknown if the overhead of embedding larger, more complex models will negate the raw sampling speed of the quantum processing unit.
- What evidence would resolve it: Benchmarking the QA-based generative replay method on larger datasets (e.g., full-resolution images) where MCMC methods become computationally prohibitive, demonstrating a maintained or increased speedup.

### Open Question 3
- Question: Can future hybrid sampling algorithms effectively leverage the statistical differences in low-probability regions to improve RBM training?
- Basis in paper: [explicit] The study concludes that the hybrid approach failed because differences from QA lie in "medium-to-low probability regions," which are "less important for the quality of the sample." However, the authors suggest "future innovations in D-Wave-based sampling may prove more effective."
- Why unresolved: The specific hybrid strategy tested (mixing seeds) failed, but this does not rule out the possibility that low-probability samples found via tunneling could be useful if integrated differently into the learning phase.
- What evidence would resolve it: A new training protocol that successfully utilizes low-probability QA samples—perhaps to regularize spurious local valleys—resulting in improved log-likelihood compared to classical Contrastive Divergence.

## Limitations

- The study's conclusions about QA's ineffectiveness for RBM training rely heavily on the assumption that low-probability region differences don't impact CD-k training quality, lacking direct experimental validation
- Pattern diversity characterization for QA-generated "memories" in CF mitigation was limited, raising questions about potential mode collapse in more complex scenarios
- The research was constrained to a relatively small RBM (74x74), leaving open questions about whether speed advantages persist at practical model scales

## Confidence

- **High confidence**: Pegasus embedding enables larger RBM representations but may degrade quality due to increased chaining - directly measured through classification error comparisons (8.3% vs 5%)
- **Medium confidence**: Hybrid sampling doesn't improve RBM training because sampling differences concentrate in low-probability regions - supported by theoretical arguments and partial empirical evidence but lacks comprehensive validation
- **Medium confidence**: D-Wave effectively mitigates catastrophic forgetting via generative replay - demonstrated through error reduction from ~40% to near 0%, though pattern diversity wasn't extensively characterized

## Next Checks

1. **Direct LV basin analysis**: Measure and compare the basin widths and depths of distinct LVs found by QA vs MCMC to quantify the actual coverage differences in low-probability regions
2. **Training sensitivity ablation**: Systematically vary the diversity requirements in CD-k training (e.g., CD-10 vs CD-1) to test whether hybrid sampling becomes beneficial when longer chains require broader exploration
3. **Pattern diversity quantification**: Characterize the inter-class and intra-class diversity of QA-generated "memories" in CF mitigation using established metrics (e.g., coverage, mode collapse indicators) to ensure they're not simply mode-collapsed representations