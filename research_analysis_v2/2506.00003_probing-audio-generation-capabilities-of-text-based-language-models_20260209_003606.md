---
ver: rpa2
title: Probing Audio-Generation Capabilities of Text-Based Language Models
arxiv_id: '2506.00003'
source_url: https://arxiv.org/abs/2506.00003
tags:
- audio
- code
- llms
- musical
- generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This research investigates whether text-only LLMs can generate
  audio, using code as an intermediary. Three tiers of audio complexity were tested:
  musical notes, environmental sounds, and human speech.'
---

# Probing Audio-Generation Capabilities of Text-Based Language Models

## Quick Facts
- arXiv ID: 2506.00003
- Source URL: https://arxiv.org/abs/2506.00003
- Authors: Arjun Prasaath Anbazhagan; Parteek Kumar; Ujjwal Kaur; Aslihan Akalin; Kevin Zhu; Sean O'Brien
- Reference count: 12
- Key outcome: Text-only LLMs generate audio via code synthesis with 22.5% accuracy for environmental sounds and FAD<10 for 6/10 musical instruments, but fail completely on human speech.

## Executive Summary
This research investigates whether text-only large language models can generate audio without explicit audio training by leveraging code as an intermediary. Three tiers of audio complexity were tested: musical notes (using NSynth dataset), environmental sounds (using FSD50K), and human speech (using TensorFlow Speech Commands). The approach achieved moderate success on musical notes with 6 of 10 instruments showing high similarity (FAD < 10), 22.5% accuracy on environmental sounds, but complete failure on speech generation. GPT-4 outperformed Llama 3.1-70b, and text-only models approached performance of audio-trained GPT-4o.

## Method Summary
The study used text-based LLMs (GPT-4 and Llama 3.1-70b) to generate Python code that synthesizes audio waveforms, bypassing the need for direct audio training. For musical notes, prompts included detailed metadata from NSynth (pitch, velocity, instrument, production method, quality description). Environmental sounds used FSD50K with detailed and description-enhanced prompts. Human speech used TensorFlow Speech Commands. Generated code was executed to produce .WAV files, which were evaluated using FAD (Fréchet Audio Distance) for musical notes and CLAP classification for environmental sounds. Compilation success rates varied from 47% to 88% depending on the model and prompt complexity.

## Key Results
- Musical note generation achieved FAD < 10 (high similarity) for 6 of 10 instruments, with mallet instruments showing best performance (FAD 1.44)
- Environmental sound generation reached 22.5% correct classification by CLAP model across 5-way evaluation
- Human speech generation failed completely with no successful generations observed
- GPT-4 achieved higher compilation success (82-88%) than Llama 3.1-70b (47-49%)
- Detailed prompts with descriptions improved Llama 3.1-70b performance (0.72→0.77 CLAP confidence) but had negligible impact on GPT-4

## Why This Works (Mechanism)

### Mechanism 1: Code-as-Intermediary Translation
Text-only LLMs can generate audio by producing executable code that synthesizes waveforms, leveraging code patterns learned during pre-training where audio synthesis examples co-occurred with descriptive text. The LLM receives structured text prompts describing audio properties and generates Python code using audio libraries that execute to produce .WAV files.

### Mechanism 2: Latent Cross-Modal Association
Text-only LLMs retain implicit associations between textual descriptions and acoustic properties from training data co-occurrence. During pre-training, the model learns statistical relationships between descriptive language ("crunchy sound with many harmonics") and acoustic concepts, which guide parameter selection in generated code without explicit audio training.

### Mechanism 3: Complexity-Dependent Performance Degradation
LLM audio generation success rate inversely correlates with target complexity across a hierarchy (musical notes > environmental sounds > speech). Musical notes have well-defined parametric representations, while environmental sounds require modeling multiple overlapping acoustic events, and speech demands precise temporal-spectral control mimicking biological vocalization.

## Foundational Learning

- **Concept: Fréchet Audio Distance (FAD)**
  - Why needed here: Primary evaluation metric for musical notes; scores below 10 indicate "high similarity" while above 15 indicates "significantly distinct."
  - Quick check question: Given two audio files with FAD scores of 5 and 25, which would human evaluators categorize as "highly similar" to ground truth?

- **Concept: CLAP (Contrastive Language-Audio Pre-training)**
  - Why needed here: Evaluation metric for environmental sounds using shared embeddings between text and audio; determines if generated audio matches target class.
  - Quick check question: If CLAP evaluates generated audio against 5 classes (1 target + 4 random), what does 22.5% classification accuracy indicate about generation quality?

- **Concept: Audio Synthesis via Code Parameters**
  - Why needed here: Understanding how frequency, amplitude, envelope (ADSR), and harmonics translate to Python code is essential for debugging failed generations.
  - Quick check question: What code parameters would you need to synthesize a 440Hz sine wave versus a 440Hz square wave?

## Architecture Onboarding

- **Component map:** Input Layer: Structured text prompts → LLM Layer: GPT-4 / Llama 3.1-70b → Code Generation: Python scripts using scipy, numpy, midiutil → Execution Environment: Sandboxed Python runtime → Output: .WAV files → Evaluation: FAD (VGGish features) for notes; CLAP embeddings for environmental sounds

- **Critical path:** Prompt engineering with detailed metadata → code generation → compilation success (49%-88% depending on model) → audio evaluation

- **Design tradeoffs:**
  - Code vs. direct audio generation: Code provides interpretable intermediate representation but limits expressiveness to available library functions
  - Detailed prompts vs. minimal prompts: Detailed prompts + descriptions improved Llama 3.1-70b (0.72→0.77 CLAP confidence) but had negligible impact on GPT-4
  - Model selection: GPT-4 achieved higher compilation success (82-88%) vs. Llama 3.1-70b (47-49%) but both approached GPT-4o performance on musical notes

- **Failure signatures:**
  - Code execution errors: Deprecated or non-existent imports (e.g., `midutil` instead of `MIDIutil`, `pydsmid` instead of `pydsmidi`)
  - Low FAD scores for wind instruments: Bass (FAD ~200), vocal, reed underperformed percussion/keyboard
  - CLAP misclassification: 77.5% of environmental sounds classified incorrectly despite "distinguishable characteristics"

- **First 3 experiments:**
  1. Validate pipeline on single instrument: Generate 10 keyboard notes with full metadata, verify code compiles, check FAD distribution to confirm "high similarity" threshold (< 10)
  2. Model comparison baseline: Run identical prompts on GPT-4 and Llama 3.1-70b for mallet instrument (best performer, FAD 1.44), quantify compilation success rate and FAD gap
  3. Complexity breakpoint identification: Attempt environmental sound generation for 5 simple classes (alarm, bell, knock) using detailed prompt method, measure CLAP accuracy drop relative to musical notes

## Open Questions the Paper Calls Out

### Open Question 1
What specific techniques or prompting strategies can effectively bridge the performance gap between text-only LLMs and audio-trained models like GPT-4o for complex audio tasks? The abstract concludes that "Further research into techniques that can enhance the quality and diversity of LLM-generated audio can lead to an improvement in the performance."

### Open Question 2
Why does the code-intermediary approach succeed for musical notes and environmental sounds but fail completely for human speech generation? The paper reports zero successful generations for speech despite success in other categories, leaving the specific bottleneck undefined.

### Open Question 3
To what extent is the generated audio derived from the model's understanding of auditory physics versus simple memorization of code snippets? Section 2.3 discusses "Code Memorization," and Section 4.1.1 notes failures due to hallucinated Python modules (e.g., `midutil`), questioning if the model synthesizes new solutions.

## Limitations
- Complete failure to generate human speech, suggesting fundamental barriers in mapping text descriptions to vocal tract dynamics
- Environmental sound generation accuracy (22.5%) indicates the approach works best for sounds with clear parametric representations
- Evaluation relies heavily on automated metrics (FAD, CLAP) rather than human perceptual studies

## Confidence

**High Confidence (95%+):** The core finding that musical note generation via code synthesis works for certain instrument classes, with 6/10 instruments achieving FAD < 10. The comparative performance between GPT-4 and Llama 3.1-70b is robust, with clear differences in compilation success rates (82-88% vs 47-49%).

**Medium Confidence (70-95%):** The claim that text-only LLMs approach GPT-4o performance on musical notes. While FAD scores support this, the lack of detailed GPT-4o prompting methodology introduces uncertainty. The complexity hierarchy (notes > environmental > speech) appears valid but wasn't systematically tested across all three tiers.

**Low Confidence (30-70%):** The assertion that this approach demonstrates "latent understanding of the auditory world." The metrics show pattern matching through code rather than genuine audio comprehension. The failure modes for speech generation remain unexplained.

## Next Checks
1. **Human Perceptual Validation**: Conduct blinded human evaluation comparing generated notes with ground truth across all 10 instruments, measuring both similarity ratings and ability to identify instruments.
2. **Speech Generation Breakpoint Analysis**: Systematically test speech synthesis by providing code templates with vocal tract modeling parameters (formants, pitch contours) to determine if library availability or representation gaps cause failure.
3. **Temporal Coherence Extension**: Modify the pipeline to generate multi-note sequences with proper temporal spacing and evaluate continuity metrics beyond individual note FAD scores.