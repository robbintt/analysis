---
ver: rpa2
title: In-context Language Learning for Endangered Languages in Speech Recognition
arxiv_id: '2505.20445'
source_url: https://arxiv.org/abs/2505.20445
tags:
- language
- languages
- selection
- samples
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that large language models (LLMs) can learn
  new languages for speech recognition through in-context learning (ICL) using only
  a few hundred sample sentences, without requiring model fine-tuning. The authors
  propose an in-context language learning (ICLL) approach that provides linguistic
  knowledge of target languages to LLMs through carefully selected examples.
---

# In-context Language Learning for Endangered Languages in Speech Recognition

## Quick Facts
- arXiv ID: 2505.20445
- Source URL: https://arxiv.org/abs/2505.20445
- Reference count: 0
- Primary result: LLMs can learn new languages for speech recognition through in-context learning using only a few hundred sample sentences, without requiring model fine-tuning

## Executive Summary
This paper introduces In-context Language Learning (ICLL) for endangered languages in speech recognition, demonstrating that large language models can learn new languages through in-context examples without parameter updates. The authors show that probability-based hypothesis selection significantly outperforms traditional instruction-based methods, achieving ASR performance comparable to or better than dedicated language models specifically trained for these languages. Their approach uses example-specific retrieval with ASR hypotheses as search keys, even with high error rates, and achieves improvements across four endangered languages (Khinalug, Kichwa, Mboshi, Japhug) in both language modeling and automatic speech recognition.

## Method Summary
The ICLL approach provides linguistic knowledge of target languages to LLMs through carefully selected examples for hypothesis re-ranking. The method generates 10-best ASR hypotheses using Wav2Vec2 MMS-300, retrieves similar training samples via SONAR embeddings using ASR hypotheses as search keys, and re-ranks hypotheses using Llama-3-8B probability combined with acoustic scores. The probability-based approach leverages the model's cross-entropy computation given retrieved context, avoiding the instruction-following limitations observed with traditional methods. Example-specific retrieval outperforms corpus-level approaches by aligning context with locally relevant linguistic patterns.

## Key Results
- Probability-based hypothesis selection significantly outperforms instruction-based methods
- Example-specific retrieval outperforms corpus-level sample selection by 15-30% perplexity reduction
- ASR hypotheses serve as effective search keys even with high word error rates
- Performance comparable to or better than dedicated language models specifically trained for these languages

## Why This Works (Mechanism)

### Mechanism 1: Probability-Based Hypothesis Selection
The LLM assigns likelihood scores to each hypothesis based on in-context language patterns. Unlike instruction-based methods, probability-based selection leverages the model's core language modeling objective directly, bypassing the need for instruction-following capability in the target language. The mechanism assumes the LLM's probability distribution reflects meaningful linguistic patterns even for languages not seen during pre-training.

### Mechanism 2: Example-Specific Retrieval for In-Context Adaptation
Selecting different ICL samples per test instance outperforms using a fixed corpus-level sample set. Instance-specific retrieval aligns the context window with linguistic patterns relevant to the specific utterance, allowing the LLM to condition on locally similar examples rather than globally "representative" ones that may not match the test case.

### Mechanism 3: Robust Retrieval Under ASR Noise
ASR hypotheses can serve as effective search keys for sample retrieval even when the initial ASR has high word error rates. Retrieval similarity appears robust to local word errors because embeddings capture broader semantic/structural patterns rather than exact lexical matches.

## Foundational Learning

- **In-Context Learning (ICL)**: The entire method relies on LLMs learning from prompt demonstrations without parameter updates. Why needed: Enables language learning without fine-tuning. Quick check: Can you explain why scaling up examples in the prompt can improve performance without any gradient updates?

- **ASR n-best Hypothesis Re-ranking**: The LLM's role is to select among acoustic model candidates, not generate transcriptions directly. Why needed: Allows combining acoustic and language model scores. Quick check: Given an n-best list with acoustic scores, how would you combine them with LM log-probabilities?

- **Embedding-Based Retrieval (k-NN Search)**: Sample selection uses cosine similarity over SONAR embeddings to find relevant training examples. Why needed: Enables example-specific retrieval for in-context adaptation. Quick check: How does the choice of embedding model affect retrieval quality for out-of-distribution languages?

## Architecture Onboarding

- **Component map**: Wav2Vec2 MMS-300 -> SONAR embeddings -> Retrieval module -> Llama-3-8B -> Scoring combiner
- **Critical path**: Pre-compute embeddings for all training samples (offline) → For each test audio: ASR → hypotheses → embed query → retrieve samples → LLM scoring → combine scores → select best hypothesis
- **Design tradeoffs**: More ICL samples → lower perplexity but linear memory growth (OOM at ~250-300 samples with 48GB GPU); Example-specific vs. corpus-level retrieval → ~15-30% perplexity reduction but requires per-instance inference
- **Failure signatures**: Perplexity plateau or increase → check vocabulary coverage; Instruction-based performance degrades with more samples → expected; Audio retrieval underperforms text → avoid audio embeddings for sample selection
- **First 3 experiments**: 1) Baseline calibration: Run acoustic-only selection and n-gram LM rescoring to establish WER floor/ceiling for each language; 2) Sample scaling curve: Test ICLL with [0, 10, 50, 100, 150] samples using random selection to identify the compute-performance sweet spot; 3) Retrieval ablation: Compare random vs. corpus-level vs. example-specific (hyp) retrieval at the optimal sample count from experiment 2

## Open Questions the Paper Calls Out

- **Generalizability beyond tested languages**: Can ICLL effectiveness be generalized to a broader range of endangered languages beyond the four specific languages tested? The study's restriction to Khinalug, Kichwa, Mboshi, and Japhug limits broader applicability claims.

- **Performance across different LLMs**: How does the ICLL approach perform when applied to large language models other than Llama v3? The experiments exclusively utilized Llama-3-8B, leaving transferability to other model architectures unverified.

- **Sample count scaling limits**: To what extent does increasing the number of in-context samples further improve ASR performance when computational constraints are removed? The current study was limited by a single 48GB GPU, preventing identification of a performance ceiling.

## Limitations

- Sample selection protocol underspecified, with optimal sample count varying by language family and orthographic complexity
- Generalizability to non-endangered low-resource languages remains unproven
- Computational trade-offs create fundamental constraints between performance and scalability

## Confidence

**High Confidence**: Probability-based hypothesis selection outperforms instruction-based methods; Example-specific retrieval provides measurable improvements; ASR hypotheses serve as effective search keys; Combined acoustic + LM scoring consistently improves ASR performance

**Medium Confidence**: ICLL performance comparable to or better than dedicated language models; The method preserves LLM's original capabilities while learning new languages; Text embeddings outperform audio embeddings for sample retrieval

**Low Confidence**: Mechanism by which probability-based selection enables language learning; Generalizability to non-endangered low-resource languages; Optimal sample selection strategies across diverse language families

## Next Checks

1. **Vocabulary Coverage Analysis**: Systematically test ICLL performance across languages with varying degrees of vocabulary overlap with Llama-3's pre-training data to quantify the impact of tokenization quality on perplexity and WER.

2. **Sample Count Scaling Study**: Conduct controlled experiments varying sample count from 10-300 per language to identify optimal trade-offs between computational cost and performance, particularly for languages with different morphological complexity.

3. **Cross-Linguistic Generalization Test**: Apply the ICLL method to a diverse set of low-resource languages from different language families (e.g., Austronesian, Dravidian, Uralic) to validate whether observed improvements generalize beyond the endangered language context.