---
ver: rpa2
title: Do LVLMs Know What They Know? A Systematic Study of Knowledge Boundary Perception
  in LVLMs
arxiv_id: '2508.19111'
source_url: https://arxiv.org/abs/2508.19111
tags:
- confidence
- lvlms
- answer
- methods
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper systematically investigates the knowledge boundary\
  \ perception of large vision-language models (LVLMs), addressing the problem of\
  \ hallucination in visual question answering. The authors evaluate three types of\
  \ confidence signals\u2014probabilistic confidence, answer consistency-based confidence,\
  \ and verbalized confidence\u2014across three LVLMs on three VQA datasets."
---

# Do LVLMs Know What They Know? A Systematic Study of Knowledge Boundary Perception in LVLMs

## Quick Facts
- arXiv ID: 2508.19111
- Source URL: https://arxiv.org/abs/2508.19111
- Reference count: 29
- Primary result: Answer consistency-based confidence achieves highest alignment between confidence and performance in LVLMs, while verbalized confidence tends to be overconfident

## Executive Summary
This paper investigates whether large vision-language models (LVLMs) can accurately perceive their knowledge boundaries when answering visual questions. The authors evaluate three types of confidence signals—probabilistic confidence, answer consistency-based confidence, and verbalized confidence—across three LVLMs on three VQA datasets. They find that answer consistency-based confidence achieves the highest alignment between confidence and performance, while verbalized confidence tends to be overconfident. The study also adapts several confidence calibration methods from LLMs and proposes three new methods tailored for LVLMs. Additionally, comparing LVLMs with their LLM counterparts reveals that LVLMs have lower VQA accuracy but higher confidence-accuracy alignment due to reduced overconfidence, attributed to their awareness of multimodal integration limitations.

## Method Summary
The paper systematically evaluates knowledge boundary perception in LVLMs by testing three confidence signal types across three model architectures (Qwen2.5-VL-7B, DeepSeek-VL2-16B, LLaVA-v1.5-7B) on three VQA datasets (Visual7W, Dyn-VQA, MMMU Pro). Confidence signals include probabilistic confidence (token-level probability aggregation), answer consistency-based confidence (semantic equivalence across multiple samples), and verbalized confidence (models explicitly stating confidence). The study also adapts five LLM confidence calibration methods (CoT, Punish, Explain, Challenge prompts) and proposes three new LVLM-specific methods (Img-CoT, Prob-Thr, Cross-Model). Primary metric is alignment between confidence and correctness, calculated as (TP+TN)/Total.

## Key Results
- Answer consistency-based confidence achieves highest alignment (70.2% on MMMU Pro) among all tested methods
- LVLMs show lower VQA accuracy but higher confidence-accuracy alignment than LLM counterparts due to reduced overconfidence
- Reasoning elicitation methods (CoT, Explain) improve both accuracy and calibration in LVLMs
- Smaller LVLMs (<32B) fail to follow instruction-based calibration prompts effectively

## Why This Works (Mechanism)

### Mechanism 1: Answer Consistency Captures Semantic Certainty
If a model genuinely knows an answer, multiple sampled responses will converge semantically; uncertainty produces divergent outputs. Consistency-based confidence samples N responses (default: 10) at temperature=1.0, then measures semantic equivalence between the greedy reference answer and each sample using a smaller model (Qwen2.5-0.5B). Higher consistency scores indicate the model's internal representation is stable, suggesting genuine knowledge rather than hallucination. Semantic stability across stochastic sampling reflects epistemic certainty, not just lexical repetition patterns.

### Mechanism 2: Cross-Modal Integration Constraint Induces Uncertainty Awareness
LVLMs exhibit better confidence-accuracy alignment than LLMs because multimodal integration difficulty reduces overconfidence. Processing visual and textual modalities simultaneously taxes model capacity, lowering accuracy. However, this difficulty also produces higher uncertainty rates (lower confidence), which paradoxically improves alignment by reducing false certainty. The paper hypothesizes LVLMs develop implicit awareness of their cross-modal limitations. Reduced confidence from processing difficulty reflects genuine epistemic uncertainty, not just miscalibration.

### Mechanism 3: Reasoning Elicitation Activates Self-Calibration
Prompting for intermediate reasoning (CoT, Explain, Img-CoT) improves both answer accuracy and confidence calibration. Reasoning prompts distribute cognitive load: instead of directly outputting answers, models generate justifications first. This creates more opportunities for self-correction and uncertainty detection. Img-CoT additionally converts visual information to textual descriptions, making implicit visual processing explicit for self-evaluation. Models can detect inconsistencies in their own reasoning when forced to articulate it step-by-step.

## Foundational Learning

- **Concept: Confidence Calibration**
  - Why needed: The paper measures "alignment" between model confidence and actual correctness; understanding calibration metrics (Expected Calibration Error, Brier score) is prerequisite.
  - Quick check question: If a model claims 80% confidence on 100 answers, how many should be correct for the model to be well-calibrated?

- **Concept: Multimodal Fusion Architectures**
  - Why needed: LVLMs combine visual encoders (CLIP/ViT) with LLM backbones via projection layers; understanding where fusion occurs explains why cross-modal integration creates bottlenecks.
  - Quick check question: In LLaVA-style architectures, does the visual encoder or the LLM backbone primarily determine instruction-following ability?

- **Concept: Sampling-Based Uncertainty Estimation**
  - Why needed: Answer consistency methods require understanding temperature sampling, semantic equivalence evaluation, and trade-offs between computational cost and reliability.
  - Quick check question: Why does temperature=1.0 produce more diverse outputs than temperature=0, and why is diversity useful for uncertainty estimation?

## Architecture Onboarding

- **Component map:** Visual Input → Visual Encoder → Projection Layer → LLM Backbone → Text Output
- **Critical path:** Visual encoder processes image → produces visual tokens → Projection aligns visual tokens with LLM embedding space → LLM backbone generates response conditioned on visual + text tokens → Confidence estimation (parallel paths for probabilistic/consistency/verbalized)
- **Design tradeoffs:** Answer Consistency has highest alignment but high cost (10× samples); Probabilistic has medium alignment but low cost; Verbalized has lowest alignment but good generalization and black-box compatibility
- **Failure signatures:** Overconfidence on out-of-domain queries (verbalized confidence); Consistent but wrong answers (Qwen2.5-Vl with Random sampling); Instruction-following failures in smaller models (<32B) — ignore Punish/Explain prompts; Double-step confidence worse than single-step (self-generated answers reinforce overconfidence)
- **First 3 experiments:** Baseline alignment measurement: Run Vanilla, PPL-Thr, and Random sampling on Dyn-VQA subset (100 samples); Scale sensitivity test: Compare LLaVA-1.5-7B vs. LLaVA-1.5-13B on Explain prompt; Cross-modal awareness validation: Run same LVLM on text-only QA vs. image+text VQA (using Dyn-VQA parallel queries)

## Open Questions the Paper Calls Out

### Open Question 1
How do internal mechanistic differences in LVLMs drive knowledge boundary perception compared to LLMs? The authors state they "did not examine internal model states, leaving internal mechanistic differences in knowledge boundary perception underexplored." This study relied on output-level signals rather than analyzing hidden states or architectural dynamics. Probing internal activations or attention maps during multimodal integration could identify distinct uncertainty representations.

### Open Question 2
Can extending analysis to continuous confidence scales yield finer-grained insights into LVLM capability awareness? The authors note that focusing "on binary confidence measures" is a limitation and that "extending this to continuous confidence scales could yield finer-grained insights." Binary judgments (certain/uncertain) lack the nuance required to distinguish between varying degrees of uncertainty or partial knowledge. Evaluating models using continuous probability outputs (e.g., 0.0 to 1.0) against ground-truth correctness distributions could provide better insights.

### Open Question 3
How can confidence calibration methods be improved to generalize across different LVLM architectures without relying on in-domain threshold tuning? The authors observe that probabilistic and consistency methods rely "on in-domain data for binarization" and existing consistency-based methods "do not generalize well to LVLMs." High-performing methods like PPL-Thr require dataset-specific thresholds, limiting their robustness in dynamic, real-world applications. Development of tuning-free or self-adaptive calibration techniques could maintain high alignment across diverse, unseen datasets.

## Limitations

- Findings are based on three specific datasets and three model architectures, limiting generalizability
- Semantic equivalence evaluation using Qwen2.5-0.5B introduces potential circularity and bias
- Cross-modal integration awareness hypothesis lacks direct evidence and might stem from other factors
- Paper doesn't address temporal stability of confidence signals or performance under domain shift conditions

## Confidence

**High Confidence:**
- Answer consistency-based confidence shows higher alignment than probabilistic or verbalized methods
- LVLMs exhibit better confidence-accuracy alignment than their LLM counterparts due to reduced overconfidence
- Reasoning elicitation (CoT, Explain) improves both accuracy and calibration in LVLMs
- Smaller LVLMs (<32B) fail to follow instruction-based calibration prompts effectively

**Medium Confidence:**
- The proposed Img-CoT and Prob-Thr methods provide meaningful improvements over baseline calibration
- LVLMs' awareness of multimodal integration limitations explains their reduced overconfidence

**Low Confidence:**
- The specific mechanism of cross-modal integration constraint directly induces uncertainty awareness in LVLMs

## Next Checks

1. **Dataset Generalization Test:** Evaluate the same confidence methods on at least two additional VQA datasets (e.g., GQA, VQA-v2) to verify if answer consistency consistently outperforms other methods across diverse visual reasoning tasks.

2. **Ablation on Semantic Equivalence:** Replace Qwen2.5-0.5B with an alternative semantic similarity model (e.g., BERTScore) to test whether the observed benefits of answer consistency are robust to the choice of semantic evaluation method.

3. **Cross-Modal Integration Experiment:** Design a controlled experiment where the same visual reasoning task is presented in text-only format and multimodal format to the same LVLM, measuring whether the model's confidence calibration genuinely differs based on modality rather than task complexity.