---
ver: rpa2
title: Lossless Vocabulary Reduction for Auto-Regressive Language Models
arxiv_id: '2510.08102'
source_url: https://arxiv.org/abs/2510.08102
tags:
- language
- vocabulary
- vsub
- tokens
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method for reducing the vocabulary size
  of autoregressive language models without loss of accuracy. The core idea is to
  define a "lossless vocabulary reduction" framework that allows a model trained with
  a large vocabulary to be converted into an equivalent model using an arbitrarily
  smaller vocabulary.
---

# Lossless Vocabulary Reduction for Auto-Regressive Language Models

## Quick Facts
- arXiv ID: 2510.08102
- Source URL: https://arxiv.org/abs/2510.08102
- Authors: Daiki Chijiwa; Taku Hasegawa; Kyosuke Nishida; Shin'ya Yamaguchi; Tomoya Ohba; Tamao Sakao; Susumu Takeuchi
- Reference count: 39
- Primary result: Method to reduce autoregressive language model vocabulary size without accuracy loss by preserving probabilistic text generation behavior

## Executive Summary
This paper introduces a method for reducing the vocabulary size of autoregressive language models without loss of accuracy. The core idea is to define a "lossless vocabulary reduction" framework that allows a model trained with a large vocabulary to be converted into an equivalent model using an arbitrarily smaller vocabulary. This is achieved by introducing the concept of "nested tokenization," which enables re-tokenizing the original tokens into the target sub-vocabulary while preserving the probabilistic text generation behavior. An efficient algorithm is derived to compute the next-token distribution for the reduced vocabulary by recursively aggregating probabilities from the original model. The method is evaluated on language modeling tasks, showing that reduced-vocabulary models maintain accuracy comparable to the original models.

## Method Summary
The method introduces nested tokenization to re-tokenize original tokens into a sub-vocabulary while preserving probabilistic text generation behavior. It maintains relative covers - sets of original token sequences whose nested tokenization starts with a given sub-token prefix - to efficiently compute next-token distributions without exhaustive enumeration. The algorithm recursively aggregates probabilities from the original model to compute the distribution over the reduced vocabulary, using a top-K approximation (K=300) for efficiency. For model ensemble, the method constructs a maximal common vocabulary from BPE tokenizers and applies vocabulary reduction to each model before combining them via product of experts.

## Key Results
- Reduced-vocabulary models maintain accuracy comparable to original models across different sub-vocabulary sizes
- Models with different vocabularies can be efficiently ensembled using their maximal common vocabulary
- Ensemble via maximal common vocabulary achieves similar accuracy to byte-level ensemble but with faster inference
- Top-K approximation (K=300) introduces negligible error while providing significant computational speedup

## Why This Works (Mechanism)

### Mechanism 1: Nested Tokenization Preserves Probabilistic Generation
The method introduces nested tokenization T_{V→Vsub}, which re-tokenizes tokens from vocabulary V into a sub-vocabulary Vsub. Since each token represents a byte sequence (UTF-8), any token can be decomposed into smaller tokens that represent the same bytes. The algorithm computes p_{V→Vsub}(y_{k+1}|y_{1:k}) by marginalizing over all original token sequences that map to the current sub-token prefix, using relative covers C_{V,Vsub}(y_{1:k}) to track which original token prefixes are compatible. The validity condition ensures the original model assigns zero probability to invalid token sequences, making the relative cover tractable.

### Mechanism 2: Recursive Probability Aggregation via Relative Covers
The algorithm maintains C_{V,Vsub}(y_{1:k}), the set of original token sequences that are valid and whose nested tokenization starts with y_{1:k}. For each new sub-token y_{k+1}, the algorithm extends this cover by considering sequences already in the previous cover that can be extended, and new sequences formed by appending tokens whose nested tokenization starts with y_{k+1}. The probability p_{V→Vsub}(y_{1:k+1}*) is computed by summing p_V(x_{1:t}*) over all x_{1:t} in the extended cover. This recursive decomposition enables efficient computation without computing intractable sentence-level probabilities.

### Mechanism 3: Top-K Approximation for Computational Efficiency
Instead of iterating over all |V| tokens when extending relative covers, the algorithm only considers the top-K tokens by probability. This reduces the second nested loop from |Vsub| × |V| to K iterations. The approximation is justified because the softmax distribution over vocabulary typically has a heavy tail - most probability mass concentrates on a small subset of tokens. The paper uses K=300 and empirically validates that this introduces negligible error across different sub-vocabulary sizes.

## Foundational Learning

- Concept: **Autoregressive Language Modeling and Next-Token Distributions**
  - Why needed here: The entire method operates on next-token distributions p_V(x_t|x_{1:t-1}), not on sentence-level probabilities. Understanding that autoregressive LMs generate text by iteratively sampling from conditional distributions is prerequisite to understanding how vocabulary reduction preserves generation behavior.
  - Quick check question: Given a vocabulary {a, b, c} and the sequence "ab", can you compute p(next_token | "ab") if given the raw logits [1.0, 2.0, 0.5]?

- Concept: **Deterministic Tokenization and Validity**
  - Why needed here: The validity condition (p_V = 0 for invalid token sequences) is the key theoretical assumption that makes the relative cover tractable. Understanding that modern tokenizers like BPE are deterministic functions from text to token sequences - and that most arbitrary token sequences are invalid - is essential.
  - Quick check question: If a BPE tokenizer merges "th" and "e" into "the", is the token sequence ["the", "e"] valid for the text "thee"? Why or why not?

- Concept: **Conditional Probability and Marginalization**
  - Why needed here: The core computation involves p_{V→Vsub}(y_{k+1}|y_{1:k}) = p_{V→Vsub}(y_{1:k+1}*) / Σ_y p_{V→Vsub}(y_{1:k}y*), which requires understanding how to compute conditional probabilities from joint/marginal distributions and how to marginalize over hidden variables (original token sequences).
  - Quick check question: If P(A,B) = 0.3, P(A,¬B) = 0.2, and P(¬A,B) = 0.4, what is P(B|A)?

## Architecture Onboarding

- Component map: Input: Previously sampled sub-tokens y_{1:k} -> Re-tokenization Module: Convert y_{1:k} to original tokens x_{1:t} = [[y_{1:k}]_A]_V -> Original LM Forward Pass: Compute p_V(x_{1:t} x*) for all x ∈ V (single forward pass, cached in P) -> Relative Cover Manager: Maintain and extend C_{V,Vsub}(y_{1:k}) using cached probabilities -> Probability Aggregator: Sum p_V(x_{1:t}*) over all x_{1:t} in C_{V,Vsub}(y_{1:k+1}) for each y_{k+1} -> Normalization: Compute p_{V→Vsub}(y_{k+1}|y_{1:k}) by normalizing aggregated sums -> Output: Next-token distribution over Vsub

- Critical path: Cache management is critical - the algorithm stores all computed p_V(x_{1:t}*) values in global cache P to avoid redundant forward passes. Relative cover maintenance must be efficiently updated each iteration using the decomposition in Theorem 3.4. The single forward pass per iteration is the only LM access point; everything else is bookkeeping.

- Design tradeoffs:
  - K parameter (top-K pruning): Lower K reduces computation but risks approximation error. Paper uses K=300. Assumption: This works for typical low-entropy LMs; not validated for high-entropy distributions.
  - Cache size vs. memory: Storing all intermediate probabilities enables reuse but may require significant memory for long sequences.
  - Sub-vocabulary choice: The paper shows MCV (maximal common vocabulary) for ensemble, but the framework supports arbitrary sub-vocabularies. Smaller Vsub = more reduction but more aggregation overhead.
  - Algorithm 1 vs. Algorithm 2: Algorithm 1 is exact but O(|Vsub| × |V|). Algorithm 2 is O(|C| + K) but approximate. The paper claims approximation is negligible but doesn't provide theoretical bounds.

- Failure signatures:
  - Exponential cover growth: If validity condition is violated, C_{V,Vsub}(y_{1:k}) may grow exponentially with k. Signature: Memory usage spikes, inference slows dramatically.
  - Approximation error accumulation: If top-K pruning is too aggressive or model has high-entropy outputs, generated text diverges from original model. Signature: Greedy decoding produces different outputs between original and reduced models (noted in Section B for 1-byte case).
  - Tokenizer mismatch: If sub-vocabulary tokenizer T_{Vsub} doesn't satisfy the compatibility assumptions (e.g., produces different segmentations for the same text), the lossless guarantee may not hold. Signature: Different byte-level outputs between models.

- First 3 experiments:
  1. Validate single-model vocabulary reduction: Take a pretrained LM (e.g., Llama-3.2-3B), reduce vocabulary to V_{≤8} (tokens ≤ 8 bytes), run greedy decoding on GSM8K questions. Compare accuracy to original model. Expect near-identical results per Table 1. Monitor for cases where greedy decoding diverges (as discussed in Section B).
  2. Profile computational overhead: Measure wall-clock time and memory for Algorithm 2 with varying K (50, 100, 300, 500) on a fixed sequence length. Identify the bottleneck - should be the original LM forward pass, not cover maintenance. If cover management dominates, check for validity condition violations.
  3. Ensemble two models via MCV: Take Qwen2.5-3B and Falcon3-7B (as in the paper), compute their maximal common vocabulary V_∩, apply LVR to both, and ensemble via product-of-experts. Compare to byte-level ensemble baseline and naive restriction. Expect comparable accuracy with faster inference (Table 3). Verify that naive baseline catastrophically fails per Table 2.

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of top-K hyperparameter in the efficient LVR algorithm affect the trade-off between computational efficiency and theoretical guarantees of losslessness? The efficient algorithm uses top-K filtering (K=300) for approximation, but the paper does not systematically study how different K values impact accuracy or formally characterize when the approximation remains lossless. Only one K value is used without ablation study or theoretical analysis of approximation error.

### Open Question 2
To what extent does the discrepancy between greedy decoding outputs of original and vocabulary-reduced models affect practical applications? Section B demonstrates that even when token distributions are equivalent at the byte level, "the texts obtained by greedy decoding may be different from each other in general." The phenomenon is observed but its practical implications are not thoroughly investigated beyond showing final answers remain consistent in some cases.

### Open Question 3
How does lossless vocabulary reduction perform across languages with different tokenization patterns and character encodings? All experiments are conducted on English-language benchmarks, and the paper assumes UTF-8 byte-level encoding throughout without testing other languages. The theoretical framework should generalize, but tokenization patterns vary significantly across languages, potentially affecting the practical efficiency and accuracy of the reduction.

## Limitations

- The theoretical foundation relies critically on the validity condition (p_V = 0 for invalid token sequences) without formal proof or characterization of when this might be violated
- The top-K approximation introduces unquantified error that could accumulate over long sequences, particularly for high-entropy distributions
- All experiments are conducted on English-language benchmarks without testing multilingual performance or different character encodings

## Confidence

- **High Confidence**: The lossless property for valid token sequences (Theorem 3.1) - the mathematical derivation is sound given the validity assumption
- **Medium Confidence**: The efficiency gains from top-K approximation - empirically validated but lacks theoretical bounds on approximation error
- **Medium Confidence**: The practical applicability to ensemble models - demonstrated on specific model pairs but not systematically explored across different vocabulary sizes or model architectures

## Next Checks

1. **Validity Condition Stress Test**: Systematically generate and test invalid token sequences across multiple LMs and tokenizers to characterize the frequency and impact of validity violations. Measure relative cover growth rates on these cases to identify practical limits of the algorithm.

2. **Approximation Error Analysis**: Quantify the relationship between next-token entropy and top-K approximation error. Test with synthetic LMs having controlled entropy levels to establish error bounds and identify thresholds where K=300 becomes insufficient.

3. **Cross-Tokenizer Compatibility**: Evaluate LVR performance when applying models to text tokenized by different tokenizer variants (e.g., different BPE merge orders or vocabularies). Measure accuracy degradation and identify scenarios where nested tokenization assumptions break down.