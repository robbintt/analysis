---
ver: rpa2
title: 'AudioCodecBench: A Comprehensive Benchmark for Audio Codec Evaluation'
arxiv_id: '2509.02349'
source_url: https://arxiv.org/abs/2509.02349
tags:
- audio
- speech
- acoustic
- codecs
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AudioCodecBench, a comprehensive evaluation
  framework for audio codecs that addresses the lack of unified benchmarks for comparing
  different audio tokenization methods. The authors propose clear definitions for
  semantic tokens (text-describable) and acoustic tokens (text-indescribable), along
  with three hybrid types (fused and decoupled features).
---

# AudioCodecBench: A Comprehensive Benchmark for Audio Codec Evaluation

## Quick Facts
- **arXiv ID:** 2509.02349
- **Source URL:** https://arxiv.org/abs/2509.02349
- **Reference count:** 18
- **Key outcome:** Introduces AudioCodecBench, a framework evaluating audio codecs across reconstruction fidelity, LM perplexity, ID stability, and 11 downstream probe tasks, revealing trade-offs between semantic and acoustic tokenization.

## Executive Summary
This paper presents AudioCodecBench, a comprehensive evaluation framework addressing the lack of unified benchmarks for audio codec comparison. The authors define semantic tokens as text-describable features and acoustic tokens as text-indescribable features, with three hybrid types combining these characteristics. The framework evaluates codecs across four dimensions: reconstruction fidelity (PESQ, STOI, WER, CER), codebook index stability under noise, LM perplexity, and downstream task performance on 11 probe tasks spanning speech, music, and sound domains. Experiments with eight codecs and two SSL models demonstrate that semantic tokens are easier for LMs to model (lower perplexity) and perform better on semantic-driven tasks like ASR and music tagging, while acoustic tokens excel in reconstruction fidelity.

## Method Summary
The benchmark evaluates audio codecs through four key dimensions: reconstruction fidelity using standard metrics (PESQ, STOI, WER, CER, speaker similarity), codebook index stability under perturbation (10-round reconstruction and 2ms time shifts), language modeling perplexity using a 100M parameter decoder-only transformer (Qwen2 architecture) trained from scratch, and downstream probe task performance on 17 datasets across 11 tasks. The framework uses LibriTTS and GTZAN for reconstruction, Emilia-EN and MTG-Jamendo for perplexity, and various datasets for probe tasks. Codebook size normalization ensures fair PPL comparisons across different codec configurations.

## Key Results
- Semantic tokens achieve lower perplexity scores and better performance on semantic-driven tasks (ASR, music tagging) compared to acoustic tokens
- Acoustic tokens demonstrate superior reconstruction fidelity (higher PESQ/STOI scores) but higher perplexity
- Strong correlation exists between LM perplexity and downstream task performance, with semantic-rich tokens improving LM modeling while acoustic-focused tokens enhance reconstruction quality
- Codebook index stability serves as a proxy for semantic robustness, with semantic tokens showing greater stability under noise perturbations

## Why This Works (Mechanism)

### Mechanism 1
The framework establishes that semantic tokens (text-describable) are more efficient for Language Model (LM) modeling, whereas acoustic tokens (text-indescribable) are superior for high-fidelity signal reconstruction. By categorizing features based on "text-describability" and measuring LM Perplexity (PPL) against reconstruction metrics (PESQ/STOI), the benchmark reveals a trade-off. Semantic tokens reduce the entropy of the token sequence (lower PPL), making them easier for LMs to predict. Acoustic tokens preserve high-frequency details required for reconstruction but increase sequence entropy, raising PPL.

### Mechanism 2
Codebook Index (ID) stability under perturbation serves as a proxy for a token's semantic robustness versus acoustic sensitivity. The benchmark applies perturbations (multi-round reconstruction, temporal shifts). Acoustic tokens, which aim to encode absolute signal details including noise and phase, exhibit "ID drift" (low stability). Semantic tokens, which abstract away acoustic details to represent content, show high ID stability because the underlying content remains unchanged despite minor signal shifts.

### Mechanism 3
Downstream probe task performance is correlated with the specific information type (semantic vs. acoustic) preserved in the discrete tokens. The benchmark uses lightweight probe heads (convolutions/attentions) on frozen token embeddings. High performance on ASR or Music Tagging (semantic tasks) indicates the tokens have discarded irrelevant acoustic variance to isolate concepts. High performance on Speaker Identification (acoustic/paralinguistic task) indicates the preservation of fine-grained acoustic fingerprints.

## Foundational Learning

- **Discrete Audio Tokenization (VQ/RVQ)**: The entire benchmark evaluates the quality of these discrete representations. You must understand how continuous audio waveforms are mapped to finite codebooks (Vectors) and how Residual Vector Quantization (RVQ) layers multiple codebooks to capture details. *Quick check:* Does increasing the number of codebooks in an RVQ system typically capture higher-level semantics or finer acoustic details? (Answer: Finer acoustic details/residuals).

- **Autoregressive Perplexity (PPL)**: This is the primary metric for "LM-friendliness." You need to understand that PPL measures the "surprise" of the model; lower surprise implies the token sequence has statistical regularities (semantics) similar to language. *Quick check:* If a codec produces random integer tokens for a constant sine wave, would the PPL likely be high or low? (Answer: High, because the sequence is unpredictable).

- **The Semantic-Acoustic Trade-off**: The paper's core thesis is that you generally optimize for one at the expense of the other. Reconstruction requires preserving errors/noise (high entropy), while Understanding requires discarding them (low entropy). *Quick check:* Why might a codec with perfect reconstruction (high PESQ) perform poorly on an ASR task? (Answer: It preserves background noise and speaker timbre as distinct tokens, confusing the text-semantic model).

## Architecture Onboarding

- **Component map:** Input Audio → Codec (Encoder → Quantizer → Decoder) → Evaluation Branches: Reconstruction (PESQ/STOI/WER), LM (PPL via 100M Transformer), Probe (17 datasets, 11 tasks), Stability (ID drift analysis)

- **Critical path:** The **Perplexity Experiment** is the novel critical path introduced here. While reconstruction and probes exist in other benchmarks, training a specific Qwen2-100M model on the tokens to measure "learnability" is the central validation step for MLLM readiness.

- **Design tradeoffs:** Codebook Size vs. PPL (normalized to reference 1024); Probe Complexity (kept lightweight to reflect token quality rather than probe capacity).

- **Failure signatures:** The "Drift" Failure (excellent PESQ but IDs change drastically on 2nd round of encoding); The "Semantic Void" Failure (excellent PESQ but PPL extremely high >200).

- **First 3 experiments:** (1) Sanity Reconstruction: Run EnCodec on LibriTTS, verify PESQ > 3.0 and Speaker Similarity > 0.9; (2) PPL Baseline: Train 100M Qwen2 on HuBERT tokens for "Low PPL" baseline, random VQ for "High PPL" baseline; (3) ID Stability Check: Perform 10-round reconstruction test on acoustic codec, plot Same ID Ratio curve (should be monotonically decreasing).

## Open Questions the Paper Calls Out

- **Open Question 1:** What specific latent space properties cause EnCodec to exhibit low perplexity and Mimi to underperform, contradicting their classification as acoustic and semantic-distilled models respectively? The paper states the exact reasons are still unknown and need further exploration.

- **Open Question 2:** Is there an inherent trade-off that prevents a single codec from simultaneously minimizing language modeling perplexity and maximizing reconstruction fidelity? The paper identifies the correlation but does not determine if this is a theoretical limit or a limitation of current optimization methods.

- **Open Question 3:** Does the requirement that semantic features be "strictly described by text" exclude valid non-linguistic semantic information (e.g., abstract musical structure) from being optimized in future codecs? The benchmark evaluates performance using text-aligned tasks but does not validate if non-text audio semantics are preserved.

## Limitations
- The framework evaluates 8 codecs on specific datasets, and generalization to all audio codecs and diverse real-world scenarios remains untested
- The definition of "semantic" as strictly text-describable is novel but subjective, potentially oversimplifying the spectrum of audio features
- Several critical hyperparameters for the 100M LM and probe networks are unspecified, making exact reproduction challenging

## Confidence

- **Semantic vs. Acoustic Trade-off:** High Confidence - well-supported by experimental data and consistent across multiple metrics
- **Perplexity as LM-friendliness Metric:** High Confidence - robust correlation between lower PPL and better semantic task performance
- **ID Stability as Semantic Robustness Proxy:** Medium Confidence - theoretical mechanism sound but implementation details unspecified
- **Downstream Probe Task Validity:** Medium Confidence - lightweight probe networks reasonable but may underestimate token potential

## Next Checks

1. **Perplexity Reproduction Test:** Reproduce PPL experiments for 3 codecs (semantic, acoustic, hybrid) on smaller dataset subset, verify normalized PPL values match within 10% tolerance

2. **Probe Network Ablation Study:** Conduct ablation study varying probe network depth/width, assess if lightweight design is bottleneck by checking performance improvement with more capacity

3. **Cross-Dataset Generalization Check:** Evaluate top 3 codecs on external diverse audio dataset (e.g., Mozilla Common Voice different language, DNS-Challenge), compare relative rankings to assess robustness across data distributions