---
ver: rpa2
title: 'From Task-Specific Models to Unified Systems: A Review of Model Merging Approaches'
arxiv_id: '2503.08998'
source_url: https://arxiv.org/abs/2503.08998
tags:
- merging
- arxiv
- methods
- weights
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides a comprehensive survey and taxonomy of model
  merging techniques, addressing the lack of a unified framework and systematic comparative
  analysis in this rapidly evolving field. The authors classify existing methods into
  seven categories: permutation-based, direct merging, magnitude-based pruning, activation-based
  pruning, hybrid pruning, optimization-based, and LoRA-specific merging approaches.'
---

# From Task-Specific Models to Unified Systems: A Review of Model Merging Approaches

## Quick Facts
- arXiv ID: 2503.08998
- Source URL: https://arxiv.org/abs/2503.08998
- Reference count: 8
- Provides comprehensive survey and taxonomy of model merging techniques, addressing lack of unified framework

## Executive Summary
This paper addresses the growing need for unified models that can handle multiple tasks without requiring separate fine-tuned checkpoints. As foundation models proliferate across applications, storing and deploying numerous task-specific models becomes computationally prohibitive. Model merging offers a solution by combining multiple fine-tuned models into a single unified system, eliminating the need for task-specific training data and reducing computational overhead. The authors systematically survey and classify existing merging approaches into seven categories, providing a foundational reference for researchers entering this rapidly evolving field.

## Method Summary
The paper conducts a comprehensive survey of model merging techniques, classifying them into seven categories: permutation-based (weight/activation matching), direct merging (Model Soup, Task Arithmetic), magnitude-based pruning (DARE, TIES-Merging), activation-based pruning (ZipIt!, SurgeryV2), hybrid pruning (MACL, PCB-Merging), optimization-based (Fisher-weighted, RegMean), and LoRA-specific (LoraHub). The survey identifies key challenges including performance degradation with increasing task numbers, knowledge interference between models, and the lack of comprehensive theoretical frameworks. The authors propose future directions including integrating model compression with merging, exploring task-level separation strategies, and developing architecture-aware merging methods.

## Key Results
- Classification of model merging methods into seven distinct categories based on their underlying mechanisms
- Identification of parameter interference and sign conflicts as primary challenges in merging multiple task models
- Recognition of performance degradation as task numbers increase as a fundamental limitation of current approaches
- Proposal of promising future directions including compression-merged integration and architecture-aware techniques

## Why This Works (Mechanism)

### Mechanism 1: Task Arithmetic and Vector Addition
- **Claim:** Model capabilities can be encoded as directional vectors in weight space and combined linearly
- **Mechanism:** Task-specific knowledge resides in the delta between fine-tuned and pre-trained weights. These "task vectors" can be linearly combined to inject new capabilities or remove undesirable behaviors
- **Core assumption:** Task-specific knowledge is additive without destructive interference (linear superposition holds)
- **Evidence anchors:** [abstract] "merging model parameters to create a unified model with broad generalization"; [section 2.2] "task vectors... can be applied to modify model weights through simple operations like addition, subtraction"
- **Break condition:** High interference occurs when task vectors have opposing signs for same parameters

### Mechanism 2: Interference Suppression via Pruning (DARE/TIES)
- **Claim:** Randomly or deterministically resetting delta parameters to zero reduces conflict during merging
- **Mechanism:** Fine-tuned models contain significant redundancy. Dropping up to 90% of changed weights and rescaling remainder reduces collision surface between different task vectors
- **Core assumption:** Magnitude of parameter change correlates with importance, or small/noisy updates are dispensable
- **Evidence anchors:** [section 2.3] "DARE... randomly drops a proportion of delta parameters... eliminating up to 90% of redundant delta parameters"; [section 2.3] "TIES-MERGING... resolves conflicting parameter signs by selecting the dominant direction"
- **Break condition:** Aggressive pruning beyond redundancy threshold degrades accuracy

### Mechanism 3: Permutation Invariance and Alignment
- **Claim:** Models trained independently inhabit different coordinate systems and must be aligned before merging
- **Mechanism:** Neuron i in model A might correspond to neuron j in model B. Permutation methods seek alignment matrix to place both models in same loss basin
- **Core assumption:** Valid alignment exists that places both models in same loss basin
- **Evidence anchors:** [section 2.1] "neurons within a layer can be arbitrarily reordered without altering the model's functionality... naive merging can degrade model performance"
- **Break condition:** Typically works for same dataset/task but fails for completely disjoint domains

## Foundational Learning

- **Concept: Linear Mode Connectivity (LMC)**
  - **Why needed here:** LMC defines "navigability" of loss landscape. Without LMC, simple weight averaging creates high-loss model
  - **Quick check question:** Can you draw straight line in weight space between two trained models where accuracy remains high?

- **Concept: Delta Parameters (Task Vectors)**
  - **Why needed here:** Most modern merging techniques operate on delta from pre-trained base, not raw weights
  - **Quick check question:** If Model A is base and Model B is fine-tuned, what is "delta"? (Answer: θ_B - θ_A)

- **Concept: Parameter Interference (Sign Conflict)**
  - **Why needed here:** This is the "Hard Problem" - if Task 1 wants weight positive and Task 2 wants negative, merging cancels learning
  - **Quick check question:** If you average +5 and -5, you get 0. How does your merging strategy prevent this "destructive zeroing"?

## Architecture Onboarding

- **Component map:** Pre-trained Backbone (θ_pre) + N Fine-tuned Checkpoints (θ_ft^1... θ_ft^n) -> Delta Extraction (τ_i = θ_ft^i - θ_pre) -> Conflict Resolution (masks/alignment) -> Aggregation (sum/average) -> Merged Unified Model (θ_merged)

- **Critical path:** The Conflict Resolution step (e.g., dropping low-magnitude weights or resolving sign conflicts) is primary lever for performance

- **Design tradeoffs:**
  - Simple Averaging (Model Soup) vs. Pruning (TIES/DARE): Simple averaging is O(1) computationally cheap but suffers interference; pruning adds computational overhead but scales better
  - Data-Free vs. Optimization: Fisher merging requires inputs to calculate importance, while Task Arithmetic/DARE are strictly data-free

- **Failure signatures:**
  - Catastrophic Forgetting: Merged model fails on Task A after adding Task B (likely sign conflict)
  - Performance Plateau: Adding more tasks yields diminishing returns or degrades overall performance

- **First 3 experiments:**
  1. **Baseline Weight Averaging:** Implement "Model Soup" (simple average of weights) to establish lower bound
  2. **Task Arithmetic:** Implement basic vector addition (θ_pre + λ∑τ_t) and tune scaling factor λ
  3. **Interference Suppression:** Implement TIES-Merging or DARE; compare against baseline to quantify gain from resolving sign conflicts

## Open Questions the Paper Calls Out

- **Open Question 1:** How can model merging methods maintain performance consistency as number of merged tasks scales significantly beyond current benchmarks?
  - Basis in paper: [explicit] "as the number of tasks increases, merged models often underperform compared to independent expert models"
  - Why unresolved: Current methods demonstrate effectiveness on limited task combinations; linear/non-linear degradation patterns with task scaling remain uncharacterized
  - What evidence would resolve it: Systematic benchmarks showing merged model performance across progressively larger task sets

- **Open Question 2:** What theoretical frameworks can accurately predict merged model performance without empirical trial-and-error?
  - Basis in paper: [explicit] "The lack of comprehensive theoretical frameworks for model merging limits the ability to predict and guarantee performance"
  - Why unresolved: Relationship between parameter interference patterns, task similarity, and final performance lacks formal characterization
  - What evidence would resolve it: Development of provable bounds linking task vector properties and expected merged model accuracy

- **Open Question 3:** What is optimal integration strategy for combining model compression with model merging to maximize efficiency while preserving multi-task capabilities?
  - Basis in paper: [explicit] "Combining model compression with model merging represents one of the promising future directions"
  - Why unresolved: Interaction between compression-induced information loss and merging-induced interference remains poorly understood
  - What evidence would resolve it: Comparative studies of compression-before-merging, merging-before-compression, and joint optimization approaches

## Limitations

- The paper is fundamentally a review without original empirical validation - it synthesizes existing literature but doesn't provide new experimental results
- Theoretical claims about permutation invariance and mode connectivity remain largely unproven
- Practical effectiveness of different methods across diverse model architectures and tasks is not empirically demonstrated
- Scalability limits for merging many models lack quantitative validation

## Confidence

- **High**: The taxonomy structure and classification of methods into seven categories is well-supported and logically organized
- **Medium**: The identification of interference as central challenge is consistent with cited literature
- **Low**: Quantitative claims about method superiority or performance thresholds lack experimental backing

## Next Checks

1. **Benchmark Implementation**: Implement at least three representative methods (Task Arithmetic, TIES-Merging, DARE) on common dataset with measurable metrics to verify claimed performance relationships
2. **Interference Analysis**: Conduct ablation studies measuring how sign conflicts and feature drift affect merged model performance across different task combinations
3. **Scalability Testing**: Systematically evaluate performance degradation as task count increases, measuring point at which interference becomes prohibitive