---
ver: rpa2
title: 'Losing our Tail -- Again: On (Un)Natural Selection And Multilingual Large
  Language Models'
arxiv_id: '2507.03933'
source_url: https://arxiv.org/abs/2507.03933
tags:
- language
- languages
- they
- more
- linguistic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This opinion paper warns that multilingual Large Language Models
  (LLMs) are accelerating the loss of linguistic diversity, a process likened to losing
  our "language tails." Unlike human-driven language evolution, which balances convergence
  with divergence, LLMs amplify statistically dominant forms while neglecting rare
  linguistic features, thereby flattening diversity within and across languages. Drawing
  on studies in machine translation, iterated learning, and model collapse research,
  the paper shows that LLMs forget low-probability words, reduce lexical richness,
  and propagate averaged biases.
---

# Losing our Tail -- Again: On (Un)Natural Selection And Multilingual Large Language Models

## Quick Facts
- **arXiv ID**: 2507.03933
- **Source URL**: https://arxiv.org/abs/2507.03933
- **Reference count**: 16
- **Primary result**: Multilingual LLMs are accelerating linguistic diversity loss through frequency bias, model collapse, and averaged bias propagation, requiring urgent evaluation and training reforms.

## Executive Summary
This opinion paper warns that multilingual Large Language Models (LLMs) are accelerating the loss of linguistic diversity, a process likened to losing our "language tails." Unlike human-driven language evolution, which balances convergence with divergence, LLMs amplify statistically dominant forms while neglecting rare linguistic features, thereby flattening diversity within and across languages. Drawing on studies in machine translation, iterated learning, and model collapse research, the paper shows that LLMs forget low-probability words, reduce lexical richness, and propagate averaged biases. It highlights methodological blind spots in evaluating diversity and stresses that LLMs lack the critical self-reflection and creativity that sustain human language. The work calls for rethinking evaluation and training practices to protect expressive multilingual diversity and prevent irreversible linguistic erosion.

## Method Summary
This is an opinion/perspective paper synthesizing findings from machine translation, iterated learning, and model collapse research to argue that multilingual LLMs are accelerating linguistic diversity loss. The paper draws on empirical studies (e.g., Vanmassenhove et al. 2021 on MT bias propagation, McCoy et al. 2023 on low-probability sequence decay, Kobak et al. 2024 on discourse convergence) to support its claims about frequency-dependent selection, model collapse via self-consuming training loops, and averaged bias propagation. No new experiments are conducted; the work is a theoretical synthesis calling for methodological reforms in LLM evaluation and training.

## Key Results
- LLMs systematically amplify high-frequency linguistic forms while eroding rare words, morphological variants, and low-probability constructions through frequency-dependent selection bias.
- Model collapse via self-consuming training loops causes progressive loss of linguistic diversity as synthetic outputs re-enter training data, converging toward averaged, probable sequences.
- LLMs propagate averaged, dominant biases without human-like critical reflection or self-correction, exacerbating discourse convergence and flattening multilingual diversity.

## Why This Works (Mechanism)

### Mechanism 1: Model Collapse via Self-Consuming Training Loops
- **Claim:** When models are repeatedly trained on their own (or other models') outputs, they progressively lose low-probability linguistic forms and converge toward high-probability sequences.
- **Mechanism:** LLMs amplify statistically dominant forms while "forgetting" rare words, morphological variants, and low-frequency constructions. Over iterative training cycles, this creates a self-reinforcing bias toward averaged, probable outputs.
- **Core assumption:** The paper assumes that AI-generated content will increasingly re-enter training data pipelines—a "when," not "if" scenario.
- **Evidence anchors:**
  - [abstract] "Model collapse refers to the eventual consequence of self-consuming training loops, where models reinforce their own biases and lose linguistic diversity."
  - [Section 3.3.2] Shumailov et al. (2023) showed that "at first, the tails of our data (the low-probability events) begin to disappear. Over time they observed a convergence toward more probable token sequences."
  - [corpus] Weak direct evidence; corpus papers focus on multilingual training methods but do not address collapse dynamics.
- **Break condition:** If training data curation rigorously excludes synthetic outputs, or if explicit diversity-preserving regularization is applied during training.

### Mechanism 2: Frequency-Dependent Selection Bias
- **Claim:** Next-token prediction objectives systematically privilege high-frequency forms, causing lexical and morphological erosion.
- **Mechanism:** Beam search and decoding strategies bias outputs toward probable sequences; rare but grammatically correct forms are under-generated or lost entirely.
- **Core assumption:** The paper assumes this bias compounds over time when synthetic outputs re-enter the data ecosystem.
- **Evidence anchors:**
  - [Section 3.3.1] Vanmassenhove et al. (2021) showed MT systems over-generate masculine "président" vs. feminine "présidente"; after iterations, the plural feminine "présidentes" disappeared entirely.
  - [Section 3.3.2] McCoy et al. (2023) found GPT-4's cipher-decoding accuracy dropped from 51% to 13% when outputs were low-probability sequences—"embers of autoregression."
  - [corpus] No direct replication in corpus; adjacent work on low-resource languages (e.g., African NLP landscape) notes underrepresentation but not collapse mechanisms.
- **Break condition:** If decoding strategies are modified to explicitly sample from or boost low-probability candidates (e.g., diversity-promoting sampling).

### Mechanism 3: Averaged Bias Propagation Without Self-Correction
- **Claim:** Unlike humans—who hold diverse, sometimes conflicting biases and can critically reflect on them—LLMs propagate averaged, dominant biases uniformly.
- **Mechanism:** Training data contains dominant cultural and linguistic patterns; models learn these as monolithic biases without mechanisms for self-correction or critical reflection.
- **Core assumption:** Assumption: Human diversity of biases serves as a protective mechanism against convergence; removing this leads to accelerated flattening.
- **Evidence anchors:**
  - [Section 4] "Rather than capturing the diversity and heterogeneity of biases in human reasoning, by letting LLMs drive language change we risk exacerbating and normalizing dominant biases without having a critical self-reflection or self-correction component."
  - [Section 3.3.2] Kobak et al. (2024) showed terms like "delve," "crucial," and "significant" surged in academic writing post-LLM release—evidence of discourse convergence.
  - [corpus] Corpus papers on multilingual LLMs (e.g., LangGPS, African NLP) note performance gaps for low-resource languages but do not address bias averaging.
- **Break condition:** If models are trained with explicit mechanisms for bias diversity, adversarial correction, or human-in-the-loop oversight during generation.

---

## Foundational Learning

- **Concept: Long-tailed distributions in language**
  - **Why needed here:** The paper's central metaphor—losing our "tails"—depends on understanding that language has power-law distributions: a few elements are very frequent, most are rare but meaningful.
  - **Quick check question:** Can you explain why "the" vs. "dispersed" exemplifies a long-tailed distribution, and why rare forms matter for linguistic richness?

- **Concept: Iterated learning framework**
  - **Why needed here:** The paper contrasts human iterated learning (which produces structure and compositionality) with LLM iterated learning (which produces collapse).
  - **Quick check question:** In iterated learning, why do cognitive bottlenecks (limited memory, sparse input) help humans converge toward productive, compositional language rather than impoverished output?

- **Concept: Beam search bias**
  - **Why needed here:** Understanding why MT/LLM outputs converge requires knowing that beam search favors high-probability sequences, systematically reducing output diversity.
  - **Quick check question:** If beam search selects the top-k most probable next tokens at each step, what happens to rare but valid linguistic forms over a long sequence?

---

## Architecture Onboarding

- **Component map:** Training data layer (web-scale corpora → synthetic content) → Pretraining objective (next-token prediction/cross-entropy) → Decoding layer (beam search/nucleus sampling) → Feedback loop (synthetic outputs re-scraped)
- **Critical path:** Data provenance tracking → synthetic content detection → diversity-preserving training interventions → evaluation metrics beyond fluency/accuracy
- **Design tradeoffs:**
  - Fluency vs. diversity: Optimizing for smooth, probable outputs trades off against preserving rare forms
  - Scale vs. representation: Larger models on dominant-language data amplify existing imbalances
  - Efficiency vs. oversight: Faster inference (greedy/beam) reduces diversity vs. slower sampling methods
- **Failure signatures:**
  - Lexical erosion: Repeated generations show shrinking vocabulary overlap with human baselines
  - Morphological loss: Specific grammatical forms (e.g., gendered inflections) disappear over iterations
  - Discourse convergence: Sudden surges in specific vocabulary (e.g., "delve," "crucial") across unrelated documents
  - Cross-lingual bleed: Dominant language structures (e.g., SVO order) appear in typologically distant target languages
- **First 3 experiments:**
  1. **Iterated translation test:** Translate a corpus through MT → use output as new input → measure lexical diversity (type-token ratio) and morphological form retention across 5+ iterations.
  2. **Tail-frequency probing:** Identify low-frequency but grammatically correct forms in a held-out test set; measure generation rates in LLM vs. human baselines.
  3. **Synthetic content detection baseline:** Build a classifier to distinguish human vs. AI-generated text; track what linguistic features (rare words, syntactic constructions) most distinguish them.

## Open Questions the Paper Calls Out
None

## Limitations
- Core claims rest on extrapolating model collapse findings from narrow experimental settings (cipher translation, synthetic text loops) to broad linguistic diversity loss; severity and universality across multilingual contexts remain uncertain.
- Assumes synthetic content will inevitably re-enter training loops at scale, but real-world data curation practices and synthetic content detection tools could mitigate this.
- Comparison between human iterated learning (which produces structure) and LLM iterated learning (which produces collapse) may oversimplify the mechanisms at play.

## Confidence
- **High confidence**: Model collapse occurs under controlled self-consuming training conditions; frequency bias is inherent to next-token prediction objectives.
- **Medium confidence**: The mechanisms of lexical erosion and morphological loss will compound across languages; discourse convergence effects are observable but attribution to LLMs alone is complex.
- **Low confidence**: The inevitability of irreversible linguistic flattening; the exact magnitude of diversity loss across all language families; whether diversity-preserving interventions can fully counteract these effects.

## Next Checks
1. **Longitudinal corpus analysis**: Track changes in lexical richness and rare word usage in web-crawled corpora before and after widespread LLM deployment, controlling for other linguistic trends.
2. **Cross-lingual ablation study**: Train identical model architectures with and without diversity-preserving objectives (e.g., explicit low-frequency word retention loss), then compare tail form preservation across 10+ typologically diverse languages.
3. **Human-in-the-loop intervention trial**: Deploy a multilingual LLM with active human oversight on low-frequency form generation; measure whether real-time correction prevents iterative collapse over multiple generations.