---
ver: rpa2
title: 'ImmunoFOMO: Are Language Models missing what oncologists see?'
arxiv_id: '2506.11478'
source_url: https://arxiv.org/abs/2506.11478
tags:
- language
- llms
- hallmarks
- immune
- tier
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates language models\u2019 ability to identify\
  \ expert-level concepts of immunotherapy in breast cancer abstracts. It compares\
  \ five small language models (SLMs) and five large language models (LLMs) against\
  \ clinician annotations across three hierarchical tiers of hallmarks."
---

# ImmunoFOMO: Are Language Models missing what oncologists see?

## Quick Facts
- arXiv ID: 2506.11478
- Source URL: https://arxiv.org/abs/2506.11478
- Reference count: 40
- SLMs outperform LLMs in identifying low-level (Tier-III) concepts in immunotherapy hallmarks

## Executive Summary
This study evaluates five small language models (SLMs) and five large language models (LLMs) for identifying expert-level concepts of immunotherapy in breast cancer abstracts. Using a hierarchical hallmark taxonomy spanning three tiers, the authors find that SLMs excel at fine-grained (Tier-III) concept identification while LLMs perform better at high-level (Tier-I) and mid-level (Tier-II) concepts. PubmedBERT achieves the highest SLM performance (42% F1-score), while Gemma-2-9B leads among LLMs (40% F1-score). Both model types show low agreement with expert annotations (Cκ < 0.13), with LLMs exhibiting high hallucination rates at Tier-III (57%).

## Method Summary
The study compares SLMs and LLMs using 188 breast cancer immunotherapy abstracts from ESMO congresses (2020-2024). SLMs employ embedding-based similarity classification where abstract embeddings are compared to hallmark label embeddings, selecting the highest-scoring label. LLMs use zero-shot prompting with explicit label lists, followed by GPT-3.5-turbo post-processing to extract predictions. The hierarchical hallmark taxonomy contains 9 Tier-I categories, 27 Tier-II subcategories, and 177 Tier-III keywords, with explicit mappings between tiers. Models are evaluated using F1-score, Cohen's kappa for model-expert agreement, Krippendorff's alpha for model-model consensus, and hallucination rate detection.

## Key Results
- SLMs outperform LLMs in identifying low-level (Tier-III) concepts (PubmedBERT: 42% F1 vs. Gemma-2-9B: 40% F1)
- LLMs excel at high-level (Tier-I) and mid-level (Tier-II) concept identification
- Both model types show low agreement with expert annotations (Cκ < 0.13)
- LLMs exhibit 8% hallucination rates for Tiers I-II and 57% for Tier-III

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Domain-specific SLMs outperform LLMs on fine-grained concept identification due to focused pretraining corpora that reduce semantic dilution.
- **Mechanism:** SLMs like PubmedBERT are trained exclusively on biomedical literature (PubMed abstracts), producing embeddings where domain-specific terminology occupies distinct regions of the representation space. This concentrated exposure strengthens token-level associations for rare, specialized terms (Tier-III keywords like "HLA heterozygosity" or "indoleamine-2,3-dioxygenase"), enabling more accurate similarity-based classification at the low-level.
- **Core assumption:** The performance difference stems from training data composition rather than model architecture or scale (Assumption: other architectural factors held constant).
- **Evidence anchors:**
  - [abstract] "pre-trained language models have potential to outperform large language models in identifying very specific (low-level) concepts"
  - [section 4] "PubmedBERT consistently obtained the highest F1-score... for all the three tiers" achieving 42% overall; SLMs showed 0% hallucination vs. 57% for LLMs at Tier-III
  - [corpus] Weak direct evidence; neighbor papers focus on sequence length and bias dynamics rather than domain-specific pretraining effects
- **Break condition:** If task requires integrating domain knowledge with general reasoning beyond surface-level term matching, this mechanism degrades (evidenced by low expert agreement Cκ < 0.12).

### Mechanism 2
- **Claim:** LLMs excel at high-level concept identification through broader knowledge coverage but sacrifice specificity due to semantic compression in their representation space.
- **Mechanism:** LLMs trained on diverse corpora encode general semantic patterns that map abstracts to broad categories (9 Tier-I hallmarks) effectively. However, the same breadth causes interference when discriminating among many specific labels—the model's learned associations become "smeared" across similar concepts, reducing precision at Tier-III.
- **Core assumption:** The trade-off is inherent to the scale/diversity of pretraining data rather than a prompting artifact.
- **Evidence anchors:**
  - [abstract] "LLMs excel at high-level (Tier-I) and mid-level (Tier-II) concept identification"
  - [section 5] "LLMs having a broader coverage of information sources leading them to capture high-level or general concepts better"
  - [corpus] No directly comparable evidence in neighbor papers
- **Break condition:** When explicit label lists are provided but the LLM ignores them—producing generic outputs like "immunotherapy" or "breast cancer" instead of Tier-III keywords—this mechanism fails to constrain outputs.

### Mechanism 3
- **Claim:** Hallucination rates scale with label-space complexity because LLMs fall back to high-probability generic tokens when specific options lack strong priors.
- **Mechanism:** At Tier-III (177 keywords), many specialized terms have low frequency in general training data. When prompted to select from this list, LLMs default to more probable but incorrect responses (Tier-I/II terms or generic concepts) rather than low-frequency correct labels. This manifests as 57% hallucination at Tier-III vs. 8% at Tiers I-II.
- **Core assumption:** Hallucinations reflect training distribution priors rather than prompting format issues.
- **Evidence anchors:**
  - [key outcome] "LLMs exhibiting 8% hallucination rates for Tiers-II and 57% for Tier-III"
  - [section 5] "when LLMs are asked to identify low-level HoI... they hallucinate producing Tier-II or Tier-I related hallmarks or more generic outputs"
  - [corpus] No direct evidence; neighbor papers do not address hallucination in structured classification
- **Break condition:** If label lists are refined with additional context or retrieval-augmented grounding, hallucination rates may decrease (suggested in Limitations).

## Foundational Learning

- **Concept: Hierarchical multi-label text classification**
  - **Why needed here:** The hallmark taxonomy spans 3 tiers with inheritance relationships (Tier-I → Tier-II → Tier-III). Understanding that predictions at lower tiers must aggregate coherently to higher tiers is essential for interpreting model behavior.
  - **Quick check question:** Can you explain why a Tier-III prediction of "estrogens" maps to Tier-I "Systemic factors"?

- **Concept: Embedding-based similarity classification (zero-shot)**
  - **Why needed here:** SLMs used unsupervised similarity matching between abstract embeddings and label embeddings. This differs from supervised fine-tuning and explains lower but non-trivial performance.
  - **Quick check question:** How does cosine similarity between a [CLS] token embedding and label name embeddings enable classification without training data?

- **Concept: Inter-annotator agreement metrics (Cohen's κ, Krippendorff's α)**
  - **Why needed here:** The study reports Cκ < 0.13 for model-expert agreement and Kα < 0.1 for model-model consensus. Understanding these metrics reveals that "better performance" still means poor alignment with expert reasoning.
  - **Quick check question:** Why might κ = 0.13 indicate "minimal agreement" even if percentage agreement appears higher?

## Architecture Onboarding

- **Component map:** ESMO abstracts (filtered → 188) → Hierarchical hallmark taxonomy (Tier-I → Tier-II → Tier-III) → SLM pipeline (embed → similarity → select) / LLM pipeline (prompt → parse → map) → Evaluation (F1, Cκ, Kα, hallucination)
- **Critical path:** SLM embedding quality at Tier-III → LLM prompt design for constrained output → post-processing to map predictions to valid taxonomy labels
- **Design tradeoffs:**
  - SLMs: Higher precision on specific terms, no hallucinations, but require similarity threshold tuning and struggle with abstract concepts
  - LLMs: Better high-level reasoning, easier deployment via prompting, but high hallucination at specificity and potential inconsistency across runs
- **Failure signatures:**
  - SLM: Low consensus (C% = 0%) at Tier-II/III indicates embedding space does not cluster fine-grained concepts consistently
  - LLM: Outputs generic terms ("immunotherapy", "breast cancer") when Tier-III labels lack strong priors; 57% of Tier-III outputs are invalid
  - Both: Low expert agreement (Cκ < 0.13) suggests models capture surface patterns rather than clinician-level conceptual reasoning
- **First 3 experiments:**
  1. **Hybrid approach:** Use SLM for Tier-III retrieval to generate candidates, then LLM for Tier-I/II validation—test whether this reduces hallucination while preserving specificity
  2. **Retrieval-augmented prompting:** Provide LLM with few-shot examples of correct Tier-III mappings before prediction—measure hallucination reduction
  3. **Ensemble calibration:** Compare SLM confidence scores with LLM output probabilities to identify cases where models disagree and may benefit from human review

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does expanding the Tier-III keyword list significantly reduce the high hallucination rates (57%) observed in Large Language Models?
- Basis in paper: [explicit] The authors state in the Limitations section that "Adding tier-III categories could potentially diminish hallucinations from LLMs as more specific data could be identified."
- Why unresolved: The current fixed list of 177 keywords may be insufficient, causing LLMs to generate valid medical terms that are flagged as hallucinations simply because they are absent from the reference list.
- What evidence would resolve it: A follow-up evaluation measuring hallucination rates using an expanded, comprehensive Tier-III taxonomy compared to the current baseline.

### Open Question 2
- Question: How does the inclusion of multi-tier expert annotations (Tier-II and Tier-III) affect the assessment of model reasoning alignment?
- Basis in paper: [explicit] The paper notes a "lack of expert annotation at tier-II and tier-III" and suggests that such annotations "would help better understand the logic behind the abstract tier-I annotation."
- Why unresolved: Without ground truth for lower-level concepts, it is difficult to determine if a model's Tier-I error stems from a failure to grasp the high-level concept or a misidentification of a specific keyword.
- What evidence would resolve it: A dataset annotated by oncologists across all three hierarchical tiers to allow for direct, granular comparison of model predictions at every level of specificity.

### Open Question 3
- Question: To what extent does inter-annotator variability among clinicians explain the low agreement scores (Cκ < 0.13) observed between models and the expert?
- Basis in paper: [explicit] The authors suggest in the Limitations that "having more annotators could help in explaining the model's disagreement" and that "lack of agreement... could be reflected in the disagreements between human annotators."
- Why unresolved: The low agreement might reflect the inherent subjectivity of the single clinical expert or the difficulty of the task, rather than a deficiency in the models' capabilities.
- What evidence would resolve it: A study establishing a multi-expert baseline (e.g., 3+ oncologists) to measure human inter-annotator agreement against model performance.

## Limitations

- **Dataset scope and generalizability:** The study uses 188 breast cancer immunotherapy abstracts from ESMO congresses (2020-2024), representing a narrow domain within oncology that may not generalize to other cancer types or broader contexts.
- **Expert annotation reliability:** The inter-annotator agreement metrics (Cκ < 0.13) suggest either high concept ambiguity or inconsistent expert interpretation, raising questions about whether hallucinations reflect model failures or taxonomy ambiguity.
- **Hallucination measurement methodology:** The 57% hallucination rate at Tier-III assumes all predictions outside the reference list are hallucinations, potentially oversimplifying the semantic complexity of hallmark identification.

## Confidence

**High confidence:** The comparative performance trends between SLMs and LLMs (SLMs better at Tier-III specificity, LLMs better at Tier-I/II generalization) are supported by robust statistical evidence (F1-scores, κ statistics). The hallucination rates show clear scaling with label-space complexity.

**Medium confidence:** The mechanistic explanations for performance differences (domain-specific pretraining benefits, semantic compression trade-offs) are logically consistent but rely on indirect evidence. The core assumptions about training data composition effects cannot be directly verified from the presented results.

**Low confidence:** The claim that models "miss what oncologists see" overstates the findings. The low expert agreement (Cκ < 0.13) could indicate model limitations, expert inconsistency, or inherent ambiguity in the hallmark taxonomy rather than clear model failure.

## Next Checks

1. **Cross-domain validation:** Test the same model comparison on immunotherapy abstracts from different cancer types (e.g., lung, melanoma) to assess whether the SLM vs. LLM performance patterns hold across broader oncology contexts.

2. **Annotation reliability assessment:** Conduct inter-annotator reliability analysis among multiple oncology experts to establish whether the hallmark taxonomy has consistent interpretation, and measure how this affects model performance metrics.

3. **Prompt optimization study:** Systematically vary LLM prompt structure, temperature settings, and few-shot examples to determine whether hallucination rates at Tier-III can be reduced through better prompting without fine-tuning.