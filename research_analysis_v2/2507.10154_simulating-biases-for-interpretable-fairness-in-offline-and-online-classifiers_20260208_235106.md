---
ver: rpa2
title: Simulating Biases for Interpretable Fairness in Offline and Online Classifiers
arxiv_id: '2507.10154'
source_url: https://arxiv.org/abs/2507.10154
tags:
- support
- reweight
- mitigator
- demographic
- parity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an agent-based model (ABM) for generating synthetic
  loan application datasets with controllable bias injection, and evaluates bias mitigation
  methods using both offline and online classifiers. The ABM simulates systemic biases
  between two demographic groups, producing data reflecting disparities in socioeconomic
  attributes.
---

# Simulating Biases for Interpretable Fairness in Offline and Online Classifiers

## Quick Facts
- arXiv ID: 2507.10154
- Source URL: https://arxiv.org/abs/2507.10154
- Reference count: 32
- Primary result: In-processing Exponentiated Gradient with Demographic Parity constraints achieves fairest outcomes (lowest SPD/EOD) at the cost of predictive performance.

## Executive Summary
This paper presents an agent-based model (ABM) for generating synthetic loan application datasets with controllable bias injection, and evaluates bias mitigation methods using both offline and online classifiers. The ABM simulates systemic biases between two demographic groups, producing data reflecting disparities in socioeconomic attributes. Classifiers trained on this data exhibit group-level performance gaps, particularly favoring the privileged group. Pre-processing reweighing (automatic or manual) yields modest fairness improvements with minimal performance loss. In-processing mitigation via Exponentiated Gradient with Demographic Parity constraints achieves the fairest outcomes, closely approaching zero in Statistical Parity Difference (SPD) and Equal Opportunity Difference (EOD) metrics. Equalized Odds constraints also improve fairness but at higher performance cost. Shapley value analysis shows mitigation methods redistribute feature contributions, diluting the influence of highly predictive features and interactions, promoting more balanced predictions. The framework supports interpretable fairness auditing and highlights trade-offs between predictive performance and bias mitigation.

## Method Summary
The method uses an agent-based model to generate synthetic loan datasets with controllable systemic bias between two demographic groups. The ABM assigns agents to groups with distinct attribute distributions and enforces biased decision rules. Two classifier pipelines are evaluated: offline (XGBoost with pre-processing reweighing) and online (Hoeffding tree with incremental mitigation). Fairness mitigation techniques include pre-processing reweighing (auto/manual), in-processing Exponentiated Gradient with Demographic Parity/Equalized Odds constraints, and post-processing. Shapley value analysis (including second-order interactions) quantifies how mitigation methods redistribute feature contributions to dilute highly predictive feature dominance.

## Key Results
- In-processing Exponentiated Gradient with Demographic Parity constraints achieves the fairest outcomes (lowest SPD/EOD), closely approaching zero.
- Pre-processing reweighing yields modest fairness improvements with minimal performance loss.
- Equalized Odds constraints improve fairness but at higher performance cost than Demographic Parity.
- Shapley value analysis shows mitigation methods redistribute feature contributions, diluting the influence of highly predictive features and interactions.

## Why This Works (Mechanism)

### Mechanism 1: Structural Bias Injection via Agent-Based Modeling
Generating synthetic data through agent-based models (ABM) allows for the isolation and control of systemic biases that are otherwise difficult to separate in real-world datasets. The ABM assigns agents to demographic groups (Privileged/Protected) with distinct initial attribute distributions (e.g., wealth, employment probability) and enforces biased decision rules (e.g., label boosting/penalizing). Peer networks further amplify these disparities via trust-based clustering. This creates a dataset where bias is not merely a correlation but a causal outcome of the simulated environment.

### Mechanism 2: Fairness-Performance Trade-off via Constraint Optimization
In-processing mitigation via Exponentiated Gradient reduction achieves higher fairness (lower Statistical Parity Difference) at the cost of raw predictive performance compared to pre-processing reweighing. By imposing constraints like Demographic Parity during the model training loop, the optimizer forces the classifier to sacrifice accuracy on the privileged group to balance outcomes across groups. Pre-processing (reweighing) merely adjusts sample importance without fundamentally altering the decision boundary logic, resulting in modest fairness gains but better performance retention.

### Mechanism 3: Feature Influence Diffusion via Shapley Interactions
Mitigation methods work by "diluting" the predictive dominance of highly biased features (hubs), redistributing their influence across a broader set of features. When constraints are applied, the model cannot rely heavily on single highly predictive features (which correlate with the protected attribute) without violating the fairness constraint. Shapley value analysis reveals this as a reduction in the magnitude of individual feature contributions and a breakdown of strong pairwise interactions.

## Foundational Learning

- **Concept: Agent-Based Modeling (ABM)**
  - Why needed here: This paper uses ABM not just for simulation, but as the *data generation mechanism*. You cannot interpret the bias results without understanding how the agents' rules (wealth ranges, peer networks) created the biased distribution in the first place.
  - Quick check question: Can you explain how the "trust" attribute and peer network topology create a feedback loop that disadvantages Group B even before the loan decision is made?

- **Concept: Fairness Metrics (SPD vs. EOD)**
  - Why needed here: The paper evaluates success based on Statistical Parity Difference (SPD) and Equal Opportunity Difference (EOD). Distinguishing between independence (SPD) and separation (EOD) is critical to understanding why "Demographic Parity" and "Equalized Odds" constraints produce different trade-offs.
  - Quick check question: Does Demographic Parity require the true positive rates to be equal across groups, or just the probability of a positive prediction?

- **Concept: Shapley Values (Second-Order)**
  - Why needed here: The paper moves beyond standard feature importance to explain *interactions*. Understanding how pairwise Shapley values represent the combined contribution of two features is necessary to interpret the "network plots" in the results.
  - Quick check question: If a Shapley interaction value is negative, does it mean the combination of those two features reduces the probability of the positive class?

## Architecture Onboarding

- **Component map:**
  ABM Engine (Python/NetworkX) -> Generates `(X, y, sensitive_attribute)` with injected `lbl` and `rep` bias.
  Pipeline A (Offline): Batch Data -> Pre-processing (Reweighing) -> XGBoost (Calibrated) -> Evaluation.
  Pipeline B (Online): Stream Data -> River (Hoeffding Tree) -> Sliding Window Mitigation -> Evaluation.
  XAI Layer: Shapley Interaction Calculator -> Visualizes feature network.

- **Critical path:** The interaction between the **Exponentiated Gradient** wrapper and the base classifier. This is the "in-processing" step where fairness constraints actively modify the loss function. If this wrapper is misconfigured, the model will either ignore the constraint (too much slack) or fail to converge (too strict).

- **Design tradeoffs:**
  - Reweighing (Pre-process): Low complexity, model-agnostic, preserves accuracy better, but leaves bias on the table (modest improvement).
  - Exponentiated Gradient (In-process): High complexity, model-specific, aggressive bias reduction, but significant accuracy drop.

- **Failure signatures:**
  - Randomized Ensemble Collapse: The Exponentiated Gradient method produces a randomized ensemble. If it defaults to a single classifier, it may indicate the constraints were impossible to meet, resulting in a degenerate model.
  - Proxy Leakage: If you accidentally include `trust` or `wealth` in the training set (which the paper hides), fairness metrics will report as "fair" while the model simply learns the proxy, failing the audit.

- **First 3 experiments:**
  1. Baseline Calibration: Run the ABM with `lbl=0.0` and `rep=0.5` (no explicit bias). Verify that the classifier achieves high accuracy and near-zero SPD to establish a control group.
  2. Stress Test: Maximize bias (`lbl=0.6`, `rep=0.8`). Apply *Demographic Parity* constraint. Observe the gap between Group A and Group B approval rates and confirm if the Shapley network "diffuses" the `loan_amount` feature.
  3. Online Drift: In the online pipeline, introduce a sudden shift in `rep` halfway through the stream. Check if the "stabilized online reweigher" (using EMA) adapts faster than the static baseline.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the proposed agent-based model and mitigation framework effectively generalize to scenarios involving multiple intersecting protected groups rather than a binary privileged/protected distinction?
- **Open Question 2:** How does the adapted incremental XGBoost wrapper used for online in-processing compare in stability and convergence to native online fairness algorithms?
- **Open Question 3:** Does the observed dilution of Shapley interaction values represent a global structural change in the model or is it local to specific instance clusters?
- **Open Question 4:** To what extent do the specific parameterizations of the ABM (e.g., wealth ranges, trust thresholds) determine the success of pre-processing versus in-processing mitigation techniques?

## Limitations
- The synthetic nature of the data limits generalizability to real-world bias patterns.
- The ABM's simplified structural assumptions (fixed wealth ranges, binary demographic groups) may not capture the full complexity of systemic discrimination.
- Online mitigation performance could degrade under more severe concept drift or in high-dimensional feature spaces not tested here.

## Confidence
- **High confidence:** The core mechanism of fairness-performance trade-off via constraint optimization is well-established in the literature and clearly demonstrated.
- **Medium confidence:** The feature influence diffusion via Shapley interactions is observed but could be influenced by model variance or specific training dynamics.
- **Low confidence:** The online mitigation results are based on a single scenario and may not generalize to different drift patterns or data distributions.

## Next Checks
1. **Robustness Test:** Run the online pipeline with multiple types of concept drift (sudden vs. gradual) to verify the online reweigher's adaptability.
2. **Realism Check:** Compare the ABM-generated bias patterns against a real-world loan dataset to validate the simulation's structural assumptions.
3. **Shapley Stability:** Run the second-order Shapley analysis multiple times with different random seeds to assess the stability of the observed feature interaction patterns.