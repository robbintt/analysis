---
ver: rpa2
title: 'No Stupid Questions: An Analysis of Question Query Generation for Citation
  Recommendation'
arxiv_id: '2506.08196'
source_url: https://arxiv.org/abs/2506.08196
tags:
- questions
- question
- queries
- mmr-rbo
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates using LLM-generated questions as search
  queries for citation recommendation in scientific articles. The authors prompt GPT-4o-mini
  to generate questions from masked excerpts of scientific papers, then evaluate these
  questions as retrieval queries using a SPECTER-2-based pipeline.
---

# No Stupid Questions: An Analysis of Question Query Generation for Citation Recommendation

## Quick Facts
- arXiv ID: 2506.08196
- Source URL: https://arxiv.org/abs/2506.08196
- Reference count: 11
- Primary result: LLM-generated questions can outperform extractive keywords as search queries for citation recommendation in some cases

## Executive Summary
This paper investigates whether LLM-generated questions from masked scientific excerpts can serve as effective search queries for citation recommendation. Using GPT-4o-mini to generate questions from single-citation paragraphs, the authors evaluate these questions as retrieval queries using a SPECTER-2-based pipeline. They find that questions can retrieve unique result sets and outperform keywords in reranking, particularly when the question encodes semantic intent beyond the source text. However, identifying high-performing questions remains challenging, as the relationship between MMR-RBO selection scores and actual query utility is sensitive to batch size.

## Method Summary
The authors extract 566 single-citation paragraphs from EMNLP 2024 papers, mask the target citation, and prompt GPT-4o-mini to generate 20 questions and 5 extractive keywords per paragraph. These queries are encoded with SPECTER-2 and used to retrieve top-50 candidates from a 94,885-article ACL Anthology index in FAISS. A MiniLM-v2 cross-encoder re-ranks results. To select the best question from each batch, they propose MMR-RBO, a Maximal Marginal Relevance variant using Rank-Biased Overlap for similarity measurement. Performance is measured by MRR and Hit Ratio at top-50.

## Key Results
- Questions retrieve distinct result sets from keywords, with RBO similarity scores between 0.35-0.45
- In reranking, the best questions achieve ~0.35 MRR improvement over keyword baseline
- MMR-RBO shows significant correlation with MRR only for batch sizes ≥10 questions (ρ = 0.294 at λ=1.0, 20 questions)
- Approximately 32% of question batches contain the target document in top-50 retrieval results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-generated questions can retrieve relevant citations that extractive keywords miss because questions encode semantic intent beyond source text.
- Mechanism: The model is prompted to generate questions that "could expose new insights" about a scientific excerpt. Unlike keywords extracted from the text itself, questions leverage the model's latent knowledge to hypothesize what related work might exist—effectively querying for documents the model "knows" about but cannot directly cite.
- Core assumption: The LLM's pre-training corpus contains sufficient coverage of scientific literature that its questions will plausibly align with existing paper titles/abstracts.
- Evidence anchors:
  - [abstract] "In some cases, generated questions ended up being better queries than extractive keyword queries generated by the same model."
  - [Page 1] "Questions, unlike claims, have no truth value while also semantically conveying inquisitive intent."
  - [Page 2, Table 1] Shows a question retrieving methodology-focused articles while keywords are limited to terms present in the source excerpt.
  - [corpus] Related work on question-based retrieval (e.g., "Transforming Questions and Documents for Semantically Aligned RAG") supports decomposition strategies, though corpus evidence for direct question-as-query citation retrieval is nascent.
- Break condition: If target documents are outside the LLM's training distribution, or if the masked excerpt provides insufficient signal for plausible inference, question quality degrades.

### Mechanism 2
- Claim: MMR-RBO selects questions that balance similarity to a known-good baseline while maintaining result-set diversity.
- Mechanism: Maximal Marginal Relevance (λ-weighted) combines: (1) simQ = RBO between candidate question's retrieved results and the keyword baseline's results, and (2) simD = max RBO between candidate and other questions in the batch. Higher λ prioritizes similarity to the baseline; lower λ prioritizes diversity.
- Core assumption: Questions that retrieve result sets moderately similar to the keyword baseline but distinct from each other are more likely to include the target document.
- Evidence anchors:
  - [Page 3, Equation 5] Formal definition of MMR-RBO(Di) = λ · simQ − (1 − λ) · simD
  - [Page 4, Table 2] Shows Spearman correlation between MMR-RBO and MRR increases with batch size (ρ = 0.294 at λ=1.0, 20 questions, p<0.05 across 36.8% of batches).
  - [Page 4] "As the number of questions per batch increases, the relationship between these values becomes significant across more batches."
  - [corpus] MMR for diversity in retrieval is well-established (Carbonell & Goldstein, 1998), but corpus does not directly validate RBO-based MMR variants for query selection.
- Break condition: With small batch sizes (≤5 questions), MMR-RBO shows no significant correlation with retrieval utility (Table 2: batch ratio 0.085–0.099 with p<0.05).

### Mechanism 3
- Claim: Questions outperform keywords most reliably in the reranking stage, not initial retrieval.
- Mechanism: Initial retrieval with SPECTER-2 embeddings returns top-50 candidates. A cross-encoder (MiniLM-v2) then scores query-document pairs using full semantic matching. Questions—being natural language with richer semantics—provide better signals for the cross-encoder than sparse keyword lists.
- Core assumption: The cross-encoder can interpret question semantics more effectively than keyword semantics for relevance judgment.
- Evidence anchors:
  - [Page 4, Figure 3] Shows best questions achieve ~0.35 MRR improvement over keyword baseline after reranking, while average questions underperform keywords.
  - [Page 3] "Around 32% of question batches contain at least one question with the target document in its retrieved results."
  - [Page 4] "In reranking, MMR-RBO can provide limited assistance in identifying the best query in a batch."
  - [corpus] Cross-encoder reranking for scientific retrieval is validated in related benchmarks (e.g., LitSearch), though question-specific reranking remains underexplored.
- Break condition: If the initial retriever fails to include the target in top-50, reranking cannot recover it. Context queries (full paragraph) outperform questions in initial retrieval hit rate.

## Foundational Learning

- Concept: **Rank-Biased Overlap (RBO)**
  - Why needed here: RBO measures similarity between ranked lists with indefinite length, weighting top positions more heavily. It is the core similarity function in MMR-RBO for comparing result sets.
  - Quick check question: Given two ranked lists [A, B, C] and [A, C, B], does RBO return a value closer to 1.0 or 0.5? (Answer: Closer to 1.0, because top-ranked overlap is weighted higher.)

- Concept: **Maximal Marginal Relevance (MMR)**
  - Why needed here: MMR balances relevance and diversity. Understanding λ-weighting is critical for tuning MMR-RBO and interpreting why small batches fail.
  - Quick check question: If λ=0.8, does MMR prioritize similarity to the reference query or diversity from other candidates? (Answer: Similarity to reference query.)

- Concept: **Citation Recommendation as Retrieval**
  - Why needed here: The task is framed as ad-hoc retrieval over a corpus (ACL Anthology) with a masked target. Understanding the retrieval-rerank paradigm clarifies where questions help most.
  - Quick check question: Why does hit rate at top-50 matter before reranking? (Answer: Reranking cannot surface documents not retrieved initially.)

## Architecture Onboarding

- Component map:
  1. GROBID extracts full text and citation spans from PDFs → mask citations → filter to single-citation paragraphs (566 samples)
  2. GPT-4o-mini generates 20 questions + 5 extractive keywords per paragraph
  3. SPECTER-2 encodes title+abstract for 94,885 ACL articles → stored in FAISS
  4. SPECTER-2 ad-hoc query adapter encodes each query → FAISS returns top-50
  5. MiniLM-v2 cross-encoder scores query vs. (title + abstract) → re-sort by score
  6. MMR-RBO scores questions → select top-scoring question for final ranking

- Critical path:
  1. Parse and mask citations (GROBID quality affects downstream signal)
  2. Generate questions (prompt design determines question relevance)
  3. Retrieve top-50 (if target not in top-50, pipeline fails for that sample)
  4. Rerank with cross-encoder (where questions show largest gain)
  5. Apply MMR-RBO only with batch size ≥10 for meaningful selection

- Design tradeoffs:
  - **Batch size vs. selection reliability**: MMR-RBO requires ≥10–20 questions per sample for stable correlation with MRR. Smaller batches increase variance.
  - **Context vs. question queries**: Context queries (full paragraph) have higher hit rate but are not realistic user queries. Questions approximate user intent but require selection.
  - **λ tuning**: λ=1.0 (maximize similarity to keyword baseline) shows strongest correlation with MRR at batch size 20, but may over-constrain diversity.

- Failure signatures:
  1. **Zero hit rate for all queries in a batch**: Likely excerpt is too short or topic is out-of-distribution for ACL Anthology.
  2. **MMR-RBO shows no correlation with MRR**: Batch size too small (≤5) or λ poorly tuned.
  3. **Reranking degrades performance**: Cross-encoder may misinterpret question semantics; verify prompt phrasing.
  4. **High variance in question quality**: GROBID extraction errors propagated to context; inspect malformed excerpts.

- First 3 experiments:
  1. **Baseline replication**: Reproduce the SPECTER-2 + MiniLM-v2 pipeline with keyword queries only; verify MRR matches paper baseline (~0.12).
  2. **Batch size ablation**: Generate 5, 10, 20 questions per paragraph; measure correlation between MMR-RBO and MRR at each size to validate sensitivity claim.
  3. **Reranking gain analysis**: For each question batch, compute MRR delta (question − keyword) after reranking; quantify what fraction of batches contain at least one outperforming question (paper reports ~32% for retrieval, reranking not explicitly reported).

## Open Questions the Paper Calls Out

- **Question**: Do LLM-generated questions provide genuine utility to human researchers conducting literature search, beyond their performance on automated retrieval metrics?
  - **Basis in paper**: [explicit] Appendix A.1 states: "Whether these questions would be genuinely useful to a human remains an open question and requires further evaluation."
  - **Why unresolved**: All evaluation in the paper uses automated metrics (MRR, Hit Ratio); no human evaluation was conducted to assess whether questions aid researchers' actual search workflows.
  - **What evidence would resolve it**: User studies where researchers perform literature search tasks using question-based queries, with qualitative feedback and task success rates compared to keyword baselines.

- **Question**: Can linking generated questions to specific claims within target documents improve citation recommendation performance?
  - **Basis in paper**: [explicit] Section 3.2 states: "Linking questions to claims made within a target document could be a future direction for scientific article retrieval."
  - **Why unresolved**: The current approach evaluates questions only by retrieval metrics, without establishing connections between question content and specific claims in retrieved documents.
  - **What evidence would resolve it**: Experiments mapping generated questions to claims in target documents and measuring whether claim-question alignment predicts retrieval success.

- **Question**: How can the utility of question queries be predicted at inference time when the target document is unknown?
  - **Basis in paper**: [inferred] Section 3 states "finding the best question in a batch is a non-trivial process" and MMR-RBO shows inconsistent correlation with MRR depending on batch size (Table 2 shows only 36.8% of batches achieve significance with λ=1.0).
  - **Why unresolved**: MMR-RBO is sensitive to dataset size and variance; no reliable method exists to predict which questions will retrieve relevant documents before executing the search.
  - **What evidence would resolve it**: Identification of question features (semantic, structural, or LLM-based scores) that correlate consistently with retrieval success across varying batch sizes and domains.

## Limitations

- MMR-RBO selection mechanism shows significant correlation with MRR only for batch sizes ≥10 questions, making reliable question selection challenging in real-world scenarios where generating 20 questions per query may be impractical.
- The method's effectiveness depends heavily on the LLM's training corpus coverage; questions degrade significantly when target documents lie outside the model's training distribution.
- With only ~25% hit rate at top-50 retrieval, the evaluation is inherently noisy and may not generalize well to broader scientific domains.

## Confidence

- **High confidence**: The core finding that questions can retrieve unique result sets distinct from keywords is well-supported by RBO analysis and multiple retrieval runs. The observation that questions outperform keywords in reranking (achieving ~0.35 MRR improvement) is consistently demonstrated across experiments.
- **Medium confidence**: The MMR-RBO selection mechanism shows promise but is undermined by its sensitivity to batch size and inconsistent correlation with actual query utility. While the mathematical formulation is sound, its practical effectiveness remains questionable.
- **Low confidence**: Claims about "no stupid questions" are overstated given that only ~32% of question batches contain the target document in top-50 results. The assertion that all questions produce "distinct result sets" does not address whether these distinct sets are useful or merely different.

## Next Checks

1. **Batch size sensitivity validation**: Systematically vary batch size from 5 to 50 questions per paragraph and measure the correlation between MMR-RBO scores and MRR at each size. This would confirm whether the 10-20 question threshold is indeed critical for selection reliability.

2. **Cross-encoder sensitivity analysis**: Compare the MiniLM-v2 cross-encoder's performance on questions versus keywords across different λ values in MMR-RBO. Determine whether the reranking advantage is consistent or dependent on specific similarity thresholds.

3. **Out-of-distribution robustness test**: Identify paragraphs whose masked excerpts contain highly specialized terminology or reference papers outside the ACL Anthology's typical scope. Measure whether question generation fails more frequently on these cases, validating the corpus coverage assumption.