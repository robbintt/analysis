---
ver: rpa2
title: FusionAdapter for Few-Shot Relation Learning in Multimodal Knowledge Graphs
arxiv_id: '2510.00894'
source_url: https://arxiv.org/abs/2510.00894
tags:
- multimodal
- fusionadapter
- knowledge
- fusion
- modalities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles few-shot relation learning in multimodal knowledge
  graphs (MMKGs) where structural, textual, and visual data provide complementary
  but modality-specific information. Existing methods suffer from either rigid modality
  alignment or insufficient adaptation in low-resource settings.
---

# FusionAdapter for Few-Shot Relation Learning in Multimodal Knowledge Graphs

## Quick Facts
- **arXiv ID**: 2510.00894
- **Source URL**: https://arxiv.org/abs/2510.00894
- **Reference count**: 40
- **Primary result**: Outperforms state-of-the-art baselines by up to 43.1% in MRR and 29.8% in Hit@5 on WN9-IMG, FB-IMG, and FB-IMG (F) datasets

## Executive Summary
This paper addresses few-shot relation learning in multimodal knowledge graphs where structural, textual, and visual data provide complementary but modality-specific information. Existing methods struggle with rigid modality alignment or insufficient adaptation in low-resource settings. The authors propose FusionAdapter, a lightweight adapter-based fusion framework that preserves modality-specific characteristics via a diversity loss while adapting to unseen relations efficiently. Extensive experiments demonstrate FusionAdapter's superiority over state-of-the-art baselines, with robust performance even when modalities are missing.

## Method Summary
FusionAdapter builds on MetaR meta-learning framework, incorporating lightweight adapter modules for text and image modalities. The model uses pre-trained entity embeddings (structural, textual, visual) and applies element-wise summation for fusion. During meta-training, the meta-learned prior, adapters, and embeddings are jointly optimized. For meta-testing, the prior and embeddings are frozen while only adapter parameters are updated using the support set. The framework includes a contrastive diversity loss that encourages modality-specific embeddings to maintain distinct characteristics rather than collapsing into a shared space.

## Key Results
- Achieves up to 43.1% improvement in MRR and 29.8% improvement in Hit@5 over state-of-the-art baselines
- Demonstrates robustness to missing modalities, maintaining performance when image or text data is unavailable
- Parameter-efficient design adds minimal parameters while preventing overfitting in few-shot scenarios
- Ablation studies confirm the importance of both diversity loss and adapter modules for optimal performance

## Why This Works (Mechanism)

### Mechanism 1: Modality-Specific Preservation via Diversity Loss
The contrastive diversity loss penalizes excessive similarity between adapted textual/visual embeddings and structural embeddings, forcing the model to retain complementary information rather than collapsing all modalities into an averaged representation. This prevents the erasure of modality-specific signals that are critical for predicting unseen relations.

### Mechanism 2: Parameter-Efficient Adaptation
By freezing the majority of the model and training only lightweight adapter modules, FusionAdapter achieves effective generalization to unseen relations in low-resource settings. The small feed-forward networks allow for efficient "re-weighting" of pre-trained knowledge without overfitting to limited examples.

### Mechanism 3: Meta-Learned Modality Alignment
The meta-learning pipeline teaches adapters how to fuse structural, text, and image data effectively across varying contexts. This learned initialization transfers the ability to combine modalities from seen to unseen relations, enabling rapid adaptation during meta-testing.

## Foundational Learning

- **Concept**: Meta-Learning (MAML/MetaR)
  - **Why needed here**: The paper explicitly builds on MetaR, requiring understanding of support vs. query sets and learning initialization for rapid adaptation.
  - **Quick check question**: Can you explain why standard gradient descent fails in a 1-shot setting compared to meta-learning optimization?

- **Concept**: Multimodal Knowledge Graph Embeddings (TransE family)
  - **Why needed here**: The baseline representations and scoring function rely on translational distance concepts.
  - **Quick check question**: In the equation E = E_S + E'_T + E'_V, what does it imply geometrically when we sum these vectors?

- **Concept**: Adapter Networks (Bottleneck Layers)
  - **Why needed here**: The core contribution is the adapter module, which uses low-dimensional projection followed by expansion for parameter-efficient tuning.
  - **Quick check question**: Why would a bottleneck adapter generally preserve information better than simply zeroing out gradient updates for most layers?

## Architecture Onboarding

- **Component map**: Pre-trained Embeddings -> Modality Adapters (Text, Image) -> Fusion Layer (element-wise summation) -> Loss Aggregator (ranking + diversity loss) -> Meta-Learner
- **Critical path**: The gradient flow from Task Loss and Diversity Loss back to the Adapters is most critical. The diversity loss prevents adapters from converging to solutions where adapted embeddings become identical to structural embeddings.
- **Design tradeoffs**: Adapter size (small m ensures efficiency but may bottleneck complex concepts; large m risks overfitting), Diversity Coefficient (controls trade-off between task fitting and modality distinctiveness)
- **Failure signatures**: Mode Collapse (adapted embeddings become identical to structural embeddings), Overfitting to Support Set (query set performance significantly worse than support set)
- **First 3 experiments**: 1) Diversity Ablation (run with α=0 to confirm drop in MRR), 2) Modality Robustness (remove Image modality entirely to verify structural+text fusion holds up), 3) Shot Sensitivity (run with K=1, 3, 5, 10 shots to verify parameter-efficient adapters utilize increasing data volume efficiently)

## Open Questions the Paper Calls Out

### Open Question 1
How does FusionAdapter perform when integrated with meta-learning backbones other than MetaR? The current evaluation is strictly limited to MetaR architecture, so compatibility with metric-based or gradient-based meta-learning methods remains unknown.

### Open Question 2
Can the fusion strategy effectively scale to incorporate modalities beyond text and images, such as audio or video? The diversity loss is tuned for three specific modalities, and adding more might increase the risk of over-homogenization or noise.

### Open Question 3
To what extent is the model dependent on the quality of pre-trained multimodal embeddings? The lightweight adapter design may lack capacity to correct significant noise or misalignment in low-quality input embeddings.

## Limitations

- The diversity loss mechanism relies on an assumption that modality-specific features are inherently beneficial, but lacks ablation studies testing when modalities are actually redundant
- The meta-learning framework is assumed to provide strong initialization, but no comparison is made against simpler transfer learning baselines
- Claims of robustness to missing modalities are tested only on the FB-IMG (F) dataset, limiting generalizability

## Confidence

- **High Confidence**: Parameter-efficient adapter design contribution to preventing overfitting in few-shot scenarios
- **Medium Confidence**: Diversity loss mechanism's benefit, as it shows strong ablation results but lacks analysis of failure modes
- **Medium Confidence**: Meta-learning pipeline's effectiveness, as it builds on MetaR but doesn't isolate meta-initialization vs. adapter contributions

## Next Checks

1. **Modality Redundancy Test**: Force textual/visual embeddings to be identical to structural embeddings and evaluate if diversity loss prevents catastrophic collapse or degrades performance
2. **Transfer Learning Baseline**: Implement simple fine-tuning baseline on pre-trained embeddings and compare against FusionAdapter in 1-shot and 5-shot settings
3. **Margin Sensitivity Analysis**: Systematically vary the diversity loss margin γ (e.g., 0.1, 0.5, 1.0) and measure impact on MRR to identify if fixed γ=0 is optimal or brittle