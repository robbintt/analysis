---
ver: rpa2
title: 'DoubleAgents: Interactive Simulations for Alignment in Agentic AI'
arxiv_id: '2509.12626'
source_url: https://arxiv.org/abs/2509.12626
tags:
- email
- system
- doubleagents
- user
- policies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DoubleAgents is an agentic planning tool that embeds transparency
  and control through user intervention, value-reflecting policies, rich state visualizations,
  and uncertainty flagging for human coordination tasks. A built-in respondent simulation
  generates realistic scenarios, allowing users to rehearse and refine policies and
  calibrate their use of agentic behavior before live deployment.
---

# DoubleAgents: Interactive Simulations for Alignment in Agentic AI

## Quick Facts
- **arXiv ID:** 2509.12626
- **Source URL:** https://arxiv.org/abs/2509.12626
- **Reference count:** 40
- **Primary result:** Interactive simulation enables users to safely probe and refine agent policies, improving alignment and trust in agentic systems

## Executive Summary
DoubleAgents is an interactive agentic planning tool that uses built-in simulation to help users align AI behavior with their preferences for high-stakes social coordination tasks. The system generates realistic scenarios through simulated respondents, allowing users to test policies and refine their approach before live deployment. A two-day lab study (n=10) and deployment studies show that users initially hesitate to delegate but gradually increase reliance as simulation helps them identify misalignments and adjust policies accordingly.

## Method Summary
DoubleAgents implements a ReAct-style coordination agent using GPT-o3 for reasoning and GPT-4o for email drafting and simulation. The system uses summary-based context to improve policy selection (F1 0.70 vs 0.27 baseline) and includes an edge-case detector to flag situations outside policy coverage. Users interact through a Flask dashboard with policy panels, chat interface, assignment grid, and calendar view. The technical evaluation tested policy selection and edge detection, while user studies examined trust-building and real-world deployment effectiveness.

## Key Results
- Summary-based context improved policy selection F1 score from 0.27 to 0.70
- Edge-case detector achieved 93% accuracy in identifying situations requiring user intervention
- User study participants increased delegation as simulation helped align agent behavior with their intentions
- Deployment results showed real-world relevance and usefulness for managing complex, uncertain tasks

## Why This Works (Mechanism)

### Mechanism 1: Iterative Value Alignment via Simulation
- **Claim:** Interactive simulation accelerates bidirectional alignment by allowing users to safely probe system behavior and refine preferences before live execution.
- **Mechanism:** Users test policies in a sandbox where the Coordination Agent proposes actions and Simulated Respondents reply realistically. Users intervene, edit drafts, and update policies, transforming abstract values into executable rules.
- **Core assumption:** Users possess latent preferences they cannot fully articulate upfront and need concrete scenarios to externalize them.
- **Evidence anchors:** Abstract states simulation helps users "rehearse and refine policies"; section 5.3.3 shows simulations helped participants identify where agent behavior diverged from expectations.

### Mechanism 2: Transparency-Induced Reliance
- **Claim:** Explicit state visualization and uncertainty flagging increase user reliance by making system reasoning legible and bounding autonomy.
- **Mechanism:** The architecture exposes the "Plan -> Action -> Email" chain and visualizes assignment state. An "Edge Case Detector" identifies scenarios outside policy coverage and pauses execution, allowing users to calibrate trust based on observed reasoning.
- **Core assumption:** Users prefer controlled autonomy over full automation in high-stakes social tasks.
- **Evidence anchors:** Abstract mentions "transparency and control through user intervention"; section 5.3.2 shows increases in reliance reinforced by successful issue flagging via stop hooks.

### Mechanism 3: Context Distillation for Policy Selection
- **Claim:** Summarizing raw interaction history into structured progress state improves the agent's ability to retrieve and apply relevant coordination policies.
- **Mechanism:** Instead of passing raw email logs to the policy selector, the system generates a "Progress Summary" (JSON) listing filled slots, pending speakers, and recent actions. This reduces noise for the reasoning model.
- **Core assumption:** LLMs struggle to select relevant rules from long, unstructured conversation histories and benefit from intermediate structured reasoning steps.
- **Evidence anchors:** Section 4.1.2 shows summary-based context achieves highest F1 score (0.70) compared to raw context (0.27).

## Foundational Learning

- **Concept: ReAct Loop (Reasoning + Acting)**
  - **Why needed here:** The core engine driving the Coordination Agent, interleaving "Thoughts" (summarizing progress, selecting policies) with "Actions" (drafting emails, waiting).
  - **Quick check question:** How does separating the *plan generation* (reasoning) from the *action execution* (acting) improve error recovery compared to a standard request-response model?

- **Concept: Bidirectional Alignment**
  - **Why needed here:** The paper frames alignment not just as training the AI, but as the user learning how to operate the AI. Users refine the system (policies), and the system refines the user's mental model of what is possible.
  - **Quick check question:** In the DoubleAgents framework, does "alignment" refer to fine-tuning model weights (RLHF) or updating the context/prompts (policies/templates)? (Answer: Context/Prompts).

- **Concept: Simulation Fidelity**
  - **Why needed here:** The utility of the system depends on "Simulated Respondents" behaving like real humans. Low fidelity creates false security; overly complex fidelity may slow onboarding.
  - **Quick check question:** What specific persona attributes (e.g., responsiveness, constraints) are encoded in the simulation to ensure the "stop hooks" are triggered realistically?

## Architecture Onboarding

- **Component map:** User Goal, Speaker Personas, Seed Policies, Email Templates -> Context Manager (JSON state) -> Coordination Agent (GPT-o3) -> Tools (Email Drafting/Sending) -> Respondent Agents (GPT-4o/o3) + Edge Case Detector (GPT-4o) -> Interface (Interactive Chat, Dashboard)

- **Critical path:**
  1. Setup: User defines goals and uploads historical emails (style transfer)
  2. Simulation Loop: Agent generates Progress Summary -> Selects Policy -> Proposes Plan -> User Approves -> Agent Drafts Email
  3. Feedback: Simulated Respondent replies. If reply is outside policy scope, Edge Case Detector fires a Stop Hook for user input

- **Design tradeoffs:**
  - **Seed Policies vs. Blank Slate:** Starts with generic "seed policies" to bootstrap the agent, but risks anchoring bias if users don't edit them
  - **Summary vs. Raw Context:** Using summarized context improves policy selection accuracy (F1 0.70 vs 0.27) but introduces a point of failure if the summary is incorrect

- **Failure signatures:**
  - **Policy Drift:** Agent ignores "wait" policy and spams non-responsive speaker (likely due to context window truncation or summary error)
  - **Persona Collapse:** Simulated respondents stop role-playing constraints, rendering simulation useless
  - **Alert Fatigue:** Edge Case Detector flags routine scheduling conflicts as "edge cases," causing users to auto-approve without reading

- **First 3 experiments:**
  1. **Context Ablation:** Compare Policy Selection accuracy using "Speaker-only" context vs. "Summary-based" context on test set of 30 scenarios
  2. **Edge Case Precision:** Run Edge Case Detector against "Clear" vs. "Flag" test suite to measure False Positive rates
  3. **Persona Stress Test:** Create conflicting persona (e.g., "A distinguished professor who is extremely responsive but vague") to test simulation and edge case detector handling

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can interactive simulation frameworks be extended to support alignment in other high-stakes, interpersonal agentic domains such as hiring, negotiation, or caregiving?
- **Basis in paper:** Discussion section explicitly lists these areas as "new design directions" for extending the simulation approach beyond seminar organizing.
- **Why unresolved:** Current work focused solely on seminar scheduling; authors haven't tested if specific simulation mechanisms generalize to domains with different social norms or higher stakes.
- **What evidence would resolve it:** Deployment studies in hiring or caregiving contexts demonstrating successful alignment in these new domains.

### Open Question 2
- **Question:** How can interactive simulations be designed to support "collaborative alignment" where multiple users with shared authority or conflicting preferences must negotiate policies?
- **Basis in paper:** Limitations section identifies "collaborative alignment" and "multi-organizer decision-making" as areas current system doesn't support but represent important future work.
- **Why unresolved:** Current architecture assumes single organizer with unified preferences and policies.
- **What evidence would resolve it:** System iteration allowing multiple users to input/merge policies, followed by user study evaluating conflict resolution.

### Open Question 3
- **Question:** How effective is grounding respondent simulations in users' real historical data compared to the current persona-based approach for improving alignment fidelity?
- **Basis in paper:** Limitations section notes current persona-based simulations "cannot fully capture the diversity and unpredictability of real-world interactions" and suggests grounding in real historical data.
- **Why unresolved:** Authors curated personas manually; didn't evaluate if training simulations on actual past correspondence improves behavioral modeling or user trust.
- **What evidence would resolve it:** Technical evaluation comparing behavioral fidelity of LLM agents grounded in real email history versus text descriptions.

### Open Question 4
- **Question:** What specific interface designs are most effective at scaffolding early-stage alignment for users who don't yet know their own preferences or delegation boundaries?
- **Basis in paper:** Discussion section explicitly asks how to "better scaffold early-stage alignment for users who don't yet know their own preferences."
- **Why unresolved:** While study showed users learned over time, specific mechanisms for "cold-starting" alignment in novices remain open design challenge.
- **What evidence would resolve it:** Comparative study of different onboarding interfaces measuring speed at which novice users establish stable policies.

## Limitations

- Small sample size (n=10) in lab study limits generalizability of user experience findings
- Edge-case detector's false positive rate and simulation fidelity for complex social scenarios are not fully characterized
- Long-term effectiveness of policy refinement through simulation remains unclear - whether users develop lasting mental models or overfit to simulated personas

## Confidence

- **High Confidence:** Technical evaluation showing summary-based context improves policy selection (F1 0.70 vs 0.27 baseline) and edge-case detection accuracy (93%) is well-supported by controlled experiments
- **Medium Confidence:** User study findings about gradual trust building and increased delegation through simulation are plausible but limited by sample size and potential novelty effects
- **Medium Confidence:** Deployment study results demonstrating real-world usefulness are promising but lack quantitative metrics beyond qualitative feedback

## Next Checks

1. **Context Ablation Test:** Replicate Table 1 experiment comparing policy selection accuracy using "Speaker-only" context vs. "Summary-based" context on 30 test scenarios to validate summary mechanism's contribution

2. **Edge Case Precision Audit:** Run Edge Case Detector against "Clear" vs. "Flag" test suite (Table 2) to measure False Positive rates and ensure routine cases aren't over-flagged

3. **Persona Stress Test:** Create conflicting persona (e.g., "A distinguished professor who is extremely responsive but vague") to test whether simulation and edge case detector handle nuance or default to generic behavior