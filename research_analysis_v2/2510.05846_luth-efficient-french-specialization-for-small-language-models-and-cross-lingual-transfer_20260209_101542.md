---
ver: rpa2
title: 'Luth: Efficient French Specialization for Small Language Models and Cross-Lingual
  Transfer'
arxiv_id: '2510.05846'
source_url: https://arxiv.org/abs/2510.05846
tags:
- french
- english
- language
- arxiv
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving French language
  performance in Small Language Models (SLMs), which are significantly less effective
  in French compared to English due to English-centric training. The authors introduce
  Luth, a family of French-specialized SLMs ranging from 350M to 1.7B parameters,
  developed through targeted post-training on curated French data.
---

# Luth: Efficient French Specialization for Small Language Models and Cross-Lingual Transfer

## Quick Facts
- arXiv ID: 2510.05846
- Source URL: https://arxiv.org/abs/2510.05846
- Authors: Maxence Lasbordes; Sinoué Gad
- Reference count: 22
- One-line primary result: French-specialized SLMs (350M–1.7B) achieve +11.26% average improvements on 6 French benchmarks while improving or retaining English performance through targeted fine-tuning and model merging.

## Executive Summary
This paper addresses the challenge of improving French language performance in Small Language Models (SLMs), which are significantly less effective in French compared to English due to English-centric training. The authors introduce Luth, a family of French-specialized SLMs ranging from 350M to 1.7B parameters, developed through targeted post-training on curated French data. The key method involves fine-tuning multilingual models (Qwen3 and LFM2) on the Luth-SFT dataset (570k French instruction-response pairs) and enhancing performance via model merging techniques. Results show absolute average improvements of up to +11.26% on six French benchmarks (IFEval, MMLU, GPQA-Diamond, Math500, Arc-Challenge, HellaSwag) compared to comparable models, while retaining or even improving English performance. The approach demonstrates that language-specific adaptation with high-quality data and model merging can substantially boost multilingual SLMs without degrading performance in other languages.

## Method Summary
The methodology involves three main components: (1) creating a high-quality French instruction dataset (Luth-SFT) by combining French samples from multilingual datasets, translating English datasets while regenerating responses, and creating a Scholar subset from expert-validated French examination materials; (2) applying full fine-tuning (not LoRA) to multilingual base models (Qwen3-0.6B/1.7B and LFM2-350M/1.2B) on the curated dataset for 3 epochs using Axolotl with FlashAttention and sequence packing; and (3) merging the fine-tuned models with their base versions using either LERP or SLERP interpolation to mitigate catastrophic forgetting and achieve cross-lingual transfer. The process is computationally efficient, requiring only 1× NVIDIA H100 (80GB) and produces models that outperform comparable open-source alternatives on French benchmarks while maintaining or improving English capabilities.

## Key Results
- Absolute average improvements of up to +11.26% on six French benchmarks compared to comparable models
- French fine-tuning and merging preserves or improves English performance (cross-lingual transfer)
- Luth-1.7B-Instruct achieves 63.88% on IFEval-French, outperforming all comparable open-source models
- The approach works across model sizes (350M-1.7B) with consistent improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Targeted supervised fine-tuning (SFT) on curated, language-specific instruction data substantially improves target-language performance in multilingual SLMs without requiring costly continual pre-training.
- Mechanism: The Luth-SFT dataset (570k French instruction-response pairs, 338M tokens) exposes the model to French vocabulary, grammatical structures, and domain terminology. Data is curated via: (1) extracting French samples from multilingual datasets (AYA, Smoltalk2, CroissantLLM) using langdetect, (2) translating high-quality English datasets (Tülu 3, OpenHermes) while regenerating responses rather than translating answers, and (3) filtering for linguistic quality and content relevance. Full fine-tuning updates all parameters rather than using LoRA.
- Core assumption: The base multilingual model's pre-trained representations are sufficiently robust to be refined via post-training alone, without needing extensive continual pre-training on raw French text.
- Evidence anchors:
  - [abstract] "through targeted post-training on curated, high-quality French data, our models outperform all open-source counterparts of comparable size"
  - [section 5.1] "fine-tuning strongly improves French capabilities... infuses them with a richer understanding of French, specific vocabulary, domain-specific terminology"
  - [corpus] Corpus evidence is weak/missing; no direct validation of French-specialization-via-SFT mechanism was found in corpus neighbors.
- Break condition: Fails if base model has poor foundational French representations, if SFT dataset quality is insufficient, or if fine-tuning causes catastrophic forgetting of other languages.

### Mechanism 2
- Claim: Model merging via linear (LERP) or spherical (SLERP) interpolation between fine-tuned and base models can mitigate catastrophic forgetting and may yield improvements in both target and original languages.
- Mechanism: After French-only fine-tuning, the fine-tuned model's weights (w₁) are merged with the original base model's weights (w₀) using: LERP (w = (1-α)w₀ + αw₁) or SLERP (spherical interpolation along a unit sphere arc). Merging coefficients range from 0.3-0.7 depending on model size. The paper attributes observed English improvements to "cross-lingual transfer."
- Core assumption: Weight spaces contain complementary capabilities that can be linearly combined, and cross-lingual transfer from French training to English performance is a generalizable phenomenon.
- Evidence anchors:
  - [abstract] "strategic model merging enhances performance in both languages"
  - [section 5.2] "merging not only recovers lost English performance but also improves overall results across both languages. Moreover, merging provides a natural way to mitigate catastrophic forgetting"
  - [corpus] Corpus evidence is weak/missing; related work on cross-lingual transfer exists but doesn't validate this specific French-to-English transfer via merging.
- Break condition: Fails if merging coefficient is poorly tuned, if models are too dissimilar in weight space, or if cross-lingual transfer is benchmark-specific rather than generalizable.

### Mechanism 3
- Claim: Non-synthetic, expert-validated domain-specific data improves reasoning and knowledge-intensive task performance.
- Mechanism: The Scholar subset (30,300 samples) uses French examination materials (Baccalauréat, CPGE) from 1980-2025, processed via: (1) PDF crawling, (2) LLM-assisted extraction (Gemini 2.5 Flash), (3) LaTeX refinement and explanatory enrichment (Gemini 2.5 Pro), and (4) anomaly filtering. Distribution: Mathematics (67.23%), Physics-Chemistry (10.61%), CS (9.08%), Engineering (6.04%), Biology (5.51%).
- Core assumption: Expert-created examination materials provide higher-quality training signals than synthetic or web-scraped data, and LLM-assisted enrichment preserves quality without introducing hallucinations.
- Evidence anchors:
  - [section 4] "A key objective was to build a resource that is non-synthetic and rooted in expert knowledge. Examination materials... are typically accompanied by official solutions... authored and validated by domain experts."
  - [section 4] "Refining LaTeX formatting for equations and enriching the solutions with additional explanatory details"
  - [corpus] Corpus evidence is weak/missing; no corpus validation of quality-over-quantity data curation was found.
- Break condition: Fails if PDF extraction introduces errors, if LLM enrichment adds hallucinations, or if heavy mathematics skew limits generalization.

## Foundational Learning

- **Concept: Catastrophic Forgetting**
  - Why needed here: The paper explicitly addresses the risk that French-only fine-tuning degrades English capabilities. Model merging is introduced as a mitigation strategy for this phenomenon.
  - Quick check question: Why might updating parameters on French-only data degrade English performance, and how does weight blending conceptually address this?

- **Concept: Cross-Lingual Transfer**
  - Why needed here: The paper reports unexpected English improvements after French training, attributing this to cross-lingual transfer—central to understanding the work's broader implications.
  - Quick check question: Why might improving French capabilities also benefit English performance on certain benchmarks?

- **Concept: Full Fine-Tuning vs. Parameter-Efficient Methods**
  - Why needed here: The authors chose full fine-tuning over LoRA for "better learning." Understanding this trade-off is essential for grasping efficiency claims.
  - Quick check question: What are the trade-offs between full fine-tuning and LoRA in terms of compute, memory, and learning capacity for language specialization?

## Architecture Onboarding

- **Component map:**
  Base Multilingual SLM (Qwen3-0.6B/1.7B or LFM2-350M/700M/1.2B) -> Luth-SFT Dataset (570k samples: Gathered + Translated + Filtered + Scholar) -> Full Fine-Tuning (Axolotl, 3 epochs, FlashAttention, sequence packing) -> Model Merging (MergeKit: LERP or SLERP, α=0.3-0.7) -> Luth Model (5 variants: 350M-1.7B parameters)

- **Critical path:** Dataset quality -> Fine-tuning stability -> Merging coefficient selection. The Scholar subset creation (PDF extraction -> LLM parsing -> enrichment -> filtering) is the most failure-prone pipeline stage.

- **Design tradeoffs:**
  - Full fine-tuning vs. LoRA: Authors chose full fine-tuning for "better learning" at cost of higher compute/memory
  - Merging method: SLERP used for Qwen3, LERP for LFM2—no universal best method; requires empirical evaluation
  - Data translation vs. response regeneration: Translating prompts but regenerating responses (rather than translating answers) may produce more natural French but increases compute cost

- **Failure signatures:**
  - French-only fine-tuning without merging -> English degradation (Figure 4 shows this pattern)
  - Poor merging coefficient -> either insufficient French gains or continued English degradation
  - Noisy Scholar data (PDF extraction errors, LLM hallucinations) -> degraded reasoning benchmarks

- **First 3 experiments:**
  1. Reproduce the fine-tuning + merging pipeline on one model variant (e.g., Qwen3-0.6B on Luth-SFT for 3 epochs, then merge with α=0.7 via SLERP). Verify French benchmark improvement and English stability.
  2. Ablate the merging coefficient: Test α ∈ {0.1, 0.3, 0.5, 0.7, 0.9} to observe the trade-off curve between French gains and English retention on 2-3 benchmarks (e.g., IFEval, MMLU).
  3. Ablate the Scholar subset: Train with and without the 30,300-sample Scholar portion to isolate its contribution to Math500 and GPQA-Diamond performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Luth fine-tuning and merging methodology scale effectively to models larger than 2 billion parameters, and do performance gains diminish, persist, or increase at larger scales?
- Basis in paper: [explicit] "We expect that similar improvements could extend to larger architectures and other languages; verifying this remains a direction for future work." (Section 7); "Our experiments were also restricted to Small Language Models (under 2 billion parameters), which may limit the extent to which our approach unlocks potential gains at larger scales." (Section 8)
- Why unresolved: The authors only experimented with SLMs (350M–1.7B parameters); the relationship between model scale and language-specific adaptation efficiency remains untested for their approach.
- What evidence would resolve it: Applying identical Luth-SFT fine-tuning and merging to models in the 3B–70B range, benchmarking on French and English tasks to compare relative gains against SLM baselines.

### Open Question 2
- Question: Can the Luth methodology be applied to other mid- and low-resource languages with comparable cross-lingual transfer benefits, or is French-specific linguistic or data properties critical to the observed gains?
- Basis in paper: [explicit] "An efficient and reproducible methodology for language-specific adaptation, easily extendable to other mid- and low-resource languages" (Section 2); "verifying this remains a direction for future work" (Section 7)
- Why unresolved: No experiments were conducted on languages other than French and English; the transferability claim is methodological but empirically unverified.
- What evidence would resolve it: Replicating the pipeline (curated SFT dataset + model merging) for 2–3 typologically diverse languages (e.g., Spanish, Arabic, Vietnamese) and measuring both target-language and cross-lingual performance changes.

### Open Question 3
- Question: What is the theoretical mechanism driving cross-lingual transfer from French fine-tuning to improved English performance, and under what conditions does this transfer fail?
- Basis in paper: [inferred] The authors observe English improvements but state: "We attribute this phenomenon to cross-lingual transfer from French to English" without mechanistic analysis (Section 6.3); Figure 4 shows this effect but offers no causal explanation.
- Why unresolved: The paper empirically documents cross-lingual improvement but provides no ablation or analysis of why monolingual French SFT improves English capabilities, leaving the underlying mechanism opaque.
- What evidence would resolve it: Layer-wise analysis of representation changes before/after French fine-tuning; ablations isolating dataset components; experiments with language-pair combinations known to have low/high cross-lingual transfer to identify predictive factors.

### Open Question 4
- Question: To what extent do Luth models retain capabilities in languages beyond French and English after French-specialized fine-tuning and merging?
- Basis in paper: [explicit] "we assessed stability primarily in English, without thoroughly evaluating whether the models retain their abilities in other languages" (Section 8)
- Why unresolved: The base models (Qwen3, LFM2) support many languages, but evaluation was limited to French and English benchmarks; degradation or preservation in other multilingual capabilities is unknown.
- What evidence would resolve it: Benchmarking Luth models on standardized multilingual benchmarks (e.g., mmmlu, xcopa, xnli) covering 10+ languages before and after adaptation to quantify retention or degradation.

## Limitations

- The methodology hasn't been validated on languages other than French and English, limiting claims about generalizability to other mid- and low-resource languages.
- The quality superiority of the Scholar subset over synthetic alternatives is asserted but not empirically validated through controlled comparisons.
- The model merging coefficients appear to be tuned per model size rather than being theoretically derived, suggesting potential overfitting to the test setup.

## Confidence

- **High confidence**: The core mechanism that French-specialized SFT on curated data improves French performance is well-supported by direct comparisons and ablation studies.
- **Medium confidence**: The claim that model merging recovers English performance while improving French is supported but depends on careful coefficient tuning that isn't fully explained.
- **Medium confidence**: The cross-lingual transfer claim (French training improving English benchmarks) is intriguing but based on limited benchmark evidence without theoretical explanation.
- **Low confidence**: The quality superiority of the Scholar subset over synthetic alternatives is asserted but not empirically validated through controlled comparisons.

## Next Checks

1. **Cross-lingual transfer generalization test**: Train the same pipeline on a different language (e.g., German or Spanish) and test whether improvements transfer to both the target language and English across the same benchmark suite, controlling for baseline differences.
2. **Scholar subset ablation with controlled synthetic comparison**: Create a synthetic alternative to the Scholar subset with similar size and domain coverage, then conduct head-to-head comparisons on reasoning benchmarks to validate the "quality over quantity" claim.
3. **Parameter-efficient fine-tuning comparison**: Replicate the entire pipeline using LoRA fine-tuning instead of full fine-tuning, measuring both performance trade-offs and computational efficiency differences across all model sizes.