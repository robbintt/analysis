---
ver: rpa2
title: Scaling Laws for Downstream Task Performance of Large Language Models
arxiv_id: '2402.04177'
source_url: https://arxiv.org/abs/2402.04177
tags:
- pretraining
- translation
- scaling
- dataset
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies scaling laws for downstream task performance
  in transfer learning settings, specifically focusing on machine translation. While
  prior work has examined scaling laws for pretraining loss, this research investigates
  how pretraining data size and distribution alignment affect downstream translation
  quality metrics like BLEU, ROUGE, and COMET scores.
---

# Scaling Laws for Downstream Task Performance of Large Language Models

## Quick Facts
- arXiv ID: 2402.04177
- Source URL: https://arxiv.org/abs/2402.04177
- Reference count: 40
- Primary result: Downstream translation metrics follow log-law scaling with pretraining data when distributions are aligned, but can degrade despite improving cross-entropy under misalignment.

## Executive Summary
This paper investigates how pretraining data size and distribution alignment affect downstream translation performance in transfer learning settings. The authors find that translation quality metrics (BLEU, ROUGE, COMET) improve monotonically with more pretraining data when distributions are sufficiently aligned, following a log-law scaling relationship. However, when pretraining and downstream data distributions are poorly aligned, translation scores may fluctuate or even decrease with additional pretraining data, while downstream cross-entropy continues to improve monotonically. The paper proposes a practical guide for assessing pretraining data value and demonstrates that downstream cross-entropy is not always a reliable indicator of task performance.

## Method Summary
The authors pretrain T5-3B encoder-decoder models on multilingual MC4 data with various pretraining mixtures, then finetune on WMT translation datasets. They systematically vary pretraining data composition and size, measuring BLEU/ROUGE/COMET scores and downstream cross-entropy at each checkpoint. Scaling laws are fit using Huber loss with L-BFGS optimization, comparing log-law behavior for translation metrics against power-law scaling for cross-entropy. The study examines multiple language pairs (en-de, en-fr, en-ro) and finetuning dataset sizes to characterize pretraining benefits.

## Key Results
- Translation quality metrics follow log-law scaling with pretraining data when distributions are aligned (BLEU, ROUGE, COMET improve monotonically)
- Poor distribution alignment can cause translation scores to fluctuate or degrade despite improving cross-entropy
- Downstream cross-entropy is not always a reliable indicator of task performance when alignment is moderate
- The proposed log-law scaling relationship provides good predictive accuracy for translation scores under sufficient alignment

## Why This Works (Mechanism)

### Mechanism 1
Translation quality metrics follow a log-law scaling relationship with pretraining dataset size when distributions are sufficiently aligned. The proposed law f(D_p) = (log(A·D_p^α))^β captures the relationship between pretraining data size (D_p) and downstream translation scores. Unlike cross-entropy which follows power-law scaling, task-specific metrics exhibit logarithmic growth patterns, suggesting diminishing returns in quality improvements per unit of added pretraining data. This scaling law fails when alignment score drops below ~0.7.

### Mechanism 2
Distribution alignment between pretraining and downstream data acts as a gatekeeper for monotonic scaling behavior. The Translation Alignment Score quantifies language overlap, and when alignment is insufficient, continued pretraining can actively harm downstream translation scores even as cross-entropy improves. This occurs because distribution mismatch causes model capacity to be allocated to less transferable patterns, leading to non-monotonic behavior in translation scores.

### Mechanism 3
Downstream cross-entropy is an unreliable proxy for task performance when alignment is moderate. Cross-entropy follows power-law scaling regardless of alignment, measuring next-token prediction accuracy. However, BLEU/COMET scores measure n-gram precision and semantic quality respectively, and these objectives can diverge when the model optimizes for probability distributions that don't align with translation-specific patterns. Observed divergence between monotonic cross-entropy decrease and non-monotonic task metric behavior signals unreliable proxy relationship.

## Foundational Learning

- **Transfer Learning Pipeline (Pretraining → Finetuning)**: The entire paper analyzes scaling in a transfer setting where unsupervised pretraining precedes supervised finetuning on translation tasks. Quick check: Can you explain why the relationship between pretraining data size and downstream performance is non-trivial compared to pretraining loss?

- **Scaling Laws (Power Laws vs Log Laws)**: The paper distinguishes between power-law scaling (cross-entropy) and log-law scaling (task metrics)—understanding this distinction is essential for interpreting results. Quick check: What functional form does cross-entropy follow as pretraining data scales, and how does this differ from BLEU/COMET scaling?

- **Translation Quality Metrics (BLEU, ROUGE, COMET)**: The paper's central claim is that these metrics behave differently from cross-entropy; BLEU measures n-gram precision, COMET uses neural evaluation models. Quick check: Why might n-gram precision (BLEU) and perplexity (cross-entropy) diverge even when both are computed on the same translation output?

## Architecture Onboarding

- **Component map**: MC4 pretraining data → T5-3B pretraining (24 encoder/decoder layers, 1024 embed dim, 32 heads) → periodic checkpoints → WMT finetuning → BLEU/ROUGE/COMET/Cross-entropy evaluation → scaling law fitting

- **Critical path**: Select pretraining data mixture → compute alignment score A(D,T) → pretrain for N steps, periodically checkpointing → finetune each checkpoint on downstream task → record (tokens_seen, translation_score) pairs → fit log-law if monotonic; diagnose misalignment if non-monotonic

- **Design tradeoffs**: High alignment (A=1.0) vs moderate alignment (A=0.7): High alignment enables predictable scaling but requires balanced multilingual data; moderate alignment is cheaper but risks non-monotonic behavior. Small finetuning dataset vs large: Small datasets show stronger pretraining benefit; large datasets may eliminate pretraining value entirely. Cross-entropy monitoring vs task metrics: Cross-entropy is cheaper to compute but can mislead; BLEU/COMET are reliable but require finetuning checkpoints.

- **Failure signatures**: Translation scores decrease or fluctuate with more pretraining tokens (red curves in Figure 1); scaling law fitting fails to converge (non-monotonic data points); cross-entropy improves while BLEU/COMET stagnates or degrades; finetuning dataset sufficiently large such that pretraining provides negligible gain (solid lines in Figure 2)

- **First 3 experiments**: 
  1. Alignment validation: Pretrain on 50%-50% en-fr MC4 mixture (A=1.0) and 100% en-MC4 (A=0.7), finetune on WMT-15 en-fr with 42M tokens. Verify that the aligned mixture produces monotonic BLEU scaling while unaligned shows break point.
  2. Finetuning data sensitivity: Hold pretraining data fixed, vary finetuning dataset size (6M, 31M, 3B tokens for en-de task). Confirm that pretraining benefit diminishes as finetuning data increases.
  3. Cross-entropy vs BLEU divergence check: For en-fr translation with 100% en pretraining, plot BLEU vs cross-entropy on log-log scale. Identify the region where the exponential relationship (Gordon et al., 2021) breaks down and document the pretraining token threshold.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can a theoretically grounded metric replace the heuristic "Translation Alignment Score" proposed in the paper? The current definition relies on hand-picked coefficients (e.g., 0.7, 0.8) and simple token overlap, which may not capture deeper linguistic nuances required for reliable scaling predictions. A formal alignment metric derived from linguistic theory that consistently predicts the monotonicity or breakdown of scaling laws across a wider variety of language pairs would resolve this.

- **Open Question 2**: Why does pretraining on destination language data yield better translation performance than source language data? For en→de/fr tasks, pretraining on 100% de/fr data outperforms 100% en data, which is counter-intuitive without a theoretical or mechanistic explanation regarding why target-side pretraining is more beneficial than source-side.

- **Open Question 3**: Do the proposed log-laws for downstream metrics generalize to decoder-only architectures and non-translation tasks? The experiments are restricted to T5 (encoder-decoder) models and translation/SuperGLUE tasks, and the paper does not verify if these laws hold for causal decoder-only models or distinct domains like coding.

## Limitations

- The paper uses language-level overlap (Translation Alignment Score) as a proxy for pretraining-downstream alignment, which may not fully capture semantic or task-relevant alignment
- The proposed log-law scaling relationship assumes a specific functional form that may be an empirical fit specific to the T5-3B architecture and translation task combination
- The scaling behavior could potentially depend on finetuning hyperparameters, optimization stability, or whether adapters versus full fine-tuning are used

## Confidence

- **High Confidence**: The observation that cross-entropy can improve while translation metrics degrade under misalignment is well-supported by experimental evidence
- **Medium Confidence**: The log-law scaling relationship shows good predictive accuracy in the paper's experiments, but its robustness across different tasks, model sizes, and alignment scenarios needs further validation
- **Low Confidence**: The claim that downstream cross-entropy is "not always a reliable indicator of task performance" requires careful qualification—the paper shows this is true for translation metrics specifically, but the relationship may hold better for other task types

## Next Checks

1. **Cross-Domain Alignment Test**: Validate whether the alignment score methodology generalizes beyond language overlap by testing on domain-specific translation tasks (e.g., medical, legal) where pretraining and downstream distributions may share languages but differ in terminology and style.

2. **Architecture Scaling Verification**: Test whether the log-law scaling relationship holds for different model architectures (e.g., GPT-style decoders, BERT-style encoders) and sizes (1B, 10B, 100B parameters) to establish the law's architectural independence.

3. **Finetuning Hyperparameter Sensitivity**: Systematically vary finetuning learning rates, optimization algorithms, and training duration to determine whether the observed scaling behaviors are robust to finetuning choices or whether they introduce confounding factors in the scaling analysis.