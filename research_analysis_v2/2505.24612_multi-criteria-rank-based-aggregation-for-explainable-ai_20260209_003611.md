---
ver: rpa2
title: Multi-criteria Rank-based Aggregation for Explainable AI
arxiv_id: '2505.24612'
source_url: https://arxiv.org/abs/2505.24612
tags:
- explanations
- stability
- methods
- aggregate
- explainer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of aggregating multiple local
  explanation models in Explainable AI (XAI) to create more robust explanations. The
  authors propose a multi-criteria rank-based weighted aggregation method that balances
  multiple quality metrics simultaneously.
---

# Multi-criteria Rank-based Aggregation for Explainable AI

## Quick Facts
- arXiv ID: 2505.24612
- Source URL: https://arxiv.org/abs/2505.24612
- Reference count: 40
- Authors: Sujoy Chatterjee; Everton Romanzini Colombo; Marcos Medeiros Raimundo
- Primary result: Multi-criteria rank-based aggregation using TOPSIS-weighted WSUM achieves top-2 performance across multiple explanation quality metrics

## Executive Summary
This paper addresses the challenge of combining multiple local explanation models to create more robust and reliable explanations in Explainable AI. The authors propose a multi-criteria rank-based weighted aggregation method that balances three quality metrics simultaneously: complexity (NRC), faithfulness, and stability. The method transforms existing XAI metrics into rank-based versions, uses an autoencoder-based approach to generate realistic noise for stability calculations, and employs MCDM algorithms (TOPSIS and EDAS) to assign weights to component explanations. Rank aggregation algorithms (WSUM, Condorcet, and BordaFuse) then combine the explanations. Experiments on five datasets demonstrate that the aggregate explainer consistently achieves the best or second-best performance across key metrics while avoiding worst-case scenarios.

## Method Summary
The method proposes a multi-criteria rank-based weighted aggregation approach for local explanation models in XAI. The core innovation involves transforming three existing XAI metrics (complexity, faithfulness, and stability) into rank-based versions to better evaluate ranked feature importance explanations. The aggregation process uses an autoencoder to generate realistic noise for stability calculations, then applies MCDM algorithms (TOPSIS and EDAS) to compute weights for each explainer based on their performance across the three metrics. Finally, rank aggregation algorithms (WSUM, Condorcet, and BordaFuse) combine the explanations using the computed weights. The method was evaluated on five datasets using three component explainers (LIME, SHAP, ANCHOR) with both Random Forest and MLP predictor models.

## Key Results
- TOPSIS and WSUM algorithms achieved the best performance for this use case, with the aggregate explainer consistently ranking first or second across multiple metrics
- The aggregate explainer avoided worst-case performance scenarios while maintaining strong average performance across all datasets
- Autoencoder-based noise generation produced more realistic stability evaluations compared to traditional Gaussian noise approaches
- The rank-based metric transformations successfully normalized evaluation across explainers with different output scales

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Aggregating multiple explainers with MCDM-weighted scores produces more consistent rankings across evaluation metrics than any single explainer.
- **Mechanism:** TOPSIS computes per-explainer weights by measuring distance to an ideal solution across three criteria simultaneously (NRC, faithfulness, stability). These weights then scale each explainer's contribution during WSUM rank aggregation, ensuring that weaker explainers contribute less but still provide signal.
- **Core assumption:** Explainer performance on the three rank-based metrics is predictive of overall explanation quality, and linear combination of weighted ranks preserves relative feature importance meaningfully.
- **Evidence anchors:**
  - [abstract] "Comparative analyses of various multi-criteria decision-making and rank aggregation algorithms showed that TOPSIS and WSUM are the best candidates for this use case."
  - [section VI] Tables III-VII show aggregate explainer achieving rank 1-2 on NRC and faithfulness while never ranking worst on stability across 5 datasets.
  - [corpus] Weak direct evidence; corpus focuses on XAI evaluation pitfalls but does not test aggregation methods.
- **Break condition:** If criteria weights are highly unequal or one metric dominates decision-making, MCDM may over-weight a poor explainer that excels at one metric only.

### Mechanism 2
- **Claim:** Rank-based metrics reduce sensitivity to explainer-specific scaling differences compared to raw importance scores.
- **Mechanism:** By converting feature importance scores to ranks before aggregation (using squared inverse 1/rank² as the aggregation value), the method normalizes across explainers with different output scales (e.g., SHAP values vs. LIME weights vs. ANCHOR coverage ratios).
- **Core assumption:** Feature ordering is more stable and meaningful than absolute importance magnitudes across different explanation methods.
- **Evidence anchors:**
  - [abstract] "We propose rank-based versions of existing XAI metrics (complexity, faithfulness and stability) to better evaluate ranked feature importance explanations."
  - [section V.A] Equations 1-3 define NRC, rank-based faithfulness, and rank-based stability using rank transformations.
  - [corpus] No direct validation; corpus papers do not compare rank-based vs. score-based aggregation.
- **Break condition:** If top features have nearly identical importance scores (rank ties), rank-based methods may arbitrarily order them, reducing fidelity to true importance differences.

### Mechanism 3
- **Claim:** Autoencoder-based noise generation produces more realistic stability evaluations than Gaussian noise for tabular data.
- **Mechanism:** An autoencoder learns a latent representation of the training data. For stability testing, K-nearest neighbors in latent space are found, and a subset of feature values are swapped with neighbor values. This preserves categorical feature validity and data distribution structure.
- **Core assumption:** Perturbations that remain within the training data distribution are more appropriate for testing stability than random noise that may create out-of-distribution samples.
- **Evidence anchors:**
  - [abstract] "The proposed method uses an autoencoder-based approach to generate noisy data for stability calculations."
  - [section V.A, "Generating noise on tabular data..."] "This method not only ensures that categorical features are handled appropriately, but also provides a more intuitive way to control the noise level."
  - [corpus] No direct comparison of autoencoder vs. Gaussian noise in stability metrics found.
- **Break condition:** If the autoencoder fails to learn a meaningful latent space (undertrained or insufficient capacity), neighbors may not represent realistic perturbations, compromising stability metric validity.

## Foundational Learning

- **Concept:** Multi-Criteria Decision Making (MCDM) — specifically TOPSIS and EDAS
  - **Why needed here:** These algorithms convert multi-dimensional performance matrices into scalar weights; understanding normalization, ideal/negative-ideal solutions, and distance metrics is required to interpret why certain explainers get higher weights.
  - **Quick check question:** Given a 3×3 decision matrix (3 explainers × 3 metrics), can you manually compute the TOPSIS relative closeness score for one explainer?

- **Concept:** Rank aggregation algorithms (Borda, Condorcet, weighted sum)
  - **Why needed here:** The final explanation is a merged ranking; each algorithm has different properties (majority preference vs. point accumulation vs. weighted averaging) that affect which features rise to the top.
  - **Quick check question:** For three rankings [A>B>C], [B>C>A], [C>A>B], what is the Condorcet winner? What does Borda count produce?

- **Concept:** Local explanation methods (LIME, SHAP, ANCHOR)
  - **Why needed here:** Each explainer has different assumptions and output formats; the aggregation method must handle these differences. ANCHOR produces rule-based explanations that require conversion to feature importance scores.
  - **Quick check question:** Why might LIME and SHAP produce different feature rankings for the same prediction on the same model?

## Architecture Onboarding

- **Component map:** Predictor model -> Explainer suite (LIME, SHAP, ANCHOR) -> Autoencoder -> Rank-based metric evaluators -> MCDM module -> Rank aggregator

- **Critical path:**
  1. Train predictor on dataset
  2. Train autoencoder on same data (500 epochs, MSE loss)
  3. For each instance to explain:
     - Generate explanations from all component explainers
     - Evaluate each explanation on NRC, faithfulness, stability
     - Run MCDM to get weights
     - Run rank aggregation with those weights
  4. Validate aggregate explanation against same three metrics

- **Design tradeoffs:**
  - **Equal vs. differential criteria weights:** Paper uses equal weights (1/3 each) due to lack of consensus; domain-specific weighting may improve stability but requires justification
  - **Number of explainers:** Three used (LIME, SHAP, ANCHOR); adding more increases computational cost linearly but may improve robustness
  - **α parameter in NRC (currently 0.5):** Controls rank dispersion penalty; higher α penalizes explanations with flat importance distributions

- **Failure signatures:**
  - Aggregate explanation ranks worst on all three metrics → MCDM weights may be corrupted or rank aggregation implementation error
  - High variance in aggregate rankings across similar instances → check autoencoder latent space quality or stability metric implementation
  - SHAP dominates weights consistently → TOPSIS may be over-weighting stability (SHAP's strength); consider differential criteria weights

- **First 3 experiments:**
  1. **Baseline replication:** Replicate Table IIIa (German dataset, TOPSIS, 10 samples) to verify aggregate explainer achieves average rank ~1.6-2.4 across metrics
  2. **Ablation — single explainer removal:** Remove ANCHOR and re-run; compare aggregate performance to understand marginal contribution of each explainer
  3. **Criteria weight sensitivity:** Set weights to [0.5, 0.25, 0.25] favoring faithfulness; measure impact on aggregate NRC and stability rankings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does assigning differential weights to evaluation criteria (complexity, faithfulness, stability) improve the aggregation performance, particularly regarding stability?
- Basis in paper: [explicit] The authors state in the Limitations section that "further study is needed to understand the impact of differential weights, particularly to improve performance on stability."
- Why unresolved: The current study assumed equal weights due to insufficient information on criteria importance.
- What evidence would resolve it: A sensitivity analysis measuring the trade-offs between complexity, faithfulness, and stability when using non-equal MCDM criteria weights.

### Open Question 2
- Question: How does varying the hyperparameter $\alpha$ in the Normalized Rank-based Complexity (NRC) metric affect the trade-off between lower-ranked feature importance and rank dispersion?
- Basis in paper: [explicit] The paper notes that $\alpha=0.5$ was selected based on preliminary experiments to maintain a nominal penalty, implying a lack of comprehensive validation.
- Why unresolved: The specific value was chosen heuristically rather than optimized for each dataset.
- What evidence would resolve it: Experiments benchmarking NRC scores against ground-truth complexity while varying $\alpha$ across different feature distributions.

### Open Question 3
- Question: Can visualization techniques be developed to enhance the information content of the aggregate rank-based explanations beyond a simple list?
- Basis in paper: [explicit] The authors identify the "Lack of visual explanations" as a limitation, noting that the rank output does not add information compared to a list.
- Why unresolved: The method focuses on robustness metrics rather than user interface or cognitive load.
- What evidence would resolve it: Designing visual representations of the aggregated ranks and validating them through user studies measuring comprehension speed and accuracy.

## Limitations

- **Rank-based metrics vs. score-based:** The paper assumes rank-based metrics are superior for normalization but lacks direct comparison to raw score-based aggregation methods
- **Equal criteria weighting:** The choice of equal weights (1/3 each) for NRC, faithfulness, and stability is pragmatic but may not reflect optimal trade-offs for different use cases
- **Human interpretability validation:** The method focuses on quantitative metrics but lacks user studies to validate whether high-scoring explanations are actually more interpretable to humans

## Confidence

- **High confidence:** The empirical demonstration that aggregate explanations consistently rank in top-2 positions across multiple datasets and metrics
- **Medium confidence:** The effectiveness of TOPSIS-weighted WSUM aggregation specifically, as results show this combination works well but alternatives weren't exhaustively tested
- **Medium confidence:** The rank-based metric transformations preserve meaningful evaluation signals, though this transformation hasn't been validated against raw score metrics

## Next Checks

1. **Ablation study on criteria weights:** Systemmatically vary the weights assigned to NRC, faithfulness, and stability to determine if equal weighting is optimal or if domain-specific weighting improves results

2. **Direct noise generation comparison:** Compare autoencoder-based stability metrics against Gaussian noise perturbations to quantify the practical benefit of the more complex approach

3. **Human evaluation correlation:** Test whether explanations that score highly on the proposed rank-based metrics also receive higher human interpretability scores in user studies