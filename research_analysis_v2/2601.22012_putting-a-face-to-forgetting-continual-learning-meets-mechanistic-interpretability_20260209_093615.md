---
ver: rpa2
title: 'Putting a Face to Forgetting: Continual Learning meets Mechanistic Interpretability'
arxiv_id: '2601.22012'
source_url: https://arxiv.org/abs/2601.22012
tags:
- feature
- task
- features
- forgetting
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces a mechanistic framework to understand catastrophic\
  \ forgetting in continual learning by analyzing feature-level transformations. It\
  \ proposes that forgetting arises from two primitive transformations\u2014rotations\
  \ and scaling\u2014of feature vectors, which can degrade allocated capacity or cause\
  \ readout misalignment."
---

# Putting a Face to Forgetting: Continual Learning meets Mechanistic Interpretability

## Quick Facts
- arXiv ID: 2601.22012
- Source URL: https://arxiv.org/abs/2601.22012
- Reference count: 40
- This work introduces a mechanistic framework to understand catastrophic forgetting in continual learning by analyzing feature-level transformations.

## Executive Summary
This paper presents a mechanistic framework to understand catastrophic forgetting in continual learning by analyzing feature-level transformations. The authors propose that forgetting arises from two primitive transformations—rotations and scaling—of feature vectors, which can degrade allocated capacity or cause readout misalignment. Using a tractable feature-reader model, the framework formalizes these dynamics and identifies best- and worst-case forgetting scenarios based on task similarity and probe alignment. Experiments validate theoretical predictions, showing that depth exacerbates forgetting through feature fading and that overlapping feature sets lead to greater capacity degradation. The framework is scaled to practical models using crosscoders, applied to a Vision Transformer trained on sequential CIFAR-10, revealing that feature fading and readout misalignment dominate forgetting, especially in later layers.

## Method Summary
The paper introduces a mechanistic framework analyzing catastrophic forgetting through feature-level transformations. It proposes two primitive transformations—rotations and scaling—of feature vectors that degrade allocated capacity or cause readout misalignment. The framework uses a tractable feature-reader model with shared encoder and task-specific probes to formalize these dynamics. To scale to practical models, the authors develop crosscoders—SAE variants with shared latent space across checkpoints—applied to a Vision Transformer trained on sequential CIFAR-10. The analysis tracks feature transformations through metrics like F-Accuracy, F-Gamma, F-Norm, and F-Capacity Norm across layers and tasks.

## Key Results
- Feature fading (scaling) is the dominant forgetting mechanism in deep networks, particularly at later layers
- Best-case forgetting occurs with disjoint feature sets and orthogonal probes; worst-case with overlapping features and correlated probes
- Earlier layers can gain capacity over time while later layers lose it, showing depth-driven reallocation of representational resources

## Why This Works (Mechanism)

### Mechanism 1: Feature Vector Transformations Drive Forgetting
- Claim: Forgetting arises from two primitive transformations to feature vectors—rotations and scaling—that degrade allocated capacity or cause readout misalignment.
- Mechanism: Rotation of feature vector ϕi toward ϕj increases overlap (ϕᵀᵢϕⱼ), reducing allocated capacity Ci for both. Scaling (particularly fading, ∥ϕi∥→0) weakens representation strength. Even without capacity loss, transformations can misalign downstream readouts (rᵀᵢa(x) ≉ fi(x)).
- Core assumption: Linear representation hypothesis holds—features are encoded as linear directions in activation space.
- Evidence anchors:
  - [abstract] "These transformations can lead to forgetting by reducing the allocated capacity of features (worse representation) and disrupting their readout by downstream computations."
  - [section 3.1] Formal definitions of overlap, fading, and readout misalignment with capacity formula Ci = (ϕᵀᵢϕᵢ)²/Σⱼ(ϕᵀᵢϕⱼ)²
  - [corpus] Related work (Measuring Representational Shifts in CL) corroborates representation-focused forgetting analysis.
- Break condition: If features are not linearly encoded (multidimensional representations), or if superposition is absent, the geometric decomposition may not hold.

### Mechanism 2: Depth Exacerbates Feature Fading
- Claim: Deeper networks suffer more severe capacity degradation, primarily through fading rather than overlap.
- Mechanism: In deep linear networks, scaling feature magnitudes provides a simpler optimization path than coordinated rotations across layers. This causes feature norms to decrease during task transitions, especially for shared-but-misaligned features.
- Core assumption: Depth effect generalizes from linear model to practical architectures.
- Evidence anchors:
  - [section 4.1] "In full, deeper encoders markedly reduce feature norms" (Figure 4).
  - [section 5.1] ViT experiments show fading dominates forgetting at penultimate layer, with norms decreasing after task completion.
  - [corpus] Related work on width/depth effects (Mirzadeh et al., 2022; Guha & Lakshman, 2024) shows architectural factors influence forgetting, though without mechanistic decomposition.
- Break condition: If non-linearities enable different optimization dynamics, or if explicit norm-preserving regularization is applied, fading may be mitigated.

### Mechanism 3: Task Similarity Determines Forgetting Bounds
- Claim: Best-case forgetting occurs with disjoint feature sets and orthogonal probes; worst-case with overlapping active subspaces and correlated probes.
- Mechanism: Theorem 3.3 shows features absent from new task receive ≈0 gradient pressure. Theorem 3.4 quantifies exact loss change: ΔL(A) = ½∥αv(B) - v(A)∥²_Σ, where α captures probe alignment and v captures feature regression coefficients.
- Core assumption: Fixed probes (no probe adaptation during task B), which the paper acknowledges overestimates forgetting.
- Evidence anchors:
  - [section 3.3] Formal derivation of best-case (inactive features, orthogonal probes) and worst-case (aligned probes, misaligned v vectors).
  - [section 4.1] Figure 2 confirms "none" condition (disjoint features) shows no accuracy forgetting, "full" condition (shared features) shows complete loss.
  - [corpus] Prior work (Ramasesh et al., 2021; Lee et al., 2021) observed U-shaped forgetting-similarity relationship without mechanistic explanation.
- Break condition: If probes co-adapt with features (Corollary B.2), gradient load is shared and forgetting is overestimated by fixed-probe analysis.

## Foundational Learning

- Concept: Linear representation hypothesis
  - Why needed here: Entire framework assumes features are linear directions in activation space (a(x) = Σᵢfᵢ(x)ϕᵢ).
  - Quick check question: Can you express a layer's activation as a weighted sum of feature vectors?

- Concept: Superposition and allocated capacity
  - Why needed here: Features compete for finite dimensions; capacity Ci quantifies how "cleanly" a feature is represented given overlap.
  - Quick check question: Why does Ci decrease when two feature vectors have positive cosine similarity?

- Concept: Readout vectors vs. feature vectors
  - Why needed here: Distinguishes inherent readability (capacity) from actual readout quality; misalignment occurs when rᵢ doesn't track changes in ϕᵢ.
  - Quick check question: Can a feature have high allocated capacity but poor readout quality?

## Architecture Onboarding

- Component map: Synthetic data (20,000 samples, 80 features) -> Feature-reader model (shared encoder Φ, 5 task-specific heads) -> ViT (224×224, patch 16, hidden 128, 6 layers) -> Crosscoder (TopK SAE, dict size 192) -> Feature metrics (F-Accuracy, F-Gamma, F-Norm, F-Capacity Norm)

- Critical path: Train sequential tasks → extract crosscoder at each checkpoint → compute decoder vectors W_dec^{m,t} as feature proxies → track γᵢ, ∥ϕᵢ∥, and Ci across tasks

- Design tradeoffs:
  - Fixed vs. co-adapting probes: Fixed probes isolate feature dynamics but overestimate forgetting (Corollary B.2)
  - SAE dictionary size: Larger d_cross improves feature separation but increases computational cost
  - Crosscoder regularization λ: Higher values push non-shared features toward zero, improving isolation but potentially losing task-specific features

- Failure signatures:
  - Near-zero F-Capacity Norm change but large F-Gamma drop → fading dominant, not rotation
  - Earlier layers gaining capacity while later layers lose it → depth-driven reallocation (Figure 9)
  - Shared features between early tasks showing norm increases → feature reinforcement, not forgetting

- First 3 experiments:
  1. Replicate feature-reader model with 5 synthetic tasks; verify "full" vs. "none" conditions match Figure 2 (accuracy and capacity metrics)
  2. Add depth to encoder (1→9 layers) on "full" condition; confirm fading (F-Norm) increases with depth while overlap (F-Capacity Norm) remains secondary
  3. Train crosscoder on ViT Split CIFAR-10 checkpoints; plot γᵢ, ∥ϕᵢ∥, Ci for top-5 features per task at layers -1, -3, -6. Validate that penultimate layer shows fading-dominant forgetting while earlier layers show capacity gain

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the preference for scaling over coordinated rotations in deep linear networks be formally proven as the dominant optimization pathway for minimizing loss?
- Basis: [explicit] Section 4.1 notes that depth causes severe fading and states, "We leave a formal analysis of this hypothesis to future work."
- Why unresolved: The paper empirically observes that deeper encoders reduce feature norms (fading) rather than rotating them, but lacks a theoretical derivation for why scaling is the preferred solution.
- What evidence would resolve it: A formal proof showing that gradient descent in deep linear networks minimizes loss more efficiently through magnitude scaling than directional rotation.

### Open Question 2
- Question: How does the feature-centric framework of rotations and scaling explain the transient performance drops known as the "stability gap" in continual learning?
- Basis: [explicit] Section 6 suggests that "in future work, the framework could be used to gain such understanding about other continual learning phenomena, such as the stability gap."
- Why unresolved: The current work focuses on catastrophic forgetting (permanent degradation) rather than the temporary plasticity loss observed during task transitions.
- What evidence would resolve it: An analysis tracking feature capacity and alignment dynamics specifically during the transient phase of learning a new task to identify the mechanistic cause of the temporary gap.

### Open Question 3
- Question: To what degree does weight decay specifically induce feature fading in inactive task subspaces, thereby acting as a catalyst for catastrophic forgetting?
- Basis: [explicit] Section 3.3 identifies that "weight decay introduces fading in inactive or weakly contributing features... making some optimizer choices a potential risk factor."
- Why unresolved: While the theoretical mechanism is identified, the specific contribution of weight decay to forgetting (versus the standard gradient pressure of new tasks) is not isolated or quantified in the experiments.
- What evidence would resolve it: Ablation studies comparing feature fading (F-Norm) and accuracy retention in models trained with and without weight decay on the inactive features of previous tasks.

## Limitations
- Probe adaptation assumption may overestimate forgetting severity as practical CL typically involves co-adapting probes
- Framework relies on linear representation hypothesis which may not hold for all neural network architectures
- Crosscoder fidelity depends on SAE's ability to accurately recover shared feature directions across checkpoints

## Confidence
- High Confidence: Feature fading as dominant forgetting mechanism in deep networks; capacity degradation patterns in synthetic feature-reader model; best-case/worst-case bounds based on feature overlap
- Medium Confidence: Depth effect generalization from linear models to ViT; relative importance of fading vs. rotation in practical architectures; crosscoder's ability to capture all relevant feature dynamics
- Low Confidence: Exact quantitative contribution of each mechanism in real CL scenarios; impact of probe adaptation; generalization to non-ViT architectures and different CL protocols

## Next Checks
1. **Probe Co-adaptation Experiment**: Modify the feature-reader model to allow probe adaptation during task transitions. Compare forgetting severity and feature dynamics against the fixed-probe baseline to quantify the overestimation effect.

2. **Non-linear Activation Test**: Replace the linear encoder in the feature-reader model with a non-linear architecture (e.g., multi-layer perceptron). Analyze whether feature transformations remain decomposable into rotations and scalings or if new forgetting mechanisms emerge.

3. **Crosscoder Ablation Study**: Systematically vary dictionary size, regularization strength, and layer selection in the crosscoder. Evaluate reconstruction quality and feature tracking accuracy to establish robust parameter ranges for reliable mechanistic analysis.