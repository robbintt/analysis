---
ver: rpa2
title: 'Video-As-Prompt: Unified Semantic Control for Video Generation'
arxiv_id: '2510.20888'
source_url: https://arxiv.org/abs/2510.20888
tags:
- video
- reference
- generation
- semantic
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Video-As-Prompt (VAP) addresses the challenge of unified, generalizable
  semantic control in video generation by treating reference videos as unified prompts
  for in-context generation. It uses a frozen Video Diffusion Transformer (DiT) augmented
  with a plug-and-play Mixture-of-Transformers (MoT) expert to interpret video prompts
  without catastrophic forgetting.
---

# Video-As-Prompt: Unified Semantic Control for Video Generation

## Quick Facts
- **arXiv ID:** 2510.20888
- **Source URL:** https://arxiv.org/abs/2510.20888
- **Reference count:** 40
- **Primary result:** Achieves 38.7% user preference rate, rivaling leading commercial models for semantic-controlled video generation

## Executive Summary
Video-As-Prompt (VAP) introduces a unified approach to semantic-controlled video generation by treating reference videos as in-context prompts. The method employs a frozen Video Diffusion Transformer (DiT) augmented with a trainable Mixture-of-Transformers (MoT) expert to process reference videos without catastrophic forgetting. A temporally biased position embedding removes spurious pixel-mapping priors for robust context retrieval. Trained on VAP-Data (over 100K paired videos across 100 semantic conditions), VAP demonstrates strong zero-shot generalization and supports various downstream applications, achieving performance rivaling leading commercial models.

## Method Summary
VAP processes reference videos as unified prompts for semantic control by encoding them into token sequences alongside noisy target video tokens and captions. A frozen Video DiT processes the target while a parallel Mixture-of-Transformers expert processes the reference prompt, communicating via full attention to enable semantic transfer without pixel-wise mapping. The approach uses temporally biased Rotary Position Embeddings (RoPE) that shift reference tokens before target tokens temporally, eliminating false pixel-level mapping priors. The model is trained on VAP-Data with over 100K paired videos across 100 semantic conditions, using a Flow Matching objective with the expert as the only trainable component to prevent catastrophic forgetting.

## Key Results
- Achieves 38.7% user preference rate compared to leading commercial models
- Strong zero-shot generalization demonstrated on unseen semantic conditions
- Supports diverse downstream applications including style transfer, motion guidance, and concept learning
- Maintains semantic alignment while preserving base model generative capabilities

## Why This Works (Mechanism)

### Mechanism 1: In-Context Semantic Transfer
Treating reference videos as in-context prompts enables unified semantic control across diverse conditions without task-specific designs. The model encodes reference and target videos into token sequences, allowing semantic information to transfer through token interactions rather than requiring pixel-level correspondence.

### Mechanism 2: MoT Architecture for Catastrophic Forgetting Prevention
The Mixture-of-Transformers design with parallel expert and frozen backbone isolates the new in-context learning task to the expert, preserving the pre-trained DiT's generative capabilities while learning to route semantic guidance.

### Mechanism 3: Temporally Biased RoPE for Context Retrieval
Temporally biased Rotary Position Embeddings shift reference tokens before target tokens temporally, removing spurious pixel-mapping priors and signaling the model to treat references as prior context rather than aligned frames.

## Foundational Learning

- **In-Context Learning in Transformers**: Why needed - VAP's core mechanism relies on learning to use reference videos as prompts to influence generation. Quick check - Can you explain how concatenating reference tokens before target tokens enables conditional generation?

- **Catastrophic Forgetting in Transfer Learning**: Why needed - The MoT architecture prevents the pre-trained DiT from losing generative abilities when adapted for semantic control. Quick check - If you fine-tuned the entire Video DiT, what negative outcome would you expect?

- **Rotary Position Embeddings (RoPE)**: Why needed - Understanding how RoPE encodes relative position via rotation is necessary to grasp why shifting indices changes the model's prior. Quick check - Why would applying the same RoPE indices to reference and target imply a pixel-wise mapping prior?

## Architecture Onboarding

- **Component map**: VAE Encoder/Decoder -> Pre-trained Video DiT (frozen) -> In-Context DiT Expert (trainable) -> Full Attention Layer -> RoPE Layer -> Text Encoders

- **Critical path**: 1) Encode reference/target videos and captions, noise target 2) Apply temporally biased RoPE with offset Î” 3) Process reference through expert, target through frozen DiT with full bidirectional attention 4) Backbone outputs noise prediction, VAE decoder generates final video

- **Design tradeoffs**: MoT preserves backbone quality but doubles parameters/inference cost vs efficient finetuning that risks forgetting; bidirectional guidance outperforms unidirectional; synthetic data enables controlled alignment but limits real-world diversity

- **Failure signatures**: Copy-paste artifacts (appearance leakage), identity loss (subject mismatch), temporal instability (flickering motion)

- **First 3 experiments**: 1) Verify in-context copying with frame-from-reference test 2) Ablation comparing MoT vs single-branch finetuning 3) Zero-shot test on held-out semantic condition

## Open Questions the Paper Calls Out

- How does training on large-scale, real-world video data compare to synthetic data regarding mitigation of inherited stylistic biases and artifacts?
- To what extent do instruction-style captions improve semantic alignment and control precision compared to standard descriptive captions?
- What specific architectural or positional embedding designs are required to support robust multi-reference prompting without causing feature blending?

## Limitations

- Relies on synthetic VAP-Data with only 100 semantic conditions, limiting real-world diversity
- Performance depends heavily on reference video quality and semantic clarity
- MoT architecture doubles inference cost compared to simple finetuning
- True open-world generalization to arbitrary semantic prompts remains untested

## Confidence

- **Unified semantic control via video-as-prompt**: High - Well-justified by in-context learning literature with strong ablation support
- **Catastrophic forgetting prevention**: Medium - Ablation shows benefit but long-term stability across many tasks untested
- **Robust zero-shot generalization**: Medium - Limited held-out tests show promise but true open-world generalization untested
- **Superior user preference**: Medium - 38.7% rate reported but exact comparison conditions and testing protocol unclear

## Next Checks

1. **Open-set semantic generalization test**: Evaluate on unseen conditions (Origami, Breakdance, Slow-motion waterfall) compared to strong baseline to measure true zero-shot transfer

2. **Reference video quality sensitivity analysis**: Systematically vary reference video quality and ambiguity to quantify impact on semantic alignment and artifact rates

3. **Long-term catastrophic forgetting probe**: Test frozen backbone performance on original video generation tasks after fine-tuning to directly validate capability preservation