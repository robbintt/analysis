---
ver: rpa2
title: 'Preventing Another Tessa: Modular Safety Middleware For Health-Adjacent AI
  Assistants'
arxiv_id: '2509.07022'
source_url: https://arxiv.org/abs/2509.07022
tags:
- safety
- prompts
- input
- output
- patterns
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper tackles the unsafe\u2011behaviour of health\u2011adjacent\
  \ chatbots exemplified by the NEDA Tessa incident, where lack of safety engineering\
  \ let the system dispense harmful weight\u2011loss advice. It proposes a lightweight,\
  \ modular safety middleware that fuses a deterministic keyword/regex gate with an\
  \ in\u2011line LLM\u2011based policy filter, returning a single JSON verdict (issafe)\
  \ and enforcing a fail\u2011closed rule that routes unsafe queries to human escalation."
---

# Preventing Another Tessa: Modular Safety Middleware For Health-Adjacent AI Assistants  

## Quick Facts  
- **arXiv ID:** 2509.07022  
- **Source URL:** https://arxiv.org/abs/2509.07022  
- **Reference count:** 3  
- **Primary result:** A single‑call JSON safety verdict blocks 100 % of malicious health‑adjacent prompts with 0 % false positives while incurring no extra latency or cost.  

## Executive Summary  
Health‑adjacent conversational agents can cause real harm when they dispense unsafe advice, as illustrated by the NEDA “Tessa” incident. Reddy & Reddy propose a lightweight, modular safety middleware that sits in‑line with the language model. The middleware first applies a deterministic keyword/regex gate, then runs an LLM‑based policy filter, and finally emits a single JSON flag (`is_safe`). Unsafe queries are automatically routed to human escalation (fail‑closed). In a synthetic benchmark of 100 balanced prompts, the system achieved perfect interception of malicious requests without degrading response time or cost, outperforming conventional multi‑stage safety pipelines.  

## Method Summary  
The approach couples two complementary safeguards:  

1. **Deterministic gate** – a fast regex/keyword matcher that catches obvious unsafe patterns (e.g., “rapid weight loss”, “dangerous diet”).  
2. **Inline LLM policy filter** – a single LLM call that evaluates the full user utterance against a safety policy and returns a JSON verdict (`{ "is_safe": true/false }`).  

The middleware is inserted between the user front‑end and the downstream generative model. If `is_safe` is false, the request is diverted to a human operator; otherwise it proceeds to the normal LLM response generation. The design eliminates the need for separate post‑generation filters, keeping the inference path a single call.  

## Key Results  
- **True‑positive rate:** 100 % (all 50 malicious prompts blocked).  
- **False‑positive rate:** 0 % (all 50 safe prompts passed).  
- **Latency & cost:** Matched baseline LLM inference; no additional infrastructure required.  

## Why This Works (Mechanism)  
- **Last‑mile explicit check:** By placing a deterministic gate followed by an LLM policy filter immediately before generation, unsafe content is intercepted before any token is emitted.  
- **Single‑call JSON verdict:** Consolidates safety decision into one lightweight response, avoiding the latency overhead of multi‑stage pipelines.  
- **Fail‑closed escalation:** Guarantees that any flagged query is never answered by the model, removing the risk of accidental release.  

## Foundational Learning  
| Concept | Why needed | Quick check |
|---------|------------|-------------|
| Keyword/regex gating | Captures high‑confidence unsafe patterns with near‑zero latency. | Verify that a test phrase (“lose 20 kg in 2 weeks”) is flagged. |
| LLM policy filter | Handles nuanced, context‑dependent safety judgments that regex cannot. | Run a few borderline prompts and confirm the JSON verdict aligns with expert labeling. |
| JSON safety protocol | Provides a machine‑readable, atomic decision that downstream components can trust. | Parse the output with a simple JSON parser; ensure `is_safe` field exists. |
| Fail‑closed routing | Prevents any unsafe generation even if downstream components malfunction. | Simulate a false `is_safe` and confirm the request is sent to a mock human queue. |
| Prompt‑injection robustness | Guards against adversarial phrasing that could bypass the gate. | Craft an obfuscated unsafe prompt and verify it is still blocked. |
| Latency profiling | Confirms the middleware truly adds no overhead. | Measure end‑to‑end response time with and without the middleware on identical hardware. |

## Architecture Onboarding  

**Component map**  
User Query → Keyword/Regex Gate → Inline LLM Policy Filter → JSON Verdict Generator → Decision Router (Safe → LLM response, Unsafe → Human escalation)  

**Critical path**  
The request traverses the deterministic gate, then a single LLM inference that produces the JSON verdict; the verdict immediately determines the routing decision.  

**Design trade‑offs**  
- *Simplicity vs coverage*: Deterministic gate is fast but limited; the LLM filter adds coverage at modest compute cost.  
- *Single‑call latency vs multi‑stage safety*: Consolidating checks into one call removes extra round‑trips but places all safety responsibility on that call.  
- *Fail‑closed strictness vs user experience*: Aggressive escalation avoids harm but may increase human workload.  

**Failure signatures**  
- Missed unsafe content (false negative) → unsafe response generated.  
- JSON parsing error → middleware crashes or defaults to unsafe routing.  
- Gate bypass (e.g., malformed input) → unsafe prompt reaches LLM.  

**First three experiments**  
1. **Gate unit test:** Feed a curated list of known unsafe phrases; confirm the regex gate flags them before the LLM call.  
2. **End‑to‑end synthetic benchmark:** Run the full middleware on the 100‑prompt balanced set; record TP/FP rates and compare latency to baseline.  
3. **Latency & cost profiling:** Deploy the middleware on the target hardware (e.g., A100 GPU) and measure per‑request latency and token‑cost versus a vanilla LLM call.  

## Open Questions the Paper Calls Out  
- How does the middleware perform on large, real‑world health‑assistant logs beyond the 100‑prompt synthetic set?  
- What are the precise latency and cost implications across diverse deployment stacks (CPU vs GPU, batch sizes, model sizes)?  
- How robust is the inline LLM policy filter to adversarial prompt‑injection, multilingual phrasing, or obfuscation techniques?  

## Limitations  
- Evaluation uses only 100 synthetic prompts, which may not capture the diversity of real user queries.  
- Latency and cost claims lack detailed hardware or profiling methodology.  
- Robustness of the JSON‑based filter to sophisticated adversarial attacks is not empirically validated.  

## Confidence  
| Claim | Confidence |
|-------|------------|
| 100 % true‑positive and 0 % false‑positive interception on the synthetic set | Medium |
| No added inference latency or cost compared with a vanilla LLM | Low |
| Fail‑closed escalation reliably prevents unsafe output in production | Low |

## Next Checks  
1. **Scale‑up benchmark:** Test the middleware on ≥5 k real health‑assistant prompts, measuring true‑positive, false‑negative, and false‑positive rates.  
2. **Latency & cost profiling:** Replicate the experiment on the intended deployment stack (GPU/CPU, batch size, token limits) and report end‑to‑end response time and per‑token cost versus the baseline model.  
3. **Robustness stress test:** Generate adversarial prompts (prompt‑injection, obfuscation, multilingual variants) to probe the LLM policy filter and verify that the fail‑closed path triggers reliably.