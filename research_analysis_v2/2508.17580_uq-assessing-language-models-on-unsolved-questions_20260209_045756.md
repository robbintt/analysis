---
ver: rpa2
title: 'UQ: Assessing Language Models on Unsolved Questions'
arxiv_id: '2508.17580'
source_url: https://arxiv.org/abs/2508.17580
tags:
- uq-logo-2
- questions
- stanford
- users
- kenziyuliu
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: UQ introduces a new evaluation paradigm by assessing models on
  unsolved questions sourced from Stack Exchange. A three-stage pipeline combines
  rule-based filters, LLM judges, and human review to curate 500 high-quality, challenging
  questions.
---

# UQ: Assessing Language Models on Unsolved Questions

## Quick Facts
- arXiv ID: 2508.17580
- Source URL: https://arxiv.org/abs/2508.17580
- Reference count: 40
- 500 high-quality unsolved questions curated from Stack Exchange; top models validate correct answers on only 15% of questions

## Executive Summary
UQ introduces a novel evaluation paradigm for language models by assessing them on genuinely unsolved questions sourced from Stack Exchange. A three-stage pipeline combines rule-based filters, LLM judges, and human review to curate 500 high-quality, challenging questions. To handle the absence of ground-truth answers, UQ-Validators employ compound strategies leveraging the generator-validator gap, achieving up to 73% validation accuracy on surrogate data. Experiments show frontier models pass UQ-Validator on only 15% of questions, with human review confirming very few correct answers. UQ provides a dynamic, community-based benchmark for real-world, open-ended challenges, advancing both model evaluation and human knowledge.

## Method Summary
UQ curates a dataset of 500 unsolved questions through a three-stage pipeline: rule-based filtering of 3M+ unanswered questions from 80 Stack Exchange sites based on engagement metrics, LLM-based filtering using GPT-4o for answer generation and o4-mini for quality judgment, and final human review by PhD-level annotators. UQ-Validators employ compound validation strategies that leverage the generator-validator gap, using a 3-iter pipeline with cycle consistency, fact/logic checks, and correctness verification, each with 3× unanimous voting. The framework enables oracle-free evaluation of candidate answers without ground truth, validated through surrogate datasets like Humanity's Last Exam.

## Key Results
- UQ-Validators achieve 73% accuracy and 20% precision on HLE surrogate data
- Top model (o3-pro) passes validation on only 15% of UQ questions
- 3-iter validation pipeline achieves 81.65% accuracy and 30.99% precision using o3 as judge
- Compound validation strategies improve performance over one-shot prompting baselines
- Human review confirms very few validated answers are actually correct

## Why This Works (Mechanism)

### Mechanism 1: Generator-Validator Gap Enables Oracle-Free Evaluation
LLMs are systematically better at validating candidate answers than generating them, and this asymmetry increases with model capability. Stronger models develop more robust internal verification circuits that can detect errors in others' outputs even when they cannot produce correct outputs themselves. The paper shows o3 achieves 20% answer accuracy but 65% validation accuracy on the same questions, and this gap transfers across datasets.

### Mechanism 2: Compound Validation Hierarchies Reduce False Positives
Stacking multiple validation strategies (low-level checks → mid-level refinement → high-level aggregation) improves precision at the cost of recall, which is preferable for unsolved questions where false positives are costly. Each stage filters progressively, catching different types of errors at increasing levels of abstraction.

### Mechanism 3: Unsolved Questions Resolve Difficulty-Realism Tension
Unsolved questions from real Q&A platforms are inherently difficult (no known solution exists) AND realistic (posed by humans with genuine information needs), avoiding the tradeoff in existing benchmark paradigms. They naturally satisfy both properties because they emerge organically from human information-seeking while remaining unsolved despite expert attention.

## Foundational Learning

- **Generator-Validator Asymmetry**:
  - Why needed here: Understanding why validation is easier than generation is crucial for designing oracle-free evaluators
  - Quick check question: On a difficult question you cannot answer, can you still recognize when someone else's answer is wrong? What cues do you use?

- **Precision-Recall Tradeoff in High-Stakes Validation**:
  - Why needed here: UQ-Validators prioritize precision (low false positives) over recall because human verification is costly
  - Quick check question: In a medical diagnosis system, would you prefer fewer false positives or fewer false negatives? How does the answer change when verification requires invasive testing?

- **Engagement Signals as Quality Proxies**:
  - Why needed here: UQ filters 3M+ questions to 500 using engagement (views, votes, age)
  - Quick check question: On Stack Exchange, why might a question with 1000 views and 50 upvotes over 3 years be harder than one with 100 views and 5 upvotes over 1 month? What confounding factors might weaken this inference?

## Architecture Onboarding

- **Component map**:
  Raw crawl (3M questions from 80 Stack Exchange sites) → Rule-based filtering (engagement thresholds) → LLM-based filtering (GPT-4o generation + o4-mini judgment) → Human review (PhD-level annotators) → 3-iter UQ-Validator pipeline → UQ-Platform (community verification)

- **Critical path**: The LLM-based filtering stage is the bottleneck, requiring ~4-5 API calls per candidate question (answer generation + three quality judgment calls)

- **Design tradeoffs**:
  - Unanimous vs. majority voting: Unanimous improves precision (30.99% vs 25.87%) but slashes recall (34.38% vs 56.92%)
  - Pipeline depth: 5-iter pipeline doesn't improve over 3-iter (81.50% vs 81.65% accuracy) but costs more
  - Multi-model ensembles: 2-model pipeline (o3 + Gemini 2.5 Pro) achieves best precision (40%) but doubles inference cost

- **Failure signatures**:
  - Self-bias: Simple validators overrate answers from same model family
  - Hallucinated citations: Validators fail to catch fabricated references
  - Ranking instability: Weak validators produce inconsistent model rankings

- **First 3 experiments**:
  1. Validate generator-validator gap on your domain with 100 questions with known answers
  2. Ablate validation pipeline stages: run 1-iter, 2-iter, 3-iter pipelines on held-out set with ground truth
  3. Test self-bias in your validator: compare validation accuracy across different answer-generator/validator pairs

## Open Questions the Paper Calls Out

### Open Question 1
Can oracle-free validators be designed with fine-grained control over the precision–recall tradeoff, analogous to confidence thresholding in probabilistic classifiers?

### Open Question 2
What is the optimal generator-validator interaction strategy when the same system both produces and evaluates candidate solutions?

### Open Question 3
Can domain-specific validation strategies (e.g., proof assistants for math, test suites for code) be systematically integrated into UQ-Validators to improve precision?

### Open Question 4
How can validator performance be reliably evaluated without access to surrogate datasets, which may not match the target distribution?

## Limitations
- 70% false-positive rate indicates fundamental limits to oracle-free evaluation
- Self-bias remains a concern even with compound validation strategies
- Performance transfer from surrogate datasets to truly unsolved questions is unverified

## Confidence
- **High Confidence**: Generator-validator gap exists and scales with model capability
- **Medium Confidence**: Engagement-based filtering successfully identifies genuinely difficult questions
- **Low Confidence**: Compound validation strategies achieve claimed precision gains

## Next Checks
1. Test UQ-Validator performance on questions from domains with no overlap to UQ's source domains
2. Calculate the false-positive rate × human verification cost vs. potential knowledge gain
3. Track how many UQ questions get solved in the next 6-12 months and compare validator predictions to actual community answers