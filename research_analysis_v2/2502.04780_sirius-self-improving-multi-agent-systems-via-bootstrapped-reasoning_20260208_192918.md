---
ver: rpa2
title: 'SiriuS: Self-improving Multi-agent Systems via Bootstrapped Reasoning'
arxiv_id: '2502.04780'
source_url: https://arxiv.org/abs/2502.04780
tags:
- player
- agent
- reasoning
- arxiv
- multi-agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses optimizing multi-agent AI systems by improving
  credit assignment across agents and enabling self-improvement through learning from
  successful and failed interactions. The proposed SIRIU S framework constructs an
  experience library of successful reasoning trajectories and augments unsuccessful
  ones with feedback to generate additional training data.
---

# SiriuS: Self-improving Multi-agent Systems via Bootstrapped Reasoning

## Quick Facts
- **arXiv ID:** 2502.04780
- **Source URL:** https://arxiv.org/abs/2502.04780
- **Reference count:** 40
- **Primary result:** 2.86% to 21.88% performance improvements on reasoning and biomedical QA tasks using experience library construction and trajectory augmentation

## Executive Summary
SiriuS addresses the challenge of optimizing multi-agent AI systems by constructing experience libraries from successful reasoning trajectories and augmenting failed ones with feedback-driven regeneration. The framework enables self-improvement through supervised fine-tuning on specialized agent roles without requiring fine-grained human supervision. Tested across problem-solving, actor-critic, and competitive negotiation settings, SiriuS demonstrates consistent performance gains over baselines while sidestepping the multi-agent credit assignment problem.

## Method Summary
SiriuS implements a self-improving multi-agent system that builds experience libraries from successful task completions and augments failed trajectories through feedback-driven regeneration. The framework operates on directed acyclic graphs defining agent interactions, where each agent receives inputs from predecessors and generates outputs sequentially. Successful trajectories are retained per-agent, while failed attempts undergo augmentation: an external critic generates feedback, the original agent regenerates a solution incorporating this feedback, and successful regenerations are rephrased to remove correction artifacts before joining the experience library. Each agent then undergoes supervised fine-tuning on its specialized trajectory set, with role specialization enabling more effective collaboration than single-agent or mixed-role approaches.

## Key Results
- **PubMedQA performance:** 74.20% accuracy with GPT-3.5-turbo (2.86% improvement over base model)
- **Competitive negotiation:** 10.77% average utility improvement in resource exchange and ultimatum games
- **Actor-Critic accuracy:** 35.00% True Positive rate for solution validation, up from 11.80% baseline

## Why This Works (Mechanism)

### Mechanism 1: Success-Based Trajectory Filtering for Experience Library Construction
The system collects complete reasoning trajectories from successful task completions, treating entire successful interaction chains as positive examples. This approach sidesteps multi-agent credit assignment by assuming successful trajectories contain useful collaboration patterns even without identifying specific crucial steps.

### Mechanism 2: Feedback-Grounded Trajectory Augmentation
Failed trajectories are augmented through external feedback generation, solution regeneration incorporating the feedback, and rephrasing to remove correction traces. This process substantially expands training data while preserving solution quality through multiple regeneration attempts.

### Mechanism 3: Role-Specialized Joint Fine-Tuning
Distinct agent models are fine-tuned for specialized roles using their respective trajectory sets. This approach outperforms single-agent systems and single LLMs trained on combined multi-role data, suggesting different agent roles require meaningfully different reasoning patterns.

## Foundational Learning

- **Multi-agent credit assignment problem:** Understanding this challenge explains why SiriuS avoids per-step supervision entirely. *Quick check:* Given a 3-agent pipeline (Physicist → Mathematician → Summarizer) that produces a correct final answer, can you determine which agent's contribution was most critical?

- **Bootstrapping reasoning from self-generated data:** SiriuS builds on STaR paradigm of iteratively training on self-generated reasoning chains. *Quick check:* If a model generates a correct answer with flawed intermediate reasoning, should that trajectory be included in training data?

- **Topological ordering in DAGs:** The paper formalizes multi-agent communication as a directed graph where sequential action sampling follows topological order. *Quick check:* Given agents {A, B, C, D} with edges (A→B), (A→C), (B→D), (C→D), what is a valid topological order?

## Architecture Onboarding

- **Component map:** Agent pool A(1)...A(N) with role-specific prompts → Interaction graph G defining predecessor relationships → Experience library with per-agent trajectory sets C(n) → Augmentation module with external critic feedback → Fine-tuning loop via SFT on C(n) sets

- **Critical path:** Initialize policies → sample actions via topological order → evaluate trajectories → store successful ones in C(n) → augment failed trajectories → SFT each agent on accumulated C(n) → repeat for T iterations

- **Design tradeoffs:** Augmentation ratio vs. quality (higher expansion risks feedback issues); number of agents vs. coordination overhead (more agents enable specialization but complicate credit assignment); ground truth access (Actor-Critic setting vs. Problem-Solving setting requirements)

- **Failure signatures:** Judgment agent errors misclassifying correct answers as incorrect; TextGrad failures on long contexts due to instruction-following limitations; competitive setting asymmetry showing role-specific bias in pre-trained negotiation behavior

- **First 3 experiments:**
  1. Reproduce PubMedQA with 2-agent setup (Context Analyst → Problem Solver) using provided prompts
  2. Ablate augmentation depth by varying max_feedback_tries (1, 2, 3) to measure marginal gains
  3. Test generalization in competitive setting by training on (25X, 5Y) vs (5X, 25Y) and evaluating on (35X, 15Y) vs (15X, 35Y)

## Open Questions the Paper Calls Out

The paper implicitly leaves several questions unresolved through its methodology choices and experimental design. The experience library construction mechanism sidesteps direct credit assignment, leaving untested whether incorporating explicit per-step supervision could improve sample efficiency. The competitive setting experiments demonstrate improved payoffs against specific baselines but do not analyze long-term strategy stability or robustness against diverse opponent strategies. Additionally, the framework's performance in cold-start scenarios where initial success rates approach zero remains unexplored, as the method relies on successful trajectories to seed the experience library.

## Limitations

- **Experience library construction limitations:** Relies on successful trajectories to seed learning, potentially limiting effectiveness in cold-start scenarios where base model success rates are near zero
- **Feedback quality uncertainty:** Augmentation mechanism assumes high-quality feedback generation without systematic analysis of feedback accuracy or impact on trajectory quality
- **Specialization granularity unclear:** Role specialization benefits lack systematic study of optimal agent count, leaving uncertain how specialization value scales with task complexity

## Confidence

- **High confidence:** Experience library construction mechanism - consistent ablation results show 0.8-2.0% absolute improvements when disabled
- **Medium confidence:** Augmentation mechanism - quantitative improvements shown but limited analysis of feedback quality impact
- **Medium confidence:** Role specialization benefits - ablation confirms specialization helps but lacks systematic study of optimal granularity

## Next Checks

1. **Feedback quality analysis:** Instrument augmentation pipeline to measure feedback accuracy and identify patterns where incorrect feedback degrades trajectory quality in experience library
2. **Specialization sensitivity study:** Systematically vary agent count (2-5 agents) on PubMedQA to determine whether gains from specialization diminish with additional agents
3. **Generalization stress test:** Evaluate trained SiriuS agents on out-of-distribution resource configurations in competitive settings to verify learned strategies transfer beyond training scenarios