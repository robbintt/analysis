---
ver: rpa2
title: 'P2W: From Power Traces to Weights Matrix -- An Unconventional Transfer Learning
  Approach'
arxiv_id: '2502.14968'
source_url: https://arxiv.org/abs/2502.14968
tags:
- accuracy
- data
- power
- training
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents P2W, a novel transfer learning approach that
  extracts knowledge from machine learning models running on embedded systems without
  direct access to their parameters. The method captures power consumption traces
  from a target SoC executing a trained ML model and uses an encoder-decoder deep
  neural network to translate these traces into an approximated weights matrix.
---

# P2W: From Power Traces to Weights Matrix -- An Unconventional Transfer Learning Approach

## Quick Facts
- **arXiv ID:** 2502.14968
- **Source URL:** https://arxiv.org/abs/2502.14968
- **Reference count:** 40
- **One-line primary result:** P2W achieves up to 97% accuracy on binary/ternary classification tasks by extracting knowledge from proprietary ML models via power traces.

## Executive Summary
P2W presents a novel transfer learning approach that extracts knowledge from machine learning models running on embedded systems without direct access to their parameters. The method captures power consumption traces from a target SoC executing a trained ML model and uses an encoder-decoder deep neural network to translate these traces into an approximated weights matrix. This matrix serves as a starting point to initialize and fine-tune new ML models for similar tasks.

Experiments demonstrate that P2W significantly improves model accuracy compared to traditional training methods using limited datasets. When applied to binary and ternary classification tasks with balanced datasets, the approach achieved up to 97% accuracy compared to 37% without P2W. The method also showed robustness to imbalanced training data, maintaining high accuracy and F1-score when datasets had uneven class distributions.

## Method Summary
P2W operates in three phases: (1) EDNN training on surrogate models using power traces captured from a programmable clone SoC, (2) power analysis on the target SoC to capture traces from the black-box model, and (3) initialization of a new model with the extracted weights matrix followed by fine-tuning. The EDNN architecture consists of a convolutional encoder that compresses power traces and a transposed convolutional decoder that reconstructs the weights matrix. The approach assumes knowledge of the target model's topology and access to a clone SoC with similar hardware characteristics.

## Key Results
- P2W achieved up to 97% accuracy on binary and ternary classification tasks compared to 37% without P2W initialization
- The method demonstrated robustness to imbalanced training data, maintaining high accuracy and F1-score when datasets had uneven class distributions
- EDNN initialization provided up to 60% knowledge transfer before fine-tuning, reducing the data requirements for effective model training

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If a clone SoC is available, the non-linear relationship between power consumption traces and weight matrices can be learned by an Encoder-Decoder Deep Neural Network (EDNN).
- **Mechanism:** The system trains the EDNN on a programmable *clone* SoC using surrogate models with known weights. The encoder compresses power traces, and the decoder reconstructs the weight matrix. This mapping is then applied to the *target* SoC's traces.
- **Core assumption:** The power consumption characteristics of the clone SoC statistically approximate those of the target SoC, and the ML model topology is known a priori.
- **Evidence anchors:**
  - [Section 4.1] describes Algorithm 1 which trains the EDNN using pairs of power traces (x) and weights (y) from the clone SoC.
  - [Section 5.4] demonstrates that EDNN-initialized models achieve significant accuracy (>60% knowledge transfer) even before fine-tuning.
  - [Corpus] General transfer learning concepts (e.g., [arxiv:2601.21873]) rely on matrix transformations, but P2W uniquely derives these matrices from physical signals rather than software access.
- **Break condition:** The hardware variation between the clone and target SoC exceeds the EDNN's generalization capability, or the signal-to-noise ratio in the power trace is insufficient for the specific processor architecture.

### Mechanism 2
- **Claim:** Initializing a new model with an approximated weights matrix derived from power traces creates a better loss landscape basin than random initialization, particularly for small datasets.
- **Mechanism:** Instead of starting from a random distribution, the P2W output initializes the weights to a state structurally similar to the target model. Fine-tuning then requires fewer epochs and less data to converge.
- **Core assumption:** The "approximated" weights preserve the topological feature extraction capabilities of the original model despite potential numerical precision loss from the side-channel extraction.
- **Evidence anchors:**
  - [Abstract] reports improving accuracy from 37% (random init) to 97% (P2W init).
  - [Section 4.2] Equation 5 formalizes the transfer: `newML_final = Train(newML_init(W), D_small)`.
  - [Corpus] No direct corpus evidence for side-channel weight initialization; standard literature assumes direct weight access.
- **Break condition:** The approximation error in the weight matrix acts as adversarial noise, pushing the optimization into a poor local minimum compared to random initialization.

### Mechanism 3
- **Claim:** P2W reduces overfitting to class imbalances in small datasets by biasing the model toward the feature representation learned by the (presumably well-trained) target model.
- **Mechanism:** The pre-acquired knowledge acts as a regularizer. The model focuses its limited capacity on adjusting decision boundaries rather than learning low-level features from scratch, which is where imbalanced data typically causes skew.
- **Core assumption:** The target model was trained on a balanced or representative dataset, or at least possesses robust feature extractors.
- **Evidence anchors:**
  - [Section 5.6] shows F1-score drops of only 7% with P2W on imbalanced data vs. 72% without.
  - [Figure 5] visualizes the stability of P2W against the variance of imbalanced training.
  - [Corpus] [arxiv:2510.15837] supports the general principle that auxiliary structures mitigate data heterogeneity, though P2W uses physical extraction as the source.
- **Break condition:** The target model itself was overfitted to a specific bias, which P2W faithfully extracts and transfers to the new model.

## Foundational Learning

- **Concept:** Side-Channel Analysis (Specifically Power Analysis)
  - **Why needed here:** P2W relies on the physical reality that processing logic (0s vs 1s) draws different currents. You must understand that power traces are not random noise but correlated data leaks.
  - **Quick check question:** Can you explain how the Hamming weight of data processed by a CPU register influences the instantaneous power consumption?

- **Concept:** Encoder-Decoder Architectures (Autoencoders)
  - **Why needed here:** The core engine of P2W is an EDNN that "translates" a time-series signal (trace) into a spatial matrix (weights). Understanding the bottleneck (latent space) is key to tuning the compression.
  - **Quick check question:** What happens to the reconstruction fidelity if the latent space of the EDNN is too small to encode the weight matrix?

- **Concept:** Transfer Learning vs. Fine-tuning
  - **Why needed here:** P2W is a method to *enable* transfer learning without software access. You need to distinguish between "initializing weights" (transfer) and "training the head" (fine-tuning).
  - **Quick check question:** Why is freezing layers during fine-tuning potentially dangerous when the source weights (from P2W) are approximations rather than exact copies?

## Architecture Onboarding

- **Component map:** Target SoC -> Acquisition HW -> Clone SoC -> EDNN Pipeline -> New Model
- **Critical path:** The **Data Preparation step in Phase 1** (Algorithm 1). If the dataset $D$ created on the Clone SoC does not correlate with the Target SoC's behavior, the EDNN will never converge to the correct weights.
- **Design tradeoffs:**
  - **Sampling Rate vs. PCA Compression:** Higher sampling rates capture more detail but generate massive traces requiring aggressive PCA reduction (Section 4.1), which might discard subtle weight signatures.
  - **EDNN Complexity vs. Overfitting:** A deeper EDNN captures complex trace-weight mappings but may overfit to the Clone SoC's specific noise floor, failing to generalize to the Target SoC.
- **Failure signatures:**
  - **Flat Accuracy (~37%):** The EDNN failed to extract weights; the new model is effectively randomly initialized.
  - **High Training Accuracy / Low Validation Accuracy:** The extracted weights are valid, but $D_{small}$ is too imbalanced (Section 5.6) or the fine-tuning learning rate is too high, destroying the transferred features.
- **First 3 experiments:**
  1. **Clone Sanity Check:** Train the EDNN on the clone SoC. Verify if the EDNN can reconstruct the weights of the clone model itself from a *known* trace (Phase 1 validation).
  2. **Topology Ablation:** Vary the topology of the surrogate model (e.g., change neuron count in Model 1) to test if the EDNN can handle weight matrices of different dimensions.
  3. **Noise Robustness:** Add synthetic Gaussian noise to the power traces from the Target SoC before feeding them into the EDNN to simulate cheaper measurement hardware or environmental interference.

## Open Questions the Paper Calls Out
- **Open Question 1:** Can the P2W approach effectively scale to extract weight matrices from very large machine learning models, given the method's current validation on relatively small embedded models?
  - **Basis in paper:** [explicit] The authors state in the "Future work" section that studying the "scalability of the P2W approach to very large models could be a valuable path for future research."
  - **Why unresolved:** The current experiments utilized MLPs with limited layers (e.g., 128 neurons), and it is unclear if the EDNN architecture or the power trace resolution can handle the complexity and parameter volume of large-scale models.
  - **What evidence would resolve it:** Successful reconstruction and fine-tuning of a large model (e.g., a deep CNN or Transformer) using power traces captured from an embedded SoC.

- **Open Question 2:** How does the parallel processing nature of hardware architectures like FPGAs and GPUs impact the quality of power traces and the subsequent weight extraction accuracy?
  - **Basis in paper:** [explicit] The authors explicitly identify "exploring the effectiveness of P2W on different types of hardware, such as FPGAs and GPUs" as a promising direction for future research.
  - **Why unresolved:** The study currently relies on a sequential ARM Cortex-M4 processor; parallel execution on FPGAs or GPUs may obfuscate the sequential data-dependent power consumption patterns the current EDNN relies upon.
  - **What evidence would resolve it:** Experimental results showing P2W's reconstruction accuracy when deployed on an FPGA or GPU-based embedded platform.

- **Open Question 3:** Is the P2W method effective for Convolutional Neural Networks (CNNs), given that the experimental evaluation was restricted to Multi-Layer Perceptrons (MLPs)?
  - **Basis in paper:** [inferred] The introduction claims the approach works for "MLP or CNN," but Section 5.1 specifies that the surrogate and target models used for validation were "fully connected neural networks" (MLPs).
  - **Why unresolved:** While the theory suggests applicability to CNNs, the paper provides no empirical evidence that the EDNN can successfully approximate the shared weights and filters characteristic of convolutional layers.
  - **What evidence would resolve it:** Demonstration of the P2W pipeline successfully initializing and fine-tuning a CNN model using power traces.

## Limitations
- **Hardware Dependence:** The approach assumes the clone SoC is a near-identical replica of the target SoC. Any architectural variation may break the trace-to-weights mapping.
- **Model Topology Assumption:** The method requires knowledge of the target model's architecture in advance and cannot infer unknown layer sizes or activation functions.
- **Trace Acquisition Fidelity:** Real-world environmental noise, clock jitter, or electromagnetic interference could degrade trace quality beyond the EDNN's noise tolerance.

## Confidence
- **High Confidence:** The EDNN can learn a mapping from power traces to weights on the clone SoC (validated by Section 5.4's >60% knowledge transfer claim).
- **Medium Confidence:** The extracted weights improve initialization for small datasets (supported by accuracy gains from 37% to 97%, but only for simple MLP architectures).
- **Low Confidence:** The method generalizes to imbalanced data and robust against varying SoC noise floors (Section 5.6 shows promising F1-stability, but limited to 3 binary/ternary tasks).

## Next Checks
1. **Cross-SoC Transfer:** Test P2W when the clone and target SoCs are from different manufacturers or generations. Measure accuracy degradation as a function of hardware variation.
2. **Architecture Scaling:** Apply P2W to a target model with 10Ã— more parameters (e.g., a small CNN). Evaluate if the EDNN's latent space can compress the larger weight matrix without losing critical features.
3. **Noise Sensitivity Analysis:** Systematically degrade trace quality (lower sampling rate, added Gaussian noise) and measure the minimum SNR required for the EDNN to maintain 85% weight reconstruction accuracy.