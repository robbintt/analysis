---
ver: rpa2
title: 'Theoretical Tensions in RLHF: Reconciling Empirical Success with Inconsistencies
  in Social Choice Theory'
arxiv_id: '2506.12350'
source_url: https://arxiv.org/abs/2506.12350
tags:
- preference
- rlhf
- majority
- rule
- consistency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper examines the tension between RLHF's empirical success\
  \ and its violation of key axioms from social choice theory. The authors show that\
  \ under a mild and empirically plausible assumption on the preference data\u2014\
  namely, that each pairwise comparison is labeled by at most one annotator\u2014\
  RLHF does satisfy pairwise majority consistency and Condorcet consistency."
---

# Theoretical Tensions in RLHF: Reconciling Empirical Success with Inconsistencies in Social Choice Theory

## Quick Facts
- arXiv ID: 2506.12350
- Source URL: https://arxiv.org/abs/2506.12350
- Reference count: 40
- Primary result: Under single-annotator constraint, RLHF satisfies pairwise majority and Condorcet consistency; Copeland RLHF satisfies all four social choice axioms

## Executive Summary
This paper investigates the apparent contradiction between RLHF's empirical success and its theoretical violations of social choice axioms. The authors demonstrate that under the practical assumption that each pairwise comparison is labeled by at most one annotator, RLHF's reward modeling satisfies pairwise majority consistency and Condorcet consistency. This explains RLHF's strong performance in practice. The paper further proposes a modified reward modeling objective (Copeland RLHF) that ensures these properties even under general preference profiles, and introduces new alignment criteria tailored for learning distributions over responses.

## Method Summary
The paper analyzes RLHF through the lens of social choice theory, examining how reward modeling's maximum likelihood estimation (MLE) objective relates to voting mechanisms. Under the Bradley-Terry model, standard RLHF implements Borda count aggregation, which violates Condorcet consistency. However, when each comparison has exactly one labeler (Assumption 3.1), the pairwise preference probabilities become binary, transforming the aggregation into a mechanism that satisfies both pairwise majority and Condorcet consistency. The authors propose Copeland RLHF as a modification that ensures all four social choice axioms by aggregating multi-annotator comparisons via majority vote before training.

## Key Results
- Under single-annotator constraint (Assumption 3.1), RLHF satisfies pairwise majority consistency and Condorcet consistency
- Standard RLHF violates Pareto optimality and majority consistency under Assumption 3.1 (expected due to cyclic preferences)
- Copeland RLHF satisfies all four axioms (Pareto optimality, majority consistency, pairwise majority consistency, Condorcet consistency)
- RLHF satisfies preference matching and preference equivalence but fails group preference matching
- Group preference matching distribution p* exists and is unique for non-BT profiles with Condorcet cycles

## Why This Works (Mechanism)

### Mechanism 1: Single-Annotator Constraint Enables Axiom Satisfaction
When each (yi, yj) comparison has exactly one labeler, the pairwise preference P(yi ≻ yj) becomes binary (0 or 1). This transforms the reward modeling objective to implicitly implement a voting mechanism that correctly identifies Condorcet winners when they exist. The MLE solution satisfies ri > rj ⇔ Copeland score wi > wj.

### Mechanism 2: Copeland RLHF via Majority-Vote Aggregation
Aggregating multi-annotator comparisons via majority vote before training ensures satisfaction of all four social choice axioms. Replace individual comparison losses with a single term weighted by 1[P(yi ≻ yj) > 1/2]. This modification causes the MLE to implement the Copeland rule: ri > rj ⇔ wi > wj, where wi counts pairwise majority wins.

### Mechanism 3: Group Preference Matching Distribution
For non-BT preference profiles with Condorcet cycles, a unique group preference matching distribution p* exists and is well-defined. Partition voters into BT-embeddable subgroups, compute each subgroup's preference-matching distribution, then aggregate: p* = Σ(|Uk|/|U|)p*k.

## Foundational Learning

- **Bradley-Terry (BT) Model**
  - Why needed here: BT model P(yi ≻ yj) = exp(ri)/(exp(ri) + exp(rj)) is the foundation of standard RLHF reward modeling. Non-BT preferences (Condorcet cycles) create irreconcilable reward contradictions.
  - Quick check question: Given three responses with cyclic majority preferences (y1≻y2≻y3≻y1), can scalar rewards r1, r2, r3 satisfy all pairwise comparisons simultaneously?

- **Condorcet Winner and Condorcet Paradox**
  - Why needed here: A Condorcet winner defeats all others in pairwise majority comparisons. Its existence is the key condition for evaluating whether alignment methods respect collective preferences.
  - Quick check question: In a profile where 33% prefer A≻B≻C, 33% prefer B≻C≻A, and 33% prefer C≻A≻B, does a Condorcet winner exist?

- **Aggregation Rules (Borda vs. Copeland)**
  - Why needed here: Standard RLHF implements Borda count (sum of P(yi ≻ yk)), which violates Condorcet consistency. Copeland rule (count of majority wins) satisfies all axioms.
  - Quick check question: If candidate A defeats 3 opponents by majority and loses to 1, while B defeats 2 by landslide margins, which wins under Copeland? Under Borda?

## Architecture Onboarding

- **Component map**: Preference data (Dcomp) → Reward modeling loss (Equation 2 or 3) → MLE optimization → Scalar rewards r1,...,rn → Softmax → Distribution p over responses → Policy optimization (KL-regularized)
- **Critical path**: 1) Verify annotation structure: single-annotator per comparison or multi-annotator; 2) If multi-annotator: aggregate via majority vote before training; 3) Train reward model using Equation (3) for Copeland RLHF; 4) For distribution preservation: estimate p* ≈ #(ranked first)/m, apply weighted loss (Equation 4)
- **Design tradeoffs**: Standard RLHF vs. Copeland RLHF (Standard requires less data; Copeland needs multiple annotators); Borda vs. Copeland (Borda considers preference intensity; Copeland only considers majority direction); Practical alignment vs. theoretical guarantees (Current practices accidentally satisfy axioms; future settings require explicit Copeland modification)
- **Failure signatures**: Cyclic aggregate preferences without Condorcet winner (Standard RLHF assigns equal rewards to all candidates in symmetric cycles); Multi-annotator conflicts without aggregation (Borda behavior emerges, potentially ranking non-Condorcet winners first); Group preference mismatch (RLHF fails to preserve subgroup-level preference distributions)
- **First 3 experiments**: 1) Verify Assumption 3.1 in your annotation pipeline: Confirm each (prompt, response-pair) has ≤1 labeler; log any violations; 2) Synthetic benchmark with known Condorcet winners: Construct profiles with ground-truth Condorcet winners, compare reward rankings from standard vs. Copeland-modified loss; 3) Estimate group preference matching distribution: For diverse preference profiles, compute p* ≈ (# ranked first)/m and compare to RLHF's softmax output; measure divergence

## Open Questions the Paper Calls Out

1. **Future alignment methods**: How can future alignment methods be designed to simultaneously satisfy preference matching, preference equivalence, and group preference matching? The authors conclude by discussing this direction, noting that while RLHF satisfies the first two criteria, it fails the third (group preference matching), and no existing method is proposed that satisfies all three.

2. **Sample efficiency of Copeland RLHF**: Can the Copeland RLHF objective be adapted to require fewer labelers while maintaining its theoretical guarantees (Condorcet and pairwise majority consistency)? The conclusion lists this as a specific limitation, noting that the proposed Copeland RLHF ensures consistency under general profiles but is practically limited by its high data requirements compared to standard RLHF.

3. **Robustness of p* approximation**: Is the heuristic approximation $p_i^* \approx \#(y_i \text{ ranked first})/m$ robust enough to implement the proposed weighted loss (Eq. 4) in practice? Section 4.3 proposes this approximation but notes "We leave a more detailed analysis of this approximation to the appendix."

## Limitations
- Theoretical analysis assumes exact MLE solutions and infinite data, with limited discussion of finite-sample convergence rates
- Proposed Copeland RLHF requires multiple annotators per comparison, creating tension with current RLHF practices that typically use single annotator comparisons
- Group preference matching distribution p* is mathematically defined but practical estimation methods remain underdeveloped, relying on heuristic approximations without quantified accuracy guarantees

## Confidence
- **High confidence**: The theoretical derivation that under Assumption 3.1 (single annotator per comparison), RLHF's reward modeling satisfies pairwise majority and Condorcet consistency
- **Medium confidence**: The claim that standard RLHF violates Pareto optimality and majority consistency under Assumption 3.1 is correct but represents expected behavior given the model's assumptions about cyclic preferences
- **Medium confidence**: The proposed Copeland RLHF modification (Equation 3) is theoretically sound for ensuring all four axioms, but practical implementation challenges and finite-sample behavior require validation

## Next Checks
1. **Finite-sample convergence**: Empirically measure how quickly RLHF's reward rankings converge to theoretical predictions under varying numbers of comparisons and annotators per comparison
2. **Practical p* estimation**: Evaluate the accuracy of "#(ranked first)/m" approximation for group preference matching across different preference profile structures and determine error bounds
3. **Real-world annotation structure analysis**: Analyze actual RLHF datasets to quantify the prevalence of Assumption 3.1 violations (multiple annotators per comparison) and measure the resulting impact on axiom satisfaction