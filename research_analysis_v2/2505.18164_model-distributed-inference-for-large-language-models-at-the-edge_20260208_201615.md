---
ver: rpa2
title: Model-Distributed Inference for Large Language Models at the Edge
arxiv_id: '2505.18164'
source_url: https://arxiv.org/abs/2505.18164
tags:
- nodes
- inference
- node
- parallelism
- edge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MDI-LLM, a framework for deploying large
  language models (LLMs) at the edge by partitioning the model across multiple low-power
  devices and enabling collaborative inference. The key innovation is "recurrent pipeline
  parallelism," which minimizes idle time and allows parallel generation of multiple
  text sequences by reusing cached activation vectors (KV caching).
---

# Model-Distributed Inference for Large Language Models at the Edge

## Quick Facts
- arXiv ID: 2505.18164
- Source URL: https://arxiv.org/abs/2505.18164
- Reference count: 30
- Key outcome: Introduces MDI-LLM framework for deploying LLMs at the edge by partitioning models across multiple low-power devices, achieving higher throughput and reduced per-device memory usage through recurrent pipeline parallelism and distributed KV caching.

## Executive Summary
This paper presents MDI-LLM, a framework for deploying large language models at the edge by partitioning the model across multiple low-power devices and enabling collaborative inference. The key innovation is "recurrent pipeline parallelism," which minimizes idle time and allows parallel generation of multiple text sequences by reusing cached activation vectors (KV caching). The framework adapts KV caching and Grouped Query Attention (GQA) techniques to a distributed environment. Experiments on Nvidia Jetson TX2 boards demonstrate that MDI-LLM enables inference on models exceeding individual device memory capacity, with token generation throughput increasing and memory usage per device decreasing as the number of devices increases.

## Method Summary
MDI-LLM partitions transformer models across edge devices using a ring topology with asymmetric node roles: starter nodes handle input/output layers and token sampling, while secondary nodes process transformer blocks only. The framework implements recurrent pipeline parallelism where multiple text samples flow through the distributed pipeline simultaneously, keeping all nodes active. Distributed KV caching stores per-sample activation vectors on each node, enabling incremental extension rather than recomputation. TCP/IP socket communication with FIFO queues facilitates the ring overlay, while HTTP handles coordination. The approach specifically targets edge scenarios where models exceed single-device memory capacity.

## Key Results
- Deploying TinyLlama 1.1B across three Jetson TX2 devices reduced per-device memory usage by 1.3 GB compared to two devices
- Token generation throughput increased as the number of devices increased
- Successfully demonstrated inference on models exceeding individual device memory capacity
- Pipeline fill behavior achieved when processing multiple samples (≥ node count)

## Why This Works (Mechanism)

### Mechanism 1: Recurrent Pipeline Parallelism
Processing multiple text sequences in a pipelined manner across distributed nodes reduces idle time and increases throughput. When a node completes processing one sample and forwards it, it immediately begins processing the next sample. With n samples and n nodes, all devices remain active simultaneously after the initial pipeline fill. Core assumption: Processing time is approximately balanced across nodes; network latency is negligible compared to computation.

### Mechanism 2: Distributed KV Caching with Per-Sample Rotation
Adapting KV caching to a distributed ring reduces message sizes from O(context × embedding) to O(embedding) per forward pass. Each node maintains separate KV caches per sample. When switching samples, the active cache is swapped. Only the last token embedding is transmitted; K/V matrices are extended incrementally rather than recomputed. Core assumption: Sufficient device memory to hold multiple KV caches; cache swap overhead is small relative to computation savings.

### Mechanism 3: Ring Overlay with Starter/Secondary Node Partitioning
A ring topology with asymmetric node roles enables autoregressive feedback while keeping secondary nodes stateless about global inference. Starter node holds both input embedding layers and output logits layer, enabling it to sample tokens and feed them back. Secondary nodes process only transformer blocks and forward activations. The ring closes when the last node sends output back to starter. Core assumption: TCP/IP sockets provide sufficiently low-latency, reliable communication; starter node is not a bottleneck for token sampling.

## Foundational Learning

- **Concept: Pipeline Parallelism**
  - Why needed here: Understanding how samples flow through stages and why filling the pipeline creates a transient before steady-state throughput.
  - Quick check question: If you have 4 nodes and 2 samples, will all nodes stay busy? Why or why not?

- **Concept: KV Caching in Transformers**
  - Why needed here: Grasping why caching K/V matrices avoids redundant computation and how this changes message sizes in distributed settings.
  - Quick check question: What is the memory complexity of a KV cache as a function of context length, layers, and embedding dimension?

- **Concept: TCP/IP Socket Communication + Threading**
  - Why needed here: The implementation uses separate threads for receive/process/transmit; understanding blocking I/O vs. queue-based decoupling is essential.
  - Quick check question: Why use FIFO queues between threads rather than direct function calls?

## Architecture Onboarding

- **Component map**: HTTP coordinator -> Starter node (input/output layers + tokenizer + sampling) -> Secondary nodes (transformer blocks only) -> Ring closure back to starter

- **Critical path**: 1. Prompt encoding at starter → 2. Forward through starter's input layers → 3. Ring traversal through secondary nodes → 4. Return to starter → 5. Output logits + sampling → 6. Feed sampled token back to step 2

- **Design tradeoffs**: More nodes → lower per-device memory but higher total overhead (Python libraries, HTTP servers, communication buffers); larger batch (more samples) → better pipeline utilization but more KV cache memory; partition balance critical: imbalance causes stragglers and idle time

- **Failure signatures**: Memory overflow on one node (partition unbalanced or KV cache too large for context length); transient slowdown at generation start (normal behavior - KV cache + state initialization); throughput degrades with more nodes (likely communication overhead dominating computation)

- **First 3 experiments**: 1. Single-device baseline: Run NanoLlama (304M) on one Jetson TX2; measure tokens/sec and memory. Confirm baseline before distribution. 2. Two-node partition: Split NanoLlama (5 + 7 blocks); measure throughput and per-device memory. Verify pipeline fill behavior with 2+ samples. 3. Three-node scaling with TinyLlama: Deploy TinyLlama (1.1B, cannot fit on one device); confirm inference succeeds and measure memory reduction vs. two-node case.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MDI-LLM perform when deployed across heterogeneous edge devices with varying computational capabilities?
- Basis in paper: [inferred] The experiments use homogeneous Nvidia Jetson TX2 boards, but the paper states layer assignment "should reflect the computation capabilities of each specific device" without testing heterogeneous configurations.
- Why unresolved: Real-world edge deployments typically involve devices with different processors, memory, and speeds, which could create bottlenecks or imbalance in the pipeline.
- What evidence would resolve it: Experiments deploying MDI-LLM across mixed hardware (e.g., Jetson TX2, Nano, and Orin) with adaptive partitioning strategies.

### Open Question 2
- Question: How does MDI-LLM scale as the number of participating devices increases beyond three nodes?
- Basis in paper: [inferred] All experiments are limited to 1, 2, or 3 devices; the paper claims throughput increases and memory decreases with more devices but provides no empirical validation beyond three nodes.
- Why unresolved: Pipeline parallelism efficiency depends on the ratio of computation to communication; adding more devices may introduce latency that outweighs memory benefits.
- What evidence would resolve it: Benchmarks scaling to 5, 10, or more devices, measuring throughput, latency, and overhead trends.

### Open Question 3
- Question: What is the impact of network latency, jitter, and bandwidth limitations on MDI-LLM performance in realistic wireless edge environments?
- Basis in paper: [inferred] The testbed uses gigabit ethernet with minimal jitter, but real edge deployments often rely on WiFi or cellular links with higher variability.
- Why unresolved: The paper notes network jitter affects generation rate but does not quantify performance under degraded or variable network conditions.
- What evidence would resolve it: Experiments under controlled latency/jitter conditions or over wireless links, measuring throughput degradation.

### Open Question 4
- Question: Can the substantial Python and communication overhead be eliminated through optimized implementation, and what efficiency gains would result?
- Basis in paper: [explicit] The authors state "We believe that most of these overheads will likely disappear with an optimized implementation," noting ~450MB for libraries and 150-200MB for HTTP/communication channels.
- Why unresolved: The current prototype's overhead may obscure the true potential of recurrent pipeline parallelism; the magnitude of achievable improvement is unknown.
- What evidence would resolve it: A production-grade implementation (e.g., in C++/Rust with optimized serialization) comparing memory and throughput to the current Python prototype.

## Limitations

- Missing implementation details: No code repository or specific implementation details for socket communication protocol, message serialization, or HTTP coordination endpoints
- Limited experimental validation: Results only on Nvidia Jetson TX2 boards with two specific model sizes; no statistical variations or comparisons with alternative distributed inference approaches
- Simplified communication model: Assumes reliable TCP/IP communication; does not address fault tolerance or performance under real-world network conditions with packet loss or high latency

## Confidence

- **High Confidence**: The core concept of recurrent pipeline parallelism and distributed KV caching is theoretically sound and aligns with established distributed systems principles
- **Medium Confidence**: The specific performance improvements reported are reasonable given the experimental setup but lack statistical analysis and broader experimental scope
- **Low Confidence**: Claims about general applicability to various edge devices and network conditions are not substantiated; superiority over existing methods not directly tested

## Next Checks

1. **Implement and Validate the Ring Communication Protocol**: Create a minimal working implementation of the starter-secondary node architecture with TCP socket communication and per-sample KV cache rotation. Test with at least three nodes and verify that the pipeline stays filled when processing multiple samples. Measure communication overhead and identify potential bottlenecks in the token feedback loop.

2. **Characterize Performance Across Varying Context Lengths**: Systematically evaluate token generation throughput and memory usage as context length increases from 128 to 2048 tokens. Identify the breaking point where per-device memory becomes insufficient even with distributed KV caching. This will validate the claimed memory reduction benefits and reveal practical limitations.

3. **Test Fault Tolerance and Network Unreliability**: Simulate network partitions, packet loss, and node failures in the distributed setup. Measure the framework's ability to recover and continue inference. This validation is critical for real-world edge deployments where network conditions are often unpredictable and nodes may fail.