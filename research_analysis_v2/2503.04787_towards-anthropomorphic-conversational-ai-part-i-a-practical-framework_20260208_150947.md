---
ver: rpa2
title: 'Towards Anthropomorphic Conversational AI Part I: A Practical Framework'
arxiv_id: '2503.04787'
source_url: https://arxiv.org/abs/2503.04787
tags:
- conversational
- user
- conversation
- intelligence
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces a multi-module framework for building anthropomorphic
  conversational AI that goes beyond single LLM calls. The framework incorporates
  thinking modules for reasoning, resource modules for managing knowledge and external
  information, and response modules for contextually appropriate interactions.
---

# Towards Anthropomorphic Conversational AI Part I: A Practical Framework

## Quick Facts
- arXiv ID: 2503.04787
- Source URL: https://arxiv.org/abs/2503.04787
- Reference count: 21
- One-line primary result: Multi-module framework significantly improves anthropomorphic conversation quality without fine-tuning underlying LLM

## Executive Summary
This paper introduces a practical framework for building conversational AI systems that exhibit human-like thinking and expression. The framework decomposes anthropomorphic conversation into specialized modules with independent LLM calls, including thinking modules for reasoning, resource modules for managing knowledge and external information, and response modules for contextually appropriate interactions. Through a human evaluation study with over 3,000 conversation rounds, the framework demonstrated significant improvements in social and conversational intelligence compared to single-prompt approaches, while avoiding the need for fine-tuning the underlying large language model.

## Method Summary
The framework uses a multi-module architecture built on a base LLM (Qwen-max) without fine-tuning. It comprises three main components: a Memory Manager that handles dialog history, internal memory extraction/retrieval, and external knowledge via RAG; Awareness Managers that run in parallel to extract self-awareness (agent state) and other-awareness (user state); and Response Generators that include both quick reflexive responses and analytical responses with a rethinking loop. The system is configured with personality/identity settings and orchestrated through a workflow that updates dialog history, retrieves memory, runs parallel awareness extraction, generates responses through reflexive and analytical modes, and updates self-awareness post-response.

## Key Results
- Human evaluators rated the framework significantly higher on 8 criteria (7-point scale) including conversational intelligence (continuity, proactivity, topic transition, linguistic competence) and social intelligence (self-emotion, self-preference, user-emotion, user-preference)
- The framework achieved improved proactivity and emotional awareness without requiring fine-tuning of the underlying LLM
- Over 3,000 conversation rounds were evaluated, demonstrating consistent performance improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing anthropomorphic conversation into specialized modules with independent LLM calls improves social and conversational intelligence over single-prompt approaches.
- Mechanism: The framework isolates cognitive functions—reasoning, memory retrieval, awareness extraction, and response generation—into separate model calls. Each module receives focused instructions and relevant context, reducing task interference and allowing parallel execution. Outputs from awareness and memory modules are synthesized by response generators.
- Core assumption: Anthropomorphic behavior emerges from coordinated specialized processes rather than monolithic instruction-following; LLMs struggle to maintain multiple distinct cognitive perspectives within a single call.
- Evidence anchors:
  - [abstract]: "a single call of foundational models might be insufficient"
  - [section 3.1]: "when the LLMs are not sufficiently powerful, multi-calls are often a more effective approach... the results of one task often interfere with those of another"
  - [corpus]: Weak direct support; neighboring papers focus on emotion recognition and multi-agent systems but not module decomposition specifically
- Break condition: If underlying LLMs become sufficiently capable of maintaining multiple cognitive perspectives without interference, module overhead may outweigh benefits.

### Mechanism 2
- Claim: Parallel extraction of self-awareness (agent state) and other-awareness (user state) creates more dynamic, contextually appropriate responses than unified analysis.
- Mechanism: Two managers analyze dialog history independently—Self-Awareness Manager extracts the agent's satisfaction, planned actions, emotions, and tone; Other-Awareness Manager extracts user emotions, interests, task strategy, and conversation stage. Their outputs jointly inform response generation, enabling the agent to balance its own perspective against user needs.
- Core assumption: Human-like social intelligence requires simultaneously holding and reconciling multiple perspectives; sequential or unified extraction causes perspective bleed or oversimplification.
- Evidence anchors:
  - [section 3.3]: "conflicts can arise due to differing perspectives (agent vs. user). Executing the tasks separately helps prevent such conflicts and can enhance the dynamism of responses"
  - [abstract]: "thinking modules for reasoning... response modules for generating contextually appropriate interactions"
  - [corpus]: Indirect support from JELLY paper on joint emotion recognition and context reasoning; no direct validation of parallel awareness extraction
- Break condition: When task-oriented dialogue dominates and user/agent perspective divergence is minimal, parallel extraction adds latency without proportional gains.

### Mechanism 3
- Claim: A reflexive-analytical response loop with explicit rethinking better approximates human conversational cognition than single-pass generation.
- Mechanism: Quick Response Generator provides immediate reactions for simple inputs or buffers complex queries. Analytical Response Generator then incorporates full awareness and knowledge context. A rethinking loop evaluates whether responses adequately address requirements before finalizing.
- Core assumption: Human conversation involves both reflexive (fast, heuristic) and analytical (slow, deliberative) processing; rethinking enables self-correction and coherence.
- Evidence anchors:
  - [section 3.4]: "reflexive mode handles simple queries... analytic mode processes more complex queries"
  - [section 3.4]: "the 'think-response-rethink-response' cycle mirrors how humans respond to complex queries"
  - [corpus]: No direct corpus validation for this specific loop architecture
- Break condition: Under strict latency constraints (e.g., real-time voice), the full loop may be infeasible; simplified paths needed.

## Foundational Learning

- Concept: **Instruction-following vs. emergent behavior trade-offs in LLMs**
  - Why needed here: The paper's core argument is that detailed prompts for anthropomorphism often produce mechanical responses; understanding why LLMs over-adhere to explicit rules helps diagnose when module decomposition is necessary.
  - Quick check question: Can you explain why instructing an LLM to "be proactive" might result in repetitive questioning behavior?

- Concept: **Dialog state tracking and memory management**
  - Why needed here: The Memory Manager distinguishes dialog history (raw context) from extracted memory pieces (structured preferences, events); this separation is non-obvious and critical for avoiding stereotypical behavior.
  - Quick check question: What is the difference between injecting raw dialog history versus extracted memory pieces into a prompt, and what failure modes does each approach risk?

- Concept: **Parallel vs. sequential orchestration in multi-component systems**
  - Why needed here: The framework runs awareness managers in parallel for latency and independence; understanding orchestration trade-offs is essential for implementation.
  - Quick check question: Under what conditions would parallel execution of awareness modules produce worse outcomes than sequential execution?

## Architecture Onboarding

- Component map:
  - Memory Manager: Dialog History (raw context window) -> Internal Memory Manager (user/agent memory pieces) -> External Knowledge Manager (RAG, databases)
  - Awareness Managers: Self-Awareness Manager (agent emotions, satisfaction, planned actions) -> Other-Awareness Manager (user emotions, interests, task strategy)
  - Response Generators: Quick Response Generator (reflexive) -> Analytical Response Generator (deliberative, looped with rethinking)
  - Personality/Identity Configuration: Static settings governing tone, humor, cultural sensitivity

- Critical path:
  1. User input arrives -> Dialog History updated
  2. Internal Memory retrieves relevant prior context
  3. Self-Awareness and Other-Awareness Managers run in parallel
  4. Quick Response Generator produces initial output (may be final for simple queries)
  5. Analytical Response Generator synthesizes all inputs, produces deeper response
  6. Rethinking loop evaluates sufficiency; may trigger additional analytical passes
  7. Final response emitted; Self-Awareness Manager runs post-response for "psychodynamical inertia"

- Design tradeoffs:
  - Latency vs. anthropomorphism: More modules and loops improve quality but increase response time
  - Module independence vs. coherence: Fully isolated modules reduce interference but require careful output synchronization
  - Memory granularity: Fine-grained memory pieces improve personalization but increase retrieval complexity and storage

- Failure signatures:
  - Mechanical proactivity: Excessive questioning without genuine engagement (suggests awareness managers not influencing response strategy)
  - Repetitive phrases/emojis: Over-reliance on Personality/Identity injection without sufficient context variation
  - Emotional misalignment: Responses that ignore user emotion state (Other-Awareness Manager not properly integrated)
  - Stereotypical behavior: Agent repeatedly references configured backstory elements (memory extraction insufficiently filtering)

- First 3 experiments:
  1. **Ablation: Single-call baseline vs. full framework** — Run identical conversations with (a) single-prompt approach and (b) full multi-module framework; evaluate on proactivity, emotional awareness, and topic transition metrics to quantify framework contribution.
  2. **Parallel vs. sequential awareness extraction** — Compare response quality and latency when Self/Other-Awareness Managers run in parallel versus sequentially; measure interference rates and coherence scores.
  3. **Reflexive-only vs. reflexive+analytical** — Disable the Analytical Response Generator for a subset of conversations; evaluate where quality degrades (complex queries, emotional situations) to validate the dual-response hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effectively can the conversational data generated by this multi-module framework be utilized to train foundational LLMs via reinforcement learning?
- Basis in paper: [explicit] The paper outlines a "two-stage solution" but explicitly states that the second stage, which involves using filtered and labeled data for reinforcement learning (e.g., DPO, PPO), "is left for future work."
- Why unresolved: The current study only validates the framework's ability to generate high-quality conversational data and reasoning logs (Stage 1); it does not demonstrate the efficacy of using this data to update model weights.
- What evidence would resolve it: A follow-up study showing performance metrics of an LLM fine-tuned on this framework's data using reinforcement learning, compared to the current baseline.

### Open Question 2
- Question: How does the removal of specific modules (ablation) or the use of different foundational models impact the framework's ability to exhibit social intelligence?
- Basis in paper: [explicit] The authors explicitly state in Section 4.1 that they "haven't involved large-scale human evaluation and ablation study regarding various LLMs in this report."
- Why unresolved: The experiments were conducted exclusively with the Qwen-max model, and the specific contribution of individual modules (e.g., the Quick Response Generator vs. Analytical Response Generator) was not isolated.
- What evidence would resolve it: Results from experiments running the framework with different underlying LLMs (e.g., GPT-4, Llama) and ablation tests where individual managers (like the Awareness Manager) are disabled.

### Open Question 3
- Question: Does the multi-module approach actually reduce response latency compared to a single-call approach when accounting for the overhead of managing independent thoughts?
- Basis in paper: [inferred] The authors claim in Section 3.1 that a multi-call approach offers "Better Response Time" through parallel execution, but the paper provides no latency metrics or system performance data to substantiate this claim against the overhead of multiple inference calls.
- Why unresolved: While parallel processing theoretically saves time, the cumulative latency of initializing multiple modules and synthesizing their outputs was not quantified in the experiment section.
- What evidence would resolve it: Quantitative benchmarks comparing the end-to-end latency of the multi-module framework against a single-prompt baseline handling the same complexity of context and instructions.

## Limitations
- Specific prompt templates and orchestration logic are not fully disclosed, making faithful reproduction challenging
- Evaluation relied on human raters for only 3,000 conversation rounds, which may not capture long-term behavioral consistency
- Framework's performance on languages other than English and across different cultural contexts remains untested

## Confidence
- **High confidence**: The multi-module architecture itself and its basic design principles (parallel awareness extraction, reflexive-analytical response loop) are sound and well-justified by the mechanism analysis.
- **Medium confidence**: The evaluation results showing improved social and conversational intelligence are credible but limited by sample size and rater diversity; the claim of avoiding fine-tuning is technically true but may mask prompt engineering complexity.
- **Low confidence**: The specific claim that anthropomorphic behavior emerges specifically from this module decomposition pattern (versus other architectural approaches) lacks comparative validation against alternative designs.

## Next Checks
1. **Ablation study with industrial-scale conversations**: Scale human evaluation to 10,000+ conversation rounds across diverse user demographics and cultural contexts; compare not just quality metrics but also response latency, computational cost, and long-term coherence.
2. **Cross-lingual and cross-cultural validation**: Deploy the framework in at least two non-English languages and two distinct cultural contexts (e.g., Japanese and Arabic); measure whether the anthropomorphic improvements transfer or require substantial re-engineering.
3. **Long-term behavioral consistency test**: Run continuous conversations spanning 50+ turns with the same user; analyze for mechanical patterns, memory consistency failures, or emergent stereotypes that single-session evaluations might miss.