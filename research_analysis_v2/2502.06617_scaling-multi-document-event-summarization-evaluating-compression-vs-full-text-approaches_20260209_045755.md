---
ver: rpa2
title: 'Scaling Multi-Document Event Summarization: Evaluating Compression vs. Full-Text
  Approaches'
arxiv_id: '2502.06617'
source_url: https://arxiv.org/abs/2502.06617
tags:
- llama-3
- methods
- full-context
- retrieval
- hierarchical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates compression-based and full-text approaches
  for large-scale multi-document summarization, using three datasets with approximately
  100 documents each. The study contrasts hierarchical, incremental, and retrieval
  compression methods against full-context approaches across transformers like Llama-3.1,
  Command-R, and Jamba-1.5-Mini.
---

# Scaling Multi-Document Event Summarization: Evaluating Compression vs. Full-Text Approaches

## Quick Facts
- **arXiv ID**: 2502.06617
- **Source URL**: https://arxiv.org/abs/2502.06617
- **Reference count**: 23
- **Primary result**: Full-context and retrieval methods outperform compression-based approaches for large-scale multi-document summarization, with compression methods suffering information loss despite strong intermediate recall.

## Executive Summary
This paper evaluates compression-based and full-text approaches for large-scale multi-document summarization using datasets with approximately 100 documents each. The study contrasts hierarchical, incremental, and retrieval compression methods against full-context approaches across transformers like Llama-3.1, Command-R, and Jamba-1.5-Mini. Results show full-context and retrieval methods perform best overall, with compression methods suffering information loss in their multi-stage pipelines despite strong intermediate recall. The analysis reveals that iterative compression methods degrade significantly in large-scale settings compared to their performance in smaller document collections. The paper recommends hybrid approaches combining input compression with long-context models for optimal performance on large-scale summarization tasks.

## Method Summary
The study compares four summarization methods across three datasets with ~100 documents each: full-context (direct processing), hierarchical (iterative merging), incremental (running summary updates), and retrieval (embedding-based document selection). Methods are evaluated using Llama-3.1-8B/70B, Command-R-32B, and Jamba-1.5-Mini 12B active / 52B total models with temperature 0.5. The evaluation uses A3CU (Atomic Content Unit) metrics for content selection quality and ROUGE for n-gram overlap. Iterative methods use 4K chunk sizes, with full-context limited to 128K tokens and retrieval selecting documents up to 32K tokens.

## Key Results
- Full-context and retrieval methods achieve the best performance, particularly on query-focused SummHay dataset
- Compression methods show strong intermediate recall but suffer catastrophic information loss in final summaries due to multi-stage pipelines
- Hierarchical methods produce increasingly abstract summaries losing entities and numerals at higher merge levels
- Incremental methods get distracted by non-salient information in later documents, degrading from earlier content

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Full-context methods achieve strong performance by maintaining global context throughout summarization, avoiding the cumulative information loss inherent in multi-stage pipelines.
- Mechanism: Long-context transformers process the entire document collection (~100 docs, up to 128k tokens) in a single forward pass, enabling the model to attend to relationships between information units distributed across documents without intermediate compression steps that discard details.
- Core assumption: The model's effective context window genuinely supports reasoning across the full input length without significant attention degradation at distant positions.
- Evidence anchors:
  - [abstract] "Full-text methods promise a lossless summary by relying on recent advances in long-context reasoning."
  - [section 3.2] "Full-context and retrieval perform the best, being particularly effective on the query-focused SummHay dataset."
  - [corpus] Related work on Cognitive Memory in LLMs (arXiv 2504.02441) emphasizes importance of short-term memory processing for context-rich responses.
- Break condition: When input exceeds the model's *effective* (not claimed) context window, or when salient information requires integrating distant positions that receive degraded attention.

### Mechanism 2
- Claim: Retrieval-based compression matches full-context performance by concentrating salient information into a manageable context window while discarding redundant or irrelevant documents.
- Mechanism: Embedding-based document ranking (using SFR Embedding-2) identifies the most query-relevant documents, selecting top-k up to 32k tokens—within the range where all tested transformers perform reliably—then applies full-context processing to this filtered set.
- Core assumption: Salient content units cluster in a subset of documents identifiable via semantic similarity; information in low-ranked documents is redundant or non-essential.
- Evidence anchors:
  - [abstract] "Overall, we find that full-text and retrieval methods perform the best in most settings."
  - [section 3.4] "For Llama-3.1-8B on SummHay, we find the final summary to be better than the best intermediate output... This is the optimal scenario."
  - [corpus] Estimating Optimal Context Length (arXiv 2504.12972) directly addresses hybrid retrieval-augmented approaches for this task.
- Break condition: When salient information is evenly distributed across many documents (high redundancy datasets like WCEP may tolerate this better) or when query-document similarity fails to surface key evidence.

### Mechanism 3
- Claim: Iterative compression methods (hierarchical and incremental) suffer catastrophic information loss at large scale despite strong intermediate recall, due to error accumulation across many compression stages.
- Mechanism: Each compression step abstracts content, with hierarchical methods producing increasingly generic summaries (losing entities/numerics) and incremental methods being distracted by non-salient information in later documents—effects that compound as document count increases from ~10 (book summarization) to ~100 (this study).
- Core assumption: Information loss per stage is manageable at small scale but becomes fatal when the number of stages scales with document count.
- Evidence anchors:
  - [abstract] "compression-based methods show strong promise at intermediate stages, even outperforming full-context. However, they suffer information loss due to their multi-stage pipeline and lack of global context."
  - [section 3.4] "iterative methods suffer catastrophic information loss in their multistage pipeline... hierarchical method tends to generate increasingly abstract summaries... incremental method... often gets distracted by non-salient information."
  - [corpus] MRGSEM-Sum (arXiv 2507.23400) addresses similar multi-document complexity via graph clustering.
- Break condition: When stage count exceeds a threshold (approximately 10-20 compression steps), cumulative loss outweighs any benefit from processing documents individually.

## Foundational Learning

- Concept: **Atomic Content Units (A3CU) evaluation**
  - Why needed here: The paper's central finding relies on measuring *what information* is retained vs. lost across methods; A3CU provides recall/precision/F1 for content unit overlap.
  - Quick check question: Given a reference summary with 10 atomic facts and a generated summary containing 6 of them plus 2 hallucinations, what are precision, recall, and F1?

- Concept: **RoPE scaling strategies for long context**
  - Why needed here: The transformers tested use different approaches—Llama-3.1 uses non-uniform RoPE dimension scaling; Command-R uses NTK-aware interpolation with large base frequency. Understanding these explains performance differences.
  - Quick check question: Why might non-uniform scaling of position dimensions outperform simply increasing the base frequency for extending context windows?

- Concept: **Multi-stage pipeline information flow**
  - Why needed here: Diagnosing *where* information loss occurs requires tracing content through hierarchical merge steps or incremental updates.
  - Quick check question: In hierarchical summarization of 100 documents with chunk size 4K, approximately how many compression stages occur before producing a single summary?

## Architecture Onboarding

- Component map:
  - **Input preprocessing**: Truncate to 128k tokens (budget across timestamps for timeline data), filter documents below 128 tokens
  - **Compression pathways**: (a) Full-context: direct pass; (b) Retrieval: embed → rank → select top-k to 32k → summarize; (c) Hierarchical: summarize each doc → iteratively merge; (d) Incremental: sequential running-summary updates
  - **Transformers**: Llama-3.1-8B/70B (RoPE scaling), Command-R-32B (NTK interpolation), Jamba-1.5-Mini 12B active / 52B total (Transformer-Mamba hybrid)
  - **Evaluation**: A3CU recall for intermediate stages; A3CU precision/recall/F1 for final summaries

- Critical path:
  1. Load document collection; apply truncation strategy (longest-doc truncation or timestamp budgeting)
  2. Route to selected method; if retrieval, compute embeddings and select top documents; if iterative, set chunk size (default 4K)
  3. Generate with temperature 0.5, respecting dataset-specific max summary lengths
  4. Compute A3CU against reference; for iterative methods, log intermediate recall to diagnose loss points

- Design tradeoffs:
  - Full-context: Highest potential quality; requires genuine 128k+ effective context; highest compute/memory cost
  - Retrieval: 4x context reduction (128k→32k); strong on query-focused tasks; risks missing distributed evidence
  - Hierarchical: Theoretically unlimited scale; empirically loses details (entities, numerics) at merge stages
  - Incremental: Preserves temporal flow; susceptible to distraction from later non-salient documents

- Failure signatures:
  - Hierarchical: Final summaries abstract, missing specific entities and numbers; "increasingly abstract summaries at higher levels" (§3.4)
  - Incremental: Summary drifts from earlier salient content; "model often gets distracted by non-salient information seen in documents thereafter" (§3.4)
  - Full-context with undertrained models: 70B underperforms 8B on SummHay retrieval task, suggesting need for long-context post-training
  - Retrieval: High example-level variance; works when salient docs are top-ranked, fails when evidence is distributed

- First 3 experiments:
  1. **Baseline reproduction**: Run all four methods on SummHay with Llama-3.1-8B; report A3CU F1 and plot intermediate vs. final recall to confirm the intermediate-advantage-final-loss pattern.
  2. **Retrieval budget sweep**: Vary post-retrieval token limit (16k, 32k, 64k) on Background dataset; measure quality degradation vs. compute savings to identify optimal operating point.
  3. **Hybrid prototype**: Implement retrieval-first filtering (top-64k) followed by full-context synthesis; compare against pure full-context on the oracle SummHay setting to test the paper's recommendation for hybrid approaches.

## Open Questions the Paper Calls Out
None

## Limitations
- The study's findings depend critically on the effectiveness of the 128k token context windows across tested models, which may not hold when inputs exceed the *effective* context length where attention mechanisms degrade.
- The hierarchical and incremental methods' catastrophic information loss appears to compound with stage count, but the specific threshold where this becomes prohibitive is not quantified across different document counts.
- The retrieval method's success depends on the assumption that salient content clusters in query-similar documents, which may not generalize to datasets with more distributed evidence patterns.

## Confidence
- **High confidence**: Full-context and retrieval methods outperform compression approaches on tested datasets; iterative methods show strong intermediate recall but poor final summaries due to information loss.
- **Medium confidence**: The recommendation for hybrid approaches combining retrieval filtering with full-context synthesis is supported by results but not directly tested.
- **Medium confidence**: The characterization of why compression methods fail (error accumulation across stages) is well-supported by intermediate metric tracking.

## Next Checks
1. **Context window boundary test**: Systematically evaluate full-context methods on progressively larger document collections to identify the actual effective context limit where performance degrades, comparing against claimed 128k token capacity.

2. **Stage count sensitivity analysis**: Vary the number of compression stages in hierarchical and incremental methods (e.g., 10, 20, 50, 100 documents) to quantify the relationship between stage count and information loss, establishing specific thresholds where iterative approaches become unviable.

3. **Hybrid method ablation**: Implement and test the recommended hybrid approach (retrieval-first filtering to 64k tokens followed by full-context synthesis) across all three datasets to validate whether this combination achieves better performance than either pure approach alone.