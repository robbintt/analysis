---
ver: rpa2
title: 'ProRe: A Proactive Reward System for GUI Agents via Reasoner-Actor Collaboration'
arxiv_id: '2509.21823'
source_url: https://arxiv.org/abs/2509.21823
tags:
- agents
- policy
- agent
- evaluator
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PRORE is a proactive reward system for GUI agents that addresses
  limitations of trajectory-based LLM-as-a-judge methods by introducing reasoner-actor
  collaboration. The system employs a general-purpose reasoner to schedule state probing
  tasks, which are executed by domain-specific evaluator agents to collect additional
  observations.
---

# ProRe: A Proactive Reward System for GUI Agents via Reasoner-Actor Collaboration

## Quick Facts
- arXiv ID: 2509.21823
- Source URL: https://arxiv.org/abs/2509.21823
- Reference count: 40
- Key outcome: ProRe achieves up to 5.3% improvement in reward accuracy and 19.4% in F1 score across over 3K trajectories, improving policy success rates by up to 22.4%.

## Executive Summary
ProRe addresses limitations of trajectory-based LLM-as-a-judge methods for GUI agent reward assignment by introducing a proactive probing mechanism. The system employs a general-purpose reasoner to schedule state probing tasks, which are executed by domain-specific evaluator agents to collect additional observations. This transforms reward assignment from passive monitoring to active evidence gathering, effectively overcoming incomplete state observability and domain-specific knowledge gaps. When integrated with policy agents, ProRe improves success rates by up to 22.4% while maintaining high reward accuracy of 93.7%.

## Method Summary
ProRe implements a proactive reward system for GUI agents through reasoner-actor collaboration. The general-purpose reasoner analyzes task expectations and schedules targeted state probing tasks, which domain-specific evaluator agents execute by actively interacting with the environment to collect additional observations. Both policy trajectories and probed states are summarized into structured claims, which the reasoner analyzes through chain-of-claims reasoning to determine task completion. This approach decouples general reasoning from GUI-specific evaluation, enabling more accurate verification of task completion while addressing incomplete state observability.

## Key Results
- ProRe achieves up to 5.3% improvement in reward accuracy (93.7% vs. baselines) and 19.4% in F1 score across over 3K trajectories
- When integrated with policy agents, ProRe improves success rates by up to 22.4% through test-time scaling
- Fine-tuned V-Droid-8B as evaluator achieves 93.1% accuracy vs. GPT-4o at 88.3%, validating domain-specialized evaluators

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Proactive state probing overcomes incomplete GUI observability by actively collecting missing evidence.
- **Mechanism:** The general-purpose reasoner analyzes task expectations and schedules targeted state probing tasks. Evaluator agents then execute these tasks to retrieve UI states that the policy agent's trajectory did not capture. This transforms reward assignment from passive screenshot monitoring to active evidence gathering.
- **Core assumption:** Key verification states exist in the environment but are not captured in the policy agent's execution trajectory.
- **Evidence anchors:**
  - [abstract]: "The reasoner schedules targeted state probing tasks, which the evaluator agents then execute by actively interacting with the environment to collect additional observations."
  - [section 1, page 2]: "GUI task states are typically monitored passively through specific modalities, such as screenshots...these states frequently remain incompletely observable."
  - [section 3.3, page 5]: "The state probing task is generally easier than other types of execution tasks...probing only requires navigating to the correct page and does not demand consecutive error-free execution."
- **Break condition:** If probing tasks require complex multi-step operations or if evaluators cannot access necessary UI states, the mechanism fails.

### Mechanism 2
- **Claim:** Decoupling general reasoning from domain-specific GUI evaluation improves reward accuracy by aligning tasks with model competencies.
- **Mechanism:** A general-purpose LLM (reasoner) handles high-level expectation analysis and consistency verification—tasks within its core competencies. Domain-specific evaluator agents (e.g., V-Droid) handle GUI navigation and state observation, leveraging their specialized UI knowledge. This separation prevents general LLMs from making judgments requiring domain expertise they lack.
- **Core assumption:** General LLMs excel at logical consistency verification but struggle with GUI-specific state interpretation; domain-specific agents have complementary strengths.
- **Evidence anchors:**
  - [abstract]: "enabling more accurate verification of task completion" through "decoupling general reasoning from GUI-specific evaluation"
  - [section 1, page 2-3]: "evaluating GUI task states requires domain-specific GUI knowledge and expertise, which general-purpose LLMs...fundamentally lack."
  - [table 5, page 9]: Fine-tuned V-Droid-8B achieves 93.1% accuracy as evaluator vs. M3A (GPT-4o) at 88.3%, showing domain-specialized evaluators outperform generalist systems.
- **Break condition:** If evaluator agents lack sufficient GUI competence or reasoners cannot perform reliable consistency reasoning, decoupling provides no benefit.

### Mechanism 3
- **Claim:** Chain-of-claims reasoning enables accurate reward assignment by analyzing structured relationships between policy and evaluator observations.
- **Mechanism:** Both policy trajectories and probed states are summarized into discrete, verifiable claims. The reasoner then identifies relationships (confirming, contradicting, complementary, unrelated) between policy claims and evaluator claims, performing chain-of-claims reasoning to determine task completion.
- **Core assumption:** Structured claims capture task-relevant information more effectively than raw trajectories, and consistency analysis between claim sets reveals ground truth.
- **Evidence anchors:**
  - [section 3.4, page 5]: "the reasoner performs final judgment through chain-of-claims reasoning, which analyzes the consistency between the policy agent's claims and those generated from the evaluators' probing."
  - [figure 1, page 2]: Shows concrete example with policy claims and evaluator claims being compared to reach judgment.
  - [table 3, page 8]: Ablation shows chain-of-claims improves accuracy from 91.4% to 93.1%.
- **Break condition:** If claim generation loses critical information or if reasoners cannot reliably identify claim relationships, the mechanism degrades.

## Foundational Learning

- **Concept: Reinforcement Learning with Verifiable Rewards (RLVR)**
  - **Why needed here:** ProRe is designed to provide verifiable binary rewards for GUI agent training. Understanding RLVR clarifies why accurate rewards are critical for policy improvement.
  - **Quick check question:** Can you explain why reward accuracy >50% is necessary for test-time scaling to improve success rates?

- **Concept: LLM-as-a-Judge Paradigm**
  - **Why needed here:** ProRe addresses limitations of trajectory-based LLM-as-a-judge approaches. Understanding this baseline helps contextualize the proactive probing innovation.
  - **Quick check question:** What are the two main failure modes of LLM-as-a-judge for GUI agents that ProRe identifies?

- **Concept: Test-Time Scaling for Agents**
  - **Why needed here:** ProRe improves policy agent success rates through test-time scaling. Lemma 1 provides the theoretical foundation for how reward accuracy affects final success rates.
  - **Quick check question:** Given a policy agent with 50% success rate and reward accuracy of 90%, can you calculate the theoretical upper bound on final success rate with 8 trials?

## Architecture Onboarding

- **Component map:** Policy Agent -> Reasoner -> Evaluator Agents -> Chain-of-Claims Generator
- **Critical path:**
  1. Policy agent executes task → produces trajectory
  2. Reasoner analyzes task expectations → schedules probing tasks
  3. Evaluator agents probe environment → collect additional states
  4. Claims generated from both policy trajectory and evaluator observations
  5. Reasoner performs chain-of-claims reasoning → assigns binary reward
- **Design tradeoffs:**
  - **Cost vs. Accuracy:** ProRe costs ~$0.06/task vs. free rule-based methods; achieves 5.3-19.4% improvement
  - **Parallel vs. Sequential Probing:** Parallel probing shows larger gains than iterative refinement (Figure 7, page 8)
  - **Evaluator Selection:** Fine-tuned small models (V-Droid-8B) outperform large generalist systems (GPT-4o) for evaluation
- **Failure signatures:**
  - **Evaluator claims about own actions:** Must filter claims describing evaluator-caused outcomes (Appendix E.3)
  - **Incomplete probing:** Single probing trial insufficient for complex tasks; requires test-time scaling
  - **Loop detection:** Policy agent loops in trajectory can be discarded if unrelated to task goal
- **First 3 experiments:**
  1. **Reproduce baseline comparison:** Implement DigiRL, DistRL, WebRL reward models on AndroidWorld subset; verify ProRe achieves reported 5.3% accuracy improvement
  2. **Ablate chain-of-claims:** Run ProRe without structured claims (pass raw observations to reasoner); expect ~1.7% accuracy drop per Table 3
  3. **Test execution-probing gap:** Measure success rate and trajectory length difference between state probing tasks and execution tasks on held-out tasks; expect ~24% higher SR and ~50% shorter trajectories per Table 1

## Open Questions the Paper Calls Out
- **Online RL Integration:** The paper defers full exploration of online RL with ProRe to future work, noting it currently only supports online RL with "moderate overhead." It's unclear if probing task latency hinders sample efficiency required for rapid online training convergence.
- **Evaluator Co-evolution:** The hypothesis of a "virtuous cycle" where stronger policies enable better probing, which in turn improves the policy, is not empirically demonstrated. Iterative training showing stability or divergence is needed.
- **Minimal Execution-Probing Gap:** The method relies on probing being simpler than execution. For tasks where verification requires re-executing logic (e.g., "verify the code compiles"), the evaluator may fail as often as the policy agent, potentially reducing reward accuracy to random chance.

## Limitations
- **Reproducibility of V-Droid Evaluator:** The paper relies heavily on V-Droid as the primary evaluator agent, but specific model weights or inference code are not provided. Substituting with UI-TARS or GPT-4o may affect results.
- **Probing Task Scheduling:** The probing task generation references "Style Examples" not fully detailed in the appendix, introducing potential variability in probing effectiveness.
- **Cost-Benefit Tradeoffs:** While ProRe achieves 5.3-19.4% improvement in reward accuracy, it incurs ~$0.06 per task compared to free rule-based methods. The cost-effectiveness depends on application scale.

## Confidence
- **High Confidence:** Reward accuracy improvements (93.7% vs. baselines) and F1 score gains are well-supported by ablation studies and controlled experiments.
- **Medium Confidence:** Policy success rate improvements (up to 22.4%) depend on test-time scaling assumptions and may vary with different policy agents or benchmarks.
- **Medium Confidence:** The decoupling mechanism's effectiveness relies on the assumption that general LLMs lack GUI-specific knowledge, which is plausible but not exhaustively validated across diverse GUI domains.

## Next Checks
1. **Baseline Reproduction:** Implement DigiRL, DistRL, and WebRL reward models on AndroidWorld subset to verify ProRe's 5.3% accuracy improvement claim.
2. **Evaluator Substitution:** Replace V-Droid with UI-TARS or GPT-4o as the evaluator agent to assess sensitivity to evaluator choice and validate the cost-effectiveness claim.
3. **Probing Task Overhead Analysis:** Measure the success rate and trajectory length difference between state probing tasks and execution tasks on held-out tasks to validate the 24% higher success rate and 50% shorter trajectories claim.