---
ver: rpa2
title: Efficient Extractive Text Summarization for Online News Articles Using Machine
  Learning
arxiv_id: '2509.15614'
source_url: https://arxiv.org/abs/2509.15614
tags:
- summarization
- extractive
- text
- news
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores extractive text summarization for online news
  articles using machine learning techniques. The authors use the Cornell Newsroom
  dataset, which contains 1.3 million article-summary pairs from 39 major publications.
---

# Efficient Extractive Text Summarization for Online News Articles Using Machine Learning

## Quick Facts
- arXiv ID: 2509.15614
- Source URL: https://arxiv.org/abs/2509.15614
- Authors: Sajib Biswas; Milon Biswas; Arunima Mandal; Fatema Tabassum Liza; Joy Sarker
- Reference count: 40
- Primary result: LSTM networks outperform baselines, achieving F1=0.599 and ROUGE-1=0.585

## Executive Summary
This paper presents an extractive text summarization approach for online news articles using machine learning techniques. The authors frame summarization as a binary classification problem where sentences are classified as summary-worthy or not. Using the Cornell Newsroom dataset (1.3M article-summary pairs from 39 publications), they employ BERT embeddings with additional position features. The proposed LSTM models, particularly bidirectional variants, demonstrate superior performance over simpler baselines like Lede-3, achieving significant improvements in F1 score and ROUGE-1 metrics.

## Method Summary
The approach uses sentence-level binary classification with BERT embeddings (768-dim) concatenated with normalized position indices. Sentences are labeled positive if their cosine similarity with gold summaries exceeds an unspecified threshold. The pipeline segments articles using SpaCy, generates frozen BERT embeddings, and trains models including logistic regression, feed-forward networks, and bidirectional LSTM (50-75 hidden units). Models are trained for 50 epochs using binary cross-entropy loss, with evaluation based on F1-score and ROUGE-1 metrics against a Lede-3 baseline.

## Key Results
- BiLSTM model achieves F1=0.599 and ROUGE-1=0.585, outperforming Lede-3 (F1=0.589)
- Sequential modeling provides significant advantage over non-sequential baselines
- Position features contribute substantially, with models showing strong preference for early sentences
- Logistic regression with BERT embeddings matches neural network performance when sequential information is absent

## Why This Works (Mechanism)

### Mechanism 1
Bidirectional LSTM models outperform non-sequential baselines because they capture sequential dependencies between sentences in news articles. The BiLSTM processes sentence embeddings in both forward and backward directions, allowing the model to learn contextual relationships—what comes before and after each sentence influences its summary-worthiness score. Gating mechanisms regulate information flow across long sequences, mitigating vanishing gradient problems.

Core assumption: News articles exhibit sequential structure where a sentence's importance depends on surrounding context, not just its isolated semantic content.

Evidence anchors:
- [abstract] "LSTM networks, with their ability to capture sequential dependencies, outperform baseline methods like Lede-3 and simpler models in F1 score and ROUGE-1 metrics."
- [section 4.2] Table 4 shows LSTM Bi 50 achieves F1=0.599 vs. Lede-3's 0.589
- [section 4.2] Table 6 shows LSTM and Lede-3 outputs heavily overlap, both selecting early sentences

### Mechanism 2
BERT embeddings provide semantically rich sentence representations that enable linear models to achieve competitive performance without architecture complexity. BERT's bidirectional transformer pre-training produces dense 768-dimensional vectors encoding syntactic, semantic, and some positional information. When fed into logistic regression, these embeddings create nearly linearly separable decision boundaries for summary-worthy vs. non-summary sentences.

Core assumption: Pre-trained language model representations transfer effectively to sentence-level importance classification without task-specific fine-tuning.

Evidence anchors:
- [section 4.1] Table 3 shows logistic regression (F1=0.416) matches neural network (F1=0.415) when both use only embedding features
- [abstract] "we developed a pipeline leveraging BERT embeddings to transform textual data into numerical representations"

### Mechanism 3
Sentence position features provide a strong inductive bias for news summarization because journalistic writing places critical information early. Position encoding (sentence index normalized by document length) is concatenated with BERT embeddings, giving models explicit access to structural priors that news follows inverted-pyramid style. Lede-3 exploits this directly; ML models learn to weight it.

Core assumption: Target documents follow journalistic conventions where lead sentences summarize key information.

Evidence anchors:
- [section 3.2] "Lede-3... in which the first three sentences of the text are selected as the summary... performs very well and is competitive with more sophisticated summarization techniques."
- [section 4.2] Table 6 shows LSTM and Lede-3 outputs heavily overlap, both selecting early sentences

## Foundational Learning

- **Binary cross-entropy loss for sentence classification**
  - Why needed here: The paper frames extractive summarization as binary classification (summary-worthy = 1, not = 0). Understanding BCE loss explains why logistic regression and neural networks produce probability outputs.
  - Quick check question: Given true label y=1 and predicted probability ŷ=0.2, compute the BCE loss contribution. (Answer: -log(0.2) ≈ 1.61)

- **ROUGE-N evaluation metrics**
  - Why needed here: The paper reports ROUGE-1 as the primary content-overlap metric. Understanding n-gram recall helps interpret why models with similar F1 may differ in summary fluency or coverage.
  - Quick check question: If a reference summary has 50 unigrams and the generated summary matches 35 of them, what is ROUGE-1 recall? (Answer: 35/50 = 0.70)

- **LSTM gating mechanisms (forget, input, output gates)**
  - Why needed here: The paper's best model is BiLSTM. Understanding gates explains how the model selectively retains or discards information across 150,000+ sentences during training.
  - Quick check question: If the forget gate outputs near-zero values for all dimensions at time step t, what happens to the previous cell state? (Answer: It is effectively erased; the cell state is reset.)

## Architecture Onboarding

- **Component map:**
  Raw article text -> SpaCy sentence segmentation -> BERT base model (uncased) -> 768-dim sentence embeddings -> Position index (normalized) concatenated -> 769-dim vector -> BiLSTM (50-75 hidden units/direction) -> sigmoid per sentence -> probabilities -> Top-k sentences by probability

- **Critical path:**
  1. Data filtering: Retain only extractive-summary pairs from Cornell Newsroom (5,000 articles, ~150K sentences)
  2. Label assignment: Cosine similarity between document sentences and gold summary sentences -> threshold -> binary labels
  3. Embedding generation: BERT forward pass (no fine-tuning) -> cache embeddings
  4. Model training: 50 epochs, binary cross-entropy, Adam optimizer
  5. Evaluation: ROUGE-1, F1, precision, recall against gold summaries

- **Design tradeoffs:**
  - Frozen vs. fine-tuned BERT: Paper uses frozen embeddings (faster, less data). Tradeoff: loses task-specific semantic adaptation.
  - LSTM hidden size: 25 vs. 50 vs. 75 units tested. Larger improves F1 marginally but increases compute.
  - Positional features: Adding position helps logistic regression (+0.109 F1 vs. embedding-only) but makes models biased toward lead sentences.

- **Failure signatures:**
  - Low recall, high precision: Model too conservative; threshold too high or class imbalance untreated.
  - Lede-3 parity: If LSTM ≈ Lede-3, sequential model learned position-only heuristic; embeddings underutilized.
  - High variance across publications: Suggests overfitting to specific journalistic styles in training set.

- **First 3 experiments:**
  1. Baseline replication: Implement Lede-3 and logistic regression (embedding-only) on 500-article subset. Verify F1 ≈ 0.41-0.42 before scaling.
  2. Ablation on positional features: Train BiLSTM with and without position encoding. Expect small F1 gap (<0.02); larger gap indicates position over-reliance.
  3. Cross-publication generalization: Train on 3 news outlets, test on held-out 4th. If F1 drops >15%, model overfits publication style rather than generalizable summary patterns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do Large Language Models (LLMs) compare to the proposed LSTM models for both extractive and abstractive summarization tasks?
- Basis in paper: [explicit] The authors state they aim to "investigate the effectiveness of LLMs for both extractive and abstractive summarization" in future work.
- Why unresolved: The current study limits its scope to logistic regression, feed-forward networks, and LSTMs.
- What evidence would resolve it: Direct benchmarking of LLMs against the LSTM Bi 50 model using F1 and ROUGE metrics on the Newsroom dataset.

### Open Question 2
- Question: Do the current findings generalize to diverse datasets and non-news domains?
- Basis in paper: [explicit] The conclusion notes plans to "extend our evaluations across diverse datasets and domains to improve generalization and robustness."
- Why unresolved: The experimental results are derived solely from the Cornell Newsroom dataset, which consists of specific journalistic structures.
- What evidence would resolve it: Evaluation of the models on distinct corpora (e.g., scientific or legal documents) to assess cross-domain performance.

### Open Question 3
- Question: Can the model architecture be improved to capture critical information located later in the document, overcoming the observed positional bias?
- Basis in paper: [inferred] Analysis of Table 6 notes the LSTM output shows a "strong preference for early sentences," limiting its ability to capture details occurring later in the text.
- Why unresolved: The model currently relies heavily on sentence position, similar to the Lede-3 baseline, potentially missing distributed key information.
- What evidence would resolve it: Performance metrics specifically measuring recall of summary-worthy sentences in the middle and final sections of articles.

## Limitations
- The experimental setup relies on an unspecified similarity threshold for labeling sentences, which critically affects class balance and model performance
- The 5,000-article subset selection method is unclear, raising questions about whether results generalize across the full dataset
- The frozen BERT embeddings may limit the model's ability to capture domain-specific language patterns in news text

## Confidence
- **High confidence**: The BiLSTM architecture with positional features consistently outperforms simpler baselines in both F1 and ROUGE-1 metrics
- **Medium confidence**: BERT embeddings provide sufficient semantic representation for sentence classification without fine-tuning
- **Low confidence**: The sequential modeling advantage over Lede-3 is primarily due to learned position encoding rather than true semantic understanding

## Next Checks
1. Implement controlled ablation testing by training models with position features removed to quantify positional bias versus semantic learning
2. Test cross-publication generalization by training on 3-4 outlets and evaluating on held-out 5th to assess overfitting to specific journalistic styles
3. Conduct threshold sensitivity analysis by varying cosine similarity cutoffs (0.7-0.95) to establish optimal labeling strategy and class balance