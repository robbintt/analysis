---
ver: rpa2
title: 'STEP: Success-Rate-Aware Trajectory-Efficient Policy Optimization'
arxiv_id: '2511.13091'
source_url: https://arxiv.org/abs/2511.13091
tags:
- tasks
- training
- uni00000013
- step
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inefficiency of trajectory-level reinforcement
  learning in multi-turn decision-making, where uniform sampling and coarse reward
  assignment limit learning from failed or partially correct trajectories. The authors
  propose STEP, a success-rate-aware, step-level policy optimization framework that
  dynamically adjusts sampling budgets based on task success rates, decomposes successful
  trajectories into step-level samples, and applies success-rate-weighted advantages.
---

# STEP: Success-Rate-Aware Trajectory-Efficient Policy Optimization

## Quick Facts
- **arXiv ID:** 2511.13091
- **Source URL:** https://arxiv.org/abs/2511.13091
- **Reference count:** 6
- **Key outcome:** STEP achieves 62.5% training success on OSWorld and 45.7% overall, surpassing baselines by 7.1–14.7 points with 74–86% reduction in training time per step.

## Executive Summary
This paper addresses the inefficiency of trajectory-level reinforcement learning in multi-turn decision-making, where uniform sampling and coarse reward assignment limit learning from failed or partially correct trajectories. The authors propose STEP, a success-rate-aware, step-level policy optimization framework that dynamically adjusts sampling budgets based on task success rates, decomposes successful trajectories into step-level samples, and applies success-rate-weighted advantages. Step-level GRPO augmentation further enriches learning from low-success tasks. Experiments on OSWorld and AndroidWorld show that STEP achieves higher sample efficiency and better performance compared to trajectory-level GRPO.

## Method Summary
STEP is a success-rate-aware, step-level policy optimization framework for multi-turn decision-making tasks. It maintains a smoothed success-rate record per task to guide adaptive trajectory resampling, allocating more effort to harder tasks. The method decomposes successful trajectories into step-level samples and applies success-rate-weighted advantages. For low-success tasks, STEP augments step-level samples by generating alternative action variants through model prompting. The framework combines three mechanisms: SR-Traj for sampling budget reallocation, SR-Adv for success-rate-weighted advantage computation, and SL-GRPO for step-level augmentation. The method is evaluated on OSWorld and AndroidWorld benchmarks using the UI-Tars-DPO-7B model, showing improved sample efficiency and performance compared to trajectory-level GRPO baselines.

## Key Results
- STEP achieves 62.5% training success rate on OSWorld, surpassing trajectory-level GRPO by 7.1 points
- STEP reduces training time per step by 74-86% compared to trajectory-level sampling
- The method shows 45.7% overall success rate on AndroidWorld, outperforming baselines by 14.7 points

## Why This Works (Mechanism)

### Mechanism 1: Success-Rate-Guided Sampling Reallocation
Dynamically reallocating sampling budgets toward low-success tasks improves sample efficiency by reducing redundant training on mastered tasks. The system maintains a smoothed success-rate record per task and probabilistically replaces high-success tasks (above threshold s₀=0.6) with tasks from a cache of intermediate-difficulty tasks using a logistic replacement function. This ensures that learning effort is focused where gradients are most informative.

### Mechanism 2: Step-Level Decomposition with Success-Rate-Weighted Advantages
Decomposing successful trajectories into step-level samples with success-rate-weighted advantages provides finer-grained learning signals and reduces credit misattribution. By filtering to successful trajectories and weighting their advantages by (1 - ŝᵢ), the method amplifies signals from harder tasks while uniformly distributing the weighted advantage across all steps in a successful trajectory.

### Mechanism 3: Step-Level GRPO Augmentation for Low-Success Tasks
Generating alternative action variants for steps in low-success tasks enriches training data without additional environment interaction. For tasks with ŝᵢ ≤ s_low (0.2), each step sample is expanded into N/2-1 variants by prompting the model for alternative actions. Augmented advantages are computed via GRPO-style group normalization against the original action as reference, providing local action exploration around high-value steps.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed here: STEP extends GRPO from single-turn to multi-turn; understanding baseline GRPO (group-based advantage estimation without value function) is prerequisite.
  - Quick check question: Can you explain how GRPO estimates advantages from a group of samples for the same query?

- **Concept: Credit Assignment in Reinforcement Learning**
  - Why needed here: The paper diagnoses trajectory-level credit misattribution; understanding temporal credit assignment clarifies why step-level decomposition helps.
  - Quick check question: In a 15-step trajectory where only step 12 fails, how should rewards be distributed across steps?

- **Concept: Multi-Turn Environment Interaction Dynamics**
  - Why needed here: The paper highlights that environment interaction dominates rollout time (~8.5× slower than parallel single-step inference).
  - Quick check question: Why does sequential environment interaction create a bottleneck for trajectory-level sampling?

## Architecture Onboarding

- **Component map:** SR Recorder → Sampling Budget Reallocator → Trajectory Collector → Success Filtering → Advantage Weighting → Step Decomposition → Augmentation (conditional) → Policy Optimizer → SR Recorder Update
- **Critical path:** SR Recorder → Sampling Reallocator → Trajectory Collection → Success Filtering → Advantage Weighting → Step Decomposition → Augmentation (conditional) → Policy Update → SR Recorder Update
- **Design tradeoffs:**
  - s₀ threshold (0.6): Higher = more aggressive reallocation, risk of under-sampling easy tasks
  - s_low threshold (0.2): Lower = less augmentation, less noise; higher = more augmentation but may dilute signals
  - Training only on successful trajectories: Cleaner signals but discards potentially useful partial trajectories
  - History window (3 responses, 0 screenshots): Shorter context reduces compute but may lose long-horizon dependencies
- **Failure signatures:**
  - Cache C_Q empties early → all tasks either solved or unsolved → reverts to near-uniform sampling
  - Augmentation threshold too high → generalization plateaus or declines
  - Very low success rates across all tasks → insufficient successful trajectories for training
  - Context window mismatch → step-level samples lose critical history information
- **First 3 experiments:**
  1. **Baseline validation:** Run T-GRPO vs. GiGRPO vs. STEP-w/o-Both on a 20-task subset; verify that step-level decomposition alone provides measurable gain.
  2. **Threshold sensitivity:** Sweep s₀ ∈ {0.4, 0.5, 0.6, 0.7} and s_low ∈ {0.1, 0.2, 0.3} on held-out tasks; plot success-rate trajectories over training.
  3. **Ablation by component:** Disable SR-Traj, SR-Adv, and SL-GRPO independently; measure training time per step and final success rate to quantify each component's contribution.

## Open Questions the Paper Calls Out

### Open Question 1
How can fine-grained, step-level reward mechanisms be designed to selectively leverage informative sub-trajectories from failed trajectories? The authors note that failed trajectories are entirely discarded, potentially leaving valuable sub-trajectories unused. A mechanism that successfully extracts intermediate "correct" actions from failed episodes and demonstrates improved sample efficiency would resolve this.

### Open Question 2
How can the framework distinguish and penalize sub-optimal actions within successful trajectories? The current method assigns the same advantage to all steps in a successful trajectory, but even successful trajectories may contain sub-optimal or ineffective actions. An extension incorporating step-level verification or value function to weight actions would address this.

### Open Question 3
Is the reliance on fixed success-rate thresholds (s₀, s_low) robust across environments with different reward densities? The method relies on manually tuned thresholds, but it's unclear if these static thresholds generalize to tasks with denser reward signals or varying difficulty distributions without requiring extensive re-tuning.

## Limitations
- STEP discards failed trajectories entirely, potentially losing informative partial trajectories
- The method relies on manually tuned success-rate thresholds that may not generalize across environments
- SL-GRPO augmentation effectiveness depends on the model's ability to generate semantically meaningful variants without environment feedback

## Confidence

- **High confidence:** Core mechanism of success-rate-guided sampling reallocation and its implementation (s₀=0.6 threshold, logistic replacement function). Empirical gains in sample efficiency (74-86% training time reduction) are well-supported.
- **Medium confidence:** Step-level decomposition benefits. While manual analysis supports the claim that failed trajectories contain misleading gradients, the 38.56% step-level validity metric suggests significant information loss when discarding failed trajectories.
- **Low confidence:** SL-GRPO augmentation effectiveness. Limited direct evidence in corpus; performance gains appear sensitive to s_low threshold selection, with high thresholds causing plateauing or decline.

## Next Checks

1. **Trajectory utility ablation:** Implement a variant that retains and reweights failed trajectories based on step-level success rather than discarding them entirely. Compare final success rates to validate the discard assumption.
2. **Augmentation grounding test:** For low-success tasks, execute generated action variants in the environment and measure their actual success rates versus the model's predicted rewards. This validates whether synthetic augmentation produces meaningful variations.
3. **Threshold sensitivity sweep:** Systematically vary s₀ ∈ {0.4, 0.5, 0.6, 0.7} and s_low ∈ {0.1, 0.2, 0.3} on held-out tasks, measuring both final success rates and training time efficiency to identify optimal operating points.