---
ver: rpa2
title: Deep Learning for Continuous-Time Stochastic Control with Jumps
arxiv_id: '2505.15602'
source_url: https://arxiv.org/abs/2505.15602
tags:
- control
- optimal
- gpi-cbu
- value
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces two deep learning algorithms, GPI-PINN and
  GPI-CBU, for solving finite-horizon continuous-time stochastic control problems
  with jumps. The methods iteratively train neural networks to approximate both the
  value function and optimal policy by leveraging the Hamilton-Jacobi-Bellman equation.
---

# Deep Learning for Continuous-Time Stochastic Control with Jumps

## Quick Facts
- arXiv ID: 2505.15602
- Source URL: https://arxiv.org/abs/2505.15602
- Reference count: 40
- Key outcome: Two deep learning algorithms, GPI-PINN and GPI-CBU, solve high-dimensional stochastic control problems with jumps by training neural networks to approximate both value function and optimal policy using the Hamilton-Jacobi-Bellman equation.

## Executive Summary
This paper introduces two deep learning algorithms for solving finite-horizon continuous-time stochastic control problems with jumps. The methods iteratively train neural networks to approximate both the value function and optimal policy by leveraging the Hamilton-Jacobi-Bellman equation. GPI-PINN uses a physics-informed neural network approach with residual minimization, while GPI-CBU employs a continuous-time Bellman updating rule that avoids explicit gradient computations. The algorithms are model-based, incorporating the system dynamics directly rather than relying on sampling from the environment. Numerical experiments demonstrate that both methods achieve high accuracy in high-dimensional problems, with GPI-CBU being particularly efficient for problems with jumps.

## Method Summary
The paper proposes two model-based deep learning algorithms that solve continuous-time stochastic control problems with jumps by approximating the value function V(t,x) and optimal control α(t,x) using neural networks. Both methods leverage the Hamilton-Jacobi-Bellman equation and employ Generalized Policy Iteration: first training a value network to satisfy the HJB equation, then updating a policy network to maximize the Hamiltonian. GPI-PINN minimizes the residual of the HJB PDE directly, while GPI-CBU uses a continuous-time Bellman updating rule that avoids explicit gradient computations by replacing expectations with single-sample operators. The methods use DGM architecture (3 hidden layers, 50 neurons) and are implemented in TensorFlow/Keras.

## Key Results
- Both GPI-PINN and GPI-CBU achieve high accuracy in high-dimensional problems with jumps, with GPI-CBU being significantly faster
- The methods scale well to problems with up to 150 dimensions
- For an LQR problem with jumps in 50 dimensions, GPI-CBU achieves MAEV = 0.0147
- Both algorithms outperform model-free reinforcement learning approaches like PPO and SAC in terms of sample efficiency and accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Incorporating system dynamics directly into the loss function provides higher sample efficiency and accuracy than model-free sampling approaches in high-dimensional spaces.
- **Mechanism:** The algorithms enforce the Hamilton-Jacobi-Bellman (HJB) partial integro-differential equation (PIDE) as a hard constraint (PINN) or a recursive update target (CBU). Instead of learning value estimates purely from reward trajectories, the network minimizes the residual of the known governing equation, effectively regularizing the learning with the "physics" of the environment.
- **Core assumption:** The system dynamics (drift β, diffusion σ, jump intensity λ, and jump size γ) are fully known and differentiable.
- **Evidence anchors:** The loss function explicitly minimizes the HJB residual and terminal condition error; neighbor papers emphasize moving away from discrete-time sampling for continuous control.
- **Break condition:** Performance degrades if the provided dynamics model is misspecified or if the system is not Markovian.

### Mechanism 2
- **Claim:** Separating the approximation of the value function and the optimal policy into two distinct networks allows for solving "implicit" control problems where the optimal action is not analytically derivable from the value function.
- **Mechanism:** The framework employs Generalized Policy Iteration (GPI). In Step 1, the "Critic" (V_θ) is trained to evaluate the current policy by satisfying the PIDE. In Step 2, the "Actor" (α_φ) is updated to maximize the Hamiltonian given the new value estimate.
- **Core assumption:** The optimal control exists and is smooth enough to be approximated by a neural network over the domain [0, T] × D.
- **Evidence anchors:** The paper iteratively approximates value function V and optimal control α* with neural networks; such implicit optimal control problems require specifically designed iterative approximation procedures.
- **Break condition:** Instability arises if the policy update step is too aggressive relative to the value function convergence.

### Mechanism 3
- **Claim:** Replacing expectation integrals with single-sample operators enables the solution of high-dimensional control problems with jumps, bypassing the computational intractability of numerical integration.
- **Mechanism:** GPI-CBU utilizes Proposition 4.1, which shows that the value function can be updated by minimizing the squared difference between the current estimate V_θ(t, x) and an operator G_ζ. This operator uses a single random sample Z_1 rather than the Monte Carlo expectation required by GPI-PINN.
- **Core assumption:** The scaling factor ζ must be positive, and the expectation over the random variable Y_t and Z_1 is finite.
- **Evidence anchors:** GPI-CBU circumvents the computation of gradients, Hessians and jump-expectations altogether; it avoids the costly computation of the jump-expectations of V_θ at each sample point.
- **Break condition:** Convergence fails if the scaling factor ζ is negative or if the variance of the single-sample gradient is too high to allow stable learning.

## Foundational Learning

- **Concept:** Hamilton-Jacobi-Bellman (HJB) Equation
  - **Why needed here:** This is the "ground truth" physics the network must learn. Understanding that the HJB equation describes the conservation of value over time (and includes terms for drift, diffusion, and jumps) is necessary to interpret the loss functions.
  - **Quick check question:** Can you identify which term in Equation (4) represents the cost/benefit of random jumps in the state?

- **Concept:** Itô's Lemma / Taylor Expansion for Jump-Diffusions
  - **Why needed here:** The derivation of the update rules relies on how functions of stochastic processes change. The "gradient trick" (Proposition 3.1) is essentially a numerical method to approximate the Itô expansion terms without explicit Hessian computation.
  - **Quick check question:** Why does the standard chain rule fail for Brownian motion, necessitating the inclusion of the second derivative (∇²_x V) in the expansion?

- **Concept:** Actor-Critic Reinforcement Learning
  - **Why needed here:** The paper maps mathematical control theory onto deep learning architecture. Recognizing the "Policy Network" (Actor) and "Value Network" (Critic) distinction helps in debugging and tuning the two separate loss functions.
  - **Quick check question:** In Algorithm 1, which step corresponds to the "Actor" update, and what mathematical quantity is it maximizing?

## Architecture Onboarding

- **Component map:** Time t ∈ [0, T] and State x ∈ ℝ^d -> DGM networks (V_θ, α_φ) -> Dynamics Engine (β, σ, γ, λ) -> Loss Modules (HJB residual or target G_ζ) -> Value/Policy updates

- **Critical path:**
  1. Sampling: Draw points (t, x) from the domain and Z from jump distribution
  2. Forward Pass: Compute V_θ(t, x) and α_φ(t, x)
  3. Dynamics Calculation: Use closed-form dynamics to compute Hamiltonian or target terms
  4. Value Update: Backprop on V_θ to minimize PDE residual or prediction error
  5. Policy Update: Backprop on α_φ to maximize the Hamiltonian

- **Design tradeoffs:**
  - GPI-PINN vs. GPI-CBU:
    - PINN: Use for problems without jumps or when high precision is critical. It is stable but computationally heavy because it requires approximating expectations and 3rd-order derivatives
    - CBU: Use for problems with jumps in high dimensions. It is significantly faster (single-sample updates) but can be noisier/less stable
  - Network Size: The paper uses 4 hidden layers with 50 neurons. Smaller networks may fail to capture the curvature of the value function in 150 dimensions

- **Failure signatures:**
  - Exploding Loss: If using GPI-CBU with a negative scaling factor ζ, the loss will diverge immediately
  - Slow Convergence (GPI-PINN): If runtime is excessive, check if you are computing expectation integrals over too many samples; switch to GPI-CBU
  - Poor Generalization: If the test set includes regions outside the sampled distribution μ, errors will be high

- **First 3 experiments:**
  1. 1D/2D Baseline: Implement the LQR problem (Eq 18) in 1 dimension with Λ_1=0 (no jumps). Compare V_θ against the analytical solution (Eq 26) to verify the basic PINN loop works
  2. Scalability Test: Run the LQR problem in 50 dimensions with jumps using GPI-CBU. Verify that the runtime scales linearly with dimensions and check MAEV against the Runge-Kutta reference solution
  3. Stability Check: Run the same 50D problem with GPI-PINN. Observe the drastic increase in runtime per epoch to confirm the trade-off described in Section 6

## Open Questions the Paper Calls Out

- Can the GPI-PINN and GPI-CBU algorithms be effectively extended to model-free settings where the underlying state dynamics must be learned concurrently with the control policy?
- Does an adaptive scaling factor ζ_k improve the convergence speed or accuracy of GPI-CBU compared to the fixed ζ=1 used in the experiments?
- Can the stability of GPI-CBU be improved to match GPI-PINN without sacrificing the computational efficiency gained by avoiding explicit jump-expectation sampling?

## Limitations

- The algorithms require full knowledge of system dynamics (drift, diffusion, jump intensity, and jump size), making them unsuitable for purely model-free settings
- GPI-CBU can be unstable when the scaling factor ζ is negative, requiring careful hyperparameter tuning
- The numerical experiments focus on relatively low-noise, well-conditioned problems, leaving open questions about robustness to highly nonlinear dynamics or heavy-tailed jump distributions

## Confidence

- **High** - The core mechanism of using HJB equation residuals for training is mathematically sound and well-supported by the derivation
- **Medium** - The claim of superior sample efficiency over model-free RL is supported by runtime comparisons, but the benchmarks are not state-of-the-art continuous control methods
- **Low** - The assertion that GPI-CBU achieves linear scaling with dimension is based on a single 50D experiment; the 150D results only evaluate GPI-PINN

## Next Checks

1. **Hyperparameter sensitivity**: Systematically vary ξ₁, ξ₂ and ζ to quantify their impact on convergence speed and final accuracy across different problem types
2. **Robustness to model misspecification**: Introduce small errors in the drift/diffusion functions and measure performance degradation to establish practical limits of the model-based assumption
3. **Benchmark against modern continuous control RL**: Compare against state-of-the-art model-free methods like TD3 or SAC with entropy regularization to strengthen the efficiency claims