---
ver: rpa2
title: A Causal Lens for Evaluating Faithfulness Metrics
arxiv_id: '2502.18848'
source_url: https://arxiv.org/abs/2502.18848
tags:
- explanations
- metrics
- faithfulness
- explanation
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CAUSALDIAGNOSTICITY, a testbed framework
  for evaluating faithfulness metrics in natural language explanations from LLMs.
  The method leverages model-editing techniques to generate controlled pairs of faithful
  and unfaithful explanations, operationalizing diagnosticity to measure how well
  metrics distinguish between them.
---

# A Causal Lens for Evaluating Faithfulness Metrics

## Quick Facts
- arXiv ID: 2502.18848
- Source URL: https://arxiv.org/abs/2502.18848
- Reference count: 40
- Primary result: Filler Tokens consistently performs best among evaluated metrics, while continuous variants generally outperform binary ones

## Executive Summary
This paper introduces CAUSALDIAGNOSTICITY, a framework for evaluating faithfulness metrics in natural language explanations from LLMs. The method leverages model-editing techniques to generate controlled pairs of faithful and unfaithful explanations, operationalizing diagnosticity to measure how well metrics distinguish between them. Experiments across four tasks and two models show that Filler Tokens consistently performs best, while continuous metrics generally outperform binary ones. The results highlight the need for more robust faithfulness metrics that are less sensitive to noise and model-specific variations.

## Method Summary
The CAUSALDIAGNOSTICITY framework evaluates faithfulness metrics by creating controlled pairs of faithful and unfaithful explanations through knowledge editing (ICE or MEMIT). For each task, counterfactual knowledge is injected into the model, then synthetic explanations are generated that are either aligned with the edited facts (faithful) or the original facts (unfaithful). The framework computes diagnosticity scores by measuring the proportion of pairs where a metric assigns higher faithfulness to the faithful explanation. The method supports multiple metrics including Simulatability, Filler Tokens, Adding Mistakes, and CC-SHAP, using a modular wrapper interface.

## Key Results
- Filler Tokens metric consistently achieves the highest diagnosticity scores across all four tasks (0.84 aggregate)
- Continuous metric variants outperform their binary counterparts in distinguishing faithful from unfaithful explanations
- The framework successfully identifies which metrics are more reliable for measuring faithfulness in CoT explanations

## Why This Works (Mechanism)
The framework works by establishing ground truth faithfulness through causal intervention - when knowledge is edited, the corresponding explanation must be faithful to the new knowledge. This creates a controlled experimental setup where the true faithfulness relationship is known. Metrics are then evaluated on their ability to recover this ground truth relationship. The diagnosticity score directly measures a metric's ability to distinguish between these controlled faithful/unfaithful pairs, providing a causal rather than correlational evaluation.

## Foundational Learning
- **Faithfulness vs. Plausibility in NLP**: An explanation can be fluent and sound correct (plausible) but not reflect the model's actual reasoning (unfaithful). Quick check: A model outputs "The answer is B because all mammals live on land" for a question about whales. The answer is correct. Is this explanation faithful? What else do you need to know?
- **Knowledge Editing (in LLMs)**: A technique to modify a model's internal factual knowledge without retraining the entire model. Quick check: How is model editing different from fine-tuning? What is a potential risk of applying an edit to a model?
- **Chain-of-Thought (CoT) Faithfulness Metrics**: Metrics designed to measure faithfulness in CoT explanations. Quick check: Describe the "Filler Tokens" metric. What does it measure and what is a key design choice one must make when using it?

## Architecture Onboarding
- **Component map**: Data & Knowledge Editing Module -> Faithfulness Metric Wrapper -> Diagnosticity Evaluator
- **Critical path**: (1) Knowledge Editing -> (2) Explanation Pair Generation -> (3) Diagnosticity Evaluation. If edits are unreliable or explanation pairs are not distinct, the entire evaluation is invalid.
- **Design tradeoffs**: ICE vs MEMIT (ICE chosen for flexibility, speed, and better performance on multi-step reasoning), Synthetic vs Model-Generated Explanations (synthetic ensures ground truth but reduces realism), Continuous vs Binary Scoring (continuous variants show better diagnosticity)
- **Failure signatures**: Unreliable edits (faithful explanation perplexity not lower than unfaithful), OOD corruption (Early Answering producing incomplete sentences), Noise sensitivity in continuous metrics
- **First 3 experiments**: 1) Validate the Editing Pipeline by checking edit reliability, 2) Baseline Metric Diagnosticity on FactCheck task, 3) Sensitivity Ablation for Filler Tokens with different corruption strategies

## Open Questions the Paper Calls Out
- **Open Question 1**: Can faithfulness metrics be developed to identify specific unfaithful segments within an explanation rather than assigning a single aggregate score? [explicit] The paper notes future work should focus on developing more interpretable faithfulness assessments revealing which parts of an explanation diverge from the model's actual reasoning.
- **Open Question 2**: How can continuous faithfulness metrics be redesigned to maintain high diagnosticity while minimizing sensitivity to noise and out-of-distribution perturbations? [explicit] The paper concludes that while continuous metrics outperform binary ones, they "can be overly sensitive to noise and model choice," creating a need for "more robust faithfulness metrics."
- **Open Question 3**: Can the Causal Diagnosticity framework be adapted to evaluate faithfulness metrics like Counterfactual Edits that inherently require regenerating explanations rather than scoring static text? [inferred] The Limitations section explicitly states that the framework "cannot evaluate metrics like Counterfactual Edits... because such metrics inherently require regenerating explanations."

## Limitations
- The synthetic explanations, while ensuring controlled faithful/unfaithful pairs, reduce ecological validity and may not transfer to real model-generated explanations
- The study evaluates only six metrics on four tasks, potentially limiting generalizability to other tasks, models, or metric variants
- Task-specific failures exist, such as edit reliability issues (<50% success rate) for the Analogy task and WikiData coverage limitations for Object Counting

## Confidence
- **High Confidence**: The core finding that continuous metrics outperform binary ones and that Filler Tokens is most diagnostic across tasks
- **Medium Confidence**: The claim that Filler Tokens is the "best" metric overall, though confidence intervals overlap with other metrics in some tasks
- **Low Confidence**: The transferability of synthetic evaluation results to real-world explanations, as the ablation study with model-generated explanations shows less consistency

## Next Checks
1. **Edit Reliability Validation**: For each task, compute the proportion of edited examples where the faithful explanation has lower perplexity than the unfaithful one. Diagnose and report task-specific failure rates.
2. **Implementation Sensitivity Test**: For Filler Tokens, systematically vary the corruption token type (dots, stars, dashes) and replacement strategy (repeating vs. non-repeating) and report how diagnosticity scores change.
3. **Real Explanation Transfer**: Generate model-generated explanations for a subset of the FactCheck task and compute diagnosticity scores. Compare these to the synthetic evaluation results to quantify the gap in ecological validity.