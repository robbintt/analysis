---
ver: rpa2
title: Multi-Step Visual Reasoning with Visual Tokens Scaling and Verification
arxiv_id: '2506.07235'
source_url: https://arxiv.org/abs/2506.07235
tags:
- visual
- reasoning
- reasoner
- sdpo
- verifier
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework for inference-time visual token
  scaling that enables multimodal large language models to perform iterative, verifier-guided
  reasoning over images. By formulating the task as a Markov Decision Process with
  a reasoner that proposes visual actions and a verifier trained via multi-step Direct
  Preference Optimization (DPO) that evaluates and terminates reasoning, the approach
  addresses the static nature of existing MLLM inference paradigms.
---

# Multi-Step Visual Reasoning with Visual Tokens Scaling and Verification

## Quick Facts
- arXiv ID: 2506.07235
- Source URL: https://arxiv.org/abs/2506.07235
- Reference count: 40
- Key outcome: +7.58% accuracy improvement on visual reasoning benchmarks through iterative visual token scaling and verifier-guided termination

## Executive Summary
This paper introduces VTS-V, a framework that enables multimodal large language models to perform iterative, verifier-guided visual reasoning by dynamically scaling visual tokens during inference. The approach formulates visual reasoning as a Markov Decision Process where a reasoner proposes visual actions (like ZoomIn, Grounding, OCR) and a verifier trained via multi-step Direct Preference Optimization evaluates these actions and decides when to terminate. The authors construct the VTS dataset containing 315K supervised reasoning trajectories and 301K preference-labeled comparisons, demonstrating significant performance gains across diverse visual reasoning benchmarks. The method addresses the static nature of existing MLLM inference paradigms and offers theoretical guarantees on bounded reasoning steps while providing more interpretable reasoning traces.

## Method Summary
VTS-V operates through a reasoner-verifier architecture where the reasoner (trained via supervised fine-tuning on VTS-SFT) generates planning text and selects from a toolkit of 10 vision tools (GroundingAction, DepthAction, ZoomInAction, OCR, Crop, Segment, Caption, Similarity, VisualSearch, Overlay) to iteratively expand visual context. At each step, the reasoner observes the current state (history of prior steps), generates planning text, selects an action, and receives a deterministic observation from the tool. The verifier (trained via multi-step DPO on VTS-DPO) assigns scalar rewards to partial trajectories using log-likelihood ratios and terminates reasoning when the reward difference between steps falls below threshold ε. The framework achieves bounded reasoning steps through theoretical guarantees under mild KL-divergence conditions between the reasoner and verifier policies.

## Key Results
- Qwen2.5-VL-7B achieves 56.65% accuracy on BLINK (up from 49.07% baseline)
- GPT-4o with VTS-V achieves 61.93% accuracy on BLINK (up from 54.35% baseline)
- Up to 7.58% improvement over strong baselines across visual reasoning benchmarks
- Consistent improvements on counting, functional correlation, and visual correlation tasks
- More interpretable reasoning traces compared to static CoT approaches

## Why This Works (Mechanism)

### Mechanism 1: Iterative Visual Token Expansion via Tool-Augmented Actions
The framework dynamically scales visual tokens during inference rather than encoding the full image upfront. At each reasoning step, the reasoner generates planning text and selects from 10 vision tools (GroundingAction, DepthAction, ZoomInAction, OCR, etc.) to obtain new visual observations. This iterative expansion enables finer-grained reasoning on tasks requiring localization, counting, or multi-step correlation by enriching the visual context with task-relevant evidence like cropped regions, depth maps, or segmentation masks.

### Mechanism 2: Multi-Step DPO Trains a Verifier to Assess Trajectory Quality and Trigger Early Stopping
The verifier is trained on preference pairs (τw, τl) where τw represents correct reasoning trajectories and τl represents synthetically corrupted ones. Using multi-step Direct Preference Optimization, the verifier learns to assign scalar rewards via log-likelihood ratios that approximate human-aligned quality assessment. During inference, the verifier terminates reasoning when the reward difference between consecutive steps falls below threshold ε, preventing both under-reasoning and wasted computation.

### Mechanism 3: Reasoner-Verifier Coupling Guarantees Bounded Reasoning Steps
The interaction between reasoner and verifier ensures finite reasoning depth almost surely. Under mild KL-divergence conditions (the reasoner's policy closer to the verifier's reward than to the base model), the verifier score forms a submartingale with bounded expectation, implying convergence. This provides theoretical guarantees that reasoning terminates after a finite number of steps while allowing variable-length inference appropriate to task complexity.

## Foundational Learning

- **Markov Decision Processes (MDPs)**
  - Why needed here: The entire VTS-V framework formalizes visual reasoning as an MDP with states (trajectory history), actions (tool calls), and rewards (verifier scores).
  - Quick check question: Can you explain how state transitions differ from standard RL settings (where observations are stochastic vs. deterministic tool outputs here)?

- **Direct Preference Optimization (DPO)**
  - Why needed here: The verifier is trained via multi-step DPO, not RL with explicit reward models. Understanding the Bradley-Terry model and KL-regularized objective is essential.
  - Quick check question: Why does DPO avoid training a separate reward model, and how does the paper extend DPO to multi-step trajectories?

- **Tool-Augmented / Visual Programming Paradigms**
  - Why needed here: The reasoner invokes external vision tools; this requires familiarity with prior work like VisProg, ViperGPT, and Visual Sketchpad.
  - Quick check question: How does VTS-V differ from Visual Sketchpad's multi-step approach in terms of termination criteria and verifier integration?

## Architecture Onboarding

- **Component map:** Base VLM → SFT-trained reasoner → Tool library (10 vision tools) → Verifier (DPO-trained) → Termination check → Output
- **Critical path:** Start with base VLM → Fine-tune reasoner on VTS-SFT → Train verifier on VTS-DPO → Deploy with threshold ε → Monitor reward differences for termination
- **Design tradeoffs:** Tool richness vs. error propagation (more tools increase expressivity but also failure points); Threshold ε vs. compute budget (low ε yields longer reasoning, high ε may truncate early); SFT-only vs. SFT+DPO (experiments show SFT alone insufficient for open-source models)
- **Failure signatures:** Applying VTS-V to untrained open-source models often degrades performance (Qwen2VL: 48.01%→38.54% on BLINK); Verifier removal causes consistent accuracy drops (Qwen2.5VL: 54.44%→47.09%); Empty or invalid tool outputs can break the observation pipeline
- **First 3 experiments:** 1) Sanity check on BLINK subset: Run VTS-V on 2-3 subtasks and compare against CoT baseline (expect +3-10% gains); 2) Ablate verifier: Disable verifier and use fixed step count (e.g., H=3) to measure accuracy drop; 3) Tool usage profiling: Log which tools are invoked per task type and verify appropriate tool selection

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the traditional sense. However, based on the methodology and results presented, several implicit questions emerge from the work:

- How sensitive is VTS-V performance to the choice of the reward difference threshold ε, and can this threshold be learned adaptively rather than predetermined?
- Why does VTS-V degrade performance on certain task types (e.g., MathVista for LLaMA3.2-11B), and can task-aware adaptation prevent this?
- Can the reasoner and verifier be trained jointly in an end-to-end manner to improve coordination, rather than through separate SFT and DPO stages?
- Does synthetic data generation using Qwen-2.5-VL-72B propagate or amplify the teacher model's biases and error patterns, and what is the upper bound on achievable performance?

## Limitations

- Dataset-dependent nature requiring large amounts of training data (315K trajectories for SFT, 301K preference pairs for DPO) that may not be available for all domains
- Reliance on tool reliability - invalid tool outputs (empty crops, malformed bounding boxes, segmentation failures) could break the reasoning chain
- Computational overhead of iterative tool invocation and verifier evaluation may limit practical deployment in latency-sensitive applications
- Theoretical termination guarantees assume specific KL-divergence conditions that may not hold in practice, particularly when reasoner and verifier become misaligned

## Confidence

- **High confidence:** The core MDP formulation (reasoner-verifier architecture), the existence of performance improvements on reported benchmarks, and the dataset construction methodology
- **Medium confidence:** The theoretical termination guarantees (Theorem 3.2) and the mechanism by which multi-step DPO produces meaningful reward signals
- **Low confidence:** The generalizability of performance gains beyond the evaluated benchmarks, particularly for domains requiring domain-specific tools not included in the current toolkit

## Next Checks

1. **Tool failure stress test:** Systematically introduce invalid tool outputs (empty crops, malformed bounding boxes, segmentation failures) into the inference pipeline and measure performance degradation to validate tool reliability requirements and identify failure propagation patterns.

2. **Cross-domain generalization:** Apply the trained VTS-V model to a novel visual reasoning task outside the BLINK/V*Bench/MMStar suite (e.g., medical imaging reasoning or scientific diagram analysis) and measure accuracy drop to test generalizability claims.

3. **Verifier threshold sensitivity:** Conduct a systematic sweep of the termination threshold ε across multiple orders of magnitude and plot the accuracy-latency tradeoff curve to quantify the practical impact of threshold selection on both performance and computational cost.