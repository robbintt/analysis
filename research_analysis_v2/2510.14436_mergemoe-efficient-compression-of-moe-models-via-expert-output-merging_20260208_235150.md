---
ver: rpa2
title: 'MergeMoE: Efficient Compression of MoE Models via Expert Output Merging'
arxiv_id: '2510.14436'
source_url: https://arxiv.org/abs/2510.14436
tags:
- experts
- merging
- mergemoe
- number
- compression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MergeMoE, a method to compress Mixture-of-Experts
  (MoE) models by merging experts based on their outputs rather than parameters. The
  key insight is that expert merging can be reformulated as inserting additional matrices
  into the forward computation, leading to an optimization formulation.
---

# MergeMoE: Efficient Compression of MoE Models via Expert Output Merging

## Quick Facts
- arXiv ID: 2510.14436
- Source URL: https://arxiv.org/abs/2510.14436
- Reference count: 23
- Primary result: Outperforms baselines (M-SMoE, Average, ZipIt) on NLP tasks while maintaining performance close to full MoE models

## Executive Summary
MergeMoE introduces an efficient compression method for Mixture-of-Experts (MoE) models by merging experts based on their outputs rather than parameters. The key insight is reformulating expert merging as inserting compression matrices into the forward computation, transforming a heuristic parameter averaging problem into a tractable linear optimization problem. Experiments on DeepSeekMoE, Qwen1.5-MoE, and Qwen3 demonstrate consistent improvements over baselines across multiple NLP tasks while maintaining performance close to full models.

## Method Summary
MergeMoE compresses MoE models by clustering experts and computing compression matrices that minimize output approximation error. The method uses usage-frequency weighted averaging for initial clustering, then optimizes compression matrices via least-squares. For each layer, T_1 is computed as T_1 = QP^† using calibration samples, while T_2 and T_3 are fixed as weighted averages. The approach preserves the routing mechanism while reducing expert count, achieving significant compression ratios without retraining.

## Key Results
- Consistently outperforms M-SMoE, Average, and ZipIt baselines across seven NLP tasks
- Maintains performance within 1-2% of full MoE models on ARC, Hellaswag, and other benchmarks
- Demonstrates cross-dataset generalization: compression using WinoGrande samples transfers to other tasks
- Shows sensitivity to calibration sample size: performance collapses below ~32 samples, recovers at 36 samples
- Enhanced with knowledge distillation for improved instruction-following tasks

## Why This Works (Mechanism)

### Mechanism 1
Reframing expert merging as output-space optimization reduces approximation error compared to parameter-space averaging. The method reformulates merging as inserting compression matrices B and A into forward pass: Y·B·A·mask_top_K(softmax(WrX))^T. This transforms heuristic parameter averaging into tractable linear optimization minimizing Frobenius norm between merged and original outputs.

### Mechanism 2
Usage-frequency weighted averaging is provably optimal for minimizing expected output distortion under the Quasi-Frobenius lower bound. Theorem 1 proves that setting weights v_i[j] = f_j / Σ_{k∈C_i} f_k minimizes the lower bound Y_0((BA-I_N)QF)×[f_1,...,f_N]^T via quadratic optimization with convexity guarantees.

### Mechanism 3
Least-squares optimization of T_1 compression matrix recovers fine-grained output approximation that weighted averaging alone cannot achieve. Given sampled inputs X̂, compute P = σ(T_2W'_G X̂) ⊙ (T_3W'_U X̂) and Q = σ(W'_G X̂) ⊙ (W'_U X̂), then solve T_1 = QP^† (Moore-Penrose pseudoinverse). This decouples T_1 optimization from non-linear T_2, T_3.

## Foundational Learning

- **MoE Routing and Sparse Activation**: Understanding top-K gating and routing weights is essential since MergeMoE preserves the routing mechanism while compressing experts. Quick check: Given 8 experts with top-2 routing, if experts 2 and 3 are merged into one cluster, what happens to their routing weights during forward pass?

- **SwiGLU MLP Architecture**: MergeMoE's theoretical analysis relies on E_i(X) = W_D(σ(W_G X) ⊙ (W_U X)); the Hadamard product structure constrains how compression matrices can be inserted. Quick check: Why does the Hadamard product ⊙ prevent T_2 and T_3 from being jointly optimized with T_1 via linear least squares?

- **Moore-Penrose Pseudoinverse and Least Squares**: The core optimization for T_1 reduces to solving T_1 P = Q; understanding when this has unique solution versus when regularization is needed is critical. Quick check: If P has rank deficiency due to insufficient samples, what happens to the closed-form solution T_1 = QP^†?

## Architecture Onboarding

- **Component map**: Input Layer -> Clustering Module -> Compression Matrix T_1 -> Compression Matrices T_2, T_3 -> Routing Update
- **Critical path**:
  1. Collect calibration samples (128 batch size, BFLOAT32 to fit GPU memory)
  2. Compute expert usage frequencies from calibration forward pass
  3. Cluster experts (top-M as centers, assign by W_U || W_G similarity)
  4. For each layer (back-to-front to avoid activation contamination):
     - Hook intermediate activations
     - Compute P, Q matrices
     - Solve T_1 = Q P^†
     - Release memory
  5. Update expert references to point to merged experts
- **Design tradeoffs**:
  - Compression ratio vs. accuracy: Reducing expert count per layer has larger impact than increasing compressed layers
  - Sample count vs. speed: Below ~32-36 samples, performance collapses; above threshold, diminishing returns
  - Clustering granularity: Using W_U || W_G similarity reduces approximation error but is more expensive
- **Failure signatures**:
  - Random-level performance: Sample count below critical threshold (~32 for Qwen1.5)
  - GPU OOM during T_1 computation: Batch size too large; reduce or use gradient checkpointing
  - Sudden drop on specific tasks: Clustering may have merged task-critical experts
- **First 3 experiments**:
  1. Sanity check on sample threshold: Run MergeMoE on Qwen1.5 WinoGrande with sample counts [16, 24, 32, 40, 64]
  2. Ablation on T_1 vs. weighted-only: Compare MergeMoE vs. variant where T_1 is also set to weighted average
  3. Cross-dataset generalization: Compress using only WinoGrande samples, evaluate on all 7 tasks

## Open Questions the Paper Calls Out

- Can the compression matrices T1, T2, T3 be jointly optimized rather than optimized separately, and would this improve compression quality? The non-linearity prevents closed-form solutions for joint optimization.

- What is the theoretical minimum sample size required for reliable compression, and can it be estimated a priori for different model architectures? The threshold appears empirically determined without theoretical analysis.

- How valid is the independence assumption between router logits and expert outputs in the theoretical analysis, and does violating it affect compression quality? The assumption simplifies analysis but may not hold in practice.

## Limitations
- Performance collapses sharply below 32-36 calibration samples, showing limited robustness to data scarcity
- Theoretical assumptions about expert-output routing independence may not hold in practice
- Layer-by-layer optimization requires significant memory and computational resources despite avoiding retraining

## Confidence

**High Confidence**: Core mathematical formulation and empirical results showing consistent improvement over baselines across multiple models and tasks.

**Medium Confidence**: Claims about cross-dataset generalization and specific sample threshold values, as these depend on experimental setup.

**Low Confidence**: Theoretical optimality claims under real-world conditions, particularly regarding independence assumptions between routing and expert outputs.

## Next Checks

1. **Sample Threshold Robustness**: Systematically vary calibration sample counts (16, 24, 32, 40, 64) on Qwen1.5-MoE across multiple tasks to precisely map performance collapse threshold.

2. **Task-Specific Routing Impact**: Design experiments where routing patterns are deliberately task-specific to test whether usage-frequency weighting remains optimal when expert-output routing independence is violated.

3. **Architecture Generalization**: Apply MergeMoE to MoE architectures with different expert computation patterns (e.g., Transformer-XL) to test whether theoretical framework extends beyond SwiGLU-based architectures.