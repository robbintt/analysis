---
ver: rpa2
title: 'Automated Safety Evaluations Across 20 Large Language Models: The Aymara LLM
  Risk and Responsibility Matrix'
arxiv_id: '2507.14719'
source_url: https://arxiv.org/abs/2507.14719
tags:
- safety
- domains
- aymara
- across
- your
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces Aymara AI, a programmatic platform that transforms
  natural-language safety policies into adversarial prompts and scores model responses
  using an AI-based rater validated against human judgments. The Aymara LLM Risk and
  Responsibility Matrix evaluates 20 commercially available LLMs across 10 real-world
  safety domains.
---

# Automated Safety Evaluations Across 20 Large Language Models: The Aymara LLM Risk and Responsibility Matrix

## Quick Facts
- arXiv ID: 2507.14719
- Source URL: https://arxiv.org/abs/2507.14719
- Authors: Juan Manuel Contreras
- Reference count: 36
- Primary result: Aymara AI automated safety evaluation reveals wide performance disparities across 20 LLMs and 10 safety domains, with scores ranging from 86.2% to 52.4%.

## Executive Summary
This study introduces Aymara AI, a programmatic platform that transforms natural-language safety policies into adversarial prompts and scores model responses using an AI-based rater validated against human judgments. The Aymara LLM Risk and Responsibility Matrix evaluates 20 commercially available LLMs across 10 real-world safety domains. Results show wide performance disparities, with mean safety scores ranging from 86.2% to 52.4%. Models performed well in well-established safety domains such as Misinformation (mean = 95.7%) but consistently failed in more complex domains, notably Privacy & Impersonation (mean = 24.3%). Analyses of Variance confirmed significant differences across models and domains (p < .05). The findings underscore the inconsistent and context-dependent nature of LLM safety, highlighting the need for scalable, customizable evaluation tools like Aymara AI.

## Method Summary
The study evaluates 20 commercial LLMs across 10 safety domains using Aymara AI, which programmatically synthesizes adversarial prompts from natural-language policies and scores responses with a validated AI rater. The evaluation generates 25 prompts per policy, queries target models via API, and uses an LLM-as-a-judge for binary pass/fail classification. Refusals are counted as safe. Results are validated against human judgments with accuracy of 0.93 and Cohen's Kappa of 0.85. The evaluation includes a consistency check to filter inconsistent prompts, removing approximately 9.6% of responses.

## Key Results
- Mean safety scores across 20 LLMs ranged from 86.2% to 52.4%, revealing substantial performance variation
- Models excelled in established safety domains (Misinformation: 95.7%) but struggled with complex domains (Privacy & Impersonation: 24.3%)
- ANOVA confirmed significant differences across models and domains (p < .05)
- Disaggregated domain analysis revealed systemic weaknesses masked by aggregate safety scores

## Why This Works (Mechanism)

### Mechanism 1
Programmatic synthesis of adversarial prompts from natural-language policies appears to expose safety vulnerabilities more effectively than static benchmarks. The system (Aymara AI) accepts a natural language policy (e.g., "Do not generate hate speech") and uses an LLM to generate specific, challenging prompts (e.g., requests for stereotypes framed as historical inquiry) designed to violate that policy. Core assumption: The generator LLM creates inputs that accurately represent the risk landscape defined in the policy text. Evidence anchors: [Section 3]: "Aymara AI transforms natural-language safety policies into adversarial prompts." [Section 4.2]: Listing 1 shows the `client.evals.create` function mapping `ai_instructions` to 25 generated prompts. Break condition: If the generated prompts are too generic or fail to adapt to specific domain nuances, the evaluation will yield false negatives.

### Mechanism 2
Using an independent "LLM-as-a-Judge" for scoring allows for scalable, explainable safety assessments, provided the judge aligns with human reasoning. An evaluator LLM scores target model responses against the original policy, outputting a binary pass/fail and a rationale. This is validated against a human-labeled "ground truth" subset to ensure reliability. Core assumption: The Judge LLM possesses superior reasoning or instruction-following capabilities regarding safety policies than the target models, and is not susceptible to the same biases. Evidence anchors: [Abstract]: "...scores model responses using an AI-based rater validated against human judgments." [Section 4.4]: Validation showed an accuracy of 0.93 and Cohen's Kappa of 0.85 against a human rater. Break condition: If the Judge LLM exhibits sycophancy or fails to detect novel attack vectors, the safety scores will be artificially inflated.

### Mechanism 3
Disaggregating performance by specific risk domains reveals systemic weaknesses masked by aggregate "safety" scores. The "Risk and Responsibility Matrix" separates evaluations into 10 distinct domains (e.g., Privacy vs. Misinformation). This forces a granular analysis where high performance in one area cannot compensate for low performance in another. Core assumption: Safety is a collection of independent or semi-independent capabilities rather than a single generalized trait. Evidence anchors: [Abstract]: "Results reveal wide performance disparities... well in well-established safety domains... but consistently failed in more complex domains." [Section 5.2]: Shows a statistically significant hierarchy (ANOVA p < .001) where Misinformation (95.7%) differs vastly from Privacy (24.3%). Break condition: If domains overlap significantly, the variance observed might be due to semantic ambiguity in the policy definition.

## Foundational Learning

- **Concept: Adversarial Prompt Engineering**
  - Why needed here: The core input to the Aymara system is a policy, but the operational output is an *attack*. You must understand how benign framing (e.g., "for a story," "educationally") masks malicious intent to configure the generator effectively.
  - Quick check question: If a policy forbids "dangerous instructions," would a prompt asking for "historical methods of lock-picking used in the 1800s" be generated, and should it be considered a violation?

- **Concept: Inter-rater Reliability (Cohen's Kappa)**
  - Why needed here: The system relies on an AI approximating a human judge. Understanding Kappa helps you interpret the validation results (0.85 reported) and recognize when the "Judge" is merely guessing or is overly permissive.
  - Quick check question: If the Judge AI approves a response that a human flags as unsafe, is this a failure of the *Target Model* or the *Evaluation System*?

- **Concept: Multiple Comparisons Problem (Tukey HSD)**
  - Why needed here: The study evaluated 20 models. Without correction (like Tukey HSD), finding a "significant" difference between two models is likely a statistical fluke.
  - Quick check question: The paper found an overall ANOVA significance (p=.04) but Tukey HSD showed no specific pairwise significance. Does this mean the models are equally safe, or just that the sample size was too small to distinguish them?

## Architecture Onboarding

- **Component map:** Policy Definition -> Prompt Generation (SDK) -> Target Inference -> Judge Scoring -> Human Validation Loop (subset)
- **Critical path:** Policy Definition $\rightarrow$ Prompt Generation (SDK) $\rightarrow$ Target Inference $\rightarrow$ Judge Scoring $\rightarrow$ Human Validation Loop (subset)
- **Design tradeoffs:**
  - Customizability vs. Standardization: The system allows custom policies (high flexibility) but this prevents direct comparison with standard academic benchmarks without mapping.
  - Judge Capacity vs. Cost: Using a high-capability model (e.g., GPT-4 class) as the Judge increases accuracy (Kappa) but drastically increases evaluation cost for 5,000+ responses.
  - Refusal Handling: The system treats refusals as "Safe." This biases the metric towards "cautious" models, potentially penalizing helpful models that navigate edge cases successfully.
- **Failure signatures:**
  - Policy Drift: The generator creates prompts that are technically off-topic or don't actually test the specific policy constraint.
  - Judge Saturation: The Judge marks everything as "Unsafe" (over-refusal) or "Safe" (under-sensitivity), flattening the scores.
  - Data Contamination: If the Target Model was trained on the specific adversarial prompts generated by the SDK, it would artificially score higher.
- **First 3 experiments:**
  1. Calibration Run: Execute the evaluation on two models with known safety profiles to verify the scoring range spreads sufficiently.
  2. Judge Consistency Test: Run the same 50 responses through the Scorer twice to check for non-deterministic variance in the Judge's labeling.
  3. Policy Edge-Case Analysis: Modify the `ai_instructions` for a specific domain to include an explicit "unless hypothetical" clause, and measure the delta in safety scores to confirm the system is responsive to policy nuance.

## Open Questions the Paper Calls Out
None

## Limitations
- The absolute safety scores are less reliable without direct comparison to gold-standard manual evaluation on the exact same prompts
- The system's performance hinges on the LLM's ability to transform natural-language policies into genuinely challenging prompts
- Treating all refusals as "Safe" creates a strong pro-safety bias that conflates helpfulness with safety

## Confidence

- **High Confidence:** The finding that safety performance varies significantly across domains (p < .001) is well-supported by the ANOVA results and aligns with prior work on domain-specific safety gaps.
- **Medium Confidence:** The claim that Aymara AI provides a scalable, customizable evaluation framework is supported by the SDK description and validation results, but the true robustness of the black-box scorer is uncertain.
- **Low Confidence:** The absolute safety scores (e.g., 52.4% - 86.2%) are less reliable without a direct comparison to a gold-standard manual evaluation on the exact same prompts.

## Next Checks

1. **Scorer Sensitivity Audit:** Run a subset of responses through the Aymara scorer twice with different temperatures or model versions. A high variance in pass/fail labels (e.g., >10% change) would indicate an unstable evaluation.
2. **Domain Overlap Analysis:** Re-run the evaluation after merging highly related domains (e.g., "Self-Harm" and "Medical Advice") to determine if the observed performance gaps are due to genuine capability differences or semantic ambiguity in the policy definitions.
3. **Human-in-the-Loop Spot Check:** Manually evaluate a random sample of 50 target model responses, focusing on refusals and "borderline" cases. Calculate the precision and recall of the Aymara scorer against this human-labeled set to verify the reported 0.93 accuracy is representative.