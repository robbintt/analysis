---
ver: rpa2
title: Convergence Rates for Softmax Gating Mixture of Experts
arxiv_id: '2503.03213'
source_url: https://arxiv.org/abs/2503.03213
tags:
- expert
- page
- experts
- function
- gating
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the convergence rates of parameter and expert
  estimation under mixture of experts (MoE) models with softmax gating mechanisms.
  The authors examine standard softmax gating, dense-to-sparse gating, and hierarchical
  softmax gating variants.
---

# Convergence Rates for Softmax Gating Mixture of Experts

## Quick Facts
- arXiv ID: 2503.03213
- Source URL: https://arxiv.org/abs/2503.03213
- Reference count: 40
- Primary result: Polynomial convergence for strongly identifiable experts; exponential slowdown for linear experts

## Executive Summary
This paper establishes convergence rates for parameter and expert estimation in Mixture of Experts (MoE) models with softmax gating mechanisms. The authors demonstrate that strongly identifiable expert functions (including two-layer feed-forward networks with GELU or sigmoid activation) achieve polynomial convergence rates requiring only polynomially many data points. In contrast, linear experts exhibit exponentially slow convergence requiring exponentially many data points due to intrinsic parameter interactions. The work provides theoretical guidance for designing efficient expert and gating structures in MoE models.

## Method Summary
The paper analyzes convergence rates for MoE models using least squares estimation to minimize the sum of squared errors. The analysis considers standard softmax gating, dense-to-sparse gating with temperature parameters, and hierarchical softmax gating variants. The authors establish a "strong identifiability" condition that ensures polynomial convergence rates and examine how parameter interactions between gating and expert components affect convergence. Theoretical results are derived using Voronoi loss functions and partial differential equation analysis to characterize the statistical efficiency of different MoE configurations.

## Key Results
- Strongly identifiable expert functions achieve polynomial convergence rates (O_P([log(n)/n]^(1/2)))
- Linear experts exhibit exponentially slow convergence (O_P(1/log^λ(n))) due to parameter interactions
- Dense-to-sparse gating with linear routers causes slow convergence regardless of expert structure, but can be fixed using algebraically independent router-expert pairs
- Similar patterns hold for hierarchical MoE where strongly identifiable experts maintain polynomial rates while linear experts suffer slow convergence

## Why This Works (Mechanism)

### Mechanism 1: Strong Identifiability Enables Polynomial Convergence
The strong identifiability condition ensures that expert functions and their partial derivatives form linearly independent sets, eliminating local parameter interactions that obscure distinctions during Taylor expansions of regression discrepancy.

### Mechanism 2: Parameter Interaction Causes Exponential Slowdown
Linear experts violate strong identifiability, creating intrinsic PDE interactions between gating and expert parameters that prevent isolation of parameter error terms, resulting in exponentially slow convergence.

### Mechanism 3: Algebraic Independence Fixes Dense-to-Sparse Gating
Using a general router (e.g., non-linear activation) instead of a linear router breaks the temperature-gating interaction via algebraic independence, restoring polynomial convergence rates.

## Foundational Learning

### Concept: Strong vs. Weak Identifiability
Why needed: Theoretical pivot distinguishing efficient (polynomial) from inefficient (exponential) learning in MoE.
Quick check: Can you distinguish two parameters by looking at first and second derivatives of expert function? (Yes = Strong).

### Concept: Voronoi Loss Function
Why needed: Custom loss based on Voronoi cells to precisely measure parameter estimation rates.
Quick check: Does the loss function penalize parameters in over-specified Voronoi cells differently than exactly-specified ones? (Yes, squared error vs linear).

### Concept: Partial Differential Equations (PDEs) in Parameter Interactions
Why needed: Understanding why linear experts fail requires grasping how parameter derivatives create mathematical dependencies.
Quick check: Does the interaction ∂²F/∂ω∂b = ∂F/∂a help or hurt convergence? (It hurts/slows it down).

## Architecture Onboarding

### Component map:
Gating Network -> Experts -> Parameter Estimation

### Critical path:
Select expert structure (Strongly Identifiable) → Select router (Algebraically Independent if using Temperature) → Verify derivatives are non-zero/independent

### Design tradeoffs:
- Complexity vs. Speed: Linear experts are simple but statistically inefficient; two-layer FFNs are slightly more complex but statistically efficient
- Dense-to-Sparse Stability: Adding temperature stabilizes training but introduces "Linear Router Trap"; must upgrade router to maintain efficiency

### Failure signatures:
- Slow Convergence: Model learns exceptionally slowly despite large data volumes (characteristic of linear experts)
- Parameter Ambiguity: Gating weights fail to specialize because parameters are interacting (PDE coupling)

### First 3 experiments:
1. Expert Ablation: Train MoE with Linear Experts vs. GELU Experts on synthetic regression task; plot parameter estimation error vs. sample size n
2. Router Interaction Test: Implement Dense-to-Sparse gating with Linear Router vs. Sigmoid Router; monitor for temperature-gating interaction causing slow convergence
3. Hierarchical Stress Test: Build 2-level HMoE with linear experts at leaf nodes; check if parameter estimation errors propagate compared to flat MoE

## Open Questions the Paper Calls Out

### Open Question 1
Can the convergence analysis be extended to settings where the regression function takes an arbitrary form rather than being generated by a well-specified MoE model? Current techniques are tailored to well-specified scenarios; non-convexity of parameter space makes analyzing arbitrary regression functions mathematically intractable without new methods.

### Open Question 2
What are the convergence rates for expert estimation in deep architectures utilizing multiple stacked MoE layers? Stacking layers introduces complex interactions between gating and expert parameters across levels that may violate identifiability conditions.

### Open Question 3
Do the derived convergence rates for the global least squares estimator hold for the local minima typically found by gradient-based optimization in practice? The theoretical rates rely on the estimator being a global minimizer; it is unknown if local minima prevalent in MoE training landscapes satisfy the Voronoi loss bounds necessary for polynomial rates.

## Limitations
- Assumes strongly identifiable expert functions and specific distributional conditions (bounded input support, Gaussian noise)
- Results may not extend to non-differentiable experts or heavy-tailed noise distributions
- Does not address practical implementation challenges like local minima in non-convex optimization

## Confidence
- **High confidence**: Polynomial convergence rates for strongly identifiable experts (Theorems 1-2) - rigorous mathematical proofs with theoretical literature support
- **Medium confidence**: Exponential slowdown for linear experts (Theorem 4) - theoretically sound but requires careful parameter initialization verification
- **Medium confidence**: Dense-to-sparse gating limitations (Theorem 3) - theoretically proven but implementation details may affect PDE interactions

## Next Checks
1. **Empirical verification of convergence rates**: Implement synthetic experiments comparing linear vs. two-layer FFN experts to empirically validate polynomial vs. exponential convergence patterns
2. **Router interaction testing**: Systematically test dense-to-sparse gating with linear, sigmoid, and MLP routers to confirm algebraic independence requirement for polynomial rates
3. **Hierarchical scaling study**: Construct multi-level MoE models to validate Theorem 6's prediction that linear experts at any level cause slow convergence across the entire hierarchy