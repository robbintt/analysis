---
ver: rpa2
title: 'Prior-Aligned Meta-RL: Thompson Sampling with Learned Priors and Guarantees
  in Finite-Horizon MDPs'
arxiv_id: '2510.05446'
source_url: https://arxiv.org/abs/2510.05446
tags:
- prior
- lemma
- algorithm
- mtsrl
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses meta-reinforcement learning in finite-horizon
  MDPs where tasks share structure in their optimal Q-functions. The authors propose
  Thompson-style algorithms that learn Gaussian priors over optimal Q-parameters across
  tasks, enabling transfer learning.
---

# Prior-Aligned Meta-RL: Thompson Sampling with Learned Priors and Guarantees in Finite-Horizon MDPs

## Quick Facts
- arXiv ID: 2510.05446
- Source URL: https://arxiv.org/abs/2510.05446
- Authors: Runlin Zhou; Chixiang Chen; Elynn Chen
- Reference count: 40
- Primary result: Meta-regret guarantees for Thompson sampling with learned Gaussian priors in finite-horizon MDPs

## Executive Summary
This paper introduces Thompson Sampling with Learned Priors (TSRL) for meta-reinforcement learning in finite-horizon MDPs where tasks share structure in their optimal Q-functions. The authors propose two algorithms: MTSRL for known covariance and MTSRL+ for unknown covariance, both using a novel prior-alignment technique to achieve meta-regret guarantees. Through simulations on a stateful recommendation environment, they demonstrate that these algorithms significantly outperform prior-independent RL baselines by leveraging shared structure across tasks.

## Method Summary
The method employs Thompson-style posterior sampling where tasks share a Gaussian prior over optimal Q*-parameters. MTSRL learns only the prior mean assuming known covariance, while MTSRL+ additionally estimates the covariance with prior widening for robustness. The key innovation is a prior-alignment technique that couples the learned-prior posterior to a meta-oracle with the true prior, yielding theoretical meta-regret guarantees. For known covariance, MTSRL achieves Õ(H⁴S³/²√(ANK)) meta-regret; for unknown covariance, MTSRL+ achieves Õ(H⁴S³/²√(AN³K)).

## Key Results
- MTSRL achieves Õ(H⁴S³/²√(ANK)) meta-regret with known covariance
- MTSRL+ achieves Õ(H⁴S³/²√(AN³K)) meta-regret with unknown covariance
- Simulations show MTSRL/MTSRL+ significantly outperform prior-independent RL and bandit-only baselines
- Meta-regret bounds follow theoretical predictions with proper scaling in task count K

## Why This Works (Mechanism)

### Mechanism 1: Prior Alignment via Change of Measure
The paper achieves meta-regret guarantees by coupling the algorithm's learned-prior posterior to a meta-oracle with the true prior. Using a change-of-measure argument, they map the noise realization of the learned-prior algorithm to the noise realization of the true-prior oracle. This ensures the "statistical distance" between these two imaginary trajectories is bounded, allowing the regret of the learner to be bounded by the regret of the oracle plus a penalty proportional to the prior estimation error. The Gaussian assumptions enable closed-form linear mapping between noise realizations.

### Mechanism 2: Prior Widening (Covariance Inflation)
MTSRL+ explicitly inflates the estimated covariance matrix by adding a scalar multiple of the identity matrix to preserve theoretical guarantees when the true covariance is unknown. This over-exploration ensures the estimated posterior is "wider" than the true posterior, preventing premature convergence to suboptimal actions due to underestimated uncertainty and keeping alignment importance weights finite.

### Mechanism 3: Hierarchical OLS Aggregation
The algorithm uses hierarchical averaging of task-specific Ordinary Least Squares estimates to provide consistent estimation of the shared meta-prior mean. Each task produces an OLS estimate, which are then averaged across tasks. The Law of Large Numbers ensures this average converges to the true shared prior mean as the number of tasks grows, enabling effective transfer learning.

## Foundational Learning

### Concept: Thompson Sampling (Posterior Sampling)
**Why needed here:** This is the base exploration strategy. Instead of using optimism (UCB) or ε-greedy, the agent samples model parameters from its belief distribution and acts optimally according to that sample.
**Quick check question:** How does sampling from a posterior distribution encourage exploration compared to selecting the action with the highest expected value?

### Concept: Linear MDP / Linear Q*
**Why needed here:** The entire theoretical machinery relies on the fact that the value function Q* can be represented as a linear combination of features Φ and a parameter vector θ.
**Quick check question:** If Q(s,a) = Φ(s,a)ᵀθ, how does updating θ affect the value of states not explicitly visited?

### Concept: Meta-Regret
**Why needed here:** This is the performance metric. Unlike standard regret (loss vs. optimal policy for one task), meta-regret measures the loss against an oracle that already knows the shared structure across many tasks.
**Quick check question:** Why is "meta-regret" a better metric for this algorithm than the average standard regret of each task?

## Architecture Onboarding

### Component map:
Task Buffer -> Prior Estimator -> Widener -> Posterior Sampler -> Actor

### Critical path:
1. **Warm-up:** For first K₀ tasks, run standard RLSVI (ignore prior)
2. **Prior Update:** After task k finishes, compute OLS estimate θ̇^(k). Update global average θ̄
3. **Covariance Update (MTSRL+):** Update global covariance estimate Σ̄ and apply widening
4. **Inference:** For new task k+1, initialize posterior with (θ̄, Σ_w). Sample and act

### Design tradeoffs:
- **MTSRL vs. MTSRL+:** MTSRL assumes known covariance (faster convergence Õ(√K)) but is brittle if the variance assumption is wrong. MTSRL+ learns covariance (slower Õ(√KN²)) but is robust.
- **Widening (w):** Higher w increases exploration (safer but slower); lower w risks under-exploration if covariance estimate is tight but wrong.

### Failure signatures:
- **Stagnant Regret:** Regret curve flattens and doesn't follow √K decay. Diagnosis: Prior estimation error is not decreasing.
- **Covariance Collapse:** Algorithm becomes deterministic (no exploration). Diagnosis: Covariance estimate became non-positive definite or w is insufficient.
- **High Variance:** Performance varies wildly between runs. Diagnosis: Initialization phase K₀ too short.

### First 3 experiments:
1. **Baseline Replication:** Implement the "Stateful Recommendation" environment. Plot Meta-Regret vs. Task Count K for MTSRL+ vs. RLSVI (prior-free). Verify the gap widens as K increases.
2. **Widening Ablation:** Run MTSRL+ with w=0 (no widening) on the same environment. Observe if regret diverges or stalls compared to w > 0.
3. **Task Scaling:** Measure Meta-Regret at K=50 vs K=200. Confirm the slope on a log-log plot corresponds to the theoretical √K or √N³K scaling.

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** Can the theoretical guarantees for prior-aligned meta-RL be extended to non-linear function approximation classes (e.g., neural networks)?
**Basis in paper:** [explicit] The conclusion states that future work includes "nonlinear function classes."
**Why unresolved:** The current theoretical analysis and algorithm design rely strictly on linear structure for closed-form OLS estimation and Gaussian posterior updates.
**What evidence would resolve it:** A derivation of meta-regret bounds for a neural tangent kernel (NTK) setting or empirical evidence showing the prior-alignment technique remains robust without the linear assumption.

### Open Question 2
**Question:** Can the meta-regret bound for unknown covariance (MTSRL+) be tightened from Õ(√N³) to Õ(√N) to match the known covariance setting?
**Basis in paper:** [inferred] Theorem 3 shows a dependence on N³/² for unknown covariance, whereas Theorem 2 has N¹/². The paper notes this gap arises from the warm-up requirement K₁ ≈ Õ(H²N²).
**Why unresolved:** The current analysis requires a large number of tasks to estimate the covariance matrix stably under Bellman error propagation and prior widening.
**What evidence would resolve it:** A refined proof technique that controls covariance estimation error without requiring N² tasks for initialization.

### Open Question 3
**Question:** Does the prior-alignment technique apply effectively to infinite-horizon discounted MDPs or continuing environments?
**Basis in paper:** [explicit] The conclusion lists "adaptive horizons" as a direction for future work. [inferred] The analysis depends heavily on decoupling errors across finite stages h ∈ [H].
**Why unresolved:** The current proofs utilize backward induction and stage-specific Bellman dependencies that do not directly translate to stationary, infinite-horizon settings.
**What evidence would resolve it:** Extending the definitions of meta-regret and the prior alignment lemmas to the discounted setting while maintaining √T-rate bounds.

## Limitations
- Theoretical guarantees critically depend on the prior alignment assumption, which may fail if the meta-prior is too diffuse relative to task-level noise
- Linear MDP assumption may not hold in many real-world scenarios, limiting practical applicability
- Algorithm assumes stationary task distributions and known feature matrices Φ_h, which may not be realistic in dynamic environments

## Confidence

### High Confidence:
- The prior alignment technique (change-of-measure argument) is mathematically sound given the Gaussian assumptions
- The posterior sampling framework and meta-regret formulation are well-established in the literature

### Medium Confidence:
- The covariance widening analysis in MTSRL+ is rigorous, but the practical choice of widening parameter w remains somewhat heuristic
- The linear MDP assumption, while common, may not generalize well

### Low Confidence:
- The experimental evaluation is limited to a single synthetic recommendation environment
- The generalizability to other domains and the impact of misspecified priors are not thoroughly explored

## Next Checks

1. **Alignment Robustness Test:** Implement a variant where tasks are drawn from two distinct clusters (rather than one meta-prior). Measure how quickly MTSRL/MTSRL+ degrade compared to prior-independent methods, validating the cluster assumption's importance.

2. **Feature Mismatch Analysis:** Modify the recommendation environment to use a different feature matrix Φ' for some tasks. Measure the meta-regret and compare against theoretical predictions to quantify sensitivity to feature misspecification.

3. **Scaling Law Verification:** Run experiments varying S (state space size) and A (action space size) independently. Plot meta-regret vs. √(S³/²√(ANK)) and √(AN³K) respectively to confirm the predicted scaling behavior.