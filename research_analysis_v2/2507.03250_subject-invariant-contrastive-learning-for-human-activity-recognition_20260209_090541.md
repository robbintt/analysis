---
ver: rpa2
title: Subject Invariant Contrastive Learning for Human Activity Recognition
arxiv_id: '2507.03250'
source_url: https://arxiv.org/abs/2507.03250
tags:
- contrastive
- learning
- subject
- data
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Subject-Invariant Contrastive Learning (SICL)
  to address the challenge of subject variability in human activity recognition. The
  key insight is that standard contrastive learning methods fail to generalize to
  unseen subjects because they capture subject-specific variations rather than activity-specific
  features.
---

# Subject Invariant Contrastive Learning for Human Activity Recognition

## Quick Facts
- **arXiv ID**: 2507.03250
- **Source URL**: https://arxiv.org/abs/2507.03250
- **Authors**: Yavuz Yarici; Kiran Kokilepersaud; Mohit Prabhushankar; Ghassan AlRegib
- **Reference count**: 0
- **Primary result**: Improves HAR performance by up to 11% over traditional contrastive learning by addressing subject variability

## Executive Summary
This paper addresses a critical limitation in human activity recognition: standard contrastive learning methods capture subject-specific variations rather than activity-specific features, leading to poor generalization on unseen subjects. The authors propose Subject-Invariant Contrastive Learning (SICL), which modifies the contrastive loss by decomposing negative samples into same-subject and different-subject groups, then reweighting the same-subject negatives to encourage greater separation. This promotes learning subject-invariant representations that generalize across subjects.

SICL is evaluated on three public benchmarks (UTD-MHAD, MMAct, DARai) and demonstrates consistent improvements of up to 11% over traditional contrastive learning methods. The approach is shown to be adaptable across various settings including multimodal scenarios, supervised contrastive learning, and pre-training for downstream tasks, consistently outperforming baseline approaches in linear evaluation protocols and supervised classification tasks.

## Method Summary
SICL modifies standard contrastive learning by decomposing negative samples into same-subject and different-subject groups within each batch. For each anchor sample, it computes a weighting function Q_Si that normalizes and reweights the same-subject negative samples in the denominator of the InfoNCE loss. This encourages the model to push apart samples from the same subject while maintaining activity-specific similarity. The method uses 1D-CNNs with Transformers for inertial data and Co-occurrence feature learning networks for skeleton data, with all sequences resampled to 100 timesteps. Subject IDs are required during training to enable the negative sample decomposition.

## Key Results
- Improves performance by up to 11% over traditional contrastive learning methods on three benchmark datasets
- Demonstrates consistent improvements across inertial, skeleton, and pressure sensor modalities
- Outperforms baseline approaches in both linear evaluation protocols and supervised classification tasks
- Shows better generalization to unseen subjects compared to standard contrastive learning

## Why This Works (Mechanism)
Standard contrastive learning fails in HAR because it learns representations that capture both activity and subject-specific variations. When training and test sets have different subjects, this leads to poor generalization. SICL addresses this by explicitly encouraging separation between samples from the same subject in the embedding space, forcing the model to learn features that are invariant to subject identity while preserving activity-discriminative information. The reweighting function Q_Si amplifies the contrastive pressure on same-subject negatives, effectively suppressing subject-specific cues in the learned representations.

## Foundational Learning

- **Concept**: Contrastive Learning (Self-Supervised)
  - **Why needed here**: This is the core framework SICL modifies. You must understand that contrastive learning works by learning an embedding space where "positive" pairs (augmented views of the same sample) are close, and "negative" pairs (all other samples in a batch) are far apart. SICL operates by changing *how* the negative pairs are treated.
  - **Quick check question**: Can you explain, in one sentence, what the "positive" and "negative" pairs are in a standard contrastive learning setup for a time-series signal?

- **Concept**: Distribution Shift / Domain Shift
  - **Why needed here**: The core problem this paper solves is a form of domain shift. The data distribution for a new, unseen subject is different from the subjects in the training set. The model must learn a representation that is invariant to this shift (i.e., works across domains).
  - **Quick check question**: If you train a model on data from 10 subjects and test it on an 11th subject, why might performance drop, even if the 11th subject is performing the same actions?

- **Concept**: Representation Learning & Linear Evaluation Protocol
  - **Why needed here**: The paper uses a linear evaluation protocol to measure the quality of the learned representations. This means the encoder is frozen, and only a simple linear classifier is trained on top. This ensures that improvements are due to better features from the encoder, not from a more powerful classifier.
  - **Quick check question**: Why is a linear evaluation protocol often preferred for comparing the quality of features learned by different self-supervised methods?

## Architecture Onboarding

- **Component Map**: Raw Sensor Data -> Data Augmentation -> Encoder Network (1D-CNN + Transformer) -> Projection Network -> SICL Loss Module -> Embeddings
- **Critical Path**: The critical path for a new engineer is implementing the Q_Si weighting function correctly. The formula for it involves normalizing exponentiated cosine similarities of same-subject negatives. A mistake here will break the entire method.
- **Design Tradeoffs**:
  - **Simplicity vs. Information Loss**: SICL is a simple loss modification, which makes it easy to implement and integrate. The trade-off is that it may be a less powerful approach than a more complex adversarial training regime for disentanglement. The paper argues simplicity and effectiveness are preferred.
  - **Reliance on Subject IDs**: The method requires subject IDs to be known at training time. This is a minimal form of supervision that is often available (you know who you collected data from).
- **Failure Signatures**:
  - **Performance Collapse (Mode Collapse)**: If the re-weighting Q_Si is too aggressive, it might push all same-subject samples infinitely far apart, collapsing the embedding space. This would likely manifest as a failure to train (loss diverges) or random performance.
  - **No Improvement Over Baseline**: If the re-weighting is too weak, you will see no performance gain over a standard SimCLR baseline. This indicates the hyperparameter for the weighting (implicitly controlled by the normalization) is not set correctly.
  - **Degraded Same-Subject Performance**: A significant drop in performance on a *seen-subject* test set could indicate the model is now underfitting the training distribution by being overly focused on generalization.
- **First 3 Experiments**:
  1. **Baseline Reproduction**: Implement a standard SimCLR pipeline for the UTD-MHAD dataset. Reproduce the baseline numbers from Table 1 to ensure your encoder, augmentation, and basic contrastive loss are working correctly.
  2. **Implement SICL Loss**: Add the subject decomposition and Q_Si weighting logic to your contrastive loss function. Run the same experiment as #1 and verify that you see a performance improvement, matching the paper's claim of ~2% gain on UTD-MHAD.
  3. **Ablation on Weighting**: The paper's description of Q_Si is high-level. An important first experiment is to ablate this component. Try a simplified version: simply multiply the loss term for same-subject negatives by a fixed scalar (e.g., 2.0 or 5.0). Does this simpler re-weighting also provide a benefit? This helps isolate the *mechanism* of re-weighting from the specific Q_Si formula used.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does SICL perform when subject labels are unavailable, noisy, or only partially annotated during training?
- **Basis in paper**: [explicit] The method explicitly "modifies the standard contrastive loss function to effectively utilize subject labels" and decomposes negatives based on subject identity.
- **Why unresolved**: Real-world HAR deployments may lack reliable subject metadata, yet the method fundamentally depends on this information for negative reweighting.
- **What evidence would resolve it**: Experiments with varying levels of subject label availability (e.g., 25%, 50%, 75% labeled) and robustness analysis to label noise.

### Open Question 2
- **Question**: How sensitive is the batch-wise weighting function QSi to batch size and subject distribution within batches?
- **Basis in paper**: [inferred] The paper states QSi "is calculated batch-wise" and constructed through normalization, but provides no analysis of how batch composition affects the weighting stability or learning dynamics.
- **Why unresolved**: Uneven subject representation in batches could lead to inconsistent reweighting, potentially affecting convergence and representation quality.
- **What evidence would resolve it**: Ablation studies varying batch sizes and enforcing different subject-per-batch ratios, measuring variance in learned representations and downstream performance.

### Open Question 3
- **Question**: Can SICL's subject-invariance principle be extended to address other domain shifts in HAR, such as sensor placement variations, device heterogeneity, or environmental factors?
- **Basis in paper**: [inferred] The paper frames the problem as addressing "distributional shifts caused by subject variability" but acknowledges broader domain shift challenges in HAR literature.
- **Why unresolved**: The method specifically targets subject identity, while real-world deployments face multiple simultaneous domain shift sources that may interact.
- **What evidence would resolve it**: Experiments on datasets with annotated sensor placement positions or device types, applying analogous domain-aware negative reweighting strategies.

### Open Question 4
- **Question**: Does aggressive suppression of subject-specific features risk discarding genuinely useful activity-relevant information that correlates with subject characteristics?
- **Basis in paper**: [inferred] The method "encourages greater separation between samples from the same subject" to suppress subject-specific cues, but some subject-specific patterns (e.g., gait dynamics) may be activity-discriminative.
- **Why unresolved**: The trade-off between subject invariance and preserving all discriminative activity features remains unquantified.
- **What evidence would resolve it**: Analysis of feature importance before and after SICL training, and comparison with methods that explicitly model subject-activity feature sharing.

## Limitations
- **Reliance on subject ID availability**: The method fundamentally depends on having subject identity information during training, which may not always be available in real-world deployment scenarios.
- **Sensitivity to batch composition**: The effectiveness of the Q_Si weighting function may be affected by how subjects are distributed across training batches, potentially leading to inconsistent learning dynamics.
- **Potential information loss**: Aggressive suppression of subject-specific features might inadvertently discard activity-relevant information that correlates with subject characteristics.

## Confidence
- **High Confidence**: The core experimental results showing SICL outperforming standard contrastive learning methods on the three benchmark datasets (UTD-MHAD, MMAct, DARai). The linear evaluation protocol results are robust and well-documented.
- **Medium Confidence**: The generalizability claims across different modalities and settings. While the paper demonstrates effectiveness across inertial, skeleton, and pressure data, the specific implementations for each modality are not fully detailed in the provided text.
- **Low Confidence**: The exact formulation and hyperparameter sensitivity of the Q_Si weighting function, as the precise mathematical operations are not explicitly specified in the available information.

## Next Checks
1. **Hyperparameter Sensitivity Analysis**: Systematically vary the temperature parameter Ï„ and the normalization strength in Q_Si to identify optimal settings and determine robustness to hyperparameter choices.
2. **Cross-Modal Generalization**: Test SICL's effectiveness when transferring representations learned from one modality (e.g., inertial data) to another (e.g., skeleton data) to validate true subject-invariance.
3. **Real-World Deployment Simulation**: Evaluate performance when subject IDs are partially missing or noisy in the training data to assess practical applicability beyond controlled experimental conditions.