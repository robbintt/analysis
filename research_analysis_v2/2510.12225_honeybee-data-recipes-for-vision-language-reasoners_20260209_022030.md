---
ver: rpa2
title: 'HoneyBee: Data Recipes for Vision-Language Reasoners'
arxiv_id: '2510.12225'
source_url: https://arxiv.org/abs/2510.12225
tags:
- data
- reasoning
- image
- arxiv
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the data design principles behind vision-language
  (VL) reasoning models, focusing on how to construct high-quality chain-of-thought
  (CoT) datasets for training. The authors systematically evaluate different data
  curation strategies, including context sourcing, targeted data interventions (e.g.,
  visual perturbations, text-only reasoning integration), and scaling along multiple
  axes (images, questions, and CoTs per pair).
---

# HoneyBee: Data Recipes for Vision-Language Reasoners

## Quick Facts
- arXiv ID: 2510.12225
- Source URL: https://arxiv.org/abs/2510.12225
- Reference count: 40
- 3B-parameter model outperforms prior SOTA by 7.8% and base model by 24.8% on MathVerse

## Executive Summary
This paper investigates data design principles for vision-language (VL) reasoning models, focusing on how to construct high-quality chain-of-thought (CoT) datasets for training. The authors systematically evaluate different data curation strategies, including context sourcing, targeted data interventions (e.g., visual perturbations, text-only reasoning integration), and scaling along multiple axes (images, questions, and CoTs per pair). They introduce HoneyBee, a large-scale VL reasoning dataset of 2.5M examples (350K unique image-question pairs), built using these insights. VL models trained on HoneyBee achieve state-of-the-art performance across model sizes, with a 3B-parameter model outperforming prior SOTA by 7.8% on MathVerse.

## Method Summary
The authors curate VL reasoning datasets through a multi-stage pipeline: (1) select best source dataset (ViRL) from 6 candidates after decontamination and CoT generation using Llama-4-Scout; (2) apply targeted data interventions including "caption-and-solve" (prepending generated image captions to CoTs) and mixing text-only reasoning data; (3) scale along three axes: 16 CoTs per (image, question) pair (filtered by answer correctness), 14 synthetic questions per image (with majority voting for quality), and varying numbers of unique images. The final HoneyBee dataset contains 2.5M instances (1.5M VL + 1M text-only). PLMs (1B/3B/8B) are trained for 5 epochs with specific hyperparameters and evaluated on multiple VL reasoning benchmarks.

## Key Results
- VL models trained on HoneyBee achieve state-of-the-art performance across model sizes
- A 3B-parameter model outperforms prior SOTA by 7.8% and base model by 24.8% on MathVerse
- Test-time scaling strategy reduces decoding cost by 73% without accuracy loss through shared caption caching

## Why This Works (Mechanism)

### Mechanism 1: Caption-and-Solve Perceptual Grounding
Augmenting CoT with generated image captions before the solution step improves VL reasoning by providing explicit perceptual grounding. The model learns to first generate `<caption>description</caption>` before the solution, creating a structured reasoning pathway: visual perception → explicit description → problem decomposition → solution. This reduces the cognitive load of simultaneously extracting visual information and reasoning.

### Mechanism 2: Cross-Modal Reasoning Transfer via Text-Only Data Mixing
Mixing VL reasoning data with text-only reasoning data improves performance through cross-modal skill transfer. Text-only examples expose the model to diverse problem-solving patterns (planning, verification, reflection) that transfer to visual contexts. The model learns general reasoning schemas that are modality-agnostic.

### Mechanism 3: Test-Time Scaling via Shared Caption Caching
Generating one caption and reusing it across multiple solution attempts reduces inference cost by ~73% without accuracy loss. The CoT structure separates perception (caption) from problem-solving (solution). In self-consistency with N attempts, caching eliminates redundant perception computation since the caption captures all visual information needed.

## Foundational Learning

- **Concept: Chain-of-Thought (CoT) Distillation**
  - Why needed here: HoneyBee generates CoTs from a 109B teacher to train 1B–8B students. Understanding how CoT supervision transfers reasoning is essential.
  - Quick check question: Why might a smaller model trained on CoTs outperform a larger model without CoT supervision on reasoning tasks?

- **Concept: Multi-Axis Data Scaling**
  - Why needed here: The paper scales three axes independently: unique images, questions per image, and CoTs per question, each with different cost-benefit tradeoffs.
  - Quick check question: If compute budget allows 1M training examples, what factors determine whether to allocate to more images, more questions per image, or more CoTs per question?

- **Concept: Knowledge Distillation vs Self-Improvement**
  - Why needed here: The paper primarily uses distillation (large teacher → small student) but also shows self-improvement (teacher fine-tuned on its own data).
  - Quick check question: Under what conditions would self-improvement outperform distillation from a larger model?

## Architecture Onboarding

- **Component map:** Source Datasets (6 candidates) → Decontamination (pHash) → CoT Generation (Llama-4-Scout) → Final Answer Filtering → Context Selection (ViRL chosen) → Data Interventions (Caption-and-Solve + Text-Only Mixing) → Multi-Axis Scaling (16 CoTs/pair + 14 questions/image) → Final Dataset: HoneyBee (2.5M instances) → SFT Training (PLM-1B/3B/8B)

- **Critical path:**
  1. Context source selection is the highest-leverage decision (11.4pp gap between worst and best)
  2. Caption-and-solve and text-only mixing are the only interventions that improve over baseline
  3. Answer-based filtering helps some datasets but not others

- **Design tradeoffs:**
  - Caption generation overhead adds upfront inference cost but enables efficient test-time scaling
  - Text-only ratio optimization not explored (1M text + 1.5M VL used)
  - Answer filtering trades data quantity for quality but may filter out creative reasoning paths

- **Failure signatures:**
  - Perceptual redundancy filtering removes 42% of data if too aggressive
  - Uniform difficulty filtering performed worst (34.6% vs 40.1% baseline)
  - Mixing diverse sources degraded performance

- **First 3 experiments:**
  1. Reproduce context source ranking on your domain with 50K samples each
  2. Ablate caption-and-solve variants with different generation strategies
  3. Validate shared-caption efficiency on your target tasks with 16/32/64 solutions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can complex data mixing strategies overcome the performance degradation observed when naively combining diverse VL reasoning sources?
- Basis: Section 3.1, Footnote 7 states this as future work
- Why unresolved: Naively mixing data from top sources degraded performance compared to using single best source
- What evidence would resolve it: Applying domain-weighted mixing algorithms (e.g., DoReMi) to determine if optimal non-uniform mixture outperforms best single-source

### Open Question 2
- Question: Do the specific data curation principles identified for reasoning effectively transfer to general-purpose VL tasks like VQA?
- Basis: Conclusion suggests assessing impact on general-purpose VL training
- Why unresolved: Study was constrained to reasoning tasks to isolate data design effects
- What evidence would resolve it: Training models using HoneyBee recipe on general VQA datasets and measuring performance changes

### Open Question 3
- Question: How should the data curation pipeline be adapted to support chain-of-thought reasoning over multiple images?
- Basis: Conclusion states focus was only on single images
- Why unresolved: Current pipeline assumes single-image context and doesn't account for inter-image relationships
- What evidence would resolve it: Extending scaling experiments to multi-image datasets and evaluating whether scaling CoTs remains beneficial

## Limitations
- Caption quality and visual perception accuracy of Llama-4-Scout are never validated
- 73% token reduction assumes initial caption captures all visual information needed across all solution attempts
- Data curation insights demonstrated on specific data generation pipeline and model family, unclear if they transfer to different VL models or reasoning tasks

## Confidence
- **High confidence:** Multi-axis scaling methodology and shared caption caching mechanism
- **Medium confidence:** Core claims about ViRL being best source and text-only data mixing benefits
- **Low confidence:** Specific claim that "caption-and-solve" is best perception intervention (depends on caption quality)

## Next Checks
1. Evaluate the quality and relevance of captions generated by Llama-4-Scout on a held-out validation set of 100 images from target reasoning tasks
2. Design an ablation study varying text-to-VL data ratio (0%, 25%, 50%, 75%, 100% text-only) on a held-out reasoning task
3. For each target task, generate 16 solutions with shared captions and 16 solutions with independent captions, compare accuracy and identify specific solution types where shared caption caching hurts performance