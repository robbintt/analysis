---
ver: rpa2
title: Task Scheduling & Forgetting in Multi-Task Reinforcement Learning
arxiv_id: '2503.01941'
source_url: https://arxiv.org/abs/2503.01941
tags:
- learning
- tasks
- forgetting
- task
- scheduling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates forgetting behavior in reinforcement learning
  (RL) agents during multi-task training, drawing parallels to human forgetting curves.
  The authors examine whether scheduling strategies from learning theory can improve
  RL task retention.
---

# Task Scheduling & Forgetting in Multi-Task Reinforcement Learning

## Quick Facts
- arXiv ID: 2503.01941
- Source URL: https://arxiv.org/abs/2503.01941
- Authors: Marc Speckmann; Theresa Eimer
- Reference count: 3
- Key outcome: RL agents exhibit forgetting curves similar to humans, but scheduling methods like Leitner and SuperMemo don't outperform Prioritized Level Replay (PLR) in multi-task settings.

## Executive Summary
This paper investigates forgetting behavior in multi-task reinforcement learning by drawing parallels to human forgetting curves. The authors find that RL agents exhibit similar forgetting patterns when switching between tasks, with some agents showing progressively slower forgetting across repetitions while others display periodic forgetting behavior. They adapt human learning scheduling methods (Leitner and SuperMemo) to RL curriculum generation but find these produce different but not necessarily better performance compared to PLR. The study reveals that tasks have different asymmetrical retention patterns that are not captured by simple repetition or performance-based curricula.

## Method Summary
The authors conduct experiments on the MiniGrid benchmark suite using a PPO agent. They first demonstrate forgetting curves by training an agent on SimpleCrossing until mastery, then switching to Empty until forgetting occurs, and repeating this cycle. They then compare four curriculum scheduling methods: random sampling, PLR (value error-based), Leitner (stage-based), and SuperMemo (solution quality-based). Each method uses the same PLR buffer infrastructure but different sampling strategies. The experiments run across 10 seeds for each method on 15 MiniGrid tasks, measuring mean evaluation reward and per-task performance.

## Key Results
- RL agents exhibit forgetting curves similar to humans, with some showing decreasing forgetting rates across repetitions (6/10 seeds) and others showing periodic forgetting (4/10 seeds)
- Leitner and SuperMemo scheduling methods produce different performance patterns compared to PLR but not necessarily superior results
- Related task pairs show asymmetric retention relationships that current scheduling methods fail to capture (e.g., training on DoorKey helps Unlock more than vice versa)

## Why This Works (Mechanism)

### Mechanism 1: Ebbinghaus-Style Forgetting Curves Emerge in RL
When an agent trained on Task A switches to Task B, interference in the shared policy network causes performance degradation on Task A. With repeated cycles, some agents develop representations that resist interference, extending retention intervals. Core assumption: forgetting stems from policy weight interference rather than memory capacity limits.

### Mechanism 2: Spaced Repetition Scheduling via Leitner/SuperMemo Adaptation
Leitner stages tasks across 5 bins (correct → advance, incorrect → reset). SuperMemo weights intervals by solution quality. Both replace PLR's value-error-based sampling while retaining its buffer infrastructure. Core assumption: Task difficulty and solution quality in RL correlate with retention needs similarly to human factual learning.

### Mechanism 3: Asymmetric Inter-Task Retention Dependencies
Training on Task A may improve Task B retention (positive transfer) while B→A transfer is weaker or absent. This arises from shared skill hierarchies where one task's policy subsumes another's. Core assumption: Task relationships are stable during training and detectable from performance co-variation.

## Foundational Learning

- Concept: Ebbinghaus Forgetting Curve
  - Why needed here: The paper directly analogizes RL forgetting to human memory decay; understanding the original curve shape (exponential decay with slowing rate per repetition) clarifies what the authors measured.
  - Quick check question: If an agent achieves 80% on Task A, trains on Task B for 100k steps, then drops to 20% on A, what would you expect on the third A→B→A cycle?

- Concept: Prioritized Level Replay (PLR)
  - Why needed here: PLR serves as the baseline curriculum method; it samples tasks proportional to value prediction error (TD error proxy), prioritizing "surprising" levels.
  - Quick check question: Why might high value error correlate with both learning opportunity and forgetting risk?

- Concept: Curriculum Learning in RL
  - Why needed here: The study sits within curriculum learning literature; tasks are scheduled to improve final policy quality rather than just minimize loss on a fixed distribution.
  - Quick check question: If you randomly sample tasks vs. curating them by difficulty, what tradeoffs do you expect in final multi-task performance?

## Architecture Onboarding

- Component map: PPO agent with shared policy/value networks -> Task/level buffer (PLR) -> Curriculum scheduler module (swappable: Random, PLR, Leitner, SuperMemo) -> Evaluation harness for per-task probing

- Critical path: Implement PPO on MiniGrid with multi-task support -> Integrate PLR buffer and value-error sampling as baseline -> Add Leitner stage-tracking and SuperMemo interval logic as alternative samplers -> Build cross-training evaluation loop (train on A, evaluate on A∪B)

- Design tradeoffs: PLR is responsive to current learning gaps but reactive (only schedules after forgetting manifests). Leitner/SuperMemo are proactive scheduling based on past performance but ignore inter-task relationships. Random has no overhead, surprisingly competitive mean performance, high variance.

- Failure signatures: Leitner/SuperMemo show larger max-to-final performance drops → overfitting to recently scheduled tasks. High per-task variance on Dynamic/KeyCorridor/RedBlueDoors → these tasks may have complex dependencies. Periodic forgetting (4/10 seeds) → some seeds never develop stable retention.

- First 3 experiments: Replicate the SimpleCrossing↔Empty alternation study on 10 seeds; classify seeds into "improving retention" vs. "periodic forgetting" categories. Implement Leitner and SuperMemo schedulers within PLR's buffer framework; compare final mean reward and per-task variance against PLR and random baselines. Run cross-training on Unlock↔DoorKey and GoToDoor↔GoToObject pairs while evaluating both tasks; quantify directional transfer asymmetry (A→B vs. B→A retention curves).

## Open Questions the Paper Calls Out

### Open Question 1
How can multi-task scheduling methods be designed to capture asymmetric retention relationships between tasks? The authors conclude that "effective scheduling methods would benefit from capturing the relationship between different tasks," noting that current strategies ignore asymmetrical patterns where Task A helps Task B more than vice versa. Why unresolved: Current methods like PLR (performance-based) and SuperMemo (repetition-based) treat tasks relatively independently or symmetrically, failing to model the specific cross-task retention dynamics observed in the study. What evidence would resolve it: The development of a scheduling algorithm that explicitly models and exploits task-pair retention asymmetries, demonstrating superior data efficiency compared to PLR.

### Open Question 2
Can task relationships and retention rates be accurately estimated on the fly during training? The authors suggest that "Studying and identifying such relationships on the fly could lead to improved scheduling and thus more efficient training." Why unresolved: The paper characterizes these relationships post-hoc but does not propose a mechanism for an agent to identify them dynamically during the learning process without explicit cross-training evaluation. What evidence would resolve it: An algorithm that successfully infers latent task relationships from training signals in real-time and adjusts the curriculum accordingly.

### Open Question 3
What distinct mechanisms lead to "periodic" forgetting behavior in RL agents as opposed to the decreasing forgetting curves seen in humans? The authors observe two distinct categories of forgetting: one mimicking human retention improvements, and another showing "relatively regular periodic behavior," but they do not explain the underlying cause of this periodic instability. Why unresolved: The existence of periodic forgetting suggests a failure to consolidate knowledge, but the paper does not investigate if this is caused by network architecture, optimization dynamics, or specific task features. What evidence would resolve it: An ablation study identifying the conditions (e.g., capacity, learning rate) that switch an agent from periodic forgetting to increasing retention.

## Limitations

- Unspecified PPO hyperparameters and SuperMemo/Leitner adaptation formulas make precise replication difficult
- Directional asymmetry findings rely on specific task pairs whose generalizability across domains is unknown
- The study doesn't provide quantitative evidence of optimal scheduling strategy or demonstrate that observing task relationships leads to better performance

## Confidence

High Confidence: RL agents exhibit forgetting curves similar to human memory (supported by direct measurements and 10-seed experiments).  
Medium Confidence: Leitner and SuperMemo methods show different but not superior performance to PLR (replication requires unspecified hyperparameters).  
Low Confidence: Asymmetric task relationships are not captured by existing methods (limited task pairs tested; no comparative optimization of scheduling strategies).

## Next Checks

1. Replicate the SimpleCrossing↔Empty forgetting study across 10 seeds with standardized hyperparameters to verify the decreasing vs periodic forgetting pattern distribution (6/10 vs 4/10).  
2. Implement Leitner and SuperMemo schedulers within the PLR framework using the exact stage/interval rules from the paper to compare mean reward and per-task variance against PLR and random baselines.  
3. Run systematic cross-training experiments on all 15 MiniGrid tasks, measuring directional transfer asymmetry (Task A→B vs B→A retention) to determine whether the Unlock↔DoorKey and GoToDoor↔GoToObject patterns generalize.