---
ver: rpa2
title: Causal Prompting for Implicit Sentiment Analysis with Large Language Models
arxiv_id: '2507.00389'
source_url: https://arxiv.org/abs/2507.00389
tags:
- causal
- reasoning
- sentiment
- prompting
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses implicit sentiment analysis (ISA), where sentiment
  is not explicitly stated but must be inferred from subtle contextual cues. Existing
  LLM-based ISA methods often rely on chain-of-thought reasoning with self-consistency,
  which can be biased by spurious correlations.
---

# Causal Prompting for Implicit Sentiment Analysis with Large Language Models

## Quick Facts
- arXiv ID: 2507.00389
- Source URL: https://arxiv.org/abs/2507.00389
- Authors: Jing Ren; Wenhao Zhou; Bowen Li; Mujie Liu; Nguyen Linh Dan Le; Jiade Cen; Liping Chen; Ziqi Xu; Xiwei Xu; Xiaodong Li
- Reference count: 40
- Primary result: CAPITAL achieves significant F1 improvements (up to ~62 F1 vs ~55) on implicit sentiment analysis by applying front-door adjustment to causal prompting.

## Executive Summary
This paper addresses implicit sentiment analysis (ISA), where sentiment is not explicitly stated but must be inferred from subtle contextual cues. Existing LLM-based ISA methods often rely on chain-of-thought reasoning with self-consistency, which can be biased by spurious correlations. To address this, the authors propose CAPITAL, a causal prompting framework that uses front-door adjustment to estimate unbiased causal effects of prompts on sentiment predictions. CAPITAL generates multiple reasoning paths, clusters them, and applies a normalized weighted geometric mean (NWGM) approximation to estimate causal effects without relying on confounders. Experiments on benchmark datasets with three LLMs show that CAPITAL outperforms strong baselines, achieving significant improvements in accuracy and robustness, especially under adversarial conditions.

## Method Summary
CAPITAL implements a three-stage pipeline: (1) Generate 20 diverse Chain-of-Thought (CoT) paths using temperature-adjusted LLM sampling; (2) Encode CoTs with a separate encoder and cluster into 8-12 groups via K-means, selecting representative centroid paths; (3) For each centroid, retrieve similar "incorrect" CoTs from training data, construct prompts with these demonstrations, query LLM 5 times per centroid, and aggregate via weighted geometric mean to estimate P(Y|do(X)) using front-door adjustment. The framework targets implicit sentiment analysis on SemEval14 datasets, classifying sentiment polarity toward aspect terms.

## Key Results
- CAPITAL achieves 62.04 F1 on SemEval14 Restaurant dataset (vs 55.03 for THOR baseline)
- On adversarial OOD data, CAPITAL shows 4.12% improvement over CoT-SC
- Hyperparameter analysis shows K=8-12 clusters optimal; performance degrades with too few or too many clusters

## Why This Works (Mechanism)

### Mechanism 1: Front-Door Adjustment Decomposes Confounded Effects
CAPITAL reduces bias from latent confounders by decomposing the causal effect of input prompt X on output Y through intermediate mediator T (Chain-of-Thought reasoning paths). The framework applies Pearl's front-door adjustment formula: P(Y|do(X)) = Σ_T P(T|do(X)) · P(Y|do(T)). This avoids needing to observe the unmeasured confounder Z directly, which standard back-door adjustment requires. CoT paths serve as the front-door variable because they fully mediate X→Y, have no unblocked back-door paths from X, and block back-door paths from T to Y when conditioning on X.

### Mechanism 2: Clustering Approximates the Interventional Distribution P(T|do(X))
Empirical clustering of multiple CoT samples provides a tractable approximation of the causal effect of the prompt on reasoning paths. The LLM generates 20 distinct CoTs by sampling at higher temperature. These are encoded and clustered with K-means into 8-12 groups. The relative cluster size estimates P(T|do(X)). Representative "centroid" CoTs are selected as anchors for downstream estimation.

### Mechanism 3: NWGM Approximation Enables Practical Back-door Estimation Over Prompts
A Normalized Weighted Geometric Mean (NWGM) approximation, combined with retrieval-augmented prompting, estimates P(Y|do(T)) without enumerating all possible prompts. For each centroid CoT, the framework retrieves semantically similar "incorrect" CoTs from the training set as in-context demonstrations. The LLM is queried 5 times with these demonstrations; the empirical frequency of each polarity label approximates E_X[P(Y|T, X]).

## Foundational Learning

- **Concept: Structural Causal Models (SCMs) and the do-operator**
  - Why needed here: The entire CAPITAL framework is built on Pearl's causal formalism. Without understanding directed acyclic graphs, confounders, and interventional distributions (P(Y|do(X)) vs. P(Y|X)), the rationale for front-door adjustment is opaque.
  - Quick check question: Given a DAG where X→Y and Z→X and Z→Y, explain why P(Y|X) ≠ P(Y|do(X)) and what variable(s) could block the back-door path.

- **Concept: Front-door vs. Back-door Adjustment**
  - Why needed here: The paper explicitly rejects back-door adjustment (unobservable confounder Z) and adopts front-door adjustment. Understanding why the front-door works—and its three criterion conditions—is essential to evaluate whether the CoT mediator is valid.
  - Quick check question: List the three conditions for a variable W to satisfy the front-door criterion relative to (X, Y). Which condition would break if the LLM's internal bias directly shaped CoT content?

- **Concept: Chain-of-Thought Prompting and Self-Consistency**
  - Why needed here: CAPITAL builds on (and critiques) CoT-SC methods like THOR. Understanding how standard CoT-SC uses majority voting—and why it can amplify spurious correlations—clarifies what CAPITAL improves upon.
  - Quick check question: In self-consistency voting, why might the most frequent answer still be wrong? How does CAPITAL's causal-effect ranking differ in principle?

## Architecture Onboarding

- **Component map:** Prompt Constructor -> CoT Generator -> Encoder -> K-means Clusterer -> Demonstration Retriever -> Re-Prompting Module -> Causal Aggregator
- **Critical path:** CoT generation → encoding → clustering → centroid selection → demonstration retrieval → re-prompting → frequency counting → weighted aggregation
- **Design tradeoffs:**
  - Cluster count K: Too few clusters under-represent reasoning diversity; too many dilute signal. Paper finds K=8 (Restaurant) and K=12 (Laptop) optimal.
  - Number of CoT samples A: More samples improve distribution estimation but increase cost. A=20 is chosen as a compute-performance tradeoff.
  - Using only "incorrect" CoTs for retrieval: Motivated by capturing error patterns, but excludes direct exposure to correct reasoning—assumed beneficial for avoiding "leakage," yet unverified empirically.
  - Encoder choice: Paper uses a standard encoder; performance depends on encoder-LLM alignment. Contrastive learning (mentioned but not detailed in excerpts) may help, but is not deeply analyzed.

- **Failure signatures:**
  - Flat cluster distribution: If |T_k|/A is near-uniform, the P(T|do(X)) term provides little discriminative signal—suggests temperature too high or CoT space insufficiently structured.
  - High variance across N re-prompts: If polarity frequencies fluctuate widely for a given centroid, the P(Y|do(T)) estimate is unreliable—may indicate weak demonstration relevance or LLM instability.
  - Degraded performance on adversarial data: CAPITAL's advantage should be most pronounced here; if it collapses similar to baselines, the causal adjustment may not be effectively blocking confounders.

- **First 3 experiments:**
  1. Reproduce main results on Restaurant ISA subset with LLaMA-2: Verify F1 improvements over CoT-SC and THOR match reported values (~62 vs. ~55 F1).
  2. Ablate clustering: Replace K-means with random selection of K CoTs (equal weights). Expect drop in ISA F1 (~2-4 points per Table III), confirming clustering's role.
  3. Stress-test retrieval alignment: Swap the encoder for a different pre-trained model (e.g., RoBERTa vs. BERT) without re-alignment. Monitor performance change; a significant drop suggests encoder-LLM alignment is critical.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can CAPITAL's causal prompting framework generalize effectively to reasoning-intensive NLP tasks beyond sentiment analysis, such as question answering or natural language inference?
- Basis in paper: The conclusion states the "causal effect estimation strategy offers a generalisable method applicable to a wide range of reasoning-intensive NLP tasks" without providing empirical validation.
- Why unresolved: Experiments are limited to implicit sentiment analysis benchmarks (SemEval14 Restaurant and Laptop); no cross-task evaluation was conducted.
- What evidence would resolve it: Empirical results showing CAPITAL's performance on diverse reasoning tasks like commonsense QA, reading comprehension, or logical inference benchmarks.

### Open Question 2
- Question: Does the CoT variable T strictly satisfy all three front-door criterion conditions in practice, particularly the requirement of no unblocked back-door paths from T to Y?
- Basis in paper: The paper assumes T satisfies the front-door criterion but acknowledges the latent confounder Z influences both X and Y. Whether Z also confounds T and Y remains theoretically unverified.
- Why unresolved: The structural causal model is presented as a conceptual framework; no empirical verification of the front-door assumptions is provided.
- What evidence would resolve it: Interventional studies examining whether reasoning paths are independent of latent confounders, or sensitivity analysis testing robustness to assumption violations.

### Open Question 3
- Question: How can the optimal number of clusters K be determined a priori for new datasets without extensive hyperparameter tuning?
- Basis in paper: Section V-F shows different optimal K values for Restaurant (K=8) and Laptop (K=12), determined empirically. No principled method for selecting K is proposed.
- Why unresolved: The paper observes that too few clusters underrepresent the CoT distribution while too many dilute information, but offers no theoretical guidance.
- What evidence would resolve it: A theoretical or heuristic rule relating K to dataset characteristics (e.g., sample size, semantic diversity) validated across multiple benchmarks.

### Open Question 4
- Question: Is the exclusive use of incorrect CoTs for demonstration retrieval universally optimal, or could correct CoTs provide complementary value in certain scenarios?
- Basis in paper: The paper motivates using only incorrect CoTs to avoid "direct leakage of ideal reasoning paths," but this design choice is not ablated against retrieval using correct CoTs or mixed sets.
- Why unresolved: The assumption that incorrect CoTs better reflect model uncertainty is intuitively plausible but empirically untested against alternatives.
- What evidence would resolve it: Ablation experiments comparing retrieval strategies (incorrect-only, correct-only, mixed) on ISA performance.

## Limitations
- The core assumption that CoT paths satisfy the front-door criterion is not empirically validated; latent confounders could still directly influence both the prompt and the reasoning chain.
- The encoder's alignment with the LLM's reasoning space is critical but untested in depth; performance may depend heavily on this match.
- Retrieval of "incorrect" demonstrations is justified as capturing error patterns, but excluding correct demonstrations may be suboptimal and is not systematically evaluated.

## Confidence

**High confidence:** CAPITAL's overall architecture and the general causal adjustment principle are sound and mathematically well-grounded in Pearl's framework.

**Medium confidence:** The empirical clustering and NWGM approximation methods are reasonable and improve over baselines, but their exact robustness and sensitivity to hyperparameters (K, L, N) are not fully explored.

**Low confidence:** The assumption that CoT paths fully satisfy the front-door criterion and that incorrect CoT retrieval is optimal; these are plausible but not validated.

## Next Checks

1. **Test front-door validity empirically:** Design a synthetic experiment where a known confounder influences both prompts and outputs. Measure whether CAPITAL's predictions are less affected than standard CoT-SC; a failure here would indicate the front-door criterion is violated.

2. **Compare retrieval strategies:** Ablate the retrieval module by using correct CoTs, random CoTs, or a mix of correct/incorrect CoTs. Quantify the impact on ISA F1 to test whether incorrect CoT retrieval is truly optimal.

3. **Probe encoder-LLM alignment:** Swap the encoder with a pre-trained model from a different family (e.g., RoBERTa for BERT-based LLM). Measure performance drop; if large, it confirms alignment is critical and may necessitate the contrastive learning component.