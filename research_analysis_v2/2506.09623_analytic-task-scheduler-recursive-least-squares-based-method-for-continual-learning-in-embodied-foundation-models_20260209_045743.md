---
ver: rpa2
title: 'Analytic Task Scheduler: Recursive Least Squares Based Method for Continual
  Learning in Embodied Foundation Models'
arxiv_id: '2506.09623'
source_url: https://arxiv.org/abs/2506.09623
tags:
- task
- learning
- scheduler
- continual
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes ATS (Analytic Task Scheduler), a continual learning
  framework for embodied foundation models that addresses catastrophic forgetting.
  ATS uses a task-specific model library where each task is independently fine-tuned,
  and an analytic scheduler trained with recursive least squares (RLS) to map language
  instructions to appropriate task models.
---

# Analytic Task Scheduler: Recursive Least Squares Based Method for Continual Learning in Embodied Foundation Models

## Quick Facts
- arXiv ID: 2506.09623
- Source URL: https://arxiv.org/abs/2506.09623
- Authors: Lipei Xie; Yingxin Li; Huiping Zhuang
- Reference count: 26
- Key outcome: Near-perfect accuracy (98.61%) across continual learning phases with only 2.34% average forgetting rate on a real-world RM65B robot platform

## Executive Summary
This paper introduces ATS (Analytic Task Scheduler), a continual learning framework designed to address catastrophic forgetting in embodied foundation models. The core innovation lies in combining a task-specific model library with an analytic scheduler trained using recursive least squares (RLS). This approach enables the system to maintain high performance across multiple tasks without requiring access to historical data, making it particularly suitable for real-world robotic applications where data storage is constrained.

## Method Summary
ATS employs a dual-component architecture: a task-specific model library and an analytic scheduler. The model library stores independently fine-tuned models for each task, preventing parameter interference. The analytic scheduler, trained with recursive least squares, maps language instructions to appropriate task models by maintaining sufficient statistics (autocorrelation and cross-correlation matrices) rather than historical data. This enables forgetting-resistant learning through analytical computation. The system was evaluated on a real-world RM65B robot platform using multi-modal inputs (proprioception, vision, language) and demonstrated superior resistance to catastrophic forgetting compared to traditional fine-tuning methods.

## Key Results
- Achieved near-perfect accuracy (98.61%) across continual learning phases
- Maintained only 2.34% average forgetting rate during task sequence execution
- Significantly improved task execution scores compared to traditional fine-tuning methods on real-world robot platform

## Why This Works (Mechanism)

### Mechanism 1: Architectural Isolation via Model Library
The system stores independently fine-tuned models for each task, preventing the parameter interference that causes catastrophic forgetting in sequential training. This approach assumes sufficient memory for storing model copies and that tasks are sufficiently distinct to benefit from isolated optimization.

### Mechanism 2: Recursive Least Squares (RLS) for Forgetting-Free Routing
The analytic scheduler learns to map language instructions to task IDs incrementally without losing accuracy on old tasks by solving a recursive ridge regression problem. It maintains sufficient statistics rather than raw data, allowing incremental updates without revisiting historical data.

### Mechanism 3: Feature Expansion for Linear Separability
Projecting language embeddings into a higher-dimensional space enhances the scheduler's ability to distinguish between task instructions. This increases the likelihood that the mapping from instructions to task labels can be solved by the linear ridge regression at the scheduler's core.

## Foundational Learning

- **Ridge Regression (L2 Regularization)**: Essential for preventing overfitting and ensuring numerical stability in the scheduler's core mathematical engine. Quick check: How does increasing the regularization coefficient γ affect the scheduler's sensitivity to noise in the language instructions?

- **Woodbury Matrix Identity**: The specific mathematical tool that allows RLS to update the inverse of a matrix without recomputing the full inverse from scratch. Quick check: Why is applying the Woodbury identity computationally cheaper than recalculating (X^T X + γI)^(-1) every time new data arrives?

- **Catastrophic Forgetting (in Gradient Descent)**: Understanding this problem is crucial to appreciate the solution. Standard SGD updates weights to minimize loss on current batch, implicitly maximizing loss on previous data distributions not present in the batch. Quick check: Why does the "Plasticity-Stability" dilemma make it difficult to use standard fine-tuning for continual learning of robots?

## Architecture Onboarding

- **Component map**: Input Layer (Proprioception, Vision, Language) -> Backbone (Frozen RDT-170m) -> Analytic Scheduler (Feature Extractor -> Expansion Layer -> RLS Core) -> Model Library (Dictionary of fine-tuned model weights {M1, ..., MK})

- **Critical path**: 1) Calibration: Extract language features using frozen CL 2) Expansion: Pass features through expansion layer 3) RLS Update: Update R and Q matrices 4) Weight Solve: Compute new scheduler weights W 5) Model Train: Fine-tune copy of backbone on new task data 6) Register: Add new model to library

- **Design tradeoffs**: Replay-Free vs. Storage (save memory by not storing replay buffers but consume storage by saving full model copies), Speed vs. Adaptability (slightly slower inference but ensures task success), Scheduler plasticity (zero forgetting of mapping but cannot adapt if language encoder becomes obsolete)

- **Failure signatures**: Scheduler Confusion (wrong model selection - check feature expansion dimension), Matrix Singularity (numerical instability - check regularization γ), Library Bloat (system OOM - check pruning or capacity)

- **First 3 experiments**: 1) Scheduler Validation: Train RLS scheduler sequentially, verify Task 1 accuracy remains ~100% after Task 10 2) Ablation on Feature Expansion: Run continual learning with de=df, compare forgetting rates 3) Robot Deployment (Real): Execute "Pick Banana" then "Pick Corn" sequence, show ATS succeeds while "B after A" baseline fails

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability challenges due to reliance on growing library of independently fine-tuned models for long task sequences
- Limited evaluation to only two robot manipulation tasks and 10-category language classification problem
- Model library assumption may not hold for real-world deployments with many tasks due to storage constraints

## Confidence

- **High confidence**: Core RLS scheduler mechanism and its forgetting-resistant properties (well-established mathematical techniques)
- **Medium confidence**: Overall framework effectiveness (strong empirical results but limited task diversity)
- **Medium confidence**: Architectural separation approach (model library assumption may not scale)

## Next Checks
1. Evaluate ATS on substantially larger and more diverse task set (e.g., 20+ manipulation tasks) to assess library scalability and scheduler generalization
2. Conduct ablation studies specifically isolating contribution of feature expansion versus RLS mechanism
3. Test ATS under realistic operational conditions where task distributions change over time, including scenarios requiring re-execution of previously learned tasks after learning many new tasks