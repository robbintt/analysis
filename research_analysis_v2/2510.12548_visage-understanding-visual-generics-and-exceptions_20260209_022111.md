---
ver: rpa2
title: 'VISaGE: Understanding Visual Generics and Exceptions'
arxiv_id: '2510.12548'
source_url: https://arxiv.org/abs/2510.12548
tags:
- exception
- conceptual
- image
- have
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VISaGE, a dataset for studying how vision-language
  models (VLMs) handle visual generics and exceptions. The dataset includes pairs
  of typical and exceptional images linked to conceptual norms, allowing researchers
  to probe how VLMs reason about conceptual knowledge versus individual instances.
---

# VISaGE: Understanding Visual Generics and Exceptions

## Quick Facts
- arXiv ID: 2510.12548
- Source URL: https://arxiv.org/abs/2510.12548
- Reference count: 12
- VLMs struggle to balance conceptual generalizations with visual evidence, particularly in the presence of exceptions

## Executive Summary
This paper introduces VISaGE, a dataset for studying how vision-language models (VLMs) handle visual generics and exceptions. The dataset includes pairs of typical and exceptional images linked to conceptual norms, allowing researchers to probe how VLMs reason about conceptual knowledge versus individual instances. The authors test several open-weight VLMs across four conditions that vary the type of image (typical or exceptional), the type of query (conceptual or instance-level), and the text reference (category or exception name). They find that VLMs' conceptual understanding degrades when images are incongruent with the text, showing that the pragmatic prior to attend to images can override semantic knowledge. Instance-level recognition is also affected by semantic bias, though some models show sensitivity to image atypicality.

## Method Summary
The study evaluates VLMs on conceptual vs instance-level reasoning using typical and exceptional images from the VISaGE dataset. The dataset contains 1601 exceptional images across 437 exception subcategories, 296 category-attribute pairs, and 171 categories. VLMs are tested across 7 conditions varying image type (typical/exceptional), query type (conceptual/instance), and text reference (category/exception name). Accuracy is measured on yes/no responses, and MM-SHAP (Shapley values) is used for feature attribution to quantify the contribution of image vs. text to predictions.

## Key Results
- VLMs' conceptual understanding degrades when the assumption of congruency underlying the pragmatic prior is violated with incongruent images
- Instance-level recognition is affected by semantic bias from category names, though some models show sensitivity to image atypicality
- Feature attribution analysis shows images contribute more to instance queries and models attend more to exceptional images

## Why This Works (Mechanism)

### Mechanism 1: Pragmatic Prior Interference
- Claim: VLMs' learned expectation that text and images are congruent overrides conceptual knowledge retrieval when visual inputs contradict text-based queries.
- Core assumption: Models lack explicit query-type routing to flexibly weight modalities.
- Evidence anchors: "conceptual understanding degrades when the assumption of congruency underlying the pragmatic prior is violated with incongruent images"; "pragmatic prior, i.e., assuming the image is relevant, is overriding the correct retrieval of conceptual knowledge"
- Break condition: If models had explicit query-type classification (conceptual vs. instance) with modality gating, this interference pattern would not occur.

### Mechanism 2: Semantic Prior Overgeneralization
- Claim: Category names activate prototypical attribute representations that bias predictions regardless of contradictory visual evidence.
- Core assumption: Semantic representations are learned as deterministic rather than probabilistic or defeasible.
- Evidence anchors: "many models still appear to ignore the visual features, relying instead on language-based conceptual cues"; "exception categories are lower frequency than the general categories, so exception attribute knowledge may be less well developed"
- Break condition: If semantic representations encoded uncertainty or exception frequency, models could down-weight prototypical attributes appropriately.

### Mechanism 3: Attention Modulation by Atypicality Detection
- Claim: Models allocate more attention weight to exceptional images compared to typical images, but this increased attention does not translate to correct instance recognition.
- Core assumption: Models have implicit surprise/conflict detection but lack exception-handling circuits.
- Evidence anchors: "conditions with exception images have higher V-SHAP compared to typical images (p<0.01; d=0.62 for concept queries and d=0.56 for instance queries, both indicating a medium effect)"; "models are recruiting visual information to supplement and possibly counteract the conceptual information from the text"
- Break condition: If increased attention to exceptions led to improved instance recognition accuracy, this would indicate functional exception handling rather than just conflict detection.

## Foundational Learning

- **Concept: Generics vs Quantified Statements**
  - Why needed here: The dataset relies on the semantic property that generics ("cats have four legs") tolerate exceptions, unlike universal quantifiers ("all cats have four legs").
  - Quick check question: If "birds fly" is true, does observing a penguin falsify it?

- **Concept: Pragmatic vs Semantic Priors**
  - Why needed here: The paper's core tension distinguishes learned expectations about communication (both modalities are relevant) from learned conceptual content (categories have attributes).
  - Quick check question: Someone asks "Do cats have tails?" while showing a Manx cat—which source should determine your answer?

- **Concept: Shapley Value Attribution**
  - Why needed here: MM-SHAP quantifies proportional contribution of image vs. text to predictions; understanding this is essential for interpreting mechanism evidence.
  - Quick check question: If V-SHAP increases for exceptional images but accuracy decreases, what does this imply about how the model uses visual information?

## Architecture Onboarding

- **Component map:** Image encoder → visual tokens from typical/exceptional images → Fusion layers → where pragmatic prior emerges from training on congruent pairs → Concept representation store → semantic prior (category-attribute mappings) without variance annotations → Output head → binary yes/no token generation

- **Critical path:**
  1. Input (image, text query) → both encoded
  2. Fusion with implicit congruence assumption (pragmatic prior)
  3. Semantic activation from category name triggers prototypical attributes (semantic prior)
  4. Prediction modulated by attention to both modalities
  5. Failure: when priors conflict, models cannot consistently privilege the correct one

- **Design tradeoffs:**
  - Training on congruent pairs → improves instance understanding BUT creates strong pragmatic prior
  - Category name in query → provides semantic context BUT primes prototypical attributes
  - Exception names often contain category name ("lioness" contains "lion") → may still trigger category priors

- **Failure signatures:**
  - Conceptual query + exceptional image → answers from image instead of knowledge (pragmatic override)
  - Instance query + exceptional image + category name → answers from category instead of image (semantic override)
  - Instance query + exception name → still degraded due to category-name substring priming

- **First 3 experiments:**
  1. Replicate Experiment 1's 4 conditions on a new VLM to quantify pragmatic prior strength in your architecture.
  2. Run MM-SHAP analysis comparing a model showing atypicality sensitivity vs. one without it to identify where attention patterns diverge.
  3. Test whether training with explicit variance annotations ("cats USUALLY have four legs") reduces semantic overgeneralization on instance queries.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the trade-off between pragmatic and semantic priors in VLMs generalize across languages with different conceptual structures or category boundaries?
  - Basis in paper: The authors state that conceptual spaces are language-dependent and current results are limited to American English, though they hypothesize patterns might hold.
  - Why unresolved: The VISaGE dataset relies on English-specific norms (THINGS) and the study does not include multilingual evaluation.
  - What evidence would resolve it: Translating or reconstructing the VISaGE dataset for languages with distinct conceptual norms and testing if the pragmatic prior still overrides semantic knowledge in incongruent settings.

- **Open Question 2:** Does the increased visual attention (V-SHAP) for instance and exceptional queries observed in small models scale to larger VLMs?
  - Basis in paper: The Shapley value feature attribution analysis was restricted to the `smolvlm2` model, while accuracy experiments covered a wider range of larger models.
  - Why unresolved: It is unknown if the finding that models "recruit visual information" for exceptions is consistent across different scales or architectures.
  - What evidence would resolve it: Applying the MM-SHAP analysis methodology to the larger models tested in the paper (e.g., InternVL3, Gemma3-12B) to measure if image contribution scales similarly.

- **Open Question 3:** To what extent does lexical overlap between exception names and category names drive the observed semantic bias in instance queries?
  - Basis in paper: The authors suggest that exception names often include the category name (e.g., "lioness" contains "lion"), which could lead to semantic priming of the general category.
  - Why unresolved: The current experimental design does not isolate the effect of the exception's lexical form from the visual properties of the image.
  - What evidence would resolve it: Conducting ablations using exceptions with morphologically unrelated names or synthetic identifiers to determine if semantic bias persists without linguistic containment.

## Limitations

- The paper's central claim about pragmatic prior interference lacks direct empirical backing from existing literature
- The VISaGE dataset construction involves human-generated typical-instance pairs and GPT-3.5-generated exceptions, introducing potential confounds
- MM-SHAP values indicate that exceptional images receive higher attention weights, but the paper cannot definitively establish whether this reflects genuine atypicality detection or simply stronger prediction conflict

## Confidence

- **High Confidence**: The empirical findings that VLMs show degraded conceptual understanding with incongruent images, and that instance recognition is affected by semantic bias
- **Medium Confidence**: The claim that pragmatic prior interference is the primary mechanism driving conceptual query failures
- **Medium Confidence**: The semantic prior overgeneralization mechanism explaining instance query failures

## Next Checks

1. **Ablation study on pragmatic prior strength**: Train or fine-tune a VLM on a dataset with explicit incongruity (some image-text pairs deliberately mismatched) and test whether this reduces the conceptual understanding degradation pattern. This would directly test whether the pragmatic prior is learnable and malleable.

2. **Semantic representation analysis**: Use probing techniques (e.g., linear classifiers on frozen representations) to test whether category names in VLMs trigger deterministic prototypical attribute representations versus probabilistic distributions that could encode exception frequency.

3. **Attention pathway isolation**: Run controlled experiments where the category name is present but visually degraded (blurred text, masked tokens) to determine whether semantic bias operates through language pathway alone or requires visual modality interaction.