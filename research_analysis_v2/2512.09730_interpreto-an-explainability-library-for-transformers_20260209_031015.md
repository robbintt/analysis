---
ver: rpa2
title: 'Interpreto: An Explainability Library for Transformers'
arxiv_id: '2512.09730'
source_url: https://arxiv.org/abs/2512.09730
tags:
- concept
- methods
- arxiv
- attribution
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: INTERPRETO is a unified Python library for post-hoc explainability
  of HuggingFace transformer models, supporting both attribution and concept-based
  explanations. It offers 11 attribution methods (4 perturbation-based, 7 gradient-based)
  and 15 concept-discovery options, with 4 and 7 built-in metrics respectively.
---

# Interpreto: An Explainability Library for Transformers

## Quick Facts
- arXiv ID: 2512.09730
- Source URL: https://arxiv.org/abs/2512.09730
- Authors: Antonin Poché; Thomas Mullor; Gabriele Sarti; Frédéric Boisnard; Corentin Friedrich; Charlotte Claye; François Hoofd; Raphael Bernas; Céline Hudelot; Fanny Jourdan
- Reference count: 9
- Primary result: INTERPRETO is a unified Python library for post-hoc explainability of HuggingFace transformer models, supporting both attribution and concept-based explanations.

## Executive Summary
INTERPRETO provides a unified Python library for post-hoc explainability of HuggingFace transformer models. It supports both classification and generation models through a consistent API, offering 11 attribution methods (4 perturbation-based, 7 gradient-based) and 15 concept-discovery options. The library addresses the fragmentation in interpretability tools by providing a single interface for both attribution and concept-based explanations, with built-in metrics for evaluation and detailed documentation for reproducibility.

## Method Summary
The library implements two main explainability approaches: attribution methods and concept-based explanations. Attribution methods include both perturbation-based (LIME, SHAP, Occlusion, Sobol) and gradient-based (Integrated Gradients, Saliency, SmoothGrad) approaches. Concept-based explanations decompose model activations into interpretable dimensions using dictionary learning techniques like sparse autoencoders (SAEs), PCA, NMF, and ICA. The unified API abstracts model-type differences through consistent interfaces, supporting both classification and generation models via HuggingFace's standard architectures.

## Key Results
- Unified API supporting 11 attribution methods across perturbation and gradient-based families
- 15 concept-discovery options including multiple SAE variants and matrix factorization methods
- Built-in evaluation metrics: 4 faithfulness metrics for attributions, 7 quality metrics for concepts
- Modular design enabling easy extension with new methods

## Why This Works (Mechanism)

### Mechanism 1
Attribution methods estimate feature contributions by systematically varying inputs or computing gradients. Perturbation-based methods modify input tokens and measure output changes, while gradient-based methods compute derivatives of outputs with respect to inputs. The paper notes these are "intuitively linked as the gradient corresponds to the limit of infinitesimal perturbations."

### Mechanism 2
Concept-based explanations decompose model activations into interpretable dimensions via dictionary learning. The library splits the model at a specified layer, collects activations on a dataset, then applies dictionary learning (SAEs, PCA, NMF, ICA) to map high-dimensional activations to a lower-dimensional concept space.

### Mechanism 3
A unified API reduces integration friction by abstracting model-type differences behind common interfaces. INTERPRETO wraps HuggingFace models through NNsight for activation access and provides consistent explainer classes that handle tokenization, target specification, and visualization internally.

## Foundational Learning

- **Attribution taxonomy (perturbation vs. gradient-based)**
  - Why needed here: The library exposes 11 methods across both families; selecting appropriately requires understanding their trade-offs
  - Quick check question: Given a model with ReLU activations and sparse gradients, which attribution family would you try first?

- **Dictionary learning / sparse autoencoders**
  - Why needed here: Core to the concept module; SAEs and related methods map activations to interpretable directions
  - Quick check question: If an SAE has high reconstruction MSE but high sparsity, what does that suggest about the learned concepts?

- **HuggingFace model architecture (encoder/decoder/encoder-decoder)**
  - Why needed here: Splitting points and activation granularity differ by architecture
  - Quick check question: For a T5 model, where would you insert a split point to inspect cross-attention contributions?

## Architecture Onboarding

- **Component map:** interpreto.attributions -> AttributionExplainer + perturbators + aggregators + metrics; interpreto.concept_based -> ModelWithSplitPoints (via NNsight) + concept models (via overcomplete) + interpretation methods + importance estimators
- **Critical path:** 1) For attribution: Load HF model → instantiate AttributionExplainer → call explainer(inputs, targets) → visualize; 2) For concepts: Wrap model with split points → collect activations → fit concept explainer → interpret concepts → estimate importance
- **Design tradeoffs:** Granularity (token-level vs word-level), inference mode (logits vs softmax), SAE architecture choice (Vanilla vs Top-k vs JumpReLU)
- **Failure signatures:** Attribution all-zeros (check inference_mode, gradient flow, targets), concept labels incoherent (inspect activation dataset, split point, reconstruction metrics), OOM on concept fitting (reduce batch size, subset dataset, smaller dimension)
- **First 3 experiments:** 1) Run LIME attribution on DistilBERT fine-tuned on SST-2 with word-level granularity; 2) Fit a small SAE on GPT-2 small using 100 samples from AG News; 3) Compare Integrated Gradients vs Occlusion using Insertion/Deletion faithfulness metrics

## Open Questions the Paper Calls Out
- How can input-level attributions and concept-based explanations be effectively unified to provide end-to-end traceability from input tokens to latent concepts?
- How do different attribution methods and concept-based approaches compare in terms of faithfulness and stability when evaluated under a unified API?
- How does the choice of layer split point in the concept-based pipeline impact the interpretability and utility of the discovered concepts?

## Limitations
- Library effectiveness depends on quality and interpretability of learned concepts, which remains an open challenge
- Scope limited to HuggingFace transformer models, excluding CNN/LSTM architectures and multimodal models
- Practical interpretability of concept-based explanations requires empirical validation beyond technical implementation

## Confidence
- High confidence: Library implementation and API design are well-specified with clear dependency requirements
- Medium confidence: Two-family taxonomy of attribution methods is well-established in literature
- Low confidence: Practical interpretability and usefulness of concept-based explanations extracted by the library

## Next Checks
1. Reproduce concept extraction pipeline on 100 samples from AG News using JumpReLU SAE on GPT-2, measuring reconstruction MSE and inspecting top-activating tokens
2. Compare faithfulness metrics (Insertion/Deletion/AOPC) across multiple attribution methods on DistilBERT sentiment classification task
3. Test unified API's robustness by running attribution and concept explanations on both DistilBERT and GPT-2 using identical code patterns