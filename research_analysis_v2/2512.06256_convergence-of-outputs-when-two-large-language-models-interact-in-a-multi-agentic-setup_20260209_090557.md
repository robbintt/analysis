---
ver: rpa2
title: Convergence of Outputs When Two Large Language Models Interact in a Multi-Agentic
  Setup
arxiv_id: '2512.06256'
source_url: https://arxiv.org/abs/2512.06256
tags:
- each
- convergence
- arxiv
- distance
- setup
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examined long-term conversation stability between two
  large language models (Mistral Nemo Base 2407 and Llama 2 13B hf) in a multi-agent
  setup without external prompts. Models alternated text generation over 25 turns
  using various seed sentences.
---

# Convergence of Outputs When Two Large Language Models Interact in a Multi-Agentic Setup

## Quick Facts
- **arXiv ID**: 2512.06256
- **Source URL**: https://arxiv.org/abs/2512.06256
- **Reference count**: 24
- **Primary result**: 35 out of 50 conversations between two LLMs converged into repetitive loops over 25 turns

## Executive Summary
This study examines long-term conversation stability between two large language models (Mistral Nemo Base 2407 and Llama 2 13B hf) in a multi-agent setup without external prompts. Models alternated text generation over 25 turns using various seed sentences. Results showed that most conversations initially maintained coherence but eventually converged into repetitive output loops. Convergence occurred in 35 out of 50 trials, with both models producing similar or identical text after a critical point. Quantitative analysis using cosine distance, Jaccard distance, BLEU score, and coherence metrics confirmed this pattern.

## Method Summary
Two independent LLMs (Mistral and Llama) alternated text generation for 25 turns using 50 seed sentences from diverse sources. Models communicated via plain text files, with generation parameters set to top-p=0.95, temperature=0.7, and max_tokens=50. Convergence was detected automatically using threshold-based analysis of four distance metrics (cosine, Jaccard, BLEU, coherence) and confirmed through t-SNE visualization of embedding trajectories.

## Key Results
- 35 out of 50 trials showed convergence into repetitive output loops
- Convergence detected through multiple metrics: cosine distance, Jaccard distance, BLEU-based distance, and coherence difference
- t-SNE visualizations demonstrated output trajectories collapsing into narrow regions
- Convergence occurred despite architectural differences and absence of shared context

## Why This Works (Mechanism)

### Mechanism 1
Conversations between LLMs tend to converge into repetitive loops when models condition on each other's outputs without external intervention. Once a low-diversity phrase appears, it becomes the conditioning context for the next model, which then produces similar output, creating a feedback loop where both models converge on a narrow region of language space rather than exploring new directions.

### Mechanism 2
Convergence manifests in both semantic and lexical spaces, detectable through multiple distance metrics. As conversations progress, both embedding-based distances (cosine similarity in semantic space) and lexical distances (Jaccard, BLEU) decrease, indicating outputs become similar both in meaning and surface form. t-SNE visualizations confirm trajectories collapse into narrow regions.

### Mechanism 3
Convergence occurs despite architectural differences, training data differences, and absence of shared context between models. Models with different tokenizers, training corpora, and optimization choices still converge, suggesting the behavior stems from fundamental properties of autoregressive generation rather than model-specific artifacts.

## Foundational Learning

- **Autoregressive Language Modeling**: Understanding that LLMs generate tokens sequentially, conditioning each prediction on prior context, explains why repetitive input begets repetitive output.
  - Quick check: Can you explain why an autoregressive model given the input "The cat sat on the cat sat on the" might continue the pattern?

- **Embedding Spaces and Distance Metrics**: The paper relies on cosine distance in embedding space and t-SNE visualization to measure semantic convergence; understanding these is essential for interpreting results.
  - Quick check: What does a decreasing cosine distance between consecutive outputs indicate about the conversation's trajectory?

- **Multi-Agent LLM Systems**: The experimental setup differs from single-model prompting; understanding agent communication protocols clarifies why convergence emerges in this specific configuration.
  - Quick check: How does text-only communication between agents (no shared memory or state) differ from a single model generating multiple turns?

## Architecture Onboarding

- **Component map**: Seed sentence → File I/O → Model A → File I/O → Model B → Repeat 25 times → Metric analysis → Convergence detection
- **Critical path**: 1. Seed sentence written to file 2. Model A reads file → generates response → writes output 3. Model B reads Model A's output → generates response → writes output 4. Repeat steps 2-3 for 25 turns 5. Apply distance metrics at each step to detect convergence
- **Design tradeoffs**: 50-token max output may truncate sentences but enables more turns; incomplete sentences were still interpretable by the receiving model. File-based communication ensures isolation but adds latency. Threshold-based convergence detection requires manual calibration against labeled runs.
- **Failure signatures**: Distance metrics dropping below threshold for 3+ consecutive steps, manual inspection showing identical or near-identical outputs across turns, t-SNE trajectory collapsing to a point rather than exploring the space
- **First 3 experiments**: 1. Reproduce with same model pair and seed sentences to validate convergence rate (35/50 in paper) 2. Test with different generation parameters (temperature=1.0, top-p=0.99) to assess sampling sensitivity 3. Swap one model for a different architecture (e.g., GPT-style vs LLaMA-style) to test generalizability of convergence behavior

## Open Questions the Paper Calls Out

### Open Question 1
Can novelty-based fine-tuning or external interventions prevent models from entering repetitive loops? The conclusion states that findings suggest "potential interventions through novelty-based fine-tuning or external controls." This remains unresolved as the study observed behavior in base models without specific anti-repetition fine-tuning or injected controls.

### Open Question 2
How does increasing the maximum token output length affect the timing and nature of convergence? The authors note "Token limits may also affect the behavior" and suggest that "Testing longer outputs may change the pattern." This is unresolved as the strict 50-token cap often produced incomplete sentences, potentially triggering specific syntactic completion patterns in the subsequent turn.

### Open Question 3
Does varying the size or capacity of the interacting models alter the convergence dynamics? The conclusion identifies model size as "another factor," noting the current setup used models with similar capacities (12B and 13B). It is unknown if larger models resist collapse better or if asymmetric pairings (e.g., 7B vs 70B) change the interaction dynamics.

## Limitations
- Only tested with one specific model pair (Mistral Nemo Base 2407 and Llama 2 13B hf), limiting generalizability
- Convergence detection relies on manually set thresholds without systematic sensitivity analysis
- Fixed generation parameters (temperature=0.7, top-p=0.95, max_tokens=50) without exploring parameter space
- 50 seed sentences from specific sources may not represent all conversation types

## Confidence

**High Confidence**: The observation that 35 out of 50 conversations showed convergence into repetitive loops is well-supported by the data and multiple metrics. The consistency across cosine distance, Jaccard distance, BLEU-based distance, and coherence metrics strengthens this claim significantly.

**Medium Confidence**: The claim that convergence represents a fundamental limitation in long-form generative dialogue systems is plausible but not fully proven. The mechanism explanation (autoregressive feedback loops) is reasonable but not directly validated through ablation studies or alternative model architectures.

**Low Confidence**: The generalization claim that this convergence behavior will occur across diverse model pairs and architectures lacks direct empirical support. The paper only tested one specific model combination.

## Next Checks
1. **Cross-Architecture Validation**: Test convergence behavior with a more diverse set of model pairs (e.g., GPT-4o + LLaMA-3, Claude + Mistral) to establish whether the phenomenon generalizes beyond the specific Mistral-Llama combination studied.

2. **Threshold Sensitivity Analysis**: Systematically vary the convergence detection thresholds and analyze how false positive/negative rates change. This would quantify the robustness of the automatic detection method and identify optimal threshold ranges.

3. **Intervention Effectiveness Study**: Test whether introducing novelty-maximizing sampling strategies (higher temperature, top-k sampling, diversity penalties) or external prompts can prevent convergence. Measure both success rates and any degradation in conversation quality.