---
ver: rpa2
title: 'Fine-Tuned Large Language Models for Logical Translation: Reducing Hallucinations
  with Lang2Logic'
arxiv_id: '2512.02987'
source_url: https://arxiv.org/abs/2512.02987
tags:
- language
- logical
- natural
- output
- lang2logic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study presents Lang2Logic, a framework that converts natural
  language into Conjunctive Normal Form (CNF) for logical reasoning tasks. It addresses
  hallucination issues in LLMs during logical translation by combining fine-tuned
  models, symbolic computation libraries, and custom grammar-based parsing.
---

# Fine-Tuned Large Language Models for Logical Translation: Reducing Hallucinations with Lang2Logic

## Quick Facts
- arXiv ID: 2512.02987
- Source URL: https://arxiv.org/abs/2512.02987
- Authors: Muyu Pan; Dheeraj Kodakandla; Mahfuza Farooque
- Reference count: 13
- Primary result: 100% improvement in handling specific hallucination types after feedback loop

## Executive Summary
Lang2Logic presents a framework that converts natural language text into Conjunctive Normal Form (CNF) for logical reasoning tasks, addressing the challenge of LLM hallucinations during logical translation. The approach combines fine-tuned models, symbolic computation libraries, and custom grammar-based parsing to process input text sentence-by-sentence, translating each into logical expressions and converting them to CNF using SymPy. A feedback loop mechanism identifies and corrects hallucinations, with experimental results showing 100% improvement in handling specific hallucination types. The framework simplifies final CNF expressions for computational efficiency and SAT solver compatibility.

## Method Summary
The method processes input text through a three-stage pipeline: (1) NLTK sentence tokenization followed by ChatGPT o1-mini API calls to generate logical expressions, (2) Lark parser with custom grammar to validate and parse expressions, then SymPy conversion to CNF, and (3) SymPy's simplify_logic function to produce compact CNF output. The system employs a feedback loop where detected hallucinations (such as misclassified operators or extra keywords) are fed back to fine-tune the model, enabling intentional correction of recurring error patterns. The approach leverages NLTK for syntactic parsing and relies on structured validation through symbolic computation to ensure accurate logical representation.

## Key Results
- 100% improvement observed on specific hallucination types after feedback loop training
- Successful CNF conversion of natural language to four-clause format (e.g., "If A then B" → And(Or(Not(A), B), Or(Not(B), A), Or(Not(A), Not(B)), Or(A, B)))
- Reliable handling of logical operators including And, Or, Not, and Implies through symbolic computation validation

## Why This Works (Mechanism)

### Mechanism 1: Symbolic Validation as Ground Truth
Symbolic computation libraries provide deterministic validation that catches LLM translation errors. The Lark parser generates structured parse trees from logical expressions, which SymPy then converts to CNF using mathematically guaranteed transformations. When the LLM output violates grammar rules or produces malformed logic, the parser fails explicitly rather than silently accepting errors.

### Mechanism 2: Sentence-Level Isolation Prevents Error Propagation
Processing text sentence-by-sentence limits hallucination scope and improves translation accuracy. NLTK's Punkt tokenizer splits input paragraphs at sentence boundaries, with each sentence routing independently to the o1-mini model. This prevents context bleeding where the model merges or skips sentences during translation.

### Mechanism 3: Feedback Loop Targets Specific Hallucination Patterns
Feeding identified hallucinations back into fine-tuning corrects error patterns for future inputs. When symbolic validation detects a hallucination, the system collects the original input, hallucinated output, and corrected output as training triplets, enabling the fine-tuned model to recognize and avoid specific error types.

## Foundational Learning

- **Conjunctive Normal Form (CNF)**: Required input format for SAT solvers; understanding why `And(Or(A, B), Or(¬C, D))` is valid CNF while nested implications are not is essential for debugging conversion failures. Quick check: Convert `(A → B) ∧ C` to CNF. Answer: `(¬A ∨ B) ∧ C`

- **Context-Free Grammar (CFG)**: The framework uses custom CFG with NLTK to parse sentences into trees; understanding non-terminals like IFCLAUSE and ORCLAUSE helps extend grammar for new sentence structures. Quick check: What parse tree would CFG produce for "If it rains then the ground is wet"?

- **LLM Hallucination Taxonomy**: The detection mechanism targets specific hallucination types (variable reuse errors, misclassified literals); distinguishing pattern-based vs. random errors informs where feedback loops help. Quick check: In Figure 2, why is treating "implies" as a logical operator when it appears lexically a hallucination?

## Architecture Onboarding

- **Component map**: Input Text → NLTK Tokenizer → Sentences → ChatGPT o1-mini API → Logical Expressions → Lark Parser + Grammar → SymPy to CNF → Simplify Logic → Final CNF

- **Critical path**: The o1-mini API call is both the highest-latency and highest-cost component. Parser failures here cascade—validate grammar coverage before scaling.

- **Design tradeoffs**: API vs. local model (o1-mini provides better translation but introduces cost and rate limits), sentence-level vs. document-level processing (isolation improves accuracy but loses cross-sentence coreference), grammar complexity (larger CFG handles more cases but increases parsing ambiguity).

- **Failure signatures**: Parser timeout or malformed tree (grammar does not cover input sentence structure), variable name inconsistency across clauses (LLM failed variable unification; check o1-mini prompt), CNF explosion (exponential clause growth; input contains complex nested implications; pre-simplify).

- **First 3 experiments**: 1) Run the provided test case (Figure 1) through the full pipeline; verify CNF output matches expected four-clause result. 2) Intentionally inject a hallucination (e.g., add extra "implies" in logical expression) and confirm the detection mechanism catches it. 3) Extend the CFG to handle a new sentence pattern (e.g., "Unless P, Q") and validate parse tree generation before connecting to SymPy.

## Open Questions the Paper Calls Out

### Open Question 1
How does Lang2Logic's performance and stability scale with increasing input size and complexity beyond sentence-level processing? The paper explicitly notes "higher costs and potential instability in performance as the input size increases" as a limitation, but experiments appear preliminary and focused on sentence-level processing.

### Open Question 2
To what extent does the fine-tuned model generalize to correct hallucination types not encountered during training? The paper claims 100% improvement on "the same hallucination type of inputs" after feedback, but does not demonstrate generalization to novel hallucination patterns.

### Open Question 3
What is the coverage of the custom-defined context-free grammar for handling diverse linguistic constructions and edge cases? The paper references a "large-scale, custom-defined context-free grammar" but provides no quantitative assessment of grammar coverage or failure modes.

### Open Question 4
Can Lang2Logic extend to more expressive logical formalisms such as first-order logic with quantifiers? The framework targets CNF for SAT solving, yet related work addresses first-order logic; the paper does not discuss extending beyond propositional logic.

## Limitations
- Grammar completeness: The custom CFG handles examples shown but may not cover edge cases like nested quantifiers or temporal logic, requiring manual extension for production deployment.
- API reliability: o1-mini's structured output quality varies by prompt; no ablation study confirms the chosen prompt format maximizes CNF accuracy.
- Generalization of feedback loop: The 100% improvement metric applies only to hallucination types present in the feedback set; novel error patterns from unseen inputs may not benefit from fine-tuning.

## Confidence
- **High confidence**: The CNF conversion pipeline (Lark → SymPy → simplify_logic) produces mathematically valid outputs when parser succeeds, following standard symbolic computation practice.
- **Medium confidence**: Sentence-level isolation reduces hallucinations based on engineering intuition and preliminary results, but lacks systematic comparison to paragraph-level processing.
- **Low confidence**: The fine-tuning feedback mechanism's scalability to diverse hallucination types remains unproven beyond the specific error patterns tested.

## Next Checks
1. **Grammar robustness test**: Feed the system adversarial natural language inputs (e.g., ambiguous pronouns, nested conditionals) and document parser failure rates versus human-annotated CNF ground truth.
2. **API variance analysis**: Run the same input batch through o1-mini multiple times with identical prompts; measure variance in generated logical expressions and resulting CNF outputs.
3. **Cross-sentence coreference evaluation**: Design test cases requiring variable references across sentences; measure accuracy loss when processing sentence-by-sentence versus with context window.