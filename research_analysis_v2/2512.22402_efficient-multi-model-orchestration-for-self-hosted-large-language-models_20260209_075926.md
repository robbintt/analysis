---
ver: rpa2
title: Efficient Multi-Model Orchestration for Self-Hosted Large Language Models
arxiv_id: '2512.22402'
source_url: https://arxiv.org/abs/2512.22402
tags:
- routing
- latency
- distilbert
- orchestration
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Pick and Spin addresses the challenge of efficiently orchestrating
  multiple self-hosted large language models (LLMs) by dynamically routing queries
  to the most suitable model and backend combination while minimizing resource usage.
  It integrates intelligent routing using keyword and DistilBERT-based complexity
  classification with orchestration-aware scaling, all deployed via a unified Helm
  chart on Kubernetes.
---

# Efficient Multi-Model Orchestration for Self-Hosted Large Language Models

## Quick Facts
- arXiv ID: 2512.22402
- Source URL: https://arxiv.org/abs/2512.22402
- Authors: Bhanu Prakash Vangala; Tanu Malik
- Reference count: 4
- Primary result: Up to 21.6% higher success rates, 30% lower latency, and 33% lower GPU cost per query compared to static deployments

## Executive Summary
Pick and Spin addresses the challenge of efficiently orchestrating multiple self-hosted large language models (LLMs) by dynamically routing queries to the most suitable model and backend combination while minimizing resource usage. It integrates intelligent routing using keyword and DistilBERT-based complexity classification with orchestration-aware scaling, all deployed via a unified Helm chart on Kubernetes. Evaluations with four models (Llama-3, Gemma-3, Qwen-3, DeepSeek-R1) across eight benchmarks show significant improvements in success rate, latency, and cost efficiency compared to static deployments.

## Method Summary
The system implements a hybrid routing approach that classifies query complexity using both keyword heuristics and a lightweight DistilBERT classifier trained on 31,019 prompts from eight benchmarks. It maintains a service matrix of model-backend pairs and applies multi-objective optimization to select the best combination based on accuracy, latency, and cost. Dynamic scaling uses Little's Law for capacity planning with warm pools to prevent cold starts, and scale-to-zero automation to reduce costs during idle periods. The entire system is deployed as a unified Helm chart on Kubernetes, leveraging Knative for low-latency inference and KEDA for background scaling.

## Key Results
- 21.6% higher success rate compared to static deployments
- 30% lower latency through intelligent routing and warm pool management
- 33% lower GPU cost per query via orchestration-aware scaling and scale-to-zero automation
- 96.8% classification accuracy on 3-way complexity classification (low/medium/high)
- 75% reduction in recovery time through dynamic orchestration

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Query complexity classification enables cost-aware routing that preserves accuracy while reducing GPU usage.
- Mechanism: A lightweight DistilBERT classifier (trained on 31,019 prompts from 8 benchmarks) predicts complexity as LOW/MEDIUM/HIGH. Simpler queries route to smaller models (Gemma-3 27B), complex queries to larger models (DeepSeek-R1 685B). Keyword-based routing provides a fast-path alternative with near-zero latency overhead.
- Core assumption: Prompt complexity correlates with model size requirements, and a 3-tier classification is sufficient to capture this relationship.
- Evidence anchors: [abstract] "hybrid routing module that balances cost, latency, and accuracy using both keyword heuristics and a lightweight DistilBERT classifier"; [section] "DistilBERT was fine tuned for three way classification... achieved 96.8 percent accuracy on a 10 percent held out validation set"

### Mechanism 2
- Claim: Orchestration-aware scaling with warm pools reduces cold-start latency while maintaining cost efficiency.
- Mechanism: Algorithm 1 applies Little's Law (target = request_rate × latency / concurrency) to determine replica count. Warm pools maintain minimum replicas per tier, preventing full cold starts. Idle models scale to zero after timeout τ. Cooldown periods prevent oscillation.
- Core assumption: Request patterns are stable over the 5-minute telemetry window; cold-start cost exceeds warm-pool holding cost for frequently-accessed models.
- Evidence anchors: [abstract] "adaptive scale-to-zero automation" and "33% lower GPU cost per query"; [section] "Dynamic orchestration reduced recovery time by over 75 percent and decreased cost by approximately one third through on demand scaling"

### Mechanism 3
- Claim: Multi-objective score normalization enables meaningful trade-offs between accuracy, latency, and cost across heterogeneous hardware.
- Mechanism: Equation (2) normalizes relevance (R̂), latency (T̂), and cost (Ĉ) to [0,1], then applies operator-defined weights (wR, wT, wC). Four profiles (quality, cost, speed, balanced) derived via grid search on 3,000 validation prompts.
- Core assumption: Historical statistics provide stable min-max bounds for normalization; operator preferences remain consistent within a profile.
- Evidence anchors: [abstract] "dynamically routing queries to the most suitable model and backend combination while minimizing resource usage"; [section] "These profiles were obtained by grid search over 3,000 validation prompts, optimizing the system objective corresponding to each operator preference"

## Foundational Learning

- Concept: **Kubernetes autoscaling primitives (HPA, KEDA, Knative)**
  - Why needed here: Pick and Spin relies on Knative for low-latency inference and KEDA for background scaling. Understanding event-driven scaling is essential for debugging cold-start issues.
  - Quick check question: Can you explain how KEDA determines when to scale a deployment from zero?

- Concept: **Inference backend trade-offs (vLLM vs TensorRT-LLM vs TGI)**
  - Why needed here: The service matrix M ∈ R^(L×I) requires selecting backends based on latency (TensorRT-LLM) vs throughput (vLLM) vs memory efficiency (TGI).
  - Quick check question: Which backend would you choose for a latency-sensitive chat application vs a batch document processing pipeline?

- Concept: **Little's Law for capacity planning**
  - Why needed here: Algorithm 1 uses Little's Law to compute target replicas from request rate and latency. Misunderstanding this leads to over/under-provisioning.
  - Quick check question: Given 10 requests/sec average rate and 2-second average latency, how many concurrent replicas are needed?

## Architecture Onboarding

- Component map: API Gateway -> Router (complexity classification) -> Service Registry (score lookup) -> Orchestrator (scaling decisions) -> Backend Pool (inference) -> Telemetry (feedback)

- Critical path: Prompt → API Gateway → Router (complexity classification) -> Service Registry (score lookup) → Orchestrator (ensure model active) → Backend Pool (inference) → Response. The Router classification and Orchestrator scaling decisions are the two latency-sensitive decision points.

- Design tradeoffs:
  - Keyword routing: Lower latency (~45s TTFT) but lower semantic accuracy; DistilBERT routing: Higher accuracy (+8.6%) but higher latency (~56s TTFT, +23.5%)
  - Warm pools: Reduce cold-start but incur holding cost; scale-to-zero: Saves cost but risks cold-start latency
  - Unified Helm chart: Simplifies deployment but requires careful versioning across models and backends

- Failure signatures:
  - High TTFT with low throughput: Check if warm pool size is insufficient or cooldown expired prematurely
  - Routing accuracy degradation on new domains: DistilBERT classifier may be out-of-distribution; fall back to keyword routing
  - GPU underutilization with high latency: Service matrix may have stale cost/latency estimates; refresh telemetry
  - Scaling oscillation: Cooldown period τ may be too short; increase to prevent thrashing

- First 3 experiments:
  1. **Baseline calibration**: Deploy all four models with static configuration, measure success rate/latency/cost per benchmark to establish reference points matching Table 1.
  2. **Routing ablation**: Compare keyword-only vs DistilBERT-only vs hybrid routing on a held-out benchmark subset, measuring accuracy-latency tradeoff as in Figure 7.
  3. **Scaling stress test**: Send bursty workload (10→1000 QPS) and observe TTFT, recovery time, and replica count to validate Algorithm 1 behavior under load.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can reinforcement learning (RL) agents outperform the current hybrid routing strategy in adapting to dynamic query distributions and drift?
- Basis in paper: [explicit] The authors state in the Discussion and Conclusion that future work will explore "reinforcement based routing for adaptive decision making."
- Why unresolved: The current system relies on static keyword heuristics and a pre-trained DistilBERT classifier, which may not adapt to shifting workload patterns without manual retraining.
- What evidence would resolve it: Benchmarks comparing RL-based routing convergence time and accuracy against the hybrid DistilBERT approach under simulated workload drift.

### Open Question 2
- Question: Does integrating energy-aware scheduling significantly reduce operational power consumption without degrading the latency and cost efficiency achieved by Pick and Spin?
- Basis in paper: [explicit] The Conclusion identifies "energy efficient scheduling" as a necessary extension for sustainable multi-model deployment.
- Why unresolved: The current optimization function balances relevance, latency, and cost, but lacks an explicit constraint or weight for energy consumption or carbon footprint.
- What evidence would resolve it: Measurements of total power draw (kWh) alongside success rates when an energy penalty term is added to the orchestration score $f(p, S_{x,y})$.

### Open Question 3
- Question: How does the Pick and Spin architecture perform when orchestrating multimodal models (e.g., vision-language) compared to the text-only LLMs evaluated?
- Basis in paper: [explicit] The Conclusion lists the "integration of multimodal models" as a specific direction for future work.
- Why unresolved: The current routing relies on a text-based DistilBERT classifier and text keywords, which cannot assess the complexity of image or audio inputs included in multimodal prompts.
- What evidence would resolve it: Evaluation of routing accuracy and GPU memory overhead when deploying multimodal architectures within the existing Kubernetes Helm charts.

## Limitations

- Generalization concerns: The DistilBERT classifier was trained on 31,019 prompts from eight specific benchmarks, raising questions about performance on real-world queries outside these domains.
- Hardware dependencies: Results are heavily dependent on specific GPU configurations and backend implementations, with the 33% cost reduction claim assuming particular hardware availability.
- Multi-objective optimization validity: The normalization approach assumes stable min-max bounds for latency, cost, and accuracy across different workloads, with no sensitivity analysis provided.

## Confidence

- **High confidence**: The fundamental architecture combining intelligent routing with orchestration-aware scaling is technically sound and the core mechanisms are well-established approaches. The 21.6% higher success rate claim is supported by comparative evaluations.
- **Medium confidence**: The specific performance improvements (30% latency reduction, 33% cost reduction) are likely reproducible under similar hardware conditions but may not generalize to different infrastructure setups or workload patterns.
- **Low confidence**: The absolute numerical improvements are difficult to verify without access to the exact benchmark datasets, hardware specifications, and implementation details.

## Next Checks

1. **Domain generalization test**: Evaluate the DistilBERT classifier on real-world production queries from domains not represented in the original benchmark set (e.g., medical, legal, or domain-specific technical queries). Measure accuracy degradation and determine if fallback mechanisms are sufficient.

2. **Workload pattern sensitivity**: Test the system under different traffic patterns (constant vs bursty vs periodic) and measure how the scaling algorithm's performance varies. Specifically, validate that warm pool configurations and cooldown periods prevent oscillation under realistic production workloads.

3. **Hardware infrastructure variation**: Deploy the system on different GPU configurations (e.g., A100 vs H100, varying GPU counts) and measure how the cost reduction and latency improvements scale with hardware changes. This would validate whether the 33% cost reduction is infrastructure-dependent or more general.