---
ver: rpa2
title: 'Using Generative AI in Software Design Education: An Experience Report'
arxiv_id: '2506.21703'
source_url: https://arxiv.org/abs/2506.21703
tags:
- design
- genai
- chatgpt
- students
- software
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces generative AI (ChatGPT) into an undergraduate
  software design course, requiring 179 students to use it for a team-based design
  assignment. Students engaged ChatGPT for ideation, knowledge seeking, design validation,
  and collaboration, while applying expert design practices taught in class.
---

# Using Generative AI in Software Design Education: An Experience Report

## Quick Facts
- arXiv ID: 2506.21703
- Source URL: https://arxiv.org/abs/2506.21703
- Reference count: 40
- Key outcome: Generative AI (ChatGPT) integration into an undergraduate software design course helped students with ideation and design validation, but required human oversight to address inaccuracies and complexity.

## Executive Summary
This study introduces generative AI (ChatGPT) into an undergraduate software design course, requiring 179 students to use it for a team-based design assignment. Students engaged ChatGPT for ideation, knowledge seeking, design validation, and collaboration, while applying expert design practices taught in class. Most found ChatGPT helpful for generating ideas and starting designs but noted the need for human oversight due to inaccuracies, complexity, and superficiality in responses. Students appreciated the learning experience and productivity gains, yet emphasized that human designers must lead the process. Key lessons include the importance of reflection, prompt engineering training, and using expert practices to anchor AI usage. The study highlights that while ChatGPT is a valuable tool in design education, it should supplement—not replace—human-centered design processes.

## Method Summary
The study integrated ChatGPT into a third-year undergraduate software design course (Design Studio 2 - Part 1). Students were trained in expert design practices and UML modeling before a dedicated GenAI module. One lecture demonstrated three prompt strategies (Brute Force, Start Small and Grow, Focus on Goal) and using ChatGPT to generate PlantUML text for visualization. Teams were required to engage in at least 10 distinct conversation attempts, submit conversation logs, and write reflections on their experience. Qualitative analysis of reflections and logs identified usage patterns and perceived strengths/weaknesses.

## Key Results
- Students found ChatGPT helpful for ideation and starting designs but needed human oversight due to inaccuracies and complexity.
- Expert design practices provided a framework for critically evaluating AI outputs and guiding iterative refinement.
- Structured reflection consolidated learning about tool capabilities and limitations, with some students demonstrating advanced prompting strategies.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prior training in expert design practices improves students' ability to use GenAI effectively for software design tasks.
- Mechanism: Expert practices provide a cognitive framework that helps students evaluate AI outputs critically, recognize when responses are overly complex or superficial, and guide iterative refinement.
- Core assumption: Students can transfer expert practices learned without GenAI to GenAI-assisted work.
- Evidence anchors:
  - [abstract] "the design practices taught in class helped them anchor their GenAI usage"
  - [section 3.1] "This delayed introduction would allow the students to receive some instruction on foundational expert design practices... Introducing GenAI early in the class could lead to a fixation on using it rather than learning how to design"
  - [section 5.1, Lesson #3] "Expert design practices help in anchoring GenAI usage... served as a crucial framework, guiding students in their use of GenAI"
- Break condition: If students lack foundational design knowledge, they may accept GenAI outputs uncritically, leading to superficial or incorrect designs.

### Mechanism 2
- Claim: Positioning GenAI as a collaborative tool rather than an autonomous designer preserves human agency and improves design outcomes.
- Mechanism: When students treat GenAI as an ideation partner, validator, or "second pair of eyes," they maintain ownership of decisions, synthesize multiple perspectives, and catch errors—leading to more coherent, context-appropriate designs.
- Core assumption: Students are willing and able to critique AI outputs rather than defer to them.
- Evidence anchors:
  - [abstract] "students identified numerous ways ChatGPT helped them in their design process while recognizing the need to critique the response before incorporating it into their design"
  - [section 4.4.2] "Overwhelmingly, the students felt the human designers needed to lead the design rather than being wholly dependent on ChatGPT"
- Break condition: If students outsource decisions entirely to GenAI (observed in some teams asking ChatGPT to choose between options), they may produce designs they do not fully understand.

### Mechanism 3
- Claim: Structured reflection on GenAI use consolidates learning about tool capabilities and limitations.
- Mechanism: Written reflection forces students to articulate what worked, what failed, and why—reinforcing metacognitive awareness and surfacing diverse prompting strategies for peer learning.
- Core assumption: Students engage honestly and thoughtfully in reflection exercises.
- Evidence anchors:
  - [section 5.1, Lesson #2] "asking the students to reflect on their experiences of using GenAI helped them reinforce the advantages and disadvantages... notably, some students demonstrated advanced prompting strategies, presenting an opportunity for knowledge sharing"
- Break condition: If reflection prompts are too narrow, students may not surface broader insights (e.g., the paper notes no reflection on societal issues like ethics or sustainability).

## Foundational Learning

- Concept: **Expert design practices (e.g., focusing on essence, generating alternatives, addressing knowledge gaps)**
  - Why needed here: These practices anchor GenAI use, enabling students to critique outputs for complexity, superficiality, and relevance.
  - Quick check question: Given a design problem, can you identify the core essence and list at least three alternative approaches before refining one?

- Concept: **UML class diagrams and pseudocode fundamentals**
  - Why needed here: Students must understand correct structure and semantics to evaluate GenAI-generated models and detect missing relationships or syntactic errors.
  - Quick check question: Can you manually sketch a simple UML class diagram with appropriate relationships (association, aggregation, dependency) for a basic domain?

- Concept: **Prompt engineering basics (context-setting, iterative refinement, decomposition)**
  - Why needed here: Effective prompting improves response quality; poor prompts lead to irrelevant or overly generic outputs.
  - Quick check question: How would you reframe a vague prompt like "design a traffic simulator" to include context, constraints, and a specific artifact request?

## Architecture Onboarding

- Component map: Expert design training -> GenAI demonstration -> Assignment (10 conversations + reflection) -> Analysis and feedback
- Critical path:
  1. Teach expert practices and design fundamentals (no GenAI).
  2. Demonstrate GenAI-assisted design (show, don't prescribe).
  3. Students complete GenAI-required assignment with conversation logging.
  4. Students submit reflections; instructor surfaces novel strategies for class discussion.
  5. Optional GenAI use in subsequent assignments with continued logging/reflection.

- Design tradeoffs:
  - Training depth vs. exploration: More training reduces struggle but may constrain creative prompting approaches.
  - Mandatory vs. optional use: Mandatory ensures exposure but may feel forced; optional risks dropout (observed ~50% reduction in use when optional).
  - Conversation count vs. depth: Requiring many short conversations may discourage deeper iterative dialogs.

- Failure signatures:
  - Over-reliance: Students accept GenAI outputs without critique (look for uncritical copy-paste in logs).
  - Complexity drift: GenAI proposes out-of-scope features; students fail to filter (look for "essence" violations in reflections).
  - Superficiality: Generic responses accepted without domain-specific refinement.
  - Syntax issues: PlantUML or pseudocode errors not caught by students.

- First 3 experiments:
  1. **Pilot with a single design artifact**: Require GenAI for UML only; log prompts and collect reflections to identify common issues.
  2. **A/B prompt strategy comparison**: Ask half the teams to use "brute force" and half to use "start small and grow"; compare output quality and reflection insights.
  3. **Validation-focused exercise**: Provide a pre-generated flawed GenAI design and ask students to identify and fix issues—tests critique skills before full design tasks.

## Open Questions the Paper Calls Out
None

## Limitations
- Small N for qualitative claims: Findings are suggestive rather than statistically generalizable.
- Single-context deployment: Results are specific to a third-year undergraduate software design course with prior expert-design training.
- No control group: Without a comparison cohort, it is unclear how much gains are attributable to GenAI versus other factors.

## Confidence
- **High confidence**: GenAI was perceived as helpful for ideation and starting designs; students valued structured reflection and prompt engineering training.
- **Medium confidence**: Expert design practices improved students' ability to critically evaluate GenAI outputs; human oversight remained essential.
- **Low confidence**: Broader societal implications (ethics, sustainability) were not addressed in reflections, limiting claims about responsible AI use in design education.

## Next Checks
1. **A/B test with control group**: Run parallel design assignments—one with GenAI, one without—to quantify differences in design quality, student confidence, and perceived productivity.
2. **Cross-disciplinary pilot**: Deploy the same module in a non-technical design course (e.g., architecture or product design) to test transferability of expert practice anchoring.
3. **Longitudinal reflection study**: Collect reflections over multiple assignments to track changes in students' GenAI literacy, critique skills, and agency over time.