---
ver: rpa2
title: Towards Reliable and Interpretable Document Question Answering via VLMs
arxiv_id: '2509.10129'
source_url: https://arxiv.org/abs/2509.10129
tags:
- document
- vlms
- spatial
- answer
- bounding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of reliable answer localization
  in document question answering using vision-language models (VLMs), which, despite
  strong textual accuracy, often fail to correctly identify where answers are located
  within documents. To tackle this, the authors propose DocExplainerV0, a plug-and-play
  bounding-box prediction module that decouples answer generation from spatial localization,
  allowing it to be applied to existing VLMs, including proprietary ones.
---

# Towards Reliable and Interpretable Document Question Answering via VLMs

## Quick Facts
- arXiv ID: 2509.10129
- Source URL: https://arxiv.org/abs/2509.10129
- Reference count: 40
- VLMs excel in textual accuracy but fail in spatial localization for document QA

## Executive Summary
This paper addresses the critical challenge of reliable answer localization in document question answering using vision-language models (VLMs). While VLMs demonstrate strong performance in generating correct textual answers, they consistently fail to accurately identify where those answers are located within documents. The authors introduce DocExplainerV0, a novel plug-and-play bounding-box prediction module that decouples answer generation from spatial localization, enabling application to existing VLMs including proprietary models. Experiments on the BoundingDocs v2.0 dataset reveal that while VLMs achieve high textual accuracy (ANLS up to 0.737), their spatial localization remains poor (IoU as low as 0.011). DocExplainerV0 significantly improves localization performance (IoU up to 0.188) but still falls short of naive OCR-based baselines, establishing a benchmark for future research in interpretable document VQA.

## Method Summary
DocExplainerV0 employs a dual-branch architecture that processes both image and text embeddings from SigLiP2 to predict bounding boxes independently of answer generation. The method decouples spatial localization from textual reasoning, allowing it to be applied as a plug-and-play module to existing VLMs, including proprietary models. Trained using Smooth L1 loss on normalized bounding box coordinates, the approach aims to bridge the gap between strong textual performance and weak spatial grounding in document QA systems.

## Key Results
- VLMs achieve high textual accuracy (ANLS up to 0.737) but poor spatial localization (IoU as low as 0.011)
- DocExplainerV0 improves IoU to 0.188, significantly outperforming vanilla VLMs
- Naive OCR-based baseline achieves IoU of 0.405-0.494, outperforming DocExplainerV0
- The method successfully decouples answer generation from localization, validating the plug-and-play design

## Why This Works (Mechanism)
The approach works by separating the spatial reasoning task from the textual comprehension task, allowing specialized training for localization. By using SigLiP2 embeddings and a dual-branch architecture, DocExplainerV0 can focus on learning the spatial relationships within documents without being constrained by the answer generation process. The Smooth L1 loss function provides stable training for bounding box regression, while the plug-and-play design enables integration with existing VLMs without requiring retraining of the base model.

## Foundational Learning
- Vision-Language Models (VLMs): Multimodal models that process both visual and textual information; needed for document understanding; quick check: can process image+text inputs
- Bounding Box Prediction: Task of identifying spatial regions in images; needed for localizing answers; quick check: outputs normalized coordinates (x,y,w,h)
- SigLiP2 Embeddings: Specialized embeddings for document understanding; needed for effective spatial reasoning; quick check: captures layout and text features
- Smooth L1 Loss: Robust loss function for regression tasks; needed for stable bounding box training; quick check: less sensitive to outliers than L2 loss
- Mean IoU: Intersection-over-Union metric for localization accuracy; needed for evaluating spatial performance; quick check: measures overlap between predicted and ground truth boxes
- ANLS: Answer-based Normalized Levenshtein Similarity; needed for measuring textual answer quality; quick check: higher values indicate better textual accuracy

## Architecture Onboarding

Component Map: Input Document -> SigLiP2 Encoder -> Dual Branch (Answer Branch + Localization Branch) -> Bounding Box Output

Critical Path: Document image → SigLiP2 embeddings → Dual-branch processing → Bounding box prediction → Evaluation via IoU

Design Tradeoffs: Decoupling localization from answer generation enables plug-and-play deployment but may introduce latency; dual-branch architecture increases model complexity but improves specialization; reliance on SigLiP2 embeddings provides strong performance but may limit generalizability.

Failure Signatures: Poor IoU scores indicate spatial reasoning failures; low ANLS suggests textual comprehension issues; discrepancies between textual accuracy and localization performance reveal the core challenge being addressed.

First Experiments:
1. Evaluate DocExplainerV0 on additional document QA datasets to test generalizability
2. Measure latency and computational overhead compared to end-to-end VLM approaches
3. Test integration with multiple proprietary VLMs to validate plug-and-play claims

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions in the text provided.

## Limitations
- Modest improvement in IoU compared to naive OCR baseline suggests localization remains challenging
- Performance evaluation limited to single dataset (BoundingDocs v2.0) may not reflect real-world diversity
- Plug-and-play validation on proprietary VLMs remains theoretical without empirical testing
- Decoupling approach may introduce deployment complexity and latency

## Confidence
- High: Textual accuracy (ANLS) results for VLMs are reliable and well-supported
- Medium: IoU improvement with DocExplainerV0 is real but not yet competitive with baselines
- Medium: Plug-and-play nature is plausible but not exhaustively tested across diverse VLMs

## Next Checks
1. Evaluate DocExplainerV0 on multiple document QA datasets with varying layouts and question types to assess generalizability
2. Compare latency and computational overhead of decoupled architecture against end-to-end VLM approaches in real-time scenarios
3. Test the method on additional proprietary VLMs to confirm plug-and-play claim and assess performance consistency