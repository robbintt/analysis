---
ver: rpa2
title: 'Bidirectional Soft Actor-Critic: Leveraging Forward and Reverse KL Divergence
  for Efficient Reinforcement Learning'
arxiv_id: '2506.01639'
source_url: https://arxiv.org/abs/2506.01639
tags:
- policy
- divergence
- forward
- distribution
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the instability and sample inefficiency of
  policy updates in Soft Actor-Critic (SAC) caused by gradient-based optimization
  of reverse KL divergence. The authors propose Bidirectional SAC, which leverages
  both forward and reverse KL divergence: forward KL provides an explicit optimal
  projection policy (the mean and variance of the Boltzmann distribution''s marginals),
  and reverse KL refines this initial policy.'
---

# Bidirectional Soft Actor-Critic: Leveraging Forward and Reverse KL Divergence for Efficient Reinforcement Learning

## Quick Facts
- arXiv ID: 2506.01639
- Source URL: https://arxiv.org/abs/2506.01639
- Reference count: 40
- Key outcome: Bidirectional SAC leverages forward and reverse KL divergence to improve sample efficiency and stability, achieving up to 30% higher episodic rewards on MuJoCo and Box2D benchmarks compared to standard SAC and other baselines.

## Executive Summary
This paper addresses the instability and sample inefficiency of policy updates in Soft Actor-Critic (SAC) caused by gradient-based optimization of reverse KL divergence. The authors propose Bidirectional SAC, which leverages both forward and reverse KL divergence: forward KL provides an explicit optimal projection policy (the mean and variance of the Boltzmann distribution's marginals), and reverse KL refines this initial policy. A VDN-a network is introduced to efficiently approximate the marginal Q-values needed for computing the forward KL projection. Experiments on MuJoCo and Box2D benchmarks show Bidirectional SAC achieves up to 30% higher episodic rewards and faster convergence compared to standard SAC and other baselines like A2C, PPO, and DDPG. The method demonstrates improved sample efficiency and stability by integrating complementary advantages of both KL directions.

## Method Summary
Bidirectional SAC addresses SAC's limitations by combining forward and reverse KL divergence for policy optimization. The method introduces a two-step actor update: first, a forward KL step computes an initial policy as the mean and variance of the Boltzmann distribution's marginals, using a VDN-a network to approximate these marginals efficiently; second, a reverse KL step refines this initial policy. This bidirectional approach provides a good initialization (forward KL) and fine-tuning (reverse KL), leading to more stable and efficient learning. The VDN-a network structure is designed to facilitate marginal recovery by sharing parameters across dimensions while allowing independent scaling.

## Key Results
- Bidirectional SAC achieves up to 30% higher episodic rewards compared to standard SAC and other baselines on MuJoCo and Box2D benchmarks.
- The method demonstrates faster convergence, requiring fewer samples to reach optimal performance.
- Improved sample efficiency and stability are attributed to the complementary advantages of forward and reverse KL divergence in policy optimization.

## Why This Works (Mechanism)
The paper addresses the instability in SAC's policy updates caused by the difficulty of gradient-based optimization of reverse KL divergence. Forward KL divergence provides an explicit optimal projection policy (the mean and variance of the Boltzmann distribution's marginals), which serves as a good initialization for the actor. Reverse KL divergence then refines this initial policy through gradient descent. By leveraging both directions, Bidirectional SAC combines the stability of the forward KL initialization with the fine-tuning capability of reverse KL, resulting in more efficient and stable learning.

## Foundational Learning
- **Soft Actor-Critic (SAC):** A maximum entropy reinforcement learning algorithm that optimizes both expected return and policy entropy. Why needed: Provides the baseline algorithm that Bidirectional SAC improves upon.
- **KL Divergence (Forward and Reverse):** Measures the difference between probability distributions. Forward KL (P||Q) penalizes missing modes in Q, while reverse KL (Q||P) penalizes extra modes in Q. Why needed: The paper exploits the complementary properties of both KL directions for policy optimization.
- **Boltzmann Distribution:** A probability distribution over actions proportional to the exponential of the Q-values. Why needed: The optimal policy in the maximum entropy framework is derived from the Boltzmann distribution.
- **VDN-a Network:** A value decomposition network with auxiliary outputs to approximate marginal Q-values. Why needed: Efficiently computes the marginals required for the forward KL projection.
- **Maximum Entropy Reinforcement Learning:** A framework that maximizes both expected return and policy entropy. Why needed: The theoretical foundation for the optimality of the Boltzmann policy and the use of KL divergence in policy optimization.

## Architecture Onboarding

Component Map:
- Environment states -> Q-networks (twin) -> Q-values
- Q-values -> Boltzmann distribution -> Optimal policy (forward KL)
- Q-values -> VDN-a network -> Marginal Q-values
- Marginal Q-values -> Mean and variance -> Initial policy (forward KL step)
- Initial policy + Q-values -> Actor network -> Refined policy (reverse KL step)
- Actor network + Q-networks -> Critic loss -> Q-network update

Critical Path:
1. Sample environment states and actions
2. Compute Q-values using twin Q-networks
3. Approximate marginal Q-values using VDN-a network
4. Compute forward KL projection (mean and variance)
5. Initialize actor policy with forward KL solution
6. Refine actor policy using reverse KL divergence
7. Update Q-networks using TD3-style critic loss

Design Tradeoffs:
- The VDN-a network adds computational overhead but enables efficient marginal recovery for the forward KL step.
- Bidirectional optimization increases training complexity but provides better initialization and fine-tuning.
- The method assumes Gaussian policies with diagonal covariance, which may not be optimal for all environments.

Failure Signatures:
- Poor performance if the VDN-a network fails to accurately approximate marginal Q-values.
- Instability if the forward KL initialization is far from the optimal policy.
- Limited effectiveness in environments with complex action correlations or multi-modal distributions.

Three First Experiments:
1. Implement and test the VDN-a network architecture on a simple environment to verify its ability to recover marginal Q-values.
2. Validate the forward KL projection calculation on a toy problem with known optimal policy.
3. Compare the performance of Bidirectional SAC against standard SAC on a simple continuous control task.

## Open Questions the Paper Calls Out
### Open Question 1
- **Question:** Can the theoretical gap between the VDN-a approximation and the true optimal projection policy be closed without relying on gradient descent?
- **Basis in paper:** [Explicit] The conclusion states, "further research could aim to bridge the gap between these practical approximations and the theoretically true optimal projection policy."
- **Why unresolved:** The current implementation relies on neural networks (VDN-a) to approximate marginals and gradient descent for actor updates, which introduces approximation errors compared to the theoretical ideal.
- **What evidence would resolve it:** A theoretical framework or closed-form solution that computes the optimal projection without approximation errors, or empirical proof that the approximation error converges to zero.

### Open Question 2
- **Question:** How does Bidirectional SAC perform when target marginal distributions are highly multi-modal or contain strong inter-dimensional dependencies?
- **Basis in paper:** [Inferred] Appendix C.2.4 notes that VDN-a performance is challenged by complex marginal structures (e.g., bimodal) because the auxiliary network must model intricate dependencies to isolate marginal terms.
- **Why unresolved:** The VDN-a network assumes a structure that facilitates marginal recovery, but Figure 9 indicates difficulty in approximating bimodal target marginals compared to unimodal ones.
- **What evidence would resolve it:** Benchmark results on environments specifically designed to induce multi-modal Boltzmann distributions or complex action correlations.

### Open Question 3
- **Question:** Can the Bidirectional SAC framework be generalized to non-Gaussian policy families where explicit forward KL solutions are intractable?
- **Basis in paper:** [Inferred] Section 3.2 explicitly derives the optimal projection based on the assumption that the policy is Gaussian with diagonal covariance. The method relies on this tractability for the "Forward SAC" initialization step.
- **Why unresolved:** The core contribution relies on the explicit calculation of mean and variance, which may not be available or sufficient for other policy distributions (e.g., mixture models).
- **What evidence would resolve it:** A derivation of the forward KL projection for a non-Gaussian policy class or a modified algorithm that does not require an explicit analytical solution for initialization.

## Limitations
- The VDN-a network may overfit to specific task distributions and may not generalize well to environments with significantly different dynamics.
- The bidirectional optimization introduces computational overhead, and the paper does not thoroughly address the added complexity.
- Ablation studies do not isolate the specific contributions of forward versus reverse KL components, making it difficult to quantify their individual impacts.

## Confidence
- High: The claim that Bidirectional SAC leverages both forward and reverse KL divergence is well-supported by the algorithm description and mathematical formulation.
- Medium: The assertion of up to 30% higher episodic rewards and faster convergence is based on benchmark experiments, but lacks extensive ablation studies to isolate the source of gains.
- Medium: The improvement in sample efficiency and stability is plausible given the bidirectional optimization, but the paper does not provide a detailed comparison of computational costs or robustness to hyperparameter changes.

## Next Checks
1. Perform ablation studies to separately evaluate the impact of forward and reverse KL divergence components on final performance and sample efficiency.
2. Test Bidirectional SAC on more diverse and challenging environments, including sparse-reward tasks and high-dimensional state spaces, to assess generalizability.
3. Conduct a thorough analysis of computational overhead and training stability across a range of hyperparameters to confirm robustness and practical applicability.