---
ver: rpa2
title: 'Probing In-Context Learning: Impact of Task Complexity and Model Architecture
  on Generalization and Efficiency'
arxiv_id: '2505.06475'
source_url: https://arxiv.org/abs/2505.06475
tags:
- learning
- tasks
- function
- in-context
- regression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper explores in-context learning (ICL) across varied model\
  \ architectures and task complexities. A framework is set up where four models\u2014\
  GPT2-style Transformer, FlashAttention Transformer, Hyena, and Mamba\u2014are trained\
  \ from scratch on synthetic tasks: linear regression, Gaussian kernel regression,\
  \ and nonlinear dynamical systems."
---

# Probing In-Context Learning: Impact of Task Complexity and Model Architecture on Generalization and Efficiency

## Quick Facts
- arXiv ID: 2505.06475
- Source URL: https://arxiv.org/abs/2505.06475
- Reference count: 23
- This paper evaluates in-context learning across four model architectures on synthetic tasks to reveal how architectural design affects generalization and efficiency.

## Executive Summary
This paper systematically evaluates in-context learning (ICL) performance across four distinct architectures—GPT2-style Transformer, FlashAttention Transformer, Hyena, and Mamba—on synthetic function approximation tasks of varying complexity. The authors construct a controlled experimental framework where models must predict query outputs from prompt input-output pairs without parameter updates. Their findings reveal that architectural choices significantly impact ICL effectiveness: Transformers generalize well across tasks, Mamba excels in temporally structured dynamics, Hyena captures long-range dependencies with higher early variance, and FlashAttention offers efficiency but is sensitive in low-data regimes. The work provides concrete insights into how architectural inductive biases align with task structures and suggests design considerations for future ICL-focused models.

## Method Summary
The authors evaluate ICL across four model architectures on three synthetic function families: linear regression, Gaussian kernel regression, and nonlinear dynamical systems. Each model processes a prompt of input-output pairs to predict a new output without parameter updates. The training protocol uses AdamW optimizer with cosine learning rate schedules, curriculum learning (starting at low dimensions and increasing), and on-the-fly data generation. Performance is measured by mean squared error compared to baseline estimators like least squares and nearest neighbors. The architectures vary in their computational approaches: Transformers use full-context attention, Hyena employs subquadratic convolutional operators, and Mamba uses selective state-space modeling.

## Key Results
- Transformers achieve the best overall generalization across all task types, validating their versatility in ICL settings
- Mamba demonstrates superior performance on temporally structured dynamics due to its state-space design that naturally captures sequential recurrence
- Input scaling (expanding from [-1,1] to [-2,2]) significantly improves feature separability and reduces MSE in nonlinear tasks
- Curriculum learning accelerates convergence in high-dimensional settings by preventing early-stage gradient starvation

## Why This Works (Mechanism)

### Mechanism 1: Architectural Inductive Bias Aligns with Function Structure
Models perform better when their architectural inductive biases match the underlying structure of the target function class. State-space models like Mamba encode sequential recurrence natively via implicit continuous-time dynamics, enabling efficient tracking of temporal dependencies in nonlinear dynamical systems. Transformers use full-context attention, providing uniform access to all prompt positions but lacking built-in recurrence priors.

### Mechanism 2: Input Scaling Enhances Geometric Separability in Nonlinear Tasks
Expanding input range amplifies nonlinear term variations, improving the model's ability to distinguish support points from query points. When inputs are confined to narrow ranges, higher-order terms show minimal variation. Scaling increases curvature and gradient magnitudes, making derivative patterns more salient for attention weights and convolutional state updates to detect.

### Mechanism 3: Curriculum Learning Prevents Gradient Starvation in High Dimensions
Gradual dimension and context-length escalation preserves gradient signal strength, preventing early-training stagnation. Starting directly with high-dimensional prompts causes orthogonality and uniform input influence, leading to negligible gradients until a mechanism is discovered. Low-dimensional warm-start allows mechanism discovery first, then progressive expansion maintains signal quality.

## Foundational Learning

- Concept: In-Context Learning (ICL) as Meta-Learning
  - Why needed here: The entire framework treats ICL as learning to approximate functions from a task distribution without parameter updates.
  - Quick check question: Can you explain why ICL differs from fine-tuning at inference time?

- Concept: State-Space Models (SSMs) and Selective Recurrence
  - Why needed here: Mamba's superior performance on dynamics tasks requires understanding how SSMs differ from attention in processing sequential dependencies.
  - Quick check question: How does a state-space model's hidden state carry information across timesteps differently from a Transformer's attention pattern?

- Concept: Kernel Regression and Locality
  - Why needed here: The Gaussian kernel task reveals how locality can create trivial shortcuts; understanding kernel bandwidth effects is essential.
  - Quick check question: What happens to a Gaussian kernel's smoothing behavior as bandwidth h approaches zero versus infinity?

## Architecture Onboarding

- Component map:
  GPT2-style Transformer: 12 decoder layers, 8 attention heads, 256-dim embeddings, causal self-attention
  FlashAttention Transformer: Same architecture with IO-aware attention optimization
  Hyena: Attention replaced with subquadratic convolutional operators (implicit long convolutions + gating)
  Mamba: 24 layers, 256-dim embeddings, selective state-space blocks with no attention

- Critical path:
  1. Generate synthetic task (sample function f, inputs x₁...xₙ, compute outputs)
  2. Construct prompt: [(x₁, y₁), ..., (xₖ, yₖ), xₖ₊₁]
  3. Forward pass through model (no gradient updates at test time)
  4. Compute MSE between predicted ŷₖ₊₁ and true yₖ₊₁

- Design tradeoffs:
  Transformer: Best generalization, quadratic compute O(T²), limited context length
  FlashAttention: Same expressivity with memory efficiency, but sensitive in low-data regimes
  Hyena: Long-range dependency capture, subquadratic cost, higher early-training variance
  Mamba: Linear-time O(T), excels on temporal tasks, slower early learning on non-temporal tasks

- Failure signatures:
  Flat loss curves on Gaussian kernel → likely exploiting local interpolation shortcuts; add linear readout layer
  High variance early training with Hyena → expected; requires longer warmup or curriculum
  FlashAttention underperforms with few examples → increase prompt length or switch to baseline Transformer
  Mamba stagnates on non-dynamic tasks → verify task has temporal structure or use Transformer

- First 3 experiments:
  1. Replicate linear regression baseline across all 4 architectures; verify Transformer and Mamba converge to least-squares baseline performance
  2. Test input scaling hypothesis: train on Gaussian kernel with inputs in [-1,1] vs. [-2,2]; measure MSE reduction
  3. Compare curriculum vs. non-curriculum training on 100-dimensional nonlinear dynamics; track convergence speed and final MSE

## Open Questions the Paper Calls Out

### Open Question 1
Can hybrid architectures combining Transformer attention mechanisms with Mamba's state-space modeling achieve superior ICL performance by leveraging the strengths of both? The study evaluates architectures in isolation but does not test combined models.

### Open Question 2
How can input feature scaling be automated or optimized to maximize geometric separability and sample efficiency in non-linear ICL tasks? The current finding relies on manual domain scaling rather than a generalized solution.

### Open Question 3
Do the observed architectural advantages, such as Mamba's proficiency in temporal dynamics, transfer to noisy, unstructured real-world data distributions? The framework focuses on synthetic tasks which may not fully capture real-world complexity.

## Limitations
- The synthetic function families, while carefully designed, may not fully capture real-world task distributions
- The Gaussian kernel experiment's shortcut learning issue was addressed through architectural modification rather than understanding why models initially took the shortcut path
- The curriculum learning benefits, while observed, lack theoretical grounding for why gradient starvation occurs specifically in high dimensions

## Confidence
- High Confidence: Transformer vs. Mamba performance differences on temporal vs. non-temporal tasks; input scaling effects on nonlinear separability
- Medium Confidence: Curriculum learning acceleration claims; FlashAttention sensitivity in low-data regimes
- Low Confidence: The exact mechanism of Hyena's higher early variance; generalizability beyond the specific synthetic distributions tested

## Next Checks
1. Test whether the curriculum learning benefits transfer to real-world datasets (e.g., time series forecasting or function approximation benchmarks) beyond synthetic functions
2. Investigate whether models trained on Gaussian kernels with shortcut solutions can be probed to reveal what internal mechanisms they're using, rather than just modifying the architecture
3. Extend the architectural analysis to include newer architectures like RWKV or other attention alternatives to validate whether the observed patterns hold across the broader architectural landscape