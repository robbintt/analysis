---
ver: rpa2
title: Reinforcement Learning From State and Temporal Differences
arxiv_id: '2512.08855'
source_url: https://arxiv.org/abs/2512.08855
tags:
- state
- policy
- function
- states
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the problem of policy degradation in temporal\
  \ difference (TD) learning when using function approximation. The core issue is\
  \ that TD(\u03BB) minimizes error in state values, but for policy optimization,\
  \ the relative ordering of states is critical."
---

# Reinforcement Learning From State and Temporal Differences

## Quick Facts
- arXiv ID: 2512.08855
- Source URL: https://arxiv.org/abs/2512.08855
- Authors: Lex Weaver; Jonathan Baxter
- Reference count: 24
- Primary result: STD(λ) prevents policy degradation in function approximation by learning relative state values between siblings, converging to optimal policy while TD(λ) can converge to sub-optimal policies even from optimal starting parameters

## Executive Summary
This paper addresses the fundamental problem of policy degradation in temporal difference learning when using function approximation. The core issue is that TD(λ) minimizes error in state values, but for policy optimization, the relative ordering of states is critical. Through examples and theoretical analysis, the authors demonstrate that TD(λ) can converge to sub-optimal policies even when starting from optimal ones. They introduce STD(λ) (State-Temporal Difference), a modified algorithm that operates on differences between sibling states in binary Markov decision processes, learning relative state values rather than absolute state values to directly improve policy.

## Method Summary
STD(λ) modifies the standard TD(λ) update rule to operate on differences between sibling states in Binary Markov Decision Processes (BMDPs). The algorithm maintains eligibility traces on the difference between feature vectors of sibling states φ(s) - φ(s'), and computes TD errors using value differences between these sibling states. This targets a weighted combination of true state differences that preserves correct sign under reasonable conditions. The key insight is that policy depends only on the sign of value differences between states, not the absolute values themselves, making relative state values more policy-relevant than absolute values.

## Key Results
1. STD(λ) converges to parameters that minimize weighted temporal differences between sibling states in BMDPs
2. For the two-state system, STD(λ) never produces worse policies than the starting policy, unlike TD(λ)
3. On the modified acrobot problem, STD(λ) learns the superior policy while TD(λ) learns the inferior converse policy

## Why This Works (Mechanism)

### Mechanism 1
Learning relative state values rather than absolute state values can prevent policy degradation in function approximation settings. STD(λ) operates on the difference between feature vectors of sibling states rather than individual state features, directly optimizing the pairwise ordering critical for policy selection. This works because the problem is formulated as a Binary Markov Decision Process where each state has at most two successor states.

### Mechanism 2
TD(λ)'s objective (minimizing squared value error weighted by stationary distribution) can be orthogonal to policy quality. TD(λ) minimizes Σ_π(s) [V̂(s) - V^π(s)]², but policy depends only on sign(V̂(s) - V̂(s')). A function approximator with high squared error but correct pairwise ordering outperforms one with low squared error but wrong ordering.

### Mechanism 3
STD(λ)'s weighting scheme targets a weighted combination of true state differences that preserves correct sign under reasonable conditions. STD(λ) targets α·V(s) + (1-α)·V(s') rather than (V(s) - V(s')), where α is the conditional transition probability. When α/(1-α) approximates the value ratio, or when the policy has no preference between siblings, the target has correct sign.

## Foundational Learning

- **Temporal Difference Learning TD(λ)**: Why needed: STD(λ) modifies TD(λ)'s update rule; understanding eligibility traces and the λ parameter is prerequisite. Quick check: Can you explain why TD(λ) with linear function approximation converges to the solution minimizing weighted squared error?

- **Markov Decision Processes and Policy Improvement**: Why needed: The paper assumes familiarity with value functions, optimal policies, and the relationship between state ordering and greedy policy selection. Quick check: In a two-state MDP with actions leading probabilistically to either state, what determines the optimal one-step greedy policy?

- **Linear Function Approximation in RL**: Why needed: All theoretical results assume V̂(s) = w·φ(s); the geometry of the approximation directly determines when policy degradation occurs. Quick check: Given feature vectors φ(A) and φ(B), what constraint on weight vector w ensures V̂(A) > V̂(B)?

## Architecture Onboarding

- Component map: BMDP environment -> Linear approximator V̂(s) = w·φ(s) -> STD(λ) update core -> Policy extractor

- Critical path:
  1. At each transition s → s', identify sibling state s_alt that could have been reached
  2. Compute feature difference: Δφ = φ(s') - φ(s_alt)
  3. Compute temporal difference including value difference term
  4. Update eligibility trace: e ← λγe + Δφ
  5. Update weights: w ← w + α·δ·e
  6. Extract policy via argmax over sibling values

- Design tradeoffs:
  - STD(λ) vs TD(λ): STD optimizes policy-relevant differences but requires sibling state knowledge; TD is more general but may degrade policy
  - STD(λ) vs DT(λ) (differential training): STD uses correct distribution over state pairs (only observed siblings) but approximated error function; DT uses wrong distribution (all pairs weighted by stationary probability) but true error function
  - The paper notes for physical systems with O(|S|) sibling pairs but O(|S|²) total pairs, "nearly all the state-pairs considered in DT(λ) are misleading"

- Failure signatures:
  - Policy performance declining while approximation error decreases (TD(λ) on backgammon)
  - Convergence to parameter region implementing sub-optimal policy despite starting near optimal
  - For STD(λ): inability to identify sibling states, or observing policy that violates equation (10) condition

- First 3 experiments:
  1. Implement the paper's two-state system (Table 1 transition matrix, Figure 1 rewards), run both TD(λ) and STD(λ) from w₀ = -1.5. Verify TD converges to w ≈ 0.33 (sub-optimal) while STD converges to w ≈ -0.5 (optimal).
  2. Construct a three-state BMDP where STD(λ)'s weighted target should have correct sign. Verify by checking that [α/(1-α)] ≤ [V*(s)/V*(s')] holds for all sibling pairs under the observing policy.
  3. Implement the modified acrobot with the hand-coded policy from equation (12) as the training signal. Compare TD(λ) and STD(λ) convergence using the feature function from equation (13). Confirm TD learns the inferior converse policy (positive w) while STD learns the superior policy (negative w).

## Open Questions the Paper Calls Out

1. Can STD(λ) be extended to general Markov Decision Processes where states may have more than two successor states?
2. Is it possible to minimize the true error function (equation 9) directly while observing only a single trajectory, or is the weighted approximation (equation 8) unavoidable?
3. How does STD(λ) perform on complex, high-dimensional domains like backgammon compared to TD(λ)?

## Limitations

- Theoretical analysis is limited to binary MDPs where sibling states can be identified
- The proof of monotonic improvement for two-state systems does not generalize to multi-state settings
- The comparison with differential training lacks empirical validation

## Confidence

- High confidence: The two-state examples clearly demonstrate TD(λ) policy degradation and STD(λ) ability to maintain optimal policy from optimal starting parameters
- Medium confidence: The theoretical convergence results for STD(λ) in BMDPs, as the proofs appear sound but rely on restrictive assumptions about state structure
- Low confidence: The practical advantages over differential training and the scalability claims for general MDPs, as these are argued but not empirically validated

## Next Checks

1. Implement STD(λ) on a 3-4 state BMDP with known optimal policy, verify that starting from optimal parameters STD(λ) maintains or improves policy while TD(λ) degrades
2. Implement DT(λ) alongside STD(λ) on the acrobot task, measure convergence speed and final policy performance to empirically validate the claimed advantage of sibling-state weighting
3. Apply STD(λ) principles to a non-BMDP by approximating sibling relationships (e.g., using k-nearest neighbors in feature space), evaluate whether policy degradation is reduced compared to TD(λ)