---
ver: rpa2
title: 'Beyond a Million Tokens: Benchmarking and Enhancing Long-Term Memory in LLMs'
arxiv_id: '2510.27246'
source_url: https://arxiv.org/abs/2510.27246
tags:
- bullet
- question
- user
- conversation
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BEAM, a new benchmark for evaluating long-term
  memory in large language models (LLMs) through automatically generated coherent
  conversations up to 10M tokens, accompanied by 2000 validated probing questions
  targeting ten memory abilities including contradiction resolution and event ordering.
  To enhance LLM performance, the authors propose LIGHT, a cognitive-inspired framework
  that combines episodic memory (conversation retrieval), working memory (recent turns),
  and a scratchpad (salient facts accumulation).
---

# Beyond a Million Tokens: Benchmarking and Enhancing Long-Term Memory in LLMs

## Quick Facts
- **arXiv ID**: 2510.27246
- **Source URL**: https://arxiv.org/abs/2510.27246
- **Reference count**: 40
- **Primary result**: Introduces BEAM benchmark for 10M-token conversations and LIGHT framework improving LLM long-term memory by 3.5%–12.69% over baselines.

## Executive Summary
This paper addresses the challenge of long-term memory in LLMs by introducing BEAM, a benchmark for evaluating memory across conversations up to 10M tokens, and LIGHT, a cognitive-inspired framework that combines episodic memory, working memory, and a scratchpad. Experiments show that even models with 1M-token context windows struggle with long conversations, while LIGHT significantly improves performance across diverse memory abilities. The framework demonstrates robustness across architectures and scales effectively to extreme context lengths.

## Method Summary
The authors propose LIGHT, a framework that equips LLMs with three complementary memory systems: episodic memory (retrieving relevant conversation segments via dense retrieval), working memory (recent dialogue turns), and a scratchpad (accumulating and compressing salient facts). The scratchpad is updated iteratively using GPT-4.1-nano to compress key-value pairs extracted by Qwen2.5-32B-AWQ when exceeding 30K tokens. At inference, the system aggregates retrieved segments, recent turns, and filtered scratchpad chunks to condition the LLM for answering probing questions.

## Key Results
- LLMs with 1M-token context windows struggle significantly as conversations lengthen beyond 100K tokens.
- LIGHT improves performance by 3.5%–12.69% over baselines depending on the backbone model.
- Scratchpad and noise filtering components show increasing importance at longer contexts, with ablation studies confirming their contribution.
- Performance gains are consistent across diverse architectures and extreme context lengths up to 10M tokens.

## Why This Works (Mechanism)

### Mechanism 1: Semantic Consolidation via Scratchpad
- **Claim:** Explicitly accumulating and compressing salient facts into a "scratchpad" appears to mitigate context dilution, potentially improving Information Extraction and Knowledge Update tasks compared to raw retrieval.
- **Mechanism:** An LLM extracts key entities and facts from dialogue turns. These are iteratively merged and compressed (e.g., from 30K to 15K tokens) to maintain a persistent semantic state, functioning as a cognitive "notebook" separate from the raw conversation history.
- **Core assumption:** Assumes that a compressed summary of facts retains sufficient nuance for multi-hop reasoning and that the compression model (GPT-4.1-nano in this case) reliably prioritizes critical information.
- **Evidence anchors:**
  - [section] Section 3.2 describes the scratchpad construction: "The resulting 'scratchpad' is iteratively merged... once content exceeds a 30K-token threshold... it is compressed into a 15K-token summary."
  - [section] Appendix F demonstrates that removing the scratchpad leads to failures in recalling specific tool versions and updated deadlines, while the full model succeeds.
  - [corpus] Corpus evidence is weak or missing for this specific compression mechanism; related work like *InfiniteHiP* focuses on extending raw context rather than semantic consolidation.
- **Break condition:** Performance may degrade if the compression step strips away subtle cues required for "Contradiction Resolution" or if the extraction model hallucinates facts during the accumulation phase.

### Mechanism 2: Dynamic Context Integration
- **Claim:** Combining recent turns (Working Memory), retrieved segments (Episodic Memory), and filtered notes (Scratchpad) conditions the LLM to handle different temporal horizons more effectively than single-source context.
- **Mechanism:** At inference, the system concatenates three distinct inputs: the last $z$ turns, top-$k$ retrieved dialogue segments, and query-relevant scratchpad chunks. This mimics human cognitive recall by separating immediate awareness from long-term retrieval.
- **Core assumption:** Assumes the LLM can effectively weigh and resolve potential conflicts between these three information sources without getting confused by redundancy.
- **Evidence anchors:**
  - [abstract] States the framework "equips LLMs with three complementary memory systems: a long-term episodic memory, a short-term working memory, and a scratchpad."
  - [section] Figure 2 illustrates the aggregation of these three streams into the final LLM prompt.
  - [corpus] *EverMemBench* corroborates the difficulty of multi-party, long-term memory, though it focuses on interactive evaluation rather than this specific architectural integration.
- **Break condition:** The mechanism likely fails if the retrieval step fetches irrelevant noise that contradicts the scratchpad, or if the "Working Memory" window ($z$) is too small to capture immediate conversational dependencies.

### Mechanism 3: Noise-Filtered Retrieval
- **Claim:** Filtering retrieved information and scratchpad chunks based on query relevance is correlated with performance gains, particularly as context length scales to 10M tokens.
- **Mechanism:** Before passing the scratchpad to the LLM, the system chunks it and labels each chunk as "relevant" or "irrelevant" relative to the current query. This pruning reduces the token burden and distractors.
- **Core assumption:** Assumes that a binary classifier (Qwen2.5-32B-AWQ) can accurately determine relevance for complex, multi-hop questions.
- **Evidence anchors:**
  - [section] Section 3.2 details "Filtering Scratchpad": "Only the chunks judged relevant are retained, producing a condensed representation."
  - [section] Table 8 (Ablation) shows that removing noise filtering causes a performance drop (e.g., -8.3% at 10M), indicating its increasing importance at scale.
  - [corpus] *Enhancing Long-term RAG Chatbots* supports the general principle of filtering/importance scoring to combat memory load degradation.
- **Break condition:** If the filter is too aggressive, it may prune necessary context for complex questions requiring indirect evidence (e.g., "Abstention" tasks might be harder if evidence is accidentally filtered out).

## Foundational Learning

- **Concept: Dense Retrieval (RAG)**
  - **Why needed here:** The "Episodic Memory" component relies on embedding dialogue turns and retrieving them via vector similarity. Understanding how embedding models (e.g., BAAI/bge-small-en-v1.5) represent conversational context is critical.
  - **Quick check question:** How does the system handle retrieval when a user asks a follow-up question that uses different terminology than the original dialogue turn?

- **Concept: Prompt Chaining & Context Management**
  - **Why needed here:** The framework relies on an "Update" prompt to maintain the scratchpad and a "Filter" prompt to clean it. These are sequential LLM calls, not a single inference step.
  - **Quick check question:** What is the latency implication of running an LLM extraction step after *every* user-assistant turn to update the scratchpad?

- **Concept: Cognitive Memory Models**
  - **Why needed here:** The architecture is explicitly inspired by human cognition (Episodic vs. Semantic vs. Working). Distinguishing between "what happened" (episodic) and "what is true" (scratchpad/semantic) is central to the design.
  - **Quick check question:** In this architecture, where would a user's preference (e.g., "Always respond in JSON") be stored: Episodic Memory or the Scratchpad?

## Architecture Onboarding

- **Component map:** User Query -> Episodic Memory (Retrieve top-k segments) -> Working Memory (Last z pairs) -> Scratchpad (Filter relevant chunks) -> Aggregate Context -> Main LLM

- **Critical path:** The **Scratchpad Update Loop**. If the extraction model fails to identify a "Knowledge Update" (e.g., a user correcting a previous fact), the scratchpad will retain stale data, propagating errors to all future queries.

- **Design tradeoffs:**
  - **Token Budget vs. Granularity:** The paper sets a 15K token limit for the compressed scratchpad. This trades off detailed history for cost/latency, potentially losing information needed for "Event Ordering" tasks.
  - **Retrieval Budget ($k$):** Table 9 shows $k=15$ is optimal. Increasing $k$ to 20 introduces noise and degrades performance.

- **Failure signatures:**
  - **Stale Facts:** The model answers with outdated information (Scratchpad update missed a correction).
  - **Context Confusion:** The model hallucinates by combining a retrieved episodic turn with a scratchpad note that contradict each other.
  - **Recency Bias:** The model ignores the scratchpad entirely because the Working Memory (recent turns) is too large or dominant.

- **First 3 experiments:**
  1. **Baseline Validation:** Reproduce the "Vanilla" vs. "LIGHT" result on a 100K conversation to ensure the scratchpad is actually being utilized (check log probs for scratchpad usage).
  2. **Retrieval Ablation:** Run the system with $k=0$ (Episodic off) and Scratchpad off to isolate which component drives the "Information Extraction" metric.
  3. **Compression Sensitivity:** Vary the compression threshold (e.g., trigger compression at 10k vs 30k tokens) to measure the trade-off between answer quality and scratchpad freshness.

## Open Questions the Paper Calls Out

- **Question:** How does a natively long-context model with a 10M token window (e.g., Llama-4-Scout) perform on the BEAM benchmark compared to the retrieval-augmented LIGHT framework?
  - **Basis in paper:** [explicit] The authors explicitly state in the experimental setup that "Among available models, only Llama-4-Scout supports 10M-token context windows; however, due to its extreme computational requirements, we were unable to include it in our experiments."
  - **Why unresolved:** The paper benchmarks models up to 1M tokens natively or uses retrieval to handle 10M tokens. It does not evaluate whether the expensive but natively supported 10M context windows offer better reasoning capabilities than the modular LIGHT approach.
  - **What evidence would resolve it:** Evaluating the BEAM benchmark using Llama-4-Scout (or similar models) with the full 10M context provided as input, without external retrieval mechanisms, and comparing the results against the LIGHT baselines.

- **Question:** What specific architectural or reasoning enhancements are required to improve performance on contradiction resolution, which consistently scored lowest across all tested models?
  - **Basis in paper:** [explicit] In the analysis of results, the authors note that "all methods—including ours—perform strongest in abstention and weakest in contradiction resolution, indicating that contradiction detection remains a challenging open problem."
  - **Why unresolved:** While the LIGHT framework improves average performance, the absolute scores for contradiction resolution remain very low (often near 0.0). The current mechanisms (retrieval + scratchpad) are insufficient for resolving global inconsistencies across 10M tokens.
  - **What evidence would resolve it:** Developing new modules specifically designed for consistency checking (e.g., a "contradiction detector" agent) or modifying the scratchpad accumulation logic to explicitly track and flag conflicting assertions, followed by empirical testing on the BEAM contradiction subset.

- **Question:** To what extent does the synthetic nature of the BEAM conversations influence the generalizability of the findings to real-world human-AI interactions?
  - **Basis in paper:** [inferred] The benchmark is constructed using a framework that automatically generates "coherent, and topically diverse conversations" using LLMs (Section 2.2). While human evaluation validates flow and realism, the generated dialogue is inherently "cleaner" and more structurally coherent than naturally occurring human conversations, which often contain more noise, interruptions, and incoherence.
  - **Why unresolved:** Models optimized for the logical, narrative-driven structure of synthetic BEAM conversations might struggle with the chaotic nature of real user data, or conversely, the lack of natural noise might hide brittleness in the models.
  - **What evidence would resolve it:** Constructing a "Real-BEAM" dataset comprising actual long-term human chat logs (appropriately anonymized) and comparing model performance between the synthetic and real datasets.

- **Question:** How sensitive is the LIGHT framework's performance to the specific choice of models used for scratchpad summarization and noise filtering?
  - **Basis in paper:** [inferred] The framework relies on specific model choices for sub-tasks: Qwen2.5-32B for noise filtering and GPT-4.1-nano for scratchpad summarization (Section 3.2). The paper does not ablate these specific choices to determine if the framework degrades significantly with smaller, cheaper, or less capable sub-models.
  - **Why unresolved:** If the scratchpad summarization requires a frontier model (like GPT-4.1-nano) to maintain utility, the system may be prohibitively expensive or slow for practical deployment. If a smaller model suffices, it suggests a more robust scalability.
  - **What evidence would resolve it:** An ablation study replacing the summarization and filtering models with smaller, open-source alternatives (e.g., Llama-3-8B) and measuring the resulting impact on the final answer accuracy.

## Limitations
- **Scalability bottlenecks**: The LIGHT framework requires sequential LLM calls for scratchpad extraction, compression, and filtering, which could make it computationally prohibitive at extreme scales (e.g., 10M tokens with frequent updates). The paper does not report per-turn latency or throughput measurements.
- **Dependence on proprietary models**: The system relies on GPT-4.1-nano for scratchpad compression and Qwen2.5-32B-AWQ for key-value extraction and filtering—both are non-public or API-only models, limiting reproducibility and cost transparency.
- **Evaluation subjectivity**: The nugget-based scoring uses an LLM judge, which may introduce bias or inconsistency, especially for complex abilities like "Contradiction Resolution" where human validation is sparse.

## Confidence
- **High confidence**: The core empirical finding that vanilla LLMs struggle with 10M-token contexts and that LIGHT improves performance by 3.5%–12.69% is well-supported by ablation studies and cross-architecture experiments.
- **Medium confidence**: The mechanism explanations (scratchpad consolidation, noise filtering) are plausible but rely on assumptions about model behavior that are not fully validated with ablation at scale or alternative compression strategies.
- **Low confidence**: Claims about cognitive plausibility (mimicking human memory systems) are illustrative but not empirically grounded in cognitive science or human behavioral studies.

## Next Checks
1. **Latency and cost profiling**: Measure end-to-end inference time and token cost for LIGHT at 1M, 5M, and 10M tokens, including all auxiliary LLM calls, to assess real-world feasibility.
2. **Alternative compression baselines**: Replace GPT-4.1-nano with a smaller, open-source model (e.g., LLaMA-3.1 8B) for scratchpad compression and compare performance to isolate the impact of model size versus architecture.
3. **Real-world corpus validation**: Apply LIGHT to a subset of long, real-world conversations (e.g., from customer support logs or multi-session technical support) and compare performance against BEAM to test generalizability.