---
ver: rpa2
title: 'MOGNET: A Mux-residual quantized Network leveraging Online-Generated weights'
arxiv_id: '2501.09531'
source_url: https://arxiv.org/abs/2501.09531
tags:
- mognet
- weights
- convolution
- quantized
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MOGNET introduces a compact CNN architecture for resource-limited
  hardware using quantized weights and activations. The core innovation combines a
  MUX-based residual block for efficient skip connections with a convolution factorization
  leveraging online-generated weights via Cellular Automata.
---

# MOGNET: A Mux-residual quantized Network leveraging Online-Generated weights

## Quick Facts
- arXiv ID: 2501.09531
- Source URL: https://arxiv.org/abs/2501.09531
- Reference count: 28
- Primary result: MOGNET achieves up to 1% higher accuracy than state-of-the-art compression methods while using less than 2Mb memory and 3-bit activations on CIFAR-10/100

## Executive Summary
MOGNET introduces a compact CNN architecture designed for resource-limited hardware, achieving state-of-the-art accuracy in the tiny model size regime. The core innovations combine a MUX-based residual block for efficient skip connections with convolution factorization leveraging online-generated weights via Cellular Automata. The model employs a novel balanced ternary quantization training method to optimize ternary weight distributions, enabling deployment with integer-only MAC operations while maintaining competitive accuracy on CIFAR-10 and CIFAR-100.

## Method Summary
MOGNET combines three key innovations: (1) a MUX Residual Block that replaces standard addition with a hardware-efficient multiplexer mechanism controlled by a thresholded global average pooling signal, (2) a CFLOG block that factorizes convolutions and replaces the final expansion layer weights with online-generated patterns from Wolfram's Rule 30 Cellular Automata, and (3) Balanced Ternary Quantization that dynamically adjusts quantization thresholds based on weight distribution tertiles. The architecture is trained in two stages using Adam optimizer with exponential learning rate decay, first with full-precision activations then with quantized activations, achieving sub-2Mb models with 3-bit activations on CIFAR datasets.

## Key Results
- Achieves up to 1% higher accuracy than state-of-the-art compression methods in the tiny model size regime
- Maintains accuracy with less than 2Mb memory footprint and 3-bit activations
- Demonstrates optimal performance specifically in the tiny model size regime, with accuracy gains diminishing at larger model sizes

## Why This Works (Mechanism)

### Mechanism 1
The MUX Residual Block replaces standard residual addition with a multiplexer mechanism that uses a thresholded global average pooling signal to select between transformed output and rescaled skip connection. This prevents bit-width expansion that occurs when adding two k-bit activations (requiring k+1 bits) while preserving feature reuse through conditional path selection rather than summation.

### Mechanism 2
Convolution factorization combined with Cellular Automata-generated weights reduces parameter storage by replacing learned expansion weights with procedurally generated patterns. The factorized block splits convolution into Pointwise → Group-wise → Pointwise, with the final expansion layer weights generated online using Wolfram's Rule 30 CA, eliminating memory overhead while relying on CA complexity for effective random projections.

### Mechanism 3
Dynamic step-size adjustment in Balanced Ternary Quantization maintains accuracy by aligning quantization thresholds with weight distribution statistics. BTQ calculates the step size s at each epoch based on the sum of absolute values of weight distribution tertiles, forcing the quantization grid to adapt to the histogram and keeping values balanced across -1, 0, +1.

## Foundational Learning

- **Straight-Through Estimator (STE):** Required for quantized activations and weights since quantization is a step function with zero derivatives. STE approximates gradients by copying them from output to input, enabling backpropagation through discrete logic.
- **Cellular Automata (Rule 30):** Generates chaotic, aperiodic patterns from simple rules, providing complex pseudo-random patterns for weight generation without storage. The aperiodic nature is critical for approximating random projections versus repeating patterns.
- **Channel-wise Attention (Squeeze-and-Excitation):** The MUX block implements low-cost attention. Understanding standard attention (GAP → FC → Sigmoid) helps contrast how MOGNET's TGAP → Binarize achieves similar recalibration with 1-bit logic.

## Architecture Onboarding

- **Component map:** Input → 1x1 Conv (Compression) → Grouped Conv (Processing) → 1x1 Conv (Expansion, CA-Generated) → Batch Norm → QReLU → Bitshift → MUX (TGAP-controlled) → Output
- **Critical path:** The BTQ Training Loop with specific initialization and dynamic step-size update (s = |q1| + |q2|) defined in Algorithm 1. Incorrect implementation (e.g., per-batch instead of per-epoch updates) causes ternary weights to fail convergence.
- **Design tradeoffs:** Memory vs. Depth optimization for tiny regime (<2Mb) with diminishing accuracy gains at larger sizes; Bit-width vs. Accuracy with 2-3 bits as operational floor for this topology.
- **Failure signatures:** "Dead" MUX if TGAP threshold is incorrect (constant control signal); Bitshift Overflow if rescaling logic not enforced during inference.
- **First 3 experiments:**
  1. Replace CA-generated weights with standard random initialized weights to quantify accuracy drop from online generation constraint
  2. Compare MUX-based residual path versus standard Add-based residual path with bit-shifting/clipping
  3. Compare standard fixed-step ternary quantization versus proposed Balanced Ternary Quantization to isolate dynamic histogram equalization impact

## Open Questions the Paper Calls Out

- How can MOGNET architecture be adapted to maintain superior accuracy when scaling to model sizes larger than 2Mb?
- Is Wolfram's rule 30 optimal for Cellular Automata weight generation, or can accuracy be improved using different update rules?
- What are the actual energy and latency trade-offs of the proposed MUX-residual mechanism and online weight generation when deployed on physical hardware?

## Limitations
- Exact network depth and layer configuration not specified, making precise reproduction challenging
- Cellular Automata weight generation lacks specification of seed initialization method
- Limited comparison to standard ResNet architectures beyond tiny model regime

## Confidence

| Claim | Confidence |
|-------|------------|
| 1% higher accuracy than state-of-the-art in tiny model regime | High |
| Effectiveness of MUX-based residual connections | Medium |
| Effectiveness of CA-generated weights | Medium |
| Hardware efficiency claims without actual measurements | Low |

## Next Checks

1. Implement ablation study comparing CA-generated weights versus learned weights in expansion layer to quantify online generation contribution
2. Validate MUX residual block against standard addition-based residuals in low-bitwidth settings
3. Test BTQ quantization with static thresholds versus proposed dynamic tertile-based approach to isolate adaptive step size impact