---
ver: rpa2
title: On the Spectral Flattening of Quantized Embeddings
arxiv_id: '2602.00969'
source_url: https://arxiv.org/abs/2602.00969
tags:
- spectral
- quantization
- matrix
- noise
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper formalizes how uniform quantization of large language
  model embeddings degrades spectral structure by truncating the heavy-tailed tail
  of singular values, leading to a provable increase in stable rank and subsequent
  representational collapse. The authors derive bounds showing that quantization noise
  disproportionately affects low-magnitude singular values due to the power-law decay
  inherited from Zipfian token statistics, establishing a clear link between quantization
  step size, spectral fidelity, and representational capacity.
---

# On the Spectral Flattening of Quantized Embeddings

## Quick Facts
- arXiv ID: 2602.00969
- Source URL: https://arxiv.org/abs/2602.00969
- Reference count: 40
- One-line primary result: Uniform quantization increases stable rank by disproportionately truncating low-magnitude singular values, leading to representational collapse.

## Executive Summary
This paper establishes that uniform quantization of LLM embeddings degrades spectral structure by truncating the heavy-tailed singular value distribution inherited from Zipfian token statistics. The authors prove that quantization noise introduces a noise floor that disproportionately affects low-magnitude singular values, increasing stable rank and potentially causing representational collapse. Through theoretical analysis and empirical validation across multiple architectures, the work demonstrates that spectral flattening is a geometric degradation of the optimization landscape, establishing spectral fidelity as a necessary condition for stable low-bit optimization.

## Method Summary
The study simulates block-wise uniform quantization (NVFP4/E2M1 style) on LLM weight matrices, primarily focusing on MLP layers in GPT-2 and TinyLlama architectures. The method involves extracting BF16 weight matrices from pre-trained or training models, applying simulated quantization, computing SVD to analyze spectral properties, and measuring stable rank changes. Training experiments use W4A4G4 quantization with AdamW optimizer, specific learning rates, and gradient clipping. The analysis tracks spectral evolution over training steps to distinguish early flattening from later collapse.

## Key Results
- Quantization noise disproportionately truncates tail singular values, increasing stable rank from 2.52 to 2.71 in MLP layers
- Relative quantization error scales linearly with 1/σ_k for tail components (R² > 0.9)
- Temporal analysis shows spectral flattening occurs early in training, potentially preceding representational collapse
- The effect is consistent across diverse architectures including GPT-2, TinyLlama, Phi-1.5, Pythia, DeepSeek-Coder, and Qwen2.5

## Why This Works (Mechanism)

### Mechanism 1: Zipfian Token Statistics Impose Power-Law Spectral Decay
Token probabilities follow Zipf's law (p_k ∝ k^(-α)), and under quasi-orthogonality of embedding vectors, the covariance matrix eigenvalues inherit this power-law decay. This translates to singular values σ_k(X) ∝ k^(-α/2) for dominant components, establishing the spectral foundation that quantization subsequently degrades.

### Mechanism 2: Quantization Noise Disproportionately Truncates Tail Singular Values
Uniform quantization introduces bounded noise with spectral norm ||E||_2. By Weyl's inequality, the absolute error |σ̃_k - σ_k| ≤ ||E||_2 is uniform, but since tail singular values σ_k are small, the relative error ϵ_k = |σ̃_k - σ_k|/σ_k ∝ 1/σ_k grows for tail components, effectively truncating them to the noise floor.

### Mechanism 3: Spectral Flattening Strictly Increases Stable Rank
The power-law structure creates distinct head (1≤k≤r) and tail (r<k) regimes. Quantization adds energy proportionally more to the tail due to relative error asymmetry. Since stable rank Sr(A) = ||A||²_F / ||A||²_2 is the reciprocal of energy concentration F_A(1), and quantization inflates tail energy (F_Ã(1) < F_A(1)), the stable rank strictly increases: Sr(Ã) > Sr(A).

## Foundational Learning

- Concept: **Singular Value Decomposition (SVD) and spectral structure**
  - Why needed here: The entire analysis operates on singular value spectra; understanding how σ_k represents directional energy is essential for interpreting "tail truncation" and "flattening."
  - Quick check question: If a matrix has σ_1=10, σ_2=1, σ_3=0.1, what happens to the relative error of σ_3 versus σ_1 when additive noise of magnitude 0.05 is introduced?

- Concept: **Stable rank and effective dimensionality**
  - Why needed here: The paper uses stable rank as the primary metric for spectral flattening; distinguishing it from algebraic rank clarifies why "higher stable rank" here is pathological (noise-driven) rather than beneficial (isotropy-driven).
  - Quick check question: For a rank-100 matrix with σ_1=100 and σ_100=0.01, what is the approximate stable rank? What if all σ_k were equal?

- Concept: **Concentration inequalities for random matrices**
  - Why needed here: The proof that quantization noise has bounded spectral norm relies on matrix Bernstein bounds; understanding this justifies treating quantization error as a controlled perturbation.
  - Quick check question: Why does the spectral norm of a sum of independent random matrices concentrate, rather than growing linearly with matrix dimensions?

## Architecture Onboarding

- Component map: Embedding matrices X -> MLP linear layers -> Output Y=XW
- Critical path: 1) Extract embedding/weight matrices from target layer (BF16 reference) 2) Apply simulated block-wise uniform quantization to obtain quantized counterpart 3) Compute SVD of both matrices, extract singular value spectra 4) Calculate stable rank ratio: Sr(Ã) / Sr(A) 5) Plot spectral decay (log-log) and relative error per component ϵ_k vs. 1/σ_k 6) Monitor across training steps to distinguish early flattening from late collapse
- Design tradeoffs:
  - Block size in quantization: Smaller blocks preserve local dynamic range but may break spectral coherence; larger blocks amplify outlier effects
  - Simulation vs. hardware: BF16 simulation of NVFP4 is accurate for spectral analysis but may miss hardware-specific rounding behaviors
  - Layer selection: MLP layers show clearest degradation; attention projections exhibit more variance due to softmax interactions
- Failure signatures:
  - Stable rank fails to increase post-quantization: Check if original spectrum is already flat (pre-trained models may differ)
  - Linear relationship ϵ_k vs. 1/σ_k not observed: Quantization may be non-uniform or error is biased; verify Assumption 4.2 empirically
  - Representational collapse occurs prematurely: Gradient scaling may be insufficient; check gradient clipping and learning rate relative to noise floor
- First 3 experiments:
  1. Spectral flattening verification: Take a pre-trained GPT-2-124M checkpoint, apply NVFP4 quantization to MLP weights, compute stable rank before/after. Expect Sr(Ã)/Sr(A) > 1.0 across most layers.
  2. Relative error linearity test: For each layer, plot ϵ_k against 1/σ_k. Fit linear regression; expect R² > 0.9 for deeper layers. This validates Theorem 4.5.
  3. Temporal spectral evolution: Train GPT-2-124M from scratch with W4A4G4 quantization. Record stable rank at steps 0, 10, 50, 100. Expect early increase (flattening) followed by potential decline (collapse) in later stages.

## Open Questions the Paper Calls Out

### Open Question 1
How do non-linear operations, specifically softmax and attention mechanisms, alter the spectral decay properties derived from Zipfian statistics under quantization? The theoretical framework focuses on linear layers, but non-linearities may saturate or distort this transmission, breaking the proven link between Zipfian statistics and the spectral tail.

### Open Question 2
Can non-uniform quantization schemes explicitly designed around power-law statistics preserve the spectral tail and prevent representational collapse? The paper demonstrates that uniform quantization fails because the noise floor truncates low-magnitude singular values, but it remains unverified if a tailored non-uniform scheme can maintain spectral fidelity.

### Open Question 3
Is spectral regularization a viable algorithmic intervention to counteract the increase in stable rank induced by quantization noise? The paper suggests employing spectral regularization to prevent rank collapse, but does not test if adding a loss term to penalize deviations from ideal power-law decay can successfully stabilize the optimization landscape.

## Limitations
- The quasi-orthogonality assumption for embedding vectors may not hold in practice due to semantic clustering
- Hardware-specific quantization effects (NVFP4) may introduce systematic biases not captured by the theoretical bounds
- Attention mechanisms, which constitute significant parameter volume, are not explicitly examined for spectral degradation patterns
- Sample size per architecture is limited, and temporal dynamics of spectral evolution are only partially characterized

## Confidence

**High Confidence:** The mathematical proofs establishing that quantization increases stable rank are rigorous and rely on well-established results from random matrix theory and concentration inequalities. The claim that quantization noise disproportionately affects tail singular values due to relative error asymmetry is strongly supported by both theory and empirical validation.

**Medium Confidence:** The connection between Zipfian token statistics and power-law spectral decay is plausible and supported by lemma proofs, but the quasi-orthogonality assumption is not extensively validated empirically. The claim that spectral flattening precipitates representational collapse during training is demonstrated across multiple architectures, but the causal mechanism linking stable rank increase to performance degradation requires further investigation.

**Low Confidence:** The assertion that spectral fidelity is a "necessary condition" for stable low-bit optimization is presented without comprehensive ablation studies across different quantization schemes, training durations, or architectural variations.

## Next Checks

1. **Quasi-orthogonality verification**: Extract embedding matrices from multiple pre-trained LLMs and compute pairwise cosine similarities between random token vectors. Calculate the proportion of pairs with cosine similarity below 1/√d. If >95% of pairs satisfy this threshold, it validates the core assumption underlying the power-law spectral decay derivation.

2. **Block size sensitivity analysis**: Repeat the spectral flattening experiment using different block sizes for the NVFP4 quantization (e.g., 16, 32, 64, 128 elements per block). Plot stable rank increase ratio Sr(Ã)/Sr(A) against block size to identify whether smaller blocks preserve spectral coherence or whether larger blocks amplify outlier effects.

3. **Temporal collapse threshold identification**: Extend the training experiment to monitor stable rank continuously across 500+ steps rather than just 0, 10, 50, 100. Record validation perplexity alongside stable rank to identify the precise step where spectral collapse correlates with performance degradation. Test whether early intervention at the flattening stage prevents subsequent collapse.