---
ver: rpa2
title: 'Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models
  with Diffusion Language Model Backbone'
arxiv_id: '2512.22615'
source_url: https://arxiv.org/abs/2512.22615
tags:
- arxiv
- diffusion
- action
- wang
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the limitations of autoregressive vision-language
  models (VLMs) in visual planning and robotic control by introducing diffusion-based
  VLMs and vision-language-action models (VLAs). The authors propose Dream-VL, a diffusion-based
  VLM built on the diffusion language model (dLLM) Dream 7B, and Dream-VLA, a diffusion-based
  VLA developed through continuous pretraining on open robotic datasets.
---

# Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone

## Quick Facts
- **arXiv ID**: 2512.22615
- **Source URL**: https://arxiv.org/abs/2512.22615
- **Reference count**: 28
- **Primary result**: Dream-VL & Dream-VLA achieve top-tier performance on both visual planning and robotic manipulation tasks, surpassing autoregressive baselines

## Executive Summary
This paper introduces Dream-VL and Dream-VLA, the first diffusion-based Vision-Language Models (VLMs) and Vision-Language-Action models (VLAs) built on the diffusion language model (dLLM) Dream 7B. By leveraging bidirectional attention and diffusion-based generation, these models demonstrate superior performance in visual planning and robotic control tasks compared to autoregressive baselines. The authors systematically evaluate their approach across multiple benchmarks, showing that diffusion-based models can achieve competitive performance on general visual understanding while excelling at planning and manipulation tasks.

## Method Summary
The authors develop Dream-VL through a three-stage training process: pretraining a projector on multimodal data, training on single-image data, and finally training on multi-image/video data using discrete diffusion loss. Dream-VLA extends this approach by pretraining on the Open-X-Embodiment dataset and finetuning with Flow Matching for continuous action control. The models utilize a Qwen2ViT vision encoder and implement action chunking with diffusion-based generation, enabling both high-level planning and low-level control capabilities.

## Key Results
- Dream-VL achieves competitive performance with top-tier autoregressive VLMs on general visual understanding tasks while showing superior performance in visual planning tasks such as ViPlan and LIBERO
- Dream-VLA establishes top-tier performance on both LIBERO and SimplerEnv benchmarks, surpassing leading models such as π0 and GR00T-N1
- Dream-VLA achieves 97.2% average success rate on LIBERO, 71.4% overall average on SimplerEnv-Bridge, and 60.5% overall average on SimplerEnv-Fractal

## Why This Works (Mechanism)
The diffusion backbone enables bidirectional attention for richer visual-text fusion, enhanced visual planning through text planning capabilities, and support for action chunking and parallel generation. The discrete diffusion approach provides more stable training dynamics compared to autoregressive methods, while the ability to generate tokens in parallel improves efficiency. The combination of high-level planning and low-level control in a unified framework allows for more coherent and effective robotic manipulation.

## Foundational Learning
- **Discrete Diffusion Training**: Uses noise schedules and reverse diffusion for stable multimodal learning; needed for training stability with large-scale multimodal data
- **Flow Matching**: Alternative to traditional diffusion for continuous action generation; enables smoother and more efficient control learning
- **Vision Encoder Integration**: Qwen2ViT provides visual features compatible with diffusion backbone; essential for bridging visual and language modalities
- **Action Chunking**: Groups continuous actions into manageable segments; enables parallel generation and improves control efficiency
- **LoRA Fine-tuning**: Parameter-efficient adaptation for downstream tasks; reduces computational cost while maintaining performance
- **Camera Pose Randomization**: Simulates different camera perspectives during training; critical for robust sim-to-real transfer

## Architecture Onboarding
- **Component Map**: Dream 7B dLLM -> Qwen2ViT Vision Encoder -> Diffusion Projector -> Action/Chunk Generator
- **Critical Path**: Vision encoder → diffusion projector → token generation → action chunking → control output
- **Design Tradeoffs**: Discrete diffusion provides stability but may limit fine-grained control; action chunking improves efficiency but requires careful segment design
- **Failure Signatures**: Training instability with multimodal data; sim-to-real transfer failures due to camera placement sensitivity
- **First Experiments**: 1) Verify projector pretraining on LCS-558K, 2) Test full model training on single-image data, 3) Evaluate action chunking performance on LIBERO

## Open Questions the Paper Calls Out
- Can joint mixture training on dLLM backbones simultaneously improve high-level semantic planning and low-level continuous action control?
- Can improved discrete action representations (like FAST) or continuous-space pretraining close the performance gap with continuous action spaces for dVLAs?
- How does Dream-VLA generalize in realistic environments when scaled to diverse real-world training data compared to autoregressive baselines?

## Limitations
- Exact composition of 12M multimodal training data not fully specified
- Action representation details for VLA component lack complete specification
- Camera pose randomization importance for sim2real transfer not thoroughly validated across different robotic setups
- Focus on open environments without clutter limits real-world applicability

## Confidence
- **High confidence**: Core architecture design, pretraining/finetuning procedures, and main benchmark results
- **Medium confidence**: Generalization claims to real robotics due to limited real-world testing scope
- **Medium confidence**: Ablation results showing diffusion advantages given complexity of isolating diffusion-specific effects

## Next Checks
1. Replicate the ViPlan benchmark results on a held-out subset to verify the claimed advantage in visual planning tasks
2. Test sim2real transfer performance with and without camera pose randomization on PiPER to quantify its impact
3. Conduct an ablation study comparing diffusion vs autoregressive models using identical training data and hyperparameters to isolate diffusion-specific benefits