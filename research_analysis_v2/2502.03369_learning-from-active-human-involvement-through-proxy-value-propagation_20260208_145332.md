---
ver: rpa2
title: Learning from Active Human Involvement through Proxy Value Propagation
arxiv_id: '2502.03369'
source_url: https://arxiv.org/abs/2502.03369
tags:
- human
- learning
- agent
- policy
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Proxy Value Propagation (PVP), a reward-free
  method for learning from active human involvement that integrates into existing
  value-based RL algorithms. The method labels high Q-values to human actions and
  low Q-values to intervened agent actions, then propagates these proxy values through
  TD-learning to unlabeled data.
---

# Learning from Active Human Involvement through Proxy Value Propagation

## Quick Facts
- arXiv ID: 2502.03369
- Source URL: https://arxiv.org/abs/2502.03369
- Authors: Zhenghao Peng; Wenjie Mo; Chenda Duan; Quanyi Li; Bolei Zhou
- Reference count: 40
- One-line primary result: PVP integrates into existing value-based RL algorithms to learn from active human interventions, achieving superior performance with less human effort than baselines.

## Executive Summary
This paper introduces Proxy Value Propagation (PVP), a reward-free method for learning from active human involvement in reinforcement learning. The method works by labeling human actions with high proxy values (+1) and rejected agent actions with low proxy values (-1), then propagating these values through temporal difference learning to unlabeled data. PVP integrates into existing value-based RL algorithms like TD3 and DQN without requiring reward functions. The method is evaluated across multiple environments including MetaDrive, CARLA, GTA V, and MiniGrid with different control devices, showing superior performance compared to baselines like HG-DAgger, IWR, and HACO while requiring less human intervention.

## Method Summary
PVP is a reward-free method that learns from active human interventions by propagating proxy values through existing value-based RL algorithms. The method maintains two buffers: a novice buffer storing agent transitions when no intervention occurs, and a human buffer storing intervened transitions containing both the agent's rejected action and the human's demonstrated action. During training, PVP minimizes a combined loss consisting of a Proxy Value Loss (targeting +1 for human actions and -1 for agent actions in intervened steps) and a standard TD loss for reward-free Bellman backups. The method uses balanced 1:1 sampling from both buffers to prevent catastrophic forgetting of sparse human demonstrations. PVP is compatible with TD3 for continuous control and DQN for discrete control, with deterministic policies to reduce user fatigue.

## Key Results
- PVP achieved 350 returns in 37K steps in MetaDrive tasks versus TD3's failure to achieve comparable results even after 300K steps
- User studies confirmed PVP's superior user experience compared to alternatives, with higher ratings on compliance, performance, and lower stress
- PVP requires less human intervention than baselines like HG-DAgger, IWR, and HACO while achieving better performance
- The method successfully handles both continuous control (driving) and discrete control (navigation) across multiple environments

## Why This Works (Mechanism)
PVP works by creating a proxy reward signal through value labeling rather than requiring explicit reward functions. When a human intervenes, the method labels the human's action with a high proxy value (+1) and the agent's rejected action with a low proxy value (-1). These proxy values are then propagated backward through the state-action space using temporal difference learning, effectively creating a value landscape that guides the agent toward human-preferred behaviors. The balanced buffer sampling ensures that the sparse human demonstrations don't get overwhelmed by the vast amount of agent exploration data, preventing catastrophic forgetting. By integrating directly into existing value-based RL algorithms, PVP avoids the need for reward engineering while still leveraging the power of value-based learning methods.

## Foundational Learning
- **Reward-free RL**: Learning without explicit reward signals by using proxy objectives; needed because human demonstrations may not come with clear reward functions, quick check: verify the TD loss doesn't use any reward term
- **Value propagation through TD-learning**: Using temporal difference updates to spread labeled values across the state-action space; needed to generalize human interventions to similar states, quick check: ensure Q-values converge to the ±1 targets for intervened actions
- **Catastrophic forgetting prevention**: Balanced sampling between demonstration and exploration data; needed because human demonstrations are sparse compared to agent exploration, quick check: monitor agent performance on early tasks over training
- **Proxy value labeling**: Assigning +1 to human actions and -1 to rejected agent actions; needed to create a simple, bounded target for the value function, quick check: verify Q-values for intervened actions converge to approximately ±1
- **Deterministic policy for human-in-the-loop**: Using deterministic rather than stochastic policies to reduce user fatigue; needed because stochastic policies would require humans to repeatedly correct the same behavior, quick check: confirm policy gradient updates use max Q rather than expectation

## Architecture Onboarding

**Component Map:**
Human Input -> State-Action-Next State -> Buffer (B_h/B_n) -> Balanced Sampler -> Q-network Update -> Policy Update -> Agent Action

**Critical Path:**
Human intervention occurs → Both agent and human actions stored in B_h → Balanced sampling pulls from B_h and B_n → Proxy Value Loss and TD Loss computed → Q-network updated → Policy improved → Agent generates new actions

**Design Tradeoffs:**
- **Bounded proxy values (±1)**: Simplifies learning and prevents Q-value explosion but may limit fine-grained distinctions between good and bad actions
- **Deterministic policies**: Reduces human fatigue by providing consistent responses but eliminates exploration benefits of stochastic policies
- **Balanced 1:1 buffer sampling**: Prevents forgetting of human demonstrations but may slow down learning from large amounts of exploration data

**Failure Signatures:**
- Agent fails to start moving despite demonstrations → catastrophic forgetting of initial maneuvers
- Q-values grow unbounded or loss becomes unstable → incorrect loss formulation or missing value constraints
- Agent intentionally performs poorly to trigger interventions → reward signal leaking into TD loss

**First Experiments:**
1. Implement basic PVP with TD3 in MetaDrive using keyboard intervention to verify the core mechanism works
2. Test catastrophic forgetting by running with imbalanced buffers (all agent data) to observe performance degradation
3. Validate value propagation by checking Q-values for intervened actions converge to ±1 targets

## Open Questions the Paper Calls Out
- **Open Question 1**: How can the PVP framework be adapted to effectively handle suboptimal or erroneous human demonstrations? The current proxy value mechanism treats all human actions as ground truth (+1) without filtering for human error.
- **Open Question 2**: Does integrating PVP with advanced reinforcement learning techniques, such as prioritized experience replay or bootstrapped exploration, significantly improve sample efficiency? The current implementation uses standard TD3 and DQN without these enhancements.
- **Open Question 3**: Can the demand for constant human attentiveness be reduced by switching to uncertainty-based passive involvement? The current framework requires active human monitoring and intervention throughout training.

## Limitations
- Evaluation relies heavily on simulated environments rather than real-world testing, limiting generalizability
- User study sample size is small (5 subjects per condition) and focuses on subjective experience
- Method requires active human intervention, which may be impractical for long-term deployment
- Ablation study only tests one alternative regularization method (CQL) without exploring other baselines

## Confidence
- **High Confidence**: The core PVP algorithmic contribution is clearly specified and reproducible
- **Medium Confidence**: Empirical performance claims are supported by results but limited by small user study sample size and simulation reliance
- **Low Confidence**: Claims about user experience superiority due to subjective metrics and limited sample size

## Next Checks
1. Implement PVP in a different continuous control domain (e.g., robotic manipulation) to test generalizability beyond driving scenarios
2. Conduct a larger-scale user study with at least 20 subjects per condition measuring both subjective experience and objective task completion metrics
3. Test PVP with alternative base algorithms (SAC, PPO) to verify compatibility and performance across different RL frameworks