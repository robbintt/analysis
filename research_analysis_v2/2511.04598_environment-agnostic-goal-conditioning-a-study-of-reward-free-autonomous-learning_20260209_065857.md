---
ver: rpa2
title: Environment Agnostic Goal-Conditioning, A Study of Reward-Free Autonomous Learning
arxiv_id: '2511.04598'
source_url: https://arxiv.org/abs/2511.04598
tags:
- goal
- goals
- learning
- success
- environment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work demonstrates that agents can learn to solve tasks autonomously
  and reward-free by transforming regular reinforcement learning environments into
  goal-conditioned ones. The proposed method uses goal selection to guide training
  without external rewards, achieving comparable performance to traditional reward-based
  RL methods.
---

# Environment Agnostic Goal-Conditioning, A Study of Reward-Free Autonomous Learning

## Quick Facts
- arXiv ID: 2511.04598
- Source URL: https://arxiv.org/abs/2511.04598
- Authors: Hampus Åström; Elin Anna Topp; Jacek Malec
- Reference count: 5
- Agents learn reward-free by transforming RL environments into goal-conditioned ones, achieving comparable performance to traditional reward-based methods.

## Executive Summary
This work demonstrates that agents can learn to solve tasks autonomously and reward-free by transforming regular reinforcement learning environments into goal-conditioned ones. The proposed method uses goal selection to guide training without external rewards, achieving comparable performance to traditional reward-based RL methods. Experiments across three environments show that while individual goal success rates fluctuate during training, the average goal success rate gradually improves and stabilizes. The approach is environment-agnostic and independent of the underlying off-policy learning algorithm, enabling flexible post-training task assignment. A modular wrapper for Stable-Baselines3 facilitates application to non-goal environments, supporting various goal evaluation and selection strategies.

## Method Summary
The method wraps non-goal reinforcement learning environments to enable reward-free, autonomous learning through goal-conditioning. A Stable-Baselines3 wrapper intercepts environment observations, appends self-selected goals, and evaluates success via modular distance/matching functions. Hindsight Experience Replay (HER) enables sample-efficient learning by reinterpreting achieved observations as valid training targets. Three goal selection strategies (uniform, novelty, intermediate difficulty) guide the learning process, with all strategies interleaving 10% uniform random selection to avoid local goal subsets. The approach uses off-policy algorithms (DQN for discrete, SAC confirmed working) and stores experiences in a HER buffer for relabeling and replay.

## Key Results
- Average goal success rate improves and stabilizes during training despite individual goal fluctuations
- The method achieves comparable performance to traditional reward-based RL methods
- Goal selection strategy impacts learning dynamics, with intermediate difficulty selection underperforming in stochastic environments
- The approach is environment-agnostic and independent of the underlying off-policy learning algorithm

## Why This Works (Mechanism)

### Mechanism 1: Goal-Conditioning Wrapper Transforms Reward Structure
The wrapper substitutes external rewards with internally generated goal-achievement signals by intercepting environment observations, pairing them with self-selected goals, and evaluating success via modular distance functions. This converts the problem from "maximize external reward" to "reach any observation state." The approach assumes achieved observations represent learnable sub-skills that generalize to unseen goals (borrowed from HER theory). This works well in deterministic environments but wastes samples on impossible goals in environments with large fractions of unreachable states.

### Mechanism 2: Hindsight Experience Replay Enables Sample-Efficient Goal Learning
HER allows learning from every episode by reinterpreting achieved observations as valid training targets. Each trajectory is stored in replay buffer, and during sampling, "future" goal reselection relabels episodes as successful demonstrations of reaching whatever state was actually achieved. The approach assumes the agent's policy can generalize across goal-conditioned policies without explicit external prioritization. This works effectively in deterministic environments but may reinforce suboptimal paths in stochastic environments since achieved states depend on transition stochasticity rather than policy quality.

### Mechanism 3: Goal Selection Strategy Shapes Exploration Curriculum
Different goal selection strategies produce different learning dynamics. Novelty selection prioritizes less-visited observation regions while intermediate difficulty selection targets cells with success rates near a target value. Both interleave 10% uniform random selection to avoid local goal subsets. The approach assumes curriculum-like goal progression improves over uniform sampling, which is partially supported by results. However, when optimal success rates for some goals match the target success rate (due to stochasticity), intermediate difficulty selection over-prioritizes already-optimized goals, degrading performance in stochastic environments.

## Foundational Learning

- **Goal-Conditioned Reinforcement Learning (GCRL)**: The entire method builds on conditioning policies on goal observations rather than maximizing scalar rewards. Quick check: Can you explain why a goal-conditioned policy π(a|s,g) differs fundamentally from a standard policy π(a|s) with shaped rewards?

- **Off-Policy Learning and Replay Buffers**: HER requires storing and re-sampling experiences with relabeled goals. Understanding why off-policy algorithms can reuse old data is essential for debugging training instability. Quick check: Why can DQN reuse experiences from a replay buffer while on-policy methods like PPO cannot?

- **Intrinsic Motivation / Autonomous Learning**: This work positions itself as reward-free autonomous learning. Understanding the broader framing helps contextualize why "selecting your own goals" is a meaningful research contribution. Quick check: What problem does intrinsic motivation solve that external reward shaping does not?

## Architecture Onboarding

- **Component map**: Environment Wrapper -> Goal Evaluator Module -> Goal Selector Module -> HER Buffer -> Base Agent
- **Critical path**: 1. Wrap environment → 2. Configure goal evaluator (match environment type) → 3. Select goal strategy → 4. Initialize HER-compatible off-policy agent → 5. Train with goal-termination enabled → 6. Post-training: inject task-specific goals for evaluation
- **Design tradeoffs**: Uniform selection is environment-agnostic but sample-inefficient if unreachable states dominate; curriculum methods can fail in stochastic environments. Point goals fail when goals should be under-specified (e.g., Mountain Car position regardless of velocity). Average success stabilizes while individual goals fluctuate; if specific-goal reliability is needed, current method is insufficient.
- **Failure signatures**: Stochastic environments + intermediate difficulty selection degrades performance (Frozen Lake). Continuous observation spaces with point goals terminate prematurely on velocity-matched observations that don't represent task completion. Individual goal success rates fluctuate wildly even in deterministic environments.
- **First 3 experiments**: 1. Train DQN with external rewards on Cliff Walker as baseline. 2. Apply uniform goal selection wrapper to Cliff Walker; compare training steps to reach 80% average goal success vs. baseline. 3. Run all three goal selection strategies on Frozen Lake; observe intermediate difficulty degradation and confirm novelty/uniform methods achieve comparable-to-baseline reward within 1M steps.

## Open Questions the Paper Calls Out

### Open Question 1
Can model-based methods improve training efficiency by decoupling transition model learning from goal-based evaluation? The authors state interest in whether model-based methods can improve training efficiency by decoupling the learning of a transition model from the goal-based evaluation. This remains unresolved as the current study relies on model-free off-policy algorithms without evaluating the impact of separate transition models.

### Open Question 2
Is the instability in individual goal success rates caused by policy forgetting or environmental stochasticity? Regarding fluctuating goal success rates, the paper asks if this is due to forgetting, the stochastic nature of the environment, or something else. Experiments showed performance volatility on individual goals in both stochastic and deterministic settings, but the root cause was not isolated.

### Open Question 3
Can goals be effectively selected within an embedding space for complex or partially observed environments? The conclusion notes interest in exploring if goals can be selected within an embedding space for complex and partially observed environments. The current method uses raw observations or simple grid overlays, which may not scale to high-dimensional or partial observation spaces.

## Limitations

- Intermediate difficulty goal selection fails in stochastic environments (Frozen Lake) due to goal selection bias, limiting the environment-agnostic claim
- Individual goal success rates fluctuate wildly during training, which may be unacceptable for applications requiring reliable specific-goal performance
- Point goals fail for tasks requiring under-specified objectives (e.g., Mountain Car position regardless of velocity)

## Confidence

- High confidence: The wrapper architecture works as described for deterministic discrete environments; average goal success rates improve as reported
- Medium confidence: The environment-agnostic claim holds for specific goal selection strategies but fails for others; individual goal instability is documented but not fully explained
- Low confidence: The claim that this method "enables flexible post-training task assignment" is partially supported but lacks evaluation of task generalization quality

## Next Checks

1. Test the wrapper with continuous action spaces (SAC) to verify the "environment-agnostic" claim extends beyond discrete DQN
2. Implement a pre-screening method to detect stochastic environments where intermediate difficulty selection will fail
3. Evaluate post-training task generalization by measuring performance on held-out goals not encountered during training