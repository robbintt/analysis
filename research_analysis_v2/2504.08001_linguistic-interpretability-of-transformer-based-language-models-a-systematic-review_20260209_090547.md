---
ver: rpa2
title: 'Linguistic Interpretability of Transformer-based Language Models: a systematic
  review'
arxiv_id: '2504.08001'
source_url: https://arxiv.org/abs/2504.08001
tags:
- language
- computational
- linguistics
- linguistic
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This systematic review surveys 160 research works investigating
  the linguistic interpretability of Transformer-based pre-trained language models
  (PLMs). It focuses on studies analyzing internal representations to uncover how
  these models encode linguistic knowledge across syntax, semantics, morphology, and
  discourse.
---

# Linguistic Interpretability of Transformer-based Language Models: a systematic review

## Quick Facts
- arXiv ID: 2504.08001
- Source URL: https://arxiv.org/abs/2504.08001
- Reference count: 40
- Key outcome: This systematic review surveys 160 research works investigating the linguistic interpretability of Transformer-based pre-trained language models (PLMs), revealing that while PLMs generally encode linguistic information well, particularly in middle layers for syntax and semantics, their understanding is model-specific and not always aligned with human linguistic theories.

## Executive Summary
This systematic review examines 160 research works analyzing how Transformer-based pre-trained language models encode linguistic knowledge across syntax, semantics, morphology, and discourse. The review finds that linguistic information is generally well-encoded in PLMs, with middle layers showing particular strength in syntactic and semantic representations. However, systematic failures occur, particularly in low-frequency verb forms and negation processing. The review highlights methodological challenges in distinguishing correlation from causation in model representations and notes that most studies focus on English, limiting generalizability.

## Method Summary
The review employs PRISMA methodology to systematically identify and analyze relevant literature. Initial Google Scholar search retrieved ~5,580 articles, which were filtered by citation count (>3), split by source (ArXiv/Non-ArXiv), and screened by six human annotators using inclusion criteria. Final corpus consists of 160 papers selected through dual annotation with Cohen's kappa ~0.56. The review focuses specifically on studies analyzing internal representations of pre-trained (not fine-tuned) Transformer models.

## Key Results
- Middle layers of BERT-based models are most relevant for encoding syntactic information
- Specific attention heads and neurons specialize in distinct linguistic phenomena, though redundancy exists
- Models generally encode linguistic information well but show systematic failures in low-frequency forms and negation
- Distinguishing correlation from causation remains a key methodological challenge

## Why This Works (Mechanism)

### Mechanism 1: Layer-wise Linguistic Encapsulation
If Transformer-based models are sufficiently pre-trained, linguistic information encodes hierarchically across layers rather than uniformly, with syntax tending to peak in middle layers. Lower layers process surface features, middle layers aggregate syntactic structures, and upper layers handle task-specific abstractions. This mimics a "classical NLP pipeline" within the deep network. The core assumption is that the model learns a hierarchical representation during pre-training rather than a flat statistical association.

### Mechanism 2: Sparse Functional Specialization
Specific attention heads and individual neurons within the network specialize in distinct linguistic phenomena (e.g., dependency relations, object-number agreement) rather than all heads learning redundant global representations. The multi-head attention mechanism allows different heads to attend to different subspaces of the input. The core assumption is that linguistic competence is distributed but modular, allowing for the isolation of specific functions via probing or ablation.

### Mechanism 3: Correlation vs. Causal Usage
High performance on probing classifiers indicates that linguistic information is encoded in the representations, but this does not prove the model uses this information for its predictions. Probing classifiers are trained to extract linguistic features from embeddings. If the classifier succeeds, the information exists in the vector space. However, this is correlation, not causation. The core assumption is that a high-accuracy probe implies the existence of information, but not its causal role in the model's inference process.

## Foundational Learning

- **Concept: Probing Classifiers**
  - Why needed here: This is the dominant methodology (used in 122/160 surveyed works) for "looking inside" the black box. Understanding the difference between training a model and training a probe on a frozen model is critical.
  - Quick check question: If I train a logistic regression classifier on frozen BERT embeddings to predict tense, am I changing BERT's internal weights?

- **Concept: Tokenization (BPE/WordPiece)**
  - Why needed here: The review notes that morphological study is hindered because tokenizers split words by compression statistics, not linguistic roots (e.g., "ing" might be a separate token). This creates a disconnect between the model's input units and human morphological concepts.
  - Quick check question: Why might BERT struggle to learn that "running" and "runner" share a morphological root compared to a model using character-level inputs?

- **Concept: Multilingual Alignment**
  - Why needed here: A major section of the review analyzes how models like mBERT handle multiple languages. The hypothesis of a "shared interlingual space" is key to understanding how these models transfer linguistic knowledge across languages.
  - Quick check question: Does mBERT learn language-specific syntax trees or a universal grammar shared across all its languages?

## Architecture Onboarding

- **Component map:** Raw Text -> Tokenizer (BPE/WordPiece) -> Embedding Layer (lexical/surface info) -> Lower Layers (1-3, word order/positional) -> Middle Layers (4-8, syntax/semantics) -> Upper Layers (9-12, task-specific/discourse) -> Specific Heads (dependency relations)
- **Critical path:** When analyzing a model, prioritize the Middle Layers. The survey indicates this is where the primary encoding of syntax and semantics occurs before becoming too abstract (upper) or too surface-level (lower).
- **Design tradeoffs:**
  - Probe Complexity: Lightweight probes (linear) may miss complex non-linear linguistic structures; complex probes may memorize the task rather than probe the model
  - Architecture Choice: Encoder-only models (BERT) are more transparent to probing than decoder-only models (GPT) or large LLMs
- **Failure signatures:**
  - Negation Blindness: Models often fail to process negation correctly, representing "X is not Y" similarly to "X is Y"
  - Frequency Bias: Agreement errors appear in low-frequency verb forms, suggesting statistical memorization rather than rule generalization
  - Word Order Over-reliance: Models may rely on word order even when language structure allows variations
- **First 3 experiments:**
  1. Structural Probe: Implement the Hewitt & Manning probe to verify if Euclidean distances in middle-layer embeddings correlate with syntactic tree distances
  2. Layer-wise Performance Sweep: Train identical probes for a specific task (e.g., Part-of-Speech tagging) on each layer of BERT to plot the "linguistic curve" and identify the peak layer
  3. Attention Head Ablation: Mask specific attention heads identified as "syntactic" and measure the performance drop on a grammaticality judgment task to test for causal necessity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do linguistically-interpretable representations in PLMs play a causal role in downstream processing, or are they merely epiphenomenal correlations?
- Basis in paper: Section 6 highlights the "dichotomy between ‘correlation’ and ‘causation’," noting that detecting a phenomenon via probing does not prove the model uses that information during inference.
- Why unresolved: Standard probing classifiers often identify patterns without determining if those patterns are functionally utilized by the network, leading to potential "interpretability illusions."
- What evidence would resolve it: Widespread adoption and validation of "causal probing" methods (e.g., amnesic probing or interventions) that demonstrate performance degradation when specific linguistic circuits are ablated.

### Open Question 2
- Question: How can interpretability methodologies be adapted for modern Large Language Models (LLMs) where internal parameter analysis is computationally prohibitive?
- Basis in paper: Section 4.1 states that interpretability techniques for internal parameters have become "unaffordable" for state-of-the-art models (e.g., Llama, BLOOM) due to size and closed architectures.
- Why unresolved: There is a shift toward prompting (black-box) over internal analysis, creating a gap in understanding the internal mechanics of the most powerful current models.
- What evidence would resolve it: The development of efficient, layer-wise interpretability frameworks or sparse activation analysis techniques that can scale to models with billions of parameters.

### Open Question 3
- Question: Do multilingual models encode linguistic knowledge in a universal, shared subspace or in language-specific subspaces?
- Basis in paper: Section 5.5.2 discusses the "hypothesis of a shared linguistic space" but presents contradicting evidence, such as shared morphosyntactic neurons versus poor cross-lingual semantic retrieval.
- Why unresolved: Different studies report conflicting layerwise distributions and neuron overlaps across languages, making it difficult to determine if a unified "interlingua" exists within the vector space.
- What evidence would resolve it: Systematic analysis showing that identical linguistic concepts across typologically diverse languages occupy consistent geometric subspaces within the model's hidden layers.

## Limitations
- Most studies focus on English, limiting generalizability across languages
- Current probing methods may not capture non-linear relationships due to use of lightweight probes
- Cannot distinguish causal from correlational representation with standard probing approaches

## Confidence
- Claims about layer-wise syntactic encoding: High
- Claims about attention head specialization: Medium
- Claims about cross-linguistic alignment: Low

## Next Checks
1. Replicate causal probing experiments (amnesic/indirect intervention) to verify if identified linguistic representations are functionally necessary rather than merely correlated
2. Test whether syntactic layer peaks identified in English BERT transfer to structurally different languages like Turkish or Japanese
3. Compare linear vs. non-linear probe performance on the same linguistic tasks to determine if current lightweight probes capture the full complexity of encoded linguistic knowledge