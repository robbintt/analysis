---
ver: rpa2
title: Model of human cognition
arxiv_id: '2512.00683'
source_url: https://arxiv.org/abs/2512.00683
tags:
- percepts
- percept
- temporal
- neurons
- neuron
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes a neuro-theoretical framework for human cognition
  consisting of four main modules: a visual processing module, a semantic hub module,
  a prediction module, and an executive control module. The core innovation is the
  complementary plasticity hypothesis (CPH), which describes how combinations of presynaptic
  input firing patterns determine postsynaptic neuron firing and learning.'
---

# Model of human cognition

## Quick Facts
- arXiv ID: 2512.00683
- Source URL: https://arxiv.org/abs/2512.00683
- Authors: Wu Yonggang
- Reference count: 0
- Proposes neuro-theoretical framework with four modules addressing LLM limitations

## Executive Summary
This paper presents a neuro-theoretical framework for human cognition consisting of four main modules: visual processing, semantic hub, prediction, and executive control. The framework introduces the complementary plasticity hypothesis (CPH), which describes how combinations of presynaptic input firing patterns determine postsynaptic neuron firing and learning. The model aims to address limitations in large language models by providing biological plausibility and computational efficiency while explaining cognitive processes like decision-making and problem-solving.

## Method Summary
The framework proposes a four-module cognitive architecture: a hierarchical visual processing system using complementary plasticity for feature learning and invariance; a semantic hub using population coding and relational propagation; a prediction module combining temporal and percept spaces; and an executive control module with feedforward action networks. The core innovation is CPH, which strengthens weights for specific combinations of presynaptic firing patterns rather than individual synapses. The model integrates principles from STDP, complementary learning systems, and hub-and-spoke theories to create a biologically plausible alternative to current AI architectures.

## Key Results
- Demonstrates how complex invariances, specificity, and generalization can emerge through neural mechanisms
- Shows ability to solve problems across different knowledge domains through recursive thinking and dynamic relational propagation
- Proposes computational efficiency advantages over current large language models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Complementary input combinations, not individual connections, drive postsynaptic strengthening and feature encoding.
- Mechanism: When a postsynaptic neuron fires, it strengthens weights for all non-redundant combinations of presynaptic firing patterns that preceded firing (e.g., A9B5, C6D5, A9C6), not just individual synapses. Higher-weighted combinations encode features closer to the neuron's tuning preference.
- Core assumption: Neurons perform combinatorial integration where subsets of inputs can jointly provide sufficient excitation; homeostatic plasticity prevents any single combination from dominating.
- Evidence anchors:
  - [abstract] "complementary plasticity hypothesis (CPH), which describes how combinations of presynaptic input firing patterns determine postsynaptic neuron firing and learning"
  - [section 2.1] "postsynaptic neuron undergoes synaptic change to strengthen all possible combinations of presynaptic firing patterns... Each combination is termed a complementary input"
  - [corpus] Limited direct validation; corpus focuses on LLM explainability gaps rather than neurophysiological mechanisms. Weak external validation.
- Break condition: If homeostatic mechanisms fail or input combinations remain sparse, weights may not converge; single inputs or noise may dominate responses.

### Mechanism 2
- Claim: Excitatory lateral connections between neurons with overlapping receptive fields create invariant object representations through temporally coincident activation.
- Mechanism: Sequential firing of neurons tuned to the same feature but different positions (e.g., a moving ball across receptive fields) strengthens bidirectional lateral connections via STDP. This forms cell assemblies where activation of any member activates the assembly, achieving positional invariance.
- Core assumption: Stimuli occurring close in time likely represent the same object from different reference points (saccades, egocentric movement).
- Evidence anchors:
  - [abstract] "demonstrates how complex invariances, specificity, and generalization can emerge through neural mechanisms"
  - [section 2.4] "Invariance through excitatory lateral connections utilises the assumption that two stimuli occurring closely in time tend to encode varying reference points belonging to the same object"
  - [corpus] No corpus papers validate this specific mechanism; gap in external evidence.
- Break condition: If temporal windows for coincident firing are too narrow or stimuli change too rapidly, lateral connections won't form; invariance fails to develop.

### Mechanism 3
- Claim: Relational propagation through consolidated associations enables generalization to novel inputs without explicit training.
- Mechanism: When an input percept activates, it spreads activation through strengthened connections to related percepts (hetero-associations). Supplementary percepts provide contextual constraints, enabling many-to-one retrieval. This allows novel combinations to activate appropriate targets via shared overlapping representations.
- Core assumption: Hippocampal sharp-wave ripples replay sequences offline, creating sufficient synaptic strength for reliable cortical propagation; overlapping representations support generalization.
- Evidence anchors:
  - [abstract] "ability to solve problems across different knowledge domains through recursive thinking and dynamic relational propagation"
  - [section 3.5] "Relational propagation between percepts allows for specific percepts to be activated in a wider range of situations, without requiring a specific stimulus"
  - [corpus] "Bridging the Gap: Toward Cognitive Autonomy" discusses self-monitoring and dynamic context regulation—conceptually aligned but not mechanistically validating.
- Break condition: If consolidation is disrupted or overlapping representations lack sufficient relational structure, propagation chains break; novel inputs fail to activate relevant targets.

## Foundational Learning

- Concept: Spike-Timing-Dependent Plasticity (STDP)
  - Why needed here: CPH extends STDP from pairwise spike relationships to combinatorial input patterns; understanding STDP is prerequisite.
  - Quick check question: Can you explain why presynaptic firing that consistently precedes postsynaptic firing strengthens connections, while reversed timing weakens them?

- Concept: Tuning Curves and Receptive Fields
  - Why needed here: The model relies on neurons having preferred features and declining responses to dissimilar stimuli; tuning curves formalize this.
  - Quick check question: If a neuron is tuned to circles and fires maximally for perfect circles, what firing rate would you expect for a slightly elongated ellipse?

- Concept: Population Coding
  - Why needed here: Object recognition and semantic representations use distributed populations, not grandmother cells; single-neuron responses are ambiguous without population context.
  - Quick check question: If 100 neurons encode "dog" features and 100 encode "cat" features with 10 overlapping, how would population activity change as a stimulus morphs from dog to cat?

## Architecture Onboarding

- Component map:
  - VPS (V1→V2→V4→IT): Hierarchical feature extraction; simple features → complex features; CPH-based tuning with excitatory/inhibitory lateral connections for invariance/decorrelation
  - ATL Semantic Hub: Transmodal integration via hub-and-spoke; population coding; hetero-associations through hippocampal SWR consolidation
  - Prediction Module (3.1 & 3.2): Temporal percept space (time/sequence cells) + percept space; generalized sequences vs. episodic replay (hippocampus)
  - Executive Control (PFC): Rostral-caudal FFA (feedforward action network) with mixed-selectivity neurons; CBGT action selection; relation store (DLPFC) + fast buffer (VLPFC); reward prediction (VMPFC/OFC)

- Critical path:
  1. Visual input → VPS feature extraction with CPH learning → position/scale invariance via lateral connections
  2. IT output → ATL integration → relational propagation through consolidated associations
  3. Percept contexts → Prediction module (temporal sequences + episodic replay) → predicted states
  4. Predicted states + current percepts → FFA mixed-selectivity layers → motor actions (with CBGT gating)

- Design tradeoffs:
  - **Specificity vs. Invariance**: Excessive specificity → sparse firing, slow learning; excessive invariance → poor discrimination. Balance via mixed features with different invariance degrees.
  - **Automatic vs. Controlled Retrieval**: Automatic = fast, one-to-one propagation but context-limited; Controlled = flexible, many-to-one but requires PFC working memory.
  - **Fast vs. Slow Learning**: Hippocampus (one-shot, context-specific) vs. cortex (gradual, decontextualized). CLS framework prevents catastrophic interference.

- Failure signatures:
  - **No invariance development**: Check lateral connection formation; temporal windows may be too narrow or receptive field overlap insufficient.
  - **Catastrophic interference**: New learning overwrites old; check if complementary input orthogonalization is working, or if hippocampal replay interleaving is disrupted.
  - **Propagation stalls**: Chains of related concepts don't activate; verify consolidation through SWRs and check if complementary input weights are below threshold.

- First 3 experiments:
  1. **Validate CPH on synthetic visual data**: Create a VPS layer with simple edge detectors; test whether combinatorial input weighting outperforms single-input STDP for recognizing composite shapes across noise variations.
  2. **Test invariance development**: Simulate a moving object across overlapping receptive fields; verify bidirectional lateral connection formation and measure position-invariant recognition accuracy.
  3. **Probe relational propagation**: Build a simplified ATL semantic network with word-object associations consolidated via simulated replay; test whether novel word combinations activate appropriate targets through intermediate associations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Complementary Plasticity Hypothesis (CPH) be computationally implemented to achieve efficiency and generalization superior to current Large Language Models?
- Basis in paper: [explicit] The abstract claims the model provides a "computationally efficient approach" for AI, but the paper presents a theoretical framework without experimental benchmarks.
- Why unresolved: The paper describes the architecture theoretically but does not provide a working software implementation or comparative performance metrics against existing AI models.
- What evidence would resolve it: A functioning artificial neural network built on CPH principles that demonstrates resource-efficient learning and task performance comparable to or better than current LLMs.

### Open Question 2
- Question: Do the specific biological mechanisms proposed for Complementary Plasticity (e.g., dendritic remodeling) exist and function as described in real neural tissue?
- Basis in paper: [inferred] Section 2.1 states that changes to complementary input weights "may be facilitated by mechanisms such as the remodelling of dendritic branches," presenting this as a hypothesis rather than an established fact.
- Why unresolved: While the paper cites general plasticity mechanisms, the specific rule that combinations of inputs alter dendritic structure to strengthen specific "complementary" paths requires distinct biological validation.
- What evidence would resolve it: Neurobiological imaging data (e.g., two-photon microscopy) showing structural changes in dendritic spines specifically correlating with the co-occurrence of distinct presynaptic firing patterns.

### Open Question 3
- Question: Does the proposed orthogonalization via complementary inputs effectively prevent catastrophic interference without requiring offline hippocampal replay?
- Basis in paper: [explicit] Section 3.10 states, "We suggest reasons why catastrophic interference may not occur even in their absence [of standard CLS mechanisms like replay]," contradicting standard Complementary Learning Systems theory.
- Why unresolved: The paper argues that "complementary inputs" orthogonalize representations naturally, but this theoretical claim challenges the established necessity of sleep-dependent consolidation and interleaved replay for preventing interference.
- What evidence would resolve it: Experiments or simulations where the model learns new information sequentially without replay and retains previously learned information with high fidelity, specifically isolating the "complementary input" mechanism as the cause.

## Limitations
- Theoretical framework without experimental validation or working implementation
- Mathematical formulation for combinatorial input patterns not fully specified
- No quantitative metrics or datasets provided to evaluate system performance

## Confidence

- **High confidence**: The general architecture design (hierarchical visual processing, semantic hub-and-spoke, prediction modules, executive control) aligns with established cognitive neuroscience models and addresses known LLM limitations.
- **Medium confidence**: The CPH mechanism is logically coherent and extends STDP principles, but lacks mathematical formalization and experimental validation to confirm computational efficiency claims.
- **Low confidence**: Claims about achieving biological plausibility and computational efficiency through CPH remain unverified without concrete implementation details and performance benchmarks.

## Next Checks

1. **Implement CPH Mathematical Formulation**: Develop the exact weight update equations for complementary input combinations, including normalization procedures and pruning strategies to prevent combinatorial explosion. Test on synthetic visual data with known feature combinations.

2. **Validate Invariance Learning Mechanism**: Create a controlled simulation of moving objects across overlapping receptive fields. Measure whether bidirectional lateral connections form and quantify position-invariant recognition accuracy compared to standard STDP approaches.

3. **Test Relational Propagation with Consolidaton**: Build a simplified semantic network with simulated hippocampal sharp-wave ripple replay. Verify that consolidated associations enable novel input combinations to activate appropriate targets through intermediate pathways, measuring generalization accuracy across unseen combinations.