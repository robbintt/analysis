---
ver: rpa2
title: How Generative AI supports human in conceptual design
arxiv_id: '2502.00283'
source_url: https://arxiv.org/abs/2502.00283
tags:
- design
- generative
- group
- conceptual
- designers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated how Generative AI supports humans in conceptual
  design, focusing on the roles of humans and AI across different design stages. Novice
  designers completed two design tasks with or without Generative AI assistance (ChatGPT
  or Midjourney).
---

# How Generative AI supports human in conceptual design

## Quick Facts
- **arXiv ID:** 2502.00283
- **Source URL:** https://arxiv.org/abs/2502.00283
- **Reference count:** 6
- **Primary result:** AI-assisted designers outperformed human-only groups in novelty, cost, and overall performance metrics

## Executive Summary
This study investigates how Generative AI supports humans in conceptual design across different design stages. Using novice designers working on two design tasks (baby chair and tangible music bricks), researchers compared human-only versus AI-assisted approaches (ChatGPT, Midjourney, or both). The findings reveal that AI primarily assists in problem definition and idea generation stages, while humans remain predominantly responsible for idea selection and evaluation. Expert ratings showed AI-assisted groups achieved superior novelty and overall performance, though AI generated solutions tended to have higher cost estimates. The study highlights the distinct roles of text-to-text and text-to-image models, with ChatGPT excelling at structural requirements and Midjourney at visual exploration.

## Method Summary
The experiment involved 20 novice designers (aged 18-26 with <4 years experience) divided into four groups: Human-only, ChatGPT, Midjourney, and Combined. After 20-minute training on tools and design process, participants completed two 20-minute design sessions. Outputs were collected as images with text descriptions. Five expert evaluators (with >5 years experience) rated the 40 total design solutions on five criteria (Novelty, Feasibility, Usability, Functional diversity, Cost) using 7-point Likert scales. The study employed ANOVA and Kruskal-Wallis H tests for statistical analysis.

## Key Results
- AI-assisted groups significantly outperformed human-only groups in novelty, cost efficiency, and overall expert ratings
- Generative AI expands solution space by lowering cognitive cost of early-stage exploration
- Text-to-text models (ChatGPT) excel at structural requirements while text-to-image models (Midjourney) excel at visual exploration
- Idea selection and evaluation stages remain predominantly human-led, even with AI assistance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Generative AI expands the solution space by lowering the cognitive cost of early-stage exploration (problem definition and idea generation).
- **Mechanism:** Text-to-text models (e.g., GPT-3.5) leverage large knowledge bases to instantly map requirements and functionalities, while text-to-image models (e.g., Midjourney) convert abstract concepts into visual stimuli. This reduces the "blank page" friction for novice designers, allowing rapid iteration.
- **Core assumption:** The quantity and diversity of AI-generated stimuli effectively translate to a broader exploration of the design space, rather than causing immediate fixation on the first output.
- **Evidence anchors:**
  - [abstract]: "The findings suggest Generative AI expands designers' solution exploration space and improves solution quality."
  - [section 4.2]: "Midjourney achieved the highest scores in terms of... diversity, novelty, and triggering more ideas... benefiting from its visual representation."
  - [corpus]: The paper "IDEA: Augmenting Design Intelligence through Design Space Exploration" supports the concept of using computational methods to explore feasible solutions where human experience is limited.
- **Break condition:** If the user accepts the first generated solution without iteration (design fixation), the expansion mechanism fails.

### Mechanism 2
- **Claim:** Introducing AI artifacts forces a "critique and filter" dynamic that activates the human-led selection and evaluation stage.
- **Mechanism:** When novices work alone, they often skip rigorous evaluation (satisficing). When AI generates a solution, the human role shifts from "creator" to "critic," necessitating a judgment on feasibility and alignment, which triggers the evaluation stage.
- **Core assumption:** Novice designers possess sufficient domain knowledge to evaluate AI outputs critically, even if they could not generate those outputs themselves.
- **Evidence anchors:**
  - [abstract]: "Idea selection and evaluation remain predominantly human-led."
  - [section 5.1]: "With the assistance of Generative AI... the solution selection and evaluation stage may be overlooked [by humans working alone]... The pattern changes when designers collaborate with Generative AI."
  - [corpus]: Corpus signals (e.g., "When not to help") suggest the importance of planning interaction; this mechanism relies on the AI providing content that *requires* human filtering.
- **Break condition:** If the AI output is perceived as flawless or overly authoritative, the human may abdicate the evaluation role.

### Mechanism 3
- **Claim:** Text-to-Text and Text-to-Image models provide distinct affordances that support different cognitive steps in the design process.
- **Mechanism:** Text models excel at structural decomposition (listing requirements, materials, logic), whereas image models excel at semantic ambiguity and visual style exploration. The paper implies a sequential dependency: Text defines the constraints, and Image explores the form.
- **Core assumption:** The user can mentally synthesize the structural logic from the text model with the visual styling from the image model without a unified interface.
- **Evidence anchors:**
  - [section 4.3.1]: "ChatGPT was able to outline key points of product design... Midjourney... requires users to input solution-oriented prompts."
  - [section 5.1]: "ChatGPT excelled in helping designers analyze individual design elements... Midjourney’s advantage in visualization saves designers time."
  - [corpus]: Weak direct support in provided corpus for modality split; inference is primarily paper-driven.
- **Break condition:** If the user attempts to use the image model for logical requirement gathering or the text model for precise visual styling, the collaboration fails.

## Foundational Learning

- **Concept:** Double Diamond / 4-Stage Design Process (Problem Definition, Idea Generation, Selection, Evolution)
  - **Why needed here:** The paper explicitly maps AI utility against these specific stages. Without understanding this workflow, one cannot interpret *where* the AI adds value versus where it distracts.
  - **Quick check question:** Can you distinguish between "Problem Definition" (understanding constraints) and "Idea Generation" (exploring solutions) in your current task?

- **Concept:** Modality Alignment (Text vs. Image)
  - **Why needed here:** The study highlights a non-synergistic effect when users simply copy-paste between ChatGPT and Midjourney. Understanding the distinct "language" of each model is crucial.
  - **Quick check question:** Are you trying to generate a functional list (use Text) or a stylistic reference (use Image)?

- **Concept:** Design Fixation vs. Stimulation
  - **Why needed here:** A core risk identified is that AI might limit exploration if the designer fixates on the first output.
  - **Quick check question:** Are you iterating on the AI's output to create something new, or just selecting the best option it provided?

## Architecture Onboarding

- **Component map:** User (Novice) -> Text-LLM (ChatGPT: Functional Analyst) -> Image-Gen (Midjourney: Visualizer) -> Human Evaluator (Final Filter)
- **Critical path:**
  1. **Problem Definition:** Use Text-LLM to list constraints/users (do not start with Image-Gen)
  2. **Idea Generation:** Use Text-LLM for concepts, then Image-Gen for visual variants
  3. **Selection:** *Human-only step.* Filter AI outputs for feasibility (AI often suggests high-cost/complex solutions)
  4. **Evolution:** Refine prompts based on selected human feedback
- **Design tradeoffs:**
  - **Control vs. Novelty:** Midjourney offers high novelty (score 5.4) but lower requirement satisfaction (score 4.8) compared to ChatGPT
  - **Speed vs. Detail:** ChatGPT is faster (score 6.0) for structure, but lacks visual "overview"
  - **Integration Cost:** The "Combined Group" did *not* outperform single-tool groups significantly, suggesting the friction of switching tools currently outweighs the benefits of multimodal input
- **Failure signatures:**
  - **"Copy-Paste" Friction:** Directly copying ChatGPT text into Midjourney results in misinterpretation (Text-to-Image models struggle with complex logical prompts)
  - **Cost Blindness:** AI-assisted groups generated solutions with higher "Cost" scores (worse performance) in expert ratings
  - **Evaluation Skipping:** Relying on AI for the "Selection" phase often leads to impractical designs being chosen
- **First 3 experiments:**
  1. **Constraint-First Prompting:** Run a design task using ChatGPT to define 5 constraints before opening Midjourney. Compare result quality to a "Visual-First" approach
  2. **Forced Divergence:** Generate 5 distinct design concepts with AI. Force a selection of the "least obvious" one to combat design fixation
  3. **Cost-Feasibility Audit:** Take an AI-generated design and explicitly ask the AI (or a human) to estimate manufacturing complexity/cost to check against the "Cost Blindness" failure mode

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does reliance on Generative AI for idea generation lead to design fixation?
- Basis in paper: [explicit] The authors note that focusing on triggering ideas may "overlook the issue of design fixation potentially caused by Generative AI."
- Why unresolved: The study measured the quantity of ideas but not whether designers became cognitively stuck on the AI's specific outputs.
- What evidence would resolve it: Empirical studies measuring the variance and originality of designs relative to the specific attributes of the initial AI-generated stimuli.

### Open Question 2
- Question: Can specific workflow methodologies produce a synergistic effect between text-to-text and text-to-image models?
- Basis in paper: [explicit] The authors found the Combined Group did not outperform single-tool groups and called for "methodologies and system designs that integrate" these models.
- Why unresolved: Participants manually transferring prompts between tools caused frustration and reduced effectiveness ("synergistic effect").
- What evidence would resolve it: Testing a unified system that semantically bridges text and image generation stages versus using disjointed tools.

### Open Question 3
- Question: How does the order of tool usage (text-to-text vs. text-to-image) impact design outcomes?
- Basis in paper: [explicit] The authors observed participants consistently used ChatGPT before Midjourney and suggest "investigating the impact of the sequence of tool usage."
- Why unresolved: The experiment allowed free choice of order, so the causal effect of sequence on performance remains unknown.
- What evidence would resolve it: A controlled experiment randomizing the required sequence of tools to analyze differences in efficiency, creativity, and user experience.

## Limitations

- The study population (20 novice designers aged 18-26 with <4 years experience) may not represent professional design contexts
- The two specific design tasks may not capture the full spectrum of conceptual design challenges
- The 20-minute time constraint per task may not reflect real-world design timelines
- Expert evaluation relies on subjective judgment despite reported inter-rater reliability (κ=0.66)

## Confidence

- **High Confidence:** The finding that AI-assisted groups outperform human-only groups in novelty and overall performance is well-supported by expert ratings (p<0.05 significance) and consistent across multiple metrics.
- **Medium Confidence:** The claim about distinct roles for text-to-text and text-to-image models in different design stages is supported by performance differences but could benefit from more nuanced analysis of modality interactions.
- **Low Confidence:** The assertion that the "Combined Group" did not outperform single-tool groups due to switching friction is speculative and not directly tested, as the study design doesn't isolate integration costs.

## Next Checks

1. **Professional Designer Validation:** Replicate the study with professional designers (5+ years experience) to assess whether the AI assistance benefits hold across expertise levels and whether expert designers can better integrate multiple AI tools.

2. **Longitudinal Performance Assessment:** Conduct a follow-up study with extended time horizons (e.g., 60-90 minutes per task) to determine if the benefits of AI assistance persist or change when designers have more time for iteration and refinement.

3. **Cost Estimation Protocol Validation:** Develop and test a standardized cost estimation protocol for AI-generated designs, addressing the identified "cost blindness" failure mode by incorporating explicit manufacturing feasibility checks into the AI-assisted workflow.