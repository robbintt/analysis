---
ver: rpa2
title: Generating Robot Constitutions & Benchmarks for Semantic Safety
arxiv_id: '2503.08663'
source_url: https://arxiv.org/abs/2503.08663
tags:
- should
- constitution
- constitutions
- safety
- shall
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces ASIMOV, a large-scale benchmark for evaluating
  semantic safety of AI models used in robotics. The benchmark includes multimodal
  datasets generated from real-world scenarios, such as injury reports and robot-collected
  videos, to cover long-tail safety situations.
---

# Generating Robot Constitutions & Benchmarks for Semantic Safety

## Quick Facts
- arXiv ID: 2503.08663
- Source URL: https://arxiv.org/abs/2503.08663
- Reference count: 40
- Primary result: Constitution generation pipeline achieves 84.3% alignment with human safety preferences, outperforming baselines and human-written constitutions

## Executive Summary
This work introduces ASIMOV, a large-scale benchmark for evaluating semantic safety of AI models used in robotics. The benchmark includes multimodal datasets generated from real-world scenarios, such as injury reports and robot-collected videos, to cover long-tail safety situations. To improve safety, the authors propose automatically generating robot constitutions from data using multimodal generative models. They also introduce an auto-amending process to refine these constitutions, making them more general and aligned with human preferences. Experiments show that generated constitutions achieve up to 84.3% alignment with human safety preferences, outperforming baselines and human-written constitutions. The approach also enhances resilience against adversarial attacks, demonstrating its potential for safer AI-driven robotics.

## Method Summary
The authors introduce ASIMOV, a benchmark for semantic safety evaluation of AI models in robotics, comprising multimodal datasets from real-world safety scenarios. They propose automatically generating robot constitutions using multimodal generative models by first generating unsafe scenarios (via image edits with Imagen 3) and then creating rules and instructions from these scenarios. An auto-amending process iteratively refines constitutions by generating counterfactuals and proposing amendments. The approach is evaluated on alignment with human safety preferences and resilience against adversarial attacks, showing constitutions outperform baselines and human-written ones.

## Key Results
- Generated constitutions achieve up to 84.3% alignment with human safety preferences
- Constitutions show enhanced resilience against adversarial attacks
- Constitutions outperform baselines and human-written constitutions in safety alignment

## Why This Works (Mechanism)
The approach leverages multimodal generative models to create realistic, unsafe scenarios from real-world data, then extracts safety rules and instructions. By iteratively amending these rules through counterfactual generation, the constitutions become more robust and general. The auto-amending process allows the system to refine rules based on edge cases and adversarial inputs, improving alignment with human safety preferences and resilience against attacks.

## Foundational Learning
- **Multimodal safety data generation**: Needed to create realistic edge cases for constitution training; quick check: verify image edits produce semantically meaningful unsafe scenarios
- **Rule extraction from scenarios**: Required to translate visual/contextual information into actionable safety rules; quick check: ensure extracted rules are interpretable and specific
- **Auto-amending algorithm**: Critical for refining constitutions through counterfactuals; quick check: confirm amendments improve rule generality without redundancy
- **Adversarial robustness testing**: Essential to validate constitutions under flipped safety understanding; quick check: constitutions maintain alignment when instruction desirableness is inverted
- **Human preference alignment measurement**: Core metric for evaluating constitution quality; quick check: binary classification accuracy on human-labeled safety judgments
- **Constitution redundancy management**: Important to prevent bloated, conflicting rule sets; quick check: monitor rule overlap after auto-amending

## Architecture Onboarding

### Component Map
VLM (Gemini 1.5 Pro) -> Image generator (Imagen 3) -> Rule generator (VLM) -> Constitution assembler (Auto-Merge) -> Auto-amender -> Safety Brain (inference module)

### Critical Path
Scenario generation → Rule generation → Constitution assembly → Auto-amending → Inference

### Design Tradeoffs
- Prompt engineering vs. model generalization: careful prompts needed for consistent generation but may limit model creativity
- Constitution length vs. specificity: longer constitutions capture more edge cases but risk redundancy
- Human labels vs. synthetic data: human labels provide ground truth but are limited; synthetic data is scalable but requires validation

### Failure Signatures
- Short constitutions (~10 lines) show high normal alignment but drop significantly in adversary mode
- Auto-amending can produce redundant rules that converge to similar general concepts
- VLM may refuse to generate unsafe scenarios if not properly prompted

### First 3 Experiments
1. Generate a baseline constitution using the VLM-Imagen pipeline on 100 RoboVQA frames
2. Apply Auto-Amend for 5 iterations and measure change in alignment score
3. Test the amended constitution in both normal and adversary modes

## Open Questions the Paper Calls Out
**Open Question 1**: How can generated constitutions be optimized to detect and resolve internal logical conflicts while minimizing redundancy? The current "Auto-Merge" and "Auto-Amend" processes reduce overlap but do not guarantee elimination of contradictory rules. An algorithm that produces concise constitutions maintaining high alignment (>80%) while verifying zero logical contradictions via a formal solver would resolve this.

**Open Question 2**: What secure system architectures are required to deploy constitutions so they resist tampering and verify trusted inputs? The paper proposes a "Safety Brain" concept but does not implement the physical decoupling or anti-tampering measures necessary to protect the constitution from a compromised base model or adversarial attack. A deployed system architecture where the constitution module successfully rejects commands from a compromised main brain or manipulated sensor input in a red-team security evaluation would resolve this.

**Open Question 3**: Can a standardized, universal robot constitution be derived through democratic alignment that holds across diverse cultures and environments? Current experiments rely on specific datasets and limited human voting, not modeling cross-cultural value discrepancies. A constitution validated against a globally sourced dataset of desirability labels that achieves statistically high alignment across distinct cultural and legal contexts would resolve this.

## Limitations
- Prompt engineering and model API interactions are not fully specified, introducing reproducibility uncertainty
- Auto-Merge and Auto-Amend algorithms lack precise implementation details, particularly for conflict resolution
- Binary classification for alignment may oversimplify nuanced human safety preferences
- Constitution generation depends heavily on model capabilities and may not generalize to all safety scenarios

## Confidence
- **High confidence**: Benchmark creation methodology (ASIMOV datasets) and overall constitution generation framework are clearly described and reproducible
- **Medium confidence**: Quantitative results are credible given clear experimental setup, but prompt engineering and model interactions introduce uncertainty
- **Low confidence**: Precise behavior of Auto-Merge and Auto-Amend algorithms, especially conflict resolution and amendment acceptance logic, cannot be fully verified without additional details

## Next Checks
1. **Prompt Sensitivity Analysis**: Systematically vary the VLM generation prompts and measure impact on constitution length, diversity, and alignment scores to assess prompt robustness
2. **Algorithm Implementation Verification**: Implement Auto-Merge and Auto-Amend with multiple reasonable interpretations and compare resulting constitutions' performance and redundancy
3. **Adversary Mode Validation**: Design and apply an independent adversary strategy to test whether constitutions maintain robustness beyond the reported flipped-understanding baseline