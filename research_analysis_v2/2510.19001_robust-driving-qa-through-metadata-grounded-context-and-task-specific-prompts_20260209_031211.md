---
ver: rpa2
title: Robust Driving QA through Metadata-Grounded Context and Task-Specific Prompts
arxiv_id: '2510.19001'
source_url: https://arxiv.org/abs/2510.19001
tags:
- driving
- prompt
- visual
- reasoning
- scene
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a vision-language QA system for autonomous driving
  that uses two phases to improve accuracy and robustness. Phase-1 employs a multimodal
  LLM with chain-of-thought prompting, few-shot exemplars, and self-consistency ensemble.
---

# Robust Driving QA through Metadata-Grounded Context and Task-Specific Prompts

## Quick Facts
- **arXiv ID**: 2510.19001
- **Source URL**: https://arxiv.org/abs/2510.19001
- **Reference count**: 27
- **Primary result**: 67.37% overall accuracy on driving QA benchmark, maintaining 96% accuracy under severe visual corruption

## Executive Summary
This paper presents a two-phase vision-language QA system for autonomous driving that significantly improves accuracy and robustness. The approach combines chain-of-thought prompting with self-consistency ensemble in Phase-1, then adds nuScenes metadata grounding and task-specific prompts in Phase-2. The system achieves 67.37% overall accuracy on DriveBench, outperforming baseline models, while maintaining 96% accuracy under severe visual corruption. The method addresses key challenges in driving scene understanding including VLM hallucination, category-specific reasoning requirements, and robustness to corrupted inputs.

## Method Summary
The approach uses Qwen2.5-VL-32B deployed on 4× NVIDIA A6000 GPUs. Phase-1 employs chain-of-thought prompting with 5-10 shot exemplars and self-consistency ensemble (5 samples with majority voting). Phase-2 adds nuScenes metadata grounding through 3D bounding boxes and ego-state text, plus category-specific prompts routed by a rule-based classifier for perception, prediction, and planning tasks. The system uses 6-camera multi-view inputs with 5-frame temporal history, enforcing structured Observations→Reasoning→Answer format with controlled vocabulary to prevent hallucination.

## Key Results
- **67.37% overall accuracy** on DriveBench driving QA benchmark
- **96% accuracy under severe visual corruption** (up from 82.69% baseline)
- **Self-consistency improves Phase-1 accuracy** from 64.56% to 66.85%
- **Task-specific prompts + metadata in Phase-2** achieve 67.37% overall (vs 66.85% Phase-1)

## Why This Works (Mechanism)

### Mechanism 1: Self-Consistency Ensemble for Answer Reliability
Sampling multiple reasoning chains and aggregating via majority voting improves answer accuracy by filtering stochastic reasoning errors. The model generates N independent reasoning paths, then aggregates final answers through voting where correct answers tend to converge while errors remain distributed.

### Mechanism 2: Task-Specific Prompting via Category Routing
Separate prompting strategies for perception, prediction, and planning queries improve accuracy over unified prompts by focusing reasoning on task-relevant evidence. A lightweight rule-based router classifies questions by category, then dispatches to specialized prompt templates optimized for each domain.

### Mechanism 3: Metadata Grounding via Anchor Injection
Explicit scene metadata (object annotations, ego-vehicle state) injected as text/visual prompts reduces hallucination and anchors model responses in verifiable scene content. Structured metadata compensates for VLM visual perception failures, especially under corruption.

## Foundational Learning

- **Chain-of-Thought (CoT) Prompting**: Decomposes complex questions into perceptual evidence + inference steps. *Quick check*: Can you explain why CoT helps decompose "Will the pedestrian cross?" into perceptual evidence + inference steps?
- **Few-Shot In-Context Learning**: Demonstrates reasoning style through exemplars; performance degrades beyond ~10 shots due to context saturation. *Quick check*: What happens to accuracy if you provide 50-shot examples instead of 10?
- **Vision-Language Model Grounding Failures**: Addresses VLM hallucination where models rely on language priors over visual evidence. *Quick check*: Why might a VLM hallucinate a "street sign" when viewing a bus stop advertisement?

## Architecture Onboarding

**Component map:**
- Input Layer: 6-camera multi-view images + 5-frame temporal history + nuScenes metadata
- Router: Rule-based category classifier → dispatches to Perception/Prediction/Planning templates
- Core VLM: Qwen2.5-VL-32B (4× A6000 distributed inference)
- Prompt Assembly: System prompt + driving knowledge + task-specific template + few-shot exemplars + metadata serialization
- Self-Consistency Module: N-way sampling + majority voting aggregator
- Output Parser: Enforces structured Observations→Reasoning→Answer format

**Critical path:**
1. Question arrives → router classifies category
2. Metadata retrieved via scene_token → serialized to anchor text + visual overlays
3. Prompt assembled (task template + few-shot + driving knowledge + system constraints)
4. VLM generates N reasoning chains → aggregator selects final answer
5. Output validated against anchor list (no hallucinated objects)

**Design tradeoffs:**
- **5 vs. 10 history frames**: 5 frames chosen; more frames increase latency without proportional accuracy gain
- **5-shot vs. 10-shot vs. 15-shot**: 10-shot optimal; 15-shot degrades (context saturation)
- **7B vs. 32B model**: 32B required for robust performance; 7B shows weaker visual grounding
- **Visual prompts**: Attempted but failed on 32B (language prior dominates visual cues at scale)

**Failure signatures:**
- Cross-view hallucination: Model reuses evidence from front camera and projects to rear views without verification
- Language prior dominance: Bus stop ads misclassified as traffic signs; construction sites invented when urban context suggested them
- MCQ format mismatch: "A" marked incorrect while "A. [Answer text]" is correct

**First 3 experiments:**
1. Ablate self-consistency: Run Phase-1 with N=1, 3, 5, 7 samples; measure accuracy vs. latency tradeoff
2. Ablate metadata grounding: Run Phase-2 with/without anchor text; measure hallucination rate on corruption scenarios
3. Router stress test: Inject ambiguous questions; measure misclassification rate and downstream accuracy impact

## Open Questions the Paper Calls Out

**Open Question 1**: How can evaluation metrics be refined to robustly assess scene understanding in driving VLMs without being biased by output text length or failing to verify visual grounding?
- The authors note that "longer output texts tend to receive higher evaluation scores" and that current metrics may evaluate correctness "without actually inspecting the image"
- Current GPT-based scoring often rewards plausible language hallucinations over factual visual reasoning

**Open Question 2**: How can LiDAR data be effectively integrated into Vision-Language Models to improve 3D scene understanding and feature accuracy?
- Current VLM architectures lack native mechanisms to process point clouds, relying solely on camera views and text metadata
- The paper states improving image features through LiDAR "will be a key research direction in the future"

**Open Question 3**: What principled methods can be developed for uncertainty estimation in VLMs to detect hallucinations caused by strong language priors in safety-critical contexts?
- The system currently lacks a mechanism to self-correct when it "relies on language priors" (e.g., hallucinating construction sites)
- The authors aim to "develop principled methods for VLM uncertainty estimation in safety-critical driving contexts"

## Limitations
- **Few-shot exemplar selection**: Paper specifies category-based selection but provides no concrete examples or retrieval methodology
- **Self-consistency hyperparameters**: Beyond "5 samples" and "majority voting," critical parameters like temperature remain unspecified
- **Model scaling behaviors**: Visual guidance methods work on 7B but fail on 32B, suggesting architectural mismatches at scale

## Confidence
- **High confidence**: Phase-1 CoT prompting with self-consistency achieves stated accuracy (66.85%) and corruption robustness gap
- **Medium confidence**: Category routing improves accuracy over unified prompts (insufficient ablation evidence provided)
- **Low confidence**: Visual prompt methods (vanishing point, gradient orientation) failing on 32B can be reliably adapted or replaced

## Next Checks
1. **Ablate self-consistency sampling**: Run Phase-1 with N=1, 3, 5, 7 samples measuring accuracy-latency tradeoff; verify diminishing returns beyond N=5
2. **Metadata grounding ablation**: Run Phase-2 with/without anchor text under corruption scenarios; measure hallucination rate and robustness gap
3. **Router stress test**: Inject ambiguous questions spanning multiple categories; measure misclassification rate and downstream accuracy degradation, particularly for planning queries