---
ver: rpa2
title: Sparsity-Controllable Dynamic Top-p MoE for Large Foundation Model Pre-training
arxiv_id: '2512.13996'
source_url: https://arxiv.org/abs/2512.13996
tags:
- dtop-p
- experts
- performance
- routing
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DTop-p MoE introduces a sparsity-controllable dynamic Top-p routing
  mechanism for large foundation models. The method employs a Proportional-Integral
  controller to dynamically adjust the probability threshold, maintaining precise
  control over the number of activated experts while enabling adaptive allocation
  across tokens and layers.
---

# Sparsity-Controllable Dynamic Top-p MoE for Large Foundation Model Pre-training

## Quick Facts
- **arXiv ID:** 2512.13996
- **Source URL:** https://arxiv.org/abs/2512.13996
- **Reference count:** 40
- **Primary result:** DTop-p MoE consistently outperforms Top-k and fixed-threshold Top-p baselines while maintaining equivalent average FLOPs through sparsity-controllable dynamic routing

## Executive Summary
DTop-p MoE introduces a sparsity-controllable dynamic Top-p routing mechanism that addresses key limitations in existing sparse MoE architectures. The method employs a Proportional-Integral controller to dynamically adjust the probability threshold, maintaining precise control over the number of activated experts while enabling adaptive allocation across tokens and layers. A dynamic routing normalization mechanism further allows different layers to learn distinct expert-selection patterns. Extensive experiments demonstrate that DTop-p consistently outperforms both Top-k and fixed-threshold Top-p baselines, maintaining equivalent average FLOPs while achieving better performance across multiple foundation model tasks.

## Method Summary
The DTop-p MoE framework introduces two key innovations: a Proportional-Integral controller that dynamically adjusts the probability threshold to maintain target sparsity, and a dynamic routing normalization mechanism that applies layer-wise learnable temperature scaling to routing logits. The PI controller treats the number of activated experts as a process variable and the probability threshold as a control variable, using both proportional and integral terms to eliminate steady-state error. The dynamic normalization applies z-score normalization followed by layer-specific learnable scaling before softmax, enabling different layers to develop distinct expert-selection patterns while maintaining a global probability threshold constraint.

## Key Results
- DTop-p consistently outperforms Top-k and fixed-threshold Top-p baselines across Large Language Models and Diffusion Transformers
- The method maintains equivalent average FLOPs while achieving better performance on 13 benchmarks across 5 skill areas
- DTop-p demonstrates strong scaling properties across expert granularity, capacity, model size, and dataset size
- The model autonomously learns hierarchical sparsity patterns where shallow layers use fewer experts and deeper layers recruit more experts for complex processing

## Why This Works (Mechanism)

### Mechanism 1: PI Controller for Dynamic Threshold Adjustment
A feedback control loop stabilizes the non-differentiable probability threshold in Top-p routing without requiring gradient-based optimization. The PI controller treats activated expert count as a process variable and probability threshold as a control variable, with proportional terms addressing immediate deviations and integral terms eliminating steady-state bias. The controller exploits the monotonicity of nucleus sampling—higher probability threshold strictly requires more experts. This creates a negative feedback loop where activation below target increases the threshold, forcing selection of more experts.

### Mechanism 2: Layer-Wise Learnable Temperature Scaling
Dynamic Routing Normalization applies z-score normalization to raw logits, then multiplies by layer-specific learnable scalars before softmax. Higher scalars sharpen the probability distribution (fewer experts activated), while lower scalars flatten it (more experts). This decouples local statistical properties of each layer's logits from the global threshold constraint, enabling distinct sparsity patterns across network depth while maintaining overall sparsity control.

### Mechanism 3: Autonomous Hierarchical Sparsity Emergence
When dynamic routing normalization combines with PI-controlled thresholds, hierarchical sparsity patterns emerge autonomously. Shallow layers learn to set scaling parameters high (sharp distributions, fewer experts) because early representations are more generic, while deeper layers learn lower parameters (flat distributions, more experts) because semantic specialization requires broader expert consultation. This pattern emerges from gradient optimization under global sparsity constraints rather than explicit programming.

## Foundational Learning

- **Concept: Sparse Mixture-of-Experts (MoE) Routing**
  - Why needed here: DTop-p modifies standard routing. Understanding baseline Top-k vs. Top-p is crucial for motivation.
  - Quick check question: Given router probabilities [0.4, 0.35, 0.15, 0.1] for 4 experts, how many experts are selected by Top-k(k=2) vs. Top-p(p=0.7)?

- **Concept: Control Theory (PID Controllers)**
  - Why needed here: The PI controller is the core contribution. The integral term prevents settling at suboptimal steady-state error.
  - Quick check question: If the proportional term alone drives error to 10% of target, what does adding an integral term accomplish?

- **Concept: Softmax Temperature Scaling**
  - Why needed here: Dynamic Routing Normalization is essentially learnable temperature scaling applied layer-wise.
  - Quick check question: Does increasing temperature before softmax make the distribution more uniform or more peaked?

## Architecture Onboarding

- **Component map:**
  Input Token → Router (W·x) → Dynamic Normalization (θ_l scaling) → Softmax → Top-p Selection (using global threshold p_t) → Expert Dispatch → MoE Output
  Parallel feedback path: Batch → Count Activated Experts (a_t) → PI Controller (error vs. target T) → Update p_{t+1}

- **Critical path:** The PI controller update must occur after each batch before the next forward pass. Incorrect ordering breaks the control loop's assumption of stable statistics per batch.

- **Design tradeoffs:**
  - High K_pro/K_int: Faster convergence to target sparsity but risk of oscillation
  - Low K_pro/K_int: Stable threshold but slow convergence
  - Strong LB loss: Better expert utilization but may conflict with hierarchical sparsity learning
  - Router Z-loss: DTop-p paper finds it counter-productive, unlike prior MoE work that recommends it

- **Failure signatures:**
  - Threshold drifts to 0 or 1: PI controller may be unstable; reduce K_pro/K_int
  - All experts selected uniformly: θ_l values may not be learning; check Dynamic RN gradients
  - Training loss spikes: Check for OOM from uncontrolled expert activation
  - Standard deviation of activated experts remains high (>2): PI controller not converging

- **First 3 experiments:**
  1. Validate PI controller in isolation: Train small MoE with DTop-p, plot p_t and activated experts over time, confirm convergence to target within ~1B tokens
  2. Ablate Dynamic Routing Normalization: Compare DTop-p with and without Dynamic RN (fixed θ=1.0 globally), expect higher loss without Dynamic RN
  3. Hyperparameter sensitivity sweep: Test K_pro/K_int values and initial p_0, confirm robustness to initialization but sensitivity to PI gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does DTop-p maintain stability and performance advantages when scaled to frontier-sized models (>100B active parameters) and trillion-token training datasets?
- Basis in paper: Section F states validating effectiveness on "frontier-scale foundation models (e.g., exceeding 100B active parameters or training on trillion-token corpora) remains future work."
- Why unresolved: Current experiments are limited to 13.6B total parameters and 300B training tokens; behavior at extreme scales is unknown.
- What evidence would resolve it: Successful training runs of DTop-p MoE models with active parameters exceeding 100B, demonstrating comparable sparsity control and loss curves.

### Open Question 2
- Question: Can the dynamic threshold mechanism be effectively integrated with non-standard MoE architectures, specifically expert-choice routing or architectures with heterogeneous expert sizes?
- Basis in paper: Section F notes that "the interaction between dynamic sparsity control and other advanced MoE techniques, such as expert-choice routing... or heterogeneous expert architectures... are not explored."
- Why unresolved: The current PI controller is designed for token-choice routing; it's unclear how it would function with expert-choice or variable expert costs.
- What evidence would resolve it: Experiments applying DTop-p to heterogeneous MoE variants or expert-choice routing paradigms, analyzing if PI controller requires modification.

### Open Question 3
- Question: Why does Router Z-Loss prove counter-productive and cause probability threshold collapse in DTop-p framework?
- Basis in paper: Section 4.6 observes that Router Z-Loss causes the probability threshold to drop dramatically by forcing flatter routing distributions.
- Why unresolved: Paper identifies the phenomenon but doesn't provide theoretical justification for why standard regularization fails with dynamic routing normalization.
- What evidence would resolve it: Theoretical analysis or ablation study isolating the interaction between Router Z-Loss penalty and learnable layer-wise temperature parameters.

## Limitations
- CV experiments lack dataset transparency, making it difficult to assess generalization beyond NLP domain
- The exact contribution of each component (PI control, Dynamic RN, their interaction) isn't isolated through targeted ablations
- Claims of autonomous hierarchical sparsity emergence lack rigorous quantitative validation

## Confidence

**High Confidence** (mechanistic soundness, well-defined experiments): The PI controller's mathematical formulation is rigorous with clear update rules and stability considerations. Dynamic Routing Normalization follows established temperature scaling principles. Experimental methodology is sufficiently detailed for NLP domain replication.

**Medium Confidence** (empirical claims, generalization): While consistent performance improvements are demonstrated, exact component contributions aren't isolated. Autonomous hierarchical sparsity emergence is supported by visualizations but lacks quantitative validation.

**Low Confidence** (external validation, dataset transparency): CV experiments cannot be fully reproduced due to lack of dataset specification. Scaling claims are based on internal benchmarks without independent verification.

## Next Checks

1. **Component Ablation Study**: Implement DTop-p variants with PI controller disabled, Dynamic Routing Normalization disabled, and both disabled. Compare training loss trajectories and final performance to isolate each mechanism's contribution.

2. **PI Controller Stability Analysis**: Train DTop-p with systematically varied K_pro/K_int values while monitoring threshold stability metrics: standard deviation of p_t over 1B-token windows, convergence time to target T, and frequency of threshold saturation.

3. **Hierarchical Sparsity Validation**: After training DTop-p, measure correlation between layer depth and θ_l values across random seeds, test whether manually enforcing uniform θ_l degrades performance, and analyze whether learned sparsity pattern aligns with linguistic processing depth.