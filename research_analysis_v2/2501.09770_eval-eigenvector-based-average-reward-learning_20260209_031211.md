---
ver: rpa2
title: 'EVAL: EigenVector-based Average-reward Learning'
arxiv_id: '2501.09770'
source_url: https://arxiv.org/abs/2501.09770
tags:
- learning
- policy
- optimal
- average-reward
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EVAL, the first model-free algorithm for
  solving entropy-regularized average-reward reinforcement learning (RL) in continuous
  state spaces using function approximation. The approach leverages spectral properties
  of a tilted transition matrix to learn both the optimal policy and average reward
  rate through a value-based method.
---

# EVAL: EigenVector-based Average-reward Learning

## Quick Facts
- arXiv ID: 2501.09770
- Source URL: https://arxiv.org/abs/2501.09770
- Reference count: 22
- Primary result: First model-free algorithm for entropy-regularized average-reward RL in continuous state spaces using function approximation

## Executive Summary
This paper introduces EVAL, a novel approach to average-reward reinforcement learning that leverages spectral properties of tilted transition matrices. The method learns both the optimal policy and average reward rate through eigenvector analysis, avoiding the need for discount factors in continuing tasks. By combining this with posterior policy iteration, EVAL can solve both entropy-regularized and unregularized average-reward problems. Experiments on classic control benchmarks demonstrate superior performance compared to DQN and soft Q-learning, with improved stability and sample efficiency.

## Method Summary
EVAL solves entropy-regularized average-reward MDPs by learning the dominant eigenvector of a tilted transition matrix. Rather than explicitly constructing this matrix (infeasible for continuous states), the algorithm uses stochastic approximation to learn only the eigenvector u(s,a). The optimal Q-values are recovered via Q(s,a) = (1/β)log u(s,a), and the average reward rate θ is extracted from the dominant eigenvalue. This yields an off-policy TD learning rule with exponential backups. Posterior Policy Iteration extends the method to unregularized average-reward RL by iteratively setting the prior policy equal to the current optimal policy, converging to the greedy solution.

## Key Results
- EVAL achieves higher and more stable performance than DQN and soft Q-learning on classic control benchmarks
- The method demonstrates reduced variance and better sample efficiency across all tested environments
- EVAL maintains performance on continuing tasks (100,000+ steps) where discounted methods degrade
- Minimal implementation changes required from existing DQN-style architectures

## Why This Works (Mechanism)

### Mechanism 1
The entropy-regularized average-reward optimal policy and reward rate can be extracted from the dominant eigenvector/eigenvalue of a "tilted" transition matrix. The algorithm constructs an implicit tilted matrix eP(s',a'),(s,a) = p(s'|s,a)π₀(a'|s')e^(βr(s,a)). Rather than building this matrix explicitly (infeasible for continuous states), it learns only the left dominant eigenvector u(s,a) via stochastic approximation. The Perron-Frobenius theorem guarantees a unique positive eigenvector and real eigenvalue under irreducibility assumptions.

### Mechanism 2
The eigenvector update equation naturally yields an off-policy, model-free temporal-difference learning rule. From the eigenvector equation e^(βθ)u(s,a) = E_{s',a'}[u(s',a')]eP, one obtains u(s,a) = e^(β(r(s,a)-θ))E_{s'~p,a'~π₀}[u(s',a')]. This resembles a TD backup with: (1) no discount factor, (2) reward-rate subtraction (θ), (3) exponential rather than linear accumulation. The expectation uses the prior policy π₀, not the current policy, enabling off-policy learning from any rollout policy.

### Mechanism 3
Posterior Policy Iteration (PPI) converges to the unregularized greedy optimal policy by iteratively setting the prior equal to the current optimal policy. At each PPI iteration, π₀ ← π* (the current optimal policy). This progressively tightens entropy regularization toward β→∞ behavior without explicit temperature annealing. Theoretical guarantee comes from Rawlik (2012), Theorem 4: this fixed-point iteration converges to the greedy solution.

## Foundational Learning

- **Average-reward vs. discounted objectives**: Why needed here: EVAL optimizes the limit of average per-step reward, not discounted sum. The discount factor γ is "unphysical" for continuing tasks—the paper argues it's a proxy with no grounded timescale. Quick check question: If an agent runs forever, what's the difference between maximizing Σγ^t r_t and lim(N→∞)(1/N)Σr_t?

- **Entropy regularization (KL penalty)**: Why needed here: The ERAR objective includes -1/β·log(π/π₀), trading reward maximization against staying close to the prior. This prevents policy collapse to deterministic greediness and improves exploration. Quick check question: What happens to the optimal policy as β→∞? As β→0?

- **Perron-Frobenius theorem and spectral gaps**: Why needed here: Guarantees the tilted matrix has a unique positive dominant eigenvector. The spectral gap determines the mixing time, which the paper argues should inform discount factor choice in related discounted formulations. Quick check question: Why does irreducibility matter for the existence of a unique stationary distribution?

## Architecture Onboarding

- Component map: State -> u-network(s,a) -> Q-values; State -> Target u-network(s,a) -> Stable targets; Replay buffer D; θ scalar; (PPI) π₀-network

- Critical path: 1) Sample action a ~ π₀ (or composed policy in PPI); 2) Store transition (s,a,r,s') in buffer; 3) Sample minibatch B ⊂ D; 4) Compute TD target û(s,a) = e^(β(r-θ)) · E_{a'~π₀}[ū(s',a')] using target networks; 5) Update u-networks via MSE loss: J(ψ) = E[(u_ψ(s,a) - û(s,a))²]; 6) Update θ via batch-averaged eigenvalue estimate; 7) (PPI only) Periodically update π₀ toward current π* via KL minimization

- Design tradeoffs: Aggregation function: Paper finds max over twin networks outperforms min (unlike TD3/SAC). Treat as tunable. Hard vs. soft target updates: Paper uses τ=1.0 (hard updates) for simplicity. Fixed β vs. PPI: Fixed β yields stochastic optimal policies; PPI converges to deterministic greedy. Network count: ≥2 networks required; more networks can help but with diminishing returns.

- Failure signatures: NaN/Inf in u or θ: Exponential overflow from large β×reward products; consider reward scaling or lower β. θ diverging: May indicate reducible chain or insufficient exploration; check that π₀ has full action support. Policy oscillation in PPI: Update frequency too high; reduce π₀ update frequency or increase Polyak weight.

- First 3 experiments: 1) Sanity check on tabular grid: Implement EVAL on a small discrete gridworld with known optimal solution. Verify that Q(s,a) = (1/β)log u(s,a) matches analytical solution and that θ converges to the true ERAR rate. 2) Ablation on aggregation function: On CartPole-v1, compare min vs. max vs. mean aggregation over twin networks. Paper claims max is optimal—replicate and test sensitivity. 3) Continuing-task stress test: Replicate Figure 3 experiment—train on 500-step CartPole episodes, then evaluate on 100,000+ step continuing episodes. Compare EVAL+PPI vs. SQL to verify long-horizon stability.

## Open Questions the Paper Calls Out

### Open Question 1
Can the EVAL framework be extended to handle stochastic transition dynamics without requiring model-based techniques? Basis in paper: The authors note the current method focuses on deterministic dynamics and identify stochastic dynamics as a key avenue for future exploration. Why unresolved: The current spectral method relies on deterministic dynamics to avoid the complexity of learning iterative biases for the transition function. What evidence would resolve it: A model-free derivation of the update equations and experimental results on environments with known stochasticity.

### Open Question 2
What are the formal sample complexity and convergence properties of the EVAL algorithm with function approximation? Basis in paper: The paper lists "studying the sample complexity and convergence properties of the proposed algorithm" as a necessary contribution for future work. Why unresolved: The paper provides empirical results but lacks theoretical analysis of the stochastic approximation stability or convergence bounds. What evidence would resolve it: Formal proofs establishing convergence rates or sample complexity bounds under standard function approximation assumptions.

### Open Question 3
Can the algorithm be effectively adapted for continuous action spaces? Basis in paper: The authors state that the current method features discrete action spaces and suggest adapting the approach to actor-based methods. Why unresolved: The current output architecture outputs a discrete vector of values, which is intractable for continuous action dimensions. What evidence would resolve it: A modified actor-critic variant of the algorithm demonstrated on standard continuous control benchmarks (e.g., MuJoCo).

## Limitations

- The method assumes deterministic transitions and full support over actions, limiting applicability to stochastic or sparse-action domains
- Theoretical guarantees rely on irreducibility assumptions that are difficult to verify with neural network function approximation
- Lack of precise architectural details (network depths, layer widths, initialization schemes) creates significant reproducibility gaps

## Confidence

- **High confidence**: The fundamental spectral learning mechanism (Mechanism 1) and its connection to the tilted transition matrix - the mathematical framework is well-established and the eigenvector extraction method is clearly specified
- **Medium confidence**: The TD-style learning rule (Mechanism 2) and its off-policy properties - while derived from the spectral formulation, numerical stability concerns and the interaction between θ estimation and exponential backups require empirical validation
- **Medium confidence**: Posterior Policy Iteration convergence (Mechanism 3) - theoretical guarantees exist but practical convergence depends heavily on update frequency and the quality of intermediate Q-estimates

## Next Checks

1. **Tabular verification**: Implement EVAL on a small discrete gridworld with known analytical solution to verify that Q(s,a) = (1/β)log u(s,a) recovers the true ERAR optimal values and that θ converges to the expected reward rate

2. **Stochastic transition robustness**: Extend the method to handle stochastic transitions by adding reward/dynamics biasing (as suggested in the paper) and test on a simple stochastic gridworld to verify performance degradation is bounded

3. **Aggregation sensitivity**: Systematically compare min vs. max vs. mean aggregation over twin networks across multiple environments to quantify the sensitivity of performance to this architectural choice beyond the single "max is best" claim