---
ver: rpa2
title: 'TMT: A Simple Way to Translate Topic Models Using Dictionaries'
arxiv_id: '2509.00822'
source_url: https://arxiv.org/abs/2509.00822
tags:
- topic
- translation
- table
- language
- voting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TMT introduces a lightweight approach to translate monolingual
  topic models across languages using basic dictionaries and voting models, without
  requiring parallel corpora or embeddings. The method translates each topic word-by-word,
  aggregates candidate translations via voting techniques, and normalizes scores to
  preserve topic coherence.
---

# TMT: A Simple Way to Translate Topic Models Using Dictionaries

## Quick Facts
- **arXiv ID**: 2509.00822
- **Source URL**: https://arxiv.org/abs/2509.00822
- **Reference count**: 30
- **Primary result**: NDCG@3 average of 0.770 for TMT vs. 0.516 best baseline on aligned Wikipedia test set

## Executive Summary
TMT offers a lightweight approach to translate monolingual topic models across languages using only basic dictionaries, without requiring parallel corpora or embeddings. The method translates each topic word-by-word, aggregates candidate translations via voting techniques, and normalizes scores to preserve topic coherence. Evaluated on a 251k aligned Wikipedia corpus (250k training, 1k test), TMT outperforms baselines like DeepL and direct probability assignment, achieving NDCG@3 of 0.770 compared to 0.516 for the best baseline. Qualitative analysis confirms TMT's ability to enrich translations via controlled fan-out while maintaining semantic focus, though challenges remain with hyperfocusing and homonym noise.

## Method Summary
TMT translates monolingual LDA topic models across languages using only dictionaries and voting models, avoiding the need for parallel corpora or embeddings. The approach first trains an LDA model on source language documents, then translates each topic word-by-word using a composite dictionary. For each source word, all possible translations are gathered, and these are re-translated back to create voter sets. Candidate translations are scored and aggregated using voting techniques (CombSUM, CombNOR, CombGSUM) with top-n filtering, and the maximum score per candidate is retained. The resulting topic distributions are normalized to preserve coherence. The method was evaluated on WikiCOMP-2014 DE-EN aligned Wikipedia corpus using NDCG@3 to compare inferred topic rankings on aligned test documents.

## Key Results
- TMT achieves NDCG@3 average of 0.770 (CombNOR, P5) vs. 0.516 best baseline on 1,000 aligned test articles
- 84% of translated articles retain all three top topics compared to source
- Topic sharpness (probability decay slope) correlates with translation quality, suggesting potential as corpus-independent quality metric

## Why This Works (Mechanism)
TMT works by leveraging dictionary-based word translation combined with voting aggregation to transfer topic distributions across languages. By translating each topic word individually and using back-translation to create voters, the method captures multiple translation candidates and their relative confidence. The voting models (CombSUM, CombNOR, CombGSUM) aggregate these scores while top-n filtering controls fan-out to prevent topic drift. Score normalization preserves the relative importance of terms within topics. This approach maintains semantic coherence better than direct probability assignment or neural translation baselines because it operates at the word-topic level rather than document level.

## Foundational Learning

**LDA Topic Modeling**
- Why needed: Core representation of topics as word probability distributions
- Quick check: Can reproduce tomotopy LDA training with specified hyperparameters

**Dictionary-based Translation**
- Why needed: Enables cross-lingual topic transfer without parallel corpora
- Quick check: Can construct composite dictionary from 9 specified sources

**Voting Aggregation Models**
- Why needed: Combines multiple translation candidates while controlling noise
- Quick check: Can implement CombSUM/NOR/GSUM with TOPn filtering

**NDCG@3 Evaluation**
- Why needed: Measures ranking quality of top-3 inferred topics
- Quick check: Can compute NDCG@3 on aligned test documents

## Architecture Onboarding

**Component Map**
Composite Dictionary -> Word Translation -> Voter Generation -> Score Aggregation -> Topic Assembly -> NDCG@3 Evaluation

**Critical Path**
Train LDA -> Translate Topics -> Aggregate Scores -> Normalize Distribution -> Evaluate on Aligned Corpus

**Design Tradeoffs**
- Dictionary-only vs. embedding-based: Lighter but more prone to homonym noise
- Word-by-word vs. document-level: More granular control but loses context
- Simple max aggregation vs. advanced techniques: Easier to implement but potentially suboptimal

**Failure Signatures**
- Topic drift: Excessive fan-out causing semantic inconsistency
- Homonym noise: Unrelated terms appearing due to ambiguous translations
- Rank hyperfocusing: Voting models concentrating probability on too few terms

**3 First Experiments**
1. Translate single topic and inspect top-10 words for semantic coherence
2. Compare NDCG@3 scores for different voting models (CombSUM vs CombNOR)
3. Measure topic sharpness correlation with NDCG@3 performance

## Open Questions the Paper Calls Out

**Context Awareness Integration**
Can context-aware mechanisms be integrated into TMT to reduce homonym noise without sacrificing the method's lightweight, dictionary-based nature? The current implementation is context-ignorant by design, treating all dictionary candidates equally; adding context usually requires heavy resources that TMT aims to avoid. A modified pipeline incorporating lightweight part-of-speech tagging or domain filtering that statistically significantly reduces homonym-induced errors would resolve this.

**Corpus Quality Assessment**
Can TMT be effectively utilized as a tool to assess the quality of aligned multilingual corpora? The paper currently uses aligned corpora to evaluate TMT; the inverse application—using TMT's translation consistency to flag misaligned or poor-quality document pairs—remains untested. A study where low NDCG@3 scores from TMT successfully predict semantically divergent or misaligned document pairs would resolve this.

**Advanced Aggregation Techniques**
Do advanced aggregation techniques outperform the `max` function during the "Build Topic" step of the TMT algorithm? The current method simply takes the maximum score for a candidate term, potentially ignoring useful signal from the distribution of voter scores. Comparative tests using weighted averages or probabilistic merging functions for the "Assembly of Scores" step, showing improved NDCG@3 or coherence over the `max` baseline, would resolve this.

## Limitations

- Exact dictionary sources and extraction scripts are unspecified, making faithful reproduction difficult
- Homonym noise remains a challenge due to context-agnostic translation
- The method's effectiveness depends heavily on dictionary quality and coverage

## Confidence

- **High confidence**: Overall conceptual framework and NDCG@3 evaluation methodology
- **Medium confidence**: Voting model implementation and normalization steps
- **Low confidence**: Exact hyperparameter settings and dictionary-specific preprocessing details

## Next Checks

1. Reconstruct the composite dictionary from all nine sources and verify pair count and format match specifications
2. Reproduce NDCG@3 scores on the 1k test set using the same preprocessing pipeline and tomotopy hyperparameters
3. Perform qualitative inspection of top 10 translated topics to detect semantic drift or homonym noise