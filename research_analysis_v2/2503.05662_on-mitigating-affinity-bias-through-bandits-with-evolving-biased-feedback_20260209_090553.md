---
ver: rpa2
title: On Mitigating Affinity Bias through Bandits with Evolving Biased Feedback
arxiv_id: '2503.05662'
source_url: https://arxiv.org/abs/2503.05662
tags:
- bias
- lemma
- tbias
- algorithm
- regret
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies affinity bandits, a new variant of non-stationary
  multi-armed bandits where only biased feedback is observed. The bias evolves based
  on the fraction of times each arm has been selected, while the real reward of each
  arm remains unchanged.
---

# On Mitigating Affinity Bias through Bandits with Evolving Biased Feedback

## Quick Facts
- **arXiv ID:** 2503.05662
- **Source URL:** https://arxiv.org/abs/2503.05662
- **Reference count:** 40
- **Primary result:** Round-robin elimination algorithm nearly matches instance-dependent regret lower bound in affinity bandits with evolving biased feedback.

## Executive Summary
This paper introduces affinity bandits, a novel non-stationary multi-armed bandit problem where only biased feedback is observed and the bias evolves based on the fraction of times each arm has been selected. Classical algorithms like UCB fail catastrophically due to a "rich-get-richer" feedback loop effect. The authors prove a new instance-dependent regret lower bound that is larger than standard bandit settings by a multiplicative factor of the number of arms K. They design a round-robin elimination algorithm that achieves near-optimal regret by giving every group a chance to be selected, effectively stabilizing the relative bias between arms despite never observing true rewards.

## Method Summary
The method is a phased elimination algorithm that operates in rounds, where each round consists of a round-robin sampling phase followed by an elimination phase. In round r, each active arm is pulled m_r times in round-robin fashion, then arms are eliminated if their empirical average is more than 2^{-r} below the best arm's average. The algorithm is designed to be agnostic to the specific bias function f, requiring only that it exists and is Lipschitz continuous. The key insight is that this round-robin approach synchronizes the fraction of times each arm is pulled, making the bias term negligible compared to the true reward gap.

## Key Results
- Proved instance-dependent regret lower bound that is larger than standard bandit setting by multiplicative factor K
- Demonstrated that standard algorithms (UCB, EXP3) fail with linear regret due to feedback loop
- Designed elimination algorithm achieving near-optimal regret without observing true rewards
- Showed naive debiasing fails due to exploding variance when dividing by small bias weights

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Standard optimistic algorithms (e.g., UCB) fail because they reinforce a "rich-get-richer" feedback loop where initial bias distorts perceived value.
- **Mechanism:** The observed reward Y_t is a product of the true reward μ_i and a bias weight W_i(t). If an arm has a large initial bias (T_i^0), its observed reward appears artificially high. An algorithm maximizing observed reward pulls this arm more frequently, which further increases its bias weight W_i(t), creating a self-reinforcing cycle of error even if the true reward is suboptimal.
- **Core assumption:** The bias weight W_i(t) is a non-decreasing function of the fraction of times arm i has been pulled.
- **Evidence anchors:**
  - [abstract] Mentions that classical algorithms fail due to the feedback loop effect where bias evolves based on selection fraction.
  - [section 3] "Why is the problem difficult?" demonstrates that UCB suffers linear regret because the bias structure makes a suboptimal arm appear empirically optimal.
  - [corpus] Neighbor papers on "Biased LLM Hiring" provide thematic context that bias in assessment is pervasive, though they do not validate the specific MAB feedback loop.
- **Break condition:** If the bias function f(·) were decreasing or independent of the selection history, the feedback loop would vanish, and standard UCB would likely work.

### Mechanism 2
- **Claim:** A round-robin elimination strategy succeeds by stabilizing the *relative* bias between arms, making the bias term negligible compared to the true reward gap.
- **Mechanism:** By cycling through active arms equally (Algorithm 1), the fraction of times each arm is pulled remains roughly synchronized. Consequently, the difference in bias weights W_{i*}(t) - W_i(t) is bounded and small (scales as Δ^2). This allows the algorithm to compare arms based on their true value differences despite never observing the true values directly.
- **Core assumption:** The time horizon n is sufficiently large to dilute initial biases, and the bias function f is Lipschitz.
- **Evidence anchors:**
  - [abstract] States the key insight is that "giving every group a chance... is essentially the best policy."
  - [section 4] Eq. (4) and (5) show the decomposition of observed gaps, noting that the bias term (5) is negligible relative to the elimination criterion under this strategy.
  - [corpus] No direct mechanism validation in corpus; this is a novel contribution of the paper.
- **Break condition:** If the time horizon is too short or the Lipschitz constant is too large, the "negligible bias term" assumption fails, causing incorrect eliminations.

### Mechanism 3
- **Claim:** Naive "debiasing" (multiplying observations by inverse weights) fails due to exploding variance.
- **Mechanism:** To recover an unbiased sample from the biased observation Y_t, one might calculate Z_t = Y_t / W_i(t). However, the variance of Z_t scales as 1/W_i(t)^2. For arms rarely pulled, W_i(t) is small, causing variance to explode, which invalidates standard concentration bounds used by algorithms like UCB-V.
- **Core assumption:** The noise in feedback is sub-Gaussian.
- **Evidence anchors:**
  - [section 3] "Exploding variance when 'unbiasing' the feedback" explicitly details this scaling issue.
  - [figure 4] Visualizes how UCB-V with debiasing fails to achieve logarithmic regret.
- **Break condition:** If the bias function f(x) were bounded below by a constant (never near zero), the variance would remain bounded.

## Foundational Learning

- **Concept: Multi-Armed Bandits (MAB)**
  - **Why needed here:** This is the base mathematical framework. You must understand the trade-off between exploration (gathering info) and exploitation (using info) to see why the evolving bias breaks standard exploration strategies.
  - **Quick check question:** If an arm's observed mean changes every time you pull it, can you still treat it as a stationary stochastic bandit?

- **Concept: Sub-Gaussian Distributions**
  - **Why needed here:** The paper assumes noise is sub-Gaussian to bound tail probabilities. This is essential for proving that the elimination criterion (Algorithm 1) fails with only exponentially small probability.
  - **Quick check question:** Does a bounded random variable (like a hiring score of 0-1) satisfy the sub-Gaussian condition?

- **Concept: Lipschitz Continuity**
  - **Why needed here:** The algorithm relies on the bias function f(x) being Lipschitz continuous. This ensures that small changes in the fraction of hires (input) result in bounded changes in bias (output), allowing the round-robin strategy to stabilize the bias differences.
  - **Quick check question:** Why would a discontinuous bias function break the guarantees of the elimination algorithm?

## Architecture Onboarding

- **Component map:** Environment -> Policy (Algo 1) -> Oracle (feedback loop)
- **Critical path:**
  1. **Initialization:** Pull each arm once to establish baseline history.
  2. **Round-Robin Phase:** In round r, cycle through all active arms i ∈ A_r, pulling each exactly m_r times.
  3. **Aggregation:** Compute the average biased feedback μ̂_i(r) for the round.
  4. **Elimination:** Drop arm i if its feedback is smaller than the best observed feedback by a margin > Δ̃_r.

- **Design tradeoffs:**
  - **Knowledge of Bias:** The algorithm is designed to be "agnostic" to the bias function f, requiring only that it exists and is Lipschitz. This makes it robust but potentially slower than an oracle algorithm that knows f exactly.
  - **Batching:** The algorithm operates in phases/batches rather than strictly online. This batching is necessary to average out the noise and bias fluctuations before making a decision.

- **Failure signatures:**
  - **Linear Regret:** If the algorithm starts favoring one arm overwhelmingly (like UCB in Fig 3), it has failed to correct the bias.
  - **Optimal Arm Elimination:** If the time horizon n is insufficient or initial bias T_max^0-T_min^0 is too large, the optimal arm may be incorrectly dropped early.

- **First 3 experiments:**
  1. **Reproduce Linear Regret:** Run standard UCB on a 2-armed instance with high initial bias for the suboptimal arm to verify the "rich-get-richer" failure mode (Fig 3).
  2. **Scaling Check:** Run Algorithm 1 with increasing number of arms K to verify that regret scales with the multiplicative factor of K as predicted by the lower bound.
  3. **Sensitivity Analysis:** Vary the Lipschitz constant of the bias function f(x)=x^α to find the threshold where Algorithm 1 fails to converge (Fig 10).

## Open Questions the Paper Calls Out

- **Question:** Can the affinity bias model be extended to contextual bandits to handle group definitions based on sensitive attributes rather than skill sets?
  - **Basis:** [explicit] The discussion section states that assuming different performance expectations for groups defined by sensitive attributes is problematic and suggests contextual bandits as a direction for future work.
  - **Why unresolved:** The current model assumes stationary real rewards for each arm, which implies inherent performance differences between groups; this assumption is ethically and legally problematic when applied to protected demographic classes.
  - **What evidence would resolve it:** A formal extension of the affinity bandit model to the contextual setting with associated regret bounds.

- **Question:** How do the regret guarantees and optimal strategies change if the bias feedback depends only on a bounded history (the last M hires) rather than the entire past?
  - **Basis:** [explicit] The discussion highlights that the current model assumes an "ever-growing hiring committee" and leaves the bounded case for future work.
  - **Why unresolved:** The current mathematical proofs rely on the bias fraction accumulating over infinite time; a finite window introduces sliding-window dynamics that break the current analysis.
  - **What evidence would resolve it:** A regret analysis of the affinity bandit setting where the bias term W_i(t) depends only on the frequency of arm pulls within the last M steps.

- **Question:** Is it possible to achieve sublinear regret in the affinity bandit setting when the time horizon n is unknown?
  - **Basis:** [inferred] Theorem 4.1 requires a known time horizon n, and the text notes that standard "doubling trick" methods fail because the bias feedback depends on the specific history of the policy's actions.
  - **Why unresolved:** Standard techniques for unknown horizons rely on resetting the algorithm or ignoring the history dependency, which invalidates the feedback loop model central to this problem.
  - **What evidence would resolve it:** An algorithm that achieves sublinear regret without knowing n, or a proof showing that linear regret is inevitable in this setting.

## Limitations

- The algorithm assumes knowledge of the Lipschitz constant L of the bias function, though not the function itself. The practical impact of misspecifying L is not explored.
- The results are asymptotic; finite-time performance for realistic horizons is not characterized beyond simulations.
- The "agnostic" algorithm does not leverage any structural knowledge of the bias function, which may be overly conservative in practice.

## Confidence

- **High Confidence:** The lower bound (Theorem 3.1) and its multiplicative factor K are mathematically sound and well-proven. The failure of standard algorithms (UCB/EXP3) is clearly demonstrated through theoretical analysis and simulations.
- **Medium Confidence:** The elimination algorithm's regret bound (Theorem 4.1) follows logically from the analysis, but depends critically on the "sufficiently large n" assumption which is not quantified. The variance explosion when debiasing (Mechanism 3) is theoretically sound but the modified UCB-V approach is not fully specified.
- **Low Confidence:** The empirical validation is limited to simulations with known parameters. No real-world data or user studies validate the model assumptions or algorithm performance.

## Next Checks

1. **Robustness to Misspecification:** Test Algorithm 1 with incorrect Lipschitz constant estimates (e.g., L/2 or 2L) to quantify performance degradation.
2. **Finite-Time Analysis:** Characterize the exact time horizon n required for the elimination algorithm to succeed as a function of initial bias T^0_max - T^0_min and K.
3. **Structural Knowledge Integration:** Design and test a variant algorithm that uses partial knowledge of the bias function (e.g., knowing f(x)=x^α but not α) to see if it improves regret.