---
ver: rpa2
title: 'Attention Sink Forges Native MoE in Attention Layers: Sink-Aware Training
  to Address Head Collapse'
arxiv_id: '2602.01203'
source_url: https://arxiv.org/abs/2602.01203
tags:
- attention
- sink
- head
- heads
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work identifies that the attention sink in vanilla attention
  and sink attention naturally forms a mixture-of-experts mechanism within attention
  layers, explaining the head collapse phenomenon observed in prior work. To mitigate
  this issue, the authors propose a sink-aware training algorithm with an auxiliary
  load balancing loss that encourages balanced utilization across attention heads.
---

# Attention Sink Forges Native MoE in Attention Layers: Sink-Aware Training to Address Head Collapse

## Quick Facts
- arXiv ID: 2602.01203
- Source URL: https://arxiv.org/abs/2602.01203
- Reference count: 35
- Key outcome: Attention sink creates native MoE mechanism; sink-aware training with load balancing loss mitigates head collapse and improves performance across model sizes and attention types

## Executive Summary
This paper identifies that attention sinks in vanilla and sink attention naturally form a mixture-of-experts (MoE) mechanism within attention layers, explaining the head collapse phenomenon where only a subset of attention heads contribute to generation. To address this, the authors propose a sink-aware training algorithm with an auxiliary load balancing loss that encourages balanced utilization across attention heads. Experiments across models of different sizes (0.6B, 1B, 2B) and attention mechanisms (vanilla, sink, gated) show that the method effectively reduces head collapse and improves model performance, with consistent gains in both training from scratch and fine-tuning scenarios.

## Method Summary
The method introduces an auxiliary load balancing loss based on the coefficient of variation (CV) of per-head importance scores during training. For training from scratch, the loss is applied uniformly across all heads (λ≈10⁻⁴). For fine-tuning existing models, a shared/routed head split is used to avoid head pinning effects, with the loss applied only to routed heads (λ≈10⁻²). The approach works with different attention mechanisms by computing gating factors (implicit via sink attention or explicit via sigmoid gates) and using these to calculate importance scores. Flash Attention's log-sum-exp values enable efficient gate computation without materializing full attention matrices.

## Key Results
- The auxiliary load balancing loss consistently improves head utilization balance across all attention mechanisms (vanilla, sink, gated)
- Training from scratch with auxiliary loss shows reduced head collapse and improved downstream task accuracy (MMLU, GSM8K, HumanEval, etc.)
- Fine-tuning with shared/routed head split avoids head pinning while maintaining performance gains
- Gated attention performs best among mechanisms, followed by sink attention, then vanilla attention
- Models with auxiliary loss show lower coefficient of variation in head importance scores during training

## Why This Works (Mechanism)

### Mechanism 1: Implicit Gating via Attention Sink
The attention weight allocated to the sink token functions as an implicit gating factor, scaling each head's contribution. In vanilla attention, softmax normalization forces weights to sum to 1, with surplus flowing to the first token (value near-zero). This yields O ≈ (1 − A_sink) × Σ Ã÷v, where (1 − A_sink) is the implicit gate. Sink attention replaces the first token with a learnable sink parameter; gated attention uses an explicit sigmoid gate. All converge to the same functional form.

### Mechanism 2: Native MoE Structure Induces Head Collapse
Attention layers inherently form an MoE structure where each head is an expert and gating factors route contributions, leading to expert/head collapse. With implicit/explicit gating, certain heads receive consistently higher gate values during training, attracting more gradient updates. This self-reinforcing specialization mirrors MoE expert collapse—dominant heads become essential while others go dormant.

### Mechanism 3: Auxiliary Load Balancing Loss Redistributes Head Utilization
Adding a CV-based auxiliary loss on gating/importance scores encourages balanced head utilization and improves performance. L_aux = λ × Σ CV(Imp_l,h)² penalizes high variance in importance scores per layer. For fine-tuning, top-m heads are frozen as "shared" while remaining "routed" heads are balanced, avoiding head pinning where dominant heads resist change and dormant heads artificially inflate their gates.

## Foundational Learning

- Concept: Attention Sink Phenomenon
  - Why needed here: The entire paper reframes attention sink as a functional gating mechanism rather than a bug; understanding this is prerequisite.
  - Quick check question: Why does softmax normalization force some token to absorb "redundant" attention weight?

- Concept: Mixture-of-Experts (MoE) and Expert Collapse
  - Why needed here: The paper analogizes heads to experts; expert collapse explains why load balancing is necessary.
  - Quick check question: In a standard MoE layer, what two components are required, and what problem does expert collapse describe?

- Concept: Coefficient of Variation (CV) as an Imbalance Metric
  - Why needed here: CV(·) = std/mean quantifies head load imbalance; the auxiliary loss directly penalizes squared CV.
  - Quick check question: If a layer has 4 heads with importance scores [0.1, 0.1, 0.1, 0.7], is CV high or low? What if scores were [0.25, 0.25, 0.25, 0.25]?

## Architecture Onboarding

- Component map: 
  - Vanilla Attention: Standard softmax over q·k/√d, first token serves as implicit sink (requires value drain)
  - Sink Attention: Modified softmax denominator adds learnable sink parameter; no value vector associated with sink
  - Gated Attention: Explicit head-wise sigmoid gate computed from x·W_θ; attention computed normally
  - Unified view: All three yield G^l,h_t · Σ Ã÷v where G is implicit (1 − A_sink) or explicit (sigmoid gate)
  - Auxiliary loss module: Computes per-head importance scores Imp = mean(G across tokens), calculates CV per layer, applies λ-weighted penalty

- Critical path:
  1. Forward pass: Compute attention outputs via Flash Attention, extract LSE values
  2. Gate computation: G = σ(LSE − sink) for Sink Attention; G = σ(LSE_¬0 − q·k_0/√d) for Vanilla; use explicit gate for Gated
  3. Importance aggregation: Mean gate per head → Imp_l,h
  4. Loss computation: CV(Imp) per layer, squared and summed
  5. For fine-tuning: Identify top-m heads as shared, exclude from loss

- Design tradeoffs:
  - Training from scratch: Apply aux loss to all heads; λ ≈ 10⁻⁴ works well
  - Fine-tuning existing models: Must use shared/routed split (Equation 8) to avoid head pinning; λ ≈ 10⁻²
  - Mechanism choice: Gated Attention > Sink Attention > Vanilla Attention in terms of final performance, but all benefit from aux loss
  - Overhead: <2% training latency increase when using Flash Attention LSE trick

- Failure signatures:
  - Head pinning effect (fine-tuning only): If shared/routed split is not used, all heads show uniformly high importance scores but no real balancing occurs; dormant heads inflate gates without contributing semantically
  - Excessive λ: Training loss dominates language modeling loss, validation BPB plateaus higher
  - Flash Attention incompatibility with Sink Attention: Naive implementations require eager mode; must use LSE-based gate computation to retain efficiency

- First 3 experiments:
  1. Diagnose head collapse in your model: Run inference on 500 samples, compute Imp_l,h per head, visualize heatmaps and CV values. Confirm collapse severity before intervention.
  2. Train from scratch with aux loss (small scale): Train a 0.6B model with λ = 10⁻⁴, compare validation BPB and downstream task accuracy against baseline. Verify that CV decreases during training.
  3. Fine-tune with shared/routed split: Take an existing LLM (e.g., LLaMA3-8B), identify top-3 heads per layer as shared, apply aux loss to remaining heads with λ = 10⁻² on a reasoning dataset. Compare against fine-tuning without aux loss.

## Open Questions the Paper Calls Out

- What is the optimal ratio of shared heads to routed heads (parameter m) when fine-tuning existing LLMs with the sink-aware load balancing loss?
- Why does head collapse intensify with model scale, and can this trend be reversed through architectural modifications?
- Does enforced head load balancing prevent beneficial functional specialization from emerging during extended training?
- Can the quantified head importance scores be leveraged for dynamic inference-time optimization, such as selective head activation or early exit?

## Limitations

- Implementation details unclear: The number of shared heads (m) for fine-tuning and exact training steps/epochs are not specified, making faithful reproduction challenging
- Experimental scope limited: Evaluation focused on specific model architectures (20 layers, 10 heads, hidden=1280), with uncertain performance impact on larger, more complex models
- Generalizability uncertain: Validated primarily on English-language datasets and FineWeb-Edu pretraining data; effectiveness on multilingual models or specialized domains not demonstrated

## Confidence

**High Confidence**: The identification of attention sink as creating an implicit gating mechanism and its connection to head collapse phenomena. This is supported by clear mathematical derivations and extensive experimental validation showing consistent improvements.

**Medium Confidence**: The effectiveness of the auxiliary load balancing loss (L_aux) across different attention mechanisms. While the method shows consistent improvements, the specific formulation using coefficient of variation squared is not widely validated in prior work.

**Low Confidence**: The fine-tuning procedure's robustness, particularly the shared/routed head split approach. The paper identifies the head pinning problem but doesn't provide comprehensive ablation studies on different m values or demonstrate effectiveness across diverse fine-tuning scenarios.

## Next Checks

1. **Head Collapse Diagnosis**: Implement the importance score computation and visualize head utilization heatmaps on an existing pretrained model. Verify the presence and severity of head collapse before applying any interventions.

2. **Shared/Routed Split Sensitivity**: Systematically vary the number of shared heads (m) in the fine-tuning setup and measure the trade-off between load balancing effectiveness and performance preservation.

3. **Cross-Mechanism Comparison**: Train three identical 1B models from scratch using Vanilla, Sink, and Gated attention mechanisms, all with auxiliary loss. Compare not only final performance but also training dynamics, head utilization patterns, and computational overhead.