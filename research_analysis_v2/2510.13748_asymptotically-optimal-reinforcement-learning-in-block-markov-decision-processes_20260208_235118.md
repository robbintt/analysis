---
ver: rpa2
title: Asymptotically optimal reinforcement learning in Block Markov Decision Processes
arxiv_id: '2510.13748'
source_url: https://arxiv.org/abs/2510.13748
tags:
- lemma
- follows
- proof
- then
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of reinforcement learning in
  large observation spaces where the transition dynamics are determined by latent
  states. The authors propose a two-phase algorithm for Block Markov Decision Processes
  (BMDPs) that first learns the latent structure through random exploration, then
  switches to an optimism-guided strategy adapted to the uncovered structure.
---

# Asymptotically optimal reinforcement learning in Block Markov Decision Processes

## Quick Facts
- arXiv ID: 2510.13748
- Source URL: https://arxiv.org/abs/2510.13748
- Reference count: 40
- Primary result: Achieves O(√T + n) regret on Block Markov Decision Processes by learning latent structure then exploiting it

## Executive Summary
This paper tackles reinforcement learning in environments with large observation spaces where observations are noisy emissions from a smaller set of latent states. The authors develop a two-phase algorithm that first learns the latent structure through random exploration, then uses this structure to achieve asymptotically optimal regret. The key insight is that by recovering the underlying block structure, the algorithm can aggregate visitation counts and reduce exploration bonuses, dramatically improving performance compared to treating each observation as unique.

The theoretical contribution establishes that this approach achieves O(√T + n) regret on a large class of BMDPs, improving upon the best prior bound of O(√T + n²). The authors also prove this bound is optimal - no algorithm can uniformly achieve lower regret on this class of problems. The method leverages recent advances in clustering techniques to efficiently recover latent structure, then adapts optimism-based exploration to exploit this structure through tighter confidence bounds.

## Method Summary
The algorithm operates in two distinct phases. Phase 1 uses a uniform random policy for Θclust transitions to gather data about the transition dynamics between contexts. This data is used to estimate a decoding function ĥ that maps each context to its latent state through spectral clustering followed by iterative likelihood improvement. Phase 2 switches to an optimism-based planner that computes Q-values using aggregated transition counts at the latent state level, adding exploration bonuses that are smaller than those used for individual contexts. The algorithm treats the latent states as the effective state space for planning, achieving the standard √T regret bound on this reduced space while paying only a linear n cost upfront.

## Key Results
- Achieves O(√T + n) regret on BMDPs, improving upon prior O(√T + n²) bound
- Proves this bound is optimal - no algorithm can uniformly achieve lower regret
- Shows theoretical advantage over standard approaches through numerical experiments
- Method works by reducing effective state space from n to S through clustering

## Why This Works (Mechanism)

### Mechanism 1: Spectral Recovery of Latent Structure
Random exploration generates transition data that reveals the latent block structure of the BMDP if the environment is sufficiently regular. The agent employs a uniform random policy to populate a transition count matrix between contexts. Under the assumption of uniform reachability, this matrix approximates the structure of a Stochastic Block Model (SBM). The algorithm applies spectral clustering to this matrix to obtain an initial decoding function ĥ₀, which is then refined via a likelihood-based iterative improvement step to minimize misclassification. The core assumption is that the BMDP is η-reachable (all states are visited roughly equally) and I-identifiable (latent states are sufficiently distinct).

### Mechanism 2: Variance Reduction via State Aggregation
Aggregating visitation counts at the latent state level (rather than the context level) yields tighter exploration bonuses and lower regret. Once the latent structure ĥ is estimated, the algorithm computes maximum likelihood estimators for transition probabilities and emission kernels using aggregated data for each latent state. The exploration bonus b̂ₖ(s,a) is calculated based on the visitation count of the latent state s. This reduces the variance of the estimates compared to treating every context independently, effectively shrinking the size of the state space from n to S for the purpose of confidence bounds. The core assumption is that the decoding function ĥ recovered in Phase 1 is exact (or has sufficiently low error) so that aggregated statistics are not biased.

### Mechanism 3: Phase-Specific Regret Minimization
Decoupling the learning process into a "structure learning" phase and a "policy optimization" phase matches the asymptotically optimal regret bound. The algorithm accepts a fixed, upfront regret cost linear in the number of contexts n during Phase 1 (random exploration). In exchange, Phase 2 operates on a reduced MDP of size S, achieving the standard regret bound of Õ(√T) rather than the Õ(√nT) typical of large state spaces. The core assumption is that the duration of Phase 1 Θclust is set correctly (sufficient to cluster but not excessive) and T is large enough for the asymptotic benefits to outweigh the linear startup cost.

## Foundational Learning

- **Concept: Block Markov Decision Processes (BMDPs)**
  - Why needed here: This is the fundamental assumption that observations are "emissions" from a smaller set of latent states. Without this structure, the paper's dimensionality reduction is impossible.
  - Quick check question: Can you explain how the transition dynamics P(y|x,a) depend only on the latent state of x, not x itself?

- **Concept: Optimism-in-the-Face-of-Uncertainty (UCB)**
  - Why needed here: Phase 2 relies on UCB bonuses added to rewards to encourage exploration of under-visited state-action pairs. The paper modifies this by applying bonuses to latent states.
  - Quick check question: Why does adding a bonus proportional to √(1/N(s,a)) to the reward ensure the agent explores optimally?

- **Concept: Spectral Clustering**
  - Why needed here: The initial estimation of the latent structure (Phase 1) depends on spectral methods (SVD of transition matrices). Understanding why eigenvalues/eigenvectors reveal clusters is key to the algorithm's "warm start."
  - Quick check question: In the context of this paper, what property of the transition matrix allows the top S singular vectors to align with the latent states?

## Architecture Onboarding

- **Component map:** Environment -> Phase 1 Controller (Uniform Random) -> Clustering Engine (Spectral + Iterative) -> Phase 2 Controller (UCBVI) -> Value Estimator (Aggregated Counts)
- **Critical path:** The Clustering Engine is the critical dependency. The accuracy of ĥ completely determines the validity of the Phase 2 Value Estimator. If ĥ is garbage, the aggregated transition probabilities are biased, and the UCB bonuses are meaningless.
- **Design tradeoffs:**
  - Threshold Θclust: You must tune the duration of Phase 1. If too short, ĥ is inaccurate (high error in Phase 2). If too long, you waste time on random exploration (high linear regret O(n)).
  - Assumption Rigidity: The code assumes strict η-reachability. If the environment has "dead-end" states or disconnected components, the uniform exploration of Phase 1 fails to collect necessary data.
- **Failure signatures:**
  - Stagnant Phase 2 Regret: If regret in Phase 2 does not scale as √T, it suggests the latent structure was misidentified, causing the "aggregated" bonuses to be incorrectly scaled.
  - Cluster Instability: If the spectral clustering output fluctuates wildly with small data changes, the identifiability assumption (I) is likely violated.
- **First 3 experiments:**
  1. Synthetic Validation: Generate a BMDP with known S and f. Run Phase 1 with varying Θclust and plot the misclassification rate of ĥ versus n to verify the Õ(n) scaling.
  2. Baseline Comparison: Compare the two-phase algorithm against a standard UCBVI algorithm that treats every context as unique. Plot total regret over T to visualize the "crossover point" where the BMDP algorithm becomes superior.
  3. Ablation on Identifiability: Systematically reduce the separation between latent states (decrease I) and observe the degradation in clustering accuracy and the resulting explosion in Phase 2 regret.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the regret analysis and algorithm be extended to BMDPs with infinite or continuous context spaces?
- Basis in paper: Section 1.5.1 discusses function approximation typically used for infinite spaces but notes the current method relies on tabular approaches and finite spaces.
- Why unresolved: The theoretical guarantees depend on properties like η-reachability and finite context identifiability which rely on discrete probability spaces.
- What evidence would resolve it: A theoretical framework integrating function approximation that maintains the regret bounds for continuous observation distributions.

### Open Question 2
- Question: Can an online (single-phase) algorithm achieve similar asymptotic optimality on this class of BMDPs?
- Basis in paper: Section 5.1 states the two-phase approach was adopted "To avoid this source of approximation error" associated with structural uncertainty.
- Why unresolved: The authors suggest that capturing uncertainty about the latent structure within exploration bonuses is difficult for standard online optimism-based strategies.
- What evidence would resolve it: An algorithm that dynamically balances structure learning and exploitation without a dedicated initial phase, achieving Õ(√T + n) regret.

### Open Question 3
- Question: Does the asymptotic optimality hold if the reward function is unknown?
- Basis in paper: Section 2.4 explicitly defines the episodic learning algorithm assuming the reward function r is known and provided as input.
- Why unresolved: The analysis focuses solely on the regret arising from learning the transition dynamics and latent structure, ignoring potential errors from reward estimation.
- What evidence would resolve it: An analysis incorporating reward estimation errors into the regret bound without worsening the leading-order terms.

## Limitations
- Algorithm depends critically on identifiability and reachability assumptions; if latent states are not sufficiently distinct or exploration fails, clustering fails and causes unbounded regret
- Switching threshold Θclust is a source of fragility with no explicit guidance beyond asymptotic bounds
- Numerical experiments provide limited validation with highly structured synthetic environments and narrow parameter ranges

## Confidence
- **High Confidence:** The regret decomposition into O(√T) + O(n) is mathematically sound and the lower bound proof is rigorous
- **Medium Confidence:** The asymptotic optimality claim holds under stated assumptions, but practical performance heavily depends on proper tuning of Θclust and environment satisfying regularity conditions
- **Low Confidence:** The numerical experiments provide limited validation with highly structured synthetic environments and narrow parameter range

## Next Checks
1. **Clustering Robustness:** Systematically vary the separation parameter I and measure the misclassification rate of ĥ versus the resulting Phase 2 regret. Identify the failure threshold where clustering accuracy breaks down.
2. **Threshold Sensitivity:** Run experiments with multiple values of Θclust (under, at, and over the theoretical bound) to map the tradeoff between clustering accuracy and linear regret cost.
3. **Non-Stationary Stress Test:** Modify the synthetic environment to introduce rare "dead-end" states or disconnected components. Verify whether the uniform exploration of Phase 1 can still collect sufficient data for reliable clustering.