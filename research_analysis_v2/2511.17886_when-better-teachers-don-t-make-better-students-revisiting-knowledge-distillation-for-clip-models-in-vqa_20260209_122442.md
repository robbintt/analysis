---
ver: rpa2
title: 'When Better Teachers Don''t Make Better Students: Revisiting Knowledge Distillation
  for CLIP Models in VQA'
arxiv_id: '2511.17886'
source_url: https://arxiv.org/abs/2511.17886
tags:
- performance
- multimodal
- vision
- teacher
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines whether larger CLIP-style teacher models consistently
  improve student performance in multimodal tasks. While existing knowledge distillation
  literature suggests stronger teachers yield better students, this study finds that
  in vision-language tasks, larger teachers often fail to improve student models and
  sometimes even degrade performance.
---

# When Better Teachers Don't Make Better Students: Revisiting Knowledge Distillation for CLIP Models in VQA

## Quick Facts
- arXiv ID: 2511.17886
- Source URL: https://arxiv.org/abs/2511.17886
- Authors: Pume Tuchinda; Parinthapat Pengpun; Romrawin Chumpu; Sarana Nutanong; Peerat Limkonchotiwat
- Reference count: 40
- Key outcome: Larger CLIP-style teacher models often fail to improve student performance in multimodal VQA tasks and sometimes degrade performance.

## Executive Summary
This study systematically examines whether larger CLIP-style teacher models consistently improve student performance in multimodal vision-language tasks. While knowledge distillation literature suggests stronger teachers yield better students, this research finds that in vision-language tasks, larger teachers often fail to improve student models and sometimes even degrade performance. The authors conduct extensive experiments across multiple teacher-student configurations, revealing that students struggle to align with larger teachers' representations, as measured by Centered Kernel Alignment (CKA). They analyze factors like training duration, loss functions, and dataset choices, finding that longer training helps only in single-domain tasks like ImageNet classification but not in multimodal benchmarks.

## Method Summary
The study employs a two-stage pipeline: (1) CLIP-KD distillation on CC12M dataset with multi-objective loss (CLIP+FD+ICL+CKD), 32 epochs, lr=5e-4, batch=256; (2) VLM training via TinyLLaVA/Bunny with frozen vision encoder, trained adapter/LLM, 1 epoch each for pretrain+SFT. The evaluation uses a comprehensive benchmark suite including ImageNet-1k for classification and Cambrian-1 multimodal benchmarks (MMEp, MMBench, GQA, ScienceQA, etc.), with CKA analysis measuring representation alignment between teacher and student models.

## Key Results
- Larger CLIP teachers (SigLIP-so400m, DFN-ViT-L, CLIP-ViT-L) consistently fail to improve over smaller teachers (DFN-ViT-B, CLIP-ViT-B) for multimodal benchmarks
- Students struggle to align with larger teachers' representations, measured by decreasing CKA values as teacher size increases
- Longer training helps only in single-domain tasks like ImageNet classification but not in multimodal benchmarks
- Current KD loss functions fail to scale to more performant teachers on both ImageNet and multimodal benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Representation Alignment Through Feature Mimicry
Knowledge distillation effectiveness depends on student capacity to align with teacher representations, measured via Centered Kernel Alignment (CKA). Distillation transfers knowledge by training students to mimic teacher outputs and intermediate representations. When teacher representations exceed student capacity, alignment degrades, causing performance drops. Evidence shows that using smaller CLIP-ViT-B as teacher results in higher CKA while larger CLIP-ViT-L teacher results in lower CKA, with direct correspondence between CKA drops and performance decreases across all evaluated benchmarks.

### Mechanism 2: Task-Specific Optimization Divergence
Current KD loss functions optimize for vision-only tasks (classification), not multimodal reasoning tasks. Distillation objectives derived from image classification benchmarks produce representations that transfer poorly to vision-language tasks requiring cross-modal reasoning. Evidence shows ImageNet accuracy increases from ~26-30% to >40% over 96 epochs while multimodal benchmarks stay flat at 30-32%, demonstrating that existing loss functions fail to scale to more performant teachers on multimodal benchmarks.

### Mechanism 3: Representational Capacity Mismatch
Larger vision encoders develop feature distributions that smaller student architectures cannot effectively approximate regardless of training duration. Teacher models with ~428M parameters (SigLIP-so400m) create distributed feature spaces optimized for their scale; students with ~5.5M parameters lack expressivity to capture these patterns. Performance gaps widen from 3.70 to 8.38 when scaling from CLIP-ViT-B to SigLIP-so400m teachers, confirming that large vision encoders develop complex feature spaces that smaller student models cannot approximate effectively.

## Foundational Learning

- Concept: **Knowledge Distillation (KD) Basics**
  - Why needed here: The entire paper evaluates whether standard KD methods transfer to multimodal settings; understanding what KD normally optimizes for (soft labels, feature matching) is prerequisite.
  - Quick check question: Can you explain why soft teacher outputs might contain more information than hard labels for student learning?

- Concept: **CLIP Architecture and Vision-Language Alignment**
  - Why needed here: The study distills CLIP-style vision encoders; understanding how CLIP aligns image and text embeddings via contrastive learning is essential to interpret why these representations might not transfer to VQA.
  - Quick check question: What objective does CLIP optimize, and how might that differ from what VQA tasks require?

- Concept: **Centered Kernel Alignment (CKA)**
  - Why needed here: CKA is the paper's primary diagnostic tool for measuring representation similarity between teacher and student.
  - Quick check question: If two models have CKA of 0.95 on ImageNet features but 0.70 on VQA features, what does that suggest about their alignment?

## Architecture Onboarding

- Component map:
  Teacher Vision Encoder (SigLIP/DFN/CLIP, 85M-428M params) -> [CLIP-KD Framework: Feature Distillation + Contrastive Losses] -> Student Vision Encoder (ViT-T/Mobile-ViT/Swin-Tiny, 5M-53M params) -> [VLM Training: TinyLLaVA or Bunny Framework] -> LLM (Qwen2-0.5B / SmolLM-360M) -> Multimodal Benchmarks

- Critical path:
  1. Select teacher-student pair and confirm parameter ratio (start with CLIP-ViT-B → ViT-T, ~15:1 ratio)
  2. Run distillation on CC12M with CLIP-KD defaults (32 epochs, batch 256, lr 5e-4)
  3. Train VLM adapter with TinyLLaVA/Bunny (freeze vision encoder, train projection only)
  4. Evaluate on Cambrian-1 benchmark suite (MME, MMBench, GQA, ScienceQA, etc.)

- Design tradeoffs:
  - Larger teacher → more potential knowledge but harder student alignment (CKA drops)
  - Longer distillation → better ImageNet but flat multimodal gains
  - Smaller student → faster inference but lower ceiling even with perfect distillation

- Failure signatures:
  - CKA drops when scaling teacher: student cannot align representations
  - ImageNet improves but multimodal flat: loss function wrong objective
  - Adding VLM pretrain data hurts ImageNet: distribution mismatch

- First 3 experiments:
  1. **Baseline replication**: Distill ViT-T from CLIP-ViT-B and CLIP-ViT-L on CC12M (32 epochs), measure both ImageNet and average multimodal score. Confirm that larger teacher degrades multimodal performance.
  2. **CKA diagnostic**: Compute CKA between teacher and student final-layer representations on ImageNet vs. VQA validation sets. Check whether alignment gap explains performance gap.
  3. **Training duration sweep**: Extend distillation to 64 and 96 epochs for one teacher-student pair. Plot ImageNet accuracy vs. multimodal average to verify the divergence pattern claimed in Section 6.1.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can distillation loss functions be redesigned to effectively generalize beyond single-domain image classification to support complex multimodal tasks?
- Basis in paper: Section 6.2 notes the "need for new loss designs that generalize beyond single-domain benchmarks" after finding that existing losses failed to scale with larger teachers.
- Why unresolved: While existing losses (MSE, ICL, KL) work for standard vision tasks, they fail to facilitate knowledge transfer from large teachers to small students in multimodal settings.
- Evidence: A novel loss function that enables a small student model to match or exceed the multimodal performance of its larger teacher without dropping unimodal classification accuracy.

### Open Question 2
- Question: What specific optimization signals or training regimes can ensure that distillation improvements on visual benchmarks transfer to vision-language tasks?
- Basis in paper: Section 6.1 concludes that "the distillation process provides no consistent optimization signal for this task," observing that longer training aids ImageNet but not multimodal benchmarks.
- Why unresolved: Extending training duration decouples representation alignment (CKA) from downstream performance for multimodal tasks, creating an erratic optimization landscape.
- Evidence: A training schedule where monotonic improvement in representational alignment (CKA) correlates directly with monotonic improvement in VQA benchmarks.

### Open Question 3
- Question: How can the representational capacity mismatch between large teachers and small students be bridged to allow for effective knowledge alignment?
- Basis in paper: Section 5.2 identifies "alignment bottlenecks" via low CKA scores, attributing the failure of scaling to a "representational capacity mismatch" that current methods cannot overcome.
- Why unresolved: Students struggle to mimic the complex feature spaces of large teachers, often resulting in lower CKA and degraded performance compared to distilling from smaller teachers.
- Evidence: A mechanism (e.g., intermediate projections or curriculum learning) that maintains high CKA alignment between small students and large teachers, resulting in consistent VQA performance gains.

## Limitations
- The observed performance degradation with larger teachers may reflect implementation-specific issues rather than fundamental capacity mismatches
- CKA correlation with task performance remains correlational rather than proven causal
- The two-stage pipeline (distillation → VLM training) introduces compounding uncertainties where failure could occur at either stage
- Limited exploration of student architecture variations - all tested students are small vision transformers

## Confidence
- **High confidence**: Larger teachers fail to improve multimodal student performance (Section 5.1, Table 2); CKA decreases with larger teachers (Section 5.2, Figure 2); longer training helps ImageNet but not multimodal benchmarks (Section 6.1, Figures 3a/3b)
- **Medium confidence**: Current loss functions are fundamentally unsuited for multimodal distillation (Section 6.2, Table 4); representational capacity mismatch is the primary bottleneck (Section 5.2)
- **Low confidence**: The paper's conclusion that new distillation approaches are needed - this assumes no combination of existing techniques could solve the problem

## Next Checks
1. Test whether intermediate teacher checkpoints (rather than final weights) provide better distillation signals - measure CKA and performance across training epochs to identify optimal distillation timing
2. Evaluate alternative loss functions specifically designed for multimodal alignment (e.g., contrastive loss between student/teacher multimodal embeddings) rather than CLIP-style vision-only losses
3. Scale up student architectures proportionally to teacher size (e.g., ViT-S/16 instead of ViT-T) to test whether capacity mismatch is truly architectural rather than representational