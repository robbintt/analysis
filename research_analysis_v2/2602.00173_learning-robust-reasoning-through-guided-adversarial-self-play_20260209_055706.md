---
ver: rpa2
title: Learning Robust Reasoning through Guided Adversarial Self-Play
arxiv_id: '2602.00173'
source_url: https://arxiv.org/abs/2602.00173
tags:
- spoll
- wfix
- reasoning
- learning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "GASP trains robust reasoning under fallible context by adversarial\
  \ self-play: a polluter corrupts local reasoning windows while an agent learns to\
  \ detect and repair under the same corrupted conditioning. To overcome scarce recoveries\
  \ early on, it adds lightweight in-distribution repair guidance\u2014imitating self-generated\
  \ fixes under deployment context\u2014yielding faster recovery learning with minimal\
  \ representation drift."
---

# Learning Robust Reasoning through Guided Adversarial Self-Play

## Quick Facts
- **arXiv ID:** 2602.00173
- **Source URL:** https://arxiv.org/abs/2602.00173
- **Reference count:** 38
- **One-line primary result:** GASP markedly improves diagnosability, recoverability, and reliability under misleading or perturbed context across 1.5B-8B models.

## Executive Summary
GASP trains robust reasoning under fallible context through adversarial self-play: a polluter corrupts local reasoning windows while an agent learns to detect and repair under the same corrupted conditioning. To overcome scarce recoveries early on, it adds lightweight in-distribution repair guidance—imitating self-generated fixes under deployment context—yielding faster recovery learning with minimal representation drift. Across four open-weight models (1.5B–8B), GASP markedly improves diagnosability, recoverability, and reliability under misleading or perturbed context, while often improving clean accuracy and self-revision success.

## Method Summary
GASP uses two-role self-play with GRPO to train reasoning models under corrupted context. The polluter generates locally coherent corruptions (~10% token windows) of reliably-solved problems, while the agent learns to diagnose and recover correct answers under polluted conditioning. In-distribution repair guidance accelerates learning by imitating self-generated fixes. Training alternates between 5 agent steps and 5 polluter steps, using only verifiable outcome rewards. The method focuses on problems the base model solves reliably, decoupling robustness from competence acquisition.

## Key Results
- **Robustness gains:** GASP markedly improves diagnosability, recoverability, and reliability under misleading or perturbed context across 1.5B-8B models
- **Curriculum effectiveness:** Adversarial corruptions induce an adaptive curriculum that strengthens robustness as the polluter discovers increasingly effective corruption patterns
- **Guidance acceleration:** In-distribution repair guidance accelerates recovery learning with minimal representational drift, often improving clean accuracy and self-revision success

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Adversarial self-play creates an adaptive curriculum that strengthens robustness to fallible context.
- **Mechanism:** The polluter learns to generate locally coherent corruptions that maximize agent failure. As the agent improves at detecting and recovering from these corruptions, the polluter must discover increasingly effective corruption patterns, progressively training stronger verification and repair behaviors.
- **Core assumption:** The base model can generate semantically meaningful corruptions (not just noise) when prompted in the polluter role.
- **Evidence anchors:**
  - [abstract] "adversarial corruptions induce an effective curriculum"
  - [section 3.5] "self-play induces an adaptive curriculum: as the agent becomes harder to fool, the polluter must discover increasingly effective corruption patterns"
  - [corpus] Related work "Towards Understanding Self-play for LLM Reasoning" explores self-play mechanisms but doesn't specifically address adversarial corruption curricula; direct corpus evidence for this specific mechanism is weak.
- **Break condition:** If polluter-generated corruptions become trivially detectable or nonsensical, the curriculum collapses. Fixed-polluter baselines (GASP-P-FIXED in Table 2) show degraded performance relative to adaptive polluter.

### Mechanism 2
- **Claim:** In-distribution repair guidance accelerates recovery learning under sparse outcome rewards while preserving prior capabilities.
- **Mechanism:** Self-generated repair snippets have higher likelihood under the current policy than teacher-generated fixes (π_θ(w_fix^θ|s_poll) ≫ π_θ(w_fix^T|s_poll)). First-order policy improvement scales linearly with this likelihood, so same-sized updates yield larger expected gains. Smaller likelihood adjustments also reduce representation drift.
- **Core assumption:** Near-orthogonality of score-function gradients for different token sequences (standard in high-dimensional parameter spaces; formalized in Assumption A.2).
- **Evidence anchors:**
  - [abstract] "in-distribution guidance accelerates recovery with minimal representational drift"
  - [section 3.4] "π_θ(w_fix^θ|s_poll) ≫ π_θ(w_fix^T|s_poll) ⇒ |∆J(w_fix^θ)| ≫ |∆J(w_fix^T)|"
  - [section 4.4, Figure 5] GASP shows smaller PCA shift and better TruthfulQA retention than teacher-guided GASP-T.
  - [corpus] No direct corpus validation of in-distribution vs. teacher guidance comparison; this appears novel.
- **Break condition:** If guidance weight λ is too high, the agent may overfit to specific repair patterns rather than learning general recovery. Paper uses λ=0.04-0.07 with linear annealing.

### Mechanism 3
- **Claim:** Training under corrupted conditioning transfers to deployment because robustness arises from learned detection-and-repair behaviors, not test-time prompting.
- **Mechanism:** The agent receives the same polluted context during training that it will encounter at deployment. Standard RLVR optimizes only final-answer correctness under clean conditioning; GASP explicitly trains the agent to diagnose inconsistencies and recover correct answers under corrupted context.
- **Core assumption:** The model already possesses base task competence (e.g., from prior RLVR or instruction tuning); training focuses on robustness, not capability acquisition.
- **Evidence anchors:**
  - [abstract] "agent learns to diagnose and recover under the same corrupted conditioning"
  - [section 3.1] "robustness arises from learning rather than from specialized test-time prompting"
  - [section 4.1] Training restricted to problems the initial checkpoint "solves reliably"—decoupling robustness from competence.
  - [corpus] "Training Reasoning Models on Saturated Problems via Failure-Prefix Conditioning" similarly conditions on failures, but uses fixed prefixes rather than adversarial generation.
- **Break condition:** If base competence is insufficient, failures may reflect inability to solve rather than susceptibility to corruption. Paper filters to "reliably solved" problems to isolate robustness.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed here:** GASP builds on GRPO for both agent and polluter updates. Understanding group-relative advantages is essential for implementing the alternating optimization.
  - **Quick check question:** Given a batch with 1 success and 63 failures (G=64), what are the normalized advantages for successful vs. failed trajectories? (Answer: a_k ≈ √(63/1) ≈ 7.9 for success, b_k ≈ -√(1/63) ≈ -0.13 for failures; Lemma A.5)

- **Concept: Verifiable Rewards**
  - **Why needed here:** GASP uses only terminal correctness rewards (no process supervision). Understanding sparse reward limitations motivates the guidance term.
  - **Quick check question:** Why does outcome-only GRPO struggle when recovery probability p_θ ≪ 1/G? (Answer: Most batches contain zero successes, yielding uninformative near-zero advantages; Section A.1)

- **Concept: Policy Gradient Variance and Distribution Mismatch**
  - **Why needed here:** The in-distribution guidance mechanism relies on understanding why off-distribution targets yield smaller effective updates per gradient step.
  - **Quick check question:** If π_θ(w_fix|s) = 0.001 and you take a gradient step to increase log-likelihood, what happens to KL divergence from the reference policy? (Answer: Large KL increase for low-probability targets; Section 3.4 and A.2)

## Architecture Onboarding

- **Component map:**
  ```
  ┌─────────────────────────────────────────────────────────────┐
  │                    Shared Model π_θ                         │
  │  ┌──────────────────┐         ┌──────────────────┐         │
  │  │    POLLUTER      │────────▶│      AGENT       │         │
  │  │  (role prompt)   │  s_poll │  (role prompt)   │         │
  │  │                  │         │                  │         │
  │  │ Input: clean ctx │         │ Input: s_poll    │         │
  │  │ Output: w_poll   │         │ Output: τ_off    │         │
  │  └──────────────────┘         └──────────────────┘         │
  │          │                            │                    │
  │          │ R_poll = 1 - I{correct}    │ R_rec = I{correct} │
  │          ▼                            ▼                    │
  │  ┌─────────────────────────────────────────────────┐      │
  │  │           GRPO + Guidance Optimizer             │      │
  │  │  J_agent = J_GRPO(R_rec) + λ·J_guide            │      │
  │  │  J_polluter = J_GRPO(R_poll)                    │      │
  │  └─────────────────────────────────────────────────┘      │
  └─────────────────────────────────────────────────────────────┘
  ```

- **Critical path:**
  1. Sample clean trajectory from reliably-solved problems (K=4 correct solutions at temperature 0.7)
  2. Extract prefix c_{0:α} and clean window w_clean (next ~10% of tokens)
  3. Polluter generates corrupted window w_poll under polluter prompt
  4. Form polluted steer s_poll = (q, c_{0:α}, w_poll)
  5. Agent generates completion τ_off under s_poll
  6. Compute terminal rewards; update agent (5 steps) then polluter (5 steps)
  7. For guidance: sample repair w_fix^θ from diagnosis prompt; add imitation term

- **Design tradeoffs:**
  | Parameter | Low Value | High Value | Paper Default |
  |-----------|-----------|------------|---------------|
  | Group size G | Higher variance, may see zero successes | Computationally expensive | 64 |
  | Guidance weight λ | Slower learning, more exploration | May overfit, less exploration | 0.04-0.07, annealed |
  | KL coefficient β | More drift from base | Constrained updates | 0 (no explicit KL) |
  | Truncation points α | Earlier corruption | Later corruption | {0, 0.25, 0.5, 0.75} |

- **Failure signatures:**
  - **Zero-success batches:** Group-relative advantages collapse to near-zero; check recovery rate p_θ and increase guidance weight or group size.
  - **Representation drift:** Large PCA shift on probe distributions; reduce guidance weight or add KL regularization.
  - **Polluter collapse:** Corruptions become nonsensical; verify polluter prompt constraints and check that agent isn't trivially robust.
  - **No clean accuracy improvement:** May indicate overfitting to corruption patterns; verify training on diverse α values.

- **First 3 experiments:**
  1. **Baseline sanity check:** Implement fixed-polluter GASP (no polluter learning) to verify that adaptive corruptions matter. Expect GASP-P-FIXED to underperform full GASP (Table 2 shows ~15-20% avg gap).
  2. **Guidance ablation:** Compare GASP vs. ASP (outcome-only, no guidance) vs. GASP-T (teacher guidance) under matched rollout budget. Expect ASP < GASP-T < GASP in learning speed (Figure 6).
  3. **Representation drift measurement:** Train GASP and GASP-T on same data; measure PCA shift on held-out probe (TruthfulQA/CommonsenseQA). Expect GASP-T to show larger d(*) and larger accuracy drop (Figure 5).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does GASP scale to significantly larger models (e.g., 70B+ parameters) and does the in-distribution guidance advantage persist?
- Basis in paper: [explicit] Experiments are limited to "four open-weight models (1.5B–8B)."
- Why unresolved: Larger models may have different representation stability properties and recovery capabilities that could change the relative effectiveness of self-guided versus teacher-guided repairs.
- What evidence would resolve it: Run GASP and baselines on models of 70B, 405B, etc., measuring recoverability, representation drift, and clean accuracy retention.

### Open Question 2
- Question: Does GASP transfer to non-mathematical reasoning domains (e.g., code, logical reasoning, scientific QA)?
- Basis in paper: [inferred] Training uses only "7.5k problems from the MATH training split" and evaluation focuses on GSM8K, MR-GSM8K, and CommonsenseQA.
- Why unresolved: Mathematical reasoning has verifiable ground-truth answers; domains without clear outcome verifiers may not admit the same outcome-only training.
- What evidence would resolve it: Apply GASP to code debugging benchmarks (e.g., MBPP with corrupted partial solutions) or multi-hop reasoning tasks with perturbed context.

### Open Question 3
- Question: What are the failure modes of the adversarial self-play curriculum, and when does the polluter-agent game collapse or plateau?
- Basis in paper: [explicit] The paper notes "self-play induces an adaptive curriculum" but does not analyze convergence or failure modes of this dynamic.
- Why unresolved: Self-play games can exhibit cycles, collapse to trivial solutions, or fail to improve if one role becomes too strong too quickly.
- What evidence would resolve it: Track polluter success rates and agent recovery rates over extended training (10× longer); analyze corruption patterns to detect whether the polluter explores meaningfully or stalls.

## Limitations

- **Underspecified training duration:** Total training steps/epochs not specified—only hyperparameters given.
- **Custom evaluation benchmarks:** Improvements rely heavily on MR-GSM8K and MHPP benchmarks using local perturbations rather than learned adversarial corruptions from training.
- **Narrow domain focus:** Training uses only mathematical reasoning problems, limiting generalizability to other reasoning domains.

## Confidence

- **High confidence**: The core mechanism of adversarial self-play creating adaptive curricula is well-supported by ablation studies (GASP-P-FIXED vs GASP) and theoretical analysis of group-relative advantages.
- **Medium confidence**: The in-distribution guidance acceleration claim is supported by likelihood comparisons and PCA shift measurements, but lacks direct comparison to alternative guidance strategies beyond teacher guidance.
- **Medium confidence**: The transferability claim (robustness under corrupted conditioning transferring to deployment) is demonstrated on synthetic perturbations but would benefit validation on real-world fallible contexts.

## Next Checks

1. **Cross-contamination validation**: Verify that improvements on MR-GSM8K/MHPP don't simply reflect overfitting to their specific corruption patterns by testing on held-out perturbation types.
2. **Guidance generalization test**: Compare GASP's in-distribution guidance against a variant using random repair snippets sampled from a held-out corpus to isolate the benefit of on-policy vs off-policy guidance.
3. **Real-world robustness probe**: Evaluate trained models on naturally occurring fallible contexts (e.g., noisy web data, user prompts with implicit assumptions) rather than synthetic corruptions.