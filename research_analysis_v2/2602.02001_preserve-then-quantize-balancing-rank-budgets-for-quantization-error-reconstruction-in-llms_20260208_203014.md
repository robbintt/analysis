---
ver: rpa2
title: 'Preserve-Then-Quantize: Balancing Rank Budgets for Quantization Error Reconstruction
  in LLMs'
arxiv_id: '2602.02001'
source_url: https://arxiv.org/abs/2602.02001
tags:
- quantization
- rank
- error
- reconstruction
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel approach to address the inefficiency
  of quantization error reconstruction (QER) in low-bit quantization of large language
  models (LLMs). The core idea is to explicitly allocate a fixed rank budget between
  preserving dominant structure and reconstructing quantization error, rather than
  solely focusing on error reconstruction as previous methods do.
---

# Preserve-Then-Quantize: Balancing Rank Budgets for Quantization Error Reconstruction in LLMs

## Quick Facts
- arXiv ID: 2602.02001
- Source URL: https://arxiv.org/abs/2602.02001
- Reference count: 40
- Primary result: SRR improves 2-bit QPEFT GLUE accuracy by 5.9 percentage points and PTQ perplexity by up to 12.2%

## Executive Summary
This paper addresses the inefficiency of quantization error reconstruction (QER) in low-bit quantization of LLMs by proposing a "preserve-then-quantize" framework. Rather than using all available rank budget solely for error reconstruction as previous methods do, SRR explicitly allocates rank between preserving dominant singular directions and reconstructing quantization error. The method uses a theory-guided criterion to select the optimal rank split, showing consistent improvements in both post-training quantization (PTQ) and quantized parameter-efficient fine-tuning (QPEFT) settings across multiple model scales.

## Method Summary
SRR operates by first computing an activation-aware scaling matrix S from calibration data, then extracting the top-k singular subspace of the scaled weight SW via truncated SVD. This dominant structure is preserved as L₁R₁, while only the residual W - L₁R₁ is quantized. The remaining rank budget reconstructs quantization error through another SVD on the error matrix. In QPEFT settings, gradient scaling is applied to the preserved directions to stabilize fine-tuning. The rank split k⋆ is selected via a lightweight random-matrix proxy that avoids expensive per-k quantization and SVD enumeration.

## Key Results
- 12.2% perplexity reduction in PTQ across LLaMA-2/3.1 models (7B-70B)
- 5.9 percentage-point average gains on GLUE tasks under 2-bit QPEFT
- Consistent performance across model scales from 1B to 70B parameters
- SRR shows advantages persist without gradient scaling, indicating rank-budget allocation as primary mechanism

## Why This Works (Mechanism)

### Mechanism 1
Allocating rank budget to preserve dominant singular directions before quantization improves reconstruction over using all rank for error correction. SRR extracts the top-k subspace of the activation-scaled weight SW via truncated SVD, preserves it as L₁R₁, then quantizes only the residual W − L₁R₁. The remaining rank r−k reconstructs quantization error. This prevents quantization from corrupting high-energy directions. Core assumption: Transformer weights in the scaled space SW are highly anisotropic; dominant singular directions carry disproportionately large signal energy.

### Mechanism 2
A lightweight random-matrix proxy can identify near-optimal rank split k⋆ without enumerating quantization errors. Under assumptions of constant relative quantization error scale and k-insensitive normalized error spectrum, the optimal split minimizes ρₖ(SW)·ρᵣ₋ₖ(SE), where E is a random probe matrix. This avoids expensive per-k quantization and SVD. Core assumption: Quantization error behaves like unstructured random noise after normalization, and relative error energy is approximately constant for a fixed quantizer/bitwidth.

### Mechanism 3
In QPEFT, attenuating gradients along preserved directions stabilizes fine-tuning and improves final accuracy. Preserved component L₁R₁ has much larger singular values than residual L₂R₂. Without scaling, updates over-modify dominant directions or under-utilize residual capacity. Gradient scaling (∇L₁R₁ ← γ·∇L₁R₁, γ∈(0,1)) regularizes this imbalance. Core assumption: The preserved subspace should remain stable; adaptation should primarily occur in residual directions.

## Foundational Learning

- **Truncated SVD and low-rank approximation**: SRR's core operation is extracting top-k singular directions via SVD and using remaining rank for reconstruction. Quick check: Given a matrix A with singular values [10, 5, 1, 0.1, 0.01], what fraction of Frobenius norm energy is captured by rank-2 approximation?

- **Activation-aware scaling in quantization**: SRR operates in scaled space SW; understanding why scaling matters is critical for interpreting the rank allocation. Quick check: Why would scaling by activation statistics change which singular directions are "dominant"?

- **LoRA-style parameter-efficient fine-tuning**: SRR extends naturally to QPEFT; understanding LoRA's W ≈ Q + LR parameterization is prerequisite. Quick check: In LoRA, why does freezing the base model and training only low-rank adapters reduce overfitting risk?

## Architecture Onboarding

- **Component map**: Calibration data → Scaling matrix S → Random probe E → Rank selection k⋆ → SVD on SW → Preserve L₁R₁ → Quantize residual → SVD on quantization error → Reconstruct L₂R₂ → Output Q + [L₁, L₂], [R₁; R₂]

- **Critical path**: 1) Scaling matrix computation (dominant runtime cost), 2) Two randomized SVDs (SW and SE) using r≪min(m,n) power iterations, 3) Rank selection via closed-form criterion, 4) Actual quantization and final SVD reconstruction

- **Design tradeoffs**: Larger k⋆ better protects dominant structure but leaves less capacity for error reconstruction; smaller k⋆ provides more error-reconstruction capacity but risks corrupting high-energy directions. Paper shows projection-type variation: Q/K get higher k⋆ than V, reflecting structural differences.

- **Failure signatures**: Selected k⋆ varies wildly across random seeds → proxy assumption violated; check quantizer structure. QPEFT training unstable despite gradient scaling → check magnitude ratio between L₁R₁ and L₂R₂. PTQ perplexity worse than baseline → possible that SW is isotropic; verify spectral decay.

- **First 3 experiments**: 1) Plot reconstruction error vs k for single layer and verify SRR's k⋆ approximates true minimum. 2) Run SRR with S=I vs activation-aware S to quantify how scaling changes spectral decay and optimal k⋆ distribution. 3) Fix 2-bit quantization, vary γ∈{0, 0.1, 0.5, 1.0} on GLUE task to confirm U-shaped performance curve.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several implications emerge from the methodology:

1. Does the spectral anisotropy assumption required for the "preserve-then-quantize" split hold effectively for Mixture-of-Experts (MoE) or State Space Models (SSMs)?

2. Can the rank split k be dynamically updated during QPEFT to improve adaptation, rather than remaining fixed after initialization?

3. Does the SRR framework maintain its superiority in weight-activation quantization scenarios where activation outliers introduce noise that scales differently than weights?

## Limitations
- Method relies on specific quantization error structure assumptions that may not hold for all quantizers
- Performance depends on activation scaling matrix computation, which can be expensive
- Gradient scaling in QPEFT requires hyperparameter tuning and may not generalize to all fine-tuning scenarios

## Confidence

- **High confidence**: Empirical results showing consistent perplexity and accuracy improvements across multiple model scales and tasks
- **Medium confidence**: Theoretical framework and rank selection criterion, which align with classical quantization noise theory but lack extensive external validation
- **Low confidence**: Generalizability beyond MXINT quantizer to other quantization schemes with different error characteristics

## Next Checks

1. **Spectral Decay Verification**: Plot the singular value spectrum of SW for multiple layers across different models to empirically confirm the "sharp spectral decay" assumption underlying SRR's mechanism.

2. **Quantizer Sensitivity Test**: Apply SRR with alternative quantization schemes (e.g., symmetric/affine 4-bit) to verify whether the method's effectiveness depends specifically on MXINT's quantization noise characteristics.

3. **Error Structure Analysis**: Compare the actual low-rank structure of quantization error E_k against the random-noise approximation used in the proxy criterion by computing the reconstruction error for both SRR-selected k⋆ and the true optimal k (via exhaustive search on a subset of layers).