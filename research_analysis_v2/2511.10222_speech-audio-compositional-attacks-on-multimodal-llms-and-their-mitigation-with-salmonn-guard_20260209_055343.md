---
ver: rpa2
title: Speech-Audio Compositional Attacks on Multimodal LLMs and Their Mitigation
  with SALMONN-Guard
arxiv_id: '2511.10222'
source_url: https://arxiv.org/abs/2511.10222
tags:
- audio
- speech
- harmful
- attacks
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces SACRED-Bench, a novel benchmark for evaluating
  the robustness of multimodal large language models (LLMs) against complex audio-based
  jailbreaking attacks. Unlike prior methods relying on noise optimization or adversarial
  training, SACRED-Bench exploits speech-audio composition mechanisms: (a) speech
  overlap and multi-speaker dialogue to embed harmful prompts beneath benign speech;
  (b) speech-audio mixture to imply unsafe intent via non-speech audio alongside benign
  speech; and (c) diverse spoken instruction formats (e.g., open-ended QA, yes/no)
  to evade text-only filters.'
---

# Speech-Audio Compositional Attacks on Multimodal LLMs and Their Mitigation with SALMONN-Guard

## Quick Facts
- arXiv ID: 2511.10222
- Source URL: https://arxiv.org/abs/2511.10222
- Authors: Yudong Yang; Xuezhen Zhang; Zhifeng Han; Siyin Wang; Jimin Zhuang; Zengrui Jin; Jing Shao; Guangzhi Sun; Chao Zhang
- Reference count: 10
- Primary result: Introduces SACRED-Bench benchmark and SALMONN-Guard defense, reducing attack success rate from 66% to 20%

## Executive Summary
This paper presents SACRED-Bench, a novel benchmark for evaluating multimodal LLM robustness against speech-audio composition attacks, and SALMONN-Guard, the first safeguard LLM that jointly inspects speech, audio, and text to mitigate these attacks. Unlike prior methods relying on noise optimization, SACRED-Bench exploits complex audio-based jailbreaking through speech overlap, multi-speaker dialogue, and speech-audio mixtures. Experiments demonstrate that even state-of-the-art models like Gemini 2.5 Pro exhibit a 66% attack success rate, which SALMONN-Guard reduces to 20% through joint audio-text safety inspection.

## Method Summary
The method involves creating SACRED-Bench using synthetic data generation with GPT-4o and ChatTTS, then training SALMONN-Guard (a LoRA-adapted Qwen2.5-Omni-7B model) on this dataset through two-stage supervised fine-tuning. The defense model processes interleaved audio-text tokens to perform binary safety classification, reducing attack success rates by learning to detect subtle acoustic malicious signals alongside textual content. The training uses cross-entropy loss with synthetic harmful and benign samples, including multi-speaker dialogue scenarios.

## Key Results
- Proprietary models like Gemini 2.5 Pro exhibit 66% attack success rate on SACRED-Bench
- SALMONN-Guard reduces attack success rate to 20% through joint audio-text inspection
- "Contextual Audio Cues" (non-speech audio) show highest vulnerability at 88.56% ASR
- Benign audio accuracy reaches 100% while maintaining safety performance

## Why This Works (Mechanism)

### Mechanism 1: Speech Overlap and Multi-Speaker Dialogue
- **Claim:** Malicious instructions embedded within benign speech bypass safety filters through acoustic masking and stream fusion
- **Mechanism:** Harmful speech is attenuated, accelerated, or nested within benign dialogue, exploiting limitations in audio stream disentanglement and text-only guardrail interpretation
- **Core assumption:** Target models process audio holistically but safety alignment relies primarily on explicit textual instructions or dominant speech signals
- **Evidence anchors:** Abstract mentions "speech overlap and multi-speaker dialogue... embeds harmful prompts beneath or alongside benign speech"; section 3.2.1 describes acoustic masking via volume and speed adjustments

### Mechanism 2: Speech-Audio Mixtures
- **Claim:** Decoupling harmful intent (non-speech audio) from benign instruction (speech) causes cross-modal blindness
- **Mechanism:** Innocuous spoken queries paired with harmful background audio bypass text-centric filters that prioritize transcribable speech while ignoring non-speech context
- **Core assumption:** Safety training over-weights linguistic semantic content and under-weights auditory scene context
- **Evidence anchors:** Abstract highlights "speech-audio mixture implies unsafe intent via non-speech audio"; section 5.2 shows "Contextual Audio Cues" have highest ASR at 88.56% on Gemini 2.5 Pro

### Mechanism 3: Joint Audio-Text Safety Classification
- **Claim:** SALMONN-Guard mitigates attacks by performing safety classification on joint audio-text embedding space
- **Mechanism:** Fine-tuning multimodal LLM on adversarial audio-text pairs enables learning to attend to subtle acoustic malicious signals and cross-reference with text prompts
- **Core assumption:** Synthetic training data covers acoustic variations of real-world attacks sufficiently for generalization
- **Evidence anchors:** Abstract proposes "SALMONN-Guard... jointly inspects speech, audio, and text, reducing ASR to 20%"; section 4.2 details architecture based on Qwen2.5-Omni-7B and LoRA fine-tuning

## Foundational Learning

- **Concept: The "Semantic Gap" in Multimodal Safety**
  - **Why needed here:** To understand why text-only guardrails fail when instruction is benign but context is malicious
  - **Quick check question:** If a user asks "Can you help me?" in calm voice while break-in sounds play in background, should model refuse? Why would text-only guard fail?

- **Concept: Audio Source Separation and Diarization**
  - **Why needed here:** To comprehend difficulty LLMs face in "Speech Overlap" attacks where distinguishing malicious speaker from benign one is non-trivial
  - **Quick check question:** How does increasing "overlap" duration or accelerating "malicious payload" speed affect signal-to-noise ratio available to LLM's encoder?

- **Concept: Cross-Modal Alignment in LLMs**
  - **Why needed here:** To grasp how SALMONN-Guard worksâ€”mapping audio features to text embedding space allows LLM to "reason" about sounds using text-based safety training
  - **Quick check question:** In multimodal LLM, what is the role of "aligner module" during fine-tuning process?

## Architecture Onboarding

- **Component map:** Raw audio waveform and text prompt -> Pre-trained audio encoder -> SALMONN-Guard (LoRA-adapted Qwen2.5-Omni-7B) -> Interleaved audio-text tokens -> Binary safety classification
- **Critical path:** Efficacy rests on LoRA adapters applied to audio encoder and LLM; if not trained on sufficient "Speech-Audio Mixture" examples, model reverts to base behavior
- **Design tradeoffs:**
  - Synthetic vs. Real Data: Uses GPT-4o + ChatTTS for training data (scales well) but risks "sim-to-real" gap
  - Standalone vs. Interceptor: Can act as standalone classifier or refusal generator (better UX but adds latency)
- **Failure signatures:**
  - High ASR on Overlap (47%+): Indicates model struggles with source separation
  - False Positives on Background Noise: Strict guard might flag benign speech in noisy environments
- **First 3 experiments:**
  1. Baseline Validation: Run SACRED-Bench test set against deployed LLM to confirm "Contextual Audio Cues" vulnerability
  2. Guard Integration Test: Deploy SALMONN-Guard as pre-filter; measure ASR drop and latency overhead
  3. Generalization Check: Test against perturbation-based attacks (e.g., "Speech Insertion" from JALMBench) to verify defense generalizes beyond specific composition attacks

## Open Questions the Paper Calls Out

- **Open Question 1:** Does SALMONN-Guard deployment degrade general utility of multimodal LLMs on standard non-adversarial audio tasks?
  - Basis: Authors demonstrate high accuracy on safety metrics but don't evaluate impact on general audio understanding benchmarks
  - Why unresolved: Paper focuses strictly on safety metrics, leaving trade-off with general model utility unexplored
  - What evidence would resolve it: Evaluation on general audio understanding benchmarks to measure false refusal rate and task performance impact

- **Open Question 2:** Can residual 20% ASR be further reduced without inducing over-refusal?
  - Basis: While guard reduces ASR significantly, 20% success rate remains, indicating subset of attacks still bypass defense
  - Why unresolved: Paper doesn't analyze characteristics of successful attacks that evade guard or explore tighter safety tuning leading to over-refusal
  - What evidence would resolve it: Detailed error analysis of remaining 20% successful attacks and Pareto curve analysis of safety versus utility trade-offs

- **Open Question 3:** Do "Speech Overlap" attacks primarily exploit audio encoder's source separation failures or LLM's semantic prioritization?
  - Basis: Authors hypothesize attacks work through "auditory stream fusion" and masking but rely on empirical success rates rather than internal model probing
  - Why unresolved: Unclear if model fails to segregate audio streams at signal level or processes both but semantically prioritizes harmful "subliminal" instruction
  - What evidence would resolve it: Mechanistic interpretability studies probing attention weights and encoder outputs for overlapping benign and malicious audio streams

## Limitations
- Synthetic training data may not capture full acoustic complexity of real-world attacks, creating potential sim-to-real gap
- Evaluation methodology uses Gemini 2.5 Pro as oracle judge, introducing potential circularity with same model family vulnerabilities
- Paper doesn't report robustness against perturbed versions of same attacks or adaptive adversaries

## Confidence
- **High confidence**: Core vulnerability demonstration (66% ASR on Gemini 2.5 Pro) is methodologically sound and reproducible
- **Medium confidence**: Defense mechanism (SALMONN-Guard architecture and training procedure) is well-specified and should achieve similar performance when implemented
- **Low confidence**: Generalization to real-world attacks and robustness against adaptive adversaries remains unproven

## Next Checks
1. **Real-world data test**: Evaluate SALMONN-Guard on held-out set of real-world harmful audio (scraped from social media) rather than synthetic data to measure sim-to-real transfer
2. **Adversarial robustness**: Apply standard audio adversarial attacks (FGSM, PGD) to SACRED-Bench samples and measure whether SALMONN-Guard maintains 20% ASR reduction
3. **Cross-model generalization**: Test whether SALMONN-Guard trained on Gemini 2.5 Pro vulnerabilities also reduces ASR on other multimodal LLMs (e.g., GPT-4o, Claude) to validate defense's model-agnostic properties