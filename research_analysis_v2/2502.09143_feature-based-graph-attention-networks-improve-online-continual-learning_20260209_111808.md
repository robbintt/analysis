---
ver: rpa2
title: Feature-based Graph Attention Networks Improve Online Continual Learning
arxiv_id: '2502.09143'
source_url: https://arxiv.org/abs/2502.09143
tags:
- graph
- learning
- feature
- attention
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Feature-based Graph Attention Networks (FGAT)
  for online continual learning in image classification. The method addresses catastrophic
  forgetting by transforming images into graph representations using hierarchical
  feature maps from a pre-trained feature extractor.
---

# Feature-based Graph Attention Networks Improve Online Continual Learning

## Quick Facts
- arXiv ID: 2502.09143
- Source URL: https://arxiv.org/abs/2502.09143
- Reference count: 30
- Primary result: FGAT achieves 26%, 6%, and 18% improvements on CIFAR10, CIFAR100, and MiniImageNet respectively compared to state-of-the-art methods

## Executive Summary
This paper introduces Feature-based Graph Attention Networks (FGAT) for online continual learning in image classification. The method transforms images into graph representations using hierarchical feature maps from a pre-trained ResNet18, then processes these graphs with a Graph Attention Network that dynamically updates node representations through learned attention weights. Comprehensive experiments on SVHN, CIFAR10, CIFAR100, and MiniImageNet demonstrate significant improvements over existing state-of-the-art methods, particularly on datasets with complex multi-task scenarios. The approach addresses catastrophic forgetting through a combination of weighted global mean pooling and rehearsal memory duplication strategies.

## Method Summary
FGAT converts images to graph representations via hierarchical feature maps from a frozen ResNet18 feature extractor. The model constructs graphs by treating each pixel location in feature maps as nodes, connecting them via spatial k-NN, and concatenating with spatial coordinates. A 2-layer GATv2 processes these graphs with learned attention weights, followed by SPN normalization and weighted global mean pooling. The approach incorporates rehearsal memory with sample duplication (×15-20) to improve past-task representation within memory constraints. Training uses Adam optimizer with combined cross-entropy and LwF distillation loss.

## Key Results
- Achieves 26%, 6%, and 18% improvements on CIFAR10, CIFAR100, and MiniImageNet respectively compared to state-of-the-art methods
- Demonstrates superior performance on complex multi-task scenarios while maintaining effectiveness on simpler datasets
- Shows effectiveness of weighted pooling and memory duplication strategies for catastrophic forgetting mitigation

## Why This Works (Mechanism)

### Mechanism 1: Feature-based Graph Construction for Multi-Scale Representation
Converting images to graph representations via hierarchical feature maps enables richer semantic encoding than raw pixel or RGB-only superpixel methods. A frozen ResNet18 extracts features from final two convolutional blocks, which are upsampled via repetition and concatenated with spatial coordinates. Nodes connect via spatial k-NN (k=8), leveraging pre-trained ImageNet features for target datasets.

### Mechanism 2: Attention-Driven Selective Aggregation for Catastrophic Forgetting Mitigation
GATv2's dynamic attention mechanism enables task-relevant feature aggregation, reducing interference from irrelevant neighborhood information. The model computes normalized importance weights and updates node representations as weighted sums of neighbor features, allowing selective preservation of critical features across tasks without explicit task boundaries.

### Mechanism 3: Weighted Global Mean Pooling + Rehearsal Duplication for Stability-Plasticity Balance
Weighted pooling and memory duplication together improve past-task representation without increasing memory budget. Weighted global mean pooling learns channel-wise importance weights for adaptive node contribution, while rehearsal sample duplication (×15-20) emulates joint training by balancing representation against current task data.

## Foundational Learning

- **Graph Neural Networks (GNNs) and Message Passing**: Essential for understanding node representation updates via neighbor aggregation in GATv2 layers. Quick check: Can you explain how a node aggregates information from its 8-nearest neighbors in a single GATv2 layer?

- **Catastrophic Forgetting in Continual Learning**: Critical for understanding why the entire paper focuses on mitigating forgetting when learning tasks sequentially. Quick check: What happens to a model's performance on Task 1 after training on Task 2 without any mitigation strategy?

- **Knowledge Distillation (Learning without Forgetting)**: Important for understanding LwF regularization that prevents model drift from previous task outputs. Quick check: How does soft-target distillation differ from hard-label cross-entropy, and why might it help continual learning?

## Architecture Onboarding

- **Component map**: Image → ResNet18 features → Graph construction → GATv2 (×2) → SPN → Weighted pooling → Classifier
- **Critical path**: Image → ResNet18 features → Graph construction → GATv2 (×2) → SPN → Weighted pooling → Classifier
- **Design tradeoffs**: Tessellated vs. non-tessellated pooling (tessellated uses fewer parameters but may lose expressiveness); duplication level vs. overfitting (higher duplication improves past-task representation but requires LwF+SPN); attention heads (3 optimal for CIFAR100, 4 for simpler datasets)
- **Failure signatures**: SVHN underperformance due to labeling inconsistencies; performance drop with >3 attention heads on CIFAR100 (over-parameterization); tessellated pooling fails on SVHN due to graph structure differences
- **First 3 experiments**: 1) Baseline on CIFAR10 with 5-task split, duplication ×15, 4 attention heads; 2) Ablation comparing pooling methods on CIFAR100; 3) Duplication level sweep on CIFAR100 with/without LwF+SPN

## Open Questions the Paper Calls Out

### Open Question 1
Can the attention mechanism be modified to suppress task-irrelevant visual artifacts, such as secondary digits in SVHN, without manual intervention? The current GATv2 implementation assigns high attention weights to these secondary numbers, causing misinterpretation of the primary subject.

### Open Question 2
Does the FGAT framework maintain stability if the pre-trained feature extractor is made trainable rather than frozen? Unfreezing the backbone could improve feature representation for specific tasks but introduces catastrophic forgetting risk at the feature extraction level.

### Open Question 3
How does the Rehearsal Memory Duplication strategy impact performance in extremely low-memory regimes compared to standard rehearsal sampling? Duplicating limited samples may cause overfitting to specific examples rather than generalizing to the class.

## Limitations
- Memory duplication strategy validated only within combined LwF+SPN regularization context
- SPN normalization loosely defined with limited implementation guidance
- Dataset-specific hyperparameter choices not theoretically grounded

## Confidence
- **High confidence**: Feature-based graph construction, GATv2 attention mechanism, overall continual learning framework
- **Medium confidence**: Weighted pooling mechanism effectiveness, memory duplication strategy synergy with regularization
- **Low confidence**: SPN normalization details, dataset-specific hyperparameter selection rationale, generalization beyond evaluated datasets

## Next Checks
1. **Ablation on SPN**: Remove SPN normalization and measure performance drop on CIFAR100 to quantify its contribution
2. **Memory duplication alone**: Test duplication strategy without LwF+SPN to determine standalone benefits vs. synergistic effects
3. **Cross-dataset feature transfer**: Evaluate FGAT with domain-specific pre-trained features rather than ImageNet-trained ResNet18