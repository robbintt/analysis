---
ver: rpa2
title: 'Multi-Party Conversational Agents: A Survey'
arxiv_id: '2505.18845'
source_url: https://arxiv.org/abs/2505.18845
tags:
- dialogue
- pages
- association
- computational
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey reviews the progress of multi-party conversational
  agents (MPCAs), systems that can engage with more than two participants at once.
  Unlike traditional two-party agents, MPCAs must interpret both semantic content
  and social dynamics.
---

# Multi-Party Conversational Agents: A Survey

## Quick Facts
- arXiv ID: 2505.18845
- Source URL: https://arxiv.org/abs/2505.18845
- Reference count: 40
- Primary result: Survey of multi-party conversational agents organized around mental state modeling, semantic understanding, and agent action modeling

## Executive Summary
This survey comprehensively reviews progress in multi-party conversational agents (MPCAs), systems designed to engage with more than two participants simultaneously. Unlike traditional two-party agents, MPCAs must interpret both semantic content and complex social dynamics. The paper organizes existing research into three interconnected themes: modeling participants' mental states (emotion, personality, engagement, dialog acts), semantic understanding (dialogue summarization, disentanglement, discourse parsing, representation learning), and agent action modeling (turn detection, addressee selection, response generation/selection). It covers traditional ML, LLM, and multi-modal methods, surveys available datasets and evaluation metrics, and identifies key research gaps.

## Method Summary
The survey synthesizes research across three methodological approaches: traditional machine learning models, large language models (LLMs), and multi-modal methods. For each approach, it examines how mental state modeling, semantic understanding, and agent action modeling are implemented. The analysis draws from 40+ references spanning different architectures including graph neural networks for speaker-utterance relationships, cross-modal attention mechanisms for multi-modal fusion, and specialized fine-tuning procedures for LLMs. The survey methodology involves systematic categorization of existing works, identification of common patterns and limitations, and synthesis of findings into actionable research directions.

## Key Results
- LLM-based approaches show promise for response generation but underperform on dialog act recognition and suffer from hallucination in summarization tasks
- Graph-structured speaker-utterance representations outperform sequential models for conversation disentanglement and semantic understanding
- Multi-modal fusion techniques improve mental state detection but lack robust fusion methods for spatial audio and face tracking
- Theory of Mind integration shows measurable performance improvements when emotion context is provided to dialog act classifiers

## Why This Works (Mechanism)

### Mechanism 1: Theory of Mind as Cross-Task Contextual Signal
- Claim: Providing mental state context (emotions, beliefs) improves downstream MPCA task performance
- Mechanism: Emotion and persona information is injected as additional context tokens or embeddings before action prediction, allowing the model to condition dialogue act or response generation on inferred participant states
- Core assumption: Mental states are partially observable from utterances and provide explanatory power for predicting speaker intentions
- Evidence anchors: LLaMA3.1 8B dialog act F1 improved from 55.01% to 55.80% with emotion context; DIAMONDs dataset explicitly targets dynamic mental modeling

### Mechanism 2: Graph-Structured Speaker-Utterance Representation
- Claim: Modeling dialogue as heterogeneous graphs of speakers, utterances, and discourse relations captures multi-party dynamics better than sequential models
- Mechanism: Graph neural networks (GNNs) propagate information across speaker nodes and utterance nodes, enabling models to track who-said-what-to-whom and maintain speaker-specific context over long conversations
- Core assumption: Conversational structure can be meaningfully represented as a graph with learnable edge types (reply-to, address, discourse relation)
- Evidence anchors: Heterogeneous discourse graphs combined with GCN have outperformed GPT-4 on conversation disentanglement

### Mechanism 3: Multi-Modal Cue Fusion for Social Signal Detection
- Claim: Integrating visual and auditory cues with text improves detection of mental states and turn dynamics
- Mechanism: Cross-modal attention mechanisms align text embeddings with visual (facial expressions, gaze) and audio (prosody, pauses) features, enabling richer representations of engagement, emotion, and turn boundaries
- Core assumption: Non-verbal cues are temporally aligned with utterances and provide complementary signal not recoverable from text alone
- Evidence anchors: Multi-modality integrated speech pauses, gaze, and listener behaviors for turn detection; Triadic VAP applies voice activity projection to multi-party turn-taking

## Foundational Learning

- **Graph Neural Networks (GNNs) for Dialogue**
  - Why needed here: Core architecture for speaker-utterance relationship modeling across all three MPCA themes
  - Quick check question: Can you explain how message passing would propagate emotional state from Speaker A's utterance node to Speaker B's response node?

- **Multi-Modal Fusion (Attention-Based)**
  - Why needed here: Required for State of Mind Modeling and turn detection where non-verbal cues are critical
  - Quick check question: What is the difference between early fusion (concatenating features) and late fusion (attention-based alignment), and when would each fail?

- **Dialogue Context Encoding (Speaker-Aware)**
  - Why needed here: All MPCA tasks require distinguishing who said what; standard LLM context windows don't natively track speaker identity across turns
  - Quick check question: How would you modify a standard transformer's positional encoding to include speaker identity?

## Architecture Onboarding

- **Component map:** Input layer (text utterances + optional audio/video features) → State of Mind encoder (emotion/persona/engagement classifiers) → Semantic Understanding module (discourse parser + disentanglement + summarization) → Agent Action decoder (turn detection → addressee selection → response generation) → Orchestration (graph structure maintained across turns)

- **Critical path:** 1. Speaker disambiguation (who is speaking now) 2. Mental state update (what is their emotional/engagement state) 3. Context aggregation (what thread/topic are they responding to) 4. Action decision (should agent respond, to whom, with what)

- **Design tradeoffs:** LLM vs. specialized models: LLMs improve response generation but underperform on DAR and suffer hallucination in summarization; Multi-modal vs. text-only: Multi-modal adds robustness for mental state detection but increases inference cost and requires aligned data; Unified vs. modular: Unified models simplify deployment but may sacrifice task-specific optimization

- **Failure signatures:** Hallucinated summaries: LLM generates plausible but unsupported details; Addressee confusion: Model fails to disambiguate threads in fast-paced multi-party chat; Modality dropout degradation: Performance collapses when video/audio is unavailable at inference

- **First 3 experiments:**
  1. **Baseline speaker tracking:** Implement MPC-BERT-style speaker embeddings on a standard LLM backbone; evaluate on emotion recognition (MELD) vs. text-only baseline
  2. **Ablate mental state context:** Train dialog act classifier with and without emotion context (replicate Table 2 methodology); measure delta
  3. **Minimal multi-modal probe:** Add single-frame facial expression features to emotion recognition; compare F1 against text-only to justify full multi-modal investment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Theory of Mind (ToM) capabilities, specifically belief tracking, be effectively integrated into downstream MPCA tasks to improve performance?
- Basis in paper: The authors note there is "little work in integrating ToM capabilities into other MPCA tasks" and suggest that combining belief tracking with state-of-mind modeling could lead to more intelligent agents
- Why unresolved: Current ToM benchmarks rely on synthetic data that does not emulate realistic multi-party settings, and it remains debated whether LLMs truly reason about mental states or simply memorize patterns
- What evidence would resolve it: Demonstrable performance improvements on diverse MPCA tasks (e.g., dialogue act recognition) when belief tracking is explicitly incorporated into the modeling pipeline

### Open Question 2
- Question: How can multi-modal fusion techniques be advanced to effectively utilize spatial audio and face tracking for conversation disentanglement?
- Basis in paper: The survey states that "conversation disentanglement... could benefit from integrating modalities like spatial audio and/or face tracking, but most works are still text-based"
- Why unresolved: Current multi-modal dialogue models with video context often fail to consistently outperform text-only baselines, indicating a lack of robust fusion methods
- What evidence would resolve it: New architectures that leverage non-verbal cues (gaze, pitch) to achieve state-of-the-art results on disentanglement benchmarks, surpassing text-only limitations

### Open Question 3
- Question: What methodologies are required to create real-world multi-modal benchmarks that utilize simulation-based metrics for evaluating MPCA interactions?
- Basis in paper: The authors argue that future research "must create real-world multi-modal benchmarks with simulation-based metrics where an MPCA is inserted into multi-party settings to interact in real time"
- Why unresolved: Current evaluation relies on static metrics (BLEU, accuracy) and single-skill datasets, which fail to capture the dynamic nature of multi-party interactions
- What evidence would resolve it: The adoption of standardized simulation environments where agent success is measured by interaction quality and task completion in real-time group settings

## Limitations

- LLM performance claims are qualified by limitations in dialog act recognition and hallucination issues in summarization tasks
- Multi-modal approaches lack systematic ablation studies showing performance degradation when modalities are missing or misaligned
- Theory of Mind findings are based on limited datasets (EMOTyDA) and model architectures, raising generalizability concerns
- Current evaluation metrics fail to capture the dynamic nature of multi-party interactions and real-time agent performance

## Confidence

- **High Confidence**: Graph-structured representation approaches show consistent improvements across multiple tasks (dialogue disentanglement, semantic understanding)
- **Medium Confidence**: Multi-modal fusion benefits for mental state detection, though evidence is more fragmented and lacks comprehensive ablation analysis
- **Low Confidence**: LLM generalization claims for MPCA tasks beyond response generation, with acknowledged limitations (hallucination, DAR underperformance)

## Next Checks

1. **Multi-Modal Ablation Study**: Implement a controlled experiment where video/audio features are progressively removed from a multi-modal MPCA system. Measure performance degradation on turn detection and addressee selection to quantify the actual contribution of non-text modalities versus their added complexity.

2. **LLM Hallucination Analysis**: Collect a corpus of LLM-generated summaries from multiparty dialogues and systematically annotate hallucinated content versus supported information. Quantify the frequency and severity of hallucinations across different dialogue lengths and speaker counts.

3. **Theory of Mind Robustness Test**: Evaluate ToM-enhanced MPCA systems on datasets with varying quality of emotion/personality annotations. Test whether noisy or missing mental state labels degrade downstream task performance, and whether the system can gracefully handle such uncertainty.