---
ver: rpa2
title: 'XDoGE: Multilingual Data Reweighting to Enhance Language Inclusivity in LLMs'
arxiv_id: '2512.10545'
source_url: https://arxiv.org/abs/2512.10545
tags:
- languages
- language
- data
- training
- weights
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces XDoGE, an extension of the DoGE algorithm
  for optimizing multilingual data distribution in large language model (LLM) pre-training.
  The method addresses the challenge of improving LLM performance across multiple
  languages with varying resource availability by training a small proxy model to
  estimate optimal language weights, then using these weights to resample and train
  full-size models from scratch or through continual pre-training.
---

# XDoGE: Multilingual Data Reweighting to Enhance Language Inclusivity in LLMs

## Quick Facts
- arXiv ID: 2512.10545
- Source URL: https://arxiv.org/abs/2512.10545
- Reference count: 29
- Primary result: XDoGE outperforms uniform weighting baselines in most target languages, particularly in continual pre-training scenarios

## Executive Summary
XDoGE extends the DoGE algorithm to optimize multilingual data distribution in LLM pre-training. The method addresses the challenge of improving LLM performance across multiple languages with varying resource availability by training a small proxy model to estimate optimal language weights, then using these weights to resample and train full-size models. The framework incorporates weight-clipping to ensure fair representation of all target languages and corpus rescaling to balance data repetition. Evaluation across six languages shows consistent improvements over uniform baselines, particularly in continual pre-training scenarios.

## Method Summary
XDoGE uses a bi-level optimization framework where a small proxy model (70M-500M parameters) is trained using gradient alignment to estimate optimal data weights for each language-domain pair. The outer loop updates these weights based on their alignment with aggregate gradients, while the inner loop trains the proxy with weighted losses. Weight-clipping (γ=0.02) prevents language collapse by ensuring minimum representation for all languages. The learned weights are then used to resample data for training full-size models from scratch or through continual pre-training, with corpus rescaling to balance repetition between high-resource and low-resource languages.

## Key Results
- XDoGE consistently outperforms uniform weighting baselines across five of six target languages in continual pre-training
- Weight-clipping mechanism effectively prevents language collapse while maintaining stable training dynamics
- Models trained with XDoGE weights achieve strong performance across the target language set, particularly when applied to continual pre-training
- The method demonstrates robust performance across different model scales (125M-500M parameters)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient alignment between language-domain pairs estimates which data sources improve cross-lingual transfer
- Mechanism: The outer loop computes W^(t)_ℓ,i as the inner product between each source's gradient and the aggregate gradient across all sources
- Core assumption: Gradient alignment during proxy training correlates with downstream transfer utility across languages
- Evidence anchors: [Section III-B], [Section V-A], and related work (arXiv:2510.25947)
- Break condition: If languages have fundamentally incompatible gradient directions, optimization may favor one at the expense of others

### Mechanism 2
- Claim: Weight-clipping prevents language collapse and stabilizes multilingual weight convergence
- Mechanism: A projection operator enforces α_ℓ,i ≥ γ (γ = 0.02 empirically) while preserving sum-to-one
- Core assumption: Some minimum representation per language is necessary for the proxy to learn meaningful cross-lingual signals
- Evidence anchors: [Section III-A], [Section V-A]
- Break condition: If γ is set too high, optimization is overly constrained; if too low, collapse can still occur

### Mechanism 3
- Claim: Learned weights are more effective in continual pre-training (CPT) than from-scratch training
- Mechanism: CPT starts from an already-multilingual model, requiring fewer tokens to adapt
- Core assumption: Pre-trained models have latent multilingual capacity that can be steered via resampling without full retraining
- Evidence anchors: [Section V-D], [Section VIII], and related work (arXiv:2502.09331)
- Break condition: If the pre-trained model lacks sufficient representation of target languages, CPT cannot create capacity from scratch

## Foundational Learning

- Concept: Bi-level optimization (inner/outer loops)
  - Why needed here: XDoGE alternates between updating model parameters (inner) and updating data weights (outer)
  - Quick check question: Can you explain why the inner loop uses weighted losses while the outer loop uses gradient alignment?

- Concept: Distributionally robust optimization
  - Why needed here: DoGE/XDoGE extends DRO principles to avoid overfitting to any single language or domain
  - Quick check question: How does weight-clipping relate to the robustness objective?

- Concept: Cross-lingual transfer
  - Why needed here: The core hypothesis is that optimizing for gradient alignment improves transfer between typologically related languages
  - Quick check question: Why might Basque (a language isolate) show different weight dynamics than Romance languages?

## Architecture Onboarding

- Component map: Proxy model -> Weight aggregation -> Base model training/CPT phase
- Critical path:
  1. Prepare language-split corpora (Wikipedia + OSCAR/FineWeb)
  2. Train thresholded proxy models (multiple sizes/seeds)
  3. Aggregate and smooth weights over final 10K steps
  4. Rescale corpus to match weight distribution
  5. Train base model or run CPT with resampled data
- Design tradeoffs:
  - Proxy size vs. stability: Larger proxies (500M) yield more stable weights but cost more
  - γ threshold: 0.02 ensures ~2–3 instances per source per batch
  - Repetition tolerance: Low-resource languages may see 100–400× repetition
- Failure signatures:
  - Weight collapse: Some languages drop to <1% weight early in proxy training
  - CPT degradation: Validation loss increases after 40–60K steps with uniform weights
  - Cross-lingual interference: Simultaneously reducing weights for related languages
- First 3 experiments:
  1. Train unthresholded vs. thresholded 125M proxy on 2–3 languages; visualize weight dynamics
  2. Compare from-scratch training with XDoGE weights vs. uniform baseline on 250M model
  3. Run CPT on pre-trained Salamandra-2B using XDoGE vs. ad hoc weights; measure perplexity delta

## Open Questions the Paper Calls Out

- Can integrating data availability explicitly into weight computation improve optimization for low-resource languages that require substantial data repetition?
- Does a staged approach to changing target languages outperform holistic language inclusion when adapting highly multilingual models?
- Can explicit language-family correspondences improve XDoGE's capture of interdependencies among related languages?
- How does XDoGE scale to highly multilingual scenarios with 35+ languages?

## Limitations

- Language scope and generalizability: Results may not generalize to language families with different typological properties or extreme resource imbalances
- Evaluation scope: Primary evaluation uses IberoBench, limiting assessment of cross-lingual transfer to non-Iberian languages
- Resource constraints and data repetition: Corpus rescaling introduces significant data repetition for low-resource languages (100-400×)

## Confidence

- High confidence: Core mechanism of using gradient alignment for data weight optimization is theoretically sound
- Medium confidence: Superiority in continual pre-training over from-scratch training is demonstrated but needs additional context
- Low confidence: Long-term stability of models trained with extreme data repetition is not fully characterized

## Next Checks

1. Evaluate XDoGE-trained models on zero-shot/few-shot tasks in non-Iberian languages to assess generalizability
2. Train models with XDoGE weights for extended periods (2-3× current steps) and monitor degradation patterns
3. Apply XDoGE to a language set with more severe resource disparities to test robustness under challenging conditions