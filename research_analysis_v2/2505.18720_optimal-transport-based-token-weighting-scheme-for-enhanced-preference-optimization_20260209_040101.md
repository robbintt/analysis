---
ver: rpa2
title: Optimal Transport-Based Token Weighting scheme for Enhanced Preference Optimization
arxiv_id: '2505.18720'
source_url: https://arxiv.org/abs/2505.18720
tags:
- response
- otpo
- token
- reward
- length
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes OTPO, a token weighting scheme for preference
  optimization that addresses the problem of treating all tokens equally in DPO. OTPO
  leverages optimal transport to dynamically assign weights to token pairs based on
  their semantic similarity, emphasizing semantically meaningful tokens while de-emphasizing
  less relevant ones.
---

# Optimal Transport-Based Token Weighting scheme for Enhanced Preference Optimization

## Quick Facts
- **arXiv ID:** 2505.18720
- **Source URL:** https://arxiv.org/abs/2505.18720
- **Reference count:** 40
- **Primary result:** OTPO achieves up to 10.9% increase in length-controlled win rate on AlpacaEval2 compared to standard DPO

## Executive Summary
This paper proposes OTPO, a token weighting scheme for preference optimization that addresses the problem of treating all tokens equally in DPO. OTPO leverages optimal transport to dynamically assign weights to token pairs based on their semantic similarity, emphasizing semantically meaningful tokens while de-emphasizing less relevant ones. This leads to a more contrastive reward difference estimate, improving reward stability and interpretability. Experiments across various settings show OTPO achieves up to 10.9% increase in length-controlled win rate on AlpacaEval2 compared to DPO, demonstrating its effectiveness in improving instruction-following ability.

## Method Summary
OTPO is a token weighting scheme that modifies standard DPO by applying dynamic weights to token pairs based on their semantic similarity. The method constructs a cost matrix using Euclidean distances between last-layer token embeddings from chosen and rejected responses, then solves an unbalanced optimal transport problem to find a transport plan. This plan is marginalized to derive token weights, which are applied to the log-likelihood ratios in the DPO loss function. The total weight budget is normalized by the minimum response length to mitigate length bias. The approach emphasizes semantically aligned tokens while de-emphasizing noise and style differences.

## Key Results
- OTPO achieves up to 10.9% increase in length-controlled win rate on AlpacaEval2 compared to DPO
- Successfully reduces average response length from 2168 to 1791 tokens compared to DPO
- Shows improved reward stability and more varied, context-aware reward differences compared to standard DPO
- Demonstrates effectiveness across multiple evaluation datasets including MT-Bench and IFEval

## Why This Works (Mechanism)

### Mechanism 1: Semantic Alignment via Optimal Transport
If token representations effectively capture semantic meaning, then minimizing the transport cost between the chosen ($y_c$) and rejected ($y_r$) responses identifies shared, relevant content. The algorithm constructs a cost matrix $M$ using Euclidean distances between last-layer token embeddings. It solves an Unbalanced Optimal Transport problem to find a transport plan $\Gamma$ that couples tokens from $y_c$ to $y_r$ with minimum effort. This coupling is then marginalized to derive token weights. The core assumption is that tokens that are semantically similar (low distance) or structurally aligned across the two responses are the "meaningful" parts relevant to the instruction, whereas high-distance tokens represent noise or style differences.

### Mechanism 2: Contrastive Reward Sharpening
If the optimization focuses on semantically aligned tokens, the estimated reward difference $\Delta r$ becomes a more stable signal for optimization compared to uniform weighting. Standard DPO sums log-likelihoods uniformly (Eq. 4). OTPO applies the derived weights $\omega$ to the per-token log-likelihood ratios (Eq. 11). By down-weighting unique/noisy tokens in the sum, the variance of the reward estimate is reduced, preventing "less relevant signals" from dominating the loss gradient. The core assumption is that irrelevant tokens contribute noise that dilutes the preference signal; removing this noise improves the signal-to-noise ratio of the gradient.

### Mechanism 3: Implicit Length Normalization
If the total transport mass is constrained to the minimum response length, the algorithm mitigates the length bias inherent in standard DPO. The transport plan is normalized by $\tau = \min(|y_c|, |y_r|)$ (Eq. 10). This fixes the total weight budget, ensuring the longer response (usually the chosen one) cannot accumulate higher reward simply by having more tokens. The core assumption is that length bias is driven by the summation of token weights; capping the sum removes the incentive for verbosity.

## Foundational Learning

- **Optimal Transport (OT)**: Understanding how OT moves "mass" (token weights) from one distribution to another based on a cost function is essential to understand why specific tokens are up-weighted. Quick check: How does the entropy regularizer ($\epsilon_1$) in Eq. 9 affect the sparsity of the transport plan $\Gamma$?

- **DPO Loss Decomposition**: Understanding the transition from the integral form (response-level) to the summation form (token-level) in Eq. 4 is required to implement the weighting logic. Quick check: In Eq. 5, if $\omega_i = 1$ for all $i$, does the loss function remain mathematically equivalent to standard DPO?

- **Token Embeddings & Representation Spaces**: The method relies entirely on the geometric relationship of tokens in the model's hidden state. Quick check: Why does the paper use the Euclidean distance of the *last-layer* hidden states rather than cosine similarity of intermediate layers for the cost matrix?

## Architecture Onboarding

- **Component map:** Forward Pass -> OT Solver -> Weight Extractor -> Loss Calculation
- **Critical path:** The OT Solver is the bottleneck. It sits between the forward pass and the backward pass. Efficient implementation (batching the OT solve) is critical for training speed.
- **Design tradeoffs:** OT Hyperparameters ($\epsilon_1, \epsilon_2$) control the smoothness of the weighting. Low $\epsilon$ leads to sparse/sharp weights (risk of ignoring context); high $\epsilon$ approximates uniform weights (risk of converging to standard DPO). Normalization ($\tau$) using `min` vs `mean` length. The paper argues `min` is better for length bias, but it may discard information in the longer response.
- **Failure signatures:** Vanishing Gradients if OT solver assigns near-zero weights to critical tokens. Uniform Weights if cost matrix is uninformative or regularization is too high. CUDA OOM storing the cost matrix $M \in \mathbb{R}^{|y_c| \times |y_r|}$ for long sequences.
- **First 3 experiments:** 1) Sanity Check: Visualize transport plan (Sankey diagram) on simple prompt to verify "shared" tokens receive higher weights. 2) Ablation: Compare `min` vs `mean` vs `none` normalization on small dataset to observe impact on length and reward stability. 3) Hyperparameter Sensitivity: Sweep $\epsilon_1$ to find sweet spot where Length-Controlled Win Rate peaks without collapsing raw Win Rate.

## Open Questions the Paper Calls Out

1. **Iterative on-policy setups:** Recent research has highlighted that iterative on-policy setups may yield larger improvements in instruction-following performance. We did not explore such setups... This leaves room for future work to further enhance model performance using OTPO under iterative on-policy setups.

2. **Large model scalability:** We conducted our experiments on relatively small models... the scalability and generalizability of our algorithms to larger models... remain to be validated.

3. **Integration with other DPO variants:** We leave the incorporation of an optimal transport-based weighting scheme to these DPO variants for future work.

4. **Multilingual generalizability:** Our training experiments were conducted mainly on English datasets. We have not verified the generalizability or effectiveness of our algorithm on non-English datasets.

## Limitations

- **Computational Overhead:** The OT solver introduces significant computational cost and memory usage (O(L²) for cost matrix storage), potentially limiting applicability to very long sequences or large-scale training.
- **Gradient Flow Ambiguity:** Unclear whether gradients flow through the OT solver or if weights are treated as fixed during each forward pass, significantly impacting training dynamics.
- **Representation Dependency:** The method's effectiveness critically depends on the semantic quality of the model's hidden states, becoming arbitrary if representations fail to capture true semantic similarity.

## Confidence

**High Confidence Claims:**
- OTPO outperforms standard DPO on AlpacaEval2 Length-Controlled Win Rate metrics (up to 10.9% improvement)
- The method successfully reduces average response length compared to DPO (1791 vs 2168 tokens)
- Weighting tokens based on semantic similarity is theoretically sound and implementable

**Medium Confidence Claims:**
- The specific normalization strategy (min-length) is optimal for length control
- The Unbalanced OT formulation with KL regularization is superior to other variants
- The observed improvements are primarily due to semantic weighting rather than other confounding factors

**Low Confidence Claims:**
- The method generalizes equally well across all model scales and domains
- The specific OT hyperparameters (ε₁=1.0, ε₂=0.2) are optimal across different training setups
- The method's benefits persist when training data quality varies significantly

## Next Checks

1. **Gradient Flow Verification:** Implement and compare two versions - one where OT weights are fixed during backpropagation (detached) versus one where gradients flow through the OT solver. Measure training stability, final performance, and computational overhead for each approach.

2. **Representation Layer Sensitivity:** Repeat OTPO experiments using hidden states from different layers (e.g., layer 12, 24, 32 for Llama models) to determine whether last-layer embeddings are indeed optimal for semantic weighting, or if intermediate layers might perform better for certain tasks.

3. **Cross-Domain Generalization:** Apply OTPO to a non-instruction-following task (e.g., code generation, mathematical reasoning) with different response characteristics. Measure whether the length control benefits persist and whether the semantic weighting mechanism adapts appropriately to domain-specific token importance patterns.