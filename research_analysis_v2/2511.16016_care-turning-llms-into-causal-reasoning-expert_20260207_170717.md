---
ver: rpa2
title: 'CARE: Turning LLMs Into Causal Reasoning Expert'
arxiv_id: '2511.16016'
source_url: https://arxiv.org/abs/2511.16016
tags:
- causal
- llms
- discovery
- data
- algorithms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper tackles the inability of large language models (LLMs)\
  \ to perform genuine causal discovery from observational data, showing that na\xEF\
  ve prompting\u2014even with algorithmic outputs\u2014fails and often leads LLMs\
  \ to rely on variable\u2011name semantics. CARE addresses this by supervised fine\u2011\
  tuning (SFT) an LLM to synthesize its world\u2011knowledge priors with the sufficient\u2011\
  statistics outputs of established causal\u2011discovery algorithms (e.g., PC, GES,\
  \ LiNGAM)."
---

# CARE: Turning LLMs Into Causal Reasoning Expert  

## Quick Facts  
- **arXiv ID:** 2511.16016  
- **Source URL:** https://arxiv.org/abs/2511.16016  
- **Reference count:** 40  
- **Primary result:** A 1.5 B‑parameter Qwen2.5 model fine‑tuned with CARE attains >80 % F1 on the ASIA benchmark, outperforming much larger LLMs.  

## Executive Summary  
The paper identifies a fundamental limitation of current large language models: they cannot reliably discover causal structure from observational data, often defaulting to superficial cues such as variable‑name semantics. CARE (Causal‑Reasoning Expert) remedies this by supervised fine‑tuning an LLM to combine its world‑knowledge priors with the sufficient‑statistics outputs of established causal‑discovery algorithms (PC, GES, LiNGAM). Trained on synthetic datasets that pair graph queries, raw data, algorithmic suggestions, and ground‑truth DAGs, the approach enables a modest 1.5 B‑parameter model to surpass baseline LLMs (low‑50 % F1) and even models more than a thousand times larger, achieving >80 % F1 on the ASIA benchmark under random naming and permutation conditions.  

## Method Summary  
CARE constructs a synthetic training corpus where each instance contains: (1) a causal‑graph query, (2) the corresponding observational dataset, (3) algorithmic suggestions from classical causal‑discovery methods, and (4) the true DAG. These triples are used to supervise fine‑tuning of the Qwen2.5‑1.5B model, teaching it to reconcile algorithmic sufficient statistics with its internal knowledge. During inference, the fine‑tuned model receives the raw data plus algorithmic hints in a structured prompt and outputs a predicted causal graph. The pipeline thus leverages algorithmic rigor while preserving the LLM’s flexible reasoning.  

## Key Results  
- **>80 % F1** on the ASIA benchmark (random‑named & permuted variables), a jump from the low‑50 % range of baseline LLMs.  
- **Outperforms** traditional causal‑discovery algorithms and state‑of‑the‑art LLMs that are >1 000× larger.  
- Demonstrates **robustness** to variable‑name semantics, indicating genuine causal reasoning rather than lexical shortcuts.  

## Why This Works (Mechanism)  
- **Supervised alignment:** Fine‑tuning aligns the LLM’s prior knowledge with algorithmic sufficient statistics, forcing the model to respect statistical evidence rather than surface cues.  
- **Algorithmic grounding:** By feeding outputs of proven causal‑discovery methods (PC, GES, LiNGAM) into the prompt, CARE provides a concrete, data‑driven scaffold that the LLM can refine.  
- **Semantic decoupling:** Training on randomly renamed and permuted variables eliminates reliance on variable‑name semantics, compelling the model to infer structure from the data itself.  

## Foundational Learning  
1. **Paper input requirements** – *Why needed:* The analysis framework depends on concrete sections (abstract, methods, results) to anchor claims.  
   *Quick check:* Have you supplied the paper’s abstract and at least one method or results section?  

2. **Evidence anchoring methodology** – *Why needed:* Claims must be linked to explicit textual evidence to avoid speculation.  
   *Quick check:* Can each mechanistic claim be traced to a specific line or figure in the paper?  

3. **Conditional claims and uncertainty labeling** – *Why needed:* When evidence is weak or missing, the framework flags uncertainty rather than fabricating links.  
   *Quick check:* Are you distinguishing between proven results and hypothesized mechanisms in the source material?  

4. **Algorithmic cue integration** – *Why needed:* Understanding how algorithmic outputs are merged with LLM priors is essential for reproducing the method.  
   *Quick check:* Does the paper describe the prompt format and weighting scheme for algorithmic hints?  

5. **Synthetic data generation pipeline** – *Why needed:* The quality and diversity of synthetic graphs directly affect fine‑tuning effectiveness.  
   *Quick check:* Are the graph generation parameters (size, density, noise) documented?  

## Architecture Onboarding  
- **Component map:** Synthetic data generation → Algorithmic causal discovery (PC/GES/LiNGAM) → Prompt construction with algorithmic hints → SFT of LLM → Inference (graph prediction)  

- **Critical path:**  
  1. Generate synthetic datasets (must complete before any training).  
  2. Run causal‑discovery algorithms to obtain hints (sequential after data generation).  
  3. Fine‑tune the LLM using the combined inputs (depends on steps 1‑2).  
  4. Inference can be parallelized across queries once the model is fine‑tuned.  

- **Design tradeoffs:**  
  - *Model size vs. data diversity*: Smaller LLMs require richer synthetic diversity to close the performance gap.  
  - *Algorithmic hint fidelity vs. LLM prior reliance*: Over‑weighting algorithmic outputs may suppress useful world knowledge; under‑weighting may re‑introduce semantic shortcuts.  
  - *Compute cost vs. performance*: Extensive SFT on large synthetic corpora increases training time but yields higher F1 gains.  

- **Failure signatures:**  
  - Sudden drop in F1 when variable names are not permuted (indicates lingering lexical bias).  
  - Over‑fitting to specific algorithmic patterns, visible as high performance on synthetic benchmarks but poor transfer to real datasets.  
  - Inconsistent graph predictions across runs, suggesting instability in the fine‑tuned model.  

- **First 3 experiments:**  
  1. **Reproduce CARE on the released synthetic dataset** and verify the reported >80 % F1 on ASIA.  
  2. **Ablation study:** Remove algorithmic suggestions from the prompt to quantify their contribution.  
  3. **Real‑world transfer test:** Apply the fine‑tuned model to a real observational dataset (e.g., Sachs protein network) with and without variable‑name permutation.  

## Open Questions the Paper Calls Out  
- How well does CARE generalize from synthetic benchmarks to complex real‑world observational datasets?  
- What are the exact hyper‑parameters and SFT recipe (learning rate, batch size, epochs) used in training?  
- Which baseline LLMs and prompting strategies were employed for the “>1 000× larger” comparison, and are they fully comparable?  
- How is the fusion of algorithmic outputs with LLM priors implemented (prompt format, weighting, conditioning)?  

## Limitations  
- Evaluation is limited to synthetic benchmarks; real‑world applicability remains unproven.  
- Training details (hyper‑parameters, data generation specifics) are not fully disclosed, hindering reproducibility.  
- Baseline comparisons may lack transparency regarding prompting style and few‑shot settings.  

## Confidence  
- **Algorithmic clues prevent reliance on variable‑name semantics** → **High**  
- **CARE improves causal‑graph recovery on synthetic benchmarks** → **Medium**  
- **Supervised fine‑tuning lets a 1.5 B model beat much larger LLMs** → **Low**  

## Next Checks  
1. **Acquire the full paper** (methods, hyper‑parameters, full result tables) to verify experimental protocols and baseline configurations.  
2. **Re‑implement the SFT pipeline** using the released synthetic dataset and reproduce the reported F1 scores for CARE versus baseline LLMs.  
3. **Evaluate CARE on at least two real‑world observational datasets** (e.g., Sachs protein network, climate variables) with and without variable‑name permutation to test generalisation.