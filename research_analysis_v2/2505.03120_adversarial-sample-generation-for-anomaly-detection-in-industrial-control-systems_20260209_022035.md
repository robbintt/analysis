---
ver: rpa2
title: Adversarial Sample Generation for Anomaly Detection in Industrial Control Systems
arxiv_id: '2505.03120'
source_url: https://arxiv.org/abs/2505.03120
tags:
- attack
- adversarial
- systems
- water
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the vulnerability of machine learning-based
  intrusion detection systems (IDS) to adversarial attacks in Industrial Control Systems
  (ICS). The researchers generated adversarial samples using the Jacobian Saliency
  Map Attack (JSMA), which was originally designed for image media but was successfully
  adapted for time series data from sensors and actuators in a water treatment testbed
  (SWaT).
---

# Adversarial Sample Generation for Anomaly Detection in Industrial Control Systems

## Quick Facts
- arXiv ID: 2505.03120
- Source URL: https://arxiv.org/abs/2505.03120
- Reference count: 40
- Primary result: Achieved 95% detection accuracy on real-world attacks by training with JSMA-generated adversarial samples

## Executive Summary
This study addresses the vulnerability of machine learning-based intrusion detection systems (IDS) to adversarial attacks in Industrial Control Systems (ICS). The researchers generated adversarial samples using the Jacobian Saliency Map Attack (JSMA), which was originally designed for image media but was successfully adapted for time series data from sensors and actuators in a water treatment testbed (SWaT). By training classifiers with these adversarial samples, the system's ability to detect real-world attacks was enhanced. The model achieved a 95% detection accuracy on attack data not used during training. The study demonstrates that incorporating adversarial samples into IDS training improves robustness against cyber-attacks without requiring direct exposure to attack data. This approach offers a scalable and generalizable method for strengthening ML-based security systems in ICS environments.

## Method Summary
The researchers adapted the Jacobian Saliency Map Attack (JSMA) from image processing to generate adversarial samples from time-series ICS sensor data. They used the SWaT dataset containing 51 features from sensors and actuators, generating 112,480 adversarial samples from attack data. These samples were merged with 410,400 normal samples to create a training set of 522,880 instances. Three classifiers (CART, Random Forest, and Gradient Boosting) were trained on this merged dataset and evaluated on a separate real-world attack dataset. The Gradient Boosting Classifier achieved the best performance with 95% accuracy, precision, and 1.0 recall.

## Key Results
- Gradient Boosting Classifier achieved 95% accuracy, precision, and 1.0 recall on real-world attack detection
- Random Forest achieved 88% accuracy but failed to detect any attacks (0 true positives)
- Adversarial training improved detection capability without requiring exposure to actual attack data during training
- JSMA successfully adapted from image domain to time-series ICS sensor data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adapting JSMA from image media to time-series data provides a viable method for generating adversarial samples in ICS environments.
- Mechanism: JSMA computes the Jacobian matrix of the classifier's output with respect to its input, identifies critical features via saliency maps, and applies minimal perturbations to these features to cause misclassification.
- Core assumption: Feature sensitivity exploited by JSMA in spatial image data has a functional analog in the temporal features of ICS sensor data.
- Evidence anchors: Abstract states JSMA was "successfully adapted for time series data"; section IV describes using saliency maps for time-series perturbations.
- Break condition: May fail if temporal dependencies in ICS data are not captured by per-feature perturbations.

### Mechanism 2
- Claim: Training IDS with synthetically generated adversarial samples improves ability to detect real-world attacks.
- Mechanism: Merging adversarial samples with normal data exposes the classifier to attack characteristics, forcing it to learn robust decision boundaries.
- Core assumption: Synthetic adversarial samples share fundamental feature-space characteristics with real-world cyber-attacks.
- Evidence anchors: Abstract notes system's ability to detect real-world attacks was enhanced; section V.D highlights examples proved useful without needing attack data training.
- Break condition: Success depends on quality of underlying model used for generation.

### Mechanism 3
- Claim: Gradient Boosting ensemble trained on adversarial samples provides more robust defense than single trees or random forests.
- Mechanism: Gradient Boosting builds stage-wise ensembles optimizing for previous errors, better modeling complex decision boundaries from adversarial samples.
- Core assumption: Complex decision boundaries from adversarial samples are better captured by sequential error correction than parallel voting.
- Evidence anchors: Section V.D shows GBC performed better than CART in detecting attacks and classifying normal instances; Table I shows GBC achieved 95% across all metrics.
- Break condition: May overfit to specific perturbation types if real attacks differ significantly.

## Foundational Learning

### Concept: Jacobian Saliency Map Attack (JSMA)
- Why needed here: Core technique used in the paper for generating adversarial samples
- Quick check question: Does JSMA perturb all input features by a small amount or does it perturb a small set of critical features significantly?

### Concept: Adversarial Training as a Defense
- Why needed here: Paper's primary goal is using adversarial samples for defense, not attack
- Quick check question: What is the primary benefit of merging adversarial samples with normal data before training the final classifier?

### Concept: Model Transferability
- Why needed here: Paper uses one model (MLP) to generate samples and different models (CART, RF, GBC) for classification
- Quick check question: If adversarial samples are generated to fool an MLP, why might they also be useful for training a Gradient Boosting Classifier?

## Architecture Onboarding

### Component map:
Data Preprocessing -> MLP Training -> JSMA Generation -> Sample Merger -> Classifier Training -> Evaluation

### Critical path:
1. Preprocess SWaT normal dataset
2. Train initial MLP model on normal data as JSMA oracle
3. Use JSMA to generate adversarial samples by perturbing normal data based on MLP's gradients
4. Merge generated samples with original normal data
5. Train final GBC classifier on merged set
6. Evaluate on held-out SWaT attack dataset

### Design tradeoffs:
- **Perturbation parameters (θ, γ, ϵ):** Tradeoff between attack stealthiness and success rate
- **Classifier Selection:** RF offers robustness to false positives but fails to detect attacks; GBC offers better balance
- **Underlying Model for JSMA:** Quality of generated samples depends entirely on the MLP used

### Failure signatures:
- **Perfectly Clean Confusion Matrix on Attack Data:** Model classifies all attack data as normal, indicating adversarial training failure
- **High False Positive Rate (FPR):** Model is over-regularized or adversarial perturbations pushed decision boundary too far

### First 3 experiments:
1. Train GBC on only normal data and evaluate on attack dataset to establish baseline poor performance
2. Build pipeline to generate adversarial samples from normal data using JSMA; verify samples are misclassified by source MLP
3. Train CART, RF, and GBC on merged dataset; evaluate all three on attack dataset and compare confusion matrices

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the adversarial training approach generalize effectively to other ICS datasets outside the water treatment domain?
- Basis in paper: [explicit] Authors state in conclusion they "plan to extend our evaluation to other ICS datasets"
- Why unresolved: Current study validates method exclusively on SWaT testbed
- What evidence would resolve it: Replicating methodology on datasets like HAI or GAS with comparable detection accuracy

### Open Question 2
- Question: Does training on JSMA-generated samples provide robustness against other adversarial attack methods such as FGSM or C&W?
- Basis in paper: [explicit] Conclusion notes future work includes "exploring other adversarial attack methods"
- Why unresolved: Study only evaluates model's robustness against JSMA-specific perturbations
- What evidence would resolve it: Testing JSMA-trained models against FGSM and C&W attack samples

### Open Question 3
- Question: Why does adversarial training significantly improve GBC performance but cause RF to fail to detect any attacks?
- Basis in paper: [inferred] Table III shows GBC achieved 95% accuracy while RF resulted in 0 True Positives
- Why unresolved: Paper highlights utility of adversarial samples but doesn't analyze why RF reacted negatively compared to GBC
- What evidence would resolve it: Analysis of decision boundaries and feature importance weights for RF versus GBC before and after JSMA perturbations

## Limitations
- Relies on synthetic adversarial samples rather than real attack data for training
- JSMA adaptation from image to time-series data demonstrated but not extensively validated across different ICS systems
- 95% detection rate based on single testbed (SWaT) may not generalize to other ICS environments

## Confidence
- **High Confidence:** Methodology for generating adversarial samples using JSMA and basic premise that adversarial training can improve IDS robustness
- **Medium Confidence:** Specific adaptation of JSMA to time-series ICS data and resulting 95% detection accuracy claim
- **Low Confidence:** Generalization claims beyond SWaT testbed and assertion that approach works "without requiring direct exposure to attack data"

## Next Checks
1. Test the same adversarial training approach on a different ICS testbed (e.g., WADI or HAI) to verify if 95% detection rate generalizes across different industrial control environments
2. Generate adversarial samples using different attack models (e.g., FGSM or PGD) and compare their effectiveness against JSMA-generated samples for improving IDS robustness
3. Deploy the trained IDS in a live ICS environment with actual attack attempts to validate practical effectiveness of the adversarial training approach