---
ver: rpa2
title: Triplet Loss Based Quantum Encoding for Class Separability
arxiv_id: '2509.15705'
source_url: https://arxiv.org/abs/2509.15705
tags:
- encoding
- quantum
- amplitude
- proposed
- circuit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a quantum encoding method based on triplet
  loss that improves class separability in the Hilbert space for image classification
  tasks. Inspired by classical face recognition, the method constructs an embedding
  circuit by selecting triplets of anchor, positive, and negative samples to maximize
  inter-class distances and minimize intra-class distances.
---

# Triplet Loss Based Quantum Encoding for Class Separability

## Quick Facts
- **arXiv ID:** 2509.15705
- **Source URL:** https://arxiv.org/abs/2509.15705
- **Reference count:** 37
- **Primary result:** Triplet loss-based quantum encoding improves class separability and accuracy on MNIST/MedMNIST with much lower circuit depth than amplitude encoding

## Executive Summary
This work introduces a quantum encoding method based on triplet loss that improves class separability in Hilbert space for image classification. Inspired by classical face recognition, the method constructs an embedding circuit by selecting triplets of anchor, positive, and negative samples to maximize inter-class distances and minimize intra-class distances. Results on MNIST and MedMNIST datasets show significant accuracy improvements over amplitude encoding with much lower circuit depth, particularly for low-resolution images. The method achieves 96.8% accuracy on 8x8 MNIST with 6 qubits and demonstrates better class separation, especially in simple datasets, while maintaining efficiency for near-term quantum hardware.

## Method Summary
The method uses hard triplet mining to select anchor (class median), positive (farthest same-class), and negative (closest other-class) samples. A greedy combinatorial algorithm incrementally builds the encoding circuit by evaluating different gate-qubit combinations from a fixed pool (RX, RY, RZ, CNOT, CZ) to minimize triplet loss. Features are mapped to rotation angles of the learned circuit structure. After encoding, a standard variational quantum classifier (RY rotations + circular CNOTs) is applied for classification. The approach aims to balance expressiveness with NISQ-era hardware constraints.

## Key Results
- Achieves 96.8% accuracy on 8x8 MNIST binary classification with 6 qubits
- Shows significant improvement over amplitude encoding baseline on low-resolution images
- Demonstrates much lower circuit depth compared to Mottonen state preparation (228 CNOTs for 6 qubits)
- Better class separation in t-SNE visualization, particularly for simpler datasets
- Performance degrades on complex MedMNIST at low resolution (8x8) but recovers at higher resolution

## Why This Works (Mechanism)

### Mechanism 1
If a triplet loss function drives the encoding circuit construction, then the resulting quantum states form distinct clusters based on class labels in the Hilbert space. The algorithm selects "hard" triplets where the positive is the farthest intra-class sample and the negative is the closest inter-class sample. By minimizing the trace distance between anchor/positive and maximizing it for anchor/negative, the circuit geometry enforces metric learning constraints directly into the quantum embedding. Core assumption: The "hard mining" strategy effectively approximates the global class distribution, and the trace distance on density matrices is a sufficient statistic for class separability in this context.

### Mechanism 2
A greedy combinatorial search yields a shallower, task-specific encoding circuit compared to generic amplitude encoding. The algorithm builds the circuit incrementally, selecting the gate-qubit combination that minimizes the current triplet loss at each step. This avoids the exponential depth growth of amplitude encoding by only introducing gates that strictly contribute to the loss objective. Core assumption: A locally optimal gate choice at step i contributes meaningfully to the global optimum, and feature correlations can be captured sufficiently by sequential greedy additions.

### Mechanism 3
Encoding features directly into rotation angles of a structured circuit improves noise resilience and efficiency relative to amplitude encoding. Features are mapped to rotation parameters, maintaining a 1:1 feature-to-parameter mapping efficiency closer to angle encoding, but with learned entanglement structure. Core assumption: The chosen gate pool is expressive enough to map the data manifold to the Hilbert space without requiring the exponential state preparation overhead of amplitude encoding.

## Foundational Learning

### Concept: Metric Learning (Triplet Loss)
**Why needed here:** To understand how the "loss" function shapes the geometry of the quantum state space, pushing classes apart.
**Quick check question:** Can you explain why the algorithm chooses the *farthest* positive and *closest* negative (hard mining) rather than random samples?

### Concept: Trace Distance
**Why needed here:** This is the specific metric used to quantify "distance" between quantum states (density matrices) in the paper.
**Quick check question:** How does trace distance differ from fidelity when measuring class overlap?

### Concept: NISQ Devices and Circuit Depth
**Why needed here:** The primary motivation is reducing circuit depth to mitigate noise on current hardware.
**Quick check question:** Why does amplitude encoding typically require deeper circuits that are infeasible on NISQ devices?

## Architecture Onboarding

### Component map:
Input -> Preprocessed image (resized, not normalized) -> Triplet Generator (selects Anchor/Positive/Negative) -> Greedy Constructor (builds encoding circuit) -> VQC Layer (RY + circular CNOTs) -> Output (binary classification via last qubit measurement)

### Critical path:
The Greedy Constructor is the bottleneck. It requires simulating density matrices for every candidate gate at every step.

### Design tradeoffs:
**Depth vs. Accuracy:** The algorithm accepts a potentially suboptimal global encoding (greedy) for drastically reduced depth.
**Resolution vs. Efficiency:** The paper shows performance drops on MedMNIST at low resolution (8x8) but recovers at higher resolution (28x8), implying a trade-off between input information and circuit complexity.

### Failure signatures:
**Class Collapse:** On complex datasets (MedMNIST 8x8), the model predicts a single class for all inputs.
**Hardware Drift:** On Rigetti Ankaa-3, noise degraded accuracy to 50% (random guessing).
**Qubit Imbalance:** The greedy algorithm may preferentially load operations onto the first qubits, leaving others redundant.

### First 3 experiments:
1. **Sanity Check (MNIST 0 vs 1):** Replicate the 8x8 experiment to verify the t-SNE cluster visualization shows distinct separation.
2. **Ablation on "Hard Mining":** Compare the proposed "hardest" triplet selection against random triplet selection to quantify the specific contribution of the mining strategy.
3. **Depth Scaling:** Test the constructed circuit depth vs. number of features to empirically verify the linear/bounded growth compared to exponential growth of amplitude encoding.

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** How does the triplet loss encoding perform in multiclass classification scenarios?
**Basis in paper:** The authors list testing "in multiclass scenarios" as future work.
**Why unresolved:** The study restricts all experimental evaluations to binary classification tasks.
**What evidence would resolve it:** Benchmark accuracy and cluster separation metrics on datasets with more than two classes.

### Open Question 2
**Question:** Can more effective triplet mining strategies improve performance on complex datasets?
**Basis in paper:** The paper notes that the current "hard" selection is "too simplistic" for complex images.
**Why unresolved:** Hard mining selects only the hardest examples, potentially failing to capture general class structures in complex data.
**What evidence would resolve it:** Comparative results using semi-hard or stochastic triplet mining on complex datasets like MedMNIST.

### Open Question 3
**Question:** Can the greedy circuit construction be modified to balance gate distribution across qubits?
**Basis in paper:** The text states that the current algorithm prefers the first qubits, leading to "redundant" later qubits.
**Why unresolved:** The greedy selection heuristic does not actively balance the circuit depth, causing structural inefficiencies.
**What evidence would resolve it:** A refined algorithm producing balanced circuits that maintains or improves classification accuracy.

### Open Question 4
**Question:** Can the encoding circuit generation time be reduced for high-resolution images?
**Basis in paper:** The conclusion proposes refining the algorithm to "reduce the encoding circuit generation time."
**Why unresolved:** The authors had to use smaller training sets for high-resolution datasets to manage computational costs.
**What evidence would resolve it:** Efficient generation of 28x28 image encodings without requiring training data subsetting.

## Limitations

- Performance significantly degrades on complex datasets (MedMNIST) at low resolutions (8x8), showing the encoding scheme's limitations for certain data types
- Hardware-specific noise patterns significantly impact results, with Rigetti Ankaa-3 showing degradation to random guessing accuracy
- The greedy algorithm may produce qubit imbalance, preferentially loading operations onto first qubits while leaving later qubits redundant
- The method is currently restricted to binary classification, with multiclass performance untested

## Confidence

- **High confidence:** The mechanism of triplet loss driving class separation in Hilbert space (supported by clear trace distance optimization and t-SNE visualizations)
- **Medium confidence:** The circuit depth reduction claims (supported by comparison to Mottonen encoding, but hardware-specific results vary)
- **Low confidence:** Generalization to multi-class problems beyond binary classification (only tested on binary MNIST/MedMNIST pairs)

## Next Checks

1. **Hard mining ablation:** Implement random triplet selection and compare accuracy to verify the specific contribution of the "hardest sample" mining strategy to performance gains.
2. **Depth scaling verification:** Systematically measure constructed circuit depth versus number of features to empirically confirm the claimed linear/bounded growth compared to amplitude encoding's exponential scaling.
3. **Multi-class extension:** Test the binary classification pipeline on multi-class MNIST (e.g., 0-4 vs 5-9) to evaluate whether the triplet loss framework extends beyond pairwise class separation.