---
ver: rpa2
title: 'ZeroShotOpt: Towards Zero-Shot Pretrained Models for Efficient Black-Box Optimization'
arxiv_id: '2510.03051'
source_url: https://arxiv.org/abs/2510.03051
tags:
- optimization
- functions
- function
- learning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents ZeroShotOpt, a pretrained transformer model
  for zero-shot black-box optimization. The method trains on 20 million synthetic
  optimization trajectories generated via 12 Bayesian optimization variants on Gaussian
  process functions across 2D-20D.
---

# ZeroShotOpt: Towards Zero-Shot Pretrained Models for Efficient Black-Box Optimization

## Quick Facts
- arXiv ID: 2510.03051
- Source URL: https://arxiv.org/abs/2510.03051
- Reference count: 40
- Primary result: ZeroShotOpt achieves 0.647±0.011 mean normalized performance on GP functions, outperforming all baselines zero-shot

## Executive Summary
ZeroShotOpt presents a pretrained transformer model for zero-shot black-box optimization that eliminates the need for hand-tuned hyperparameters. The method trains on 20 million synthetic optimization trajectories generated from 12 Bayesian optimization variants on Gaussian process functions across 2D-20D. At inference, the model generates 4 candidate points per step, selecting the best via expected improvement. Evaluated on GP, BBOB, VLSE, and HPO-B benchmarks, ZeroShotOpt achieves strong performance while offering faster runtime than GP-based BO methods.

## Method Summary
ZeroShotOpt adapts Decision Transformer to black-box optimization by framing it as sequence modeling. The model trains on expert trajectories from 12 Bayesian optimization variants (EI, LogEI, UCB, JES, MES, TS with RBF/Matern kernels) on synthetic GP functions. Inputs include regret, length, actions (per-dimension), and states, embedded via sinusoidal + positional encodings. The transformer predicts binned action/state distributions using cross-entropy loss. At inference, the model generates 4 candidates per step, selecting via expected improvement. Training uses 200M parameters, 16 layers, 16 heads, 1024 dim, and takes ~3 days on 4×H100 GPUs.

## Key Results
- ZeroShotOpt achieves 0.647±0.011 mean normalized performance on GP functions, outperforming all baselines
- The method matches or exceeds top BO methods on out-of-distribution tests (BBOB, VLSE, HPO-B)
- ZeroShotOpt offers 15-50× faster runtime than GP-based BO methods while maintaining strong performance

## Why This Works (Mechanism)

### Mechanism 1
Offline RL pretraining on diverse expert trajectories yields a generalizable optimization policy. The model conditions on trajectory regret and step length, learning to predict actions that produce high-quality optimization sequences. By training on 12 BO variants with different exploration-exploitation strategies, the transformer internalizes diverse search patterns without hand-tuned hyperparameters. Core assumption: GP-based synthetic functions sufficiently cover real-world black-box landscape diversity.

### Mechanism 2
GP-based synthetic function generation provides curriculum diversity for robust policy learning. Randomizing kernel type (78 total combinations), length scale (0.1-10), and input point density (10d-30d) creates varied smoothness, modality, and scaling properties. This prevents overfitting to specific landscape geometries. Core assumption: Kernel diversity correlates with optimization difficulty diversity.

### Mechanism 3
Inference-time parallel candidate generation with EI selection improves sample efficiency over single-point prediction. At each step, the model generates 4 candidate points with predicted state distributions. An expected improvement acquisition function selects the best candidate, combining learned policy with classical BO reasoning. Core assumption: Model-predicted state distributions are calibrated enough for EI to meaningfully rank candidates.

## Foundational Learning

- **Offline Reinforcement Learning / Decision Transformer**: Why needed: ZeroShotOpt adapts Decision Transformer to frame optimization as sequence modeling, conditioning on desired regret. Quick check: Can you explain how a transformer predicts actions conditioned on target return, without online environment interaction?

- **Gaussian Processes as Function Priors**: Why needed: Training data is generated via GP samples; understanding kernel properties explains function diversity. Quick check: What landscape properties change when switching from RBF to Matérn kernels with varying length scales?

- **Acquisition Functions (Expected Improvement)**: Why needed: EI is used both in baseline BO and in ZeroShotOpt's inference-time candidate selection. Quick check: How does EI balance exploration vs. exploitation differently from UCB?

## Architecture Onboarding

- **Component map**: Input tokens (regret, length, actions, states) -> Sinusoidal + positional embeddings -> 16-layer causal decoder transformer -> Binned distributions -> Top-p sampling -> EI selection

- **Critical path**: 1. Normalize actions/states to [0,1] using training bounds 2. Embed via sinusoidal + positional encodings 3. Forward through transformer 4. Sample from predicted distribution (top-p=0.9) 5. Convert bin to continuous value 6. Generate 4 candidates, compute EI, select best

- **Design tradeoffs**: Binning (2000 bins) enables cross-entropy loss but discretizes continuous space; single model across 2D-20D improves parameter efficiency but may dilute dimension-specific patterns; inference scaling strategy adds hyperparameters but empirically performs best

- **Failure signatures**: Performance degradation on functions with properties absent from GP training (discontinuities, categorical variables); runtime scaling issues (20D is ~15× slower than 2D); state distribution miscalibration causing EI selection to become unreliable

- **First 3 experiments**: 1. In-distribution validation: Run ZeroShotOpt on held-out GP functions (2D, 5D, 10D, 20D) with 50 evaluations; compare normalized performance vs. GP-LogEI baseline 2. Ablation on candidate count: Test inference with 1, 2, 4, 8 candidates per step; measure performance vs. runtime tradeoff 3. Out-of-distribution stress test: Evaluate on BBOB functions not resembling GP priors; compare vs. CMA-ES and PSO

## Open Questions the Paper Calls Out

- **Can incorporating semantic metadata (parameter names, historical experiment data) into ZeroShotOpt significantly improve optimization performance?**: Authors state "ZeroShotOpt could be augmented to include semantic information that is difficult to incorporate with BO-based methods." The current model architecture only processes numerical query-value pairs.

- **Does generating synthetic functions from distributions beyond GP posteriors improve zero-shot generalization?**: Authors note "further work in generating synthetic functions could improve the function diversity" and acknowledge "while GPs may not perfectly represent real-world function spaces."

- **Can ZeroShotOpt be extended to reliably outperform the best-tuned BO variant on individual problem distributions?**: Authors state "while ZeroShotOpt is competitive with all baseline BO variants, it is occasionally outperformed by the methods with the best variants for individual evaluation distributions."

## Limitations

- GP-generated functions may not fully capture real-world black-box landscape diversity, particularly for functions with discontinuities or discrete variables
- State distribution calibration is critical for EI-based candidate selection, but no quantitative validation of prediction accuracy is provided
- Runtime claims are based on synthetic GP functions; real-world black-box problems may have different computational overhead

## Confidence

- High confidence: In-distribution performance on GP functions (extensive evaluation, multiple random seeds reported)
- Medium confidence: Out-of-distribution generalization to BBOB and HPO-B (fewer random seeds, benchmarks may not fully represent target domain diversity)
- Low confidence: Runtime efficiency claims without real-world black-box function evaluation overhead included

## Next Checks

1. Test state distribution calibration by comparing predicted vs. empirical state histograms across multiple held-out GP functions
2. Evaluate on real-world black-box problems with known optima (e.g., hyperparameter tuning for deep learning models) to verify practical runtime and performance claims
3. Compare ZeroShotOpt against specialized solvers on BBOB functions with known structural properties (e.g., high-conditioning, multi-modality) to identify specific failure modes