---
ver: rpa2
title: On the Adversarial Robustness of Large Vision-Language Models under Visual
  Token Compression
arxiv_id: '2601.21531'
source_url: https://arxiv.org/abs/2601.21531
tags:
- token
- tokens
- compression
- visual
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the adversarial robustness of large vision-language
  models (LVLMs) under visual token compression. Existing encoder-based attacks overestimate
  robustness because they optimize perturbations over full token sets while inference
  is performed on compressed representations, creating an optimization-inference mismatch.
---

# On the Adversarial Robustness of Large Vision-Language Models under Visual Token Compression

## Quick Facts
- arXiv ID: 2601.21531
- Source URL: https://arxiv.org/abs/2601.21531
- Reference count: 40
- Primary result: CAGE attack achieves up to 39.2% lower robust accuracy than baseline encoder-based attacks on compressed LVLMs

## Executive Summary
This paper identifies a critical optimization-inference mismatch in evaluating the adversarial robustness of large vision-language models (LVLMs) under visual token compression. While encoder-based attacks optimize perturbations over full token sets, inference is performed on compressed representations, leading to overestimated robustness. The authors propose CAGE (Compression-Aligned attack), which aligns perturbation optimization with compression inference without requiring knowledge of the deployed compression mechanism or token budget. CAGE combines Expected Feature Disruption (EFD) to concentrate distortion on tokens likely to survive compression and Rank Distortion Alignment (RDA) to ensure adversarial evidence passes the compression bottleneck.

## Method Summary
CAGE is a gray-box attack that targets LVLMs with visual token compression. It uses white-box access to the vision encoder E and black-box access to the compression module C and LLM F. The attack combines two objectives: EFD, which weights the feature distortion loss by the probability that each token survives compression (derived from a prior distribution over possible token budgets), and RDA, which aligns the selection score distribution with the distortion distribution to promote retention of highly distorted tokens. The attack is implemented using PGD with ℓ∞ constraint ε=2/255, step α=0.5/255, and T=100 iterations. CAGE is evaluated against five compression mechanisms (VisionZIP, VisPruner, DivPrune, FlowCut, PruMerge) on three datasets (VQA-v2, TextVQA, GQA) using LLaVA as the primary victim model.

## Key Results
- CAGE consistently achieves lower robust accuracy than the baseline VEAttack across all tested compression mechanisms and datasets
- Maximum robust accuracy reduction of 39.2% on TextVQA dataset
- Compression-aware defenses show an informativeness-robustness trade-off, with robustness-aware selection improving robustness but degrading clean accuracy
- Single-statistic detection frameworks fail under cross-attack evaluation, highlighting the need for multi-layer/multi-statistic detectors

## Why This Works (Mechanism)

### Mechanism 1: Distortion Concentration via Optimization-Inference Mismatch
Existing encoder-based attacks distribute perturbation energy across all visual tokens, but compression modules select tokens based on attention-based saliency. Since adversarial perturbations correlate with salient regions, compression acts as a "distortion concentrator," filtering out clean tokens and leaving a survivor set with higher adversarial noise density. This mismatch leads to overestimated robustness in baseline attacks.

### Mechanism 2: Expected Feature Disruption (EFD)
EFD models the unknown deployment token budget as a probabilistic distribution rather than a fixed guess. It replaces deterministic Top-K selection with "survival probability" π_i(x) derived from a prior distribution P(K_model). By weighting the cosine distance loss by π_i, optimization allocates perturbation budget to tokens ranked highly enough to survive even extreme pruning, mitigating "budget dilution" on tokens destined for deletion.

### Mechanism 3: Rank Distortion Alignment (RDA)
RDA explicitly aligns the selection score distribution with the distortion distribution to stabilize the attack. It converts token distortions and attention scores into probability distributions via softmax, then maximizes the log-likelihood of the selection distribution given the distortion distribution. This forces the model to assign higher attention scores to already-heavily-distorted tokens, ensuring adversarial evidence passes the compression bottleneck.

## Foundational Learning

- **Concept:** **Projected Gradient Descent (PGD)**
  - **Why needed here:** You must understand how to iteratively update the adversarial noise δ while keeping it within an imperceptible ℓ∞ bound.
  - **Quick check question:** Does the attack optimize the image pixels directly or the latent token features?

- **Concept:** **Visual Token Pruning/Merging**
  - **Why needed here:** The attack exploits the specific mechanics of "Plug-and-Play" compression (selecting K tokens from N) to mislead the model.
  - **Quick check question:** If a model uses VisionZip, does it discard low-attention tokens or merge them?

- **Concept:** **Cosine Similarity in Latent Space**
  - **Why needed here:** The EFD objective relies on maximizing the distance (1 - cosine similarity) between clean and adversarial features.
  - **Quick check question:** Why is cosine distance preferred over Euclidean distance for measuring semantic deviation in visual features?

## Architecture Onboarding

- **Component map:** Input Image -> Vision Encoder E -> Feature Vectors z & Attention Scores s -> Compression Module C -> Top-K Selection -> LLM F -> Output

- **Critical path:** Input Image -> Forward Pass (Encoder) -> Extract Features & Attention -> Compute Survival Probabilities -> Calculate EFD & RDA Losses -> Backprop to Image -> Project Perturbation

- **Design tradeoffs:**
  - **Prior Range [K_min, K_max]:** A narrow range focuses distortion but risks missing the actual budget; a wide range dilutes the attack.
  - **Lambda (λ):** Balances payload (EFD) vs. delivery (RDA). High RDA weight may distort the semantic structure required to generate high attention scores.

- **Failure signatures:**
  - **Robust Accuracy Plateau:** Attack effectiveness saturates at extremely low token budgets (e.g., 16 tokens) because the model relies on language priors rather than visual tokens (approaching "Blind" behavior).
  - **Budget Dilution:** Robust accuracy remains high (attack fails) because perturbations were spread over tokens that the compressor ultimately discarded.

- **First 3 experiments:**
  1. **Verify Mismatch:** Compare baseline attack vs. CAGE on a compressed LVLM (e.g., LLaVA with VisionZip) at K=64 to reproduce the robust accuracy gap.
  2. **Ablate RDA:** Run CAGE with λ=0 (EFD only) vs. λ=0.005 to measure the contribution of rank alignment.
  3. **Budget Sensitivity:** Sweep deployment budgets K ∈ {576, 192, 64, 16} to observe where the attack gains are maximized.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the optimization-inference mismatch manifest in inner-LLM or training-based compression architectures compared to the plug-and-play methods studied?
  - **Basis in paper:** [explicit] Section G states the study has not explored "inner-LLM or training-based compression methods (e.g., FastV... or MobileVLM)" because these methods modify token dynamics in fundamentally different ways.
  - **Why unresolved:** The proposed CAGE attack targets the token selection stage common in outer-LLM methods; inner-LLM methods integrate compression into transformer layers, potentially altering the vulnerability surface.
  - **What evidence would resolve it:** Experiments applying compression-aware attacks to models like FastV or MobileVLM to measure if the robustness overestimation gap persists or changes magnitude.

- **Open Question 2:** Does the compression bottleneck amplify error accumulation or degrade temporal consistency in video, multi-image, or agentic settings?
  - **Basis in paper:** [explicit] Section G notes the study is limited to single-image tasks and calls for research into how "temporal consistency across frames, cross-image evidence aggregation, and error accumulation over multiple steps" interact with token compression.
  - **Why unresolved:** Current evaluations are static (VQA); dynamic settings involve visual evidence evolving over time, where losing tokens might have compounding negative effects on reasoning.
  - **What evidence would resolve it:** Evaluation of compressed LVLMs on video understanding benchmarks or multi-turn agentic tasks to measure performance degradation under compression-aligned attacks.

- **Open Question 3:** Can detection frameworks utilizing multi-layer or multi-statistic signals effectively generalize to unseen attack strategies?
  - **Basis in paper:** [explicit] Section F.2 shows that a detector based on a single attention statistic (Top-K mass) fails under cross-attack evaluation (high False Positive Rate). Section G explicitly calls for "combining multi-layer/multi-statistic signals" to develop more robust detection frameworks.
  - **Why unresolved:** The paper demonstrates that single statistics are brittle to distribution shifts in attack strategies (e.g., training on VEAttack, testing on CAGE).
  - **What evidence would resolve it:** Development and testing of a multi-modal detector that analyzes variance across multiple layers or combines attention mass with feature magnitude, showing stable performance against diverse, unseen attacks.

## Limitations

- **Compression mechanism implementation gaps:** The paper specifies five compression methods but does not provide full implementation details or official code links, potentially affecting reproducibility.
- **Attention score source ambiguity:** The exact vision encoder attention layer used (CLS-to-patch vs. averaged across heads) is not explicitly clarified, which could lead to variations in attack effectiveness.
- **Hyperparameter sensitivity:** The performance of CAGE depends on the chosen prior range for the deployment token budget and the RDA trade-off parameter λ, with sensitivity to these choices not extensively explored.

## Confidence

**High Confidence:**
- The existence of an optimization-inference mismatch between encoder-based attacks and compressed LVLMs is well-supported by empirical results showing higher robust accuracy for baseline attacks compared to CAGE.
- The claim that CAGE consistently achieves lower robust accuracy than the baseline attack, with average reductions up to 39.2% on TextVQA, is directly supported by experimental data.

**Medium Confidence:**
- The effectiveness of EFD is supported by ablation studies, but its specific contribution relative to RDA is not isolated in a controlled experiment.
- The assertion that CAGE operates without requiring knowledge of the deployed compression mechanism or token budget is theoretically sound but not validated against extreme budget choices outside the assumed prior range.

**Low Confidence:**
- The claim that RDA is "required to stabilize the attack" is plausible given the mechanism description, but the paper lacks direct ablation evidence isolating RDA's effect from other factors.

## Next Checks

1. **Verify Compression Mechanism Integration:** Implement or obtain official code for each of the five compression methods (VisionZIP, VisPruner, DivPrune, FlowCut, PruMerge) and validate their integration with the LLaVA token pipeline. Ensure attention-based token selection scores are correctly extracted and used for Top-K selection and merging steps.

2. **Ablate RDA in Isolation:** Conduct a controlled ablation study where CAGE is run with RDA disabled (λ=0) and compared against full CAGE with RDA (λ=0.005) and baseline VEAttack. This will isolate RDA's contribution to attack effectiveness across multiple compression budgets.

3. **Stress Test Budget Prior Sensitivity:** Systematically vary the assumed prior range for the deployment token budget [K_min, K_max] in the EFD calculation (e.g., [16,192], [32,128], [64,96]). For each range, run CAGE and measure resulting robust accuracy to assess sensitivity to this hyperparameter.