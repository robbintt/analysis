---
ver: rpa2
title: A Simple DropConnect Approach to Transfer-based Targeted Attack
arxiv_id: '2504.18594'
source_url: https://arxiv.org/abs/2504.18594
tags:
- adversarial
- attack
- surrogate
- methods
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MCD, a simple DropConnect approach, significantly enhances targeted
  transfer-based attacks by mitigating perturbation co-adaptation. By creating diverse
  surrogate model variants through random masking of model parameters, MCD improves
  adversarial example transferability, particularly from CNN-based to Transformer-based
  models.
---

# A Simple DropConnect Approach to Transfer-based Targeted Attack

## Quick Facts
- arXiv ID: 2504.18594
- Source URL: https://arxiv.org/abs/2504.18594
- Authors: Tongrui Su; Qingbin Li; Shengyu Zhu; Wei Chen; Xueqi Cheng
- Reference count: 40
- Primary result: MCD improves targeted transfer-based attack success rates by 13% on average compared to state-of-the-art baselines

## Executive Summary
This paper introduces MCD (Masked Co-adaptation Disruption), a simple DropConnect-based approach that significantly enhances targeted transfer-based adversarial attacks. The key insight is that adversarial perturbations often overfit to the surrogate model by developing highly dependent pixel interactions. MCD addresses this by randomly masking model weights during optimization, creating diverse surrogate variants that force perturbations to learn robust, transferable features. Experiments show MCD achieves 13% higher average attack success rates compared to baselines across various CNN and Transformer architectures.

## Method Summary
MCD applies DropConnect (random weight masking) to Linear and Normalization layers of the surrogate model during targeted attack optimization. At each iteration, multiple masked variants of the surrogate are created, gradients are averaged across these variants, and the adversarial example is updated. This self-ensembling approach mitigates perturbation co-adaptation - where pixels develop brittle, model-specific dependencies. The method uses MI-FGSM as the base optimizer with Logit loss, and requires tuning DropConnect probability per architecture (typically 0.01-0.07).

## Key Results
- Achieves 13% higher average attack success rates compared to state-of-the-art baselines
- Shows consistent improvement across CNN-to-CNN, CNN-to-Transformer, and Transformer-to-Transformer transfer scenarios
- Outperforms defensive mechanisms including adversarial training, input transformation, and detection methods
- Ablation study confirms optimal application to Linear and Normalization layers rather than Convolutional layers

## Why This Works (Mechanism)

### Mechanism 1: Perturbation Co-adaptation Mitigation
The paper hypothesizes that adversarial perturbations overfit to surrogate models by developing dependent pixel interactions. DropConnect breaks these dependencies by forcing the optimization to consider diverse model variants, creating perturbations that survive structural changes.

### Mechanism 2: Implicit Self-Ensembling
By generating multiple masked model variants at each iteration, MCD approximates an ensemble attack without training multiple models. This smooths the loss landscape and encourages perturbations valid across a distribution of model parameters.

### Mechanism 3: Semantic Feature Disturbance
Applying DropConnect to high-level Linear and Normalization layers perturbs semantic features while preserving input structure. This specifically targets co-adapted features without destroying image recognizability.

## Foundational Learning

- **Transfer-based Black-box Attacks:** The setting where attackers have full access to a surrogate model but zero access to the target model. Understanding this is critical because high white-box success doesn't guarantee black-box success.
- **DropConnect vs. Dropout:** DropConnect masks weights while Dropout masks activations. This distinction is critical for correct MCD implementation.
- **Co-adaptation in Neural Networks:** Units relying too heavily on specific other units. MCD applies this concept to perturbations, arguing that reducing co-adaptation improves adversarial robustness.

## Architecture Onboarding

- **Component map:** Clean Image -> Surrogate Model (with MCD wrapper) -> Target Model
- **Critical path:** 1) Identify Linear/Norm layers, 2) Implement fresh DropConnect masks per iteration, 3) Aggregate gradients from S variants
- **Design tradeoffs:** DropConnect Probability (too low fails to break co-adaptation, too high destroys utility), Compute vs Transferability (higher S increases both)
- **Failure signatures:** High white-box but low black-box ASR indicates overfitting, convergence failure suggests probability too high
- **First 3 experiments:** 1) Pilot validation showing masking ratio affects ASR, 2) Layer ablation comparing FC vs Conv layer application, 3) Probability sweep across 0.01-0.09 range

## Open Questions the Paper Calls Out

### Open Question 1
Why does applying DropConnect to convolutional layers result in lower attack success rates compared to linear or normalization layers? The paper conjectures Conv layers have sparser weights but lacks formal analysis.

### Open Question 2
Can the DropConnect probability be determined adaptively rather than through manual tuning? Current approach requires model-specific grid search.

### Open Question 3
Does the "perturbation co-adaptation" hypothesis provide a mechanistic explanation distinct from standard ensemble regularization? The paper doesn't fully distinguish between co-adaptation breaking and general regularization effects.

## Limitations

- Method requires careful tuning of DropConnect probability per architecture, limiting generalization
- Computational overhead scales linearly with inferences per iteration (S=5)
- Theoretical explanation of co-adaptation remains incomplete despite strong empirical results

## Confidence

**High Confidence:** Empirical demonstration of 13% improvement across diverse architectures and defensive mechanisms
**Medium Confidence:** Theoretical explanation of perturbation co-adaptation as primary mechanism
**Low Confidence:** Claims about superiority over alternative ensemble approaches lack direct comparative analysis

## Next Checks

1. **Architectural Generalization Test:** Apply MCD to broader model families including MobileNet variants, hybrid CNN-Transformer architectures, and Vision Transformers with different patch sizes

2. **Theoretical Formalization:** Develop mathematical framework relating weight masking diversity to decision boundary smoothness with quantitative co-adaptation metrics

3. **Computational Efficiency Analysis:** Comprehensive cost-benefit analysis comparing MCD to alternatives across different hardware platforms and attack scenarios<|end_of_text|><|begin_of_text|>4. **Dynamic Probability Scheduling:** Implement adaptive mechanism for DropConnect probability that maintains diversity-utility balance during optimization without manual tuning

5. **Co-adaptation Measurement:** Develop experiments isolating perturbation dependency structures to demonstrate MCD specifically reduces high-order dependencies more than alternative regularization methods