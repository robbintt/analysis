---
ver: rpa2
title: Knowing or Guessing? Robust Medical Visual Question Answering via Joint Consistency
  and Contrastive Learning
arxiv_id: '2508.18687'
source_url: https://arxiv.org/abs/2508.18687
tags:
- medical
- arxiv
- learning
- consistency
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper identifies a critical robustness issue in medical visual
  question answering (Med-VQA): current state-of-the-art models exhibit inconsistent
  answers to semantically equivalent question rephrasings, with performance drops
  up to 40% in recall when evaluated on a newly constructed robustness dataset, RoMed.
  To address this, the authors propose a joint Consistency and Contrastive Learning
  (CCL) framework that integrates knowledge-anchored consistency learning to align
  model outputs across perturbations with medical knowledge, and bias-aware contrastive
  learning to refine representations through comparative understanding.'
---

# Knowing or Guessing? Robust Medical Visual Question Answering via Joint Consistency and Contrastive Learning

## Quick Facts
- **arXiv ID:** 2508.18687
- **Source URL:** https://arxiv.org/abs/2508.18687
- **Reference count:** 32
- **Key outcome:** CCL achieves SOTA on Med-VQA benchmarks and improves answer consistency by 50% on RoMed, addressing robustness issues with semantically equivalent rephrasings.

## Executive Summary
This paper identifies a critical robustness problem in medical visual question answering (Med-VQA): current models show inconsistent answers to semantically equivalent question rephrasings, with up to 40% performance drops. The authors propose a joint Consistency and Contrastive Learning (CCL) framework that combines knowledge-anchored consistency learning with bias-aware contrastive learning to improve both accuracy and robustness. Evaluated on three major Med-VQA benchmarks (Rad-VQA, SLAKE, PathVQA), CCL achieves state-of-the-art performance while significantly improving answer consistency metrics on a newly constructed robustness dataset.

## Method Summary
The proposed CCL framework addresses Med-VQA robustness through two complementary learning strategies. First, knowledge-anchored consistency learning aligns model outputs across semantically equivalent question perturbations using external medical knowledge as an anchor, ensuring consistent answers despite phrasing variations. Second, bias-aware contrastive learning refines representations by encouraging the model to distinguish between correct and incorrect answer candidates through comparative understanding. The method is evaluated on three major Med-VQA benchmarks and a new robustness dataset (RoMed) constructed using GPT-4o to generate semantically equivalent question rephrasings.

## Key Results
- CCL achieves state-of-the-art performance on three major Med-VQA benchmarks (Rad-VQA, SLAKE, PathVQA)
- Answer consistency improves by 50% on the RoMed test set (measured by CV and MAD metrics)
- Model exhibits only marginal gains when perturbation data is doubled, indicating potential saturation effects
- Performance drops up to 40% in recall when evaluated on RoMed, highlighting robustness issues in current models

## Why This Works (Mechanism)
The CCL framework improves robustness by addressing two key failure modes in Med-VQA models. Knowledge-anchored consistency learning ensures that semantically equivalent questions receive consistent answers by anchoring predictions to medical knowledge bases, preventing arbitrary variations in model responses. Bias-aware contrastive learning reduces the impact of dataset biases by forcing the model to learn discriminative features that distinguish correct answers from plausible but incorrect alternatives. Together, these mechanisms create a more robust representation that generalizes better to question variations while maintaining clinical accuracy.

## Foundational Learning
- **Semantic equivalence in medical questions**: Different phrasings can refer to the same clinical finding; models must recognize these equivalences to provide consistent answers. Why needed: Clinical queries naturally vary in wording while seeking identical information.
- **Knowledge-anchored learning**: Using external medical knowledge bases to ground model predictions ensures clinical accuracy and consistency. Why needed: Prevents models from hallucinating answers based solely on training data biases.
- **Contrastive representation learning**: Learning by comparing correct and incorrect answers helps models develop discriminative features. Why needed: Standard cross-entropy loss doesn't explicitly teach models to distinguish between similar but incorrect options.
- **Consistency metrics (CV/MAD)**: Quantitative measures of answer stability across perturbations. Why needed: Traditional accuracy metrics don't capture robustness to linguistic variations.
- **Multi-agent collaboration systems**: Using multiple AI agents (like GPT-4o) to generate diverse, semantically equivalent question variations. Why needed: Creates robust test sets that expose model inconsistencies.

## Architecture Onboarding

**Component Map:** Image Encoder -> Question Encoder -> Joint Representation -> Answer Predictor -> Consistency Loss & Contrastive Loss

**Critical Path:** Image and question embeddings are jointly processed to generate answer predictions, with consistency and contrastive losses applied during training to enforce robustness and discriminative learning.

**Design Tradeoffs:** The method trades increased computational complexity (additional consistency and contrastive loss calculations) for improved robustness and consistency. The reliance on external knowledge bases adds dependency but improves grounding.

**Failure Signatures:** Models may still fail on truly novel clinical scenarios not covered by the knowledge base, and the consistency improvements might mask systematic errors if the anchor knowledge itself is flawed.

**First Experiments:**
1. Evaluate baseline model consistency on RoMed dataset to quantify current robustness issues
2. Test CCL framework on individual Med-VQA benchmarks (Rad-VQA, SLAKE, PathVQA) to verify SOTA claims
3. Perform ablation study to isolate contributions of knowledge-anchored consistency vs. contrastive learning components

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can the robustness achieved via synthetic perturbations (generated by GPT-4o) generalize to the natural linguistic variations found in real-world clinical queries from human practitioners?
- Basis in paper: The RoMed dataset is constructed using a "medical multi-agent collaboration system" involving "GPT-4o" rather than human annotators.
- Why unresolved: The evaluation is performed on the model-generated test set of RoMed, leaving a potential distribution shift between synthetic rephrasings and actual clinical phrasing unresolved.
- What evidence would resolve it: Evaluation of CCL-trained models on a held-out benchmark containing human-generated question rephrasings from doctors or patients.

### Open Question 2
- Question: Does the proposed CCL framework improve robustness against visual perturbations (e.g., imaging artifacts, noise) or is the improvement restricted to textual rephrasings?
- Basis in paper: The method focuses exclusively on "question rephrasings" and text-level perturbations ($T_q$ vs $T_{q_i}$) while keeping the image input ($V_I$) static during the consistency loss calculation.
- Why unresolved: Medical images vary significantly in quality and modality, but the paper does not report results on image corruptions or adversarial visual attacks.
- What evidence would resolve it: Testing the CCL model on datasets with image-level noise, blur, or domain shifts to measure visual robustness alongside textual robustness.

### Open Question 3
- Question: What architectural or algorithmic modifications are required to overcome the observed saturation where doubling perturbation data yields only marginal robustness gains?
- Basis in paper: The authors note in the "Effect of Scaling Data" section that "doubling the dataset size yields only marginal gains" compared to the standard expansion.
- Why unresolved: The paper identifies this plateau but does not explain the underlying mechanism (e.g., model capacity limits or data redundancy) or propose a solution to utilize larger data effectively.
- What evidence would resolve it: Experiments combining CCL with larger backbone models (e.g., 70B parameters) or diverse data curation strategies to determine if the saturation is model-capacity bound.

### Open Question 4
- Question: Does enforcing consistency across perturbations risk reinforcing confident hallucinations if the model's initial anchor knowledge for a specific case is incorrect?
- Basis in paper: The consistency loss ($L_{consistency}$) explicitly forces the model to produce the same output for perturbed inputs, which could theoretically reinforce a wrong answer if the "medical knowledge" alignment fails for that instance.
- Why unresolved: The paper associates low CV/MAD scores with robustness, but does not analyze the "consistency of errors"â€”cases where the model is robustly wrong.
- What evidence would resolve it: A failure analysis quantifying the rate of "consistent false positives" or "consistent hallucinations" compared to the baseline model.

## Limitations
- The 40% performance drop in recall may be influenced by dataset-specific biases in RoMed rather than purely semantic equivalence issues
- The "50% improvement in answer consistency" is measured using specific metrics (CV and MAD) that may not fully capture clinical relevance or real-world robustness
- The effectiveness of knowledge-anchored consistency learning depends heavily on the quality and coverage of the external medical knowledge used, which is not extensively validated
- The paper does not address potential trade-offs between robustness and accuracy in diverse clinical settings

## Confidence
- **High confidence** in the identification of robustness issues in Med-VQA models, as this is supported by empirical evidence from the RoMed dataset
- **Medium confidence** in the effectiveness of the CCL framework, as improvements are demonstrated on benchmark datasets but may not generalize to all clinical scenarios
- **Low confidence** in the long-term clinical applicability of the proposed method, as real-world validation is limited

## Next Checks
1. Conduct a thorough ablation study to isolate the contributions of knowledge-anchored consistency learning and bias-aware contrastive learning to the overall performance improvements
2. Validate the CCL framework on a broader range of clinical datasets, including those with different imaging modalities and question types, to assess generalizability
3. Perform a user study with medical professionals to evaluate the clinical relevance and interpretability of the improved answer consistency in real-world diagnostic scenarios