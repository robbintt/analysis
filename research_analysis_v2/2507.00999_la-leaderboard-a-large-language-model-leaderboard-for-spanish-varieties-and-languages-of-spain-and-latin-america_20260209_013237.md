---
ver: rpa2
title: 'La Leaderboard: A Large Language Model Leaderboard for Spanish Varieties and
  Languages of Spain and Latin America'
arxiv_id: '2507.00999'
source_url: https://arxiv.org/abs/2507.00999
tags:
- dataset
- data
- language
- leaderboard
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: La Leaderboard is the first open-source leaderboard to evaluate
  generative large language models (LLMs) in languages and varieties of Spain and
  Latin America, including Spanish, Catalan, Basque, and Galician. It consists of
  66 datasets across 50 models, covering tasks like question answering, summarization,
  linguistic acceptability, and counter-narrative generation.
---

# La Leaderboard: A Large Language Model Leaderboard for Spanish Varieties and Languages of Spain and Latin America

## Quick Facts
- **arXiv ID:** 2507.00999
- **Source URL:** https://arxiv.org/abs/2507.00999
- **Authors:** María Grandury; Javier Aula-Blasco; Júlia Falcão; Clémentine Fourrier; Miguel González; Gonzalo Martínez; Gonzalo Santamaría; Rodrigo Agerri; Nuria Aldama; Luis Chiruzzo; Javier Conde; Helena Gómez; Marta Guerrero; Guido Ivetta; Natalia López; Flor Miriam Plaza-del-Arco; María Teresa Martín-Valdivia; Helena Montoro; Carmen Muñoz; Pedro Reviriego; Leire Rosado; Alejandro Vaca; María Estrella Vallecillo-Rodríguez; Jorge Vallego; Irune Zubiaga
- **Reference count:** 40
- **Primary result:** First open-source leaderboard evaluating LLMs on Spanish, Catalan, Basque, and Galician with 66 datasets across 50 models

## Executive Summary
La Leaderboard is the first open-source evaluation framework specifically designed for generative large language models in languages and varieties of Spain and Latin America, including Spanish, Catalan, Basque, and Galician. The leaderboard consists of 66 datasets covering diverse tasks such as question answering, summarization, linguistic acceptability, and counter-narrative generation, evaluated across 50 different models. The evaluation uses a calibrated 0-4 shot approach per task to balance accessibility, fairness, and efficiency, with results normalized to enable cross-task comparison. The framework also measures energy consumption on GPU hardware, revealing strong correlations between model size and resource usage, with summarization tasks being the most energy-intensive.

## Method Summary
The leaderboard evaluates generative LLMs using 66 datasets across four languages: Spanish, Catalan, Basque, and Galician. Models are assessed using 0-4 few-shot examples per task, calibrated by task complexity to prevent base models from being disadvantaged while maintaining efficiency. Evaluation metrics include MCQA log probabilities (LOGPROBS), BLEU, ROUGE, and Semantic Answer Similarity (SAS), all normalized against random baselines using the formula: (raw_value - random_baseline) / (max_value - random_baseline). The system uses a fork of the LM Evaluation Harness as backend, hosted on Hugging Face Spaces with Gradio frontend, and runs on MareNostrum 5 HPC with NVIDIA H100 GPUs. Energy consumption is tracked via Energy Aware Runtime (EAR), with GPU memory tracking disabled for quantization evaluation.

## Key Results
- Gemma-2-9B and its instruction-tuned version, along with Llama-3.1-8B-IT and quantized Qwen-2.5 models, achieve the best overall performance across all languages
- Energy analysis reveals strong correlation between model size and consumption, with summarization tasks being the most resource-intensive
- Base models consume more energy than instruction-tuned models due to "verbosity" - generating unnecessary tokens before concluding
- Quantized models (e.g., Qwen-2.5-32B-Int4) outperform full-precision smaller models, demonstrating efficiency trade-offs

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Few-Shot Calibration
Reducing the number of few-shot examples (0-4 shots) maintains evaluation validity for low-resource languages while lowering computational barriers and mitigating order-bias. The authors limit in-context examples based on task complexity and context windows, using 3 shots for convoluted tasks and 0-2 shots for straightforward tasks. This prevents base models from being disadvantaged by long prompts while reducing inference cost and energy consumption.

**Core assumption:** Fewer shots do not significantly degrade the signal-to-noise ratio for the specific linguistic capabilities being measured in Spanish, Catalan, Basque, and Galician.

**Evidence anchors:**
- [section 3.4.1]: "The number of shots should allow for a fair evaluation of the base models without helping instruct models too much... strict limit of 2,048 tokens."
- [abstract]: "...rationale for using fewer few-shot examples... aiming to reduce environmental impact."
- [corpus]: Neighbor papers (e.g., *IberBench*) support the broader move toward efficient evaluation suites for Iberian languages.

**Break condition:** If tasks require complex reasoning formats not captured by the 0-4 shot window, resulting in random baseline performance across all models.

### Mechanism 2: Normalized Aggregation of Heterogeneous Tasks
The leaderboard enables comparison across disparate tasks (QA, summarization, NLI) by normalizing raw scores against random baselines. Results are processed using the formula: normalized_value = (raw_value - random_baseline) / (max_value - random_baseline), creating a unified scale where 0 represents random guessing and 1 represents perfection.

**Core assumption:** The random baseline for a 4-choice MCQA is sufficiently stable to serve as a universal floor for normalization across different model architectures.

**Evidence anchors:**
- [section 3.3.1]: "The results... are normalized according to the following formula... where random_baseline is 0 for generative tasks and 1/n for MCQA tasks."

**Break condition:** If the "random baseline" for specific generative tasks is ill-defined or highly variable, causing skewed average rankings.

### Mechanism 3: Native-First Data Curation
Prioritizing datasets originally written in the target language (rather than translated) improves the measurement of cultural and linguistic adequacy. The system weights dataset composition toward "Original" sources (55%) or human-translated sources (38%), explicitly filtering out machine-translated datasets unless reviewed.

**Core assumption:** Native speakers are more likely to identify culturally relevant nuances that machine translation misses, and these nuances are critical for the "linguistic competence" metric.

**Evidence anchors:**
- [section 3.2.1]: "We only include datasets that have been annotated or revised by at least one native speaker... prioritize datasets originally created in the language."
- [corpus]: *EsBBQ and CaBBQ* and related works confirm the difficulty of finding non-translated benchmarks for these specific language pairs.

**Break condition:** If the "original" datasets suffer from domain narrowness (e.g., over-indexing on news), failing to generalize to broader linguistic variety.

## Foundational Learning

- **Concept: Log-Likelihood Evaluation (LOGPROBS)**
  - **Why needed here:** The backend evaluates Multiple Choice Question Answering (MCQA) not by generating text, but by calculating the probability the model assigns to specific token labels (A, B, C, D). Understanding this is vital for debugging why a model might "fail" a task without ever generating a wrong word.
  - **Quick check question:** Can you explain why `leniachat-qwen2-1.5B` might score higher on MCQA than a larger generative model simply due to tokenization probability shifts?

- **Concept: Cognitive Bias in Few-Shot Prompting**
  - **Why needed here:** The architecture specifically limits shots to combat "recency bias" and "majority label bias" where models simply repeat the answer pattern of the examples rather than solving the prompt.
  - **Quick check question:** If you provided 4 examples where the answer was always "A", how might a base model react compared to an instruction-tuned model?

- **Concept: Quantization and Energy Trade-offs**
  - **Why needed here:** The results show quantized models (e.g., Qwen-2.5-32B-Int4) outperforming full-precision smaller models. To onboard effectively, you must understand that "parameters" are not the only proxy for capability—VRAM usage and precision matter.
  - **Quick check question:** Why does the paper suggest that summarization tasks consume the most energy, and how does token generation speed relate to this?

## Architecture Onboarding

- **Component map:** Gradio frontend -> Hugging Face Spaces -> LM Evaluation Harness fork -> 66 datasets (Hugging Face Hub) -> NVIDIA H100 GPUs (MareNostrum 5) -> Energy Aware Runtime (EAR) for consumption tracking

- **Critical path:**
  1. User submits a Hugging Face model ID via the Gradio form
  2. The backend pulls the model commit hash for reproducibility
  3. The Evaluation Harness runs the model on all 66 datasets using the specified 0-4 shot prompts
  4. Raw scores are generated and normalized using the random baseline formula
  5. Energy data is collected via EAR and correlated with task duration

- **Design tradeoffs:**
  - **Openness vs. Contamination:** The leaderboard is fully open (Apache 2.0) to maximize community engagement, but admits the risk of data contamination (models training on test sets) which they mitigate by using "niche/recent" datasets
  - **Efficiency vs. Depth:** They cap few-shot tokens at 2,048 to fit diverse context windows, potentially sacrificing nuanced reasoning depth for accessibility to smaller models

- **Failure signatures:**
  - **Verbose Base Models:** Section 4.2 notes base models consume more energy than instruct models due to "verbosity"—generating unnecessary tokens before concluding
  - **Context Overflow:** If a model has a tiny context window (<2048 tokens), the standard few-shot prompts will truncate or crash the evaluation run

- **First 3 experiments:**
  1. **Local Reproduction:** Clone the `somosnlp/lm-evaluation-harness` fork and run the `gemma-2-9b-it` against the `xstorycloze_es` task to verify your setup matches the normalized scores in Table 3
  2. **Ablation on Shots:** Re-run a low-resource task (e.g., `eus_reading`) on `salamandra-2b` using 0-shot vs. 4-shot. Compare the drop in performance to validate the paper's claim that fewer shots are sufficient
  3. **Energy Profiling:** Measure the inference time and energy of `Qwen2.5-32B-Int4` vs. `Llama-3.1-8B` on the `xlsum_es` (summarization) task to observe the "size vs. quantization" efficiency crossover

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can benchmarks be designed to effectively measure cultural adequacy specific to individual Spanish-speaking countries?
- **Basis in paper:** [explicit] The authors state in Future Work: "Moreover, we are organizing a hackathon to create a benchmark to measure cultural adequacy in each Spanish-speaking country."
- **Why unresolved:** Current datasets focus on linguistic proficiency (e.g., grammar, reading comprehension) and domain knowledge (e.g., legal, medical), but lack a systematic method to evaluate whether models capture the specific cultural nuances of distinct regions.
- **What evidence would resolve it:** The creation and validation of a new benchmark suite specifically targeting cultural knowledge, including the resulting evaluation scores of current models on this new metric.

### Open Question 2
- **Question:** Do the significant performance discrepancies in math reasoning and language proficiency tasks stem primarily from dataset characteristics or inherent model limitations?
- **Basis in paper:** [explicit] In the Results section (4.1), the authors note: "Further analysis is needed to understand whether these differences are due to the datasets used or indeed to the models’ performance."
- **Why unresolved:** The leaderboard results show low scores in math (potentially due to strict metrics like exact match) and language proficiency tests, but the root cause—whether the data is flawed or the models lack the capability—remains undiagnosed.
- **What evidence would resolve it:** A comparative analysis using alternative metrics (e.g., fuzzy matching for math) or error analysis on the specific dataset samples where models failed to distinguish between data noise and model incompetence.

### Open Question 3
- **Question:** To what extent does the current under-representation of Latin American (LATAM) Spanish datasets bias the comparative evaluation of models?
- **Basis in paper:** [explicit] In the Limitations section: "We estimate that less than 25% of all the Spanish datasets in the leaderboard come from LATAM. We plan to increase this percentage... to reflect the diversity of the Spanish-speaking community."
- **Why unresolved:** The current leaderboard relies heavily on Peninsular Spanish or translated datasets, potentially skewing rankings toward models optimized for those varieties while penalizing models specialized in LATAM dialects.
- **What evidence would resolve it:** A re-evaluation of the current model set using a re-balanced dataset composition with significantly higher LATAM coverage, specifically noting any shifts in the top-performing model rankings.

### Open Question 4
- **Question:** How does data contamination in widely used benchmarks affect the reliability of the leaderboard's current rankings?
- **Basis in paper:** [explicit] In the Limitations section: "Another pending task is to analyse potential contamination... We have not addressed this yet... we have started to evaluate contamination to ensure in the short-term future that we provide high-quality results."
- **Why unresolved:** While the authors use niche datasets to mitigate this, the potential for models having seen test examples during pre-training remains an unmeasured variable affecting the validity of the reported performance spikes.
- **What evidence would resolve it:** The completion of the planned contamination analysis (e.g., using N-gram overlap or membership inference attacks) and the subsequent adjustment of scores or dataset selection for compromised benchmarks.

## Limitations
- **Data contamination risk:** Many high-performing models may have been trained on datasets overlapping with evaluation benchmarks, though mitigation strategies are being developed
- **Normalization assumption:** The random baseline stability across tasks may not hold for complex generative tasks where defining "random" outputs is ambiguous
- **Linguistic coverage bias:** Less than 25% of Spanish datasets come from Latin American sources, potentially skewing model rankings toward Peninsular Spanish varieties

## Confidence
- **High Confidence:** Relative performance rankings of models (e.g., Gemma-2-9B-IT outperforming larger models, quantization benefits) are well-supported by methodology and reproducible results
- **Medium Confidence:** The claim that 0-4 shots are sufficient for fair evaluation across task types requires more extensive ablation studies
- **Low Confidence:** Cultural adequacy claims based on native-first data curation lack direct validation through human evaluation studies

## Next Checks
1. **Shot-Ablation Study:** Systematically evaluate a representative sample of tasks (high, medium, low complexity) using 0, 1, 2, 3, and 4 shots across multiple model sizes to empirically validate the 0-4 shot framework
2. **Contamination Audit:** Perform systematic overlap analysis between the 66 evaluation datasets and the training corpora of the top 10 performing models to quantify actual contamination risk
3. **Cross-Linguistic Generalization:** Evaluate the same model set on a held-out benchmark from a non-Iberian language (e.g., Portuguese or Italian) using identical shot counts and normalization to test framework generalizability