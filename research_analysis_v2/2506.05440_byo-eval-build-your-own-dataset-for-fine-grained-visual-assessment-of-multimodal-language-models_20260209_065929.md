---
ver: rpa2
title: 'BYO-Eval: Build Your Own Dataset for Fine-Grained Visual Assessment of Multimodal
  Language Models'
arxiv_id: '2506.05440'
source_url: https://arxiv.org/abs/2506.05440
tags:
- chess
- accuracy
- gpt-4
- blur
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a diagnostic evaluation framework for Vision-Language
  Models (VLMs) using procedurally generated synthetic images to systematically assess
  visual perception skills such as counting, localization, and identification. By
  controlling visual parameters and varying task difficulty, the approach enables
  fine-grained failure analysis and reveals model biases that aggregate benchmarks
  obscure.
---

# BYO-Eval: Build Your Own Dataset for Fine-Grained Visual Assessment of Multimodal Language Models

## Quick Facts
- arXiv ID: 2506.05440
- Source URL: https://arxiv.org/abs/2506.05440
- Authors: Ludovic Arnould; Salim Khazem; Hugues Ali Mehenni
- Reference count: 40
- Key outcome: Diagnostic evaluation framework using procedurally generated synthetic images reveals fine-grained VLM perception failures in counting, localization, and identification tasks

## Executive Summary
This paper introduces a diagnostic evaluation framework for Vision-Language Models (VLMs) using procedurally generated synthetic images to systematically assess visual perception skills such as counting, localization, and identification. By controlling visual parameters and varying task difficulty, the approach enables fine-grained failure analysis and reveals model biases that aggregate benchmarks obscure. Experiments on eight state-of-the-art VLMs (including GPT-4.1, LLaMA-4, and Gemma) across chess and poker domains show consistent performance degradation under controlled stress conditions like blur and occlusion.

## Method Summary
The framework procedurally generates synthetic chess and poker images using Blender, systematically varying one visual parameter while holding others constant to isolate failure modes. For each image, a JSON legend provides exact ground truth metadata capturing scene parameters. VLMs are queried with formatted prompts including debiased preprompts to suppress prior knowledge. Performance is evaluated across counting (accuracy, MAE, MSE), localization (accuracy, F1, LLOC), and identification (accuracy, F1) metrics, with synthetic-to-real correlation coefficients exceeding 0.99 for leading models.

## Key Results
- GPT-4.1 achieves highest accuracy (>0.8) in most tasks, while open-source models show greater sensitivity to prompting strategies
- Performance consistently degrades under stress conditions like blur and occlusion across all tested VLMs
- Synthetic-to-real image correlation coefficients exceed 0.99 for leading models, validating synthetic data as reliable proxy for real-world evaluation
- LLaMA-4-Scout exhibits strong contextual bias, predicting out-of-bounds positions due to 8x8 chessboard priors

## Why This Works (Mechanism)

### Mechanism 1: Parameter Space Isolation via Procedural Generation
The framework uses Blender to systematically vary a single visual parameter while holding others constant, creating a "stress test" that links performance degradation directly to specific visual attributes. This enables isolation of specific failure modes in VLMs.

### Mechanism 2: Ground-Truth Anchoring via Metadata Legends
Generating a structured "legend" (metadata file) alongside each image provides exact, automated ground truth for evaluating open-ended model responses, bypassing need for human annotation or model-based answer extraction.

### Mechanism 3: Debiased Prompting to Isolate Perception
Specific prompt designs mitigate model's strong priors (e.g., assuming standard 8x8 chessboard), forcing reliance on visual perception rather than memorized knowledge, thereby exposing raw visual capabilities.

## Foundational Learning

- Concept: **Procedural Generation (Blender/bpy)**
  - Why needed here: Core engine of framework; must understand how to script 3D scene creation to define "variable of interest" effectively
  - Quick check question: Can you write a script to render a scene with 5 cubes arranged in a line, varying camera distance from 1m to 10m?

- Concept: **VLM Biases (Priors vs. Perception)**
  - Why needed here: Central thesis is VLMs often answer using "priors" rather than "perception"; understanding this distinction is key to designing right test
  - Quick check question: If a model identifies a chess piece correctly on 4x4 board as "knight" but places it at row 7, is this visual failure or prior bias?

- Concept: **Correlation Analysis (Synthetic-to-Real)**
  - Why needed here: Framework's validity rests on synthetic tests predicting real-world performance; need to understand correlation metrics to validate generated datasets
  - Quick check question: If a model scores 99% on synthetic test but 60% on real images, what does low correlation coefficient tell you about synthetic data validity?

## Architecture Onboarding

- Component map: Config (YAML) -> Generator (Blender `bpy`) -> Image + Legend -> Evaluator (VLM API) -> Analysis
- Critical path: Configuration-to-Legend consistency; if YAML defines "count=5" but Blender script fails to place 5 objects, ground truth is corrupted
- Design tradeoffs: Control vs. Realism; framework uses Chess/Poker domains because they are rule-bound and easy to control
- Failure signatures: Hallucinated Priors; clear signature is when model predicts coordinates (e.g., "row 7") on 4x4 grid
- First 3 experiments:
  1. Reproduce "Localization Bias": Set up 4x4 grid, place piece, run standard prompt; confirm model hallucinates row/col indices >3
  2. Stress Test with Blur: Take counting task (e.g., 5 objects) and systematically increase blur parameter; plot MAE
  3. Correlation Check: Generate 50 synthetic images of simple task; manually recreate 10 scenes with real objects; run both sets through VLM and calculate correlation coefficient

## Open Questions the Paper Calls Out

### Open Question 1
Can module-level failure attribution be achieved for VLMs by training linear probes on visual versus merged representations to isolate whether errors originate in visual encoder, alignment module, or language decoder? The current framework only diagnoses overall model performance on tasks, not which internal component causes failures.

### Open Question 2
Does increasing photorealism of synthetic images alter diagnostic conclusions about VLM failure modes, or do findings from simplified Blender scenes transfer to more realistic industrial scenarios? High synthetic-to-real correlation (>0.99) was demonstrated only on structured chess scenes.

### Open Question 3
How does the diagnostic framework generalize to domains beyond chess and poker, particularly for real-world industrial applications involving irregular object geometries and unstructured layouts? No experiments were conducted outside structured game domains.

## Limitations
- Generalizability of synthetic-to-real correlation claim limited to chess and poker domains demonstrated
- Ground-truth anchoring mechanism depends entirely on accuracy of Blender rendering engine and completeness of legend metadata
- Framework may not capture real-world visual complexities such as dynamic lighting variations, occlusion patterns from moving objects

## Confidence
- High confidence: Parameter space isolation mechanism via procedural generation
- Medium confidence: Debiased prompting mechanism
- Low confidence: Synthetic-to-real proxy validity claim extending beyond demonstrated domains

## Next Checks
1. Cross-domain correlation validation: Generate synthetic dataset for household objects counting and test whether synthetic-to-real correlation >0.99 holds for same models as claimed for chess/poker
2. Legend-ground truth consistency audit: For random sample of 100 generated images, manually verify rendered visual content matches legend metadata
3. Prompt debiasing effectiveness test: Design controlled experiment comparing model performance on standard vs. debiased prompts across multiple grid sizes (3x3, 4x4, 8x8)