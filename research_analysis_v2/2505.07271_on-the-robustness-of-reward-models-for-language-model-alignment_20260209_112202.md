---
ver: rpa2
title: On the Robustness of Reward Models for Language Model Alignment
arxiv_id: '2505.07271'
source_url: https://arxiv.org/abs/2505.07271
tags:
- reward
- https
- language
- robustness
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies over-optimization in reward models (RMs)
  as a key challenge in reinforcement learning from human feedback (RLHF). The authors
  show that excessive dispersion of hidden state norms in RMs trained with the Bradley-Terry
  model leads to poor generalization on unseen data.
---

# On the Robustness of Reward Models for Language Model Alignment

## Quick Facts
- **arXiv ID**: 2505.07271
- **Source URL**: https://arxiv.org/abs/2505.07271
- **Reference count**: 40
- **One-line primary result**: BSR regularization reduces reward model over-optimization, achieving 40% shorter generations with 7% higher win rates in RLHF.

## Executive Summary
This paper identifies over-optimization in reward models (RMs) as a key challenge in reinforcement learning from human feedback (RLHF). The authors show that excessive dispersion of hidden state norms in RMs trained with the Bradley-Terry model leads to poor generalization on unseen data. To address this, they propose batch-wise sum-to-zero regularization (BSR), which constrains reward magnitudes by enforcing zero-centered reward sums per batch. Across four generalization scenarios (in-domain, prompt-disjoint, response-disjoint, and mutual-disjoint), BSR consistently improves robustness. When applied to high-quality data and Llama-3.1-8B models, BSR surpasses state-of-the-art RMs by over 5% in complex preference prediction tasks. In RLHF, BSR reduces generation length by 40% while increasing win rate by 7% on AlpacaEval 2.0, demonstrating that robust RMs lead to more effective and stable alignment.

## Method Summary
The paper proposes batch-wise sum-to-zero regularization (BSR) to mitigate reward model over-optimization. BSR adds a regularization term to the Bradley-Terry loss that penalizes non-zero-centered reward sums per batch, constraining hidden state norm dispersion. The method is evaluated across four generalization scenarios using UltraFeedback data and multiple model sizes (1B-8B parameters). SFT models are trained on UltraChat, then RMs are trained with LBT-BSR and compared against baselines. The approach is validated through both RM evaluation metrics (accuracy, Kendall's τ) and downstream RLHF performance using RLOO.

## Key Results
- BSR consistently improves RM generalization across all four scenarios: in-domain, prompt-disjoint, response-disjoint, and mutual-disjoint
- When applied to Llama-3.1-8B models with high-quality data, BSR surpasses state-of-the-art RMs by over 5% on complex preference prediction tasks
- In RLHF, BSR reduces generation length by 40% while increasing win rate by 7% on AlpacaEval 2.0

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Excessive dispersion of hidden state norms causes reward model over-optimization.
- Mechanism: The Bradley-Terry loss maximizes the reward margin Δr = r(x,yw) - r(x,yl). Since the projection head norm ||Wp|| remains near its initialized value of 1 throughout training, the model achieves larger margins primarily by inflating ||h(x,yw) - h(x,yl)||. This norm inflation produces right-skewed distributions with extreme outliers, analogous to the over-confidence issue in classification. These inflated norms then fail to generalize to unseen prompt/response distributions.
- Core assumption: The relationship between norm dispersion and generalization failure transfers from classification tasks to reward modeling.
- Evidence anchors: [abstract] "excessive dispersion of hidden state norms in RMs trained with the Bradley-Terry model leads to poor generalization on unseen data"; [section 4.1] Figure 2 shows consistent increase in both average norm and variance of ||h(x,yw) - h(x,yl)|| across training epochs.

### Mechanism 2
- Claim: Batch-wise sum-to-zero regularization (BSR) constrains hidden state norm dispersion by penalizing reward outliers proportionally.
- Mechanism: BSR adds a regularization term L_BSR = (1/2|B| * Σr(x,y))² to the BT loss. The gradient ∂L_BSR/∂h(xi,yi,j) = (1/|B|)r(xi,yi,j)·Wp is proportional to the reward magnitude, providing stronger corrective signals for outliers while being symmetric for chosen/rejected pairs. This soft constraint prevents extreme values in either direction without eliminating norm information entirely.
- Core assumption: Soft proportional regularization is more effective than hard bounds or full normalization for preserving useful reward scale information.
- Evidence anchors: [abstract] "batch-wise sum-to-zero regularization (BSR), which constrains reward magnitudes by enforcing zero-centered reward sums per batch"; [section 4.2] Figure 3 shows RMs trained with BSR have significantly lower dispersion across all generalization scenarios.

### Mechanism 3
- Claim: Robust reward models propagate stability to downstream RLHF training through consistent reward signals in high-reward regions.
- Mechanism: Over-optimized RMs provide unreliable gradients when policy outputs enter high-reward regions where ||h|| tends to be large. BSR-trained RMs maintain stable hidden state norms across distributions, enabling sustained alignment with the true preference model throughout RL training. This reduces reward hacking behaviors like verbosity.
- Core assumption: The gold preference model (ArmoRM) used for evaluation accurately represents true preferences.
- Evidence anchors: [abstract] "In RLHF, BSR reduces generation length by 40% while increasing win rate by 7% on AlpacaEval 2.0"; [section 5.2] Figure 5c/5d shows π_BT-BSR maintains increasing alignment with gold RM while π_BT stagnates in later training.

## Foundational Learning

- Concept: Bradley-Terry preference model
  - Why needed here: The paper's core diagnosis is about BT loss dynamics; understanding P(yw≻yl|x) = e^r/(e^r_w + e^r_l) is essential for grasping why margin maximization drives norm inflation.
  - Quick check question: Given two responses with rewards 2.0 and 0.5, what is the BT probability? (Answer: e²/(e²+e⁰·⁵) ≈ 0.82)

- Concept: Reward model over-optimization
  - Why needed here: The paper defines this precisely as in-domain accuracy increasing while out-of-domain (prompt-disjoint, response-disjoint, mutual-disjoint) performance degrades.
  - Quick check question: If a model achieves 85% accuracy on training prompts but 52% on new response styles from unseen models, is this over-optimization? (Answer: Yes, per Section 2.2 definition)

- Concept: Effective rank of representations
  - Why needed here: Table 1 uses effective rank to measure representation quality; large train-eval erank gaps indicate overfitting.
  - Quick check question: If erank_train = 23 and erank_eval = 34, what does the +11 gap suggest? (Answer: Model overfitted to train set, failing to generalize)

## Architecture Onboarding

- Component map: Prompt x + Response y → Backbone LM → Hidden state h(x,y) ∈ R^H → Projection head Wp ∈ R^H×1 → Scalar reward r(x,y) = Wp^T · h → BT Loss: -log σ(Δr) + BSR: (1/2|B|·Σr)² → Combined loss L_BT-BSR

- Critical path: Hidden state extraction → projection → BT margin computation + BSR penalty → gradient backprop through both Wp and backbone. The BSR gradient flows to h(x,y) proportionally to reward magnitude.

- Design tradeoffs:
  - λ hyperparameter (tested: 10⁻², 10⁻³, 10⁻⁴): Higher λ improves hard task accuracy but may reduce easy task performance
  - Soft vs hard constraints: L_BT-Hinge (hard bound) occasionally matches BSR but L_BT-Norm (full normalization) consistently underperforms—norm information should be preserved, not eliminated
  - Batch size affects BSR: Regularization operates per-batch, so batch composition diversity matters

- Failure signatures:
  - Training: ||h(x,yw) - h(x,yl)|| variance growing across epochs
  - Evaluation: >10% drop in Kendall's τ from prompt-disjoint to response-disjoint scenarios
  - RLHF: Policy reward increases but gold RM score stagnates; generation length increases without quality gain

- First 3 experiments:
  1. Replicate norm dispersion dynamics: Train Llama-3.2-1B with L_BT on UltraFeedback subset; log ||h(x,yw)-h(x,yl)|| distribution per epoch. Expect right-skewed growth matching Figure 2.
  2. Ablate λ on held-out generalization splits: Train RMs with λ∈{0, 10⁻⁴, 10⁻³, 10⁻²}; evaluate on all four scenarios. Confirm λ=10⁻³ balances hard/easy accuracy.
  3. Minimal RLHF propagation test: Run RLOO for 1 epoch with Qwen-1.5B policy using RM_BT vs RM_BT-BSR; plot reward trajectories and check if RM_BT shows early KL spike.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the efficacy of BSR persist when evaluated against verified human preferences rather than synthetic "gold" reward models?
- Basis in paper: Section 3.1 explicitly relies on ArmoRM as the ground truth ($r^*$) to define generalization and over-optimization.
- Why unresolved: Synthetic proxies may carry inherent biases (e.g., verbosity) that differ from actual human judgment, leaving the alignment with real human intent uncertain.
- What evidence would resolve it: Evaluation of BSR-trained models on datasets with high-quality, non-synthetic human annotations.

### Open Question 2
- Question: Is there a theoretical scaling law for the regularization hyperparameter $\lambda$ relative to model dimensionality or dataset size?
- Basis in paper: Section 5.3 and Appendix A.2 select the weight $\lambda$ solely through empirical grid search ($10^{-2}, 10^{-3}, 10^{-4}$).
- Why unresolved: The paper provides no principled method for setting $\lambda$, creating a tuning burden when applying BSR to new architectures.
- What evidence would resolve it: A theoretical analysis deriving an optimal or adaptive schedule for $\lambda$ based on gradient dynamics or hidden size.

### Open Question 3
- Question: How does the hidden state norm dispersion manifest in Mixture-of-Experts (MoE) architectures, and does BSR interact negatively with routing mechanisms?
- Basis in paper: Section 3.1 limits experiments to dense transformer models (Llama, Qwen) under 8B parameters.
- Why unresolved: MoE routing may inherently alter hidden state distributions; constraining norms might unintentionally destabilize expert selection or gradient flow.
- What evidence would resolve it: Replicating the norm dispersion analysis and BSR application on MoE-based reward models.

## Limitations

- The core mechanism (norm dispersion causing over-optimization) assumes Bradley-Terry loss dynamics uniquely drive this phenomenon, but cross-validation with other preference losses is absent.
- The ArmoRM gold preference model used for RLHF evaluation is not publicly available, making independent verification difficult.
- The claim that BSR "surpasses state-of-the-art RMs by over 5%" doesn't benchmark against recently published robust RM architectures on identical tasks.

## Confidence

**High confidence**: The norm dispersion mechanism and BSR's ability to constrain it are well-supported by controlled experiments across four generalization scenarios with clear quantitative improvements.

**Medium confidence**: The RLHF propagation claim is supported by qualitative reward trajectory analysis but lacks ablation studies across different RL algorithms or policy initializations.

**Low confidence**: The superiority claim over "state-of-the-art RMs" is weakly supported, as the paper doesn't benchmark against the most recent robust RM architectures on identical tasks.

## Next Checks

1. **Cross-loss validation**: Train RMs with Cross-Entropy RLHF loss and evaluate norm dispersion dynamics. If over-optimization persists without BT-specific dynamics, the core mechanism is incomplete.

2. **ArmoRM independence test**: Replace ArmoRM with an independently trained RM as the gold preference model. If BSR's RLHF gains vanish or reverse, the propagation claim depends critically on the specific gold RM used.

3. **Alternative regularization ablation**: Implement and compare L_BT-Hinge with explicit bounds on ||h(x,yw) - h(x,yl)||, and L_BT-Norm with post-projection normalization. If either matches or exceeds BSR's generalization gains, the soft constraint assumption is unnecessary.