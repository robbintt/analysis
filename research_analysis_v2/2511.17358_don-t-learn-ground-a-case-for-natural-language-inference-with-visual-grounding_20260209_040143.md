---
ver: rpa2
title: 'Don''t Learn, Ground: A Case for Natural Language Inference with Visual Grounding'
arxiv_id: '2511.17358'
source_url: https://arxiv.org/abs/2511.17358
tags:
- language
- visual
- inference
- computational
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a zero-shot method for Natural Language Inference
  (NLI) that leverages visual grounding. Instead of relying on fine-tuning, the approach
  uses text-to-image models to generate visual representations of premise texts, then
  performs inference by comparing these visual representations with textual hypotheses
  using either cosine similarity or visual question answering.
---

# Don't Learn, Ground: A Case for Natural Language Inference with Visual Grounding

## Quick Facts
- arXiv ID: 2511.17358
- Source URL: https://arxiv.org/abs/2511.17358
- Reference count: 25
- The paper introduces a zero-shot method for Natural Language Inference (NLI) that leverages visual grounding.

## Executive Summary
This paper presents a zero-shot approach to Natural Language Inference (NLI) that avoids task-specific fine-tuning by grounding premise texts in visual representations. Instead of learning text-to-text patterns, the method uses text-to-image models to generate visual scenes of premises, then compares hypotheses against these scenes using either cosine similarity (CSS) or visual question answering (VQA). The approach demonstrates high accuracy on SNLI and adversarial datasets while showing robustness against common textual biases and hypothesis-side heuristics that plague fine-tuned models.

## Method Summary
The method consists of two main inference pipelines: CSS (Contrastive Similarity Search) and VQA (Visual Question Answering). For both, premise texts are converted to images using either DALL-E 3 or Stable Diffusion XL. CSS computes image-hypothesis similarity using BLIP embeddings and ranks hypotheses by cosine similarity. VQA uses a vision-language model (GPT-4o) to assess whether each hypothesis is "accurate," "contradicting," or "neither" given the generated image. Five images are generated per premise with aggregation via majority vote, average value, or oracle-guided selection. The approach achieves comparable accuracy to fine-tuned models without task-specific training.

## Key Results
- VQA-DALL-E achieves 85% accuracy on adversarial data vs. 65.5% for fine-tuned RoBERTa
- CSS and VQA show reduced hypothesis-side bias (8.5% vs. 23.3% accuracy delta on easy/hard subsets)
- Visual grounding approach achieves 74.6% overall accuracy, comparable to fine-tuned models
- Both methods struggle with neutral class (~37-58% accuracy) due to forced visual commitments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Visual grounding forces explicit scene representation, reducing reliance on textual surface heuristics.
- Mechanism: Text-to-image generation converts the premise into a concrete visual world state. Inference then compares the hypothesis against this visual scene rather than computing text-to-text patterns. This bypasses lexical overlap and negation-based shortcuts common in fine-tuned NLI models.
- Core assumption: The text-to-image model correctly renders the situation described in the premise.
- Evidence anchors:
  - [abstract] "Our method achieves high accuracy without task-specific fine-tuning, demonstrating robustness against textual biases and surface heuristics."
  - [Section 3.2.2] On uninformative premises ("Something is happening"), RoBERTa shows 23.3% accuracy delta between easy/hard subsets vs. 8.5% for VQA-DALL-E, indicating reduced hypothesis-side bias.
  - [corpus] Weak direct corpus support; related work (VTPerception-R1) shows explicit perceptual grounding improves multimodal reasoning, which aligns conceptually.
- Break condition: If the TTI model introduces systematic artifacts (e.g., concept bleeding where properties incorrectly apply to multiple objects), inference fails regardless of grounding benefits.

### Mechanism 2
- Claim: VQA-based inference leverages pretrained multimodal reasoning capabilities for zero-shot NLI.
- Mechanism: A vision-language model (GPT-4o) receives the generated image and all three hypotheses simultaneously, predicting labels based on learned visual-linguistic alignment. The model applies its pretrained understanding of what statements are consistent with visual scenes.
- Core assumption: The VLM has sufficient compositional understanding to assess hypothesis-image relationships without task-specific training.
- Evidence anchors:
  - [Section 3.3.1] VQA-DALL-E achieves 85% accuracy on adversarial data vs. 65.5% for fine-tuned RoBERTa, which relies on overlap heuristics.
  - [Section 3.1.1] VQA outperforms CSS on entailment (90.1% vs. 69.7%) and contradiction (95.0% vs. 80.6%) but struggles with neutral.
  - [corpus] No directly comparable corpus evidence for this specific VQA-as-NLI mechanism.
- Break condition: If the VLM has its own biases (e.g., over-predicting contradiction when visual details are ambiguous), it will propagate errors without the correction signal from fine-tuning.

### Mechanism 3
- Claim: Multi-image aggregation reduces TTI stochasticity and improves robustness.
- Mechanism: Generating five images per premise and aggregating predictions (majority vote, average value, or oracle-guided) provides a softening mechanism against single-image generation failures.
- Core assumption: Multiple generations produce meaningful diversity in the visual aspects that affect inference.
- Evidence anchors:
  - [Section 4.2] Aggregation is proposed to address neutrality errors where TTI must commit to specific details (lake vs. sea) that may inadvertently entail or contradict hypotheses.
  - [Section 3, Table 1 vs. Table 2] Aggregation improves overall NLI accuracy (74.3% vs. 74.6% for VQA) and neutral class performance.
  - [corpus] No corpus evidence directly addresses multi-image aggregation for NLI.
- Break condition: If TTI produces near-identical images across seeds (low diversity), aggregation provides no benefit. The paper notes this limitation: "even with varying random seeds and temperature values, the model-generated images either consistently omitted or consistently included the problematic visual objects."

## Foundational Learning

- Concept: **Text-to-image generation limitations (concept bleeding, factual errors)**
  - Why needed here: Understanding TTI failure modes is essential for debugging why visually-grounded NLI fails. The paper identifies concept bleeding (properties incorrectly spreading across objects) as a major error source.
  - Quick check question: Given premise "The girl who greets the dog laughs," would you expect both figures to have similar expressions in generated images? If so, what inference errors might result?

- Concept: **NLI dataset biases (hypothesis-side heuristics, lexical overlap)**
  - Why needed here: The paper's core motivation is avoiding biases that fine-tuned models learn. Understanding SNLI's "easy" vs. "hard" subsets helps interpret why grounding helps.
  - Quick check question: If a model can predict NLI labels from the hypothesis alone (without the premise), what does this indicate about the dataset?

- Concept: **Contrastive vision-language embeddings (CLIP, BLIP)**
  - Why needed here: The CSS method uses BLIP embeddings for image-text similarity. Understanding their limitations (compositional weakness, object-overlap bias) explains CSS underperformance.
  - Quick check question: Why might a contrastive model rate "The boat worker works hard to establish the line" as more similar to an image of a worker securing a line than "A worker is doing something to a boat"?

## Architecture Onboarding

- Component map: Premise input -> Text-to-image model (Stable Diffusion XL or DALL-E 3) -> Generated images (5 per premise) -> Either BLIP embeddings (CSS path) or VLM input (VQA path) -> Hypotheses -> Paired with images for similarity scoring (CSS) or prompt construction (VQA) -> Aggregation layer -> Majority vote / average value / oracle-guided (research only)

- Critical path: Premise -> TTI generation -> VQA inference -> Label prediction. The VQA path outperforms CSS and is the recommended implementation.

- Design tradeoffs:
  - DALL-E 3 vs. Stable Diffusion: DALL-E reduces concept bleeding but is proprietary; SDXL is open but more error-prone.
  - Single vs. multi-image: 5 images improve robustness but increase latency and cost 5x.
  - CSS vs. VQA: CSS is faster and fully open-source but exhibits overlap bias; VQA requires proprietary API but achieves higher accuracy.

- Failure signatures:
  - Neutral class collapse: Both methods struggle with neutral hypotheses (~37-58% accuracy) because TTI forces specific visual commitments that may inadvertently entail or contradict.
  - Concept bleeding: TTI applies described properties to unintended objects (e.g., both characters share facial expressions).
  - Non-visual predicates: Hypotheses about thoughts, intentions, or social relations cannot be verified visually.

- First 3 experiments:
  1. **Reproduce VQA pipeline on 20 SNLI examples** using DALL-E 3 and GPT-4o. Manually inspect generated images for factual errors and compare predicted vs. gold labels. Focus on identifying concept bleeding instances.
  2. **Ablate aggregation strategies** on a subset of 50 premises. Compare single-image vs. 5-image majority vote vs. average value aggregation on overall accuracy and neutral-class accuracy specifically.
  3. **Test adversarial robustness** using the paper's template: "The [noun1] who [transitive_verb] the [noun2] [intransitive_verb]." Generate 20 such premises with entailed vs. non-entailed hypotheses and compare VQA-DALL-E against a text-only baseline (BLEU or BERT-based similarity) to quantify overlap bias.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can improved aggregation strategies over multiple diverse generated images effectively resolve the "neutrality errors" inherent in visual grounding for NLI?
- **Basis in paper:** [Explicit] The authors state in the Conclusion that they "believe that aggregation of predictions from multiple diverse images can help bridge this gap" regarding the difficulty of handling neutral hypotheses.
- **Why unresolved:** The current experiments showed that Text-to-Image (TTI) models often fail to generate sufficiently diverse images (e.g., consistently rendering "water" as a "lake"), forcing a specific visual interpretation where the text was agnostic, leading to misclassification.
- **What evidence would resolve it:** A demonstration of a prompting or generation technique that produces visually diverse sets for underspecified premises, resulting in statistically significant accuracy improvements on the Neutral class.

### Open Question 2
- **Question:** Can the visual grounding framework be successfully extended to spatial reasoning tasks or narrative generation, such as maintaining coherence with interleaved illustrations?
- **Basis in paper:** [Explicit] The Conclusion suggests the approach "can be extended to other reasoning tasks where relevant information can be visualized, such as spatial reasoning" and mentions "story generation with interleaved generated illustrations."
- **Why unresolved:** The study restricted its validation to the SNLI dataset, which focuses on descriptive scenes. It is unknown if the pipeline can handle complex spatial predicates or the temporal coherence required for narrative generation.
- **What evidence would resolve it:** Successful application of the zero-shot visual grounding pipeline to a spatial reasoning benchmark (e.g., SpartQA) or a narrative task requiring consistency between generated text and images.

### Open Question 3
- **Question:** Does extending the grounding modality from static images to video or audio improve the ability to handle abstract or dynamic predicates?
- **Basis in paper:** [Explicit] The Conclusion notes, "we find it promising to eventually extend this approach to more diverse types of grounding, such as audio or video."
- **Why unresolved:** The current method relies on static images, which struggle with "abstract entities" and dynamic actions. The paper acknowledges this limitation but does not test alternative modalities.
- **What evidence would resolve it:** Implementation of the inference pipeline using video-generation or audio-generation models, showing improved performance on NLI examples involving temporal changes or non-visual sounds.

## Limitations
- Text-to-image fidelity: The approach depends entirely on TTI models correctly rendering premise semantics, with concept bleeding affecting ~15% of generated images.
- Neutral class collapse: Both CSS and VQA struggle with neutral hypotheses (~37-58% accuracy), suggesting fundamental misalignment with neutral class definition.
- Limited dataset validation: Results are only demonstrated on SNLI and synthetic adversarial data, not tested on other NLI benchmarks or non-English datasets.

## Confidence
- **High confidence**: The core mechanism (VQA-based visual grounding achieving high accuracy without fine-tuning) is well-supported by experimental results.
- **Medium confidence**: Claims about CSS underperformance due to overlap bias are plausible but rely on indirect evidence.
- **Low confidence**: The assertion that multi-image aggregation meaningfully improves robustness is weakly supported.

## Next Checks
1. **Cross-dataset generalization**: Test the VQA pipeline on MultiNLI and ANLI to assess whether visual grounding robustness extends beyond SNLI's specific biases and construction patterns.
2. **Concept bleeding quantification**: Systematically analyze 100 generated images to measure the actual frequency and impact of concept bleeding, then compare against the paper's 15% estimate and assess its correlation with inference errors.
3. **Alternative TTI model comparison**: Replace DALL-E 3 with a different high-quality TTI model (e.g., Imagen 2) on the same 50 premise-hypothesis pairs to determine whether visual grounding benefits are model-specific or represent a general principle.