---
ver: rpa2
title: 'Quartet II: Accurate LLM Pre-Training in NVFP4 by Improved Unbiased Gradient
  Estimation'
arxiv_id: '2601.22813'
source_url: https://arxiv.org/abs/2601.22813
tags:
- nvfp4
- quantization
- pass
- training
- quartet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of stable LLM pre-training\
  \ using the NVFP4 microscaling format, which offers higher computational efficiency\
  \ but suffers from quantization-induced accuracy loss. The authors introduce MS-EDEN,\
  \ a novel unbiased gradient quantization scheme that significantly reduces quantization\
  \ error by applying stochastic rounding to microscaling factors rather than individual\
  \ elements, achieving more than 2\xD7 lower error compared to standard stochastic\
  \ rounding."
---

# Quartet II: Accurate LLM Pre-Training in NVFP4 by Improved Unbiased Gradient Estimation

## Quick Facts
- arXiv ID: 2601.22813
- Source URL: https://arxiv.org/abs/2601.22813
- Reference count: 23
- Up to 4.2× speedup over BF16 on NVIDIA Blackwell GPUs while reducing validation loss increase by 20%+

## Executive Summary
This paper addresses the challenge of stable LLM pre-training using the NVFP4 microscaling format, which offers higher computational efficiency but suffers from quantization-induced accuracy loss. The authors introduce MS-EDEN, a novel unbiased gradient quantization scheme that significantly reduces quantization error by applying stochastic rounding to microscaling factors rather than individual elements, achieving more than 2× lower error compared to standard stochastic rounding. They integrate MS-EDEN into Quartet II, a fully-NVFP4 linear layer training scheme that combines high-capacity forward pass quantization with unbiased backward pass quantization, yielding consistently better gradient estimates.

## Method Summary
Quartet II is a fully-NVFP4 training scheme for LLMs that achieves both accuracy and efficiency. The forward pass uses RTN quantization with native 1x16 microscaling groups and a "4/6" heuristic that selects between scale factors 4.0 and 6.0 based on minimum MSE. The backward pass employs MS-EDEN: randomized Hadamard transforms (RHT) on 128-element chunks, RTN quantization, and EDEN correction factors applied via stochastic rounding to FP8 scales. This preserves unbiasedness while reducing quantization error by >2× compared to standard stochastic rounding.

## Key Results
- Achieves up to 4.2× speedup over BF16 on NVIDIA Blackwell GPUs
- Demonstrates 20%+ lower validation loss increase compared to prior NVFP4 methods across various LLM sizes
- MS-EDEN reduces MSE quantization error by >2× (9.8×10⁻³ vs 23.5×10⁻³) compared to standard stochastic rounding
- Validates unbiasedness through concentration tests showing 1/B convergence vs SR plateauing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Moving stochastic rounding from individual FP4 elements to FP8 microscale factors reduces quantization error by >2× while preserving unbiasedness.
- Mechanism: MS-EDEN applies a randomized Hadamard transform (RHT) to input vectors, quantizes with round-to-nearest (RTN) to FP4, then computes an EDEN correction factor S. Instead of requiring high-precision storage for S (incompatible with NVFP4), it stochastically rounds S into the FP8 group scales. Since FP8 scale precision (minimum relative update ×1.0625) is much finer than FP4 element precision, variance from scale SR is an order of magnitude smaller than element-wise SR.
- Core assumption: Rotation group size (128) is large enough for EDEN's asymptotic unbiasedness guarantee to hold approximately; RHT with identical seeds on both GEMM operands causes rotations to cancel without explicit inverse.
- Evidence anchors:
  - [abstract] "MS-EDEN...applying stochastic rounding to microscaling factors rather than individual elements, achieving more than 2× lower error compared to standard stochastic rounding"
  - [Section 3.3, Table 1] MS-EDEN MSE=9.8×10⁻³ vs SR=23.5×10⁻³ on N(0,1) data
  - [corpus] Related work (TetraJet-v2, NVIDIA recipe) all use element-wise SR; no prior work applies SR to scales specifically for unbiasedness.
- Break condition: If rotation groups cannot be aligned with microscaling groups along the inner GEMM dimension, MS-EDEN becomes inapplicable; if FP8 scale dynamic range is insufficient for EDEN correction factors, clipping introduces bias.

### Mechanism 2
- Claim: Forward pass quantization should prioritize representation capacity via RTN with adaptive scale selection, not unbiasedness.
- Mechanism: The "4/6" heuristic evaluates two candidate scale factors (4.0 and 6.0) per block, selecting the lower-MSE option. This is deterministic and biased but maximizes representable range. Critically, selecting the better scale introduces bias even if both branches use SR, making 4/6 incompatible with unbiased backward passes—but ideal for forward-only use.
- Core assumption: Forward pass error accumulates as representation degradation (amortized), while backward pass bias accumulates as systematic gradient error (unbounded).
- Evidence anchors:
  - [Section 4.2] "this idea is not utilized [in prior work] due to the use of square-block-quantization...we validate this by measuring the quadratic error improvement"
  - [Figure 2] 1x16gs + 4/6 shows ~2× improvement over 16x16gs + 4/6 on C4 validation loss
  - [corpus] Four Over Six paper (Cook et al.) proposed the heuristic but combined with SR backward, which authors note is biased.
- Break condition: If backward pass accidentally uses 4/6, bias accumulates; Figure 9 shows NVIDIA+4/6 plateauing instead of converging as 1/B.

### Mechanism 3
- Claim: Storing temporary E8M3 pseudo-scales during quantization avoids double tensor loads, enabling efficient kernel implementation.
- Mechanism: Native NVFP4 quantization requires a global AbsMax reduction before scale alignment—forcing two kernel passes. The post-hoc range alignment approach: (1) skip range alignment, use E8M3 scales in BF16, compute EDEN correction and reduce global max in same kernel; (2) lightweight second kernel converts E8M3→FP8 with EDEN correction applied via SR. Since kernel 2 only touches scales (N/16 elements vs N), it's >10× faster.
- Core assumption: E8M3 dynamic range (represented in BF16) is sufficient to capture all scale values before final FP8 quantization.
- Evidence anchors:
  - [Section 7, Table 2] "GMEM→SM: 4.5+4.5 [naïve] vs 4.5+1 [post hoc] bits moved per element"
  - [Figure 7 vs 8] Kernel diagram comparison
  - [corpus] No corpus papers discuss this kernel optimization; it appears novel to Quartet II.
- Break condition: If E8M3 range is exceeded during rotation, scale values saturate before correction can be applied.

## Foundational Learning

- Concept: **Stochastic Rounding and Unbiasedness**
  - Why needed here: MS-EDEN's core innovation is preserving unbiasedness while reducing variance. Understanding why E[Q(x)] = x matters for gradient accumulation over millions of steps.
  - Quick check question: If you quantize gradient g repeatedly with operator Q and average, does (1/B)ΣQ(gᵢ) converge to g? Under what condition?

- Concept: **Microscaling Format Structure (NVFP4)**
  - Why needed here: MS-EDEN exploits the two-level scale hierarchy (FP8 per 16 elements, FP32 global). Understanding where precision lives is essential for modifying the scheme.
  - Quick check question: Why can't the EDEN correction factor S be stored in the global FP32 scale instead of the group FP8 scales?

- Concept: **Randomized Hadamard Transform for Smoothing**
  - Why needed here: RHT makes vector coordinates approximately i.i.d., enabling EDEN's co-linearity guarantee. Also used for outlier smoothing in GEMMs.
  - Quick check question: Why must RHT be applied with the same seed to both operands of a GEMM for the rotation to "cancel out"?

## Architecture Onboarding

- Component map:
  Forward Pass: Activations/Weights → 4/6 RTN Quantize → NVFP4 GEMM → Output
                                    ↓
                            (Save quantized tensors)
                                    ↓
  Backward Pass: Saved tensors → Dequantize → Transpose → RHT → MS-EDEN Quantize
                Error signal (E) → RHT → MS-EDEN Quantize ──────────┐
                                                                    ↓
                                                            NVFP4 GEMMs (×2)
                                                                    ↓
                         Gradient output (rotations cancel on inner dim)

- Critical path: The MS-EDEN re-quantization in the backward pass is the accuracy-critical component. Errors in EDEN correction factor computation or RHT seed synchronization will corrupt unbiasedness.

- Design tradeoffs:
  - Square-block quantization (16×16) vs native scales (1×16): Prior work chose square-block to enable weight reuse without re-quantization; Quartet II chooses native scales for better forward capacity, accepting re-quantization cost because MS-EDEN error is low enough.
  - Rotation group size (128) vs quantization group (16): Larger rotations improve EDEN asymptotics but constrain hardware; 128 is the minimum efficient for Blackwell `mma.m16n8k16`.

- Failure signatures:
  - Validation loss diverging slowly over training → likely biased backward quantization (verify with Figure 9 test: does gradient average converge as 1/B?)
  - Sudden loss spikes → scale underflow/overflow in MS-EDEN (check FP8 scale distribution)
  - No speedup despite FP4 → kernel overhead dominates; verify tensor dimensions are large enough for GEMM speedup to amortize quantization cost.

- First 3 experiments:
  1. **Unbiasedness verification**: Implement the Figure 9 test—run B quantized backward passes on a frozen batch, plot ||(1/B)ΣĜ - G||² / ||G||² vs B. Confirm Quartet II follows 1/B scaling; NVIDIA+4/6 should plateau.
  2. **Ablation on rotation group size**: Test d=64, 128, 256 to verify unbiasedness and latency tradeoffs. Authors note d=128 is hardware-motivated; smaller may break unbiasedness.
  3. **Scale distribution monitoring**: Log FP8 scale values and EDEN correction factors S during early training. Verify S stays in representable range (~[0.94, 1.06]) and FP8 scales don't underflow.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Quartet II maintain its accuracy improvements and training stability when scaled to models with 70B+ parameters and trillions of training tokens?
- Basis in paper: [explicit] The authors validate on models "up to 1.9B parameters on 38B tokens" but state the hardware format "promises to allow... end-to-end fully-quantized pre-training of massive models such as LLMs."
- Why unresolved: The validation scale is orders of magnitude smaller than production LLMs where quantization artifacts accumulate over longer training runs.
- What evidence would resolve it: Successful pre-training runs of 7B–70B parameter models with Quartet II showing comparable loss curves to BF16 baselines.

### Open Question 2
- Question: How sensitive is MS-EDEN's unbiasedness guarantee to the hardware-driven compromises made in practice (rotation dimension d=128, identical rotations per group, no SR on underflowing scales)?
- Basis in paper: [explicit] The authors note EDEN guarantees unbiasedness "in the d→∞ limit" but use d=128 "to allow efficient rotation on Blackwell GPUs," and verify unbiasedness only numerically (Appendix A).
- Why unresolved: The theoretical guarantee requires conditions not met in practice; numerical verification on one model architecture doesn't prove general robustness.
- What evidence would resolve it: Systematic ablation of rotation dimension and sampling strategies across diverse model architectures and training regimes.

### Open Question 3
- Question: Can an unbiased variant of the "Four Over Six" scale selection heuristic be developed for the backward pass?
- Basis in paper: [explicit] The authors identify that "4/6" scale selection "does not constitute an unbiased estimation, as the act of picking a lower MSE scale branch introduces bias, even if both scale branches are individually unbiased via SR."
- Why unresolved: The authors circumvent the issue by only using "4/6" on the forward pass, leaving backward pass capacity improvements unexplored.
- What evidence would resolve it: A modified "4/6" scheme with provable unbiasedness, validated on backward pass quantization ablations.

### Open Question 4
- Question: Does the proportion of time spent on non-FP4 operations (attention, normalization) actually decrease as claimed for larger model sizes, and what is the realistic end-to-end speedup ceiling?
- Basis in paper: [inferred] At 1.1B parameters, "about 60% of the time is spent on operations untouched by the FP4 training recipe" and "this ratio is expected to drastically decrease as model size grows"—but this is not validated.
- Why unresolved: The claim is based on intuition about matmul scaling, but attention complexity and memory bandwidth constraints may dominate differently at scale.
- What evidence would resolve it: Kernel timing breakdowns for 7B–70B models showing actual scaling of FP4 vs. non-FP4 operation ratios.

## Limitations

- Relies heavily on specific NVFP4 hardware features (RHT support, scale granularity) that may not generalize to other platforms
- Implementation details for "4/6" heuristic and post-hoc range alignment kernel remain underspecified
- Downstream Nanochat results need external validation beyond the authors' internal tests

## Confidence

- **High**: MS-EDEN's unbiasedness proof and error reduction claims (Section 3, Table 1)
- **Medium**: Overall accuracy improvements and speedup claims (Figures 5-6, Table 2), 4/6 heuristic effectiveness
- **Low**: Nanochat downstream results (Table 6), CUDA kernel optimization specifics

## Next Checks

1. **Unbiasedness Verification**: Implement the gradient concentration test from Figure 9 - run multiple quantized backward passes on a fixed batch and plot mean squared error vs iteration count. Quartet II should show 1/B convergence while SR methods plateau.

2. **Rotation Group Scaling**: Systematically vary RHT group sizes (d=64, 128, 256) to verify the unbiasedness guarantee and identify the minimum group size where EDEN's asymptotic behavior holds. Measure both accuracy impact and computational overhead.

3. **Scale Distribution Monitoring**: During early training, log the distribution of FP8 scales and EDEN correction factors S. Verify S remains within the SRFP8 representable range (approximately [0.94, 1.06]) and that no scale underflow/overflow occurs, which would introduce bias.