---
ver: rpa2
title: 'Bandits with Preference Feedback: A Stackelberg Game Perspective'
arxiv_id: '2406.16745'
source_url: https://arxiv.org/abs/2406.16745
tags:
- theorem
- regret
- feedback
- function
- confidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles online optimization over infinite action spaces
  when only preference feedback (pairwise comparisons) is available. The authors model
  the unknown utility as lying in a Reproducing Kernel Hilbert Space and propose MAXMINLCB,
  a sample-efficient algorithm that selects action pairs via a zero-sum Stackelberg
  game between a Leader (optimizing for utility) and a Follower (providing competitive
  feedback).
---

# Bandits with Preference Feedback: A Stackelberg Game Perspective

## Quick Facts
- arXiv ID: 2406.16745
- Source URL: https://arxiv.org/abs/2406.16745
- Reference count: 40
- Primary result: O(γT√T) dueling regret via a Stackelberg game formulation

## Executive Summary
This paper introduces MAXMINLCB, an algorithm for kernelized dueling bandits where only pairwise preference feedback is available. The method frames action pair selection as a zero-sum Stackelberg game between a Leader (optimizing for utility) and a Follower (providing competitive feedback). A key technical contribution is constructing anytime-valid confidence sequences for kernelized logistic estimators under preference feedback, enabling efficient uncertainty quantification. Experiments on standard optimization benchmarks and real-world Yelp data show MAXMINLCB consistently outperforms existing baselines in balancing exploration and exploitation.

## Method Summary
The MAXMINLCB algorithm selects action pairs by solving a zero-sum Stackelberg game where the Leader maximizes a lower confidence bound while the Follower minimizes it. The utility function is assumed to lie in a Reproducing Kernel Hilbert Space, and preference feedback is modeled via a Bradley-Terry likelihood. The algorithm constructs anytime-valid confidence sequences for the kernelized logistic estimator, enabling the computation of uncertainty-aware action pairs. The method achieves O(γT√T) dueling regret, where γT is the information gain of the kernel.

## Key Results
- Achieves O(γT√T) dueling regret via Stackelberg game formulation
- Novel anytime-valid confidence sequences for kernelized logistic estimators under preference feedback
- Consistently outperforms baselines on synthetic benchmarks and real-world Yelp data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Joint action selection via a zero-sum Stackelberg game can balance exploration and exploitation in preference-based bandits.
- **Mechanism:** At each step, a Leader (maximizing a lower confidence bound) selects an action xt, and a Follower (minimizing the same bound) selects a competitive action x't. This formulation treats the pair selection as a game where the Leader anticipates the Follower's response, encouraging informative pairs that test the Leader's optimism.
- **Core assumption:** The utility function f lies in a Reproducing Kernel Hilbert Space (RKHS) with bounded norm; confidence sequences derived from preference feedback are valid and sufficiently tight.
- **Evidence anchors:**
  - [abstract] "emulates this trade-off as a zero-sum Stackelberg game, and chooses action pairs that are informative and yield favorable rewards"
  - [Section 5.2] "Equation (7) forms a zero-sum Stackelberg (Leader–Follower) game where the actions xt and x't are chosen sequentially"
  - [corpus] Related work "Stackelberg Learning from Human Feedback" similarly frames preference optimization as a sequential game, supporting the game-theoretic approach.
- **Break condition:** If confidence sequences are too loose (e.g., βt grows too quickly) or the kernel fails to capture function structure, the game's equilibrium may not sufficiently reduce uncertainty, leading to linear regret.

### Mechanism 2
- **Claim:** Anytime-valid confidence sequences for kernelized logistic estimators under preference feedback enable efficient uncertainty quantification.
- **Mechanism:** Preference feedback P(xt ≻ x't) = s(f(xt) - f(x't)) is modeled via a Bradley-Terry likelihood. A "dueling kernel" kD is defined, allowing preference-based loss to be treated as kernelized logistic regression. Confidence sets around the estimator ht are constructed using information-gain terms γt and a exploration coefficient βDt, ensuring uniform validity over time and domain.
- **Core assumption:** f ∈ Hk with ||f||k ≤ B; the sigmoid link function is Lipschitz; the kernel's eigenfunctions have zero mean (satisfied for common stationary kernels).
- **Evidence anchors:**
  - [abstract] "due to our novel preference-based confidence sequences for kernelized logistic estimators"
  - [Corollary 5] "gives valid confidence sets for kernelized utility functions under preference feedback"
  - [corpus] "Generalized Kernelized Bandits" provides related self-normalized bounds for exponential-family rewards, supporting the technical approach.
- **Break condition:** If the RKHS assumption is violated (e.g., f is very irregular) or the logistic model is misspecified, confidence sequences may not cover the true f, leading to invalid uncertainty estimates and potential algorithmic divergence.

### Mechanism 3
- **Claim:** The MAXMINLCB algorithm achieves sublinear dueling regret O(γT√T).
- **Mechanism:** The Stackelberg game's equilibrium ensures that selected pairs are both high-utility (exploitation) and high-information (exploration). The regret proof bounds each step's regret by a term involving the confidence width βDtσDt(xt, x't). Summing over T steps and using properties of the information gain γT yields sublinear growth.
- **Core assumption:** The confidence sequences are valid (as in Mechanism 2); the kernel's information gain γT is well-defined and sublinear in T.
- **Evidence anchors:**
  - [Theorem 6] "MAXMINLCB satisfies the anytime dueling regret of P(∀T ≥ 0 : RD(T) ≤ C3βDT(δ)√TγDT = O(γDT√T)) ≥ 1-δ"
  - [Section 6] "Experiments on standard optimization benchmarks and real-world Yelp data show MAXMINLCB consistently outperforms existing baselines"
  - [corpus] "Offline Safe Policy Optimization From Heterogeneous Feedback" discusses related regret bounds in preference-based settings, corroborating the importance of sublinear guarantees.
- **Break condition:** If the information gain γT grows linearly with T (e.g., for a non-smooth kernel with high effective dimension), the regret bound may become linear, failing to provide learning guarantees.

## Foundational Learning

- **Concept: Reproducing Kernel Hilbert Spaces (RKHS)**
  - **Why needed here:** The method assumes the unknown utility f lies in an RKHS, which provides a structured function space where kernel-based regression and confidence sequences are theoretically tractable. Understanding RKHS is essential to grasp how kernelization generalizes linear bandits to non-linear settings.
  - **Quick check question:** Can you explain why the kernel trick allows the algorithm to handle non-linear utility functions without explicitly computing high-dimensional feature maps?

- **Concept: Logistic Regression for Preference Feedback**
  - **Why needed here:** Preference feedback is modeled using the Bradley-Terry model, where the probability of preferring one action over another is a sigmoid function of the utility difference. Logistic regression is the natural tool for estimating f from such binary outcomes, and its kernelized extension is central to the paper's confidence sequences.
  - **Quick check question:** How does the loss function in Equation (6) differ from standard logistic regression, and why is the "dueling kernel" kD necessary?

- **Concept: Bandit Regret and the Exploration-Exploitation Trade-off**
  - **Why needed here:** The goal is to minimize cumulative dueling regret, which quantifies the cost of not selecting optimal actions. Understanding this trade-off—and how the Stackelberg game balances it at two levels (within-pair and across iterations)—is crucial for evaluating algorithm performance.
  - **Quick check question:** In the dueling regret definition (Equation 1), why is the regret zero when both chosen actions are optimal, and how does this relate to the sigmoid function's properties?

## Architecture Onboarding

- **Component map:** History Ht -> Preference Model (Bradley-Terry likelihood, dueling kernel kD, kernelized logistic estimator ht) -> Confidence Engine (βDt, σDt using γDt and B) -> Action Selector (Stackelberg game solver over M_t) -> Actions xt, x't -> Preference observation yt -> Update Ht

- **Critical path:** At iteration t: (a) Update the preference model ht using all past comparisons Ht; (b) Compute confidence intervals [s(ht(xt,x't)) ± βDtσDt(xt,x't)] for all candidate pairs; (c) Define Mt as actions where the upper confidence bound is ≥ 0.5; (d) Solve the max-min Stackelberg game over Mt to select xt, x't; (e) Observe preference yt and append to Ht. Repeat.

- **Design tradeoffs:** (i) **Tightness vs. robustness:** Smaller βDt yields tighter confidence bounds and faster convergence but risks invalidity if assumptions are mildly violated; larger βDt is safer but more conservative. (ii) **Computation vs. optimality:** Solving the Stackelberg game exactly over continuous Mt is NP-hard; in practice, approximate solvers or discretizations are used, trading optimality for scalability. (iii) **Kernel choice:** Smooth kernels (e.g., RBF) yield lower γT and faster regret, but may poorly capture non-smooth f; Matérn kernels offer a tunable smoothness parameter.

- **Failure signatures:** (i) **Linear regret growth:** May indicate kernel misspecification (e.g., using RBF for a discontinuous f) or incorrect norm bound B. (ii) **Overly wide confidence intervals:** Often due to poorly tuned λ or κ, or insufficient data in critical regions. (iii) **Numerical instability:** Can arise from ill-conditioned kernel matrices Kt; mitigation includes regularization or using Cholesky decomposition with jitter.

- **First 3 experiments:**
  1. **Sanity check on synthetic data:** Implement MAXMINLCB for a simple 1D quadratic utility with RBF kernel. Verify that regret grows sublinearly and that confidence intervals contain the true utility with high probability over 20 random seeds.
  2. **Ablation on acquisition strategies:** Compare MAXMINLCB against baselines (DOUBLER, IDS) on the Ackley function (2D, many local optima). Use identical confidence sequences (from Corollary 5) to isolate the impact of the Stackelberg game. Report mean and std of cumulative regret over T=2000 steps.
  3. **Real-world transfer test:** Apply MAXMINLCB to the Yelp dataset (restaurant embeddings, user preferences). Compare against a linear baseline (LOG-UCB1) to assess the benefit of kernelization in a noisy, high-dimensional setting. Monitor computational time and regret scaling with domain size.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does MAXMINLCB maintain its theoretical regret guarantees if the action selection domain is not restricted to the set of plausible maximizers Mt?
  - **Basis in paper:** [explicit] Section 5.2 states, "We conjecture that MAXMINLCB would enjoy similar regret guarantees without restricting the selection domain to Mt as done in Equation (7)."
  - **Why unresolved:** While empirical results support this claim, the theoretical proof relies on the restriction.
  - **What evidence would resolve it:** A formal proof of Theorem 6 that holds for the entire domain X rather than just Mt.

- **Open Question 2:** Can the Stackelberg game framework and preference-based confidence sequences be extended to mechanism design and social choice theory?
  - **Basis in paper:** [explicit] The conclusion identifies "preference elicitation and welfare optimization from multiple feedback sources for social choice theory" as future work.
  - **Why unresolved:** The current paper focuses on single-agent regret minimization rather than multi-agent mechanism design or welfare aggregation.
  - **What evidence would resolve it:** A theoretical extension of the utility estimation and action selection to multi-user feedback settings with welfare guarantees.

- **Open Question 3:** Is the achieved regret rate of O(γT√T) minimax optimal for kernelized dueling bandits?
  - **Basis in paper:** [inferred] The paper improves the time-dependency from T1/4 (seen in concurrent work) to √T, but does not provide a lower bound to prove optimality.
  - **Why unresolved:** Establishing tight lower bounds for the kernelized preference feedback setting remains an open theoretical challenge.
  - **What evidence would resolve it:** Derivation of a lower bound matching the O(γT√T) upper bound provided in Theorem 6.

## Limitations

- Confidence sequences depend critically on RKHS assumptions and logistic model correctness; misspecification may invalidate theoretical guarantees.
- Computational cost of exhaustive search over continuous action space scales poorly with dimension.
- Hyperparameter sensitivity (λ, B) may affect performance, though tuning is mentioned, exact values are not specified.

## Confidence

- **High:** Sublinear dueling regret bound (O(γT√T)) and its derivation, core algorithmic structure, empirical performance comparisons.
- **Medium:** Tightness and practical validity of confidence sequences under all conditions, real-world robustness.
- **Low:** Exact reproducibility of hyperparameters and convergence tolerances for the logistic optimizer.

## Next Checks

1. **Theoretical sanity:** For a 1D quadratic utility with known RBF kernel, verify empirically that confidence intervals contain the true utility with high probability and that regret is sublinear.
2. **Algorithmic isolation:** Replicate ablation experiments (Ackley function) using fixed, shared confidence sequences to isolate the impact of the Stackelberg game from other components.
3. **Hyperparameter sweep:** Systematically vary λ and B on synthetic data to identify ranges where regret remains sublinear and performance is robust.