---
ver: rpa2
title: Adversarially-Aware Architecture Design for Robust Medical AI Systems
arxiv_id: '2510.23622'
source_url: https://arxiv.org/abs/2510.23622
tags:
- adversarial
- attacks
- training
- data
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates adversarial vulnerabilities in medical
  AI systems, focusing on dermatological image classification using the ISIC dataset.
  The research characterizes threats including data poisoning, evasion attacks, and
  model extraction, then evaluates three defensive strategies: adversarial training,
  defensive distillation, and hybrid approaches combining adversarial training with
  input preprocessing.'
---

# Adversarially-Aware Architecture Design for Robust Medical AI Systems

## Quick Facts
- arXiv ID: 2510.23622
- Source URL: https://arxiv.org/abs/2510.23622
- Authors: Alyssa Gerhart; Balaji Iyangar
- Reference count: 11
- Primary result: Hybrid adversarial training plus input preprocessing achieves 72% attack resistance while preserving 89% clean accuracy on dermatological image classification

## Executive Summary
This study investigates adversarial vulnerabilities in medical AI systems through dermatological image classification using the ISIC dataset. The research evaluates three defensive strategies—adversarial training, defensive distillation, and hybrid approaches combining adversarial training with input preprocessing. Results demonstrate that while adversarial training significantly reduces attack success rates, it comes at the cost of clean accuracy. The hybrid approach emerges as the most balanced solution, maintaining high diagnostic accuracy while providing substantial protection against adversarial inputs.

## Method Summary
The study employs the ISIC 2020 Challenge dataset containing 33,000+ dermoscopic images across five lesion classes. A ResNet-50 CNN architecture serves as the baseline model. Three attack methodologies are implemented: Nightshade for data poisoning, FGSM and PGD for evasion attacks. Three defense strategies are evaluated: adversarial training using perturbed examples during training, defensive distillation through teacher-student probability transfer, and a hybrid approach combining adversarial training with input preprocessing (JPEG compression, median filtering, feature denoising). Models are evaluated on accuracy, precision, recall, F1-score, and attack success rate metrics.

## Key Results
- Adversarial training reduced evasion attack success by 58% but decreased clean accuracy by 12%
- Defensive distillation achieved 38% attack resistance but was less effective against strong iterative attacks
- Hybrid approach preserved 89% of baseline clean accuracy while blocking 72% of adversarial inputs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Adversarial training reduces evasion attack success by exposing the model to perturbed examples during training, forcing learned decision boundaries to become more robust.
- **Mechanism:** Training data augmentation with FGSM/PGD-generated adversarial samples causes the model to learn invariant features across perturbed and clean inputs. The model's gradient-based learning incorporates adversarial loss signals, effectively regularizing against small input manipulations.
- **Core assumption:** The adversarial examples used in training are representative of attacks encountered at inference time (assumes attack method transferability).
- **Evidence anchors:**
  - [abstract]: "adversarial training reduced attack success by 58% but decreased clean accuracy by 12%"
  - [section]: "adversarial training reduced the success rate of evasion attacks by 58%... this gain came at the cost of a 12% drop in clean accuracy, indicating a shift in the model's internal representation that degraded its performance on unaltered inputs"
  - [corpus]: "Robust Training with Data Augmentation for Medical Imaging Classification" confirms data augmentation approaches improve robustness, with FMR=0.665 indicating moderate-to-strong relevance
- **Break condition:** If attack methodologies shift significantly from gradient-based methods (FGSM/PGD), or if adaptive attackers use stronger iterative attacks not represented in training, robustness gains may not transfer.

### Mechanism 2
- **Claim:** Defensive distillation provides moderate protection against simple adversarial attacks by smoothing decision boundaries through teacher-student probability transfer.
- **Mechanism:** A teacher model generates softened output probability distributions (via temperature scaling), which a student model learns to replicate. This smoothing reduces gradient magnitude with respect to inputs, making gradient-based attack generation less effective.
- **Core assumption:** Attackers rely primarily on gradient information to craft adversarial examples.
- **Evidence anchors:**
  - [abstract]: "Defensive distillation achieved 38% attack resistance but was less effective against strong attacks"
  - [section]: "its impact was less pronounced against iterative attacks like PGD... remains insufficient against strong white-box attacks"
  - [corpus]: Weak direct corpus support for distillation specifically; related work focuses on training augmentation rather than distillation methods
- **Break condition:** Iterative attacks (PGD) and non-gradient-based attacks can circumvent the smoothed boundaries; Carlini-Wagner attacks have historically broken distillation defenses.

### Mechanism 3
- **Claim:** Hybrid approaches combining adversarial training with input preprocessing achieve superior robustness-accuracy trade-offs by creating multiple defensive barriers.
- **Mechanism:** Input preprocessing (JPEG compression, median filtering, feature denoising) removes or attenuates adversarial perturbations before inference, while adversarial training provides learned robustness as a secondary defense. This redundancy means attackers must circumvent both layers simultaneously.
- **Core assumption:** Perturbations introduced by attacks are detectable/removable by preprocessing without destroying clinically relevant features.
- **Evidence anchors:**
  - [abstract]: "hybrid approach preserved 89% of baseline clean accuracy while blocking 72% of adversarial inputs"
  - [section]: "By filtering potential noise prior to model inference and reinforcing robustness through adversarial training, this approach creates multiple barriers for adversaries while maintaining usability"
  - [corpus]: "Filtered-ViT: A Robust Defense Against Multiple Adversarial Patch Attacks" validates filtering-based defenses (FMR=0.624), supporting preprocessing as a viable defense layer
- **Break condition:** If preprocessing inadvertently removes diagnostically relevant texture features (e.g., subtle lesion borders), clinical utility degrades; adaptive attackers may design perturbations robust to specific preprocessing pipelines.

## Foundational Learning

- **Concept: Adversarial perturbations and gradient-based attacks**
  - **Why needed here:** Understanding how small, imperceptible input modifications exploit model gradients to cause misclassification is foundational to all defense strategies in this paper.
  - **Quick check question:** If you add a perturbation ε·sign(∇x L) to an input image, what property of the loss landscape does this exploit?

- **Concept: Robustness-accuracy trade-off**
  - **Why needed here:** All three defense mechanisms exhibit this trade-off; the paper quantifies it (12% accuracy drop for 58% robustness gain with adversarial training).
  - **Quick check question:** Why might a model that is more robust to perturbations perform worse on clean data? What does this suggest about the geometry of the decision boundary?

- **Concept: Transferability of adversarial examples**
  - **Why needed here:** The paper discusses model extraction enabling surrogate-based attacks; transferability determines whether attacks crafted on one model affect another.
  - **Quick check question:** If you train a surrogate model to mimic a target medical AI system, will adversarial examples generated against the surrogate always transfer to the target? What factors influence this?

## Architecture Onboarding

- **Component map:** ISIC dataset -> preprocessing (rescaling, contrast normalization, stratified splitting) -> clean/poisoned variants -> ResNet-50 CNN -> attack layer (Nightshade, FGSM/PGD, API probing) -> defense modules (adversarial training, defensive distillation, hybrid preprocessing) -> evaluation (confusion matrix metrics, attack success rate, perturbation norms)

- **Critical path:**
  1. Establish baseline clean performance on ISIC (target: ≥90% accuracy)
  2. Characterize vulnerability: run FGSM/PGD attacks, measure success rate
  3. Implement hybrid defense first (best accuracy/robustness trade-off per paper)
  4. Validate on held-out test set with both clean and adversarial samples
  5. Measure latency impact (distillation: +15% inference time; adversarial training: 1.8× training time)

- **Design tradeoffs:**
  - Adversarial training: Best robustness (58%), worst accuracy loss (12%), high training cost
  - Defensive distillation: Moderate robustness (38%), low accuracy impact, ineffective against PGD
  - Hybrid: Balanced (72% robustness, 89% accuracy retention), added inference latency from preprocessing
  - Assumption: Real-world deployment context determines which trade-off is acceptable (e.g., emergency triage prioritizes speed; oncology prioritizes robustness)

- **Failure signatures:**
  - Clean accuracy drops >15%: Defense overfitting to adversarial patterns
  - High variance across lesion classes: Uneven robustness (check if malignant classes are more vulnerable)
  - Transfer attacks from surrogate succeed: Model extraction vulnerability, consider rate-limiting API access
  - Preprocessing removes diagnostic features: Check if recall on malignant lesions degrades despite overall accuracy holding

- **First 3 experiments:**
  1. **Baseline vulnerability assessment:** Train ResNet-50 on clean ISIC, evaluate against FGSM (ε=0.01, 0.03, 0.05) and PGD (10 iterations), record attack success rate per lesion class
  2. **Defense ablation study:** Implement all three defenses independently on same baseline, measure: (a) clean accuracy, (b) attack success rate under same attack conditions, (c) inference latency
  3. **Hybrid composition test:** Combine adversarial training with each preprocessing method individually (JPEG only, median filter only, denoising only), then all three together; identify which preprocessing component contributes most to the 72% robustness figure

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do the observed adversarial vulnerabilities and the efficacy of the hybrid defense strategy generalize to other medical imaging modalities, such as radiology, histopathology, and multimodal clinical decision systems?
- **Basis in paper:** [explicit] The authors state in "Future Work" that "future research should expand beyond dermatology to include radiological scans, histopathology slides, and multimodal clinical decision systems."
- **Why unresolved:** The study explicitly limited its scope to dermatological image classification using the ISIC dataset and a ResNet-50 architecture. It is unknown if the 72% attack resistance achieved by the hybrid method transfers to 3D imaging (MRI/CT) or slide-based data where feature representations differ significantly.
- **What evidence would resolve it:** Empirical results benchmarking the proposed hybrid defense (adversarial training + input preprocessing) against FGSM and PGD attacks on datasets such as ChestX-ray14 (radiology) or PatchCamelyon (histopathology).

### Open Question 2
- **Question:** Can federated learning architectures be effectively employed to limit the attack surface for model extraction and data poisoning while preserving data sovereignty in decentralized healthcare networks?
- **Basis in paper:** [explicit] The authors identify a "critical need to explore federated learning and privacy-preserving AI in healthcare settings... to limit the attack surface exposed to adversaries."
- **Why unresolved:** The paper notes the risk of model extraction via APIs in centralized models but does not experimentally validate whether decentralized training mitigates this or introduces new vulnerabilities (e.g., Byzantine attacks from compromised nodes).
- **What evidence would resolve it:** A comparative security analysis measuring the success rate of model extraction and poisoning attacks in a federated learning setup versus the centralized approach described in the paper.

### Open Question 3
- **Question:** Is it possible to develop lightweight, explainable defense mechanisms that maintain clinical accuracy while operating within the computational constraints of low-resource or mobile diagnostic platforms?
- **Basis in paper:** [explicit] The authors call for "further investigation... into lightweight, explainable defense mechanisms that maintain clinical performance while remaining computationally efficient" for global health contexts.
- **Why unresolved:** The paper found that the most effective defense (the hybrid approach) introduced latency and computational overhead, which the authors admit "may limit their feasibility in low-resource settings, such as rural clinics."
- **What evidence would resolve it:** A study demonstrating a defense mechanism (e.g., a simplified denoiser or efficient adversarial training variant) that maintains >85% clean accuracy and >60% attack resistance on edge hardware (e.g., mobile devices) without significant inference latency.

## Limitations

- The study lacks comprehensive ablation studies for hybrid defense components, making it difficult to attribute the 72% attack blocking to specific preprocessing methods versus adversarial training.
- No analysis of worst-case perturbations near clinically critical decision boundaries between malignant and benign classes is provided.
- The paper does not report latency measurements for the hybrid approach despite mentioning "added inference time" for distillation.

## Confidence

- High confidence: Adversarial training reduces evasion attacks by 58% (supported by ablation study showing 12% accuracy drop)
- Medium confidence: Defensive distillation achieves 38% attack resistance (limited evaluation against strong iterative attacks)
- Medium confidence: Hybrid approach preserves 89% clean accuracy while blocking 72% attacks (no component attribution or latency data)

## Next Checks

1. **Component attribution study:** Isolate which preprocessing method (JPEG, median filter, or denoising) contributes most to the hybrid defense's 72% attack blocking performance
2. **Decision boundary analysis:** Generate adversarial examples targeting borderline cases between malignant and benign classes to assess clinical safety impact
3. **Cross-architecture transfer test:** Evaluate whether attacks successful against ResNet-50 transfer to ViT or EfficientNet models when using the hybrid defense