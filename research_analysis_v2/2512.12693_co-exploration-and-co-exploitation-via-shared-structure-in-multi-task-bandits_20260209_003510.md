---
ver: rpa2
title: Co-Exploration and Co-Exploitation via Shared Structure in Multi-Task Bandits
arxiv_id: '2512.12693'
source_url: https://arxiv.org/abs/2512.12693
tags:
- users
- user
- reward
- each
- bandits
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses efficient exploration in multi-task contextual
  bandit settings where context is partially observable and tasks share latent structure.
  The authors propose a Bayesian framework that models the joint distribution over
  tasks and rewards using a particle-based approximation of a log-density Gaussian
  process.
---

# Co-Exploration and Co-Exploitation via Shared Structure in Multi-Task Bandits

## Quick Facts
- arXiv ID: 2512.12693
- Source URL: https://arxiv.org/abs/2512.12693
- Reference count: 40
- This work addresses efficient exploration in multi-task contextual bandit settings where context is partially observable and tasks share latent structure.

## Executive Summary
This paper proposes a Bayesian framework for multi-task contextual bandits with partially observable context. The method models joint distributions over tasks and rewards using a particle-based approximation of a log-density Gaussian process, enabling flexible discovery of dependencies between arms and tasks. The approach identifies two key sources of epistemic uncertainty—structural uncertainty across tasks and user-specific uncertainty due to incomplete context—and develops acquisition strategies that balance individual and population-level learning. Empirically, the method outperforms hierarchical model bandits, particularly under model misspecification or complex latent heterogeneity, converging toward oracle performance.

## Method Summary
The method addresses multi-task contextual bandits where users arrive in batches with fixed context (observed plus unobserved). It constructs a GP-based log-density model over the joint space of arm means and observed contexts, using a discretization grid with 20 points per arm mean in [-2,2] and 10 points for context in [-1,1]. SMC inference maintains N=200 particles, updating weights based on observed rewards, resampling when ESS drops below N/2, and applying Langevin rejuvenation. Two acquisition strategies are implemented: NPM-TS (Thompson Sampling) samples particles to compute arm optimality probabilities, while GIDS (Global Information Directed Sampling) optimizes an information-regret trade-off. The framework enables co-exploration (learning shared structure) and co-exploitation (leveraging learned structure across tasks).

## Key Results
- The method outperforms hierarchical model bandits, particularly under model misspecification or complex latent heterogeneity
- Achieves convergence toward oracle performance in both cumulative multi-task and Bayes regret across synthetic benchmarks
- Successfully handles partially observable context while discovering shared latent structure across users

## Why This Works (Mechanism)
The method works by explicitly modeling the joint distribution over tasks and rewards, which captures dependencies that traditional approaches miss. By maintaining a particle-based approximation of the log-density GP, it can represent complex, non-parametric relationships between contexts and rewards across different tasks. The two types of epistemic uncertainty (structural and user-specific) are naturally handled through the Bayesian framework, allowing the acquisition functions to balance exploration and exploitation at both individual and population levels.

## Foundational Learning
- Gaussian Process log-density modeling: Needed to represent complex joint distributions over tasks and rewards without parametric assumptions. Quick check: verify GP kernel eigendecomposition captures sufficient variance with M=80 basis functions.
- Sequential Monte Carlo inference: Required for posterior approximation in high-dimensional joint space. Quick check: monitor ESS to ensure particles maintain diversity.
- Tempered transitions in SMC: Helps prevent particle collapse during weight updates. Quick check: verify ESS remains above threshold after resampling.
- Thompson Sampling vs Information-Directed Sampling: Different exploration strategies for balancing individual vs population learning. Quick check: compare regret curves under each acquisition strategy.
- Discretization of continuous joint space: Enables tractable computation of densities on grid. Quick check: validate softmax normalization produces valid probability distributions.

## Architecture Onboarding
- Component map: Data → SMC Inference → Particle Set → Acquisition Function → Action → Reward → Update
- Critical path: SMC inference loop (weight update → ESS check → resample → rejuvenate) → acquisition function computation → action selection
- Design tradeoffs: Grid resolution vs computational cost, particle count vs convergence speed, tempering schedule vs exploration-exploitation balance
- Failure signatures: Particle degeneracy (low ESS), invalid density normalization, multimodal posterior collapse
- First experiments: 1) Test SMC convergence with synthetic data where ground truth is known; 2) Validate grid-based density computation by comparing to analytical solutions; 3) Compare Thompson Sampling vs GIDS performance on simple bandit problems

## Open Questions the Paper Calls Out
None

## Limitations
- Implementation details for joint space discretization and indexing are underspecified
- SMC tuning parameters (tempering schedule, rejuvenation step size) are described but not precisely specified
- Limited comparison to more recent multi-task bandit methods beyond hierarchical approaches

## Confidence
- Conceptual framework and Bayesian formulation: High
- Empirical performance results: Medium (synthetic benchmarks only)
- Precise numerical results: Low (implementation-sensitive components)

## Next Checks
1. Verify SMC convergence by logging ESS and particle diversity across rounds; confirm that tempered transitions with specified η prevent premature collapse while maintaining exploration
2. Test grid resolution sensitivity by comparing results across different (K_x, K_µ) configurations to ensure the M=80 KL basis is sufficient for capturing the true posterior
3. Validate acquisition function implementations by checking that Thompson Sampling produces valid probability distributions over arms and that GIDS correctly optimizes the information-regret trade-off under varying batch sizes