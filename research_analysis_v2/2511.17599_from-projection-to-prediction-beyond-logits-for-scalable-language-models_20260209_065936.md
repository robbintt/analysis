---
ver: rpa2
title: 'From Projection to Prediction: Beyond Logits for Scalable Language Models'
arxiv_id: '2511.17599'
source_url: https://arxiv.org/abs/2511.17599
tags:
- memory
- logits
- training
- loss
- vocabulary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a fused kernel for large language model (LLM)
  training that integrates output projection and cross-entropy loss computation into
  a single operation, thereby avoiding the explicit materialization of large logits
  tensors. By computing logits on-the-fly within register-local scopes, the method
  significantly reduces GPU memory footprint and bandwidth pressure.
---

# From Projection to Prediction: Beyond Logits for Scalable Language Models

## Quick Facts
- arXiv ID: 2511.17599
- Source URL: https://arxiv.org/abs/2511.17599
- Reference count: 40
- Primary result: Fused kernel achieves up to 44.5% latency reduction and 96.8% memory savings in LLM training by eliminating large logits tensor materialization

## Executive Summary
This paper introduces a fused kernel optimization for large language model training that combines output projection and cross-entropy loss computation into a single operation. By computing logits on-the-fly within register-local scopes rather than materializing the full logits tensor, the approach dramatically reduces GPU memory footprint and bandwidth pressure. Experimental results on NVIDIA GB200 GPUs demonstrate significant performance improvements, particularly at large vocabulary sizes and batch-sequence configurations.

## Method Summary
The method replaces the standard two-stage pipeline (projection + loss) with a fused kernel that streams through the vocabulary, computing logits for each token position on-demand. This eliminates the need to store the intermediate logits tensor (dimensions B×T×V), reducing memory from O(B·T·V) to O(B·T). The kernel uses a streaming softmax algorithm to compute the normalization constant and maximum logit in a single pass, and performs gradient computation by re-computing logits during the backward pass. The implementation leverages NVIDIA CUTEDSL and Triton for fine-grained register control.

## Key Results
- Achieves up to 44.5% latency reduction on NVIDIA GB200 GPUs
- Reduces memory usage by over 96.8% compared to standard two-stage pipeline
- Maintains numerical equivalence with standard PyTorch implementation
- Performance gains scale with vocabulary size and batch-sequence configurations
- Memory savings are most pronounced at large V (262k) and moderate B×T (32k)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fusing projection and loss significantly reduces memory footprint by avoiding logits tensor materialization
- **Mechanism:** Computes logits on-the-fly in GPU registers, retaining only target logit and running sum for denominator
- **Core assumption:** GPU register file has sufficient capacity to handle partial logits computation without spilling
- **Evidence anchors:** Abstract states method "avoids the explicit materialization of large logits tensors... computing logits on-the-fly within register-local scopes"

### Mechanism 2
- **Claim:** Latency reductions achieved by restructuring softmax computation into streaming reduction
- **Mechanism:** Uses online softmax algorithm to compute normalization constant in single pass, preventing need to write full logits matrix to memory
- **Core assumption:** Numerical stability logic holds for BF16/FP32 mixed precision without divergence
- **Evidence anchors:** Section 3.2 notes "dominator term is evaluated via a streaming reduction, avoiding storage of all intermediate logits"

### Mechanism 3
- **Claim:** Gradient computation remains accurate without materializing full Jacobian by re-computing logits
- **Mechanism:** Re-calculates logit z_v and probability p_v for each vocabulary entry during backward pass, accumulating gradients on-the-fly
- **Core assumption:** Re-computation cost is cheaper than materialization cost
- **Evidence anchors:** Appendix A.1 states "Gradients propagate without materializing the logits tensor... re-compute forward logit z_v"

## Foundational Learning

- **Concept: Online Softmax (Streaming Reduction)**
  - Why needed here: Allows computing normalization sum without seeing all values at once
  - Quick check question: If processing vocabulary items in chunks, how do you correctly update normalization sum if new chunk contains larger maximum value?

- **Concept: GPU Memory Hierarchy (HBM vs. Registers)**
  - Why needed here: Relies on speed difference between High Bandwidth Memory and on-chip registers
  - Quick check question: Why is computing value in register faster than reading from HBM, even with high bandwidth?

- **Concept: Kernel Fusion & Arithmetic Intensity**
  - Why needed here: Replaces two kernels with one, increasing operations per byte of memory traffic
  - Quick check question: In standard pipeline, why is writing logits tensor to memory considered "redundant" if immediately read back?

## Architecture Onboarding

- **Component map:** Hidden States H -> Output Weights W -> Target Tokens Y -> Fused Kernel (GEMV + Safe Softmax + Log-loss) -> Scalar Loss, Gradient ∂H, Gradient ∂W

- **Critical path:** Vocabulary Loop (V) is performance bottleneck, iterating through entire vocabulary for every token position

- **Design tradeoffs:** Trade lower memory usage (O(B·T) vs O(B·T·V)) for higher compute (recomputing logits in backward pass)

- **Failure signatures:** Low Occupancy with small B×T, Numerical Drift from max/accumulator logic failure, Atomic Contention in gradient accumulation

- **First 3 experiments:**
  1. Micro-benchmark Memory Ceiling: Run standard vs. fused kernel on fixed V=128k while increasing Batch×Sequence product until OOM occurs
  2. Numerical Equivalence Test: Train small model with fused and standard kernels, plot loss curves side-by-side
  3. Latency Breakdown: Profile kernel to measure ratio of time spent in "logit compute" vs. "reduction/accumulation"

## Open Questions the Paper Calls Out

- Can fused kernel approach be generalized to loss functions requiring full probability distribution access, such as label smoothing or sampled softmax?
- How does partial output aggregation impact latency and scalability in multi-node Tensor/Sequence Parallelism configurations?
- Can compiler-based automation within NVIDIA CUDA ecosystem generate these fused kernels while preserving register-level optimization?
- Does the radical backward pass strategy (accumulating gradients during forward pass) offer significant throughput advantage over standard recompute method?

## Limitations

- Numerical stability under extreme conditions (very large/small vocabulary sizes or clustered target tokens) not thoroughly validated
- Implementation heavily NVIDIA-specific, limiting portability across architectures
- Effectiveness with alternative mixed precision formats (FP8, TF32) remains untested
- Claims assume typical LLM training conditions that may not hold universally

## Confidence

- **Memory Savings Claim:** High confidence - theoretically sound reduction from O(B·T·V) to O(B·T) with strong empirical validation
- **Latency Reduction Claim:** Medium confidence - demonstrated on GB200 but architecture dependence not fully characterized
- **Generalizability to Other Loss Functions:** Low confidence - mentioned as future work with no validation provided

## Next Checks

1. **Numerical Stability Stress Test:** Systematically evaluate across vocabulary sizes (1k to 262k) and batch-sequence configurations, measuring loss divergence from standard implementation

2. **Cross-Architecture Performance Benchmark:** Implement and test on A100 and H100 GPUs, measuring memory savings and latency improvements relative to standard implementation

3. **Alternative Precision Format Validation:** Re-implement using FP8 for inputs and TF32 for accumulation, validating numerical equivalence and performance gains compared to BF16/FP32 baseline