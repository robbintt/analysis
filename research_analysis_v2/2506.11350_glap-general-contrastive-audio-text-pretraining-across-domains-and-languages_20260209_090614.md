---
ver: rpa2
title: 'GLAP: General contrastive audio-text pretraining across domains and languages'
arxiv_id: '2506.11350'
source_url: https://arxiv.org/abs/2506.11350
tags:
- speech
- audio
- glap
- sound
- music
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GLAP is a new pretraining method that extends CLAP to support multilingual
  speech content retrieval in addition to sound and music understanding. It uses a
  unified audio encoder and multilingual text embeddings trained on diverse audio-text
  datasets across domains and languages.
---

# GLAP: General contrastive audio-text pretraining across domains and languages

## Quick Facts
- **arXiv ID:** 2506.11350
- **Source URL:** https://arxiv.org/abs/2506.11350
- **Reference count:** 0
- **Primary result:** Unified audio encoder achieves competitive audio-text retrieval across sound, music, and multilingual speech domains.

## Executive Summary
GLAP extends Contrastive Language-Audio Pretraining (CLAP) to support multilingual speech content retrieval in addition to sound and music understanding. The method uses a unified audio encoder (Dasheng) and multilingual text embeddings trained on diverse audio-text datasets across domains and languages. GLAP achieves competitive results on standard audio-text retrieval benchmarks while significantly outperforming prior methods on speech retrieval and keyword spotting across 50 languages. The approach demonstrates that incorporating speech data into CLAP training improves multilingual and multi-domain audio understanding without compromising performance in sound and music tasks.

## Method Summary
GLAP extends CLAP by using a unified audio encoder (Dasheng) pretrained on diverse audio data and a multilingual text encoder (Sonar). The model employs sigmoid loss with learnable temperature and bias parameters for contrastive learning, using equal sampling across four data groups (sound+music, English speech, Chinese speech, other languages). Machine-translated captions extend multilingual coverage for sound and music datasets. Training uses 8-bit Adam with cosine decay scheduling and batch size 1024.

## Key Results
- GLAP achieves 93.8% R@1 on LibriSpeech speech retrieval and 98.5% R@1 on AISHELL-2
- Up to 70% accuracy in 50-language keyword spotting (e.g., 70% in Oriya)
- Competitive performance across all domains: 55.8/60.1/20.3/94.8/99.0 on AudioCaps, ACD, MusicCaps, LibriSpeech, AISHELL-2 respectively
- Maintains strong zero-shot performance on sound-event classification tasks

## Why This Works (Mechanism)

### Mechanism 1
A unified general-purpose audio encoder enables cross-domain alignment without catastrophic forgetting between speech, sound, and music. GLAP uses Dasheng (masked audio encoder pretrained on diverse audio) as the shared audio backbone, maintaining representation quality across all three domains by having been exposed to broad audio during pretraining. The contrastive objective then aligns this general representation with multilingual text embeddings.

### Mechanism 2
Sigmoid loss improves retrieval performance over cross-entropy when training with large batch sizes and diverse multilingual data. Standard cross-entropy normalizes across all negatives in the batch, which can become unstable with 1024-sample batches and heterogeneous data. Sigmoid loss treats each pair independently with a binary classification objective, decoupling optimization from batch composition.

### Mechanism 3
Balanced sampling across four data groups (sound+music, English speech, Chinese speech, other languages) prevents domain collapse while machine-translated captions extend multilingual coverage. The 400k-hour YODAS corpus dominates by volume but is noisy. Equal sampling ensures each gradient update sees representative data from all domains.

## Foundational Learning

- **Contrastive Language-Audio Pretraining (CLAP):** Understanding how contrastive objectives align audio and text in shared embedding space via cosine similarity is critical since GLAP extends CLAP. Quick check: Given audio embedding `ea` and text embedding `et`, what does high cosine similarity indicate, and how does contrastive loss encourage this for positive pairs while discouraging it for negatives?

- **Audio Encoder Architectures (Masked vs. Supervised):** Understanding why Dasheng (masked autoencoding) generalizes better than Whisper (weakly supervised ASR) or CED (labeled sound events) is critical for encoder selection. Quick check: Why would an encoder trained only on sound events (e.g., AudioSet) fail to produce useful embeddings for spoken content retrieval?

- **Sigmoid Loss for Contrastive Learning:** Understanding the formulation difference between sigmoid and cross-entropy losses explains reported performance gains. Quick check: In sigmoid loss, each pair is treated as independent binary classification. How does this differ from cross-entropy's global normalization, and why might it help with large, diverse batches?

## Architecture Onboarding

- **Component map:** Audio encoder (Dasheng) -> Projection MLP (MLPA) -> Cosine similarity -> Sigmoid loss. Text encoder (Sonar) -> Projection MLP (MLPT) -> Cosine similarity -> Sigmoid loss.

- **Critical path:** Load and resample all audio to 16kHz mono. Extract audio features via pretrained Dasheng checkpoint. Tokenize text and extract embeddings via Sonar. Pass through respective MLP projection heads. Compute pairwise cosine similarities within batch. Apply sigmoid loss and backpropagate.

- **Design tradeoffs:** Dasheng offers versatility across domains; Whisper/WavLM if speech-only is acceptable; CED/BEATs if sound/music-only. Single encoder simplifies serving but may sacrifice peak performance on any single domain. Equal sampling prevents dominance by large speech corpora but increases training time per epoch.

- **Failure signatures:** Speech retrieval near zero indicates sound/music specialized encoder (e.g., CED); switch to Dasheng or Whisper. Sound/music retrieval drops sharply indicates speech-specialized encoder (e.g., WavLM) or speech data overrepresentation. Multilingual retrieval inconsistent across languages indicates translation quality varies; verify Sonar output for low-resource languages.

- **First 3 experiments:**
  1. Reproduce the encoder ablation: Train GLAP with CED-Base, Whisper-Base, and Dasheng on subset of data; verify Table 4 trends on AudioCaps and LibriSpeech test splits.
  2. Ablate sigmoid vs. cross-entropy: With fixed encoder (Dasheng) and data, compare retrieval R@1 on AudioCaps and LibriSpeech using both loss functions at batch sizes 256, 512, 1024.
  3. Probe multilingual transfer: Evaluate zero-shot keyword spotting on held-out language from MSW corpus not in training; assess whether translated captions generalize or if performance correlates with translation quality.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the traditional sense, but several areas remain unexplored based on the methodology and results presented.

## Limitations
- Sigmoid loss performance claims are anchored only in internal ablation studies without external validation or comparison to other large-batch contrastive methods.
- Translation quality is unverified for low-resource languages, with significant performance drops suggesting potential noise in automated translation pipeline.
- Data balancing assumptions may not hold for different dataset mixes, and optimal balance may shift with different corpus compositions or target tasks.

## Confidence
- **Unified encoder for cross-domain alignment (High):** Direct ablation in Table 4 shows Dasheng outperforms domain-specific encoders across all retrieval tasks.
- **Sigmoid loss improves retrieval at large batch sizes (Medium):** Performance gains are reported but lack external validation or comparison to other loss functions in audio-text pretraining.
- **Balanced sampling + translation extends multilingual coverage (Medium):** Domain balance prevents collapse, but translation quality and its impact on alignment are not independently verified.
- **Zero-shot speech retrieval and keyword spotting (High):** Strong results on LibriSpeech (93.8% R@1), AISHELL-2 (98.5% R@1), and 50-language keyword spotting are directly measured and reproducible.

## Next Checks
1. **Encoder ablation with held-out test set:** Reproduce Table 4 trends using consistent test split across CED-Base, Whisper-Base, WavLM-Base, and Dasheng. Verify generalization to unseen audio samples.
2. **Sigmoid loss ablation at variable batch sizes:** Train with both sigmoid and cross-entropy losses at batch sizes 256, 512, 1024. Measure R@1 on AudioCaps and LibriSpeech to confirm large-batch performance gains.
3. **Multilingual translation quality probe:** Select low-resource language from MSW not in training. Compare GLAP retrieval performance against model trained on native multilingual captions to isolate translation noise effects.