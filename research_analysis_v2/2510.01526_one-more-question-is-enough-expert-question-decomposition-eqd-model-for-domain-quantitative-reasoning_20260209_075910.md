---
ver: rpa2
title: One More Question is Enough, Expert Question Decomposition (EQD) Model for
  Domain Quantitative Reasoning
arxiv_id: '2510.01526'
source_url: https://arxiv.org/abs/2510.01526
tags:
- question
- llms
- reasoning
- questions
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Expert Question Decomposition (EQD), a two-step
  fine-tuning approach that improves large language models' domain-specific quantitative
  reasoning by generating concise, targeted supporting questions rather than verbose
  step-by-step guidance. The method uses a small financial dialogue dataset for domain
  fine-tuning and reinforcement learning with a novel answer comparison reward to
  align the decomposition model with QA model preferences.
---

# One More Question is Enough, Expert Question Decomposition (EQD) Model for Domain Quantitative Reasoning

## Quick Facts
- **arXiv ID:** 2510.01526
- **Source URL:** https://arxiv.org/abs/2510.01526
- **Reference count:** 25
- **Primary result:** EQD improves LLM financial QA performance by 0.6% to 10.5% across four benchmarks by generating concise, targeted supporting questions

## Executive Summary
This paper introduces Expert Question Decomposition (EQD), a two-step fine-tuning approach that enhances large language models' domain-specific quantitative reasoning by generating concise, targeted supporting questions rather than verbose step-by-step guidance. The method uses a small financial dialogue dataset for domain fine-tuning and reinforcement learning with a novel answer comparison reward to align the decomposition model with QA model preferences. Across four financial QA benchmarks, EQD consistently improves performance by 0.6% to 10.5% on various LLMs, outperforming both domain-tuned models and advanced prompting strategies. The approach requires only a single A100 GPU for fine-tuning and maintains inference time comparable to zero-shot prompting, demonstrating that a single well-chosen supporting question is more effective than detailed reasoning chains for domain-specific reasoning tasks.

## Method Summary
EQD employs a two-step process: First, a LoRA adapter is fine-tuned on ConvFinQA (3,073 entries) to teach the model financial question decomposition using supervised learning. Second, Proximal Policy Optimization (PPO) is applied using FinQA (6,250 entries) to align the decomposition model with the downstream QA model's success criteria through an answer comparison reward function. The reward structure incentivizes the model to find sub-questions that correct incorrect answers (+2) rather than just preserving correct ones (+1). The final system generates a single, concise supporting question that, when prepended to the original context, improves the QA model's accuracy. The entire process trains on a single A100 GPU and maintains inference time comparable to zero-shot prompting.

## Key Results
- EQD improves Llama3.1-8B-Instruct performance by 1.7% to 3.8% across FinQA, TAT-QA, ECTQA, and EDTQA benchmarks
- Outperforms verbose Chain-of-Thought prompting by 1.4% to 10.5% on various LLMs including GPT-4o and Claude-3.5-Sonnet
- Maintains conciseness with average 20.2 words per decomposed question versus 35.1 for base models
- Reduces average time per example to 8.7 seconds versus 19.3 seconds for Chain-of-Thought prompting

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Targeting question decomposition at the "critical path" of reasoning improves accuracy more effectively than verbose step-by-step chains.
- **Mechanism:** The paper suggests that general LLMs often fail on domain questions not because they lack reasoning ability, but because they miss specific intermediate context (e.g., extracting two specific numbers before calculating a percentage). By generating a single, high-leverage sub-question that retrieves this missing link, the EQD model bridges the knowledge gap without introducing the noise associated with long Chain-of-Thought (CoT) prompts.
- **Core assumption:** Assumes that the reasoning capability for the final step (e.g., percentage calculation) is already present in the base model, and the failure is primarily due to missing intermediate domain entities or context.
- **Evidence anchors:** [abstract] "...a single well-chosen supporting question is more effective than detailed reasoning chains..."; [page 5] "...excessive guidance can be redundant, or even harmful... brevity is an inherent requirement of effective QA support."; [corpus] Paper `2504.10342` (VisualPuzzles) supports the hypothesis of decoupling general reasoning from domain knowledge; EQD targets the domain knowledge gap specifically.

### Mechanism 2
- **Claim:** Reinforcement Learning (RL) with a specific answer-comparison reward aligns the decomposition model with the downstream QA model's success criteria.
- **Mechanism:** Instead of mimicking human decomposition (supervised learning), the EQD model uses Proximal Policy Optimization (PPO) to maximize a reward function based on QA correctness. The reward differentiates between "preserving a correct answer" (+1) and "correcting an incorrect answer" (+2), explicitly incentivizing the model to find sub-questions that fix errors rather than just restating the problem.
- **Core assumption:** Assumption: The scalar reward signal from the final answer accuracy is sufficient feedback to guide the decomposition model away from generating distracting or misleading sub-questions.
- **Evidence anchors:** [page 4] Eq. (2) defines the reward structure emphasizing the delta between direct and assisted answers; [page 8] Figure 3 shows that removing the RL alignment step (Step 2) results in higher word counts and lower accuracy compared to the full two-step process; [corpus] Paper `2509.09727` highlights the difficulty of capturing specialized reasoning; EQD's reward function serves as a mechanism to capture this nuance via outcome optimization.

### Mechanism 3
- **Claim:** Lightweight instruction tuning on a small domain dataset (ConvFinQA) suffices to inject necessary domain vocabulary without full model fine-tuning.
- **Mechanism:** The authors fine-tune a LoRA adapter on a financial dialogue dataset containing ~3,000 entries. This teaches the model how to "speak the language" of the domain (e.g., understanding "cash flow hedges") specifically for the purpose of decomposition, rather than trying to encode all financial knowledge into the weights.
- **Core assumption:** Assumes that the decomposition task requires only surface-level domain alignment (terminology and question structure) rather than deep factual knowledge retrieval from the model weights.
- **Evidence anchors:** [page 3] "We use ConvFinQA... containing around only 3,000 entries, to fine-tune our QD model."; [page 8] Ablation study shows removing Step 1 (domain fine-tuning) drops performance to near-baseline levels (42.5% vs 54.0%), confirming its necessity; [corpus] Paper `2512.23848` (Integrating Domain Knowledge) notes errors in financial QA due to lack of domain knowledge; EQD addresses this via the Step 1 adapter.

## Foundational Learning

- **Concept:** Proximal Policy Optimization (PPO)
  - **Why needed here:** Used in Step 2 to update the EQD model weights based on the custom reward function.
  - **Quick check question:** How does the KL-divergence constraint in PPO prevent the model from drifting too far from its original domain knowledge during alignment?

- **Concept:** LoRA (Low-Rank Adaptation)
  - **Why needed here:** Enables the entire system to train on a single A100 GPU by freezing base model weights and only updating a small adapter.
  - **Quick check question:** Why is "LoRA Continue" (training the same adapter across both steps) preferred over training separate adapters for Step 1 and Step 2?

- **Concept:** Exact Match Accuracy (EmAcc)
  - **Why needed here:** The primary evaluation metric; crucial for understanding the reward signal which depends on binary correctness.
  - **Quick check question:** Since financial answers are often numerical, how does the evaluation handle formatting differences (e.g., "5%" vs "0.05")?

## Architecture Onboarding

- **Component map:** Base LLM (Llama3.1-8B-Instruct) -> LoRA adapter (22M params) -> PPO-trained QD model -> QA Model (Llama, GPT-4o, Claude) -> Answer

- **Critical path:**
  1. **Step 1 (SFT):** Train LoRA adapter on `(Question, Dialogue_SubQuestions)` pairs from ConvFinQA.
  2. **Step 2 (RL):**
     - Initialize Reference Model (frozen) from Step 1 adapter.
     - For a given financial Q: QD Model (Active Adapter) generates Decomposed Questions (DQs).
     - QA Model answers using DQs.
     - Calculate Reward based on comparison to Direct QA answer.
     - Update Active Adapter via PPO.
  3. **Inference:** QD Model generates DQ -> DQ prepended to Context -> QA Model generates final Answer.

- **Design tradeoffs:**
  - **Verbosity vs. Precision:** The reward function penalizes verbose, distracting steps, forcing the model to learn brevity, but this risks omitting necessary context in highly complex multi-hop queries.
  - **Model Specificity:** The QD model is trained on Llama3.1 but is applied to others. While efficient, it lacks the "native" understanding of the target QA model's specific reasoning quirks (though the paper claims generalization).
  - **Resource constraints:** Limited to a single GPU; forces the use of smaller base models (8B) for the decomposition engine, potentially capping the complexity of the decomposition logic.

- **Failure signatures:**
  - **Regressing to Verbose CoT:** If the PPO reward scaling is too weak, the model reverts to generating lists of questions (average word count > 100) similar to the base model.
  - **Incorrect Value Extraction:** The paper notes reliance on regex for evaluation; if the EQD model generates a question that leads to a number with different formatting (e.g., currency symbols), the exact match reward fails.
  - **Compatibility Issues:** Specialized reasoning models (like o3-mini) that are optimized for "direct" prompts may see performance drops when augmented with EQD sub-questions.

- **First 3 experiments:**
  1. **Verify Reward Impact:** Run Step 2 training with random rewards vs. the Answer Comparison Reward to confirm that the performance gain is driven by the specific reward signal and not just generic fine-tuning.
  2. **Cross-Model Transfer:** Train the EQD model using Llama3.1 as the QA model in Step 2, but evaluate the generated questions using GPT-4o as the QA model to test transferability claims.
  3. **Ablation on Conciseness:** Analyze the correlation between the "word count" of generated questions and accuracy to verify the paper's claim that "single" questions are optimal.

## Open Questions the Paper Calls Out

The paper identifies several open questions:

- **Cross-domain generalization:** Can EQD's effectiveness be generalized to other specialized domains beyond finance (e.g., legal, medical, engineering) where quantitative reasoning is required? The authors state "our method's design and underlying principles are domain-agnostic, not specifically tied to financial knowledge" but note they only evaluated on finance due to dataset availability constraints.

- **Mechanism of conciseness:** Why does the RL alignment process naturally produce concise single-question decompositions without explicit length penalization? The paper notes: "Our training objectives do not explicitly penalize or limit generation length... The natural emergence of conciseness in the model's outputs indicates that brevity is an inherent requirement of effective QA support."

- **Complexity-level variation:** How does the performance improvement from EQD vary across different complexity levels of quantitative reasoning tasks? The paper observes greater improvements on FinQA (multi-step calculations) than EDTQA (simpler value extraction), but does not systematically categorize and test across reasoning complexity.

## Limitations

- **Numerical answer brittleness:** The method relies on exact string matching for numerical answers, creating brittleness in real-world deployment, particularly with currency formatting and percentage representations that may not match regex patterns.

- **Single-question constraint:** The approach is optimized for single critical-path decomposition and may fail on complex multi-hop reasoning tasks requiring 3+ hops across disparate documents.

- **Generalization claims:** While the paper claims the QD model generalizes across different QA models (Llama3.1, GPT-4o, Claude), the lack of rigorous systematic evaluation leaves questions about robustness across diverse model architectures.

## Confidence

- **High Confidence:** The core finding that EQD outperforms verbose CoT prompting on financial QA benchmarks is well-supported by extensive ablation studies and comparison with strong baselines (40.4% vs 42.8% on FinQA, 69.5% vs 70.1% on TAT-QA, etc.). The experimental design is rigorous with clear control conditions and the results are consistent across multiple datasets.

- **Medium Confidence:** The mechanism that "single well-chosen questions" are superior to detailed reasoning chains is plausible but not definitively proven. While the paper shows that reducing word count correlates with improved accuracy, the causal relationship between question conciseness and performance could also be influenced by the reward function's specific structure or the nature of the financial QA tasks.

- **Medium Confidence:** The claim that lightweight domain fine-tuning on ConvFinQA suffices for effective domain alignment is supported by ablation studies showing performance drops when Step 1 is removed, but the small size of ConvFinQA (3,073 entries) raises questions about whether the model truly captures deep domain knowledge versus surface-level terminology.

- **Low Confidence:** The assertion that the EQD model generalizes well across different QA models (Llama3.1, GPT-4o, Claude) lacks rigorous testing. The paper mentions this capability but doesn't provide systematic evaluation showing consistent performance across diverse model architectures and reasoning styles.

## Next Checks

1. **Reward Function Robustness Test:** Implement and test the exact regex patterns and numerical parsing logic used in the reward function across edge cases (currency formatting, percentage representations, large numbers with words vs digits) to validate that the brittleness in answer comparison doesn't artificially inflate performance metrics.

2. **Cross-Model Generalization Study:** Train the EQD model using Llama3.1 as the QA model in Step 2, then evaluate the generated questions using GPT-4o, Claude, and o3-mini as QA models to systematically measure performance degradation and validate the claimed generalization capability.

3. **Multi-Hop Reasoning Evaluation:** Design a test suite of questions requiring 3+ hops of reasoning across multiple documents to determine whether the single-question decomposition approach breaks down on complex queries, and if so, identify the failure threshold.