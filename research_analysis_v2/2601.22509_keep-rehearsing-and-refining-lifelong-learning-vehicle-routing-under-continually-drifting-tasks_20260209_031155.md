---
ver: rpa2
title: 'Keep Rehearsing and Refining: Lifelong Learning Vehicle Routing under Continually
  Drifting Tasks'
arxiv_id: '2601.22509'
source_url: https://arxiv.org/abs/2601.22509
tags:
- learning
- tasks
- dree
- task
- lifelong
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies lifelong learning for neural VRP solvers under
  continual task drift, where tasks change over time and per-task training is limited.
  The authors propose DREE, a framework combining problem instance replay, behavior
  replay, and experience enhancement to preserve and refine knowledge.
---

# Keep Rehearsing and Refining: Lifelong Learning Vehicle Routing under Continually Drifting Tasks

## Quick Facts
- arXiv ID: 2601.22509
- Source URL: https://arxiv.org/abs/2601.22509
- Reference count: 40
- Primary result: DREE outperforms existing lifelong VRP methods in AP (3.11 vs 6.16), AFB (0.23 vs 0.09), and AMFB (0.42 vs 0.10) metrics.

## Executive Summary
This paper addresses lifelong learning for neural vehicle routing solvers under continual task drift, where tasks change over time and per-task training is limited. The authors propose DREE, a framework combining problem instance replay, behavior replay, and experience enhancement to preserve and refine knowledge. Experiments on TSP and CVRP show DREE outperforms existing lifelong learning methods in stability-plasticity balance while generalizing well to unseen benchmark tasks.

## Method Summary
DREE buffers both instances and solver behaviors, replays them during learning, and updates them with better solutions found later. The framework uses reservoir sampling to maintain a fixed-size buffer of experiences, periodically re-solves buffered instances (PIR), regularizes current policies toward buffered behaviors (BR), and upgrades buffered experiences when better solutions are discovered (EE). The loss combines base solver RL loss with weighted BR and PIR terms, balancing stability against plasticity.

## Key Results
- DREE achieves significantly better AP (3.11 vs 6.16), AFB (0.23 vs 0.09), and AMFB (0.42 vs 0.10) compared to fine-tuning and LLR-BC baselines
- The framework generalizes well to unseen benchmark tasks from TSPLIB and CVRPLIB
- DREE remains effective across different base solvers (POMO, Omni, INViT) and in periodically stationary scenarios
- Ablation shows each component (PIR, BR, EE) contributes meaningfully to performance

## Why This Works (Mechanism)

### Mechanism 1: Problem Instance Replay (PIR)
Re-solving buffered instances from past tasks mitigates catastrophic forgetting by interleaving old-task learning with new-task learning. The solver periodically samples buffered instances, re-solves them with current parameters, and applies the base solver's loss on the new trajectory. The replay interval adapts based on improvement rate to balance plasticity vs. stability.

### Mechanism 2: Behavior Replay (BR)
Regularizing the solver to imitate its own past high-quality behaviors preserves task-specific policy knowledge more efficiently than instance replay alone. For each buffered state, BR computes a weighted KL-divergence between current and buffered policies, penalizing deviation with confidence-based weights emphasizing reliable behaviors.

### Mechanism 3: Experience Enhancement (EE)
Actively upgrading buffered behaviors when better solutions are found prevents low-quality experiences from misdirecting future learning. During PIR, when the current solver finds a better solution than buffered, the buffer updates with the new objective and trajectory, ensuring BR always targets best-so-far behaviors.

## Foundational Learning

- **Catastrophic Forgetting in Sequential Learning**: Why needed: The paper addresses how sequential task learning overwrites parameters, degrading prior task performance. Quick check: Can you explain why fine-tuning on new tasks hurts old-task performance?

- **Experience Replay (Rehearsal) in Continual Learning**: Why needed: DREE's core contribution is dual-replay mechanism. Prior familiarity with replay buffers and reservoir sampling helps understand design choices. Quick check: How does reservoir sampling differ from FIFO or prioritized replay?

- **Markov Decision Process (MDP) Formulation for Combinatorial Optimization**: Why needed: VRP is framed as instance-conditioned MDPs with drifting distributions. Policy gradient (e.g., REINFORCE) is assumed in base solvers like POMO. Quick check: How does the state space vary across VRP instances while action space remains fixed?

## Architecture Onboarding

- **Component map**: Base Solver -> Experience Buffer -> PIR Module -> BR Module -> EE Module -> Loss Aggregator
- **Critical path**: 1) Sample batch from current task → solve → L_DRL; 2) Sample experience batch from buffer → compute L_BR; 3) If PIR interval met → run PIR → compute L_PIR and M+ → update N; 4) If better solution found → update buffer via EE; 5) Aggregate losses, optimize θ, insert new experiences via reservoir sampling
- **Design tradeoffs**: Buffer size |B|: larger improves coverage but increases memory; α (BR weight): higher values prioritize stability; PIR interval range: adaptive scheduling based on M+/M balances computation vs. consolidation; Single model vs. task-specific models: DREE uses one model to avoid O(T) storage
- **Failure signatures**: High AFB/AMFB with low AP → insufficient replay; High ABPl with high AFB → increase α or decrease LB; Diverging training loss → check EE updates; No improvement on replay → reduce UB
- **First 3 experiments**: 1) Ablation sanity check: Run DREE, nPIR, nBR, nEE on task order 1; 2) Buffer size sweep: Test |B|∈{64, 128, 256, 512} on CVRP; 3) Base solver swap: Replace POMO with Omni or INViT

## Open Questions the Paper Calls Out

- **Open Question 1**: Can selectively emphasizing time steps that accumulate significant drift improve DREE's learning efficiency compared to treating all tasks equally? The authors state a potential future direction is to "selectively emphasize steps that accumulate significant drift and are most distinctive to others."

- **Open Question 2**: How can DREE be extended to handle scenarios with continually drifting constraints (transition dynamics) rather than just drifting node distributions? The authors list extending DREE to "scenarios with continually drifting constraints" as a promising future direction.

- **Open Question 3**: Does prioritized experience replay outperform the reservoir sampling strategy currently used in DREE for managing the experience buffer? The paper utilizes reservoir sampling to ensure equal retention probability, but does not investigate if prioritizing "harder" or more representative instances improves consolidation.

## Limitations

- The paper does not specify exact implementation details of the confidence-based weighting (CaEW) for behavior replay
- Buffer size effects plateau at |B|=256, but optimal buffer size for different task complexities remains unknown
- Performance under more extreme distribution drift (beyond the tested linear interpolation) has not been evaluated

## Confidence

- **High**: DREE's superiority over fine-tuning and LLR-BC on six core VRP tasks with AP, AFB, and AMFB metrics
- **Medium**: Generalization to periodically stationary scenarios and different base solvers (single experiment per setting)
- **Medium**: Component ablation results (consistent with proposed mechanism but rely on specific CaEW implementation)

## Next Checks

1. Test DREE on more extreme distribution drift scenarios (e.g., >50% change in node distributions) to assess limits of adaptive PIR interval
2. Conduct comprehensive ablation study on CaEW weighting scheme to isolate its contribution to behavior replay effectiveness
3. Evaluate DREE's performance when base solver is trained from scratch vs. pre-trained on stationary task, measuring continual learning vs. multi-task pre-training benefits