---
ver: rpa2
title: 'RigoChat 2: an adapted language model to Spanish using a bounded dataset and
  reduced hardware'
arxiv_id: '2503.08188'
source_url: https://arxiv.org/abs/2503.08188
tags:
- language
- data
- dataset
- table
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper demonstrates that a state-of-the-art LLM can be efficiently
  adapted to a specific language task using minimal resources and a small, high-quality
  dataset. The authors introduce a methodology for collecting and filtering Spanish
  conversational data, augmenting it with LLM-generated responses, and applying automated
  evaluation methods to generate preference pairs for training.
---

# RigoChat 2: an adapted language model to Spanish using a bounded dataset and reduced hardware

## Quick Facts
- arXiv ID: 2503.08188
- Source URL: https://arxiv.org/abs/2503.08188
- Reference count: 40
- Demonstrates efficient LLM adaptation to Spanish using minimal resources and a small, high-quality dataset

## Executive Summary
This paper presents RigoChat-7b-v2, a Spanish-adapted language model built using Qwen2.5-7B-Instruct as a base model and Direct Preference Optimization (DPO) with a carefully curated dataset. The authors introduce a methodology for collecting and filtering Spanish conversational data, augmenting it with LLM-generated responses, and applying automated evaluation methods to generate preference pairs for training. The resulting model outperforms other models of similar size and even larger ones like GPT-4o in Spanish language tasks while maintaining general capabilities, demonstrating that high-quality, representative data can be more important than computational power in aligning LLMs to specific tasks.

## Method Summary
The authors developed a methodology that starts with a high-quality Spanish conversational dataset collected from various sources and filtered for quality. They augmented this dataset by generating multiple LLM responses for each conversation thread and using an automated evaluator (Llama-3.1-8B-Instruct) to score and rank them. This process created preference pairs for training. Using Qwen2.5-7B-Instruct as the base model, they applied Direct Preference Optimization (DPO) with Low-Rank Adaptation (LoRA) to efficiently fine-tune the model on a single GPU. The approach specifically avoided supervised fine-tuning before DPO, finding that starting from the instruction-tuned model directly yielded better results. They also produced quantized versions of the model for deployment on resource-constrained hardware.

## Key Results
- RigoChat-7b-v2 outperforms other models of similar size and even larger models like GPT-4o in Spanish language tasks
- The model achieves 79.55 average score using Qwen2.5-7B-Instruct + DPO, outperforming the same pipeline with SFT + DPO (72.43)
- Quantized versions (3-bit) maintain reasonable performance while enabling efficient inference on constrained hardware

## Why This Works (Mechanism)

### Mechanism 1: Preference Dataset Construction via LLM-Augmented Distillation
The authors constructed high-quality preference pairs without human annotation at scale by using LLMs to generate alternative responses and automated evaluators to rank them. Expert-annotated responses were treated as "chosen" by assumption, while multiple LLMs generated alternatives for each conversation thread. An automated evaluator (Llama-3.1-8B-Instruct) scored all responses, enabling selection of "rejected" examples from lower-scored outputs. This approach assumes expert annotations are consistently superior to model-generated alternatives and that the automated evaluator approximates human judgment sufficiently for preference learning. Evidence shows Llama-3.1-8B-Instruct achieved 43 matches out of 90 with human evaluations and Pearson correlation of 0.62.

### Mechanism 2: Starting from Instruction-Tuned Base Model
The authors found that applying DPO directly to an instruction-tuned model (Qwen2.5-7B-Instruct) outperforms multi-stage pipelines involving supervised fine-tuning first. The instruction-tuned model already possesses conversational capabilities, and DPO refines preference alignment without disrupting learned instruction-following behaviors. Adding SFT before DPO degraded performance. This approach assumes the instruction-tuned model's existing capabilities are sufficiently aligned with the target domain that preference optimization alone can specialize it.

### Mechanism 3: Low-Rank Adaptation for Memory-Efficient Preference Learning
LoRA with rank-64 adaptation enabled DPO on a 7B model using a single GPU while preserving model quality. Instead of updating all parameters, LoRA adds low-rank matrices to linear layers, and gradient checkpointing further reduces memory. This constrains the optimization space, potentially acting as implicit regularization. The approach assumes the preference alignment signal can be captured in a low-rank subspace without requiring full-model updates.

## Foundational Learning

- **Direct Preference Optimization (DPO)**: Essential for understanding the training approach that reformulates RL as a classification loss on preference pairs, eliminating the need for a separate reward model.
- **LoRA (Low-Rank Adaptation)**: Critical for understanding memory constraints and the relationship between rank, alpha scaling, and which modules to target for efficient fine-tuning.
- **LLM-as-Judge Evaluation**: Important for understanding automated preference generation and the balance between evaluator accuracy and computational efficiency.

## Architecture Onboarding

- **Component map**: Data Sources → Quality Filter → Thread Decomposition → LLM Response Generation → Automated Evaluation → Preference Pairs → DPO Training (TRL + PEFT/LoRA) → Quantization
- **Critical path**: Preference dataset quality (21,975 conversations from OpenAssistant + IIC private), evaluator selection (Llama-3.1-8B-Instruct chosen for balance of accuracy and efficiency), DPO hyperparameters (learning rate 5×10⁻⁶ identified as critical)
- **Design tradeoffs**: Data volume vs. quality (smaller curated dataset over massive noisy corpora), evaluator accuracy vs. cost (Llama-3.1-8B-Instruct vs. higher-correlation but more expensive quantized Llama-3.1-70B), quantization level vs. performance (3-bit viable, below 2.5-bit severe degradation)
- **Failure signatures**: Learning rate > 10⁻⁵ → catastrophic forgetting, adding SFT before DPO on instruction-tuned model → ~7 point average score drop, aggressive quantization below 3-bit → severe degradation
- **First 3 experiments**: 1) Reproduce evaluator correlation by sampling 90 examples and comparing against Llama-3.1-8B-Instruct evaluator using Kendall correlation, 2) Ablate preference pair construction by training with only expert-vs-worst pairs vs. all three pair types, 3) Test quantization bounds by evaluating task-specific degradation at 4-bit, 3-bit, and 2-bit

## Open Questions the Paper Calls Out

1. Can combining Group Relative Policy Optimization (GRPO) with Direct Preference Optimization (DPO) effectively transfer high-quality responses from a small preference corpus to a larger instruction corpus? The authors plan to apply GRPO on tasks where outcome quality can be easily measured in conjunction with their best-performing DPO-trained model on their large-scale instruction corpus.

2. How can the accuracy of automated LLM-based evaluation be improved beyond the 45.2% match rate with human judgments achieved in this study? The best automated evaluator achieved only 0.452 average accuracy and 36/90 matches with human evaluation, leaving significant room for improvement.

3. Does the assumption that expert-annotated responses are always preferable to model-generated responses introduce systematic biases in preference learning? This assumption simplifies preference pair construction but may not hold when expert responses are factually correct but less fluent, or when advanced models generate responses that humans might prefer.

## Limitations

- The preference dataset construction relies entirely on automated LLM evaluation without extensive human validation beyond the initial 90-sample correlation test
- The reported correlations (0.62) are reasonable but not definitive proof of human-level judgment, particularly given the sparse human evaluation data
- The methodology assumes that expert-annotated responses are consistently superior to LLM-generated alternatives without systematic error analysis of expert annotations

## Confidence

*High confidence*:
- The preference-based training methodology with automated evaluation produces a Spanish-adapted model that outperforms similarly-sized alternatives on established benchmarks
- Qwen2.5-7B-Instruct + DPO without SFT achieves superior results compared to the same pipeline with SFT
- Quantized models (3-bit) maintain reasonable performance while enabling efficient inference on constrained hardware

*Medium confidence*:
- The automated evaluator approximates human judgment sufficiently for preference learning (supported by correlation but limited human validation)
- LoRA with rank-64 is sufficient for capturing preference alignment without full fine-tuning (no direct comparison provided)
- The distilled preference pairs contribute meaningfully to performance gains (ablation not performed)

*Low confidence*:
- The approach would generalize to base models without instruction tuning (never tested)
- The evaluator's judgments would remain consistent across different Spanish dialects or domains
- Learning rate sensitivity is the only critical hyperparameter (other factors not thoroughly explored)

## Next Checks

1. **Human validation expansion**: Collect human preference judgments for 500 randomly sampled conversation pairs from the preference dataset to assess evaluator consistency, identify systematic biases, and estimate the false positive rate in the automated preference selection process.

2. **Ablation study of preference pair types**: Train three identical models varying only in preference pair composition: (a) expert-vs-best only, (b) expert-vs-worst only, (c) all three pair types. Compare performance on Spanish benchmarks to isolate the contribution of the LLM-distilled preference pairs versus expert-annotated data.

3. **Quantization performance floor mapping**: Systematically evaluate model performance across quantization levels (4-bit through 1-bit in 0.5-bit increments) on Spanish-specific tasks and general capabilities to identify the precise hardware-constrained deployment threshold where performance becomes unacceptable for target applications.