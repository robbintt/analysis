---
ver: rpa2
title: Direct Alignment with Heterogeneous Preferences
arxiv_id: '2502.16320'
source_url: https://arxiv.org/abs/2502.16320
tags:
- reward
- preference
- policy
- preferences
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of aligning AI systems to heterogeneous
  human preferences, where different user types have distinct reward functions. The
  authors formalize this heterogeneity and show that standard alignment methods like
  DPO fail to maximize the average reward across user types, instead aligning with
  Borda count, which is sensitive to dataset sampling distribution and can lead to
  suboptimal outcomes.
---

# Direct Alignment with Heterogeneous Preferences

## Quick Facts
- **arXiv ID:** 2502.16320
- **Source URL:** https://arxiv.org/abs/2502.16320
- **Reference count:** 40
- **Primary result:** Standard DPO aligns to Borda count rather than user-weighted average reward when preferences are heterogeneous

## Executive Summary
This paper addresses the challenge of aligning AI systems to heterogeneous human preferences, where different user types have distinct reward functions. The authors formalize this heterogeneity and show that standard alignment methods like DPO fail to maximize the average reward across user types, instead aligning with Borda count, which is sensitive to dataset sampling distribution and can lead to suboptimal outcomes. The paper proposes methods that use minimal annotator information to improve alignment through first-order corrections, and designs a consistent loss function when full annotator information is available, while proving that no sample-efficient consistent direct loss exists in this setting.

## Method Summary
The paper formalizes heterogeneous preference alignment using a Bradley-Terry model where each user type has its own reward function. The key insight is that DPO's implicit objective under this model is to maximize normalized Borda count (NBC) rather than the user-weighted average reward. To address this, the authors propose two approaches: (1) a first-order Taylor expansion correction to DPO that incorporates variance between user types, requiring only paired preferences from the same annotator, and (2) a consistent loss function that conditions on unanimous agreement across all user types when full annotator information is available. The methods are validated on synthetic and semi-synthetic datasets, showing improved alignment accuracy compared to standard DPO.

## Key Results
- Standard DPO implicitly optimizes for normalized Borda count (NBC) rather than user-weighted average reward when preferences are heterogeneous
- A first-order corrected DPO loss using variance estimation from paired annotations improves alignment accuracy by 4.3% in experiments
- With full annotator information, a consistent loss exists but must discard all disagreeing samples, proving no sample-efficient consistent direct loss is possible
- Experiments on synthetic and semi-synthetic datasets validate the proposed methods

## Why This Works (Mechanism)

### Mechanism 1
Standard DPO implicitly optimizes for normalized Borda count (NBC) rather than the user-weighted average reward when preferences are heterogeneous. DPO maximizes the likelihood of observed preferences, and in the heterogeneous setting, this converges to minimizing cross-entropy between the predicted preference probability and the population-level preference distribution. The first-order condition shows the optimal policy is monotone in NBC(y|x), which depends on the sampling distribution D(·|x) of alternative responses—not just the underlying rewards. This works under the core assumption that preferences follow a Bradley-Terry-like model where Pr(y₂ ≻ y₁ | u) = σ(r*(y₂;u) - r*(y₁;u)). The break condition is when preferences are homogeneous (single user type), where NBC aligns with the true reward and DPO works correctly.

### Mechanism 2
A first-order Taylor expansion correction to DPO can improve alignment using only minimal annotator linkage (pairs of preferences from the same annotator). The variance term Varᵤ[Δr*(y₁,y₂;u)] captures preference heterogeneity. Estimating this requires a joint likelihood model J trained on preference pairs from the same user. The corrected objective becomes: σ(h(y₁,y₂;π)) + (α/2)σ''(h(y₁,y₂;π))·V(y₁,y₂). This assumes the Taylor expansion approximation is reasonable (σ is sufficiently smooth) and paired annotations from the same user are available. The break condition is if σ is highly nonlinear or variance is extreme, where higher-order terms become significant.

### Mechanism 3
With full annotator information (labels from all user types per sample), a consistent loss exists but must discard all disagreeing samples—proving no sample-efficient consistent direct loss is possible. Conditioned on unanimous agreement, Pr(y₂ ≻ y₁ | agreement) ∝ exp(Σᵤ Δr*(y₁,y₂;u)), which can be expressed via policy ratios. However, Theorem 6.2 proves any consistent loss cannot use samples with disagreement when |U| > 3. This assumes user types are finite and equally represented, with preferences following Bradley-Terry. The break condition is if you relax direct optimization and train personalized reward models, you can regain sample efficiency at the cost of additional storage/computation.

## Foundational Learning

- **Concept: Bradley-Terry (BT) Preference Model**
  - Why needed here: The entire analysis builds on BT, where Pr(y₂ ≻ y₁) = σ(r(y₂) - r(y₁)). Understanding this is essential to see why mixture of BT models cannot be represented by a single BT (Proposition C.1).
  - Quick check question: Given rewards r(A)=2, r(B)=1, r(C)=0, what is Pr(A ≻ B) under BT with sigmoid σ?

- **Concept: Policy-Reward Duality in KL-Regularized RL**
  - Why needed here: Eq. (3-4) establish the closed-form relationship π*(y|x) ∝ πref(y|x)·exp(r*(x,y)/β), enabling DPO to bypass explicit reward modeling.
  - Quick check question: If π*(y|x)/πref(y|x) = exp(1), what is the induced reward r(y) when β=1?

- **Concept: Borda Count and Social Choice Distortion**
  - Why needed here: Understanding why NBC can differ from average reward—and why it's sensitive to sampling distribution—is central to grasping DPO's failure mode.
  - Quick check question: With three candidates {A,B,C} and voters preferring A>B>C (60%) or C>B>A (40%), who wins under Borda count vs. average utility?

## Architecture Onboarding

- **Component map:**
  [Preference Data] → [Annotator Linkage] → [Joint Likelihood Model J] → [Variance Estimator V] → [Policy π] ← [First-Order Corrected DPO Loss] ← [Standard DPO Loss + α·σ''·V] → [Induced Reward] → [Agreement with Average Reward (Evaluation)]

  Alternative path with full information:
  [Full-Type Labels] → [Filter to Agreement Cases] → [Temperature-Adjusted DPO] → [Consistent Policy]

- **Critical path:**
  1. Collect paired preferences with annotator linkage (same annotator labels two comparisons)
  2. Train joint likelihood model J(x,y₁,y₂,x',y'₁,y'₂) to estimate Eᵤ[σ(Δr*)·σ(Δr'*)]
  3. Compute V(y₁,y₂) via Lemma 5.3 and apply corrected loss
  4. Alternatively, with full-type labels, use Proposition 6.1 loss on agreement-only data

- **Design tradeoffs:**
  - Minimal information (paired labels) → First-order improvement, retains sample efficiency, approximate
  - Full information → Consistent but discards disagreeing samples (sample-inefficient)
  - Indirect approach (personalized reward models) → Both consistent and sample-efficient, but requires training/storing |U| models

- **Failure signatures:**
  - DPO produces "mediocre" outputs that no user type strongly prefers (Section 4.3 example)
  - Rankings shift when dataset sampling distribution changes (Section 7.1: 20% of Pew questions showed ranking changes)
  - First-order correction with α=0 (standard DPO) shows no improvement; α too large may destabilize training

- **First 3 experiments:**
  1. **Synthetic validation:** Replicate Figure 4 environment with n=40, |U|=3 types. Verify πDPO peaks at δ≈0 (mediocre) while π* peaks at δ∈{-10,10}. Test corrected DPO with α∈{0.5,1,2}.
  2. **Sensitivity stress test:** Using Pew survey data, systematically vary the sampling distribution D and measure minimum TV distance to flip NBC rankings. Confirm 0.23 TV distance suffices for 50% of questions.
  3. **Semi-synthetic LLM alignment:** Relabel HH-RLHF with heterogeneous length-based rewards (as in Section 7.3). Compare: (a) vanilla DPO, (b) first-order corrected DPO, (c) agreement-only loss. Report agreement with ground-truth average reward.

## Open Questions the Paper Calls Out

### Open Question 1
What data collection structures beyond paired preferences and full annotator labeling can enable identifiability of user types from anonymous datasets? The paper states that "Further research should explore the additional structures that, when used during data collection, can help with identifiability" and notes that "unsupervised methods might be able to identify annotator types from anonymous datasets." This is unresolved because the paper proves learning the optimal policy from anonymous data is impossible, and even first-order correction requires annotator linkage. No systematic study of what minimal data structures enable type identification exists. Evidence would require a characterization of data collection protocols (e.g., clustering constraints, temporal patterns) that provably enable unsupervised type identification, validated on real preference datasets with known demographics.

### Open Question 2
Can hybrid approaches combine the sample efficiency of approximate methods with the consistency guarantees of the agreement-only loss? Theorem 6.2 proves that any consistent direct loss must discard disagreement samples when |U| > 3. The authors suggest "an alternative approach—individual reward training and aggregation—may be more practical" but note it incurs "additional training for each user type and memory." This is unresolved because the paper establishes a fundamental tension but does not explore whether relaxation of either direct optimization or strict consistency could yield practical middle-ground solutions. Evidence would require a method that achieves bounded regret on disagreement samples while maintaining consistency on agreement samples, with theoretical sample complexity bounds and empirical validation on heterogeneous preference benchmarks.

### Open Question 3
How sensitive are the proposed methods to misspecification of the number of user types or the preference model (beyond Bradley-Terry)? The experiments assume known user types and BT preference model. Section 4.3 briefly notes sensitivity to preference model temperature, but the proposed corrections (Section 5.1 and 6) assume BT. Real-world heterogeneity may involve unknown type counts and non-BT preferences. This is unresolved because theoretical guarantees depend on correct model specification, but practical deployment cannot verify these assumptions from anonymous preference data. Evidence would require robustness experiments varying the true number of types versus assumed number, and extending Proposition 6.1's consistent loss to non-BT preference models with theoretical analysis.

## Limitations
- Analysis critically depends on Bradley-Terry preference model assumption and finite, known user types
- First-order correction assumes mild nonlinearity in σ; extreme variance could invalidate the Taylor approximation
- Consistent loss with full information is sample-inefficient, discarding all disagreeing samples
- Semi-synthetic experiments use simulated heterogeneity rather than real-world heterogeneous human feedback

## Confidence

- **High confidence:** Standard DPO converges to NBC rather than average reward under BT preferences (Section 4.2 proof). The consistent loss formulation and impossibility result (Theorem 6.2) are mathematically rigorous.
- **Medium confidence:** First-order correction improves alignment in synthetic settings. The variance estimator J is well-defined but practical stability and generalization are untested at scale.
- **Low confidence:** The 4.3% improvement claim relies on semi-synthetic data with simulated heterogeneity. Real-world performance with actual heterogeneous human feedback remains unvalidated.

## Next Checks

1. **Agreement rate audit:** Measure the proportion of preference pairs receiving unanimous labels from all user types in the semi-synthetic dataset. If <50%, the consistent loss may be too sample-inefficient for practical use.
2. **Noise sensitivity:** Add observation noise to synthetic preferences (e.g., logistic noise in BT model) and measure degradation in both standard and corrected DPO alignment accuracy. Verify robustness claims.
3. **User-type recovery baseline:** Compare against a simple indirect approach: train three separate reward models (one per user type), then combine into a single policy. Measure sample efficiency vs. the direct methods proposed.