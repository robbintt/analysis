---
ver: rpa2
title: Outside Knowledge Conversational Video (OKCV) Dataset -- Dialoguing over Videos
arxiv_id: '2506.09953'
source_url: https://arxiv.org/abs/2506.09953
tags:
- dialogue
- dataset
- knowledge
- video
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces OKCV, a dataset of 2,017 videos with 5,986
  human-annotated dialogues (40,954 turns) that require models to combine video understanding,
  external knowledge retrieval, and conversational reasoning. Unlike prior datasets,
  OKCV decomposes complex questions into multi-turn dialogues, ensuring semantic consistency.
---

# Outside Knowledge Conversational Video (OKCV) Dataset -- Dialoguing over Videos

## Quick Facts
- **arXiv ID:** 2506.09953
- **Source URL:** https://arxiv.org/abs/2506.09953
- **Reference count:** 20
- **Primary result:** OKCV dataset of 2,017 videos with 5,986 human-annotated dialogues requiring video understanding + external knowledge + conversational reasoning.

## Executive Summary
The OKCV dataset addresses a critical gap in video-language understanding by requiring models to engage in multi-turn dialogues about videos while retrieving external knowledge not present in the visual content. Unlike previous datasets that treat video QA as isolated single-turn tasks, OKCV decomposes complex questions into coherent dialogue chains where each turn depends on the previous answer. This creates a more realistic conversational scenario that better tests a model's ability to maintain semantic consistency while integrating visual grounding with external knowledge retrieval. The dataset demonstrates that current open-source models struggle significantly with this combined task, highlighting the need for more sophisticated video-language architectures.

## Method Summary
The OKCV dataset was constructed using a human-AI collaborative pipeline where GPT-4 generated partial dialogue drafts from video transcripts, followed by human annotators completing answers and ensuring coherence. The dataset includes 2,017 videos (30s-10min) with 5,986 dialogues (40,954 turns) that require both video grounding and external knowledge. Dialogues are structured as multi-turn conversations where questions are decomposed from high-level multifaceted queries into dependent sub-questions. Temporal certificates indicate which video segments are needed for answering questions. The dataset was evaluated using three metrics: Bleurt for semantic similarity, BartScore for response quality, and Prometheus-2 for conversational quality.

## Key Results
- Current open-source models (Llama3-8B, Mistral-7B, Phi-3) perform significantly worse than closed-source models (Gemini-1.5-pro) on OKCV.
- Fine-tuning on OKCV improves dialogue-style metrics (Bleurt/BartScore) but decreases Prometheus-2 scores, suggesting potential overfitting to reference answers.
- Approximately 58% of questions require external knowledge that cannot be answered from video content alone.
- The dataset successfully creates semantically consistent multi-turn dialogues where each question depends on the previous answer.

## Why This Works (Mechanism)

### Mechanism 1: Dialogue Decomposition for Semantic Consistency
- Claim: Decomposing complex questions into multi-turn dialogues improves semantic consistency and reasoning traceability compared to single-turn QA.
- Core assumption: Semantic dependencies between dialogue turns create a coherent reasoning chain that better reflects real conversational scenarios.
- Evidence: Qualitative experiments showed top-down strategy yielded more coherent dialogues requiring knowledge integration compared to bottom-up approach.

### Mechanism 2: Human-AI Collaboration in Dataset Creation
- Claim: Human-AI collaboration reduces annotation burden while preserving reasoning quality.
- Core assumption: AI-generated questions based on transcripts are sufficiently aligned with video content for humans to meaningfully complete, and humans add necessary reasoning judgment.
- Evidence: GPT-4 generated partial dialogue drafts (questioner side only), human annotators completed answers, refined questions, and provided source citations and temporal certificates.

### Mechanism 3: Combined Video Grounding and External Knowledge
- Claim: Requiring both video grounding and external knowledge creates a diagnostic benchmark that separates models with integrated reasoning from those relying on single-modality shortcuts.
- Core assumption: External knowledge cannot be inferred from visual content alone; models must have or retrieve this information.
- Evidence: 57.93% of questions require external knowledge validated via DeepSeek-R1 → Llama-8B, forcing models to retrieve beyond visual content while maintaining temporal video grounding.

## Foundational Learning

- **Outside-Knowledge Visual Question Answering (OK-VQA)**
  - Why needed: OKCV extends OK-VQA from images to video + dialogue; understanding the base task clarifies why external knowledge retrieval is non-optional.
  - Quick check: Can you explain why a VQA system might fail on "What brand of car is shown?" if it only processes visual features without external knowledge?

- **Temporal Certificates**
  - Why needed: OKCV uses temporal certificates to annotate which video segments are actually needed; this informs model architecture choices about temporal attention.
  - Quick check: Given an average video of 178s and average temporal certificate of 42s, what does this suggest about the efficiency of uniform frame sampling?

- **Multi-Modal Retrieval-Augmented Generation**
  - Why needed: Baseline experiments use Clip4Clip for retrieval + LLMs for generation; understanding retrieval-generation coupling is essential for building competitive systems.
  - Quick check: Why might retrieving Wikidata entities via video-text embeddings fail for domain-specific knowledge not well-represented in Wikidata?

## Architecture Onboarding

- **Component map:** Video Encoder -> Video Caption/Feature Extractor -> Knowledge Retrieval Module -> Dialogue State Tracker -> Response Generator -> Evaluation Layer
- **Critical path:** Video → Video Encoder → Visual features or captions → Visual features → Knowledge Retrieval → External knowledge entities → [Dialogue history + Current question + Video caption + Retrieved knowledge] → LLM → Response → Evaluation metrics → Performance signal
- **Design tradeoffs:**
  - Zero-shot vs. Fine-tuning: Zero-shot leverages parametric knowledge; fine-tuning adapts to dialogue style but may overfit to dataset quirks.
  - Caption-based vs. Native Video-LM: Captioning enables any text LLM but loses fine-grained visual detail; Video-Llava processes video directly but has lower baseline performance.
  - Wikidata vs. broader retrieval: Wikidata is structured but limited; retrieved knowledge was "neutral at best" for fine-tuned models.
- **Failure signatures:**
  - Knowledge not integrated: Model answers using only parametric knowledge, ignoring retrieved entities.
  - Temporal shortcut: Model answers from single frames when temporal certificate indicates 42s needed.
  - Dialogue incoherence: Responses ignore prior turns; Prometheus-2 penalizes for indirect or overly lengthy responses.
  - Caption quality bottleneck: Poor captions from mPLUG-2 propagate errors; Bleurt scores plateau despite knowledge augmentation.
- **First 3 experiments:**
  1. Reproduce zero-shot baselines: Run Llama3-8B with "Nothing," "Caption," and "Caption+Knowledge" configurations; verify Bleurt scores ~0.50 range.
  2. Ablate knowledge retrieval: Compare Clip4Clip retrieval vs. no retrieval vs. dense passage retrieval on Wikidata subset. Measure impact on the 57.93% of questions marked as requiring external knowledge.
  3. Analyze temporal certificate alignment: For dialogues with temporal certificates >60s, compare uniform frame sampling vs. certificate-guided cropping.

## Open Questions the Paper Calls Out

- **Can knowledge retrieval methods specifically tailored for video modalities outperform the generic text-based baselines currently evaluated on the OKCV dataset?**
  - Basis: Authors state "Future work should explore knowledge retrieval methods more tailored to video and different knowledge bases."
  - Why unresolved: Current evaluations used general retrieval methods (Clip4Clip and Wikidata) which yielded mixed or negative results in fine-tuning settings.

- **How does model performance scale when conversation depth is increased beyond the current average of four dialogue turns per topic?**
  - Basis: Limitations section notes conversations typically contain up to four turns and suggests longer dialogues could be interesting.
  - Why unresolved: Current dataset structure limits ability to test long-context reasoning and semantic consistency over extended dialogues.

- **Why does the inclusion of explicit external knowledge during fine-tuning degrade performance for strong backbone models like Llama3?**
  - Basis: Table 4 shows Llama3-8B performance drops from 0.69 to 0.51 when external knowledge is added.
  - Why unresolved: Unclear if performance drop is due to specific Wikidata source, retrieval method's inability to align with video context, or model's difficulty in fusing explicit knowledge with implicit reasoning.

## Limitations
- Dataset construction relies on GPT-4 for initial dialogue generation, introducing potential bias in question framing and reasoning patterns.
- Knowledge retrieval system uses Wikidata as a fixed corpus, limiting coverage of domain-specific knowledge.
- Evaluation framework depends on reference-based metrics that may not fully capture semantic consistency in multi-turn dialogues.

## Confidence
- **High Confidence:** Dataset statistics and baseline evaluation results are verifiable through the provided repository.
- **Medium Confidence:** Effectiveness of the top-down dialogue decomposition strategy and human-AI collaboration pipeline.
- **Low Confidence:** Generalizability of performance gaps between open-source and closed-source models, as closed-source models were not directly evaluated on the same exact pipeline.

## Next Checks
1. Conduct an ablation study comparing dialogue coherence scores between top-down and bottom-up question generation strategies using human evaluation on a subset of 100 dialogues.
2. Test knowledge retrieval performance on a domain-specific corpus (e.g., medical or technical knowledge) to assess whether Wikidata limitations explain the "neutral at best" fine-tuning results.
3. Implement temporal certificate-guided video sampling and measure impact on dialogue response accuracy compared to uniform sampling, particularly for videos with certificates exceeding 60 seconds.