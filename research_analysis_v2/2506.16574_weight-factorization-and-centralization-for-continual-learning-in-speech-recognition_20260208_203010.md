---
ver: rpa2
title: Weight Factorization and Centralization for Continual Learning in Speech Recognition
arxiv_id: '2506.16574'
source_url: https://arxiv.org/abs/2506.16574
tags:
- learning
- speech
- adapters
- datasets
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles continual learning for speech recognition models,
  specifically addressing catastrophic forgetting when adapting foundational models
  to new code-switching datasets without access to original training data. The proposed
  approach, inspired by the human wake-sleep cycle, introduces a two-phase framework:
  a factorization stage that distributes incoming data across multiple low-rank adapters,
  and a centralization stage that periodically merges these adapters via averaging
  into the base model.'
---

# Weight Factorization and Centralization for Continual Learning in Speech Recognition

## Quick Facts
- arXiv ID: 2506.16574
- Source URL: https://arxiv.org/abs/2506.16574
- Reference count: 0
- Primary result: 24.6% relative WER improvement vs baseline Whisper on code-switching datasets

## Executive Summary
This paper tackles continual learning for speech recognition models, specifically addressing catastrophic forgetting when adapting foundational models to new code-switching datasets without access to original training data. The proposed approach, inspired by the human wake-sleep cycle, introduces a two-phase framework: a factorization stage that distributes incoming data across multiple low-rank adapters, and a centralization stage that periodically merges these adapters via averaging into the base model. This simple Gaussian prior-based averaging strategy effectively mitigates catastrophic forgetting while enabling adaptation to new data. Experiments on six code-switching datasets show a 24.6% relative improvement in overall error rate compared to the baseline Whisper model, with a 11.7% improvement in backward transfer performance. The method demonstrates strong balance between stability (preserving previous knowledge) and plasticity (learning new tasks), outperforming competitive continual learning approaches using weight averaging and distillation. The approach is scalable, requiring only one hyperparameter (iteration size before centralization) and minimal additional memory overhead.

## Method Summary
The method employs a two-phase continual learning framework for speech recognition. During factorization, incoming code-switching datasets are assigned dedicated LoRA adapters that modify query/key projection layers through low-rank decomposition (ΔW = (α/r)AB). The base Whisper model remains frozen during this phase, preventing interference between sequential tasks. After processing K datasets, the centralization phase computes the average of all trained adapters and merges them into the base model using a lora_merge operation. This averaging leverages the Gaussian prior assumption (weight decay) to reduce variance in the merged weights. The merged model then becomes the new base for subsequent adapters, enabling both forward adaptation and backward knowledge preservation.

## Key Results
- 24.6% relative improvement in overall WER compared to baseline Whisper model
- 11.7% improvement in backward transfer performance (preserving capabilities on previously seen languages)
- Outperforms competitive continual learning approaches using weight averaging and distillation
- Demonstrates positive backward transfer on Turkish (14.3→14.3), Mandarin (9.3→6.9), and Spanish (6.1→5.8) after centralization

## Why This Works (Mechanism)

### Mechanism 1: Low-Rank Adapter Factorization Distributes Learning Burden
Scattering incoming data across multiple low-rank adapters reduces interference between sequential learning tasks. Each dataset Di gets a dedicated adapter Δθi, parameterized as ΔW = (α/r)AB where A ∈ R^(d_out×r), B ∈ R^(r×d_in). The adapter learns task-specific modifications while the base model θ₀ remains frozen. Data segments have distinguishable distributional characteristics that benefit from isolated parameter updates. Evidence shows individual adapters cause severe degradation on non-target languages (e.g., SEAME adapter causes 132.4% WER on Arabic), but this is resolved by centralization. Break condition: If adapters share significant parameter overlap with conflicting gradients, averaging may not reduce variance as predicted.

### Mechanism 2: Gaussian Prior Averaging Reduces Weight Variance
Averaging multiple independently-trained adapters produces weights closer to zero, reducing disruption when merged into the base model. If adapter weights w_i ∼ N(0, σ²), then the averaged weights w̄ = (1/N)Σw_i have variance Var(w̄) = σ²/N. Standard deviation decreases as 1/√N, concentrating the merged update near zero. Weight decay during training imposes the Gaussian prior. Evidence shows the averaged model has much higher sparsity than any individual adapter. Break condition: If regularization is dominated by task loss, weight distributions may deviate significantly from N(0, σ²).

### Mechanism 3: Periodic Centralization Enables Backward Transfer
Merging averaged adapters into the base model after K datasets preserves prior knowledge while incorporating new capabilities. After training K adapters, compute Δ_avg = (1/t)ΣΔθj and merge via lora_merge(θ_{t-1}, Δ_avg). The merged model becomes the new knowledge base for subsequent adapters. Evidence shows second centralization achieves 9.8% average backward WER vs. 11.1% baseline, demonstrating positive backward transfer on multiple languages. Break condition: If K is too small or too large, the stability-plasticity balance degrades.

## Foundational Learning

- Concept: **Catastrophic Forgetting in Sequential Optimization**
  - Why needed here: Addresses the cumulative risk problem where optimizing on dataset D_t deteriorates performance on D_{<t}
  - Quick check question: Can you explain why sequential gradient updates on non-IID data cause interference?

- Concept: **Low-Rank Adaptation (LoRA)**
  - Why needed here: The factorization stage relies on LoRA's parameterization ΔW = (α/r)AB to add <1% parameters while enabling efficient fine-tuning
  - Quick check question: What is the rank r and scaling factor α in LoRA, and how do they affect expressivity vs. regularization?

- Concept: **Stochastic Weight Averaging (SWA)**
  - Why needed here: The centralization mechanism draws inspiration from SWA, which averages model checkpoints to find flatter minima
  - Quick check question: Why does averaging weights from different training stages improve generalization?

## Architecture Onboarding

- Component map:
```
Base Model (Whisper Large-v3-turbo, frozen θ₀)
    ↓
LoRA Adapters (rank r, attached to query/key layers)
    ├── Δθ₁ ← Dataset D₁
    ├── Δθ₂ ← Dataset D₂
    └── Δθₖ ← Dataset Dₖ
    ↓
Averaging Module: Δ_avg = (1/K)ΣΔθᵢ
    ↓
Merge Operation: θ_new = lora_merge(θ₀, Δ_avg)
    ↓
Updated Knowledge Base → Next iteration
```

- Critical path:
1. Configure LoRA rank r and scaling α
2. Set centralization interval K
3. Train adapters sequentially with weight decay
4. Trigger centralization every K datasets
5. Resume training with updated base model

- Design tradeoffs:
- **K (centralization interval)**: Larger K → more averaging, better regularization, but delayed consolidation. Smaller K → faster integration, but higher variance risk.
- **Rank r**: Higher rank → more adapter capacity, but less parameter efficiency. Paper does not ablate this.
- **Weight decay strength**: Controls Gaussian prior tightness. Too weak → high variance in averaged weights. Too strong → underfitting.

- Failure signatures:
- Individual adapters show catastrophic forgetting on non-target languages (Table 1: SEAME causes 132.4% WER on Arabic). This is expected and resolved by centralization.
- If centralization increases error rates, check: (a) K too small, (b) weight decay insufficient, (c) adapter initialization issues.
- If forward transfer degrades, the merged base model may have over-regularized, losing task-specific features.

- First 3 experiments:
1. **Baseline replication**: Train individual LoRA adapters on each code-switching dataset, evaluate on all backward benchmarks to confirm catastrophic forgetting pattern matches Table 1.
2. **Ablation on K**: Test K ∈ {1, 3, 6} to verify the 1/√N variance reduction correlates with backward transfer performance.
3. **Weight distribution analysis**: Plot histograms of adapter weights before and after averaging to validate the Gaussian assumption and sparsity increase claim.

## Open Questions the Paper Calls Out

- Question: Why does simple Gaussian prior-based averaging of adapters effectively mitigate catastrophic forgetting, and can this theoretical understanding be formalized beyond the current empirical observation?
- Basis in paper: Conclusion states explaining the effectiveness of using such simple intuition is important further work; Section 3.2 notes variance reduction explanation but acknowledges regularization is dominated by the main cross entropy loss.
- Why unresolved: The mathematical justification assumes ideal Gaussian distributions, but the authors acknowledge real weights deviate from this assumption. The unexpected backward transfer effect remains unexplained.
- What evidence would resolve it: Ablation studies analyzing weight distribution changes during centralization; theoretical analysis connecting averaging to optimization landscape properties; controlled experiments isolating the mechanisms behind backward transfer.

- Question: How can the factorization-centralization framework be modified for true online continual learning where the model must be immediately serviceable after each new sample or dataset?
- Basis in paper: Conclusion states the approach needs further modification to be used in online continual learning, in which the model needs to be instantly serviceable given new datasets, or even new samples.
- Why unresolved: The current approach requires periodic centralization after K datasets, creating latency. Adapter assignment also assumes some data segmentation strategy.
- What evidence would resolve it: Development and evaluation of streaming centralization mechanisms; experiments with varying centralization frequencies; latency measurements in production-like scenarios.

- Question: What is the optimal centralization frequency (K), and how does it depend on dataset characteristics, model scale, and task diversity?
- Basis in paper: Paper mentions K is the only hyper-parameter to select but does not systematically explore its sensitivity; experiments use K=3 without ablation studies across different values.
- Why unresolved: The trade-off between more frequent centralization (lower memory, faster knowledge consolidation) versus less frequent (more adapters to average, potentially better variance reduction) remains unexplored.
- What evidence would resolve it: Systematic ablation across K values; analysis of K's interaction with dataset size, number of languages, and adapter rank.

## Limitations

- Critical hyperparameters (LoRA rank r and scaling factor α) are underspecified, directly affecting adapter capacity and regularization strength
- Gaussian prior assumption underlying the averaging mechanism lacks direct empirical validation
- Paper does not provide ablation studies on the centralization interval K
- Computational overhead of maintaining multiple adapters and precise lora_merge() implementation details are not fully clarified

## Confidence

- **High Confidence**: The empirical results showing 24.6% relative WER improvement and positive backward transfer are well-supported by the experimental data in Tables 1 and 2.
- **Medium Confidence**: The theoretical justification for the Gaussian prior averaging mechanism is mathematically sound, but the assumption that adapter weights follow N(0, σ²) is not empirically validated in the paper.
- **Low Confidence**: The choice of specific hyperparameters (LoRA rank, scaling factor, weight decay strength) appears critical but is not justified or ablated in the paper.

## Next Checks

1. **Gaussian Assumption Validation**: Generate histograms of adapter weights before and after averaging to empirically verify the N(0, σ²) distribution assumption and the expected 1/√N variance reduction.

2. **Hyperparameter Sensitivity Analysis**: Systematically vary K (centralization interval) and LoRA rank r to quantify their impact on backward transfer performance and stability-plasticity tradeoff.

3. **Comparative Consolidation Analysis**: Compare the proposed averaging-based consolidation against alternative methods like Elastic Weight Consolidation (EWC) or regularization-based approaches on the same code-switching datasets.