---
ver: rpa2
title: Dense Retrieval for Low Resource Languages -- the Case of Amharic Language
arxiv_id: '2503.18570'
source_url: https://arxiv.org/abs/2503.18570
tags:
- retrieval
- amharic
- languages
- language
- colbert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of dense retrieval for Amharic,\
  \ a low-resource language spoken by over 120 million people. Traditional retrieval\
  \ methods like BM25 struggle with Amharic\u2019s complex morphology, and limited\
  \ linguistic resources further complicate effective retrieval."
---

# Dense Retrieval for Low Resource Languages -- the Case of Amharic Language

## Quick Facts
- arXiv ID: 2503.18570
- Source URL: https://arxiv.org/abs/2503.18570
- Authors: Tilahun Yeshambel; Moncef Garouani; Serge Molina; Josiane Mothe
- Reference count: 16
- Key outcome: Fine-tuning dense retrievers on as few as 150 Amharic relevance examples yields NDCG@10 scores of 0.704 on 2AIRTC and 0.177 on AfriCLIRMatrix, outperforming BM25 baselines and demonstrating feasibility for low-resource languages.

## Executive Summary
This paper addresses the challenge of dense retrieval for Amharic, a low-resource language spoken by over 120 million people. Traditional retrieval methods like BM25 struggle with Amharic's complex morphology, and limited linguistic resources further complicate effective retrieval. The authors evaluate dense retrievers, specifically ColBERT, using BERT-based models pre-trained on Amharic data. While ColBERT with pre-trained Amharic models showed no significant improvement before fine-tuning, fine-tuning on a small Amharic training dataset (150 examples) yielded substantial gains, achieving NDCG@10 scores of 0.704 on the 2AIRTC collection and 0.177 on AfriCLIRMatrix. These results suggest that dense retrieval models can be competitive with BM25 even with minimal training data, though computational resource limitations remain a key bottleneck. The work highlights the feasibility of advancing retrieval systems for low-resource languages through targeted fine-tuning and underscores the need for local AI infrastructure in resource-constrained settings.

## Method Summary
The study evaluates ColBERT dense retrieval using BERT-based models pre-trained on Amharic data, comparing against BM25 baselines on two Amharic collections (2AIRTC and AfriCLIRMatrix). The authors fine-tune pre-trained Amharic BERT models on a small training dataset of 152 relevance-labeled query-document pairs. Three Amharic BERT variants are tested: rasyosef/bert-medium-amharic, roberta-base-amharic, and roberta-amharic-text-embedding-base. SPLADE models are also attempted but fail to produce non-null results. The evaluation uses NDCG@10 as the primary metric, with additional measurements of indexing and retrieval times for computational cost comparison.

## Key Results
- Fine-tuning on just 150 Amharic relevance examples yielded NDCG@10 of 0.704 on 2AIRTC and 0.177 on AfriCLIRMatrix, significantly outperforming BM25 baselines
- Pre-trained Amharic models without fine-tuning showed near-zero performance (0.000-0.005 NDCG@10), confirming fine-tuning necessity
- The best-performing model (roberta-amharic-text-embedding-base) was a sentence transformer that doesn't benefit from ColBERT's late interaction mechanism
- SPLADE models failed entirely, producing null results even with Amharic pre-training
- Dense retrieval indexing took 29-46x longer than BM25, though retrieval times were comparable (0.78-1.17x BM25)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task-specific fine-tuning, not pre-training alone, enables effective dense retrieval for low-resource languages.
- Mechanism: Pre-trained language models encode general linguistic knowledge but lack retrieval-specific alignment. Fine-tuning on query-document relevance pairs adapts embeddings to the ranking objective, bridging the gap between language understanding and retrieval task performance.
- Core assumption: The fine-tuning dataset quality and domain alignment matter more than quantity for low-resource settings.
- Evidence anchors:
  - [abstract]: "ColBERT with pre-trained Amharic models showed no significant improvement before fine-tuning, fine-tuning on a small Amharic training dataset (150 examples) yielded substantial gains"
  - [section 3]: "before fine tuning on the Training Amharic collection, the results remain very poor, from 0.000 to 0.005"
  - [corpus]: Neighbor paper "Optimized Text Embedding Models and Benchmarks for Amharic Passage Retrieval" similarly finds transformer-based models require task-specific adaptation for morphologically rich languages.
- Break condition: If the target domain differs radically from fine-tuning data (e.g., legal documents when fine-tuned on news), gains may not transfer.

### Mechanism 2
- Claim: Language-specific MLM pre-training provides necessary but insufficient representations for retrieval.
- Mechanism: Amharic pre-training encodes morphological patterns and vocabulary, enabling the model to generate meaningful token embeddings. However, without retrieval fine-tuning, these embeddings lack query-document matching alignment, resulting in near-random ranking.
- Core assumption: The MLM training corpus shares vocabulary and domain with the retrieval collection.
- Evidence anchors:
  - [abstract]: "BERT-based models pre-trained on Amharic data" were necessary components
  - [section 3]: English-based BERT models achieved 0.000-0.061 NDCG@10; Amharic pre-trained models still achieved only 0.000-0.005 without fine-tuning
  - [section 3]: Authors note potential "collection contamination" where "MLM has been trained on the same documents than the collection used to evaluate IR"
- Break condition: If pre-training corpus is too small or domain-mismatched, fine-tuning gains will be limited regardless of examples.

### Mechanism 3
- Claim: Fine-tuning quality outweighs architectural sophistication for low-resource dense retrieval.
- Mechanism: The best-performing model (roberta-amharic-text-embedding-base) was already fine-tuned as a sentence transformer before retrieval training. This prior fine-tuning provided better initialization than architecture-specific features like ColBERT's late interaction.
- Core assumption: Existing sentence embedding fine-tuning transfers to retrieval tasks.
- Evidence anchors:
  - [section 3]: "the best performing model using ColBERT is a sentence transformer RoBERTa model, which produces a single sentence-level representation and does not benefit from ColBERT late interaction system"
  - [section 3]: "performance advantage is not coming from the architecture but rather the additional finetuning that has been done"
  - [corpus]: Weak direct corpus evidence on this specific architectural comparison for low-resource languages.
- Break condition: For longer documents where token-level matching matters more, late interaction architectures may recover advantage.

## Foundational Learning

- Concept: **Late Interaction (ColBERT)** — query and document tokens are embedded separately; similarity computed via max-similarity per query token rather than single vector dot product.
  - Why needed here: Explains why ColBERT could work despite the best model being a sentence transformer; understanding what the architecture provides helps diagnose when it's unnecessary.
  - Quick check question: If a model produces single sentence embeddings, what component of ColBERT's design becomes unused?

- Concept: **NDCG@10 (Normalized Discounted Cumulative Gain)** — retrieval metric accounting for graded relevance and position; higher positions weighted more heavily; normalized against ideal ranking.
  - Why needed here: Primary evaluation metric in the paper; understanding what it measures (ranking quality with position sensitivity) is essential for interpreting the 0.704 vs. 0.177 results.
  - Quick check question: Why might a system achieve 0.704 on one dataset but 0.177 on another with the same model?

- Concept: **MLM (Masked Language Modeling) Pre-training** — self-supervised training where random tokens are masked and predicted; learns contextual representations without labeled data.
  - Why needed here: All BERT-based models in this paper started from MLM pre-training; understanding what it provides (and doesn't) explains the fine-tuning requirement.
  - Quick check question: What retrieval-specific knowledge does MLM pre-training NOT provide?

## Architecture Onboarding

- Component map:
  Raw Amharic Text → Tokenizer (SentencePiece/BPE) → Pre-trained BERT/RoBERTa → [Optional: Sentence Embedding Fine-tune] → Retrieval Fine-tune (ColBERT) → Index/Retrieve
  Baseline comparison path: Raw Text → BM25 (Indri) → Index/Retrieve

- Critical path: 
  1. Obtain or train Amharic MLM (290M tokens used in this work)
  2. Create/query relevance pairs (152 minimum per this paper)
  3. Fine-tune dense retriever on pairs
  4. Index document collection
  5. Evaluate against BM25 baseline

- Design tradeoffs:
  - **ColBERT vs. BM25**: 29-46x longer indexing time for dense approach; retrieval times comparable (0.78-1.17x)
  - **Data creation vs. transfer learning**: Manual relevance annotation (152 examples) vs. relying on multilingual models
  - **Architecture complexity vs. fine-tuning investment**: Simple sentence embeddings with more fine-tuning may outperform complex late interaction with less fine-tuning
  - **SPLADE failed entirely** (null results) — sparse expansion may not suit Amharic morphology

- Failure signatures:
  - NDCG@10 near 0.000-0.005: Using pre-trained model without retrieval fine-tuning
  - Null results with SPLADE: Sparse lexical expansion ineffective for morphologically complex languages
  - Large performance gap between datasets (0.704 vs. 0.177): Domain mismatch between training and evaluation; potential contamination if MLM and test data overlap

- First 3 experiments:
  1. **Establish BM25 baseline**: Run Indri with Amharic stopword list on target collection; record NDCG@10, indexing time, retrieval time.
  2. **Zero-shot dense retrieval test**: Load pre-trained Amharic BERT into ColBERT without fine-tuning; confirm near-zero performance to validate fine-tuning necessity.
  3. **Minimal fine-tuning run**: Create 150 query-document relevance pairs from target domain; fine-tune ColBERT; compare NDCG@10 against BM25 baseline to determine if dense retrieval is viable for the use case.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can sparse neural models like SPLADE be adapted for Amharic, given their complete failure in this study?
- Basis in paper: [explicit] The authors report they "could not achieve non-null results" using DistilSPLADE v2 with Amharic BERT variants.
- Why unresolved: The paper documents the failure but provides no error analysis or diagnosis for why sparse expansion failed.
- What evidence would resolve it: A study analyzing token expansion behavior in SPLADE for Amharic or the application of Amharic-specific sparse variants.

### Open Question 2
- Question: To what extent does pre-training data contamination inflate dense retrieval metrics in low-resource settings?
- Basis in paper: [inferred] The authors note that MLMs are likely trained on the same documents used for evaluation, constituting "collection contamination."
- Why unresolved: The reliance on third-party pre-trained models makes data provenance difficult to verify, casting doubt on the generalizability of the high NDCG scores.
- What evidence would resolve it: Experiments comparing models pre-trained on strictly disjoint corpora against current benchmarks.

### Open Question 3
- Question: Does ColBERT's late interaction mechanism offer advantages over simpler bi-encoders for Amharic?
- Basis in paper: [inferred] The best-performing model was a sentence transformer that "does not benefit from ColBERT late interaction," suggesting the architecture may be redundant.
- Why unresolved: It remains unclear if the success is due to the dense retrieval mechanism or simply the quality of the underlying embedding model.
- What evidence would resolve it: An ablation study comparing late interaction scores against single-vector similarity using identical backbones.

## Limitations

- The "Train Amharic" dataset with 152 relevance-labeled examples is not publicly available, making exact reproduction difficult
- SPLADE models failed entirely to produce any meaningful results for Amharic, with no explanation for this complete failure
- Fine-tuning hyperparameters (learning rate, batch size, epochs) were not specified, affecting reproducibility
- Potential "collection contamination" exists where MLM pre-training may overlap with evaluation datasets, potentially inflating performance metrics

## Confidence

**High Confidence:** The finding that pre-trained Amharic models require fine-tuning for effective dense retrieval is well-supported, with clear evidence showing near-zero performance (0.000-0.005 NDCG@10) before fine-tuning and substantial gains (0.704 NDCG@10) after training on just 150 examples. The comparison showing sentence transformers with additional fine-tuning can outperform complex architectures like ColBERT is also robust, directly demonstrated in the results section.

**Medium Confidence:** The claim that task-specific fine-tuning matters more than architectural sophistication has good supporting evidence but relies on limited direct comparisons. The observation about SPLADE's complete failure for Amharic morphology is documented but lacks explanation for why sparse methods failed entirely.

**Low Confidence:** The assertion that MLM pre-training provides "necessary but insufficient" representations is partially supported but could be stronger with ablation studies comparing to multilingual models without Amharic-specific pre-training.

## Next Checks

1. **Hyperparameter sensitivity analysis:** Systematically vary learning rate (1e-5, 2e-5, 5e-5), batch size (8, 16, 32), and training epochs (1, 3, 5) on the 152 training examples to determine which settings produce the 0.704 NDCG@10 result and whether this performance is robust to hyperparameter choices.

2. **Data efficiency evaluation:** Test the minimum number of training examples needed for effective retrieval by training on subsets of 50, 75, 100, and 150 examples to quantify the relationship between training data quantity and retrieval performance, addressing whether 150 is truly minimal or if fewer examples would suffice.

3. **Cross-domain transferability assessment:** Evaluate whether the fine-tuned model trained on the 152 examples transfers to a different Amharic document collection (e.g., using a portion of AfriCLIRMatrix for fine-tuning and testing on 2AIRTC) to determine if the gains are domain-specific or generalize across different Amharic text types.