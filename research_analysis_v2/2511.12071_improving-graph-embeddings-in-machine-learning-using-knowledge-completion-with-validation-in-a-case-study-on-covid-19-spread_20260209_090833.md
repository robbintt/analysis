---
ver: rpa2
title: Improving Graph Embeddings in Machine Learning Using Knowledge Completion with
  Validation in a Case Study on COVID-19 Spread
arxiv_id: '2511.12071'
source_url: https://arxiv.org/abs/2511.12071
tags:
- graph
- knowledge
- transitive
- embedding
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the limitation of traditional Graph Machine
  Learning (GML) pipelines, which often miss implicit knowledge hidden in sparse Knowledge
  Graphs (KGs), leading to suboptimal graph embeddings. To overcome this, the authors
  propose integrating a Knowledge Completion (KC) phase before the embedding step,
  using scalable transitive relationship inference with decay-based functions to uncover
  latent connections.
---

# Improving Graph Embeddings in Machine Learning Using Knowledge Completion with Validation in a Case Study on COVID-19 Spread

## Quick Facts
- **arXiv ID:** 2511.12071
- **Source URL:** https://arxiv.org/abs/2511.12071
- **Reference count:** 24
- **Primary result:** Integrating a Knowledge Completion (KC) phase before embedding generation significantly improves graph representation quality, demonstrated by large shifts in embedding space geometry and changes in PageRank rankings.

## Executive Summary
This study addresses the limitation of traditional Graph Machine Learning (GML) pipelines, which often miss implicit knowledge hidden in sparse Knowledge Graphs (KGs), leading to suboptimal graph embeddings. To overcome this, the authors propose integrating a Knowledge Completion (KC) phase before the embedding step, using scalable transitive relationship inference with decay-based functions to uncover latent connections. This approach reshapes graph topology, enhancing the semantic richness of the KG. Experiments on a real-world COVID-19 face-to-face contact network demonstrate that the KC-enhanced pipeline significantly alters embedding space geometry: GraphSAGE embeddings shifted by an average Euclidean distance of 0.041, and Node2Vec by 0.83. The KC phase also changed the top PageRank nodes, indicating a redefinition of influential elements. Overall, the results show that KC before embedding is a transformative step that improves graph representation quality, enabling more accurate and meaningful embeddings in both GraphSAGE and Node2Vec models.

## Method Summary
The proposed method integrates a Knowledge Completion (KC) phase before standard graph embedding generation. The KC step uses transitive closure via modified BFS to infer indirect contacts and applies a decay-based function to compute the strength of these inferred connections, adding them to the graph with associated weights. This completed graph is then used as input to embedding algorithms (GraphSAGE and Node2Vec). The COVID-19 face-to-face contact network from a French office building was used, with node attributes including contagion probability, degree centrality, and contact times. GraphSAGE was configured with 25 and 10 neighbors for two layers, mean aggregator, and 16-dim embeddings; Node2Vec used 10 walks, length 80, window 10, p=q=1, and 16-dim embeddings. Validation included Euclidean distance between embeddings, PageRank ranking changes, and 2D PCA visualizations.

## Key Results
- GraphSAGE embeddings shifted by an average Euclidean distance of 0.041 after KC phase.
- Node2Vec embeddings shifted by an average Euclidean distance of 0.83 after KC phase.
- KC phase changed the top PageRank nodes, indicating a redefinition of influential elements in the graph.
- Embeddings with KC showed less highly isolated nodes and increased variance, suggesting enhanced semantic richness.

## Why This Works (Mechanism)

### Mechanism 1: Transitive Topology Expansion via Decay Functions
The KC phase expands the graph adjacency space by inferring latent edges through transitive paths (e.g., $A \to B \to C$ implies $A \to C$), with a strength computed by a decay function (e.g., $e^{-\beta t}$) to ensure only relevant implicit connections are solidified. This increases the semantic richness of the KG. The core assumption is that the dataset contains meaningful implicit transitive relationships. A break condition is if the domain logic is non-transitive or if the decay threshold is misconfigured, leading to noisy or disconnected graphs.

### Mechanism 2: Probabilistic Re-weighting of Random Walks (Node2Vec)
Inferred edges alter the traversal probabilities in random-walk-based embeddings, shifting nodes closer in the vector space if they share implicit connections. The KC phase increases the probability that a node is visited during a walk and modifies the transition probability using the transitive strength coefficient, biasing the walk toward high-strength inferred connections. The core assumption is that increased connectivity accurately reflects semantic similarity. A break condition is if inferred edges create shortcuts across distinct clusters, leading to over-smoothed embeddings.

### Mechanism 3: Weighted Neighborhood Aggregation (GraphSAGE)
Structural augmentation increases the receptive field of node aggregation, allowing features to propagate through previously disconnected paths. The aggregation influence of a node increases as its k-hop neighborhood expands, and the update rule incorporates the strength as a multiplicative weight, allowing the GNN to prioritize information flowing through high-confidence inferred edges. The core assumption is that the weighting function correctly down-weights weak transitive paths. A break condition is if the aggregation depth is too low, failing to reach the newly bridged nodes.

## Foundational Learning

- **Concept: Transitive Closure in Graphs**
  - **Why needed here:** This is the mathematical basis of the "Knowledge Completion" step. You must understand that if $A \to B$ and $B \to C$, the system creates $A \to C$ to close the triangle, but assigns it a weight based on distance.
  - **Quick check question:** Given a path $A \to B \to C \to D$ with decay function $f(h) = 0.5^h$, what is the strength of the inferred edge $A \to D$?

- **Concept: Random Walk Biases ($p$ and $q$ in Node2Vec)**
  - **Why needed here:** The paper claims the KC phase changes how nodes are traversed. Understanding how the return parameter ($p$) and in-out parameter ($q$) control depth-first vs. breadth-first search is required to diagnose why the embedding space shifted.
  - **Quick check question:** If we add many transitive edges (increasing connectivity), would we expect random walks to localize more (BFS-like) or explore further (DFS-like), assuming default parameters?

- **Concept: Inductive vs. Transductive Learning**
  - **Why needed here:** GraphSAGE is an inductive framework (learns aggregation functions), whereas standard Node2Vec is often transductive. The KC phase changes the input structure for both, but the generalization mechanism for GraphSAGE relies on sampling this new topology.
  - **Quick check question:** Does adding edges via KC require retraining the embedding model, or can the weights be transferred? (Hint: Check if the input dimension or adjacency structure changes).

## Architecture Onboarding

- **Component map:** Input (Sparse KG) -> KC Engine (BFS + Transitive Inference + Strength Calculation) -> Completed Graph (KG_KC) -> Embedding Layer (GraphSAGE or Node2Vec) -> Validation (Euclidean Distance & PageRank Shift)
- **Critical path:** The **Strength Function (Definition 8 & Eq 1)**. This dictates which implicit edges are kept ($S > \tau$) and their weight. The specific decay function used for the COVID-19 case ($P_B = P_A - e^{-\beta t}$) is domain-specific; swapping domains requires defining a new decay function here.
- **Design tradeoffs:**
  - **Dense vs. Sparse:** Lowering the threshold $\tau$ creates a denser graph (better connectivity, higher risk of noise/smoothing). Raising it keeps the graph sparse (high precision, potentially missing latent links).
  - **Algorithm Sensitivity:** Node2Vec showed a massive shift (0.83 distance) compared to GraphSAGE (0.041). This suggests the KC phase transforms random walk contexts drastically but only slightly modifies the local aggregation neighborhoods used by GraphSAGE.
- **Failure signatures:**
  - **Homogenization:** If the decay function is too weak (weights are too high), all nodes may collapse into a single cluster in the embedding space.
  - **PageRank Instability:** If top-k influential nodes change radically (as seen in Table I), verify this reflects reality. If the "super-spreader" nodes change simply because of math artifacts, the KC logic is flawed.
  - **Performance Bottleneck:** Calculating transitive closure on large graphs is expensive ($O(V+E)$ per source or worse); watch for timeouts in the KC pre-processing step.
- **First 3 experiments:**
  1. **Sanity Check (Topology):** Run PageRank on $KG_{raw}$ vs $KG_{KC}$. Verify that the "Top 10" nodes change as expected (Table I). If they don't, the KC inference logic is not impacting the graph structure.
  2. **Geometric Shift (Node2Vec):** Generate embeddings for both graphs using fixed parameters ($p=1, q=1$). Compute the average Euclidean distance. Confirm the shift is >0.5 (high impact) as reported.
  3. **Downstream Task (Classification):** The paper leaves this as "Future Work," but a valid next step is to train a simple classifier (e.g., Logistic Regression) on the embeddings to predict a node label (e.g., "Infected" vs "Healthy") to see if the geometric shift translates to accuracy gains.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the introduction of the Knowledge Completion (KC) phase improve performance in downstream supervised tasks, such as classification and regression, across different GNN architectures? [explicit] The authors state they plan to study how KC-enriched embeddings impact downstream supervised tasks and assess whether the additional semantic structure translates into better generalization capabilities and higher accuracy. This is unresolved because the current study only measures shifts in embedding space geometry and PageRank, not predictive accuracy in end-to-end machine learning tasks. Comparative benchmarks showing F1-scores or accuracy metrics for node classification tasks would resolve it.

- **Open Question 2:** Is the proposed KC methodology scalable and performant when applied to large, complex graph datasets? [explicit] The authors note the need to conduct an in-depth analysis on the scalability of the KC methodology and explore performance optimizations for large and complex graph datasets. This is unresolved because the experiments were conducted on a specific, relatively small real-world dataset, and the transitive closure algorithms may face computational bottlenecks on massive graphs. Runtime and memory consumption analysis on standard large-scale benchmark graphs would resolve it.

- **Open Question 3:** To what extent are the embedding improvements dependent on the specific COVID-19 probabilistic model used for the decay-based inference function? [inferred] While the authors propose a general framework, the experimental validation relies on a specific infection probability formula derived from epidemiological literature. It is unclear if the "transformative" results hold when applying generic decay functions or to domains lacking a clear physical transmission model. Ablation studies applying the pipeline to non-biological datasets using generic decay functions would resolve it.

## Limitations

- The exact COVID-19 contact dataset used is not publicly available, preventing full independent verification of the results.
- The paper does not specify the initial feature vectors for GraphSAGE or the training epochs for either embedding method, which could influence the results.
- The practical significance of the embedding shifts is unclear without validation on a downstream task, which is left for future work.

## Confidence

- **High Confidence:** The mechanism of using transitive inference with decay-based weighting to augment graph topology is sound and well-explained. The PageRank changes observed are a direct consequence of the altered graph structure.
- **Medium Confidence:** The reported Euclidean distance shifts (0.041 for GraphSAGE, 0.83 for Node2Vec) are significant and align with the mechanism, but without access to the dataset and code, exact replication is impossible.
- **Low Confidence:** The practical significance of the embedding shifts is unclear without validation on a downstream task, which is left for future work.

## Next Checks

1. **Dataset Acquisition:** Obtain the French office COVID-19 contact network dataset (referenced as Genois et al. 2015) and parse it into the specified schema in Neo4j.
2. **Replication of Embedding Shifts:** Re-run the KC phase with the specified decay function (β=0.01, τ=0.2) and generate embeddings with the stated hyperparameters, verifying the reported Euclidean distance shifts.
3. **Downstream Task Evaluation:** Train a simple classifier (e.g., logistic regression) on the KC-enhanced embeddings to predict a node label (e.g., "likely infected") and compare performance against embeddings from the original graph to assess practical utility.