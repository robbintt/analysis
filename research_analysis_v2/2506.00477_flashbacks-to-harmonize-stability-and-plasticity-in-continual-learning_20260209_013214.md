---
ver: rpa2
title: Flashbacks to Harmonize Stability and Plasticity in Continual Learning
arxiv_id: '2506.00477'
source_url: https://arxiv.org/abs/2506.00477
tags:
- knowledge
- methods
- task
- learning
- phase
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Flashback Learning (FL), a novel method designed
  to harmonize stability and plasticity in Continual Learning (CL). Unlike prior approaches
  that primarily focus on regularizing model updates to preserve old information while
  learning new concepts, FL explicitly balances this trade-off through a bidirectional
  form of regularization.
---

# Flashbacks to Harmonize Stability and Plasticity in Continual Learning

## Quick Facts
- **arXiv ID:** 2506.00477
- **Source URL:** https://arxiv.org/abs/2506.00477
- **Reference count:** 40
- **Primary result:** Flashback Learning (FL) improves average accuracy by up to 4.91% in Class-Incremental and 3.51% in Task-Incremental settings while enhancing stability-plasticity balance

## Executive Summary
This paper introduces Flashback Learning (FL), a novel method designed to harmonize stability and plasticity in Continual Learning (CL). Unlike prior approaches that primarily focus on regularizing model updates to preserve old information while learning new concepts, FL explicitly balances this trade-off through a bidirectional form of regularization. This approach effectively guides the model to swiftly incorporate new knowledge while actively retaining its old knowledge. FL operates through a two-phase training process and can be seamlessly integrated into various CL methods, including replay, parameter regularization, distillation, and dynamic architecture techniques.

## Method Summary
Flashback Learning operates through a two-phase training process that explicitly balances stability and plasticity. In Phase 1, the model trains on new task data for a limited number of epochs (E1) to capture task-specific knowledge, creating a "plastic knowledge base" (PKB). In Phase 2, the model re-initializes to the previous task's state and trains with both the stable knowledge base (SKB) from the old model and the PKB from Phase 1, using bidirectional regularization. This approach preserves the same total training budget as the baseline method while improving the stability-plasticity balance. The PKB is constructed to match the SKB format for each CL category, enabling effective bidirectional transfer.

## Key Results
- FL demonstrates average accuracy improvements of up to 4.91% in Class-Incremental and 3.51% in Task-Incremental settings on standard image classification benchmarks
- FL reduces the Stability-Plasticity Ratio (SPR) across all tested methods, confirming improved balance
- FL outperforms state-of-the-art CL methods on challenging datasets like ImageNet
- The method successfully integrates with diverse CL categories including replay (iCaRL, LUCIR), regularization (oEWC), distillation (LwF), and dynamic architecture (X-DER) methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Bidirectional regularization (from both stable and plastic knowledge bases) improves the stability-plasticity balance compared to unidirectional stability-focused regularization.
- **Mechanism:** During Phase 2 training, the gradient decomposes into an interpolation between stable model outputs ($f(x; \theta_s)$) and primary model outputs ($f(x; \theta_p)$). The update direction becomes $\nabla_\theta L_{FL}(\theta) = \nabla_\theta L_c(\theta) + (\alpha_s + \alpha_p)\nabla_\theta f(x; \theta)^\top \left(f(x; \theta) - \frac{\alpha_s f(x; \theta_s) + \alpha_p f(x; \theta_p)}{\alpha_s + \alpha_p}\right)$, which explicitly balances old and new knowledge rather than anchoring primarily to the old model.
- **Core assumption:** The primary model from Phase 1 has captured meaningful task-specific knowledge that can serve as a valid reference for plasticity regularization.
- **Evidence anchors:**
  - [abstract] "FL explicitly balances this trade-off through a bidirectional form of regularization"
  - [section 4.1] Theorem 1 shows gradient interpolation term drives output toward balance between $f(x; \theta_s)$ and $f(x; \theta_p)$
  - [corpus] Limited direct corpus validation for bidirectional CL regularization specifically

### Mechanism 2
- **Claim:** The two-phase training structure enables extraction of task-specific plastic knowledge before stability-constrained training, preventing the "stability dominance" problem common in CL.
- **Mechanism:** Phase 1 prioritizes rapid acquisition of new task information without stability constraints. This creates a "plastic snapshot" in PKB. Phase 2 then re-initializes to the old model and trains with both knowledge sources, allowing plasticity to counteract excessive stability regularization.
- **Core assumption:** The model can effectively re-learn new task knowledge in Phase 2 even after re-initialization, guided by PKB.
- **Evidence anchors:**
  - [section 3.3.1] "Phase 1 prioritizes the rapid acquisition of task $T_t$ information, deferring the incorporation of previously learned knowledge"
  - [section 3.3.3] Training budget preserved: $E_{FL} = E_1 + E_2 = E_{CL}$

### Mechanism 3
- **Claim:** Category-specific knowledge base construction ensures compatibility between SKB and PKB, enabling effective bidirectional transfer across diverse CL method types.
- **Mechanism:** For each CL category (distillation, replay, regularization, dynamic architecture), PKB is constructed to match SKB's representational format. Distillation uses model snapshots; replay uses logits/embeddings on memory samples; regularization uses parameters + Fisher matrices; architecture uses expanded modules.
- **Core assumption:** The representational format used by the host CL method for stability is also appropriate for encoding plasticity.
- **Evidence anchors:**
  - [section 3.1-3.2] Detailed PKB/SKB definitions per category (Eq. 1-13)
  - [section 5] "FL aligns the content of the plastic knowledge base with retained information from previous tasks"

## Foundational Learning

- **Concept: Stability-Plasticity Dilemma**
  - **Why needed here:** The entire FL method is designed to address the trade-off between retaining old knowledge (stability) and learning new tasks (plasticity).
  - **Quick check question:** Can you explain why traditional CL methods that prioritize stability (e.g., strong distillation from old model) may harm learning on new tasks?

- **Concept: Knowledge Distillation for CL**
  - **Why needed here:** FL builds on distillation-based knowledge transfer but makes it bidirectional.
  - **Quick check question:** How does distillation from an old model differ from simple fine-tuning, and what information does it preserve?

- **Concept: Fisher Information Matrix for Parameter Importance**
  - **Why needed here:** For parameter regularization methods (EWC family), FL constructs PKB using Fisher matrices.
  - **Quick check question:** What does the Fisher Information Matrix represent in the context of EWC, and why does FL need to compute it for both old ($F_s$) and new ($F_p$) tasks?

## Architecture Onboarding

- **Component map:**
  - Host CL Method -> Phase 1 Trainer (E1 epochs) -> Primary Model (PKB source) -> Phase 2 Trainer (E2 epochs) -> Final Model with bidirectional regularization

- **Critical path:**
  1. Identify host CL method category â†’ determine SKB/PKB format
  2. Implement Phase 1 training loop with early stopping ($E_1$ epochs)
  3. Extract PKB from $\theta_p$ (match SKB structure)
  4. Re-initialize model to $\theta_s$
  5. Implement Phase 2 with both $L_s$ and $L_p$ loss terms
  6. Tune $\alpha_p$ (optimal range 0.001-0.01)

- **Design tradeoffs:**
  - Memory: $M_{FL} \approx 2 \times M_{CL}$ (doubles memory for PKB)
  - Training time: Same total epochs as baseline, but added complexity of two phases
  - $E_1$ vs $E_2$ split: Table 6 shows $E_1=10, E_2=50$ outperforms balanced or reverse splits
  - $\alpha_p$ tuning: Method-specific; iCaRL optimal at 0.001, LUCIR at 0.01

- **Failure signatures:**
  - Negative accuracy change (e.g., LUCIR+FL shows -0.05% on Split CIFAR-10 TI)
  - High SPR indicates imbalance
  - Forgetting increase despite Flashback

- **First 3 experiments:**
  1. Integrate FL with a replay method (e.g., iCaRL or X-DER) on Split CIFAR-100, reporting Average Accuracy and Forgetting. Use $E_1=10$, $E_2=E_{CL}-10$, $\alpha_p=0.001$ as starting point.
  2. On same setup, vary $\alpha_p \in \{0.0001, 0.001, 0.01, 0.1\}$ and $E_1 \in \{5, 10, 20\}$ to find optimal configuration. Monitor SPR to assess stability-plasticity balance.
  3. Implement FL for at least one method from a different category (e.g., LwF for distillation or oEWC for regularization) to verify cross-category compatibility.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can Flashback Learning be effectively integrated with parameter-efficient techniques like prompt learning or Low-rank adaptation (LoRA) for large pre-trained models?
  - **Basis in paper:** Page 28 states future research will explore integrating FL with emerging techniques such as prompt learning and LoRA.
  - **Why unresolved:** The paper currently validates FL on ResNet-18 architectures trained from scratch, rather than on frozen backbones utilizing parameter-efficient tuning.
  - **What evidence would resolve it:** Empirical evaluation of FL on Transformer-based models using prompts or adapters within a continual learning benchmark.

- **Open Question 2:** How can the two-phase FL mechanism be adapted for online continual learning scenarios where data is strictly single-pass?
  - **Basis in paper:** Page 28 lists online learning and task-agnostic scenarios as specific future directions for extending the FL mechanism.
  - **Why unresolved:** The current FL formulation requires a distinct Phase 1 with multiple epochs to compile "plastic knowledge," which violates the single-pass constraint of online learning.
  - **What evidence would resolve it:** A modified FL variant capable of operating in a streaming fashion, benchmarked against online CL baselines.

- **Open Question 3:** Can the memory overhead of maintaining dual knowledge bases be reduced without compromising the stability-plasticity balance?
  - **Basis in paper:** Page 12 acknowledges that FL "approximately doubles the memory requirement" ($M_{FL} = 2M_{CL}$) compared to the host method.
  - **Why unresolved:** The requirement to store both a Stable Knowledge Base (SKB) and a Plastic Knowledge Base (PKB) may be prohibitive for memory-constrained environments.
  - **What evidence would resolve it:** Ablation studies demonstrating the performance impact of compressing the PKB or sharing structural components between the two bases.

## Limitations

- FL requires tuning method-specific hyperparameters ($\alpha_p$, $E_1/E_2$ split), which may not generalize across all CL scenarios
- The theoretical analysis doesn't fully explain why bidirectional regularization outperforms unidirectional approaches in all scenarios
- Memory overhead approximately doubles compared to baseline methods due to dual knowledge bases

## Confidence

- **High Confidence:** The bidirectional regularization mechanism (Mechanism 1) and its mathematical formulation are well-established and empirically validated
- **Medium Confidence:** The two-phase training structure (Mechanism 2) shows consistent improvements but depends on proper hyperparameter tuning
- **Medium Confidence:** Category-specific knowledge base construction (Mechanism 3) is validated across CL categories but may require adaptation for hybrid or novel methods

## Next Checks

1. **Generalization to More Complex Datasets:** Test FL on datasets beyond image classification (e.g., NLP or reinforcement learning tasks) to assess broader applicability
2. **Efficiency Analysis:** Evaluate the computational overhead of FL in resource-constrained environments to ensure practical viability
3. **Robustness to Noisy Data:** Investigate how FL performs when memory buffer or plastic knowledge base contains noisy or corrupted samples