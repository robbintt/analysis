---
ver: rpa2
title: Towards Safe Robot Foundation Models
arxiv_id: '2503.07404'
source_url: https://arxiv.org/abs/2503.07404
tags:
- safety
- safe
- control
- learning
- robot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a safety layer for generalist robot policies
  that ensures safe actions during deployment by leveraging system dynamics and domain-specific
  constraints. The approach uses ATACOM, a safe reinforcement learning algorithm,
  to create a safe action space by mapping potentially unsafe actions from a pre-trained
  Robot Foundation Model (RFM) onto the tangent space of a constraint manifold.
---

# Towards Safe Robot Foundation Models

## Quick Facts
- arXiv ID: 2503.07404
- Source URL: https://arxiv.org/abs/2503.07404
- Reference count: 35
- This paper introduces a safety layer for generalist robot policies that ensures safe actions during deployment by leveraging system dynamics and domain-specific constraints.

## Executive Summary
This paper presents a safety layer that guarantees safe state transitions for pre-trained Robot Foundation Models during deployment. The approach uses ATACOM, a safe reinforcement learning algorithm, to project potentially unsafe actions from the RFM onto the tangent space of a constraint manifold. This enables safe deployment without requiring additional safety fine-tuning of the base policy. The method is evaluated on an air hockey task using a fine-tuned OCTO policy, successfully preventing constraint violations while maintaining or improving success rates.

## Method Summary
The method integrates an ATACOM safety layer on top of a pre-trained OCTO Robot Foundation Model. The RFM outputs end-effector velocities, which are converted to joint velocities via Inverse Kinematics. The ATACOM layer then maps these potentially unsafe joint velocities onto the tangent space of a constraint manifold defined by system dynamics and differentiable safety constraints. This projection ensures the resulting actions respect safety boundaries while attempting to fulfill the original task intent.

## Key Results
- Successfully prevents constraint violations (e.g., collisions with table) while maintaining or improving success rates
- Enables safe deployment of generalist policies in safety-critical scenarios without requiring additional safety fine-tuning
- Demonstrates that the safety layer can guarantee safe state transitions even when fine-tuning data doesn't explicitly incorporate safety constraints

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Mapping raw policy outputs to the tangent space of a constraint manifold prevents constraint violations without altering the base policy.
- **Mechanism:** The ATACOM layer acts as a projection function that takes unsafe actions and projects them into the tangent space of the safe set, ensuring the resulting velocity vector respects system dynamics and moves parallel to (or away from) the constraint boundary.
- **Core assumption:** The system dynamics are control affine and constraints are continuously differentiable.
- **Evidence anchors:** Abstract states "mapping potentially unsafe actions... onto the tangent space of a constraint manifold" and section II describes constructing a constraint manifold of safe configurations.
- **Break condition:** If constraints are non-differentiable or dynamics cannot be modeled as control affine.

### Mechanism 2
- **Claim:** Decoupling safety from the foundation model's training objective allows for retroactive safety guarantees on pre-trained generalist policies.
- **Mechanism:** The system treats the RFM as a black box generator of desired velocities, with the safety layer intercepting commands before they reach the controller, ensuring the RFM doesn't need to learn physical constraints from data.
- **Core assumption:** The safety layer can compute projections fast enough to not introduce significant latency that destabilizes the feedback loop.
- **Evidence anchors:** Abstract notes "ensures safe state transitions without requiring additional safety fine-tuning."
- **Break condition:** If base policy produces actions so aggressive that projection layer creates unstable oscillations near constraint boundary.

### Mechanism 3
- **Claim:** Exploiting the robot's null space allows the system to satisfy safety constraints while minimizing interference with the primary task objective.
- **Mechanism:** By formulating safety as a constraint manifold, the robot can utilize redundant degrees of freedom to satisfy safety while still attempting to fulfill the high-level intent of the RFM.
- **Core assumption:** The robot has sufficient kinematic redundancy or constraints are loose enough to allow valid solutions approximating original desired action.
- **Evidence anchors:** Section I mentions "utilizing system dynamics to control actions within the robot's null space."
- **Break condition:** If robot is in singularity or forced into corner where no valid action satisfies both safety constraint and task direction.

## Foundational Learning

**Concept: Control Affine Systems**
- **Why needed here:** ATACOM algorithm fundamentally requires robot dynamics to be expressed as $\dot{s} = f(s) + G(s)a$ to construct the constraint manifold.
- **Quick check question:** Can you write the state-space equation for your robot arm where the control input appears linearly?

**Concept: Tangent Space & Manifolds**
- **Why needed here:** Safety is defined as remaining on a "manifold" of valid states, and motion must occur along the "tangent" of this manifold.
- **Quick check question:** If a constraint is $z > 0$, what is the tangent vector at $z=0$ that violates safety vs. the one that preserves it?

**Concept: Distribution Shift in Imitation Learning**
- **Why needed here:** The paper addresses failure of RFMs trained via Behavior Cloning, where policies can drift into unsafe states not covered by expert data.
- **Quick check question:** Why does a policy trained to imitate safe experts eventually crash when deployed in a closed loop?

## Architecture Onboarding

**Component map:** Multimodal observations -> RFM (OCTO) -> End-effector Velocity -> Inverse Kinematics -> **ATACOM Projection** -> Robot Action

**Critical path:** Defining the constraints $g(x)$ in code to match physical safety requirements and providing the dynamics model $G(s)$.

**Design tradeoffs:**
- **Inductive Bias vs. Generality:** Gain formal safety guarantees but lose purely data-driven simplicity by re-introducing analytical dynamics models
- **Conservatism vs. Success Rate:** Method suggests layer doesn't hurt success rates, but highly constrained environments might see task completion drop if safe tangent space points away from goal

**Failure signatures:**
- **Deadlock:** Robot stops moving or vibrates because desired action points directly out of safe manifold and projection nullifies it
- **Constraint Jitter:** Rapid oscillation when policy tries to push against constraint boundary repeatedly
- **Dynamics Mismatch:** Safety failures occurring because analytical model $G(s)$ used in layer doesn't match real robot physics

**First 3 experiments:**
1. **Constraint Validation:** Drive robot toward table surface using manual velocity command; verify safety layer stops motion exactly at constraint boundary without penetration
2. **Policy Stress Test:** Run unmodified OCTO policy on task to observe specific failure modes (e.g., does it crash? where?)
3. **Integrated Evaluation:** Run OCTO policy with ATACOM layer enabled; compare "Max Violation" metric and "Success Rate" against baseline

## Open Questions the Paper Calls Out

**Open Question 1:** How can a generalizable concept of safety be formulated and applied across different embodiments, environments, and tasks without manual re-specification?
- **Basis:** Conclusion states "it remains an open research question of how a more generalizable concept of safety can be formulated and applied across different embodiments, environments, and tasks"
- **Why unresolved:** Current approach relies on practitioners manually defining constraints $g(x)$ for specific scenarios
- **What evidence would resolve it:** Framework that autonomously adapts safety definitions to new robot morphologies or environments without explicit human engineering

**Open Question 2:** Can Vision-Language Models (VLMs) be leveraged to automate the formulation of scene-specific geometric safety constraints?
- **Basis:** Conclusion suggests "One intuitive research direction is to automate the process by leveraging the inherent knowledge of vision-language models (VLMs)"
- **Why unresolved:** VLMs currently excel at semantic constraints but haven't been demonstrated to reliably generate differentiable geometric constraints required by ATACOM
- **What evidence would resolve it:** Successful integration of VLM that translates visual observations directly into valid constraint manifolds

**Open Question 3:** Does the reliance on explicit state access and control-affine dynamics limit applicability to purely vision-based Robot Foundation Models?
- **Basis:** Method lists "Access to the system's state $s$ and a control affine system" as Requirement 1, whereas many RFMs operate purely on pixels
- **Why unresolved:** Paper assumes rigid-body dynamics and state availability, leaving performance untested for RFMs that don't expose explicit state vectors
- **What evidence would resolve it:** Demonstrating safety layer's effectiveness on policy operating on high-dimensional images without ground-truth state vectors

**Open Question 4:** Does mapping actions to the tangent space of the constraint manifold result in overly conservative behavior in highly constrained environments?
- **Basis:** Paper notes method didn't generate "overly conservative control actions" for air hockey task, but this may not hold for tasks with narrower feasible corridors
- **Why unresolved:** Evaluation covers relatively open task; unclear if strictly adhering to tangent space restricts robot from completing complex manipulation tasks where safety manifold is complex
- **What evidence would resolve it:** Benchmarking success rates on high-precision tasks where safe action space is significantly smaller than raw action space

## Limitations

- The safety guarantee fundamentally depends on robot dynamics being control affine and constraints being continuously differentiable - conditions that may not hold for complex systems
- Potential for chattering near constraint boundaries when base policy repeatedly attempts unsafe actions
- Empirical validation limited to single task with specific constraints (table collisions and joint limits), representing low sample diversity

## Confidence

**Major limitations:** Medium confidence due to reliance on specific system assumptions
- The safety guarantee depends on control affine dynamics and continuously differentiable constraints
- Potential for chattering near boundaries and conservatism in highly constrained environments
- Assumes perfect knowledge of system dynamics model with any mismatch potentially causing failures

**Empirical validation:** Low sample diversity
- Promising safety improvements without sacrificing success rate
- Limited to single air hockey task with specific constraints
- Results show success but evaluation scope is narrow

## Next Checks

1. Test the safety layer under perturbed dynamics where $G(s)$ differs from the real robot model by introducing latency or unmodeled inertia
2. Evaluate performance in a multi-constraint environment with conflicting safety requirements (e.g., avoiding both table and robot base simultaneously)
3. Measure the trade-off between conservatism and task success across varying constraint strictness by adjusting slack parameters in $g(x)$