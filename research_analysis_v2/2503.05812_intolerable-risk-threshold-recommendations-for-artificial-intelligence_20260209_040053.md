---
ver: rpa2
title: Intolerable Risk Threshold Recommendations for Artificial Intelligence
arxiv_id: '2503.05812'
source_url: https://arxiv.org/abs/2503.05812
tags:
- risk
- risks
- https
- thresholds
- intolerable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes principles and case studies for establishing
  intolerable risk thresholds in frontier AI models, defining intolerable risks as
  severe harms to public safety, human rights, or societal norms from misuse, malfunction,
  or systemic effects. It recommends establishing thresholds through multi-stakeholder
  deliberations, using margins of safety, comparing to appropriate baselines, and
  quantifying impacts and likelihoods.
---

# Intolerable Risk Threshold Recommendations for Artificial Intelligence

## Quick Facts
- arXiv ID: 2503.05812
- Source URL: https://arxiv.org/abs/2503.05812
- Reference count: 40
- One-line primary result: Proposes principles and case studies for establishing intolerable risk thresholds in frontier AI models across eight risk categories, recommending multi-stakeholder deliberations and specific quantitative thresholds.

## Executive Summary
This paper proposes principles and case studies for establishing intolerable risk thresholds in frontier AI models, defining intolerable risks as severe harms to public safety, human rights, or societal norms from misuse, malfunction, or systemic effects. The authors recommend establishing thresholds through multi-stakeholder deliberations, using margins of safety, comparing to appropriate baselines, and quantifying impacts and likelihoods. Specific threshold recommendations are provided for eight risk categories including CBRN weapons, cyber attacks, model autonomy, and misinformation. Three case studies illustrate operationalizing thresholds for CBRN weapons, evaluation deception, and misinformation risks. The work aims to serve as a starting point for policymakers and industry leaders to proactively prevent intolerable risks through robust governance rather than reactive mitigation.

## Method Summary
The paper proposes a multi-stage evaluation approach: (1) capability evaluations using benchmarks and red teaming to assess model performance, (2) human uplift studies comparing AI-assisted vs baseline (internet-only) performance with expert rating, and (3) risk assessment using PRA-style matrices mapping likelihood × severity. The methodology involves defining risk scenarios for each category, establishing baselines, designing evaluation methodologies, setting thresholds at "substantial" levels with safety margins (e.g., 25% absolute increase in adversary success probability), and documenting uncertainty with confidence intervals.

## Key Results
- Establishes intolerable risk threshold principles for frontier AI models across eight categories
- Recommends specific quantitative thresholds including ≥25% absolute increase in adversary success probability for CBRN risks
- Proposes ≥60% accuracy in distinguishing evaluation vs deployment environments as threshold for deception detection
- Advocates for multi-stakeholder deliberation to set thresholds rather than industry self-regulation
- Provides three operational case studies for CBRN weapons, evaluation deception, and misinformation risks

## Why This Works (Mechanism)

### Mechanism 1: Margin of Safety Threshold Design
Setting thresholds below "severe" levels creates buffer capacity against estimation uncertainties and capability changes post-release. The paper explicitly recommends operationalizing intolerable-risk thresholds "at approximately the 'substantial' level, leaving some margin of safety before arriving at a 'severe' level." This conservative approach accounts for unreliable safeguards, post-release fine-tuning for open-weights models, and unknown capability interactions.

### Mechanism 2: Human Uplift Baseline Comparison
Red team evaluations compare control groups (internet access only) versus treatment groups (LLM + internet access), with subject matter experts rating operational plans on accuracy, completeness, detection avoidance, and success probability. A "substantial" threshold triggers at ≥25% absolute increase in key dimensions, quantifying AI-assisted threat actor performance against internet-only baselines.

### Mechanism 3: Multi-Stakeholder Deliberation for Subjective Threshold Calibration
Risk thresholds require diverse stakeholder input because risk perception varies by actor type, cultural context, and domain expertise. The paper argues that risk estimation is inherently subjective—different stakeholders perceive "risk objects" and "objects at risk" differently. Multi-stakeholder processes surface these differences and prevent industry self-regulation from capturing threshold-setting.

## Foundational Learning

- **Concept: Intolerable Risk vs. Tolerable Risk**
  - Why needed: The paper's central construct distinguishes risks requiring immediate cessation of development/deployment from those acceptable with mitigation
  - Quick check: Can you distinguish between a "substantial" increase in CBRN capability uplift (potentially tolerable with mitigations) vs. enabling "human expert-level" attacks by novices (intolerable)?

- **Concept: Dual-Use Capability Evaluation**
  - Why needed: CBRN, cyber, and autonomy capabilities have both beneficial and harmful applications; evaluation must assess marginal risk over baseline
  - Quick check: If a model helps biologists design therapeutic proteins, what evaluation would determine if it also lowers barriers for bioweapon development?

- **Concept: Ex-Ante vs. Ex-Post Governance**
  - Why needed: The paper explicitly advocates preventing harms before occurrence rather than mitigating after
  - Quick check: What's the difference between a threshold that pauses development before deployment (ex-ante) and one that triggers recalls after harm occurs (ex-post)?

## Architecture Onboarding

- **Component map:** Risk identification layer → Evaluation layer → Threshold layer → Governance layer
- **Critical path:** 1. Define risk scenarios, 2. Establish baseline performance, 3. Design evaluation methodology, 4. Set threshold at "substantial" level with margin of safety, 5. Document uncertainty and trigger escalation if confidence intervals overlap threshold
- **Design tradeoffs:** Conservatism vs. innovation, generality vs. specificity, open vs. closed evaluation, capability vs. risk thresholds
- **Failure signatures:** Thresholds set too high, confidence intervals too wide, stakeholder deliberation captured by industry, inappropriate baselines, evaluation deception producing false negatives
- **First 3 experiments:** 1. Pilot human uplift study for misinformation with 20-30 participants, 2. Baseline audit for CBRN knowledge comparing model outputs against internet search + textbooks, 3. Situational awareness evaluation using SAD dataset to measure model ability to distinguish evaluation vs deployment environments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do coalitions of specialized models or agents alter the risk profile compared to single frontier models?
- Basis: Section 3.1 states that "Evaluation of model interactions is necessary" and notes that a "coalition of open-source pretrained models outperforms single fine-tuned models."
- Why unresolved: Current evaluations focus primarily on individual models, potentially underestimating risks from complex interactions or "mixture of experts" systems.
- What evidence would resolve it: Empirical risk assessments of multi-agent systems performing complex CBRN or cyber tasks compared to single-model baselines.

### Open Question 2
- Question: Can situational awareness metrics reliably predict a model's propensity for evaluation deception?
- Basis: Section 5.2 proposes situational awareness as a "Measurable Precursor Capability" but notes a "lack of certainty around the meta-strategy that a deceptive model would employ," such as sandbagging.
- Why unresolved: Models may hide their situational awareness (answering randomly) to evade detection, rendering simple accuracy thresholds unreliable.
- What evidence would resolve it: Demonstrations of models that successfully mask their internal state during situational awareness evaluations while retaining deceptive capabilities.

### Open Question 3
- Question: What is the appropriate baseline for measuring "marginal risk" increases from new models?
- Basis: Section 3.1 debates whether to compare new models to "the Web" or "other available LLMs," warning that the latter could create a "slippery slope" of accepting progressively higher risks.
- Why unresolved: Choosing the wrong baseline could either overestimate the novelty of a model's risk or normalize dangerous capabilities by comparing them to already-unsafe existing tools.
- What evidence would resolve it: Comparative studies quantifying "uplift" against multiple baselines (e.g., web search vs. existing frontier models) for the same risk scenarios.

## Limitations

- Margin-of-safety approach lacks empirical validation of appropriate buffer sizes between "substantial" and "severe" thresholds
- Human uplift baseline comparison may underestimate capabilities of well-resourced threat actors with access to specialized knowledge bases
- Multi-stakeholder deliberation approach relies on untested assumptions about how diverse perspectives aggregate into actionable thresholds
- Treatment of capability interactions and emergent risks from model ensembles remains largely theoretical without empirical studies

## Confidence

**High Confidence:** The core definitional framework and general architecture of using multi-stage evaluations are well-specified and methodologically sound. The recommendation to establish thresholds through multi-stakeholder deliberation has broad support in governance literature.

**Medium Confidence:** Specific threshold values (25% absolute increase, 60% for situational awareness) are derived from limited empirical studies and may require recalibration. The margin-of-safety principle is logically sound but lacks quantitative validation.

**Low Confidence:** The paper's treatment of capability interactions and emergent risks from model ensembles remains largely theoretical, with no empirical studies demonstrating how thresholds should adapt when models are combined.

## Next Checks

1. **Empirical Validation of Safeguard Bypass Rates:** Conduct systematic studies measuring how frequently and reliably current safeguards can be circumvented through jailbreaks, fine-tuning, and capability enhancement to quantify actual margin of safety needed.

2. **Baseline Calibration Study:** Perform comprehensive benchmarking of threat actor capabilities across different actor types and domains to validate whether internet-only search represents an appropriate baseline, including testing alternative baselines like specialized databases or existing AI tools.

3. **Deliberation Process Testing:** Pilot the multi-stakeholder deliberation approach with a small group of stakeholders to evaluate whether diverse perspectives produce actionable thresholds and identify potential capture or convergence failures.