---
ver: rpa2
title: 'Beyond Recognition: Evaluating Visual Perspective Taking in Vision Language
  Models'
arxiv_id: '2505.03821'
source_url: https://arxiv.org/abs/2505.03821
tags:
- humanoid
- minifigure
- visual
- east
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work evaluates the ability of vision-language models to perform
  visual perspective taking using carefully controlled LEGO-based scenes featuring
  a humanoid minifigure and an object. The authors create 144 tasks with systematic
  spatial configurations and assess models on three question categories: scene understanding,
  spatial reasoning, and visual perspective taking.'
---

# Beyond Recognition: Evaluating Visual Perspective Taking in Vision Language Models

## Quick Facts
- arXiv ID: 2505.03821
- Source URL: https://arxiv.org/abs/2505.03821
- Reference count: 0
- Vision-language models struggle with visual perspective taking despite excelling at basic scene understanding

## Executive Summary
This work evaluates vision-language models' ability to perform visual perspective taking using controlled LEGO-based scenes with humanoid minifigures. The authors create 144 systematic tasks and assess models across three question categories: scene understanding, spatial reasoning, and visual perspective taking. While models achieve near-perfect performance on basic scene comprehension, their performance drops significantly for spatial reasoning and further deteriorates for perspective-taking tasks. GPT-4o achieves the highest accuracy at 87.5% for visual perspective questions, but all models struggle with understanding relative positions from the minifigure's viewpoint. The analysis reveals that models often rely on linguistic priors or memorized defaults rather than genuine geometric reasoning.

## Method Summary
The study evaluates five VLMs (GPT-4-Turbo, GPT-4o, Llama-3.2-11B-Vision-Instruct, Claude 3 Sonnet, Claude 3.5 Sonnet) on 144 controlled LEGO-based tasks featuring humanoid minifigures and objects. Models answer 7 diagnostic questions across three categories: scene understanding (Q1-Q3), spatial reasoning (Q4-Q5), and visual perspective taking (Q6-Q7). Questions use open-ended format with temperature=0 and max 128 tokens. Performance is measured using prediction correctness (|M∩G|/|M|) with 95% CIs from 10,000 bootstrap iterations.

## Key Results
- GPT-4o achieved perfect performance (100%) on scene understanding questions (Q1-Q3)
- Spatial reasoning performance dropped significantly, with GPT-4o at 72.9% and GPT-4-Turbo at 41.7% for orientation detection (Q5)
- Visual perspective taking accuracy was lowest, with GPT-4o achieving 87.5% on Q6 (object visibility) but all models struggling with Q7 (relative position from figure's view)
- Directional bias emerged across models, with GPT-4-Turbo predominantly predicting "east" regardless of actual orientation

## Why This Works (Mechanism)

### Mechanism 1
VLMs succeed at scene understanding through well-trained pattern recognition for object identification and counting, but this capability doesn't generalize to relational spatial reasoning. Object recognition operates on local visual features that map directly to learned semantic concepts, while counting and surface detection rely on feature extraction without requiring coordinate transformations.

### Mechanism 2
Spatial reasoning failures stem from difficulty with intrinsic reference frames. Models perform well on extrinsic reference frames (Q4: 98.6% for GPT-4o) but collapse on intrinsic reference frames requiring agent-relative orientation (Q5: 72.9% for GPT-4o, 41.7% for GPT-4-Turbo).

### Mechanism 3
Visual perspective taking fails because models rely on linguistic priors and statistical defaults rather than computing geometric transformations. Directional biases persist across interventions (zoom, object removal, cardinal labels, human faces, prompt permutations), and providing ground-truth orientation hints produces only marginal Q6 improvement.

## Foundational Learning

- **Visual Perspective Taking (VPT) Levels 1 and 2**: The paper evaluates Level 1 (what others see—Q6) and Level 2 (how objects appear from another's viewpoint—Q7). Understanding this distinction is essential for interpreting why models might pass Q6 but fail Q7. *Quick check*: Can you explain why determining whether a figure sees an object requires different computation than determining where the object appears from the figure's perspective?

- **Intrinsic vs. Extrinsic Reference Frames**: The core finding hinges on models succeeding with extrinsic (image-relative) but failing with intrinsic (agent-relative) spatial coordinates. This distinction explains the Q4/Q5 performance gap. *Quick check*: Given an image with a person facing northeast, would "the cup is to their left" use an intrinsic or extrinsic reference frame?

- **Theory of Mind in AI Systems**: VPT is a component of theory of mind. The paper situates its work within psychological literature on human perspective-taking, and corpus neighbors reference similar cognitive foundations. *Quick check*: Why would a robot performing human-robot interaction need to model another agent's visual perspective, not just the robot's own sensory input?

## Architecture Onboarding

- Component map: Input Image → Vision Encoder → [Spatial Feature Extraction] → Multimodal Fusion → LLM Reasoning → Text Output
  ↑
  FAILURE POINT: No explicit geometric representation layer

- Critical path: Scene understanding succeeds because vision encoder features map directly to object tokens. Perspective taking fails because no component performs coordinate transformation from camera frame to agent frame.

- Design tradeoffs:
  - End-to-end training vs. explicit geometric modules: Current models trade interpretability for seamless multimodal fusion
  - Open-ended vs. multiple-choice evaluation: Paper uses open-ended to avoid positional bias, but this increases evaluation complexity
  - Synthetic vs. natural images: LEGO scenes enable controlled experiments but may not transfer to real-world applications

- Failure signatures:
  - Directional bias: Systematic preference for specific outputs (GPT-4-Turbo: east/south dominance) regardless of input
  - Premise rejection: Claude 3 Sonnet refused to engage with VPT questions, claiming minifigures cannot see (45/144 instances)
  - Compound answer confusion: Models output "northeast" when only "north" is correct, indicating fuzzy spatial boundaries

- First 3 experiments:
  1. **Probe intrinsic reference frame capability directly**: Isolate Q5-type orientation detection by training a small classifier on figure orientation only.
  2. **Intervention test with explicit spatial embeddings**: Add a geometric transformation layer that computes agent-relative coordinates before LLM fusion.
  3. **Cross-dataset generalization check**: Test whether directional biases persist on non-LEGO images (real photos, rendered scenes).

## Open Questions the Paper Calls Out
None

## Limitations
- Cannot definitively distinguish between visual encoding failures versus reasoning failures as the cause of perspective-taking errors
- Synthetic LEGO dataset may not generalize to natural images, though directional biases persisted with human faces
- Directional bias findings suggest dataset-agnostic failure mode, but causal mechanisms remain ambiguous

## Confidence
- **High confidence**: Models excel at basic scene understanding but fail on spatial reasoning and visual perspective taking
- **Medium confidence**: Interpretation that failures stem from linguistic priors rather than visual encoding issues
- **Medium confidence**: The distinction between Level 1 and Level 2 VPT and their computational requirements

## Next Checks
1. **Isolate visual encoding quality**: Train a simple classifier to detect figure orientation from images alone to determine if the vision encoder can extract necessary posture information
2. **Test directional bias in natural images**: Evaluate the same VPT tasks on real photographs rather than LEGO scenes to assess whether the east/south bias is dataset-specific or represents a fundamental architectural limitation
3. **Implement explicit geometric transformation**: Add a coordinate transformation layer that computes agent-relative positions before LLM reasoning, then compare Q7 performance to establish whether the bottleneck is visual encoding or reasoning