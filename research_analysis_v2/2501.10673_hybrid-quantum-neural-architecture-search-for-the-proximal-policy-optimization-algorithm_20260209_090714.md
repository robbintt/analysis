---
ver: rpa2
title: Hybrid-Quantum Neural Architecture Search for The Proximal Policy Optimization
  Algorithm
arxiv_id: '2501.10673'
source_url: https://arxiv.org/abs/2501.10673
tags:
- quantum
- layer
- classical
- arxiv
- hybrid
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the gap in hybrid classical-quantum architecture
  design by automating the search for optimal neural architectures for the Proximal
  Policy Optimization (PPO) algorithm using Regularized Evolution. The study explored
  over 1000 architectures on the CartPole-v1 environment, revealing that purely classical
  models consistently outperformed hybrid models, with the best hybrid architecture
  ranking 11th among unique models.
---

# Hybrid-Quantum Neural Architecture Search for The Proximal Policy Optimization Algorithm

## Quick Facts
- **arXiv ID:** 2501.10673
- **Source URL:** https://arxiv.org/abs/2501.10673
- **Reference count:** 14
- **Primary result:** Automated NAS found classical architectures consistently outperformed hybrid models for PPO on CartPole-v1

## Executive Summary
This work automates the search for optimal neural architectures for the Proximal Policy Optimization (PPO) algorithm using Regularized Evolution, exploring over 1000 hybrid classical-quantum architectures on the CartPole-v1 environment. The study reveals that purely classical models consistently outperformed hybrid models, with the best hybrid architecture ranking 11th among unique models. Key findings include significant instability in hybrid models due to small network sizes and quantum layer configurations, with quantum layers having high qubit counts being harder to train. The research suggests that current variational quantum circuits offer no advantage over well-designed classical architectures in reinforcement learning tasks, highlighting the need for better hybrid design strategies and more challenging environments for future investigation.

## Method Summary
The study employed Regularized Evolution to systematically explore hybrid classical-quantum architecture spaces for PPO. The algorithm maintained a population of architectures, iteratively sampling candidates, mutating the highest-scoring one, training it, and deleting the oldest architecture regardless of fitness. Over 1000 architectures were tested on CartPole-v1, with quantum layers implemented as variational quantum circuits using PennyLane and classical layers as PyTorch linear layers. The search considered 10 mutation types including adding/removing layers, altering neurons/qubits, changing entanglement types, and modifying activation functions, with constraints of maximum 10 layers, 64 neurons/classical layer, and 10 qubits/quantum layer.

## Key Results
- Classical models consistently outperformed hybrid models, with the best hybrid architecture ranking 11th among unique architectures
- Hybrid models showed significant instability due to small network sizes and quantum layer configurations
- Quantum layers with high qubit counts (3-4+) were harder to train and rarely survived beyond initial population
- Models with excessive entanglement without proper backpropagation performed poorly

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Regularized Evolution can systematically explore hybrid classical-quantum architecture spaces with reduced human bias.
- **Mechanism:** The algorithm maintains a population P of architectures, iteratively samples S candidates, mutates the highest-scoring one, trains it, and deletes the oldest architecture regardless of fitness. This "aging evolution" ensures exploration while statistically guaranteeing rediscovery of good architectures.
- **Core assumption:** The mutation operations span the meaningful architectural search space without introducing implicit constraints that bias results toward poor configurations.
- **Evidence anchors:**
  - [Section 3.2] "we start with a P number of randomly generated architectures... iteratively and randomly pick a sample of size S from the population and mutate the highest scoring one"
  - [Section 4.1] Lists 10 mutation types including add/remove classical/quantum layers, alter neurons/qubits, change entanglement type
  - [corpus] FAQNAS paper applies similar genetic algorithms to HQNN architecture search, suggesting the approach is transferable
- **Break condition:** If mutations are too unconstrained for small networks (as the paper suggests), search becomes inefficient and instability increases.

### Mechanism 2
- **Claim:** Hybrid models with partial measurement (measuring fewer qubits than present) underperform because gradients cannot propagate effectively through unmeasured parameters.
- **Mechanism:** When a quantum layer outputs only n expectation values but contains more than n qubits, the optimizer tunes parameters that don't directly affect the loss—relying on entanglement to propagate information rather than direct backpropagation.
- **Core assumption:** Gradient-based optimization requires each parameter to have a direct, differentiable path to the loss function for efficient learning.
- **Evidence anchors:**
  - [Section 5] "most of the lowest performing models share that feature... having a healthy backpropagation throughout your model, where all parameters do make a difference to the loss value, is far better"
  - [Section 4.2] Describes two quantum layer types: one outputting n expectation values, another outputting 2^n measurements
  - [corpus] Weak direct evidence; related papers don't address this specific partial-measurement mechanism
- **Break condition:** If entanglement could reliably substitute for direct gradient paths (unproven), this mechanism would not hold.

### Mechanism 3
- **Claim:** Small hybrid networks exhibit higher variance in performance than equivalent classical networks due to quantum layer sensitivity.
- **Mechanism:** Quantum layers introduce additional hyperparameters (qubit count, entanglement type, ansatz repetitions) that interact non-linearly with network size, making small networks particularly unstable under random mutations.
- **Core assumption:** The instability stems from architectural factors rather than implementation bugs or hyperparameter choices.
- **Evidence anchors:**
  - [Section 5] "the training is not stable... the smallness of the networks which makes them so sensitive to any change in the network is also a big contributor"
  - [Table 1] Model 9 shows extreme variance: three perfect 500-score episodes and one episode of only 17
  - [corpus] Lockwood & Si (2021) found similar issues with small hybrid models failing on complex tasks
- **Break condition:** With different hyperparameter tuning or larger networks, this sensitivity might decrease.

## Foundational Learning

- **Concept: Proximal Policy Optimization (PPO)**
  - Why needed here: PPO is the target algorithm being optimized; understanding its clipped objective function explains why architecture stability matters.
  - Quick check question: Can you explain why PPO clips the probability ratio and how this relates to training stability?

- **Concept: Variational Quantum Circuits (VQCs)**
  - Why needed here: VQCs serve as quantum "layers" in hybrid models; their properties (entanglement, expressibility) directly affect learnability.
  - Quick check question: What is the difference between a VQC used standalone versus embedded as a layer in a hybrid neural network?

- **Concept: Regularized/Aging Evolution**
  - Why needed here: This is the search algorithm; understanding its population/sample mechanics is necessary to modify or extend the approach.
  - Quick check question: Why does deleting the oldest architecture (rather than the worst) not lose good solutions?

## Architecture Onboarding

- **Component map:** Environment observation → classical preprocessing layers → (optional) quantum layer → classical output layers → action probabilities and value estimate → PPO loss computation
- **Critical path:** Observation → classical layers (2-64 neurons, Tanh/ReLU) → quantum layer (2-10 qubits, Basic/Strong entanglement) → classical output layers → PPO loss
- **Design tradeoffs:**
  - More qubits = higher expressibility but harder training (paper shows 3-4 qubit layers were selected against)
  - Strong entanglement (F) vs Basic (L): paper suggests Strong performs better for small qubit counts
  - Two adjacent quantum layers require log₂ relationship between qubit counts, severely constraining architecture depth
- **Failure signatures:**
  - Perfect scores on some episodes with near-zero scores on others (high variance)
  - Quantum layers with >4 qubits rarely surviving beyond initial population
  - Models relying on partial measurement consistently ranking at the bottom
- **First 3 experiments:**
  1. Replicate the best classical architecture (C 37, T, C 41, T, C 2, R, C 24, T, C 5, T, C, 1) to establish baseline performance on your environment.
  2. Ablate the 11th-place hybrid model by replacing its 2-qubit quantum layer with equivalent classical neurons to quantify the quantum contribution.
  3. Test whether constraining mutations to only add quantum layers with full measurement (all qubits measured) improves hybrid model stability compared to the paper's partial-measurement approach.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can hybrid quantum-classical PPO architectures demonstrate a measurable advantage over classical models in more complex reinforcement learning environments?
- **Basis in paper:** [explicit] The author states the need to "test more environments to get more coherent data" because CartPole-v1 is considered "quite simple" and may not provide sufficient challenge.
- **Why unresolved:** The computational demand of the search algorithm (20 hours for a simple environment) prevented testing on harder benchmarks during this study.
- **What evidence would resolve it:** Comparative benchmarks of hybrid vs. classical architectures on high-dimensional control tasks (e.g., MuJoCo or Atari) showing hybrid convergence or superior sample efficiency.

### Open Question 2
- **Question:** Can specialized hybrid quantum-classical Neural Architecture Search (NAS) algorithms outperform the general Regularized Evolution approach?
- **Basis in paper:** [explicit] The author suggests that "new custom hybrid Quantum-Classical NAS Algorithms should be developed" to assist adoption in real-world applications.
- **Why unresolved:** This study utilized Regularized Evolution, a general algorithm, which may not be optimized for the specific constraints and nuances of quantum circuit training.
- **What evidence would resolve it:** The development of a NAS algorithm tailored to quantum parameter spaces that yields higher-performing hybrid architectures than regularized evolution.

### Open Question 3
- **Question:** Does enforcing stricter gradient propagation constraints during architecture search improve the stability of hybrid quantum models?
- **Basis in paper:** [inferred] The author notes that instability arose because mutations were "too free" and that models relying on entanglement for information propagation rather than direct backpropagation performed poorly.
- **Why unresolved:** The current search space allowed for "unhealthy" architectures where parameters did not effectively influence the loss function, but the specific search constraints required to prevent this were not tested.
- **What evidence would resolve it:** Ablation studies comparing random mutation strategies against constrained searches that require direct measurement of all active qubits to ensure gradient flow.

## Limitations
- Conclusions based on single environment (CartPole-v1) which may not adequately stress-test quantum architectures
- PPO hyperparameters and Regularized Evolution parameters (population size, sample size) were not reported
- The role of partial measurement in quantum layers remains theoretically questionable with limited experimental validation

## Confidence
- **High confidence:** Hybrid models consistently ranked below classical models in this search; small networks show high variance
- **Medium confidence:** Quantum layers with >4 qubits are harder to train; partial measurement mechanisms negatively impact learning
- **Low confidence:** Variational quantum circuits offer no advantage over classical architectures for RL tasks - this claim requires testing on more complex environments

## Next Checks
1. Test the best classical architecture on more challenging RL environments (MountainCar-v0, LunarLander-v2) to verify it maintains superiority
2. Compare full-measurement quantum layers against partial-measurement layers in ablation studies to quantify the gradient propagation effect
3. Implement architecture regularization (weight decay, dropout) specifically in hybrid models to test if this reduces the observed instability