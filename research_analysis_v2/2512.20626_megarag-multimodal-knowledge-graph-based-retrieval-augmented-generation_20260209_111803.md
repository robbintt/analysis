---
ver: rpa2
title: 'MegaRAG: Multimodal Knowledge Graph-Based Retrieval Augmented Generation'
arxiv_id: '2512.20626'
source_url: https://arxiv.org/abs/2512.20626
tags:
- answer
- entity
- retrieval
- image
- megarag
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MegaRAG, a multimodal knowledge graph-based
  retrieval augmented generation system designed to address limitations in existing
  RAG methods for long-form, domain-specific content. MegaRAG automatically constructs
  multimodal knowledge graphs by extracting entities and relations from text, figures,
  and tables, and then refines them to capture cross-modal and cross-page relationships.
---

# MegaRAG: Multimodal Knowledge Graph-Based Retrieval Augmented Generation

## Quick Facts
- **arXiv ID:** 2512.20626
- **Source URL:** https://arxiv.org/abs/2512.20626
- **Reference count:** 40
- **Primary result:** MegaRAG achieves up to 89.5% overall win rates on multimodal datasets and 64.85% accuracy on local slide QA tasks.

## Executive Summary
MegaRAG is a multimodal knowledge graph-based retrieval augmented generation system designed to address limitations in existing RAG methods for long-form, domain-specific content. The system automatically constructs multimodal knowledge graphs by extracting entities and relations from text, figures, and tables, and then refines them to capture cross-modal and cross-page relationships. A two-stage retrieval and generation approach is used to mitigate modality bias and improve reasoning. Experiments on global and local QA tasks across textual and multimodal datasets show that MegaRAG consistently outperforms strong baselines such as GraphRAG and LightRAG.

## Method Summary
MegaRAG constructs a multimodal knowledge graph through a two-stage process: initial parallel page-level extraction followed by refinement using retrieved subgraphs. The system extracts entities, relations, and visuals from documents using MinerU, builds an initial graph with GPT-4o-mini, then refines it by retrieving relevant subgraphs and running them through the MLLM again. Retrieval uses GME-Qwen2-VL-2B to embed entities, relations, and page images in a unified space. Two-stage answer generation produces separate textual and visual responses before synthesis. The approach is evaluated on global QA (pairwise win rates) and local QA (accuracy) across multiple multimodal benchmarks.

## Key Results
- MegaRAG achieves 89.5% overall win rates on multimodal datasets compared to 75.3% for GraphRAG and 69.2% for LightRAG.
- On local slide QA tasks, MegaRAG achieves 64.85% accuracy, significantly outperforming baselines.
- Ablation studies show that both the refinement stage and two-stage generation are critical for performance gains.

## Why This Works (Mechanism)

### Mechanism 1: Iterative Graph Refinement for Cross-Modal Context
The system first builds an initial graph ($G_0$) by extracting entities from pages in isolation, then retrieves a context-specific subgraph ($G_0^i$) for each page and feeds it back to the MLLM. This allows the model to "see" relevant global concepts while processing local page visuals, enabling it to infer missing links (e.g., connecting a chart on page 10 to a concept defined on page 2).

### Mechanism 2: Decoupled Generation to Mitigate Modality Bias
Instead of forcing the model to process a complex graph and images simultaneously (which causes it to over-focus on text), the model generates an intermediate answer based on the retrieved subgraph and another based on retrieved page images. A final synthesis step merges these distinct perspectives.

### Mechanism 3: Unified Embedding for Hybrid Retrieval
The system encodes entity descriptions, relation descriptions, and raw page images into the same vector space using GME-Qwen2-VL-2B. During retrieval, it extracts both high-level keywords (for entities) and low-level keywords (for visuals) to query this unified index.

## Foundational Learning

- **Concept: Modality Bias in MLLMs**
  - Why needed here: MegaRAG's two-stage generation is explicitly designed to counter this. Without understanding that models tend to ignore images when text is present, the architecture's complexity seems unnecessary.
  - Quick check question: Why doesn't simply feeding the image and the graph into the LLM at the same time work effectively?

- **Concept: Entity-Relationship Extraction vs. Summarization**
  - Why needed here: The system relies on extracting structured triplets (Entity, Relation, Entity) rather than just chunking text. Distinguishing "knowledge graph construction" from "vector indexing" is vital.
  - Quick check question: In the initial construction phase, does the system summarize the page or extract discrete nodes and edges?

- **Concept: Context Window Constraints vs. Global Context**
  - Why needed here: The core problem MegaRAG solves is "deep reasoning over long-form content" where the context window is insufficient for the whole document.
  - Quick check question: How does the "Refinement" stage provide global context without exceeding the MLLM's input token limit?

## Architecture Onboarding

- **Component map:** MinerU -> Initial MMKG Construction -> Refinement -> GME Embedding -> Dual-level Retrieval -> Two-stage Generation -> Final Synthesis
- **Critical path:** The Refinement Stage. This is where the paper claims its novelty lies (capturing cross-modal links). If the subgraph retrieval here fails to provide relevant global context, the resulting Knowledge Graph remains fragmented and the quality drops to baseline levels.
- **Design tradeoffs:**
  - Cost vs. Cohesion: The refinement stage requires a second pass over the document with the MLLM, effectively doubling the inference cost for graph construction compared to NaiveRAG.
  - Latency vs. Accuracy: The two-stage generation increases inference latency but is required to force visual reasoning.
- **Failure signatures:**
  - Missing Visual Entities: If the prompt or layout detection ignores decorative vs. informative images, the graph lacks visual nodes.
  - Fragmented Graph: If `top-k` (set to 60 in paper) in refinement is too low, the subgraph misses the "long-range" dependencies required to link concepts.
  - Modality Bias: If the final synthesis prompt is weak, the model produces a text-heavy answer that ignores the visual intermediate step.
- **First 3 experiments:**
  1. Sanity Check (Ablation A1): Run the pipeline on a slide deck using only the text content (no images/tables). Verify the performance drop to confirm the value of multimodal input.
  2. Retrieval Stress Test: Manually query for a relationship that spans non-consecutive pages (e.g., "How does the chart on page 5 relate to the conclusion on page 20?"). Check if `MegaRAG` retrieves both pages and the connecting graph path.
  3. Modality Bias Verification: Compare the output of "Single-pass Generation" vs. "Two-stage Generation" on a figure-heavy question. Check if the single-pass version hallucinates or ignores the figure details.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the performance and computational cost of MegaRAG scale when the refinement stage is applied iteratively beyond the single round used in the current implementation?
  - Basis in paper: The authors state in Section 3.1, "Although we perform only a single refinement step, the process can be applied iteratively to further improve graph completeness," but they adopt one round to "balance effectiveness and efficiency" without providing empirical data on multi-round convergence.

- **Open Question 2:** Do the reported improvements in global QA hold when evaluated by human experts rather than LLM-based judges?
  - Basis in paper: The evaluation of global QA (Section 4.2) relies entirely on LLM-based pairwise comparison (GPT-4.1-mini) using synthetic questions, as no ground truth exists.

- **Open Question 3:** How robust is the MMKG construction pipeline to errors in the initial visual entity extraction phase?
  - Basis in paper: The refinement stage (Section 3.1) relies on the initial graph $G_0$ to retrieve subgraphs; however, the paper assumes the MLLM can successfully identify meaningful visual entities initially.

## Limitations

- **Modality Alignment Reliability:** The effectiveness depends heavily on initial graph density and retrieval's ability to surface relevant global context, with no quantitative analysis of graph connectivity or subgraph relevance scores.
- **Embedding Model Generalization:** The unified embedding space using GME-Qwen2-VL-2B is untested on specialized visual domains (medical imaging, CAD drawings) where pre-training data coverage may be limited.
- **Ablation Completeness:** While the paper presents several ablations, confidence intervals for performance metrics are not reported, and the ablation space doesn't explore sensitivity to critical parameters like `top-k` in refinement.

## Confidence

- **High Confidence:** The two-stage generation mechanism effectively mitigates modality bias (supported by ablation A3 showing performance drops without it).
- **Medium Confidence:** The iterative refinement stage captures cross-modal relationships (mechanism is sound and shows performance gains, but lacks quantitative analysis of graph connectivity).
- **Medium Confidence:** The unified embedding approach enables effective hybrid retrieval (supported by benchmark results and related work, but embedding model's generalization to specialized domains is untested).

## Next Checks

1. **Graph Connectivity Analysis:** Analyze the distribution of node degrees and connected components in the initial graph $G_0$ across all test documents. Measure the subgraph retrieval precision/recall for the refinement stage to quantify how often relevant global context is surfaced versus noise.

2. **Visual Entity Preservation Test:** Design a controlled experiment with figure-heavy documents where some images are visually similar but semantically distinct. Verify that MegaRAG's graph construction preserves these distinctions and that the two-stage generation actually incorporates visual details rather than defaulting to text-only reasoning.

3. **Specialized Domain Stress Test:** Apply MegaRAG to a domain with specialized visuals (e.g., medical imaging reports or engineering schematics) and compare performance against general documents. Measure the drop in retrieval accuracy and answer quality to establish the embedding model's domain generalization limits.