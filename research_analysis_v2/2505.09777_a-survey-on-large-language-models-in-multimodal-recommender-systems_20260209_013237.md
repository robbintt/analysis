---
ver: rpa2
title: A Survey on Large Language Models in Multimodal Recommender Systems
arxiv_id: '2505.09777'
source_url: https://arxiv.org/abs/2505.09777
tags:
- multimodal
- https
- recommendation
- llms
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey presents a comprehensive review of large language model
  (LLM) integration in multimodal recommender systems (MRS), addressing the challenge
  of unifying heterogeneous user and item data. The paper introduces a novel taxonomy
  organizing LLM-based techniques into prompting strategies, training approaches,
  and data adaptation methods, while also covering MRS-specific methods like disentanglement,
  alignment, and fusion.
---

# A Survey on Large Language Models in Multimodal Recommender Systems

## Quick Facts
- arXiv ID: 2505.09777
- Source URL: https://arxiv.org/abs/2505.09777
- Reference count: 40
- Key outcome: Presents comprehensive taxonomy of LLM integration strategies in multimodal recommender systems, organizing techniques into prompting, training, and data adaptation approaches while identifying research gaps and underexplored modalities.

## Executive Summary
This survey comprehensively reviews the integration of large language models into multimodal recommender systems, addressing the challenge of unifying heterogeneous user and item data. The paper introduces a novel taxonomy organizing LLM-based techniques into prompting strategies, training approaches, and data adaptation methods, while also covering MRS-specific methods like disentanglement, alignment, and fusion. The survey highlights underexplored modalities such as tabular data, examines transferable techniques from adjacent recommendation domains, and synthesizes emerging trends. Key contributions include a new classification framework for LLM-MRS interactions, mapping of current trends and research gaps, and expanded resources on metrics and datasets. The work aims to clarify the emerging role of LLMs in multimodal recommendation and guide future research.

## Method Summary
The survey synthesizes findings from 40+ papers to create a comprehensive taxonomy of LLM integration in multimodal recommender systems. The methodology involves systematic literature review focusing on three main integration strategies: prompting (hard and soft prompts), training approaches (LoRA, adapters, full fine-tuning), and data adaptation methods. The survey maps these techniques to specific MRS tasks including disentanglement of modality-specific information, cross-modal alignment, and fusion strategies. The approach emphasizes practical implementation considerations while identifying theoretical gaps and research opportunities.

## Key Results
- Introduces novel taxonomy organizing LLM-based MRS techniques into prompting, training, and data adaptation methods
- Identifies underexplored modalities including tabular data and knowledge graphs in LLM-MRS context
- Highlights critical research gaps around prompt-adapter interactions, MLLM latency optimization, and LLM evaluator reliability
- Provides comprehensive resource mapping of metrics, datasets, and emerging trends in the field

## Why This Works (Mechanism)

### Mechanism 1: Prompt-Based Adaptation Without Retraining
Prompting strategies enable LLMs to adapt to multimodal recommendation tasks without updating internal weights, reducing deployment overhead. Hard prompts (manual templates, structured formats like JSON) and soft prompts (learnable embeddings) inject task-specific context at inference time, leveraging the LLM's pretrained reasoning capabilities. The LLM processes multimodal summaries or structured inputs directly through its language interface.

### Mechanism 2: Disentanglement of Modality-Specific and Shared Representations
Separating modality-specific information from shared cross-modal signals improves generalization and interpretability in multimodal recommendation. Contrastive learning, attention-based reweighting, clustering, and VAE-based approaches isolate distinct latent factors (e.g., visual style vs. textual semantics). This prevents modality dominance and enables fine-grained preference modeling.

### Mechanism 3: Cross-Modal Alignment via Projection and Contrastive Learning
Alignment mechanisms synchronize heterogeneous modalities into a shared embedding space, enabling consistent cross-modal reasoning. Contrastive losses (InfoNCE), adapter modules, projection layers, and VAE-based alignment map modality-specific representations to a unified space. This bridges collaborative signals (user-item interactions) with semantic content (text, images).

## Foundational Learning

- **Prompt Engineering for Structured Inputs**: Why needed - LLM-MRS systems increasingly use JSON or code-like prompts to represent user profiles, item attributes, and interaction logs in a structured, LLM-parseable format. Quick check - Can you design a prompt template that converts tabular user interaction data into a JSON structure that an LLM can reason over for next-item prediction?

- **Parameter-Efficient Fine-Tuning (LoRA, QLoRA, Adapters)**: Why needed - Full fine-tuning of LLMs is computationally prohibitive for most recommendation applications; lightweight adaptation methods are the standard approach. Quick check - Explain how LoRA modifies only low-rank matrices during fine-tuning while keeping the base LLM frozen, and why this matters for production recommender systems.

- **Contrastive Learning for Cross-Modal Alignment**: Why needed - Aligning text, image, and behavioral signals requires learning shared representations where semantically similar items cluster regardless of modality. Quick check - Given user-item interaction pairs and multimodal item features, how would you construct positive and negative samples for contrastive alignment?

## Architecture Onboarding

- **Component map**: Data Adaptation Layer -> LLM Processing Layer -> MRS-Specific Layer
- **Critical path**: Start with data adaptation (e.g., image-to-text summarization using MLLMs, tabular-to-text conversion via templates) -> choose integration strategy (prompting for lightweight deployment, LoRA for moderate adaptation, full fine-tuning for resource-rich settings) -> apply alignment (contrastive learning, projection heads) if modalities are misaligned -> select fusion point (early fusion before LLM, intermediate via cross-attention, or late fusion of predictions)
- **Design tradeoffs**: (1) Prompting vs. fine-tuning: Prompting is faster to deploy and interpret but may underperform on specialized domains; fine-tuning improves accuracy at higher computational cost and risk of overfitting. (2) Frozen vs. tuned LLM: Frozen models preserve generalization but cannot learn domain-specific patterns; tuning enables adaptation but requires careful regularization. (3) Coarse vs. fine-grained fusion: Coarse fusion is computationally efficient but may lose modality-specific details; fine-grained attention preserves details but increases complexity and latency.
- **Failure signatures**: (1) Modality dominance – one modality (often text) overwhelms others, indicated by performance drops when that modality is ablated; (2) Cold-start persistence – new items/users still perform poorly despite multimodal content, suggesting alignment failure; (3) Hallucination in generation – LLM produces factually incorrect recommendations, indicating insufficient grounding in item catalog; (4) Latency explosion – inference time exceeds production thresholds, common with MLLMs or multi-step prompting pipelines.
- **First 3 experiments**:
  1. Baseline prompting benchmark: Implement hard prompt templates for your domain (structured JSON format with user history and candidate items), evaluate top-K recommendation metrics (Recall@K, NDCG@K) using a frozen LLM API.
  2. Soft prompt + LoRA adaptation: Add learnable soft prompts for user/item IDs and apply LoRA to the last 4 transformer layers. Train on interaction data and compare against the baseline to quantify the gap between prompting and lightweight tuning.
  3. Ablation of alignment components: Remove contrastive alignment loss from a multimodal model (e.g., keep only supervised recommendation loss), measure performance drop on cold-start items.

## Open Questions the Paper Calls Out

- **Question**: What are the theoretical and empirical impacts of prompt-adapter interactions, fusion order, and tuning interference when combining soft prompting with LoRA/QLoRA? Basis - Section 4 notes this combination "introduces new questions around prompt–adapter interaction... warranting further theoretical and empirical investigation." Why unresolved - While efficient, the interaction between learnable soft prompts and low-rank adapter modules creates potential interference that current taxonomies do not fully explain. What evidence would resolve it - Ablation studies comparing different fusion orders and tuning schedules to quantify their effect on convergence and personalization accuracy.

- **Question**: How can Multimodal LLMs (MLLMs) be optimized to meet the low-latency constraints of real-time recommendation systems? Basis - Section 4 identifies that MLLMs are currently "impractical for real-time recommendation due to their inference overhead." Why unresolved - Most MLLM-based approaches operate in a frozen or lightly fine-tuned state, useful for offline enrichment but too slow for dynamic user interaction. What evidence would resolve it - Development of hybrid architectures or distilled MLLMs that retain semantic alignment capabilities while achieving sub-second inference times.

- **Question**: Can LLM-based evaluators reliably substitute human judgment for subjective recommendation qualities like helpfulness and coherence? Basis - Section 4 invites "systematic studies on the reliability, bias, and calibration of LLM evaluators" as a low-cost alternative to human evaluation. Why unresolved - LLM evaluators offer efficiency but suffer from potential alignment gaps and reproducibility issues; their correlation with human satisfaction in multimodal contexts is not guaranteed. What evidence would resolve it - Benchmarks comparing LLM evaluator scores against human annotations across diverse recommendation tasks to validate calibration.

## Limitations
- Relies heavily on theoretical synthesis rather than empirical validation of proposed mechanisms
- Does not provide systematic performance comparisons across integration strategies
- Focuses primarily on well-established modalities while only briefly mentioning underexplored domains like tabular data
- Does not address computational complexity trade-offs between different integration approaches in production settings

## Confidence

- **Prompt-based adaptation effectiveness**: Medium confidence - Supported by theoretical framework and adjacent research, but lacks direct empirical validation in the MRS context
- **Disentanglement benefits**: Low confidence - Mechanism is theoretically sound but specific efficacy in LLM-MRS remains underexplored per the survey itself
- **Cross-modal alignment importance**: Medium confidence - Well-established in multimodal learning literature, but optimal implementation details for recommendation remain unclear
- **Taxonomy comprehensiveness**: High confidence - The classification framework appears thorough and well-structured based on the systematic literature review

## Next Checks
1. **Implementation fidelity validation**: Reproduce a basic prompting-based MRS system using the proposed JSON template approach on a standard multimodal dataset (e.g., Amazon Beauty) to verify that the theoretical framework translates to practical implementation

2. **Performance gap analysis**: Systematically compare the performance difference between hard prompts, soft prompts, and LoRA-based adaptation on the same task to quantify the actual benefit of parameter-efficient fine-tuning versus prompting alone

3. **Modality ablation study**: Conduct controlled experiments removing individual modalities from a multimodal LLM-MRS system to empirically measure the contribution of each modality and test the disentanglement hypothesis about modality-specific versus shared representations