---
ver: rpa2
title: Sparse Linear Regression is Easy on Random Supports
arxiv_id: '2511.06211'
source_url: https://arxiv.org/abs/2511.06211
tags:
- theorem
- have
- algorithm
- probability
- preconditioner
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper studies the computational-statistical gap in sparse linear
  regression, where polynomial-time algorithms require O(d) samples while information-theoretic
  limits are O(k log d). The authors address this by showing that when the support
  of the sparse regression vector is chosen uniformly at random, the gap vanishes.
---

# Sparse Linear Regression is Easy on Random Supports

## Quick Facts
- **arXiv ID:** 2511.06211
- **Source URL:** https://arxiv.org/abs/2511.06211
- **Authors:** Gautam Chandrasekaran; Raghu Meka; Konstantinos Stavropoulos
- **Reference count:** 8
- **Primary result:** For random sparse regression vectors, achieves O(k log d) sample complexity vs. O(d) for arbitrary supports

## Executive Summary
The paper addresses the computational-statistical gap in sparse linear regression, where polynomial-time algorithms traditionally require O(d) samples while information-theoretic limits are O(k log d). The authors show that when the support of the sparse regression vector is chosen uniformly at random, this gap vanishes. Their key insight is that random supports admit efficient preconditioners - basis transformations that localize the signal's energy, enabling recovery with near-optimal sample complexity. The algorithm achieves prediction error ε with N = poly(k, log d, 1/ε) samples and poly(d,N) runtime for design matrices with condition number up to 2^poly(d).

## Method Summary
The method involves two main phases. First, an iterative "win-win" algorithm constructs a preconditioner (B, I) that transforms the regression vector to have bounded ℓ∞ norm outside I and bounded ℓ₁ norm on I. This preconditioner exploits structural properties of ill-conditioned submatrices by repeatedly solving convex programs to identify column approximations. Second, a hybrid norm-constrained regression problem is solved in the transformed basis Z = XB^⊤, constraining the ℓ₁ norm of the predictor outside I. The solution is then inverse transformed back to the original basis.

## Key Results
- Achieves sample complexity N = O(σ²k⁴(log log κ(X))² log(d)/(ε²δ)) for random supports
- Runs in polynomial time when condition number κ(X) ≤ 2^poly(d)
- Extends to random design settings with correlated Gaussian covariates without knowledge of covariance
- Provides lower bounds showing preconditioner construction is nearly tight

## Why This Works (Mechanism)

### Mechanism 1: Basis Transformation for Norm Localization
- **Claim:** Random supports admit efficient basis transformations that localize signal energy
- **Mechanism:** The preconditioner (B, I) transforms the regression vector so that with high probability over random supports, the transformed vector has bounded ℓ∞ norm outside I and bounded ℓ₁ norm on I, enabling efficient recovery
- **Core assumption:** Support is uniformly random; design matrix has bounded condition number
- **Evidence anchors:** [abstract], [section 3.2] (Definition 3.1), weak corpus support
- **Break condition:** Adversarial support selection breaks the localization property

### Mechanism 2: Iterative "Win-Win" Preconditioner Improvement
- **Claim:** Iterative refinement exploits structural properties of ill-conditioned submatrices
- **Mechanism:** Starting from a non-good preconditioner, the algorithm identifies sets of columns that can be approximated by small subsets of other columns, using this structure to update B and reduce the condition parameter κ
- **Core assumption:** Efficient solution of convex sub-problems; convergence in log log κ(X) steps
- **Evidence anchors:** [abstract], [section 3.3], weak corpus support
- **Break condition:** Numerical instability or noise exceeding bounds stalls or diverges iteration

### Mechanism 3: Hybrid Norm-Constrained Regression
- **Claim:** Standard regression succeeds with low sample complexity under specific norm constraints
- **Mechanism:** After transformation, solves regression in new basis Z = XB^⊤ using convex program constraining ℓ₁ norm outside I, leveraging easy case of low ℓ₁ norm for bulk coordinates
- **Core assumption:** Set I remains small relative to d for sample complexity advantage
- **Evidence anchors:** [section 3.1], [section 5] (Theorem 5.1), tangential corpus support
- **Break condition:** Large I (linear in d) loses sample complexity advantage

## Foundational Learning

- **Concept:** **Sparse Condition Number (κ(X))**
  - **Why needed here:** Critical for understanding sample complexity dependence on κ(X) and why log log κ(X) is a major improvement
  - **Quick check question:** How does k-sparse condition number differ from standard matrix condition number, and why does large κ(X) force O(d) samples?

- **Concept:** **Computational-Statistical Gap**
  - **Why needed here:** Central problem addressed - information-theoretic O(k log d) vs. computational O(d) sample requirements
  - **Quick check question:** Does "closing the gap" mean matching O(k log d) exactly or up to polynomial factors?

- **Concept:** **Sub-gaussian Noise and Concentration**
  - **Why needed here:** Required for theoretical error bounds in Theorem 1.2
  - **Quick check question:** Why is sub-gaussian assumption necessary for error bound, and what happens with heavy-tailed noise?

## Architecture Onboarding

- **Component map:** Input (X, y) -> Phase 1 (ImproveNorm) -> Transformation (Z = XB^⊤) -> Phase 2 (Hybrid-Lasso) -> Output (ŵ)

- **Critical path:** Iterative loop in Phase 1 (Algorithm 1) is computational bottleneck; failure to find large enough update sets forces resampling

- **Design tradeoffs:**
  - Random vs. Adversarial Support: Efficiency requires random support; no guarantee for worst-case
  - Runtime vs. Failure Probability (δ): Runtime scales as poly(1/δ); reducing failure probability increases computation

- **Failure signatures:**
  - Stalling in Phase 1: Max iterations without κ reduction suggests structural violations
  - Vacuous Sample Complexity: Large log log κ(X) makes N exceed O(d)

- **First 3 experiments:**
  1. Synthetic validation: Generate X with moderate κ and random supports; verify preconditioner construction and compare against Lasso
  2. Adversarial support test: Construct X, w* with correlated support; confirm degradation to O(d) samples
  3. Condition number scaling: Vary κ(X) systematically to verify log log κ(X) dependence empirically

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can ε dependence improve from O(1/ε²) to O(1/ε)?
- **Basis in paper:** [explicit] "Due to the use of the 'slow rate' analysis of the Lasso, our current sample complexity scales as (1/ϵ²). A natural goal is to improve this dependence to (1/ϵ)."
- **Why unresolved:** Slow rate Lasso analysis inherently yields 1/ε²; achieving 1/ε requires different techniques
- **What evidence would resolve it:** Algorithm achieving O(k log d/ε) sample complexity, or lower bound showing 1/ε² is unavoidable

### Open Question 2
- **Question:** Can results extend to much weaker support distribution assumptions?
- **Basis in paper:** [explicit] "A natural open question is to determine whether similar results can be established under much weaker assumptions on the support distribution."
- **Why unresolved:** Current analysis requires high min-entropy; weaker assumptions may need new techniques
- **What evidence would resolve it:** Algorithm handling sub-linear min-entropy distributions, or lower bound showing high min-entropy is necessary

### Open Question 3
- **Question:** Is (log log κ)² factor in preconditioner size tight?
- **Basis in paper:** [inferred] Theorem 7.1 shows Ω(k²/δ) lower bound vs. Theorem 3.2 achieving O(k²(log log κ)²/δ)
- **Why unresolved:** Iterative improvement requires log log κ rounds; whether fewer suffice is unclear
- **What evidence would resolve it:** Improved construction achieving O(k²/δ) size, or refined lower bound incorporating log log κ

### Open Question 4
- **Question:** What is precise computational-statistical gap for worst-case supports?
- **Basis in paper:** [inferred] Paper shows random supports are easy but gap remains for worst-case w*
- **Why unresolved:** Hardness results only apply to restricted algorithm classes; no widely accepted conjectured hard instance
- **What evidence would resolve it:** Polynomial-time algorithm achieving O(k log d) for worst-case, or unconditional lower bounds

## Limitations
- Random support assumption is critical and breaks for adversarial supports
- Extremely high condition numbers (2^poly(d)) may be impractical for many real-world matrices
- Lack of empirical validation across diverse scenarios and noise distributions

## Confidence
- **Theoretical framework:** Medium - relies on specific random structure assumptions
- **Algorithm correctness:** Medium - iterative preconditioner depends on numerical stability of convex programs
- **Practical applicability:** Low - permissive condition number bounds and lack of empirical validation

## Next Checks
1. **Empirical Validation of Preconditioner Construction**: Implement Algorithm 1 on synthetic datasets with varying condition numbers and support structures to verify convergence rates and preconditioner quality match theoretical predictions.

2. **Robustness to Support Structure**: Test algorithm performance when support is only approximately random (e.g., with mild correlations) to determine sensitivity to the random support assumption.

3. **Scalability Analysis**: Evaluate runtime and sample complexity empirically as k and d scale, particularly focusing on cases where κ(X) is large to validate the log log κ(X) dependence.