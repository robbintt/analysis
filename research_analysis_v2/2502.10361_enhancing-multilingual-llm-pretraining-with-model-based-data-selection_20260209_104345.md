---
ver: rpa2
title: Enhancing Multilingual LLM Pretraining with Model-Based Data Selection
arxiv_id: '2502.10361'
source_url: https://arxiv.org/abs/2502.10361
tags:
- data
- dataset
- multilingual
- performance
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a model-based filtering framework for multilingual\
  \ web-scale pretraining datasets, aiming to enhance the quality of data by identifying\
  \ structured and knowledge-rich samples. The method uses embedding-based classifiers\u2014\
  specifically FastText and Transformer (XLM-RoBERTa)\u2014trained on a carefully\
  \ curated multilingual dataset (MKC+), which includes sources like Aya Collection,\
  \ MMLU, and Include-Base-44."
---

# Enhancing Multilingual LLM Pretraining with Model-Based Data Selection

## Quick Facts
- **arXiv ID:** 2502.10361
- **Source URL:** https://arxiv.org/abs/2502.10361
- **Reference count:** 40
- **One-line primary result:** Model-based filtering achieves competitive LLM performance with ~15% of training tokens.

## Executive Summary
This paper introduces a model-based filtering framework for multilingual web-scale pretraining datasets, aiming to enhance the quality of data by identifying structured and knowledge-rich samples. The method uses embedding-based classifiers—specifically FastText and Transformer (XLM-RoBERTa)—trained on a carefully curated multilingual dataset (MKC+), which includes sources like Aya Collection, MMLU, and Include-Base-44. Ablation studies across diverse languages (Chinese, German, French, Arabic, Danish) show that the MLP-based approach with MKC+ achieves competitive performance with as little as 15% of the training tokens compared to baseline FineWeb-2. The approach also generalizes well to English, outperforming existing model-based filtering methods. The refined dataset covering 20 languages is publicly released. Results demonstrate improved downstream benchmark performance while maintaining data diversity, with no significant gains from mixing in unfiltered data.

## Method Summary
The paper proposes a two-stage model-based filtering pipeline for multilingual pretraining. First, a binary classifier is trained to distinguish high-quality (positive) from low-quality (negative) documents, using curated datasets (MKC+) as positive examples and a sample of the raw corpus (FineWeb-2) as negatives. Two classifier architectures are compared: FastText with n-gram features and an MLP on XLM-RoBERTa embeddings. Second, the trained classifier scores all documents in the corpus, and only those above a retention threshold are kept for LLM pretraining. The filtered datasets are used to train 1B Llama models, which are evaluated on multilingual benchmarks (FineTasks), showing competitive performance with significantly fewer training tokens.

## Key Results
- **10-15% data retention:** MLP-based filtering with MKC+ matches baseline performance using only 10-15% of the original training tokens for high-resource languages.
- **Generalization to English:** The multilingual classifier trained on MKC+ generalizes well to English, outperforming previous model-based filtering approaches.
- **No benefit from mixing:** Adding unfiltered data to the filtered dataset does not improve downstream performance, indicating the filter preserves necessary diversity.

## Why This Works (Mechanism)
The method works by training a classifier to identify documents with structural and semantic features indicative of high-quality knowledge sources. By using curated datasets (MKC+) as positive examples, the classifier learns to recognize patterns associated with structured, factual, and well-formed text. This selective filtering reduces noise and redundancy in the pretraining data, allowing models to learn more efficiently from a smaller, higher-quality corpus. The use of multilingual embeddings (XLM-RoBERTa) enables the filter to generalize across languages, capturing quality signals that transcend linguistic boundaries.

## Foundational Learning
- **Concept:** Binary Classification for Data Filtering
  - **Why needed here:** The core of the proposed framework uses a classifier to decide which documents to keep (positive class) and which to discard (negative class). Understanding how a model learns to separate these two classes based on training examples is fundamental.
  - **Quick check question:** Given a dataset of positive (e.g., Wikipedia) and negative (e.g., random web text) examples, how would a classifier use features like n-grams or embeddings to score a new, unseen document?

- **Concept:** Text Embeddings (FastText vs. Transformer)
  - **Why needed here:** The paper compares two fundamentally different ways of representing text numerically. FastText uses subword information and is computationally efficient, while Transformers (XLM-RoBERTa) use contextual embeddings. The choice of embedding dictates the type of semantic information the filter can leverage.
  - **Quick check question:** What is the primary difference between the embedding produced by FastText for a sentence versus the embedding produced by a Transformer model like XLM-RoBERTa?

- **Concept:** LLM Pretraining Scaling Laws & Data Quality
  - **Why needed here:** The paper's central claim is about achieving comparable performance with fewer tokens. This challenges or refines the notion that simply scaling up on *more* data is always the best path, introducing *data quality* as a multiplier on compute efficiency.
  - **Quick check question:** If a smaller model trained on a filtered dataset matches a larger model trained on a raw dataset, what does that imply about the composition of the raw dataset?

## Architecture Onboarding
- **Component map:** MKC+ datasets -> Binary Classifier (FastText or MLP) -> Quality Scores -> Filtered Corpus -> LLM Pretraining
- **Critical path:** The quality of the final LLM is most sensitive to the **Training Data Curation** (MKC+). If the positive examples for the classifier are not truly high-quality or representative, the classifier will learn to select the wrong data, and the entire pipeline fails.
- **Design tradeoffs:**
  - **FastText vs. MLP (XLM-RoBERTa):** FastText is significantly faster and cheaper to run on web-scale data but is less performant. The MLP with Transformer embeddings is more accurate but has a much higher computational cost.
  - **Retention Threshold:** A lower threshold (e.g., 10%) increases data efficiency but risks losing diversity. A higher threshold preserves more data but dilutes quality. The optimal threshold depends on the initial corpus size.
- **Failure signatures:**
  - **Classifier overfitting:** The classifier learns features specific to the small, curated positive dataset (e.g., "has a reference section") that are not general indicators of quality in the web corpus.
  - **Domain Collapse:** The filtered dataset becomes too narrow, causing the LLM to fail on general knowledge benchmarks outside the domains present in the positive training data.
  - **Language Bias:** The multilingual classifier performs poorly on low-resource languages, leading to a filtered dataset that is disproportionately skewed towards high-resource languages.
- **First 3 experiments:**
  1. **Reproduce classifier training:** Train a FastText classifier on a small, curated dataset (e.g., Wikipedia vs. Common Crawl) and score a held-out set of documents. Manually inspect the highest and lowest-scoring examples to verify the classifier's "quality" signal aligns with human judgment.
  2. **Ablation on positive dataset:** Train separate classifiers using different subsets of the MKC+ dataset (e.g., one with only MMLU, one with only Aya). Compare the downstream LLM performance to understand which data sources contribute most to the quality signal.
  3. **Threshold sweep:** For a single language, filter the corpus at multiple retention rates (e.g., 5%, 10%, 20%, 50%). Pretrain small LLMs on these datasets and plot performance vs. token count to find the point of diminishing returns where aggressive filtering hurts performance.

## Open Questions the Paper Calls Out
### Open Question 1
- **Question:** What specific factors beyond document length bias determine the optimal data retention threshold for different filtering methods (e.g., MLP vs. FastText)?
- **Basis in paper:** [inferred] Section 4.2.2 discusses threshold selection and notes that while MLP MKC tends to retain shorter documents, optimal performance occurs at different retention rates (e.g., 10% vs. 20%) depending on the method, "suggesting that factors beyond the retained document length bias may influence performance."
- **Why unresolved:** The paper observes the discrepancy in optimal thresholds and length biases but does not isolate the semantic or structural features driving these differences.
- **What evidence would resolve it:** A detailed analysis correlating retention thresholds with semantic diversity metrics or syntactic complexity of the retained documents across different filter types.

### Open Question 2
- **Question:** Is the diversity resulting from combining various training datasets (MKC+) strictly necessary for high performance, or can a single high-quality source suffice?
- **Basis in paper:** [explicit] Section 4.2.3 explicitly asks, "But is the diversity introduced by combining various base datasets truly necessary?" The results show MKC+ is best, but the Aya Collection alone yields strong performance.
- **Why unresolved:** While the combined MKC+ dataset achieves the highest average rank, the strong individual performance of the Aya Collection leaves the trade-off between data scale and source diversity ambiguous.
- **What evidence would resolve it:** Controlled experiments matching token counts between the combined MKC+ mixture and individual components to isolate the specific contribution of dataset diversity versus volume.

### Open Question 3
- **Question:** Does mixing in unfiltered data (replay) provide performance benefits specifically for FastText-based filtering, given the mixed signals observed compared to Transformer-based methods?
- **Basis in paper:** [explicit] Section 4.2.4 asks, "But does our model-based filtering introduce unwanted biases?" regarding the replay of original data, noting that while MLP MKC+ shows no gain, "FT MKC+ filters shows mixed signal."
- **Why unresolved:** The paper concludes replay is unnecessary for MLP but acknowledges ambiguous results for FastText, leaving the efficacy of replay for simpler filtering architectures unsettled.
- **What evidence would resolve it:** A targeted ablation study varying replay rates specifically for FastText classifiers to determine if they require raw data to maintain diversity that Transformer embeddings might preserve naturally.

## Limitations
- **Black-box quality signal:** The specific linguistic or structural features that drive classifier decisions remain opaque, risking domain collapse on out-of-distribution tasks.
- **Subjective curation bias:** The MKC+ selection process may embed biases toward certain knowledge domains or writing styles, limiting generalizability.
- **Negative example ambiguity:** Using held-out web text as negatives may train the classifier to distinguish "curated" from "web" text, not necessarily "good" from "bad" quality.

## Confidence
- **High Confidence:** Model-based filtering achieves comparable performance with 10-15% of training tokens (well-supported by ablation studies across five diverse languages).
- **Medium Confidence:** Competitive English performance (presented as supplemental result with less detailed methodology).
- **Low Confidence:** No benefit from mixing filtered and unfiltered data (contradicts other literature, lacks rigorous ablation).

## Next Checks
1. **Human Evaluation of Classifier Output:** Manually annotate the top 100 and bottom 100 documents scored by the classifier for each language (Chinese, German, French) to verify that high scores correlate with human judgments of quality and knowledge richness.
2. **Domain Generalization Test:** Evaluate the filtered dataset LLMs on domain-specific benchmarks outside the MKC+ source domains (e.g., medical knowledge, programming, or creative writing) to detect potential domain collapse.
3. **Threshold Sensitivity Analysis:** Systematically vary the retention threshold (5%, 10%, 20%, 30%, 50%) for one language and measure both downstream performance and diversity metrics (vocabulary richness, topic coverage) to quantify the tradeoff between efficiency and data diversity.