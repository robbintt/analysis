---
ver: rpa2
title: Dynamic Mixture of Experts Against Severe Distribution Shifts
arxiv_id: '2511.18987'
source_url: https://arxiv.org/abs/2511.18987
tags:
- learning
- experts
- arxiv
- expert
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper evaluates DynamicMoE, a method that dynamically adds
  new experts to mixture-of-experts layers to address loss of plasticity in continual
  learning and reinforcement learning. The key idea is to periodically add experts
  while maintaining parameter efficiency through bottlenecked expert architectures.
---

# Dynamic Mixture of Experts Against Severe Distribution Shifts

## Quick Facts
- arXiv ID: 2511.18987
- Source URL: https://arxiv.org/abs/2511.18987
- Authors: Donghu Kim
- Reference count: 14
- The paper evaluates DynamicMoE, a method that dynamically adds new experts to mixture-of-experts layers to address loss of plasticity in continual learning and reinforcement learning.

## Executive Summary
This paper proposes DynamicMoE, a method that addresses plasticity loss in continual learning and reinforcement learning by dynamically adding new experts to mixture-of-experts layers. The approach maintains parameter efficiency through bottlenecked expert architectures while periodically expanding capacity to handle severe distribution shifts. Experiments demonstrate that DynamicMoE with granularity ≥ 2 maintains initial performance throughout all tasks, while baseline methods show performance degradation. In reinforcement learning on Craftax, dynamically growing from 1 to 2 experts improved performance compared to using only 1 expert and even outperformed methods using full capacity from the start.

## Method Summary
DynamicMoE replaces standard neural network layers with mixture-of-experts (MoE) layers that contain multiple expert sub-networks and a router/gating network. The key innovation is periodically adding new experts while maintaining parameter efficiency through bottlenecked expert architectures. When new experts are added, all expert hidden dimensions are rescaled to keep the total parameter count constant. The router learns to allocate samples to appropriate experts based on learned affinities. The method was evaluated on synthetic continual learning (Tiny ImageNet split into 10 chunks) and reinforcement learning (Pixel-based Craftax with 7+ stages), using a granularity parameter that controls the number of experts to start with and add during growth.

## Key Results
- In synthetic continual learning on Tiny ImageNet, DynamicMoE with granularity ≥ 2 maintained initial performance throughout all tasks, while baseline methods showed performance degradation
- In reinforcement learning on Craftax, dynamically growing from 1 to 2 experts improved performance compared to using only 1 expert, and even outperformed methods using full capacity from the start
- Analysis revealed that newly added experts specialized for distinct distributions in different stages of the environment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Periodically adding new experts preserves trainability under distribution shift.
- Mechanism: Fresh, randomly initialized experts introduce new parameters that have not been updated on prior distributions, avoiding the feature exhaustion and gradient magnitude collapse that fixed-capacity networks experience. The router learns to allocate new experts to novel distributions while preserving prior expert weights.
- Core assumption: Loss of plasticity stems primarily from parameter exhaustion rather than optimization dynamics alone.
- Evidence anchors:
  - [abstract] "dynamically adds new experts to mixture-of-experts layers to address loss of plasticity"
  - [section 3.1] "DynamicMoE (g2, g4) is able to maintain its initial performance until the final task. Other expansion methods did not exhibit this property."
  - [corpus] Fresh-CL and MINGLE both use expert-based routing for continual learning, supporting the expert specialization hypothesis, though neither focuses on dynamic growth specifically.
- Break condition: If distribution shift is gradual rather than severe, the router may fail to differentiate distributions, causing redundant expert usage without true specialization.

### Mechanism 2
- Claim: Newly added experts specialize on distinct distributions as identified by the router.
- Mechanism: The gating network assigns samples to experts based on learned affinity. When new experts are added mid-training, they receive gradients primarily from the current distribution, enabling specialization. The visualization shows router weights shifting from old to new experts at stage boundaries.
- Core assumption: The router can learn to distinguish distributions without explicit task indices.
- Evidence anchors:
  - [section 3.2] "As the agent enters second stage (dungeon) at t=221, the router weight promptly shifts from the old expert to the new one."
  - [section 3.2] "distinct distributions (that arise in different stages of Craftax) are allocated to newly added experts"
  - [corpus] Corpus lacks direct replication of this routing-behavior-at-transition finding; most related work assumes static expert counts.
- Break condition: If expert granularity is too coarse (g1), individual experts may lack capacity to adequately represent distributions, causing under-specialization.

### Mechanism 3
- Claim: Growing capacity dynamically outperforms starting with full capacity.
- Mechanism: Starting with full capacity causes early experts to overfit early distributions, reducing their effectiveness for later distributions. Dynamic growth reserves fresh capacity for later stages. The comparison shows 1-to-2 grow outperforms both 1 expert and 2 experts from start.
- Core assumption: Early-training parameter commitment reduces representational flexibility for future distributions.
- Evidence anchors:
  - [section 3.2] "dynamically growing experts even surpasses the two baselines that use the full capacity from the start"
  - [section 3.2] "This could be explained that the capacity is wasted on overfitting early distributions"
  - [corpus] No corpus papers directly compare dynamic vs. static capacity allocation; this claim relies solely on the paper's experiments.
- Break condition: If growth timing misaligns with distribution shift boundaries, newly added experts may not receive sufficiently distinct gradient signals to specialize.

## Foundational Learning

- Concept: **Mixture-of-Experts routing (gating networks)**
  - Why needed here: Understanding how differentiable routers assign samples to experts is essential for debugging why new experts do or don't receive gradient updates.
  - Quick check question: Can you explain why a softmax router might collapse to using only one expert, and how load-balancing auxiliary losses address this?

- Concept: **Plasticity vs. stability tradeoff**
  - Why needed here: The paper positions dynamic growth as a solution to plasticity loss; distinguishing this from catastrophic forgetting clarifies what problem is being solved.
  - Quick check question: If a network maintains training accuracy but validation accuracy degrades across tasks, is this primarily a plasticity or forgetting problem?

- Concept: **Bottleneck architectures for parameter budgeting**
  - Why needed here: DynamicMoE controls parameter count by scaling expert inner dimensions; understanding bottleneck tradeoffs is necessary for choosing granularity.
  - Quick check question: Given a fixed parameter budget, how does increasing the number of experts while decreasing each expert's hidden dimension affect representational capacity per expert?

## Architecture Onboarding

- Component map:
  - MoE Layer: Replaces linear/MLP layers with expert sub-networks and a router
  - Bottlenecked Expert: Input → expanded hidden → bottleneck hidden (controlled) → expanded hidden → output
  - Router/Gating Network: Produces softmax weights over experts for each input
  - Growth Controller: Periodically adds new experts with reduced hidden dimensions; timing is fixed (e.g., every 500M steps in RL)

- Critical path:
  1. Implement bottlenecked expert architecture with configurable hidden dimension
  2. Integrate router that produces per-sample expert weights
  3. Implement growth operation: add expert, rescale all expert hidden dimensions to maintain parameter budget
  4. Initialize new expert randomly (function preservation not used)

- Design tradeoffs:
  - Granularity: Higher granularity (more experts per growth) maintains performance better (g2, g4 succeed; g1 does not), but reduces per-expert capacity
  - Growth timing: Too frequent growth wastes capacity; too infrequent misses distribution shifts
  - Load balancing: Paper does not add auxiliary loss; relies on natural router learning. Assumption: This works because distributions are sufficiently distinct.

- Failure signatures:
  - Router collapse: All samples assigned to single expert (check router entropy)
  - No specialization gain: New expert performance identical to old expert (check if router weights shift at distribution boundaries)
  - Granularity mismatch: g1 fails to maintain performance while g2 succeeds; suggests minimum expert capacity threshold exists

- First 3 experiments:
  1. **Synthetic CL sanity check**: Replicate Tiny ImageNet split experiment with g1, g2, g4; verify that training accuracy is maintained only for g≥2
  2. **Router visualization**: Log router weights across training; verify weight shifts occur at distribution boundaries
  3. **Growth timing ablation**: Test growth intervals at 0.5x, 1x, 2x the baseline interval; measure performance vs. alignment with true shift points

## Open Questions the Paper Calls Out

- **Question**: Does incorporating function preservation techniques when initializing new experts improve stability compared to the random initialization used in this study?
  - Basis in paper: [explicit] The authors state, "For simplicity, we do not consider function preservation when adding new experts," citing prior work [2] while deliberately omitting it.
  - Why unresolved: The paper chooses random initialization for new experts to simplify the proposed approach, leaving the potential benefits of maintaining the output function during expansion untested.
  - What evidence would resolve it: A comparison of DynamicMoE performance when new experts are initialized via function-preserving methods versus random initialization.

- **Question**: What is the optimal trade-off between expert granularity (number of experts) and individual expert capacity (hidden dimension) under a fixed parameter budget?
  - Basis in paper: [explicit] The results show DynamicMoE with Granularity 1 failed, while Granularities 2 and 4 succeeded. The paper notes that naive adoption "would limit the size of the feature space," implying a tension between width and count.
  - Why unresolved: While the paper demonstrates that higher granularity works better for the tested setups, it does not establish a general principle for finding the optimal balance point.
  - What evidence would resolve it: An ablation study sweeping across various granularity levels while monitoring feature representation rank and final performance.

- **Question**: Can the expansion of experts be triggered adaptively based on online performance metrics rather than fixed time intervals?
  - Basis in paper: [inferred] The method adds experts periodically at fixed steps (e.g., every 500M steps), which relies on arbitrary scheduling rather than detecting actual distribution shifts or plasticity loss.
  - Why unresolved: Fixed schedules may add capacity too late (losing plasticity) or too early (wasting parameters), suggesting a need for a data-driven trigger mechanism.
  - What evidence would resolve it: Implementing an adaptive growth mechanism based on metrics like gradient norm or validation loss plateau, and comparing its efficiency to the fixed schedule.

## Limitations
- The paper's claims rely heavily on synthetic experiments with controlled distribution shifts that may not reflect real-world gradual changes
- The Craftax results are based on a single environment with fixed stage boundaries
- The fixed growth timing assumes knowledge of when distribution shifts occur, which may not be available in practical applications

## Confidence
- **High confidence**: The mechanism of periodically adding new experts preserves trainability under severe distribution shifts
- **Medium confidence**: Newly added experts specialize on distinct distributions as identified by the router
- **Low confidence**: Starting with full capacity causes early experts to overfit early distributions

## Next Checks
1. **Distribution ambiguity test**: Create synthetic CL tasks where distribution boundaries are blurred (e.g., gradual feature drift rather than discrete chunks) and measure whether DynamicMoE maintains performance without explicit shift timing

2. **Router behavior under uncertainty**: Log router entropy and expert assignment patterns when distributions overlap significantly; verify whether the router can learn soft specialization rather than collapsing to single-expert assignment

3. **Real-world deployment simulation**: Apply DynamicMoE to a benchmark with unknown and variable distribution shifts (e.g., CORe50 or real-world sensor data) without pre-programmed growth timing to assess practical applicability