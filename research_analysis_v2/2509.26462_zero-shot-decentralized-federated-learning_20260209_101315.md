---
ver: rpa2
title: Zero-Shot Decentralized Federated Learning
arxiv_id: '2509.26462'
source_url: https://arxiv.org/abs/2509.26462
tags:
- learning
- prompt
- clients
- prompts
- federated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ZeroDFL addresses federated zero-shot learning by enabling fully
  decentralized prompt learning across distributed clients. Unlike existing federated
  prompt learning methods that rely on central servers, ZeroDFL allows clients to
  independently optimize and exchange textual prompts through a peer-to-peer iterative
  mechanism.
---

# Zero-Shot Decentralized Federated Learning

## Quick Facts
- **arXiv ID:** 2509.26462
- **Source URL:** https://arxiv.org/abs/2509.26462
- **Reference count:** 29
- **Primary result:** Achieves state-of-the-art performance on zero-shot image classification with 118× less communication than FedTPG

## Executive Summary
ZeroDFL introduces a fully decentralized approach to federated zero-shot learning by enabling clients to collaboratively optimize and exchange textual prompts without a central server. The method uses prompt learning on a frozen CLIP backbone, allowing each client to adapt the model to its local class set while preserving privacy and reducing communication overhead. Through weighted peer selection and diversity-prioritized prompt sampling, ZeroDFL balances knowledge propagation and generalization across heterogeneous data distributions.

Evaluated on nine diverse image classification datasets, ZeroDFL outperforms or matches centralized federated prompt learning baselines while dramatically reducing communication costs. The approach demonstrates strong convergence properties, maintaining high accuracy even under sparse prompt exchanges, and shows promise for scalable, privacy-preserving zero-shot learning in fully decentralized settings.

## Method Summary
ZeroDFL enables fully decentralized federated zero-shot learning through prompt learning on a frozen CLIP model. Each client independently optimizes M=4 learnable textual prompts prepended to class names using cross-entropy loss over cosine similarities between image and text embeddings. Clients exchange prompts with S=5 peers per round using inverse-frequency weighted selection to balance knowledge propagation. A diversity-prioritized sampling strategy ensures prompt pools contain heterogeneous knowledge. The method operates without a central server, making it privacy-preserving and scalable.

## Key Results
- Achieves 76.19% average accuracy on zero-shot classification across nine datasets, matching or outperforming centralized FedTPG and FedCoOp baselines
- Reduces communication overhead by 118× compared to FedTPG while maintaining classification accuracy
- Demonstrates strong generalization with only 807 MB of total communication in minimal exchange scenario
- Shows consistent convergence with low per-client accuracy variance (avg 1.68% for N=10, 0.39% for N=20)

## Why This Works (Mechanism)

### Mechanism 1: Frozen-Backbone Prompt Learning
- **Claim:** Learnable textual prompts enable CLIP adaptation for zero-shot classification without updating model weights, reducing per-client computation and communication.
- **Mechanism:** Each client prepends M learnable prompt vectors to class name embeddings before passing them through a frozen CLIP text encoder. The prompts are optimized via cross-entropy loss over cosine similarities between image and text embeddings (Eq. in Section III-A). This keeps CLIP frozen while adapting its effective behavior.
- **Core assumption:** The frozen CLIP representations are already sufficiently general; only the textual interface needs calibration.
- **Evidence anchors:**
  - [abstract] "ZeroDFL allows clients to independently optimize and exchange textual prompts through a peer-to-peer iterative mechanism"
  - [Section III-A] "we employ prompt learning, i.e. instead of updating the entire CLIP's parameters, a set of learnable textual prompt vectors is prepended to class names"
  - [corpus] FLIP (arXiv:2503.22263) confirms prompt learning reduces communication in federated settings, supporting this design pattern.
- **Break condition:** If local data domains are extremely distant from CLIP's pre-training distribution (e.g., specialized medical modalities), prompt-only tuning may underfit.

### Mechanism 2: Inverse-Frequency Weighted Peer Selection
- **Claim:** Prioritizing under-contacted clients for prompt exchange improves knowledge propagation balance and convergence stability.
- **Mechanism:** Each client maintains selection frequency counters for all peers. Recipients are sampled with probability proportional to inverse frequency (w = 1/(F + ε), Eq. 1-2). This gradually equalizes update receipt across the federation.
- **Core assumption:** The network topology allows any client to reach any other client over multiple rounds; no persistent partitions.
- **Evidence anchors:**
  - [Section III-B] "This formulation ensures that clients who have been selected less frequently by c_i are given higher priority in subsequent rounds"
  - [Table III] Low standard deviation across clients (avg 1.68% for N=10, 0.39% for N=20) suggests convergence consistency supported by balanced exchange.
  - [corpus] DFed-SST (arXiv:2508.11530) highlights topology-aware routing in decentralized FL; ZeroDFL's frequency weighting is a simpler heuristic that assumes connectivity.
- **Break condition:** If the network partitions or if malicious clients spam exchanges to manipulate frequency counters, balance degrades.

### Mechanism 3: Diversity-Prioritized Prompt Pool Sampling
- **Claim:** Sampling received prompts from diverse sources per round improves generalization to unseen classes by aggregating heterogeneous knowledge.
- **Mechanism:** Each client accumulates received prompts into a local pool. When selecting M prompts for the next local training epoch, the client samples preferentially from different source clients to avoid over-reliance on any single peer.
- **Core assumption:** Different clients learn complementary prompt representations due to non-IID data; diversity correlates with generalization.
- **Evidence anchors:**
  - [Section III-B] "To maximize the diversity of received knowledge, each client then samples M prompt vectors following a strategy that, whenever possible, ensures each selected prompt originates from a different client"
  - [Table V] Exchanging even 1 prompt per round (vs 0) lifts average accuracy from 71.14% to 75.82%, and full exchange (4) reaches 76.19%, supporting that cross-client prompt diversity aids generalization.
  - [corpus] Token-Level Prompt Mixture (arXiv:2504.21063) proposes routing-based prompt mixing for FedDG; ZeroDFL uses a simpler uniformity heuristic without learned routing.
- **Break condition:** If most clients converge to similar prompts (e.g., under homogeneous data), diversity sampling yields diminishing returns.

## Foundational Learning

- **Concept: CLIP frozen encoder + prompt tuning**
  - **Why needed here:** ZeroDFL assumes engineers understand that CLIP has separate vision and text encoders, and that prepending learnable vectors to text inputs changes model behavior without weight updates.
  - **Quick check question:** Can you explain why cosine similarity between image and text embeddings drives the classification decision?

- **Concept: Decentralized vs. centralized federated learning**
  - **Why needed here:** The method eliminates the central server; engineers must reason about peer-to-peer communication patterns and the absence of FedAvg-style aggregation.
  - **Quick check question:** What happens if a client goes offline mid-round in decentralized FL vs. server-based FL?

- **Concept: Non-IID data partitions and zero-shot generalization**
  - **Why needed here:** Each client has disjoint class sets; the goal is to classify held-out classes never seen during training.
  - **Quick check question:** Why does training on classes A–J and testing on K–T require different techniques than standard supervised FL?

## Architecture Onboarding

- **Component map:** Client node -> Frozen CLIP backbone -> Text encoder with prepended prompts -> Cross-entropy loss -> Local optimizer; Prompt exchange module -> Inverse-frequency weighted peer selector -> Sender/receiver buffers -> Prompt pool

- **Critical path:**
  1. Initialize each client with M random prompt vectors
  2. For R rounds:
     - Sample M prompts from local pool (diversity-prioritized)
     - Run E local epochs optimizing prompts via cross-entropy
     - Compute peer weights via inverse frequency
     - Send updated prompts to S selected peers
     - Receive prompts into pool
  3. At test time: classify unseen classes using final prompt + frozen CLIP

- **Design tradeoffs:**
  - S (recipients per round): Higher S improves convergence speed and robustness but increases communication (Fig. 2 shows ~4 GB at S=5 vs ~46 GB at S=C for 59 clients/500 rounds).
  - M (prompts per client): More prompts increase representational capacity but also per-round transmission size.
  - Ms (prompts exchanged per round): Partial exchange (Ms < M) reduces communication but may slow generalization (Table V shows gradual accuracy drop as Ms decreases).
  - Network degree: Sparse connectivity risks slow propagation; full mesh maximizes speed at bandwidth cost.

- **Failure signatures:**
  - High variance in per-client test accuracy: may indicate unbalanced exchange frequencies or network partitions.
  - Stagnant accuracy over rounds: check learning rate, prompt initialization, or whether pool diversity is collapsing.
  - Communication bottlenecks: if S is set too high for available bandwidth, round times spike.
  - Divergence to poor local minima: may occur if local epochs E are too high relative to exchange frequency.

- **First 3 experiments:**
  1. **Baseline sanity check:** Run ZeroDFL with S=5, M=4, R=50 on a single dataset split (e.g., Caltech101) with 5–10 clients. Compare final average accuracy and std dev vs. FedTPG and FedCoOp baselines to validate implementation.
  2. **Communication vs. accuracy sweep:** Vary S ∈ {1, 3, 5, C} and Ms ∈ {1, 2, 4} while holding other parameters fixed. Plot cumulative bytes transmitted vs. final accuracy to internalize the tradeoff surface shown in Fig. 2 and Table V.
  3. **Convergence and consistency check:** Run 3 random seeds for N=10 vs. N=20 classes-per-client on a heterogeneous multi-dataset setup (e.g., 3 datasets, 10 clients). Record per-round average accuracy and per-client std dev to reproduce Table III's convergence behavior and confirm your frequency-weighted selector is functioning.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can prompt-sharing mechanisms be made adaptive, automatically adjusting the number and content of exchanged prompts based on dataset properties, task complexity, and communication constraints?
- **Basis in paper:** [explicit] The authors state: "we aim to explore dynamic prompt-sharing mechanisms, where the number of exchanged prompts is automatically adjusted based on dataset properties, learning objectives, and communication constraints."
- **Why unresolved:** Current work uses fixed configurations (M=4 prompts, S=5 recipients); the optimal exchange strategy likely varies across domains and network conditions.
- **What evidence would resolve it:** A mechanism that dynamically tunes M_s during training and demonstrates improved accuracy-efficiency trade-offs across diverse datasets and network topologies.

### Open Question 2
- **Question:** Can maintaining a specialized subset of prompts locally at each client while exchanging only general prompts improve the balance between personalization and generalization?
- **Basis in paper:** [explicit] The authors explicitly state this as future work: "investigating how to maintain a specialized subset of prompts locally at each client while exchanging more general prompts throughout the federation."
- **Why unresolved:** Current approach treats all prompts uniformly for exchange; some datasets showed better performance with partial exchange, suggesting local specialization may be beneficial.
- **What evidence would resolve it:** A comparative study showing that separating local vs. shared prompts improves per-client accuracy on heterogeneous data without sacrificing generalization.

### Open Question 3
- **Question:** How can sparse prompt distribution be addressed in decentralized federated learning to ensure reliable convergence when some clients receive few or no updates?
- **Basis in paper:** [explicit] The paper notes: "Addressing the potential challenges of sparse prompt distribution lies outside the scope of this work but represents a promising direction for future research."
- **Why unresolved:** The best-case scenario (minimal communication) transmits only 807 MB but risks uneven knowledge propagation; balancing sparsity with convergence remains unexplored.
- **What evidence would resolve it:** Robustness experiments showing convergence under varying sparsity levels, or mechanisms guaranteeing minimum update frequency per client.

### Open Question 4
- **Question:** What formal privacy guarantees does ZeroDFL provide in decentralized settings with potential adversarial clients?
- **Basis in paper:** [inferred] The paper claims ZeroDFL "enhances privacy and security" and that "text-based prompts inherently limits the exposure of sensitive information," but provides no formal analysis (e.g., differential privacy bounds) against membership inference or reconstruction attacks.
- **Why unresolved:** Eliminating the central server removes one attack vector but introduces peer-to-peer exposure; empirical or theoretical privacy analysis is absent.
- **What evidence would resolve it:** Formal differential privacy analysis, or empirical evaluation against membership inference attacks showing privacy-utility trade-offs.

## Limitations

- The method assumes full or near-full network connectivity; performance under sparse or partitioned topologies is not evaluated.
- No formal privacy analysis is provided despite claims of enhanced security; vulnerability to adversarial clients remains unexplored.
- Critical hyperparameters (optimizer, learning rate, temperature τ, initialization) are not specified, potentially affecting reproducibility.

## Confidence

- **High confidence:** The mechanism of prompt learning on frozen CLIP for zero-shot adaptation is well-supported and empirically validated across 9 datasets.
- **Medium confidence:** The weighted selection strategy improves knowledge balance, but its robustness to network partitions or malicious clients is not tested.
- **Medium confidence:** Diversity-prioritized prompt sampling aids generalization, though the exact sampling heuristic could be more precisely specified.

## Next Checks

1. **Hyperparameter sensitivity test:** Vary optimizer, learning rate, and prompt initialization; measure impact on final accuracy and convergence speed to identify critical unreported design choices.
2. **Robustness to network partitions:** Simulate sparse or partitioned topologies; verify whether weighted selection still balances knowledge or if convergence stalls for isolated clients.
3. **Prompt pool diversity audit:** Log prompt source distributions over rounds; quantify diversity decay and its correlation with accuracy drop when Ms < M.