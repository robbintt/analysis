---
ver: rpa2
title: 'Elastic Weight Consolidation for Knowledge Graph Continual Learning: An Empirical
  Evaluation'
arxiv_id: '2512.01890'
source_url: https://arxiv.org/abs/2512.01890
tags:
- learning
- task
- forgetting
- tasks
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated Elastic Weight Consolidation (EWC) for continual
  learning in knowledge graphs using TransE embeddings on FB15k-237. EWC protects
  important parameters learned in previous tasks by adding a regularization penalty
  based on the Fisher Information Matrix, enabling new-task learning while preserving
  prior knowledge.
---

# Elastic Weight Consolidation for Knowledge Graph Continual Learning: An Empirical Evaluation

## Quick Facts
- arXiv ID: 2512.01890
- Source URL: https://arxiv.org/abs/2512.01890
- Authors: Gaganpreet Jhajj; Fuhua Lin
- Reference count: 40
- Primary result: EWC reduces catastrophic forgetting from 12.62% to 6.85% (45.7% reduction) on FB15k-237 with relation-based task partitioning

## Executive Summary
This study evaluates Elastic Weight Consolidation (EWC) for continual learning in knowledge graphs using TransE embeddings on FB15k-237. EWC protects important parameters learned in previous tasks by adding a regularization penalty based on the Fisher Information Matrix, enabling new-task learning while preserving prior knowledge. Experiments with five random seeds showed EWC reduced catastrophic forgetting from 12.62% to 6.85% compared to naive sequential training. Notably, task partitioning strategy significantly affected forgetting: relation-based partitioning caused 9.8 percentage points higher forgetting than random partitioning, highlighting that evaluation protocol design influences continual learning metrics.

## Method Summary
The study evaluates EWC on FB15k-237 (14,505 entities, 237 relations, 272,115 training triples) using TransE embeddings (50-dim) with margin-based ranking loss. Two task partitioning strategies were used: relation-based (round-robin assignment by relation frequency into 4 tasks) and random (shuffle and split). EWC regularization was applied with Fisher diagonal computed from all previous task triples. Five random seeds (42, 123, 456, 789, 2024) were used. Evaluation metrics included filtered MRR and forgetting rate (MRR_post_task − MRR_final). The Fisher Information Matrix was computed using mean squared gradients across all batches of previous tasks, with regularization strength λ ∈ {0.1, 1.0, 10.0}.

## Key Results
- EWC reduced catastrophic forgetting from 12.62% to 6.85% (45.7% reduction) compared to naive training on relation-based partitioning
- Task partitioning strategy significantly impacted forgetting: relation-based caused 9.8 percentage points higher forgetting than random partitioning
- EWC outperformed replay baselines, particularly when replay buffer was limited to 500 examples per task
- Optimal regularization strength depended on task structure: λ=10 best for relation-based, but worse than naive for random partitioning

## Why This Works (Mechanism)

### Mechanism 1: Fisher Information-Based Parameter Importance Estimation
EWC reduces catastrophic forgetting by identifying and protecting parameters critical to previously learned tasks through Fisher Information Matrix diagonal approximation. After training on task i-1, EWC computes Fk (diagonal Fisher values) by accumulating squared gradients across all training batches. During task i training, a quadratic penalty λ/2 Σ Fk(θk - θ*k,i-1)² constrains important parameters from deviating far from their optimal previous-task values. The core assumption is that parameters with larger gradient contributions are more important for encoding task knowledge, with the Laplace approximation reasonably capturing parameter importance.

### Mechanism 2: Task Partitioning Determines Distribution Shift Magnitude
The magnitude of catastrophic forgetting is significantly influenced by how tasks are partitioned, independent of the learning algorithm. Relation-based partitioning groups triples by relation type, creating task coherence—each task focuses on ~59 distinct relations. This induces large distribution shifts when transitioning between tasks. Random partitioning distributes all relation types across tasks, creating natural overlap that acts as implicit regularization. The core assumption is that forgetting correlates with distribution shift magnitude between sequential tasks, with relation types being meaningful semantic boundaries in KG embeddings.

### Mechanism 3: Regularization Outperforms Replay for Limited Memory Regimes
Principled parameter protection via Fisher-based regularization is more effective than simple replay for KG continual learning when buffer size is limited. Replay methods store exemplars and revisit them, but without explicit parameter protection, gradient updates from replay samples can still interfere with learned representations. EWC explicitly constrains important parameter directions, preventing interference regardless of what samples are seen. The core assumption is that the Fisher diagonal adequately captures parameter importance when the replay buffer is small relative to task data.

## Foundational Learning

- **Fisher Information Matrix**
  - Why needed here: EWC uses the diagonal approximation of Fisher Information to estimate how much each parameter contributes to the loss; understanding this connects regularization strength to parameter importance.
  - Quick check question: Given a parameter θk with Fisher value Fk = 0.001 vs Fk = 10.0, which would EWC protect more strongly during subsequent task training?

- **Knowledge Graph Embeddings (TransE)**
  - Why needed here: The paper evaluates EWC on TransE embeddings where relations are translations (h + r ≈ t); the structure of the embedding space affects which parameters matter for which relations.
  - Quick check question: In TransE, if you update entity embeddings to learn a new relation type, how might this disrupt previously learned relation patterns that share those entities?

- **Catastrophic Forgetting Metrics (Forgetting Rate)**
  - Why needed here: The paper measures forgetting as MRR degradation from post-task peak performance; understanding this metric is essential for interpreting the 12.62% → 6.85% reduction claim.
  - Quick check question: If a model achieves MRR = 0.30 on Task 1 immediately after training, and MRR = 0.27 after learning Task 2, what is the forgetting F¹₂?

## Architecture Onboarding

- **Component map**: Task Partitioner -> TransE Model -> Fisher Computer -> EWC Regularizer -> Evaluator
- **Critical path**: 1) Train on Task 1 → save optimal parameters θ*₁ 2) Compute Fisher F₁ using Task 1 data 3) Train on Task 2 with loss = L₂ + λ/2 Σ F₁,k(θk - θ*₁,k)² 4) Compute Fisher F₂, combine with F₁ for Task 3, etc. 5) Evaluate MRR on all tasks after each training stage
- **Design tradeoffs**: λ selection (λ=10 minimizes forgetting for relation-based but increases it for random), partitioning strategy (Relation-based = harder benchmark, more realistic; Random = easier), Fisher computation (all triples vs subsample)
- **Failure signatures**: Forgetting increases with λ (random partitioning) indicates over-regularization; replay worse than naive suggests buffer too small; high variance across seeds indicates sensitivity
- **First 3 experiments**: 1) Reproduce naive vs EWC (λ=10) on relation-based partitioning with 5 seeds to confirm 12.62% → 6.85% forgetting reduction 2) Sweep λ ∈ {0.1, 1, 5, 10, 50} on both partitioning strategies to verify optimal λ differs by task construction 3) Increase replay buffer size (500 → 2000 → 5000) to test hypothesis that replay fails due to limited buffer capacity

## Open Questions the Paper Calls Out

### Open Question 1
Does EWC effectively mitigate catastrophic forgetting across more complex KG embedding architectures (RotatE, ComplEx, TuckER) and diverse datasets (WN18RR, YAGO, Wikidata)? The authors state results may not generalize to complex embedding methods and other datasets, calling for evaluation across multiple embedding methods and datasets to assess generalizability.

### Open Question 2
What is the formal relationship between task partitioning strategies (relation-based, entity-based, domain-based) and catastrophic forgetting magnitude in KG continual learning? The authors note that alternative strategies warrant investigation and call for systematic investigation to formalize the relationship between task partitioning and forgetting.

### Open Question 3
Does EWC maintain its effectiveness over extended task sequences (10+ tasks), or does regularization accumulate and over-constrain learning? The authors note the limitation to four tasks and state that scaling studies with 10+ tasks would reveal the long-term dynamics of continual learning.

### Open Question 4
Why does optimal regularization strength depend inversely on task overlap—stronger λ benefits relation-based partitions while weaker λ benefits random partitions? The authors hypothesize about over-constraint but do not resolve the mechanism explaining why λ=10 minimizes forgetting for relation-based but maximizes it for random partitioning.

## Limitations

- The Fisher diagonal computation details are underspecified, particularly gradient accumulation methods
- Study focuses on FB15k-237 with TransE, limiting generalizability to other KGs or embedding methods
- Does not explore sensitivity to buffer size for replay methods or alternative negative sampling strategies

## Confidence

- **High Confidence**: EWC reduces catastrophic forgetting compared to naive training (45.7% reduction confirmed across five seeds)
- **Medium Confidence**: Relation-based partitioning causes significantly higher forgetting than random partitioning (9.8 percentage point difference observed)
- **Medium Confidence**: EWC outperforms replay with limited buffer size (though replay performance degrades sharply with smaller buffers)

## Next Checks

1. Implement the Fisher diagonal computation with both per-sample and per-batch gradient accumulation to determine which matches the reported results
2. Conduct a systematic sweep of replay buffer sizes (500, 1000, 2000, 5000 examples) to quantify the buffer capacity threshold where replay matches EWC performance
3. Test alternative negative sampling strategies (e.g., entity-frequency based, adversarial) to determine if the observed forgetting patterns are sampling-method dependent