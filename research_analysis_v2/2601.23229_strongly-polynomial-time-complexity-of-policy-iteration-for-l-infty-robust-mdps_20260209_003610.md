---
ver: rpa2
title: Strongly Polynomial Time Complexity of Policy Iteration for $L_\infty$ Robust
  MDPs
arxiv_id: '2601.23229'
source_url: https://arxiv.org/abs/2601.23229
tags:
- policy
- lemma
- polynomial
- algorithm
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the problem of determining whether policy\
  \ iteration algorithms for robust Markov decision processes (RMDPs) with L\u221E\
  \ uncertainty sets run in strongly polynomial time, which is a fundamental open\
  \ question in the study of sequential decision making under uncertainty. The core\
  \ method idea involves analyzing the robust policy iteration algorithm and establishing\
  \ a potential function that tracks the effects of changing the optimal policy within\
  \ the uncertainty set."
---

# Strongly Polynomial Time Complexity of Policy Iteration for $L_\infty$ Robust MDPs

## Quick Facts
- **arXiv ID:** 2601.23229
- **Source URL:** https://arxiv.org/abs/2601.23229
- **Reference count:** 40
- **Primary result:** The robust policy iteration algorithm for $(s,a)$-rectangular $L_\infty$ RMDPs with constant discount factor runs in strongly polynomial time

## Executive Summary
This paper resolves a fundamental open question about the computational complexity of policy iteration for robust Markov decision processes with $L_\infty$ uncertainty sets. The authors establish that the robust policy iteration algorithm terminates in strongly polynomial time, meaning the runtime depends only on the problem size (number of states and actions) and not on the numerical precision of the input data. This is achieved through a novel combinatorial analysis of the most significant bits in binary representations of probability differences, combined with a specialized homotopy algorithm for the policy improvement step that replaces standard linear programming solvers.

## Method Summary
The authors analyze the robust policy iteration algorithm by introducing a potential function that tracks the discrepancy between current and optimal policies within the uncertainty set. They prove that this potential must decrease exponentially fast by combining geometric contraction arguments with a novel combinatorial bound on the number of distinct scales the probability differences can occupy. For the policy improvement step, they replace the standard LP solver with a specialized homotopy algorithm that efficiently handles the $L_\infty$ uncertainty structure by sorting states and shifting probability mass within box constraints.

## Key Results
- The robust policy iteration algorithm terminates in $O(n^4 \log n \cdot \log((1-\gamma)/n^2)/\log\gamma)$ iterations for RMDPs with $n$ states and constant discount factor $\gamma$
- The algorithm runs in $O(n \cdot m \cdot \log(1-\gamma)/\log\gamma)$ iterations for RMDPs with $n$ states and $m$ actions
- The policy improvement step runs in $O(n \log n)$ time per iteration using the homotopy method
- These results establish strong polynomiality, showing runtime depends only on problem size, not numerical precision

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** With constant discount factor, RPI terminates in strongly polynomial time because probability discrepancy between current and optimal policy has limited distinct scales before vanishing.
- **Mechanism:** The authors map optimization to a combinatorial problem on bits, defining a potential function based on transition matrix differences. They prove this potential decreases by factor of 2 every $L$ steps, and that MSBs of probability differences can only change $O(n \log n)$ times.
- **Core assumption:** Probability differences arise from linear combinations of nominal probabilities and uncertainty radii (finite set $X$).
- **Evidence anchors:** Abstract mentions "novel combinatorial result over number of most significant bits"; section 4 combines "Geometric Contraction" with "Combinatorial Finiteness" to bound iterations.
- **Break condition:** Mechanism fails if discount factor $\gamma$ is not constant, as bound scales with $1/(1-\gamma)$.

### Mechanism 2
- **Claim:** Policy improvement step runs in strongly polynomial time per iteration, specifically $O(n \log n)$, avoiding bit-length dependency of standard LP.
- **Mechanism:** The paper uses a "homotopy algorithm" instead of LP solver. For $L_\infty$ balls, optimal response involves shifting probability mass from donor to receiver states. The algorithm uses two-pointer approach to maximize inner product subject to box constraints.
- **Core assumption:** Uncertainty set is $(s,a)$-rectangular, allowing environment's optimization to be solved independently per state-action pair.
- **Evidence anchors:** Section 3 describes "Algorithm 2... proceeds by adapting a two-pointer technique"; page 5 contrasts this with LP dependent on bit precision.
- **Break condition:** If uncertainty set were not rectangular or not $L_\infty$, homotopy method might not apply, reverting to NP-hard or bit-dependent LP solutions.

### Mechanism 3
- **Claim:** Algorithm cannot iterate indefinitely without significant progress because "value error" contracts exponentially, forcing "potential" to drop.
- **Mechanism:** Proof-by-contradiction strategy. Authors establish lower bound on value error using potential function and upper bound based on same potential. If potential remains high too long, lower bound contradicts exponential convergence guaranteed by discount factor, forcing potential to decrease.
- **Core assumption:** Bellman operator is contraction mapping (standard for discounted MDPs).
- **Evidence anchors:** Section 4 establishes relationship between value contraction and potential reduction; page 10 shows "This conclusion... directly contradicts the premise."
- **Break condition:** Failure of contraction property (e.g., undiscounted/average reward settings) would break geometric convergence argument used to force potential down.

## Foundational Learning

- **Concept:** Strongly Polynomial Time
  - **Why needed here:** Main contribution distinguishes runtime depending only on $n$ and $m$ vs. runtime depending on bit-size of numbers.
  - **Quick check question:** If I double precision of transition probabilities (bits), should strongly polynomial algorithm take longer?

- **Concept:** $(s,a)$-Rectangular Uncertainty
  - **Why needed here:** Structural assumption allows complex robust optimization problem to decompose into manageable sub-problems solvable via homotopy method.
  - **Quick check question:** Does environment choose transition uncertainty before or after seeing agent's action in this model?

- **Concept:** Potential Function Analysis
  - **Why needed here:** Standard convergence proofs show "it gets closer," but this paper uses potential function to show "it gets closer at a specific measurable rate relative to probability mass transfers."
  - **Quick check question:** In this paper, does potential function measure value directly or "mass transfer" discrepancy between policies?

## Architecture Onboarding

- **Component map:** RPI Loop (Algorithm 1/3) -> Policy Evaluator -> Homotopy Improver
- **Critical path:** The Homotopy Improver is architectural novelty ensuring strong polynomiality per step; the Potential Function logic is analytical tool proving global convergence bounds.
- **Design tradeoffs:** Homotopy method is highly efficient ($O(n \log n)$) but strictly valid for $L_\infty$ balls. If need $L_2$ constraints, might need to revert to slower QP/LP solvers, losing strong polynomial guarantee. Analysis relies on $\gamma$ being constant; implementations for $\gamma \to 1$ will degrade predictably.
- **Failure signatures:** Non-rectangular sets cause decomposition to fail; numerical instability may obscure "potential drop" if precision is insufficient.
- **First 3 experiments:**
  1. **Scalability Run:** Generate random RMDPs with increasing $n$ and fixed $\gamma$. Plot iterations vs. $n$ to verify $O(n^4 \log n)$ scaling trend.
  2. **Homotopy vs. LP:** Benchmark Homotopy Algorithm 2 against standard LP solver for policy improvement step. Verify speedup and insensitivity to bit-precision.
  3. **Stress Test $\gamma$:** Run algorithm with $\gamma$ approaching 1 (e.g., 0.99, 0.999). Observe iteration count explosion predicted by $\log(1-\gamma)$ term in bounds.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does policy iteration for standard MDPs admit polynomial or strongly polynomial time bounds for all variants?
- **Basis in paper:** [explicit] Conclusion states that while one variant is known to take exponential time, it remains a "related major question" whether other variants run in polynomial time.
- **Why unresolved:** Existing lower bounds apply only to specific variants, and strongly polynomial results often rely on fixed discount factors or specific algorithmic structures not yet generalized.
- **What evidence would resolve it:** Proof of polynomial upper bound for general policy iteration variant or specific exponential lower bound construction for standard variant.

### Open Question 2
- **Question:** Can the combinatorial bound in Theorem 20 regarding degree of dyadic intervals be improved from $O(|X| \log |X|)$ to $O(|X|)$?
- **Basis in paper:** [explicit] In Appendix G, authors explicitly ask: "Can the bound in Theorem 20 be improved to $|Dyad(A(X))|=O(|X|)$?"
- **Why unresolved:** Current proof technique relies on Siegel's lemma which introduces logarithmic factor; tighter linear bound requires different mathematical approach.
- **What evidence would resolve it:** Theorem proving linear upper bound or construction of set $X$ requiring $\Omega(|X| \log |X|)$ intervals.

### Open Question 3
- **Question:** Can the potential function defined in this work be used to design improved stopping criteria or practical variants for robust policy iteration?
- **Basis in paper:** [explicit] Authors list exploring whether "potential-based arguments suggest improved practical variants or stopping criteria" as interesting future direction.
- **Why unresolved:** Current work focuses on worst-case theoretical complexity rather than optimizing for average-case speed or early stopping heuristics.
- **What evidence would resolve it:** Algorithm variant incorporating potential function that demonstrably reduces iteration count in empirical studies on data-driven uncertainty sets.

## Limitations
- Analysis critically depends on $(s,a)$-rectangular structure of $L_\infty$ uncertainty set, restricting applicability to settings where adversary's uncertainty can be decomposed per state-action pair
- Combinatorial bound on MSBs relies on assumption of finite, discrete probability values; practical implementations with floating-point arithmetic may face numerical precision challenges
- Behavior when $\gamma \to 1$ is theoretically bounded but may face practical numerical challenges not fully explored

## Confidence

- **High Confidence:** The claim that policy iteration terminates in strongly polynomial time for $(s,a)$-rectangular $L_\infty$ RMDPs with constant discount factor is well-supported by novel combinatorial analysis and potential function arguments
- **Medium Confidence:** Specific iteration bounds are derived rigorously, but empirical validation across diverse RMDP instances would strengthen confidence
- **Low Confidence:** Behavior when $\gamma \to 1$ is theoretically bounded but may face practical numerical challenges not fully explored

## Next Checks

1. **Numerical Precision Test:** Implement algorithm with varying floating-point precisions (32-bit vs 64-bit) and measure whether MSB-based potential function still decreases predictably, or if numerical errors accumulate

2. **Non-Rectangular Comparison:** Construct small $(s,a)$-rectangular and non-rectangular $L_\infty$ RMDP with same nominal parameters. Compare policy iteration performance and verify whether non-rectangular version indeed degrades to LP-based solutions with worse complexity

3. **Discount Factor Stress Test:** Generate family of RMDPs with increasing $n$ and systematically vary $\gamma$ from 0.5 to 0.999. Empirically measure iteration counts and verify they align with predicted $\log(1-\gamma)$ scaling, particularly for $\gamma$ approaching 1