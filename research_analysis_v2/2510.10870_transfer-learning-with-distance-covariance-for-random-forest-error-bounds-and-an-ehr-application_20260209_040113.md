---
ver: rpa2
title: 'Transfer Learning with Distance Covariance for Random Forest: Error Bounds
  and an EHR Application'
arxiv_id: '2510.10870'
source_url: https://arxiv.org/abs/2510.10870
tags:
- random
- forest
- learning
- transfer
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a transfer learning framework for random forest
  regression using distance covariance-based feature weighting. The method assumes
  the regression functions differ between source and target domains for only a few
  features (sparsely different).
---

# Transfer Learning with Distance Covariance for Random Forest: Error Bounds and an EHR Application

## Quick Facts
- arXiv ID: 2510.10870
- Source URL: https://arxiv.org/abs/2510.10870
- Reference count: 40
- Authors: Chenze Li; Subhadeep Paul
- Primary result: Distance covariance-based transfer learning for random forest regression shows theoretical error bounds and practical improvements when source data is much larger than target data and domain differences are sparse

## Executive Summary
This paper introduces a transfer learning framework for random forest regression that leverages distance covariance to identify features responsible for domain shift. The method assumes the source and target regression functions differ only on a sparse subset of features. It trains a centered random forest on source data, computes residuals on target data, and fits a second centered random forest to these residuals using feature splitting probabilities weighted by distance covariance. The authors provide theoretical error bounds showing the method's benefits when the source domain has substantially larger sample size than the target domain. The approach is validated through simulations and an electronic health record application predicting ICU mortality.

## Method Summary
The method involves four key steps: (1) train a centered random forest (CRF) on source domain data using uniform feature weights, (2) compute residuals in the target domain by subtracting source predictions from actual target values, (3) split target data into two parts - estimate distance covariance between residuals and features on one part, then train a second CRF on the other part using feature weights proportional to these distance covariances, and (4) combine predictions by adding the source and residual predictions. The theoretical analysis provides non-asymptotic error bounds for the CRF variant, while simulations also demonstrate performance with the standard random forest variant.

## Key Results
- The transfer learning approach improves prediction accuracy over target-only random forest when source sample size is much larger than target sample size (n_s >> n_t)
- Theoretical error bounds show benefits specifically when the difference function is sparse (affects only l << d features)
- Simulation results demonstrate significant performance gains, particularly when the domain difference involves only a small subset of features
- Application to eICU dataset shows substantial improvements for smaller hospitals (fewer than 100 patients) by leveraging data from larger hospitals

## Why This Works (Mechanism)

### Mechanism 1
Distance covariance-based feature weighting enables "soft feature screening" to identify features responsible for domain shift. The algorithm computes sample distance covariance between each feature and the residuals from the source model applied to target data. Features with higher covariance receive higher splitting probabilities in the residual random forest, effectively downweighting features unrelated to the domain difference. This relies on the assumption that features independent of the difference function R(x) = f_t(x) - f_s(x) have small population distance covariance with residuals.

### Mechanism 2
Fitting a second CRF to residuals (rather than directly fine-tuning the source model) isolates domain-specific corrections while preserving transferred knowledge. The source CRF captures structure shared across domains, while the residual CRF learns only the correction term R(x). Final predictions are additive: Ŷ_final = Ŷ_source + Ŷ_residual. This approach assumes the difference function R(x) is L-Lipschitz and involves only l << d features.

### Mechanism 3
Sample splitting between distance covariance estimation and residual CRF training prevents overfitting and ensures weights approximate population quantities. Target data is divided into two parts - distance covariances are estimated on one split, while the residual CRF is trained on the other using those weights. This procedure is necessary for the theoretical guarantees but may reduce effective training data when target sample size is small.

## Foundational Learning

**Centered Random Forest (CRF)**: A random forest variant where splits are made at random midpoints rather than using data-driven criteria like information gain. This simplifies theoretical analysis because the splits are not data-dependent. *Why needed*: Theoretical analysis is tractable for CRF due to its random nature, while standard RF's greedy splits complicate error bounds.

**Distance Covariance (dCov)**: A measure of dependence between random vectors that detects arbitrary non-linear relationships, unlike Pearson correlation which only captures linear dependence. *Why needed*: dCov can capture complex non-linear relationships between features and residuals, identifying which features are responsible for domain shift. *Quick check*: What is the key advantage of distance covariance over standard correlation for detecting feature-response relationships?

**Transfer Learning with Posterior Drift**: A transfer learning setting where the conditional distribution P(Y|X) differs between domains, but the difference is assumed to be sparse. *Why needed*: This distinguishes the framework from covariate shift settings where only P(X) changes. *Quick check*: How does the "sparsely different" assumption in this paper differ from assuming only covariate shift?

## Architecture Onboarding

**Component map**: Source data (n_s large) → CRF with uniform weights → Ŷ_source → Residuals = Y_t - Ŷ_source ← Distance covariances on split target data ← Target data split into D̃'_t, D̃''_t → Residual CRF on D̃'_t with weights ∝ dCov → Final: Ŷ_final = Ŷ_source + Ŷ_residual

**Critical path**: Quality of distance covariance estimates → accuracy of feature weights → convergence rate of residual CRF. If dCov estimates are noisy, weights may misallocate splitting probability and harm performance.

**Design tradeoffs**: Larger D̃''_t (dCov estimation) improves weight estimates but reduces data for residual CRF training; deeper tree depth k_n reduces bias but increases variance; CRF has theoretical guarantees while SRF performs better empirically but lacks proofs.

**Failure signatures**: MSE increases with transfer learning when discrepancy ratio r > 0.3; high variance across runs when n_t < 100 due to insufficient data for sample splitting; transfer learning underperforms target-only RF when source sample n_s is not much larger than n_t.

**First 3 experiments**: 1) Discrepancy ratio sweep: Fix n_s=20000, n_t=500, d=50; vary r ∈ {0.1, 0.15, 0.2, 0.3} to validate sparse-difference assumption; 2) Target sample size sensitivity: Fix n_s=10000, d=50, r=0.1; vary n_t ∈ {50, 100, 200, 500, 1000} to identify minimum viable target sample size; 3) Ablate distance covariance weighting: Compare TLCRF with dCov weights vs. TLCRF with uniform weights vs. target-only CRF to isolate contribution of feature weighting.

## Open Questions the Paper Calls Out

**Open Question 1**: Can the theoretical error bounds derived for the centered random forest (CRF) be extended to the standard random forest (SRF) with data-driven splits? The authors note that while SRF performs better empirically, the statistical theoretical analysis of the full method is quite difficult.

**Open Question 2**: How does the performance of the transfer learning procedure degrade if the feature distribution is non-uniform? The theoretical analysis explicitly restricts predictors X to be uniformly distributed on [0,1]^d, a standard but restrictive condition.

**Open Question 3**: Is the assumption that the difference function is independent of the inactive features necessary for the distance covariance weighting to be effective? Lemma 1 proves distance covariance between residuals and features is small only if the feature is independent of the difference function R(X).

## Limitations

- The primary limitation is the assumption that regression functions differ only on a sparse subset of features (l << d), which may not hold in many real-world transfer learning scenarios where domain shifts affect many features.
- The sample splitting procedure, while necessary for theoretical guarantees, may be suboptimal in practice, particularly when target sample sizes are small.
- For the standard random forest variant (SRF) which reportedly performs better empirically, theoretical guarantees are not yet established.

## Confidence

- **High Confidence**: The theoretical error bounds (Theorems 1-4) and their derivations, assuming the stated assumptions hold
- **Medium Confidence**: The simulation results showing performance improvements under sparse difference conditions
- **Medium Confidence**: The eICU application demonstrating practical benefits for smaller hospitals
- **Low Confidence**: Generalization to non-sparse domain differences or when source sample size is not substantially larger than target

## Next Checks

1. **Discrepancy ratio sensitivity**: Systematically vary r (fraction of features with different regression functions) to identify the threshold beyond which transfer learning provides no benefit or degrades performance.

2. **Target sample size threshold**: Determine the minimum target sample size required for the method to outperform target-only random forest, considering the trade-off between adaptive weighting benefits and reduced training data from sample splitting.

3. **Non-sparse difference stress test**: Evaluate performance when the difference function involves a substantial fraction of features (r > 0.3) to validate the method's limitations and identify failure modes.