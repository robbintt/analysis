---
ver: rpa2
title: 'Stochastic Actor-Critic: Mitigating Overestimation via Temporal Aleatoric
  Uncertainty'
arxiv_id: '2601.00737'
source_url: https://arxiv.org/abs/2601.00737
tags:
- learning
- critic
- overestimation
- uncertainty
- actor-critic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses overestimation bias in off-policy actor-critic\
  \ reinforcement learning, where critic networks systematically overestimate value\
  \ estimates, leading to training instability. The core method introduces Stochastic\
  \ Actor-Critic (STAC), which uses temporal aleatoric uncertainty\u2014arising from\
  \ stochastic transitions, rewards, and policy-induced variability in Bellman targets\u2014\
  to scale pessimistic updates, rather than relying on epistemic uncertainty."
---

# Stochastic Actor-Critic: Mitigating Overestimation via Temporal Aleatoric Uncertainty

## Quick Facts
- arXiv ID: 2601.00737
- Source URL: https://arxiv.org/abs/2601.00737
- Authors: Uğurcan Özalp
- Reference count: 24
- Primary result: Temporal aleatoric uncertainty alone suffices to mitigate overestimation bias, with dropout improving stability; achieves competitive performance with fewer computational resources than double-critic methods.

## Executive Summary
This paper addresses overestimation bias in off-policy actor-critic reinforcement learning by introducing Stochastic Actor-Critic (STAC), which uses temporal aleatoric uncertainty to scale pessimistic updates. Rather than relying on epistemic uncertainty or critic ensembles, STAC employs a single distributional critic network to model variance induced by stochastic transitions, rewards, and policy-induced variability in Bellman targets. The method applies pessimism (μ - βσ) to both critic TD targets and actor policy improvement objectives, maintaining Bellman consistency. Dropout regularization on both networks further improves training stability. STAC achieves competitive performance with state-of-the-art methods while requiring fewer computational resources, making it promising for real-world RL applications.

## Method Summary
STAC extends SAC by using a single distributional critic that outputs Gaussian parameters (μ, σ²) per state-action pair. The TD target incorporates pessimism: r + γ(μ_θ̄(s',ã') - βσ_θ̄(s',ã') - α log π_φ(ã'|s')), and the actor objective mirrors this: E[μ_θ(s,a) - βσ_θ(s,a) - α log π_φ(a|s)]. Both critic and actor networks include dropout (0.01 rate) after each hidden layer with layer normalization. The critic is trained via negative log-likelihood loss on the pessimistic TD target. β is a hyperparameter tuned per environment, ranging from 0.0 to 0.5. The method relies on the theoretical result that temporal aleatoric uncertainty alone can bound overestimation without requiring epistemic uncertainty estimates.

## Key Results
- Temporal aleatoric uncertainty modeling suffices to mitigate overestimation bias, with pessimism based on distributional critic alone preventing overestimation in stochastic environments
- Unified pessimism applied to both critic and actor maintains Bellman consistency and prevents actor exploitation of overestimated values
- Actor dropout consistently improves performance across environments, while critic dropout is required for convergence in some cases (e.g., Hopper-v4)
- STAC achieves competitive performance with state-of-the-art double-critic methods while using fewer computational resources (single critic)

## Why This Works (Mechanism)

### Mechanism 1: Temporal Aleatoric Uncertainty as Overestimation Source
If overestimation stems from one-step variability in Bellman targets (transitions, rewards, policy-induced), then modeling this temporal aleatoric uncertainty and applying pessimism should suffice for bias mitigation. A single distributional critic outputs (μ, σ²), and the TD target becomes μ - βσ. Theorem 3.2 provides an overestimation bound ϵ(s,a) ≤ (γ/2ᾱ)E[max_a' σ²(s',a')], and Corollary 3.1.1 establishes β ≥ max(½ᾱ⁻¹σ) prevents overestimation. This targets bootstrap error propagation. The core assumption is that critic distributions are approximately sub-Gaussian with bounded support, and policy improvement dynamics induce irreducible target variability even in deterministic environments.

### Mechanism 2: Unified Pessimism for Consistency Between Evaluation and Improvement
Applying the same pessimistic objective to both critic (policy evaluation) and actor (policy improvement) maintains Bellman consistency—T*Qᵏ = T^π^{k+1} Qᵏ—which prevents the actor from exploiting overestimated regions the critic has not corrected. The actor objective mirrors the critic's pessimistic TD target, ensuring policy updates do not chase values the critic considers unreliable. The core assumption is that the policy improvement step is sufficiently deterministic given a fixed critic that the same β is appropriate for both updates.

### Mechanism 3: Dropout as Regularization for Single-Critic Stability
Since STAC uses a single critic, it is more susceptible to overfitting and noise amplification; dropout on both networks provides implicit regularization and improves optimization stability. Dropout (rate 0.01) is applied after each hidden layer in critic and actor. For the critic, this reduces overfitting to noisy TD targets; for the actor, it stabilizes policy gradient variance. Layer normalization further prevents numerical instabilities. The core assumption is that dropout's regularization benefit outweighs any potential interference with the distributional critic's variance modeling.

## Foundational Learning

- **Concept: Aleatoric vs. Epistemic Uncertainty**
  - Why needed: STAC's core claim hinges on using aleatoric (data-inherent, irreducible) rather than epistemic (model-ignorance, reducible) uncertainty. Without this distinction, the mechanism for pessimism scaling is unclear.
  - Quick check: Given a deterministic environment with a stochastic policy, which uncertainty type appears in the Bellman target? (Answer: Aleatoric—from policy-induced variability in action sampling)

- **Concept: Distributional RL / Bellman Operators**
  - Why needed: The paper extends standard Q-learning to distributional Q, where Z(s,a) or Q(s,a) are random variables. Understanding T^π and T* as operators on distributions (not scalars) is prerequisite to following Theorem 3.1.
  - Quick check: If Q(s,a) ~ N(μ, σ²), what is T*Q(s,a) under deterministic transitions? (Answer: Still a distribution; Theorem 3.1 shows it's bounded by μ + σ²/2ᾱ terms)

- **Concept: Overestimation Bias in Actor-Critic**
  - Why needed: The problem STAC addresses; without understanding why critics overestimate (function approximation + bootstrapping + maximization), the solution lacks context.
  - Quick check: Why does min-double-Q reduce overestimation in SAC, and what tradeoff does STAC make by avoiding it? (Answer: Min reduces upward bias but requires twice the compute; STAC trades compute for a single distributional critic with pessimism)

## Architecture Onboarding

- **Component map:**
  - State/Action → Critic Q_θ (2-layer MLP, 256 units, dropout+LayerNorm, outputs μ,σ²) → TD Target
  - TD Target → Critic Q_θ (via NLL loss)
  - State → Actor π_φ (2-layer MLP, 256 units, dropout+LayerNorm, outputs tanh-Gaussian) → Policy Gradient
  - Policy Gradient + TD Target → Actor π_φ (via pessimistic objective)
  - Target Critic Q_θ̄ (Polyak-averaged ρ=0.995) → TD Target computation

- **Critical path:**
  1. Sample batch from replay buffer
  2. Compute TD target: Q_TD = r + γ(μ_θ̄(s',ã') - βσ_θ̄(s',ã') - α log π_φ(ã'|s'))
  3. Update critic via negative log-likelihood loss: -log Q_θ(Q_TD | s,a)
  4. Update actor via E[μ_θ(s,a) - βσ_θ(s,a) - α log π_φ(a|s)]
  5. Update α via entropy constraint loss
  6. Polyak update θ̄ ← ρθ̄ + (1-ρ)θ

- **Design tradeoffs:**
  - β is environment-specific (Table 4: ranges from 0.0 to 0.5); no adaptive scheme is provided—requires tuning
  - Single critic saves ~2× memory/compute vs. double-critic SAC/DSAC, but requires dropout for stability in some environments
  - Normal distribution assumption simplifies loss but violates bounded-support assumption of theory (Section 4.1 acknowledges this)

- **Failure signatures:**
  - Positive value estimation error persisting → β too low (underestimation not corrected)
  - Negative value error + poor returns → β too high (excessive pessimism causes underestimation)
  - Divergence in early training → critic dropout may be required (observed in Hopper-v4)
  - High variance across seeds → β or dropout poorly tuned for environment stochasticity

- **First 3 experiments:**
  1. **Sanity check on β:** Run STAC on a simple environment (e.g., LunarLander) with β ∈ {0, 0.25, 0.5}; plot value error curves to confirm the theoretical correlation between β and error sign (Figure 6 pattern).
  2. **Dropout ablation:** Fix β at best value, compare 4 configurations (no dropout, actor-only, critic-only, both); verify actor dropout consistently helps and critic dropout is environment-dependent (Figure 7-8).
  3. **Single vs. double critic baseline:** Compare STAC (single critic + aleatoric pessimism) vs. ESTAC (double critic + epistemic min-clipping) on 2-3 environments to isolate the contribution of temporal aleatoric pessimism (Table 1 shows STAC generally outperforms ESTAC).

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical framework assumes sub-Gaussian bounded distributions, but implementation uses unbounded Normal distributions, creating a mild theoretical inconsistency
- Effectiveness depends on critic variance reliably reflecting target variability; correlation may break down in highly non-stationary or distributionally-shifted environments
- No adaptive β scheme is provided, requiring environment-specific tuning despite β's critical role in balancing overestimation and underestimation

## Confidence

**High Confidence**: The core claim that temporal aleatoric uncertainty can mitigate overestimation bias is well-supported by Theorem 3.1 and Corollary 3.1.1, which provide formal bounds under reasonable assumptions. The empirical results showing STAC achieves competitive performance with fewer computational resources than double-critic methods are robust across multiple environments.

**Medium Confidence**: The unified pessimism mechanism (applying same β to both critic and actor) is logically sound and maintains Bellman consistency, but the assumption that the same β is appropriate for both updates may not hold in all training dynamics. The necessity of dropout for single-critic stability is supported by ablation studies, but its environment-dependent effectiveness suggests this is not a universal requirement.

**Low Confidence**: The extension of the theoretical framework from bounded distributions to unbounded Normal distributions lacks rigorous justification. The paper does not address how STAC performs under distribution shift or in sparse reward environments where signal-to-noise ratios are low. The claim about real-world applicability is speculative given the evaluation is limited to standard OpenAI Gym benchmarks.

## Next Checks

1. **Distributional Robustness Test**: Evaluate STAC on environments with varying levels of stochasticity (e.g., varying noise levels in transitions/rewards) to confirm that the critic's variance reliably tracks target variability across the spectrum, and identify where the correlation breaks down.

2. **Distribution Shift Evaluation**: Test STAC's performance when training and test environments have different dynamics or reward structures to assess whether temporal aleatoric pessimism provides benefits under distribution shift compared to epistemic uncertainty methods.

3. **Adaptive β Scheme**: Implement and evaluate an adaptive β mechanism that adjusts based on observed value error trends or critic variance levels, comparing against the fixed-environment β approach to determine if automatic tuning improves generalization across environments.