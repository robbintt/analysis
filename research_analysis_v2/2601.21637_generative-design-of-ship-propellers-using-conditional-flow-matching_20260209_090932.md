---
ver: rpa2
title: Generative Design of Ship Propellers using Conditional Flow Matching
arxiv_id: '2601.21637'
source_url: https://arxiv.org/abs/2601.21637
tags:
- design
- propeller
- performance
- data
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a generative AI approach for ship propeller
  design using Conditional Flow Matching (CFM). Unlike traditional forward models
  that predict performance from design parameters, CFM generates designs that achieve
  specified performance targets by learning a bidirectional mapping between parameters
  and performance labels conditioned on simulated noise.
---

# Generative Design of Ship Propellers using Conditional Flow Matching

## Quick Facts
- arXiv ID: 2601.21637
- Source URL: https://arxiv.org/abs/2601.21637
- Reference count: 11
- Primary result: Generative AI approach using Conditional Flow Matching (CFM) for inverse design of ship propellers, achieving <25% mean relative error across performance targets while generating diverse designs.

## Executive Summary
This paper introduces a generative AI approach for ship propeller design using Conditional Flow Matching (CFM). Unlike traditional forward models that predict performance from design parameters, CFM learns a bidirectional mapping to generate designs that achieve specified performance targets. The method enables sampling multiple valid designs for the same performance requirements by learning a continuous transformation from noise vectors to design parameters conditioned on performance labels.

The authors created a dataset of 3000 parameterized propeller geometries with six design variables and three performance labels using OpenProp simulations. CFM models trained on this data achieved high accuracy in generating designs matching target performance labels, with mean relative errors below 25% for all labels. The approach also demonstrates the ability to produce diverse geometric alternatives for fixed performance targets and shows that synthetic data augmentation using surrogate models can improve model performance, particularly for complex labels when training data is limited.

## Method Summary
The method employs Conditional Flow Matching to establish a bidirectional mapping between design parameters and simulated noise conditioned on performance labels. A dataset of 3000 parameterized propeller geometries was created using OpenProp simulations, with six design variables (number of blades, pitch, chord weights, etc.) and three performance labels (efficiency, advance ratio, thrust coefficient). The CFM model learns a time-dependent vector field that transports samples from a tractable source distribution (Gaussian noise) to the target distribution of design parameters. Training used an 8-layer MLP with 500 neurons per layer, trained for 10,000 epochs with Adam optimizer. Surrogate models were trained to predict labels from parameters, enabling data augmentation by generating pseudo-labels for random design samples.

## Key Results
- High accuracy in generating designs matching target performance labels (mean relative errors below 25% for all labels)
- Ability to produce diverse geometric alternatives for fixed performance targets, including varying blade counts
- Synthetic data augmentation using surrogate models improves performance for complex labels like thrust coefficient when limited training data is available
- Successful application to real-world design task, generating multiple valid propeller designs meeting specific constraints

## Why This Works (Mechanism)

### Mechanism 1: Conditional Vector Field Transport
CFM enables inverse design by learning a time-dependent vector field that transports samples from a tractable source distribution (Gaussian noise) to the target distribution of design parameters conditioned on performance labels. Instead of predicting parameters directly, the model learns to solve an ODE where the network defines a velocity field $v_\theta(t)$ that pushes noise vectors toward valid design vectors satisfying the conditioning labels. This treats the solution as a probabilistic path, avoiding the "one-to-many" mapping ambiguity inherent in inverse problems. The method fails if the target conditional distribution is disjoint or highly multi-modal such that linear interpolation paths between noise and data cross regions of low probability.

### Mechanism 2: Surrogate-Guided Data Augmentation
Training CFM models on limited simulation data benefits from synthetic data generated by fast forward surrogate models (predicting labels from parameters), particularly for complex labels with weak parameter correlations. Forward surrogate models are cheap to evaluate compared to CFD. By sampling random parameters and labeling them with surrogates, the training dataset is expanded cheaply. The CFM model learns the inverse mapping on this larger, albeit slightly noisy, dataset. This improves generalization on complex relationships like thrust coefficient where the inverse mapping is harder to infer from sparse ground truth. The approach fails if surrogate models have high bias or systematic error for specific regions of the design space.

### Mechanism 3: Diverse Design Sampling via Latent Noise
For fixed performance targets, sampling different initial noise vectors allows the model to generate a diverse set of geometrically distinct designs, circumventing the single-solution limit of deterministic inverse methods. The learned flow is deterministic given an initial point, but because the initial point is drawn from a distribution (Gaussian), different inputs trace different paths to distinct endpoints (designs). As long as these endpoints lie on the manifold of valid solutions for a given label, the method produces diverse alternatives. The approach fails if training data lacks diversity for a specific label, causing the model to ignore the noise vector and converge to the single seen solution.

## Foundational Learning

- **Concept**: **Ordinary Differential Equations (ODEs) in Generative Modeling**
  - **Why needed here**: CFM relies on integrating a vector field over time (solving an ODE) to transform noise into data. Understanding the difference between discrete layers and continuous dynamics is essential for debugging convergence and sampling steps.
  - **Quick check question**: If the integration step size for the ODE solver is increased during inference, would you expect the sample quality to improve or degrade, and why?

- **Concept**: **Inverse Problems and Ill-Posedness**
  - **Why needed here**: Design is an inverse problem (Performance → Geometry). Unlike forward problems (Geometry → Performance), multiple geometries can yield identical performance. Understanding this non-uniqueness explains why the model uses a probabilistic approach rather than simple regression.
  - **Quick check question**: Why does a standard Mean Squared Error loss on parameters fail to capture the "one-to-many" nature of the propeller design problem?

- **Concept**: **Parametric Geometry Representation**
  - **Why needed here**: The model outputs a 6-dimensional vector of design variables that drives a parametric CAD engine. You must understand how latent variables map to physical constraints (e.g., camber range [0, 0.05]).
  - **Quick check question**: If the CFM model outputs a value of 6 for the "Number of Blades" variable, but the dataset only contains integer values 2 to 5, how should the output be interpreted or post-processed?

## Architecture Onboarding

- **Component map**: Data Generator (OpenProp + CAESES) -> Dataset (p, l) -> Surrogate Trainer (p → l) -> Augmenter (p, l_pred) -> CFM Core (v_θ(t, x_t, l)) -> Sampler (ODE Solver) -> Design

- **Critical path**: The dataset generation (OpenProp) and the Surrogate accuracy. If the surrogate provides bad pseudo-labels during augmentation, the CFM model creates a "garbage in, garbage out" loop.

- **Design tradeoffs**:
  - **Dataset Size vs. Augmentation**: Generating more ground-truth CFD data is expensive; augmentation is cheap but introduces noise
  - **Integration Steps vs. Inference Speed**: High-quality generation requires more ODE steps (slower); fewer steps are faster but may produce physically invalid geometries
  - **Complexity vs. Error**: Simple labels (linearly correlated with parameters) need less augmentation; complex labels (k_T) benefit most from augmentation but are harder to fit

- **Failure signatures**:
  - **Mode Collapse**: Generating identical propellers for different noise inputs (check variance of output distributions)
  - **Distribution Drift**: Generated parameters falling outside valid ranges (e.g., negative camber)
  - **Surrogate Error Propagation**: Validating generated designs in OpenProp reveals performance errors significantly higher than the surrogate's training error

- **First 3 experiments**:
  1. **Baseline Inversion**: Train CFM on the raw 2000-sample dataset without augmentation. Measure Mean Relative Error (MRE) on the test set to establish a lower bound on performance.
  2. **Ablation on Augmentation**: Train CFM using data augmentation. Compare MRE for complex (k_T) vs. simple (J*) labels to confirm augmentation helps complex labels more.
  3. **Diversity Check**: Fix a target label vector and generate 100 designs. Plot histograms of the design parameters to verify the model is exploring the design space rather than converging to a mean.

## Open Questions the Paper Calls Out

- **Question**: How does the CFM model's accuracy and diversity scale when expanding the design space to include secondary geometric parameters like skew and rake, and complex labels such as cavitation criteria?
  - **Basis**: The authors state that subsequent studies may investigate the performance of CFM models when increasing the number of propeller parameters and performance labels, specifically suggesting the addition of skew, rake, and cavitation.
  - **Why unresolved**: The current study fixed skew and rake to zero and used a low-fidelity vortex lattice method that does not account for cavitation dynamics.

- **Question**: Can the generative design framework maintain performance when trained on high-fidelity RANS simulations that account for propeller-hull interactions?
  - **Basis**: Section 7 notes that to be of practical use in the future, the interaction of the hull and the propeller would have to be considered, suggesting using another more sophisticated CFD method.
  - **Why unresolved**: The current dataset relies on OpenProp simulations in an "open-water set-up," which deliberately neglects propeller-hull interactions.

- **Question**: What are the limits of synthetic data augmentation via surrogate models when the underlying parameter-label relationships become highly non-linear or the available ground-truth data is extremely limited?
  - **Basis**: While Section 6.3 shows augmentation helps for complex labels like thrust coefficient, the authors note that higher complexity scenarios will require considerably more computational resources.
  - **Why unresolved**: The study only tested augmentation on a 6-parameter model; it is unclear if surrogate models would introduce excessive noise that outweighs the benefits of data volume in higher-dimensional, more complex design spaces.

## Limitations
- Reliance on surrogate model accuracy for data augmentation introduces potential error propagation that the study doesn't fully quantify
- Current framework uses low-fidelity OpenProp simulations that don't account for propeller-hull interactions or cavitation dynamics
- Performance of augmentation strategy in highly non-linear parameter-label relationships with extremely limited ground-truth data remains unproven

## Confidence
- **High Confidence**: CFM framework's ability to generate diverse designs for fixed performance targets (Section 6.2)
- **Medium Confidence**: Surrogate-guided augmentation improves performance for complex labels (Section 6.3)
- **Medium Confidence**: General applicability of CFM to other engineering inverse design problems

## Next Checks
1. **Surrogate Error Propagation Analysis**: Generate designs using CFM trained with augmented data, then compare the performance of these designs against both the target labels and the surrogate's predictions to quantify error amplification.

2. **Ablation on Design Space Coverage**: For a fixed target label, generate 1000 designs and analyze whether the distribution of generated parameters (particularly blade count) matches the training data distribution to test true exploration vs. mode collapse.

3. **Ground Truth Validation on Augmented Models**: For the thrust coefficient label, generate designs using models trained with and without augmentation, then validate both sets using full OpenProp simulations to isolate the benefit of augmentation from surrogate bias.