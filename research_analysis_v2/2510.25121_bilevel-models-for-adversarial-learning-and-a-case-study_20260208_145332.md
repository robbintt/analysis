---
ver: rpa2
title: Bilevel Models for Adversarial Learning and A Case Study
arxiv_id: '2510.25121'
source_url: https://arxiv.org/abs/2510.25121
tags:
- clustering
- learning
- adversarial
- example
- perturbation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes bilevel optimization models for adversarial
  learning and studies their properties using convex clustering as a case study. The
  authors analyze the robustness of learning models through calmness of the solution
  mapping under perturbations, identifying conditions under which clustering results
  remain unchanged.
---

# Bilevel Models for Adversarial Learning and A Case Study

## Quick Facts
- arXiv ID: 2510.25121
- Source URL: https://arxiv.org/abs/2510.25121
- Reference count: 40
- Primary result: Bilevel optimization models for adversarial attacks on convex clustering, with δ-measure analysis showing robustness conditions and deviation function properties

## Executive Summary
This paper proposes bilevel optimization models for adversarial learning, focusing on convex clustering as a case study. The authors characterize robustness through calmness of the solution mapping, establishing conditions under which clustering results remain stable under perturbations. Two bilevel models are introduced: one maximizing deviation under perturbation budget and another minimizing perturbation for target deviation. The study systematically analyzes the δ-measure as a deviation function, proving its nondecreasing property in specific 2-way and 3-way clustering scenarios. Numerical experiments on UCI datasets validate the theoretical results, demonstrating staircase behavior in deviation metrics and confirming the effectiveness of RI-based functions over NMI.

## Method Summary
The method formulates adversarial attacks as bilevel optimization problems where the upper level maximizes/minimizes deviation under perturbation constraints while the lower level solves the learning problem. For convex clustering, this involves solving the consensus vector clustering (CVC) problem with Ssnal solver. Perturbations are additive noise constrained to ~30% of feature range. The δ-measure quantifies deviation as squared Frobenius norm of pairwise relationship changes between clusterings. Numerical experiments use direct grid search and fmincon optimization on UCI datasets (Fisher Iris, Seeds, Wine) with varying regularization parameters and perturbation budgets.

## Key Results
- Calmness of solution mapping characterizes robustness, bounding solution changes relative to perturbation magnitude
- δ-measure behaves as a valid deviation function under specific conditions (2-way clustering with s < min(n₁, ⌈n/2⌉))
- Bilevel models provide principled framework for adversarial learning, with (BL1) maximizing deviation and (BL2) minimizing perturbation
- RI-based functions serve as reasonable deviation measures while NMI fails due to non-monotonic behavior
- Numerical experiments show convex clustering is robust to moderate perturbations with clear staircase behavior

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Robustness of learning models under perturbation can be characterized by calmness of the solution mapping, providing bounded solution changes relative to perturbation magnitude.
- Mechanism: When solution mapping S(·) is calm at (0, Y*), there exists a modulus L₀ such that S(ε) ⊂ S + L₀‖ε‖B. This bounds how far perturbed solutions can drift from the original solution set, directly linking input noise to output stability.
- Core assumption: The solution mapping admits a closed graph and the learning model produces well-defined solution sets (may be non-unique).
- Evidence anchors:
  - [abstract] "characterize the robustness of learning models through the calmness of the solution mapping"
  - [section 2.2] Definition 2 formalizes calmness at ε=0 with explicit neighborhood and modulus conditions
  - [corpus] No direct corpus support; related papers focus on adversarial training rather than perturbation analysis theory
- Break condition: Calmness fails when the solution mapping has discontinuous jumps or the feasible set F(X(ε)) changes structure abruptly with ε.

### Mechanism 2
- Claim: The δ-measure functions as a valid deviation function for quantifying adversarial attack effects in clustering, but only under specific conditions on cluster balance and perturbation extent.
- Mechanism: δ(ε) = ‖D̂(Y*(ε))D̂(Y*(ε))ᵀ - D̂(Y*)D̂(Y*)ᵀ‖²_F counts pairwise relationship changes between clusters. For 2-way clustering with s points moving, δ(ε) = 2s(n-s), which is nondecreasing only when s < min(n₁, ⌈n/2⌉).
- Core assumption: Perturbations cause points to spill from one cluster to others in a tractable pattern; cluster centroids remain distinct.
- Evidence anchors:
  - [abstract] "systematically study the so-called δ-measure and show that under certain conditions, it can be used as a deviation function"
  - [section 5.1] Theorem 3 proves δ(ε) = 2s(n-s) and identifies monotonicity bounds
  - [corpus] No corpus papers analyze δ-measure specifically; related work on fairness measures exists but is tangential
- Break condition: When s exceeds the monotonicity bound or clusters are highly unbalanced, δ can decrease even as more points change (see Examples 6-7, 11-12 showing non-monotonic behavior).

### Mechanism 3
- Claim: Bilevel optimization provides a principled framework for adversarial learning by nesting attack optimization (upper level) within learning solution constraints (lower level).
- Mechanism: Model (BL1) maximizes deviation U(ε) subject to ‖ε‖ ≤ a and Y*(ε) ∈ S(ε). Model (BL2) minimizes ‖ε‖ subject to U(ε) ≥ δ₀. The lower level ensures perturbed data produces valid learning outputs.
- Core assumption: Lower-level learning problem is efficiently solvable; deviation function U(·) satisfies properties (i)-(iii): nondecreasing in ‖ε‖, nonnegative, and U(0)=0.
- Evidence anchors:
  - [abstract] "propose two bilevel models for adversarial learning where the effect of adversarial learning is measured by some deviation function"
  - [section 4] Equations (BL1) and (BL2) with explicit constraint structures; Remark 2 notes deviation function design difficulty
  - [corpus] "Stochastic Regret Guarantees for Online Zeroth- and First-Order Bilevel Optimization" provides algorithmic context for bilevel methods
- Break condition: If the lower-level problem has non-unique solutions or the deviation function violates monotonicity (like NMI in Section 6.4), bilevel solutions may be inconsistent or misleading.

## Foundational Learning

- Concept: **Calmness and Metric Subregularity**
  - Why needed here: Core theoretical tool connecting perturbation size to solution drift; without calmness, there's no guarantee that small ε produces bounded output changes.
  - Quick check question: Given a learning model, can you identify conditions where the solution mapping S(ε) would fail to be calm at ε=0?

- Concept: **Bilevel Optimization Structure**
  - Why needed here: The adversarial attack models are formulated as bilevel problems; understanding upper/lower level interaction is essential for algorithm selection (KKT, value-function, hypergradient methods).
  - Quick check question: In model (BL1), what happens if the lower-level solution Y*(ε) is not unique?

- Concept: **Clustering Validation Metrics (Rand Index, NMI)**
  - Why needed here: Section 6.4 shows RI-based functions work as deviation measures while NMI fails due to non-monotonicity; understanding these metrics determines which can quantify attack success.
  - Quick check question: Why does NMI jump upward after dropping to 0, making it unsuitable as a deviation function?

## Architecture Onboarding

- Component map:
  - Data perturbation layer -> Lower-level solver -> Deviation evaluator -> Upper-level optimizer -> Recovery condition checker

- Critical path:
  1. Define perturbation type and budget a (or target deviation δ₀)
  2. Solve lower-level (CVC) for unperturbed Y* to establish baseline clustering
  3. For each candidate ε, solve perturbed (CVC) for Y*(ε)
  4. Compute deviation U(ε) and check monotonicity
  5. Optimize upper-level objective subject to constraints

- Design tradeoffs:
  - **Direct method vs. fmincon**: Direct grid search finds global maxima but is O(n·steps); fmincon is faster but returns stationary points (Table 1 shows fmincon can miss optimal ε)
  - **δ-measure vs. RI-based**: δ provides explicit formulas but fails monotonicity in unbalanced cases; RI is empirically monotonic but lacks closed form
  - **White-box vs. black-box**: Paper assumes white-box (full gradient access); black-box requires derivative-free methods (evolution strategies, PRIMA)

- Failure signatures:
  - **Non-monotonic deviation**: U(ε) decreases as ‖ε‖ increases → deviation function is ill-designed (NMI exhibits this)
  - **Lower-level non-convergence**: Ssnal fails when γ is outside [γ_min, γ_max) → check (C2') condition
  - **Staircase behavior**: Nchg(ε) plateaus then jumps → indicates discrete cluster boundary crossings, not algorithm failure

- First 3 experiments:
  1. **Robustness boundary identification**: On a 1D dataset (e.g., Fisher Iris Feature 4), sweep ε ∈ [-a, a] and plot Nchg(ε), δ(ε), RI(ε). Identify the interval where clustering remains unchanged (should match Theorem 2 predictions).
  2. **Deviation function validation**: For 2-way clustering with varying n₁/n₂ ratios, test whether δ(ε) remains monotonic for s up to min(n₁, ⌈n/2⌉). Compare against RI-based U_RI(ε) = 1 - RI(ε).
  3. **Bilevel model comparison**: Implement both (BL1) and (BL2) on a UCI dataset (Seeds or Wine). Compare direct method vs. fmincon in terms of U(ε*) and computation time. Verify that (BL2)'s minimum ‖ε‖ for δ₀=1200 aligns with (BL1)'s deviation curve.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions does the convex clustering model provide exact recovery when the assumptions of distinct centroids or strictly positive weights fail?
- Basis in paper: [explicit] "For practical clustering problems, these assumptions may fail... Therefore, it would be interesting to further study the exact recovery result when the above assumptions fail. We leave this interesting question as our future research topic to study." (p. 10)
- Why unresolved: The theoretical results presented (Theorem 1 and 2) rely on these strict assumptions to guarantee robustness and calmness; real-world data often violates them.
- What evidence would resolve it: New theoretical proofs or bounds establishing exact recovery guarantees under relaxed conditions (e.g., non-distinct centroids), or counter-examples showing failure.

### Open Question 2
- Question: How can efficient algorithms be designed to solve the bilevel model (BL2), which minimizes perturbation for a target deviation level?
- Basis in paper: [explicit] "Comparing (BL2) with (BL1), (BL2) is usually more challenging to solve... Therefore, how to design efficient algorithms to solve (BL2) is an interesting topic which is worth further investigation." (p. 11)
- Why unresolved: The feasible region in (BL2) is difficult to represent explicitly because the lower-level solution mapping $Y^*(\varepsilon)$ is implicit, making standard optimization difficult.
- What evidence would resolve it: Development of a specific optimization algorithm (e.g., using KKT conditions or value functions) that demonstrates efficient convergence on (BL2) compared to general-purpose solvers.

### Open Question 3
- Question: How can the proposed bilevel models be solved efficiently in black-box settings where the attacker lacks access to the model's internal parameters or gradients?
- Basis in paper: [explicit] "In practice, the black-box clustering attacks are more prevalent. Therefore, how to design specified and efficient algorithms to solve the bilevel model is an interesting topic..." (p. 26)
- Why unresolved: The numerical experiments in the paper were conducted in a white-box setting; black-box attacks require derivative-free methods or query-based strategies not detailed in the current work.
- What evidence would resolve it: A numerical study demonstrating the success of derivative-free algorithms (e.g., evolutionary strategies) in solving the bilevel models against black-box clustering targets.

## Limitations

- Theoretical analysis assumes well-defined solution sets and calmness properties that may not hold for all learning models
- δ-measure analysis relies heavily on 2-way clustering symmetry with limited extension to k-way cases
- Numerical experiments use relatively small datasets (n≤200) and simple additive perturbations
- Paper does not address black-box attack scenarios where model internals are unavailable

## Confidence

- **High confidence**: The calmness characterization of robustness and the connection between solution mapping properties and perturbation bounds are well-established in variational analysis theory.
- **Medium confidence**: The bilevel formulation and numerical results on UCI datasets are convincing, though limited in scope. The theoretical bounds for δ-measure monotonicity in 2-way clustering are mathematically sound.
- **Low confidence**: The extension of δ-measure behavior to 3-way clustering relies on extensive case analysis without general proof. The claim that NMI cannot serve as a deviation function needs broader validation across different clustering scenarios.

## Next Checks

1. Test the δ-measure monotonicity on 3-way clustering with n=60 (20 points per cluster) under controlled perturbations to verify the pattern holds beyond the paper's theoretical analysis.

2. Implement the bilevel models on a larger dataset (n≥1000) using both additive and graph-based perturbations to assess scalability and attack effectiveness.

3. Compare the proposed bilevel approach against gradient-based adversarial attacks (PGD, FGSM) on the same clustering models to evaluate relative attack success rates and computational efficiency.