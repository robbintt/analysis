---
ver: rpa2
title: Monte Carlo Functional Regularisation for Continual Learning
arxiv_id: '2508.13006'
source_url: https://arxiv.org/abs/2508.13006
tags:
- learning
- mcfrcl
- regularisation
- prediction
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses continual learning in neural networks, focusing
  on preventing catastrophic forgetting when adapting to new tasks. The proposed method,
  MCFRCL, uses Monte Carlo sampling to approximate prediction distributions from the
  current and previous models.
---

# Monte Carlo Functional Regularisation for Continual Learning

## Quick Facts
- **arXiv ID:** 2508.13006
- **Source URL:** https://arxiv.org/abs/2508.13006
- **Reference count:** 30
- **Primary result:** Proposed MCFRCL method achieves 93.22% accuracy on MNIST with 200 context points per task while being computationally more efficient than function-space regularisation methods.

## Executive Summary
This paper addresses catastrophic forgetting in continual learning by proposing Monte Carlo Functional Regularisation for Continual Learning (MCFRCL). The method uses Monte Carlo sampling to approximate prediction distributions from current and previous models, then regularises the functional distance between these distributions using Wasserstein or KL divergence. Experiments on MNIST and CIFAR datasets demonstrate that MCFRCL outperforms weight-space regularisation methods (EWC, SI) in both prediction accuracy and training efficiency, though it lags behind function-space regularisation methods on more complex CIFAR tasks.

## Method Summary
MCFRCL regularises the functional distance between current and previous model predictions on a memory buffer (coreset) of representative samples from previous tasks. The method approximates model prediction distributions by Monte Carlo sampling weights from variational distributions, generates output samples, and fits these to continuous densities (Gaussian, Laplace, Cauchy) using moment-based estimators. Both Wasserstein and KL distances are computed between the current and previous prediction distributions to form the regularisation term, which is added to the standard cross-entropy loss during training.

## Key Results
- On Split MNIST, MCFRCL achieves up to 93.22% accuracy with 200 context points per task, outperforming weight-space regularisation methods (EWC, SI).
- MCFRCL demonstrates computational efficiency, using less GPU memory than function-space regularisation methods like FROMP and S-FSVI.
- On CIFAR datasets, MCFRCL performs competitively but lags behind function-space regularisation methods, with accuracy dropping to 33.36% when using Cauchy distribution assumptions.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Monte Carlo sampling approximates non-linear prediction distributions more accurately than linearization methods, provided sufficient sample size for model complexity.
- **Evidence:** Competitive performance on MNIST compared to S-FSVI, which uses linearization methods.

### Mechanism 2
- **Claim:** Moment-based parameter estimation allows computationally efficient differentiable regularisation without explicit density estimation.
- **Evidence:** Performance degradation when using Cauchy distribution (33% accuracy on FMNIST) suggests sensitivity to parametric assumptions.

### Mechanism 3
- **Claim:** Regularising functional distance on a coreset preserves past knowledge by penalizing output inconsistencies rather than weight deviations.
- **Evidence:** MCFRCL significantly outperforms weight-regularisation methods (EWC, SI) on Split MNIST.

## Foundational Learning

- **Concept: Functional vs. Weight-Space Regularisation**
  - **Why needed here:** Understanding why constraining weights is ineffective because different weights can produce the same function.
  - **Quick check question:** Does penalizing the distance between weight parameters guarantee the model's output behavior remains unchanged? (Answer: No)

- **Concept: Variational Inference (VI)**
  - **Why needed here:** Understanding the source of stochasticity in MC samples from the variational distribution $q(\Theta)$.
  - **Quick check question:** What is the source of the stochasticity in the MC samples used to estimate the prediction distribution?

- **Concept: Wasserstein vs. KL Divergence**
  - **Why needed here:** Understanding when each metric is appropriate for measuring distribution similarity.
  - **Quick check question:** Why might Wasserstein distance be preferred over KL when dealing with non-overlapping distribution supports?

## Architecture Onboarding

- **Component map:** Memory Module -> Sampling Engine -> Forward Pass -> Density Estimator -> Loss Calculator
- **Critical path:** Load context points → Sample weights from current/previous distributions → Forward pass for prediction samples → Compute moments and fit distributions → Calculate functional distance loss → Backpropagate
- **Design tradeoffs:**
  - Sample Count ($S_C$): Higher counts improve density approximation but increase compute cost
  - Distribution Choice: Gaussian is robust; Cauchy introduces instability
  - Coreset Size: Larger coresets improve stability but increase memory footprint
- **Failure signatures:**
  - Performance collapse on complex data indicates insufficient MC sampling
  - High variance suggests sampling instability or poor $\lambda$ tuning
  - Negative backward transfer implies over-regularisation or under-regularisation
- **First 3 experiments:**
  1. Sanity Check: Replicate Split MNIST binary classification to verify baseline accuracy
  2. Ablation on Sample Size: Vary $S_C$ on Fashion-MNIST to observe trade-off
  3. Distribution Stress Test: Compare Gaussian vs. Cauchy on datasets with outliers

## Open Questions the Paper Calls Out

- **Question 1:** How can Monte Carlo sampling efficiency be improved for complex prediction densities in large-scale models without prohibitive computational costs?
  - **Basis:** Conclusion states MC sampling is ineffective at approximating complex densities with small sample counts on CIFAR
  - **Why unresolved:** Current method relies on small samples ($S_C=30$) that fail to capture larger model statistics
  - **What evidence would resolve it:** Modified sampling strategy achieving competitive CIFAR accuracy while maintaining low training overhead

- **Question 2:** Does removing the independence assumption between output dimensions improve regularisation performance?
  - **Basis:** Section III-B assumes independence between output dimensions to reduce computational cost
  - **Why unresolved:** Independence assumption may limit regulariser's ability to capture true prediction distribution structure
  - **What evidence would resolve it:** Ablation studies comparing independent univariate estimation against multivariate density estimation

- **Question 3:** Can MCFRCL be effectively adapted for resource-constrained edge devices without significant accuracy loss?
  - **Basis:** Conclusion notes future work will consider applying method to edge devices requiring light and fast models
  - **Why unresolved:** Feasibility of MC sampling approach on hardware with strictly limited compute remains untested
  - **What evidence would resolve it:** Benchmarks of MCFRCL performance on embedded hardware platforms

## Limitations

- MC sampling becomes computationally prohibitive when approximating complex prediction densities in large-scale stochastic models, particularly on CIFAR datasets
- Performance degrades significantly when output distributions deviate from assumed parametric forms (Gaussian/Laplace/Cauchy)
- The independence assumption between output dimensions may limit the regulariser's ability to capture correlated prediction structures

## Confidence

- **High confidence:** MNIST results showing MCFRCL outperforming weight-space methods with clear computational advantages
- **Medium confidence:** CIFAR results showing competitive but not superior performance compared to function-space methods
- **Low confidence:** Claims about MC sampling superiority over linearization methods without direct empirical comparison

## Next Checks

1. **Sample size scaling test:** Systematically vary $S_C$ from 5 to 100 on CIFAR to quantify accuracy-compute trade-off and determine if computational advantage over FROMP persists

2. **Distribution robustness evaluation:** Test MCFRCL with alternative parametric families (e.g., mixture models, skew-normal) on multi-modal synthetic dataset to validate whether Gaussian/Laplace assumptions limit performance

3. **Coreset strategy ablation:** Compare random coreset selection against k-center or gradient-based selection methods to isolate impact of data representativeness on functional regularisation effectiveness