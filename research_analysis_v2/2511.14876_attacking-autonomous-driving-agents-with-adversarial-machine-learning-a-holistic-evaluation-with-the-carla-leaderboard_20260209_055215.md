---
ver: rpa2
title: 'Attacking Autonomous Driving Agents with Adversarial Machine Learning: A Holistic
  Evaluation with the CARLA Leaderboard'
arxiv_id: '2511.14876'
source_url: https://arxiv.org/abs/2511.14876
tags:
- driving
- agents
- carla
- attack
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates adversarial attacks on autonomous driving
  agents using the CARLA simulator and leaderboard. The authors assess how adversarial
  patches designed to manipulate ML model predictions translate into actual driving
  actions, considering that agents combine ML models with PID controllers and GPS-based
  rules.
---

# Attacking Autonomous Driving Agents with Adversarial Machine Learning: A Holistic Evaluation with the CARLA Leaderboard

## Quick Facts
- arXiv ID: 2511.14876
- Source URL: https://arxiv.org/abs/2511.14876
- Authors: Henry Wong; Clement Fung; Weiran Lin; Karen Li; Stanley Chen; Lujo Bauer
- Reference count: 40
- Primary result: Adversarial patches can mislead ML models but often fail to manipulate actual driving actions due to agent-specific safety modules like GPS-based rules and PID controllers.

## Executive Summary
This paper evaluates adversarial attacks on autonomous driving agents using the CARLA simulator and leaderboard, revealing that while adversarial patches can successfully manipulate ML model predictions, the presence of agent-specific safety modules (PID controllers, GPS-based rules) often prevents these manipulations from affecting actual driving behavior. The authors demonstrate that attacks must be evaluated on complete autonomous driving systems rather than individual ML models in isolation, as downstream components can effectively mitigate adversarial effects. The study proposes developing a new leaderboard specifically for evaluating adversarial robustness in autonomous driving systems.

## Method Summary
The authors optimize adversarial patches to stop or steer autonomous driving agents by collecting RGB images from target locations in CARLA Town 02, then using AutoPGD optimization with lighting and projection awareness to generate effective patches. They stream these patches into the simulator at runtime using CARLA's texture streaming API with 'Unlit' material settings to bypass dynamic lighting effects. The patches are tested against three open-source agents (TCP, NEAT, Rails) across multiple scenarios, measuring their impact on route completion and lane infractions through the CARLA Leaderboard evaluation framework.

## Key Results
- Adversarial patches can successfully stop autonomous vehicles across all tested agents when optimized with lighting and projection awareness
- Steering attacks often fail because agent-specific GPS rules override manipulated waypoints that deviate from planned routes
- Agent-specific modules serve as implicit defense mechanisms that decouple ML model predictions from final driving actions
- The CARLA Leaderboard provides a standardized platform for evaluating adversarial robustness in autonomous driving systems

## Why This Works (Mechanism)

### Mechanism 1: Lighting-aware patch optimization
- **Claim:** Optimizing adversarial patches with lighting and projection awareness is necessary to translate ML model perturbations into successful vehicle stopping actions
- **Core assumption:** The "Unlit" material setting and color jitter robustly approximate visual domain shift
- **Evidence:** CARLA's realistic lighting makes optimization challenging; authors modified shading to 'Unlit' to ensure received RGB values match optimization loop
- **Break condition:** Dynamic exposure control that cannot be modeled by linear scaling

### Mechanism 2: GPS rule overrides as implicit defense
- **Claim:** Agent-specific GPS-based rules and PID controllers serve as implicit defense mechanisms that decouple ML predictions from driving actions
- **Core assumption:** Attacker has white-box access to ML model but black-box access to post-processing rules
- **Evidence:** Manual disabling of TCP's agent-specific rules revealed attack success only when rules were disabled
- **Break condition:** If attacker can infer GPS override thresholds

### Mechanism 3: Stopping attacks generalize better than steering
- **Claim:** Stopping attacks generalize better because "stop" action is less geometrically constrained by route planning
- **Core assumption:** Agent lacks anti-freeze module that forces movement
- **Evidence:** Contrast between successful stopping attacks and failed steering attacks due to geometric incompatibility with GPS rules
- **Break condition:** If agent employs temporal consistency checks

## Foundational Learning

- **Concept: Perspective Transformation & Homography**
  - Why needed: Projecting 2D adversarial patch onto 3D driving scenes requires understanding camera intrinsics/extrinsics
  - Quick check: How does changing camera pitch angle affect aspect ratio and gradient flow of projected patch?

- **Concept: PID Control & Waypoint Tracking**
  - Why needed: Agents output waypoints, not steering angles; PID converts waypoint error to steering command
  - Quick check: Would a small adversarial perturbation be more/less likely to affect trajectory with aggressive vs. loose PID tuning?

- **Concept: Threat Modeling (White-box vs. Black-box)**
  - Why needed: Study assumes white-box ML model but black-box agent logic, critical for interpreting results
  - Quick check: How would optimization loss function change with fully white-box threat model?

## Architecture Onboarding

- **Component map:** CARLA Sensors (RGB/GPS/LiDAR) -> ML Model (Backbone + Head) -> [Branch Point] -> Predicted Waypoints -> GPS Rule Filter (Override logic) -> PID Controller -> Vehicle Actuation (Steer/Throttle/Brake)
- **Critical path:** GPS Rule Filter identified as critical safety component; attack flow ML Model -> Waypoints is primary vulnerability surface
- **Design tradeoffs:** Trade simulation fidelity for reproducibility using "Unlit" textures; tradeoff between end-to-end differentiability and modular safety
- **Failure signatures:**
  1. Mitigated Attack: ML model diverges but vehicle steering unchanged due to GPS rule override
  2. Patch Bleaching: Optimized patch appears washed out/invisible in simulator
  3. Route Completion Failure: Vehicle stops mid-route

- **First 3 experiments:**
  1. Baseline Lighting Validation: Compare patch optimized without vs. with lighting adjustments against TCP on Road #5
  2. Rule Ablation Study: Steering attack against TCP in default vs. rules-disabled configuration, measuring aim angle vs. steering command delta
  3. Cross-Agent Transfer: Test TCP-optimized patch against Rails to determine "stopping" objective transfer across output modalities

## Open Questions the Paper Calls Out

- Can adversarial attacks effectively manipulate driving actions in end-to-end settings when targeting sensor-fusion models that utilize both LiDAR and RGB inputs?
- To what extent does an attacker's knowledge of agent-specific modules (e.g., PID controllers, GPS-based rules) increase attack success rates?
- Do mitigation effects of agent-specific modules against adversarial attacks generalize to high-fidelity platforms like Baidu Apollo or Autoware?

## Limitations

- Findings depend on specific threshold values and logic of agent-specific rules, which are not fully disclosed
- Results based on CARLA 0.9.10 may not generalize to newer simulator versions or real-world scenarios
- Study focuses on RGB-only agents, excluding sensor-fusion approaches increasingly used in production systems

## Confidence

- **High Confidence:** Mechanism of "Unlit" material settings preserving patch visibility under dynamic lighting
- **Medium Confidence:** GPS rule override mechanism and its effectiveness as implicit defense
- **Medium Confidence:** Generalization difference between stopping and steering attacks

## Next Checks

1. Systematically vary GPS deviation threshold in TCP agent to determine minimum perturbation required to bypass override mechanism
2. Evaluate optimized patches from CARLA 0.9.10 in CARLA 0.9.11/0.9.12 to assess lighting model changes' impact
3. Implement time-based consistency check in agent to detect prolonged stops and measure effect on stopping attack efficacy