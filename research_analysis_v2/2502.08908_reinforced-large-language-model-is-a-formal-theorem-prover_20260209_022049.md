---
ver: rpa2
title: Reinforced Large Language Model is a formal theorem prover
arxiv_id: '2502.08908'
source_url: https://arxiv.org/abs/2502.08908
tags:
- theorem
- formal
- proof
- framework
- adaption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a reinforcement learning framework to train
  a Large Language Model (LLM) as a formal theorem prover for the Lean proof assistant.
  The method involves two phases: first, supervised fine-tuning (SFT) to adapt a pretrained
  LLM (Qwen2.5-0.5B) to formal theorem proving with Chain-of-Thought (CoT) supervision;
  second, reinforcement learning using Group Relative Policy Optimization (GRPO) to
  iteratively optimize the model by sampling tactics and comparing them with expected
  ones.'
---

# Reinforced Large Language Model is a formal theorem prover

## Quick Facts
- arXiv ID: 2502.08908
- Source URL: https://arxiv.org/abs/2502.08908
- Reference count: 9
- 0.5B parameter LLM achieves 43% accuracy on miniF2F benchmark

## Executive Summary
This paper proposes a reinforcement learning framework to train a Large Language Model as a formal theorem prover for the Lean proof assistant. The method uses two phases: supervised fine-tuning with Chain-of-Thought supervision to adapt a pretrained LLM to formal theorem proving, followed by reinforcement learning using Group Relative Policy Optimization to iteratively optimize the model. The resulting model, Lean-Qwen0.5B-rl, achieves 43% accuracy on the miniF2F benchmark, outperforming the SFT-only baseline at 36%. Training used 25k samples from Lean-workbook, with inference on a Macbook M2 Pro taking about 1 hour.

## Method Summary
The method involves two training phases: first, supervised fine-tuning (SFT) to adapt a pretrained LLM (Qwen2.5-0.5B) to formal theorem proving with Chain-of-Thought supervision using GPT-4o-generated reasoning steps; second, reinforcement learning using Group Relative Policy Optimization (GRPO) to iteratively optimize the model by sampling tactics and comparing them with expected ones. The model uses Lean-Dojo for theorem state management and BFS tree search for proof exploration. The approach achieves 43% accuracy on the miniF2F benchmark, with training on 4×A100 GPUs and inference on MacBook M2 Pro taking about 1 hour.

## Key Results
- Lean-Qwen0.5B-rl achieves 43% accuracy on miniF2F benchmark
- Outperforms SFT-only baseline (Lean-Qwen0.5B-sft) at 36% accuracy
- Training completed on 4×A100 GPUs, inference takes ~1 hour on MacBook M2 Pro

## Why This Works (Mechanism)

### Mechanism 1
Chain-of-Thought (CoT) supervision bridges the informal-to-formal language gap by providing intermediate reasoning signals. GPT-4o generates explanatory "thought" paragraphs for each training sample, connecting the current proof state to the next tactic. This creates a structured prompt-completion pair where the model learns to generate reasoning before outputting formal Lean tactics in a specified format.

### Mechanism 2
GRPO (Group Relative Policy Optimization) improves tactic selection by comparing sampled candidates against ground-truth, reinforcing syntactically and semantically correct completions. The policy model samples a group of candidate tactics per state, computes reward by comparing candidates to the ground-truth tactic, and uses advantage-weighted loss to iteratively update model parameters.

### Mechanism 3
Breadth-first tree search with Lean compiler feedback provides verifiable exploration of the proof space. Lean-Dojo executes proposed tactics via `run_tac`, with three outcomes guiding search: proof complete (terminate), novel valid state (expand tree), or grammar/syntax error (prune branch).

## Foundational Learning

- **Concept: Lean 4 and Interactive Theorem Provers**
  - Why needed here: The entire framework operates within Lean's type theory; understanding proof states, tactics, and compiler feedback is essential to debug model outputs.
  - Quick check question: Can you explain what a "tactic" in Lean does and how it transforms a proof state?

- **Concept: Policy Gradient Methods and GRPO**
  - Why needed here: The RL phase uses GRPO, a group-relative variant of policy optimization; understanding advantage estimation and reward shaping is required to modify or diagnose training.
  - Quick check question: How does GRPO differ from standard PPO in how it computes advantages?

- **Concept: Chain-of-Thought Prompting**
  - Why needed here: The SFT phase explicitly trains the model to generate intermediate reasoning before formal output; CoT structure affects both training data design and inference behavior.
  - Quick check question: What is the expected output format after the `Think` and `answer` tags in this system?

## Architecture Onboarding

- **Component map:** Lean-workbook dataset -> GPT-4o CoT augmentation -> SFT phase -> GRPO RL phase -> Lean-Qwen0.5B-rl model
- **Critical path:** CoT data quality from GPT-4o determines SFT baseline capability -> SFT must converge sufficiently before RL phase -> RL reward signal must correlate with actual proof success
- **Design tradeoffs:** 0.5B parameter model enables MacBook M2 Pro inference (~1 hour) but limits reasoning capacity vs. larger models; exact-match reward is simple but may undervalue semantically equivalent tactics; BFS is exhaustive but may not scale to deeper proof trees
- **Failure signatures:** High format reward but flat accuracy reward (model learns output structure, not valid tactics); step-like accuracy curve suggests potential instability; grammar error rate in tree search indicating poor syntax learning
- **First 3 experiments:**
  1. Ablate CoT: Train SFT without GPT-4o-generated thoughts; measure impact on miniF2F accuracy
  2. Reward sensitivity: Replace exact-match reward with execution-based reward and compare convergence
  3. Search strategy comparison: Replace BFS with best-first search using learned state-value estimator

## Open Questions the Paper Calls Out

### Open Question 1
Does the GRPO-based reinforcement learning framework scale effectively to larger base models (e.g., >7B parameters) to outperform current state-of-the-art theorem provers? The paper validates the framework exclusively on a small Qwen2.5-0.5B model; it is unstated if the relative gains from RL over SFT persist or diminish with increased model capacity.

### Open Question 2
What is the causal mechanism behind the observed "step-like" convergence of the accuracy reward curve during reinforcement learning? The authors explicitly note this phenomenon but offer no explanation for whether it stems from the optimizer, data ordering, or policy distribution shifts.

### Open Question 3
Is the reliance on exact ground-truth tactics for reward calculation too brittle for exploring diverse but semantically valid proof paths? The method calculates reward by "comparing them with expected ones," implying a dependency on specific tactic syntax rather than proof state validity.

## Limitations

- CoT supervision quality depends entirely on GPT-4o's reasoning accuracy, which is not validated against human proofs
- Exact-match reward function may incorrectly penalize semantically equivalent but syntactically different tactics
- BFS search strategy may not scale to complex proofs and lacks heuristic pruning

## Confidence

**High Confidence:** Experimental setup is clearly described (datasets, benchmarks, evaluation procedure), and accuracy improvement from 36% to 43% on miniF2F is directly measurable.

**Medium Confidence:** Mechanism claims about CoT supervision and GRPO effectiveness are plausible but lack ablation studies or diagnostic metrics to confirm they are working as intended.

**Low Confidence:** Scalability claims (inference on MacBook M2 Pro in ~1 hour) and assertion that exact-match rewards are sufficient for learning valid tactics are not rigorously tested.

## Next Checks

1. **Ablate CoT Supervision:** Train an SFT-only baseline without GPT-4o-generated thoughts and measure the impact on miniF2F accuracy to isolate whether CoT supervision contributes meaningfully beyond standard fine-tuning.

2. **Reward Function Ablation:** Replace the exact-match reward with an execution-based reward that evaluates whether a predicted tactic advances the proof state, regardless of whether it matches the ground-truth. Compare convergence speed and final accuracy.

3. **Search Strategy Comparison:** Implement a best-first search using a learned state-value estimator and compare proofs solved per unit time against the BFS baseline to test whether heuristic-guided search can scale better to complex proofs.