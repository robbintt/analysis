---
ver: rpa2
title: Safe and Optimal Learning from Preferences via Weighted Temporal Logic with
  Applications in Robotics and Formula 1
arxiv_id: '2511.08502'
source_url: https://arxiv.org/abs/2511.08502
tags:
- robustness
- formula
- learning
- problem
- signal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a safe and optimal method for learning from
  human preferences using Weighted Signal Temporal Logic (WSTL). The approach addresses
  the challenge of learning user preferences in safety-critical domains where traditional
  preference learning methods lack rigorous safety guarantees.
---

# Safe and Optimal Learning from Preferences via Weighted Temporal Logic with Applications in Robotics and Formula 1

## Quick Facts
- arXiv ID: 2511.08502
- Source URL: https://arxiv.org/abs/2511.08502
- Authors: Ruya Karagulle; Cristian-Ioan Vasile; Necmiye Ozay
- Reference count: 28
- Primary result: 84.97% accuracy on Formula 1 learning-to-rank task

## Executive Summary
This paper presents a method for learning human preferences in safety-critical domains using Weighted Signal Temporal Logic (WSTL). The approach combines structural pruning and log-transform techniques to convert multi-linear preference learning problems into Mixed-Integer Linear Programs while preserving safety guarantees. The framework is demonstrated on robot navigation and real-world Formula 1 racing data, achieving 84.97% accuracy on test data while providing interpretable weight-based explanations of driver preferences.

## Method Summary
The method learns weights for STL formulas by optimizing preference orderings while preserving safety semantics. It introduces structural pruning to eliminate irrelevant formula components that don't affect robustness computation, and log-transform to linearize multi-linear constraints into MILP form. The framework uses warm-start with random sampling (10,000 samples) to initialize the MILP solver, then solves with a 4-hour time limit. Safety is guaranteed because the qualitative satisfaction/violation semantics of the base STL formula remain unchanged regardless of learned weights.

## Key Results
- Achieves 84.97% accuracy on Formula 1 learning-to-rank task for both training and test data
- Outperforms random sampling baselines in preference ordering accuracy
- Reveals interpretable insights: grid position is most important factor when excluding DNF/DNS cars, while lap times become most important when including them
- Successfully applies to both robot navigation and real-world racing data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structural pruning reduces problem size without changing robustness semantics
- Mechanism: Recursively traverses Robustness Computation Tree (RCT) from root to leaves, removing child subtrees whose robustness sign differs from parent's sign. Since min/max operations with opposite-sign operands cannot be influenced by weight scaling, these branches are provably irrelevant to final robustness values.
- Core assumption: Robustness values have consistent signs throughout evaluation subtrees that contribute to root value
- Evidence anchors: [abstract] structural pruning eliminates irrelevant formula parts; [Section IV.A] Theorem 1 proves preservation of quantitative semantics
- Break condition: When ρ(σ, ϕ) = 0, pruning returns at least one node but offers no optimization leverage

### Mechanism 2
- Claim: Log-transform linearizes multi-linear weight constraints while preserving optimizer
- Mechanism: Applies logarithm to both sides of robustness constraints, converting products to sums (log(w·r) = log(w) + log(r)). Decision variables change from w_i to v_i = log(w_i). Since logarithm is strictly increasing, it preserves ordering in min/max operations.
- Core assumption: All robustness values in any single computation path share same sign (achieved via pruning)
- Evidence anchors: [abstract] log-transform linearizes multi-linear constraints to MILP form; [Section IV.B] Theorem 2 proves optimizer preservation
- Break condition: Formulas not in positive normal form cannot be log-transformed; requires pushing negations to predicate level first

### Mechanism 3
- Claim: WSTL weights encode interpretable preference rankings while preserving safety guarantees
- Mechanism: Qualitative semantics (satisfaction/violation) of base STL formula remains unchanged regardless of weight valuations. Weights only affect quantitative robustness measure, determining which satisfying trajectory is "more satisfying." This decoupling means unsafe behaviors can never be preferred over safe ones.
- Core assumption: Base STL formula correctly encodes safety requirements
- Evidence anchors: [abstract] framework preserves qualitative semantics ensuring unsafe behavior never favored; [Section V.B] learned weights reveal interpretable factors in F1 data
- Break condition: If base STL formula is incomplete or incorrect, learned weights optimize within wrong safety constraints

## Foundational Learning

- Concept: Signal Temporal Logic (STL) qualitative and quantitative semantics
  - Why needed here: WSTL extends STL; understanding robustness computation is prerequisite to understanding how weights modify it
  - Quick check question: Given a signal σ = [2, 3, 1] and formula □[0,2](σ ≥ 0), what is the robustness value?

- Concept: Abstract Syntax Trees (AST) and tree traversal algorithms
  - Why needed here: Robustness Computation Trees extend ASTs; structural pruning requires recursive tree traversal
  - Quick check question: Write a recursive function that visits all nodes of a binary tree and returns the minimum leaf value

- Concept: Mixed-Integer Linear Programming (MILP) fundamentals
  - Why needed here: The transformed problem is solved as MILP; understanding big-M constraints and binary variables helps debug solver issues
  - Quick check question: How would you encode "at least one of constraints A, B, C must hold" using binary variables?

## Architecture Onboarding

- Component map:
  1. Formula Parser -> RCT Builder -> Structural Pruner -> Log-Transformer -> MILP Solver Interface -> Weight Interpreter

- Critical path:
  1. Define STL formula ϕ encoding task + safety (requires domain expertise)
  2. Collect preference data (pairwise comparisons, rankings, or demonstrations)
  3. For each signal, compute predicate values h_μ(σ(t))
  4. Apply structural pruning per signal (reduces active weights per constraint)
  5. Log-transform constraints, solve MILP with warm-start from random sampling
  6. Recover weights, validate on held-out preferences

- Design tradeoffs:
  - **Formula expressiveness vs. solvability**: Complex temporal operators increase MILP size exponentially
  - **Optimality vs. computation time**: 4-hour time limit used; solver may return suboptimal but feasible solutions
  - **Interpretability vs. flexibility**: STL requires manual formula design but yields interpretable weights; LLM-assisted formula generation is future work
  - **Warm-start investment**: Paper uses S=10,000 random samples; diminishing returns vs. direct MILP solve

- Failure signatures:
  - **Infeasible MILP**: Preference data contradicts safety constraints (user prefers unsafe behavior)
  - **Zero robustness throughout**: Formula predicates never meaningfully satisfied/violated for given signals
  - **Weight explosion**: log-transform produces numerical overflow; check signal normalization
  - **Identical trajectories despite different preferences**: Pruning may have removed too many constraints; verify formula coverage

- First 3 experiments:
  1. **Unit test pruning correctness**: Create simple formula (e.g., φ = ◇[0,2](x ≥ 0) ∧ □[0,2](x ≤ 5)) with known signal; manually compute robustness tree and verify pruned tree yields identical robustness
  2. **Synthetic preference validation**: Generate 10 random weight vectors, synthesize trajectories via PyTelo, create pairwise preferences, learn weights back, verify >90% preference accuracy on held-out pairs
  3. **Scalability benchmark**: Measure MILP solve time vs. formula complexity (number of temporal operators) and dataset size (number of preferences); identify practical limits before 4-hour timeout

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can large language models (LLMs) be effectively integrated to automatically translate plain natural language sentences into STL formulas, thereby removing the reliance on manual expert specification?
- Basis in paper: [explicit] The authors state in the conclusion: "Looking forward, we plan to integrate more streamlined task specification methods, potentially using large language models to translate plain sentences to STL formulas."
- Why unresolved: The current framework requires the STL formula structure to be defined a priori by an expert, which limits accessibility for non-expert users.
- What evidence would resolve it: A demonstration of an end-to-end pipeline where user instructions in plain English are converted by an LLM into valid STL formulas that the proposed method can successfully optimize.

### Open Question 2
- Question: How can the optimization framework be extended to accommodate non-linear objective functions or regularization terms within the learning problem?
- Basis in paper: [explicit] The authors note a specific limitation: "MILP formulation requires linear objective functions, which may restrict the applicability to problems that involve non-linear objectives or require certain forms of regularization."
- Why unresolved: The log-transform and MILP approach fundamentally rely on linearity to guarantee optimality and efficient solving; introducing non-linearity would break the current mathematical formulation.
- What evidence would resolve it: A modified algorithm or mathematical reformulation that successfully incorporates a specific non-linear objective (e.g., a quadratic penalty on weight magnitude) while maintaining safety guarantees.

### Open Question 3
- Question: Can the learning framework be generalized to infer the logical structure and temporal operators of the formula itself, rather than only learning the weights of a pre-defined structure?
- Basis in paper: [inferred] The Problem Statement explicitly assumes "Given an STL formula $\phi$," and the method focuses entirely on parameterizing weights ($\phi_W$) rather than optimizing the syntax tree structure (conjunctions, temporal bounds) itself.
- Why unresolved: Structural pruning removes irrelevant branches, but the core logical connectives and temporal intervals are fixed inputs. Simultaneous structure and weight learning creates a significantly more complex discrete-continuous optimization problem than the current MILP formulation.
- What evidence would resolve it: An experiment where the algorithm successfully identifies the necessity of specific temporal operators (e.g., "Until" vs "Always") from preference data without those operators being present in the initial template.

## Limitations
- Formula structure must be manually specified by domain experts rather than learned from data
- Log-transform and MILP formulation restrict optimization to linear objectives, limiting regularization options
- Warm-start procedure with 10,000 random samples may mask solver convergence issues and local optima problems

## Confidence
- **High Confidence**: Safety guarantee preservation - follows directly from STL theory and is well-established
- **Medium Confidence**: Log-transform linearization correctness - theoretically proven but limited empirical validation across formula types
- **Medium Confidence**: Formula weight interpretability - F1 results show plausible rankings but could be influenced by data preprocessing choices

## Next Checks
1. **Formula generalization test**: Apply the learning framework to STL formulas with nested temporal operators (e.g., multiple nested Until operators) and verify that log-transform still produces valid linearizations that converge to optimal weights
2. **Baseline comparison expansion**: Implement and compare against at least two alternative preference learning methods (e.g., Gaussian process preference learning, deep learning ranking models) on the same F1 datasets to establish relative performance
3. **Solver convergence analysis**: Run the MILP solver multiple times with different random seeds and measure weight stability and objective value consistency to quantify how often the solver finds global optima versus local optima