---
ver: rpa2
title: 'IXAII: An Interactive Explainable Artificial Intelligence Interface for Decision
  Support Systems'
arxiv_id: '2506.21310'
source_url: https://arxiv.org/abs/2506.21310
tags:
- explanations
- ixaii
- users
- explainable
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces IXAII, an interactive explainable AI interface
  that integrates four post-hoc XAI methods (LIME, SHAP, Anchors, and DiCE) to enhance
  transparency and user trust in AI systems. The prototype allows users to choose
  from five predefined perspectives and customize explanation content and format,
  promoting user agency and understanding.
---

# IXAII: An Interactive Explainable Artificial Intelligence Interface for Decision Support Systems

## Quick Facts
- arXiv ID: 2506.21310
- Source URL: https://arxiv.org/abs/2506.21310
- Reference count: 36
- Primary result: IXAII integrates four post-hoc XAI methods in an interactive interface evaluated with 7 participants (3 experts, 4 lay users)

## Executive Summary
This paper introduces IXAII, an interactive explainable AI interface that integrates four post-hoc XAI methods (LIME, SHAP, Anchors, and DiCE) to enhance transparency and user trust in AI systems. The prototype allows users to choose from five predefined perspectives and customize explanation content and format, promoting user agency and understanding. Evaluated with seven participants through semi-structured interviews, IXAII was perceived as intuitive and helpful for increasing transparency. Experts noted the need for more advanced data exploration options, while lay users appreciated the accessibility and guidance provided.

## Method Summary
IXAII is a Python-based interface using Scikit-learn and Plotly Dash to implement four post-hoc XAI methods on the IRIS dataset. The system maps explanation methods to question types (Why, Why Not, What If, When) and offers five user perspectives (developer, user, business entity, regulatory entity, affected party). Users can switch between explanation formats including text, tables, charts, and formal expressions. The evaluation involved semi-structured interviews with three experts and four lay users lasting 60-120 minutes each, focusing on perceived intuitiveness, transparency, and user agency.

## Key Results
- Multi-method XAI integration provides complementary perspectives that enhance user comprehension
- User agency through format control promotes trust and subjective understanding
- Audience-specific view tailoring reduces information overload by filtering role-relevant content
- Lay users found the interface accessible with helpful guidance, while experts requested more advanced data exploration
- The system successfully demonstrated how interactivity can transform users from passive recipients to active participants

## Why This Works (Mechanism)

### Mechanism 1: Multi-Method XAI Integration
Providing multiple post-hoc explanation methods in a unified interface enhances user comprehension by offering complementary perspectives on model behavior. IXAII integrates LIME (local feature importance), SHAP (Shapley value attributions), Anchors (rule-based if-then logic), and DiCE (counterfactual examples). Each method answers different user questions through distinct representational formats. The core assumption is that users benefit from seeing multiple explanation types rather than relying on a single method that may obscure certain aspects of model behavior.

### Mechanism 2: User Agency Through Format Control
Granting users control over explanation content and presentation format promotes trust and subjective understanding. Users actively select reference methods and switch between format methods including text, tables, charts, and formal expressions. This transforms users from data recipients to active participants in the explanation process. The core assumption is that active participation in explanation selection creates deeper engagement than passive consumption of static outputs.

### Mechanism 3: Audience-Specific View Tailoring
Pre-configured views for distinct stakeholder groups reduce information overload by filtering explanations to role-relevant content. Five perspectives (developer, user, business entity, regulatory entity, affected party) receive tailored information displays. The core assumption is that different stakeholders have fundamentally different explanatory goals and cognitive requirements, with lay users preferring local explanations while experts access global patterns.

## Foundational Learning

- **Concept: Post-hoc XAI Methods**
  - Why needed here: IXAII's core value proposition relies on understanding what LIME, SHAP, Anchors, and DiCE each contribute differently
  - Quick check question: Can you explain why SHAP values have additive properties that LIME explanations lack?

- **Concept: Local vs. Global Explanations**
  - Why needed here: The system routes lay users to local explanations while enabling experts to switch fluidly between both types
  - Quick check question: What is the difference between explaining one prediction instance versus explaining overall model behavior?

- **Concept: Reference Methods Taxonomy**
  - Why needed here: IXAII organizes explanations by question type (Why, Why Not, What If, When) rather than by algorithm
  - Quick check question: How does a "Why Not" explanation differ structurally from a counterfactual "What If" explanation?

## Architecture Onboarding

- **Component map:**
  - Data Information tab → Inputs, output ranges, prototypical examples per class
  - Explanation tabs → LIME, SHAP, Anchors, DiCE visualizations with switchable reference methods
  - Format selector → Toggle between text, tables, charts, formal expressions
  - Guide overlay → XAI method descriptions for lay users
  - Perspective selector → Five-role dropdown filtering displayed content
  - Stack: Python + Scikit-learn + Plotly Dash

- **Critical path:**
  1. User selects perspective → system applies role-based defaults
  2. User explores Data Information tab → builds mental model of input space
  3. User selects XAI method + reference method → generates explanation
  4. User adjusts format → customizes visualization
  5. User consults guide if needed → interprets output

- **Design tradeoffs:**
  - Minimal IRIS dataset reduces complexity for interface testing but limits domain validity
  - Expert feedback requested more advanced data exploration; lay users valued simplicity
  - Descriptive naming of reference methods improves findability but may oversimplify technical distinctions

- **Failure signatures:**
  - Users overwhelmed by option multiplicity without guide consultation
  - Experts abandoning tool due to limited histogram/exploration depth
  - Contradictory signals between XAI methods causing confusion
  - Lay users misinterpreting formal expressions (Anchors rules)