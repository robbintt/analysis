---
ver: rpa2
title: 'Map2Video: Street View Imagery Driven AI Video Generation'
arxiv_id: '2512.17883'
source_url: https://arxiv.org/abs/2512.17883
tags:
- video
- view
- street
- participants
- camera
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Map2Video addresses spatial inconsistency and lack of camera control
  in AI video generation by grounding video creation in real-world street view imagery.
  The system enables users to select locations on a map, place actors, sketch movement
  paths, refine camera motion, and generate spatially consistent videos using inpainting
  techniques.
---

# Map2Video: Street View Imagery Driven AI Video Generation

## Quick Facts
- arXiv ID: 2512.17883
- Source URL: https://arxiv.org/abs/2512.17883
- Reference count: 40
- Primary result: Map2Video reduces spatial inconsistency in AI video generation by grounding video creation in real-world street view imagery

## Executive Summary
Map2Video addresses the spatial inconsistency and lack of camera control in AI video generation by grounding video creation in real-world street view imagery. The system enables users to select locations on a map, place actors, sketch movement paths, refine camera motion, and generate spatially consistent videos using inpainting techniques. In a user study with 12 filmmakers, Map2Video outperformed a baseline image-to-video tool, achieving higher perceived spatial accuracy, lower cognitive effort, and fewer required iterations in a replication task. Participants also reported better creative controllability and ease of iteration, finding the map-based workflow intuitive for both scene replication and open-ended exploration.

## Method Summary
Map2Video combines a Unity frontend with a ComfyUI backend to create AI-generated videos grounded in street view imagery. Users select locations on a map, place actor masks, sketch movement trajectories, and adjust camera angles, which are then projected to street view panoramas using ENU geospatial transformations. The system renders background videos from Mapillary panoramas, then uses masked inpainting with the Wan2.1 VACE model to generate actor movements while preserving background geometry. The workflow includes background video upscaling, mask inversion for inpainting control, and diffusion sampling to produce 5-second clips at 1280×720 resolution.

## Key Results
- Map2Video achieved higher perceived spatial accuracy than baseline image-to-video tools in user study
- Participants reported lower cognitive effort (NASA-TLX) and fewer required iterations for replication tasks
- The map-based interface was found intuitive for both scene replication and open-ended creative exploration

## Why This Works (Mechanism)

### Mechanism 1: Spatial Consistency through Geographic Grounding
- Claim: Anchoring video generation in street view imagery reduces shot-to-shot spatial inconsistency compared to text-to-video generation.
- Mechanism: Street view panoramas provide a shared coordinate frame that persists across clips. The background geometry is fixed by real-world data rather than hallucinated, so scene elements remain stable even when regenerating or iterating.
- Core assumption: Users accept the constraint of existing real-world locations rather than fully imaginary environments.
- Evidence anchors:
  - [abstract] "Map2Video addresses spatial inconsistency and lack of camera control in AI video generation by grounding video creation in real-world street view imagery."
  - [Section 4.2.1] "Because the background layout and camera extrinsics are anchored to a real place, scene geography remains stable across shots and iterations."
  - [corpus] Streetscapes (arXiv:240X.XXXXX) demonstrates related principle—autoregressive video diffusion on street view sequences maintains temporal consistency.

### Mechanism 2: Background Preservation via Masked Inpainting
- Claim: Restricting generation to user-defined mask regions prevents unintended background alteration during video synthesis.
- Mechanism: The system renders a background video from street view imagery with camera motion, then uses an inverted alpha mask to constrain diffusion edits to masked regions only.
- Core assumption: The underlying video generation model respects mask constraints without bleeding artifacts at boundaries.
- Evidence anchors:
  - [abstract] "generating spatially consistent videos using inpainting techniques"
  - [Section 6.3.3] "We use the upscaled background video with an inverted alpha mask as a control video for inpainting, so that edits are fixed to specific regions while preserving the camera motion of the background."
  - [Section 7.2.4] "In the baseline, the entire image was regenerated, often altering the background unexpectedly... In contrast, our system only modified the masked area."

### Mechanism 3: Cognitive Load Reduction through Direct Manipulation
- Claim: Map-based sketching and camera preview interfaces reduce cognitive effort compared to text-only prompting for specifying spatial intent.
- Mechanism: The interface externalizes spatial reasoning—users place actors on a map, sketch trajectories, and adjust camera angles visually rather than translating spatial relationships into natural language.
- Core assumption: Users have sufficient spatial reasoning skills to work with map-based interfaces, and the interface accurately translates their intent.
- Evidence anchors:
  - [abstract] "In a user study with 12 filmmakers, Map2Video... achieving higher perceived spatial accuracy, lower cognitive effort, and fewer required iterations in a replication task."
  - [Section 7.2.2] NASA-TLX results show significantly lower mental demand, effort, and frustration with Map2Video vs. baseline (p<0.01 for all three).
  - [Section 7.3.1] "Participants highlighted the value of street view imagery backgrounds as a concrete starting point... street view imagery provided spatial and visual cues that supported creative decision-making."

## Foundational Learning

- **Video Inpainting with Diffusion Models**
  - Why needed here: Map2Video's core technical contribution is constraining video generation to masked regions. Understanding how conditional diffusion works with spatial masks is essential for debugging quality issues or extending the pipeline.
  - Quick check question: If you wanted to inpaint only the left half of a video frame while preserving the right half, how would you structure the mask input to a video diffusion model?

- **Geographic Coordinate Systems (Geodetic to Local ENU)**
  - Why needed here: The system places actors in 3D space using latitude/longitude, then projects them onto 2D panorama views. The Appendix provides the explicit transformation math, but understanding the coordinate frames is prerequisite.
  - Quick check question: Given a camera at (lat_c, lon_c) and an actor at (lat_a, lon_a), what are the steps to compute the actor's screen position in the panorama viewport?

- **Filmmaking Workflows (Blocking, Location Scouting, Rehearsal)**
  - Why needed here: The system design mirrors traditional filmmaking practices. Understanding these workflows helps evaluate whether the interface actually matches user mental models or introduces friction.
  - Quick check question: In traditional filmmaking, what is "blocking" and how does Map2Video's mask positioning + trajectory sketching map to this concept?

## Architecture Onboarding

- **Component map:**
  - Frontend (Unity): Map Panel (OpenStreetMap) -> Street View Panel (Mapillary imagery) -> Timeline Panel -> Prompt Panel -> Reference Image Window
  - Backend (ComfyUI server): Receives background video, mask video, prompts, and optional reference image; runs VACE inpainting workflow
  - Core models: Wan2.1 VACE 14B (diffusion) -> CLIP (text encoding) -> Wan 2.1 VAE (latent encode/decode) -> SeedVR (video upscaling) -> LoRA modules (CausVid, LightX2V)
  - Data sources: Mapillary (360° panoramas) -> OpenStreetMap (map tiles)

- **Critical path:**
  1. User selects location → Mapillary panorama retrieved
  2. User places mask on map → Projected to panorama viewport (ENU → pinhole projection)
  3. User sketches trajectory on map → Generates mask video with movement over 5 seconds
  4. User adjusts camera → Renders background video with pan/tilt/zoom
  5. User enters prompt → CLIP encodes text
  6. Backend: Upscale background → Invert mask → VAE encode → Diffusion sampling with VACE → VAE decode → Output video
  7. ~3 minutes per 5-second clip at 1280×720, 16 fps

- **Design tradeoffs:**
  - **Coverage vs. Control**: Mapillary has limited panorama nodes, constraining camera positions and preventing smooth lateral dolly shots. Users cannot freely position cameras—only available nodes.
  - **Quality vs. Speed**: 6-step KSampler with UniPC prioritizes fast generation (~3 min) over maximum quality. More steps would improve fidelity but increase latency.
  - **Realism vs. Flexibility**: Grounding in real street view imagery ensures consistency but limits fantastical/stylized outputs. Users noted this constrained imagination for speculative scenes.
  - **Single-mask simplicity vs. Multi-mask complexity**: System supports multiple masks but generation time doubles per additional mask.

- **Failure signatures:**
  - **Mask scaling issues**: Masks too small produce incomplete subjects (e.g., bicycle without rider); masks too large produce giant subjects. Users struggled to calibrate appropriate sizes.
  - **Vertical motion limitation**: System doesn't support animated mask shapes or vertical trajectories, so actions like jumping, flying, or explosions fail.
  - **Background quality degradation**: Low-resolution Mapillary imagery limits output quality even after upscaling; pixel artifacts persist.
  - **Prompt-model mismatch**: Complex motion descriptions ("squid rolls and explodes") fail to translate through prompts alone.

- **First 3 experiments:**
  1. **Reproduce the baseline comparison**: Implement a minimal image-to-video pipeline with the same Wan2.1 VACE model but without mask/background control. Generate the same replication task shots and measure iteration count and subjective spatial consistency.
  2. **Test mask projection accuracy**: Place actors at known lat/lon offsets from a camera position, manually compute expected screen coordinates using the ENU projection formulas, and compare to Unity-rendered mask positions. Identify any systematic projection errors.
  3. **Stress-test background preservation**: Generate videos with masks covering varying percentages of the frame (10%, 30%, 50%, 70%) and evaluate whether unmasked regions remain pixel-identical to the input panorama sequence. Document any boundary artifacts or bleeding.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can video generation systems incorporate camera translation (dolly, tracking shots) when constrained to discrete panorama viewpoints from street view imagery?
- Basis in paper: [explicit] Section 8.1 states the system "currently lacks translation control" and participants attempted to sketch trajectories to simulate dynamic movements but the system was unable to accommodate these requests.
- Why unresolved: Street view imagery provides only discrete viewpoints at fixed locations, with no continuous spatial data between panorama nodes.
- What evidence would resolve it: Integration of multi-view reconstruction techniques (e.g., panoramic NeRF) or autoregressive video diffusion (e.g., Streetscapes) enabling generated intermediate viewpoints; user study comparing shot diversity with/without translation control.

### Open Question 2
- Question: Can the street view imagery-driven approach be extended to indoor environments or custom 3D captures while preserving the same interaction paradigm?
- Basis in paper: [explicit] Section 4.3.1 states "Street view imagery usually does not cover building interiors. As a result, our approach is limited to outdoor shots."
- Why unresolved: Commercial street view datasets lack indoor coverage; indoor 3D reconstruction requires different capture and processing pipelines.
- What evidence would resolve it: Prototype supporting manually captured 4D Gaussian splatting or NeRF reconstructions as spatial references; evaluation of spatial consistency and usability compared to outdoor street view workflow.

### Open Question 3
- Question: How can systems balance the grounding benefits of real-world street view imagery with the creative flexibility to diverge stylistically from that imagery?
- Basis in paper: [explicit] Section 8.3 notes "this anchoring also constrained the expressive range of their outputs. Most clips reproduced the realism of the underlying imagery, limiting participants who wished to explore more stylized, cinematic, or speculative directions."
- Why unresolved: The inpainting pipeline preserves background pixels as-is; style transfer risks disrupting spatial consistency that the system is designed to maintain.
- What evidence would resolve it: Implementation of layout-conditioned style transfer that modifies appearance while preserving geometry; user study measuring creative satisfaction across tasks requiring non-realistic aesthetics.

## Limitations
- System cannot handle camera translation (dolly shots) due to discrete panorama viewpoints in street view data
- Mask scaling remains a significant user pain point with no automated guidance, leading to inconsistent results
- 12-person filmmaker study has limited statistical power and may not generalize to non-professional users

## Confidence

**High confidence**: The spatial consistency mechanism works as described—anchoring to real-world panoramas provides a stable coordinate frame that persists across iterations. This is directly observable in the system behavior and supported by user study feedback on reduced spatial inconsistency compared to baseline.

**Medium confidence**: The cognitive load reduction claim is supported by NASA-TLX results (p<0.01 for mental demand, effort, frustration) but relies on a small sample size (N=12) and doesn't account for potential learning curves with the map-based interface.

**Low confidence**: The quality of output videos is limited by Mapillary's imagery resolution and coverage gaps, yet the paper doesn't quantify how these technical constraints affect the user experience beyond subjective ratings.

## Next Checks

1. **Mask scaling robustness test**: Systematically vary mask sizes (10%, 30%, 50%, 70% of frame) for the same subject and motion, then measure the consistency of subject completion quality and any background bleeding artifacts. Document whether there's a sweet spot or if the system requires manual tuning for each scenario.

2. **Geographic coverage stress test**: Select 100 random locations across urban/suburban/rural areas and measure Mapillary panorama availability and resolution. For locations with panoramas, attempt to generate videos with varying camera angles and mask sizes to quantify the practical coverage limits and identify systematic quality degradation patterns.

3. **Cross-user consistency study**: Have 5-10 different users independently replicate the same simple scene (e.g., person walking across street) using both Map2Video and baseline I2V. Measure inter-user variance in iteration counts, spatial consistency ratings, and subjective effort scores to determine whether the system's advantages hold across different skill levels and spatial reasoning abilities.