---
ver: rpa2
title: 'ChartQAPro: A More Diverse and Challenging Benchmark for Chart Question Answering'
arxiv_id: '2504.05506'
source_url: https://arxiv.org/abs/2504.05506
tags:
- chart
- question
- answer
- charts
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ChartQAPro, a more diverse and challenging
  benchmark for chart question answering. Unlike existing datasets, ChartQAPro includes
  1,341 charts from 157 diverse sources, covering various chart types including infographics
  and dashboards, and features 1,948 questions spanning multiple types such as multiple-choice,
  conversational, hypothetical, and unanswerable questions.
---

# ChartQAPro: A More Diverse and Challenging Benchmark for Chart Question Answering

## Quick Facts
- arXiv ID: 2504.05506
- Source URL: https://arxiv.org/abs/2504.05506
- Reference count: 28
- Key outcome: ChartQAPro introduces a diverse, challenging benchmark with 1,341 charts from 157 sources, 1,948 multi-type questions, and shows a substantial performance drop for 21 models compared to existing datasets (e.g., Claude Sonnet 3.5: 90.5% on ChartQA, 55.81% on ChartQAPro).

## Executive Summary
ChartQAPro is a new benchmark for chart question answering (CQA) designed to address the limitations of existing datasets, which are often visually homogeneous and focused on simple, factoid questions. It comprises 1,341 charts from 157 diverse sources, including infographics and dashboards, and 1,948 questions across five types: reasoning, conversational, multiple-choice, hypothetical, and fact-checking. The benchmark introduces a relaxed accuracy metric with a 5% relative error tolerance for numeric answers, exact match for years/MCQ/fact-checking, and ANLS for textual answers. Evaluations on 21 models reveal a significant performance drop compared to prior benchmarks, highlighting the need for more robust chart understanding and reasoning capabilities in large vision-language models.

## Method Summary
The ChartQAPro benchmark was constructed through a multi-step process: (1) chart curation from 157 diverse sources to ensure visual variety, (2) question generation by 15 annotators covering five types, (3) relaxed accuracy metrics with 5% tolerance for numeric answers and exact match for years/MCQ/fact-checking, and (4) evaluation of 21 models (closed-source, open-source, and chart-specific) using direct, chain-of-thought, and program-of-thought prompting. Visual diversity was quantified via pairwise cosine distance among CLIP image embeddings, showing ChartQAPro is significantly more diverse than prior benchmarks. The dataset and evaluation code are publicly available.

## Key Results
- ChartQAPro achieves significantly higher visual diversity (average pairwise cosine distance 0.53) than prior benchmarks (e.g., ChartQA: 0.15).
- Performance of all 21 evaluated models drops substantially on ChartQAPro compared to existing datasets; e.g., Claude Sonnet 3.5: 90.5% (ChartQA) â†’ 55.81% (ChartQAPro).
- Chain-of-Thought prompting improves accuracy for closed-source models, while Program-of-Thought prompting is less effective for open-source models due to instruction-following failures.
- Common failure modes include visual perception errors on dense charts, instruction-following failures on complex prompts, and math reasoning errors on multi-step calculations.

## Why This Works (Mechanism)
ChartQAPro works by increasing the visual and question-type diversity of chart question answering benchmarks, forcing models to handle a wider range of real-world scenarios. The relaxed accuracy metric with 5% numeric tolerance and exact match for specific formats reduces false negatives due to minor errors, while still preserving precision where it matters (e.g., years, MCQ). The inclusion of unanswerable and hypothetical questions further challenges models to reason about data presence and implications, rather than just retrieval.

## Foundational Learning

- **Concept: Relaxed Accuracy Metrics for Chart QA**
  - Why needed here: Evaluation must handle numeric tolerance, textual similarity, and exact matches differently to avoid penalizing minor errors while preserving precision where it matters (e.g., years).
  - Quick check question: Given a ground-truth year of 2008 and a prediction of 2009, should a 5% relative-error tolerance mark this as correct? (Answer: No; years require exact match.)

- **Concept: Chain-of-Thought vs. Program-of-Thought Prompting**
  - Why needed here: The paper shows CoT improves closed-source model performance, while PoT is less effective for many open-source models due to instruction-following failures. Understanding when to apply each is critical for experimentation.
  - Quick check question: Which prompting strategy yielded the highest overall accuracy for Claude Sonnet 3.5 on ChartQAPro? (Answer: Chain-of-Thought, 55.81%.)

- **Concept: Visual Diversity Quantification via Embedding Distance**
  - Why needed here: The paper uses pairwise cosine distance among CLIP image embeddings to prove ChartQAPro is more visually diverse than prior benchmarks. This method is generalizable for assessing any visual dataset.
  - Quick check question: If a new benchmark has an average pairwise cosine distance of 0.15 vs. ChartQAPro's 0.53, is it more or less visually diverse? (Answer: Less diverse; lower distance means images are more similar.)

## Architecture Onboarding

- **Component map**: The benchmark consists of three main components: (1) a diverse chart corpus (1,341 images from 157 sources), (2) a multi-type question-answer set (1,948 pairs), and (3) an evaluation pipeline with relaxed accuracy metrics for numeric, textual, MCQ, and fact-checking responses.

- **Critical path**: For a new model, the path is: (1) load chart images and questions, (2) generate answers using direct, CoT, or PoT prompts (templates provided in Appendix A.4), (3) compute accuracy using the provided metric code (relaxed accuracy with 5% numeric tolerance, exact match for years/MCQ/fact-checking, ANLS for text).

- **Design tradeoffs**: The benchmark prioritizes real-world diversity and question complexity over scale (1,948 questions vs. millions in synthetic datasets). This makes it more challenging but limits statistical power for fine-grained ablations. Static screenshots of dashboards forego interactivity, which may underrepresent real-world use but simplifies evaluation.

- **Failure signatures**: Common failure modes include: (1) visual perception errors on dense or unlabeled charts, (2) instruction-following failures on CoT/PoT prompts (especially for open-source models), and (3) math reasoning errors on multi-step calculations. Chart-specific models often overfit to prior benchmarks and generalize poorly.

- **First 3 experiments**:
  1. Establish a baseline on ChartQAPro using direct prompting for your model; compare against reported scores (e.g., Claude Sonnet 3.5: 55.81% with CoT).
  2. Run an ablation by question type (factoid vs. hypothetical vs. unanswerable) to identify which categories your model struggles with most.
  3. Test performance with and without accompanying paragraphs for the subset of charts that include them; quantify the impact of text-chart fusion on accuracy.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the shift from static screenshots to dynamic, interactive chart interfaces impact the performance and architecture of Large Vision-Language Models (LVLMs) in real-world scenarios?
  - Basis in paper: [explicit] The authors explicitly identify this as future work, stating they "plan on expanding the benchmark by introducing dynamic and interactive charts and dashboards," noting that current benchmarks fail to capture real-world interactivity (e.g., hovering, filtering).
  - Why unresolved: Current evaluations rely solely on static visual data, whereas real-world business intelligence tools often require interaction to reveal data, a capability not tested in ChartQAPro.
  - What evidence would resolve it: The creation of a new benchmark with interactive elements and the subsequent evaluation of LVLMs to see if they can successfully execute multi-step interactions to retrieve hidden data.

- **Open Question 2**: Why do Chain-of-Thought (CoT) and Program-of-Thought (PoT) prompting strategies degrade performance in open-source LVLMs on complex chart tasks, and how can this be corrected?
  - Basis in paper: [explicit] In Section 4.4 and 4.5, the authors note that open-source models "perform worse when asked to produce long-form reasoning," often generating code that fails to execute, suggesting a misalignment in training for step-by-step visual reasoning.
  - Why unresolved: This contrasts with the behavior of closed-source models where reasoning traces improve accuracy; the specific failure of open-source models to utilize these strategies in the chart domain remains a technical gap.
  - What evidence would resolve it: Ablation studies comparing standard instruction tuning vs. reasoning-heavy training for open-source models, resulting in models that successfully execute generated code and improve accuracy via CoT.

- **Open Question 3**: Can the creation of large-scale training datasets specifically focused on reasoning formats significantly close the performance gap between chart-specific open-source models and general-purpose closed-source models?
  - Basis in paper: [explicit] The authors explicitly state the aim "to curate a large-scale training dataset in reasoning formats" to develop more proficient models, addressing the finding that current chart-specific models generalize poorly due to limited task diversity.
  - Why unresolved: Current chart-specific models (e.g., ChartGemma) showed near-zero performance on unanswerable questions and struggled with complex visuals, indicating current training data is insufficient for robust reasoning.
  - What evidence would resolve it: Training a new model on the proposed reasoning-focused dataset and demonstrating a substantial increase in ChartQAPro accuracy, particularly on complex or hypothetical questions.

## Limitations
- The benchmark's static nature (screenshots of interactive dashboards) and reliance on a single, publicly released dataset limit generalizability.
- The 5% numeric tolerance, while practical, may mask subtle errors in high-stakes domains.
- Open-source model performance may be further constrained by prompt adherence issues and the lack of model-specific hyperparameter tuning.

## Confidence
- **High confidence**: Claims about the visual and question-type diversity of ChartQAPro relative to prior benchmarks, and the overall performance drop observed across 21 models. The methodology for computing visual diversity and the benchmark construction process are well-documented and reproducible.
- **Medium confidence**: Claims about the relative effectiveness of CoT vs. PoT prompting, and the specific failure modes identified for open-source and chart-specific models. These are based on observed trends but may vary with different model families or hyperparameter settings.
- **Low confidence**: Extrapolation of current results to future, larger, or more diverse models; specific performance expectations for novel architectures not evaluated in the paper.

## Next Checks
1. **Prompt Robustness**: Systematically vary temperature, max tokens, and top-p for a subset of models (e.g., Claude Sonnet 3.5, Qwen-VL2-7B) and quantify the impact on accuracy. Log and categorize unparsable or failed outputs, especially for PoT.
2. **Visual Diversity Replication**: Recompute pairwise cosine distances among CLIP embeddings for ChartQAPro and three prior benchmarks (ChartQA, InfographVQA, InfoChartQA) to independently verify the claim of higher visual diversity.
3. **Error Mode Diagnosis**: Manually inspect and categorize errors for a stratified sample of 100 QA pairs across all question types, focusing on visual perception failures and instruction-following issues. Compare against the paper's error analysis to identify any new or underrepresented failure modes.