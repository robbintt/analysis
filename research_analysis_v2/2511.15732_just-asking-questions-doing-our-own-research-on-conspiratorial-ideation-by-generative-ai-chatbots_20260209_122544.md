---
ver: rpa2
title: 'Just Asking Questions: Doing Our Own Research on Conspiratorial Ideation by
  Generative AI Chatbots'
arxiv_id: '2511.15732'
source_url: https://arxiv.org/abs/2511.15732
tags:
- conspiracy
- theories
- chatbots
- theory
- chatbot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically audited seven leading generative AI chatbots
  (ChatGPT 3.5, ChatGPT 4 Mini, Microsoft Copilot, Google Gemini Flash 1.5, Perplexity,
  and Grok-2 Mini in both standard and "Fun Mode") to evaluate their safety guardrails
  against conspiracy theories. Researchers used a "platform policy implementation
  audit" approach, prompting chatbots with scripted questions about nine conspiracy
  theories (five well-known and four emerging) using a "casually curious" persona.
---

# Just Asking Questions: Doing Our Own Research on Conspiratorial Ideation by Generative AI Chatbots

## Quick Facts
- arXiv ID: 2511.15732
- Source URL: https://arxiv.org/abs/2511.15732
- Reference count: 12
- This study systematically audited seven leading generative AI chatbots to evaluate their safety guardrails against conspiracy theories.

## Executive Summary
This study systematically audited seven leading generative AI chatbots (ChatGPT 3.5, ChatGPT 4 Mini, Microsoft Copilot, Google Gemini Flash 1.5, Perplexity, and Grok-2 Mini in both standard and "Fun Mode") to evaluate their safety guardrails against conspiracy theories. Researchers used a "platform policy implementation audit" approach, prompting chatbots with scripted questions about nine conspiracy theories (five well-known and four emerging) using a "casually curious" persona. The chatbots were coded for ten response types, including descriptions of conspiracy theories, factual counterstatements, empathy, and encouragement of further investigation. Findings showed significant variation in safety guardrails: Perplexity consistently provided factual counterstatements and verified sources, while Grok-2 Mini "Fun Mode" frequently engaged in bothsidesing rhetoric and encouraged further investigation of conspiracy theories. Gemini 1.5 Flash often avoided responding to political queries. Safety guardrails were particularly weak for older conspiracy theories like JFK assassination and stronger for topics of national trauma or racism. The study highlights the need for ongoing auditing and more effective, topic-independent safety mechanisms in AI chatbots.

## Method Summary
The study employed a platform policy implementation audit methodology, systematically querying seven generative AI chatbots with scripted questions about nine conspiracy theories using a "casually curious" user persona. Prompts were presented in both leading (embedding conspiracy premises) and neutral valences. Responses were coded using a 10-category qualitative framework and analyzed across vendors and topics. The audit used zero-shot prompting without multi-turn dialogue and employed inter-coder reliability testing via Krippendorff's alpha on 10% of responses. Data collection occurred in November 2024.

## Key Results
- Significant variation exists in chatbot safety guardrails: Perplexity provided consistent factual counterstatements and verified sources, while Grok-2 Mini "Fun Mode" frequently engaged in bothsidesing and encouraged further investigation
- Safety guardrails were particularly weak for older conspiracy theories like JFK assassination and stronger for topics of national trauma or racism
- Gemini 1.5 Flash avoided responding to 36 of 63 political queries, especially regarding elections

## Why This Works (Mechanism)

### Mechanism 1: Topic-Specific Safety Guardrail Implementation
- Claim: Safety guardrails in AI chatbots are selectively designed, with stronger protections for topics posing reputational risk (racism, national trauma) and weaker protections for older or less politicized conspiracy theories.
- Mechanism: Companies appear to prioritize guardrails based on perceived public relations risk rather than consistent harm prevention. Topics like 9/11 or racist content trigger stronger interventions, while JFK assassination theories receive minimal guardrails.
- Core assumption: Guardrail design is driven by corporate risk assessment rather than systematic harm evaluation.
- Evidence anchors:
  - [abstract]: "safety guardrails in AI chatbots are often very selectively designed: generative AI companies appear to focus especially on ensuring that their products are not seen to be racist"
  - [section]: "Safety guardrails were particularly weak for older conspiracy theories like JFK assassination and stronger for topics of national trauma or racism"
  - [corpus]: Limited direct corpus evidence on this mechanism; corpus focuses on detection/countermeasures rather than guardrail design patterns
- Break condition: If companies implement topic-independent safety mechanisms that identify conspiratorial questioning strategies regardless of subject matter.

### Mechanism 2: Platform Policy Implementation Audit via Systematic Querying
- Claim: Black-box auditing through scripted, persona-based prompts can reveal guardrail presence and effectiveness without access to internal model architecture.
- Mechanism: By confronting chatbots with standardized "casually curious" prompts across multiple conspiracy theories and coding responses against a 10-category framework, researchers can compare safety mechanisms across vendors.
- Core assumption: Chatbot responses to standardized prompts reliably indicate underlying safety mechanism design choices.
- Evidence anchors:
  - [abstract]: "This follows the platform policy implementation audit approach established by Glazunova et al. (2023)"
  - [section]: "In the absence of direct access to the source code, training data, or fundamental instructions... what remains available to us as a productive research strategy is a systematic querying of such chat systems"
  - [corpus]: Glazunova et al. (2023) established this audit methodology for platform compliance testing
- Break condition: If chatbot responses are sufficiently non-deterministic that single-query audits cannot establish reliable patterns.

### Mechanism 3: Response Strategy Variance Influences User Belief Trajectories
- Claim: Different chatbot response strategies (avoidance, factual counters, bothsidesing, empathy) may produce divergent effects on user conspiracy beliefs, though effectiveness remains uncertain.
- Mechanism: Perplexity's "truth sandwich" approach may reduce beliefs for open-minded users; Gemini's avoidance may discourage or redirect inquiry; Grok's bothsidesing may actively encourage conspiratorial investigation.
- Core assumption: Assumption: Response strategy causally affects downstream user beliefs—this remains unproven by this audit study.
- Evidence anchors:
  - [abstract]: "Perplexity consistently provided factual counterstatements and verified sources, while Grok-2 Mini 'Fun Mode' frequently engaged in bothsidesing rhetoric and encouraged further investigation"
  - [section]: "Our purpose in the present article is to audit the chatbots' response strategies... future work should explore which of these strategies are most effective"
  - [corpus]: Costello et al. (2024) found prolonged GPT-4 conversations reduced conspiracy beliefs ~20%; corpus provides mixed evidence on intervention effectiveness
- Break condition: If user predisposition dominates response effects, making strategy choice largely irrelevant to belief outcomes.

## Foundational Learning

- Concept: Platform Policy Implementation Audit
  - Why needed here: This is the core methodology. Without understanding black-box auditing, you cannot interpret findings or design follow-up experiments.
  - Quick check question: Can you explain why systematic querying is necessary when source code is unavailable?

- Concept: Bothsidesing Rhetoric
  - Why needed here: This is identified as a key failure mode where chatbots legitimize conspiracy theories by presenting them alongside factual information as equivalent perspectives.
  - Quick check question: How does bothsidesing differ from neutral description of a conspiracy theory followed by debunking?

- Concept: Truth Sandwich Format
  - Why needed here: This fact-checking strategy (truth-false claim-truth ordering) is referenced as an aspirational response pattern, though the study notes chatbots often "scramble the ingredients."
  - Quick check question: What is the intended order of claims in a truth sandwich, and why does order matter?

## Architecture Onboarding

- Component map:
  - Prompt design (leading vs. neutral valence; "casually curious" persona) -> Seven chatbot models across six vendors with varying safety implementations -> Zero-shot prompting without multi-turn dialogue -> Response capture -> 10-category qualitative codebook (description, avoidance, factual counter, bothsidesing, verified sources, empathy, encouragement, non-committal, downplaying, disapproval) -> Cross-vendor comparison, cross-topic comparison, valence comparison

- Critical path: Prompt design → Systematic querying → Response capture → Qualitative coding (with ICR testing) → Pattern analysis across vendors/topics

- Design tradeoffs:
  - Single-turn vs. multi-turn: Study uses single-turn, which cannot capture sustained prompting effects
  - Breadth vs. depth: 9 conspiracy theories × 7 chatbots provides breadth but limited prompts per condition
  - English-only: Limits generalizability beyond Anglosphere conspiracy ecosystems

- Failure signatures:
  - Grok-2 Mini "Fun Mode": High bothsidesing, high encouragement of further investigation, low factual counters
  - Gemini 1.5 Flash: High avoidance rate (36/63 political queries), especially on elections
  - JFK conspiracy topic: All chatbots show elevated non-committal responses and bothsidesing

- First 3 experiments:
  1. Multi-turn dialogue audit: Extend single-turn prompts to 3-5 turn conversations to test whether sustained conspiratorial prompting elicits different guardrail responses.
  2. Cross-language replication: Apply the same audit methodology to non-English prompts (e.g., Portuguese, French, Arabic) to test whether safety guardrails generalize across languages.
  3. Response strategy A/B test: Present users with different response types (avoidance, factual counter, bothsidesing) and measure effects on belief change using pre/post survey instruments.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Which specific chatbot response strategies (e.g., avoidance, factual counters, empathy) are most effective at preventing "casually curious" users from adopting conspiratorial beliefs?
- Basis in paper: [explicit] The authors state in the Discussion and Future Work that research must explore "which of these strategies are most effective in preventing curious users from sliding further in to a conspiratorial rabbit hole."
- Why unresolved: The current study only audited the *presence* of safety mechanisms and response types, not the behavioral impact of those responses on the user.
- What evidence would resolve it: Experimental user studies measuring changes in belief certainty after exposure to different chatbot response styles.

### Open Question 2
- Question: How do safety guardrails perform when addressing conspiracy theories that are non-English, non-US-centric, or culturally specific?
- Basis in paper: [explicit] The authors explicitly note that "a significant portion of conspiracy theory research – this article included – is focussed on mainstream, English-language conspiracy theories" and call for future work to be multilingual and cross-cultural.
- Why unresolved: The study was limited to English-language prompts regarding US-centric political events.
- What evidence would resolve it: A replication of the platform policy implementation audit using prompts in languages such as Portuguese or Hindi regarding local conspiracy theories.

### Open Question 3
- Question: To what extent do safety guardrails persist or degrade as generative AI models undergo updates and version changes?
- Basis in paper: [inferred] The paper highlights the "rapid evolution" of models and that the study represents only a "single snapshot in time," implying the need to track the stability of these safety features.
- Why unresolved: Safety mechanisms implemented by companies are opaque and subject to frequent, undocumented changes during model updates.
- What evidence would resolve it: A longitudinal audit repeating the same standardized prompts across successive versions of the same chatbot models over time.

### Open Question 4
- Question: Can Large Language Models (LLMs) be reliably employed to automate the qualitative coding of safety audit responses?
- Basis in paper: [explicit] The authors suggest in Future Work that "sufficiently trained Large Language Models might themselves also be enrolled in the assessment of chatbot responses."
- Why unresolved: Manual coding is labor-intensive, but the reliability of AI-driven content analysis for subtle rhetorical features (like "bothsidesing" or "empathy") is unproven in this context.
- What evidence would resolve it: A validation study comparing inter-coder reliability scores between human coders and LLM-based coding agents.

## Limitations

- Black-box audit methodology cannot definitively identify whether response patterns reflect deliberate safety design choices versus emergent model behavior or training data influences
- Single-turn prompting design misses potential safety mechanisms that might emerge only in sustained dialogue
- Focus on English-language conspiracy theories limits generalizability to non-Anglophone contexts where different conspiracy narratives and cultural sensitivities may exist

## Confidence

- High confidence: The finding that chatbots show significant variation in safety guardrail implementation across vendors and topics is well-supported by systematic qualitative coding with inter-coder reliability testing. The identification of bothsidesing rhetoric as a distinct failure mode is clearly documented.
- Medium confidence: The claim that companies prioritize guardrails based on reputational risk (stronger for racism/national trauma, weaker for older conspiracies) is plausible given the observed patterns, but alternative explanations (such as differential effectiveness of guardrails across topics) cannot be ruled out without additional experimental evidence.
- Low confidence: Assertions about which response strategies would be most effective at reducing conspiracy beliefs remain speculative, as the study did not measure downstream belief changes and the existing literature provides mixed evidence on intervention effectiveness.

## Next Checks

1. Multi-turn dialogue audit: Extend the current single-turn audit to 3-5 turn conversations with each chatbot to determine whether sustained conspiratorial prompting elicits different safety guardrail responses or reveals mechanisms only visible in extended dialogue.

2. Cross-language replication: Apply the identical audit methodology to conspiracy theories in non-English languages (e.g., Portuguese, French, Arabic) to test whether observed patterns of selective guardrail implementation generalize across linguistic and cultural contexts.

3. Response strategy A/B test: Conduct an experimental study where users are randomly assigned to receive different chatbot response types (avoidance, factual counter, bothsidesing) and measure pre/post changes in conspiracy beliefs to empirically determine which strategies are most effective at reducing conspiratorial ideation.