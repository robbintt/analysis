---
ver: rpa2
title: What is an "Abstract Reasoner"? Revisiting Experiments and Arguments about
  Large Language Models
arxiv_id: '2507.22457'
source_url: https://arxiv.org/abs/2507.22457
tags:
- reasoning
- tasks
- visual
- abstract
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper re-examines claims that large language models (LLMs)
  are not "abstract reasoners" by exploring whether their performance on challenging
  reasoning tasks can be significantly improved with targeted input-level adaptation.
  The authors first replicate findings that frozen, pretrained LLMs perform poorly
  on tasks requiring pattern inference from limited examples.
---

# What is an "Abstract Reasoner"? Revisiting Experiments and Arguments about Large Language Models

## Quick Facts
- arXiv ID: 2507.22457
- Source URL: https://arxiv.org/abs/2507.22457
- Reference count: 31
- This paper re-examines claims that large language models (LLMs) are not "abstract reasoners" by exploring whether their performance on challenging reasoning tasks can be significantly improved with targeted input-level adaptation.

## Executive Summary
This paper challenges the claim that large language models are not "abstract reasoners" by demonstrating that frozen, pretrained transformers can achieve strong performance on challenging reasoning tasks when their input representations are adapted. The authors replicate findings showing frozen LLMs perform poorly on tasks requiring pattern inference from limited examples, but then show that fine-tuning only the embedding layer—without modifying the transformer blocks—can dramatically boost performance to match or exceed full model fine-tuning. This finding extends to visual reasoning tasks, where a frozen LLM paired with a tuned visual encoder can perform well. The paper concludes that while LLMs are not abstract reasoners in a zero-shot setting, they possess highly transferable inferential capabilities that can be unlocked through task-specific input adaptation, inviting a deeper discussion about what it means to be an "abstract reasoner."

## Method Summary
The paper evaluates frozen LLaMA2-7b performance on abstract reasoning tasks (ARC, PVR, ACRET, RAVEN for text; ACRE, MEWL for visual) that require inferring patterns from limited examples. Three experimental settings are compared: (1) frozen LLM evaluation, (2) fine-tuning only the embedding layer with AdamW for 50 epochs with early stopping, and (3) for visual tasks, freezing the LLM and fine-tuning the visual encoder plus projection layer. Visual encoders are 2-layer ViTs with 4 heads and 768 dimensions. The paper also compares embedding-only tuning against LoRA full fine-tuning baselines to determine whether reasoning capabilities reside in the frozen transformer blocks.

## Key Results
- Frozen LLaMA2-7b performs poorly on abstract reasoning tasks in zero-shot settings (e.g., ~26% on ACRET-Text, ~13% on RAVEN)
- Fine-tuning only the embedding layer can achieve near-perfect performance on the same tasks, often matching or exceeding LoRA fine-tuning results
- For visual reasoning, object-centric inputs (vs. raw images) enable significantly better performance with a frozen LLM and tuned visual encoder
- Embedding fine-tuning provides data efficiency, with 80 examples matching performance of 8,000 examples from scratch

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Frozen transformer blocks possess reusable inferential capabilities that can be activated through input representation alignment.
- Mechanism: The embedding layer functions as a task-specific translator, mapping domain inputs into a representation space the pretrained transformer expects. Once aligned, frozen blocks apply learned attention patterns and relational computations to solve novel reasoning tasks.
- Core assumption: Transformers acquire general-purpose computational patterns during pretraining that are representation-agnostic but input-format-sensitive.
- Evidence anchors:
  - [abstract] "while LLMs indeed perform poorly in a zero-shot setting, even tuning a small subset of parameters for input encoding can enable near-perfect performance"
  - [section 5] "LLaMA2 with a finetuned embedding layer can perform on par with the LoRA-finetuned LLMs"
  - [corpus] No direct corpus support for this specific mechanism; related work focuses on argumentation and reasoning evaluation rather than input adaptation
- Break condition: If embedding fine-tuning primarily encodes task-specific heuristics rather than enabling generalization, or if benefits vanish on held-out task variants with different surface features.

### Mechanism 2
- Claim: Object-centric visual representations dramatically improve frozen LLM reasoning compared to raw pixel inputs.
- Mechanism: When visual encoders output structured object-level features (attributes, positions), frozen transformers can leverage pretrained relational reasoning patterns. Raw images require the encoder to simultaneously solve perception and representation alignment, degrading performance.
- Core assumption: Language pretraining instills capacity for symbolic relational reasoning that transfers to structured visual inputs.
- Evidence anchors:
  - [section 6.4] "The large performance gap between LLaMA2-7b-Image and -Object (e.g., average of 41.1% versus 69.8% on MEWL), indicating that object-centric information is important"
  - [section 6] "a frozen pre-trained LLM can perform well on visual reasoning tasks as long as the visual encoder is fine-tuned on in-domain task data"
  - [corpus] No direct corpus support
- Break condition: If scaling visual encoder capacity closes the gap between image and object inputs, or if object-centric representations require task-specific detection that doesn't generalize.

### Mechanism 3
- Claim: Positive transfer from embedding fine-tuning derives primarily from low-level feature alignment, not abstract reasoning logic.
- Mechanism: Embedding adaptation learns to map task-relevant tokens/visual features into pretrained representation space. The transfer benefits come from reusing learned feature representations rather than transferring reasoning schemas.
- Core assumption: Generalization reflects representation quality rather than abstract rule acquisition.
- Evidence anchors:
  - [section 5] "The fairly small gap between the center-single and center-single-shuffled lines suggests that the positive transfer is primarily explained by the lower-level visual features rather than the reasoning logic of the tasks"
  - [section 5] Data efficiency analysis showing 80-example embedding tuning matches 8k-example from-scratch training
  - [corpus] No direct corpus support
- Break condition: If shuffled-label training eliminates all transfer benefits, or if embedding tuning shows strong cross-domain transfer to tasks with entirely different feature vocabularies.

## Foundational Learning

- **Frozen vs. Fine-tuned Evaluation Paradigm**
  - Why needed: The paper's central experiment compares zero-shot frozen performance against embedding-only and full fine-tuning to isolate what capabilities exist in pretrained weights.
  - Quick check question: Can you explain why freezing transformer blocks while tuning embeddings tests whether reasoning capability is already present versus learned during adaptation?

- **Multimodal LLM Input Pipeline**
  - Why needed: Section 6 requires understanding how visual encoders, projection layers, and language backbones connect.
  - Quick check question: Can you trace the path from raw image → visual encoder → projection → LLM embedding space and explain what each component contributes?

- **Zero-Shot Transfer as Evaluation Criterion**
  - Why needed: The paper challenges zero-shot performance as the definitive test for "abstract reasoning," arguing adaptation requirements don't preclude reasoning capability.
  - Quick check question: Can you articulate why Dennett's thermostat analogy undermines zero-shot transfer as a necessary condition for intelligence?

## Architecture Onboarding

- Component map:
  - Token Embedding Layer: Maps discrete tokens → dense vectors (trainable in embedding-only experiments)
  - Transformer Blocks: Attention + FFN layers (frozen; tested hypothesis that reasoning lives here)
  - Visual Encoder: ViT (2-layer, 4-head, 768-dim) or symbolic encoder (attribute embeddings + location linear layer)
  - Linear Projection: Maps visual representations → language embedding dimension
  - LoRA Adapter: Low-rank matrices injected into all layers (full fine-tuning baseline)

- Critical path:
  1. Establish zero-shot baseline with frozen model
  2. Fine-tune embedding layer only (50 epochs, AdamW, early stopping on validation)
  3. Compare against LoRA full fine-tuning upper bound
  4. For visual tasks: train visual encoder + projection with frozen LLM backbone

- Design tradeoffs:
  - Embedding-only vs. LoRA: Embedding preserves pretrained weights but requires task-specific data; LoRA achieves higher ceilings but obscures what's pretrained vs. learned
  - Symbol vs. Object vs. Image inputs: Symbol assumes oracle perception; Object requires detection; Image is end-to-end but hardest
  - Data efficiency vs. generality: Embedding tuned on one task variant shows limited transfer to others

- Failure signatures:
  - Zero-shot performance near random baseline (expected; confirms frozen model needs adaptation)
  - Embedding-only matches LoRA performance (suggests reasoning in frozen blocks)
  - Large Image vs. Object performance gap (indicates perception bottleneck)
  - Center-single vs. shuffled-label transfer similar (indicates feature-level rather than logic-level transfer)

- First 3 experiments:
  1. Replicate Table 1 zero-shot results on ARC, PVR, ACRET, RAVENT using frozen LLaMA2-7b to confirm baseline
  2. Run embedding-only fine-tuning on same tasks, comparing against LoRA to validate Figure 3 findings
  3. Implement visual encoder training from scratch on ACRE with Symbol/Object/Image variants to reproduce Table 3 object-centric gap

## Open Questions the Paper Calls Out

- **Open Question 1**: Is the primary goal of evaluating abstract reasoning in LLMs to measure human-like cognition or to facilitate efficient technological progress?
  - Basis in paper: [explicit] The authors argue in Section 7 that the community must determine why it cares about abstract reasoning—whether for "human-likeness" or "efficient technological progress"—to properly interpret experimental results.
  - Why unresolved: Current benchmarks conflate these distinct goals, leading to ambiguous conclusions where zero-shot failures are interpreted as fundamental reasoning deficits rather than alignment issues.
  - What evidence would resolve it: The development and adoption of distinct evaluation frameworks: one benchmarking cognitive plausibility and another measuring engineering efficiency/adaptability.

- **Open Question 2**: Does fine-tuning the embedding layer merely reformat inputs, or does it implicitly encode task-specific processing logic?
  - Basis in paper: [inferred] Section 7 discusses the ambiguity of the "GOFAI" analogy, noting that tuning the input layer might "encode some task-specific processing as well" rather than just re-representing data.
  - Why unresolved: It is difficult to disentangle whether the performance boost stems from the frozen transformer's generalized reasoning or from hidden computation learned within the adapted input layers.
  - What evidence would resolve it: Mechanistic interpretability studies (e.g., probing classifiers) analyzing the geometric properties of the tuned embeddings to separate representational alignment from reasoning logic.

- **Open Question 3**: To what degree does input-level adaptation transfer across diverse reasoning datasets or task domains?
  - Basis in paper: [explicit] The abstract states that the finetuning demonstrated in the paper "does not necessarily transfer across datasets," highlighting a limitation in the generalizability of the proposed method.
  - Why unresolved: While the paper demonstrates that frozen blocks can perform well with specific tuning, the boundaries of this transferability (e.g., cross-domain generalization) remain unmapped.
  - What evidence would resolve it: Systematic cross-domain experiments where input layers tuned on one reasoning task (e.g., visual analogies) are tested on distinct tasks (e.g., symbolic math) without further updates.

- **Open Question 4**: Do the findings regarding frozen transformer blocks with adapted inputs generalize to LLMs with different parameter scales or architectures?
  - Basis in paper: [inferred] Section A (Limitations) notes that experiments focused mainly on LLaMA2-7b due to compute constraints, leaving the behavior of other architectures unexplored.
  - Why unresolved: Different model families or sizes may possess different inductive biases or capacities, which could alter the efficacy of freezing the core transformer while adapting inputs.
  - What evidence would resolve it: Replication of the specific input-adaptation protocol on a diverse range of model families (e.g., GPT, Mistral) and parameter scales.

## Limitations

- The paper does not conclusively resolve what constitutes "abstract reasoning" in LLMs, leaving open whether observed performance reflects genuine abstract reasoning or sophisticated pattern matching within learned representation spaces.
- The distinction between low-level feature alignment and abstract logic transfer remains unresolved, with shuffled-label experiments suggesting feature-level rather than logic-level transfer.
- The evaluation focuses on narrow task families (pattern completion, visual reasoning) that may not generalize to broader notions of abstract reasoning.

## Confidence

**High Confidence**: Claims that frozen LLMs perform poorly on zero-shot reasoning tasks and that embedding-only fine-tuning can match full LoRA fine-tuning performance. These claims are directly supported by experimental results and replication is straightforward.

**Medium Confidence**: Claims about the mechanism of transfer—specifically that pretrained transformers possess reusable inferential capabilities activated through input representation alignment. While supported by empirical results, alternative explanations (task-specific heuristics, representation engineering) cannot be fully ruled out without additional ablation studies.

**Low Confidence**: The broader philosophical conclusion that these results "invite a deeper discussion about what it means to be an 'abstract reasoner'." This interpretation extends beyond the empirical findings and conflates technical performance with cognitive capability.

## Next Checks

1. **Cross-Domain Transfer Validation**: Test whether embedding fine-tuning on one reasoning task family (e.g., ARC) provides transfer benefits to structurally similar but semantically distinct tasks (e.g., numerical vs. symbolic patterns). This would clarify whether benefits derive from abstract reasoning transfer or task-specific feature learning.

2. **Architecture Ablation Study**: Systematically vary transformer block depth and embedding dimension while maintaining performance to determine the minimum architecture requirements for the observed reasoning capabilities. This would help distinguish between general reasoning capacity and overfitting to specific task structures.

3. **Zero-Shot Out-of-Distribution Testing**: Evaluate fine-tuned models on held-out reasoning tasks with different surface features (different number ranges, visual styles, or symbolic representations) to assess whether performance reflects abstract reasoning or pattern memorization within learned feature spaces.