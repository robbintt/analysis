---
ver: rpa2
title: 'Yan: Foundational Interactive Video Generation'
arxiv_id: '2508.08601'
source_url: https://arxiv.org/abs/2508.08601
tags:
- interactive
- video
- generation
- data
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Yan is a foundational framework for interactive video generation
  that integrates high-fidelity simulation, prompt-controllable generation, and real-time
  editing into a unified pipeline. It addresses the limitations of prior methods that
  struggle to balance visual fidelity, interactivity, and editing flexibility by disentangling
  interactive mechanics simulation from visual rendering and leveraging hierarchical
  captioning for stable content generation.
---

# Yan: Foundational Interactive Video Generation

## Quick Facts
- **arXiv ID:** 2508.08601
- **Source URL:** https://arxiv.org/abs/2508.08601
- **Reference count:** 10
- **Primary result:** Foundational interactive video generation framework achieving 1080P/60FPS with real-time editing

## Executive Summary
Yan presents a unified framework for interactive video generation that integrates high-fidelity simulation, prompt-controllable generation, and real-time editing. The framework addresses the challenge of balancing visual fidelity, interactivity, and editing flexibility by disentangling interactive mechanics simulation from visual rendering. Through hierarchical captioning and aggressive latent compression, Yan achieves real-time performance while maintaining content coherence. Evaluated in a 3D game environment, it demonstrates capabilities for both game-level simulation and open-domain content creation.

## Method Summary
Yan comprises three core components: Yan-Sim (high-fidelity simulator), Yan-Gen (prompt-driven generator), and Yan-Edit (interactive editor). The system uses a 3D-VAE with 32x spatial and 2x temporal compression, expanding latent channels to 16 to maintain fidelity. Yan-Sim learns mechanics through depth maps and action conditioning, while Yan-Gen uses hierarchical captions (global scene + local events) for stable generation. Yan-Edit disentangles structure from style, allowing on-the-fly modifications. The framework employs causal attention for real-time interaction, distribution matching distillation for speed, and classifier-free guidance for multi-modal conditioning.

## Key Results
- Achieves 1080P/60FPS rendering with low latency (<20ms for Sim, 60-80ms for Gen)
- Supports diverse action spaces and cross-domain generalization via cross-attention
- Enables real-time editing with structure and style modifications during interaction
- Demonstrates stable long-horizon generation through hierarchical caption anchoring

## Why This Works (Mechanism)

### Mechanism 1: High-Ratio Latent Compression for Throughput
- **Claim:** Real-time 60FPS simulation is conditional on aggressive spatial-temporal compression in the VAE, rather than solely relying on diffusion step reduction.
- **Mechanism:** The 3D-VAE downsamples spatially by 32x and temporally by 2x. To prevent information loss at this ratio, the latent channel dimension is expanded to 16. This shrinks the compute workload for the DiT backbone while preserving sufficient signal for high-fidelity reconstruction.
- **Core assumption:** The expanded channel dimension (C=16) adequately compensates for the loss of spatial granularity in 1080p inputs.
- **Evidence anchors:** [Section 5.1.1] "We enhance spatial compression... raising the spatial downsampling factor from 8 to 32... expand the latent channel dimension C to 16." [Table 2] Demonstrates "Low Latency (0.11s)" compared to other world simulators.
- **Break condition:** If small, high-frequency details (like thin wires or distant text) become illegible, the channel expansion is insufficient for the compression ratio.

### Mechanism 2: Hierarchical Context Anchoring for Anti-Drifting
- **Claim:** Long-horizon video generation without semantic drift relies on a two-level caption hierarchy to decouple static world rules from dynamic events.
- **Mechanism:** A "Global Caption" defines immutable topology and style (the "World"), while a "Local Caption" describes transient interactions. The model is conditioned on both, anchoring the autoregressive process to a static reference point to prevent error accumulation.
- **Core assumption:** The vision-language model (Qwen2.5-VL) can consistently separate "static layout" from "dynamic events" across diverse scenarios.
- **Evidence anchors:** [Section 5.2.1] "This dual-context strategy is crucial for producing videos that are both globally coherent... and locally accurate." [Corpus: Spatia] A neighbor paper suggests using explicit spatial memory for consistency; Yan solves this via semantic hierarchy instead.
- **Break condition:** If the model generates a permanent structure (e.g., a bridge) that disappears after 100 frames, the Global Caption conditioning is failing to override local noise.

### Mechanism 3: Depth-Disentangled Physics for Editing
- **Claim:** Interactive video editing that preserves game mechanics requires explicitly disentangling the physics simulation (structure) from the visual rendering (style).
- **Mechanism:** Yan-Edit runs a dual-stream process: a **Mechanics Simulator** (Yan-Sim) generates depth maps based on collision logic, while a **Visual Renderer** (Yan-Gen + ControlNet) paints the RGB frames. Text prompts modify the renderer (style) or the simulator (structure) independently.
- **Core assumption:** Depth maps contain sufficient information to represent all relevant interactive mechanics (collision, occlusion, ground detection).
- **Evidence anchors:** [Section 5.3.1] "A hybrid model Yan-Edit that disentangles interactive mechanics simulation from visual rendering." [Figure 14] Illustrates the split path where Action/Structure drives Depth, and Style drives RGB.
- **Break condition:** If a style change (e.g., "crystal floor") accidentally alters the friction or collision properties in the output video, the disentanglement has failed.

## Foundational Learning

- **Concept: Causal Temporal Attention**
  - **Why needed here:** Unlike standard video diffusion that looks at the whole clip, Yan uses causal masking so frame $t$ only attends to $t-1$. This is mandatory for real-time interaction where future frames do not exist yet.
  - **Quick check question:** In the attention matrix of Yan-Sim, should the upper triangle (future) be masked out or kept?

- **Concept: Distribution Matching Distillation (DMD)**
  - **Why needed here:** Yan-Gen uses DMD to convert a slow teacher model into a fast few-step (4-step) generator by matching gradients between real and fake data distributions.
  - **Quick check question:** Does DMD require a pre-trained "discriminator" network, or does it use score functions from diffusion models? (Answer: It uses score functions/real-fake score networks).

- **Concept: Classifier-Free Guidance (CFG) for Multi-Modal Inputs**
  - **Why needed here:** Yan-Gen must balance diverse inputs (Text, Image, Action). CFG allows the model to amplify the influence of these conditional inputs during generation.
  - **Quick check question:** If the `guidance_scale` is set to 1.0, does the model output the conditional result or the unconditional result?

## Architecture Onboarding

- **Component map:** Action Signals, Text/Image Prompts -> Yan-Sim (3D-VAE + DiT with Shift-Window Denoising) -> Yan-Gen (Wan-Base + LoRA + Causal Attention) -> 1080p RGB frames @ 60FPS (Sim) or 12-30FPS (Gen)
- **Critical path:**
  1. Data: Agent collection -> Filtering -> Hierarchical Captioning (Global/Local)
  2. Sim Training: Train 3D-VAE (32x compression) -> Train DiT with Diffusion Forcing
  3. Gen Training: Fine-tune Wan-Base -> Autoregressive Post-training -> DMD Distillation
- **Design tradeoffs:**
  - VAE Compression (32x) vs. Detail: Increases speed but risks losing fine texture; mitigated by increasing latent channels to 16
  - FP8 Quantization vs. Stability: Essential for 60FPS on consumer GPUs (RTX 4060/H20) but introduces quantization noise
- **Failure signatures:**
  - "Rubber-banding" Physics: The model reverts to a mean state; indicates weak action-conditioning strength
  - Semantic Drift: The environment geometry slowly morphs; indicates missing Global Caption or insufficient KV-Cache context
  - Latency Spikes: Inference time exceeds 16ms (60FPS); usually caused by CPU-GPU transfer overhead or suboptimal CUDA graphs
- **First 3 experiments:**
  1. Latent Reconstruction Test: Pass 1080p game frames through the 32x compressed VAE decoder only. Check if UI text and distant objects remain readable.
  2. Ablation on Hierarchical Captions: Run Yan-Gen with *only* local captions. Measure the frame at which the scene layout first contradicts the initial frame (drift point).
  3. Inference Latency Profiling: Benchmark Yan-Sim with and without the "Shift Window Denoising" and KV-Cache. Verify the specific speedup claimed (target: <0.11s).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can spatial and temporal consistency be rigorously maintained during infinite, long-duration interactive generation to prevent semantic drift?
- **Basis in paper:** [explicit] The authors state in the Limitations section that "ensuring visual consistency across long video durations remains a challenge" and requires "further improvements in spatial and temporal consistency."
- **Why unresolved:** Autoregressive generation is prone to error accumulation and exposure bias, causing the model to drift from the initial global context over extended sessions.
- **What evidence would resolve it:** Quantitative metrics (e.g., temporal PSNR/SSIM) and human evaluations showing stable scene geometry and identity over thousands of continuous interaction steps without degradation.

### Open Question 2
- **Question:** Can the action space and physics simulation learned from game environments effectively generalize to unstructured, complex real-world scenarios?
- **Basis in paper:** [explicit] The paper notes that "the action space and interaction complexity are still bounded by the underlying game environment, potentially restricting extensibility to some real-world applications."
- **Why unresolved:** Game engines rely on discrete, deterministic rules, whereas the physical world is continuous and stochastic; it is unclear if game-trained models capture the nuance of real physics or merely pixel correlations.
- **What evidence would resolve it:** Successful zero-shot or few-shot transfer of the Yan-Sim or Yan-Edit modules to real-world robotics control datasets or physical simulations with comparable fidelity.

### Open Question 3
- **Question:** What architectural optimizations are required to achieve 1080p/60FPS AAA-level simulation on consumer-grade hardware rather than high-end GPUs?
- **Basis in paper:** [explicit] The authors highlight that "Yanâ€™s high-fidelity, real-time performance is currently enabled by inference on powerful GPUs," which "may limit accessibility for users with limited hardware resources."
- **Why unresolved:** The current reliance on 3D-VAE and iterative denoising diffusion (even with distillation) demands significant VRAM and compute bandwidth typically found only in data center cards (e.g., NVIDIA H20).
- **What evidence would resolve it:** Demonstration of the framework running at real-time speeds (30+ FPS) on standard consumer GPUs (e.g., RTX 3060/4060) without significant loss in visual quality or mechanics accuracy.

## Limitations

- Proprietary dataset ("Yuan Meng Star") limits reproducibility and generalizability assessment
- Aggressive 32x spatial compression with 16 latent channels may lose fine-grained details like UI text
- Current performance relies on powerful GPUs (RTX 4060/H20), limiting accessibility for users with limited hardware resources

## Confidence

- **High Confidence:** The core architectural approach (disentangling mechanics from rendering, using hierarchical captions for stability) is logically sound and well-supported by ablation results.
- **Medium Confidence:** The claimed performance metrics (1080P/60FPS, <20ms latency) are plausible given the aggressive compression and quantization strategies, but the lack of open-source code makes independent verification difficult.
- **Low Confidence:** The generalizability of the framework to domains outside of the training game ("Yuan Meng Star") is not demonstrated, and the exact impact of the proprietary dataset on the results is unclear.

## Next Checks

1. **Compression Ablation Study:** Reconstruct a 1080p game frame through the 32x compressed VAE (C=16) and measure the structural similarity index (SSIM) and perceptual quality (LPIPS) compared to the original. This will validate if the channel expansion is sufficient for the compression ratio.

2. **Hierarchical Captioning Ablation:** Train a Yan-Gen baseline without Global captions. Generate a long sequence (1000+ frames) and measure the semantic drift using a CLIP-based similarity score between the first and last frames. This will quantify the importance of the static world reference.

3. **Latency Profiling with Ablations:** Benchmark Yan-Sim with and without FP8 quantization, CUDA graphs, and the Shift Window Denoising. Measure the per-frame inference time to isolate the contribution of each optimization to the final 60FPS claim.