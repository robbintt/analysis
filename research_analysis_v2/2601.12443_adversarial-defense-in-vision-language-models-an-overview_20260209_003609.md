---
ver: rpa2
title: 'Adversarial Defense in Vision-Language Models: An Overview'
arxiv_id: '2601.12443'
source_url: https://arxiv.org/abs/2601.12443
tags:
- adversarial
- defense
- robustness
- image
- clip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This survey systematically reviews defense strategies for vision-language\
  \ models (VLMs) against adversarial attacks, categorizing them into three paradigms:\
  \ training-time defense (adversarial fine-tuning and prompt tuning), test-time adaptation\
  \ defense (parameter tuning during inference), and training-free defense (input/feature\
  \ space purification). The study evaluates six representative methods\u2014TeCoA,\
  \ PMG-AFT, FARE, AOM, TTC, and COLA\u2014on nine diverse datasets against APGD attacks."
---

# Adversarial Defense in Vision-Language Models: An Overview

## Quick Facts
- arXiv ID: 2601.12443
- Source URL: https://arxiv.org/abs/2601.12443
- Authors: Xiaowei Fu; Lei Zhang
- Reference count: 11
- Primary result: Training-free methods demonstrate superior adversarial robustness, with COLA achieving highest performance across multiple datasets against APGD attacks

## Executive Summary
This survey systematically reviews defense strategies for vision-language models (VLMs) against adversarial attacks, categorizing them into three paradigms: training-time defense (adversarial fine-tuning and prompt tuning), test-time adaptation defense (parameter tuning during inference), and training-free defense (input/feature space purification). The study evaluates six representative methods—TeCoA, PMG-AFT, FARE, AOM, TTC, and COLA—on nine diverse datasets against APGD attacks. Training-free methods demonstrate superior performance, with COLA achieving the highest adversarial robustness across multiple datasets. The survey identifies key challenges including cross-modal attack resistance, adaptive defense mechanisms, and improving zero-shot robustness with minimal labeled data. Future directions include integrating generative models with defense strategies and developing more flexible, efficient solutions for real-world deployment.

## Method Summary
The survey evaluates defense strategies for CLIP-based VLMs against adversarial attacks across three paradigms. Training-time defenses modify the training process through adversarial fine-tuning (TeCoA, FARE) or prompt tuning (PMG-AFT). Test-time adaptation defenses update parameters during inference (R-TPT, TAPT) to reduce distribution gaps. Training-free defenses purify inputs or features without parameter updates (AOM, TTC, COLA). The evaluation uses CLIP ViT-B/32 backbone, nine datasets (Flower102, OxfordPet, DTD, FGVCAircraft, StanfordCars, EuroSAT, SUN397, Food101, Caltech101), and APGD attacks with L∞ bound ε=1/255. Methods are assessed on clean accuracy (CLN) and robust accuracy (ROB) metrics.

## Key Results
- Training-free methods outperform training-time and test-time approaches in robust accuracy across all evaluated datasets
- COLA achieves highest robust accuracy (e.g., 77.2% on OxfordPet vs. 38.4% for TeCoA)
- Training-time methods show significant clean accuracy degradation (TeCoA drops to 36.8% clean accuracy on Flower102 vs. 65.5% baseline)
- Training-free methods provide best efficiency-robustness trade-off for real-world deployment

## Why This Works (Mechanism)

### Mechanism 1: Adversarial Fine-tuning (Training-time Defense)
- Claim: Incorporating adversarial examples into training data can enhance model resilience to perturbations, though at cost to clean accuracy.
- Mechanism: Methods like TeCoA and FARE update the image encoder parameters using supervised or unsupervised losses that align adversarial image embeddings with clean text embeddings. FARE specifically forces perturbed feature points to remain close to unperturbed points in the original CLIP space.
- Core assumption: The model can learn invariance to adversarial perturbations without completely losing semantic grounding.
- Evidence anchors:
  - [abstract] "Training-time Defense involves modifying the training process, typically through adversarial fine-tuning to improve the robustness to adversarial examples."
  - [Page 2, Table I] TeCoA and FARE optimize the Image Encoder for zero-shot adversarial robustness.
  - [corpus] Related paper "Navigating the Trade-off" explicitly frames this as robustness-generalization trade-off.
- Break condition: When clean accuracy degrades below acceptable thresholds (e.g., TeCoA drops to 36.8% clean accuracy on Flower102 vs. 65.5% baseline), the trade-off may be untenable.

### Mechanism 2: Test-time Prompt Adaptation
- Claim: Updating prompts during inference can reduce distribution gap between adversarial and clean examples without labeled data.
- Mechanism: R-TPT reformulates marginal entropy minimization by eliminating adversarial-conflicting terms, then uses confidence-driven fusion of perturbed input variants. TAPT optimizes dual-stream (textual and visual) prompts per test sample to minimize multi-view predictive uncertainty.
- Core assumption: Entropy-based objectives remain meaningful signals under adversarial conditions.
- Evidence anchors:
  - [Page 3] "R-TPT employs a confidence-driven fusion scheme that integrates inputs from the most trustworthy perturbed variants."
  - [Page 3] TAPT "simultaneously minimizes multi-view predictive uncertainty and reduces the distribution gap."
  - [corpus] Weak corpus signal—R-TPT neighbor paper exists but provides limited additional mechanistic detail.
- Break condition: Computational overhead at inference may exceed latency budgets; adaptive methods may fail against strongly adaptive attacks.

### Mechanism 3: Feature-space Purification (Training-free Defense)
- Claim: Mapping adversarial embeddings into semantically aligned subspaces can recover correct predictions without any parameter updates.
- Mechanism: COLA uses optimal transport to re-establish global semantic correspondence and local structural coherence between perturbed image embeddings and clean text feature subspaces. TTC adds counterattack perturbations during inference to destabilize adversarial artifacts.
- Core assumption: Adversarial perturbations cause misalignment in embedding space that can be corrected via geometric/transport operations.
- Evidence anchors:
  - [Page 4, Table II] COLA achieves highest robust accuracy across datasets (e.g., 77.2% on OxfordPet vs. 38.4% for TeCoA).
  - [Page 3] COLA "maps perturbed image embeddings into the semantic subspace defined by clean text features."
  - [corpus] EigenShield neighbor paper proposes causal subspace filtering, supporting the subspace-correction hypothesis.
- Break condition: Purification may fail when adversarial perturbations saturate or when text feature subspaces lack discriminative margin.

## Foundational Learning

- Concept: **Contrastive Learning in CLIP**
  - Why needed here: All defense mechanisms operate on CLIP's joint embedding space; understanding how similarity scores are computed (Eq. 1) is prerequisite.
  - Quick check question: Given an image embedding and three text embeddings, can you compute the similarity vector and identify the predicted class?

- Concept: **Adversarial Perturbation Generation (PGD/APGD)**
  - Why needed here: Defenses are evaluated against gradient-based attacks; understanding Eq. 2 clarifies the threat model.
  - Quick check question: What constraint defines the ε-ball, and why does the paper use L∞ norm with 1/255 bound?

- Concept: **Optimal Transport / Wasserstein Distance**
  - Why needed here: COLA's training-free defense relies on optimal transport for embedding alignment.
  - Quick check question: Given two distributions in feature space, what does the transport plan optimize?

## Architecture Onboarding

- Component map:
Input Image → [Purification (optional)] → Image Encoder F_θ → Image Embedding
                                                              ↓
Text Labels → Text Encoder G_φ → Text Embeddings → [Alignment Correction] → Similarity Scoring → Prediction

- Critical path: Image embedding quality determines downstream robustness; purification (TTC) or alignment (COLA) at embedding level has highest impact.

- Design tradeoffs:
  - Training-time methods (TeCoA, FARE): Higher robustness vs. clean accuracy loss; requires compute-intensive fine-tuning.
  - Test-time methods (R-TPT, TAPT): Flexible, label-free vs. inference latency overhead.
  - Training-free methods (COLA, AOM): Best efficiency and robustness (per Table II) vs. potential fragility to adaptive attacks.

- Failure signatures:
  - Collapse to single class: Indicates embedding space distortion from over-aggressive fine-tuning.
  - Near-zero robust accuracy: Suggests purification insufficient or attack budget too large.
  - High variance across datasets: Generalization failure; method overfits to specific attack/dataset.

- First 3 experiments:
  1. **Baseline audit**: Run standard CLIP ViT-B/32 on all 9 datasets with APGD (L∞, ε=1/255) to reproduce Table II baseline numbers.
  2. **Training-free comparison**: Implement COLA and AOM on 3 datasets (OxfordPet, Flower102, Caltech101); compare robust accuracy and inference time.
  3. **Trade-off curve**: Sweep ε from 0.5/255 to 4/255 on TeCoA vs. COLA to map robustness-accuracy frontier and identify break points.

## Open Questions the Paper Calls Out
None

## Limitations
- Exclusive focus on APGD attacks with fixed ε=1/255 bound and PGD-10 iterations limits generalizability to other attack types
- Evaluation assumes white-box access but doesn't test adaptive attacks that could specifically target each defense mechanism's vulnerabilities
- Most critically, the evaluation doesn't address cross-modal attack resistance where both image and text inputs are perturbed simultaneously

## Confidence

- **High confidence**: Training-free defense superiority - Multiple independent evaluations across 9 datasets consistently show COLA and AOM achieving highest robust accuracy while maintaining acceptable clean accuracy.
- **Medium confidence**: Trade-off characterization - The robustness-clean accuracy trade-off for training-time methods is well-supported, but individual method performance varies significantly across datasets.
- **Low confidence**: Future directions - Claims about generative model integration and zero-shot robustness improvements are speculative without empirical validation.

## Next Checks

1. **Adaptive attack evaluation**: Implement expectation-over-transformation (EOT) attacks specifically targeting COLA's optimal transport mechanism and TeCoA's adversarial fine-tuning to assess true robustness.

2. **Cross-modal attack testing**: Design and evaluate attacks that simultaneously perturb both image and text inputs to test cross-modal vulnerability that current defenses don't address.

3. **Real-world deployment simulation**: Test selected defenses under realistic conditions including input distribution shifts, varying computational constraints, and mixed clean/adversarial test sets to validate practical utility.