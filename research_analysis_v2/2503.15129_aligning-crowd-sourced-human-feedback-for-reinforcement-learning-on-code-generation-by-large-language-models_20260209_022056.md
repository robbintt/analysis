---
ver: rpa2
title: Aligning Crowd-sourced Human Feedback for Reinforcement Learning on Code Generation
  by Large Language Models
arxiv_id: '2503.15129'
source_url: https://arxiv.org/abs/2503.15129
tags:
- uni00000048
- uni00000052
- code
- uni00000047
- uni00000010
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents cRLHF, a framework that integrates human feedback
  with reinforcement learning to improve code generation by large language models.
  The approach uses Bayesian inference to align rankings from multiple annotators
  and compute reward scores without additional reward modeling.
---

# Aligning Crowd-sourced Human Feedback for Reinforcement Learning on Code Generation by Large Language Models

## Quick Facts
- arXiv ID: 2503.15129
- Source URL: https://arxiv.org/abs/2503.15129
- Reference count: 40
- Key outcome: Bayesian aggregation of crowdsourced human feedback improves code generation performance on HumanEval and MBPP benchmarks

## Executive Summary
This paper introduces cRLHF, a framework that integrates crowdsourced human feedback into reinforcement learning for code generation by large language models. The approach uses Bayesian inference to aggregate rankings from multiple annotators and compute reward scores without requiring an additional reward model. Experiments on established benchmarks demonstrate performance improvements over recent baselines, with notable gains in larger models.

## Method Summary
The cRLHF framework fine-tunes a pre-trained code generation model using Proximal Policy Optimization (PPO) with rewards derived from crowdsourced human feedback. Annotators provide line-level feedback on generated code, which is aggregated using Bayesian inference to compute reliability scores. These scores are used to calculate a correction rate that serves as the reward signal for PPO, bypassing the need for a separate reward model. The framework is evaluated on 15 LeetCode problems using 10 annotators and tested on HumanEval and MBPP benchmarks.

## Key Results
- cRLHF improves Pass@1, Pass@10, and Pass@100 scores compared to recent baselines
- Notable performance gains observed in larger models
- Framework provides probabilistic interpretation akin to regularized logistic regression
- Method offers fundamental performance bounds to the learning process

## Why This Works (Mechanism)

### Mechanism 1: Bayesian Aggregation of Noisy Signals
The framework models annotator reliability as a probability and aggregates feedback via Bayesian inference, reducing noise in crowdsourced data better than simple majority voting. The probability that a line of code is correct is updated with the log-odds of each annotator's reliability.

### Mechanism 2: Direct Reward Computation
Instead of training a reward model, cRLHF calculates a scalar correction rate directly from Bayesian consensus. This score serves immediately as the reward for PPO, simplifying the pipeline and preventing compounding errors from an imperfect proxy model.

### Mechanism 3: Regularized Optimization Bounds
The Bayesian update is framed as convex optimization with L1 regularization, providing theoretical performance bounds and robustness against overfitting to specific annotators. This promotes sparsity, identifying a reliable subset of annotators.

## Foundational Learning

- **Log-Odds (Logit) Transformation**: Needed to understand how the algorithm sums log-odds rather than averaging probabilities. Quick check: If annotator reliability p=0.9, what is their logit score? (Answer: ~2.2).

- **Proximal Policy Optimization (PPO)**: Needed as the engine that updates the LLM using the engineered reward signal. Quick check: In standard PPO for text, what acts as the "Reward"? Here, what replaces it?

- **Honeypotting (Gold Standard Tasks)**: Needed to calibrate annotator reliability using initial questions with known answers. Quick check: Why is parameter pÌ„ set to 1 for Honeypot questions in Eq. 8?

## Architecture Onboarding

- **Component map**: Input (Problem Description) -> Generator (SFT Model) -> Interface (Web UI) -> Aggregator (Bayesian Engine) -> Optimizer (PPO Trainer)

- **Critical path**: Implementation of Algorithm 1, specifically lines 11-23. If the Bayesian update (Eq. 8) is not triggered correctly by the Honeypot/annotation loop, reliability scores will remain at initialization, rendering consensus meaningless.

- **Design tradeoffs**: Granularity vs. Cost (line-level vs file-level labeling); Current reliance on annotators with programming proficiency limits scalability.

- **Failure signatures**: Score Stagnation (reward variance too low); Annotator Collapse (wildly fluctuating reliability scores).

- **First 3 experiments**:
  1. Unit Test the Aggregator: Implement Eq. 7 with mock votes to verify high-reliability annotator votes outweigh multiple low-reliability votes.
  2. Honeypot Calibration: Run 10 mock users through Honeypot phase to confirm reliability scores update correctly.
  3. Overfit Check: Fine-tune small model on single problem type to test if model memorizes exact fixes rather than general logic.

## Open Questions the Paper Calls Out
- Can the framework effectively integrate AI-based evaluators (LLM-as-a-Judge) to replace human annotators for scalable feedback collection?
- How does the framework adapt to domain-specific languages (DSLs) like CVX where code correctness is defined by optimization constraints?
- Does the line-level correction rate provide sufficient reward granularity to correct complex semantic logic errors compared to execution-based rewards?

## Limitations
- Limited scope of human feedback with 10 annotators of uncharacterized expertise
- Line-level granularity may be overly sensitive to superficial errors that don't affect functional correctness
- Critical implementation details missing (exact PPO hyperparameters, specific problems, annotator interface logic)

## Confidence
- **High Confidence**: Bayesian aggregation mechanism is clearly specified and implementable
- **Medium Confidence**: Performance improvement claims supported but limited scale makes generalizability difficult
- **Low Confidence**: Claims about "fundamental performance bounds" weakly supported without concrete derivations

## Next Checks
1. Ablation Study on Aggregation Method: Compare Bayesian inference against majority voting to isolate contribution of Bayesian weighting.
2. Annotator Diversity Analysis: Characterize annotators by experience and track reliability score evolution to test if system correctly identifies unreliable annotators.
3. Reward Signal Alignment Test: Calculate correlation between computed reward score and actual functional correctness to verify feedback signal alignment.