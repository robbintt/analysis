---
ver: rpa2
title: 'MUA-RL: Multi-turn User-interacting Agent Reinforcement Learning for agentic
  tool use'
arxiv_id: '2508.18669'
source_url: https://arxiv.org/abs/2508.18669
tags:
- user
- tool
- item
- price
- order
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MUA-RL, a reinforcement learning framework
  for agentic tool use that integrates LLM-simulated users into the RL loop for the
  first time. It addresses the challenge of dynamic, uncertain user interactions by
  enabling models to iteratively refine user understanding while invoking tools.
---

# MUA-RL: Multi-turn User-interacting Agent Reinforcement Learning for agentic tool use

## Quick Facts
- **arXiv ID:** 2508.18669
- **Source URL:** https://arxiv.org/abs/2508.18669
- **Reference count:** 40
- **Key outcome:** MUA-RL-32B achieves 67.3, 45.4, 28.3, 28.4, and 82.5 on TAU2 Retail, TAU2 Airline, TAU2 Telecom, BFCL-V3 Multi Turn, and ACEBench Agent, outperforming or matching much larger open-source models in non-thinking settings.

## Executive Summary
MUA-RL introduces a novel reinforcement learning framework that integrates LLM-simulated users into the RL loop for the first time, enabling autonomous learning of user-centered interaction strategies. By employing a simplified binary reward design focused solely on task completion, MUA-RL encourages robust exploration and generalization while avoiding reward-hacking behaviors. Evaluations demonstrate strong performance across diverse agentic tool use benchmarks, particularly in non-thinking settings where it matches or exceeds much larger open-source models.

## Method Summary
MUA-RL employs a two-stage pipeline: (1) Cold-start SFT using ~2000 synthesized trajectories across 9 scenarios to establish basic tool-invocation competence, and (2) RL with GRPO algorithm integrating LLM-simulated users (GPT-4o-2024-11-20) into rollouts. The framework uses binary task-completion rewards (r=1 for success, r=0 otherwise) and loss masking on tool outputs and user messages. Training uses Qwen3-8B/14B/32B Non-thinking backbones with specific hyperparameters including batch size 32, rollout 8, max 30 turns, and temperature 1.0.

## Key Results
- MUA-RL-32B achieves 67.3 on TAU2 Retail, 45.4 on TAU2 Airline, 28.3 on TAU2 Telecom, 28.4 on BFCL-V3 Multi Turn, and 82.5 on ACEBench Agent.
- Outperforms or matches much larger open-source models like DeepSeek-V3-0324 and Qwen3-235B-A22B in non-thinking settings.
- Cold-start SFT contributes significantly (BFCL performance drops without it).
- Demonstrates strong generalization across diverse agentic tool use benchmarks.

## Why This Works (Mechanism)

### Mechanism 1
Integrating LLM-simulated users into the RL rollout loop enables agents to learn adaptive, user-centered interaction strategies rather than just static tool-calling patterns. During training rollouts, the policy model interacts with a separate LLM playing the user role, which dynamically adjusts queries and expectations based on the agent's responses. This creates a feedback loop where the agent must iteratively refine its understanding of user intent through dialogue, not just execute tool sequences. Assumes the LLM user simulator provides sufficiently realistic and diverse user behaviors that transfer to real users at test time.

### Mechanism 2
Binary task-completion rewards encourage bolder exploration and more diverse solution paths than shaped rewards with intermediate milestones. By avoiding format rewards, tool-name matching rewards, or intermediate-step rewards, the agent is not constrained to specific behavioral templates. This allows the model to discover efficient strategies rather than optimizing for reward-hacking behaviors. Assumes the task-completion signal is sufficiently frequent and unambiguous that the agent can learn meaningful policies without dense intermediate feedback.

### Mechanism 3
The cold-start SFT phase provides basic tool-invocation competence, and subsequent RL with user simulators reshapes behavior toward policy-compliant, user-centered interaction patterns. Cold-start SFT establishes tool syntax, domain knowledge, and basic dialogue fluency using synthesized trajectories. RL then optimizes for task success in dynamic user interactions, counteracting SFT biases and encouraging more robust behaviors like explicit confirmation before actions. Assumes cold-start data quality is sufficient to provide a viable starting policy, and that RL can overcome SFT distributional biases.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO):** The RL algorithm used in MUA-RL that estimates advantages by comparing group-wise rewards, avoiding value-function approximation and reducing training complexity. *Quick check:* Can you explain why GRPO uses group-relative advantages instead of a learned value function?

- **Rollout in RL for LLMs:** MUA-RL extends rollouts beyond text generation to include real-time tool execution and LLM-simulated user responses. Understanding rollout structure is essential to grasping how the agent interacts with its environment during training. *Quick check:* How does a multi-turn user-interacting rollout differ from a standard text-based rollout?

- **Model Context Protocol (MCP):** MCP servers are used in one of the cold-start data synthesis pipelines to provide real tool execution. Understanding MCP helps clarify how realistic tool responses are generated. *Quick check:* What advantage does MCP-based tool execution offer over LLM-simulated tool responses?

## Architecture Onboarding

- **Component map:** Agent LLM (policy model) -> User simulator LLM (GPT-4o) -> Tool execution environment (MCP servers or LLM-simulated tools) -> GRPO trainer -> Reward function (binary task completion) -> Cold-start dataset (~2000 trajectories)

- **Critical path:**
  1. Prepare cold-start data via synthesis pipeline (LLM-simulated tools or real MCP servers)
  2. Perform SFT cold-start training on Qwen3 backbone with AdamW optimizer, batch size 128, 2 epochs, LR 5e-6
  3. Set up RL environment with user simulator (GPT-4o), tool execution, and task pool (TAU1-Bench)
  4. Configure GRPO with β=0.001, batch size 32, rollout 8, max 30 turns, temperature 1.0
  5. Run RL training for 25 epochs, monitoring KL loss, entropy, and task success rates
  6. Evaluate on TAU2-Bench, BFCL-V3 Multi Turn, ACEBench Agent

- **Design tradeoffs:**
  - User simulator choice: GPT-4o provides realistic dynamics but adds cost; smaller models may reduce fidelity
  - Reward simplicity vs. density: Binary rewards avoid hacking but may slow learning on long-horizon tasks
  - Cold-start scale: ~2000 trajectories is lightweight; larger datasets may improve SFT but increase bias risk
  - Rollout length: 30-turn upper bound balances realism and compute; may truncate complex tasks

- **Failure signatures:**
  - KL divergence explosion: Indicates policy drifting too far; increase β or reduce learning rate
  - Entropy collapse: Model becomes deterministic too early; reduce clipping or increase temperature
  - Tool call loops: Agent repeats same tool; check loss masking and reward signal clarity
  - User simulator breakdown: Simulator generates incoherent responses; validate prompt design and model capability

- **First 3 experiments:**
  1. Ablate cold-start phase: Train MUA-RL directly from base model; expect degraded BFCL performance
  2. Vary user simulator: Replace GPT-4o with a smaller model (e.g., Qwen-72B); measure TAU2 Telecom performance change
  3. Add intermediate rewards: Introduce format or tool-matching rewards; compare task success rates and behavioral diversity

## Open Questions the Paper Calls Out
- To what extent can fine-grained intermediate rewards be reintroduced into the MUA-RL framework without impeding the learning of tool invocation patterns? The conclusion suggests exploring "more fine-grained reward shaping strategies," while noting that dialogue content requirements "impede the model's ability to learn correct tool invocation patterns."
- How does the MUA-RL framework perform when extended to richer multi-modal environments? The conclusion states that "Future research may extend this framework to richer multi-modal environments," though the current study is restricted to text-based interaction.
- Does relying on a specific LLM (GPT-4o) as the user simulator introduce distributional biases that limit the agent's generalization to real human users? The paper uses "GPT-4o-2024-11-20 as the LLM user simulator" but does not test against diverse user personalities or ablate this choice.

## Limitations
- Cold-start dataset may introduce domain-specific biases that RL cannot fully overcome, as evidenced by performance drop on TAU2 Telecom.
- Simplified binary reward design may not generalize well to more complex or long-horizon tasks where intermediate guidance is critical.
- Exact prompting strategy for the GPT-4o user simulator during RL rollouts is unspecified, making it difficult to assess simulator representativeness.

## Confidence
- **High confidence:** The ablation study showing cold-start SFT contribution (BFCL performance drop without it) is well-supported by experimental evidence.
- **Medium confidence:** The claim that binary rewards encourage bolder exploration is plausible but lacks direct comparative evidence against shaped rewards in the same framework.
- **Low confidence:** The assertion that MUA-RL-32B "outperforms or matches much larger models" is based on non-thinking settings only, and the methodology for ensuring fair comparison is not fully specified.

## Next Checks
1. Validate user simulator fidelity: Replace GPT-4o with a smaller, controlled simulator (e.g., Qwen-72B) and measure performance degradation on TAU2 Telecom to assess sensitivity to simulator quality.
2. Test reward structure impact: Introduce intermediate rewards (e.g., format or tool-matching) into MUA-RL and compare task success rates and behavioral diversity against the binary-only baseline.
3. Evaluate cross-domain robustness: Train MUA-RL on a domain-balanced cold-start dataset and test on out-of-distribution domains to quantify bias mitigation versus the current approach.