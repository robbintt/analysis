---
ver: rpa2
title: Simulating Environments with Reasoning Models for Agent Training
arxiv_id: '2511.01824'
source_url: https://arxiv.org/abs/2511.01824
tags:
- environment
- tool
- simulated
- qwen2
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that large language models (LLMs) can simulate
  realistic environment feedback without access to real testbed data or APIs, enabling
  scalable agent training. The authors propose Simia-SFT, a pipeline that synthesizes
  supervised fine-tuning (SFT) data by amplifying small seed sets into diverse trajectories
  in an environment-agnostic manner, and Simia-RL, a framework that enables reinforcement
  learning (RL) without real environment implementations through LLM-simulated feedback.
---

# Simulating Environments with Reasoning Models for Agent Training

## Quick Facts
- arXiv ID: 2511.01824
- Source URL: https://arxiv.org/abs/2511.01824
- Reference count: 40
- Primary result: LLMs can simulate realistic environment feedback without real testbed data or APIs, enabling scalable agent training that surpasses GPT-4o and approaches o4-mini on τ 2-Bench.

## Executive Summary
This paper demonstrates that large language models can effectively simulate realistic environment feedback without access to real testbed data or APIs, enabling scalable agent training. The authors propose Simia-SFT, a pipeline that synthesizes supervised fine-tuning (SFT) data by amplifying small seed sets into diverse trajectories in an environment-agnostic manner, and Simia-RL, a framework that enables reinforcement learning without real environment implementations through LLM-simulated feedback. Fine-tuning open models like Qwen2.5-32B and Qwen3-8B on simulated trajectories yields consistent improvements across multiple benchmarks, surpassing GPT-4o and approaching o4-mini on τ 2-Bench.

## Method Summary
The method involves two core frameworks: Simia-SFT for supervised fine-tuning and Simia-RL for reinforcement learning. Simia-SFT takes small seed trajectory sets, filters them for quality, and uses LLM simulation (GPT-5 or o4-mini) to generate diverse synthetic trajectories by varying task formulations and reasoning strategies while maintaining tool specification constraints. The generated trajectories undergo rule-based post-processing for JSON repair and tool validation. Simia-RL uses the same LLM to simulate environment feedback during policy optimization, generating both observations and rewards. The approach eliminates the need for heavy, brittle environment implementations by replacing them with flexible LLM-based simulation.

## Key Results
- Fine-tuning open models (Qwen2.5-32B, Qwen3-8B) on 90k synthetic trajectories outperforms training on 5k real trajectories across τ²-Bench, AgentBench, and OfficeBench
- Simia-RL with simulated environments (64.7, 34.5) outperforms real environment RL (60.8, 28.6) on OfficeBench 2-apps and 3-apps
- Performance surpasses GPT-4o and approaches o4-mini on τ 2-Bench, with GPT-5 better for τ²-Bench retail and o4-mini better for OfficeBench multi-app tasks

## Why This Works (Mechanism)

### Mechanism 1
LLMs can act as environment simulators, generating coherent state transitions and tool interactions without access to real environments. The models leverage world modeling capabilities encoded during pre-training to reason about plausible environment responses when provided with tool specifications, interaction history, and reference trajectories. The model infers logical consequences of actions rather than executing actual API calls. Core assumption: Pre-training has encoded sufficient knowledge about common domains to generate realistic feedback. Break condition: Simulation quality degrades for domains poorly represented in pre-training data or requiring precise state tracking.

### Mechanism 2
Small seed trajectory sets can be amplified into diverse training data through LLM-based simulation with controlled variation. The Simia-SFT pipeline uses seed trajectories as exemplars, then prompts an LLM to generate new trajectories with different task formulations, reasoning strategies, and action sequences. Temperature adjustment and multiple generation passes promote diversity while tool specifications constrain the action space to valid operations. Core assumption: Seed trajectories capture essential task structure that can be varied while maintaining correctness. Break condition: Seed quality issues propagate amplified; poor seeds cannot be recovered downstream.

### Mechanism 3
RL on simulated environments can outperform RL on real environments when simulated feedback provides richer, more instructive signals. Simia-RL uses an LLM to generate both environment observations and rewards. Unlike real environments that return fixed error messages, simulated environments can explain failures (e.g., "conflicts with existing event: Lunch Break"), enabling faster policy improvement through more informative gradients. Core assumption: Richer feedback correlates with faster learning; simulated errors are causally informative rather than misleading. Break condition: If simulated feedback introduces systematic biases, policies may overfit to simulator quirks.

## Foundational Learning

- **Supervised Fine-Tuning (SFT) on agent trajectories**: Why needed: Core training method; requires understanding how trajectory data (user query → reasoning → tool call → observation) is formatted and masked during training. Quick check: Can you explain why only assistant tokens are predicted while prompts are masked?

- **Multi-turn RL with environment feedback**: Why needed: Simia-RL extends SFT with policy optimization; requires understanding rollout collection, reward assignment, and GRPO algorithm. Quick check: How does reward credit assignment work across 25 interaction turns?

- **Tool specification and API schemas**: Why needed: Simulation is anchored to formal tool definitions; understanding JSON schema constraints is essential for prompt design and validation. Quick check: Given a tool spec with required vs. optional parameters, how would you validate a generated tool call?

## Architecture Onboarding

- **Component map**: Seed trajectories -> Pre-filtering (completeness/logic/format checks) -> Prompt design (tool specs + policy + format + exemplar) -> LLM simulation (GPT-5/o4-mini, temp=1.0) -> Rule-based post-processing (JSON repair, tool validation)

- **Critical path**: Seed quality -> Prompt design fidelity -> Simulation diversity -> Post-processing rigor -> Model capacity. Errors compound; poor seeds cannot be recovered downstream.

- **Design tradeoffs**: Simulator choice: GPT-5 better for τ²-Bench retail; o4-mini better for OfficeBench multi-app. Scale vs. quality: 90k synthetic trajectories outperform 5k real trajectories, but 5k synthetic ≈ 5k real—suggesting scale advantage dominates at high volumes. Real vs. simulated RL: Simulated provides richer feedback but risks distribution shift; real is sparse but faithful.

- **Failure signatures**: Tool hallucination: Generated trajectories call tools not in specification → mitigated by rule-based filtering. Format drift: Hermes XML vs. JSON inconsistency → post-processing normalizes to single format. Qwen3-8B thinking loops: Excessive CoT causes near-zero performance on some benchmarks.

- **First 3 experiments**: 1) Validate simulation quality: Take 10 seed trajectories, generate 100 synthetic, manually compare tool call validity and reasoning coherence against real environment execution. 2) Ablate feedback richness: Train RL agent with (a) real env sparse errors, (b) simulated env detailed errors, (c) simulated env sparse errors—measure sample efficiency delta. 3) Test domain transfer: Train on τ²-Bench synthetic data, evaluate on held-out OfficeBench to assess whether world-model-based simulation generalizes across domains.

## Open Questions the Paper Calls Out

- **Can LLM-simulated environments maintain logical consistency and state fidelity in domains requiring strict determinism (e.g., coding, robotics) as effectively as in the semantic tasks tested?**: Current benchmarks rely heavily on semantic coherence; it is unknown if the simulator can track precise, deterministic state changes without hallucination. Performance results on benchmarks requiring strict deterministic accuracy would resolve this.

- **Does training on synthetic, simulated trajectories introduce distributional biases that negatively impact generalization to real-world user behaviors not captured by the simulator?**: While the paper shows benchmark improvements, it does not analyze whether the model learns to exploit artifacts of the LLM simulator rather than robust environment dynamics. A "Sim-to-Real" transfer analysis comparing model performance on held-out real-user interactions versus simulated test cases would resolve this.

- **Is the superior performance of Simia-RL (simulated feedback) over real-environment training in OfficeBench primarily attributable to the higher information density of the synthesized error messages?**: The mechanism could alternatively be attributed to noise regularization or the specific prompting strategy rather than the feedback content itself. Ablation studies where simulated feedback is artificially constrained to match the brevity of real error messages would test if the performance gap persists.

## Limitations

- The simulation approach relies heavily on pre-training data coverage and may not generalize to domains with specialized terminology or state-dependent logic.
- The methodology assumes that simulated tool interactions remain faithful to real API behavior, but no systematic evaluation of simulator fidelity against ground-truth environments is provided.
- While simulation outperforms real environments in controlled settings, the paper does not address distribution shift risks when deploying policies trained on simulated feedback to actual production environments.

## Confidence

- **High confidence**: SFT fine-tuning on simulated trajectories consistently improves performance across benchmarks; the data generation pipeline produces valid tool interactions; Simia-SFT framework is reproducible given the detailed prompts and post-processing rules.
- **Medium confidence**: Simia-RL framework outperforms real environment RL in the reported cases; simulator provides richer feedback that accelerates learning; GPT-5 vs. o4-mini performance differences are stable across domains.
- **Low confidence**: Generalization of simulated policies to real environments without adaptation; long-term stability of RL policies trained purely on simulated feedback; scalability beyond the tested benchmarks.

## Next Checks

1. **Simulator Fidelity Assessment**: Implement a small-scale testbed for one benchmark (e.g., AirlineBooking) and compare 100 simulated trajectories against actual environment execution, measuring tool call accuracy, state consistency, and reasoning coherence.

2. **Distribution Shift Evaluation**: Train a policy on τ²-Bench simulated data and evaluate zero-shot on a held-out environment implementation. Measure performance degradation and identify specific failure modes (tool call errors, reasoning gaps, state tracking issues).

3. **Feedback Richness Ablation**: Train three RL agents on the same initial policy: (a) simulated environment with detailed error explanations, (b) simulated environment with sparse binary rewards, (c) real environment with actual API feedback. Compare sample efficiency and final performance to quantify the value of simulated feedback richness.