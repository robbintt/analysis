---
ver: rpa2
title: Continuous Benchmark Generation for Evaluating Enterprise-scale LLM Agents
arxiv_id: '2511.10049'
source_url: https://arxiv.org/abs/2511.10049
tags:
- agents
- services
- benchmarks
- evaluation
- changes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating enterprise-scale
  LLM agents in environments where requirements and platforms evolve continuously,
  and ground truth examples are sparse. Traditional static benchmarks fail to capture
  the dynamic nature of such tasks.
---

# Continuous Benchmark Generation for Evaluating Enterprise-scale LLM Agents

## Quick Facts
- arXiv ID: 2511.10049
- Source URL: https://arxiv.org/abs/2511.10049
- Reference count: 14
- Key outcome: Continuous benchmark generation pipeline achieves high precision (1.0) and variable recall (0.25-0.67) for enterprise LLM agent evaluation

## Executive Summary
This paper addresses the challenge of evaluating enterprise-scale LLM agents in dynamic environments where requirements and platforms evolve continuously. Traditional static benchmarks fail to capture the changing nature of such tasks, particularly when ground truth examples are sparse. The authors propose a continuous benchmark generation pipeline that leverages semi-structured Knowledge Base (KB) documents authored by developers, which describe specific sub-tasks, and maps them to relevant changes in migrated service repositories using keywords and regular expressions generated via LLMs. By linking these KB documents to reference commits from migrated services, the pipeline automatically constructs evolving ground-truth benchmarks.

The approach was evaluated on 137 KB documents across 4 enterprise services, achieving high precision and recall compared to manually constructed benchmarks. The method also proved more reliable by excluding obsolete files automatically. This framework enables longitudinal, adaptive evaluation of agents and facilitates targeted improvements as both agents and platforms evolve. The paper demonstrates that separating task specification from benchmark instantiation via KB documents enables maintainable, evolving benchmarks that better reflect real-world requirements.

## Method Summary
The paper proposes a continuous benchmark generation pipeline for evaluating enterprise-scale LLM agents performing service migration tasks. The method relies on developer-authored semi-structured Knowledge Base (KB) documents that describe specific sub-tasks and requirements. An LLM generates regular expressions and keywords from these KB documents, which are then mapped to relevant code diff hunks from migration commits in service repositories. The pipeline extracts ground-truth benchmarks directly from actual migration commits, automatically excluding obsolete files. Benchmarks are constructed as tuples of pre-migration repository states and relevant diff hunks per KB. Evaluation metrics include line-edit precision/recall (matching predicted edits to ground truth within fixed edit distance) and per-KB precision/recall (whether agent covers relevant KBs).

## Key Results
- The pipeline achieved perfect precision (1.0) across all four evaluated repositories
- Recall varied from 0.25 to 0.67 depending on repository complexity and change patterns
- Automated benchmarks excluded obsolete files automatically, making them more reliable than manually constructed benchmarks
- The approach successfully generated evolving benchmarks that reflected actual service migration changes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separating task specification ("what to test") from benchmark instantiation ("how to test") via semi-structured Knowledge Base documents enables maintainable, evolving benchmarks.
- Mechanism: Developers author KB documents describing sub-tasks (e.g., "update logging library for OS migration"). These documents serve as durable anchors. When requirements change, developers edit or add KB documents rather than reconstructing benchmarks from scratch. The pipeline then maps these KBs to concrete code diffs from migrated repositories.
- Core assumption: Developers can and will maintain KB documents with sufficient specificity (keywords, intent descriptions) for automated mapping; Assumption: KB structure remains stable enough across requirement changes to preserve mapping validity.
- Evidence anchors:
  - [abstract] "Our approach relies on semi-structured documents where developers express the high-level intent, and uses state-of-the-art LLMs to generate benchmarks from just a small number of such documents."
  - [Section 3.1] "Building these Knowledge Bases requires developers to break the high-level objective into smaller tasks, each describing a specific and related set of changes... Capturing requirements in the form of KBs imposes little developer burden – these requirements are described even today in the form of migration manuals or troubleshooting guides."
  - [corpus] Related work NetPress similarly uses automated benchmark generation for domain-specific tasks, suggesting this separation pattern has emerging validation, though the specific KB-to-commit mapping approach in this paper remains independently validated only on the reported 4 services.
- Break condition: If KB documents become stale or insufficiently specific (e.g., vague intent without actionable keywords), the mapping to code diffs will degrade in recall. If the task domain changes fundamentally (new artifact types not covered by existing KB structure), new KB schemas may be required.

### Mechanism 2
- Claim: LLM-generated regular expressions and keyword extraction can reliably map natural language task descriptions to relevant code diff hunks with high precision.
- Mechanism: KB documents either contain explicit keywords or describe patterns in natural language (e.g., "Windows drive names"). For the latter, LLMs generate regex patterns. Diff hunks from migration commits are matched against these patterns, filtering out irrelevant changes (feature additions, bug fixes unrelated to migration).
- Core assumption: LLMs can accurately translate natural language pattern descriptions into regex; Assumption: migration-related changes are sufficiently distinguishable from unrelated changes via surface-level pattern matching.
- Evidence anchors:
  - [Section 3.2] "KB documents may not have precise keywords, but rather descriptions of what 'lines of code' to look for... we can leverage the reasoning and text-generation capabilities of LLMs to generate regular expressions for these types of descriptions."
  - [Section 3.4, Table 2] Precision of 1.0 across all 4 evaluated repositories, indicating the mapping mechanism produced no false positives in the study's scope.
  - [corpus] Corpus evidence on LLM-to-regex mechanisms is limited; related benchmarks like GitGoodBench focus on task completion rather than pattern extraction. This mechanism's generalization beyond the studied migration domain remains unvalidated.
- Break condition: If code patterns are semantically complex (e.g., behavioral changes not reflected in syntax), regex-based matching will miss them. If LLMs generate overly broad or incorrect regex patterns, precision will drop.

### Mechanism 3
- Claim: Ground-truth benchmarks derived from actual migration commits automatically exclude obsolete files and produce cleaner evaluation datasets than manual curation.
- Mechanism: The pipeline extracts diff hunks from commits that occurred during actual service migrations. Because commits reference files that existed at migration time, obsolete files (since deleted or renamed) are naturally excluded from ground truth. Manual benchmarks may inadvertently include references to such files.
- Core assumption: Commits tagged as "migration-related" are correctly identified and contain primarily migration-relevant changes; Assumption: the commit history is sufficiently clean to serve as ground truth.
- Evidence anchors:
  - [Section 3.4] "Interestingly, the automated benchmarks also proved to be more reliable... A deeper inspection revealed that the manually written benchmarks included obsolete files that were still present in the repositories. Since our pipeline derives ground truth directly from reference commits, such files are automatically excluded."
  - [Section 2] Notes that manually migrated services required 20-25 weeks per service and 350+ PRs, indicating commit histories are rich but potentially noisy.
  - [corpus] No direct corpus validation for commit-derived ground truth cleanliness; this remains an internal finding from the reported case study.
- Break condition: If migration commits are mislabeled or contain mixed-purpose changes (migration + feature work), ground truth noise increases. If repository history is rewritten or squashed, commit-level granularity may be lost.

## Foundational Learning

- Concept: **Diff hunk analysis**
  - Why needed here: The pipeline operates on granular code changes (diff hunks) rather than full files, enabling fine-grained mapping between KB tasks and specific code modifications. Understanding how to parse, filter, and match hunks is essential for implementing the benchmark generator.
  - Quick check question: Given a Git diff output, can you identify which hunks relate to dependency version changes vs. logic changes?

- Concept: **Regular expression generation from natural language**
  - Why needed here: LLMs bridge the gap between human-authored KB intent and machine-executable pattern matching. Engineers must understand both the capabilities and failure modes of this translation step.
  - Quick check question: If an LLM generates the regex `C:\\` to match Windows drive paths, what edge cases might it miss (e.g., network paths, case sensitivity)?

- Concept: **Precision vs. Recall in benchmark quality**
  - Why needed here: The paper reports perfect precision but variable recall (0.25–0.67). Understanding this tradeoff is critical for interpreting whether the pipeline is conservative (safe but incomplete) or aggressive (comprehensive but noisy).
  - Quick check question: For an enterprise evaluation pipeline, would you prioritize high precision (avoid false ground truth) or high recall (cover all possible task instances)? Why?

## Architecture Onboarding

- Component map:
  KB Documents -> Pattern Generator (LLM) -> Commit Analyzer -> Hunk-to-KB Mapper -> Benchmark Assembler -> Evaluation Runner

- Critical path:
  1. Author/validate KB documents for the target task domain.
  2. Run Pattern Generator to extract keywords and produce regex patterns.
  3. Identify migrated service repositories and extract candidate commits.
  4. Apply Hunk-to-KB Mapper to filter and associate hunks with KBs.
  5. Assemble benchmark instances and run agent evaluation.

- Design tradeoffs:
  - **KB granularity**: Fine-grained KBs (one per atomic sub-task) enable precise per-task metrics but increase authoring burden. Coarse-grained KBs reduce burden but obscure performance diagnosis.
  - **Pattern specificity**: Strict regex patterns yield high precision but may miss valid variations; loose patterns increase recall but risk false matches.
  - **Commit selection**: Using all migration commits maximizes coverage but includes noise; manually curating a subset improves quality but reduces automation.

- Failure signatures:
  - **Low recall with high precision**: KB patterns are too strict; consider expanding regex or adding keyword synonyms.
  - **Obsolete file references in manual benchmarks**: Switch to commit-derived ground truth; this paper reports automated benchmarks avoid this issue.
  - **Mixed-purpose commits contaminating ground truth**: Implement commit filtering (e.g., by author, PR labels) or manual spot-checks during pipeline setup.

- First 3 experiments:
  1. **Validate KB-to-hunk mapping on a single service**: Manually inspect whether hunks matched to a KB are semantically relevant. Measure precision against human judgment.
  2. **Ablate LLM-generated regex**: Compare benchmark quality when using only explicit keywords vs. LLM-generated regex patterns. Quantify recall difference.
  3. **Longitudinal benchmark stability**: Generate benchmarks at two time points (e.g., before and after a platform requirement change). Verify that updated KBs produce appropriately changed benchmark instances.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the correctness of automatically generated benchmarks be formally assessed without relying on manual verification?
- Basis in paper: [explicit] The authors state, "A key question is how to assess the accuracy of generated benchmarks, requiring new mechanisms and metrics for correctness evaluation."
- Why unresolved: The current pipeline relies on comparing generated benchmarks against manually constructed ones to establish validity, which does not scale or remove the need for human effort entirely.
- What evidence would resolve it: The development of automated metrics or "oracle" mechanisms that can verify the semantic correctness of a generated ground-truth diff without human intervention.

### Open Question 2
- Question: Can the continuous benchmark generation methodology generalize to other enterprise domains, such as troubleshooting or incident triage?
- Basis in paper: [explicit] The authors note, "We believe this evaluation methodology may extend beyond service-migration agents... though this remains a direction for future exploration."
- Why unresolved: The paper instantiates the pipeline only for service migration; it is unproven whether the KB-to-commit mapping strategy works for tasks where changes are less structural or more logic-heavy.
- What evidence would resolve it: Successful application of the pipeline to a different agent domain (e.g., incident management) yielding similar precision and recall rates.

### Open Question 3
- Question: How can benchmarking feedback be utilized to automatically generate targeted prompts or improvements for failing agents?
- Basis in paper: [explicit] The authors ask, "How to use benchmarking feedback to better prompt agents – for instance, one can provide targeted feedback if the LLM agent performs poorly on specific tasks."
- Why unresolved: The current framework detects *where* an agent fails (per-KB metrics) but does not define a mechanism to translate that failure into corrective instructions or fine-tuning data.
- What evidence would resolve it: A closed-loop system where evaluation results automatically update agent prompts or KBs, resulting in measurable performance gains in subsequent iterations.

### Open Question 4
- Question: How can the pipeline be optimized to improve recall scores for repositories with heterogeneous or less structured changes?
- Basis in paper: [inferred] Table 2 shows high precision (1.0) but highly variable recall (ranging from 0.25 to 0.667), indicating that the regex and keyword mapping often misses valid code hunks.
- Why unresolved: The authors attribute the low recall in Repo4 (0.25) to the specific nature of changes, suggesting the current LLM-based generation of regular expressions may be too strict or conservative.
- What evidence would resolve it: Adjustments to the mapping logic (e.g., semantic code search instead of regex) that increase recall in low-performing repositories while maintaining high precision.

## Limitations

- **Generalizability**: The approach is validated on 4 enterprise migration services with 137 KB documents, but effectiveness on other enterprise domains remains unproven.
- **KB maintenance burden**: The paper claims "little burden" on developers for KB maintenance, but empirical evidence across diverse teams and domains is absent.
- **LLM dependency**: The method relies on LLMs for regex generation from natural language, but the paper doesn't specify which LLM or provide prompts, making reproduction difficult.

## Confidence

- **High Confidence**: The pipeline's core mechanism (KB-to-commit mapping via regex) works as described for the evaluated migration domain. The exclusion of obsolete files via commit-derived ground truth is empirically validated.
- **Medium Confidence**: The approach generalizes beyond the specific Windows-to-Linux migration case studied. The claimed "little burden" on developers for KB maintenance is plausible but unverified across teams.
- **Low Confidence**: The method's performance on semantically complex changes (e.g., behavioral modifications not reflected in syntax) and its scalability to hundreds of services or different migration types.

## Next Checks

1. **Cross-domain validation**: Apply the pipeline to a different enterprise migration domain (e.g., database schema migration) and measure whether precision and recall remain comparable to the reported 1.0 and 0.25-0.67 ranges.

2. **KB maintenance study**: Track the time developers spend authoring and updating KB documents over multiple requirement change cycles, comparing this to the time required for manual benchmark updates.

3. **Mixed-commit contamination test**: Introduce synthetic mixed-purpose commits (migration + unrelated changes) into the pipeline and measure the impact on benchmark quality metrics.