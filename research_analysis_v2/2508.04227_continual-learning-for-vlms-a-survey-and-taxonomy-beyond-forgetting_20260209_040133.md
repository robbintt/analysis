---
ver: rpa2
title: 'Continual Learning for VLMs: A Survey and Taxonomy Beyond Forgetting'
arxiv_id: '2508.04227'
source_url: https://arxiv.org/abs/2508.04227
tags:
- learning
- continual
- arxiv
- methods
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This survey provides the first comprehensive review of continual\
  \ learning for vision-language models (VLMs), addressing the unique challenges of\
  \ catastrophic forgetting in multimodal settings. It identifies three core failure\
  \ modes\u2014cross-modal feature drift, shared module interference, and zero-shot\
  \ capability erosion\u2014and proposes a taxonomy of solutions: multi-modal replay\
  \ strategies, cross-modal regularization, and parameter-efficient adaptation."
---

# Continual Learning for VLMs: A Survey and Taxonomy Beyond Forgetting

## Quick Facts
- **arXiv ID**: 2508.04227
- **Source URL**: https://arxiv.org/abs/2508.04227
- **Reference count**: 40
- **Primary result**: First comprehensive survey of continual learning for vision-language models, identifying three failure modes and proposing taxonomy of solutions.

## Executive Summary
This survey provides the first comprehensive review of continual learning for vision-language models (VLMs), addressing the unique challenges of catastrophic forgetting in multimodal settings. The authors identify three core failure modes - cross-modal feature drift, shared module interference, and zero-shot capability erosion - and propose a taxonomy of solutions including multi-modal replay strategies, cross-modal regularization, and parameter-efficient adaptation. The paper systematically reviews benchmarks, evaluation metrics, and performance analysis across tasks like image classification, retrieval, and VQA. It highlights the dominance of continual fine-tuning (CFT) and the need for more holistic evaluation protocols. Future directions include advancing continual pre-training, generative task learning, and theoretical understanding of modality gaps.

## Method Summary
The survey conducts a comprehensive literature review of VLM-CL methods, categorizing them into three solution taxonomies: Multi-Modal Replay (storing/generating samples), Cross-Modal Regularization (knowledge distillation to preserve alignment), and Parameter-Efficient Adaptation (PEA). The authors analyze existing benchmarks including MTIL for classification and VQACL for VQA, while evaluating methods using metrics like Average Accuracy, Forgetting, and Zero-Shot Degradation. The reproduction notes indicate that implementing a minimum viable reproduction requires setting up a pre-trained VLM backbone (e.g., CLIP ViT-B/16), configuring class/domain incremental scenarios using MTIL or ImageNet-100 datasets, and implementing taxonomy methods like ZSCL or MoE-Adapters while monitoring zero-shot transfer performance.

## Key Results
- Three distinct failure modes in VLM-CL: cross-modal feature drift, shared module interference, and zero-shot capability erosion
- Parameter-efficient adaptation combined with regularization is the dominant approach in current research
- Cross-modal alignment drift is a unique challenge requiring specialized evaluation metrics beyond standard CL measures
- Most current methods focus on discriminative tasks, with generative and interactive scenarios remaining underexplored

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Parameter-efficient adaptation (PEA) mitigates shared module interference by freezing the pre-trained backbone and updating only small, isolated modules (e.g., adapters, LoRA).
- Mechanism: The VLM backbone parameters θ are frozen. For each new task t, a small set of new parameters φ_t (e.g., a low-rank matrix ΔW_t in LoRA or an adapter module) is introduced and trained. This isolates task-specific updates, preventing direct overwrites to shared weights.
- Core assumption: The knowledge required for new tasks can be efficiently captured in a low-dimensional subspace or small module without needing to modify the pre-trained backbone.
- Evidence anchors: [abstract] "Parameter-Efficient Adaptation mitigates parameter interference with modular or low-rank updates." [section] Section II.D and IV.C: Describes methods like LoRA (ΔW = BA) and adapters. Explicitly states PEA addresses "shared module interference" by "structurally isolating updates for each task, preventing catastrophic overwrites of shared weights." [corpus] "PEFT A2Z" survey: Confirms PEFT methods update only a small subset of parameters, addressing computational cost and interference in large models.
- Break condition: The assumption holds if new tasks require significant changes to the pre-trained feature space (e.g., a drastic domain shift unanticipated by the pre-trained data), rendering small, local updates insufficient. The low-rank assumption breaks down.

### Mechanism 2
- Claim: Multi-modal replay strategies counter cross-modal feature drift by explicitly re-aligning visual and textual feature spaces during training on new tasks.
- Mechanism: A small subset of data from previous tasks is stored in a memory buffer or generated (implicit replay). During training on task t, the model is trained on both new data D_t and the replayed data D_replay. The loss on D_replay forces the updated feature extractors to produce embeddings consistent with the original alignment, preventing drift.
- Core assumption: A representative sample of past data (or its generative approximation) is sufficient to maintain the global cross-modal alignment, and replaying this data is more effective than regularization alone for preserving complex multimodal relationships.
- Evidence anchors: [abstract] "Multi-Modal Replay Strategies address cross-modal drift through explicit or implicit memory mechanisms." [section] Section IV.A: Defines Multi-Modal Replay. States it "tackle[s] the critical challenges of cross-modal feature drift... by replaying information from previous tasks, these methods help realign modality encoders." [corpus] "Synthetic Data is an Elegant GIFT...": Proposes using generated synthetic data for replay to address forgetting in VLMs, validating the replay concept.
- Break condition: The memory buffer is too small to capture the diversity of past tasks, or the generated pseudo-samples are of poor quality, failing to represent the true data distribution and thus being ineffective at preventing drift.

### Mechanism 3
- Claim: Cross-modal regularization preserves modality alignment by constraining the model update process using knowledge distillation from a stable reference model.
- Mechanism: A "teacher" model (often the pre-trained or previous task's model) provides a stable reference. A distillation loss L_KD (e.g., matching logit distributions or feature maps) is added to the primary task loss. This penalizes updates that would distort the cross-modal feature space relationships learned by the teacher.
- Core assumption: The structure of the feature space (e.g., the relative similarity between image and text embeddings) contains essential knowledge that is more important to preserve than exact parameter values, and this structure can be transferred effectively from a frozen teacher.
- Evidence anchors: [abstract] "Cross-Modal Regularization preserves modality alignment during updates." [section] Section IV.B: Describes knowledge distillation as a primary technique. Gives the example of DualTeacher: "stabilizes the modality relationship by... aligning it with both the pre-trained and latest teacher models via a weighted distillation loss." [corpus] "GNSP: Gradient Null Space Projection...": Proposes a related but distinct regularization method (projecting gradients into the null space) to preserve alignment, supporting the broader principle of constraining updates.
- Break condition: The new task's data is so different from the teacher's knowledge that the distillation constraints become overly restrictive, hindering plasticity (the model's ability to learn the new task).

## Foundational Learning

- **Catastrophic Forgetting & Stability-Plasticity Dilemma**
  - Why needed here: This is the fundamental problem VLM-CL aims to solve. Understanding that a model must be stable to retain old knowledge and plastic to learn new knowledge is crucial for evaluating any CL method.
  - Quick check question: Can you explain why simply fine-tuning a model on a new task leads to a sharp performance drop on old tasks? What are the two opposing forces a CL method must balance?

- **Cross-Modal Feature Alignment in VLMs (e.g., CLIP)**
  - Why needed here: The core of a VLM is the shared embedding space for images and text. The three failure modes (drift, interference, erosion) all relate to disrupting this alignment.
  - Quick check question: In a dual-encoder model like CLIP, how are the image and text encoders trained to work together? What would happen if you updated only the image encoder without any constraint?

- **Parameter-Efficient Fine-Tuning (PEFT) basics (LoRA, Adapters)**
  - Why needed here: The taxonomy highlights PEA as a major paradigm. The onboarding experiments involve PEFT modules. You need to know what a low-rank update or an adapter bottleneck is.
  - Quick check question: Describe how LoRA works. Instead of updating the full weight matrix W, what is it updating? Why does this require fewer parameters?

## Architecture Onboarding

- **Component map**: Frozen VLM Backbone -> PEA Modules (Adapters/LoRA/Prompts) -> Interaction Module (Memory Buffer/Reference Model/Routing Logic/Loss Combiner)
- **Critical path**: The path from input to output during training is: Input -> Frozen Encoder (+PEA Modules) -> Output. The critical path for CL is the gradient flow during the backward pass. Gradients from L_task flow through the frozen backbone (stopping) and into the PEA modules, updating them. Gradients from L_CL_reg are computed by comparing the current model's output against the Reference Model's output, pulling PEA modules towards preserving old knowledge.
- **Design tradeoffs**: Memory (Replay) - better stability vs. storage cost and privacy issues. Computation (Regularization) - forward pass through a second (reference) model doubles computation. Capacity (PEA) - high efficiency vs. limited plasticity. Hyperparameter Tuning - balancing λ in L_total is critical.
- **Failure signatures**: Zero-shot capability erosion - performance on a held-out, unrelated dataset (e.g., ImageNet-1k zero-shot) drops sharply. Cross-modal drift - Image-to-Text and Text-to-Image retrieval scores diverge or both drop significantly. Shared module interference - performance on the immediately preceding task drops dramatically after learning a new one, especially in fusion-based models.
- **First 3 experiments**:
  1. Establish a simple baseline: Implement continual fine-tuning on a sequence of 2-3 simple classification tasks (e.g., from MTIL benchmark) using only a basic PEFT method like LoRA. Measure the forgetting on the first task.
  2. Implement a single CL technique: Add a simple knowledge distillation loss (L_KD) using a frozen copy of the initial model as the teacher. Compare the forgetting rate against the baseline from experiment 1.
  3. Diagnose failure modes: Run the best model from experiment 2 on a zero-shot transfer task (not seen during training). Then, plot the t-SNE embeddings of the image and text features from the first task before and after learning the second task. Visually inspect for feature drift.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can forgetting mitigation strategies be adapted to scale for Continual Pre-training (CPT) on billion-parameter vision-language models?
- Basis in paper: [explicit] Section VI-B states that methodologies for CPT remain in their "infancy" and asks whether importance weighting methods can be made "efficient enough for billion-parameter models" or if models can learn to generate their own "pseudo-replays."
- Why unresolved: Current research predominantly operates under the Continual Fine-tuning (CFT) paradigm on smaller datasets. Scaling regularization techniques like EWC to web-scale pre-training is computationally prohibitive, and effective dynamic architectures or replay mechanisms for foundational models are not yet established.
- What evidence would resolve it: The development of algorithms that enable VLMs to ingest web-scale temporal data streams (like TiC) without suffering from zero-shot capability erosion or computational explosion.

### Open Question 2
- Question: What mechanisms are required for multimodal chatbots and robots to learn continually from interactive feedback without forgetting past interactions?
- Basis in paper: [explicit] Section VI-C identifies the "next frontier" as endowing generative and interactive models with continual learning capabilities, explicitly asking: "How can a multimodal chatbot continually learn from its conversations with a user without forgetting past interactions?"
- Why unresolved: The field currently focuses on discriminative tasks (classification/retrieval). Interactive scenarios introduce complex challenges such as learning from unstructured feedback, managing long-term memory, and ensuring safety/alignment over time, which are currently unaddressed.
- What evidence would resolve it: Frameworks that maintain consistent performance and alignment on sequential instruction-tuning benchmarks (like CoIN or MLLM-CL) while successfully integrating new skills from user interactions.

### Open Question 3
- Question: What are the theoretical bounds on the information capacity of Parameter-Efficient Adaptation (PEA) modules before they cause interference with pre-trained knowledge?
- Basis in paper: [explicit] Section VI-D calls for a "deeper theoretical understanding," specifically asking: "Can we derive theoretical bounds on how much new information a PEA module can learn before it interferes with pre-trained knowledge?"
- Why unresolved: The field is currently driven by empirical success. There is no formal mathematical understanding of the stability-plasticity trade-off specifically for low-rank updates (LoRA) or adapters within the massive embedding spaces of VLMs.
- What evidence would resolve it: Mathematical proofs or rigorous empirical studies that quantify the capacity limits of low-rank matrices in absorbing new tasks without distorting the pre-trained feature space.

## Limitations
- Most current methods prioritize stability over plasticity, potentially limiting long-term performance on evolving tasks
- Evaluation protocols across different benchmarks lack standardization, making direct comparisons difficult
- Theoretical understanding of modality gaps and their impact on catastrophic forgetting remains underdeveloped

## Confidence
- **High Confidence**: The identification of three core failure modes (cross-modal drift, shared module interference, zero-shot erosion) and the proposed taxonomy of solutions are well-supported by the literature review and align with established CL principles.
- **Medium Confidence**: The dominance of continual fine-tuning with parameter-efficient adaptation is convincingly demonstrated, though the survey acknowledges the need for more diverse approaches and comprehensive evaluations.
- **Medium Confidence**: The benchmark categorization and evaluation metrics are useful, but the lack of standardized protocols across studies introduces uncertainty in comparative conclusions.

## Next Checks
1. Implement a controlled experiment comparing dual-encoder (CLIP) and fusion-based VLMs under identical CL conditions to quantify architecture-specific forgetting patterns.
2. Develop a unified evaluation suite that combines classification, retrieval, and zero-shot transfer metrics to provide a holistic assessment of CL performance.
3. Conduct ablation studies on the relative importance of each failure mode (drift vs. interference vs. erosion) by systematically disabling different CL mechanisms in a standardized VLM-CL pipeline.