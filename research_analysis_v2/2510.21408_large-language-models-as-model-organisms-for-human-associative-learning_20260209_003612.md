---
ver: rpa2
title: Large Language Models as Model Organisms for Human Associative Learning
arxiv_id: '2510.21408'
source_url: https://arxiv.org/abs/2510.21408
tags:
- similarity
- learning
- pairs
- interference
- pair
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) exhibit structured representational
  changes during in-context learning that align with the Non-Monotonic Plasticity
  Hypothesis (NMPH), showing differentiation for moderately similar token pairs and
  integration for less similar pairs. This pattern emerges during the consolidation
  phase when models stabilize learned associations,
---

# Large Language Models as Model Organisms for Human Associative Learning

## Quick Facts
- arXiv ID: 2510.21408
- Source URL: https://arxiv.org/abs/2510.21408
- Reference count: 40
- Large language models (LLMs) exhibit structured representational changes during in-context learning that align with the Non-Monotonic Plasticity Hypothesis (NMPH), showing differentiation for moderately similar token pairs and integration for less similar pairs.

## Executive Summary
This study investigates whether large language models (LLMs) exhibit representational dynamics during in-context learning that mirror human associative learning patterns. The authors demonstrate that LLMs show structured changes in hidden state representations consistent with the Non-Monotonic Plasticity Hypothesis (NMPH), where moderately similar token pairs differentiate while low-similarity pairs integrate during the consolidation phase. These findings position LLMs as valuable model organisms for studying human learning mechanisms, particularly the complex interplay between item similarity and representational change.

## Method Summary
The study uses six transformer-based LLMs (Llama2-7b, Llama3.1-8b, Llama3.2-1b, Llama3.2-3b, Gemma2-9b, Mistral-7b) to test associative learning through repeated token pair presentations without weight updates. Token pairs are generated across 17 similarity levels using gradient-guided search, with last-layer hidden states extracted at initial and final repetitions. The analysis tracks three learning phases (Encoding, Consolidation, Forgetting) based on accuracy thresholds, measuring representational change via cosine similarity differences. Vocabulary interference is estimated through median similarity to a sampled 1,000-token subset.

## Key Results
- LLMs exhibit U-shaped representational change during consolidation: moderately similar pairs differentiate while low-similarity pairs integrate
- Higher vocabulary interference amplifies differentiation for mid- and high-similarity pairs
- Only two models (Llama2-7b and Mistral-7b) showed forgetting phases with accuracy decline

## Why This Works (Mechanism)

### Mechanism 1: Non-Monotonic Plasticity via In-Context Differentiation
- Claim: Moderately similar token pairs undergo representational differentiation during the consolidation phase of in-context learning.
- Mechanism: Repeated co-occurrence in context drives representational change. For moderately similar pairs, differentiation emerges to reduce interference and maintain behavioral accuracy (consolidation), while low-similarity pairs integrate and high-similarity pairs remain stable.
- Core assumption: Last-layer hidden states meaningfully capture representations that drive model behavior.
- Evidence anchors:
  - [abstract] "LLMs exhibit structured representational changes during in-context learning that align with the Non-Monotonic Plasticity Hypothesis (NMPH), showing differentiation for moderately similar token pairs."
  - [section 4.2] "During the Consolidation phase, a striking effect emerges for pairs that were moderately similar before learning (0.55−0.75): these groups exhibit a significant decrease in pairwise similarity... This produces a clear U-shaped pattern."
- Break condition: If consolidation-phase accuracy is unstable or token pairs are not sampled densely across mid-similarity (0.55–0.75), the U-shaped differentiation may not appear.

### Mechanism 2: Vocabulary Interference Modulates Differentiation
- Claim: Higher vocabulary interference amplifies representational differentiation for mid- and high-similarity pairs.
- Mechanism: Pairs compete with alternative completions in the vocabulary. When the target y is similar to many other tokens given x, the model reshapes representations to distinguish the learned association, increasing differentiation.
- Core assumption: A sampled 1,000-token subset of the vocabulary provides a sufficient estimate of global interference.
- Evidence anchors:
  - [abstract] "Higher vocabulary interference amplifies differentiation, suggesting that representational change is influenced by both item similarity and global competition."
  - [section 4.3] "Across all similarity levels, we observe that higher vocabulary interference is consistently associated with reduced pairwise similarity after learning."
- Break condition: If vocabulary sampling is too small or biased, interference estimates may misrepresent true competition, weakening the modulation effect.

### Mechanism 3: Phase-Structured Learning Dynamics (Encoding → Consolidation → Forgetting)
- Claim: In-context associative learning unfolds in distinct phases with different representational signatures; NMPH primarily manifests during consolidation.
- Mechanism: Initially, accuracy rises and low/mid-similarity pairs integrate (encoding). Once accuracy stabilizes, consolidation shows U-shaped differentiation for mid-similarity pairs. In some models, prolonged context leads to accuracy decline and loss of structured updates (forgetting).
- Core assumption: Phase definitions based on ±3% accuracy thresholds generalize across models and contexts.
- Evidence anchors:
  - [section 4.1] "We identified three distinct phases of learning–Encoding, Consolidation, and Forgetting... While all models exhibited the Encoding and Consolidation phases, only two models (Llama2-7b and Mistral-7b) showed a forgetting phase."
  - [section 4.2] "This non-monotonic pattern is present only during the Consolidation phase, when behavioral performance is stably high, but absent during the Encoding and Forgetting."
- Break condition: If context windows are shorter or repetitions insufficient for consolidation, U-shaped patterns may not emerge; early forgetting may occur for architectures with different attention mechanisms.

## Foundational Learning

- Concept: In-Context Learning (ICL)
  - Why needed here: The entire paradigm uses ICL to induce associations without weight updates.
  - Quick check question: Can you explain how a model predicts the next token y given repeated presentations of pair (x, y) in the input sequence?

- Concept: Representational Similarity (cosine similarity of hidden states)
  - Why needed here: The paper measures representational change as differences in cosine similarity before and after learning.
  - Quick check question: Given two vectors h_x and h_y, how would you compute their cosine similarity and interpret a decrease after learning?

- Concept: Non-Monotonic Plasticity Hypothesis (NMPH)
  - Why needed here: NMPH predicts a U-shaped curve; this is the primary hypothesis tested.
  - Quick check question: According to NMPH, what representational change is expected for low, moderate, and high similarity pairs before learning?

## Architecture Onboarding

- Component map:
  Input token pairs (x,y) -> Concatenation without separators -> Transformer layers -> Last hidden states h_x, h_y -> Cosine similarity computation -> Phase-based analysis

- Critical path:
  1. Select token pairs with controlled pre-learning similarity (via GCG-inspired search)
  2. Run ICL sequence to r repetitions, recording hidden states
  3. Identify phases by accuracy thresholds; compute ΔS per pair and similarity group
  4. Estimate vocabulary interference and test modulation effects

- Design tradeoffs:
  - Last-layer focus vs. earlier layers: last layer controls behavior but may miss earlier-layer dynamics; appendix shows NMPH weaker in earlier layers
  - Synthetic vs. WordNet tokens: synthetic enables dense similarity sampling; WordNet is more naturalistic but biased toward high interference
  - Vocabulary sampling: 1k tokens improves tractability but may underrepresent full interference structure

- Failure signatures:
  - No U-shaped pattern: may indicate insufficient consolidation-phase repetitions or sparse mid-similarity sampling
  - Early accuracy drop: potential forgetting phase; may relate to context length limits or sliding window attention effects
  - High-variance ΔS at high similarity: likely heterogeneous vocabulary interference levels; stratify by interference to diagnose

- First 3 experiments:
  1. Replicate the main NMPH analysis on one model (e.g., Llama3.1-8b) with 12 pairs per similarity group; verify U-shaped ΔS during consolidation
  2. Systematically vary vocabulary interference (Low/Mid/High) while holding pair similarity constant; confirm modulation of differentiation magnitude
  3. Extend repetitions until forgetting onset (where applicable) and track when the U-shaped pattern collapses; correlate with accuracy decline and interference shifts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the onset and occurrence of the forgetting phase in LLMs be predicted from model architecture, hyperparameters, or representational dynamics?
- Basis in paper: [explicit] "More broadly, it remains unclear how to predict if, and when, forgetting will occur. We leave this question to future work."
- Why unresolved: Only two of six models exhibited forgetting, with different onset times (r=40 vs r=3000), and preliminary analyses suggest competition from similar vocabulary items may contribute, but mechanisms remain speculative.
- What evidence would resolve it: Systematic manipulation of context length, attention mechanisms, and vocabulary interference across model families; identification of predictive markers in attention patterns or hidden states preceding accuracy decline.

### Open Question 2
- Question: Do NMPH-consistent representational dynamics emerge through developmental-like training curricula that mirror human learning trajectories?
- Basis in paper: [explicit] "This work focuses on hypotheses tested in mature adult brains, leaving open the question of how these processes emerge during development."
- Why unresolved: Current models are studied post-training; the developmental trajectory of these representational dynamics during training is unexplored.
- What evidence would resolve it: Training LLMs with staged curricula and measuring representational change dynamics at checkpoints; comparing dynamics across models trained with different curriculum schedules.

### Open Question 3
- Question: Which attention circuits and heads are responsible for detecting and resolving vocabulary interference during in-context learning?
- Basis in paper: [explicit] "Promising future directions include... analyzing attention patterns to identify circuit-level mechanisms involved in resolving interference."
- Why unresolved: The paper establishes that vocabulary interference modulates differentiation but does not examine which architectural components implement this modulation.
- What evidence would resolve it: Attention head ablation studies targeting interference-sensitive heads; causal intervention on attention patterns during high vs. low interference conditions; mechanistic interpretability analysis of attention-to-vocabulary relationships.

## Limitations
- The reliance on last-layer hidden states as proxies for representational change may not capture full network dynamics
- Vocabulary interference estimation using 1,000 sampled tokens may underrepresent true global competition patterns
- Synthetic token generation, while enabling controlled similarity sampling, may not reflect naturalistic language learning patterns

## Confidence
- **High confidence**: The basic methodology for measuring representational change via cosine similarity differences is sound and reproducible. The NMPH U-shaped pattern during consolidation is clearly demonstrated in the data.
- **Medium confidence**: The vocabulary interference modulation effect is consistently observed but may be sensitive to sampling methods and token pair selection. The phase-structured learning dynamics are well-characterized but may vary with different experimental parameters.
- **Low confidence**: The generalizability of findings to other model architectures (particularly those with different attention mechanisms or training objectives) remains unclear. The relationship between synthetic token patterns and real-world language learning is speculative.

## Next Checks
1. **Cross-layer validation**: Extend the representational analysis to earlier transformer layers to verify whether NMPH patterns are consistent throughout the network or primarily emerge in the last layer.
2. **Vocabulary sampling robustness**: Conduct sensitivity analysis by varying the vocabulary sample size (e.g., 500, 2000, 5000 tokens) and sampling methods to determine how robust the interference modulation findings are to different estimates of global competition.
3. **Architectural generalization**: Replicate the core NMPH analysis on transformer variants with different attention mechanisms (e.g., sliding window, local attention) to test whether the observed phase-structured learning dynamics and U-shaped differentiation patterns are specific to standard full-attention architectures.