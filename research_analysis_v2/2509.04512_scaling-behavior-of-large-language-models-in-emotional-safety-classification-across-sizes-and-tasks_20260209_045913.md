---
ver: rpa2
title: Scaling behavior of large language models in emotional safety classification
  across sizes and tasks
arxiv_id: '2509.04512'
source_url: https://arxiv.org/abs/2509.04512
tags:
- unsafe
- classification
- safety
- categories
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates how model scale and supervision affect the
  ability of large language models (LLMs) to detect emotionally unsafe content in
  mental health applications. Using LLaMA models (1B, 3B, 8B, 70B), the authors evaluate
  trinary (safe, unsafe, borderline) and multi-label classification across six harm
  categories.
---

# Scaling behavior of large language models in emotional safety classification across sizes and tasks

## Quick Facts
- arXiv ID: 2509.04512
- Source URL: https://arxiv.org/abs/2509.04512
- Reference count: 40
- Larger models perform better in zero-shot settings, but few-shot prompting and lightweight fine-tuning enable smaller models to achieve comparable accuracy

## Executive Summary
This study investigates how model scale and supervision affect large language models' ability to detect emotionally unsafe content in mental health applications. Using LLaMA models (1B, 3B, 8B, 70B), the authors evaluate trinary and multi-label classification across six harm categories. Results show that while larger models perform better in zero-shot settings, few-shot prompting significantly narrows the performance gap for smaller models. Crucially, lightweight fine-tuning enables the 1B model to achieve accuracy levels comparable to larger models and strong baselines like BERT, while requiring less than 2GB of VRAM. These findings demonstrate that compact, on-device models can effectively handle safety-critical classification tasks, offering a privacy-preserving alternative for emotionally sensitive applications.

## Method Summary
The authors evaluate LLaMA-3 models (1B, 3B, 8B, 70B) across three supervision conditions: zero-shot, few-shot, and fine-tuned. They use five mental health datasets merged and augmented via GPT-3.5 to generate safe/borderline/unsafe variants across six harm categories. Classification is performed using both trinary (safe, unsafe, borderline) and multi-label taxonomy tasks. The 1B model is fine-tuned using LoRA adapters with under 2GB of VRAM, then compared against larger models and DistilBERT baselines across accuracy metrics.

## Key Results
- Larger models achieve stronger average performance, particularly in zero-shot settings and nuanced multi-label classification
- Few-shot prompting dramatically boosts classification performance for smaller models, narrowing the performance gap
- Lightweight fine-tuning enables the 1B model to achieve accuracy levels comparable to larger models and strong baselines like BERT
- The fine-tuned 1B model uses under 2GB of VRAM, enabling efficient on-device deployment

## Why This Works (Mechanism)

### Mechanism 1
Scale provides zero-shot inference capability for nuanced safety classification, but this advantage diminishes with minimal supervision. Larger models encode richer representations of harm categories during pretraining, enabling them to recognize unsafe patterns without examples. However, this advantage is not intrinsic to scale alone—smaller models can approximate it when given task-specific guidance through in-context learning or fine-tuning, suggesting the capability exists latently but requires activation. Core assumption: The performance gains from scale reflect pretraining coverage breadth rather than fundamental architectural advantages. Break condition: If tasks require reasoning beyond pretraining distribution (e.g., novel harm taxonomies), scale advantages may persist even with few-shot supervision.

### Mechanism 2
Few-shot prompting narrows the performance gap between small and large models by anchoring latent capabilities to task-specific patterns. In-context examples serve as pattern recognizers, helping models map abstract safety concepts to concrete taxonomic labels. This is especially effective for smaller models whose zero-shot performance is poor, as it bypasses the need for extensive pretraining coverage by providing explicit signal. Core assumption: Smaller models possess latent safety understanding that requires explicit activation rather than additional learning. Break condition: If categories are severely underrepresented (e.g., guns, criminal planning), few-shot may be insufficient without more examples or fine-tuning.

### Mechanism 3
Lightweight fine-tuning recovers scale-dependent performance while preserving deployment efficiency. LoRA adapters modify only a small subset of parameters, allowing the model to specialize for the emotional safety taxonomy without full retraining. This demonstrates that general-purpose safety understanding can be efficiently distilled into compact models through targeted supervision. Core assumption: The fine-tuned representations transfer within but not necessarily beyond the specific taxonomy distribution. Break condition: Fine-tuning on imbalanced or noisy data may overfit to frequent categories while failing on rare harm types.

## Foundational Learning

- Concept: In-context learning vs. weight-based adaptation
  - Why needed here: The paper compares zero-shot, few-shot, and fine-tuning—understanding the distinction is essential for interpreting why small models recover performance with supervision.
  - Quick check question: Can you explain why few-shot helps without updating weights, while fine-tuning permanently alters model behavior?

- Concept: Parameter-efficient fine-tuning (PEFT/LoRA)
  - Why needed here: The 1B model achieves parity with 70B using LoRA adapters; understanding this technique is critical for reproducing the result.
  - Quick check question: What layers does LoRA modify in this study, and why does it require less VRAM than full fine-tuning?

- Concept: Taxonomic safety classification vs. binary moderation
  - Why needed here: The paper evaluates both trinary (safe/unsafe/borderline) and multi-label taxonomy tasks—each has different failure modes.
  - Quick check question: Why might a model succeed at binary classification but fail at distinguishing suicide/self-harm from violence?

## Architecture Onboarding

- Component map:
  Data pipeline: Five mental health datasets merged, then augmented via GPT-3.5 to generate safe/borderline/unsafe variants across six harm categories -> Model family: LLaMA-3 (1B, 3B, 8B, 70B) in 4-bit quantization; DistilBERT as supervised baseline -> Training: LoRA adapters (rank 16) on attention and MLP projections; SFTTrainer with response-only loss -> Evaluation: Accuracy across trinary classification and multi-label taxonomy; VRAM profiling via `torch.cuda.max_memory_allocated()`

- Critical path:
  1. Dataset construction → augmentation via GPT-3.5 reappraisal prompts
  2. Model selection (LLaMA-3 family) → quantization for on-device simulation
  3. Zero-shot/few-shot prompting → performance baseline
  4. LoRA fine-tuning on 1B model → compare to larger models and BERT
  5. VRAM measurement → deployment feasibility assessment

- Design tradeoffs:
  - Taxonomy coverage vs. evaluation reliability: Full taxonomy (6 categories) has single-run evaluation; high-data subset (3 categories) has multiple runs but excludes rare harms
  - Synthetic augmentation vs. real distribution: GPT-generated variants may introduce stylistic artifacts inflating safe/unsafe separability
  - 4-bit quantization vs. full precision: Enables on-device inference but may marginally affect accuracy

- Failure signatures:
  - Zero-shot collapse on small models: 1B model achieves 0.000 accuracy across all categories without examples
  - Non-monotonic scaling: 3B occasionally outperforms 8B on specific categories, suggesting prompt sensitivity
  - Category imbalance: Guns & illegal weapons has only 65 samples; models fail to learn this reliably

- First 3 experiments:
  1. Reproduce zero-shot vs. few-shot gap on the high-data subset (suicide, violence, sexual content) to validate scaling trends
  2. Fine-tune the 1B model with LoRA using the provided hyperparameters; compare VRAM usage and accuracy to the 70B model
  3. Test generalization by evaluating the fine-tuned 1B model on held-out borderline cases to assess ambiguity handling

## Open Questions the Paper Calls Out

### Open Question 1
Do classification gains stem from semantic understanding of safety or detection of stylistic artifacts introduced by GPT-3.5 data augmentation? Basis in paper: [explicit] The authors note that pairing real posts with LLM-generated reappraisals "may introduce stylistic artifacts that models could exploit, potentially inflating performance." Why unresolved: If models rely on the distinct style of synthetic "safe" (hopeful) vs. "unsafe" (explicit) text rather than semantic content, their reliability on real-world, non-augmented data is questionable. What evidence would resolve it: Evaluating the models on a held-out set of human-curated safe and unsafe examples that contain no synthetic augmentation.

### Open Question 2
Does the fine-tuning efficiency observed in LLaMA 3 transfer to other model architectures or tokenizer configurations? Basis in paper: [explicit] The analysis is restricted to the LLaMA family, and the authors state "conclusions may not transfer directly to other architectures... which differ in alignment and fine-tuning strategies." Why unresolved: The "threshold" where small models recover performance via supervision may be specific to LLaMA's architecture or tokenizer, limiting the generalizability of the on-device deployment claim. What evidence would resolve it: Replicating the fine-tuning protocol (LoRA, 1-epoch) on non-LLaMA families (e.g., Mistral, Gemma) to see if the 1B scaling behavior holds.

### Open Question 3
Can lightweight fine-tuning reliably rescue performance for underrepresented harm categories (e.g., Criminal Planning) where few-shot prompting currently fails? Basis in paper: [inferred] The authors show fine-tuning rescues performance in "high-data categories" (Fig 1), but Table 2 shows the 1B model fails completely (0.000 accuracy) on "Criminal Planning" even with few-shot examples. Why unresolved: It is unclear if the fine-tuning "rescue" effect applies only to frequent categories (Suicide, Violence) or if it generalizes to the long-tail of rare safety risks. What evidence would resolve it: Reporting fine-tuning accuracy specifically for the low-count taxonomy categories (Guns, Criminal Planning) excluded from the high-data analysis.

### Open Question 4
To what extent does the LLaMA Guard taxonomy alignment used for "unsafe" labels correlate with human clinical judgment? Basis in paper: [explicit] The authors acknowledge their definition of safety "may not fully capture the nuances of commercial moderation systems or human clinical judgments." Why unresolved: The dataset relies on a taxonomy-based definition of harm; without clinical validation, the "emotionally unsafe" label may not align with what therapists consider risky in a therapeutic context. What evidence would resolve it: A correlation analysis between model predictions and blinded ratings from mental health professionals on the same text samples.

## Limitations
- Reliance on GPT-3.5-generated synthetic examples for data augmentation may introduce stylistic artifacts that inflate safe/unsafe separability
- Evaluation on full taxonomy based on single-run performance without confidence intervals, making it difficult to assess statistical significance
- Excludes low-resource categories (guns, criminal planning, human trafficking) from detailed analysis due to data scarcity
- Focus on LLaMA-3 models limits generalizability to other model families or architectures

## Confidence

**High Confidence**: The claim that few-shot prompting narrows the performance gap between small and large models is well-supported by the experimental results showing dramatic improvements for 1B and 3B models with minimal examples. The VRAM efficiency claims for LoRA fine-tuning on the 1B model are also highly reliable, given the precise measurements reported.

**Medium Confidence**: The assertion that lightweight fine-tuning enables 1B models to achieve accuracy comparable to larger models and BERT baselines is supported by the data but requires careful interpretation. While accuracy metrics show parity, the evaluation is limited to specific categories and may not generalize to the full taxonomy or real-world deployment conditions.

**Low Confidence**: The broader claim that these findings enable privacy-preserving, on-device safety classification for mental health applications extends beyond the experimental scope. The study demonstrates feasibility in controlled settings but does not address deployment challenges, user acceptance, or longitudinal performance in clinical contexts.

## Next Checks

1. **Replicate the scaling trends on external datasets**: Apply the same zero-shot, few-shot, and fine-tuned evaluation pipeline to the Jigsaw Toxicity dataset and other established safety benchmarks to verify whether the observed performance patterns generalize beyond the mental health domain.

2. **Test generalization to rare categories**: Fine-tune the 1B model on an augmented version of the full taxonomy (including guns, criminal planning, human trafficking) using targeted oversampling or synthetic generation, then evaluate on held-out examples to assess whether the scaling benefits extend to low-resource harm types.

3. **Measure real-world deployment performance**: Deploy the fine-tuned 1B model in a controlled mental health chatbot environment with human evaluators, measuring not just accuracy but also false positive rates, user trust scores, and model calibration on ambiguous borderline cases over time.