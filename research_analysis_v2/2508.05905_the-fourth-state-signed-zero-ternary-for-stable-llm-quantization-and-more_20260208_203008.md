---
ver: rpa2
title: 'The Fourth State: Signed-Zero Ternary for Stable LLM Quantization (and More)'
arxiv_id: '2508.05905'
source_url: https://arxiv.org/abs/2508.05905
tags:
- ternary
- quantization
- weight
- signed-zero
- deterministic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Signed-Zero Ternary (SZT), a 2-bit quantization\
  \ scheme for Large Language Models (LLMs) that uses the fourth state in the ternary\
  \ encoding to distinguish two zero states (0+ and 0\u2212) during training. Unlike\
  \ balanced ternary quantization, SZT provides deterministic gradient information\
  \ within the dead-zone interval |w| \u2264 \u2206, eliminating the vanishing gradient\
  \ problem that occurs when weights fall in this region."
---

# The Fourth State: Signed-Zero Ternary for Stable LLM Quantization (and More)

## Quick Facts
- arXiv ID: 2508.05905
- Source URL: https://arxiv.org/abs/2508.05905
- Reference count: 40
- Primary result: SZT provides up to 30× more gradient feedback events per epoch compared to balanced ternary

## Executive Summary
This paper introduces Signed-Zero Ternary (SZT), a 2-bit quantization scheme for Large Language Models that uses an unused fourth state to distinguish two zero states (0+ and 0−) during training. Unlike balanced ternary quantization, SZT provides deterministic gradient information within the dead-zone interval |w| ≤ ∆, eliminating the vanishing gradient problem that occurs when weights fall in this region. The core idea is that SZT retains the same forward-path behavior as balanced ternary but provides sign information to the optimizer during backpropagation for sub-threshold weights. This is implemented through changes to only the encode/decode logic, leaving the matrix-multiply data path unchanged.

## Method Summary
SZT is a 2-bit quantization scheme that maps latent weights to four states: {-1, 0-, 0+, +1}. During the forward pass, both 0- and 0+ decode to numeric 0, maintaining identical behavior to balanced ternary. The key innovation is in the backward pass: when weights fall in the dead-zone (|w| ≤ ∆), the straight-through estimator multiplies the upstream gradient by the stored sign (sgn(q)), providing directional information to the optimizer. This deterministic approach eliminates the vanishing gradient problem without requiring changes to inference kernels or storage cost. The per-layer threshold ∆ is set to the standard deviation of weights by default, though this can be calibrated for optimal performance.

## Key Results
- SZT provides up to 30× more gradient-bearing feedback events per epoch compared to balanced ternary
- SZT's straight-through estimator has lower mean-squared error inside the dead zone than both balanced ternary and stochastic rounding
- Weights escape the dead zone substantially faster under SZT, with mean-first-passage time improvement scaling exponentially with the signal-to-noise ratio

## Why This Works (Mechanism)

### Mechanism 1: Deterministic Gradient Flow Through the Dead Zone
SZT provides gradient information to the optimizer for sub-threshold weights that would otherwise produce zero gradient under balanced ternary. The unused 4th codeword in 2-bit ternary encoding distinguishes 0+ (weights in (0, ∆]) from 0− (weights in [−∆, 0)). During backpropagation, the straight-through estimator multiplies the upstream gradient by the stored sign sgn(q) when |w| ≤ ∆, rather than passing it unchanged. This gives the optimizer a directional signal on every weight update, even when the numeric value remains zero. The advantage depends on weight distributions being symmetric and unimodal with p(0) > p(∆).

### Mechanism 2: Reduced Straight-Through Estimator Bias
SZT's STE has lower mean-squared error than both balanced ternary and stochastic rounding inside the dead zone when |w| < ∆/2. Balanced ternary STE passes gradient g unchanged, incurring bias ∥g∥. SZT multiplies by sgn(w), making bias proportional to distance from zero: |w|/∆ · ∥g∥. This vanishes linearly as w → 0, whereas BT maintains constant bias. The improvement holds when the loss is L-Lipschitz in each weight (normalized to L ≤ 1).

### Mechanism 3: Accelerated Dead-Zone Escape via Sign Feedback
Weights escape the dead zone faster under SZT than balanced ternary, with mean-first-passage time improvement that scales exponentially in λ² = (κ∆/σ)². Under an Ornstein-Uhlenbeck proxy for SGD dynamics, BT weights wander with no drift near zero until random noise pushes them past ±∆. SZT's sign-based gradient provides an effective alternating drift that pulls weights toward the nearest boundary, reducing MFPT from O(e^λ²/κ) to O(1/κ).

## Foundational Learning

- Concept: **Straight-Through Estimator (STE)**
  - Why needed here: SZT modifies the STE to pass sign information in the dead zone. Understanding standard STE (passing gradient unchanged through a non-differentiable quantizer) is prerequisite to seeing why the modification matters.
  - Quick check question: Given a quantizer Q(w) that outputs −1, 0, or +1, what gradient does a standard STE return for w = 0.3 when ∆ = 1.0?

- Concept: **Dead Zone in Quantization**
  - Why needed here: The entire motivation for SZT is the dead-zone problem where weights produce no state changes and no gradient signal. Recognizing this pathology is essential.
  - Quick check question: For balanced ternary with threshold ∆ = 0.5, what happens to a weight at w = 0.1 that receives updates of magnitude 0.05 for 10 consecutive steps?

- Concept: **Mean-First-Passage Time (MFPT)**
  - Why needed here: The paper's convergence argument uses MFPT to quantify how long weights "linger" in the dead zone before escaping. This is the analytical basis for claiming faster convergence.
  - Quick check question: If a random walk has a 10% chance per step of leaving an interval, what is the expected number of steps to exit?

## Architecture Onboarding

- Component map: Quantizer Qszt(w) → {-1, 0-, 0+, +1} → Decoder v(q) → {-1, 0, +1} → GEMM kernel
- Critical path: Calibration pass computes per-layer σ to set ∆ → Forward: encode → decode → GEMM (unchanged kernel) → Backward: standard gradient outside dead zone; sgn(q)·gradient inside → Optimizer update proceeds normally
- Design tradeoffs:
  - ∆ = σ vs. optimal: Default is near-optimal for Laplace (exact) and Gaussian (<5% MSE penalty). Tunable if calibration budget allows.
  - Per-layer vs. per-channel: Paper proves extensions to channel-wise ∆_c at cost of bookkeeping. Start with per-layer.
  - Activation quantization: Appendix E extends SZT to activations with half-Laplace/half-Gaussian thresholds (∆_act = σ/2). Optional; weights-only is the core contribution.
- Failure signatures:
  - No improvement over BT: Check if p(0)/p(∆) ≈ 1 (weight histogram flat near zero). SZT advantage requires peaked distributions.
  - Training instability: Verify STE is applied correctly (sign multiplication inside dead zone, not outside).
  - Reproducibility failure: Ensure no stochastic rounding is mixed in; SZT is fully deterministic.
- First 3 experiments:
  1. Histogram validation: On a pretrained checkpoint, plot weight histograms per layer. Verify p(0)/p(∆) ≥ 10 (implying ≥10× sensitivity ratio). If not, SZT may not help.
  2. Dead-zone escape timing: Train a small model (e.g., MLP on MNIST) with BT vs. SZT. Log the fraction of weights in dead zone per epoch. Expect faster decay under SZT.
  3. Bit-exact reproducibility test: Run two identical training runs with fixed seed. Confirm weight trajectories match exactly (should pass for SZT, fail for stochastic rounding baselines).

## Open Questions the Paper Calls Out

### Open Question 1
What are the empirical training dynamics and final performance metrics of Large Language Models (LLMs) trained with Signed-Zero Ternary (SZT) compared to balanced ternary baselines? The paper introduction explicitly states that the discussion section "discusses hardware implications and open empirical questions for future work," acknowledging that the current work is restricted to analytical results. The theoretical advantages (e.g., 30× gradient feedback, lower MSE) need experimental validation through end-to-end training curves and benchmark perplexity scores for modern transformer models.

### Open Question 2
Can the theoretical benefits of SZT be effectively applied to activation quantization without introducing instability near non-linearities like ReLU? Section 4.3 notes that while arguments "carry over," they do so "with the caveats discussed below" regarding the interaction with post-ReLU distributions. The analysis primarily focuses on weights; the application to activations is presented as a theoretical sketch requiring empirical verification of training stability and accuracy degradation when applying SZT to both weights and activations simultaneously.

### Open Question 3
Does the reduction in Mean First-Passage Time (MFPT) out of the dead zone persist when using modern adaptive optimizers like AdamW? The convergence analysis relies on Assumption 4, which models weight dynamics using an Ornstein-Uhlenbeck process as a proxy for simple SGD. It is unclear if the complex update rules of adaptive optimizers would preserve the predicted exponential speedup in escaping the dead zone, requiring comparison of weight escape times and convergence rates using AdamW against the theoretical baseline.

## Limitations

- The theoretical advantages depend on assumptions about weight distribution shapes (symmetric, unimodal with p(0) > p(∆)) that may not hold in all models
- The Ornstein-Uhlenbeck approximation for SGD dynamics may not accurately capture real training behavior, especially with adaptive optimizers
- The analysis does not account for potential distribution shifts during training or multi-modal weight patterns that could reduce SZT's effectiveness

## Confidence

- **High Confidence**: The deterministic gradient flow mechanism and bit-exact reproducibility are mathematically proven and directly verifiable through implementation
- **Medium Confidence**: The STE bias reduction claims are rigorously derived but depend on the Lipschitz assumption, requiring empirical validation
- **Medium Confidence**: The dead-zone escape time improvements follow from the OU model, but practical speedup depends on how well SGD approximates OU dynamics

## Next Checks

1. **Distribution validation**: On a pretrained checkpoint, compute p(0)/p(∆) per layer. If this ratio falls below 5 in any layer, SZT's sensitivity advantage may be limited there.
2. **Baseline comparison**: Implement balanced ternary with stochastic rounding and measure STE bias empirically across different dead-zone positions |w| ∈ [0, ∆].
3. **Training stability test**: Run SZT on a small transformer (e.g., 2-layer BERT) and monitor weight histogram evolution—specifically, the fraction of weights in dead zones per epoch should decay faster than balanced ternary.