---
ver: rpa2
title: Transfer Learning for Automated Feedback Generation on Small Datasets
arxiv_id: '2503.11836'
source_url: https://arxiv.org/abs/2503.11836
tags:
- feedback
- learning
- dataset
- automated
- very
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: A three-stage transfer learning pipeline was developed to train
  an automated feedback generation system on very small datasets with long sequences.
  The method involved pre-training on abstractive summarization using the Arxiv dataset,
  followed by peer-review task training on the PeerRead dataset, and finally fine-tuning
  on a small dataset of approximately 56 training samples.
---

# Transfer Learning for Automated Feedback Generation on Small Datasets

## Quick Facts
- arXiv ID: 2503.11836
- Source URL: https://arxiv.org/abs/2503.11836
- Reference count: 19
- Primary result: State-of-the-art automated feedback generation on 56-sample dataset using three-stage transfer learning pipeline

## Executive Summary
This paper addresses the challenge of training automated feedback generation (AFG) systems on extremely small datasets with long sequences (10,000-15,000 tokens). The authors propose a three-stage transfer learning pipeline using a Longformer encoder-decoder model that achieves state-of-the-art results on a fine-tuning dataset of approximately 56 training samples. The approach demonstrates that transfer learning can enable meaningful learning from very small datasets where direct training fails completely, while handling the computational challenges of long sequences through sparse attention mechanisms.

## Method Summary
The method employs a Longformer encoder-decoder model with 16,384 token capacity, trained through a three-stage pipeline: pre-training on Arxiv summarization dataset (203K samples), intermediate training on PeerRead peer-review dataset (~10K samples), and final fine-tuning on the target feedback dataset (~56 training samples). The pipeline uses AdamW optimizer with learning rates of 5×10⁻⁵ for fine-tuning and unspecified parameters for intermediate stages. The approach leverages sparse attention to handle long sequences efficiently, replacing O(n²) complexity with O(n) through sliding window patterns combined with global attention tokens.

## Key Results
- Achieved ROUGE-1 score of 0.40, ROUGE-2 of 0.15, ROUGE-L of 0.25, and ROUGE-LSUM of 0.37 on the fine-tuning dataset
- Model significantly outperformed direct training approaches, which failed to learn anything meaningful on the small dataset
- Qualitative evaluation showed generated feedback comparable to human markers on content and structure, though requiring post-processing for grammatical errors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transfer learning enables model convergence on extremely small datasets where direct training fails completely.
- Mechanism: The three-stage pipeline progressively transfers learned representations, reducing the hypothesis space for each subsequent stage.
- Core assumption: Abstractive summarization and peer-review tasks share sufficient linguistic representations with feedback generation.
- Evidence anchors: "by using a three stage transfer learning pipeline state-of-the-art results can be achieved"; "When only training on the fine-tuning dataset the model was unable to learn and perform in any meaningful way so those results are not reported"
- Break condition: If intermediate tasks share insufficient semantic overlap with the target task, transfer may provide negligible benefit or negative transfer.

### Mechanism 2
- Claim: Sparse attention mechanisms make transformer-based processing of 10,000-15,000 token sequences computationally feasible.
- Mechanism: Longformer's sliding window attention reduces memory complexity from O(n²) to approximately O(n) while preserving task-relevant context.
- Core assumption: Local attention windows capture sufficient syntactic and semantic structure for feedback generation.
- Evidence anchors: "Traditional attention mechanisms scale (memory wise) by O(n²) wrt. sequence length... the sparse attention mechanism was developed for use in the longformer models"
- Break condition: If feedback requires precise cross-document attention beyond the sliding window, important dependencies may be missed.

### Mechanism 3
- Claim: Task ordering by dataset size (largest and most distant → smallest and most specific) optimizes transfer learning efficiency.
- Mechanism: Large datasets establish general linguistic competencies; progressively smaller datasets with higher task similarity refine these competencies toward the target domain.
- Core assumption: The model retains task-relevant features across training stages without catastrophic forgetting.
- Evidence anchors: "The training pipeline is designed to use large datasets first, where the task is further removed from the desired task, followed by smaller datasets whose task is closer to that desired"; Results show progressive ROUGE improvement across stages
- Break condition: If catastrophic forgetting occurs during later stages, earlier learned representations may degrade, reducing transfer benefit.

## Foundational Learning

- Concept: **Transfer Learning in Sequence-to-Sequence Models**
  - Why needed here: The entire approach depends on understanding how pre-trained weights from one task can accelerate learning on another.
  - Quick check question: Can you explain why a model trained on abstractive summarization might help with feedback generation?

- Concept: **Sparse Attention Patterns in Transformers**
  - Why needed here: Standard transformers cannot handle 16,384-token sequences; understanding sliding window and global attention is critical for debugging memory issues.
  - Quick check question: What is the memory complexity difference between full attention and Longformer's sparse attention?

- Concept: **ROUGE Metrics for Text Generation Evaluation**
  - Why needed here: All quantitative claims rest on ROUGE-1, ROUGE-2, ROUGE-L, and ROUGE-LSUM scores; interpreting these is necessary to assess model quality.
  - Quick check question: What does a ROUGE-1 score of 0.40 indicate about the overlap between generated and reference text?

## Architecture Onboarding

- Component map: Arxiv Summarization (203K samples) → PeerRead Peer-Review (10K samples) → Fine-Tuning (~56 samples) → Generated feedback (up to 16,384 tokens input)

- Critical path: Starting from pre-trained LED checkpoint, training proceeds sequentially through PeerRead then the target dataset. Skipping any stage—particularly PeerRead—may reduce performance.

- Design tradeoffs:
  - **Sequence length vs. memory**: 16,384 tokens enables full document processing but requires substantial GPU memory; shorter sequences may truncate important content.
  - **Epoch count vs. overfitting**: 50 epochs on 56 samples risks overfitting; no validation curves reported to confirm generalization.
  - **Qualitative vs. quantitative quality**: High ROUGE scores coexist with "unhuman sounding" outputs and grammatical errors—production use requires post-processing.

- Failure signatures:
  - Model outputs random or repetitive text → likely insufficient pre-training or learning rate too high.
  - CUDA out-of-memory errors → sequence length exceeds available memory; reduce global attention positions or batch size.
  - ROUGE scores near zero on evaluation → transfer pipeline skipped or checkpoint corrupted.
  - Generated feedback contains factual contradictions not in source → hallucination; requires constrained decoding or post-hoc verification.

- First 3 experiments:
  1. **Ablate the PeerRead stage**: Train directly from Arxiv checkpoint to fine-tuning to measure PeerRead's contribution.
  2. **Reduce sequence length**: Test with 4,096 and 8,192 tokens to quantify performance degradation vs. memory savings.
  3. **Add grammar post-processing**: Implement a rule-based or small-model correction layer to reduce grammatical errors identified in qualitative evaluation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What text generation techniques or architectural modifications could improve the naturalness and fluency of generated feedback while maintaining accuracy?
- Basis in paper: The conclusion states "the model's outputs do not read particularly well, however, it is possible that further research or other text generation techniques can improve this."
- Why unresolved: The paper demonstrates functional feedback generation but acknowledges poor output fluency; no systematic investigation of generation techniques was conducted.
- What evidence would resolve it: Comparative experiments using techniques like constrained beam search, nucleus sampling, or reinforcement learning from human feedback (RLHF), evaluated via human preference ratings.

### Open Question 2
- Question: Can automated post-processing methods effectively correct grammatical errors in generated feedback without altering semantic content?
- Basis in paper: The results section notes "some post-processing would be useful to remove some grammatical mistakes in the generated text."
- Why unresolved: The paper identifies grammatical errors as a limitation but proposes no specific solution or evaluation of post-processing approaches.
- What evidence would resolve it: Implementation and evaluation of grammar correction modules (e.g., rule-based or neural correctors) measuring both error reduction and semantic preservation via human evaluation.

### Open Question 3
- Question: How vulnerable is this AFG system to adversarial inputs or manipulation attempts similar to those demonstrated against AES systems?
- Basis in paper: The discussion states "It has also been shown for AES systems that they can be fooled... It is likely that the same is true for AFG systems, potentially creating wholly inaccurate results."
- Why unresolved: No adversarial testing was conducted; the claim of vulnerability is speculative based on AES literature.
- What evidence would resolve it: Systematic adversarial evaluation using input perturbations or gaming strategies, measuring feedback quality degradation.

## Limitations

- Lack of ablation studies to quantify individual contributions of each transfer stage
- Absence of standardized human evaluation metrics beyond subjective qualitative assessment
- 56-sample fine-tuning dataset may limit generalizability to larger or differently distributed datasets

## Confidence

- **High confidence**: The mechanism of sparse attention enabling 16,384-token processing (well-established in Longformer literature)
- **Medium confidence**: The three-stage transfer pipeline's effectiveness (supported by results but lacks ablation)
- **Medium confidence**: The qualitative claim that generated feedback is "comparable to human markers" (subjective without standardized human evaluation metrics)
- **Low confidence**: The specific task ordering strategy (no comparison to alternative orderings or intermediate tasks)

## Next Checks

1. Conduct ablation studies removing each transfer stage to quantify individual contributions to final performance
2. Implement human evaluation protocols with blinded scoring of generated vs. human-written feedback to validate qualitative claims
3. Test model generalization by applying the trained system to feedback generation for different domains (e.g., technical writing, research proposals) to assess domain transfer capabilities