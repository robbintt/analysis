---
ver: rpa2
title: Which Rewards Matter? Reward Selection for Reinforcement Learning under Limited
  Feedback
arxiv_id: '2510.00144'
source_url: https://arxiv.org/abs/2510.00144
tags:
- reward
- selection
- policy
- rewards
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the problem of reward selection for reinforcement
  learning under limited feedback (RLLF), where only a subset of states can be labeled
  with rewards due to constraints. The core method involves developing heuristic strategies
  (visitation sampling, guided sampling) and training-based strategies (sequential-greedy,
  evolutionary search) to select impactful reward-labeled states.
---

# Which Rewards Matter? Reward Selection for Reinforcement Learning under Limited Feedback

## Quick Facts
- **arXiv ID:** 2510.00144
- **Source URL:** https://arxiv.org/abs/2510.00144
- **Reference count:** 40
- **Primary result:** Sequential-greedy reward selection achieves near-optimal policies with O(B|S|) training cost, while zero-cost guided sampling often matches low-cost evolutionary methods.

## Executive Summary
This paper introduces reward selection for reinforcement learning under limited feedback (RLLF), where only a subset of states can be labeled with rewards due to constraints. The core method involves developing heuristic strategies (visitation sampling, guided sampling) and training-based strategies (sequential-greedy, evolutionary search) to select impactful reward-labeled states. Results show that effective reward selection strategies, particularly sequential-greedy and well-tuned guided heuristics, can achieve near-optimal policies with significantly fewer reward labels than full supervision. Key patterns of optimal selections include prioritizing anchor points on high-return paths, states for recovery from deviations, and early labeling of penalty states. Training-based methods outperform heuristics but at higher computational cost, while zero-cost guided sampling often performs comparably to low-cost evolutionary methods. The work establishes reward selection as a powerful paradigm for scaling RL in feedback-limited settings.

## Method Summary
The RLLF framework operates on offline datasets where only a subset of states (budget B) can be labeled with rewards. The method uses UDS (Unknown rewards as zero) algorithm to impute unlabeled rewards as zero, then applies Q-learning or IQL to learn policies from partially labeled data. Selection strategies include heuristics (visitation sampling, uniform, guided sampling) and training-based methods (brute-force, sequential-greedy, evolutionary search). Guided sampling balances exploration (state visitation frequency) and exploitation (proximity to high-value states) through a decaying weight α_b that shifts from exploration to exploitation as Q-estimates become more reliable. Sequential-greedy iteratively constructs labeled state sets by selecting states that maximize marginal utility improvements, achieving near-optimal performance at O(B|S|) training cost.

## Key Results
- Sequential-greedy selection achieves near-optimal reward selection at training cost O(B|S|), far below brute-force O(|S|^B)
- Guided sampling with well-tuned decay schedules performs comparably to low-cost evolutionary methods while requiring zero training cost
- Optimal reward selections follow three structural patterns: anchor points on high-return paths, recovery states near optimal trajectories, and early labeling of penalty states
- Zero-cost guided sampling often matches the performance of low-cost evolutionary methods (ES) across prototypical and MinAtar domains

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Guided sampling balances exploration (state visitation frequency) and exploitation (proximity to high-value states) to select reward-labeled states that improve policy learning under limited budgets.
- **Mechanism:** At each iteration b, the strategy samples from a distribution q_b that blends: (1) the data-collecting policy's state visitation distribution d^π_D for exploration, and (2) the distribution of predecessor states leading to the currently highest-valued state for exploitation. The tradeoff weight α_b decays over time, shifting from exploration to exploitation as Q-estimates become more reliable.
- **Core assumption:** Q-values estimated from partially labeled data become sufficiently accurate as the budget increases to guide exploitation meaningfully.
- **Evidence anchors:**
  - [Section 3.1]: Equation (4) defines q_b(·|Q^π[b-1], b) ∝ α_b d^π_D(·) + (1-α_b) d^π_D_prev(·|argmax Q^π[b-1])
  - [Section 4.1]: "At low budgets...visitation sampling generally provides an effective auxiliary signal...At high budgets...the exploit-term of guided sampling which relies on these Q-values to discover high-value states tends to aid performance"
  - [corpus]: Weak direct evidence; related work on RL from implicit feedback and sparse rewards addresses similar constraints but not state selection heuristics.
- **Break condition:** When bottleneck states (infrequently visited but critical) exist and budget is low, guided sampling may miss these states; uniform sampling can outperform in such domains (Section 4.1, TwoRooms, FrozenLake).

### Mechanism 2
- **Claim:** Sequential-greedy selection achieves near-optimal reward selection at training cost O(B|S|), far below brute-force O(|S|^B).
- **Mechanism:** Iteratively constructs the labeled state set by selecting the state s that maximizes marginal utility Δ(s|S[b]) = P(π[S[b]∪{s}]) - P(π[S[b]]), i.e., the improvement in expected policy return from adding that state. This greedy maximization avoids combinatorial search while empirically matching optimal performance.
- **Core assumption:** The marginal utility function exhibits approximate submodularity, such that greedy selection does not significantly sacrifice optimality.
- **Evidence anchors:**
  - [Section 3.2]: "sequential-greedy is near-optimal with significantly lower training cost than brute-force, establishing that training cost linear in |S| suffices for near-optimal performance"
  - [Figure 5]: Across all prototypical domains, sequential-greedy matches brute-force performance at all feedback levels
  - [corpus]: No direct corpus evidence for this specific mechanism; evolutionary strategies for policy optimization exist but differ in objective.
- **Break condition:** When the evaluator Ξ provides noisy or biased policy performance estimates, greedy selection may amplify errors through sequential dependency.

### Mechanism 3
- **Claim:** Optimal reward selections follow three structural patterns: (1) anchor points on high-return paths, (2) recovery states near optimal trajectories, (3) early labeling of penalty states.
- **Mechanism:** These patterns reflect the informational needs of value-based RL: anchors provide positive reward propagation, recovery states handle stochastic transitions, and penalty states prevent catastrophic exploration. The UDS algorithm (setting unlabeled rewards to zero) amplifies the importance of explicitly labeling penalties.
- **Core assumption:** The underlying policy learning algorithm (Q-learning with UDS) propagates rewards through Bellman updates in the expected manner.
- **Evidence anchors:**
  - [Section 4.3]: "Optimal state sets include states that serve as anchor points to keep the agent on high-return paths...states in S*[B] include those that lie in the vicinity of optimal pathways...Penalty states get reward-labeled early on"
  - [Appendix D.5]: In CliffWalk, "optimal sets include off-path states adjacent to the cliff, effectively constraining the agent's behavior"
  - [corpus]: Related work on safe RL and sparse rewards addresses penalty avoidance but not the selection mechanism itself.
- **Break condition:** In dense reward domains with no hazards (e.g., Graph), patterns 2 and 3 become irrelevant; only path-following matters.

## Foundational Learning

- **Concept: Offline Reinforcement Learning**
  - Why needed here: The entire RLLF framework operates on pre-collected datasets without environment interaction; understanding offline RL constraints (distribution shift, pessimism) is essential.
  - Quick check question: Can you explain why Q-learning on offline data may overestimate values for out-of-distribution states?

- **Concept: Partially Observable / Missing Data in RL**
  - Why needed here: RLLF assumes only a subset of states receive reward labels; handling unlabeled samples (via UDS or adaptive Q-learning) is core to the method.
  - Quick check question: What happens to value estimates when unknown rewards are imputed as zero vs. when Q-updates are skipped entirely?

- **Concept: Exploration-Exploitation Tradeoff in Selection**
  - Why needed here: Guided sampling explicitly balances these forces; understanding when each dominates is critical for tuning α schedules.
  - Quick check question: In a sparse reward domain with stochastic transitions, should α decay faster or slower than in a dense reward deterministic domain?

## Architecture Onboarding

- **Component map:** Data Layer (offline dataset D) -> Selection Strategy Q(B) -> Reward Labeling Oracle -> Policy Learner (RLLF) -> Evaluator Ξ
- **Critical path:**
  1. Collect offline dataset with π_D
  2. Initialize S[B] = ∅
  3. For b = 1 to B: select s_b via strategy Q, query reward, update policy via RLLF
  4. Return final policy π[B]
- **Design tradeoffs:**
  - **Guided vs. Visitation vs. Uniform:** Guided is most robust across budgets but requires Q-value estimation; visitation is zero-cost but weak at high budgets; uniform excels in bottleneck domains
  - **Sequential-greedy vs. ES vs. Brute-force:** Sequential-greedy is near-optimal with O(B|S|) cost; ES scales independently of |S| but requires tuning k,m; brute-force is intractable for |S| > ~20
  - **UDS vs. Adaptive Q-learning:** UDS is simpler but imputes zeros; Adaptive Q-learning is more principled for unlabeled states but may underutilize data
- **Failure signatures:**
  - Policy collapses to behavior policy: Likely insufficient coverage in offline dataset (check unique states visited)
  - High variance across seeds with guided sampling: α schedule may be poorly tuned for domain
  - Sequential-greedy underperforms brute-force significantly: Violation of submodularity assumption; consider ES instead
  - Uniform outperforms guided in deterministic domain: Check if bottleneck states exist; guided may be over-exploiting
- **First 3 experiments:**
  1. **Baseline calibration:** Run uniform, visitation, and guided sampling on a small domain (e.g., Graph) at 10%, 30%, 50%, 70%, 90% feedback; plot optimality gap to verify implementation matches Table 1 patterns
  2. **Sequential-greedy validation:** On a domain with |S| ≤ 20, compare sequential-greedy vs. brute-force at 3 budget levels; confirm near-identical performance with timing comparison
  3. **Domain characteristic ablation:** Run guided sampling on TwoRooms with and without trap states; quantify how penalty state pattern affects convergence rate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do reward selection strategies perform when extended to policy-gradient methods instead of value-based updates?
- Basis in paper: [explicit] "While our study focuses on value-based policy updates, extending selection strategies to policy-gradient methods is a promising direction."
- Why unresolved: All experiments use Q-learning or IQL; policy-gradient methods have different gradient estimation properties that may change which states provide maximal marginal utility.
- What evidence would resolve it: Empirical comparison of guided and sequential-greedy selection strategies with PPO/SAC policy updates on the same MinAtar domains.

### Open Question 2
- Question: Can incorporating structured priors (e.g., temporal correlations) improve reward selection in time-series domains?
- Basis in paper: [explicit] "incorporating inductive biases, such as temporal correlations in time-series tasks, may further aid selection strategies. Exploring how to integrate such structured priors offers an exciting path for future work."
- Why unresolved: Current strategies treat states as independent samples without exploiting temporal structure inherent in sequential data.
- What evidence would resolve it: Evaluation of temporal-aware selection strategies on domains with explicit time-series structure (e.g., autonomous driving trajectories).

### Open Question 3
- Question: What theoretical conditions guarantee that sequential-greedy selection achieves optimality?
- Basis in paper: [inferred] The paper empirically observes sequential-greedy is "approximately optimal in many cases" but provides no formal characterization of when greedy marginal utility maximization fails or succeeds.
- Why unresolved: The combinatorial nature of the selection problem lacks submodularity guarantees, making theoretical analysis non-trivial.
- What evidence would resolve it: Formal analysis identifying MDP properties (deterministic transitions, reward density, etc.) under which sequential-greedy achieves bounded regret.

### Open Question 4
- Question: How robust is the "optimal state set" generalization when test datasets differ significantly from training distribution?
- Basis in paper: [inferred] The paper claims robustness to "moderate dataset distribution shifts" but only tests with small ϵ variations (0.45-0.55); performance under larger shifts remains unexplored.
- Why unresolved: Real-world applications may have train-test distribution shifts larger than those studied.
- What evidence would resolve it: Experiments measuring performance degradation as the gap between train and test data-collecting policies increases beyond the tested ranges.

## Limitations

- **Scalability concerns:** The reliance on state visitation distributions and Q-value estimates may not translate well to high-dimensional continuous state spaces beyond the tabular and image-based domains tested.
- **UDS assumption restrictions:** The assumption that unlabeled rewards can be safely imputed as zero may be too restrictive for domains where missing rewards cannot be safely assumed to be zero without introducing significant bias.
- **Domain-specific patterns:** While optimal selection patterns (anchors, recovery states, penalties) are consistently observed, their relative importance varies significantly across domain characteristics, suggesting no universal strategy exists.

## Confidence

- **High confidence:** The superiority of sequential-greedy over brute-force (O(B|S|) vs O(|S|^B) with comparable performance) is well-supported by the experimental results across all domains. The pattern of optimal selections (anchors, recovery states, penalty states) is consistently observed and theoretically grounded in Bellman backup propagation.
- **Medium confidence:** The robustness of guided sampling across different feedback levels is demonstrated, but the decay schedule for α_b is not fully specified, leaving implementation details uncertain. The comparative performance of zero-cost heuristics vs. training-based methods (ES) is strong but may be domain-dependent.
- **Low confidence:** The claim that training-based methods universally outperform heuristics at all feedback levels is weakened by the observation that zero-cost guided sampling often matches low-cost ES. The performance in high-dimensional continuous state spaces remains largely speculative.

## Next Checks

1. **Scalability test:** Implement the guided sampling strategy in a continuous control benchmark (e.g., HalfCheetah or Walker2D from OpenAI Gym) using a learned state visitation model. Compare against uniform sampling at 10%, 30%, and 50% feedback levels.

2. **Robustness to UDS assumption:** Modify the CliffWalk domain to include negative rewards for all unlabeled states (instead of zero). Evaluate whether guided sampling's performance degrades and identify which states it now selects.

3. **Hyperparameter sensitivity analysis:** Systematically vary the α_b decay schedule (linear, exponential, step-wise) in guided sampling across all prototypical domains. Quantify performance sensitivity and identify optimal decay patterns for different domain characteristics (sparse vs. dense rewards, deterministic vs. stochastic transitions).