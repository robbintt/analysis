---
ver: rpa2
title: Offline Safe Policy Optimization From Heterogeneous Feedback
arxiv_id: '2512.20173'
source_url: https://arxiv.org/abs/2512.20173
tags:
- reward
- cost
- safe
- safety
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Offline Safe POHF, a framework for learning
  safe policies from offline human feedback in continuous control tasks. The core
  innovation is PreSa, which combines preference alignment (learning from pairwise
  reward preferences) and safety alignment (learning from binary safety labels) in
  a unified constrained optimization framework using the Lagrangian method.
---

# Offline Safe Policy Optimization From Heterogeneous Feedback

## Quick Facts
- arXiv ID: 2512.20173
- Source URL: https://arxiv.org/abs/2512.20173
- Authors: Ze Gong; Pradeep Varakantham; Akshat Kumar
- Reference count: 40
- Primary result: PreSa learns safe policies with high rewards and safety adherence, outperforming state-of-the-art baselines including offline safe RL methods with ground-truth rewards/costs.

## Executive Summary
This paper introduces Offline Safe POHF, a framework for learning safe policies from offline human feedback in continuous control tasks. The core innovation is PreSa, which combines preference alignment (learning from pairwise reward preferences) and safety alignment (learning from binary safety labels) in a unified constrained optimization framework using the Lagrangian method. Unlike prior approaches, PreSa learns policies directly without explicit reward/cost models or constrained RL, addressing the challenge of error accumulation in sequential decision-making. Experiments on SafetyGym, BulletGym, and MetaDrive show that PreSa learns policies with high rewards and safety adherence, outperforming state-of-the-art baselines including offline safe RL methods with ground-truth rewards/costs.

## Method Summary
PreSa addresses Offline Safe POHF by optimizing a constrained objective that combines preference alignment and safety alignment. The framework pretrains a reference policy via behavior cloning, then optimizes a Lagrangian objective that encourages high preference alignment (CPL-style regret-based) while satisfying safety constraints (HALO-inspired sigmoid loss). The method uses a reference point mechanism to compute preferences and updates both the policy and Lagrangian multiplier iteratively. Unlike prior work that learns explicit reward/cost models or uses constrained RL, PreSa directly optimizes the policy in a unified framework, avoiding error accumulation in sequential decision-making.

## Key Results
- PreSa outperforms state-of-the-art baselines including offline safe RL methods with access to ground truth rewards and costs
- Achieves better safety performance than Binary Alignment, BC-Safe-Seg, and Safe-RLHF baselines
- Matches or exceeds performance of methods with access to ground truth rewards and costs
- Demonstrates high normalized reward while maintaining low normalized cost (threshold=1 indicates safety)

## Why This Works (Mechanism)
PreSa works by directly optimizing the policy to satisfy both preference and safety constraints simultaneously, avoiding the error accumulation that occurs when learning separate reward/cost models and then applying constrained RL. The unified Lagrangian framework allows for principled trade-offs between reward and safety, while the reference point mechanism provides stable preference computation without requiring explicit reward models.

## Foundational Learning
- **Lagrangian optimization**: Needed to handle the constrained optimization problem; quick check: verify multiplier update converges
- **Behavior cloning**: Used to pretrain reference policy; quick check: ensure pretraining loss is low
- **Preference learning**: CPL-style regret-based approach for pairwise preferences; quick check: verify preference probabilities are calibrated
- **Safety alignment**: HALO-inspired sigmoid loss for binary safety labels; quick check: monitor safety loss during training
- **Reference point mechanism**: Batch-wise running average for stable preference computation; quick check: verify reference point updates appropriately
- **Continuous control**: The task domain; quick check: ensure environment dynamics are properly simulated

## Architecture Onboarding
**Component Map**: Dataset -> Reference Policy (BC) -> PreSa Loss -> Lagrangian Optimizer -> Safe Policy

**Critical Path**: The most time-critical path is the policy optimization loop: computing the PreSa loss, updating the policy parameters, and adjusting the Lagrangian multiplier.

**Design Tradeoffs**: 
- Direct policy optimization vs. explicit reward/cost models (avoids error accumulation)
- Unified Lagrangian framework vs. separate preference/safety objectives (enables principled trade-offs)
- Batch-wise reference point updates vs. dataset-wide computation (computational efficiency vs. stability)

**Failure Signatures**:
- High cost despite low training loss: Check safety threshold δ and weight ratio η
- Reward-collapse with very low cost: Verify preference alignment temperature α and reference policy quality
- Unstable training: Monitor Lagrangian multiplier updates and reference point stability

**First Experiments**:
1. Verify reference policy pretraining on all trajectory segments
2. Test PreSa loss computation with synthetic feedback
3. Validate Lagrangian optimization convergence with synthetic data

## Open Questions the Paper Calls Out
None

## Limitations
- Several implementation details remain underspecified, including reference point computation method and update frequency
- Lagrangian multiplier update schedule and initialization are not fully specified
- Synthetic feedback generation process could be sensitive to hyperparameter choices
- Additional ablation studies isolating error accumulation effects would strengthen the claims

## Confidence
**High Confidence**: The PreSa framework's theoretical formulation and the core contribution of combining preference and safety alignment in a unified constrained optimization framework are well-established and clearly presented.

**Medium Confidence**: The empirical results showing PreSa's superiority over baselines are compelling, but the exact implementation details that drive these results need verification during reproduction.

**Medium Confidence**: The claim that PreSa addresses error accumulation in sequential decision-making is theoretically sound but would benefit from additional ablation studies isolating this effect.

## Next Checks
1. Verify the z_ref computation method and update frequency, ensuring it matches the paper's intended design (batch-wise vs. dataset-wide, update frequency)
2. Test different initialization values and update schedules for the Lagrangian multiplier ν to confirm robustness of the results
3. Conduct ablation studies varying the safety threshold δ and weight ratio η to understand their impact on the safety-performance tradeoff