---
ver: rpa2
title: Dual-Objective Reinforcement Learning with Novel Hamilton-Jacobi-Bellman Formulations
arxiv_id: '2506.16016'
source_url: https://arxiv.org/abs/2506.16016
tags:
- learning
- which
- problems
- reward
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces novel Bellman equations for dual-objective
  reinforcement learning, addressing the Reach-Always-Avoid (RAA) and Reach-Reach
  (RR) problems. The RAA problem involves maximizing the minimum of the best reward
  and worst penalty, while the RR problem involves maximizing the minimum of two distinct
  rewards.
---

# Dual-Objective Reinforcement Learning with Novel Hamilton-Jacobi-Bellman Formulations

## Quick Facts
- arXiv ID: 2506.16016
- Source URL: https://arxiv.org/abs/2506.16016
- Reference count: 40
- One-line primary result: Novel Bellman equations and DOHJ-PPO algorithm achieve superior performance on dual-objective tasks by decomposing them into combinations of simpler Hamilton-Jacobi-RL problems

## Executive Summary
This paper introduces novel Bellman equations for dual-objective reinforcement learning, addressing the Reach-Always-Avoid (RAA) and Reach-Reach (RR) problems. The RAA problem involves maximizing the minimum of the best reward and worst penalty, while the RR problem involves maximizing the minimum of two distinct rewards. The authors propose decomposing these problems into combinations of simpler Hamilton-Jacobi-RL problems (Reach, Avoid, and Reach-Avoid), enabling the derivation of tractable Bellman equations.

The authors introduce DOHJ-PPO, a Proximal Policy Optimization-based algorithm that leverages these decompositions. DOHJ-PPO learns decomposed objectives concurrently with the composed objective, using coupled resets and special reward functions derived from the theoretical results. The algorithm demonstrates superior performance compared to state-of-the-art baselines across various tasks, including Hopper, F16, SafetyGym, and HalfCheetah environments. DOHJ-PPO achieves higher success rates, improved safety, and faster task completion with minimal hyperparameter tuning, highlighting the effectiveness of the proposed approach for complex multi-objective decision-making.

## Method Summary
The method introduces DOHJ-PPO, which learns dual-objective tasks by decomposing them into combinations of simpler Hamilton-Jacobi-RL problems. The algorithm uses concurrent learning with coupled resets—initializing decomposed trajectory rollouts from composed trajectory states—to ensure relevant state coverage. Special Bellman operators (BγR, BγA, BγRA) update decomposed critics, while composed critics use modified rewards derived from decomposed values. The approach handles both deterministic and stochastic dynamics through SRABE approximations when needed.

## Key Results
- DOHJ-PPO achieves higher success rates than baselines across Hopper, F16, SafetyGym, and HalfCheetah environments
- The algorithm demonstrates improved safety and faster task completion while requiring minimal hyperparameter tuning
- Coupled resets are critical: omitting them causes performance to collapse to baseline levels (CPPO)
- DOHJ-PPO maintains robustness under stochastic dynamics, with performance degrading gracefully as noise increases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The RAA and RR value functions decompose into combinations of simpler Reach, Avoid, and Reach-Avoid value functions.
- Mechanism: The RAA problem (maximize min{max reward, min penalty}) equals a modified Reach-Avoid problem where the reward is capped by the optimal Avoid value: r_RAA(s) := min{r(s), V*_A(s)}. Similarly, RR decomposes into three Reach problems with composed reward r_RR(s) := max{min{r₁(s), V*_{R2}(s)}, min{r₂(s), V*_{R1}(s)}}.
- Core assumption: Deterministic dynamics; the decomposition theorems assume V*_A, V*_{R1}, V*_{R2} are accurately computed first.
- Evidence anchors:
  - [abstract] "we prove that the RAA and RR problems may be rewritten as compositions of previously studied HJ-RL problems"
  - [Theorems 1 & 2, Section 6] Full decomposition proofs
  - [corpus] Weak direct evidence—neighbor papers address multi-objective RL but not this specific HJ-based decomposition
- Break condition: If decomposed values are inaccurate (e.g., V*_A misestimated), the composed reward becomes corrupted, leading to suboptimal policies.

### Mechanism 2
- Claim: State augmentation with historical trackers (y_t, z_t) enables optimal policies without requiring full trajectory memory.
- Mechanism: Augment states with y_t = max_τ≤t r(s_τ) and z_t = min_τ≤t q(s_τ) for RAA (max for both in RR). Theorem 3 proves this is sufficient—no additional history improves performance under the optimal augmented policy.
- Core assumption: The augmentation captures all decision-relevant history; dynamics remain Markovian in the augmented space.
- Evidence anchors:
  - [Section 5.1-5.2] Formal augmentation definitions
  - [Figure 3] Demonstrates why non-augmented policies fail in ambiguous states
  - [corpus] No direct corpus evidence on this specific augmentation scheme
- Break condition: If reward/penalty functions are non-Markovian themselves, augmentation may be insufficient.

### Mechanism 3
- Claim: Coupled resets align decomposed learning with composed task requirements, preventing reward-irrelevant convergence.
- Mechanism: Decomposed trajectory initializations are sampled from composed rollouts rather than independently. This ensures the Avoid decomposition, for example, learns values relevant to the RAA task region rather than converging to safe-but-useless strategies.
- Core assumption: Composed trajectory states provide meaningful coverage of task-relevant regions early in training.
- Evidence anchors:
  - [Section 7.2] "Trajectories for training the decomposed actor and critic(s) are initialized with states sampled from the composed trajectories"
  - [Section E.2] "omitting coupled resets causes DOHJ-PPO to perform no better than standard baselines such as CPPO"
  - [corpus] No direct corpus evidence on coupled reset strategies
- Break condition: If composed policy explores poorly initially, decomposed critics may receive biased samples, creating feedback loops.

## Foundational Learning

- Concept: **Hamilton-Jacobi Reachability**
  - Why needed here: The paper builds on HJ value functions that propagate extremal (min/max) values rather than discounted sums. Understanding that V(s) > 0 means task success is achievable is essential.
  - Quick check question: Given a reach-avoid value V_RA(s) = max_t min{r(s_t), min_τ≤t q(s_τ)}, what does V_RA(s) = 0.5 indicate?

- Concept: **Bellman Equation Non-Contraction**
  - Why needed here: The special HJ Bellman equations (RABE, SBE) don't naturally contract. The paper uses discounting (γ → 1⁻) to induce contraction.
  - Quick check question: Why does the discounted RABE V^γ_RA approximate V*_RA only in the limit γ → 1?

- Concept: **State Augmentation for Non-Markovian Rewards**
  - Why needed here: Dual-objective problems create history-dependent optimal actions. The augmentation converts this to a Markovian augmented MDP.
  - Quick check question: In Figure 3 (right), why can't a Markov policy decide whether to cross the cone hazard?

## Architecture Onboarding

- Component map:
  Composed Actor (θ) -> Rollout -> Sample states for coupled resets -> Decomposed Actors (θ_i)
         |                                           |
         v                                           v
  Composed Critic (ω)                    Decomposed Critics (ω_i)
         |                                           |
         v                                           v
  Uses special reward r_RAA/r_RR computed from decomposed critics -> Provide V*_A, V*_{R1}, V*_{R2}

- Critical path:
  1. Initialize all networks
  2. Rollout composed actor to collect states
  3. Sample states for coupled reset of decomposed rollouts
  4. Update decomposed critics using B^R_γ, B^A_γ, or B^RA_γ
  5. Compute special reward r_RAA/r_RR using current decomposed critic values
  6. Update composed critic with special reward
  7. Update all actors using GAE (Reach-Avoid GAE for composed, standard for decomposed)

- Design tradeoffs:
  - Concurrent vs. sequential learning: Paper chooses concurrent for simplicity and coupling benefits, but sequential (pre-training decomposed values) might improve stability
  - Stochastic SRABE vs. deterministic RABE: SRABE enables PPO-style exploration but exchanges extrema and expectation operators (approximation)
  - Discount annealing (0.995 → 0.999): Balances contraction stability with approximation accuracy

- Failure signatures:
  - Decomposed critics diverge: Check learning rate, ensure Bellman updates match task type (R/A/RA)
  - Composed policy ignores one objective: Verify coupled resets are active; check r_RAA/r_RR computation
  - Slow convergence: Increase γ annealing speed or check if coupled resets provide sufficient state coverage
  - Safety violations after reaching goal (RAA): Avoid critic may be misaligned—verify coupled reset sampling

- First 3 experiments:
  1. **Grid-world validation**: Implement DQN with RAA/RR Bellman updates on 2D drift environment (Section 8.1). Verify trajectories differ from simple RA/R—RAA should maintain safety post-reach, RR should visit both targets.
  2. **Ablation on coupled resets**: Run DOHJ-PPO on Hopper RAA with and without coupled resets. Expect significant success rate drop without coupling (per Section E.2).
  3. **Stochastic dynamics stress test**: Add Gaussian noise to HalfCheetah velocities. Compare DOHJ-PPO vs. top baselines across noise levels (0.5, 1.0, 2.0 std) to validate SRABE robustness claims (Figure 5).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can theoretical guarantees be derived for the Stochastic Reach-Avoid Bellman Equation (SRABE) under stochastic dynamics, given that expectation and extrema operations do not commute?
- Basis in paper: [explicit] "As expectation and extrema operations do not commute, more work is needed to provide guarantees under stochastic dynamics."
- Why unresolved: The SRABE approximation interchanges these operators without formal justification; empirical validation in Section 8.3 shows robustness but lacks theoretical backing.
- What evidence would resolve it: A formal proof establishing bounds on the approximation error between the SRABE solution and the true optimal value under stochastic transitions, or conditions under which the approximation is exact.

### Open Question 2
- Question: Can the decomposition approach be generalized to derive Bellman equations for arbitrary compositions of logical operations beyond the specific RAA and RR structures?
- Basis in paper: [explicit] "doing so would require deriving generalized decomposition principles for nontrivial compositions of logical operations."
- Why unresolved: The paper only proves decomposition for min/max compositions specific to RAA and RR; general temporal logic specifications involve more complex predicate algebra that does not directly translate (Appendix L demonstrates this failure).
- What evidence would resolve it: A general theorem characterizing which logical compositions admit Bellman decompositions, with constructive rules for deriving the corresponding reward modifications.

### Open Question 3
- Question: What mechanisms can guarantee convergence of DOHJ-PPO when learning the decomposed and composed objectives concurrently?
- Basis in paper: [explicit] "a practical algorithm for solving the decomposed graph of values might benefit from... mechanisms to guarantee convergence."
- Why unresolved: The coupled resets and concurrent bootstrapping introduce interdependencies between networks that may violate standard PPO convergence assumptions.
- What evidence would resolve it: Formal convergence analysis under specific learning rate schedules or coupling constraints, or empirical demonstration of monotonic improvement bounds.

## Limitations
- The theoretical decomposition relies on accurate pre-computed decomposed values, but the paper trains these concurrently rather than sequentially
- The SRABE operator exchanges extrema and expectation, creating potential approximation errors under stochastic dynamics that aren't quantified
- The absorbing state modification f' is assumed but not empirically validated for all task types

## Confidence
- High confidence: Reach-Avoid HJ theory foundation (well-established in control literature)
- Medium confidence: Decomposition theorems and Bellman equations (theoretical proofs provided but practical approximations unverified)
- Medium confidence: DOHJ-PPO performance improvements (extensive experiments but limited ablation on key design choices)
- Low confidence: SRABE approximation quality under high stochasticity (theoretical justification provided but empirical validation limited)

## Next Checks
1. Implement a controlled ablation study on coupled resets across all environments to quantify their contribution beyond decomposed value accuracy
2. Measure approximation error between SRABE and true RABE under varying noise levels to characterize the tradeoff between tractability and accuracy
3. Test sequential pre-training of decomposed values followed by composed learning to assess whether the concurrent approach sacrifices accuracy for simplicity