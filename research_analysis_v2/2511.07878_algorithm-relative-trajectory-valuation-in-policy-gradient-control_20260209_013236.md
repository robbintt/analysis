---
ver: rpa2
title: Algorithm-Relative Trajectory Valuation in Policy Gradient Control
arxiv_id: '2511.07878'
source_url: https://arxiv.org/abs/2511.07878
tags:
- gradient
- variance
- shapley
- value
- trajectory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work investigates trajectory valuation in policy-gradient\
  \ control, focusing on how trajectory value depends on the learning algorithm. Using\
  \ Trajectory Shapley on uncertain LQR, it discovers a robust negative correlation\
  \ between Persistence of Excitation (PE) and trajectory value under vanilla REINFORCE\
  \ (Spearman r \u2248 -0.38), contradicting the intuition that more informative data\
  \ should be more valuable."
---

# Algorithm-Relative Trajectory Valuation in Policy Gradient Control

## Quick Facts
- **arXiv ID**: 2511.07878
- **Source URL**: https://arxiv.org/abs/2511.07878
- **Reference count**: 26
- **Primary result**: Trajectory value is algorithm-relative—correlation between Persistence of Excitation (PE) and Shapley value flips from negative to positive under stabilization.

## Executive Summary
This work investigates trajectory valuation in policy-gradient control, focusing on how trajectory value depends on the learning algorithm. Using Trajectory Shapley on uncertain LQR, it discovers a robust negative correlation between Persistence of Excitation (PE) and trajectory value under vanilla REINFORCE (Spearman r ≈ -0.38), contradicting the intuition that more informative data should be more valuable. The paper proves a variance-mediated mechanism: high PE yields low gradient variance, and near saddle points, higher variance increases escape probability, raising marginal contribution. When stabilized with state whitening or Fisher preconditioning, the variance channel is neutralized and the correlation flips positive (r ≈ +0.29), demonstrating that trajectory value is algorithm-relative. Experiments validate the mechanism and show LOO outperforms Shapley for pruning, while Shapley excels at identifying toxic subsets.

## Method Summary
The paper studies trajectory valuation in uncertain LQR control using REINFORCE policy gradient. It generates 50 trajectories from a 2D system, computes Persistence of Excitation (PE) as λ_min of the information matrix, and estimates Shapley values via permutation Monte Carlo. The key innovation is proving that PE controls gradient variance under fixed energy, and that variance increases escape probability near saddles. The method validates this mechanism experimentally and demonstrates that stabilization (whitening/Fisher) neutralizes the variance channel, flipping PE-value correlation from negative to positive.

## Key Results
- Vanilla REINFORCE: PE negatively correlates with Shapley value (r ≈ -0.38)
- Stabilized (whitening/Fisher): PE positively correlates with Shapley value (r ≈ +0.29)
- LOO pruning outperforms Shapley pruning for grand-coalition optimization
- Shapley excels at identifying toxic trajectory subsets (marked † in Table 2)

## Why This Works (Mechanism)

### Mechanism 1: PE Controls Gradient Variance
For fixed trajectory energy, higher PE yields lower gradient variance. PE measures λ_min(I_τ), the minimum eigenvalue of the information matrix. Under fixed energy E(τ)=E₀, higher λ_min forces lower λ_max via trace-eigenvalue constraints. Since gradient variance scales with λ_max of state covariance, high PE constrains variance. Break condition: If energy varies freely, high-PE trajectories may also have high energy, confounding the relationship.

### Mechanism 2: Variance Increases Escape Probability Near Saddles
Higher gradient variance increases marginal Shapley contribution near saddle points. In small-step stochastic approximation near saddles, gradient noise enables escape from poor basins. Diffusion approximation shows escape probability is monotonically increasing in variance σ². Break condition: If landscape has no saddles or is convex, variance becomes pure noise without exploration benefit.

### Mechanism 3: Stabilization Neutralizes Variance Channel
State whitening or Fisher preconditioning flips PE-value correlation from negative to positive. Whitening enforces E[x'x'^T] = I, making gradient variance trajectory-independent. Natural gradient progress is governed by Fisher-weighted gradient norm, which scales with information content E[I_τ]. Both remove variance as a discriminative factor. Break condition: If whitening is poorly estimated or Fisher is ill-conditioned, stabilization may fail.

## Foundational Learning

- **Shapley Value**: Core valuation metric; measures expected marginal contribution across all coalition orders. Why needed: Main valuation metric for trajectory importance. Quick check: Given 3 trajectories with Shapley values [0.4, 0.3, 0.3], what happens to total value if we remove the first? (Answer: Expected loss of 0.4 in coalition value; but note: Shapley averages over all coalition sizes, not just grand coalition.)

- **Persistence of Excitation (PE)**: Quantifies information content via λ_min of information matrix; central to the counterintuitive finding. Why needed: Key feature whose correlation with value reveals algorithm dependence. Quick check: Two trajectories with equal energy but PE₁ = 2, PE₂ = 5. Which has more balanced excitation? (Answer: PE₂; higher λ_min means all directions are excited, not just one dominant mode.)

- **REINFORCE Gradient Estimator**: Source of variance; the specific structure (G_k ε_k x_k^T) creates the PE-variance relationship. Why needed: Determines how trajectory statistics affect learning dynamics. Quick check: Why does REINFORCE variance depend on state covariance? (Answer: ε_k ~ N(0, σ²I) multiplies x_k; covariance scales with E[x_k x_k^T] ⊗ I_m.)

## Architecture Onboarding

- **Component map**: Trajectory Generation (LQR rollout) → Feature Extraction (PE, Energy, GradVar) → Valuation (Shapley via permutation MC, LOO via removal) → Correlation Analysis (PE, GradVar, Shapley) → Stabilization Layer (optional: Whitening/Fisher)

- **Critical path**: 
  1. Validate PE computation matches λ_min of stacked [x; u] outer products
  2. Confirm gradient variance proxy correctly implements Eq. (6): Σ_τ ∝ Σ_k G²_k (x_k x_k^T) ⊗ I_m
  3. Verify Shapley Monte Carlo uses both one-step proxy (80%) and full training (20%) as described

- **Design tradeoffs**: 
  - Shapley vs LOO: Shapley for subset selection/toxic detection; LOO for grand-coalition pruning
  - Vanilla vs Stabilized: Vanilla exploits variance for exploration; stabilized aligns value with information content
  - One-step proxy vs Full training: Proxy is fast but approximate; 80/20 split balances cost and accuracy

- **Failure signatures**: 
  - Catastrophic degradation from Shapley-based pruning (Table 2: -222k vs baseline -16k) — Shapley averages over all coalitions, not optimized for grand-coalition margin
  - Training instability from Shapley Bottom 30% (marked † in Table 2) — successful toxic detection
  - Weak PE-GradVar anticorrelation (|r| < 0.1) suggests energy conditioning failed or PE computation error

- **First 3 experiments**: 
  1. Generate 50 trajectories with controlled PE (vary excitation direction while fixing energy). Compute PE and gradient variance proxy. Target: r(PE, GradVar) < -0.3.
  2. Near initialization (saddle region for LQR), compute Shapley values. Correlate with gradient variance. Target: r(GradVar, Shapley) > +0.3.
  3. Implement state whitening (compute Σ_x from dataset, apply x' = Σ_x^{-1/2}(x - μ)). Re-run valuation. Target: r(PE, Shapley) > +0.2.

## Open Questions the Paper Calls Out

- **Can algorithm-aware valuation frameworks predict trajectory value a priori for a given optimizer, without expensive Shapley computation?**
  - Basis: "Future research should develop algorithm-aware valuation frameworks that predict a priori which trajectories will be valuable for a given optimizer."
  - Why unresolved: Current Shapley computation requires expensive Monte Carlo sampling over coalitions, making real-time valuation impractical.
  - What evidence would resolve it: A surrogate model that accurately predicts Shapley values from trajectory features (PE, variance) and optimizer identity, validated across multiple control tasks.

- **Does the variance-mediated mechanism persist in actor-critic methods where the critic stabilizes gradient estimates?**
  - Basis: "Extensions to actor-critic methods, where the critic stabilizes learning, would be valuable."
  - Why unresolved: The mechanism was proven and validated only for vanilla REINFORCE; critics reduce variance through value function approximation, potentially neutralizing the variance channel by default.
  - What evidence would resolve it: Measuring PE-Shapley correlations under actor-critic algorithms (A2C, PPO) to test whether correlations remain negative or flip positive without explicit stabilization.

- **Does algorithmic relativity hold for off-policy methods (Q-learning, SAC) and model-based approaches (Dyna, MuZero)?**
  - Basis: "It would be valuable to investigate algorithmic relativity in off-policy methods (Q-learning, SAC) and model-based approaches (Dyna, MuZero)."
  - Why unresolved: Off-policy learning fundamentally changes data usage through replay buffers, while model-based methods learn dynamics models—both may exhibit different data-algorithm interactions.
  - What evidence would resolve it: Replicating the PE-value correlation analysis in off-policy and model-based RL settings, testing whether stabilization flips occur similarly.

## Limitations
- The assumption of two-basin local geometry near saddles may not hold in general control problems
- Fixed-energy conditioning isolates directional richness but may not reflect practical trajectory diversity
- Whitening preconditioning assumes accurate estimation of state statistics from finite data

## Confidence
- **High confidence**: Mechanism 3 (stabilization flips correlation) - supported by experimental correlation flips in Table 1 and Figure 2
- **Medium confidence**: Mechanism 1 (PE controls variance) - theoretically proven but corpus lacks RL-specific validation; requires careful energy conditioning
- **Medium confidence**: Mechanism 2 (variance aids saddle escape) - plausible under small-step approximation but untested outside LQR; no corpus evidence

## Next Checks
1. **Energy conditioning validation**: Generate trajectories within narrow energy bands (E₀ ± 5%) and verify PE-GradVar anticorrelation persists (|r| > 0.3), confirming directional richness independence from magnitude
2. **Landscape validation**: Map the LQR cost landscape near initialization to confirm saddle structure and two-basin geometry, validating the variance-escape mechanism's assumptions
3. **Whitening robustness**: Test whitening with varying sample sizes (N = 20, 50, 100) to ensure correlation flip from negative to positive persists as state statistics converge