---
ver: rpa2
title: Multi-Modal Continual Learning via Cross-Modality Adapters and Representation
  Alignment with Knowledge Preservation
arxiv_id: '2511.06723'
source_url: https://arxiv.org/abs/2511.06723
tags:
- learning
- loss
- each
- continual
- multi-modal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multi-modal continual learning framework
  that integrates cross-modality adapters with representation alignment to address
  catastrophic forgetting across multiple input modalities. The proposed approach
  uses a mixture-of-experts structure within cross-modality adapters to capture interactions
  between modalities while freezing weights of frequently activated experts to preserve
  prior knowledge.
---

# Multi-Modal Continual Learning via Cross-Modality Adapters and Representation Alignment with Knowledge Preservation

## Quick Facts
- arXiv ID: 2511.06723
- Source URL: https://arxiv.org/abs/2511.06723
- Reference count: 40
- Primary result: 91.91% accuracy on AVE dataset, 95.54% on UESTC-MMEA dataset, and 70.90% on SAMSEMO dataset with reduced forgetting

## Executive Summary
This paper introduces a multi-modal continual learning framework that addresses catastrophic forgetting across multiple input modalities through cross-modality adapters and representation alignment. The approach uses a mixture-of-experts structure within cross-modality adapters to capture interactions between modalities while selectively freezing weights of frequently activated experts to preserve prior knowledge. A novel representation alignment loss aligns each modality-specific representation with a joint representation, preserving modality-specific information while ensuring cross-modal consistency. The framework also includes regularization to maintain relationships between previously learned representations.

## Method Summary
The method employs MMEncoder with modality-specific pre-trained models (PTMs) whose weights are frozen. Each PTM is divided into blocks with cross-modality adapters inserted after each block. These adapters use a mixture-of-experts architecture where a gating network selects top-2 experts from 10 total. After each task, experts activated above a 10% threshold are frozen. The framework uses representation alignment loss to pull modality-specific representations toward a joint representation, and a preservation loss that maintains pairwise similarity relationships between memory samples using KL divergence. The model is trained sequentially with classification, distillation, alignment, and preservation losses.

## Key Results
- Achieves 91.91% accuracy on AVE dataset compared to 90.46% with conventional contrastive loss
- Achieves 95.54% accuracy on UESTC-MMEA dataset, outperforming state-of-the-art baselines
- Achieves 70.90% accuracy on SAMSEMO dataset with notably reduced forgetting across all datasets
- Consistently outperforms traditional continual learning methods and recent pre-trained model-based approaches across class-incremental and domain-incremental learning scenarios

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modality Adapters with Expert Freezing
The mixture-of-experts adapters capture cross-modal interactions while selective freezing preserves prior knowledge. Each adapter contains 10 experts that learn modality interactions via attention maps, with a gating network selecting top-2 experts per input. Experts activated above 10% threshold are frozen after each task, preserving learned cross-modal patterns while allowing new learning in remaining experts.

### Mechanism 2: Indirect Representation Alignment via Joint Space
Instead of directly aligning modalities using contrastive loss, each modality representation is pulled toward a joint representation fused from all modalities. This indirect approach preserves modality-specific information while achieving cross-modal consistency, allowing each modality to form distinct clusters per class while maintaining proximity through the shared anchor.

### Mechanism 3: Pairwise Relation Preservation via KL Regularization
The preservation loss computes normalized pairwise similarity between joint representations using previous and current model parameters, then minimizes KL divergence. This maintains relative similarity relationships between memory sample representations across time steps, preserving learned knowledge structures. The weight √(t-1) increases preservation emphasis as more tasks accumulate.

## Foundational Learning

- **Concept: Mixture-of-Experts with Gating**
  - Why needed here: Core mechanism for scalable cross-modal learning. Without understanding expert selection and load balancing, you cannot diagnose why certain experts freeze or fail to activate.
  - Quick check question: Given 10 experts with top-2 activation, if expert 3 is frozen after task 1 but receives highest gate score for task 2 input, what happens to the output?

- **Concept: Contrastive Learning Limitations in Multi-Modal Settings**
  - Why needed here: Motivates the indirect alignment approach. Standard InfoNCE/CLIP-style losses assume modalities should occupy identical embedding regions, which conflicts with modality-specific feature preservation.
  - Quick check question: Why would aligning audio and video embeddings directly cause confusion between "guitar playing" and "violin playing" classes?

- **Concept: Catastrophic Forgetting in Sequential Task Learning**
  - Why needed here: Frames the entire problem. The paper's design choices (freezing, regularization, replay) are all responses to specific forgetting pathways.
  - Quick check question: If a model achieves 95% on task 1, then 94% on task 2 after training, but drops to 70% on task 1 test data, what type of forgetting occurred and which mechanism in this paper directly addresses it?

## Architecture Onboarding

- **Component map:** Input: K modalities → Tokenization via modality-specific PTMs (FROZEN) → MMEncoder: B blocks with PTM layers (FROZEN) + Cross-modality adapter with E experts + gating (TRAINABLE, partial freeze) → Projection layers: Per-modality → z_k (TRAINABLE) → Fusion layers: Concat(z_1...z_K) → z_joint (TRAINABLE) → Classifier: z_joint → predictions (TRAINABLE)

- **Critical path:** Cross-modality adapter output (Equation 6) directly modulates PTM features. If attention maps m^e_k are poorly formed (e.g., all zeros or uniform), adapted features collapse to raw PTM outputs, losing cross-modal benefit. Monitor g^e_k magnitudes during training.

- **Design tradeoffs:**
  - More experts (E): Higher capacity but slower inference, potential under-utilization
  - Lower freeze threshold: More preservation but reduced plasticity for new tasks
  - Larger memory buffer: Better L_preserve estimates but higher storage cost
  - Paper uses E=10, top-2 activation, 10% freeze threshold, 200-sample memory

- **Failure signatures:**
  - All experts activating equally → gating not learning discriminative selection
  - Forgetting spikes at specific task boundaries → check if relevant experts were incorrectly frozen
  - Joint representation collapsing to single modality → check projection layer gradients per modality
  - L_preserve diverging → memory samples may be unrepresentative of task distribution

- **First 3 experiments:**
  1. **Baseline sanity check:** Run without cross-modality adapters (only modality-specific PTM features concatenated) on AVE dataset. Expected: ~87-88% accuracy per Table 2. Validates PTM quality.
  2. **Expert utilization audit:** Log activation frequency per expert across all tasks. Verify experts specialize (non-uniform activation) and frozen experts remain relevant on later tasks.
  3. **Alignment loss ablation:** Train with L_align vs. standard multi-modal contrastive loss. Visualize t-SNE of joint representations for semantically similar classes (e.g., string instruments at t=3, t=5 as in Figure 7). Expected: L_align yields more compact, separated clusters.

## Open Questions the Paper Calls Out

### Open Question 1
Can the proposed framework maintain its performance in mitigating catastrophic forgetting if the memory buffer is entirely removed, converting the method into a strictly rehearsal-free approach? The current formulation of the preservation loss (L_preserve) and the distillation loss (L_distil) relies specifically on memory indices (I_M) and retained data samples (M_t) to regularize the relationships between previously learned representations. Future work will explore removing the use of memory buffers to improve scalability and broaden applicability.

### Open Question 2
How does the cross-modality adapter's performance and parameter efficiency scale when the number of sequential tasks increases significantly (e.g., from the tested ~8 tasks to 50+ tasks)? The experimental scope is limited to a small number of time steps (T=7 for AVE, T=8 for UESTC-MMEA, T=5 for SAMSEMO). The MoE freezing strategy implies a long-term upper bound on plasticity, and it is unclear if the model will eventually suffer from "expert saturation" or if cumulative frozen parameters will become computationally prohibitive.

### Open Question 3
To what extent does the effectiveness of the representation alignment loss (L_align) depend on the initialization quality of the modality-specific pre-trained models, particularly when modalities have disparate semantic densities? The method relies on frozen PTMs and adapts them via block-wise alignment, assuming the PTMs provide meaningful feature spaces to start with. If input modalities are fundamentally misaligned or if one modality's PTM is significantly weaker, the joint representation might fail to synthesize useful information, leading to negative transfer that the alignment loss cannot correct.

## Limitations
- The framework relies on memory buffers for knowledge preservation, limiting scalability in lifelong learning scenarios
- Performance may degrade with significantly more tasks due to potential expert saturation in the mixture-of-experts architecture
- Effectiveness depends heavily on the quality of pre-trained models, particularly when modalities have disparate semantic densities

## Confidence
- **High confidence**: Cross-modality adapter design with mixture-of-experts and expert freezing mechanism (supported by ablation studies in Table 5)
- **Medium confidence**: Representation alignment loss superiority (empirical results support the claim but mechanism needs theoretical analysis)
- **Medium confidence**: Knowledge preservation via pairwise similarity regularization (ablation confirms effectiveness but assumption needs broader validation)

## Next Checks
1. **Theoretical analysis of indirect alignment**: Conduct controlled experiments comparing joint-space alignment versus direct modality-to-modality contrastive learning across varying modality dominance scenarios
2. **Expert specialization audit**: Systematically analyze expert activation patterns across all tasks to verify that frozen experts remain relevant for future tasks and that new experts learn complementary cross-modal patterns
3. **Memory buffer sensitivity**: Evaluate performance across different memory buffer sizes (50, 100, 200, 500 samples) to determine the minimum buffer size required for effective knowledge preservation via L_preserve