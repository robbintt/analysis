---
ver: rpa2
title: Detecting Hallucinations in Authentic LLM-Human Interactions
arxiv_id: '2510.10539'
source_url: https://arxiv.org/abs/2510.10539
tags:
- hallucination
- cluster
- detection
- llms
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AuthenHallu, the first hallucination detection
  benchmark built entirely from authentic LLM-human interactions rather than artificially
  induced or simulated dialogues. The benchmark is constructed by filtering representative
  dialogues from a large-scale conversation dataset, then manually annotating query-response
  pairs for hallucination occurrence and category (input-conflicting, context-conflicting,
  fact-conflicting).
---

# Detecting Hallucinations in Authentic LLM-Human Interactions

## Quick Facts
- arXiv ID: 2510.10539
- Source URL: https://arxiv.org/abs/2510.10539
- Reference count: 0
- Primary result: First hallucination detection benchmark from authentic LLM-human interactions; 31.4% hallucination rate; best detection F1=63.9%

## Executive Summary
This paper introduces AuthenHallu, the first hallucination detection benchmark built entirely from authentic LLM-human interactions rather than artificially induced dialogues. The benchmark is constructed by filtering representative dialogues from a large-scale conversation dataset, then manually annotating query-response pairs for hallucination occurrence and category. Statistical analysis shows that 31.4% of query-response pairs contain hallucinations, with fact-conflicting being the most prevalent type. The paper also evaluates vanilla LLMs for hallucination detection and categorization, finding that even advanced models struggle with these tasks in real-world scenarios.

## Method Summary
The authors constructed AuthenHallu by filtering LMSYS-Chat-1M to obtain English, non-redacted, two-turn dialogues with safe content and appropriate query lengths. They encoded queries using all-mpnet-base-v2, clustered via K-means (k=45 for first-pair queries, k=20 for second-pair), and sampled dialogues proportionally to cluster size. Three trained annotators labeled each query-response pair for hallucination occurrence (binary) and category (input-conflicting, context-conflicting, fact-conflicting). Zero-shot evaluation was performed using six LLMs with provided prompt templates, testing single-model, ensemble, and in-context detection settings.

## Key Results
- 31.4% of query-response pairs contain hallucinations in authentic interactions
- Fact-conflicting hallucinations are most prevalent (157 instances), followed by input-conflicting (87), then context-conflicting (9)
- Best detection F1-score: 63.91% (Qwen-3-32B)
- Best categorization F1-score: 69.92% (Gemma-3-27B)
- Ensemble methods do not outperform single best models, suggesting correlated failure modes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Clustering-based proportional sampling preserves the natural distribution of user intents better than synthetic query generation.
- **Mechanism:** User queries from LMSYS-Chat-1M are encoded with all-mpnet-base-v2, clustered via K-means (k=45 for first-pair queries, k=20 for second-pair), then dialogues are sampled proportionally to cluster size—ensuring rare but real usage patterns aren't discarded.
- **Core assumption:** The silhouette score and inertia metrics meaningfully capture semantic coherence of user intents, and cluster centers represent meaningful topic boundaries.
- **Evidence anchors:**
  - [Section 3.1.1] "We encode all user queries using the all-mpnet-base-v2 sentence transformer... apply K-means to all first-pair queries to obtain 45 clusters... proportional sampling across clusters according to their sizes."
  - [corpus] Weak direct corpus evidence—neighbor papers focus on detection methods rather than benchmark construction; no comparative validation of clustering vs. random sampling is cited.
- **Break condition:** If user intent diversity is poorly captured by sentence embeddings, or if k selection is sensitive to dataset-specific quirks, representativeness degrades.

### Mechanism 2
- **Claim:** Human annotation with explicit three-category taxonomy captures hallucination types that LLMs systematically miss.
- **Mechanism:** Annotators label each query-response pair for occurrence (binary) and category (input-conflicting, context-conflicting, fact-conflicting) using Zhang et al.'s taxonomy. Fleiss's Kappa of 0.591 indicates moderate agreement, suggesting the task is well-defined but inherently subjective.
- **Core assumption:** Trained annotators with CS backgrounds can reliably distinguish fact-conflicting from input-conflicting hallucinations in open-domain conversations without external knowledge bases.
- **Evidence anchors:**
  - [Section 3.1.2] "Fleiss's Kappa... yields a score of 0.591, indicating a moderate level of agreement."
  - [Section 5.2.1] "LLMs perform relatively better on fact-conflicting hallucinations... less proficient at recognizing faithfulness hallucinations [input/context-conflicting]."
  - [corpus] Neighbor papers (HalluGuard) distinguish data-driven vs. reasoning-driven hallucinations—suggesting category distinctions matter for detection strategy.
- **Break condition:** If annotation subjectivity varies significantly across topics (e.g., math vs. creative writing), ground-truth labels may be noisy for certain domains.

### Mechanism 3
- **Claim:** In-context detection can improve performance for some models but introduces noise for others, suggesting context utility is model-dependent.
- **Mechanism:** The second query-response pair is evaluated with the first pair as context. Qwen-3-32B's F1 improved from 63.91% to 64.27% with context, but Mistral-7B dropped from 38.46% to 36.68%.
- **Core assumption:** The first dialogue turn provides semantically relevant grounding rather than distraction, and models can selectively attend to useful contextual signals.
- **Evidence anchors:**
  - [Section 5.1.3] "In-context detection achieves better performance than single-model detection on five individual metrics... However, for most other models, performance... slightly decreases."
  - [corpus] No direct corpus validation; neighbor papers don't test in-context detection strategies for hallucination.
- **Break condition:** If context length grows or dialogue turns become tangentially related, noise accumulation outweighs grounding benefits.

## Foundational Learning

- **Concept: Hallucination taxonomy (input-conflicting vs. context-conflicting vs. fact-conflicting)**
  - Why needed here: Detection methods must target different failure modes—input-conflicting requires instruction-following verification, fact-conflicting requires knowledge grounding.
  - Quick check question: Given a user asking about a fictional character's biography, would inconsistencies be input-conflicting or fact-conflicting?

- **Concept: Authentic vs. artificially-induced hallucination distributions**
  - Why needed here: Benchmarks like HaluEval that prompt models to "write a hallucinated answer" may produce artifacts (e.g., over-obvious errors) that don't reflect real failure modes.
  - Quick check question: Why might a model's factual error rate differ when answering "What is the capital of France?" vs. "Write a plausible but incorrect fact about France"?

- **Concept: Ensemble correlation in detection errors**
  - Why needed here: The paper finds ensembles don't outperform the best single model, suggesting correlated failure modes—critical for system design.
  - Quick check question: If three models all fail to detect hallucinations in math reasoning, what does this imply about their training data or architecture?

## Architecture Onboarding

- **Component map:** LMSYS-Chat-1M -> filtering (6 criteria) -> clustering (k=45/20) -> proportional sampling -> 400 dialogues (800 Q-R pairs) -> annotation (3 annotators) -> occurrence + category labels -> 6 LLMs evaluation (zero-shot)

- **Critical path:**
  1. Dialogue filtration removes non-English, redacted, unsafe, and non-2-turn conversations
  2. Clustering quality (silhouette ~0.29) determines topic coverage
  3. Annotation agreement (Kappa=0.591) sets ground-trust reliability ceiling
  4. Zero-shot prompting design directly affects measured LLM capabilities

- **Design tradeoffs:**
  - Two-turn dialogues only -> simpler annotation but misses multi-turn context effects
  - English-only -> reduces annotation complexity but limits cross-lingual generalization (explicitly noted as a limitation)
  - Manual annotation only -> high quality but scalability constraints (800 pairs max)

- **Failure signatures:**
  - Context-conflicting hallucinations: F1 <10% across all models (Table 4)—models cannot reliably detect self-contradictions
  - Math & Number Problems: 60% hallucination rate but detection remains weak—reasoning errors are hard to spot
  - Ensemble methods don't beat single best model -> models make correlated errors

- **First 3 experiments:**
  1. **Replicate detection baseline:** Run the provided prompt templates (Figure 5-7) on Qwen-3-32B for single-model detection; target F1 ≈64%. Deviation suggests implementation issues or prompt sensitivity.
  2. **Category-specific error analysis:** Isolate context-conflicting cases (9 instances in benchmark) and test whether providing explicit self-consistency checks improves detection—hypothesis: models lack internal consistency verification.
  3. **Topic-stratified evaluation:** Evaluate detection separately on high-hallucination topics (Math: 60%, Dates: 60%) vs. low-hallucination topics (Casual Chat: 14%) to determine if failure is domain-specific or general.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can hallucination detection benchmarks be extended to multilingual authentic LLM-human interactions?
- **Basis in paper:** [explicit] Limitations section states: "our dataset currently includes only English LLM–human conversations... As future work, we plan to extend our benchmark to multiple languages and construct a multilingual hallucination detection dataset based on authentic LLM–human interactions."
- **Why unresolved:** Multilingual annotation requires language-specific expertise; hallucination types may manifest differently across languages.
- **What evidence would resolve it:** A multilingual benchmark with comparable annotation protocols across languages, showing whether detection methods transfer or require language-specific adaptation.

### Open Question 2
- **Question:** Can retrieval-augmented or consistency-based detection methods outperform vanilla LLMs on authentic interaction data?
- **Basis in paper:** [inferred] The paper evaluates only vanilla LLMs, while Related Work (§2.3) mentions other approaches use external knowledge retrieval or internal consistency examination. The best vanilla LLM achieves only 63.91% F1.
- **Why unresolved:** It is unknown whether methods designed for synthetic benchmarks generalize to authentic interactions with more diverse, open-ended queries.
- **What evidence would resolve it:** Systematic comparison of retrieval-augmented and consistency-based detectors on AuthenHallu.

### Open Question 3
- **Question:** Why does in-context information improve detection for some models (e.g., Qwen-3-32B) but degrade performance for others?
- **Basis in paper:** [inferred] Section 5.1.3 reports mixed results: in-context detection achieves better performance on five metrics but causes degradation for most models, suggesting context may "introduce noise or confusion rather than assistance."
- **Why unresolved:** The mechanism by which context helps or harms detection is unclear; model architecture or training may influence context utilization.
- **What evidence would resolve it:** Ablation studies varying context length and relevance, coupled with attention analysis to understand how models use prior turns.

### Open Question 4
- **Question:** How can detection methods address the severe class imbalance in hallucination categories (e.g., only 9 context-conflicting instances versus 157 fact-conflicting)?
- **Basis in paper:** [inferred] Table 2 and Section 3.2 show context-conflicting hallucinations constitute only 3.6% of hallucinated pairs; all ensemble F1-scores for this category remain below 10%.
- **Why unresolved:** Rare categories are difficult to learn and evaluate; authentic data may naturally exhibit such imbalances.
- **What evidence would resolve it:** Detection methods incorporating category-aware sampling, cost-sensitive learning, or data augmentation evaluated with stratified metrics.

## Limitations
- English-only dataset limits cross-lingual generalization and may bias topic distribution
- Only two-turn dialogues miss multi-turn context effects and longer conversation patterns
- Severe underrepresentation of context-conflicting hallucinations (9 instances) limits evaluation of self-consistency detection
- Moderate inter-annotator agreement (Kappa=0.591) suggests inherent subjectivity in hallucination annotation

## Confidence

- **High Confidence:** The 31.4% hallucination rate in authentic dialogues (versus synthetic datasets), the comparative weakness of LLMs at detecting context-conflicting hallucinations, and the finding that ensembles don't outperform single best models.
- **Medium Confidence:** The clustering-based sampling methodology's effectiveness at preserving natural intent distributions, and the claim that in-context detection benefits are model-dependent rather than universal.
- **Low Confidence:** The generalizability of the three-category taxonomy to longer dialogues, and the assumption that proportional sampling adequately captures rare but important user intents.

## Next Checks

1. **Annotation guideline refinement:** Conduct a pilot annotation with detailed guidelines on fact verification methods (external sources vs. internal consistency) and disagreement resolution procedures; target Fleiss's Kappa >0.7 for reliability.

2. **Context-conflicting augmentation:** Synthesize additional context-conflicting examples (10-20 instances) through controlled prompt manipulation to evaluate whether models improve when this category is better represented.

3. **Topic-stratified reproducibility:** Evaluate detection performance separately on high-hallucination topics (Math, Dates) versus low-hallucination topics (Casual Chat) to determine if failures are domain-specific or general; test whether targeted fine-tuning on reasoning-heavy topics improves overall detection.