---
ver: rpa2
title: Benchmarking and Confidence Evaluation of LALMs For Temporal Reasoning
arxiv_id: '2505.13115'
source_url: https://arxiv.org/abs/2505.13115
tags:
- audio
- arxiv
- tasks
- reasoning
- lalms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of evaluating and understanding
  the temporal reasoning capabilities of large audio language models (LALMs). The
  authors propose a novel dataset, TREA, specifically designed to benchmark LALMs
  on fine-grained temporal reasoning tasks, including event ordering, duration, and
  counting.
---

# Benchmarking and Confidence Evaluation of LALMs For Temporal Reasoning

## Quick Facts
- arXiv ID: 2505.13115
- Source URL: https://arxiv.org/abs/2505.13115
- Authors: Debarpan Bhattacharya; Apoorva Kulkarni; Sriram Ganapathy
- Reference count: 0
- Primary result: LALMs achieve <50% accuracy on two of three temporal reasoning tasks in TREA benchmark, significantly underperforming human baselines (~90%+)

## Executive Summary
This work introduces TREA, a novel dataset specifically designed to benchmark Large Audio Language Models (LALMs) on fine-grained temporal reasoning tasks including event ordering, duration, and counting. The authors evaluate three prominent open-source LALMs—Qwen2-Audio, SALMONN, and WavLLM—and find their performance consistently falls below human capabilities, with accuracy less than 50% on two tasks. To address limitations of accuracy-only evaluation, the paper introduces a test-time uncertainty metric (EUE) that measures model resilience to semantically invariant perturbations. Their analysis reveals that accuracy and uncertainty metrics are not necessarily correlated, highlighting the need for comprehensive evaluation of LALMs beyond accuracy alone, particularly for high-stakes applications.

## Method Summary
The authors created the TREA dataset (600 samples: 200 each for event ordering, duration, and counting tasks) by combining multiple ESC-50 environmental sound recordings. They evaluated three LALMs (Qwen2-Audio-7B, SALMONN-13B, WavLLM-7B) using four prompting strategies: vanilla MCQA, chain-of-thought, explanation-based, and a caption+LLM-QA pipeline that separates perception from reasoning. For uncertainty evaluation, they applied 60 semantically equivalent perturbations (4 text paraphrases × 15 audio augmentations) to 15 samples per task, measuring prediction consistency to compute EUE. They also calculated Expected Calibration Error (ECE) to assess confidence calibration. Human baselines were established using 5 participants.

## Key Results
- LALMs achieve <50% accuracy on TREA event ordering and duration tasks, compared to human baselines of 92.2% and 98.9%
- SALMONN with LLaMa caption+LLM-QA pipeline shows 67.1% relative improvement in accuracy versus SALMONN end-to-end
- No consistent correlation between accuracy and uncertainty metrics (EUE/ECE), indicating models can be accurate but brittle
- Chain-of-thought prompting fails to improve temporal reasoning performance in LALMs, unlike text-only LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A test-time uncertainty metric (EUE) can reveal model brittleness that accuracy metrics miss, by quantifying a model's consistency to semantically invariant perturbations.
- Mechanism: For a given input, the method generates multiple perturbed versions (e.g., rephrased questions, augmented audio) that preserve the ground-truth label. The model is considered uncertain if its answers vary across these semantically equivalent inputs. The Expected Uncertainty Error (EUE) is the average rate of this inconsistency.
- Core assumption: The designed perturbations (e.g., adding silence, rephrasing text) successfully generate inputs that are perceived by an ideal model as semantically identical to the original.
- Evidence anchors:
  - [abstract] "...uncertainty metric, which computes the invariance of the model to semantically identical perturbations of the input."
  - [section] "The uncertainty (Eq. (1)) measures the lack of consistency in the model's response to semantically equivalent input perturbations."
  - [corpus] Corpus papers on LALM evaluation (e.g., AU-Harness) do not prominently feature this specific perturbation-based uncertainty metric, indicating it as a distinct contribution for this domain.
- Break condition: If the perturbations are shown to alter the semantic content or ground-truth label for a significant portion of the dataset, the EUE metric would measure noise rather than uncertainty.

### Mechanism 2
- Claim: Decomposing the task into separate perception and reasoning stages ("Audio description + LLM-QA") can improve temporal reasoning performance for certain LALMs.
- Mechanism: The LALM is first used to generate a textual description (caption) of the audio. This text is then passed to a separate, text-only LLM to perform the multiple-choice reasoning. This leverages the superior reasoning capabilities of large text LLMs while isolating the LALM's role to perception.
- Core assumption: The LALM can generate a sufficiently accurate and detailed textual description of the temporal events in the audio.
- Evidence anchors:
  - [abstract] "...observe that they are consistently behind human capabilities on the tasks in the TREA dataset." (Implicitly motivating the decoupling).
  - [section] "Using LALMs in generative mode to generate audio captions and then using a separate LLM... produces interesting results. SALMONN generated captions result in 67.1% mean relative improvement..."
  - [corpus] The paper "Thinking with Sound" explores a related decomposition via Audio Chain-of-Thought (CoT).
- Break condition: If the LALM's generated caption is factually incorrect or omits crucial temporal details, the subsequent LLM will reason from flawed premises, and performance will degrade.

### Mechanism 3
- Claim: Current LALMs fail at fine-grained temporal reasoning primarily due to a deficit in temporal perception, not just reasoning.
- Mechanism: This is inferred from low baseline accuracy on the TREA benchmark (best <50% on two tasks) and the fact that standard prompting strategies like Chain-of-Thought (CoT) do not yield consistent improvements. If the model cannot correctly "perceive" the temporal structure from the audio embeddings, reasoning strategies cannot function.
- Core assumption: The TREA benchmark tasks require genuine temporal understanding and cannot be solved via simple linguistic shortcuts or text-based priors.
- Evidence anchors:
  - [abstract] "...observe that they are consistently behind human capabilities on the tasks in the TREA dataset, with the best accuracy being less than 50% on two of the three tasks."
  - [section] "We observe that the performance on audio temporal reasoning tasks are poor for all the LALMs... highlighting their lack of understanding of audio events in the temporal domain."
  - [corpus] "Not in Sync" corroborates this, finding that LALMs exhibit significant temporal bias and struggle with timestamp prediction.
- Break condition: If a novel prompting technique or architecture is found that dramatically improves TREA scores without changing the underlying audio perception, the failure would be re-attributed to a reasoning or alignment deficit.

## Foundational Learning

- Concept: **Epistemic Uncertainty**
  - Why needed here: The paper's key contribution is an "uncertainty metric." It's crucial to understand that this targets *epistemic* uncertainty—uncertainty in the model's knowledge—which can be estimated by measuring prediction stability under input perturbations.
  - Quick check question: How does measuring prediction consistency across perturbed inputs help distinguish between a model that is genuinely confident and one that is simply brittle?

- Concept: **Multimodal Semantic Alignment**
  - Why needed here: This is the core challenge identified for LALMs. The low performance on TREA is framed as a failure of alignment—the model has not learned to map fine-grained audio features (like event order or duration) to their corresponding linguistic concepts.
  - Quick check question: Why might a model achieve high performance on speech recognition (ASR) but fail on temporal reasoning, and how does this relate to what features are aligned?

- Concept: **Expected Calibration Error (ECE)**
  - Why needed here: The paper uses ECE to evaluate if a model's confidence scores are trustworthy. Understanding ECE is necessary to interpret the finding that a model can have high accuracy but poor calibration, meaning its confidence scores are not predictive of its correctness.
  - Quick check question: What does a high ECE score indicate about the relationship between the probability a model assigns to an answer and the likelihood of that answer being correct?

## Architecture Onboarding

- Component map:
    Audio Encoder -> Projector/Adapter -> LLM Backbone

- Critical path:
    The primary path for the reported failure runs from the **Audio Encoder**, through the **Projector**, to the **LLM**. The paper suggests a breakage in this chain: the audio encoder may not provide sufficiently timestamped features, the projector may not preserve them, and the resulting representation may lack the temporal detail required for the LLM to reason over.

- Design tradeoffs:
    - **End-to-end vs. Decoupled Inference:** The end-to-end LALM is simpler but conflation errors are hard to debug. The decoupled approach isolates perception from reasoning, enabling the use of more powerful text-only LLMs, but it creates a hard dependency on the quality of the intermediate caption.
    - **Uncertainty vs. Accuracy:** A model can be optimized for accuracy but become highly brittle (high uncertainty). The paper shows these are distinct metrics, creating a tradeoff in model selection for high-stakes applications where both reliability and correctness are important.

- Failure signatures:
    - **High Uncertainty (EUE) on TREA:** Indicates the model's predictions are unstable and change unpredictably with minor input variations. This is a sign of poor generalization and brittleness.
    - **Low Accuracy with High Confidence (High ECE):** A particularly dangerous failure mode where the model is confidently wrong. The paper finds accuracy and calibration are not necessarily correlated.
    - **Ineffective Chain-of-Thought (CoT):** If CoT fails to improve performance on TREA, it signals that the deficit is likely in the initial perception stage (the model cannot "see" the temporal structure to reason about it).

- First 3 experiments:
  1.  **Baseline Re-evaluation:** Replicate the paper's key results by evaluating an open-source LALM (e.g., Qwen2-Audio or SALMONN) on the TREA dataset to obtain baseline accuracy, ECE, and EUE scores.
  2.  **Perturbation Ablation:** Run the uncertainty evaluation while isolating perturbation types (text-only vs. audio-only) to determine which modality's perturbations most degrade consistency.
  3.  **Perception vs. Reasoning Audit:** For a sample of failed questions, manually inspect the audio captions generated by the LALM. Determine the percentage of failures caused by incorrect captions (perception failure) versus correct captions but wrong answers (reasoning failure).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does Chain-of-Thought (CoT) prompting fail to elicit reasoning improvements in Large Audio Language Models (LALMs) for temporal tasks, unlike in text-only LLMs?
- Basis in paper: [explicit] The authors observe that "The benefits of prompt based reasoning strategies like CoT and explanations are not very clear unlike the case in text-based LLMs."
- Why unresolved: The paper identifies the discrepancy in performance but does not investigate the underlying mechanistic reasons for the failure of CoT in the audio modality.
- What evidence would resolve it: An analysis of cross-modal attention maps during CoT generation or ablation studies comparing internal reasoning states between text and audio inputs.

### Open Question 2
- Question: Can the trade-off between accuracy and confidence calibration be resolved in cascaded LALM systems (e.g., Audio Captioning followed by Text-QA)?
- Basis in paper: [explicit] The authors note that while the "SALMONN with LLaMa model gives the best performance [in accuracy]... the uncertainty (EUE) and calibration (ECE) errors are lower for the SALMONN-13B model."
- Why unresolved: Decoupling perception (captioning) from reasoning (external LLM) improves accuracy but degrades the system's ability to estimate its own uncertainty, and it is unclear how to achieve both.
- What evidence would resolve it: Developing joint training objectives or confidence propagation methods that maintain high reasoning accuracy while minimizing Expected Calibration Error (ECE).

### Open Question 3
- Question: Do the temporal reasoning limitations identified in the TREA dataset generalize to complex, real-world acoustic environments beyond synthetic combinations?
- Basis in paper: [inferred] The TREA dataset is derived by "combining multiple ESC-50 audio recordings," which are clean, isolated 5-second clips.
- Why unresolved: The benchmark relies on programmatically combined audio samples; it is unstated whether these results predict failure in messier, overlapping, or naturally recorded soundscapes.
- What evidence would resolve it: Evaluating the same LALMs on a "wild" dataset of naturally occurring temporal events with dense annotations to see if the <50% accuracy trend holds.

## Limitations

- TREA dataset size (200 samples per task) may not capture full complexity of real-world temporal reasoning scenarios
- Evaluation relies on specific ESC-50 environmental sounds, potentially limiting generalizability to other audio domains
- Uncertainty metric (EUE) assumes prediction variance across semantically equivalent inputs directly reflects model confidence, which may not hold
- Human baseline established with only 5 participants may not represent broader human performance

## Confidence

**High Confidence:** The finding that current LALMs perform poorly on TREA temporal reasoning tasks (accuracy <50% on two of three tasks) is well-supported by empirical results across multiple models and prompting strategies.

**Medium Confidence:** The claim that uncertainty metrics reveal brittleness not captured by accuracy alone is supported by experimental results, but interpretation requires caution and needs more extensive validation.

**Low Confidence:** The assertion that temporal reasoning failures primarily stem from perception deficits rather than reasoning deficits remains largely inferential without direct causal evidence.

## Next Checks

1. **Cross-domain Generalization Test:** Evaluate the same LALMs on temporal reasoning tasks using different audio datasets (e.g., speech commands, music) to determine if the observed performance gaps are specific to environmental sounds or reflect broader limitations in LALM temporal perception capabilities.

2. **Perturbation Semantic Fidelity Analysis:** Conduct a systematic study where human annotators verify whether generated perturbations (both text and audio) truly preserve semantic meaning and ground truth labels. This would validate whether the uncertainty metric measures genuine model uncertainty versus artifactual instability.

3. **Component Attribution Study:** For a sample of failed TREA samples, use the caption+LLM-QA pipeline to separately evaluate the LALM's caption generation accuracy versus the text LLM's reasoning accuracy. This would quantify the relative contribution of perception versus reasoning failures to overall performance degradation.