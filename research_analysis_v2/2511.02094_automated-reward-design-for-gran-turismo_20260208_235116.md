---
ver: rpa2
title: Automated Reward Design for Gran Turismo
arxiv_id: '2511.02094'
source_url: https://arxiv.org/abs/2511.02094
tags:
- reward
- agent
- function
- functions
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents an automated reward design system for Gran Turismo
  7 that leverages large language models to generate reward functions from text-based
  instructions. The system iteratively generates and trains multiple reward functions,
  using a video language model to select the best-performing agents and a trajectory
  alignment coefficient to filter misaligned rewards.
---

# Automated Reward Design for Gran Turismo

## Quick Facts
- **arXiv ID**: 2511.02094
- **Source URL**: https://arxiv.org/abs/2511.02094
- **Reference count**: 40
- **One-line primary result**: Automated reward design system generates racing agents competitive with GT Sophy using LLM-generated rewards and VLM evaluation, without ground-truth fitness metrics.

## Executive Summary
This paper presents an automated reward design system for Gran Turismo 7 that leverages large language models to generate reward functions from text-based instructions. The framework iteratively generates and trains multiple reward functions, using a video language model to select the best-performing agents and a trajectory alignment coefficient to filter misaligned rewards. Without access to a ground-truth fitness metric, the system successfully produces racing agents competitive with GT Sophy, demonstrating that current foundation models can effectively replace expert reward design and human evaluation.

## Method Summary
The framework uses an iterative process where an LLM generates M=10 reward functions from natural language task descriptions and state/action space APIs. A trajectory alignment coefficient (TAC) filters these to the top N=5 aligned rewards, which are then trained using QR-SAC reinforcement learning. Trained agents produce trajectory videos evaluated by a VLM through pairwise comparisons, with Bradley-Terry ranking selecting the best agent per iteration. Human feedback (optional) provides text grounding for subsequent iterations. The process repeats K=4 times, with a final agent trained for 1500 epochs. Key components include LLM reward generation, TAC filtering, VLM-based preference collection, and iterative refinement with human/LLM feedback.

## Key Results
- The system produces agents competitive with GT Sophy on Lake Maggiore track without ground-truth fitness metrics
- VLM-human agreement rate reaches 74.14% vs human-human agreement of 78.54% for pairwise preferences
- TAC metric becomes reliably predictive only after collecting ~100+ preferences
- Human feedback significantly improves consistency (10/10 valid agents) compared to closed-loop LLM-only (3/10 valid)

## Why This Works (Mechanism)

### Mechanism 1: LLM-Based Reward Code Generation from Natural Language
The LLM converts text-based task descriptions into functional reward function code using only the Python dataclass API of state/action spaces. The LLM's internal world model of physics and racing enables reasoning about environment dynamics without explicit dynamics code.

### Mechanism 2: VLM Preference-Based Evaluation Replaces Ground-Truth Fitness
Video-language models evaluate agent behavior through pairwise comparisons of 15-second trajectory clips, generating text descriptions for LLM preference decisions. This enables automated selection without ground-truth fitness metrics.

### Mechanism 3: Trajectory Alignment Coefficient Filters Misaligned Rewards Pre-Training
TAC computes Kendall's Tau-a correlation between reward function outputs on trajectories and preference labels, filtering rewards before costly training. This predicts which rewards will produce preferred behaviors based on historical preference data.

## Foundational Learning

- **Reinforcement Learning Fundamentals (MDPs, reward functions, policy optimization)**
  - Why needed here: The entire system outputs reward functions for RL algorithms; understanding reward shaping consequences is essential.
  - Quick check question: Can you explain why a misspecified reward can lead to reward hacking even if the policy maximizes it?

- **Preference-Based Learning (Bradley-Terry models, pairwise comparisons)**
  - Why needed here: Agent selection uses pairwise VLM preferences converted to rankings via Bradley-Terry.
  - Quick check question: How does Bradley-Terry convert pairwise win/loss data into a scalar score per agent?

- **Foundation Model Prompting (LLM code generation, VLM multimodal input)**
  - Why needed here: System relies on effective prompting strategies for both reward code generation and video-based preference elicitation.
  - Quick check question: What failure modes occur when LLM prompts lack sufficient context about API constraints?

## Architecture Onboarding

- **Component map**: Task Description → LLM Reward Generator (M candidates) → TAC Filter (keep top N aligned) → RL Training (QR-SAC, parallel) → VLM Preference Collection (pairwise over N agents) → Bradley-Terry Ranking → Best Agent → Human Feedback (optional) → Next Iteration (with previous best context)

- **Critical path**: LLM reward generation → TAC filtering → RL training → VLM evaluation → selection. Each iteration takes ~300 epochs of training; final agents train 1500 epochs.

- **Design tradeoffs**: M (generated rewards) vs. compute: More candidates increase search coverage but raise training cost linearly. N (trained rewards) vs. diversity: Smaller N saves compute but may discard viable candidates if TAC is unreliable. Human feedback vs. autonomy: Human feedback improves consistency (10/10 valid agents) vs. closed-loop LLM-only (3/10 valid) but requires manual effort.

- **Failure signatures**: VLM-only feedback produces agents exceeding collision/course-out thresholds. TAC provides minimal filtering benefit with small preference datasets (<100 samples). Generated rewards may diverge in performance without human grounding.

- **First 3 experiments**: 1) Baseline validation: Run framework with M=10, N=5, K=4 iterations on single track with human feedback, measuring final placement vs. GT Sophy. 2) VLM accuracy ablation: Collect 100+ pairwise preferences from both VLM and human experts on held-out trajectories. 3) TAC data scaling: Pre-collect 400 preferences, then measure TAC prediction accuracy at D_sizes = {25, 50, 100, 200, 400}.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can this automated reward design framework generalize effectively to environments other than Gran Turismo?
- Basis in paper: [explicit] The conclusion states, "We leave the exploration of its broader applicability in other environments for future work."
- Why unresolved: While the authors argue the components are environment-agnostic, the entire experimental validation was conducted exclusively within the Gran Turismo 7 environment.

### Open Question 2
- Question: Can a dedicated weight-tuning process improve the consistency and performance of LLM-generated reward functions?
- Basis in paper: [explicit] Section 5.4 notes that manually tuning weights improved correlation with reference rewards, but "no special care was taken towards tuning these weights" in the framework, leaving it as "an open problem."

### Open Question 3
- Question: How can the Trajectory Alignment Coefficient (TAC) be made effective in the low-data regime typical of early training iterations?
- Basis in paper: [explicit] Section 5.3 concludes that TAC is unreliable until roughly 100 preferences are collected, suggesting that "bootstrapping an initial set of preferences" is left for future work.

## Limitations

- The system requires proprietary Gran Turismo 7 access and specialized interfaces not publicly available, making direct reproduction challenging.
- QR-SAC implementation details are underspecified with missing hyperparameters and network architectures.
- Evaluation is limited to a single track and vehicle type, raising questions about generalization across diverse racing scenarios.
- Closed-loop LLM-only feedback system produces agents exceeding collision/course-out thresholds when human feedback is absent.

## Confidence

**High confidence**: LLM reward generation from natural language is well-supported by related work and demonstrated through functional code generation. VLM preference accuracy of 74.14% vs human-human agreement of 78.54% provides strong empirical support.

**Medium confidence**: TAC metric effectiveness requires substantial preference data (>100 samples) to outperform random selection. VLM preferences correlate with task performance but lack extensive ablation studies.

**Low confidence**: Closed-loop LLM-only feedback system's performance is concerning with agents exceeding safety thresholds. The claim that foundation models can "effectively replace expert reward design and human evaluation" is overstated given these failures.

## Next Checks

1. **Generalization validation**: Test trained agents on 3-5 additional Gran Turismo 7 tracks with varying layouts and conditions to assess cross-track performance stability.

2. **VLM preference ablation study**: Conduct systematic evaluation of VLM accuracy by collecting pairwise preferences on 50+ held-out trajectories, comparing video-only vs trajectory+video input formats.

3. **TAC scaling experiment**: Pre-collect 400+ trajectory preferences, then systematically measure TAC prediction accuracy at increments of 25 preferences to characterize data efficiency requirements.