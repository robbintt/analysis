---
ver: rpa2
title: Revisiting Multilingual Data Mixtures in Language Model Pretraining
arxiv_id: '2510.25947'
source_url: https://arxiv.org/abs/2510.25947
tags:
- languages
- language
- data
- english
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates multilingual data mixture strategies for
  large language model pretraining. The authors systematically train 1.1B and 3B parameter
  models on datasets containing 25 to 400 languages, examining the impact of language
  count, data distribution, and pivot language selection on model performance.
---

# Revisiting Multilingual Data Mixtures in Language Model Pretraining

## Quick Facts
- arXiv ID: 2510.25947
- Source URL: https://arxiv.org/abs/2510.25947
- Reference count: 40
- Primary result: English proportions up to 60% do not degrade multilingual performance if sufficient non-English tokens exist; "curse of multilinguality" stems from capacity and data quality, not language count.

## Executive Summary
This paper investigates multilingual data mixture strategies for large language model pretraining, training 1.1B and 3B parameter models on datasets containing 25 to 400 languages. The authors systematically examine the impact of language count, data distribution, and pivot language selection on model performance. Key findings show that English data proportions up to 60% do not degrade multilingual performance if sufficient non-English tokens are included, and that English functions as an effective pivot language across language families. The study reframes the "curse of multilinguality" as primarily driven by model capacity limitations and data quality issues rather than the number of languages trained on.

## Method Summary
The study uses decoder-only LLaMA architecture with 1.1B (24 layers, 1536 hidden dim, 16 heads) and 3B (28 layers, 2496 hidden dim, 24 heads) parameter models. Training uses mC4 corpus (30 languages) and FineWeb2 corpus (up to 1,834 languages) with Mistral-Nemo-Base-2407 tokenizer (131k vocab). Models are trained on 100B–225B tokens using AdamW optimizer with learning rate 8e-4, weight decay 0.1, and temperature sampling (τ=3.3) for language balancing. Evaluation uses validation LM loss and 10 multilingual benchmarks aggregated per language.

## Key Results
- English proportions up to 60% do not degrade multilingual performance if sufficient absolute non-English tokens exist
- English serves as an effective cross-family pivot, with similar benefits to family-specific pivots
- The "curse of multilinguality" is driven by capacity and data quality issues, not language count
- Curriculum learning strategies fail to improve multilingual performance compared to token-based approaches

## Why This Works (Mechanism)

### Mechanism 1: Absolute Token Threshold Protects Cross-lingual Performance
- Claim: English proportions up to 60% do not degrade multilingual performance if sufficient absolute non-English tokens exist.
- Mechanism: Model capacity saturates per-language; when each language exceeds a token threshold, additional English tokens use otherwise-unused capacity rather than displacing multilingual learning.
- Core assumption: The threshold is proportional to model size and token diversity; specific values may differ for larger models.
- Evidence anchors:
  - [abstract] "provided that languages have a sufficient number of tokens included in the pretraining corpus"
  - [Section 3, Figure 1b] Under Fixed Multilingual Budget, multilingual loss is stable with English up to 60%; under Fixed Total Budget, degradation begins above 50% when non-English tokens are reduced.
  - [corpus] GRAPE and other data mixture papers optimize domain weights but do not contradict the token-threshold finding.
- Break condition: If non-English tokens fall below the per-language threshold (e.g., aggressive compression under Fixed Total Budget), multilingual loss rises regardless of pivot choice.

### Mechanism 2: English as a Cross-Family Pivot via Representation Richness
- Claim: English yields cross-lingual benefits across language families; family-specific pivots do not consistently outperform it.
- Mechanism: High-resource, high-diversity English data produces broadly reusable representations; typological proximity matters most in very low-resource regimes where target-language data are minimal.
- Core assumption: English benefits stem from data diversity and quality, not language family membership; the effect may weaken if English data quality drops or tokenizer coverage is poor.
- Evidence anchors:
  - [abstract] "English as a pivot language yields benefits across language families"
  - [Section 4, Figure 3] For Slavic and Cyrillic-script languages, English and Russian perform comparably up to 50% pivot allocation; combining both yields the best loss.
  - [corpus] Pivot-based transfer in machine translation is established, but family-specific pivot advantage is not supported here.
- Break condition: If target languages share very little script/vocabulary overlap with English and have extremely few tokens, family-specific pivots may modestly outperform English.

### Mechanism 3: "Curse of Multilinguality" as Capacity and Quality Effects, Not Language Count
- Claim: Increasing language count does not inherently degrade performance; degradation is driven by finite capacity and low-quality data amplification under temperature sampling.
- Mechanism: Under natural distribution, high/mid-resource languages dominate and maintain quality; under aggressive temperature sampling, very low-resource languages receive proportionally more weight, introducing noise that consumes capacity.
- Core assumption: Quality and capacity constraints are the limiting factors; scaling model size or cleaning low-resource data should mitigate the observed degradation.
- Evidence anchors:
  - [abstract] "we do not observe a significant 'curse of multilinguality' as the number of training languages increases"
  - [Section 6, Table 1, Figure 5] Natural distribution maintains stable loss from 25 to 400 languages; temperature sampling shows ~0.1 loss increase, reduced in Controlled Growth where original language token counts are preserved.
  - [corpus] Prior work (Conneau et al., 2020) framed the curse as language count; this paper reframes it with capacity and quality.
- Break condition: If capacity is severely limited relative to total token diversity, or if low-resource data is noisy without remediation, scaling language count can still degrade performance.

## Foundational Learning

- Concept: Token Budget Regimes (Fixed Total vs. Fixed Multilingual)
  - Why needed here: Interpretation of all trade-off claims depends on whether increasing English comes at the expense of multilingual tokens (Fixed Total) or is additive (Fixed Multilingual).
  - Quick check question: Does adding more English in your planned corpus reduce the absolute tokens available for other languages?

- Concept: Temperature Sampling for Language Balancing
  - Why needed here: Explains why "curse of multilinguality" manifests differently under natural vs. temperature-sampled distributions.
  - Quick check question: What temperature τ are you using, and how does it affect the share of low-resource languages?

- Concept: Cross-lingual Transfer and Pivot Languages
  - Why needed here: Core to understanding why English can benefit languages across families and when family-specific pivots might help.
  - Quick check question: Does your corpus include a high-resource pivot with diverse domains, and do target languages share script or typological features?

## Architecture Onboarding

- Component map: LLaMA decoder -> AdamW optimizer (β=0.9/0.95) -> Mistral-Nemo-Base-2407 tokenizer -> temperature sampling -> 100B-225B token training
- Critical path: Define language set and token counts -> Choose sampling strategy (natural vs. temperature with τ) -> Ensure per-language token thresholds are met for target languages -> Train and evaluate on validation loss + downstream benchmarks
- Design tradeoffs:
  - English proportion: Up to 60% safe if multilingual tokens are sufficient; beyond 60% in Fixed Total Budget risks degradation
  - Natural vs. temperature sampling: Natural preserves high-resource quality; temperature reduces imbalance but may amplify noisy low-resource data
  - Pivot selection: English is broadly effective; combining with family-specific pivot may help in very low-resource settings
- Failure signatures:
  - Multilingual loss rises when non-English tokens are compressed below per-language thresholds
  - Curriculum learning does not improve final multilingual performance; gains in English are explained by more English tokens, not curriculum structure
  - Temperature-sampled runs show higher loss when scaling language count if low-resource data is noisy
- First 3 experiments:
  1. Run Fixed Multilingual Budget sweep with English 20%, 40%, 60% while holding non-English tokens fixed at 90B; monitor multilingual validation loss
  2. Compare natural vs. temperature sampling (τ=3.3) with 100 vs. 400 languages; track per-language loss and English benchmark performance
  3. Test English-only pivot vs. English+family-specific pivot (e.g., English+Russian for Slavic) in low-resource conditions; measure per-language loss to identify when typological proximity helps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the findings on multilingual data mixtures and the absence of a "curse of multilinguality" generalize to frontier-scale models (e.g., 70B+ parameters)?
- Basis in paper: [explicit] The conclusion states: "While these principles were established on 1.1B and 3B parameter models, future work must validate these trade-offs on larger models to explore how increased model capacity potentially alters the non-linear relationship between data composition, interference, and performance."
- Why unresolved: The study was computationally constrained to smaller model sizes, leaving open whether the observed patterns (e.g., resilience to English proportions, limited negative interference) persist or shift at scale.
- What evidence would resolve it: Training and evaluating multilingual LLMs at 30B–70B+ scales with controlled data mixture variations, comparing against the 1.1B and 3B baselines.

### Open Question 2
- Question: How do post-training processes (instruction tuning, RLHF, continued pretraining) interact with multilingual data mixture decisions?
- Basis in paper: [explicit] In the Limitations section, the authors state they "were unable to explore the impact of post-training and the effects of various data sampling strategies."
- Why unresolved: The study isolates pretraining dynamics but cannot assess whether the benefits of certain mixture strategies (e.g., high English proportions) are preserved, amplified, or degraded after alignment.
- What evidence would resolve it: Subjecting models trained under different mixture regimes to standardized post-training pipelines, then evaluating cross-lingual transfer and in-language performance.

### Open Question 3
- Question: What is the minimum "sufficient" token threshold per language required to avoid the "curse of multilinguality"?
- Basis in paper: [inferred] The paper repeatedly concludes that performance degradation arises not from language count but from insufficient high-quality tokens per language, yet never quantifies the threshold.
- Why unresolved: The experiments vary proportions and total budgets but do not systematically identify the per-language token floor below which degradation emerges, nor how this varies by language family or script.
- What evidence would resolve it: Controlled experiments that fix model size and total budget while systematically varying per-language token counts across diverse languages to identify critical minima.

## Limitations

- Findings based on relatively small models (1.1B and 3B parameters) may not generalize to frontier-scale models
- Paper does not quantify specific per-language token thresholds required for stable multilingual performance
- Curriculum learning experiments did not explore more sophisticated strategies that might yield different results
- Limited investigation of data quality remediation strategies for low-resource languages under aggressive temperature sampling

## Confidence

**High Confidence**: The findings regarding English proportions up to 60% being safe when sufficient multilingual tokens exist are well-supported by systematic experiments across different budget regimes.

**Medium Confidence**: The claim that English functions as an effective cross-family pivot language is supported by experiments but is somewhat limited to specific language families and does not comprehensively test all possible family combinations.

**Medium Confidence**: The reframing of the "curse of multilinguality" as primarily a capacity and quality issue rather than language count is supported by the comparison between natural and temperature-sampled distributions, but this conclusion may not generalize to much larger models.

## Next Checks

1. **Scale Validation**: Replicate the Fixed Multilingual Budget experiments (English 20%, 40%, 60% with fixed 90B multilingual tokens) using a 7B or 13B parameter model to test whether the English proportion thresholds remain valid at larger scales. Monitor per-language validation loss to identify any scaling-dependent break points.

2. **Threshold Quantification**: Conduct systematic experiments to determine the minimum per-language token threshold required for stable multilingual performance across different model sizes (1B, 3B, 7B). Vary non-English token allocations while holding English constant at 40% to identify the exact threshold where multilingual degradation begins.

3. **Quality Remediation Test**: Test the capacity-quality hypothesis by implementing aggressive data cleaning for low-resource languages in temperature-sampled runs (τ=3.3) with 400 languages. Compare validation loss against uncleaned temperature runs to determine whether data quality improvements can fully mitigate the observed degradation.