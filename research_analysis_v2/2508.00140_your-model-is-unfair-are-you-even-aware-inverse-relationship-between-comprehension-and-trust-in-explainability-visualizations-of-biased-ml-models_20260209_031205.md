---
ver: rpa2
title: Your Model Is Unfair, Are You Even Aware? Inverse Relationship Between Comprehension
  and Trust in Explainability Visualizations of Biased ML Models
arxiv_id: '2508.00140'
source_url: https://arxiv.org/abs/2508.00140
tags:
- trust
- bias
- comprehension
- perception
- visualization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how visualization design impacts model
  comprehension, bias perception, and trust among non-expert users. The authors systematically
  survey 26 explainability visualizations, creating a taxonomy of 54 design characteristics
  across 27 dimensions.
---

# Your Model Is Unfair, Are You Even Aware? Inverse Relationship Between Comprehension and Trust in Explainability Visualizations of Biased ML Models

## Quick Facts
- arXiv ID: 2508.00140
- Source URL: https://arxiv.org/abs/2508.00140
- Reference count: 40
- Primary result: Comprehension of biased ML models causally reduces trust by increasing perceived bias

## Executive Summary
This paper investigates how visualization design impacts model comprehension, bias perception, and trust among non-expert users. Through systematic analysis of 26 explainability visualizations and user studies with 818 participants, the authors discover an inverse relationship between comprehension and trust for biased models. The better users understand discriminatory models, the less they trust them. This counterintuitive finding is explained by a mediation mechanism: comprehension increases bias perception, which then reduces trust. The work provides empirical evidence that visualization design can significantly influence these relationships, offering practical guidance for developing responsible ML applications.

## Method Summary
The study employed 7 loan-recommendation scenarios using the Census Income dataset (14 demographic features, 48,842 people), focusing on 5 key features: age, education level, occupation, hours worked per week, and sex. A LightGBM classifier was trained on this data, creating both a biased model (31% men vs 11% women receive loans) and a fair model using Seldonian Algorithm with demographic parity constraints. Six visualization tools (SHAP waterfall, SHAP force, ELI5, LIME, CP Profiles, Anchors) were evaluated across 27 design dimensions in a taxonomy of 54 characteristics. User studies were conducted via Qualtrics on Prolific.com with 75 participants per survey (818 total), measuring comprehension (objective questions C1-C3), trust (T1-T5), and bias perception (B1-B4) across 7 scenarios per participant.

## Key Results
- Visualization explicitness significantly improves comprehension (d = 0.68, p < 0.001) by reducing cognitive load required to decode feature attribution
- Higher comprehension of biased models reduces trust through increased bias perception, mediated by the visualization's ability to reveal discriminatory patterns
- Visual complexity and alternative outputs can mask perceived bias, artificially inflating trust even when comprehension remains stable (d = -0.27 for bias perception)
- Comprehensibility and trust are positively correlated for fair models but negatively correlated for biased models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** In biased models, higher user comprehension causally reduces trust because comprehension acts as a catalyst for bias detection.
- **Mechanism:** Visualization designs that improve understanding (e.g., explicit text values) allow users to identify discriminatory patterns (e.g., sex-based loan disparities). This heightened *bias perception* then acts as the primary driver for lowering trust, rather than comprehension itself.
- **Core assumption:** Users possess the domain knowledge to recognize specific biases (e.g., gender discrimination) once the model logic is made explicit.
- **Evidence anchors:**
  - [abstract] "...relationship is strongly mediated by bias perception: more comprehensible visualizations increase people's perception of bias, and increased bias perception reduces trust."
  - [section 6] Figure 5 shows the mediation model where comprehension predicts bias perception ($b=0.27$) which negatively predicts trust ($b=-2.35$).
  - [corpus] Evidence is weak; neighbors focus on trust calibration or bias mitigation generally, not the specific mediation pathway of visualization comprehension.
- **Break condition:** If a model is perfectly fair, this mechanism predicts comprehension would not reduce trust (and might increase it), as heightened scrutiny would find no fault.

### Mechanism 2
- **Claim:** Explicit visual encoding of feature attribution (magnitude and direction) is the primary driver for user comprehension, outperforming implicit encodings like position or shape.
- **Mechanism:** Reducing the cognitive load required to decode a visualization (e.g., printing "Age: +0.10" rather than forcing estimation from a line plot) allows non-experts to accurately map input features to output probabilities.
- **Core assumption:** Users have low visual literacy or low patience for complex decoding of implicit graphs (like Ceteris-Paribus profiles).
- **Evidence anchors:**
  - [section 6] "Visualizations that explicitly showed the magnitude and direction of feature impacts had higher comprehension (vs. those where they had to be inferred, 38.97 vs. 23.46)."
  - [section 7.1] "Explicitness has a positive medium effect on comprehension ($d = 0.68$)..."
  - [corpus] "Can AI Explanations Make You Change Your Mind?" touches on explanation uptake but does not validate the specific "explicitness" taxonomy dimension.
- **Break condition:** If the number of explicit features becomes overwhelming (clutter), cognitive load may spike and reverse comprehension gains.

### Mechanism 3
- **Claim:** Visual complexity and the inclusion of "alternative outputs" can mask perceived bias, artificially inflating trust even when comprehension is stable.
- **Mechanism:** Adding interactive elements or secondary charts (like CP plots on hover) distracts users or dilutes the focus on the primary discriminatory features, reducing the *salience* of bias.
- **Core assumption:** Users satisfice; they stop looking for bias once they see complex, "scientific-looking" visual details that satisfy their need for justification.
- **Evidence anchors:**
  - [abstract] "...reducing perceived model bias, either by improving model fairness or by adjusting visualization design, significantly increases trust..."
  - [section 7.3] Adding CP plots on hover (complexity) caused a "small decrease in bias perception ($d = -0.27$)" and increased trust.
  - [corpus] No direct validation found; neighbors like "Would You Rely on an Eerie Agent?" discuss trust heuristics but not visual complexity masking.
- **Break condition:** If the bias is overt (e.g., a "Sex" feature bar dominating the chart), design masking via complexity will likely fail.

## Foundational Learning

- **Concept:** **Feature Attribution / SHAP Values**
  - **Why needed here:** The paper evaluates tools (LIME, SHAP) that visualize how much each input feature (e.g., Age, Sex) pushes a prediction up or down. Without this, the "comprehension" metric makes no sense.
  - **Quick check question:** If a model predicts "Loan Denied" and the "Debt" bar points negative and is large, does "Debt" increase or decrease the probability of getting the loan?

- **Concept:** **Statistical Mediation Analysis**
  - **Why needed here:** The paper's central insight relies on proving that *Bias Perception* mediates the relationship between *Comprehension* and *Trust*, rather than Comprehension directly reducing Trust.
  - **Quick check question:** If Variable A causes Variable B, and Variable B causes Variable C, what is the relationship between A and C called if B is the intermediary?

- **Concept:** **Demographic Parity (Fairness)**
  - **Why needed here:** The paper defines the "Biased Model" using disparate loan rates between men and women (24% vs 1.4%). Understanding this definition is required to interpret why trust drops when bias is perceived.
  - **Quick check question:** If a model approves 90% of Group A and 10% of Group B, does it satisfy demographic parity?

## Architecture Onboarding

- **Component map:** Frontend (Viz Layer: Taxonomy D1-D27) -> Backend (Model Layer: LightGBM + SHAP/LIME) -> User State (Comprehension, Bias Perception, Trust)

- **Critical path:**
  1. Select taxonomy dimensions (e.g., D16: Explicit direction)
  2. Generate explanation visualization for a prediction
  3. User observes visual cues -> Decodes feature attribution (Comprehension)
  4. User cross-references attribution with protected attributes (Bias Perception)
  5. User calibrates willingness to use the system (Trust)

- **Design tradeoffs:**
  - **Simplicity vs. Masking:** "Simpler" visualizations (removing axes) actually *increased* bias perception in Experiment 3, while complex interactive overlays *decreased* it. You trade "looking sophisticated" for "risking bias exposure."
  - **Comprehension vs. Trust:** For biased models, better design (High Comprehension) leads to lower Trust. You cannot maximize both simultaneously without fixing the model.

- **Failure signatures:**
  - **Illusion of Comprehension:** Users report high confidence (Likert) but fail objective questions (MCQ). Common in implicit visualizations like Anchors.
  - **Behavioral Misalignment:** Users trust the model because they personally get a loan, ignoring system-wide bias.
  - **Color Confusion:** Users misinterpret SHAP red/blue encodings, inverting their understanding of feature impact.

- **First 3 experiments:**
  1. **Explicitness A/B Test:** Deploy the same model with "Text Value" overlays vs. pure color bars. Measure the delta in objective comprehension scores.
  2. **Masking Stress Test:** Introduce visual noise (e.g., secondary CP plots) to a clearly biased model. Check if Bias Perception drops significantly (validating the "masking" effect).
  3. **Fairness Calibration:** Run the visualization on a "Fair" model vs. "Biased" model. Verify that Trust *increases* for the Fair model but *decreases* for the Biased model as Comprehension rises (validating appropriate trust).

## Open Questions the Paper Calls Out

- **Open Question 1:** How do different explanation types (additive feature contributions vs. if-then rules vs. counterfactual explanations) affect comprehension and bias perception when used individually versus in combination? Basis: [explicit] "Further research is necessary to better understand how different explanation types both individually and in combination can impact viewers of explainability visualizations" (Section 8).

- **Open Question 2:** How does the relationship between comprehension and trust change across more granular variations of model bias levels? Basis: [explicit] "We neither varied the model's level of bias, beyond creating one fair and one unfair version, nor the model's comprehensibility. Future work investigating more granular bias variations may provide more nuanced insights into the relationship between comprehension and trust" (Section 8).

- **Open Question 3:** What is the causal relationship between visualization complexity, explicitness, bias perception, and trust? Basis: [explicit] "Anecdotally, we notice that visualizations with lower complexity and more explicitness correlate with higher bias perception and lower trust, and this correlation should be explored in future work" (Section 9).

- **Open Question 4:** How do non-binary individuals perceive ML bias compared to binary gender groups? Basis: [explicit] "Anecdotally, all 19 non-binary participants across survey variations found both fair and biased models to be discriminatory... there is a dearth of research into non-binary individuals' perception of ML bias" (Section 9).

## Limitations
- Limited to tabular data with 5 features, restricting generalizability to high-dimensional or image/text data
- Participant pool was relatively homogeneous (US-based Prolific users), potentially missing cultural variations in trust/comprehension patterns
- Mediation model stability depends heavily on the specific demographic bias used (sex-based loan disparities)

## Confidence
- Mechanism 1 (comprehension→bias perception→trust): **Medium** - Strong statistical mediation but limited to specific bias type
- Mechanism 2 (explicitness improves comprehension): **High** - Clear effect sizes with p < 0.001, though based on single dataset
- Mechanism 3 (complexity masks bias): **Low-Medium** - Small effect sizes (d = -0.27) and speculative cognitive mechanism

## Next Checks
1. Replicate the mediation analysis with a different protected attribute (e.g., race-based bias) to test generalizability
2. Test the explicitness effect with higher-dimensional datasets (e.g., 20+ features) to check for saturation/clutter effects
3. Conduct eye-tracking studies to empirically verify whether visual complexity actually reduces attention to discriminatory features versus alternative explanations