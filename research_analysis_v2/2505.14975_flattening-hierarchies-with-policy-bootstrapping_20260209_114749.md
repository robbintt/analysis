---
ver: rpa2
title: Flattening Hierarchies with Policy Bootstrapping
arxiv_id: '2505.14975'
source_url: https://arxiv.org/abs/2505.14975
tags:
- policy
- learning
- which
- should
- offline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a method to train flat (non-hierarchical) goal-conditioned
  policies by bootstrapping on subgoal-conditioned policies. The key idea is to leverage
  the ease of training subpolicies on nearby goals and use advantage-weighted importance
  sampling to bootstrap from in-trajectory waypoint states.
---

# Flattening Hierarchies with Policy Bootstrapping

## Quick Facts
- **arXiv ID**: 2505.14975
- **Source URL**: https://arxiv.org/abs/2505.14975
- **Reference count**: 40
- **Primary result**: Achieves 95% success on 20 OGBench tasks, outperforming GCIVL+AWR and HIQL baselines, especially on long-horizon visual navigation

## Executive Summary
This work introduces SAW (Subgoal-Aware Weighting), a method that trains flat goal-conditioned policies by bootstrapping on subgoal-conditioned subpolicies without requiring a generative subgoal model. The key innovation is using advantage-weighted importance sampling over dataset trajectories to extract high-quality subgoal signals for policy bootstrapping. This approach eliminates the complexity of learned subgoal models while retaining the long-horizon strengths of hierarchical RL. Experiments show SAW matches or exceeds state-of-the-art offline GCRL methods on diverse locomotion and manipulation tasks, and notably achieves non-trivial success in challenging humanoidmaze-giant environments where prior methods fail.

## Method Summary
SAW trains a flat goal-conditioned policy through three sequential stages: (1) train a value function using GCIVL with expectile regression, (2) train a target subpolicy via AWR on k-step future states from the dataset, and (3) train the main policy using an objective that combines one-step AWR with advantage-weighted bootstrapping from the subpolicy. The method samples subgoals directly from dataset trajectories, weighted by their advantage toward the final goal, eliminating the need for a learned generative subgoal model. This importance-weighted approach balances value-based learning with policy bootstrapping, dynamically shifting emphasis as goal horizon increases.

## Key Results
- Achieves 95% success rate across 20 OGBench tasks (state- and pixel-based)
- Outperforms GCIVL+AWR and HIQL baselines on 19/20 tasks
- Only existing method to achieve non-trivial success (>25%) on humanoidmaze-giant
- Matches or exceeds state-of-the-art on complex long-horizon tasks where prior approaches fail

## Why This Works (Mechanism)

### Mechanism 1: Advantage-Weighted Importance Sampling Over Dataset Subgoals
The method samples subgoals directly from dataset trajectories, weighted by their advantage toward the final goal, eliminating the need for a learned generative subgoal model. The optimal subgoal posterior is approximated via Bayes' rule: `p(w|s,g,U=1) ∝ p_D(w|s) exp(A(s,w,g))`. This replaces the expectation over a learned `π_h(w|s,g)` with an importance-weighted expectation over dataset samples.

### Mechanism 2: Training on Nearby Goals Improves Action Quality
Dataset actions have higher advantage with respect to nearby subgoals (k steps ahead) than distant goals, yielding better policy gradients. The target subpolicy is trained via AWR on `(s, a, w)` tuples where `w` is sampled from k-step future states, leveraging the fact that actions were actually taken toward these subgoals in the data.

### Mechanism 3: Dynamic Balance Between Value-Based and Bootstrap Terms
The SAW objective automatically shifts emphasis from value-based learning to policy bootstrapping as goal horizon increases. The objective balances one-step AWR against bootstrapping, with the bootstrap term's advantage remaining meaningful for good subgoals even when the value function is noisy at long horizons.

## Foundational Learning

- **Concept**: Goal-Conditioned RL (GCRL) and Hindsight Relabeling
  - Why needed here: SAW operates on reward-free trajectories; understanding how goals are sampled from future states is essential
  - Quick check question: Can you explain why relabeling with achieved goals enables learning from any trajectory?

- **Concept**: Implicit Q-Learning (IQL) and Expectile Regression
  - Why needed here: SAW uses GCIVL (action-free IQL) for value learning; the expectile loss τ controls the bias-variance tradeoff
  - Quick check question: Why does IQL avoid querying out-of-distribution actions, and what hyperparameter controls optimism?

- **Concept**: Advantage-Weighted Regression (AWR)
  - Why needed here: Both the target subpolicy and the one-step term use AWR for policy extraction
  - Quick check question: How does the inverse temperature α in `exp(αA) log π` control the strength of policy improvement vs. staying near the data distribution?

## Architecture Onboarding

- **Component map**: Dataset D → Value Function V_φ → Target Subpolicy π_ψ → Main Policy π_θ

- **Critical path**: (1) Train V_φ until convergence → (2) Train π_ψ on subgoals → (3) Train π_θ with SAW objective using frozen π_ψ. Each stage is sequential per Algorithm 1.

- **Design tradeoffs**:
  - Subgoal steps k: Larger k captures more hierarchical structure but increases subgoal distance; paper uses k=25 for mazes, k=100 for humanoid, k=10 for manipulation
  - KL temperature β: Controls bootstrap strength; manipulation tasks need careful tuning (β=0.3–3.0), locomotion is robust at β=3.0
  - Expectile τ: Higher values (τ=0.9) help manipulation but can hurt locomotion; default τ=0.7

- **Failure signatures**:
  - Value divergence in visual long-horizon tasks: diagnose by monitoring expectile loss stability
  - Poor performance on stitching/explore datasets: importance sampling from dataset cannot synthesize novel subgoals
  - Subgoal representations hurt performance on giant mazes: compact representations limit expressiveness

- **First 3 experiments**:
  1. Sanity check on antmaze-medium-navigate: Train SAW with default hyperparameters (τ=0.7, α=3.0, β=3.0, k=25). Target: match GCIVL+AWR baseline (~72%) within 1M steps
  2. Ablate bootstrap term: Run SAW with β=0 (no bootstrapping) on antmaze-giant-navigate vs. full SAW. Expect large gap (>30% absolute)
  3. Compare to HIQL on humanoidmaze-giant: Run both methods with identical value functions. SAW should achieve >25% success without subgoal representation learning

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does policy bootstrapping reduce variance in long-horizon policy learning similar to how value bootstrapping reduces target variance?
- **Basis in paper**: The Discussion section hypothesizes that using stable action targets from subpolicies reduces the high variance of gradients typically associated with noisy value functions in long-horizon tasks
- **Why unresolved**: The paper validates performance outcomes but does not include an analysis of gradient variance or stability metrics
- **What evidence would resolve it**: A comparative analysis of gradient variance and training stability statistics between SAW and standard value-based methods on long-horizon benchmarks

### Open Question 2
- **Question**: Can alternative sampling strategies for SAW bridge the performance gap with generative subgoal methods in datasets requiring high degrees of stitching?
- **Basis in paper**: Section 7 notes that SAW performance degrades in "explore" datasets and suggests that methods generating "imagined" subgoals may perform better
- **Why unresolved**: The current method relies on sampling subgoals directly from dataset trajectories, which limits its ability to synthesize novel subgoals
- **What evidence would resolve it**: Developing a modified sampling scheme for SAW that improves performance on the "explore" datasets to match or exceed generative baselines

### Open Question 3
- **Question**: How can offline value learning objectives be modified to prevent divergence in very long-horizon tasks with high-dimensional visual observations?
- **Basis in paper**: Section 6.2 reports that value function training diverged for SAW and baselines in the `visual-antmaze-giant` and `visual-humanoidmaze` environments
- **Why unresolved**: Current value learning mechanisms (like GCIVL) struggle to maintain stability over long horizons when combined with visual encoders
- **What evidence would resolve it**: Identifying specific architectural or objective modifications that stabilize value training in pixel-based giant environments

## Limitations

- The method's performance relies heavily on dataset quality and coverage of high-advantage subgoals, with no quantification of required coverage density
- High sensitivity to hyperparameters (especially β and τ) in manipulation tasks suggests limited robustness across task categories
- Cannot synthesize novel subgoals for stitching trajectories, limiting performance on "explore" datasets compared to generative subgoal methods

## Confidence

- **High confidence**: The experimental results showing SAW outperforms GCIVL+AWR on 19/20 tasks and achieves non-trivial success on humanoidmaze-giant where prior methods fail
- **Medium confidence**: The theoretical mechanism that advantage-weighted importance sampling over dataset subgoals provides stable bootstrapping, as this relies on strong assumptions about dataset quality
- **Medium confidence**: The claim that SAW eliminates the need for subgoal representation learning, since the ablation comparing SAW to HIQL on giant mazes is compelling but doesn't test alternative representation learning approaches

## Next Checks

1. **Dataset coverage analysis**: Measure the distribution of subgoal advantages in the dataset and correlate with SAW performance to validate whether importance-weighted sampling works because the dataset contains sufficient high-advantage subgoals or despite sparse coverage.

2. **Bootstrap ablation with alternative methods**: Replace the SAW bootstrap term with a learned generative subgoal model (as in HIQL) while keeping the same value function and target subpolicy to isolate whether the improvement comes from bootstrapping itself versus the specific importance-weighted sampling approach.

3. **Value function sensitivity study**: Systematically vary the expectile parameter τ and KL temperature β across all task categories (locomotion, manipulation, navigation) to quantify the claimed robustness differences and identify regimes where SAW performance degrades.