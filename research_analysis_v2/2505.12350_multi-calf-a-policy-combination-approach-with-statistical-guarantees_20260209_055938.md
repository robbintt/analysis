---
ver: rpa2
title: 'Multi-CALF: A Policy Combination Approach with Statistical Guarantees'
arxiv_id: '2505.12350'
source_url: https://arxiv.org/abs/2505.12350
tags:
- policy
- base
- state
- policies
- goal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Multi-CALF, a policy combination approach
  that intelligently fuses a standard RL policy with a theoretically-backed alternative
  policy. The method uses relative value improvements to dynamically select between
  policies, maintaining formal stability guarantees while often achieving better performance
  than either policy individually.
---

# Multi-CALF: A Policy Combination Approach with Statistical Guarantees

## Quick Facts
- arXiv ID: 2505.12350
- Source URL: https://arxiv.org/abs/2505.12350
- Reference count: 24
- One-line primary result: Multi-CALF achieved mean episode return of 1719 ± 54 on Hopper, outperforming both constituent policies (PPO: 1684 ± 7.1 and TD3: 1632 ± 6.2) while maintaining stability guarantees

## Executive Summary
This paper introduces Multi-CALF, a policy combination approach that intelligently fuses a standard RL policy with a theoretically-backed alternative policy. The method uses relative value improvements to dynamically select between policies, maintaining formal stability guarantees while often achieving better performance than either policy individually. Theoretical analysis proves the combined policy inherits goal-reaching guarantees with known probability, along with bounds on maximum deviation and convergence time. Empirical validation on the Hopper control task demonstrates that Multi-CALF achieved a mean episode return of 1719 ± 54, outperforming both constituent policies while maintaining stability guarantees.

## Method Summary
Multi-CALF combines two pre-trained policies by selecting actions based on their relative value improvements at each timestep. The algorithm maintains running best-observed critic values (v†_base and v†_alt) for each policy, computes normalized improvements (Δ/v†), and selects the policy with higher relative improvement with geometrically decaying probability. The acceptance probability ρt(s) = λ^t × p_relax ensures eventual commitment to the alternative policy through the Borel-Cantelli lemma. The method requires the alternative policy to satisfy ε-improbable goal-reaching property, which provides theoretical stability guarantees.

## Key Results
- Multi-CALF achieved mean episode return of 1719 ± 54 on Hopper environment
- Outperformed both constituent policies: PPO (1684 ± 7.1) and TD3 (1632 ± 6.2)
- Theoretical guarantees ensure eventual commitment to alternative policy with known probability
- Geometric decay schedule ensures Borel-Cantelli summability condition is satisfied

## Why This Works (Mechanism)

### Mechanism 1: Normalized Critic Improvement Comparison
Normalizing critic improvements by best-observed values enables fair cross-policy comparison despite different value scales. At each timestep, compute Δbase = v̂πb(St) - v†_base and Δalt = v̂πa(St) - v†_alt, then compare ratios Δbase/v†_base versus Δalt/v†_alt. The policy with higher relative improvement is preferred. Core assumption: Critic networks provide meaningful value estimates across overlapping state regions; value scales differ but relative improvements correlate with local performance advantage. Break condition: If critics are poorly calibrated or divergent across policies, normalized ratios become unreliable indicators.

### Mechanism 2: Geometric Decay Ensures Asymptotic Commitment
Geometrically decaying acceptance probability ensures the combined policy eventually commits to the guaranteed-stable alternative policy almost surely. The base policy is selected with probability ρt(St) × Indicator, where ρt(s) = λ^t × p_relax with λ ∈ (0,1). Since Σρt < ∞, the Borel-Cantelli lemma guarantees only finitely many base-policy selections, so the alternative policy dominates asymptotically. Core assumption: The alternative policy πa satisfies the ε-improbable goal-reaching property; the decaying schedule is slow enough to allow meaningful performance gains before commitment. Break condition: If λ is too close to 1, convergence to the alternative policy may be impractically slow.

### Mechanism 3: Dynamic Reference Value Tracking
Dynamic reference value tracking adapts selection criteria to each policy's observed peak performance, improving selection relevance. Maintain v†_base and v†_alt as running best-observed critic values. When Δbase ≥ ν (threshold), update v†_base := v̂πb(St); similarly for v†_alt. This shifts the baseline for future improvement comparisons. Core assumption: Threshold ν is set appropriately—large enough to avoid noise-driven updates, small enough to capture genuine performance shifts. Break condition: If ν is too small, noisy critic estimates cause erratic reference updates, destabilizing the selection signal.

## Foundational Learning

- **Lyapunov Stability and KL Functions**: Theorems 1-2 rely on the alternative policy having a goal-reaching certificate β ∈ KL. Given a KL function β(d, t), can you sketch the bound it places on distance-to-goal over time for a fixed initial distance?
- **Critic Functions as Value Estimates**: The entire selection mechanism depends on critic networks v̂πb and v̂πa producing comparable relative improvements. Why might two critics trained on different policies assign systematically different value scales to the same state?
- **Borel-Cantelli Lemma and Almost-Sure Finiteness**: The proof that the algorithm eventually commits to the alternative policy hinges on showing Σρt < ∞ implies finitely many base-policy selections almost surely. If ρt = 1/t, does the summability condition hold? What about ρt = 1/t²?

## Architecture Onboarding

- **Component map**: Base policy πb + critic v̂πb -> Alternative policy πa + critic v̂πa -> Reference tracker (v†_base, v†_alt) -> Selection router -> Decay scheduler
- **Critical path**: 1) Observe state St 2) Query both critics: v̂πb(St), v̂πa(St) 3) Compute improvements: Δbase = v̂πb(St) - v†_base, Δalt = v̂πa(St) - v†_alt 4) Compute indicator: 1 if Δbase/v†_base > Δalt/v†_alt, else 0 5) Sample Ut ~ Uniform[0,1]; if Ut < ρt(St) × Indicator, select from πb, else from πa 6) Optionally update reference values if Δ ≥ ν 7) Execute action, observe next state
- **Design tradeoffs**: λ (decay rate): Higher λ extends base-policy exploitation window, improving short-horizon performance but delaying stability commitment. p_relax (initial acceptance): Controls peak probability of selecting base policy. ν (update threshold): Small ν makes selection more adaptive but noise-sensitive.
- **Failure signatures**: High variance with no mean improvement (indicator oscillates rapidly); performance identical to πa (ρt decaying too fast); instability or goal divergence (πa doesn't satisfy goal-reaching property); reference values stuck at initialization (ν too large).
- **First 3 experiments**: 1) Sanity check: Run πb, πa, and Multi-CALF separately on Hopper; confirm Multi-CALF achieves returns ≥ max(πb, πa) over ≥20 seeds. 2) Ablation on λ and p_relax: Sweep λ ∈ {0.9, 0.95, 0.99, 0.999} and p_relax ∈ {0.5, 0.8, 1.0}; measure mean return, variance, and empirical convergence time. 3) Critic calibration check: For fixed states, plot v̂πb vs. v̂πa scatter; compute correlation of normalized improvements.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can Multi-CALF be extended to combine more than two policies while preserving stability guarantees? The current theoretical analysis only addresses binary policy selection; extending to K policies introduces combinatorial complexity requiring new proof techniques.

- **Open Question 2**: How can complementary policies be automatically identified for a given domain without manual selection? The paper manually selects TD3 and PPO but provides no systematic method for determining when two policies will exhibit synergistic behavior.

## Limitations
- The alternative policy's ε-improbable goal-reaching property is assumed but not verified for PPO in the experiments
- Results are from a single environment (Hopper), limiting generalizability claims
- Theoretical guarantees assume perfect critic networks, but real-world critic errors could compromise selection mechanism

## Confidence
- **High confidence**: The mechanism of geometrically decaying acceptance probability ensuring eventual commitment to the alternative policy (Borel-Cantelli argument is mathematically sound)
- **Medium confidence**: The normalized critic improvement selection mechanism produces meaningful policy comparisons (requires calibrated critics with overlapping state visitation)
- **Medium confidence**: Multi-CALF achieves better mean performance than either constituent policy on Hopper (empirical result supported by data, but single-environment limitation exists)

## Next Checks
1. **ε-improbable goal-reaching verification**: Implement a method to verify whether the PPO policy actually satisfies the ε-improbable goal-reaching property on Hopper. Define the goal set G explicitly and compute or estimate ε.

2. **Sensitivity analysis on ν threshold**: Systematically vary ν ∈ {0.001, 0.01, 0.1, 0.5} and measure impact on performance, variance, and reference value stability. Plot reference value trajectories over time for different ν values.

3. **Cross-environment generalization**: Evaluate Multi-CALF on at least two additional continuous control tasks (e.g., Walker2d, HalfCheetah) with the same λ=0.99, p_relax=0.8 configuration. Compare mean returns and stability metrics across environments.