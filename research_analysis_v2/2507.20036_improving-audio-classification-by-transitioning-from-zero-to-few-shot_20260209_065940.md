---
ver: rpa2
title: Improving Audio Classification by Transitioning from Zero- to Few-Shot
arxiv_id: '2507.20036'
source_url: https://arxiv.org/abs/2507.20036
tags:
- audio
- embeddings
- classification
- class
- few-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitations of zero-shot audio classification,
  which relies on noisy text embeddings for audio class descriptions. The authors
  propose transitioning to a few-shot approach by replacing text embeddings with audio-based
  embeddings computed from a small set of audio samples per class.
---

# Improving Audio Classification by Transitioning from Zero- to Few-Shot

## Quick Facts
- arXiv ID: 2507.20036
- Source URL: https://arxiv.org/abs/2507.20036
- Reference count: 0
- Primary result: Few-shot methods with 10-20 audio samples per class outperform zero-shot text-based approaches by 2-10 percentage points

## Executive Summary
This paper addresses the limitations of zero-shot audio classification, which relies on noisy text embeddings for audio class descriptions. The authors propose transitioning to a few-shot approach by replacing text embeddings with audio-based embeddings computed from a small set of audio samples per class. They evaluate methods including averaging, linear discriminant analysis (LDA), and mutual information-based feature selection combined with averaging or LDA. Experiments on three datasets (BBL, ESC-50, FSD50K) show that few-shot methods consistently outperform zero-shot baselines, with accuracy improvements ranging from 2-10 percentage points.

## Method Summary
The authors propose a few-shot audio classification approach that computes class prototypes directly from audio samples rather than text descriptions. For each class, they extract embeddings from a pretrained CLAP audio encoder for a small set of labeled audio samples (10-20 per class). These embeddings are processed using averaging, LDA, or mutual information-based feature selection combined with averaging or LDA to form class prototypes. Test samples are classified by computing cosine similarity between their embeddings and the class prototypes, selecting the nearest neighbor. The approach requires minimal labeled data while providing significant accuracy improvements over zero-shot methods.

## Key Results
- FS|Ec|_AVG with |E_c|=10 achieves 0.678 accuracy on BBL compared to 0.623 for zero-shot baseline
- FS|Ec|_AVG with |E_c|=10 achieves 0.970 accuracy on ESC-50 compared to 0.948 for zero-shot baseline
- FS|Ec|_AVG consistently outperforms zero-shot baselines across all datasets and |E_c| values tested
- Mutual information feature selection provides modest additional benefits, particularly when combined with averaging

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing text embeddings with averaged audio embeddings improves classification accuracy by providing class prototypes that directly represent acoustic characteristics rather than linguistic approximations.
- Mechanism: Audio embeddings from the same class are extracted using a pretrained CLAP encoder, then averaged to form a class prototype e_c. Test samples are classified by finding the nearest prototype using cosine similarity. This bypasses the variability introduced by text prompt selection.
- Core assumption: The pretrained CLAP audio encoder produces embeddings where same-class sounds cluster together sufficiently for averaging to yield meaningful prototypes.
- Evidence anchors:
  - [abstract] "Specifically, audio embeddings are grouped by class and processed to replace the inherently noisy text embeddings."
  - [section 5.1] "For FS|Ec|_AVG, the performance is always better than the zero-shot baselines."
  - [corpus] Weak direct corpus support; neighbor paper "VocSim" addresses zero-shot content identity but does not validate this specific few-shot averaging mechanism.
- Break condition: If the encoder has not been trained on acoustically diverse examples of your target classes, averaging may produce meaningless centroids.

### Mechanism 2
- Claim: Simple averaging outperforms Linear Discriminant Analysis for few-shot audio classification when embedding dimensionality is high relative to the number of class samples.
- Mechanism: LDA computes class boundaries using covariance estimates that become unreliable in high dimensions with few samples (curse of dimensionality). Averaging ignores covariance and only estimates class means, which is more robust with limited data.
- Core assumption: The CLAP embedding dimension (L=1024 per the paper's reference to 1024 features) is high enough to cause LDA instability with |E_c| = 10-50 samples per class.
- Evidence anchors:
  - [section 5.1] "We assume that AVG is preferable over LDA here, as the audio embeddings are high-dimensional, and the curse of dimensionality reduces the performance of LDA."
  - [section 5.2] "For MI+LDA, increasing K does not automatically lead to increased performance. LDA has difficulties handling high-dimensional embeddings."
  - [corpus] No direct corpus validation of this specific LDA vs. averaging comparison in audio few-shot settings.
- Break condition: If |E_c| approaches or exceeds the embedding dimensionality, LDA may become viable again.

### Mechanism 3
- Claim: Mutual information-based feature selection provides modest improvements for averaging-based classification but does not fully rescue LDA performance.
- Mechanism: MI ranks features by their dependency on class labels, retaining only the K×|C| most informative dimensions. For averaging, this removes noisy dimensions. For LDA, it mitigates but does not solve the curse of dimensionality.
- Core assumption: Not all 1024 CLAP embedding dimensions are equally relevant for distinguishing any given set of classes.
- Evidence anchors:
  - [section 5.2] "For the largest K, around 700 to 800 features are selected. This suggests that most features are relevant to distinguish between audio classes."
  - [table 1] FS|Ec|_MI+AVG shows marginal gains over FS|Ec|_AVG (e.g., 0.681 vs. 0.678 on BBL with |E_c|=10).
  - [corpus] No corpus papers validate MI-based feature selection for few-shot audio classification.
- Break condition: If classes are acoustically similar, MI may retain features that distinguish them but still be insufficient without more training samples.

## Foundational Learning

- Concept: Contrastive Language-Audio Pretraining (CLAP)
  - Why needed here: The entire few-shot approach depends on a pretrained audio encoder that maps acoustically similar sounds to nearby embeddings. Without understanding CLAP's contrastive training objective, you cannot diagnose why certain classes cluster poorly.
  - Quick check question: Can you explain why cosine similarity is used as the distance metric rather than Euclidean distance for CLAP-derived embeddings?

- Concept: Curse of Dimensionality in Linear Classifiers
  - Why needed here: The paper's central empirical finding—that averaging beats LDA—only makes sense if you understand why high-dimensional spaces with few samples make covariance estimation unreliable.
  - Quick check question: If you have 10 samples per class in a 1024-dimensional space, why would computing a full covariance matrix for LDA be problematic?

- Concept: Zero-Shot vs. Few-Shot Classification Paradigms
  - Why needed here: The paper positions itself as a transition from zero-shot to few-shot. Understanding the tradeoff—zero-shot requires no labeled data but uses noisy text proxies; few-shot requires minimal labels but uses actual audio—is essential for deciding when to apply this method.
  - Quick check question: What is the minimum labeled data requirement before this few-shot approach becomes viable, and how does accuracy scale with |E_c|?

## Architecture Onboarding

- Component map:
  CLAP Audio Encoder (frozen) -> Prototype Computation (AVG/LDA/MI+AVG/MI+LDA) -> Distance Metric (cosine) -> Classification (nearest neighbor)

- Critical path:
  1. Collect |E_c| labeled audio samples per class (10-20 recommended minimum)
  2. Extract embeddings using CLAP 2023 audio encoder
  3. Compute class prototypes via averaging (normalize or not—limited impact per paper)
  4. At inference, extract test embedding, compute cosine similarity to all prototypes, return nearest

- Design tradeoffs:
  - Averaging vs. LDA: Averaging is simpler, more robust with few samples, and performed best. Use LDA only if |E_c| ≥ 50 and you can validate it helps.
  - MI feature selection: Adds complexity for ~0-3 percentage point gains. Only worth implementing if you have many classes and need to squeeze performance.
  - Embedding normalization: Paper found "limited influence." Default to unnormalized for simplicity.

- Failure signatures:
  - Accuracy degrades on acoustically similar classes (e.g., "car" vs. "car cabin" in BBL)—audio embeddings alone may not disambiguate context-dependent categories.
  - High variance across runs with |E_c| < 5—insufficient samples for stable prototypes.
  - LDA underperforms averaging on your validation set—curse of dimensionality is active; reduce dimensionality or switch to averaging.

- First 3 experiments:
  1. Replicate zero-shot baseline using CLAP 2023 with text prompts matching your class names; establish a baseline accuracy.
  2. Implement FS|Ec|_AVG with |E_c|=10 using randomly sampled training examples; measure accuracy gain over zero-shot.
  3. Sweep |E_c| ∈ {5, 10, 20, 50} and plot accuracy with confidence intervals; verify diminishing returns and identify minimum viable |E_c| for your use case.

## Open Questions the Paper Calls Out
- Can the proposed few-shot approach be effectively extended to hierarchical classification tasks? The conclusion states, "Future work could explore extending this approach to more complex tasks, such as hierarchical classification..." The current evaluation treats class sets as flat labels.
- How can few-shot audio classification methods be adapted to distinguish overlapping or concurrent sound events? The conclusion identifies "scenarios involving overlapping and concurrent sound events" as a target for future work to advance few-shot capabilities.
- Can non-linear classifiers or advanced dimensionality reduction techniques outperform simple averaging in high-dimensional audio embedding spaces? The authors observe that LDA underperforms compared to averaging, likely due to the "curse of dimensionality," but only explored linear methods.

## Limitations
- The BBL dataset used in experiments is not publicly available, limiting reproducibility for one of three datasets.
- The CLAP 2023 audio encoder version and specific preprocessing parameters are not fully specified.
- For FSD50K, the method of selecting samples with "least class overlap" is not detailed.

## Confidence
- High confidence: The core finding that averaging audio embeddings outperforms zero-shot text-based methods is well-supported by consistent improvements across all three datasets.
- Medium confidence: The claim that MI feature selection provides modest improvements is supported by the data, but the effect size (0-3 percentage points) suggests the benefit may be dataset-dependent.
- Medium confidence: The recommendation to avoid LDA for high-dimensional embeddings with few samples is reasonable, but the paper doesn't explore dimensionality reduction alternatives that might make LDA viable at small |E_c|.

## Next Checks
1. Implement FS|Ec|_AVG with |E_c|=10 on ESC-50 and verify the claimed 0.970 accuracy against the zero-shot baseline of 0.948.
2. Conduct ablation studies varying |E_c| from 5 to 50 samples per class, measuring accuracy and computing 95% confidence intervals across multiple random seeds.
3. Test the paper's claim about "limited influence" of embedding normalization by comparing cosine similarity classification performance with and without L2 normalization of class prototypes.