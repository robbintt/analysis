---
ver: rpa2
title: 'Sunflower: A New Approach To Expanding Coverage of African Languages in Large
  Language Models'
arxiv_id: '2510.07203'
source_url: https://arxiv.org/abs/2510.07203
tags:
- languages
- language
- arxiv
- sunflower
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Sunflower, a pair of large language models
  (14B and 32B parameters) focused on Ugandan languages. The core idea is that instead
  of building a massive multilingual model covering many languages globally, it is
  more effective to build a smaller model focused on a single region with high linguistic
  diversity.
---

# Sunflower: A New Approach To Expanding Coverage of African Languages in Large Language Models

## Quick Facts
- **arXiv ID:** 2510.07203
- **Source URL:** https://arxiv.org/abs/2510.07203
- **Reference count:** 40
- **Primary result:** Sunflower-32B achieves state-of-the-art translation accuracy for Ugandan languages with mean chrF score of 0.435 (xx→eng), outperforming GPT-4o and Gemini 2.5 Pro on 24 out of 31 Ugandan languages tested.

## Executive Summary
This paper introduces Sunflower, a pair of large language models (14B and 32B parameters) focused specifically on Ugandan languages. Rather than building massive multilingual models covering many global languages, the authors demonstrate that smaller, regionally-focused models achieve superior performance for local language tasks. The models are built by fine-tuning Qwen 3 with extensive data collection including digitized books, transcribed radio, and community-sourced materials in over 40 Ugandan languages. The primary contribution is achieving state-of-the-art translation accuracy for low-resource African languages, with Sunflower-32B achieving the highest mean chrF score (0.435) for translation from local languages to English and outperforming alternatives like GPT-4o and Gemini 2.5 Pro on most Ugandan languages tested.

## Method Summary
Sunflower is built through a three-phase training pipeline starting from Qwen 3 base models (14B or 32B parameters). First, continued pretraining on ~0.75B characters of Ugandan language data (including OCR'd books, transcribed radio, and web crawls) injects regional vocabulary and cultural knowledge. Second, instruction fine-tuning with LoRA adapters (rank 16) adds translation and conversational capabilities while preventing catastrophic forgetting through response-only loss masking. Third, Iterative Reasoning Preference Optimization (DPO variant) reduces hallucination and glitching behaviors through preference pairs for factuality and repetition avoidance. The entire pipeline uses DeepSpeed ZeRO-3 for distributed training, with carefully tuned hyperparameters including batch size 32,768 tokens and learning rate 1e-4 for pretraining, 1e-5 for SFT.

## Key Results
- Sunflower-32B achieves mean chrF score of 0.435 for xx→eng translation, outperforming GPT-4o (0.354) by 23% relative improvement
- On 24 out of 31 Ugandan languages tested, Sunflower-32B outperforms both GPT-4o and Gemini 2.5 Pro in translation accuracy
- Sunflower-14B achieves competitive performance with mean chrF of 0.366 for eng→xx translation
- Models show strong zero-shot generalization to 8 additional Ugandan languages not seen during training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Regional language grouping enables more efficient cross-lingual transfer than global multilingual models.
- Mechanism: Languages within Uganda (Bantu, Nilotic, Central Sudanic families) share structural features (agglutinative morphology), regional phonetic overlaps, and culturally-specific entities. Focused training allows denser parameter allocation for these shared patterns rather than diluting capacity across 100+ global languages.
- Core assumption: Related languages benefit from positive transfer during continued pretraining.
- Evidence anchors:
  - [abstract] "By focusing on languages within a single region, we leverage shared linguistic structures, cultural concepts, and contextual knowledge."
  - [section 2.6] "We exploit the linguistic and contextual coherence inherent across Uganda's 40+ languages... enabling more efficient parameter sharing and robust cross-lingual transfer."
  - [corpus] Related work on African language NLP (AfroScope, AfriqueLLM) discusses low-resource challenges but does not directly test regional vs. global model efficiency claims.
- Break condition: If target languages are linguistically unrelated (e.g., mixing Khoisan with Bantu), transfer benefits would diminish substantially.

### Mechanism 2
- Claim: Non-web data sources (radio, print, community archives) recover signal lost to standard crawl-based collection.
- Mechanism: Ugandan language content exists largely in non-digital formats. OCR digitization of books + Whisper transcription of 500+ hours of Simba FM radio + community-sourced cultural documents yields ~1B characters of pretraining text that global crawlers never see.
- Core assumption: Local partnerships uniquely unlock proprietary/physical archives inaccessible to remote teams.
- Evidence anchors:
  - [abstract] "A team who are physically located in the region being studied is in a strong position to form networks of data contributors."
  - [section 3] "Significant data on Ugandan languages had to be collected from printed materials, audio recordings, and cultural archives, not being easily accessible via the web."
  - [corpus] Related papers on African ASR and NLP consistently identify data scarcity as the primary bottleneck but do not evaluate alternative collection strategies.
- Break condition: If OCR/transcription error rates exceed signal gain, or if data volume remains below critical threshold for each language.

### Mechanism 3
- Claim: Three-phase training (continued pretraining → SFT with LoRA → DPO) preserves base capabilities while adding regional competence.
- Mechanism: (1) Next-token prediction on ~0.75B characters of cleaned regional text injects vocabulary and cultural knowledge; (2) LoRA rank-16 adapters enable instruction-following without full fine-tuning; (3) Iterative Reasoning Preference Optimization reduces glitching/hallucination while maintaining translation accuracy.
- Core assumption: Qwen 3's multilingual representations are sufficiently transferable to serve as a base.
- Evidence anchors:
  - [section 5.1-5.3] Detailed hyperparameters: batch size 32,768 tokens, lr 1e-4 for pretraining; LoRA rank 16, lr 1e-5 for SFT; α_RPO=1.0 for DPO variant.
  - [section 6.1] Sunflower-32B achieves 0.435 mean chrF (xx→eng) vs. GPT-4o's 0.354—a 23% relative improvement.
  - [corpus] No corpus papers validate this specific training pipeline; mechanism remains paper-internal.
- Break condition: If learning rate or batch size miscalibrated (paper notes sensitivity), catastrophic forgetting or poor convergence occurs.

## Foundational Learning

- Concept: **chrF (character n-gram F-score)**
  - Why needed here: Primary evaluation metric for morphologically rich languages where word-boundary metrics like BLEU underperform.
  - Quick check question: Why does chrF often outperform BLEU for agglutinative languages like Luganda?

- Concept: **LoRA (Low-Rank Adaptation)**
  - Why needed here: Enables fine-tuning 14B/32B models on limited VRAM by learning low-rank decomposition rather than full weight updates.
  - Quick check question: What is the tradeoff between LoRA rank size and task-specific plasticity?

- Concept: **Continued pretraining vs. fine-tuning from scratch**
  - Why needed here: Sunflower builds on Qwen 3 rather than training new models; understanding when this works is critical for replication.
  - Quick check question: Why does the paper include instruction examples in the pretraining corpus (to prevent what failure mode)?

## Architecture Onboarding

Qwen 3 Base Model (14B/32B) -> Continued Pretraining (~0.75B chars, 1 epoch, lr 1e-4) -> LoRA Adapter (rank 16, SFT on translation + QA) -> DPO/IRPO (preference pairs for factuality & glitching) -> Sunflower Final

- **Critical path:**
  1. Data collection and cleaning (OCR/transcription quality gates)
  2. Pretraining hyperparameter sweep (lr and batch size highly sensitive per paper)
  3. SFT with response-only loss (prevents instruction repetition)
  4. DPO with α_RPO=1.0 (stabilizes training vs. vanilla DPO)

- **Design tradeoffs:**
  - 14B vs. 32B: 14B faster to train, competitive eng→xx performance (0.366 vs 0.357 chrF), but 32B superior on xx→eng (0.435 vs 0.419).
  - Single-epoch pretraining: Limits overfitting risk but may underutilize data.
  - Regional focus: Higher accuracy for 24/31 Ugandan languages but no claims for other African regions.

- **Failure signatures:**
  - Model replies in wrong language (fallback to higher-resource related language; e.g., Dopadhola → Acholi).
  - Infinite repetition loops ("glitching")—mitigated by DPO but not eliminated.
  - Single-word translation errors due to missing disambiguation context.

- **First 3 experiments:**
  1. Replicate continued pretraining on a 1-language subset (e.g., Luganda only) to isolate transfer vs. direct learning effects.
  2. Ablate radio transcripts vs. digitized books to quantify contribution of non-web sources.
  3. Test zero-shot transfer to a held-out Ugandan language (not in training set) to probe generalization bounds.

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: Can reinforcement learning techniques close the performance gap between regionally-specialized models and proprietary models on knowledge and reasoning benchmarks like AfriMMLU?
- Basis in paper: [explicit] The authors state: "Several methods such as alternative reinforcement learning techniques are available to try to close this gap, but we leave this to future work" (Section 6.1.1), noting Sunflower-32B achieves 41.7% versus GPT-4o's 64.8% on AfriMMLU.
- Why unresolved: The current training prioritized translation performance; knowledge/reasoning capabilities remain "relatively unoptimised" for multiple-choice tasks.
- What evidence would resolve it: Systematic comparison of RL methods (PPO vs. DPO variants vs. RLAIF) showing statistically significant improvements on AfriMMLU scores while maintaining translation quality.

**Open Question 2**
- Question: Does retrieval-augmented generation with web search effectively improve factual grounding for low-resource Ugandan language queries?
- Basis in paper: [explicit] The authors mention: "Future work will also focus on grounding the model responses: for example, we have had encouraging initial results with incorporating web search results while generating answers" (Section 7).
- Why unresolved: Only "encouraging initial results" are reported; no quantitative evaluation of retrieval augmentation was conducted.
- What evidence would resolve it: A controlled study measuring factual accuracy and hallucination rates with and without retrieval augmentation across Ugandan languages.

**Open Question 3**
- Question: Can the regional specialization methodology be replicated effectively in other countries or regions with high linguistic diversity?
- Basis in paper: [explicit] The authors state: "In principle, the steps that we have outlined here could be carried out in any country, even those with very high linguistic diversity" (Section 7), but no replication data exists.
- Why unresolved: The paper provides only a single case study (Uganda); the generalizability of the approach remains untested.
- What evidence would resolve it: Successful training of models for other linguistically diverse regions (e.g., Nigeria, Cameroon) using the same methodology, with comparable performance gains over global baselines.

## Limitations

- **Data accessibility uncertainty:** The claim that local partnerships uniquely unlock proprietary/physical archives inaccessible to remote teams rests on undocumented operational details, with no quantification of what fraction of pretraining data came from truly non-crawlable sources.

- **Generalization boundary:** The paper demonstrates state-of-the-art performance on 24 out of 31 Ugandan languages tested, but makes no claims about performance on other African regions, limiting confidence in cross-regional applicability.

- **Mechanism validation gap:** While the three-phase training pipeline is detailed, no ablation studies are provided to isolate the contribution of each component, particularly the impact of Iterative Reasoning Preference Optimization versus standard DPO.

## Confidence

**High confidence:** Translation accuracy claims (chrF scores of 0.435 for Sunflower-32B vs. 0.354 for GPT-4o) are supported by specific evaluation metrics and direct comparisons on held-out test sets from SALT and FLORES-200.

**Medium confidence:** The regional language grouping efficiency claim (Mechanism 1) is logically sound given linguistic relatedness within Uganda, but lacks direct experimental comparison against global multilingual models of equivalent parameter count trained on the same Ugandan language data.

**Low confidence:** The assertion that local physical presence is essential for data collection (Mechanism 2) cannot be independently verified without detailed accounting of data sources and access methods.

## Next Checks

1. **Ablation study of training components:** Train identical models omitting each of the three phases (pretraining, SFT, DPO) to quantify their individual contributions to the 0.435 chrF score, particularly isolating the impact of Iterative Reasoning Preference Optimization versus standard DPO.

2. **Cross-regional generalization test:** Evaluate Sunflower-32B on a held-out set of languages from a different African region (e.g., Ethiopian Semitic or Southern African Bantu languages) to test whether the regional specialization creates negative transfer or performance degradation outside Uganda.

3. **Data source contribution analysis:** Conduct controlled experiments where web-crawled data is replaced with equivalent amounts from non-web sources (radio transcripts, digitized books) to measure the actual contribution of the "non-digital" collection strategy to final model performance.