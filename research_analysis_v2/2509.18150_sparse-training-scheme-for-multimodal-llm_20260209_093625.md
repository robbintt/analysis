---
ver: rpa2
title: Sparse Training Scheme for Multimodal LLM
arxiv_id: '2509.18150'
source_url: https://arxiv.org/abs/2509.18150
tags:
- training
- visual
- language
- mllms
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational inefficiency of training
  Multimodal Large Language Models (MLLMs), which arises from long input sequences
  due to multimodal data and low utilization of inter-layer computations. To tackle
  this, the authors propose a Sparse Training Scheme (STS) that introduces sparsity
  into visual token representations and decoder layers.
---

# Sparse Training Scheme for Multimodal LLM

## Quick Facts
- arXiv ID: 2509.18150
- Source URL: https://arxiv.org/abs/2509.18150
- Reference count: 0
- Primary result: Achieves >15% FLOPs reduction while retaining 96.9-99.7% of baseline accuracy across five multimodal benchmarks

## Executive Summary
This paper addresses the computational inefficiency of training Multimodal Large Language Models (MLLMs) through visual token redundancy and underutilized inter-layer computations. The authors propose a Sparse Training Scheme (STS) that introduces sparsity into visual token representations and decoder layers. STS achieves over 15% FLOPs reduction while maintaining 96.9-99.7% of baseline accuracy across multiple benchmarks including GQA, VQA v2, SQA, POPE, and MME. The approach provides a practical, scalable solution for efficient MLLM training with minimal performance loss.

## Method Summary
STS consists of two key components: Visual Token Compressor (VTC) and Layer Dynamic Skipper (LDS). VTC compresses visual token sequences during modality alignment by selecting a subset of tokens using uniform, random, or instruction-guided sampling. LDS dynamically skips decoder layers during instruction fine-tuning based on step-based and depth-based probabilities. The method is applied in two stages: VTC only during alignment (LLM frozen) and LDS only during fine-tuning (LLM updated), each for one epoch. The skip probability for LDS is calculated as p_l(e) = α((E−e)/E)^2 × (1 + ε(2l−(L−1))/(L−1)) with default parameters α=0.5 and ε=0.5.

## Key Results
- Achieves over 15% FLOPs reduction across multiple MLLM architectures
- Maintains 96.9-99.7% of baseline accuracy on GQA, VQA v2, SQA, POPE, and MME benchmarks
- Demonstrates task-specific sensitivity: uniform sampling preserves OCR/counting better, while instruction-guided improves reasoning
- Shows significant efficiency gains with minimal performance degradation

## Why This Works (Mechanism)

### Mechanism 1: Visual Token Redundancy Elimination
Reducing visual token sequence length via compression minimizes data redundancy without destroying global semantic structures, lowering compute costs during modality alignment. VTC applies a selection mask to projected visual tokens, discarding a percentage via uniform sampling to process shorter sequences.

### Mechanism 2: Coarse-to-Fine Gradient Routing
Early training phases are robust to layer skipping because gradients are large and "coarse," allowing significant computational reduction without convergence loss. LDS uses step-based decay function that maintains high skip probability early in training and reduces it to zero as training concludes.

### Mechanism 3: Depth-Aware Layer Utilization
Deeper layers in LLMs are often less critical for feature extraction than shallow layers and can be skipped more frequently. LDS modulates skip probability based on layer index, increasing skip probability for deeper layers while protecting shallower layers.

## Foundational Learning

- **Concept:** Autoregressive Training vs. Modality Alignment
  - **Why needed here:** STS applies different strategies to different stages. You must distinguish "alignment" phase (training the connector) from "fine-tuning" (training the LLM).
  - **Quick check question:** Is the LLM backbone frozen or unfrozen in the current step? (STS uses VTC if frozen, LDS if unfrozen).

- **Concept:** Attention Complexity (Quadratic Scaling)
  - **Why needed here:** VTC targets sequence length N. Understanding that attention cost scales as O(N^2) clarifies why dropping 20% of tokens yields non-linear FLOPs savings.
  - **Quick check question:** If you reduce tokens from 1024 to 512, does the attention FLOPs drop by 50% or 75%?

- **Concept:** Implicit Regularization
  - **Why needed here:** Skipping layers acts as a regularizer. Understanding this explains why STS sometimes matches or exceeds baseline accuracy despite fewer updates.
  - **Quick check question:** Why might a "noisy" training path (skipping layers) prevent the model from overfitting specific instruction patterns?

## Architecture Onboarding

- **Component map:** Image -> Vision Encoder -> Projector -> [VTC] -> LLM Input Embeddings -> Decoder Stack -> [LDS] per layer -> Output
- **Critical path:** Implement VTC as mask/selector after visual projector but before LLM forward pass. Implement LDS as wrapper around Decoder Layer's forward function. Calculate skip probability at start of each forward pass using current step and layer depth.
- **Design tradeoffs:** Uniform vs. Instruction-Guided VTC (uniform preserves OCR/Counting better; instruction-guided boosts reasoning but hurts OCR). Aggressive Skipping (faster but risks losing fine-grained reasoning). Memory vs. Compute (VTC reduces memory; LDS primarily reduces compute).
- **Failure signatures:** Sudden Loss Spike (check LDS implementation; skipping residual connection incorrectly will break gradient path). Degraded OCR Performance (check VTC strategy; likely dropping too many tokens or using non-uniform sampling). Slow Convergence (skip rate may be too high for dataset size; decay happening too fast).
- **First 3 experiments:** 1) Sanity Check (VTC): Train baseline vs. VTC-only on small subset with Uniform Sampling. Verify token length reduction and loss stability. 2) Sanity Check (LDS): Run LDS-only with α=0.5, ε=0.5. Plot layer utilization to verify depth-based scaling. 3) Regression Test: Run full STS on benchmark with heavy OCR to ensure VTC configuration preserves text density.

## Open Questions the Paper Calls Out

### Open Question 1
Can a text-guided visual token compression strategy be designed to maintain performance on fine-grained tasks like OCR? Table 2 shows instruction-guided sampling improves reasoning but drastically reduces OCR accuracy, suggesting current relevance-based filtering discards necessary high-frequency visual details.

### Open Question 2
Is the fixed "coarse-to-fine" schedule in LDS optimal for all model architectures and training durations? The specific decay function is manually designed and hyperparameter-dependent; it's unclear if this static rule adapts well to varying model sizes or different convergence rates.

### Open Question 3
Does applying LDS during modality alignment (pre-training) stage improve efficiency without destabilizing alignment? The paper restricts LDS to instruction fine-tuning stage but doesn't explain why skipping layers is omitted during the compute-intensive pre-training phase where the LLM is usually frozen.

## Limitations
- Critical hyperparameters (VTC retention ratio, training schedule parameters, backward pass implementation) are not specified, blocking faithful reproduction
- Task-specific performance trade-offs exist between uniform and instruction-guided sampling strategies
- Method assumes deeper layers are less critical for feature extraction, which may not hold for complex reasoning tasks

## Confidence

**High Confidence Claims:**
- The general framework of combining visual token compression with layer skipping is technically sound
- The observed FLOPs reduction of over 15% is likely achievable given the described mechanisms
- The accuracy retention range of 96.9-99.7% relative to baseline is plausible for the evaluated tasks

**Medium Confidence Claims:**
- The effectiveness of the step-based decay function for LDS is supported by general training dynamics literature
- The claim that early training phases are robust to layer skipping relies on general LLM observations
- The assertion that shallow layers are more effective at feature extraction than deeper layers is a common architectural assumption

**Low Confidence Claims:**
- The universality of the proposed configuration (α=0.5, ε=0.5) across different MLLM architectures
- The claim that STS "sometimes matches or exceeds baseline accuracy" may be task-dependent
- The absence of detailed ablation studies on VTC compression rate and LDS skipping rate interaction

## Next Checks

**Check 1: Hyperparameter Sensitivity Analysis**
Implement grid search over VTC retention ratios (p ∈ {0.3, 0.5, 0.7, 0.9}) and LDS parameters (α ∈ {0.3, 0.5, 0.7}, ε ∈ {0.3, 0.5, 0.7}) on small subset of GQA. Measure FLOPs reduction, accuracy retention, and task-specific performance degradation.

**Check 2: Cross-Architecture Generalization Test**
Evaluate STS on at least two different MLLM architectures (e.g., LLaVA-7B and MiniGPT-4) using identical configurations. Compare FLOPs reduction and accuracy retention across architectures to assess generalization.

**Check 3: Long-Training Stability Assessment**
Extend training duration beyond single epoch to 3-5 epochs while monitoring convergence behavior. Track per-layer utilization patterns, gradient norms, and accuracy trajectories to verify step-based decay function maintains stability throughout extended training.