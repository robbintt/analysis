---
ver: rpa2
title: Expected Return Causes Outcome-Level Mode Collapse in Reinforcement Learning
  and How to Fix It with Inverse Probability Scaling
arxiv_id: '2601.21669'
source_url: https://arxiv.org/abs/2601.21669
tags:
- uni00000013
- collapse
- reward
- uni00000003
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper shows that outcome-level mode collapse in reinforcement\
  \ learning is not caused by insufficient exploration but is a structural consequence\
  \ of the expected-return objective itself. Under idealized learning dynamics, the\
  \ log-probability ratio between outcomes evolves linearly in their reward difference,\
  \ causing exponential divergence and inevitable collapse\u2014regardless of exploration,\
  \ entropy regularization, or optimization algorithm."
---

# Expected Return Causes Outcome-Level Mode Collapse in Reinforcement Learning and How to Fix It with Inverse Probability Scaling

## Quick Facts
- **arXiv ID:** 2601.21669
- **Source URL:** https://arxiv.org/abs/2601.21669
- **Reference count:** 40
- **Primary result:** IPS-GRPO reduces outcome-level mode collapse by correcting the objective, not just exploration.

## Executive Summary
This paper identifies outcome-level mode collapse in reinforcement learning as a structural flaw in the expected-return objective, not an exploration deficiency. The standard RL update amplifies frequency differences via the probability multiplier, causing inevitable collapse even with perfect exploration. To fix this, the authors propose Inverse Probability Scaling (IPS), which removes outcome-frequency amplification by scaling terminal rewards by the inverse outcome probability. IPS is instantiated as IPS-GRPO, a drop-in modification to GRPO that requires no auxiliary models. Experiments across grid worlds, reasoning benchmarks, and molecular generation show IPS-GRPO consistently reduces mode collapse while matching or exceeding baseline performance.

## Method Summary
IPS rescales terminal rewards by the inverse outcome probability: $r(o)/p_\theta(o)$. Since $p_\theta(o)$ is unknown, IPS-GRPO estimates it via empirical frequency $\hat{p}(o)$ within a group of $G$ trajectories and clips to avoid extreme values. This changes the learning dynamics to balance probabilities with rewards, yielding a stationary distribution $p^*(o) \propto r(o)$. IPS-GRPO modifies the reward in the GRPO update, preserving the group-based advantage computation while correcting the probability multiplier issue.

## Key Results
- IPS-GRPO reduces outcome-level mode collapse in multimodal settings where standard RL fails.
- On the HypoSpace reasoning benchmark, IPS-GRPO achieves up to 90% recovery rate vs. 32% for GRPO.
- In molecular generation, IPS-GRPO discovers more unique high-reward molecules with fewer oracle calls.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Outcome-level mode collapse is a structural inevitability of expected-return maximization, not merely an exploration deficiency.
- **Mechanism:** In standard RL, the objective $J(\theta) = \sum p_\theta(o)r(o)$ multiplies the reward by the outcome probability. The update signal for an outcome is proportional to $p_\theta(o) \times \text{advantage}(o)$. Even if two outcomes have equal rewards (and thus equal advantages), the outcome with a slightly higher initial probability receives a disproportionately larger update. This "rich-get-richer" feedback loop causes the log-probability ratio to diverge linearly, forcing the policy to collapse onto a single mode.
- **Core assumption:** The learning dynamics follow gradient flow on a softmax-parameterized policy, and initial probabilities are not perfectly uniform (random initialization).
- **Evidence anchors:**
  - [abstract] "The core issue is the probability multiplier inside the expectation, which amplifies frequency differences."
  - [section 3] "Theorem 3.1... log-probability ratio between any two outcomes evolves linearly in their reward difference, implying exponential ratio divergence... independent of... exploration strategy."
  - [corpus] Corpus papers generally attribute collapse to exploration failure (e.g., 2509.22576 "exploration-exploitation cascade failure"); this paper uniquely identifies the objective's structure as the root cause.
- **Break condition:** If the task is strictly unimodal (only one optimal outcome exists), this mechanism describes desirable convergence rather than failure.

### Mechanism 2
- **Claim:** Inverse Probability Scaling (IPS) removes outcome-frequency amplification, stabilizing the policy at a reward-proportional distribution.
- **Mechanism:** IPS rescales the terminal reward by $1/p_\theta(o)$, changing the objective to $J_{IPS} = \mathbb{E}[r(o)/p_\theta(o)]$. This theoretically cancels the probability multiplier in the gradient update. Instead of the update being dominated by $p(o)$, the gradient dynamics shift to balance probabilities with rewards. The stationary solution becomes $p^*(o) \propto r(o)$, meaning the policy preserves multiple modes as long as they have non-zero reward.
- **Core assumption:** The gradients do not flow through the inverse probability term (treated as a constant scalar for the specific update step).
- **Evidence anchors:**
  - [abstract] "Inverse Probability Scaling... removes outcome-frequency amplification... provably yields reward-proportional terminal distributions."
  - [section 4.2] "Corollary 4.2... The distribution $p^*_i = r(i)/\sum r(k)$ is stationary."
  - [corpus] Evidence in corpus focuses on adding entropy bonuses or latent diffusion (84912); IPS is distinct as it modifies the reward attribution directly.
- **Break condition:** If the reward function $r(o)$ is extremely sparse or if $p_\theta(o)$ approaches zero for high-reward outcomes (preventing the scaling factor from being computed), the mechanism fails.

### Mechanism 3
- **Claim:** Group-based frequency estimation allows practical approximation of IPS without requiring a separate outcome-probability model.
- **Mechanism:** Since true outcome probabilities $p_\theta(o)$ are unknown, IPS-GRPO estimates them via empirical frequency $\hat{p}(o)$ within a sampled group of $G$ trajectories. It scales rewards by $1/\max(\hat{p}(o), \epsilon)$. This creates a self-correcting dynamic: if an outcome is sampled "too often" relative to its reward, its effective reward drops, allowing rarer outcomes to compete.
- **Core assumption:** The group size $G$ is sufficiently large to observe rare outcomes at least occasionally (probability estimation validity).
- **Evidence anchors:**
  - [section 4.4] "We estimate outcome probabilities by group frequency $\hat{p}(o)$... prevents extreme weights when $\hat{p}(o)$ is small."
  - [section 5.4] "Ablation: Group Size... Increasing $G$ improves the accuracy... moderate group sizes combined with mild clipping provide the best balance."
  - [corpus] Corpus does not offer comparable group-based scaling mechanisms for diversity.
- **Break condition:** If the outcome space is massive relative to $G$ (e.g., continuous generation), $\hat{p}(o)$ will be zero for most valid outcomes, rendering the scaling ineffective or reliant entirely on the clipping floor $\epsilon$.

## Foundational Learning
- **Concept:** Policy Gradient (REINFORCE)
  - **Why needed here:** You must understand that standard updates are weighted by the probability of the trajectory ($\nabla \log \pi \times R$). The paper argues this very weighting is the source of the bug.
  - **Quick check question:** If I have two paths with equal reward, but I take path A 90% of the time, which path gets a larger total gradient update in standard REINFORCE?
- **Concept:** Softmax / Logits Dynamics
  - **Why needed here:** The paper's theoretical proof relies on gradient flow over softmax logits. Understanding that small logit changes imply exponential probability changes is key to grasping the "collapse" speed.
  - **Quick check question:** In a softmax distribution, does a linear increase in the log-ratio of two probabilities mean the probability ratio grows linearly or exponentially?
- **Concept:** GRPO (Group Relative Policy Optimization)
  - **Why needed here:** IPS is implemented as a drop-in modification to GRPO. You need to know that GRPO samples groups of outputs and computes advantages relative to the group mean.
  - **Quick check question:** In GRPO, how is the baseline (advantage normalization) typically calculated within a group?

## Architecture Onboarding
- **Component map:** Sampler -> Outcome Grouper -> IPS Scaler -> Optimizer
- **Critical path:** The estimation of $\hat{p}(o)$. If this is inaccurate (due to small $G$), the variance of the scaled rewards destabilizes training. The clipping parameter $\epsilon$ is the critical safety rail.
- **Design tradeoffs:**
  - **Group Size ($G$) vs. Compute:** Larger $G$ yields accurate probability estimates but linearly increases compute/memory per step.
  - **Clipping ($\epsilon$) vs. Correction:** High $\epsilon$ stabilizes training but approximates standard GRPO (loss of IPS effect). Low $\epsilon$ maintains strict IPS logic but explodes gradients for rare samples.
- **Failure signatures:**
  - **Variance Explosion:** Loss becomes NaN or oscillates violently. Likely $\epsilon$ is too small or $G$ is too small for the outcome diversity.
  - **No Diversity Gain:** Model collapses just like baseline. Likely $\epsilon$ is too high (negating the scaling) or rewards are effectively sparse.
- **First 3 experiments:**
  1. **Synthetic Bandit Validation:** Create a 2-armed bandit with equal reward. Run standard REINFORCE vs. IPS. Verify that REINFORCE collapses to 100% on one arm, while IPS maintains ~50/50. This validates the theoretical mechanism in isolation.
  2. **Group Size Ablation:** On a fixed multimodal task (e.g., the HyperGrid), sweep $G \in \{4, 16, 64\}$ with fixed $\epsilon=0.1$. Plot the Recovery Rate (diversity) and $\ell_1$ distance to target. Identify the "knee" where diversity stops improving.
  3. **Clipping Sensitivity:** With optimal $G$, sweep $\epsilon \in \{0.01, 0.1, 0.5\}$. Monitor gradient norms. Verify that low $\epsilon$ increases gradient magnitude but risks instability, while high $\epsilon$ degrades to baseline performance.

## Open Questions the Paper Calls Out
### Open Question 1
- **Question:** Can Inverse Probability Scaling be extended to value-based and actor-critic methods (e.g., PPO with learned critics) where updates depend on value estimates rather than explicit outcome frequencies?
- **Basis in paper:** [explicit] "IPS is not directly applicable to RL methods that learn policies through Actor-Critic dynamic... Extending IPS-like corrections to value-based or actor–critic methods remains an open problem."
- **Why unresolved:** IPS currently requires explicit outcome frequency estimation from sampled groups, but actor-critic methods rely on learned value functions that don't directly provide outcome-level probabilities.
- **What evidence would resolve it:** A principled derivation showing how IPS-style corrections can be incorporated into TD-learning or advantage estimation, with convergence guarantees and empirical validation.

### Open Question 2
- **Question:** How should IPS be adapted for dense-reward settings or continuous outcome spaces where terminal states are not discrete and enumerable?
- **Basis in paper:** [explicit] "Our analysis focuses on terminal rewards; extending IPS to dense-reward or continuous-outcome settings poses additional challenges."
- **Why unresolved:** The current formulation assumes discrete terminal outcomes with well-defined probabilities; continuous spaces require density estimation and raise questions about how to define inverse probability scaling for non-atomic outcomes.
- **What evidence would resolve it:** A theoretical extension of IPS to continuous outcome distributions with an empirical demonstration on a continuous-control or dense-reward benchmark.

### Open Question 3
- **Question:** What are optimal strategies for managing the bias–variance trade-off introduced by probability clipping in large or high-cardinality outcome spaces?
- **Basis in paper:** [explicit] "In large or continuous outcome spaces, these estimates can be noisy, requiring probability clipping that introduces a bias–variance trade-off."
- **Why unresolved:** The ablation shows clipping stabilizes training but weakens the IPS correction; the optimal balance depends on outcome space structure, group size, and reward distribution in ways not yet characterized.
- **What evidence would resolve it:** A systematic study quantifying how clipping threshold, group size, and outcome space cardinality jointly affect convergence rate and final policy diversity.

## Limitations
- The theoretical proof assumes gradient flow on softmax policies and may not formally extend to discrete LLM token generation.
- IPS-GRPO's effectiveness depends on accurate group-based probability estimation, which can be unreliable for very large or continuous outcome spaces.
- The paper does not thoroughly explore interactions between IPS and reward normalization techniques or entropy regularization.

## Confidence
- **High Confidence:** The theoretical identification of the expected-return objective's structure as the root cause of outcome-level mode collapse.
- **Medium Confidence:** The theoretical guarantee that IPS yields a reward-proportional stationary distribution, conditional on the gradient flow assumption.
- **Medium Confidence:** The empirical demonstration that IPS-GRPO reduces mode collapse and improves diversity across multiple benchmarks, though results depend on careful hyperparameter tuning.

## Next Checks
1. **Continuous Action Space Validation:** Test IPS-GRPO on a continuous control task (e.g., MuJoCo locomotion) with a multimodal reward landscape (e.g., multiple distinct gaits of equal reward). Verify that IPS maintains gait diversity while standard RL collapses, and analyze the gradient variance behavior.
2. **Reward Distribution Sensitivity:** Systematically vary the reward function to create scenarios where (a) the optimal outcome has a lower *mean* reward but higher *variance*, and (b) two outcomes have nearly equal but slightly different rewards. Measure if IPS correctly preserves the outcome with higher variance/lower reward in case (a) and correctly differentiates in case (b), validating the proportional-stationary claim.
3. **Estimator Robustness Study:** Design a synthetic task where the outcome space size is much larger than the group size $G$. Measure the empirical recovery rate of IPS-GRPO as a function of $G$ and $\epsilon$. Compare this to a variant that uses a learned outcome probability model (e.g., a small classifier) instead of group frequency, to quantify the cost of the estimator bias.