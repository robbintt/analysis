---
ver: rpa2
title: Post-Training Large Language Models via Reinforcement Learning from Self-Feedback
arxiv_id: '2507.21931'
source_url: https://arxiv.org/abs/2507.21931
tags:
- rlsf
- reward
- wang
- reasoning
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents RLSF (Reinforcement Learning from Self-Feedback),
  a post-training method for LLMs that uses the model's own confidence as an intrinsic
  reward to improve calibration and reasoning. After generating multiple chain-of-thought
  solutions, RLSF ranks responses by confidence and fine-tunes the model via preference
  optimization or reinforcement learning, requiring no human labels or gold answers.
---

# Post-Training Large Language Models via Reinforcement Learning from Self-Feedback

## Quick Facts
- **arXiv ID:** 2507.21931
- **Source URL:** https://arxiv.org/abs/2507.21931
- **Reference count:** 11
- **Primary result:** RLSF improves LLM calibration and reasoning accuracy using only model's own confidence as intrinsic reward, requiring no human labels.

## Executive Summary
This paper introduces RLSF (Reinforcement Learning from Self-Feedback), a post-training method that improves LLM calibration and reasoning by using the model's own confidence as an intrinsic reward signal. The approach generates multiple chain-of-thought hypotheses per prompt, ranks them by confidence disparity (gap between top and second-best token probabilities), and fine-tunes the model via preference optimization or reinforcement learning. Experiments on arithmetic reasoning and multiple-choice QA show RLSF improves both accuracy and calibration, with RLSF+PPO outperforming DPO variants. The method also enables competitive reward model training using only prompts, achieving 76-81% accuracy on RewardBench without human preference data.

## Method Summary
RLSF uses chain-of-thought decoding to generate K hypotheses per prompt by sampling top-K first tokens and greedily completing each sequence. Answer spans are identified by appending "So the answer is" and computing probability disparity scores (gap between top-1 and top-2 token probabilities). Hypotheses are ranked by these confidence scores to create preference pairs, which train a reward model via Bradley-Terry loss. The original LLM is then fine-tuned using PPO (online) or DPO (offline) against this learned reward. The entire process requires only prompts as input, with no human labels or gold answers needed.

## Key Results
- RLSF improves calibration on MultiArith, GSM8K, CommonsenseQA, and ARC Easy datasets
- RLSF+PPO outperforms RLSF+DPO variants on all evaluated benchmarks
- Achieves 76-81% accuracy on RewardBench using only prompts, competitive with methods using full preference data
- Improves both answer accuracy and Expected Calibration Error (ECE) simultaneously

## Why This Works (Mechanism)

### Mechanism 1: Confidence-Reasoning Correlation as Intrinsic Reward
RLSF leverages the correlation between a model's answer confidence and the presence of coherent reasoning. By computing probability disparity over answer spans, the method creates an intrinsic reward signal that ranks response quality without external labels. This bootstrap process assumes the pretrained model has some baseline reasoning capability that correlates with confidence calibration.

### Mechanism 2: Chain-of-Thought Decoding for Hypothesis Diversity
The method samples diverse first tokens then greedily decodes to generate K alternative hypotheses with different reasoning trajectories. This diversity enables meaningful preference rankings by creating meaningfully different reasoning chains rather than just surface variations.

### Mechanism 3: Bradley-Terry Preference Learning from Confidence Rankings
Confidence-ranked responses substitute for human preference labels in reward model training. The Bradley-Terry loss learns a reward function from these rankings, which then guides PPO fine-tuning of the original policy.

## Foundational Learning

- **Expected Calibration Error (ECE)**: Measures the gap between confidence and accuracy - essential for evaluating whether RLSF improves calibration. *Quick check: Can you explain why a model with 90% confidence on answers that are 60% correct has poor calibration?*

- **PPO vs. DPO**: PPO requires training a separate reward model and uses online RL sampling, while DPO performs offline preference learning directly. *Quick check: Why does PPO require training a separate reward model while DPO does not?*

- **Probability Disparity**: Uses the gap between top-1 and top-2 token probabilities rather than raw probability to capture decision certainty more robustly. *Quick check: Why might raw token probability be misleading compared to the margin between top two candidates?*

## Architecture Onboarding

- **Component map:** CoT Decoder → Answer Span Extractor → Preference Dataset Builder → Reward Model → Policy Optimizer
- **Critical path:** CoT decoding (inference-heavy) → confidence computation → preference dataset → reward model training → PPO fine-tuning. The decoding step is the bottleneck (K× inference cost for dataset generation).
- **Design tradeoffs:** PPO outperforms DPO but requires reward model training and online sampling. K=10 balances diversity and computational cost. Lower discount factor (γ=0.98) slightly improves ECE over γ=1.0.
- **Failure signatures:** Reward hacking (high-confidence wrong outputs), bias amplification, and answer span detection failures.
- **First 3 experiments:** 1) Reproduce GSM8K calibration results to verify ECE and accuracy improvements. 2) Ablate K values to find diversity-quality tradeoff. 3) Test on miscalibrated baseline to verify improvements on poorly-calibrated models.

## Open Questions the Paper Calls Out
- How can RLSF be extended to handle tasks requiring long-term planning rather than single-step reasoning?
- Does reinforcing intrinsic confidence inadvertently amplify harmful biases or toxicity in models with weak safety alignment?
- How does confidence-based RLSF compare to other forms of intrinsic motivation for LLM post-training?

## Limitations
- Method requires structured answer formats with identifiable answer spans, limiting generalizability to open-ended generation
- Computational overhead scales linearly with K hypotheses per prompt
- Risk of reward hacking where models learn to produce high-confidence but incorrect outputs
- Performance on multilingual or non-English reasoning tasks remains untested

## Confidence
- **Calibration improvement claims (High confidence)**: Strong empirical support from ECE metrics across multiple datasets
- **RLSF+PPO outperforming RLSF+DPO (Medium confidence)**: Supported by results but mechanism not fully explained
- **Competitive reward modeling without human labels (Medium confidence)**: Promising RewardBench results but limited comparison baselines

## Next Checks
1. Apply RLSF to a completely different reasoning domain (e.g., code generation) to verify cross-domain generalization
2. Systematically test K ∈ {3, 5, 10, 20} to quantify diversity-quality tradeoff
3. Apply RLSF to a deliberately miscalibrated model to verify improvements on poorly-calibrated baselines