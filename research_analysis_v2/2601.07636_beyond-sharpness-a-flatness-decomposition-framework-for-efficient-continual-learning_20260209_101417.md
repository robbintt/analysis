---
ver: rpa2
title: 'Beyond Sharpness: A Flatness Decomposition Framework for Efficient Continual
  Learning'
arxiv_id: '2601.07636'
source_url: https://arxiv.org/abs/2601.07636
tags:
- learning
- training
- flad
- continual
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FLAD addresses the challenge of continual learning by decomposing
  sharpness-aware perturbations into gradient-aligned and stochastic-noise components,
  retaining only the noise component to improve generalization while maintaining computational
  efficiency. The method introduces a lightweight scheduling scheme that enables significant
  performance gains even under constrained training time.
---

# Beyond Sharpness: A Flatness Decomposition Framework for Efficient Continual Learning

## Quick Facts
- **arXiv ID:** 2601.07636
- **Source URL:** https://arxiv.org/abs/2601.07636
- **Reference count:** 39
- **Primary result:** FLAD consistently outperforms standard and sharpness-aware optimizers across diverse continual learning settings while maintaining computational efficiency through a lightweight scheduling scheme.

## Executive Summary
This paper introduces FLAD, a continual learning method that addresses the challenge of catastrophic forgetting by decomposing sharpness-aware perturbations into gradient-aligned and stochastic-noise components. The key innovation is retaining only the noise component during optimization, which promotes better generalization while maintaining computational efficiency. The method introduces a lightweight scheduling scheme that allows significant performance gains even under constrained training time, making it practical for real-world applications.

## Method Summary
FLAD is a continual learning optimizer that wraps around a base SGD optimizer, decomposing sharpness-aware perturbations into two components. It maintains EMA buffers to approximate full-batch gradients and computes orthogonal noise by projecting the current batch gradient away from the EMA estimate. The method uses a fixed cosine similarity parameter to control the decomposition and implements a lightweight scheduling scheme that applies the flatness-inducing perturbations only during specific training epochs. The optimizer requires two forward and four backward passes per iteration, making it approximately 2-4x slower than vanilla SGD per step but more efficient than double-gradient methods.

## Key Results
- FLAD consistently outperforms standard SGD and SAM optimizers across diverse continual learning benchmarks
- The method achieves significant performance gains even when applied for only 10-20% of training epochs
- FLAD can be seamlessly integrated into various CL paradigms and maintains effectiveness across different experimental settings

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Isolating the stochastic-noise component of perturbations improves generalization in Continual Learning more effectively than using the full gradient-aligned perturbation.
- **Mechanism:** Standard sharpness-aware methods use the full gradient direction to perturb parameters. FLAD decomposes the batch gradient into a component aligned with the full gradient and a stochastic-noise component (orthogonal to the EMA of gradients). By retaining only the noise component for perturbation, the method penalizes sharpness without interfering with the dominant optimization trajectory.
- **Core assumption:** The full-batch gradient direction represents a "dominated structure" that should be preserved, while the batch-specific stochastic variation contains the essential signal for finding flat minima.
- **Evidence anchors:** [abstract] "decomposes sharpness-aware perturbations into gradient-aligned and stochastic-noise components, and show that retaining only the noise component promotes generalization."
- **Break condition:** If the EMA fails to accurately approximate the full-batch gradient, the decomposition will incorrectly identify the "noise" component, potentially removing useful gradient signal.

### Mechanism 2
- **Claim:** Noise-aligned perturbations facilitate escape from sharp minima by aligning update noise with high-curvature directions of the Hessian.
- **Mechanism:** The method relies on the observation that stochastic noise in SGD is often anisotropic. By explicitly constructing perturbations from the stochastic-noise component, FLAD increases the alignment between the update direction and the top eigenvectors of the Hessian, encouraging the optimizer to traverse out of sharp valleys early in training.
- **Core assumption:** High alignment with Hessian curvature during early training correlates with finding flatter, more generalizable minima later.
- **Evidence anchors:** [section: Empirical Analysis] Fig. 1b shows FLAD variants exhibit higher $Tr(H\Sigma)$ in early training and lower trace (flatter minima) in later stages compared to standard methods.
- **Break condition:** If the loss landscape geometry lacks sufficient curvature alignment with the gradient noise, the targeted perturbation may not provide directional benefits over random noise.

### Mechanism 3
- **Claim:** Computational overhead can be minimized by applying the FLAD optimizer only during a subset of training epochs without sacrificing performance gains.
- **Mechanism:** The "lightweight scheduling scheme" implies that the flatness-seeking behavior is most critical during specific phases of optimization. The paper empirically shows that applying the complex perturbation steps for only 10-30% of the epochs captures the majority of the generalization benefit.
- **Core assumption:** The model settles into a basin of attraction relatively early; subsequent standard SGD steps are sufficient to refine the solution within that basin without needing explicit sharpness regularization.
- **Evidence anchors:** [abstract] "introduce a lightweight scheduling scheme that enables FLAD to maintain significant performance gains even under constrained training time."
- **Break condition:** If the "critical phase" is misidentified (e.g., applying FLAD too late), the model may settle into a sharp minimum before the flatness-inducing perturbations can take effect.

## Foundational Learning

- **Concept: Sharpness-Aware Minimization (SAM)**
  - **Why needed here:** FLAD modifies the core SAM logic (perturbing parameters to maximize loss before minimizing). You must understand the baseline operation $\delta = \rho \nabla L / \|\nabla L\|$ to grasp how FLAD decomposes it.
  - **Quick check question:** How does the standard SAM update step differ from a standard SGD step?

- **Concept: Gradient Orthogonalization**
  - **Why needed here:** The core contribution relies on projecting the batch gradient away from the EMA-estimated full gradient. Understanding vector projections is required to implement the decomposition in Eq. (6) and (7).
  - **Quick check question:** If vector $u$ is projected onto $v$, what does the orthogonal component $\text{Proj}^\perp_v(u)$ represent geometrically?

- **Concept: Exponential Moving Average (EMA)**
  - **Why needed here:** Calculating the true full-batch gradient is too expensive. The method uses EMA as a cheap proxy. Understanding the decay rate ($\lambda_0$) is vital for tuning.
  - **Quick check question:** Why might an EMA with a very small decay factor (high momentum) fail to track rapid changes in the gradient distribution during task switches?

## Architecture Onboarding

- **Component map:** Base Optimizer (SGD) -> State Buffer (EMA accumulators $m_t$, $n_t$) -> Perturbation Module (batch gradient computation, orthogonal noise decomposition, perturbation generation) -> Update Logic (forward pass at $w+\delta$, gradient computation, weight update)

- **Critical path:**
  1. Initialize EMA $m_0 = 0$
  2. In training loop, compute current batch gradient
  3. Update EMA before or after decomposition (verify consistency with Eq. 8)
  4. Calculate perturbation vector $\delta$ using decomposed noise vector, not raw gradient
  5. Compute gradient at perturbed point $(w+\delta)$
  6. Apply final weight update using gradient from step 5

- **Design tradeoffs:**
  - Fixed $\sigma$ vs Calculated: The paper fixes cosine similarity $\sigma$ as constant for efficiency, reducing overhead but potentially ignoring dynamic angle shifts between batch and full gradients
  - Overhead: Algorithm requires 2 forward and 4 backward passes per iteration, ~2-4x slower than vanilla SGD per step but cheaper than double-gradient methods
  - Scheduling: Integrating "partial application" schedule (e.g., using FLAD only for first 20% of epochs) is essential to match paper's efficiency claims

- **Failure signatures:**
  - Instability: If $\rho$ is too large or EMA is stale (early in training), perturbation might step too far outside trust region, causing loss divergence
  - Forgetting: If FLAD is applied too aggressively on new tasks without balancing stability, it might overfit new task's noise components specifically

- **First 3 experiments:**
  1. **Sanity Check (CIFAR-10/5 tasks):** Run Replay baseline with SGD vs. FLAD. Verify FLAD achieves higher "Average Accuracy" but tracks training loss closely
  2. **Component Ablation:** Implement FLAD vs. "Full-Perturbation" (using raw gradient instead of noise decomposition). Confirm decomposed version converges faster/flatter as per Fig. 1a
  3. **Efficiency Schedule:** Run FLAD for only first 20% of epochs per task vs. full epochs. Measure accuracy drop vs. time saved to validate "lightweight scheduling" claim

## Open Questions the Paper Calls Out

- **Question:** What are the precise theoretical mechanics by which the gradient-aligned perturbation component degrades performance, whereas the stochastic-noise component improves it?
- **Basis in paper:** [explicit] Page 4 notes that "Full" perturbation "degrades performance, likely due to introducing conflict with learning dynamics," while "Noise" component succeeds
- **Why unresolved:** Paper empirically validates decomposition but only hypothesizes ("likely") regarding exact nature of "conflict" between gradient-aligned perturbations and optimization trajectory
- **What evidence would resolve it:** Theoretical analysis showing how gradient-aligned perturbations specifically interfere with descent direction in non-convex landscapes, potentially via interference with Hessian spectrum

- **Question:** How does the approximation error of the EMA affect the fidelity of the orthogonal decomposition in non-stationary environments?
- **Basis in paper:** [inferred] Method relies on EMA to approximate full-batch gradient for decomposition. In CL, data distributions shift abruptly, potentially introducing lag or error in this approximation
- **Why unresolved:** Paper assumes EMA provides "reliable approximation" but does not analyze sensitivity of decomposition to EMA hyperparameters when data distribution is non-stationary
- **What evidence would resolve it:** Sensitivity analysis measuring angle between true full gradient and EMA estimate across task boundaries, and its correlation with forgetting metrics

- **Question:** Can the "partial application" scheduling scheme be adapted dynamically based on local loss landscape geometry rather than fixed epochs?
- **Basis in paper:** [inferred] Figure 4 demonstrates that applying FLAD for only a subset of epochs maintains performance, but selection of transition point is manually tuned
- **Why unresolved:** Paper establishes full-epoch application is unnecessary, but does not determine if there is optimal, task-specific point to switch optimizers that depends on model's current convergence state
- **What evidence would resolve it:** Development of adaptive trigger (e.g., based on gradient norm variance or local sharpness) that determines when to disable FLAD to maximize efficiency without sacrificing accuracy

## Limitations

- The optimal values for EMA decay parameters $\lambda_0, \lambda_1$ are unspecified, potentially affecting performance
- The constant $\sigma$ value is unclear despite being critical for the decomposition
- The lightweight scheduling scheme's effectiveness across diverse CL paradigms needs broader validation beyond CIFAR experiments

## Confidence

- **Mechanism 1 (stochastic-noise decomposition):** High confidence - clear theoretical formulation and ablation studies, though direct validation of gradient alignment assumption is limited
- **Mechanism 2 (curvature alignment):** Medium confidence - Fig. 1b provides compelling evidence, but theoretical connection between early-training $Tr(H\Sigma)$ and final generalization is not rigorously established
- **Mechanism 3 (lightweight scheduling):** Medium confidence - Fig. 4 demonstrates effectiveness, but critical phase identification remains heuristic without theoretical justification

## Next Checks

1. **Component Ablation Study:** Implement and compare FLAD against variants using full gradient perturbations (SAM-style) and random noise perturbations to quantify the specific contribution of the stochastic-noise decomposition mechanism.

2. **EMA Sensitivity Analysis:** Systematically vary the EMA decay parameters $\lambda_0, \lambda_1$ across orders of magnitude to determine their impact on final performance and identify optimal values for different task complexities.

3. **Cross-Paradigm Generalization:** Test FLAD's effectiveness when integrated with architectural methods (e.g., elastic weight consolidation) and expansion methods (e.g., Piggyback) beyond the replay and regularization methods evaluated in the paper.