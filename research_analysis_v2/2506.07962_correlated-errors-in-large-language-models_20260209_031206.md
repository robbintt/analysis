---
ver: rpa2
title: Correlated Errors in Large Language Models
arxiv_id: '2506.07962'
source_url: https://arxiv.org/abs/2506.07962
tags:
- qwen2forcausallm
- llamaforcausallm
- same
- accuracy
- correlated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We empirically demonstrate that errors across large language models
  are highly correlated, with pairs of models agreeing 60% of the time when both are
  incorrect on multiple choice questions. This correlation is strongest among more
  accurate models, those from the same provider, and those sharing the same base architecture.
---

# Correlated Errors in Large Language Models

## Quick Facts
- arXiv ID: 2506.07962
- Source URL: https://arxiv.org/abs/2506.07962
- Reference count: 40
- Key outcome: Models agree on incorrect answers 60% of the time, with correlation strongest among accurate models from same providers/architectures

## Executive Summary
This paper empirically demonstrates that errors across large language models are highly correlated, with pairs of models agreeing 60% of the time when both are incorrect on multiple choice questions. This correlation is strongest among more accurate models, those from the same provider, and those sharing the same base architecture. In downstream tasks like LLM-as-judge evaluations and simulated hiring markets, correlated errors lead to inflated accuracy estimates and systemic exclusion of applicants, revealing substantial convergence in model outputs that undermines assumptions of robustness and fairness in multi-agent systems.

## Method Summary
The paper analyzes error correlation across three datasets: HELM (71 LLMs, 14,042 MMLU questions), HuggingFace (349 LLMs, 12,032 MMLU questions), and a proprietary resume scoring dataset (1,800 pairs, 20 LLMs). For each pair of models, they compute agreement rates conditional on both being wrong, then regress these rates against features like same provider, same architecture, and accuracy. They simulate downstream impacts through LLM-as-judge evaluations and hiring market matching algorithms to quantify how correlated errors propagate through multi-agent systems.

## Key Results
- Error correlation is strongest among models from the same provider (+6.6% to +22% agreement boost) and same architecture (+7.6% boost)
- More accurate models have more correlated errors, even with distinct architectures and providers
- In LLM-as-judge evaluations, judges systematically inflate accuracy of models less accurate than themselves
- Simulated hiring markets with diverse models still exclude ~20% of applicants due to correlated preferences

## Why This Works (Mechanism)

### Mechanism 1: Architecture and Provider Homogeneity Drive Error Correlation
Shared architectural components and provider-specific training pipelines create convergent failure modes. When models process similar inputs through similar computational graphs trained on similar data distributions, they tend to fail in similar ways.

### Mechanism 2: Accuracy-Correlation Coupling
As models scale and improve on benchmark tasks, they converge toward similar representations of "correct" reasoning patterns. This convergence means they also converge on which edge cases they misclassify.

### Mechanism 3: Error Correlation Propagates Through Multi-Model Systems
In LLM-as-judge setups, a judge model's errors correlate with evaluated models, inflating accuracy estimates. In hiring markets, correlated rankings across firms cause systemic exclusion even when firms use "different" models.

## Foundational Learning

- **Agreement Rate Conditional on Error**: Isolates correlation in mistakes from correlation due to shared correctness; needed for the paper's core measurement approach.
- **Algorithmic Monoculture**: The hiring market simulations operationalize theoretical monoculture concerns; needed to connect empirical findings to broader fairness/robustness literature.
- **Stable Matching in Two-Sided Markets**: Section 5.2 uses deferred acceptance algorithms; needed to interpret welfare tradeoffs in applicant-firm matching.

## Architecture Onboarding

- **Component map**: Model responses extraction -> Pairwise error agreement computation -> Regression analysis -> Downstream simulation
- **Critical path**: 1) Collect multi-model responses on shared benchmark questions; 2) Compute pairwise error agreement rates; 3) Regress agreement on model features; 4) Inject correlations into downstream task simulations
- **Design tradeoffs**: Multiple-choice vs open-ended for tractability; "agreement when both wrong" metric treats all wrong answers equally; simulations vs real deployment
- **Failure signatures**: Unexpectedly high correlation between unrelated models; judge inflation without shared provider; persistent systemic exclusion despite model diversity
- **First 3 experiments**: 1) Replicate analysis on new benchmarks (GPQA, MATH); 2) Introduce explicit calibration for LLM-as-judge; 3) Test if prompting modifications reduce error correlation

## Open Questions the Paper Calls Out

- Are newer models becoming more or less homogeneous in their errors over time, independent of accuracy improvements?
- How can we develop error correlation metrics that account for varying question difficulty and "closeness" of incorrect answers to correct ones?
- How do correlated errors manifest in open-ended generation and complex reasoning tasks compared to the multiple-choice and resume scoring tasks studied?
- What interventions can reduce correlated errors across models while maintaining individual model accuracy?

## Limitations

- Findings rely heavily on MMLU-style multiple choice questions, potentially missing broader correlation patterns
- Regression analysis treats all error agreements equally, potentially underweighting more severe failures
- The accuracy-correlation coupling could reflect shared training data biases rather than fundamental representational convergence

## Confidence

**High Confidence:** Error correlation is strongest among same-provider and same-architecture models (directly observable from regression coefficients)
**Medium Confidence:** More accurate models have more correlated errors (statistically significant but could reflect shared training data biases)
**Medium Confidence:** Downstream impacts on LLM-as-judge evaluations and hiring markets (mechanisms plausible but real-world deployment may introduce additional factors)

## Next Checks

1. Cross-Benchmark Validation: Replicate error correlation analysis on diverse benchmarks (GPQA, MATH, HumanEval) to test generalization beyond MMLU
2. Debiasing Experiment: Implement explicit calibration for LLM-as-judge evaluations to estimate and correct systematic inflation bias
3. Architecture Diversity Test: Identify or train model pairs with deliberately orthogonal architectures but similar accuracy to test if accuracy-correlation coupling persists