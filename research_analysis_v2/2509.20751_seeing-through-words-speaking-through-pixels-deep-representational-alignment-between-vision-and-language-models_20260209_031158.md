---
ver: rpa2
title: 'Seeing Through Words, Speaking Through Pixels: Deep Representational Alignment
  Between Vision and Language Models'
arxiv_id: '2509.20751'
source_url: https://arxiv.org/abs/2509.20751
tags:
- alignment
- language
- image
- vision
- caption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates where and how deep vision-only and language-only
  models align in their representations, despite being trained on disjoint modalities.
  The authors systematically map alignment across network layers and test what types
  of input changes preserve or disrupt it.
---

# Seeing Through Words, Speaking Through Pixels: Deep Representational Alignment Between Vision and Language Models

## Quick Facts
- arXiv ID: 2509.20751
- Source URL: https://arxiv.org/abs/2509.20751
- Reference count: 31
- Primary result: Cross-modal alignment between vision and language models strengthens in mid-to-late layers, is driven by semantic content, and is enhanced by exemplar averaging.

## Executive Summary
This study investigates where and how deep vision-only and language-only models align in their representations, despite being trained on disjoint modalities. The authors systematically map alignment across network layers and test what types of input changes preserve or disrupt it. They find that alignment strengthens in mid-to-late layers of both model types, suggesting convergence toward shared semantic codes rather than modality-specific features. Alignment drops when semantic content is removed but is robust to appearance-only changes. A "Pick-a-Pic" task and CLIP-score ranking show that aligned embeddings mirror human preferences for image-caption matches. Surprisingly, averaging embeddings across multiple exemplars enhances alignment rather than blurring it, indicating that aggregation reveals a stable, modality-independent semantic core.

## Method Summary
The authors use frozen Vision Transformer (ViT-Large-DINOv2) and language models (BLOOM, OpenLLaMA) to extract penultimate-layer embeddings from 1,000 MS-COCO image-caption pairs and 1,000 Pick-a-Pic prompt-image pairs. They quantify cross-modal alignment using ridge regression-based linear predictivity (Pearson r averaged over units and 5-fold CV) and Centered Kernel Alignment (CKA). They systematically probe layer-wise alignment, test robustness to semantic/appearance manipulations (grayscale, rotation, thing/stuff masks, scrambled captions), and examine the effect of averaging multiple exemplars. Alignment is computed in both directions (L→V, V→L) and validated against human preference data.

## Key Results
- Alignment peaks in mid-to-late layers of both vision and language models, reflecting convergence toward shared semantic representations.
- Alignment is robust to appearance-only changes (grayscale, rotation) but collapses when semantic content is altered (object removal, word-order scrambling).
- Averaging embeddings across multiple exemplars (captions or images) amplifies alignment, suggesting aggregation reveals a stable, modality-independent semantic core.
- Aligned embeddings from this unimodal alignment correlate with human preferences for matching images to captions.

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Semantic Convergence
- **Claim:** Cross-modal alignment emerges specifically in mid-to-late network layers.
- **Mechanism:** Early layers process modality-specific features; deeper layers converge toward a shared statistical model of reality ("Platonic Representation"), discarding modality-specific details for abstract concepts.
- **Core assumption:** Training data for both models captures consistent underlying properties of the world.
- **Evidence anchors:** Both modalities exhibit low cross-modal predictivity in earliest layers and increase through mid and later layers ([abstract], [section 3.1]).

### Mechanism 2: Semantic Integrity Dependency
- **Claim:** Alignment is maintained by high-level semantic content, not low-level surface features.
- **Mechanism:** The mapping relies on presence of objects and relationships; superficial changes preserve semantic "gist" while semantic deletion destroys structure required for linear mapping.
- **Core assumption:** Aligned dimensions correspond to semantic variables rather than perceptual variables.
- **Evidence anchors:** Alignment robust to appearance-only changes but collapses when semantics are altered ([abstract], [section 3.2]).

### Mechanism 3: Exemplar Aggregation as Denoising
- **Claim:** Averaging embeddings across exemplars strengthens alignment rather than blurring it.
- **Mechanism:** Averaging acts as noise-filtering operation, canceling out idiosyncratic variances while reinforcing stable signal of shared concept.
- **Core assumption:** Noise in embeddings is isotropic or uncorrelated across exemplars while semantic signal is consistent.
- **Evidence anchors:** Averaging embeddings amplifies alignment, indicating aggregation reveals stable, modality-independent semantic core ([abstract], [section 3.4]).

## Foundational Learning

- **Concept:** Representational Similarity Analysis (RSA) & Linear Probes
  - **Why needed here:** Paper relies on "linear predictivity" (Ridge Regression) and CKA to quantify alignment. High score implies Model A features are linearly decodable from Model B.
  - **Quick check question:** If linear predictivity is high but CKA is low, what might that imply about the relationship between the two representational geometries?

- **Concept:** Modality-Specific vs. Amodal Representations
  - **Why needed here:** Core hypothesis is networks discard "modality-specific" features (pixels, syntax) for "amodal" ones (concepts).
  - **Quick check question:** In the context of this paper, would a layer highly sensitive to image rotation be considered "amodal" or "modality-specific"?

- **Concept:** Vision Transformers (ViT) & LLM Layer Dynamics
  - **Why needed here:** Paper maps alignment layer-by-layer. Need to know ViTs and LLMs process information hierarchically (low-level → high-level) to interpret "mid-to-late layer" findings.
  - **Quick check question:** Why does the paper suggest language-to-vision mapping works from early language layers to late vision layers, but not the reverse?

## Architecture Onboarding

- **Component map:** MS-COCO/Pick-a-Pic (Image + Caption pairs) -> Frozen ViT-DINOv2 + Frozen LLM (BLOOM, LLaMA) -> Extract class tokens/averaged token activations from specific transformer blocks -> Ridge Regression (Linear Predictivity) and CKA

- **Critical path:**
  1. Ensure inputs are perfectly aligned (Image ↔ Correct Caption)
  2. Extract activations from mid-to-late layers (skipping early layers avoids noise/low-level features)
  3. Normalize features (z-score) before fitting linear maps

- **Design tradeoffs:**
  - Linear vs. Non-linear Probing: Linear maps for interpretability; non-linear maps might show higher alignment but obscure geometric simplicity
  - Directionality: Mapping Vision→Language differs from Language→Vision; paper notes asymmetry (L→V often easier/robust)

- **Failure signatures:**
  - Flat alignment curves: Check if LLM is untrained/random or vision model is too shallow
  - High alignment on noise: If scrambled captions show high alignment, linear map is overfitting; increase regularization (λ)

- **First 3 experiments:**
  1. **Layer Sweep:** Extract features from every block of small ViT and LLM, fit Ridge regression, plot predictivity scores to confirm mid-to-late peak
  2. **Semantic Perturbation:** Take 100 image-caption pairs, scramble words in captions, measure drop in alignment score compared to original captions
  3. **Aggregation Test:** Take 1 image and 5 captions, compute alignment for 1 caption, then average of 3, then average of 5, plot slope to confirm "denoising" effect

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Are concrete concepts (e.g., "dog") more strongly aligned across modalities than abstract concepts (e.g., "freedom"), and how does this vary by image type?
- **Basis in paper:** Discussion asks "Are concrete concepts... more strongly aligned than abstract concepts?" and suggests different image types (photographs vs. diagrams) "may exhibit varying degrees of alignment"
- **Why unresolved:** Current study relied primarily on MS-COCO and Pick-a-Pic, which feature natural photographs of concrete everyday scenes, lacking balanced representation of abstract concepts or varied artistic styles
- **What evidence would resolve it:** Systematic evaluation using datasets specifically balanced for concreteness and visual style to measure alignment strength comparatively

### Open Question 2
- **Question:** Which specific features or dimensions within high-dimensional embeddings drive the observed cross-modal alignment?
- **Basis in paper:** Discussion states "identifying which specific features or dimensions drive this alignment remains an open question"
- **Why unresolved:** Current metrics (linear predictivity, CKA) measure global similarity between representation spaces but do not disentangle specific individual units or subspaces responsible for semantic match
- **What evidence would resolve it:** Probing experiments or ablation studies that selectively mask or perturb specific dimensions to observe effect on alignment and downstream task performance

### Open Question 3
- **Question:** How do alignment patterns evolve during the training process, and do they emerge suddenly or gradually?
- **Basis in paper:** Discussion asks "Do alignment patterns appear early in training and strengthen over time, or do they emerge suddenly...?"
- **Why unresolved:** Study analyzed pre-trained models (snapshots), providing no data on developmental trajectory or temporal dynamics of how shared semantic space is constructed
- **What evidence would resolve it:** Longitudinal analysis calculating alignment metrics at various checkpoints throughout training runs

### Open Question 4
- **Question:** Does representational alignment imply shared processing mechanisms or functional interchangeability between models?
- **Basis in paper:** Limitations note that "representational similarity is descriptive" and state "Causal interventions are needed to determine whether the aligned dimensions are necessary for each model's downstream behavior"
- **Why unresolved:** High correlation indicates shared statistical structure but does not prove models rely on these aligned dimensions to process meaning in functionally equivalent way
- **What evidence would resolve it:** Causal intervention experiments where aligned dimensions in vision model are perturbed to test if it produces predictable degradation in language model's performance

## Limitations
- The observed alignment may reflect convergence on superficial statistical regularities rather than deep semantic understanding
- Focus on linear predictivity may underestimate true complexity of alignment, as non-linear relationships could capture richer correspondences
- Pick-a-Pic task and CLIP-score ranking are indirect proxies for human preferences

## Confidence

- **High Confidence**: Empirical finding that alignment peaks in mid-to-late layers is robust, supported by multiple models and metrics; directionality asymmetry is well-established
- **Medium Confidence**: Claim that averaging exemplars amplifies alignment is plausible but relies on isotropic noise assumption; corpus lacks direct evidence for this specific mechanism
- **Low Confidence**: Assertion that alignment mirrors human preferences is based on indirect proxies; more direct human study would be needed to validate this claim

## Next Checks
1. Replace linear ridge regression with small MLP to test if non-linear mappings reveal stronger or different alignment patterns, particularly in early layers
2. Test alignment on held-out dataset (e.g., Flickr30k) to ensure findings are not specific to MS-COCO's biases and assess robustness of "shared semantic space" claim
3. Systematically ablate different types of semantic content (objects, attributes, relations) from both images and captions to pinpoint which aspects of semantics are most critical for maintaining alignment