---
ver: rpa2
title: 'AdaCoder: An Adaptive Planning and Multi-Agent Framework for Function-Level
  Code Generation'
arxiv_id: '2504.04220'
source_url: https://arxiv.org/abs/2504.04220
tags:
- code
- error
- test
- return
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the generalizability of four state-of-the-art
  multi-agent frameworks for function-level code generation across six diverse open-source
  LLMs with varying parameter sizes, architectures, and performance levels. The empirical
  study reveals that existing frameworks, such as MapCoder and INTERVENOR, show improvements
  but suffer from high inference costs, while AgentCoder and Self-Collaboration exhibit
  poor generalizability and ineffective iterative refinement.
---

# AdaCoder: An Adaptive Planning and Multi-Agent Framework for Function-Level Code Generation

## Quick Facts
- arXiv ID: 2504.04220
- Source URL: https://arxiv.org/abs/2504.04220
- Reference count: 40
- Primary result: 27.69% higher Pass@1 on average compared to best baseline

## Executive Summary
AdaCoder is a novel adaptive planning framework for function-level code generation that combines non-planning initial generation with rule-based debugging and error-adaptive planning. The framework addresses the inefficiencies of existing multi-agent systems by using a two-phase approach: starting with low-cost non-planning generation and only escalating to planning-guided regeneration when necessary. Through extensive evaluation across six diverse open-source LLMs, AdaCoder achieves significantly better performance than state-of-the-art baselines while consuming 12 times fewer tokens and running 16 times faster.

## Method Summary
AdaCoder operates through a two-phase workflow. Phase 1 uses a Programming Assistant LLM to generate code directly from task descriptions, which is then evaluated by a script-based Code Evaluator. If the code fails, Phase 2 is triggered, where a Debug Specialist (rule-based agent) attempts to fix superficial errors like indentation issues, function truncation, and missing imports. If errors persist, a Prompt Engineer LLM generates an error-adaptive plan based on the specific failure feedback, which guides the Programming Assistant to regenerate code. This process iterates up to 5 times, balancing performance and efficiency by reserving expensive planning steps for when they're most needed.

## Key Results
- Achieves 27.69% higher Pass@1 on average compared to best baseline (MapCoder)
- Reduces inference cost by 12x through reduced token consumption
- Runs 16x faster than existing multi-agent frameworks
- Demonstrates strong generalizability across six diverse LLMs with varying sizes and architectures
- Shows consistent performance improvements across both HumanEval and MBPP benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Two-Phase Strategy (Planning vs. Non-Planning)
The framework begins with low-cost non-planning generation as a capability probe, only escalating to high-cost planning when initial attempts fail. This conditional approach achieves superior performance-efficiency balance compared to single-mode approaches. The core assumption is that planning and non-planning are complementary, with many problems solvable by LLM's native capabilities. Evidence shows different subsets of problems are solved by each approach, making their combination effective.

### Mechanism 2: Rule-Based Debugging for Superficial Errors
Offloads repair of simple, predictable syntax errors to a deterministic script rather than using LLM for iterative self-repair. The Debug Specialist applies three deterministic steps: normalizing indentation, removing incomplete functions, and injecting missing imports. This targets the most common failure modes that don't require algorithmic reasoning. Analysis shows iterative LLM refinement is often ineffective for these errors.

### Mechanism 3: Error-Adaptive Planning
Generates remediation plans conditioned on concrete error feedback rather than just the original problem statement. The Prompt Engineer analyzes error messages and failed test details to create targeted step-by-step plans that directly address detected errors. This approach produces more diverse and effective guidance compared to generating multiple plans from static problem descriptions.

## Foundational Learning

- **Concept: Pass@k Metric**
  - Why needed here: This is the core evaluation metric. The paper's primary claim ("27.69% higher Pass@1") relies on understanding that Pass@1 measures the percentage of problems where at least one of the first `k` generated samples is correct.
  - Quick check question: If a model has a Pass@1 of 60% on the HumanEval benchmark, what does that signify?

- **Concept: Rule-Based vs. Learning-Based Agents**
  - Why needed here: AdaCoder's architecture is a hybrid. Understanding that the Debug Specialist is a deterministic script (rule-based), while the Programming Assistant is an LLM (learning-based), is critical to understanding its cost/performance profile.
  - Quick check question: What is the primary advantage of using a rule-based agent for debugging over an LLM-based one in this context? (Answer: Determinism and low inference cost).

- **Concept: Inference Cost (Tokens & Time)**
  - Why needed here: A major contribution of the paper is reducing inference cost by 12-16x. One must understand that inference cost is directly tied to the number of LLM calls and total tokens processed, which this framework reduces by using a script for testing and a rule-based agent for simple fixes.
  - Quick check question: How does the use of a "Code Evaluator" script, instead of an LLM-based tester, affect the overall inference cost?

## Architecture Onboarding

- **Component map:**
  1. **Programming Assistant:** LLM-based. Generates and regenerates code based on the task and optional plan.
  2. **Code Evaluator:** Script-based. Executes code in a sandbox to determine pass/fail and capture error messages.
  3. **Debug Specialist:** Rule-based agent. Performs fast, deterministic fixes for indentation, truncation, and missing imports.
  4. **Prompt Engineer:** LLM-based. Analyzes error feedback to generate a corrective step-by-step plan for the Programming Assistant.

- **Critical path:** The system operates in a two-phase loop:
  1. **Phase 1 (Probe):** `Programming Assistant` generates code -> `Code Evaluator` tests. If pass, end.
  2. **Phase 2 (Fix):** If fail, `Debug Specialist` attempts fixes -> `Code Evaluator` tests. If pass, end.
  3. **Phase 2 (Plan & Regenerate):** If still failing, `Prompt Engineer` creates a plan -> `Programming Assistant` regenerates code -> `Code Evaluator` tests.
  4. The process repeats from step 2 of Phase 2 for up to `t` iterations.

- **Design tradeoffs:**
  - **Adaptive vs. Uniform Planning:** The system trades the guaranteed consistency of always-planning (MapCoder) for the efficiency of on-demand planning, risking failure on tasks where non-planning succeeds but planning would have found a *better* solution faster.
  - **Rule-based Debugging vs. LLM Debugging:** Prioritizes speed and low cost for common errors, at the expense of flexibility. The system cannot debug complex logic errors via the Debug Specialist.

- **Failure signatures:**
  - **Syntax Loop:** Code fails with a syntax error not covered by the three rule-based fixes (e.g., a complex name error from a typo in a variable name). This forces a costly regeneration cycle that might not address the root cause.
  - **Planning Loop:** The Prompt Engineer generates a plan that is logically flawed or misunderstood, causing the Programming Assistant to regenerate equally buggy code.
  - **Context Overload:** Over multiple iterations, the error history fed to the Prompt Engineer might exceed the context window or confuse the model.

- **First 3 experiments:**
  1. **End-to-End Baseline:** Run AdaCoder on the HumanEval dataset with a mid-sized model (e.g., DeepSeek-Coder-6.7B). Measure final Pass@1 and total token consumption. Compare against the "Direct" baseline to quantify the performance gain.
  2. **Agent Ablation:** Run the system with the Debug Specialist disabled. The expected result is a significant drop in performance on tasks involving simple errors (e.g., missing imports), validating the agent's contribution.
  3. **Iteration Limit Study:** Run the system with `t=1`, `t=3`, and `t=5`. Plot the Pass@1 score against cumulative inference cost to visualize the diminishing returns of additional iterations and determine an optimal stopping point.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does AdaCoder perform on more complex, class-level or repository-level code generation tasks compared to function-level benchmarks?
- Basis in paper: [explicit] The authors state in "Threats to Validity" that "the specific datasets and foundation LLMs may restrict the broader applicability" and they "plan to validate our approach using more diverse datasets... in future research."
- Why unresolved: The current evaluation is restricted to function-level benchmarks (HumanEval and MBPP), which do not require context understanding across multiple files or classes.
- What evidence would resolve it: Evaluation results on class-level (e.g., ClassEval) or repository-level (e.g., RepoCoder) benchmarks.

### Open Question 2
- Question: To what extent does the reliance on a rule-based "Debug Specialist" limit the framework's scalability to other programming languages?
- Basis in paper: [inferred] The methodology describes the Debug Specialist as a script derived from prior work that only addresses three specific error types in Python: Inconsistent Indentation, Function Overflow, and Missing Import.
- Why unresolved: Rule-based approaches are inherently language-specific; code generation in languages with different syntax rules (e.g., C++, Java) would require rewriting these rules, potentially reducing the framework's "generalizability."
- What evidence would resolve it: Performance metrics of AdaCoder applied to multilingual code generation benchmarks (e.g., HumanEval-X).

### Open Question 3
- Question: Is there a dynamic or adaptive stopping criteria for the iteration count $t$ that optimizes the trade-off between Pass@1 and inference cost for diverse LLMs?
- Basis in paper: [explicit] The authors note in the limitations that for other foundation LLMs, "this parameter may require some adjustment to achieve an optimal balance between pass@1 and resource consumption."
- Why unresolved: The study uses a fixed maximum of $t=5$, but the results show diminishing returns for larger models (like GPT-4o) compared to smaller ones, suggesting a fixed value is inefficient.
- What evidence would resolve it: An ablation study analyzing the correlation between model capability/size and the optimal iteration threshold.

## Limitations
- The paper does not specify the exact implementation details of the Debug Specialist's module database for import injection, which could affect reproducibility.
- The analysis focuses on Python function-level generation without testing the framework's performance on larger codebases or different programming languages.
- While the framework shows 16x speed improvements, the absolute inference times are not provided, making it difficult to assess real-world deployment feasibility.

## Confidence

- **High confidence** in the comparative performance metrics (Pass@1 improvements of 27.69%) due to the use of established benchmarks (HumanEval, MBPP) and systematic ablation studies.
- **Medium confidence** in the efficiency claims (16x speed, 12x token reduction) as these are relative to baselines but absolute values are not provided.
- **Medium confidence** in the generalizability claims across diverse LLMs since the evaluation, while comprehensive, only covers six models from the same family of code-specialized LLMs.

## Next Checks

1. Implement and test the Debug Specialist's module database independently to verify its completeness and effectiveness across diverse Python packages.
2. Conduct a scaling study to measure absolute inference times and memory usage when running AdaCoder on enterprise-scale code generation tasks.
3. Extend evaluation to multi-language support by testing AdaCoder on code generation benchmarks for JavaScript and Java to validate cross-language generalizability.