---
ver: rpa2
title: 'Metaphor and Large Language Models: When Surface Features Matter More than
  Deep Understanding'
arxiv_id: '2507.15357'
source_url: https://arxiv.org/abs/2507.15357
tags:
- language
- metaphor
- metaphorical
- llms
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the capabilities of large language models
  (LLMs) in metaphor interpretation across multiple datasets, tasks, and prompt configurations.
  Previous research was limited by single-dataset evaluations, artificial data construction,
  and narrow task settings.
---

# Metaphor and Large Language Models: When Surface Features Matter More than Deep Understanding

## Quick Facts
- **arXiv ID:** 2507.15357
- **Source URL:** https://arxiv.org/abs/2507.15357
- **Reference count:** 20
- **Primary result:** LLMs perform better on metaphor interpretation tasks when lexical overlap and sentence length are high, suggesting surface features drive performance more than metaphorical understanding.

## Executive Summary
This study evaluates large language models' capabilities in metaphor interpretation across diverse datasets and tasks. Previous research was limited by single-dataset evaluations and artificial data construction. The authors conducted comprehensive experiments using five publicly available datasets with inference and metaphor annotations, focusing on natural language inference and question answering tasks. Results show that LLMs' performance is more influenced by surface features like lexical overlap and sentence length than by metaphorical content. Rather than exhibiting emergent abilities to understand metaphorical language, performance appears to result from a combination of surface-level features, in-context learning, and linguistic knowledge.

## Method Summary
The authors evaluated seven LLMs (Llama-3, Mistral, Qwen2.5, Gemma) using five prompt configurations (zero-shot, few-shot, and Chain-of-Thought) across five metaphor datasets (Figurative-NLI, IMPLI, FLUTE, Fig-QA, Meta4XNLI). Datasets were filtered for metaphorical premises/hypotheses and mapped to binary entailment labels. Literal paraphrases were generated using Command R+ API to create adversarial examples. Evaluation used accuracy metrics with Levenshtein distance analysis to measure lexical overlap. The study compared performance across original metaphor datasets versus literal paraphrases to test whether surface features drive results.

## Key Results
- LLMs perform better on datasets with higher lexical overlap between premise and hypothesis
- Chain-of-Thought prompting consistently improves performance across all models and datasets
- Larger models (Qwen2.5-72B) generally outperform smaller models on metaphor tasks
- Performance degrades significantly when metaphorical sentences are paraphrased into literal forms

## Why This Works (Mechanism)

### Mechanism 1: Surface Feature Heuristic Over Semantic Reasoning
- **Claim:** LLMs prioritize lexical overlap and sentence length over semantic processing of metaphorical content
- **Mechanism:** Models operate as pattern matchers, using "edit distance" calculations - high lexical overlap triggers entailment predictions while bypassing metaphor decoding
- **Evidence anchors:** Abstract shows results indicate performance is influenced by lexical overlap and sentence length more than metaphorical content; Section 6.1 shows Qwen2.5-72B performs best on datasets with higher overlap
- **Break condition:** Performance degrades when lexical overlap is reduced through adversarial literal paraphrases

### Mechanism 2: Prompt-Driven In-Context Simulation
- **Claim:** Few-shot and Chain-of-Thought prompts boost performance through structural templates rather than emergent reasoning
- **Mechanism:** Prompts define output format and task constraints, with CoT adding reasoning steps that prime linguistic knowledge without requiring true generalization
- **Evidence anchors:** Abstract states alleged emergent abilities are results of surface-level features, in-context learning, and linguistic knowledge; Section 5.1 shows CoT improves every model's performance
- **Break condition:** Smaller models revert to near-random performance when stripped of few-shot examples or CoT context

### Mechanism 3: Adversarial Sensitivity via Paraphrasing
- **Claim:** Literal adversarial examples expose model's inability to handle semantic shifts when surface features change
- **Mechanism:** When metaphorical sentences are paraphrased into literal ones, models lose relied-upon "anchor" words, leading to misclassification
- **Evidence anchors:** Section 5.2 shows LLMs perform better on original metaphorical datasets than literal paraphrases; Section 6.2 reveals paraphrases sometimes still contain metaphors
- **Break condition:** If adversarial paraphrase generation introduces new metaphors or changes semantic labels, evaluation becomes invalid

## Foundational Learning

- **Concept: Natural Language Inference (NLI)**
  - **Why needed here:** Primary evaluation framework to determine if model "understands" metaphor by checking if premise entails hypothesis
  - **Quick check question:** Given "He drowned his sorrow," does it entail "He drank alcohol"? Model must handle figurative meaning

- **Concept: Lexical Substitution Bias**
  - **Why needed here:** Explains why previous benchmarks gave inflated results through artificial lexical overlap created by replacing single words
  - **Quick check question:** If dataset pair shares 90% of words, is high accuracy evidence of deep understanding or pattern matching?

- **Concept: Chain-of-Thought (CoT) Prompting**
  - **Why needed here:** Produced highest results; crucial for replicating state-of-the-art baseline
  - **Quick check question:** Does asking model to "think step-by-step" guarantee reasoning is causal, or could it be post-hoc rationalization?

## Architecture Onboarding

- **Component map:** LLMs (Llama-3, Qwen2.5, Gemma, Mistral) -> Metaphor Datasets (Figurative-NLI, IMPLI, FLUTE, Fig-QA, Meta4XNLI) -> Accuracy Metrics and Levenshtein Distance Analysis

- **Critical path:**
  1. Dataset Preparation: Filter datasets for metaphorical premises/hypotheses
  2. Adversarial Generation: Use LLM to rewrite metaphorical sentences as literal paraphrases
  3. Inference: Run Zero-shot, Few-shot, and CoT prompts on original and adversarial sets
  4. Correlation Analysis: Calculate Levenshtein distance and correlate with accuracy

- **Design tradeoffs:**
  - Synthetic vs. Natural Data: Synthetic data allows controlled evaluation but introduces overlap bias; natural data is messier but more realistic
  - Prompt Complexity: Zero-shot is lower cost but fails on smaller models; CoT is computationally heavier but stabilizes performance

- **Failure signatures:**
  - The Literal Paradox: Accuracy drops on literal paraphrases compared to metaphorical originals
  - Hallucinated Metaphors: Adversarial generator fails to remove metaphors, indicating generator lacks metaphor detection capabilities

- **First 3 experiments:**
  1. Baseline Overlap Test: Evaluate Llama-8B on Figurative-NLI using standard NLI prompts; plot accuracy vs. Levenshtein distance
  2. Adversarial Stress Test: Take IMPLI dataset, generate literal paraphrases, compare Qwen-72B performance on original vs. paraphrased sets
  3. CoT vs. Few-Shot: Run Gemma-27B on Meta4XNLI with both Few-Shot QA and CoT to isolate "reasoning" context gains

## Open Questions the Paper Calls Out

- **Open Question 1:** How would human-curated literal paraphrases alter evaluation outcomes compared to LLM-generated adversarial examples? The paper notes future research could benefit from manually inspecting generated paraphrases since automatic generation often failed to remove metaphors.

- **Open Question 2:** Do findings about surface feature reliance generalize to languages with different morphological structures or resource availability? The study is restricted to English due to scarcity of metaphorical resources in other languages.

- **Open Question 3:** Can LLMs be prompted or trained to explicitly distinguish between metaphorical and literal expressions to prevent inadvertent reintroduction of metaphors during paraphrasing? The study identified this failure mode but didn't test specific interventions.

## Limitations

- **Generative Paraphrase Reliability:** Core adversarial evaluation depends on Command R+ correctly converting metaphorical to literal sentences, but the paper acknowledges this process sometimes fails by introducing new metaphors or preserving original meaning
- **Lexical Overlap-Correlation Confound:** Paper shows correlation between overlap and accuracy but doesn't definitively prove causality versus other confounding factors
- **Dataset Representativeness:** Findings may not generalize beyond tested datasets due to limited size and diversity of metaphor datasets

## Confidence

- **High Confidence:** LLMs show varying performance across metaphor datasets; CoT prompting consistently improves performance; Few-shot outperforms zero-shot for smaller models; larger models generally outperform smaller ones
- **Medium Confidence:** Performance differences are primarily driven by lexical overlap and sentence length; dataset ranking by overlap correlates with performance; CoT works through structural templates
- **Low Confidence:** LLMs lack "deep understanding" of metaphor; improvements are solely due to better task framing; all gains result from surface features rather than emergent capabilities

## Next Checks

1. **Independent Adversarial Generation:** Replicate paraphrase generation using different LLM or human annotators to verify Command R+'s outputs don't introduce systematic biases

2. **Controlled Overlap Manipulation:** Create synthetic datasets with controlled lexical overlap (holding other variables constant) to establish causality rather than relying on correlational evidence

3. **Cross-Domain Generalization:** Test same models on metaphor-rich domains not represented in current datasets (legal, medical, technical metaphors) to assess whether surface feature dominance holds across diverse metaphorical language types