---
ver: rpa2
title: 'MoPEQ: Mixture of Mixed Precision Quantized Experts'
arxiv_id: '2509.02512'
source_url: https://arxiv.org/abs/2509.02512
tags:
- expert
- index
- deepseek
- precision
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MoPEQ introduces a mixed-precision quantization method for Mixture-of-Experts
  vision-language models that assigns optimal bit widths to each expert based on sensitivity
  analysis using Hessian trace approximation. Unlike prior approaches relying on activation
  frequency, MoPEQ clusters experts by importance to balance accuracy and model size.
---

# MoPEQ: Mixture of Mixed Precision Quantized Experts

## Quick Facts
- arXiv ID: 2509.02512
- Source URL: https://arxiv.org/abs/2509.02512
- Reference count: 40
- Primary result: 1.5× memory reduction on MoE VLMs with <5% accuracy loss using Hessian-based sensitivity clustering

## Executive Summary
MoPEQ introduces a mixed-precision quantization framework for Mixture-of-Experts vision-language models that assigns optimal bit widths to each expert based on sensitivity analysis using Hessian trace approximation. Unlike prior approaches relying on activation frequency, MoPEQ clusters experts by importance to balance accuracy and model size. Evaluated on DeepSeek-VL2 and MolmoE models across nine tasks, the method achieves up to 1.5× memory reduction while maintaining accuracy within 5% of baselines. Hessian-based sensitivity proved more effective than activation frequency, especially for large models, and model-wise precision allocation outperformed layer-wise approaches.

## Method Summary
MoPEQ computes per-expert sensitivity via Hessian trace approximation using Hutchinson's algorithm with Frobenius norm proxy loss, then clusters experts using K-means and assigns bit-widths (2/3/4-bit) based on cluster importance ranking. The method operates in three stages: (1) Hessian trace computation for Gate, Up, and Down projections per expert, (2) K-means clustering on global expert importance values with C=3 clusters, and (3) quantization using AutoRound with SignRound applied per expert according to cluster assignment. Non-MoE layers use uniform precision. The approach is data-free, requiring no calibration dataset.

## Key Results
- 1.5× memory reduction achieved while maintaining <5% accuracy degradation
- Hessian-based sensitivity outperforms activation frequency, particularly for large models
- Model-wise precision allocation yields better validation performance than layer-wise (63 vs 42 scenarios)
- Consistent performance across nine benchmarks including MME, TextVQA, DocVQA, MMMU, InfoVQA, AI2D, RealWorldQA, ScienceQA, and BLINK

## Why This Works (Mechanism)

### Mechanism 1: Hessian Trace as a Data-Free Sensitivity Proxy
MoPEQ uses Hessian trace approximation to measure expert sensitivity to quantization noise, assigning higher bit-widths to more sensitive experts. The method employs Hutchinson's algorithm with Frobenius norm as a surrogate loss, measuring curvature of the loss landscape relative to weight perturbations. This works because weights with larger curvature contribute more significantly to model performance. The approach is data-free, avoiding calibration dataset requirements. However, if the surrogate loss doesn't correlate with actual downstream task loss, sensitivity ranking becomes arbitrary and accuracy degrades.

### Mechanism 2: Global Clustering for Discrete Precision Allocation
The method applies K-means clustering to the global set of expert importance values, grouping experts into clusters corresponding to available bit-widths (2, 3, 4). It assigns highest precision to the cluster with highest mean importance. This global approach assumes sensitivity is an absolute property comparable across different layers. Model-wise allocation yields better validation performance than layer-wise (63 vs 42 scenarios), but may disadvantage layers with fundamentally different sensitivity distributions if they contain mostly sensitive experts.

### Mechanism 3: Sensitivity-Aware Load Balancing
In models with uniform expert utilization enforced by auxiliary load-balancing losses, MoPEQ's sensitivity analysis differentiates critical experts where frequency analysis fails. By ignoring artificially flattened activation counts and focusing on mathematical weight sensitivity, the system identifies experts critical for minimizing inference error despite uniform activation. This assumes uniform activation frequency doesn't imply uniform importance to the loss function. If sensitive experts are never activated for specific inference tasks, high precision assignment wastes memory without accuracy gain.

## Foundational Learning

- **Mixture of Experts (MoE) Routing**: MoE models sparsely activate subsets of parameters ("experts") via a gating network. Understanding MoE routing is crucial because not all experts are used equally, and those that are used might have different sensitivities. Quick check: How does the "Top-k" routing mechanism affect which experts are active during inference, and why does this make uniform quantization inefficient?

- **Post-Training Quantization (PTQ)**: The method operates on pre-trained models without retraining. Understanding PTQ (vs. QAT) clarifies why "calibration data" is usually needed and why a "data-free" method (using Hessian) is a novelty. Quick check: Why might a data-free sensitivity metric (like Hessian trace) be preferred over calibration-based metrics (like activation frequency) when deploying to diverse, real-world tasks?

- **Hessian Trace Approximation (Hutchinson's Method)**: This is the mathematical engine of the paper. Hessian trace approximates the "curvature" or "sensitivity" of the loss landscape without computing the full Hessian matrix. Quick check: Why is computing the full Hessian matrix infeasible for LLMs/VLMs, necessitating the use of randomized trace estimation algorithms like Hutchinson's?

## Architecture Onboarding

- **Component map**: Pre-trained VLM-MoE -> Hessian Profiler (Hutchinson's algorithm) -> Importance Aggregator (K-Means clustering) -> Precision Allocator (bit-width mapping) -> Quantization Engine (AutoRound/SignRound)

- **Critical path**: Load pre-trained VLM-MoE → Iterate through MoE layers to calculate Hessian trace for Gate/Up/Down projections → Execute K-Means clustering on global expert sensitivities → Generate configuration map assigning bit-widths to expert IDs → Run quantization pass using generated map

- **Design tradeoffs**: Memory vs. Accuracy (more 2-bit reduces memory but risks accuracy if sensitive experts are downcast); Global vs. Local (model-wise optimizes global accuracy but may create load imbalance if a layer contains mostly sensitive experts); Data-Free vs. Calibration (Hessian is data-free/robust but might miss task-specific nuances)

- **Failure signatures**: Uniform Quantization Effect (if Hessian traces are uniform across experts, clustering fails to differentiate, resulting in effectively uniform quantization); Catastrophic Forgetting (aggressive 2-bit quantization on semantically vital experts for specific visual tasks)

- **First 3 experiments**: 1) Run Hessian profiler on DeepSeek-VL2-Tiny and visualize heatmap to verify important layers show higher traces; 2) Implement comparison between Layer-wise vs. Model-wise clustering using same Hessian data to check for non-uniform bit distribution; 3) Quantize using MoPEQ (mixed 2/3/4-bit) and compare MME-Perception score against uniform 4-bit baseline to validate <5% accuracy drop

## Open Questions the Paper Calls Out

- **Integration with vLLM**: The authors plan to integrate mixed-precision quantization support into vLLM and provide hardware performance evaluations, but current experiments lack end-to-end runtime metrics due to framework limitations.

- **Non-expert Layer Sensitivity**: The paper limits mixed precision to experts only, leaving unknown whether applying Hessian-based sensitivity analysis to vision encoders and attention blocks yields significant further compression.

- **Uniform-Sensitivity Models**: DeepSeek-VL2-Small exhibits uniformly high sensitivity across all experts, complicating the quantization of experts to different precisions and requiring a modified allocation strategy.

## Limitations

- Hessian trace quality depends heavily on unspecified number of Hutchinson samples (m), which could significantly impact clustering accuracy
- Assumes Frobenius norm serves as valid proxy for task-specific loss across all VLMs, which may not hold for different architectures or training objectives
- Demonstrates effectiveness primarily on DeepSeek-VL2 and MolmoE models with limited generalization testing to other MoE architectures or domains

## Confidence

- **High Confidence**: 1.5× memory reduction with <5% accuracy degradation is well-supported by experimental data across nine benchmarks
- **Medium Confidence**: Hessian-based sensitivity outperforming activation frequency is convincing but may be model-specific to VLMs with load-balancing constraints
- **Medium Confidence**: Model-wise precision allocation superiority is demonstrated but could be architecture-dependent and may not generalize to all MoE configurations

## Next Checks

1. **Sensitivity Distribution Analysis**: Reproduce Figure 3's expert importance heatmaps for multiple model sizes to verify Hessian traces show meaningful variance across experts and that model-wise clustering exploits this variance

2. **Calibration Dataset Comparison**: Implement activation frequency-based quantization baseline on same models to empirically validate that Hessian-based sensitivity consistently outperforms frequency-based methods across diverse downstream tasks

3. **Task-Specific Expert Validation**: Conduct ablation study where experts are manually assigned bit-widths based on semantic role (early vision vs. late language) to determine if Hessian traces capture task-relevant importance or merely mathematical sensitivity