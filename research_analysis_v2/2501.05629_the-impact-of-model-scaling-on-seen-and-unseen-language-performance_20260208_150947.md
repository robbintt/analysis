---
ver: rpa2
title: The Impact of Model Scaling on Seen and Unseen Language Performance
arxiv_id: '2501.05629'
source_url: https://arxiv.org/abs/2501.05629
tags:
- languages
- language
- resource
- seen
- unseen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically investigates scaling behavior of multilingual
  LLMs across 204 languages for text classification and machine translation tasks.
  Three model families (XGLM, BLOOM, BLOOMZ) with 14 different sizes (560M-7.5B parameters)
  were evaluated in zero-shot and two-shot settings.
---

# The Impact of Model Scaling on Seen and Unseen Language Performance

## Quick Facts
- arXiv ID: 2501.05629
- Source URL: https://arxiv.org/abs/2501.05629
- Authors: Rhitabrat Pokharel; Sina Bagheri Nezhad; Ameeta Agrawal; Suresh Singh
- Reference count: 8
- Primary result: Larger models show clear linear improvements in two-shot multilingual text classification but minimal scaling effects in zero-shot settings or machine translation tasks.

## Executive Summary
This study systematically investigates how model scale affects performance across 204 languages for multilingual text classification and machine translation. Three model families (XGLM, BLOOM, BLOOMZ) with 14 different sizes were evaluated in zero-shot and two-shot settings. Results show significant disparities between seen and unseen languages, with larger models benefiting from few-shot demonstrations primarily in classification tasks while smaller models perform better in zero-shot settings, especially for low-resource and unseen languages. The study reveals that instruction-tuned models like BLOOMZ show scaling benefits for translation but may degrade with few-shot examples, highlighting complex interactions between model scale, task type, and language resource levels.

## Method Summary
The study evaluated 14 multilingual LLMs (560M-7.5B parameters) across three families (XGLM, BLOOM, BLOOMZ) using two benchmark datasets: SIB-200 for topic classification and FLORES-200 for machine translation. Models were tested in zero-shot and two-shot settings using English prompts. Languages were classified as seen or unseen based on pretraining corpus documentation, and resource levels were mapped using the Joshi taxonomy. Performance was measured using macro-F1 for classification and SacreBLEU for translation. The study focused on cross-lingual generalization patterns and scaling behavior across different language resource levels.

## Key Results
- Larger models (>3B parameters) show clear linear improvements in two-shot text classification (slopes 0.05-0.07) but minimal scaling effects in zero-shot settings or machine translation tasks.
- Smaller models (<1.7B parameters) achieve comparable or superior performance to larger models in zero-shot settings, particularly for unseen and low-resource languages, showing U-shaped scaling trends.
- For translation tasks, only the instruction-tuned BLOOMZ model showed clear benefits from scaling, while other models showed negligible scaling effects.
- Performance correlations were stronger with general resource levels than with language-specific pretraining data, revealing pronounced performance disparities across different resource levels.

## Why This Works (Mechanism)

### Mechanism 1
Few-shot in-context learning (ICL) enables linear scaling gains for larger models on classification tasks, but this benefit does not transfer to zero-shot settings or generative tasks. Larger models leverage demonstration examples more effectively by conditioning label probabilities through pattern recognition within the provided context. This scaling sensitivity does not apply to generative tasks where context appears to reduce generalization ability. Evidence shows positive slopes for 2-shot classification but near-zero slopes for generation tasks across all settings.

### Mechanism 2
Smaller models achieve comparable or superior performance to larger models in zero-shot settings, particularly for unseen and low-resource languages. The paper observes U-shaped scaling trends and occasional double-descent patterns where smaller models outperform mid-sized variants. This suggests smaller models may avoid overfitting to seen language patterns, retaining more generalizable representations for out-of-distribution inputs.

### Mechanism 3
Cross-lingual transfer from linguistically related seen languages enables performance on unseen languages, often more than general resource level would predict. Unseen languages from the same language family as seen languages benefit from shared structural and lexical patterns encoded during pretraining. Correlation analysis shows poor correlation between unseen language performance and resource levels, suggesting family-based transfer dominates.

## Foundational Learning

- **In-Context Learning (ICL) scaling sensitivity**: Understanding that ICL effectiveness is not uniform—it scales differently across model sizes and task types is critical for deployment decisions. Quick check: Given a 560M parameter model and a 7B parameter model, which would you expect to benefit more from 2-shot demonstrations for topic classification? For translation?

- **Language resource taxonomy (Joshi et al. 2020 levels 0-5)**: The paper uses this 6-level categorization as a stronger predictor of performance than raw pretraining data proportions. Quick check: If a language has substantial web crawl data but limited annotated resources, would you classify it as high or medium resource under this taxonomy?

- **Curse of multilinguality**: Referenced as the phenomenon where "performance deteriorates as language coverage expands"—relevant for understanding why xglm (30 languages) sometimes outperforms bloom (46 languages) despite smaller scale. Quick check: Why might adding more languages to pretraining not proportionally improve performance across all languages?

## Architecture Onboarding

- **Component map**: Model families (XGLM, BLOOM, BLOOMZ) -> Evaluation datasets (SIB-200, FLORES-200) -> Inference settings (Zero-shot, Two-shot)

- **Critical path**: 1) Classify target languages as seen/unseen based on pretraining corpus documentation, 2) Map languages to resource levels using Joshi taxonomy, 3) Select appropriate inference setting (zero-shot for smaller models on unseen/low-resource; 2-shot for larger models on classification), 4) Use English prompts for cross-lingual consistency

- **Design tradeoffs**: Model scale vs. task type (larger models justify compute for 2-shot classification but may not improve translation), Instruction tuning (BLOOMZ enables scaling benefits for translation but degrades with demonstrations), Language coverage vs. per-language capacity (models with fewer languages may achieve higher per-language performance)

- **Failure signatures**: Performance degradation when moving from 0-shot to 2-shot (indicates model cannot leverage ICL—use zero-shot), Near-zero scaling slopes across model sizes (indicates task/setting combination is scale-insensitive), Large gaps between seen/unseen languages of same family (indicates pretraining corpus lacks coverage of that language group)

- **First 3 experiments**: 1) Baseline calibration: Run zero-shot and 2-shot classification on a 10-language subset spanning resource levels 0-5 with your smallest and largest available models to establish scaling slopes, 2) Family transfer test: Select 2 unseen languages with related seen languages and 2 unseen languages without—compare performance gaps to quantify transfer benefit, 3) Instruction tuning ablation: Compare BLOOM vs. BLOOMZ on 2-shot translation to isolate the effect of instruction tuning on scaling behavior

## Open Questions the Paper Calls Out

- **Fine-tuning vs. ICL effects**: How do fine-tuning approaches compare to in-context learning for extensive multilingual scenarios across different resource levels? The study only examined zero-shot and few-shot ICL settings, leaving the effects of fine-tuning unexplored despite evidence it may outperform ICL in some scenarios.

- **Improving unseen language generalization**: How can generalization of instruction-tuned models to unseen languages be improved when few-shot learning degrades performance? BLOOMZ showed performance degradation from 0-shot to 2-shot in text generation, contradicting benefits seen in classification.

- **Cross-lingual transfer mechanisms**: Why do unseen languages exhibit poor correlation with general resource levels but show performance closer to related seen languages? The simple correlation analyses cannot capture the cross-lingual transfer mechanisms the authors hypothesize.

## Limitations
- Results are limited to topic classification and machine translation, so mechanisms may not extend to other multilingual tasks like question answering or summarization.
- Prompt templates are not explicitly provided, creating uncertainty about whether reproduced results would match original findings due to prompt sensitivity.
- Language mapping ambiguities exist between ISO-639-1 (XGLM) and ISO-639-3 (datasets) codes, affecting seen/unseen categorization.

## Confidence
- **High Confidence**: Larger models show linear improvements in two-shot text classification while showing minimal scaling effects in zero-shot settings (positive slopes of 0.05-0.07 vs near-zero slopes).
- **Medium Confidence**: Smaller models outperform larger models on zero-shot settings for unseen and low-resource languages, though this could be influenced by prompt sensitivity and mapping uncertainties.
- **Low Confidence**: Cross-lingual transfer is primarily driven by linguistic family membership rather than general resource levels, as correlation analysis lacks direct causal evidence.

## Next Checks
1. **Prompt Sensitivity Test**: Systematically vary prompt templates across the three model families while holding other variables constant. Measure how sensitive the observed scaling patterns are to prompt formulation, particularly for the smallest (560M) and largest (7.5B) models in zero-shot vs two-shot settings.

2. **Language Mapping Verification**: Conduct an independent audit of the "seen" vs "unseen" language categorizations for all three model families. For ambiguous ISO code mappings, test multiple resolution strategies and measure how classification affects downstream analysis of scaling behavior.

3. **Family Transfer Isolation**: Design an experiment that isolates family-based transfer effects by selecting unseen languages with and without closely related seen languages while controlling for resource levels. This would validate whether family membership provides benefits beyond what resource level alone would predict.