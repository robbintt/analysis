---
ver: rpa2
title: 'TokMem: Tokenized Procedural Memory for Large Language Models'
arxiv_id: '2510.00444'
source_url: https://arxiv.org/abs/2510.00444
tags:
- generation
- classification
- answer
- question
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TokMem, a tokenized procedural memory for
  large language models that encodes recurring procedures as compact, trainable embeddings
  while keeping the backbone model frozen. Each memory token serves both as an address
  to a procedure and as a control signal that steers generation, enabling targeted
  behavior with constant-size overhead.
---

# TokMem: Tokenized Procedural Memory for Large Language Models

## Quick Facts
- arXiv ID: 2510.00444
- Source URL: https://arxiv.org/abs/2510.00444
- Reference count: 40
- Key outcome: Encodes recurring procedures as compact, trainable embeddings while keeping backbone frozen, enabling targeted behavior with constant-size overhead and strong compositional generalization

## Executive Summary
This paper introduces TokMem, a novel approach for encoding recurring procedures as trainable memory tokens that can be attached to a frozen large language model. Each memory token serves dual purposes: it acts as an address to a specific procedure and as a control signal that steers the model's generation. The key innovation is that TokMem achieves procedural memory without fine-tuning the model parameters, instead learning only the memory embeddings. This enables efficient continual learning where new procedures can be added without interfering with existing ones, and the model can route to the correct procedure with high accuracy.

## Method Summary
TokMem creates a memory bank of trainable embeddings that are appended to the model's vocabulary. During training, sequences are formatted as query + [memory_token] + response, where the memory token is a special token that the model learns to predict. Only the memory embeddings are updated during training while the LLM backbone remains frozen. The model learns to route queries to the appropriate memory token through next-token prediction, with the memory token then conditioning the generation of the response. For compositional tasks, the approach uses an adaptation phase with LoRA fine-tuning before freezing the backbone again. The method also employs a normalization mechanism to prevent newly added tokens from dominating the routing logits.

## Key Results
- Achieves high task routing accuracy (over 94%) across 1,000 atomic recall tasks from Super-Natural Instructions
- Consistently outperforms retrieval-augmented generation and fine-tuning baselines with far fewer trainable parameters
- Demonstrates strong compositional generalization on function-calling datasets, maintaining performance while fine-tuning degrades

## Why This Works (Mechanism)

### Mechanism 1: Procedure Compression via Infix Embeddings
The approach compresses recurring procedures into single token embeddings that act as control signals, steering a frozen backbone to generate complex outputs without explicit context. By injecting the memory token after the query (infix), the attention mechanism conditions generation directly on the procedure token, effectively "prompting" the model internally with a learned vector rather than text. This works because the frozen backbone possesses sufficient latent capability to execute the procedure if guided correctly.

### Mechanism 2: Parameter-Isolated Continual Learning
Isolating trainable parameters to a discrete memory bank allows continual addition of new procedures without overwriting backbone weights or previously learned memory tokens. Only the embeddings are updated while the Transformer backbone remains frozen, treating memory tokens as discrete "files" in a file system. This modular design enables adding new procedures without altering the "OS" (backbone) or other files.

### Mechanism 3: Stability via Norm Renormalization
Newly added memory tokens must be normalized to match the scale of existing tokens to prevent them from dominating selection logits and causing forgetting. Without this, new embeddings develop inflated norms that bias the softmax probability distribution regardless of semantic fit. The renormalization step rescales active (new) embeddings to match the average norm of inactive (old) embeddings, maintaining stable routing behavior.

## Foundational Learning

- **Concept: Frozen Pre-trained Backbone** - Why needed: TokMem explicitly freezes the LLM parameters to preserve "general intelligence" while adding specific skills. Quick check: If TokMem fails to learn a new style of reasoning, is the failure likely in the memory token or the backbone? (Answer: Likely the backbone lacks the capability).

- **Concept: Next-Token Prediction (Causal LM)** - Why needed: The "training" of memory tokens is standard language modeling where the memory token is just a predicted token in the sequence. Quick check: In the sequence [Query] [MEM_TOKEN] [Response], which parts contribute to the loss function? (Answer: The [MEM_TOKEN] and [Response]).

- **Concept: Softmax & Logit Dominance** - Why needed: The renormalization mechanism assumes understanding why larger vector norms bias the softmax probability distribution. Quick check: Why does a memory token with norm 100 get selected more often than one with norm 1, even for unrelated queries? (Answer: Larger dot products create larger logits).

## Architecture Onboarding

- **Component map:** Query text → Tokenizer → Model predicts memory token → Selected token ID mapped to trainable embedding in Memory Bank → Embedding injected into Transformer layers → Model generates response conditioned on embedding

- **Critical path:** 1) Input: Query text → Tokenizer 2) Routing: Model predicts next token, selecting from special memory tokens 3) Recall: Selected token ID mapped to trainable embedding in Memory Bank 4) Injection: Embedding fed into Transformer layers 5) Generation: Model generates response conditioned on embedding

- **Design tradeoffs:** Uses infix (Query → Memory → Response) rather than prefix tuning for better expressiveness; explores decoupled vs coupled embeddings but finds coupled robust enough; capacity limited by fixed dimension of backbone embeddings

- **Failure signatures:** Catastrophic forgetting (check if renormalization applied); Routing collapse (check if new tokens initialized with high norm or over-trained); Garbled output (check if backbone properly frozen or learning rate too high)

- **First 3 experiments:** 1) Vanilla Validation: Train single memory token to produce fixed string, verify embedding updates and backbone remains frozen 2) Scaling Stress Test: Train 10 distinct memory tokens sequentially, evaluate if Task 1 performance drops after Task 10 3) Norm Ablation: Disable renormalization, repeat experiment 2, plot L2 norms to observe if newer tokens dominate

## Open Questions the Paper Calls Out

- **Open Question 1:** Can TokMem be extended to support complex, interleaved composition where function calls are mixed with general NLP tasks within multi-turn interactions? (The paper states this remains unexplored).

- **Open Question 2:** Can incorporating reinforcement learning improve generalization of memory tokens for complex compositional structures compared to standard next-token prediction? (The authors identify this as a promising direction).

- **Open Question 3:** How can TokMem be adapted to support user-level personalization where distinct, private memory banks operate on a shared frozen backbone? (The paper suggests this as a potential application).

- **Open Question 4:** Is the "renormalization" heuristic sufficient for long-term continual learning, or does it limit capacity of new memories as the memory bank scales significantly? (The paper notes this fix is practical but not theoretically grounded).

## Limitations
- Task complexity scaling remains unclear - approach not tested on highly complex, multi-step reasoning tasks requiring deep hierarchical planning
- Memory capacity constraints - fixed embedding dimension limits scalability to thousands of procedures without saturation
- Cross-task interference not theoretically proven - no analysis of what happens when semantically similar procedures compete for same embedding space

## Confidence
- **High Confidence:** Core mechanism works for atomic and moderately compositional tasks with superior performance to RAG and fine-tuning baselines
- **Medium Confidence:** Continual learning claims supported empirically but rely heavily on renormalization heuristic
- **Low Confidence:** Assertion that approach handles "any recurring procedure" is overstated given limited complexity testing

## Next Checks
- Stress test memory capacity by incrementally adding 10,000+ procedures and measuring routing accuracy degradation; plot cosine similarities between embeddings to identify saturation
- Test on tasks requiring hierarchical reasoning (multi-step planning with conditional branches) to identify complexity ceiling compared to full fine-tuning
- Systematically vary semantic similarity between procedures and measure cross-task interference; create synthetic datasets with similar but distinct procedures to track separation over time