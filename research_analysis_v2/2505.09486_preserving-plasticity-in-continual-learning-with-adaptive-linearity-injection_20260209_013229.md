---
ver: rpa2
title: Preserving Plasticity in Continual Learning with Adaptive Linearity Injection
arxiv_id: '2505.09486'
source_url: https://arxiv.org/abs/2505.09486
tags:
- plasticity
- label
- adalin
- learning
- activation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses plasticity loss in continual learning, where
  neural networks gradually lose the ability to learn new tasks over time. To mitigate
  this issue, the authors propose Adaptive Linearization (AdaLin), a method that dynamically
  injects linearity into activation functions based on gradient flow.
---

# Preserving Plasticity in Continual Learning with Adaptive Linearity Injection

## Quick Facts
- arXiv ID: 2505.09486
- Source URL: https://arxiv.org/abs/2505.09486
- Reference count: 40
- Primary result: Dynamic linearity injection preserves gradient flow and sustains continual learning without additional hyperparameters

## Executive Summary
Neural networks in continual learning settings suffer from plasticity loss, where they gradually lose the ability to learn new tasks. The authors propose Adaptive Linearization (AdaLin), which dynamically injects linearity into activation functions based on gradient flow. By equipping each neuron with a learnable parameter and gating mechanism, AdaLin ensures sufficient gradient signal and sustains continual learning without introducing additional hyperparameters or requiring explicit task boundaries. When used with conventional activation functions like ReLU, Tanh, and GeLU, AdaLin significantly improves performance on standard benchmarks and shows efficacy in more complex scenarios including class-incremental learning and off-policy reinforcement learning.

## Method Summary
AdaLin modifies standard activation functions by adding a learnable linear component gated by the activation's gradient magnitude. For each neuron, the output becomes the base activation plus a gated linear term: $f_i(x) = \phi(x) + \alpha_i x [g(x)]_{sg}$, where $g(x) = \cos(\frac{\pi}{2} \cdot \frac{|\phi'(x)|}{L})$. The gating function detects saturation (low gradient magnitude) and activates the linear bypass, while stop-gradient prevents the gating dynamics from influencing α updates. Each neuron learns its own α parameter, allowing heterogeneous saturation patterns to be handled locally without manual tuning.

## Key Results
- Maintains high accuracy across 250 random label MNIST tasks where standard ReLU degrades significantly
- Improves performance on Permuted MNIST, Shuffled CIFAR-10, and Class-Split CIFAR-100 benchmarks
- Demonstrates effectiveness in off-policy reinforcement learning agents and class-incremental learning with ResNet-18
- Ablation studies confirm both gating mechanism and learnable parameters are essential for preserving plasticity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gating-based linearity injection preserves gradient flow in saturated regions
- Mechanism: A gating function g(x) = cos(π/2 · |ϕ'(x)|/L) detects saturation (low |ϕ'|) and activates a linear bypass. When |ϕ'| → 0, g(x) → 1, injecting α·x to maintain non-zero gradients.
- Core assumption: Gradient degradation in saturated activation regions is a primary driver of plasticity loss.
- Evidence anchors: [abstract] "gating mechanism that injects linearity into the activation function based on its gradient flow"; [section 2] "When the magnitude is small... g(x) is near 1 and the linear term contributes a larger derivative"
- Break condition: If base activation never saturates (e.g., Linear), gating provides no benefit.

### Mechanism 2
- Claim: Per-neuron adaptation captures heterogeneous saturation patterns across units
- Mechanism: Each neuron i learns α_i independently. Saturated neurons learn small negative α values; unsaturated neurons converge to extreme α values and stabilize.
- Core assumption: Neurons exhibit task-dependent, non-uniform saturation requiring localized adaptation.
- Evidence anchors: [abstract] "equips every neuron with a learnable parameter"; [appendix F.1] Ablation shows layer-level and network-level α sharing causes plasticity loss
- Break condition: If all neurons saturate uniformly, shared α may suffice.

### Mechanism 3
- Claim: Learnable injection avoids fixed linear/nonlinear tradeoffs
- Mechanism: Unlike Deep Fourier (fixed dual-output) or fixed interpolation, AdaLin dynamically adjusts α per neuron based on observed saturation, avoiding manual tuning.
- Core assumption: Optimal linearity ratio varies across tasks and neurons.
- Evidence anchors: [section 1] "fixed ratio of linear to non-linear outputs may not be optimal across different tasks"; [appendix F.4] Learnable interpolation without gating fails to preserve plasticity
- Break condition: If task sequence is stationary, fixed α tuning may match adaptive performance.

## Foundational Learning

- Concept: **Lipschitz continuity of activation functions**
  - Why needed here: The gating function scales by the Lipschitz constant L to normalize saturation detection across ReLU (L=1), Tanh (L=1), GELU (L≈1.12).
  - Quick check question: Given ϕ(x)=tanh(x), what is |ϕ'(x)| at saturation vs. linear regions?

- Concept: **Plasticity vs. stability tradeoff in continual learning**
  - Why needed here: AdaLin must sustain learning (plasticity) without disrupting previously learned representations (stability); linearity injection is a targeted intervention.
  - Quick check question: Why might naive weight reinitialization harm stability more than adaptive activation modulation?

- Concept: **Stop-gradient operation**
  - Why needed here: The gating term [g(x)]_sg uses stop-gradient to prevent g's derivative from influencing α updates, ensuring α learns from gradient flow needs, not gating dynamics.
  - Quick check question: What would happen to α learning if stop-gradient were removed?

## Architecture Onboarding

- Component map:
  Input: Pre-activation z_i for each neuron -> Base activation: ϕ(z_i) [ReLU, Tanh, or GELU] -> Gating: g(z_i) = cos(π/2 · |ϕ'(z_i)|/L) [stop-gradient applied] -> Learnable parameter: α_i per neuron (or per channel in CNNs) -> Output: f_i(z_i) = ϕ(z_i) + α_i · z_i · g(z_i)

- Critical path:
  1. Initialize α_i ~ Uniform(0, 1) per neuron
  2. Forward pass computes g(z_i) from ϕ'(z_i)
  3. Combine base activation with gated linear term
  4. Backprop updates α_i alongside weights (no extra hyperparameters)

- Design tradeoffs:
  - Neuron-level vs. channel-level α: Neuron-level better for MLPs; channel-level reduces parameters in CNNs (paper uses channel-level for conv layers)
  - Gating function choice: Cosine, linear, and quadratic forms perform similarly if boundary conditions hold
  - Base activation: ReLU recovers PReLU; Tanh/GELU show larger gains due to stronger saturation

- Failure signatures:
  - Shared α (layer/network level): Plasticity loss recurs (Figure 11)
  - No gating (constant linear term): Random Label MNIST shows degraded performance (Figure 12)
  - Bounded α ∈ [0,1] interpolation: Fails to prevent plasticity loss (Figure 14)

- First 3 experiments:
  1. Replicate Random Label MNIST with 2-layer MLP (100 hidden units): Compare AdaLin-ReLU vs. baseline ReLU across 250 tasks, tracking average online accuracy per task.
  2. Ablate gating mechanism: Replace g(x) with constant 1, confirm performance drop on saturated tasks.
  3. Visualize learned α distribution: Plot α_i values per layer at task boundaries to verify saturated neurons adopt small negative α while unsaturated neurons stabilize at extremes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can AdaLin effectively scale to significantly larger architectures (e.g., Transformers) and datasets?
- Basis in paper: [explicit] The Discussion section explicitly states, "Future work will explore extending these ideas to more complex architectures, larger datasets..."
- Why unresolved: The paper's experiments are limited to MLPs, simple CNNs, and ResNet-18 backbones on relatively small-scale benchmarks like MNIST and CIFAR.
- What evidence would resolve it: Demonstrating that AdaLin preserves plasticity in Large Language Models or Vision Transformers trained on large-scale datasets (e.g., ImageNet).

### Open Question 2
- Question: How does AdaLin interact with explicit anti-forgetting strategies like memory replay or elastic consolidation?
- Basis in paper: [explicit] The Discussion section suggests future work involves "...integrating them with alternative strategies to combat catastrophic forgetting—moving closer to truly lifelong, adaptable learning systems."
- Why unresolved: The paper focuses primarily on the ability to learn new tasks (plasticity) but does not deeply evaluate the trade-offs when combined with specific stability-preserving mechanisms.
- What evidence would resolve it: Experiments combining AdaLin with methods like Elastic Weight Consolidation or Experience Replay to determine if they are complementary or conflict with the adaptive linearity mechanism.

### Open Question 3
- Question: Is there a reliable proxy metric that correlates consistently with plasticity loss?
- Basis in paper: [inferred] Appendix G analyzes Sign Entropy, Weight Norm, and Feature Rank (SRank). The authors observe conflicting results, concluding that "sign entropy alone might not be an adequate measure" and "weight norm alone may not be strongly correlated."
- Why unresolved: The authors found that metrics like SRank remained high even when plasticity was lost (e.g., in CReLU), suggesting current proxies do not fully explain the phenomenon.
- What evidence would resolve it: The identification of a measurable network property that consistently predicts the onset of plasticity loss across all tested benchmarks and baselines.

## Limitations
- Performance improvements on CIFAR-100 ResNet experiments are moderate (2-3% gains), suggesting the method may be less effective on deeper architectures
- RL experiments are preliminary, showing promise but lacking systematic comparison with established continual RL methods
- The claim that gradient saturation is the primary driver of plasticity loss is plausible but not definitively proven through controlled isolation

## Confidence
- **High confidence**: AdaLin's ability to preserve plasticity in controlled MLP benchmarks (Random Label/Permuted MNIST) is well-demonstrated
- **Medium confidence**: Effectiveness on standard CNN benchmarks (CIFAR-10/100) with moderate improvements
- **Low confidence**: Claims about RL applicability and scalability to very deep networks require further validation

## Next Checks
1. Test AdaLin with different base activations (Swish, ELU) to verify mechanism generalizes beyond ReLU/Tanh/GELU
2. Conduct ablation studies isolating the contribution of learnable α vs. gating mechanism
3. Evaluate performance on non-vision continual learning tasks (e.g., language modeling) to assess domain generality