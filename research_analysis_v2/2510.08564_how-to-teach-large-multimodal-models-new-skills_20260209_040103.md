---
ver: rpa2
title: How to Teach Large Multimodal Models New Skills
arxiv_id: '2510.08564'
source_url: https://arxiv.org/abs/2510.08564
tags:
- target
- stage
- forgetting
- learning
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates how to teach large multimodal models (LMMs)
  new skills without erasing prior abilities. It explores sequential fine-tuning on
  five target skills while monitoring general ability on eight held-out benchmarks
  across three model families.
---

# How to Teach Large Multimodal Models New Skills

## Quick Facts
- arXiv ID: 2510.08564
- Source URL: https://arxiv.org/abs/2510.08564
- Reference count: 40
- The paper identifies two tuning recipes—updating only self-attention projections and updating only MLP Gate&Up while freezing the Down projection—that teach LMMs new skills while largely preserving prior abilities.

## Executive Summary
The paper investigates how to teach large multimodal models (LMMs) new skills without erasing prior abilities. It explores sequential fine-tuning on five target skills while monitoring general ability on eight held-out benchmarks across three model families. The study reveals that apparent "forgetting" after narrow fine-tuning can partly recover at later stages, traced to a shift in the output token distribution. Guided by this, the paper identifies two tuning recipes—updating only self-attention projection layers and updating only MLP Gate&Up while freezing the Down projection—that achieve strong target gains while largely preserving held-out performance. These methods are effective across models and tasks, offering a robust way to teach LMMs new skills with minimal forgetting.

## Method Summary
The paper evaluates sequential fine-tuning on five target tasks (CUB200, PixmoCount, PathVQA, TextVQA, TimeClock) while monitoring performance on eight held-out benchmarks. It tests three model families (LLaVA-OneVision-7B, LLaVA-Next-7B-V, Qwen2.5-VL-7B) with four tuning configurations: full model, full LLM, self-attention projections only, and MLP (Gate&Up) with Down frozen. The key innovation is identifying which model components to update to minimize forgetting while maximizing target skill acquisition.

## Key Results
- SA Proj. tuning achieves +23.1 Target Overall with only -0.6 Held-out Forgetting on LLaVA-OneVision-7B
- MLP (Gate&Up) tuning delivers +17.6 Target Overall with -3.0 Held-out Forgetting
- Full MLP tuning causes severe forgetting (-15.4 to -27.3 Held-out Forgetting) despite higher target gains
- Apparent forgetting can recover later in the sequence, traced to output token distribution shifts

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Apparent "catastrophic forgetting" in Large Multimodal Models (LMMs) is often a temporary manifestation of output token distribution shift rather than permanent knowledge erasure.
- **Mechanism:** Narrow fine-tuning (e.g., on counting tasks) biases the model's next-token prediction toward task-specific tokens (e.g., numeric digits). This shift lowers the probability of generating the tokens required for held-out tasks, mimicking forgetting. Because the underlying conceptual knowledge may remain intact, subsequent training on a different task can shift the distribution back, "recovering" the lost performance.
- **Core assumption:** Performance on general benchmarks relies on a balanced output distribution; biasing this distribution dominates the performance drop observed in held-out tasks.
- **Evidence anchors:**
  - [abstract] "trace this behavior to a measurable shift in the output token distribution... forgetting can partly recover at later stages."
  - [section 5.2] "The expected likelihood of number tokens... surges for LLM and MLP... held-out accuracy drops in tandem."
  - [corpus] "What Does Loss Optimization Actually Teach?" suggests viewing training as a knowledge dynamics process rather than simple optimization, aligning with the view that loss reduction may shift distributions rather than strictly acquiring skills.
- **Break condition:** If held-out performance remains permanently depressed even after the token distribution returns to baseline, the forgetting is likely conceptual erasure rather than distributional bias.

### Mechanism 2
- **Claim:** Self-attention projections function as data routing/processing layers, allowing skill acquisition with minimal interference to the output distribution.
- **Mechanism:** Self-attention layers (specifically Q, K, V, O projections) primarily route information and process input structure. Updating these layers improves the model's ability to utilize existing knowledge for new tasks without significantly altering the MLP "memory" layers that dictate output preferences.
- **Core assumption:** The division of labor in Transformers (Attention = Routing, MLP = Memory) holds for LMMs, and learning new skills can be achieved primarily via improved routing.
- **Evidence anchors:**
  - [page 2] "Self-attention projection is data processing... while MLPs perform external memory look up."
  - [section 5.1] "SA Proj. achieves high Target Overall +23.1 with minimal forgetting... Held-out -0.6."
  - [corpus] Corpus evidence specifically identifying self-attention as the primary locus for forgetting mitigation is weak/absent; this appears to be a specific contribution of the current work.
- **Break condition:** If updating self-attention causes significant output distribution shifts (high NTB), the assumption that it isolates routing from memory writing fails.

### Mechanism 3
- **Claim:** Freezing the MLP "Down" projection (while tuning "Gate" and "Up") regulates the "write-back" strength to the residual stream, limiting output distribution drift.
- **Mechanism:** In the SwiGLU architecture, the "Up" and "Gate" projections activate features (keys), but the "Down" projection writes the result back to the residual stream. Freezing "Down" prevents the model from aggressively altering the vocabulary space preferences while allowing it to adjust which features are activated for the target task.
- **Core assumption:** The Down projection ($W_{down}$) is the critical junction for influencing the final logits; gating its update limits drift.
- **Evidence anchors:**
  - [section 3.2] "Update W_gate, W_up while freezing W_down to regulate write-back."
  - [figure 5] "MLP (Gate & Up) delivers stronger target improvements with only a small held-out change."
  - [corpus] General corpus support for specific MLP sub-component freezing strategies is weak in the provided neighbors.
- **Break condition:** If target task learning is insufficient (underfitting), the constraint on the Down projection may be too restrictive, preventing the necessary model capacity from being utilized.

## Foundational Learning

- **Concept:** Residual Stream & Write-Back
  - **Why needed here:** The paper frames forgetting as an additive interference in the residual stream. Understanding that layers add signals to a shared stream is necessary to see why freezing specific weights (Down/Out projections) prevents the output distribution from shifting.
  - **Quick check question:** Can you explain why modifying $W_{down}$ in an MLP affects the final logits more directly than modifying $W_{up}$ in the context of the residual stream?
- **Concept:** Transformer Division of Labor (Attention vs. MLP)
  - **Why needed here:** The success of the "SA Proj." recipe relies on the hypothesis that attention acts as routing while MLP acts as memory.
  - **Quick check question:** If you wanted to change *how* a model processes an input versus *what* facts it recalls, which component (Attention vs. MLP) would you target for each?
- **Concept:** Token Distribution Bias (Counting Probe)
  - **Why needed here:** The paper uses a specific probe (likelihood of numeric tokens) to diagnose forgetting. Understanding that a model can "forget" a task simply by preferring to output numbers is a core insight.
  - **Quick check question:** If a model suddenly describes every image as "a photo of a dog," is this a loss of vision or an output distribution bias?

## Architecture Onboarding

- **Component map:** Vision Encoder -> Projector -> [SA (Routing) + MLP (Writing)] -> Residual Stream -> LM Head
- **Critical path:** Visual Input -> Projector -> [SA (Routing) + MLP (Writing)] -> Residual Stream -> LM Head
- **Design tradeoffs:**
  - **SA Proj. Tune:** *Stability Maximization.* Best for preserving held-out performance; slightly lower target ceiling. Use for critical general-purpose models.
  - **MLP (Gate&Up) Tune:** *Balance.* High target gain, low forgetting. Use when strong new skill acquisition is needed but safety/stability is still a concern.
  - **Full MLP Tune:** *Plasticity Maximization.* Fastest learning, highest forgetting risk. Use only for specialized domain experts where general ability is expendable.
- **Failure signatures:**
  - **The "Counter" Hallucination:** The model answers counting questions correctly but starts describing standard images with numbers (e.g., "There are 2 photos"). -> Indicates MLP over-tuning.
  - **The "Zombie" Weights:** Held-out performance drops immediately after tuning a specific task but recovers after the *next* task. -> Indicates temporary output distribution bias, not permanent erasure.
- **First 3 experiments:**
  1. **Counting Bias Probe:** Before and after fine-tuning a counting task, measure the average probability of numeric tokens on a neutral captioning set (LCS-558K). Verify that "forgetting" correlates with this probability shift.
  2. **Layer-wise Ablation:** Sequentially tune a new skill using three configurations: (A) SA Proj only, (B) Full MLP, (C) MLP (Gate&Up). Plot the trade-off curve between Target Gain and Held-out Loss.
  3. **Recovery Check:** Fine-tune on Task A (Counting), observe performance drop on Task B (Captioning). Then fine-tune on Task C (Medical VQA) without rehearsing Task B. Check if Task B performance recovers, confirming the "distribution shift" theory.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the tuning recipes (SA Proj. and MLP Gate&Up) maintain their learning-stability trade-off in models with significantly larger parameter counts (e.g., 70B+)?
- Basis in paper: [explicit] The authors explicitly list "testing with much larger models" as a limitation requiring further study.
- Why unresolved: The study only evaluates 7B and 8B models; scaling laws may alter how attention and MLP layers contribute to forgetting.
- What evidence would resolve it: Replicating the sequential tuning experiments on 70B+ parameter models and observing if held-out performance remains stable.

### Open Question 2
- Question: Does the recovery of held-out performance persist in training sequences significantly longer than the five-task curriculum tested?
- Basis in paper: [explicit] The authors note they must leave "exploration of... longer sequences" to future work.
- Why unresolved: It is unclear if "temporary" forgetting recovery is a short-sequence anomaly or a fundamental property of LMM sequential tuning.
- What evidence would resolve it: Evaluating the SA Proj. and MLP (Gate&Up) methods on a curriculum of 10, 20, or 50 tasks.

### Open Question 3
- Question: How does the relationship between MLP tuning and output distribution shift generalize to non-visual modalities, such as audio or video?
- Basis in paper: [explicit] The paper identifies "additional modalities, such as audio" as an area requiring further investigation.
- Why unresolved: The theory linking MLP "write-back" to token bias is validated on image-text tasks; audio tokens may interact differently with the residual stream.
- What evidence would resolve it: Applying the counting-bias probe and tuning recipes to an audio-enabled multimodal model.

## Limitations
- The study only evaluates 7B and 8B models, leaving scalability to larger models untested.
- The effectiveness of output distribution probes (like NTB) for diagnosing forgetting in non-counting tasks remains unproven.
- The paper does not investigate whether the tuning recipes preserve or interfere with safety alignments in the base model.

## Confidence
- **High Confidence:** The experimental observation that sequential fine-tuning causes performance drops on held-out tasks, and that these drops can be mitigated by selective parameter updates (SA Proj., MLP Gate&Up).
- **Medium Confidence:** The theoretical explanation that output distribution shifts (not permanent erasure) drive much of the observed forgetting, supported by the NTB probe and recovery patterns. The division-of-labor hypothesis (Attention = Routing, MLP = Memory) is plausible but relies on indirect evidence.
- **Low Confidence:** The specific mechanism by which freezing the Down projection prevents forgetting is not fully explained or experimentally isolated. The general applicability of the NTB probe to diagnose forgetting for all task types is assumed but not validated.

## Next Checks
1. **Probe Validation Across Task Types:** Develop and apply token-distribution probes for non-counting tasks (e.g., likelihood of chart-related tokens after ChartQA fine-tuning, medical terms after PathVQA) to verify if Mechanism 1 (distribution shift) universally explains forgetting.
2. **Component Ablation for MLP Tuning:** Conduct an ablation study that freezes different MLP sub-components (Gate, Up, Down) individually to isolate the specific role of the Down projection in preventing forgetting (Mechanism 3).
3. **Cross-Architecture Transfer Test:** Apply the SA Proj. and MLP (Gate&Up) recipes to a different LMM family (e.g., Qwen2.5-VL-72B or a non-LLaVA architecture) and evaluate on the same task sequence to test generalizability.