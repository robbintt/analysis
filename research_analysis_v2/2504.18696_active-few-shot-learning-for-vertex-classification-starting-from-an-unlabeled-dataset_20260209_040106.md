---
ver: rpa2
title: Active Few-Shot Learning for Vertex Classification Starting from an Unlabeled
  Dataset
arxiv_id: '2504.18696'
source_url: https://arxiv.org/abs/2504.18696
tags:
- learning
- vertices
- class
- sampling
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses few-shot vertex classification on graphs starting
  from an unlabeled dataset without any pre-existing labels. The authors combine active
  learning with prototypical networks, using k-medoids clustering to assign pseudo-labels
  and various sampling strategies (random, entropy, PageRank, medoid) to select vertices
  for human annotation.
---

# Active Few-Shot Learning for Vertex Classification Starting from an Unlabeled Dataset

## Quick Facts
- arXiv ID: 2504.18696
- Source URL: https://arxiv.org/abs/2504.18696
- Authors: Felix Burr; Marcel Hoffmann; Ansgar Scherp
- Reference count: 40
- One-line primary result: Prototypical models consistently outperform discriminative models when fewer than 20 samples per class are available, maintaining robustness when class oracle assumptions are dropped.

## Executive Summary
This paper addresses few-shot vertex classification on graphs starting from an unlabeled dataset without any pre-existing labels. The authors combine active learning with prototypical networks, using k-medoids clustering to assign pseudo-labels and various sampling strategies (random, entropy, PageRank, medoid) to select vertices for human annotation. They conduct three experiments with progressively relaxed assumptions: balanced sampling with class oracle, unbalanced sampling with k-medoids clustering, and unknown number of classes using estimation. Their results show that prototypical models consistently outperform discriminative models when fewer than 20 samples per class are available. While dropping the class oracle assumption reduces discriminative model performance by 9%, prototypical networks only lose 1% on average. The study demonstrates that estimating the number of classes does not significantly impact performance, and label propagation provides benefits especially in early training stages. The medoid sampling strategy performs best overall across all experiments.

## Method Summary
The method combines active learning with prototypical networks for few-shot vertex classification on graphs. Starting from an unlabeled dataset, the approach uses a GCN backbone to generate embeddings, applies k-medoids clustering to assign pseudo-labels, and employs various sampling strategies to select vertices for human annotation. The prototypical network computes class prototypes as PageRank-weighted means of embeddings, classifying based on Euclidean distance. Label propagation with entropy filtering is optionally applied to expand the training set. The active learning loop iteratively updates the model with newly labeled vertices until the annotation budget is exhausted.

## Key Results
- Prototypical models consistently outperform discriminative models when fewer than 20 samples per class are available.
- While dropping the class oracle assumption reduces discriminative model performance by 9%, prototypical networks only lose 1% on average.
- The medoid sampling strategy performs best overall across all experiments.
- Estimating the number of classes does not significantly impact performance compared to knowing the true class count.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Prototypical models maintain performance when class oracle assumptions are dropped.
- **Mechanism:** Prototypes are calculated as the PageRank-weighted mean of embeddings in a class, and classification is based on Euclidean distance to these prototypes. This distance-based approach appears more stable when the class distribution is imperfect (e.g., via k-medoids clustering) compared to the decision boundaries learned by a discriminative GCN.
- **Core assumption:** Embeddings from the GNN form a space where Euclidean distance correlates with semantic similarity.
- **Evidence anchors:**
  - [abstract] "While dropping the assumption of the class oracle for the 'Unbalanced Sampling' experiment reduces the performance of the GCN by 9%, the prototypical network loses only 1% on average."
  - [section] "The prototype $r_c$ of class $c$ is the weighted mean of the embeddings... The weight is the normalized PageRank score."
- **Break condition:** If embeddings do not form a meaningful metric space (e.g., in highly heterophilic graphs), the distance-to-prototype logic fails.

### Mechanism 2
- **Claim:** K-medoids sampling on model embeddings selects more representative vertices for annotation.
- **Mechanism:** K-medoids clustering operates directly on the learned embeddings (which combine feature and graph structure information) rather than just graph topology (like PageRank) or random selection. It identifies central points ("medoids") within clusters of the embedding space, which are more likely to be representative of their class and robust to outliers.
- **Core assumption:** The learned embeddings form clusters that roughly correspond to the true classes.
- **Evidence anchors:**
  - [abstract] "The medoid sampling strategy performs best overall across all experiments."
  - [section] "As such, PageRank does not take features from vertices into account but only operates on the edges. The k-medoids clustering is applied to the model embeddings, effectively considering processed features and edges from the graph."
- **Break condition:** If embeddings are poor (e.g., early in cold-start), clusters will not align with classes, making medoids unrepresentative.

### Mechanism 3
- **Claim:** Label propagation (LP) on homophilic graphs boosts few-shot learning performance, particularly in early stages.
- **Mechanism:** LP propagates labels from the few human-annotated vertices to their neighbors based on graph structure ($Y' = \alpha \cdot D^{-1/2}AD^{-1/2}Y + (1-\alpha)Y$). This artificially expands the training set, providing more signal for the model.
- **Core assumption:** The graph is homophilic (connected vertices tend to share labels).
- **Evidence anchors:**
  - [abstract] "Label propagation provides benefits especially in early training stages."
  - [section] "We assume that the dataset is homophilic, which is the foundation of being able to apply label propagation."
- **Break condition:** On heterophilic graphs, LP will propagate incorrect labels, actively harming model training.

## Foundational Learning

### Concept: Prototypical Networks
- **Why needed here:** The paper contrasts these with discriminative models (like GCN) as the core architecture for stable few-shot learning.
- **Quick check question:** Can you explain how a class is represented in a Prototypical Network versus a standard classifier?

### Concept: Active Learning
- **Why needed here:** The paper's entire premise is iteratively selecting the most informative vertices to label given a budget.
- **Quick check question:** What is the "cold start" problem in active learning, and how does this paper address it?

### Concept: Graph Neural Networks (GCN/GAT)
- **Why needed here:** These are the backbone models used to generate the embeddings for both the prototypical and discriminative approaches.
- **Quick check question:** How does a GCN update a node's representation based on its neighbors?

## Architecture Onboarding

### Component map:
GCN Backbone -> Embedding Space -> K-medoids Clustering -> Sampling Strategy -> Annotation Selection -> (Optional: Label Propagation) -> Model Update -> (Repeat)

### Critical path:
1. **Cold Start:** Initialize GCN. Assign pseudo-labels to $V_u$ via k-medoids on raw/embedded features.
2. **Select:** Use a sampler (e.g., Medoid) to pick a vertex from each pseudo-cluster for annotation.
3. **Propagate (Optional):** Run LP to label neighbors of the newly annotated vertex.
4. **Train:** Update GCN/GPN weights on the newly labeled set (human + pseudo/LP).
5. **Repeat:** Re-cluster embeddings, re-select, and re-train until budget $B$ is exhausted.

### Design tradeoffs:
- **Prototypical vs. Discriminative:** Prototypical is more robust to unbalanced/imperfect class sampling but requires a distance-based loss. Discriminative is standard but sensitive to class imbalance in the few-shot setting.
- **Medoid vs. Entropy Sampling:** Medoid selects representatives and is stable. Entropy is unstable at cold start because model confidence is meaningless.
- **Label Propagation:** Can boost performance early on but introduces noise. Must balance $\alpha$ and entropy threshold to filter bad labels.

### Failure signatures:
- **High variance in accuracy across runs:** Indicates sampling instability or sensitivity to initial seeds (common in discriminative models here).
- **Performance degrades with more labels (when using LP):** Suggests propagated label noise is overwhelming the signal; check entropy threshold.
- **Medoid sampler underperforms Random:** Suggests embeddings are not clustering by class; backbone model may need adjustment or pre-training.

### First 3 experiments:
1. **Baseline (Balanced):** Reproduce the "Balanced Sampling" experiment. Assume a class oracle. Compare GCN vs. GPN accuracy with random sampling. This validates the backbone implementation.
2. **Ablation (Unbalanced):** Drop the class oracle. Use k-medoids clustering on embeddings for sampling. This tests the core contribution of robustness to the "real-world" setting.
3. **Complete System:** Add the "Unknown Number of Classes" estimation component. Use the class estimator, then run the full active learning loop. Compare against the ground-truth $k$ to measure the impact of estimation error.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can active few-shot learning strategies be adapted for heterophilic graphs where label propagation is less effective?
- **Basis in paper:** [explicit] The authors state, "There is a need to investigate vertex selection strategies for few-shot learning on heterophilic graphs," noting that their current method relies on the homophily assumption.
- **Why unresolved:** The current methodology depends on homophily for label propagation to artificially expand the training set; this assumption fails in heterophilic networks, potentially rendering the sampling strategies ineffective.
- **What evidence would resolve it:** Experiments on standard heterophilic benchmark datasets (e.g., Chameleon, Actor) demonstrating that the proposed sampling strategies maintain high accuracy without relying on homophilic label propagation.

### Open Question 2
- **Question:** Can "model soup" (weight averaging) effectively mitigate the performance issues caused by fixed, potentially sub-optimal hyperparameters in a cold-start setting?
- **Basis in paper:** [explicit] The authors identify hyperparameter fixation as a limitation and explicitly propose that "using model soup [41] could alleviate the issue" of training without a validation set.
- **Why unresolved:** In a cold-start scenario, there is no labeled validation data to tune hyperparameters, leading to potential performance drops (e.g., on ogb-arXiv); it is unknown if averaging weights from randomly initialized or varied hyperparameter settings would stabilize results.
- **What evidence would resolve it:** A comparison of model performance variance and peak accuracy between standard fixed-parameter training and a "model soup" ensemble approach in the absence of a validation set.

### Open Question 3
- **Question:** How do alternative GNN architectures compare to GCN-based backbones in the active few-shot learning setting without a class oracle?
- **Basis in paper:** [explicit] The authors note that "future work may extend this study by... leveraging alternative GNN architectures," as the current study primarily utilizes Graph Convolutional Networks (GCN).
- **Why unresolved:** While GCNs are discriminative, the paper establishes that prototypical networks outperform them; it remains unclear if more complex architectures (e.g., Graph Attention Networks or GraphSAGE) would close this gap or further widen it.
- **What evidence would resolve it:** Ablation studies replacing the GCN backbone with alternative architectures (like GAT or GraphSAGE) within both the prototypical and discriminative frameworks to measure relative performance changes.

## Limitations
- The assumption of homophilic graphs for label propagation may not hold in many real-world networks, potentially limiting applicability.
- Cold-start embedding quality could affect pseudo-label assignment through k-medoids clustering.
- Results may not generalize to highly heterophilic or dynamic graphs beyond the tested citation networks and large-scale datasets.

## Confidence

**High Confidence:**
- Prototypical networks outperform discriminative models in low-shot regimes (fewer than 20 samples per class).
- The medoid sampling strategy consistently performs best across experiments.

**Medium Confidence:**
- The robustness of prototypical networks when dropping class oracle assumptions (1% vs 9% performance drop).
- Label propagation benefits in early training stages are observed but may be dataset-dependent.

**Low Confidence:**
- The impact of estimating the number of classes on final performance.
- The elbow method with Deep Graph Infomax may not be optimal for all graph types.

## Next Checks
1. **Heterophily Stress Test:** Evaluate the proposed framework on heterophilic graphs (e.g., Texas, Wisconsin datasets) to verify the limitations of label propagation and test the robustness of prototypical models.
2. **Embedding Quality Analysis:** Measure the alignment between k-medoids clusters and true classes at different training stages to quantify the impact of cold-start embedding quality on pseudo-label assignment.
3. **Hyperparameter Sensitivity:** Systematically vary the label propagation parameters (Î±, entropy threshold) and sampling batch size to identify optimal settings across different graph characteristics and dataset sizes.