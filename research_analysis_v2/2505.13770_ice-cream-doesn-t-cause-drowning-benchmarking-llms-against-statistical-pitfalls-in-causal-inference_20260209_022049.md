---
ver: rpa2
title: 'Ice Cream Doesn''t Cause Drowning: Benchmarking LLMs Against Statistical Pitfalls
  in Causal Inference'
arxiv_id: '2505.13770'
source_url: https://arxiv.org/abs/2505.13770
tags:
- causal
- llms
- inference
- reasoning
- statistical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CausalPitfalls, a comprehensive benchmark
  designed to rigorously evaluate large language models' (LLMs) capabilities in statistical
  causal inference, specifically targeting their reliability in handling common causal
  pitfalls such as confounding, selection bias, and counterfactual reasoning. Unlike
  existing benchmarks focused primarily on accuracy, CausalPitfalls assesses whether
  models can avoid misleading conclusions by incorporating both direct prompting and
  code-assisted prompting protocols.
---

# Ice Cream Doesn't Cause Drowning: Benchmarking LLMs Against Statistical Pitfalls in Causal Inference

## Quick Facts
- **arXiv ID**: 2505.13770
- **Source URL**: https://arxiv.org/abs/2505.13770
- **Reference count**: 40
- **Primary result**: Introduces CausalPitfalls, a benchmark revealing significant reliability gaps in LLMs' causal inference, with top model achieving only 44.63% average normalized score.

## Executive Summary
This paper presents CausalPitfalls, a novel benchmark designed to rigorously evaluate large language models' (LLMs) capabilities in statistical causal inference, specifically targeting their ability to avoid common pitfalls such as confounding, selection bias, and counterfactual reasoning. Unlike existing benchmarks focused primarily on accuracy, CausalPitfalls assesses whether models can avoid misleading conclusions by incorporating both direct prompting and code-assisted prompting protocols. Across 75 questions and 75 datasets structured into six major categories, evaluation of five state-of-the-art LLMs revealed significant reliability gaps, with even the best-performing model (GPT-o4-mini) achieving only 44.63% average normalized score under code-assisted prompting. The benchmark also introduced a quantitative "causal reliability" metric, validated through human expert comparison, to standardize assessments of model trustworthiness in causal reasoning tasks.

## Method Summary
The authors created a comprehensive benchmark consisting of 75 questions paired with 75 synthetic datasets, each generated via structural causal models (DAGs) with 500+ samples. The benchmark is organized into six categories covering 15 specific challenges in causal inference, ranging from confounding to external validity. Two prompting protocols were used: direct prompting where LLMs answer from raw data, and code-assisted prompting where LLMs generate executable code before answering. An automated grading system using GPT-4o validated against three human experts scored responses against detailed rubrics. The primary metric was normalized score per question, with overall "causal reliability" defined as the average normalized score across all tasks.

## Key Results
- GPT-o4-mini achieved the highest performance at 44.63% average normalized score under code-assisted prompting.
- Code-assisted prompting significantly improved performance, especially on harder tasks, though execution errors remained high in complex challenges.
- Models showed vulnerability to semantic priors (e.g., branding bias) and confounding assumptions, even when data contradicted these priors.
- Causal reliability gaps were substantial, with no model achieving reliable performance across all challenge types.

## Why This Works (Mechanism)
The benchmark works by systematically exposing LLMs to structured causal inference scenarios where intuitive but incorrect conclusions are common. By requiring both direct and code-assisted responses, it separates conceptual understanding from technical implementation skills. The synthetic datasets with known causal structures provide ground truth for evaluation, while the diverse challenge types ensure comprehensive coverage of statistical pitfalls.

## Foundational Learning
- **Structural Causal Models (DAGs)**: Why needed - to generate synthetic datasets with known causal relationships for controlled evaluation. Quick check - verify generated data follows specified causal structure using d-separation tests.
- **Counterfactual Reasoning**: Why needed - to assess models' ability to reason about what would happen under different interventions. Quick check - ensure models can correctly identify and reason about potential outcomes.
- **Code Generation for Statistical Analysis**: Why needed - to evaluate whether models can translate causal reasoning into executable statistical code. Quick check - verify generated code runs without syntax errors and produces correct statistical results.

## Architecture Onboarding

**Component Map**
- Question/Prompt Generation -> Synthetic Data Generation -> LLM Response Collection -> Automated Grading -> Causal Reliability Calculation

**Critical Path**
The most critical path is Question Generation -> LLM Response Collection -> Automated Grading, as errors at any point propagate to final reliability scores.

**Design Tradeoffs**
- Synthetic vs. real data: Synthetic data ensures known ground truth but may miss real-world complexities.
- Direct vs. code-assisted prompting: Direct is simpler but may miss implementation errors; code-assisted is more rigorous but introduces code execution failures.

**Failure Signatures**
- Code execution errors peak in mediation and external validity categories, especially for "very easy" questions where hints induce complex code.
- Models rely on semantic priors (branding bias) when data is ambiguous or contradictory.
- Confounding assumptions lead to incorrect conclusions even with contradictory data.

**3 First Experiments**
1. Run control datasets (pure noise) to confirm models cannot rely solely on priors or semantic cues.
2. Implement automated grader and validate against 3 human experts to verify 0.11 gap metric.
3. Test code execution error rates across all challenge types to confirm reported patterns.

## Open Questions the Paper Calls Out
- How can the benchmark be expanded to evaluate LLMs on advanced causal reasoning tasks such as instrumental variable analysis and latent confounding? [explicit]
- Can fine-tuning strategies guided by the benchmark successfully instill "causal robustness" in LLMs? [explicit]
- What specific mechanisms are required to reduce high code execution failure rates in code-assisted prompting for complex causal tasks? [inferred]

## Limitations
- Exact grading rubrics per challenge only referenced in appendix, not provided in text.
- Full dataset generation code (DAG structures, coefficients, noise distributions) and exact prompts for all 75 questions unspecified.
- Code-assisted sandbox configuration (libraries, execution time limits) not specified.

## Confidence
- **High confidence** in benchmark's core premise: evaluating LLMs' ability to avoid statistical causal inference pitfalls is valid and important.
- **Medium confidence** in quantitative findings: 44.63% score plausible but exact replication requires unspecified rubrics and data.
- **Low confidence** in specific causal reliability metric implementation without full validation methodology.

## Next Checks
1. Implement automated grading system and validate against 3 human experts to verify 0.11 gap metric.
2. Run benchmark with synthetic datasets containing pure noise to confirm models cannot rely solely on priors.
3. Systematically log and categorize code execution errors to verify reported error patterns, especially for "very easy" questions.