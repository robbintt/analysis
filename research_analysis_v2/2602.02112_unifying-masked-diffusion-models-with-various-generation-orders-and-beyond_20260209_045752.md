---
ver: rpa2
title: Unifying Masked Diffusion Models with Various Generation Orders and Beyond
arxiv_id: '2602.02112'
source_url: https://arxiv.org/abs/2602.02112
tags:
- diffusion
- masked
- generation
- scheduler
- oemdm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving generation quality
  in masked diffusion models (MDMs) for language modeling, where the quality heavily
  depends on the generation order. The authors propose Order-expressive Masked Diffusion
  Model (OeMDM), a generalized framework that explicitly models the generation order
  through context-aware schedulers, unifying various paradigms including ARMs, MDMs,
  BD3LMs, and GenMD4.
---

# Unifying Masked Diffusion Models with Various Generation Orders and Beyond

## Quick Facts
- arXiv ID: 2602.02112
- Source URL: https://arxiv.org/abs/2602.02112
- Reference count: 40
- Authors: Chunsan Hong; Sanghyun Lee; Jong Chul Ye
- Primary result: Introduces LoMDM that achieves 25.4 test perplexity on LM1B and 20.4 on OpenWebText, surpassing MDLM by 3+ points while using only 18% of the training steps

## Executive Summary
This paper addresses the challenge of improving generation quality in masked diffusion models (MDMs) for language modeling, where the quality heavily depends on the generation order. The authors propose Order-expressive Masked Diffusion Model (OeMDM), a generalized framework that explicitly models the generation order through context-aware schedulers, unifying various paradigms including ARMs, MDMs, BD3LMs, and GenMD4. Building on OeMDM, they introduce Learnable-order Masked Diffusion Model (LoMDM), which jointly learns the generation ordering and diffusion backbone through a single objective from scratch, enabling context-dependent ordering during inference. LoMDM outperforms various discrete diffusion baselines, achieving 25.4 test perplexity on LM1B and 20.4 on OpenWebText, surpassing MDLM by 3+ points. Notably, LoMDM matches MDLM's 1M-step performance in just 180K steps, demonstrating faster and more efficient learning. The learned scheduler also improves generation quality, as evidenced by lower generative perplexity across different sampling settings.

## Method Summary
The paper introduces a generalized masked diffusion framework where the unmasking order is explicitly modeled through a position-dependent scheduler. The key innovation is a velocity-based parameterization that determines which tokens are unmasked at each denoising step. The forward process uses a learned scheduler to generate corrupted sequences, while the reverse process jointly trains the backbone and scheduler to minimize a velocity-matched NELBO. The scheduler is trained using a REINFORCE Leave-One-Out estimator to handle the discrete nature of token masking. This unified approach encompasses various generation orders and enables context-dependent ordering during inference.

## Key Results
- LoMDM achieves 25.4 test perplexity on LM1B, surpassing MDLM by 3+ points
- On OpenWebText, LoMDM achieves 20.4 test perplexity, again outperforming MDLM
- LoMDM matches MDLM's 1M-step performance in just 180K steps (18% of steps)
- The learned scheduler shows positive correlation with reconstruction confidence, suggesting curriculum learning
- LoMDM demonstrates lower generative perplexity across different sampling settings compared to baselines

## Why This Works (Mechanism)

### Mechanism 1: Position-Dependent Velocity Control
Conditionally, generation quality improves when the unmasking order is explicitly controlled by a position-dependent "velocity" rather than random sampling. The framework replaces the standard uniform noise scheduler $\alpha(t)$ with a free-form scheduler $\alpha^{(i)}(u, t)$ that varies by position $i$ and context $u$. This induces a velocity $A^{(i)} = -\partial_t \alpha^{(i)} / (1-\alpha^{(i)})$ which dictates the priority of token denoising; high velocity tokens are unmasked earlier. The core assumption is that optimal generation requires a specific ordering rather than the order-agnostic random unmasking found in standard MDLM. If the learned velocity $A^{(i)}$ becomes uniform across all positions $i$, the mechanism collapses back to standard random-order MDM, negating the performance gains.

### Mechanism 2: Velocity-Matched NELBO
Jointly learning the scheduler and backbone is theoretically grounded by a modified Negative Evidence Lower Bound (NELBO) that penalizes mismatch between forward corruption and reverse denoising velocities. The derived NELBO adds a regularization term $L_{velocity} = A(\log A - \log \hat{A}) - (A - \hat{A})$. This term is minimized when the forward velocity $A$ matches the reverse velocity $\hat{A}$, forcing the model to learn a denoising path that mirrors the corruption path. The core assumption is that aligning the generative process with the corruption process via velocity matching provides a more stable training signal than treating them as independent. If $L_{velocity}$ is ignored or dominates the reconstruction loss $L_{main}$, the scheduler may fail to align with the backbone, leading to optimization instability.

### Mechanism 3: Confidence-Based Curriculum
Empirically, the learned scheduler acts as a curriculum, prioritizing the reconstruction of tokens the backbone predicts with high confidence. The scheduler parameter $\phi$ is trained to assign high velocity $A_\phi$ to tokens where the backbone $\theta$ achieves high reconstruction confidence $\langle x_\theta, x \rangle$. This focuses the $L_{main}$ training signal on tokens that are currently "easiest" for the model to predict given the partial context. The core assumption is that a curriculum starting with high-confidence tokens leads to better global optimization than forcing low-confidence predictions early. If the backbone is extremely undertrained and provides noise rather than signal, the scheduler cannot correlate velocity with confidence, failing to establish a useful curriculum.

## Foundational Learning

- **Concept: Continuous-Time Discrete Diffusion (MDLM)**
  - Why needed here: LoMDM modifies the fundamental equations of continuous-time masked diffusion. You must understand the baseline forward/reverse process to grasp why making the scheduler position-dependent is a generalization.
  - Quick check question: Can you derive the standard MDLM NELBO starting from the KL divergence between the forward and reverse processes?

- **Concept: REINFORCE Leave-One-Out (RLOO) Estimator**
  - Why needed here: The gradient of the scheduler $\phi$ involves an expectation over discrete samples $z_t$. The paper uses RLOO to reduce the variance of this estimator, which is critical for stable joint training.
  - Quick check question: Why is the naive REINFORCE estimator $\nabla \log q \cdot L$ high-variance in this context, and how does subtracting a baseline constructed from independent samples help?

- **Concept: SUBS Parametrization**
  - Why needed here: The derivation relies on "SUBS" parametrization where the model never predicts the [MASK] token and copies unmasked inputs. This ensures the "carry-over unmasking" property required for the NELBO derivation.
  - Quick check question: How does preventing the model from predicting [MASK] simplify the support of the distribution and enable the NELBO derivation in Appendix C?

## Architecture Onboarding

- **Component map:** Input $x$ -> Feature Extractor $f(x)$ (stop-grad) -> Forward Scheduler $\phi$ predicts velocity -> Sample mask based on velocity -> Masked $z_t$ -> Backbone $\theta$ predicts clean $x_\theta$ -> Compute $L_{main}$ and $L_{velocity}$

- **Critical path:** Input $x \rightarrow f(x)$ (stop-grad) $\rightarrow$ Scheduler $\phi$ predicts velocity $\rightarrow$ Sample mask based on velocity $\rightarrow$ Masked $z_t \rightarrow$ Backbone $\theta$ predicts clean $x_\theta$ \rightarrow Compute $L_{main}$ and $L_{velocity}$

- **Design tradeoffs:** Efficiency requires 2 forward/backward passes of the small scheduler networks and 2 forward passes of the backbone per batch, slowing down step-time. However, this is offset by faster convergence (18% of MDLM steps needed), assuming the curriculum learning effect activates correctly.

- **Failure signatures:** Collapse to Uniform if the learned velocity variance goes to zero, reverting to standard random-order MDM. Divergent Velocity if $c_1$ and $c_2$ are poorly scaled such that $A \approx 0$ or $A \to \infty$, potentially causing NELBO divergence.

- **First 3 experiments:**
  1. Ablation on Scheduler Hyperparameters: Train LoMDM with varying $(c_1, c_2)$ to replicate the finding that $c_2 \neq 0$ (learnable order) is strictly better than $c_2=0$ (fixed order).
  2. Correlation Study: Monitor the Pearson correlation between $A_\phi$ and backbone confidence $\langle x_\theta, x \rangle$. If correlation does not increase, the curriculum mechanism is failing.
  3. Speed-Efficiency Trade-off: Measure wall-clock time to reach a target perplexity (e.g., 23.0 PPL) against MDLM to verify the "efficient learner" claim despite the extra forward passes.

## Open Questions the Paper Calls Out

### Open Question 1
Does the 5x sample efficiency improvement demonstrated by LoMDM on smaller-scale models persist when scaling to multi-billion parameter architectures typically used in state-of-the-art LLMs? The paper evaluates on standard benchmarks using a 12-layer, 768-hidden-dimension model, noting that LoMDM matches MDLM's 1M-step performance in 180K steps. However, it does not validate if the "fast and efficient learning" scales linearly with parameter count or if the overhead of the scheduler networks becomes a bottleneck at larger scales. Training LoMDM on a large-scale dataset (e.g., The Pile) using a model with $\geq$1B parameters and comparing the step-to-convergence ratio and wall-clock time against baselines like MDLM would resolve this question.

### Open Question 2
Does the learned context-dependent generation order exhibit interpretable linguistic structure, such as syntax trees or semantic chunking, or is it merely a loss-minimization heuristic? Figure 4 demonstrates a high Pearson correlation between the learned velocity $A_\phi$ and reconstruction confidence, suggesting the model prioritizes "easier" tokens. However, the paper does not analyze whether this prioritization aligns with higher-level grammatical or semantic structures in the text. A qualitative analysis of the generation trajectories for specific sentences to check for alignment with dependency parses or named entity structures would resolve this question.

### Open Question 3
Can the two-sample RLOO gradient estimator required for training stability be replaced by a single-sample method to strictly improve wall-clock training throughput? Section 4.3 and Algorithm 1 explicitly state that the method requires sampling $z_t^1, z_t^2 \sim q_{\alpha_\phi}$ and an additional forward pass for the RLOO term, resulting in a lower "number of tokens seen per second" than the baseline. While RLOO reduces variance, the computational cost of double sampling per batch step is non-trivial. The development and empirical validation of a modified loss function or variance reduction technique that trains $\alpha_\phi$ effectively without requiring multiple forward passes of the diffusion backbone per batch would resolve this question.

## Limitations
- Evaluation metric inconsistency: The comparison with MDLM may be confounded by different tokenizer choices and sentence packing usage
- Scheduler architecture details: Exact hidden dimensions, activation functions, and initialization scheme for scheduler networks are unspecified
- Correlation interpretation: Correlation between velocity and confidence doesn't prove causation or interpretability
- Break condition risk: The learned velocity could collapse to uniform distribution, reverting to standard random-order MDM

## Confidence
- Performance claims (PPL improvement): Medium - Results are impressive but depend on specific evaluation settings
- Curriculum learning mechanism: Low-Medium - Correlation evidence is suggestive but not definitive proof
- Faster convergence: Medium - The 18% step claim is supported but assumes curriculum effect activates correctly
- Theoretical grounding (NELBO derivation): High - The mathematical framework appears sound

## Next Checks
1. Replication with controlled settings: Reimplement LoMDM on a small dataset (e.g., 10K samples from LM1B) with exact hyperparameter specification, focusing on whether the learned velocity shows position-dependent patterns and whether it consistently outperforms fixed-order baselines.

2. Velocity stability monitoring: During training, track (a) the variance of the learned velocity distribution across positions, (b) the correlation between velocity and reconstruction confidence over time, and (c) whether velocity ever collapses to near-uniform distribution.

3. Ablation on curriculum signal: Train LoMDM with the velocity matching loss $L_{velocity}$ disabled (only optimizing $L_{main}$) to determine whether the theoretical grounding actually contributes to performance or if the gains come primarily from the modified objective structure.