---
ver: rpa2
title: 'MathSight: A Benchmark Exploring Have Vision-Language Models Really Seen in
  University-Level Mathematical Reasoning?'
arxiv_id: '2511.23112'
source_url: https://arxiv.org/abs/2511.23112
tags:
- visual
- reasoning
- problems
- image
- mathematical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MathSight, a benchmark designed to isolate
  and quantify the role of visual information in multimodal mathematical reasoning.
  Unlike existing benchmarks that rely on a single visual variant, MathSight includes
  original, hand-drawn, and photo-captured versions of each problem, along with a
  text-only condition, enabling controlled analysis of how visual variations impact
  reasoning.
---

# MathSight: A Benchmark Exploring Have Vision-Language Models Really Seen in University-Level Mathematical Reasoning?

## Quick Facts
- arXiv ID: 2511.23112
- Source URL: https://arxiv.org/abs/2511.23112
- Reference count: 40
- Primary result: VLMs perform better on math problems without visual input, suggesting they rely on textual priors rather than visual understanding

## Executive Summary
MathSight introduces a novel benchmark to isolate and quantify the role of visual information in multimodal mathematical reasoning. Unlike existing benchmarks, it includes original, hand-drawn, and photo-captured versions of each problem, plus a text-only condition, enabling controlled analysis of how visual variations impact reasoning. Experiments reveal that visual contributions diminish as problem difficulty increases, and that performance differences across image variants are not statistically significant. Notably, Qwen3-VL without any image input achieves 50.53% accuracy, surpassing its multimodal variants and GPT-5, indicating that current VLMs rely more on textual priors than on genuine visual understanding.

## Method Summary
MathSight evaluates VLMs on university-level mathematical reasoning across visual variants (original, hand-drawn, photo-captured) and text-only conditions. The benchmark consists of 661 multimodal problems (603 graduate-level) and 1,387 text-only problems from proprietary PDFs, spanning 6 mathematical domains including 30 proving problems. Evaluation uses zero-shot inference via API or vLLM library with prompts from Appendix C.1/C.2. For non-proof questions, accuracy is computed by comparing model answers to ground truth for mathematical equivalence. For proof problems, logical consistency metrics (GOM, GSD, GCV) based on sliding-window token confidence are used to assess reasoning stability.

## Key Results
- Qwen3-VL without image input achieves 50.53% accuracy, surpassing its multimodal variants (40.85%, 38.73%, 39.33%) and GPT-5 (45.39%)
- Visual contribution diminishes as problem difficulty increases
- Performance differences across visual variants (original, hand-drawn, photo) are not statistically significant
- More than 80% of cases show complete consistency across visual variants (all correct or all incorrect)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Current VLMs predominantly rely on textual priors and linguistic abstraction rather than genuine visual understanding when solving mathematical problems.
- Mechanism: The paper demonstrates this through a controlled ablation where Qwen3-VL without any image input (text-only) achieved 50.53% accuracy, surpassing its own multimodal variants and GPT-5. This suggests that visual tokens may introduce noise that interferes with reasoning rather than enhancing it.
- Core assumption: The problem text contains sufficient information for reasoning, and images are supplementary rather than essential.
- Evidence anchors:
  - [abstract] "Notably, Qwen3-VL without any image input achieves 50.53% accuracy, surpassing its multimodal variants and GPT-5 (45.39%), indicating that current VLMs rely more on textual priors than on genuine visual understanding."
  - [section 3.6] "Removing the image input leads to a substantial performance improvement, boosting accuracy from 40.85% to 50.53% and even outperforming the state-of-the-art GPT-5 (45.39%)."
  - [corpus] Related work on multimodal reasoning (VMMU, ReadBench) does not directly replicate this text-vs-multimodal comparison for mathematical reasoning, limiting external validation.
- Break condition: If visual-only inputs (images without any textual problem statement) can be solved at comparable or higher accuracy than text-only, this mechanism would not hold.

### Mechanism 2
- Claim: Visual information contribution diminishes as problem difficulty increases in university-level mathematical reasoning.
- Mechanism: The paper analyzes performance across difficulty levels and mathematical categories, finding that higher difficulty correlates with lower visual contribution. Algebraic and probabilistic problems, where reasoning can proceed from textual/symbolic cues, show higher accuracy than calculus and analysis problems requiring conceptual abstraction or geometric interpretation.
- Core assumption: Problem difficulty can be systematically categorized and correlates with the cognitive demands of visual integration.
- Evidence anchors:
  - [abstract] "Experiments on state-of-the-art Vision-Language Models (VLMs) reveal that visual contributions diminish as problem difficulty increases."
  - [section 3.4] "Models generally achieve the highest accuracy on algebraic and probabilistic problems, where reasoning can often proceed from symbolic or textual cues alone. In contrast, performance on calculus and analysis mathematics is consistently lower."
  - [corpus] PRiSM (arXiv:2512.05930) notes scientific domains demand "conceptual understanding, symbolic reasoning, and adherence to formal laws," but does not quantify visual contribution by difficulty.
- Break condition: If easier problems show similar performance gaps between text-only and multimodal variants, or if visual aids uniquely enable solving of hard problems, this mechanism would not hold.

### Mechanism 3
- Claim: Variations in visual representation (original, hand-drawn, photo-captured) do not produce statistically significant differences in VLM reasoning performance.
- Mechanism: By augmenting each problem with multiple visual variants while keeping the problem text constant, the paper isolates the effect of visual appearance. Across models, performance differences between variants were marginal (typically 1-2 percentage points), suggesting models are insensitive to visual form and treat images as auxiliary cues.
- Core assumption: Hand-drawn and photo-captured versions are valid proxies for real-world visual variation and contain equivalent semantic content to original figures.
- Evidence anchors:
  - [section 3.3] "Across both closed-source and open-source model, we observe a consistent trend: although accuracy slightly varies across image variants, the difference remain marginal, suggesting that visual appearance has only a limited impact on university-level mathematical reasoning."
  - [section 3.7] "More than 80% of all cases remain fully consistent, either all correct (A✓B✓C✓) or all incorrect (A✗B✗C✗), while only about 18% show any variation across visual versions."
  - [corpus] Decomposing Complex Visual Comprehension (arXiv:2505.20021) decomposes visual comprehension into atomic skills, but does not test robustness to visual variants directly.
- Break condition: If models trained with data augmentation across visual variants show significantly improved cross-variant consistency or accuracy, this mechanism would indicate modifiable architectural limitations rather than inherent visual disengagement.

## Foundational Learning

- Concept: **Multimodal Disentanglement**
  - Why needed here: MathSight's core innovation is isolating visual contribution through controlled variants (original, hand-drawn, photo, text-only). Understanding this is critical to interpreting why visual inputs may degrade performance.
  - Quick check question: Given a benchmark with only one image per problem, can you definitively conclude whether a model uses visual information? Why or why not?

- Concept: **Token Confidence and Logical Consistency Metrics**
  - Why needed here: For proving problems, the paper introduces GOM, GSD, and GCV metrics based on sliding-window token confidence. These quantify reasoning stability beyond binary accuracy.
  - Quick check question: If a model outputs a correct final answer but has high Group Coefficient of Variation (GCV) across its reasoning trace, what might this indicate about its reasoning process?

- Concept: **Textual Prior Dominance in VLMs**
  - Why needed here: The counterintuitive finding that removing images improves performance suggests models may not integrate modalities effectively. Understanding this informs architecture and training design.
  - Quick check question: Why might a multimodal model perform worse with images than without, even when the images contain relevant information?

## Architecture Onboarding

- Component map: Visual Encoder -> Text Encoder -> Multimodal Fusion Layer -> Reasoning Head
- Critical path: Dataset construction (PDF extraction → QA pair generation → Image augmentation → Manual verification) → Evaluation (zero-shot inference → Accuracy calculation / Logical consistency metrics → Cross-variant consistency analysis) → Analysis (text-only vs. multimodal, variant-to-variant, difficulty-stratified performance)
- Design tradeoffs:
  - Dataset size vs. quality: MathSight has 661 multimodal items, prioritizing quality over scale
  - Variant diversity vs. semantic equivalence: Hand-drawn and photo variants introduce real-world variation but must preserve problem semantics
  - Metric depth vs. interpretability: Logical consistency metrics (GOM, GSD, GCV) add depth but require careful interpretation beyond accuracy
- Failure signatures:
  - Multimodal degradation: Accuracy drops when images are added compared to text-only
  - Visual insensitivity: No significant difference between original, hand-drawn, and photo variants
  - Difficulty stratification failure: Models do not show improved visual utilization on harder problems
- First 3 experiments:
  1. Replicate text-only vs. multimodal comparison on a subset of MathSight to validate the finding that removing images improves performance. Log token confidence traces for analysis.
  2. Introduce a gating mechanism in the multimodal fusion layer that learns to weight visual tokens based on estimated visual contribution (e.g., via attention entropy). Evaluate whether this recovers text-only performance while preserving gains on genuinely visual problems.
  3. Fine-tune a VLM on augmented visual variants (original, hand-drawn, photo) with contrastive objectives to encourage invariance to visual style. Measure cross-variant consistency and accuracy change.

## Open Questions the Paper Calls Out
None

## Limitations
- The MathSight dataset is not yet publicly available, preventing independent validation of the core findings
- The claim about diminishing visual contribution with difficulty is based on categorical grouping of problems, which may not capture continuous relationships
- The study focuses exclusively on university-level mathematics, limiting generalizability to other multimodal reasoning domains

## Confidence

- High confidence: VLMs rely more on textual priors than visual understanding is well-supported by the striking performance difference (50.53% vs 40.85%) and is consistent with the logical analysis of problem-solving strategies in mathematical reasoning.
- Medium confidence: Visual contribution diminishing with difficulty is supported by categorical analysis but could benefit from more granular difficulty scaling and additional mathematical domains.
- Low confidence: Specific quantitative thresholds (80% consistency, exact accuracy percentages) require independent verification once the dataset is available.

## Next Checks

1. **Dataset Validation**: Once MathSight is publicly available, replicate the text-only vs. multimodal comparison on a stratified sample of problems across all difficulty levels and mathematical domains. Compare results with the paper's findings and analyze token confidence distributions to understand where visual information might be beneficial versus harmful.

2. **Cross-Model Generalization**: Test the text-only superiority finding on additional VLMs beyond those evaluated in the paper, particularly models with different architectural approaches to visual processing (e.g., those with stronger visual grounding mechanisms or attention-based visual gating). This would determine whether the finding reflects a general limitation or model-specific behavior.

3. **Visual-Only Reasoning Test**: Conduct experiments where models receive only the images without accompanying text to test whether visual information alone can support mathematical reasoning. If models cannot solve problems from images alone but perform better with text-only than multimodal inputs, this would strengthen the conclusion that current VLMs treat visual information as noise rather than leveraging it for reasoning.