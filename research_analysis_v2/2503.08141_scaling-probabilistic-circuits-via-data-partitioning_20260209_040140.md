---
ver: rpa2
title: Scaling Probabilistic Circuits via Data Partitioning
arxiv_id: '2503.08141'
source_url: https://arxiv.org/abs/2503.08141
tags:
- clients
- training
- learning
- each
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces federated circuits (FCs), a novel federated
  learning framework that unifies horizontal, vertical, and hybrid federated learning
  settings by leveraging probabilistic circuits (PCs). The key insight is that sum
  nodes in PCs correspond to horizontal FL (same features, different samples), product
  nodes correspond to vertical FL (same samples, different features), and their combination
  handles hybrid FL.
---

# Scaling Probabilistic Circuits via Data Partitioning

## Quick Facts
- arXiv ID: 2503.08141
- Source URL: https://arxiv.org/abs/2503.08141
- Authors: Jonas Seng; Florian Peter Busch; Pooja Prasad; Devendra Singh Dhami; Martin Mundt; Kristian Kersting
- Reference count: 19
- Key outcome: Introduces federated circuits (FCs) that unify horizontal, vertical, and hybrid federated learning settings by leveraging probabilistic circuits, achieving better log-likelihood scores and classification performance while significantly reducing communication overhead.

## Executive Summary
This paper introduces federated circuits (FCs), a novel federated learning framework that unifies horizontal, vertical, and hybrid federated learning settings by leveraging probabilistic circuits (PCs). The key insight is that sum nodes in PCs correspond to horizontal FL (same features, different samples), product nodes correspond to vertical FL (same samples, different features), and their combination handles hybrid FL. FCs treat federated learning as a density estimation problem over distributed datasets, enabling PCs to be learned across multiple machines through data partitioning.

The authors propose a one-pass training algorithm for federated PCs (FedPCs) that allows clients to train independently without data exchange, followed by parameter inference at the network level. This approach is communication-efficient, requiring only O(|C| · (M + 1)) messages in horizontal FL compared to O(M · |C| · K) for traditional model averaging. Experiments demonstrate that FedPCs scale PCs effectively to large-scale image datasets (CelebA, Imagenet, Imagenet32), achieving better log-likelihood scores than baseline methods while significantly reducing training time.

## Method Summary
The paper proposes a federated learning framework where probabilistic circuits (PCs) are learned across distributed datasets through data partitioning. The method maps sum nodes to horizontal FL (combining distributions over same variables) and product nodes to vertical FL (combining distributions over disjoint variable sets). A one-pass training algorithm allows clients to train local PCs independently, followed by server-side parameter inference using dataset size proportions as mixture weights. The approach handles horizontal, vertical, and hybrid FL settings by constructing the FedPC structure based on feature overlap, with clients training locally and only communicating structural information and final statistics.

## Key Results
- FedPCs achieve better log-likelihood scores than baseline methods (EiNets, PyJuice) on large-scale image datasets while significantly reducing training time
- For classification tasks on tabular datasets, FCs achieve competitive or better performance compared to FedAvg, SplitNN, and FedTree across all federated settings
- One-pass training retains performance comparable to EM training while being more communication-efficient (O(|C| · (M + 1)) messages vs O(M · |C| · K))
- FedPCs scale effectively to datasets with 1.2M samples and 200K samples, demonstrating practical applicability to real-world federated scenarios

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Sum nodes and product nodes in Probabilistic Circuits naturally correspond to horizontal and vertical federated learning partitions.
- **Mechanism:** Sum nodes compute convex combinations of inputs (mixtures), mirroring horizontal FL where clients hold different samples of the same features. Product nodes compute products over disjoint scopes, mirroring vertical FL where clients hold different features of the same samples.
- **Core assumption:** Structural constraints of PCs (decomposability and smoothness) are valid representations of data distribution across clients.
- **Evidence anchors:** Abstract states "sum nodes in PCs correspond to horizontal FL... product nodes correspond to vertical FL"; section 3.2 explains sum nodes combine probability distributions over same variables while product nodes handle disjoint feature sets.

### Mechanism 2
- **Claim:** One-pass training reduces communication overhead by inferring network-level parameters locally and only aggregating sufficient statistics once.
- **Mechanism:** Clients train local PCs independently. Server constructs global FedPC structure based on feature sets (Matrix M) and infers sum-node weights using relative dataset sizes (ρ(Ni)), avoiding iterative gradient sharing.
- **Core assumption:** Relative size of client's dataset is sufficient proxy for mixture weight in global distribution (Assumption 1: Mixture Marginals).
- **Evidence anchors:** Abstract mentions "one-pass training algorithm... allows clients to train independently... followed by parameter inference at network level"; section 3.4 states training requires O(|C| · (M + 1)) messages as models are learned locally followed by parameter setting.

### Mechanism 3
- **Claim:** Introducing latent variable (cluster selector) allows capturing dependencies in vertically partitioned data without breaking product node independence requirement.
- **Mechanism:** By conditioning on latent variable L, disjoint variable sets held by different clients can be treated as independent within specific clusters. This allows product nodes to remain valid while modeling cross-client correlations via mixture over L.
- **Core assumption:** Assumption 2 (Cluster Independence): Variables are independent given correct latent cluster assignment.
- **Evidence anchors:** Section 3.1 states "Assumption 2 can be exploited to capture such dependencies... independence is only assumed within clusters"; section 3.3 explains dividing data into K clusters to ensure FedPC spans entire feature space.

## Foundational Learning

- **Concept: Probabilistic Circuits (PCs)**
  - **Why needed here:** Entire framework built on PC semantics (sum and product nodes). Understanding decomposability (children of product nodes have disjoint scopes) and smoothness (children of sum nodes have identical scopes) is essential to grasp FL mapping.
  - **Quick check question:** If you have a product node with two children representing Client A (features X₁) and Client B (features X₂), does the product node violate decomposability if X₁ and X₂ are statistically correlated?

- **Concept: Federated Learning (FL) Partitions**
  - **Why needed here:** Paper unifies Horizontal (sample-partitioned), Vertical (feature-partitioned), and Hybrid FL. Distinguishing these is required to understand why sum nodes map to former and product nodes to latter.
  - **Quick check question:** If Client 1 holds columns A-C and Client 2 holds columns D-F for same users, is this Horizontal or Vertical FL?

- **Concept: Density Estimation vs. Discriminative Tasks**
  - **Why needed here:** Paper frames FL as density estimation problem (learning p(X)) rather than purely discriminative optimization (minimizing loss). This justifies generative approach and specific log-likelihood metrics used.
  - **Quick check question:** Why might a generative model (learning p(X)) be more flexible for FL than purely discriminative model (learning p(Y|X))?

## Architecture Onboarding

- **Component map:** Client Nodes -> Server Node -> Federated Circuit; Clients hold data partitions Dc and local PC models; Server constructs FedPC topology based on feature overlap (Matrix M) and infers global mixture weights.

- **Critical path:**
  1. **Feature Registration:** Clients send feature identifiers to server
  2. **Structure Compilation:** Server analyzes Matrix M to identify disjoint vs shared subspaces; creates Sum nodes for shared subspaces and Product nodes for disjoint clusters
  3. **Local Training:** Clients train local PCs on specific data partitions/clusters (independent execution)
  4. **Weight Inference:** Server calculates global sum-node weights based on dataset sizes (local samples/total samples)

- **Design tradeoffs:**
  - **One-Pass vs EM:** One-pass is communication-efficient but assumes simplified weight inference (proportional to data size) is sufficient; Standard EM might yield higher likelihoods but requires iterative synchronization
  - **Cluster Size (K):** Higher K creates more expressive product mixtures (better for complex vertical dependencies) but increases local computation and model complexity

- **Failure signatures:**
  - **Scope Mismatch:** If feature identifiers are inconsistent (e.g., "Age" vs "age_years") across clients, Matrix M will fail to identify shared subspaces, leading to incorrect product node construction
  - **Correlated Vertical Partitions:** If Assumption 2 (Cluster Independence) is violated (cross-client features are dependent but cannot be clustered), product nodes will force independence, degrading log-likelihood

- **First 3 experiments:**
  1. **Horizontal Split Validation:** Run FedPC on MNIST with random sample partitions; verify log-likelihood matches centralized PC trained on full dataset (Table 1 sanity check)
  2. **Communication Profiling:** Measure network traffic (MB) for FedPC vs FedAvg on dummy dataset; confirm O(|C| · (M + 1)) scaling (Figure 5 reproduction)
  3. **Vertical Scaling Test:** Create synthetic dataset with known conditional independence properties; split features vertically; test if FedPC recovers correct structure without over-segmentation

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness critically depends on validity of Cluster Independence Assumption 2, which may be violated in real-world vertically partitioned data where cross-client features are correlated but cannot be cleanly clustered
- One-pass training approach trades communication efficiency for potential suboptimality in weight estimation compared to iterative EM, though this tradeoff is not extensively quantified
- Structural compilation process relies on accurate feature registration across clients; any inconsistency in feature naming or representation could lead to incorrect Matrix M construction and model failure

## Confidence
- **High Confidence**: Correspondence between sum/product nodes and horizontal/vertical FL settings is well-founded and clearly demonstrated through both theoretical exposition and experimental validation across multiple datasets
- **Medium Confidence**: Communication complexity claims (O(|C|·(M+1)) for one-pass vs O(M·|C|·K) for traditional) are logically sound based on algorithmic description but require empirical validation across diverse network conditions
- **Low Confidence**: Assumption that relative dataset sizes serve as sufficient proxies for mixture weights in global distribution may not hold when data distributions across clients are heterogeneous

## Next Checks
1. **Heterogeneous Data Distribution Test**: Evaluate FedPC performance when clients have non-IID data distributions (different class proportions or feature distributions) to verify if proportional weight inference remains effective
2. **Correlation Structure Stress Test**: Design synthetic vertical FL scenario where cross-client features have complex dependency structures that cannot be perfectly captured by K clusters, measuring degradation in log-likelihood
3. **Communication Overhead Verification**: Implement detailed network traffic measurement during FedPC training versus FedAvg on identical hardware and network conditions to empirically validate claimed O(|C|·(M+1)) scaling