---
ver: rpa2
title: Active Inference with Reusable State-Dependent Value Profiles
arxiv_id: '2512.11829'
source_url: https://arxiv.org/abs/2512.11829
tags:
- profile
- precision
- context
- policy
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces value profiles as a reusable set of parameter
  bundles (outcome preferences, policy priors, and policy precision) assigned to hidden
  states in active inference models. This approach enables state-conditional strategy
  recruitment without requiring independent parameters for each context.
---

# Active Inference with Reusable State-Dependent Value Profiles

## Quick Facts
- arXiv ID: 2512.11829
- Source URL: https://arxiv.org/abs/2512.11829
- Reference count: 15
- The paper introduces value profiles as reusable bundles of control parameters assigned to hidden states, enabling tractable state-conditional strategy recruitment without per-context parameter explosion.

## Executive Summary
This paper introduces value profiles as a mechanism for adaptive control in active inference models. Value profiles are reusable bundles of control parameters (outcome preferences, policy priors, and policy precision) assigned to hidden states, enabling state-conditional strategy recruitment without requiring independent parameters for each context. In probabilistic reversal learning tasks, model comparison using AIC strongly favors the profile-based model over simpler alternatives. The framework provides a tractable computational account of belief-conditioned value control in volatile environments and yields testable signatures of belief-dependent control and behavioral flexibility.

## Method Summary
The method implements three active inference models compared via grid search on a probabilistic reversal learning task. M1 uses static precision (γ=2.5), M2 uses entropy-coupled precision that adapts to belief uncertainty, and M3 uses two reusable value profiles with belief-weighted mixing. The generative model includes three observation modalities (hints, rewards, choices) and three state factors (context: volatile/stable; better_arm: left/right; action_state). Profile mixing uses belief-weighted linear combination: w_t(k) = Σ_s q_t(s) × Z_{s,k} to produce trial-wise effective parameters. Two-stage coarse-to-fine grid search (Table 2 specifies exact grids) with 5-fold cross-validation (80 trials/fold) compares models via AIC and parameter recovery.

## Key Results
- AIC strongly favors profile-based model M3 over M1 and M2 (about 100-point differences) in model recovery experiments
- Parameter recovery analyses support structural identifiability even when context must be inferred from noisy observations
- Adaptive control in reversal learning is driven primarily by modulation of policy priors (hint-seeking preferences) rather than policy precision
- Gradual belief-dependent profile recruitment occurs under context uncertainty, consistent with state-conditional rather than purely uncertainty-driven control

## Why This Works (Mechanism)

### Mechanism 1: Value Profile Construction and Assignment
Bundling control parameters into reusable profiles assigned to hidden states enables tractable state-conditional control without per-context parameter explosion. Profiles Ω_k = {C_k, E_k, γ_k} are defined once and assigned via assignment matrix Z ∈ [0,1]^{S×K}. Multiple states can share the same profile, compressing the value space. This works because adaptive behavior requires coordinated changes across multiple control channels that co-vary in meaningful behavioral modes.

### Mechanism 2: Belief-Weighted Profile Mixing
Effective control parameters emerge continuously from belief-weighted linear combination of profile parameters, enabling smooth strategy transitions under context uncertainty. Profile weights w_t(k) = Σ_s q_t(s) × Z_{s,k} pool posterior beliefs through the assignment matrix. Effective parameters: C_eff = Σ_k w_t(k) × C_k (similarly for E, γ). This assumes the brain represents uncertainty over latent states and uses this uncertainty to gradiently blend behavioral strategies rather than hard-switching.

### Mechanism 3: Policy Prior Modulation as Dominant Adaptive Channel
In this reversal-learning task, adaptation operates primarily through context-dependent policy priors (hint-seeking preferences) rather than policy precision changes. Recovered parameters show both profiles using identical precision (γ = 5.0) but differentiated hint preferences (ξ_hint = 6.0 volatile vs. 1.0 stable). This works because tasks differ in which control channels most efficiently express adaptation; the framework should be agnostic about which channel dominates.

## Foundational Learning

- **Concept: Active Inference Policy Selection**
  - Why needed: Understanding how policies are evaluated via expected free energy G(π) and selected via precision-weighted softmax is prerequisite for grasping how profiles modulate action.
  - Quick check: Can you explain why higher policy precision γ makes action selection more deterministic?

- **Concept: Bayesian Belief Updating in POMDPs**
  - Why needed: The mixing mechanism requires understanding how posterior beliefs q_t(s) evolve through likelihood-weighted updates under transition dynamics.
  - Quick check: Given observation o_t and prior belief q̃_t(s), how is the posterior computed?

- **Concept: Parameter Identifiability and Gauge Freedom**
  - Why needed: The paper addresses identifiability through mean-centering logits and constraining assignment rows to sum to one—understanding why this matters is critical for fitting profiles to data.
  - Quick check: Why does translation invariance in logit space create non-identifiability?

## Architecture Onboarding

- **Component map:** Belief parameters (shared world model): A (likelihood), B (transitions), D (priors) — fixed during profile fitting → Value parameters (profile-conditional): C (outcome preferences), E (policy priors), γ (precision) — bundled into profiles → Assignment layer: Matrix Z mapping hidden state factors to profiles → Mixing layer: Belief-weighted combination producing trial-wise effective parameters

- **Critical path:**
  1. Define state factor(s) for profile assignment (typically context factor)
  2. Specify profile count K and initial parameter values
  3. Implement belief-weighted mixing: w = q_context @ Z; θ_eff = w @ θ_profiles
  4. Use effective parameters in policy selection: p(π) ∝ exp(-γ_eff × G(π) + E_eff)
  5. Fit via grid search or gradient methods with identifiability constraints

- **Design tradeoffs:**
  - More profiles → more expressive but harder to identify and fit
  - Hard vs. soft assignment: hard is simpler but cannot express uncertainty-driven blending
  - Which parameters to include in profiles: bundling more provides coordination but may over-constrain
  - Grid search vs. continuous optimization: grid is robust but scales poorly with parameter count

- **Failure signatures:**
  - All profiles converging to similar values → identifiability failure or insufficient task structure
  - Profile weights not tracking context beliefs → assignment matrix misconfigured or wrong state factor targeted
  - Model fits M1/M2 data better than M3 → overfitting or inappropriate profile complexity for task
  - Erratic weight fluctuations on fast timescales → beliefs responding to noise rather than context structure

- **First 3 experiments:**
  1. Replicate model recovery: Generate data from each model variant, fit all three, confirm M3 wins on M3 data and loses on simpler data
  2. Parameter perturbation: Systematically vary profile differentiation (hint preference ratio, precision ratio) to map which task structures recruit which adaptive channels
  3. Context ambiguity stress test: Reduce discriminability between volatile/stable reward statistics to probe mixing behavior under persistent uncertainty

## Open Questions the Paper Calls Out

- **Can agents autonomously learn the optimal number, parameters, and state assignments of profiles from experience rather than relying on hand-designed specifications?**
  - The authors explicitly ask how agents should decide "when to recruit existing profiles versus instantiating new ones" and suggest developing variational approaches for structure discovery in Future Directions.

- **Do task structures lacking explicit information-seeking actions recruit policy precision modulation as the primary adaptive channel, unlike the policy-prior modulation observed in this study?**
  - The Discussion notes that the observed reliance on policy priors may be specific to the task design (which included a hint action) and suggests "alternative task structures—such as environments with speed-accuracy tradeoffs... might recruit γ-based adaptation."

- **Do human subjects adapt to volatility by modulating "informativeness" (policy priors) or "decisiveness" (precision) as predicted by the profile framework?**
  - The authors state they "have not validated the framework against human or animal behavioral data" and propose experiments to test if humans become "less confident" (precision) or "more inquisitive" (priors).

## Limitations
- The framework remains a computational proof-of-concept without empirical behavioral validation in humans or animals
- Results are based on a single task structure where preference modulation was sufficient; generalizability to tasks requiring precision modulation is untested
- All experiments used hand-designed profiles and assignment matrices; autonomous profile discovery algorithms were not implemented

## Confidence

- **High confidence:** The profile framework's ability to compress value space without sacrificing expressivity, demonstrated through model comparison and parameter recovery
- **Medium confidence:** The claim that policy prior modulation dominates adaptation in reversal learning, as this appears task-specific and lacks broader empirical validation
- **Low confidence:** Claims about smooth strategy transitions under uncertainty, as these depend on unobserved belief trajectories and mixing dynamics

## Next Checks

1. **Cross-task validation:** Apply the profile framework to a task requiring precision modulation (e.g., speed-accuracy tradeoff) to test whether control channel dominance generalizes or remains task-specific
2. **Belief uncertainty stress test:** Systematically vary the discriminability of volatile/stable contexts to map the boundary between effective mixing and hard-switching regimes
3. **Scaling experiment:** Test profile frameworks with K > 2 profiles in environments with >2 latent contexts to assess computational tractability and identifiability limits