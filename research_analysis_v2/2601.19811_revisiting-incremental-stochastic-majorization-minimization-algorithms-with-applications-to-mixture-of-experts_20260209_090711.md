---
ver: rpa2
title: Revisiting Incremental Stochastic Majorization-Minimization Algorithms with
  Applications to Mixture of Experts
arxiv_id: '2601.19811'
source_url: https://arxiv.org/abs/2601.19811
tags:
- page
- cited
- stochastic
- algorithm
- incremental
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an incremental stochastic Majorization-Minimization
  (MM) algorithm for high-volume streaming data, generalizing incremental stochastic
  EM algorithms. The method applies to complex latent models like softmax-gated Mixture
  of Experts (MoE) regression, for which existing EM variants fail due to lack of
  explicit latent-variable representations and non-exponential-family structure.
---

# Revisiting Incremental Stochastic Majorization-Minimization Algorithms with Applications to Mixture of Experts

## Quick Facts
- **arXiv ID:** 2601.19811
- **Source URL:** https://arxiv.org/abs/2601.19811
- **Reference count:** 40
- **Primary result:** Introduces incremental stochastic MM algorithm for softmax-gated MoE regression, proving convergence to stationary points and outperforming SGD, Adam, and RMSProp on synthetic and real-world datasets

## Executive Summary
This paper introduces an incremental stochastic Majorization-Minimization (MM) algorithm for high-volume streaming data, generalizing incremental stochastic EM algorithms. The method applies to complex latent models like softmax-gated Mixture of Experts (MoE) regression, for which existing EM variants fail due to lack of explicit latent-variable representations and non-exponential-family structure. The authors establish theoretical guarantees, proving consistency in convergence to stationary points. Empirically, the algorithm outperforms standard stochastic optimizers (SGD, RMSProp, Adam, Sophia) on both synthetic and real-world datasets, including dent maize genotypes and communities-crime datasets. The approach offers greater flexibility and broader applicability than EM-based methods, supporting development of new incremental stochastic algorithms for modern machine learning tasks involving heterogeneous data.

## Method Summary
The paper proposes an incremental stochastic MM framework for optimizing latent variable models with streaming data. The algorithm maintains sufficient statistics updated via stochastic approximation, constructs a tractable surrogate function majorizing the true loss, and performs deterministic parameter updates at each iteration. For softmax-gated MoE regression, the method derives a specific quadratic majorizer based on bounds of the softmax function's derivative, transforming the non-convex problem into a solvable weighted least-squares problem. The approach generalizes incremental EM by relaxing exponential-family requirements and explicit latent-variable representations.

## Key Results
- Proven convergence to stationary points of the objective function for incremental stochastic MM
- Outperforms standard stochastic optimizers (SGD, RMSProp, Adam, Sophia) on synthetic and real-world datasets
- Successfully optimizes softmax-gated MoE regression where standard EM variants fail
- Demonstrates efficiency on both low-dimensional (N=1,000) and high-dimensional (N=1,994) datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The framework generalizes incremental EM by decoupling the optimization mechanism from the statistical model's exponential-family structure.
- **Mechanism:** The algorithm replaces the intractable expected loss $E[f(\theta)]$ with a tractable **exponential family surrogate** $g(\theta, z; \tau)$ (Property P1). Instead of requiring the model to be exponential, it only requires a function majorizing the loss to be exponential, allowing standard sufficient statistics updates.
- **Core assumption:** A valid, differentiable majorizer $g$ can be constructed for the objective function (Property P1), and the sufficient statistics remain bounded (Assumption 7).
- **Evidence anchors:**
  - [Abstract]: "Our approach relaxes key EM requirements, such as explicit latent-variable representations..."
  - [Section 2.1]: Defines P1-P5, specifically relaxing the structural constraints.
  - [Corpus]: Weak/missing direct mechanistic links in neighbor papers; relies on internal proofs.
- **Break condition:** If the objective function $f$ is non-smooth or lacks a quadratic-like upper bound (violating P1), the majorizer cannot be constructed.

### Mechanism 2
- **Claim:** The algorithm achieves convergence to stationary points via a **Stochastic Approximation** update of sufficient statistics rather than direct gradient descent.
- **Mechanism:** The system maintains a statistic sequence $s_n$ updated recursively ($s_{n+1} = s_n + \gamma_{n+1}(\bar{S}(\cdot) - s_n)$). This effectively solves a fixed-point problem for the expectation of sufficient statistics, which asymptotically aligns with stationary points of the loss (Proposition 1).
- **Core assumption:** Step sizes $\gamma_n$ decrease appropriately ($\sum \gamma_n^2 < \infty$), and the data stream is i.i.d. (Assumption 8, P3).
- **Evidence anchors:**
  - [Section 3.1]: Links limiting points of $s_n$ to stationary points of the objective.
  - [Section 3.2]: Establishes consistency using standard SA convergence theory.
- **Break condition:** If the learning rate $\gamma_n$ does not vanish fast enough, the noise in statistic estimates prevents convergence.

### Mechanism 3
- **Claim:** The method enables optimization of softmax-gated MoE by exploiting a quadratic bound on the softmax derivative.
- **Mechanism:** Standard EM fails here because the softmax gate destroys the exponential family structure. This method uses a specific **quadratic majorizer** (Proposition 3, Equation 18) derived from Lemma 2 and Proposition 5 to bound the non-convex gating network, transforming the update into a solvable weighted least-squares problem.
- **Core assumption:** The Hessian of the majorizer remains positive definite (ensured by $B_{n,K}$ definition).
- **Evidence anchors:**
  - [Section 4.1.3]: Derives the specific surrogate for softmax-gated Gaussian MoE.
  - [Section 4.3.1]: Explains why this surrogate avoids the restrictive conditions of other frameworks.
- **Break condition:** If the input covariates $x$ or responses $y$ are unbounded, the bound in Proposition 5 may fail to hold, destabilizing the update.

## Foundational Learning

- **Concept: Majorization-Minimization (MM)**
  - **Why needed here:** This is the core optimization paradigm. Unlike gradient descent, it minimizes a simpler "upper bound" function of the loss, which is crucial for handling non-convex softmax gates.
  - **Quick check question:** If function $g(\theta)$ majorizes $f(\theta)$ at $\theta_n$, why does minimizing $g(\theta)$ guarantee descent in $f(\theta)$?

- **Concept: Stochastic Approximation (SA)**
  - **Why needed here:** This justifies the "incremental" nature. Understanding SA helps distinguish this method from SGD; it seeks roots of a function (solving for sufficient statistics) rather than minima of a loss directly.
  - **Quick check question:** Why does the standard SA step-size rule ($\gamma_n \propto 1/n$) differ from constant step-sizes often used in deep learning?

- **Concept: Mixture of Experts (MoE) Architecture**
  - **Why needed here:** The paper targets the specific failure case of softmax-gated MoEs. Understanding the interaction between Gating Networks and Expert Networks is required to see why the loss surface is difficult.
  - **Quick check question:** Why does the "softmax" gating function specifically prevent the application of standard incremental EM algorithms?

## Architecture Onboarding

- **Component map:** Data Ingest -> Statistic Buffer -> Surrogate Builder -> M-Step Solver
- **Critical path:** Receiving $(x_n, y_n)$ -> Compute local posterior $\tau_n$ -> Update $s_n$ via Eq (6) -> Solve minimization in Eq (7) -> Emit $\theta_{n+1}$
- **Design tradeoffs:**
  - *Surrogate Complexity vs. Generality:* Requires deriving a specific bound (like $B_{n,K}$) for each model type, reducing "plug-and-play" generality compared to Adam/SGD
  - *Memory Efficiency:* Uses constant memory $O(1)$ relative to dataset size (streaming), unlike batch EM which grows with $N$
- **Failure signatures:**
  - *Variance Explosion:* If covariance matrices $\Sigma_k$ trend toward zero, log-likelihood terms explode, breaking the boundedness assumption (Remark 1)
  - *Stability Collapse:* If $s_n$ exits the valid convex set $S$ (violating Assumption 7), the M-step may fail to find a unique root
- **First 3 experiments:**
  1. **Synthetic Stability Check:** Run Algorithm 2 on low-dimensional Gaussian data (Section 5.2) with "perturbed ground truth" initialization to verify the $s_n$ sequence converges without manual truncation
  2. **Convergence Speed Benchmark:** Compare distance-to-optimum against Adam/RMSProp on synthetic data (Figure 3) to validate the efficiency of the quadratic bound
  3. **High-Dimensional Robustness:** Apply to the "Dent maize" dataset (Section 5.4.1) using the Lasso-feature selection preprocessing to test how the algorithm handles real-world sparsity and noise

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the incremental stochastic MM framework be extended to support mini-batch updates while maintaining provable convergence guarantees?
- **Basis:** [explicit] The authors state in Section 6 that "Developing a theoretically grounded mini-batch MM algorithm with provable convergence guarantees represents an important step toward scalable incremental stochastic learning."
- **Why unresolved:** The current formulation processes data points strictly one-by-one (incrementally), which may limit efficiency in high-throughput distributed environments.
- **What evidence would resolve it:** A theoretical derivation of mini-batch update rules for the auxiliary statistics $s_n$ and a convergence proof establishing consistency for batch sizes greater than one.

### Open Question 2
- **Question:** How can dimensionality reduction or sparsity-inducing regularization be incorporated into the algorithm to mitigate overfitting in high-dimensional settings?
- **Basis:** [explicit] Section 6 notes that "addressing the challenges posed by high-dimensional datasets requires incorporating dimensionality reduction techniques or sparsity-inducing regularization within the MM framework."
- **Why unresolved:** The current algorithm minimizes a surrogate function without penalty terms, which can lead to computational inefficiency and overfitting in high-dimensional regimes.
- **What evidence would resolve it:** A modification of the surrogate minimization step to include sparsity constraints and empirical validation demonstrating improved performance on high-dimensional data.

### Open Question 3
- **Question:** Can the algorithm be generalized to handle non-diagonal covariance matrices for the Gaussian experts?
- **Basis:** [explicit] Section 4.1 assumes diagonal covariance matrices "for simplicity" and explicitly states, "We leave the extension to a general covariance matrix for future work."
- **Why unresolved:** The current mathematical construction of the surrogate function and the update rules relies on the independence assumptions of diagonal covariance.
- **What evidence would resolve it:** A reformulation of the expert surrogate functions to support full covariance matrices and verification that the resulting updates satisfy the theoretical stability conditions.

## Limitations

- The framework requires deriving problem-specific majorizers, which may be non-trivial for complex models
- The boundedness assumption for sufficient statistics (Assumption 7) is not explicitly verified for all applications
- Comparison with other incremental EM variants is limited, focusing primarily on standard stochastic optimizers

## Confidence

- **High confidence:** Theoretical convergence guarantees to stationary points (Proposition 1)
- **Medium confidence:** Empirical superiority over standard optimizers on benchmark datasets
- **Medium confidence:** Correctness of the softmax-gated MoE majorizer derivation

## Next Checks

1. Test algorithm stability with non-Gaussian noise distributions in synthetic experiments to verify Assumption 7 robustness
2. Implement and compare against incremental EM variants (when applicable) to quantify the generality advantage
3. Analyze convergence sensitivity to the majorizer tightness parameter (Îµ*) across different problem scales