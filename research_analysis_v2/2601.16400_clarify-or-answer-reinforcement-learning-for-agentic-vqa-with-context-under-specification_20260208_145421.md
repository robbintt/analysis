---
ver: rpa2
title: 'Clarify or Answer: Reinforcement Learning for Agentic VQA with Context Under-specification'
arxiv_id: '2601.16400'
source_url: https://arxiv.org/abs/2601.16400
tags:
- clarification
- question
- answer
- ambiguity
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles context-dependent visual question answering,
  where the correct answer depends on external context not observable in the image.
  The proposed Clarify-or-Answer (CoA) framework separates the decision to answer
  from clarification generation, using a controller to determine when clarification
  is needed and a clarification policy to ask targeted questions.
---

# Clarify or Answer: Reinforcement Learning for Agentic VQA with Context Under-specification

## Quick Facts
- **arXiv ID:** 2601.16400
- **Source URL:** https://arxiv.org/abs/2601.16400
- **Reference count:** 8
- **Key outcome:** CoA improves end-to-end VQA accuracy by an average of +15.3 points (83%) over prompting baselines.

## Executive Summary
This paper introduces the Clarify-or-Answer (CoA) framework to address context-dependent visual question answering, where the correct answer depends on external context not observable in the image. The framework separates the decision to answer from clarification generation using a controller and a clarification policy, trained with specialized objectives. The authors introduce CONTEXTCLARIFY, a dataset of 275 ambiguous VQA pairs, and GRPO-CR, a reinforcement learning approach for optimizing clarification quality. Across three models and datasets, CoA achieves significant improvements in end-to-end VQA accuracy, with substantial gains at both the module and system levels.

## Method Summary
The CoA framework addresses context under-specification in VQA through a modular architecture with three components: a controller that decides whether to answer or clarify, a clarification policy that generates targeted questions, and a fixed answering model. The controller is trained via supervised fine-tuning as a binary classifier, while the clarification policy uses Group Relative Policy Optimization (GRPO-CR) with a multi-signal reward function. The system is designed for single-step interactions to prevent user burden. Training involves separate optimization of the controller and clarification policy, with the answering model kept fixed.

## Key Results
- CoA improves end-to-end VQA accuracy by an average of +15.3 points (83%) over prompting baselines.
- Module-level improvements: +0.156 F1 for the controller (+103%) and +0.401 reward for the clarification policy (+73%).
- The controller achieves high precision on unambiguous questions while maintaining effective recall on ambiguous ones.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modular separation of the decision to ask from the generation of a clarification question yields more reliable and higher-quality agentic behavior than a single monolithic model prompted for both.
- Mechanism: The CoA framework isolates two distinct sub-tasks: (1) a controller π_ctrl for binary classification (ANSWER/CLARIFY) and (2) a clarification policy π_clr for targeted question generation. By training each with a specialized objective (cross-entropy for the controller, GRPO for the policy), the system reduces the compounding error of a single model attempting to decide and formulate simultaneously.
- Core assumption: The ambiguity can be reliably detected and mapped to a single missing contextual factor.
- Evidence anchors:
  - [abstract] "...proposed Clarify-or-Answer (CoA) framework separates the decision to answer from clarification generation... CoA improves end-to-end VQA accuracy by an average of +15.3 points (83%) over prompting baselines."
  - [section] Section 3.2 details the three components (controller, clarification policy, answering model).
  - [corpus] "Teaching Vision-Language Models to Ask" (ArXiv 2507.13773) explores similar interactive clarification, supporting the problem formulation.
- Break condition: If the controller's recall is low, the system fails to trigger clarification on ambiguous inputs, leading to confident but incorrect answers, which renders the clarification policy useless.

### Mechanism 2
- Claim: A reinforcement learning approach (GRPO-CR) with a multi-signal reward produces more effective clarification questions than pure supervised fine-tuning (SFT).
- Mechanism: GRPO-CR optimizes the clarification policy by sampling multiple candidate questions and computing their advantage relative to the group's mean reward. The reward is a composite score that explicitly targets key properties: format, relevance, ambiguity resolution, novelty, and similarity to human references.
- Core assumption: The designed reward components are valid proxies for human judgment of clarification quality and are not in conflict.
- Evidence anchors:
  - [abstract] "...introduce GRPO-CR... that optimizes clarification quality using multi-signal rewards... with module-level improvements of +0.401 reward for the clarification policy (+73%)."
  - [section] Section 3.4 defines the GRPO loss and the five reward components.
  - [corpus] Corpus evidence for GRPO-based clarification learning is limited; the related "Teaching Vision-Language Models to Ask" uses DPO, not GRPO.
- Break condition: If the reward signals are mis-specified, the policy may learn to game the reward (e.g., generating a well-formatted but irrelevant question), degrading human evaluation scores.

### Mechanism 3
- Claim: Constraining the agent to a single-step clarification interaction prevents user burden while effectively resolving the curated types of under-specification.
- Mechanism: The system is designed to ask exactly one question per query. The controller's binary decision gates the interaction, and the policy generates a single targeted question. This design choice explicitly trades off multi-turn resolution depth for efficiency and reduced friction.
- Core assumption: The curated ambiguous examples depend on a single missing contextual factor, which can be elicited with one question.
- Evidence anchors:
  - [abstract] "...asks a single focused question and then incorporates the response to produce the final answer."
  - [section] Section 3.5: "CoA executes a single-step clarification interaction... This design prevents over-asking while still enabling recovery of missing contextual factors."
  - [corpus] No direct corpus evidence compares single-turn vs. multi-turn VQA clarification.
- Break condition: If deployed on queries with compound ambiguities (multiple missing factors), the single-step constraint will fail to resolve the query fully, leading to an incorrect final answer despite clarification.

## Foundational Learning

- **Context Under-specification in VQA**: This occurs when an image-question pair is clear individually but the correct answer depends on external, non-visual context (e.g., local laws, cultural norms).
  - Why needed here: It is the precise problem definition the CoA framework is built to solve, distinct from perceptual or linguistic ambiguity.
  - Quick check question: How does context under-specification differ from a question being visually ambiguous?

- **Group Relative Policy Optimization (GRPO)**: A reinforcement learning algorithm where a group of outputs is generated for a single input, and the advantage of each output is calculated relative to the group's mean reward, eliminating the need for a separate critic model.
  - Why needed here: It is the core training method for the clarification policy, enabling direct optimization of generated questions based on custom rewards.
  - Quick check question: What is the primary advantage of GRPO over methods like PPO in this setting?

- **Modular Agentic Architecture**: A system design that breaks a complex task into distinct, specialized components (e.g., decision-maker, generator, executor) orchestrated to achieve a goal.
  - Why needed here: It explains the structural design of CoA, where separate modules handle the "when to ask" and "what to ask" problems.
  - Quick check question: What are the three core components of the CoA framework?

## Architecture Onboarding

- **Component map**: Image + Question → Controller → (ANSWER → Answering Model) or (CLARIFY → Clarification Policy → Simulated User Response → Answering Model)
- **Critical path**:
  1. **Input**: Image `I` and question `q` are received.
  2. **Decision**: The **Controller** classifies the input as context-sufficient or context-missing.
  3. **Execution (if CLARIFY)**: The **Clarification Policy** generates a question `qc`. The simulated or real user response `r` is obtained.
  4. **Final Answer**: The **Answering Model** generates the final answer using all available information.
- **Design tradeoffs**:
  - **Single vs. Multi-turn Interaction**: The system is limited to one question. This reduces user burden but may be insufficient for complex, multi-factor ambiguities.
  - **Controller Precision vs. Recall**: Training can be tuned to favor higher recall (safer, asks more often) or higher precision (more efficient, asks less often). The paper shows this trade-off.
  - **Reward Component Weighting**: The paper uses uniform weights for reward signals, but different weightings could optimize for specific outcomes like brevity or novelty.
- **Failure signatures**:
  - **Controller Miss (Low Recall)**: The system answers directly on an ambiguous question, resulting in a confident, hallucinated, or incorrect answer.
  - **Policy Failure**: The system asks a clarification question that is a trivial rephrasing of the original or asks for irrelevant information, failing to resolve the ambiguity.
  - **Over-asking (Low Precision)**: The controller triggers clarification for unambiguous questions, causing unnecessary user friction.
- **First 3 experiments**:
  1. **Controller Baseline**: Evaluate a zero-shot prompted VLM on the Controller's task (classify ambiguous vs. non-ambiguous) on the CONTEXTCLARIFY test set to establish a baseline F1 score.
  2. **Reward Ablation**: Train the Clarification Policy with GRPO-CR but ablate one reward component at a time (e.g., remove `γresolution`) to measure its impact on the final VQA accuracy.
  3. **End-to-End Pipeline Evaluation**: Run the full CoA system (with GRPO-CR policy) on the CONTEXTCLARIFY test set and compare the final VQA accuracy against a vanilla prompting baseline using the same backbone model.

## Open Questions the Paper Calls Out

- **How can the CoA framework be extended to support multi-turn clarification dialogues with explicit stopping rules to handle queries with multiple missing contextual factors?**
  - **Basis in paper**: [explicit] The Discussion section states: "A natural extension is a multi-turn variant of CoA that can ask a small number of clarification questions with an explicit stopping rule... This would require new training signals and benchmarks that capture dialog trajectories..."
  - **Why unresolved**: The current implementation is restricted to a single-step interaction (one question or direct answer), which limits utility in complex scenarios where multiple pieces of information are missing.
  - **What evidence would resolve it**: The development of a multi-turn training framework and a benchmark dataset containing dialog trajectories with efficiency metrics (e.g., accuracy vs. number of turns) demonstrating effective stopping behavior.

- **Can the controller module be improved by incorporating cost-sensitive decision rules or uncertainty calibration to dynamically adjust the threshold for asking versus answering based on deployment constraints?**
  - **Basis in paper**: [explicit] The Discussion section highlights: "Future work could incorporate cost-sensitive decision rules or calibration techniques so the controller can adapt its behavior to deployment constraints (e.g., 'ask only if uncertainty is high enough')."
  - **Why unresolved**: The current controller is trained via standard binary classification, which does not explicitly model the trade-off between user friction (over-asking) and error risk (under-asking).
  - **What evidence would resolve it**: An evaluation showing that a calibrated controller maintains or improves F1 scores and end-to-end accuracy while significantly reducing unnecessary clarification requests on a dataset with a low ratio of ambiguous examples.

- **Does joint optimization of the controller, clarification policy, and answering model improve performance over the current modular approach, and can the associated credit-assignment challenges be mitigated?**
  - **Basis in paper**: [explicit] The Limitations section notes: "Joint optimization of all components could yield further gains but raises credit-assignment and stability challenges."
  - **Why unresolved**: The components are currently trained separately (Controller/Policy trained, Answering model fixed), potentially missing feedback loops where the answering model's specific failure modes could better inform the clarification policy.
  - **What evidence would resolve it**: An end-to-end training schema that demonstrates stable convergence and higher final VQA accuracy compared to the modular pipeline, specifically on "hard" ambiguous cases.

## Limitations

- The CONTEXTCLARIFY dataset is relatively small (275 pairs) and curated to a specific type of single-factor ambiguity, limiting generalizability to real-world multi-turn interactions or compound ambiguities.
- The GRPO-CR reward function relies on proxy metrics that may not perfectly align with human judgment, and the uniform weighting of components is a simplification.
- The paper reports strong end-to-end improvements, but a significant portion comes from the Contrast Set of non-ambiguous questions, suggesting the controller's impact on unambiguous queries is substantial.

## Confidence

- **High Confidence**: The modular architecture design is sound and the reported improvements on the curated CONTEXTCLARIFY dataset are likely reproducible with the provided code and data.
- **Medium Confidence**: The efficacy of the GRPO-CR training method for the clarification policy is supported by the results but requires further validation on larger, more diverse datasets to confirm its robustness.
- **Low Confidence**: The practical impact of the single-step interaction constraint in real-world, open-domain scenarios with multi-factor ambiguities is uncertain and represents a significant limitation of the current approach.

## Next Checks

1. **Dataset Size and Diversity Validation**: Replicate the end-to-end CoA pipeline on a larger, more diverse VQA dataset (e.g., VizWiz, ClearVQA) to assess the framework's performance and the controller's precision-recall trade-off in a less curated setting.
2. **Reward Function Ablation Study**: Conduct a systematic ablation study on the GRPO-CR reward components, varying their weights and even removing components, to empirically determine which signals are most critical for generating high-quality clarifications that improve final answer accuracy.
3. **Multi-turn Interaction Baseline**: Implement a simple multi-turn clarification baseline (e.g., allowing up to two clarification questions) on the CONTEXTCLARIFY dataset to quantify the performance trade-off between the proposed single-step efficiency and the potential accuracy gains from deeper resolution of complex ambiguities.