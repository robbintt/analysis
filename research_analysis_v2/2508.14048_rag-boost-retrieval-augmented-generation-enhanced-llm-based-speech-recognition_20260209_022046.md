---
ver: rpa2
title: 'RAG-Boost: Retrieval-Augmented Generation Enhanced LLM-based Speech Recognition'
arxiv_id: '2508.14048'
source_url: https://arxiv.org/abs/2508.14048
tags:
- speech
- recognition
- system
- task
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of recognition errors in multilingual
  conversational speech recognition using large language models (LLMs). The proposed
  RAG-Boost system integrates a retrieval-augmented generation (RAG) module with a
  baseline LLM-based ASR system.
---

# RAG-Boost: Retrieval-Augmented Generation Enhanced LLM-based Speech Recognition

## Quick Facts
- arXiv ID: 2508.14048
- Source URL: https://arxiv.org/abs/2508.14048
- Reference count: 0
- Primary result: RAG-Boost achieves 11.67% WER and 0.9132 SEM on MLC-SLM Challenge

## Executive Summary
This paper addresses recognition errors in multilingual conversational speech by integrating a retrieval-augmented generation (RAG) module with a baseline LLM-based ASR system. The proposed RAG-Boost system uses a keyword retriever that leverages cross-modal similarity to identify domain-specific terms and audio-text pairs, which are then fused with live ASR hypotheses to correct errors before passing them to the LLM. Experiments on the MLC-SLM Challenge dataset demonstrate significant improvements in transcription accuracy and semantic relevance compared to baseline systems.

## Method Summary
RAG-Boost is built on a SLAM-ASR skeleton using Whisper-large-v3-turbo encoder and Phi-4-mini-Instruct LLM. The system employs a dual-tower keyword retriever trained with CLAP-style contrastive learning to map speech and text into a shared embedding space. During streaming inference, partial ASR hypotheses query a FAISS-indexed vector store of audio-text pairs and domain terms. Retrieved results are fused with live ASR hypotheses via weighted aggregation, with RAG-based output receiving higher weight. The LLM is fine-tuned with LoRA on 2,000 manually curated keyword correction samples to effectively utilize retrieved context.

## Key Results
- Achieves 11.67% WER on MLC-SLM Challenge dataset
- Semantic Consistency Metric (SEM) of 0.9132
- Outperforms baseline systems including Keywords RAG without fine-tuning (32.98% WER)
- Remaining 12% WER gap attributed to overlapped and rapid speech

## Why This Works (Mechanism)

### Mechanism 1
Cross-modal keyword retrieval enables direct identification of domain-specific terms from speech signals before text decoding completes. A CLAP-style dual-tower model maps speech and text into a unified embedding space, allowing cross-modal similarity matching to retrieve relevant audio-text pairs during streaming inference.

### Mechanism 2
On-the-fly fusion of retrieved corrections with live ASR hypotheses enables error correction before LLM generation. The fusion module integrates three intermediate recognition outputs, giving higher weight to RAG-based corrections to create an intermediate correction layer between acoustic encoding and language modeling.

### Mechanism 3
Fine-tuning the LLM with LoRA on keyword correction samples is essential for the model to utilize retrieved context effectively. 2,000 manually selected samples teach the model to trust and integrate retrieved corrections rather than ignore or over-weight them.

## Foundational Learning

- **Concept: Contrastive Learning (CLAP-style)**
  - Why needed here: The keyword retriever uses contrastive learning to align speech and text embeddings, creating the shared embedding space for retrieval
  - Quick check question: Can you explain why training with "one positive and ten randomly selected negative keywords per sample" encourages the model to cluster matching audio-text pairs while separating mismatches?

- **Concept: Streaming/Partial ASR Hypotheses**
  - Why needed here: The system queries the vector store with partial hypotheses during live inference, requiring understanding of how ASR systems produce incremental outputs
  - Quick check question: What is the tradeoff between querying earlier (more latency headroom) vs. later (more complete hypothesis context) in a streaming ASR pipeline?

- **Concept: Weighted Fusion of Multi-Source Outputs**
  - Why needed here: The final transcription combines three outputs with different weights, requiring understanding of ensemble/fusion strategies
  - Quick check question: Why might giving the RAG-based output "higher weight" make sense even though the Whisper decoder and original LLM outputs are also available?

## Architecture Onboarding

- **Component map**: Audio → Speech Encoder → Projector → [parallel: LLM generation + Keyword Retrieval via cross-modal similarity] → Fusion Module → Final transcription
- **Critical path**: Audio → Whisper-large-v3-turbo encoder → Conv+MLP projector → [parallel processing] → Weighted fusion → Phi-4-mini-Instruct LLM output
- **Design tradeoffs**: Compact LLM (3.8B) reduces deployment overhead but may limit reasoning capacity; reusing SLAM-ASR components simplifies architecture but inherits limitations; weighted fusion adds complexity but provides fallback
- **Failure signatures**: WER spikes above 25% indicate projector training issues; RAG output worse than baseline suggests fine-tuning was skipped; semantic consistency drops despite WER improvement indicates fusion weights may be over-emphasizing literal accuracy
- **First 3 experiments**:
  1. Reproduce SLAM-ASR baseline with specified AdamW settings on single RTX 3090
  2. Ablate RAG with/without fine-tuning to confirm the 16+ point WER gap
  3. Vary fusion weights systematically to find optimal balance for target domain

## Open Questions the Paper Calls Out

### Open Question 1
Can language modeling conditioned on speaker roles or lattice-aware retrieval effectively reduce the remaining WER gap to below 10%? The authors suggest this as future work since the current 12% WER gap is dominated by overlapped and rapid speech.

### Open Question 2
Does the cross-modal retrieval mechanism maintain robustness when applied to the non-English subsets of the MLC-SLM dataset? Experiments were only conducted on the English (American) dataset despite the system being designed for multilingual recognition.

### Open Question 3
What is the sensitivity of the final WER to the specific interpolation weights used in the weighted fusion module? The paper states RAG output receives higher weight but doesn't quantify the exact weights or methodology for determining them.

## Limitations

- Evaluation on limited dataset (MLC-SLM Challenge) with constrained conditions (oracle speaker labels)
- Manual curation of 2,000 keyword-correction samples introduces potential bias
- System shows brittleness outside training distribution (32.98% vs 16.06% WER without fine-tuning)
- Substantial error rate gap remains (12% WER) attributed to overlapped and rapid speech

## Confidence

- **High confidence**: General architecture combining RAG with LLM-based ASR is technically sound
- **Medium confidence**: 11.67% WER improvement is reproducible on MLC-SLM dataset but may not generalize
- **Low confidence**: Semantic consistency metric (0.9132) and its interpretation lacks comparison to established methods

## Next Checks

1. **Cross-domain generalization test**: Evaluate trained RAG-Boost system on AMI meeting corpus to assess whether 11.67% WER holds outside MLC-SLM domain
2. **Error pattern analysis**: Conduct detailed analysis isolating overlap-induced errors to determine if RAG actually reduces these or merely shifts error distribution
3. **Ablation of fine-tuning scale**: Test minimum effective number of fine-tuning samples (500, 1000, 1500) to determine if 2,000 sample requirement is necessary