---
ver: rpa2
title: Efficiently Solving Discounted MDPs with Predictions on Transition Matrices
arxiv_id: '2502.15345'
source_url: https://arxiv.org/abs/2502.15345
tags:
- learning
- prediction
- algorithm
- optimal
- sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies infinite-horizon discounted Markov decision
  processes (DMDPs) under a generative model with access to predictions on the transition
  matrix. The authors propose a novel framework to investigate how predictions can
  enhance sample efficiency and improve sample complexity bounds.
---

# Efficiently Solving Discounted MDPs with Predictions on Transition Matrices

## Quick Facts
- arXiv ID: 2502.15345
- Source URL: https://arxiv.org/abs/2502.15345
- Reference count: 40
- This paper proposes a novel framework to leverage transition matrix predictions for improving sample efficiency in discounted MDPs without requiring knowledge of prediction accuracy.

## Executive Summary
This paper studies infinite-horizon discounted Markov decision processes (DMDPs) under a generative model with access to predictions on the transition matrix. The authors propose a novel framework to investigate how predictions can enhance sample efficiency and improve sample complexity bounds. They first provide an impossibility result showing that without prior knowledge of prediction accuracy, no algorithm can achieve a sample complexity better than the state-of-the-art bound of $\tilde{O}((1-\gamma)^{-3}N\epsilon^{-2})$. To address this, they introduce the Optimistic-Predictive Mirror Descent (OpPMD) algorithm that leverages predictions without requiring knowledge of prediction error. The algorithm achieves a sample complexity bound depending on the prediction error, which is uniformly better than $\tilde{O}((1-\gamma)^{-4}N\epsilon^{-2})$, the previous best result from convex optimization methods.

## Method Summary
The paper proposes solving discounted MDPs using a minimax formulation that leverages transition matrix predictions. The OpPMD algorithm uses optimistic mirror descent with two gradient sources: stochastic gradients from sampling and predicted gradients from the transition matrix prediction. Key innovations include adaptive, parameter-free learning rates that eliminate dependence on unknown prediction error, and variance reduction via sample averaging that decreases gradient variance over time. The algorithm maintains robustness by gracefully degrading to prediction-free performance when predictions are inaccurate.

## Key Results
- Proves impossibility of beating $\tilde{O}((1-\gamma)^{-3}N\epsilon^{-2})$ sample complexity without knowing prediction error
- Introduces OpPMD algorithm that achieves prediction-dependent sample complexity bounds
- Shows prediction-dependent bound is uniformly better than previous $\tilde{O}((1-\gamma)^{-4}N\epsilon^{-2})$ convex optimization result
- Demonstrates algorithm's parameter-free nature requiring no prior knowledge of prediction accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimistic mirror descent can leverage transition matrix predictions to reduce sample complexity without knowing prediction accuracy.
- Mechanism: The algorithm constructs predicted gradients $\bar{g}_\mu^{t+1} = (\hat{I} - \gamma\hat{P})v^{t+1} - r$ using prediction $\hat{P}$ as a proxy for future gradients. When $\hat{P}$ is accurate, these predictions closely approximate true gradients, accelerating convergence. When $\hat{P}$ is inaccurate, stochastic gradients from sampling dominate, maintaining robustness.
- Core assumption: The prediction $\hat{P}$ is a valid probability transition matrix (rows sum to 1, entries in [0,1]).
- Evidence anchors:
  - [Section 4.1.2]: "The predicted gradient provides additional information to the algorithm, and its usefulness depends on the accuracy of $\hat{P}$."
  - [Section 4.2]: "Our approach novelly integrates two gradient sources for updating $\mu$: one from sampling and the other derived from predictions."
  - [Corpus]: Limited direct corpus support; related work on RL with imperfect predictions (arXiv:2510.18687) explores multi-step predictions but differs in approach.
- Break condition: If predictions are adversarially misleading (Dist > 1), the bound degrades gracefully to $\tilde{O}((1-\gamma)^{-4}N\epsilon^{-2})$, matching prediction-free baselines.

### Mechanism 2
- Claim: Adaptive learning rates eliminate dependence on unknown parameters (prediction error Dist, accuracy $\epsilon$).
- Mechanism: Learning rates $\eta_v^t$ and $\eta_\mu^t$ are defined using cumulative gradient norms (Equations 7, 9), enabling automatic adaptation. This parameter-free design avoids the need to tune for unknown prediction quality.
- Core assumption: Gradient norms remain bounded ($\|\tilde{g}_v^t\|_2^2 \leq 2$, $\|\tilde{g}_\mu^t\|_\infty$ bounded by $O(N(1-\gamma)^{-1})$).
- Evidence anchors:
  - [Section 4.2]: "Our algorithm is parameter-free, requiring no prior knowledge of the desired accuracy level $\epsilon$ and prediction error Dist."
  - [Lemma 1, Section C.1]: Establishes bounded variance for stochastic gradients.
  - [Corpus]: No directly comparable adaptive rate mechanisms in retrieved corpus.
- Break condition: Extremely large state-action spaces (N) may require numerical stabilization due to $\sqrt{N}$ scaling in learning rate denominators.

### Mechanism 3
- Claim: Variance reduction via sample averaging shrinks stochastic gradient variance as $O(1/t)$.
- Mechanism: The $\mu$-side stochastic gradient $\tilde{g}_\mu^t$ averages over all $t$ samples (Equation 12), reusing past observations. Lemma 2 proves $E[\|\tilde{g}_\mu^t - g_\mu(v_t, \mu_t)\|_\infty^2] \leq 9N^2(1-\gamma)^{-2}/t$.
- Core assumption: Samples are drawn from a stationary generative model.
- Evidence anchors:
  - [Section 4.1.1]: "This stochastic estimator is unbiased as well. More importantly, its variance decreases over time by reusing past samples and averaging."
  - [Lemma 2, Section C.1]: Provides explicit variance bound decreasing with $t$.
  - [Corpus]: No corpus papers address this specific variance reduction technique.
- Break condition: Non-stationary MDPs would violate the stationary sampling assumption.

## Foundational Learning

- Concept: **Infinite-horizon Discounted MDPs and Bellman Equations**
  - Why needed here: The entire framework reformulates DMDPs as minimax optimization via Bellman optimality conditions.
  - Quick check question: Can you derive why $v^*_i = \max_a (r_{i,a} + \gamma \sum_j p(j|i,a)v^*_j)$?

- Concept: **Mirror Descent with Entropic Regularizer**
  - Why needed here: The $\mu$-update uses exponentiated gradient descent (entropic Bregman divergence), standard for simplex-constrained optimization.
  - Quick check question: Why does the update $\mu^{t+1} \propto \mu^t \exp(-\eta g^t)$ ensure $\mu$ remains in the probability simplex?

- Concept: **Sample Complexity and Minimax Lower Bounds**
  - Why needed here: Theorem 1 establishes impossibility of beating $\tilde{O}((1-\gamma)^{-3}N\epsilon^{-2})$ without knowing prediction error.
  - Quick check question: What does the $\tilde{O}$ notation hide, and why does $(1-\gamma)$ appear with negative exponents?

## Architecture Onboarding

- Component map: Input (Prediction matrix $\hat{P}$, generative model sampler) -> v-updater (Stochastic gradient $\tilde{g}_v^t$ via sampling, mirror descent with $\ell_2$ projection) -> $\mu$-updater (Stochastic gradient $\tilde{g}_\mu^t$ plus predicted gradient $\bar{g}_\mu^{t+1}$, optimistic mirror descent with entropic regularizer) -> Output (Averaged iterate $(\bar{v}, \bar{\mu})$ converted to policy $\bar{\pi}$)

- Critical path: The $\mu$-update (Line 9) is algorithmically novel—combining $\tilde{g}_\mu^t - \bar{g}_\mu^t + \bar{g}_\mu^{t+1}$ implements optimistic mirror descent where $\bar{g}_\mu^t$ cancels if predictions match reality.

- Design tradeoffs:
  - Sample reuse (Equation 12) reduces variance but increases memory to $O(t)$
  - Adaptive learning rates remove parameter tuning but add computational overhead for cumulative norm tracking
  - Uniform sampling over N for $\tilde{g}_\mu^t$ may be inefficient for sparse state-action visitation

- Failure signatures:
  - If duality gap plateaus, check: (1) prediction error exceeds threshold, (2) learning rates underflow to zero, (3) numerical overflow in exponentiated gradients
  - If policy oscillates, verify that predicted gradient $\bar{g}_\mu^{t+1}$ is computed using updated $v^{t+1}$, not stale $v^t$

- First 3 experiments:
  1. **Sanity check**: Run OpPMD with $\hat{P} = P$ (perfect prediction) on a small 3-state MDP; duality gap should decay faster than prediction-free baseline.
  2. **Robustness test**: Inject controlled noise into $\hat{P}$ (vary Dist from 0 to 2) and plot sample complexity vs. Dist; expect graceful degradation.
  3. **Scaling study**: Fix $\epsilon, \gamma$, vary N from 10 to 10,000; verify empirical sample complexity scales as $\tilde{O}(N)$ not $\tilde{O}(N^2)$.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the framework incorporating transition matrix predictions be extended to Average-Reward MDPs (AMDPs) or constrained MDPs?
  - Basis in paper: [explicit] Appendix E explicitly identifies "Extensions to other MDP or RL models," specifically listing AMDPs, online tabular finite-horizon episodic MDPs, and constrained MDPs as potential future directions.
  - Why unresolved: The paper's theoretical analysis and the OpPMD algorithm are specifically tailored for infinite-horizon Discounted MDPs.
  - What evidence would resolve it: A modified algorithm and sample complexity analysis proving performance improvements in an AMDP or constrained setting when using predictions.

- **Open Question 2**: Can alternative forms of advice, such as Q-value predictions or raw historical datasets, be utilized to enhance sample efficiency?
  - Basis in paper: [explicit] Appendix E discusses "Other Forms of Predictions," suggesting the exploration of Q-value predictions (citing Golowich and Moitra [2022]) or the direct use of biased historical observations.
  - Why unresolved: The current study restricts its scope to black-box predictions of the transition matrix $\hat{P}$.
  - What evidence would resolve it: A theoretical framework or algorithm that integrates non-transition-matrix advice (e.g., Q-values) and demonstrates improved sample complexity bounds.

- **Open Question 3**: Is it possible to break the standard minimax sample complexity lower bound of $\tilde{O}((1-\gamma)^{-3}N\epsilon^{-2})$ if the algorithm is granted prior knowledge of the prediction error?
  - Basis in paper: [inferred] Theorem 1 establishes an impossibility result proving no algorithm can beat this bound *without* knowledge of prediction accuracy. The paper leaves open whether this specific condition is the sole barrier to breaking the lower bound.
  - Why unresolved: The paper proves a negative result for the "unknown accuracy" setting but does not explore if "known accuracy" permits strictly better bounds than the state-of-the-art.
  - What evidence would resolve it: An algorithm that utilizes known prediction error to achieve a sample complexity strictly lower than $\tilde{O}((1-\gamma)^{-3}N\epsilon^{-2})$.

## Limitations

- Impossibility result assumes adversarial prediction error, which may not reflect practical scenarios with consistent bias
- Adaptive learning rate mechanism assumes bounded gradient norms without empirical validation for high-dimensional settings
- Variance reduction analysis relies on stationary generative models, potentially fragile for non-stationary MDPs

## Confidence

**High Confidence**: The core impossibility result (Theorem 1) and its proof are well-established; the algorithmic framework (OpPMD) is clearly specified with explicit gradient definitions and learning rate formulas.

**Medium Confidence**: The theoretical sample complexity bound (Theorem 2) depends on prediction error Dist(P̂, P), but the proof requires careful verification of the optimistic mirror descent analysis with two gradient sources. The variance reduction claims (Lemma 2) appear sound but rely on stationary sampling assumptions.

**Low Confidence**: The numerical experiments lack detail on prediction error quantification and comparison with more than one baseline. The corpus reveals limited direct support for the adaptive learning rate mechanism and the specific variance reduction technique.

## Next Checks

1. **Robustness to Prediction Error**: Implement a systematic study varying Dist(P̂, P) from 0 to 2 and verify the sample complexity degrades gracefully to the prediction-free bound Õ((1-γ)⁻⁴Nε⁻²).

2. **Gradient Norm Stability**: Test the adaptive learning rates on MDPs with N=10,000 states and verify ηᵥᵗ and ηµᵗ remain stable and positive throughout training without numerical underflow.

3. **Non-stationary MDPs**: Evaluate OpPMD on MDPs with slowly drifting transition dynamics to quantify performance degradation and identify failure modes when the stationary sampling assumption breaks.