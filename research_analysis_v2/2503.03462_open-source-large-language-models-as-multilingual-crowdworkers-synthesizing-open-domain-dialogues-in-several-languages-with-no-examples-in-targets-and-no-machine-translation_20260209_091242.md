---
ver: rpa2
title: 'Open-Source Large Language Models as Multilingual Crowdworkers: Synthesizing
  Open-Domain Dialogues in Several Languages With No Examples in Targets and No Machine
  Translation'
arxiv_id: '2503.03462'
source_url: https://arxiv.org/abs/2503.03462
tags:
- seed
- personas
- your
- bertscore
- personachat
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the lack of multilingual and culturally diverse
  Open-Domain Dialogue (ODD) datasets and the "open-domain paradox" (ODP) where dialogues
  lack real-life premises. The proposed method leverages multilingual instruction-tuned
  LLMs to generate ODD datasets in multiple target languages using only source language
  examples, avoiding machine translation.
---

# Open-Source Large Language Models as Multilingual Crowdworkers: Synthesizing Open-Domain Dialogues in Several Languages With No Examples in Targets and No Machine Translation

## Quick Facts
- **arXiv ID:** 2503.03462
- **Source URL:** https://arxiv.org/abs/2503.03462
- **Reference count:** 40
- **Primary result:** Generated 493k multilingual dialogues in 29 languages without MT, using LLaMA3.1-8B-Instruct, achieving higher quality than baselines on most metrics.

## Executive Summary
This paper introduces a novel pipeline to synthesize large multilingual Open-Domain Dialogue (ODD) datasets using open-source LLMs as "crowdworkers," avoiding Machine Translation and English examples in target languages. The method decomposes task guidelines into structured prompts and incorporates Common Ground (CG) and Speech Events (SE) to address the Open-Domain Paradox, producing more realistic dialogues. Applied to PersonaChat, the resulting MOUD dataset (493k dialogues, 29 languages) is evaluated using GPT-4o-as-a-judge and human raters, showing LLaMA3.1-8B-Instruct's superior performance. Fine-tuned BLOOM models trained on MOUD outperform XPersona, demonstrating the dataset's value as a complementary multilingual resource.

## Method Summary
The method uses open-source instruction-tuned LLMs to generate multilingual ODD data without relying on Machine Translation. It decomposes crowdsourcing guidelines into structured prompts with instructions, target-language constraints, and few-shot examples. A "common ground" narrative and diverse "speech event" types are incorporated to mitigate the Open-Domain Paradox. The pipeline generates personas, CG, and dialogues in three LLM steps, filters outputs for language and quality, and evaluates using GPT-4o-as-a-judge and human raters. The approach is applied to PersonaChat, producing MOUD, a dataset of 493k dialogues in 29 languages.

## Key Results
- LLaMA3.1-8B-Instruct consistently outperformed Mistral-7B, Gemma-1.1, and Aya-23-8B in LLM-as-a-judge evaluations across specificity, fluency, humanness, and taxonomy relevance.
- Human evaluations for Spanish, Chinese, and French aligned with LLM judgments, ranking LLaMA3.1-8B highest in most criteria.
- BLOOM-560M models fine-tuned on MOUD achieved better average performance than XPersona across most metrics, with further gains when combining both datasets.

## Why This Works (Mechanism)

### Mechanism 1
Instruction-tuned LLMs can generate coherent, culturally nuanced dialogue data in target languages using only source-language demonstrations, without intermediate machine translation. The pipeline decomposes crowdsourcing guidelines into a structured prompt containing instructions (Ik), constraints (Ck,lT) enforcing target-language specificity, and source examples (Dk,ns,en). The LLM's multilingual instruction-following capability allows it to "think" in the target language while adhering to the task logic shown in English examples. Core assumption: The LLM possesses sufficient multilingual proficiency and cross-lingual transfer ability to generate natural, culturally appropriate text in the target language without producing "translationese." Evidence anchors: [abstract] "By eschewing explicit Machine Translation in this approach, we enhance the adherence to language-specific nuances." [section 3.2] "These are often not derived from data sourcing guidelines but rather additional statements... crucial elements that a MT module cannot provide but can be thought (inferred) by a multilingual LLM." Break condition: If the LLM lacks robust proficiency in a specific target language (e.g., low-resource languages like Yoruba), the generated content may exhibit low specificity (Table 2 shows lower scores for Yoruba) or unnatural phrasing.

### Mechanism 2
Incorporating a "common ground" (CG) narrative and diverse "speech event" (SE) types into the generation prompt mitigates the Open-Domain Paradox (ODP), producing more realistic, contextually grounded dialogues. A "Narrator" LLM instance first generates a context paragraph (the CG) that intertwines the two generated personas, a randomly sampled SE type (e.g., "Interrogation," "Reminiscing"), and target-language cultural specifics. This CG is then provided as a premise to the two conversationalist LLM instances, shaping their interaction. Core assumption: Real-life open-domain conversations typically have unstated premises (CG) and vary in purpose (SE). Explicitly modeling these for the LLM leads to dialogues that better mimic human-human interaction beyond generic "small talk." Evidence anchors: [abstract] "To enhance the openness of generated dialogues and mimic real life scenarii, we added the notion of speech events... and also that of common ground which represents the premises of a conversation." [section 4.4.2] "This step is entirely new. Instructions and constraints were designed from scratch with the following objectives: creating a CG that takes into account both speakers’ personas, the targeted SE type and lT specificity." Break condition: If the generated CG is incoherent with the personas or SE type (e.g., a CG for "making up" that doesn't involve prior conflict), the downstream conversation may become disjointed or irrelevant.

### Mechanism 3
An LLM (specifically GPT-4o) can function as a reliable, scalable evaluator for assessing the quality of generated multilingual dialogues across criteria like fluency, specificity, and humanness. GPT-4o is prompted as a "smart evaluator, native {language} speaker" with detailed rating rubrics (1-5 scales) for criteria such as specificity to language, fluency, toxicity, and relevance to taxonomies. It processes batches of generated personas, CG, and conversations to produce numerical ratings. Core assumption: The LLM evaluator's judgments correlate sufficiently with human judgments for the purpose of comparative model selection and quality filtering, despite inherent biases. Evidence anchors: [section 5.2.1] "While not a perfect substitute for comprehensive human evaluations, GPT4o provides a feasible alternative, enabling consistent and scalable quality assessments across multiple languages." [Table 3, Table 20, Table 22, Table 24] Show that for languages with human evaluation (Spanish, Chinese, French, etc.), both LLM and human raters consistently rank LLaMA3.1-8B highest, indicating aligned judgment direction. [corpus] Related work "MEDAL" proposes benchmarking LLMs as multilingual dialogue evaluators, lending indirect support to this mechanism's validity. Break condition: If the LLM evaluator exhibits systematic bias (e.g., favoring its own output or certain languages), its rankings may not reflect true human perception of quality. The paper notes toxicity correlations were not computable due to consistently high ratings.

## Foundational Learning

- **Concept:** **Open-Domain Dialogue (ODD) & The Open-Domain Paradox (ODP)**
  - **Why needed here:** The paper's core contribution addresses the ODP—the idea that "open-domain" dialogue often lacks real-life premises like shared context or specific goals. Understanding this is crucial to grasping why the authors introduce Common Ground and Speech Events.
  - **Quick check question:** What are two limitations of traditional "open-domain" dialogue data that the authors claim their method addresses?

- **Concept:** **Instruction Tuning for LLMs**
  - **Why needed here:** The entire pipeline relies on the ability of instruction-tuned LLMs to follow complex, decomposed guidelines in a prompt and generate text in a specified target language. Without this capability, the method would not be feasible.
  - **Quick check question:** How does "decomposition reframing" and "restraining reframing" (from Section 3.1/3.2) modify the original crowdsourcing guidelines for LLM consumption?

- **Concept:** **LLM-as-a-Judge Evaluation**
  - **Why needed here:** Evaluating the massive multilingual output (493k dialogues) requires a scalable method. The paper uses GPT-4o as the primary evaluator, comparing its judgments to a smaller set of human evaluations. Understanding the strengths and limitations of this approach is key to interpreting the results.
  - **Quick check question:** According to the paper, what are the limitations of using an LLM as the sole evaluator, and how did the authors attempt to validate its judgments?

## Architecture Onboarding

- **Component Map:** Persona Generation -> CG Generation -> Dialogue Generation -> Filtering -> Evaluation

- **Critical Path:** Taxonomy Augmentation (manual + LLM-assisted) -> Prompt Construction (for Personas) -> Persona Generation -> CG & SE Selection -> Prompt Construction (for CG) -> CG Generation -> Prompt Construction (for Dialogue) -> Dialogue Generation (two LLM instances) -> Filtering -> Evaluation (LLM & Human).

- **Design Tradeoffs:**
  - **Open vs. Closed-Source Models:** Chose open-source (7-8B parameters) for reproducibility and cost, potentially sacrificing some generation quality compared to larger proprietary models.
  - **0-shot vs. Few-shot for Steps:** Used few-shot for Persona Generation (has PersonaChat examples) but 0-shot for CG and Conversation Generation (no prior CG examples).
  - **Evaluation Strategy:** Heavily relied on LLM-as-a-judge for scale, supplementing with limited human evaluation for validation due to cost/logistics.

- **Failure Signatures:**
  - **Language Mixing:** Output contains English or another unintended language (caught by hits@2 language detection).
  - **Repetitive Utterances:** Conversationalist LLMs generate looping or stale responses (mitigated by repetition penalty, filtered in post-processing).
  - **Incoherent Common Ground:** CG doesn't logically connect the two personas or the chosen SE type (would lead to low "relevancy" scores).
  - **Low Specificity:** Generated content uses generic entities (e.g., "a city") instead of culturally specific ones (e.g., "Lyon"), especially for low-resource languages.

- **First 3 Experiments:**
  1. **Single-Language Pipeline Test:** Run the full pipeline (Persona -> CG -> Conversation) for one high-resource (e.g., French) and one low-resource (e.g., Swahili) language using a single generator model (e.g., LLaMA3.1-8B). Manually inspect 5-10 outputs for each step to verify language specificity, coherence, and SE/CG integration.
  2. **Prompt Ablation:** Generate conversations for a fixed language-seed pair using three prompt variants: (a) full prompt with CG and SE, (b) prompt with SE only, (c) prompt with no CG or SE (like original PersonaChat). Compare outputs using GPT-4o evaluation on "humanness" and "relevancy to taxonomies" to measure the impact of CG/SE.
  3. **Model Comparison:** For a fixed language and prompt, generate dialogues using two different generator models (e.g., LLaMA3.1-8B and Mistral-7B). Use the LLM-as-a-judge to evaluate a sample on all criteria. Compare scores to validate the paper's finding that LLaMA3.1-8B performs best.

## Open Questions the Paper Calls Out

- **Open Question 1:** How do specialized dialogue model architectures designed to explicitly process Common Ground (CG) and Speech Events (SE) compare against the shallow fine-tuning baselines presented in the paper? Basis in paper: [explicit] The Conclusion states that "promising directions for future research" include "specialized architectures tailored to [MOUD's] unique characteristics." Why unresolved: The baseline experiments utilized a standard multitask fine-tuning approach (BLOOM-560M) which was not specifically designed to handle the new constraints of CG and SE. What evidence would resolve it: A comparison of performance metrics (e.g., Hits@1, Rouge-L) between the baseline models and models architected to condition dialogue generation on the provided common ground and speech event taxonomies.

- **Open Question 2:** To what extent does the GPT-4o-as-a-judge evaluation correlate with human assessment in low-resource languages where model performance varies significantly? Basis in paper: [explicit] The Limitations section notes that evaluations performed via LLM-as-Judge "may not be as relevant as evaluations performed by humans," particularly given the lack of human evaluators for many languages. Why unresolved: While human evaluation was conducted for 5 languages, the majority of the 29 languages rely solely on the LLM judge, creating a validity gap. What evidence would resolve it: A comprehensive correlation analysis (e.g., Cohen’s Kappa or Pearson coefficient) between GPT-4o scores and human annotator scores for low-resource languages like Swahili or Yoruba.

- **Open Question 3:** Can models fine-tuned on MOUD demonstrate a measurable mitigation of the "Open-Domain Paradox" (ODP) compared to those trained on PersonaChat? Basis in paper: [inferred] The paper introduces Common Ground and Speech Events specifically to address the ODP, but the baseline evaluation relies on standard metrics (Perplexity, Rouge-L) that do not directly measure the paradox. Why unresolved: It remains unproven whether the generated "Common Ground" actually reduces the paradox in downstream conversational agents, as no specific metric for "paradox reduction" was used in the evaluation. What evidence would resolve it: A qualitative or quantitative study measuring the relevance and contextual grounding of responses in conversations initiated without specific premises, comparing MOUD-tuned models against PersonaChat-tuned models.

- **Open Question 4:** Does the proposed pipeline completely eliminate "translationese" artifacts and cultural non-specificity in low-resource languages where the underlying LLMs may have weaker internal representations? Basis in paper: [inferred] While the paper claims to eschew Machine Translation to avoid translationese, the results show lower performance in low-resource languages (e.g., Yoruba), suggesting the generator may still struggle with native cultural nuances compared to high-resource languages. Why unresolved: The analysis of "language specificity" relies heavily on automatic syntax analysis and LLM-as-judge, which may not catch subtle cultural misalignments or implicit translation artifacts in the generated data. What evidence would resolve it: A "Turing Test" style evaluation by native speakers for low-resource languages to determine if dialogues sound naturally generated in the target culture versus "Anglicized" in content.

## Limitations
- The LLM-as-a-judge evaluation, while scalable, may have systematic biases not fully mitigated by the limited human validation.
- The pipeline's effectiveness depends on the LLM's cross-lingual transfer ability, which is not uniformly verified across all 29 languages.
- The claim that the method avoids "translationese" is based on design principles and indirect evidence rather than a direct comparison with MT-based approaches.

## Confidence
- **High Confidence:** The pipeline successfully generates a large multilingual ODD dataset (MOUD) in 29 languages, as evidenced by the 493k dialogues produced and the structured evaluation showing LLaMA3.1-8B-Instruct outperforming other models in most metrics.
- **Medium Confidence:** The Common Ground and Speech Event mechanisms improve dialogue realism by addressing the Open-Domain Paradox. This is supported by the evaluation but lacks direct corpus comparison to confirm the causal impact.
- **Low Confidence:** The claim that the method avoids "translationese" is based on design principles and indirect evidence (e.g., lower specificity in low-resource languages) rather than a direct comparison with MT-based approaches.

## Next Checks
1. **Cross-Lingual Transfer Validation:** Run the full pipeline for a low-resource language (e.g., Swahili) and compare the generated dialogues' specificity and coherence to those in a high-resource language (e.g., French). Manually inspect outputs to verify if the LLM maintains target-language specificity without code-switching.

2. **Impact of Common Ground and Speech Events:** Generate dialogues for a fixed language-seed pair using two variants: (a) full prompt with CG and SE, (b) prompt with no CG or SE. Use the LLM-as-a-judge to evaluate both sets on "humanness" and "relevancy to taxonomies." Compare scores to measure the marginal impact of CG/SE on dialogue quality.

3. **Human Evaluation Expansion:** Conduct a larger-scale human evaluation (e.g., 100 dialogues per language) for a subset of target languages (e.g., Spanish, Chinese, French, Swahili) to validate the LLM-as-a-judge's rankings. Compare human and LLM judgments to assess alignment and identify potential biases.