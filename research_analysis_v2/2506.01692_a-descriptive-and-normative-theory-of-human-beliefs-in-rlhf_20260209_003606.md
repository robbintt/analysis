---
ver: rpa2
title: A Descriptive and Normative Theory of Human Beliefs in RLHF
arxiv_id: '2506.01692'
source_url: https://arxiv.org/abs/2506.01692
tags:
- agent
- human
- preferences
- post
- belief
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new perspective on human preferences in
  reinforcement learning from human feedback (RLHF) by incorporating human beliefs
  about agent capabilities into the preference model. The authors propose that humans
  consider not just observed outcomes but also their beliefs about the agent's potential
  capabilities when providing preferences.
---

# A Descriptive and Normative Theory of Human Beliefs in RLHF

## Quick Facts
- arXiv ID: 2506.01692
- Source URL: https://arxiv.org/abs/2506.01692
- Reference count: 35
- Primary result: Human beliefs about agent capabilities significantly affect RLHF preferences and can be systematically altered through priming interventions

## Executive Summary
This paper proposes that human preferences in RLHF depend not only on observed outcomes but also on beliefs about the agent's capabilities. The authors formalize this through a belief-based preference model and establish theoretical bounds on how belief-agent capability mismatches degrade policy performance. Through synthetic experiments and a human study, they demonstrate that aligning human beliefs with actual agent capabilities leads to better RLHF outcomes and that priming interventions can systematically shift preferences.

## Method Summary
The method involves a belief-based preference model that replaces the optimal advantage function with one based on the human's imagined post-RLHF agent policy. The approach uses Contrastive Preference Learning (CPL) to train policies from preferences generated via softmax over discounted advantage values along trajectories. For human studies, participants were shown priming videos (safe/unsafe driving demonstrations) before providing preferences for CARLA trajectories, with preferences collected via Prolific and analyzed using Kruskal-Wallis, Dunn-Bonferroni, and Cliff's delta tests.

## Key Results
- Belief-based preference model successfully captures how humans judge action quality relative to perceived agent capabilities
- Theoretical bound shows policy performance degradation is proportional to agent-labeler capability disagreement
- Human study demonstrates priming interventions significantly shift preference distributions (p=0.015, Cliff's delta=0.31)
- Gridworld experiments show highest returns when labeler's belief matches actual agent capability (diagonal pattern in return matrix)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human preferences depend on beliefs about agent capabilities, not just observed outcomes or theoretical optimality.
- Mechanism: The belief-based preference model replaces the optimal advantage function A* with A^π_belief, where π_belief represents the human's imagined post-RLHF agent policy. Preferences are generated via softmax over discounted A^π_belief values along trajectories (Eq. 3), capturing how humans judge action quality relative to what they believe the agent can achieve.
- Core assumption: Humans mentally simulate agent behavior beyond observed trajectory segments using an internal model of agent capability.
- Evidence anchors:
  - [abstract] "We propose that human beliefs about the capabilities of the agent being trained also play a key role in preference generation."
  - [section 4] Formal definition of belief-based preference model (Eq. 3) and agent capability belief (Definition 4.1)
  - [corpus] Related work on modeling human beliefs in learning (Reddy et al., Gong & Zhang) supports non-omniscient human assumptions
- Break condition: If humans provide preferences purely reflexively without forward simulation, or if they assume perfect optimality regardless of context, the mechanism collapses to standard regret-based models.

### Mechanism 2
- Claim: Mismatch between human beliefs and agent capabilities degrades post-RLHF policy performance, bounded by disagreement magnitude.
- Mechanism: When a human's Q^π_belief diverges from the normative ideal Q^π*_belief by δ at a state-action pair, the resulting policy can select suboptimal actions. The theoretical bound (Theorem 4.3) shows J^π_post ≥ J^π*_post − δ/(1−γ), meaning disagreement propagates through the discount horizon.
- Core assumption: RLHF produces deterministic policies that respect preferences; disagreements are localized to specific state-action pairs.
- Evidence anchors:
  - [section 4.1] Cliff example shows how preferring a risky path under incorrect optimality beliefs leads to catastrophic outcomes
  - [section 5, Table 1] Gridworld experiments show highest returns when labeler's ϵ'-greedy belief matches agent's actual ϵ (diagonal pattern)
  - [corpus] Limited direct corpus evidence for this specific bound; primarily theoretical contribution
- Break condition: If RLHF doesn't respect preferences (e.g., regularization dominates), or if function approximation enables beneficial generalization from incorrect preferences, the bound may be loose.

### Mechanism 3
- Claim: Human beliefs about agent capabilities can be systematically altered through priming interventions, changing preference distributions.
- Mechanism: Exposing labelers to demonstrations of agent behavior (safe vs. unsafe driving) shifts their prior over agent capabilities, which updates π_belief and thus A^π_belief used in preference generation. This creates an intervention pathway to reduce agent-labeler disagreement.
- Core assumption: Humans form or update beliefs about agents from observed behavior, and these beliefs persist during preference elicitation.
- Evidence anchors:
  - [abstract] "We...confirm via a human study that beliefs about agent capabilities do, in fact, significantly affect preferences and can be influenced through simple interventions."
  - [section 6] Dunn-Bonferroni test shows significant difference (p=0.015) between safe and unsafe priming groups; Cliff's delta=0.31
  - [corpus] "Influencing Humans to Conform to Preference Models for RLHF" (neighbor paper) explores related intervention mechanisms
- Break condition: If priming effects decay rapidly, if participants ignore priming context, or if pre-existing beliefs are too strongly held, interventions will have negligible effect.

## Foundational Learning

- **Advantage function A^π(s,a) = Q^π(s,a) − V^π(s)**
  - Why needed here: The entire belief-based preference model is built on advantage functions; understanding how A captures relative action quality (not absolute value) is essential for interpreting Eq. 3.
  - Quick check question: If V^π(s) = 10 and Q^π(s, a₁) = 12 and Q^π(s, a₂) = 8, which action has positive advantage?

- **Regret-based preference models (vs. partial return)**
  - Why needed here: This paper extends regret-based RLHF; you must understand that regret models judge action quality (A*) rather than summed rewards to see why beliefs about future behavior matter.
  - Quick check question: In a partial return model, would a trajectory that ends one step before a +100 reward receive that reward in preference comparison?

- **Policy performance bounds under distribution shift**
  - Why needed here: Theorem 4.3 uses the discount factor γ to bound how a single disagreement propagates; understanding how (1−γ)⁻¹ emerges from state visitation distributions is key to interpreting the result.
  - Quick check question: As γ → 1, does the bound on policy loss from a fixed disagreement δ increase or decrease?

## Architecture Onboarding

- **Component map**: Preference Dataset -> Belief Elicitation Module (Optional) -> Belief-Based Preference Model -> RLHF Trainer -> Capability Estimator
- **Critical path**:
  1. Determine agent capability constraints (e.g., representable policy class, action noise ϵ)
  2. Select or design belief alignment strategy (inform labelers, show priming demonstrations, or collect online)
  3. Collect preferences using belief-aligned instructions/interventions
  4. Train policy via RLHF on collected preferences
  5. Evaluate post-RLHF policy return; compare against theoretical bound
- **Design tradeoffs**:
  - **Informing vs. priming**: Direct information requires labeler comprehension; priming is implicit but less controllable
  - **Online vs. offline collection**: Online with intermittent priming adapts beliefs to current policy but increases complexity and cost
  - **Assuming optimality vs. estimated capability**: Optimality is simpler but can cause catastrophic failure; estimation requires accurate capability model
- **Failure signatures**:
  - **Catastrophic outcomes under assumed optimality**: Agent learns risky behaviors (e.g., cliff paths) that it cannot execute reliably
  - **Over-conservative policies**: Agent learns overly safe behaviors when labelers underestimate capabilities
  - **High variance in preferences**: Priming inconsistent or labelers ignoring instructions; check attention checks and confidence ratings
  - **Bound violation**: If observed J^π_post < J^π*_post − δ/(1−γ), assumptions (deterministic policy, preference-respecting RLHF) may be violated
- **First 3 experiments**:
  1. Replicate gridworld experiment with varied ϵ (agent) and ϵ' (labeler belief) values; verify diagonal pattern in returns matches Table 1
  2. Run human preference collection with safe/unsafe/control priming in a simple domain (e.g., navigation with obstacles); confirm preference shift via Kruskal-Wallis test
  3. Implement belief alignment by informing labelers of a known agent limitation (e.g., "agent has 20% action noise"); compare final policy return against uninformed baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What algorithmic advances can effectively mitigate the impact of incorrect human beliefs about agent capabilities during the RLHF process?
- Basis in paper: [explicit] The conclusion states: "Future work may include... algorithmic advances that mitigate the impact of incorrect beliefs on RLHF."
- Why unresolved: The paper primarily focuses on modeling beliefs, establishing theoretical bounds on the error caused by mismatches, and using manual priming interventions rather than developing automated learning algorithms to correct for belief-induced errors.
- What evidence would resolve it: An algorithm that infers or accounts for human beliefs during reward learning, resulting in policies that are robust to labeler misconceptions about agent limitations.

### Open Question 2
- Question: How can improved priming strategies be developed to ensure human beliefs are as accurate as possible regarding agent capabilities?
- Basis in paper: [explicit] The conclusion explicitly calls for "developing improved priming strategies to ensure human beliefs are as accurate as possible."
- Why unresolved: The human study demonstrated that simple video priming (safe vs. unsafe) affects preferences, but did not verify that these interventions fully aligned beliefs with the "normative ideal" ($Q^{\pi^*_{belief}}$) or optimized final policy performance.
- What evidence would resolve it: A validated priming protocol that minimizes the "agent-labeler disagreement" measure defined in the paper, confirmed through both subjective belief reporting and objective improvement in post-RLHF policy returns.

### Open Question 3
- Question: Does online preference collection with intermittent priming effectively align labeler beliefs with evolving agent capabilities?
- Basis in paper: [explicit] Section 6.1 ("Recommendations For Researchers") proposes "Online preference collection with intermittent priming" as a method where "labelers' beliefs... may be continuously updated."
- Why unresolved: This is proposed as a theoretical best practice, but the paper's experiments rely on offline data collection and single-stage priming, leaving the efficacy of a continuous, online feedback loop untested.
- What evidence would resolve it: Empirical results from an online RLHF setup where labelers are periodically primed with current agent performance data, showing reduced belief mismatch and higher final reward compared to static labeling setups.

## Limitations
- Belief modeling precision is limited by inability to directly observe human mental simulation processes
- Results primarily demonstrated in gridworld and simulated driving environments, limiting domain generalizability
- Priming intervention scalability for large-scale RLHF systems remains untested

## Confidence
- **High confidence**: The theoretical framework linking belief-agent capability mismatch to policy performance degradation is mathematically sound with supporting gridworld evidence
- **Medium confidence**: The human study demonstrating priming effects shows statistically significant results but with modest effect size and limited external validity
- **Medium confidence**: The core claim about human preferences depending on beliefs is supported by theory and experiments, though real-world applicability has uncertainty

## Next Checks
1. Test belief alignment effects in a different RLHF domain (e.g., text summarization or dialogue) to assess generalizability beyond navigation tasks
2. Measure preference stability and belief persistence over extended periods (days/weeks) to determine if priming interventions have lasting impact on preference quality
3. Evaluate how well various methods for estimating agent capabilities (oracle vs. heuristic) predict actual post-RLHF performance across multiple domains and capability regimes