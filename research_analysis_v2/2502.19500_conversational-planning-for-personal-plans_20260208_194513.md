---
ver: rpa2
title: Conversational Planning for Personal Plans
arxiv_id: '2502.19500'
source_url: https://arxiv.org/abs/2502.19500
tags:
- user
- language
- plan
- planning
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper presents a conversational AI framework for assisting
  users with long-term personal plans using large language models (LLMs). The key
  innovation is a hierarchical planning architecture where an LLM meta-controller
  decides between three macro-actions: "ask a question," "add a step," or "alter a
  step" to a user''s plan.'
---

# Conversational Planning for Personal Plans

## Quick Facts
- arXiv ID: 2502.19500
- Source URL: https://arxiv.org/abs/2502.19500
- Reference count: 34
- One-line primary result: A hierarchical LLM framework for conversational personal planning that uses a meta-controller to select between "ask," "add," and "alter" actions for long-term goal assistance.

## Executive Summary
This paper presents a conversational AI framework for assisting users with long-term personal plans using large language models. The key innovation is a hierarchical planning architecture where an LLM meta-controller decides between three macro-actions—ask a question, add a step, or alter a step—to a user's plan. These macro-actions are executed by tool-use augmented LLM sub-policies that can fetch and rank relevant content. The framework enables interactive adaptation of plans based on user feedback through natural language.

## Method Summary
The framework uses a hierarchical policy with a meta-controller LLM selecting among three macro-actions, executed by specialized sub-policies. Each policy layer uses chain-of-thought prompting and generates structured JSON outputs. Low-level policies invoke external tools (SEARCH, RECOMMEND-ENGINE) to fetch content for plan steps, then rank results using LLM judgment. The system maintains a structured plan state updated across sessions based on user feedback.

## Key Results
- Demonstrated effectiveness across domains including learning (explaining inventors to children) and health (CrossFit coaching)
- Framework enables creation of structured plans with relevant content fetched and ranked for each step
- System can adapt plans based on user interactions through natural language feedback
- Bridges gap between short-term conversational agents and long-term goal-oriented assistance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical decomposition of planning decisions enables handling of long-horizon goals that single-pass LLM responses cannot address effectively.
- Mechanism: A meta-controller LLM selects among discrete macro-actions (ask-question, add-steps, alter-steps), delegating execution to specialized sub-policies. This separates what to do from how to do it, reducing decision complexity at each level.
- Core assumption: LLMs perform better when reasoning is decomposed into discrete, scoped decisions rather than generating entire plans monolithically.
- Evidence anchors: Hierarchical architecture explicitly designed with meta-controller and sub-policies; Plan-and-Act corpus corroborates that "separating high-level planning from low-level execution" improves long-horizon task performance.

### Mechanism 2
- Claim: Chain-of-thought prompting enables structured reasoning for both action selection and execution within the LLM policies.
- Mechanism: Each policy layer uses CoT-prompted LLMs that generate intermediate reasoning tokens before outputs. Meta-controller produces thoughts τ ∈ L, actions, and arguments in structured JSON format.
- Core assumption: Explicit reasoning traces improve decision quality for planning tasks compared to direct output generation.
- Evidence anchors: All policies instantiated with CoT prompting; structured output generation required for hierarchical coordination.

### Mechanism 3
- Claim: Tool-use integration grounds abstract plan steps in retrievable, actionable content.
- Mechanism: Low-level policies invoke external tools to fetch n items per step, then rank to top-k via LLM judgment. This approximates a two-stage retrieval-ranking pipeline.
- Core assumption: LLMs can reliably select and rank relevant content when given tool outputs, and tool outputs provide grounding that pure generation cannot.
- Evidence anchors: Tool-use explicitly integrated for content fetching and ranking; retrieval-ranking pipeline approximated through tool invocation and CoT prompting.

## Foundational Learning

- **Hierarchical Reinforcement Learning (Options Framework)**
  - Why needed here: The paper explicitly frames the architecture using RL hierarchy concepts—meta-controllers, options, and sub-policies. Understanding this framing is necessary to interpret the design choices.
  - Quick check question: Can you explain why a two-level policy hierarchy might outperform a flat policy for multi-session tasks?

- **Chain-of-Thought Prompting**
  - Why needed here: Every policy layer uses CoT prompting. Without understanding how CoT structures reasoning, you cannot debug or modify the prompting strategies.
  - Quick check question: What is the difference between zero-shot prompting and chain-of-thought prompting for a multi-step planning task?

- **Partially Observable Markov Decision Process (POMDP)**
  - Why needed here: The paper frames user interactions as a POMDP where the agent receives only partial observations via natural language. This affects how state is tracked and decisions are made.
  - Quick check question: Why does partial observability matter for a conversational planning agent?

## Architecture Onboarding

- **Component map:**
  - User Goal -> Meta-Controller (LLM + CoT) -> Sub-Policies (3 LLMs + CoT) -> Low-Level Tool Policies (LLM + Tool Use) -> Plan State
  - User Feedback -> Context Window -> All Components

- **Critical path:**
  1. User provides goal → Meta-controller receives initial context
  2. Meta-controller selects macro-action (typically add-steps initially)
  3. Sub-policy generates plan steps with search keywords
  4. Low-level policy fetches and ranks content per step
  5. User provides feedback (answers questions or free-form)
  6. Feedback updates context → Meta-controller selects next macro-action (may alter or add)
  7. Loop continues across sessions

- **Design tradeoffs:**
  - Fixed macro-action space vs. extensible: Current three actions (ask, add, alter) cover plan maintenance but may not generalize to all domains (e.g., no "delete step" action mentioned)
  - Same LLM vs. specialized LLMs per policy: Paper notes "potentially the same" LLM with different CoT prompts. Tradeoff between simplicity and specialization
  - Context window accumulation: User feedback grows indefinitely; no explicit memory management described. Long conversations may hit context limits or degrade coherence

- **Failure signatures:**
  - Meta-controller oscillates between macro-actions without completing plan updates
  - Sub-policy generates steps with irrelevant or overly generic search keywords, causing poor content retrieval
  - User feedback is ignored or misinterpreted—plan fails to adapt
  - Tool outputs are low-quality, leading to unhelpful recommendations
  - Plan state becomes incoherent across sessions (e.g., duplicate or contradictory steps)

- **First 3 experiments:**
  1. Macro-action decision audit: Log meta-controller decisions over 50 user sessions. Analyze distribution of ask/add/alter actions and identify cases where the chosen action was clearly suboptimal (manual review). This validates the meta-controller's judgment calibration.
  2. Sub-policy output quality check: For 20 generated plan steps, evaluate whether search keywords lead to relevant top-3 results (human judgment). Correlate keyword specificity with retrieval quality to identify prompting improvements.
  3. Feedback incorporation test: Provide controlled user feedback (e.g., "remove all steps about X") and verify the plan updates correctly. Measure latency and accuracy of adaptation to diagnose context-handling issues.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed framework maintain coherence and personalization over multi-month timespans given context window constraints?
- Basis in paper: [inferred] The paper claims to support tasks lasting "months, or even years" (Abstract), yet relies on context concatenation for policy updates (Section 3.2), which is susceptible to context length limits.
- Why unresolved: The mechanism for "interactive planning" depends on appending history to the prompt, which becomes infeasible for very long interaction histories without external memory or summarization.
- What evidence would resolve it: A longitudinal user study demonstrating consistent plan quality as interaction history grows significantly beyond the training context window.

### Open Question 2
- Question: Does the hierarchical decomposition of the LLM into a meta-controller and sub-policies yield better planning outcomes than a single-prompt agent?
- Basis in paper: [inferred] Section 3.1 motivates the architecture by analogizing to hierarchical RL, but the paper provides no ablation comparing this complex structure against a baseline flat LLM prompted to perform the same tasks.
- Why unresolved: It is unclear if the engineering overhead of distinct modules (meta-controller, sub-policies, tool-use) is necessary, or if a standard LLM could mimic this behavior implicitly.
- What evidence would resolve it: An ablation study comparing task success rates and plan coherence between the hierarchical framework and a non-hierarchical LLM baseline.

### Open Question 3
- Question: How does the system quantitatively improve user goal achievement compared to standard conversational agents?
- Basis in paper: [inferred] The paper acknowledges the evaluation is strictly "qualitative" (Section 4), demonstrating effectiveness through examples rather than statistical metrics.
- Why unresolved: Without quantitative metrics (e.g., plan completion rates, user effort scores), the actual utility of the system over existing chatbots remains subjective.
- What evidence would resolve it: A controlled user study measuring objective success rates in learning and health domains against a control group using a standard LLM interface.

## Limitations
- Evaluation is entirely qualitative without quantitative metrics for plan quality, adaptation accuracy, or user satisfaction
- Macro-action space of three actions may not generalize to all planning domains
- Indefinite context accumulation without memory management could degrade performance over long conversations
- Tool integration effectiveness depends on external APIs and data sources not specified in the paper

## Confidence
- **High confidence:** The hierarchical decomposition mechanism is well-supported by explicit design choices and corroborated by Plan-and-Act
- **Medium confidence:** CoT prompting effectiveness is claimed but lacks direct evidence linking it to planning success in this specific architecture
- **Low confidence:** Tool-use integration benefits are asserted but the paper provides minimal evidence of how often tools improve over pure generation or what happens when tools fail

## Next Checks
1. **Macro-action calibration audit:** Track meta-controller decisions across 50 sessions, identifying patterns of oscillation or inappropriate action selection through manual review of decision sequences
2. **Content retrieval effectiveness test:** For 20 generated plan steps, evaluate whether top-3 retrieved items are relevant (human judgment) and correlate keyword specificity with retrieval quality
3. **Adaptation accuracy validation:** Provide controlled user feedback scenarios and measure whether plan updates correctly incorporate changes, tracking both latency and accuracy of modifications