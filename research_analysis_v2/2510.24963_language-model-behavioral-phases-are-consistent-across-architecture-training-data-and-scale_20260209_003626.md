---
ver: rpa2
title: Language Model Behavioral Phases are Consistent Across Architecture, Training
  Data, and Scale
arxiv_id: '2510.24963'
source_url: https://arxiv.org/abs/2510.24963
tags:
- pythia
- parc
- seed
- language
- mamba
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study demonstrates that across model architectures, training
  data, and scale, autoregressive language models exhibit consistent behavioral phases
  during pretraining. The authors show that up to 98% of variance in language model
  behavior at the word level can be explained by three simple heuristics: unigram
  probability (frequency), n-gram probability, and semantic similarity between the
  word and its context.'
---

# Language Model Behavioral Phases are Consistent Across Architecture, Training Data, and Scale

## Quick Facts
- **arXiv ID**: 2510.24963
- **Source URL**: https://arxiv.org/abs/2510.24963
- **Reference count**: 40
- **Primary result**: Across architectures, training data, and scale, autoregressive LMs exhibit consistent behavioral phases during pretraining, overfitting to n-grams of increasing order

## Executive Summary
This study demonstrates that autoregressive language models exhibit consistent behavioral phases during pretraining, regardless of architecture (transformers, Mamba, RWKV), training data (OpenWebText, The Pile), or scale (14M-12B parameters). The authors show that up to 98% of variance in language model behavior at the word level can be explained by three simple heuristics: unigram probability (frequency), n-gram probability, and semantic similarity between the word and its context. All tested models progressively overfit to n-grams of increasing n (from unigrams to higher-order n-grams) following predictable patterns during training. The findings suggest that autoregressive language modeling task itself may be the primary factor shaping behavioral phases in language models.

## Method Summary
The study analyzes 1,314 checkpoints across multiple model architectures and scales, training models on OpenWebText and The Pile. For each checkpoint, the authors compute word-level log-probabilities on a decontaminated evaluation dataset (NaWoCo), then correlate these predictions with three heuristic metrics: unigram frequency, n-gram probability (using infini-gram with Stupid Backoff), and semantic similarity (via fastText embeddings). Multiple linear regression with z-scored predictors is used to quantify how much variance in LM behavior each heuristic explains at each training step. The analysis tracks how correlation coefficients and regression weights evolve throughout training across different model architectures and scales.

## Key Results
- All tested models (transformers, Mamba, RWKV) show consistent behavioral phases during pretraining, overfitting to n-grams of increasing order
- Up to 98% of variance in LM word-level behavior can be explained by three heuristics: unigram probability, n-gram probability, and semantic similarity
- Smaller models remain more dependent on lower-order n-grams while larger models shift toward higher-order n-grams
- Correlation between LM predictions and n-gram probabilities peaks at increasing values of n as training progresses
- The three heuristics together explain over half of the variance in LM behavior at all training stages except very early training

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Autoregressive language models progress through predictable behavioral phases, overfitting to n-grams of increasing order during pretraining
- **Mechanism**: Gradient descent on next-token prediction first captures the lowest-cost statistical regularities (unigram/frequency), then incrementally learns longer-context dependencies (bigram → trigram → higher n) as optimization continues
- **Core assumption**: The autoregressive loss surface creates a natural curriculum where simpler patterns have steeper gradients early in training
- **Evidence anchors**:
  - [abstract]: "all tested models—including transformers, Mamba, and RWKV architectures—overfit to n-grams of increasing n over the course of training, following predictable patterns"
  - [section 4.2]: "we see peaks in the correlation between transformer log-probability and n-gram log-probability for increasing values of n"
  - [corpus]: Related work on curriculum learning in LLMs shows data ordering affects learning dynamics, supporting the curriculum interpretation
- **Break condition**: If pretraining used aggressive regularization (e.g., dropout annealing, explicit curriculum), phase timing could shift or compress

### Mechanism 2
- **Claim**: Model capacity determines the maximum n-gram order effectively learned; smaller models remain more dependent on lower-order n-grams
- **Mechanism**: Larger parameter count provides representational capacity to store and retrieve longer-context patterns; smaller models face capacity constraints that limit their shift to higher-order n-grams
- **Core assumption**: The relationship between parameter count and learnable n-gram order is monotonic but not necessarily linear
- **Evidence anchors**:
  - [abstract]: "smaller models relying more on lower-order n-gram predictions while larger models shift toward higher-order n-grams"
  - [section 3.2]: "larger models also see a greater decrease in the correlation to smaller n-grams, suggesting that their probability distributions shift farther from lower-order n-grams"
  - [corpus]: Limited corpus evidence for this specific capacity–n-gram relationship in non-transformer architectures; mostly transformer-focused prior work
- **Break condition**: If smaller models were trained for significantly more steps/tokens, they might eventually approach larger-model n-gram distributions

### Mechanism 3
- **Claim**: Three heuristics—unigram probability, n-gram probability, and semantic similarity—jointly explain up to 98% of variance in LM word-level behavior at any training step
- **Mechanism**: The autoregressive objective implicitly trains models to approximate these statistics as sufficient predictors for next-word probability; they emerge as stable attractors in the learning dynamics
- **Core assumption**: These heuristics capture the most learnable regularities in natural text; remaining variance reflects higher-order patterns not captured by n≤5 or static embeddings
- **Evidence anchors**:
  - [abstract]: "up to 98% of variance in language model behavior at the word level can be explained by three simple heuristics"
  - [section 4.2]: R² values showing regression fit across models and training phases; "except for in the very early stages of training, there is a specific weighting of three simple heuristics... that can explain over half of the variance"
  - [corpus]: Psycholinguistic literature uses similar decompositions (frequency, predictability, semantic similarity) for human language processing, suggesting convergent validity
- **Break condition**: On out-of-distribution domains (code, formal logic, multilingual), these heuristics likely explain less variance

## Foundational Learning

- **Concept: N-gram Language Models**
  - **Why needed here**: The paper uses n-gram probabilities (n=1–5) as explanatory variables; understanding what n-grams capture (local co-occurrence statistics, Markov assumption) is essential to interpret the phase progression
  - **Quick check question**: Why does a 5-gram model require substantially more training data than a bigram model to avoid sparsity issues?

- **Concept: Multiple Linear Regression with Collinear Predictors**
  - **Why needed here**: The heuristics are correlated (e.g., unigram and Common-Crawl similarity: r=0.67–0.69); regression coefficients isolate each predictor's unique contribution
  - **Quick check question**: If unigram and semantic similarity are highly correlated, why does including both still provide non-redundant information about LM behavior?

- **Concept: Checkpoint Analysis and Training Dynamics**
  - **Why needed here**: The findings depend on analyzing models at multiple training steps; understanding that LMs undergo qualitative behavioral shifts—not just gradual improvement—is central
  - **Quick check question**: Why might a model's correlation with bigram probability peak and then decline, rather than monotonically increase?

## Architecture Onboarding

- **Component map**:
  - **Models**: 1,314 checkpoints across Pythia/Transformer, Mamba-1, RWKV-4; scales 14M–12B parameters; seeds 1–10 per size
  - **Data**: OpenWebText and The Pile; n-grams computed via infini-gram
  - **Evaluation**: NaWoCo dataset (158K+ words in context, decontaminated, toxicity-filtered)
  - **Analysis**: Pearson/Spearman correlation tracking + multiple regression (unigram, 5-gram, semantic similarity) at each checkpoint

- **Critical path**:
  1. Train models with checkpoint saves at log-spaced intervals (0, 1, 2, 4, 8, 16... 143K steps)
  2. Build infini-gram index on training corpus; compute n-gram log-probabilities for evaluation words
  3. Compute semantic similarity via fastText embeddings (Wikipedia and Common-Crawl variants)
  4. For each checkpoint, compute word-level log-probabilities on NaWoCo
  5. Run correlation analysis and z-scored regression at each timestep; track coefficient evolution

- **Design tradeoffs**:
  - **Word-level vs token-level n-grams**: Word-level chosen for cross-tokenizer comparability; may miss subword patterns
  - **Static vs contextual embeddings**: Static fastText chosen for interpretability and computational tractability
  - **Stupid Backoff vs smoothed n-grams**: Stupid Backoff chosen for tractability at corpus scale; may underestimate unigram probability

- **Failure signatures**:
  - Early-training instability: Confidence intervals widen in first 10–100 steps (initialization sensitivity)
  - Outlier checkpoints: Single anomalous checkpoint (step 256K, seed 'beren') inflated confidence intervals
  - Architecture timing differences: Phase transitions occur at different absolute step counts; tokens/step differs across corpora

- **First 3 experiments**:
  1. **Single-architecture replication**: Train Pythia-160M with 6 seeds, compute n-gram correlations at each checkpoint; verify unigram → bigram → trigram phase progression timing
  2. **Regression decomposition at three phases**: At early, mid, and late training, fit regression with unigram, 5-gram, and semantic similarity; confirm coefficient sign flips and stabilization
  3. **Cross-architecture behavioral alignment**: Compare Parc-Pythia, Parc-Mamba, Parc-RWKV log-probability correlations at matched steps (≥80); verify r≥0.93 convergence across architectures

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the trajectory of regression coefficients for simple heuristics (frequency, n-gram, semantic similarity) during pretraining predict the susceptibility of models to errors, such as predicting high-probability n-grams in inappropriate contexts?
- **Basis in paper**: [explicit] Page 9: "An interesting line of future work would be to investigate whether it is possible to predict the extent to which language models are susceptible to these kinds of phenomena based on analyses such as those carried out in Experiment 2."
- **Why unresolved**: The current study demonstrates that these heuristics explain variance but does not link the specific weighting of these heuristics over time to specific failure modes or adversarial benchmarks
- **What evidence would resolve it**: A correlation analysis between the heuristics' coefficient trajectories and performance on adversarial benchmarks measuring susceptibility to rote memorization or semantic leakage

### Open Question 2
- **Question**: Do non-transformer architectures like Mamba and RWKV retain lower-order n-gram circuits (specifically bigrams) throughout training in the same way transformers do, even as they learn higher-order patterns?
- **Basis in paper**: [explicit] Page 9: "Whether this is also true for the RWKV and Mamba models is a question for future work."
- **Why unresolved**: The Mamba and RWKV models tested were relatively small (130–169M parameters) and trained for fewer steps; evidence for persistent bigram circuits currently exists primarily for transformers
- **What evidence would resolve it**: Mechanistic interpretability studies on larger, fully-trained Mamba and RWKV checkpoints to identify and track the persistence of bigram-dedicated neurons or subnetworks

### Open Question 3
- **Question**: To what extent does the reliance on static word embeddings limit the explanatory power of the semantic similarity heuristic, and would contextual embeddings close the gap in unexplained variance?
- **Basis in paper**: [inferred] Page 10 (Limitations): The authors note they limited analysis to static word embeddings and that "regressions still do not account for all the variance"
- **Why unresolved**: The paper used fastText embeddings, which cannot capture context-dependent word meanings, potentially missing variance explained by dynamic semantic roles
- **What evidence would resolve it**: Re-running the regression analysis using contextual similarity metrics (e.g., cosine similarity of BERT embeddings in context) to measure the increase in explained variance ($R^2$)

## Limitations
- Analysis limited to autoregressive language modeling on English text corpora; generalizability to other tasks, languages, or non-autoregressive architectures untested
- Relies on static fastText embeddings for semantic similarity rather than contextual embeddings, potentially missing important semantic nuances
- Uses word-level n-grams rather than subword tokens, potentially missing patterns that emerge at the token level in modern LMs
- Correlation-based methodology cannot establish causal mechanisms for why these phases emerge

## Confidence
- **High Confidence**: The observation that autoregressive models exhibit consistent behavioral phases during training is well-supported by the cross-architecture and cross-scale analysis
- **Medium Confidence**: The claim that three simple heuristics explain up to 98% of variance is supported by regression analysis, but exact percentage may vary with different evaluation datasets
- **Low Confidence**: The assertion that these phases represent a universal curriculum inherent to the autoregressive loss surface requires further validation

## Next Checks
1. **Cross-task generalization test**: Apply the same correlation and regression analysis to models trained on non-text tasks (code generation, mathematical reasoning, or multimodal data) to determine whether similar behavioral phases emerge in different domains
2. **Contextual embedding validation**: Repeat the semantic similarity analysis using contextual embeddings (e.g., from the model being analyzed at each checkpoint) rather than static fastText embeddings to assess whether this changes the variance explained or phase progression patterns
3. **Intervention study**: Design experiments that deliberately modify training dynamics (e.g., curriculum learning, explicit n-gram regularization, or architectural constraints) to test whether the observed phases can be accelerated, delayed, or eliminated, thereby testing the causal relationship between the autoregressive objective and phase emergence