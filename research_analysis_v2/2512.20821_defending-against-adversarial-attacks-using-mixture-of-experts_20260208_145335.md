---
ver: rpa2
title: Defending against adversarial attacks using mixture of experts
arxiv_id: '2512.20821'
source_url: https://arxiv.org/abs/2512.20821
tags:
- adversarial
- training
- learning
- attacks
- experts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces Divided We Fall (DWF), a defense system against\
  \ adversarial attacks using Mixture of Experts (MoE) architecture. The approach\
  \ involves pretraining nine ResNet-18 experts\u2014one benign, four trained on FGSM\
  \ attacks, and four on PGD attacks\u2014and then performing joint end-to-end training\
  \ with a soft MoE structure."
---

# Defending against adversarial attacks using mixture of experts

## Quick Facts
- arXiv ID: 2512.20821
- Source URL: https://arxiv.org/abs/2512.20821
- Authors: Mohammad Meymani; Roozbeh Razavi-Far
- Reference count: 40
- Key outcome: DWF achieves 91.08% clean accuracy and outperforms ADVMoE and SoE in FGSM/PGD robustness using 9 ResNet-18 experts

## Executive Summary
The paper introduces Divided We Fall (DWF), a defense system against adversarial attacks using Mixture of Experts (MoE) architecture. The approach involves pretraining nine ResNet-18 experts—one benign, four trained on FGSM attacks, and four on PGD attacks—and then performing joint end-to-end training with a soft MoE structure. During training, both benign and adversarial samples are combined in each batch, and experts are not frozen, allowing full optimization. The defense system achieves a clean accuracy of 91.08% and significantly outperforms state-of-the-art defenses like ADVMoE and SoE in robustness against FGSM and PGD attacks. It maintains strong performance across varying perturbation sizes and iteration counts, demonstrating adaptability and generalization while preserving accuracy in non-adversarial settings.

## Method Summary
The method uses a Mixture of Experts architecture with nine ResNet-18 backbones: one trained on clean data, four on FGSM attacks with varying ε, and four on PGD attacks with varying iterations. Each expert is pre-trained independently for 150 epochs, then combined into an MoE with soft gating. The joint training phase mixes benign and adversarial samples in each batch, with randomized attack parameters, and optimizes all parameters end-to-end for 100 epochs. The soft gating mechanism routes inputs to experts based on learned weights, producing final predictions as weighted combinations of expert outputs. This approach leverages specialized experts while maintaining a unified decision boundary through joint optimization.

## Key Results
- Clean accuracy: 91.08% on CIFAR-10, demonstrating the accuracy-robustness tradeoff is mitigated
- FGSM robust accuracy: 65.48% at ε=0.09, significantly outperforming SoE (58.61%) and ADVMoE (61.44%)
- PGD robust accuracy: 56.33% at ε=8/255, surpassing SoE (51.51%) and ADVMoE (55.13%)
- The system shows consistent performance across different perturbation strengths and iteration counts, with best results at moderate attack parameters

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Diversifying expert specialization across attack types improves robustness coverage without requiring a single model to generalize to all perturbation strengths.
- **Mechanism:** Nine independently pre-trained experts (1 benign, 4 FGSM, 4 PGD) expose the gating network to heterogeneous decision boundaries. During joint training, the soft gating mechanism learns to route inputs to the most suitable expert based on input characteristics, distributing adversarial detection across specialized models rather than forcing one model to defend against all attack patterns.
- **Core assumption:** The gating mechanism can learn discriminative routing patterns that correlate with attack type or perturbation strength.
- **Evidence anchors:**
  - [abstract]: "pretraining nine ResNet-18 experts—one benign, four trained on FGSM attacks, and four on PGD attacks"
  - [section 4]: "Now we have a benign expert, which is trained on benign data, four FGSM experts, and four PGD experts"
  - [corpus]: Weak direct evidence; related MoE defense work (Graph MoE) targets different domains but supports the general principle of expert diversification
- **Break condition:** If attack types at inference differ fundamentally from FGSM/PGD (e.g., C&W, AutoAttack), routing may fail to select appropriate experts.

### Mechanism 2
- **Claim:** Joint end-to-end optimization with unfrozen experts enables post-pretraining specialization that frozen-expert approaches cannot achieve.
- **Mechanism:** By allowing gradients to flow through all expert parameters during MoE training, experts refine their decision boundaries in response to the gating network's routing behavior. This creates a feedback loop where the gating learns better routing and experts learn better specialization simultaneously, rather than being constrained to fixed features from pretraining.
- **Core assumption:** Pretrained experts retain sufficient plasticity to benefit from further optimization without catastrophic forgetting of their initial specialization.
- **Evidence anchors:**
  - [abstract]: "parameters of expert models and gating mechanism are jointly updated allowing further optimization"
  - [section 3, Table 1]: DWF differs from SoE and ADVMoE specifically in using "Pre-trained+Joint" vs. their "Joint" strategies
  - [corpus]: No direct corpus comparison on frozen vs. unfrozen MoE training for adversarial defense
- **Break condition:** If training data is insufficient or learning rate too high during joint training, experts may lose their initial specialization (catastrophic forgetting).

### Mechanism 3
- **Claim:** Randomizing attack parameters during training improves generalization to unseen perturbation strengths at inference.
- **Mechanism:** By sampling ε from {0.01, 0.02, ..., 0.09} for FGSM and iterations from {10, 20, 30, 40, 50} for PGD during each batch, the model encounters a distribution of attack strengths rather than a fixed configuration. This prevents overfitting to specific perturbation magnitudes and encourages learning of transferable robust features.
- **Core assumption:** The distribution of attack parameters at training covers the range encountered at deployment.
- **Evidence anchors:**
  - [section 4, Algorithm 1]: "ϵ ← Random{0.01, 0.02, . . . , 0.09}"
  - [section 4, Algorithm 2]: "iterations ← Random{10, 20, 30, 40, 50}"
  - [corpus]: Adversarial training papers (e.g., TRADES, FFAT) support varied perturbation strategies, but specific randomization within MoE is not directly addressed
- **Break condition:** If inference-time attacks use ε > 0.09 or significantly different iteration counts, robustness may degrade (as shown in Table 2 results at higher perturbations).

## Foundational Learning

- **Concept: Soft vs. Hard Mixture of Experts Gating**
  - **Why needed here:** DWF uses soft gating (all experts contribute fractional weights) while competitors (SoE, ADVMoE) use hard gating (sparse or single expert activation). Understanding this distinction is essential for implementing the routing mechanism and interpreting the weighted output equation.
  - **Quick check question:** Can you explain why soft gating might preserve more information than selecting a single expert per input?

- **Concept: Adversarial Perturbation Generation (FGSM/PGD)**
  - **Why needed here:** The entire DWF architecture depends on generating adversarial samples during training. You must understand how ε controls perturbation magnitude in FGSM and how iterative refinement in PGD creates stronger attacks.
  - **Quick check question:** Given a loss gradient ∇xℓ(θ, x, y), write the FGSM update and explain why PGD iterates this process.

- **Concept: Robustness-Accuracy Tradeoff in Adversarial Training**
  - **Why needed here:** The paper claims DWF maintains high clean accuracy (91.08%) while improving robustness—this is non-trivial because adversarial training typically degrades clean performance. Understanding this tradeoff helps evaluate whether DWF's approach is practically viable.
  - **Quick check question:** Why does training on adversarial samples often reduce accuracy on clean data, and what mechanism might DWF use to mitigate this?

## Architecture Onboarding

- **Component map:** CIFAR-10 data → normalization → adversarial generators (FGSM/PGD) → 9 ResNet-18 experts → soft gating network → weighted sum output
- **Critical path:** 1) Pre-train 9 experts independently (150 epochs each, varying attack configurations) → 2) Assemble MoE with soft gating → 3) Joint end-to-end training with mixed batches (benign + FGSM + PGD) → 4) Deploy with frozen weights for inference
- **Design tradeoffs:**
  - **Soft vs. hard gating:** Soft preserves more information but increases inference compute (all 9 experts forward pass); hard reduces compute but may lose robustness
  - **Unfrozen vs. frozen experts:** Unfrozen enables better joint optimization but risks overfitting and longer training; frozen is faster but limits specialization
  - **More experts vs. larger backbone:** DWF uses 9× ResNet-18 vs. competitors' single larger models (VGG-19, DenseNet-169)—parameter count similar, but ensemble diversity may offer different robustness properties
- **Failure signatures:**
  - **Catastrophic forgetting:** Clean accuracy drops significantly during joint training → reduce learning rate or use regularization
  - **Gating collapse:** Gating converges to always selecting one expert → add entropy regularization to gating weights
  - **Robustness degradation at high ε:** Performance collapses at ε > 0.07 → expand randomization range during training
  - **Overfitting to training attacks:** Poor performance on new attack types → increase attack diversity (add C&W, AutoAttack during training)
- **First 3 experiments:**
  1. **Reproduce baseline expert training:** Train a single ResNet-18 on CIFAR-10 with FGSM (ε=0.03) to verify adversarial training pipeline; compare clean and robust accuracy to Table 2 baselines
  2. **Ablate joint vs. frozen training:** Run DWF with frozen expert weights and compare robust accuracy to full joint training; expect 2-5% degradation based on the paper's claims
  3. **Test out-of-distribution attack strengths:** Evaluate trained DWF on FGSM with ε ∈ {0.10, 0.15, 0.20} to probe generalization limits beyond training distribution

## Open Questions the Paper Calls Out

- **Question:** How does the DWF defense system perform against a wider range of adversarial attacks beyond FGSM and PGD, such as C&W, AutoAttack, or square attacks?
- **Basis in paper:** [explicit] "We also trained and tested our model against two popular attacks, FGSM and PGD. To further expand our method one can train and evaluate the model on a wider range of adversarial attacks."
- **Why unresolved:** The evaluation only covers two specific attack methods, leaving the defense's generalization to diverse attack strategies untested.
- **What evidence would resolve it:** Evaluate DWF against additional attack types (C&W, AutoAttack, DeepFool) on CIFAR-10 and compare robust accuracy against current baselines.

- **Question:** Can efficient adversarial training methods like FFAT be integrated with DWF without significantly compromising robustness?
- **Basis in paper:** [explicit] "To reduce training time, efficient adversarial training method such as FFAT can be used. However, efficient adversarial training reduces the robustness of the model."
- **Why unresolved:** DWF requires substantial computational resources (pretraining 9 experts plus joint end-to-end training), but the trade-off between training efficiency and robustness preservation remains unexplored.
- **What evidence would resolve it:** Implement FFAT-based expert pretraining within DWF and report training time reduction alongside robust accuracy metrics compared to the standard DWF.

- **Question:** How does DWF generalize to datasets beyond CIFAR-10, particularly higher-resolution datasets such as ImageNet or domain-specific datasets like medical imaging?
- **Basis in paper:** [inferred] The paper only evaluates on CIFAR-10 (32×32 RGB images), despite the introduction mentioning applications in healthcare, autonomous vehicles, and other domains.
- **Why unresolved:** Small-scale image classification may not represent the challenges of higher-resolution or domain-specific data where adversarial perturbations may have different properties.
- **What evidence would resolve it:** Replicate DWF training and evaluation on ImageNet or a medical imaging dataset (e.g., CIFAR-100, ChestX-ray) and report standard accuracy and robust accuracy metrics.

- **Question:** What is the optimal number and composition of experts (benign vs. adversarial, FGSM vs. PGD specialists) for balancing clean accuracy and robustness?
- **Basis in paper:** [inferred] The paper uses 9 experts (1 benign, 4 FGSM, 4 PGD) without ablation studies on expert configuration or sensitivity analysis on this design choice.
- **Why unresolved:** It remains unclear whether the selected expert ratio is optimal or whether alternative configurations could improve the accuracy-robustness trade-off.
- **What evidence would resolve it:** Conduct systematic ablation experiments varying the number and type of experts (e.g., 1-1-1, 3-3-3, 2-5-5) and report resulting standard accuracy and robust accuracy on FGSM/PGD.

## Limitations

- Only evaluates against FGSM and PGD attacks, leaving defense gaps against more sophisticated attacks (C&W, AutoAttack) untested
- Computational overhead not analyzed despite soft gating requiring 9× forward passes at inference
- Lacks ablation studies on optimal expert configuration and number, making the design choices unclear
- No comparison to non-MoE state-of-the-art defenses like TRADES or FFAT in the evaluation

## Confidence

- **High confidence**: The paper clearly specifies data preprocessing, optimizer settings, attack generation procedures, and provides sufficient implementation details for reproducing the core training pipeline
- **Medium confidence**: The claim that joint optimization with unfrozen experts improves robustness is plausible but lacks direct ablation experiments comparing to frozen-expert variants
- **Low confidence**: The assertion that soft gating is essential for the approach's success—no comparison to hard gating or analysis of gating behavior is provided

## Next Checks

1. Implement the exact data augmentation pipeline (random crops, flips) and verify clean accuracy matches the claimed 91.08% before evaluating robustness
2. Run an ablation study freezing expert weights during joint training and measure the degradation in robust accuracy to quantify the contribution of unfrozen optimization
3. Test the trained DWF against C&W and AutoAttack to establish whether the defense generalizes beyond the FGSM/PGD training distribution