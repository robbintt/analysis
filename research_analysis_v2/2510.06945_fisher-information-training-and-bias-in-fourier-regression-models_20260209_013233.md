---
ver: rpa2
title: Fisher Information, Training and Bias in Fourier Regression Models
arxiv_id: '2510.06945'
source_url: https://arxiv.org/abs/2510.06945
tags:
- training
- quantum
- random
- space
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the relationship between a model's effective
  dimension (ED), derived from the Fisher information matrix (FIM), and its training
  performance in regression tasks. The authors exploit the equivalence between quantum
  neural networks (QNNs) and Fourier models to analytically derive the FIM for the
  latter.
---

# Fisher Information, Training and Bias in Fourier Regression Models

## Quick Facts
- arXiv ID: 2510.06945
- Source URL: https://arxiv.org/abs/2510.06945
- Reference count: 40
- Primary result: Effective Dimension impacts training speed conditionally on model biasâ€”low ED benefits biased models while high ED benefits unbiased models.

## Executive Summary
This paper investigates how a model's effective dimension (ED), derived from the Fisher information matrix (FIM), relates to its training performance in regression tasks. By exploiting the equivalence between quantum neural networks (QNNs) and Fourier models, the authors analytically derive the FIM for Fourier models and identify the correlation spectrum of structure constants as the key factor controlling ED. They demonstrate that the impact of ED on training depends critically on the model's bias toward the data-generating function, challenging the notion that high ED always guarantees faster training.

## Method Summary
The authors construct Fourier regression models where the output is a linear combination of basis functions with trainable parameters. They analytically derive the Fisher Information Matrix (FIM) and Effective Dimension (ED) for these models. To study the ED-bias relationship, they create synthetic data-generating functions and construct both biased (containing the target function) and unbiased (random) models. The ED is controlled by manipulating the singular value spectrum of the model's structure constants matrix. Training is performed using the Adam optimizer, and performance is measured by comparing minimum MSE between high-ED ("Full") and low-ED ("Cutoff") model variants.

## Key Results
- The correlation spectrum (singular values) of structure constants governs Effective Dimension, not parameter count.
- For biased models (target in model space), lower ED leads to better training performance by constraining the search space.
- For unbiased models (target outside model space), higher ED is beneficial as it provides more independent directions for approximation.
- Tensor network representations enable scaling Fourier models to high dimensions while maintaining control over ED.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The Effective Dimension (ED) of a regression model is governed by the decay profile of the "correlation spectrum" (singular values of the structure constants), not the total parameter count.
- **Mechanism:** The paper posits that a model's output can be decomposed into structure constants $\Gamma$. The singular values of $\Gamma$ form a correlation spectrum. If this spectrum decays rapidly (high "purity"), the Fisher Information Matrix (FIM) becomes ill-conditioned or low-rank, effectively restricting the model to a smaller subspace of functions.
- **Core assumption:** The model can be expressed as a linear combination of orthogonal basis functions separable into input and parameter dependencies (Fourier-like).
- **Evidence anchors:**
  - [abstract] "identify the correlation spectrum of the model's structure constants as the key factor controlling the ED."
  - [section 2.2.2] "ED is sensitive only to the mutual relationship of the values of $s^2_\rho$, i.e., the decay properties..."
  - [corpus] Corpus evidence on "spectral bias" in neural networks (e.g., *Iterative Training of Physics-Informed Neural Networks*) supports the general principle that spectral properties dictate learning dynamics, though the specific $\Gamma$-mechanism is unique to this paper.
- **Break condition:** If the model's basis functions are not orthonormal or the parameter space is not effectively bounded, the analytic link between the spectrum and the FIM may degrade.

### Mechanism 2
- **Claim:** The impact of ED on training speed is conditional on the model's "bias" (alignment) with the target function.
- **Mechanism:**
  - **Biased Case (Target $\in$ Model Space):** Low ED constrains the search space around the target. This reduces the number of "distractor" dimensions the optimizer must navigate, leading to faster convergence.
  - **Unbiased Case (Target $\notin$ Model Space):** The model must approximate an out-of-distribution target. High ED maximizes the number of independent directions available, increasing the probability of finding a useful approximation.
- **Core assumption:** Gradient descent behaves like a random search constrained by the local geometry defined by the FIM.
- **Evidence anchors:**
  - [abstract] "For biased models, lower ED leads to better training... Conversely, for unbiased models, higher ED is beneficial."
  - [section 3.1] Shows $\Delta MSE > 0$ (Low ED better) for biased models and $\Delta MSE < 0$ (High ED better) for unbiased models.
  - [corpus] General literature on "inductive bias" aligns with this, but the specific trade-off against ED is the paper's contribution.
- **Break condition:** If the optimizer uses second-order methods (like Natural Gradient) that normalize the FIM geometry, the constraint of low ED might become a limitation rather than a benefit.

### Mechanism 3
- **Claim:** Tensorization (Tensor Networks) allows scaling Fourier models to high dimensions without losing control over the ED.
- **Mechanism:** Instead of storing the full structure constant matrix $\Gamma$ (exponential cost), the authors decompose it into a Tensor Network (TTN/MPS). This allows numerical simulation of larger QNN-equivalent models while manually tuning the singular values to control the ED.
- **Core assumption:** The bond dimension $\chi$ is sufficient to capture the essential correlations of the model without truncation errors dominating the training dynamics.
- **Evidence anchors:**
  - [abstract] "We furthermore introduce a tensor network representation... which could be a tool of independent interest."
  - [section 2.4] "decompose the structure constants $\Gamma$ as a tensor network... with a finite bond dimension $\chi$."
  - [corpus] Weak/None in provided neighbors regarding tensor networks for Fourier regression specifically.
- **Break condition:** If the required correlation rank exceeds the chosen bond dimension $\chi$, the model's effective capacity will be artificially capped, confounding the results.

## Foundational Learning

- **Concept: Fisher Information Matrix (FIM)**
  - **Why needed here:** The FIM defines the local "curvature" or sensitivity of the model output with respect to parameter changes. It is the mathematical substrate from which Effective Dimension is calculated.
  - **Quick check question:** Does the FIM measure the quality of the model's predictions or the geometry of the parameter space?

- **Concept: Effective Dimension (ED)**
  - **Why needed here:** The paper centralizes ED as a metric for "trainability" or "capacity." Understanding that ED normalizes the FIM to count "active" parameters is crucial for interpreting the results.
  - **Quick check question:** If a model has 100 parameters but an ED of 10, what does that imply about the rank of its FIM?

- **Concept: Structure Constants ($\Gamma$) & Correlation Spectrum**
  - **Why needed here:** The authors move beyond standard weights/biases to define the model architecture via these constants. The singular values of $\Gamma$ are the control knobs for the experiments.
  - **Quick check question:** In the context of this paper, does a "flat" correlation spectrum correspond to high or low Effective Dimension?

## Architecture Onboarding

- **Component map:** Input vector $x$ mapped to Fourier basis functions $e_\mu(x)$ -> Trainable parameters $\theta$ mapped to basis functions $\iota_\nu(\theta)$ -> Structure Constants Matrix $\Gamma$ (Tensorized or Full) -> Output $f_\theta(x)$

- **Critical path:**
  1. Define the data-generating function $y(x)$.
  2. Construct $\Gamma$ such that the model is "Biased" (contains $y(x)$) or "Unbiased" (random).
  3. Modify the singular values of $\Gamma$ to create "Full" (High ED) and "Cutoff" (Low ED) variants.
  4. Train using gradient descent (Adam) and compare Minimum MSE ($\Delta MSE$).

- **Design tradeoffs:**
  - **High ED (Flat Spectrum):** maximizes expressivity and is robust for unknown/unbiased tasks, but introduces "distractor" dimensions that can slow convergence for known tasks.
  - **Low ED (Decaying Spectrum):** acts as a strong regularizer for aligned tasks, but fails to approximate functions outside its narrow subspace.

- **Failure signatures:**
  - **Stagnation in Unbiased Models with Low ED:** The model cannot approximate the target, resulting in a high loss plateau.
  - **Local Minima in Biased Models with High ED:** The optimizer struggles to converge to the perfect solution despite it being in the function space, due to noise from redundant dimensions.

- **First 3 experiments:**
  1. **Bias vs. ED Validation:** Construct a biased target function. Train two models: one with a flat spectrum (High ED) and one with an exponential decay spectrum (Low ED). Verify that Low ED converges faster.
  2. **Unbiased Stress Test:** Generate a random target function outside the model's basis. Retrain the models from step 1. Verify that High ED now achieves lower loss.
  3. **Tensorization Scaling:** Implement the $\Gamma$ decomposition using Tensor Networks (e.g., Tensor-Train). Confirm that increasing the number of features $N$ does not alter the ED-Bias relationship observed in small-scale experiments.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the interplay between effective dimension (ED) and model bias extend to classification or generative models?
- Basis in paper: [explicit] The conclusion identifies "analysis for classification... or generative models" as a natural extension.
- Why unresolved: The study is restricted to regression models using Mean Squared Error, and the analytical derivation relies on properties of this specific setting.
- What evidence would resolve it: Theoretical extension of FIM bounds to cross-entropy loss and numerical experiments comparing training dynamics of high/low ED classifiers.

### Open Question 2
- Question: How does natural gradient descent affect the identified relationship between effective dimension, bias, and training performance?
- Basis in paper: [explicit] The authors suggest it would be interesting to investigate "in the context of natural gradient descent."
- Why unresolved: The current analysis relies on standard gradient descent (Adam), whereas natural gradient methods explicitly utilize the Fisher Information Matrix (which determines ED).
- What evidence would resolve it: Comparative training experiments using natural gradient descent on the constructed biased/unbiased Fourier models to verify if the trade-offs persist.

### Open Question 3
- Question: Can the interplay between ED and bias predict performance when comparing inherently different architectures, such as classical versus quantum neural networks?
- Basis in paper: [explicit] The authors propose investigating the mechanism when "comparing inherently different models... comparing quantum and classical NNs."
- Why unresolved: The paper controls for architecture by comparing models within the same function class (same basis functions) to isolate ED and bias effects.
- What evidence would resolve it: Benchmarking distinct classical and quantum architectures where ED and bias are quantified and correlated with training success on shared tasks.

### Open Question 4
- Question: What specific architectural features of a Quantum Neural Network (QNN) allow it to be efficiently dequantized via the introduced tensorized model?
- Basis in paper: [explicit] The authors ask what features of a QNN "make it dequantizable via the tensorized model introduced here."
- Why unresolved: The tensor network representation is introduced as a tool to handle larger instances, but the structural limits of this approximation (e.g., required bond dimension) are not characterized.
- What evidence would resolve it: Theoretical mapping of specific QNN circuit structures (e.g., entangling layers) to the bond dimension $\chi$ required for an accurate tensor representation.

## Limitations
- The framework is limited to synthetic regression tasks and may not generalize to real-world data or classification problems.
- Results depend on specific assumptions about parameter space and basis function orthogonality that may not hold in all model classes.
- The tensor network extension relies on assumptions about sufficient bond dimension without characterization of approximation limits.

## Confidence
- Core mechanism (bias-conditional ED effects): **Medium** - clear numerical demonstrations in controlled settings, but limited real-world validation
- Tensor network extension: **Low** - promising but relies on untested assumptions about bond dimension sufficiency
- Generalization to other architectures: **Low** - framework is specific to Fourier models with structure constants

## Next Checks
1. **Scaling Test**: Verify the bias-ED relationship holds for higher-dimensional inputs ($N>1$) using the tensor network implementation.
2. **Optimizer Sensitivity**: Repeat key experiments with different optimizers (e.g., SGD, L-BFGS) to check if the observed ED effects are optimizer-dependent.
3. **Real Data**: Test the framework on a non-synthetic regression dataset to assess robustness outside the controlled synthetic setting.