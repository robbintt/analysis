---
ver: rpa2
title: 'Continual Deep Active Learning for Medical Imaging: Replay-Base Architecture
  for Context Adaptation'
arxiv_id: '2501.08245'
source_url: https://arxiv.org/abs/2501.08245
tags:
- learning
- rbaca
- memory
- data
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RBACA, a Continual Active Learning (CAL)
  framework designed for medical image analysis. RBACA integrates continual learning
  with active learning to address challenges in adapting deep learning models to new
  contexts and reducing annotation effort.
---

# Continual Deep Active Learning for Medical Imaging: Replay-Base Architecture for Context Adaptation

## Quick Facts
- **arXiv ID:** 2501.08245
- **Source URL:** https://arxiv.org/abs/2501.08245
- **Reference count:** 40
- **Primary result:** RBACA framework integrates continual and active learning for medical imaging, outperforming baselines with IL-Scores of 0.445 (segmentation) and 0.111 (classification).

## Executive Summary
This paper introduces RBACA, a Continual Active Learning framework designed for medical image analysis that integrates continual learning with active learning to address challenges in adapting deep learning models to new contexts while reducing annotation effort. The framework uses a replay-based memory system to store and reuse past data, combined with active learning to select the most informative samples for annotation. RBACA supports both domain-incremental and class-incremental learning scenarios and is evaluated on cardiac image segmentation and diagnosis tasks using the M&Ms dataset. Results show RBACA outperforms baseline methods and a state-of-the-art CAL approach (CASA) across various memory sizes and annotation budgets, achieving higher IL-Scores while demonstrating improved adaptability and generalizability to diverse medical imaging contexts.

## Method Summary
RBACA employs a replay-based continual learning approach with active learning components. The framework uses a pseudo-context (PC) module to detect distribution shifts and route samples accordingly. A memory buffer stores representative samples from each context, which are replayed during training to prevent catastrophic forgetting. Active learning selects informative samples for annotation based on uncertainty thresholds. The method supports both Domain-IL (fixed output layer) and Class-IL (dynamic output layer expansion) scenarios. Training is triggered periodically based on update counts, using a combination of new data and replayed samples from memory. The system is evaluated on cardiac imaging tasks using the M&Ms dataset with specific memory and budget configurations.

## Key Results
- RBACA achieves IL-Scores of 0.445 in segmentation (vs 0.422 for CASA) and 0.111 in classification (vs 0.103 for CASA)
- The framework demonstrates superior performance across various memory sizes and annotation budgets
- RBACA shows improved adaptability and generalizability to diverse medical imaging contexts compared to baseline methods
- IL-Score metric simultaneously evaluates transfer learning, forgetting, and final model performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Replaying stored samples from previous contexts during training mitigates catastrophic forgetting while enabling incremental learning from new data streams.
- **Mechanism:** Training Memory M stores labeled samples organized by Pseudo-Contexts (PCs). When new data arrives, training combines incoming samples with replayed samples from M. Pruning strategies retain representative samples per context while respecting memory budget KM. Dynamic memory management allocates k slots per new PC until saturation, whereas Static MM forces immediate rebalancing across all PCs.
- **Core assumption:** The pruned subset adequately represents each context's data distribution; overfitting to the replay buffer remains bounded despite repeated exposure.
- **Evidence anchors:** RBACA employs a CL rehearsal method to continually learn from diverse contexts; M is balanced among existing Pseudo-Contexts; EWC-Guided Diffusion Replay demonstrates exemplar-free alternatives using generative replay.

### Mechanism 2
- **Claim:** Uncertainty-based active learning with thresholding concentrates annotation effort on informative samples, improving sample efficiency under budget constraints.
- **Mechanism:** The PC module routes incoming samples. For samples matching known PCs, the task network computes prediction uncertainty via inference. Samples exceeding uncertainty threshold Uth are sent to the oracle for labeling; others are discarded. Uth=0.025 optimally depletes budget late in the stream, enabling continuous annotation and more frequent retraining.
- **Core assumption:** Model uncertainty correlates with actual information gain for the task; the threshold can be calibrated to match budget to stream length.
- **Evidence anchors:** RBACA enables an uncertainty sampling strategy based on informativeness using a decision threshold; uncertainty of each sample is calculated by inferring the unlabeled data pool through the task network; WaveFuse-AL notes individual acquisition strategies exhibit inconsistent behavior across AL cycle stages.

### Mechanism 3
- **Claim:** Automatic pseudo-context detection enables context-aware memory partitioning and, for Class-IL, dynamic output layer expansion to accommodate emerging classes.
- **Mechanism:** A style network (ResNet-50 pretrained on ImageNet) extracts features. The PC module identifies distribution shifts—e.g., new scanner vendor or pathology. For Class-IL, the task network's final layer starts with 0 units; when new classes are detected, units are added while preserving existing weights. Memory is rebalanced across detected PCs.
- **Core assumption:** Style network features reliably distinguish meaningful contexts; detected PCs align with actual distribution shifts affecting task performance.
- **Evidence anchors:** Based on the automatic recognition of shifts in image characteristics; the final layer initially starts with 0 units and adds units when new classes are detected; Causal Attribution paper states causal mechanisms behind distribution shift performance drops remain poorly understood.

## Foundational Learning

- **Catastrophic Forgetting**
  - Why needed here: CL rehearsal directly counters forgetting; understanding why sequential fine-tuning degrades past-task performance is essential.
  - Quick check question: Why does fine-tuning a model on Center 2 data after training on Center 1 reduce performance on Center 1?

- **Uncertainty Sampling (Entropy)**
  - Why needed here: Core to AL component; selecting high-uncertainty samples for annotation.
  - Quick check question: For a 4-class segmentation output with softmax probabilities [0.7, 0.15, 0.1, 0.05], how would you quantify uncertainty?

- **Domain-IL vs. Class-IL Scenarios**
  - Why needed here: Paper evaluates both; architecture adapts differently (fixed vs. expanding output layer).
  - Quick check question: A new hospital uses a different MRI scanner but diagnoses the same pathologies—is this Domain-IL or Class-IL?

## Architecture Onboarding

- **Component map:**
  - Style Network (ResNet-50) -> PC Module -> Oracle/Outlier Memory O -> Training Memory M -> Task Network (UNet/ResNet-50/ViT)

- **Critical path:**
  1. Base training on initial labeled set (10 epochs)
  2. Stream processing: PC module evaluates each sample
  3. New PC → annotate k samples → add to M → prune to rebalance
  4. Known PC → compute uncertainty → query oracle if > Uth
  5. Training triggered when updates-without-training > 9 → 3 epochs on M + new data
  6. Evaluate via DSC/F1, BWT, FWT → compute IL-Score

- **Design tradeoffs:**
  - **Static vs. Dynamic MM:** Static forces immediate rebalancing (simpler); Dynamic allocates k per PC until saturation (better for late contexts but harder to tune)
  - **Pruning strategy:** LRU is computationally cheap but may drop representative samples; K-Means/GMM preserve distributional coverage at higher cost
  - **Uth calibration:** Lower threshold uses more budget earlier; higher threshold reserves budget for late contexts but may miss informative early samples

- **Failure signatures:**
  - IL-Score plateau despite more data: Check if memory per PC (k) is too small; verify pruning isn't collapsing diversity
  - Strong negative BWT: Forgetting dominant—memory doesn't represent past contexts adequately
  - Budget exhausted at ~50% stream: Uth too low; recalibrate upward
  - Low FWT: Transfer learning failing—verify base training convergence and task network architecture

- **First 3 experiments:**
  1. **Reproduce segmentation baseline:** β=430, KM=128, Static MM, LRU pruning, Perf AL, Uth=0.025—target IL-Score ~0.434 (Table I)
  2. **Ablate pruning method:** Fix β=430, KM=485; compare LRU vs K-Means vs DBSCAN—expect clustering-based methods to yield higher IL-Score via better distribution coverage
  3. **Stress-test memory constraints:** β=430, vary KM={200, 485, 2000} with Static MM—document IL-Score scaling and identify minimum viable k per PC

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can combining techniques from different Continual Learning (CL) families (e.g., rehearsal and regularization) within RBACA mitigate their individual drawbacks and improve performance?
- Basis in paper: The conclusion states, "A promising research direction is to combine techniques from different CL families to mitigate their respective drawbacks and enhance their capabilities."
- Why unresolved: RBACA currently relies solely on a memory-based rehearsal method; the synergistic effects or conflicts of integrating regularization-based or parameter isolation methods have not been tested.
- What evidence would resolve it: A comparative study evaluating RBACA's performance when augmented with regularization terms (e.g., Elastic Weight Consolidation) against the baseline rehearsal-only approach.

### Open Question 2
- Question: Can privacy-preserving representations, such as latent vectors, replace raw image storage in RBACA's memory without compromising model adaptability?
- Basis in paper: The authors note, "An interesting research direction is to explore solutions that incorporate privacy-preserving representations," suggesting latent storage as a feasible approach.
- Why unresolved: Medical data legislation often restricts long-term storage of raw patient data, which the current replay-based architecture requires.
- What evidence would resolve it: Implementation of a latent storage mechanism (pseudo-rehearsal) within RBACA, showing comparable IL-Scores to the raw-data version while adhering to privacy constraints.

### Open Question 3
- Question: Does the integration of pseudo-labeling for high-confidence samples effectively address the dependency on large, fully labeled base training datasets?
- Basis in paper: The discussion suggests, "A possible solution is to assign pseudo-labels to high-confidence samples, thereby expanding the base training set."
- Why unresolved: Deep Learning models, and by extension CAL methods, typically require substantial initial labeled data; the utility of self-supervised expansion in this specific CAL loop is hypothetical.
- What evidence would resolve it: Experiments measuring the IL-Score of RBACA when initialized with a small labeled set augmented by pseudo-labeled data versus a fully supervised initialization.

### Open Question 4
- Question: How can RBACA be effectively adapted for Task-Incremental Learning (Task-IL) scenarios using multiple task-specific networks?
- Basis in paper: The authors propose, "A potential approach for adapting rehearsal techniques to Task-IL is to utilize multiple task-specific networks while maintaining a single rehearsal memory shared across tasks."
- Why unresolved: The framework was validated only on Domain-IL and Class-IL scenarios; its applicability to Task-IL, where task identity is required, remains unexplored.
- What evidence would resolve it: Architectural modifications allowing shared memory across multi-head outputs, validated on a dataset with distinct, sequential tasks.

## Limitations

- Pseudo-Context detection thresholds and stability are not fully specified, creating potential variability in context partitioning
- Memory management dynamics (Static vs Dynamic MM) require careful tuning; no sensitivity analysis provided for k or pruning hyperparameters
- Annotation budget calibration (Uth=0.025) appears dataset-specific and may not transfer across domains
- Performance claims rely on M&Ms dataset characteristics that may not generalize to other imaging modalities

## Confidence

- **High confidence:** Replay-based forgetting mitigation mechanism (well-established CL literature)
- **Medium confidence:** PC module's ability to detect meaningful distribution shifts (depends on ResNet-50 feature quality)
- **Medium confidence:** Uncertainty sampling efficiency (threshold sensitivity not explored)
- **Low confidence:** IL-Score as a comprehensive metric without broader benchmarking context

## Next Checks

1. Implement ablation study varying memory per PC (k) to identify minimum viable capacity before performance degradation
2. Test Uth threshold sensitivity across multiple budget allocations to find optimal calibration strategy
3. Evaluate PC module on unseen domain shifts (different vendors/protocols) to assess generalization limits