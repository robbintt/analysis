---
ver: rpa2
title: Determinants of Training Corpus Size for Clinical Text Classification
arxiv_id: '2601.15846'
source_url: https://arxiv.org/abs/2601.15846
tags:
- data
- classification
- sample
- clinical
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of determining optimal training
  corpus sizes for clinical text classification using natural language processing.
  The research employed pre-trained BERT embeddings with Random Forest classifiers
  on MIMIC-III discharge notes, varying training data from 100 to 10,000 documents
  across 10 ICD-9 diagnosis classification tasks.
---

# Determinants of Training Corpus Size for Clinical Text Classification

## Quick Facts
- arXiv ID: 2601.15846
- Source URL: https://arxiv.org/abs/2601.15846
- Reference count: 0
- Primary result: 600 documents sufficient to achieve 95% of maximum accuracy for clinical text classification

## Executive Summary
This study investigates optimal training corpus sizes for clinical text classification using pre-trained BERT embeddings with Random Forest classifiers on MIMIC-III discharge notes. The research challenges conventional wisdom that 200-500 annotated documents are needed by demonstrating that 600 documents achieve 95% of maximum achievable accuracy across 10 ICD-9 diagnosis classification tasks. Vocabulary quality analysis reveals that strong predictors accelerate learning while noisy predictors slow it down, suggesting that simple keyword analysis can predict model performance and guide sample size decisions.

## Method Summary
The study employed pre-trained BERT embeddings with Random Forest classifiers on MIMIC-III discharge notes, varying training data from 100 to 10,000 documents across 10 ICD-9 diagnosis classification tasks. For each diagnosis, the researchers extracted binary labels and stratified random subsets at specified sizes, repeated 10 times per size. They generated BERT pooled embeddings for all documents, trained Random Forest classifiers on these embeddings, and evaluated on a held-out validation set of 5,000 documents. Parallel vocabulary analysis used Lasso logistic regression on bag-of-words representations to identify strong predictors (|coefficient| > 0.1) versus noisy predictors, correlating these counts with learning curve behavior.

## Key Results
- 600 documents sufficient to achieve 95% of maximum achievable accuracy for all 10 ICD-9 tasks
- Strong predictors increase maximum accuracy by approximately 0.04 per 100 additional words
- Noisy predictors decrease accuracy by approximately 0.02 per 100 additional words
- Simple keyword analysis can predict learning curve steepness and guide sample size decisions

## Why This Works (Mechanism)

### Mechanism 1: Transfer Learning Compensation for Limited Annotations
- Claim: Pre-trained contextual embeddings reduce sample size requirements by initializing with learned linguistic representations.
- Mechanism: BERT encodes clinical text into dense vectors that already capture semantic relationships from pre-training, so the downstream classifier requires fewer labeled examples to learn discriminative boundaries.
- Core assumption: The pre-training corpus distribution overlaps sufficiently with the target clinical domain.
- Evidence anchors:
  - [abstract] "Pre-trained BERT embeddings enable effective clinical text classification with as few as 600 gold standard annotations."
  - [section 2.4] "Transfer learning from pre-trained models allowed us to leverage knowledge from a vast number of documents... potentially compensating for when the samples had limited annotated training data."
- Break condition: Domain shift between pre-training and clinical text exceeds model's generalization capacity.

### Mechanism 2: Vocabulary Quality Determines Learning Efficiency
- Claim: The ratio of strong to noisy predictive words in a corpus predicts learning curve steepness and achievable accuracy.
- Mechanism: Strong predictors provide discriminative signal that models acquire early; noisy predictors dilute learning, requiring more samples to distinguish signal from noise.
- Core assumption: Lasso logistic regression on bag-of-words approximates the true information content available to the downstream model.
- Evidence anchors:
  - [abstract] "Every 100 additional noisy words decreased accuracy by approximately 0.02 while 100 additional strong predictors increased maximum accuracy by approximately 0.04."
  - [section 3, Figure 3] Shows clear pattern between predictor quality and learning curve steepness across 10 diagnoses.
- Break condition: Bag-of-words vocabulary analysis fails to capture multi-word expressions or negation patterns critical to clinical meaning.

### Mechanism 3: Random Forest Stability with Small Tabular Inputs
- Claim: Random Forest classifiers on fixed BERT embeddings provide stable learning curves with limited samples compared to neural fine-tuning.
- Mechanism: RF's ensemble averaging reduces variance on small datasets; using pre-computed embeddings eliminates gradient instability from joint optimization.
- Core assumption: The pooled BERT embedding contains sufficient task-relevant information without task-specific fine-tuning.
- Evidence anchors:
  - [section 2.4] "Training BERT's final layer for classification directly showed an inferior performance compared to a baseline Random Forest classifier."
  - [section 2.4] "Random Forest classifiers which work well with small sample sizes and provide better interpretability."
- Break condition: Embeddings lack task-specific refinement; complex clinical reasoning requiring token-level attention may be lost in pooled representations.

## Foundational Learning

- **Transfer Learning (Pre-trained Language Models)**
  - Why needed here: The paper's efficiency gains depend on BERT carrying prior linguistic knowledge into the clinical domain.
  - Quick check question: Can you explain why a model pre-trained on general text might still help with clinical document classification?

- **Learning Curves (Sample Size vs. Performance)**
  - Why needed here: The core finding is that learning curves plateau earlier than commonly assumed; understanding curve shapes is essential to interpret the results.
  - Quick check question: What does it mean when a learning curve reaches 95% of asymptotic performance at N=600?

- **Feature Selection / Regularization (Lasso)**
  - Why needed here: The paper uses Lasso logistic regression to identify "strong" vs. "noisy" predictors as a proxy for corpus quality.
  - Quick check question: Why would a sparse linear model's coefficients indicate vocabulary quality for a non-linear classifier?

## Architecture Onboarding

- **Component map:**
  MIMIC-III discharge notes → BERT pooled embeddings → Random Forest classifier → accuracy evaluation
  Bag-of-words representation → Lasso logistic regression → strong/noisy predictor counts → vocabulary quality metrics

- **Critical path:**
  1. Extract and preprocess discharge notes with ICD-9 labels
  2. Generate BERT pooled embeddings for all documents
  3. Sample training subsets (N = 100 to 10,000 in increments)
  4. Train Random Forest on embeddings; evaluate on held-out set
  5. Run parallel bag-of-words Lasso analysis to characterize vocabulary
  6. Correlate vocabulary metrics with learning curve behavior

- **Design tradeoffs:**
  - Fixed embeddings vs. fine-tuning: Trading potential accuracy gains for sample efficiency and training stability.
  - Bag-of-words analysis vs. contextual analysis: Simpler proxy for vocabulary quality, but may miss context-dependent signal.
  - Binary classification focus: Clear experimental control, but limits generalization to multi-label ICD coding scenarios.

- **Failure signatures:**
  - Learning curves that do not plateau by N=10,000 (suggests vocabulary analysis does not apply or embeddings are insufficient)
  - High variance across repeated samples at large N (indicates data quality or label noise issues)
  - SHAP analysis showing generic stopwords as top features (implies task lacks discriminative vocabulary)

- **First 3 experiments:**
  1. Reproduce on a single ICD-9 code: Sample N∈{100, 300, 600, 1000, 3000, 10000}, plot accuracy curve, compare to paper's 95% threshold claim.
  2. Vocabulary quality audit: Run Lasso on bag-of-words for your chosen task; count strong (|β|>0.1) vs. noisy features; predict expected learning curve shape.
  3. Embedding ablation: Compare BERT pooled embeddings vs. TF-IDF or bag-of-words as Random Forest input to validate transfer learning contribution.

## Open Questions the Paper Calls Out

- Does document length independently influence sample size requirements beyond the effect of noisy word counts?
- How do human-machine annotation approaches (e.g., LLM-assisted labeling with human review) compare to gold-standard human annotation in learning speed and final accuracy?
- How well do the 600-document threshold and vocabulary-based learning curve predictions generalize to multi-class classification and non-MIMIC datasets?
- How do co-morbidity patterns among conditions affect learning curve steepness and optimal sample size estimates?

## Limitations

- The study's findings depend heavily on pre-trained BERT embeddings providing sufficient domain-relevant features without fine-tuning.
- Vocabulary quality analysis using bag-of-words may miss clinically important multi-word expressions and negation patterns.
- Binary classification approach limits generalizability to real-world multi-label coding scenarios.

## Confidence

**High Confidence**: The finding that learning curves plateau early (around 600 documents) for the specific 10 ICD-9 tasks studied.

**Medium Confidence**: The vocabulary quality hypothesis that strong predictor counts positively correlate with accuracy while noisy predictor counts negatively correlate.

**Low Confidence**: The broader claim that keyword analysis can reliably predict model performance across diverse clinical text classification tasks.

## Next Checks

1. Apply the same methodology (BERT embeddings + Random Forest + vocabulary analysis) to a different clinical text corpus with distinct diagnostic codes to test generalizability beyond MIMIC-III discharge notes.

2. Implement a fine-tuned BERT classifier on the same 10 ICD-9 tasks and compare learning curves to determine whether findings are specific to the fixed-embedding approach.

3. Replace the bag-of-words Lasso analysis with a contextual embedding-based vocabulary quality assessment to validate whether strong/noisy predictor counts capture the most discriminative features.