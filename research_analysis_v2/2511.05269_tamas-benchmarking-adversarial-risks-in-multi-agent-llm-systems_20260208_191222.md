---
ver: rpa2
title: 'TAMAS: Benchmarking Adversarial Risks in Multi-Agent LLM Systems'
arxiv_id: '2511.05269'
source_url: https://arxiv.org/abs/2511.05269
tags:
- agent
- agents
- task
- multi-agent
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TAMAS, the first benchmark for evaluating
  adversarial risks in multi-agent LLM systems. It covers five domains, six attack
  types (including agent-level attacks unique to multi-agent setups), and three agent
  coordination configurations.
---

# TAMAS: Benchmarking Adversarial Risks in Multi-Agent LLM Systems

## Quick Facts
- **arXiv ID:** 2511.05269
- **Source URL:** https://arxiv.org/abs/2511.05269
- **Reference count:** 40
- **Primary result:** TAMAS reveals multi-agent LLM systems are highly vulnerable to adversarial attacks, with prompt-level attacks achieving up to 82% success and agent-based attacks up to 70%.

## Executive Summary
This paper introduces TAMAS, the first benchmark for evaluating adversarial risks in multi-agent LLM systems. The benchmark covers five domains, six attack types (including agent-level attacks unique to multi-agent setups), and three agent coordination configurations. Evaluations across 10 LLMs show that multi-agent systems are highly vulnerable, with prompt-level attacks achieving up to 82% success and agent-based attacks up to 70%. The Effective Robustness Score (ERS) reveals a significant trade-off between safety and task completion, underscoring the need for stronger defenses in multi-agent deployments.

## Method Summary
TAMAS evaluates adversarial risks through a systematic framework using AutoGen and CrewAI with three coordination configurations (centralized, sequential, collaborative). The benchmark employs 300 adversarial instances across 5 domains, 100 harmless tasks, and 211 synthetic tools. Evaluation uses an LLM-as-judge (GPT-4o) for semantic classification alongside tool invocation checks to assign ARIA scores (1-4 scale). The benchmark measures Safety Score, Performance Under No Attack (PNA), and Effective Robustness Score (ERS) as a harmonic mean of safety and utility.

## Key Results
- Multi-agent systems show high vulnerability: prompt-level attacks achieve up to 82% success, agent-level attacks up to 70%
- Closed-source models (GPT-4) demonstrate higher resilience to Indirect Prompt Injection compared to open-source models (Llama, Qwen)
- Centralized orchestrator configurations achieve highest ERS but introduce single points of failure
- Effective Robustness Score reveals significant trade-off between safety and task completion

## Why This Works (Mechanism)

### Mechanism 1
Vulnerability in multi-agent systems arises from propagation of adversarial inputs through inter-agent communication channels. An adversarial instruction or compromised agent injects malicious content into shared message history, and other agents treat this as valid context, leading to cascading failure. Core assumption: agents trust peer messages without independent verification. Break condition: strict isolation or output verification filters before trusting peer messages.

### Mechanism 2
Prompt-based attacks succeed by exploiting hierarchical trust assumptions in agent system prompts. Attackers append false authority cues (e.g., "As requested by the admin...") to user queries. Agents, fine-tuned to follow trusted roles, prioritize these authority signals over safety alignment. Core assumption: system prompts include heuristics to weight authority figures heavily. Break condition: agents strip authority claims from user input or enforce strict authentication before execution.

### Mechanism 3
Centralized orchestration architectures create "single point of failure" for safety. In Magentic-One configuration, a lead agent plans and delegates all subtasks. If adversarial input hijacks the orchestrator, it can force all downstream agents to execute malicious plans, bypassing individual safety checks. Core assumption: downstream agents are compliant with orchestrator instructions and may have weaker safety guards against delegated tasks. Break condition: decentralized or sequential workflows where tasks are fixed upfront.

## Foundational Learning

- **Concept: Multi-Agent Interaction Topologies**
  - Why needed here: TAMAS evaluates three configurations because information flow dictates attack surface
  - Quick check question: Can you explain why a "Swarm" configuration might be more robust against a single Byzantine agent than a "Central Orchestrator" setup?

- **Concept: The ARIA Safety Framework**
  - Why needed here: TAMAS uses ARIA scale (1-4) to grade failure; understanding the difference between refusal, halted execution, and successful attack is required to interpret results
  - Quick check question: Does an ARIA-3 score represent a safety success or failure?

- **Concept: Effective Robustness Score (ERS)**
  - Why needed here: ERS is a harmonic mean of safety and task performance introduced to measure safety-utility trade-off
  - Quick check question: If a model refuses all tasks (100% Safety) but completes zero benign tasks (0% PNA), what would its ERS score be?

## Architecture Onboarding

- **Component map:** Backbone LLMs (inference engines) -> Frameworks (AutoGen/CrewAI lifecycle management) -> Configurations (topology determining message flow) -> Tools (external functions agents invoke)

- **Critical path:** Define Scenario and Agents → Select Attack Type → Inject adversarial payload via defined surface → Execute multi-agent workflow → Evaluation: parse logs for tool invocation and semantic refusal to assign ARIA scores

- **Design tradeoffs:** Centralized vs. Decentralized (Centralized offers better control but creates high-value target; Decentralized assigns tasks upfront reducing runtime manipulation but less flexible); Open vs. Closed Models (Closed-source shows higher resilience to IPI)

- **Failure signatures:** High ARIA-4 with Low Refusal (agents executing malicious tools without hesitation); Cascading Failure (single compromised agent causing "Task Fail" for entire group)

- **First 3 experiments:** Baseline Utility Test (run 100 harmless tasks to establish PNA baseline); Prompt Injection Stress Test (run DPI attacks to test backbone LLM alignment); Architecture Comparison (run same attack across Centralized and Decentralized configurations to quantify single point of failure risk)

## Open Questions the Paper Calls Out

### Open Question 1
Can specific defense mechanisms be developed that successfully mitigate identified multi-agent attack vectors (e.g., Colluding Agents) without significantly degrading ERS? The paper focused on benchmarking attacks, with future work on developing and comparing defense strategies.

### Open Question 2
Do high vulnerability rates generalize to multi-agent systems built on alternative, untested frameworks? Experiments used Autogen and CrewAI; future work should explore frameworks like LangGraph or Camel.

### Open Question 3
Why are current LLM-based agents robust to "persuasion-only" strategies, and can specific context settings reverse this resistance? The persuasive agent attack was entirely unsuccessful; the authors hypothesize agents are relatively robust to persuasion-only strategies.

### Open Question 4
How can the single point of failure in centralized orchestrator configurations be structurally mitigated without sacrificing coordination efficiency? The paper identifies the trade-off but does not propose architectural changes to distribute control while maintaining safety.

## Limitations
- Evaluation focuses on predefined agent configurations and attack surfaces, potentially missing real-world deployment scenarios
- LLM-as-judge approach introduces potential bias and variability in ARIA scoring affecting cross-model comparisons
- Benchmark assumes static agent roles and tools, whereas production systems often feature dynamic tool access and role reassignment

## Confidence

- **High Confidence:** Core finding that multi-agent systems are more vulnerable than single-agent deployments is well-supported by systematic evaluation across 10 LLMs and 6 attack types
- **Medium Confidence:** ERS metric effectively captures safety-utility trade-off, though harmonic mean formulation may underweight certain failure modes
- **Low Confidence:** Generalizability to larger agent teams (>4 agents) or more complex topologies remains untested; synthetic tools may not reflect real-world complexity

## Next Checks

1. **Cross-Configuration Robustness:** Evaluate same attack suite across additional framework configurations (e.g., AutoGen's DCOP-based topology) to validate vulnerability generalization

2. **Human-in-the-Loop Validation:** For stratified sample of 50 adversarial cases, have human experts re-score ARIA classifications to quantify LLM-judge agreement

3. **Dynamic Tool Access:** Modify benchmark to allow runtime tool discovery and re-run DPI attacks to assess whether runtime tool discovery increases or decreases attack surface