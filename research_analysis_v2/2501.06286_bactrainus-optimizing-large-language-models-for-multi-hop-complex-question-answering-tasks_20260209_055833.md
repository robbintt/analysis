---
ver: rpa2
title: 'Bactrainus: Optimizing Large Language Models for Multi-hop Complex Question
  Answering Tasks'
arxiv_id: '2501.06286'
source_url: https://arxiv.org/abs/2501.06286
tags:
- question
- selector
- supporting
- reader
- facts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper evaluates large language models for multi-hop question
  answering on the HotpotQA dataset. A two-stage selector-reader architecture is proposed,
  where each stage uses an independent LLM.
---

# Bactrainus: Optimizing Large Language Models for Multi-hop Complex Question Answering Tasks

## Quick Facts
- arXiv ID: 2501.06286
- Source URL: https://arxiv.org/abs/2501.06286
- Reference count: 0
- Best model achieved F1 score of 89.63 on supporting facts and 79.70 on joint metrics

## Executive Summary
This paper evaluates large language models for multi-hop question answering on the HotpotQA dataset using a two-stage selector-reader architecture. Each stage uses an independent LLM with knowledge distillation techniques like chain-of-thought reasoning and question decomposition to enhance performance. The modular approach splits the complex reasoning task into sub-components, allowing specialized optimization for retrieval and inference stages. The best model achieved state-of-the-art results with F1 scores of 89.63 on supporting facts and 79.70 on joint metrics.

## Method Summary
The method employs a two-stage architecture where a selector model identifies relevant supporting facts from 10 candidate paragraphs, and a reader model performs multi-hop reasoning to generate answers. Both stages use Llama 3.1 models (8B or 70B) fine-tuned with LoRA for parameter efficiency. Knowledge distillation transfers reasoning capabilities from larger to smaller models through chain-of-thought traces. Question decomposition generates sub-questions to improve selector accuracy. The pipeline converts HotpotQA training data to Alpaca instruction format and trains selector and reader models separately before integration.

## Key Results
- Best model achieved Supporting Facts F1 of 89.63 and Joint F1 of 79.70
- Two-stage selector-reader architecture outperformed monolithic single-model approaches by approximately 2% on joint metrics
- Knowledge distillation from 70B teacher improved 8B student performance when using chain-of-thought traces
- Adding distractor paragraphs to reader input caused significant performance degradation (F1 drops from 80.04 to 58.61)

## Why This Works (Mechanism)

### Mechanism 1: Task Decomposition via Selector-Reader Split
- Decomposing multi-hop QA into retrieval and reasoning sub-tasks, each handled by independent LLMs, improves joint performance by approximately 2% over monolithic approaches
- Selector isolates relevant supporting facts from distractor paragraphs, preventing context dilution and allowing specialized optimization
- Core assumption: Models cannot effectively isolate necessary evidence when large amounts of irrelevant text are present
- Evidence: Table 11 shows scenario 5 (Bactrainus 70B) achieves EM=51.73, F1=79.70 vs scenario 1 (all-in-one) at EM=47.93, F1=75.96
- Break condition: If selector accuracy degrades significantly on datasets with more than 10 candidate paragraphs, cascading errors may negate decomposition benefits

### Mechanism 2: Knowledge Distillation via Chain-of-Thought from Larger Teacher
- Fine-tuning smaller student model (8B) with CoT reasoning traces generated by larger teacher model (70B) improves multi-hop inference quality
- Teacher generates step-by-step reasoning traces; student internalizes reasoning structure without requiring larger parameter count
- Core assumption: Larger models produce higher-quality reasoning traces that transfer more effectively
- Evidence: Table 7 shows Bactrainus 8B + CoT 70B achieves EM=74.19, F1=86.91 vs Bactrainus 8B + CoT 8B at EM=72.97, F1=85.62
- Break condition: If teacher CoT contains hallucinated reasoning steps or student lacks capacity to internalize traces, distillation may introduce noise

### Mechanism 3: Question Decomposition for Selector Enhancement
- Generating sub-questions from complex multi-hop queries and providing them to sentence selector improves supporting fact identification accuracy
- Decomposer model splits bridge-type questions into simpler sub-questions, providing additional semantic anchors for relevance signals
- Core assumption: Sub-questions reduce semantic ambiguity and help selector establish finer-grained relevance signals
- Evidence: Table 9 shows Two-stage + Sub-question achieves Supporting Facts F1=89.63 vs Two-stage without at F1=89.21
- Break condition: If sub-questions introduce ambiguity or drift from original query intent, they may mislead rather than assist selector

## Foundational Learning

- Concept: Multi-hop Question Answering (MHQA)
  - Why needed here: Architecture assumes questions require chaining information from multiple documents/sentences
  - Quick check question: Given "What is the population of the city where the author of '1984' was born?", can you identify the two reasoning hops required?

- Concept: Knowledge Distillation (Teacher-Student Paradigm)
  - Why needed here: System relies on transferring reasoning capabilities from larger to smaller models
  - Quick check question: If you have a 70B model generating CoT traces, what determines whether distillation helps vs hurts the 8B student?

- Concept: LoRA (Low-Rank Adaptation) Fine-tuning
  - Why needed here: All experiments use LoRA for parameter-efficient fine-tuning
  - Quick check question: Why does Table 6 use LoRA rank=64 for 8B model but rank=16 for 70B model? What tradeoff does this represent?

## Architecture Onboarding

- Component map:
Multi-hop Question + 10 Candidate Paragraphs
-> [Paragraph Selector - Llama 3.1 8B fine-tuned]
-> Golden Paragraphs (2 identified from 10)
-> [Question Decomposer - 8B fine-tuned] -> Sub-questions
-> [Sentence Selector - 8B fine-tuned]
-> Supporting Facts (key sentences)
-> [Reader - Llama 3.1 70B or 8B fine-tuned, optionally + CoT]
-> Final Answer

- Critical path: Selector accuracy directly bounds reader performance. Table 3 shows providing supporting facts vs gold paragraphs to reader yields similar results, but adding distractors causes significant degradation. Priority: maximize selector precision before optimizing reader.

- Design tradeoffs:
1. Single-stage vs two-stage selector: Two-stage adds complexity but shows minimal gain (Table 9: 89.21 vs 89.27 F1)
2. Reader size (8B vs 70B): 70B reader provides substantial gain (Table 11: joint F1 79.70 vs 78.02) but requires more compute
3. CoT distillation source: 70B-generated CoT helps 8B student; 8B self-generated CoT slightly hurts

- Failure signatures:
1. Selector provides gold paragraphs but reader fed all paragraphs -> Performance collapses (Table 3: F1 drops from 80.04 to 58.61 for 70B model)
2. All-in-one model handling both selection and reading -> ~2% lower joint F1 vs modular approach (Table 11)
3. Few-shot with >1 example -> Diminishing returns; 2-shot sometimes worse than 1-shot (Table 4)

- First 3 experiments:
1. Reproduce zero-shot reader baseline with perfect supporting facts on 500-sample HotpotQA subset using Llama 3.1 8B Instruct. Target: ~60 EM, ~74 F1 (per Table 1)
2. Fine-tune single-stage selector (8B, LoRA rank=64) to identify gold paragraphs from 10 candidates. Verify paragraph-level accuracy reaches ~96-97 EM before proceeding
3. Run ablation comparing: (a) reader with supporting facts only vs (b) reader with gold paragraphs + 2 distractors. Confirm distractor sensitivity matches paper findings before investing in selector optimization

## Open Questions the Paper Calls Out

- How effectively does the Bactrainus architecture transfer to specialized domains (e.g., legal or medical) or multilingual datasets?
  - Basis: "Future Works" section states extending methods to other languages and specialized fields is vital for assessing broader applicability
  - Unresolved: Current study exclusively evaluates on general-domain English HotpotQA dataset
  - Resolution: Performance benchmarks on domain-specific multi-hop datasets or multilingual multi-hop QA datasets

- Can a multi-agent system, where distinct agents handle retrieval and inference, outperform the current single-model modular design?
  - Basis: "Future Works" suggests employing multi-agent systems where specialized agents focus on particular sub-tasks could enhance flexibility and performance
  - Unresolved: Current implementation fine-tunes single LLM backbone for distinct sub-tasks rather than employing autonomous, coordinating agents
  - Resolution: Comparative study evaluating multi-agent framework against Bactrainus pipeline on complex reasoning tasks

- How does the LLM-based selector perform in a full open-domain setting where the model must retrieve facts from the entire corpus rather than pre-filtered list of 10 paragraphs?
  - Basis: Methodology restricted to HotpotQA "distractor" setting with 10 candidate paragraphs
  - Unresolved: Success in distractor setting doesn't guarantee selector can handle noise and scale of retrieving from thousands/millions of documents
  - Resolution: Evaluation of selector component's recall and precision when input pool is entire source Wikipedia dump

## Limitations

- Question decomposition contribution appears marginal (0.4 F1 improvement) with limited ablation analysis
- CoT distillation mechanism lacks detailed validation of teacher model's reasoning quality
- Choice of 15,661 "challenging samples" for 70B CoT distillation is not clearly justified, raising selection bias concerns

## Confidence

- **High confidence**: Two-stage selector-reader architecture provides ~2% performance improvement over monolithic approaches (Table 11)
- **Medium confidence**: Knowledge distillation from larger teacher models improves smaller student performance, though mechanism assumes teacher CoTs are higher quality without direct validation
- **Low confidence**: Question decomposition's contribution to selector performance due to modest improvement and lack of detailed analysis

## Next Checks

1. Replicate selector-reader ablation: Run same evaluation pipeline (reader with perfect supporting facts vs reader with gold paragraphs + distractors) to verify ~22 point F1 drop reported in Table 3
2. CoT quality audit: Generate 50 CoT traces using both 70B and 8B models on held-out samples, then manually evaluate reasoning validity and hallucination rates
3. Question decomposition impact: Remove question decomposition component and re-run two-stage pipeline to confirm whether 0.4 F1 improvement is reproducible and consistent across multiple random seeds