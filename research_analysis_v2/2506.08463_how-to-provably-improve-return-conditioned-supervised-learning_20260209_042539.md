---
ver: rpa2
title: How to Provably Improve Return Conditioned Supervised Learning?
arxiv_id: '2506.08463'
source_url: https://arxiv.org/abs/2506.08463
tags:
- policy
- optimal
- rcsl
- learning
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Reinforced RCSL (R2CSL), a framework that provably
  improves Return-Conditioned Supervised Learning by addressing its fundamental stitching
  limitation. R2CSL introduces the concept of in-distribution optimal return-to-go
  (RTG), which identifies the best achievable future return within the offline dataset
  at each state, enabling trajectory stitching that surpasses the performance of policies
  used to generate the dataset.
---

# How to Provably Improve Return Conditioned Supervised Learning?

## Quick Facts
- **arXiv ID:** 2506.08463
- **Source URL:** https://arxiv.org/abs/2506.08463
- **Reference count:** 40
- **Primary result:** Introduces R2CSL framework that provably improves RCSL by enabling trajectory stitching via in-distribution optimal RTG estimation, achieving superior performance on D4RL benchmarks.

## Executive Summary
This paper addresses a fundamental limitation of Return-Conditioned Supervised Learning (RCSL) methods like Decision Transformer: their inability to exceed the performance of the best trajectory in the offline dataset due to the "consistency condition." R2CSL introduces the concept of in-distribution optimal return-to-go (RTG), which identifies the maximum achievable future return within the dataset at each state. By conditioning the policy on this optimal RTG rather than the specific trajectory's RTG, R2CSL enables trajectory stitching that can surpass dataset policies. The method learns both a policy and an optimal RTG estimator through supervised learning, avoiding the instability of dynamic programming approaches.

## Method Summary
R2CSL consists of two main components: a policy network trained via negative log-likelihood on the offline dataset, and an RTG estimator trained via quantile regression to predict the maximum achievable return from any state at any timestep. During inference, instead of using the original trajectory's RTG as conditioning, the policy conditions on the output of the RTG estimator. This allows the policy to "stitch" optimal segments from different trajectories by always conditioning on the best possible future return achievable from the current state. The framework can be instantiated with either expectile or quantile regression for RTG estimation, with quantile regression shown to be more robust. A multi-step relabeling scheme can be applied to further improve performance, bridging the gap to dynamic programming-based approaches.

## Key Results
- R2CSL with quantile regression achieves significantly higher normalized scores than vanilla RCSL methods (RvS, DT) on D4RL Gym-MuJoCo and AntMaze benchmarks.
- The quantile regression version of R2CSL outperforms the expectile regression version, particularly when expectile regression predicts out-of-distribution returns.
- DP-R2CSL (R2CSL with dynamic programming components) achieves performance comparable to state-of-the-art QT algorithm on AntMaze tasks.
- Theoretical analysis proves R2CSL converges to an in-distribution optimal stitched policy with sample complexity comparable to classical RCSL methods.

## Why This Works (Mechanism)

### Mechanism 1: In-Distribution Optimal RTG Conditioning
- **Claim:** Replacing trajectory-specific RTG with the maximum achievable RTG enables stitching optimal segments from different trajectories.
- **Mechanism:** Standard RCSL fails because it conditions on the specific return of the trajectory the data came from. R2CSL estimates the highest return-to-go seen in any trajectory visiting a state, allowing the policy to select actions leading to the best possible outcome allowed by the dataset.
- **Core assumption:** The policy generalizes to map state and optimal RTG target to correct actions, and the dataset provides partial coverage of the optimal path.
- **Evidence anchors:** Abstract states R2CSL enables stitching that surpasses performance of policies used to generate the dataset. Section 4 shows R2CSL discards the consistency condition to search for in-distribution optimal RTG.

### Mechanism 2: Quantile Regression for Robust Estimation
- **Claim:** Quantile regression is more robust than expectile regression because it resists hallucinating out-of-distribution returns.
- **Mechanism:** Expectile regression with L2 loss can be skewed by noise, potentially predicting returns higher than anything in the dataset. Quantile regression with L1 loss is strictly more robust and provably recovers the optimal RTG in deterministic tabular settings.
- **Core assumption:** The environment is deterministic or the noise profile allows the specific quantile to capture the true optimal signal.
- **Evidence anchors:** Section 6 proves L1 loss is more robust than L2 loss for RTG estimation. Section 7.1 shows R2CSL with quantile regression succeeds where expectile fails depending on hyperparameters.

### Mechanism 3: Multi-Step Relabeling (Bridging to Dynamic Programming)
- **Claim:** Multi-step relabeling allows the supervised learner to converge to the globally optimal policy, mimicking Dynamic Programming.
- **Mechanism:** Standard R2CSL does single-step stitching. By recursively relabeling RTGs over k passes, the method propagates value information backward through time, resembling Bellman updates while retaining the supervised learning framework.
- **Core assumption:** Deterministic environments and sufficient relabeling passes.
- **Evidence anchors:** Section 8 proves that by increasing the number of steps, R2CSL can find the optimal in-distribution policy. Theorem 8.1 provides theoretical guarantees under deterministic assumptions.

## Foundational Learning

- **Concept:** **Trajectory Stitching (in Offline RL)**
  - **Why needed here:** R2CSL solves the problem of standard RCSL failing to exceed the best trajectory in the dataset.
  - **Quick check question:** *Can you explain why a standard behavioral cloning agent cannot beat the expert it clones, and how "stitching" relates to this limit in offline RL?*

- **Concept:** **Quantile vs. Expectile Regression**
  - **Why needed here:** The choice of regression loss determines if the model hallucinates out-of-distribution returns.
  - **Quick check question:** *Why does minimizing squared error (Expectile) make a model sensitive to outliers compared to absolute error (Quantile), and how does this affect "optimistic" return estimation?*

- **Concept:** **Consistency Condition in RCSL**
  - **Why needed here:** R2CSL works by discarding this condition that ties future return to current reward.
  - **Quick check question:** *In a Return-Conditioned policy, what does the "Consistency Condition" enforce regarding the relationship between the target return at step t and step t+1?*

## Architecture Onboarding

- **Component map:**
  - Policy Network (RvS/DT architecture) -> Trained via MLE on offline dataset
  - RTG Estimator (Quantile Regression MLP) -> Trained to predict max achievable return
  - Inference Controller -> Replaces trajectory RTG with RTG estimator output

- **Critical path:**
  1. Train Policy Network on static dataset using standard RCSL
  2. Train RTG Estimator using Quantile Regression (L1 loss) to predict returns
  3. At inference, feed state to Estimator to get target return, then feed (state, target) to Policy Network

- **Design tradeoffs:**
  - Expectile vs. Quantile: Expectile is faster but risks OOD predictions; Quantile is robust but requires careful α tuning
  - R2CSL vs. DP-R2CSL: Pure R2CSL is simpler; DP-R2CSL offers higher performance on sparse-reward tasks but adds complexity

- **Failure signatures:**
  - OOD Collapse: RTG Estimator predicts return higher than dataset allows, causing policy to output garbage actions
  - Sparsity Failure: If dataset has zero-reward trajectories dominating a state, Quantile Regression may predict 0 return, causing agent to give up

- **First 3 experiments:**
  1. PointMaze Visualization: Replicate toy environment to visually confirm agent switches from "Type II" to "Type I" trajectories when using Quantile R2CSL
  2. D4RL Gym Comparison: Run R2CSL-Quantile vs. RvS on halfcheetah-medium to verify performance gains
  3. Alpha Sensitivity Analysis: Sweep α ∈ {0.9, 0.99, 0.999} on hopper-medium-replay to observe stability of stitching capability

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the theoretical limitations of RCSL-based algorithms in stochastic environments be overcome?
- **Basis in paper:** The conclusion states, "A notable limitation of the RCSL-type algorithms is that they can fail in stochastic environments... It remains an open problem to theoretically address this limitation based on the RCSL framework."
- **Why unresolved:** The current R2CSL framework recovers an objective policy π*β which prior works suggest can be arbitrarily suboptimal in stochastic settings.
- **What evidence would resolve it:** A theoretical modification to the R2CSL framework or a new objective function that guarantees near-optimal performance in stochastic MDPs without relying on dynamic programming.

### Open Question 2
- **Question:** Can the sample complexity convergence rate of R2CSL be improved from N^(-1/4) to N^(-1/2)?
- **Basis in paper:** Section 5.1 notes the convergence rate is N^(-1/4), "which is slower than the standard rate of N^(-1/2)... We believe this discrepancy is due to a limitation in the current analysis."
- **Why unresolved:** It remains unclear if the slower rate is intrinsic to the algorithm's dependence on the "in-distribution optimal RTG" estimation or merely an artifact of the proof technique.
- **What evidence would resolve it:** A refined theoretical analysis achieving N^(-1/2) convergence or a proof showing that N^(-1/4) is a tight lower bound for R2CSL.

### Open Question 3
- **Question:** Does the multi-step relabeling scheme recover the globally optimal policy in stochastic environments?
- **Basis in paper:** Theorem 8.1 proves that multi-step relabeling recovers the optimal policy "Under deterministic environments," but the paper does not extend this theoretical guarantee to stochastic MDPs.
- **Why unresolved:** The recursive relabeling update relies on achievable quantities rather than expected values, making the theoretical extension to stochastic transitions non-trivial.
- **What evidence would resolve it:** An extension of Theorem 8.1 providing convergence guarantees for stochastic MDPs or empirical evidence demonstrating that increasing relabeling passes bridges the gap to the optimal policy in stochastic benchmarks.

## Limitations
- The theoretical guarantees rely on deterministic environment assumptions, which rarely hold in real-world robotics tasks.
- The multi-step relabeling scheme's computational complexity scales with the horizon H, potentially limiting applicability to long-horizon tasks.
- The framework requires careful tuning of the quantile parameter α to balance between sufficient coverage and avoiding out-of-distribution predictions.

## Confidence

- **High Confidence:** The stitching mechanism and superiority of quantile regression over expectile regression are well-supported by both theory and experimental evidence.
- **Medium Confidence:** The sample complexity bounds and convergence guarantees assume specific dataset coverage conditions that may not hold in practice.
- **Low Confidence:** The multi-step relabeling claims are primarily theoretical, with practical feasibility and performance benefits not extensively validated experimentally.

## Next Checks
1. **Stochastic Environment Test:** Evaluate R2CSL on a stochastic variant of the PointMaze environment to test the robustness of the stitching mechanism beyond deterministic assumptions.
2. **Alpha Sensitivity on Sparse Rewards:** Systematically sweep α values on AntMaze tasks to identify the optimal quantile level and test whether extremely high α values (>0.999) lead to OOD collapse.
3. **Multi-Step Relabeling Scalability:** Implement and benchmark the multi-step relabeling scheme on a medium-complexity D4RL task to measure both convergence behavior and computational overhead across different numbers of relabeling passes.