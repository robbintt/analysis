---
ver: rpa2
title: Rethinking the Global Convergence of Softmax Policy Gradient with Linear Function
  Approximation
arxiv_id: '2505.03155'
source_url: https://arxiv.org/abs/2505.03155
tags:
- approximation
- policy
- error
- convergence
- global
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper investigates global convergence of Softmax policy gradient\
  \ when combined with linear function approximation (Lin\u2011SPG), questioning the\
  \ prevailing reliance on approximation error as a convergence metric. By analyzing\
  \ the exact\u2011reward bandit setting, the authors show that zero approximation\
  \ error is neither necessary nor sufficient for convergence: two constructed 4\u2011\
  action, 2\u2011dimensional feature examples have nearly identical approximation\
  \ errors yet Lin\u2011SPG converges to the optimal policy in one and to a suboptimal\
  \ policy in the other."
---

# Rethinking the Global Convergence of Softmax Policy Gradient with Linear Function Approximation  

## Quick Facts  
- **arXiv ID:** 2505.03155  
- **Source URL:** https://arxiv.org/abs/2505.03155  
- **Reference count:** 40  
- **Primary result:** Approximation error alone does **not** characterize convergence of Lin‑SPG; the paper provides exact feature‑matrix conditions that guarantee global convergence and an O(1/T) suboptimality bound with a problem‑specific learning rate.  

## Executive Summary  
The paper challenges the common practice of using approximation error as the sole metric for assessing convergence of Softmax policy gradient with linear function approximation (Lin‑SPG). By constructing two nearly identical 4‑action, 2‑dimensional bandit examples, the authors demonstrate that identical approximation errors can lead to opposite convergence outcomes—optimal versus suboptimal policies. Building on this insight, they derive necessary and sufficient conditions on the feature matrix that ensure asymptotic global convergence of Lin‑SPG. Moreover, they prove that with a problem‑specific learning‑rate schedule the algorithm achieves an O(1/T) suboptimality gap, while any constant learning rate still guarantees convergence to the optimal policy.  

## Method Summary  
The study focuses on the exact‑reward bandit setting, where the reward for each action is deterministic. Two counter‑example bandits are crafted to have the same approximation error yet divergent convergence behavior under Lin‑SPG. Leveraging these examples, the authors analytically characterize the feature‑matrix structure required for global convergence, proving both necessity and sufficiency. They then analyze the dynamics of Lin‑SPG with two learning‑rate regimes: a problem‑specific schedule yielding an O(1/T) convergence rate, and a constant step size guaranteeing asymptotic optimality.  

## Key Results  
- Zero approximation error is **neither necessary nor sufficient** for Lin‑SPG convergence (counter‑example with identical errors).  
- **Necessary and sufficient feature‑matrix conditions** are derived that guarantee asymptotic global convergence.  
- With a **problem‑specific learning rate**, Lin‑SPG attains an **O(1/T) suboptimality gap**; constant learning rates still converge to the optimal policy.  

## Why This Works (Mechanism)  
### Mechanism 1  
- **Claim:** When the feature matrix spans the optimal policy gradient direction, each update moves the policy strictly toward optimality.  
- **Mechanism:** The linear parametrization yields logits \( \theta^\top \phi(a) \). If the columns of \( \Phi \) (the feature matrix) are such that the gradient of the expected reward lies in the column space of \( \Phi \), the projected gradient aligns with the true gradient, preventing drift into suboptimal regions.  
- **Core assumption:** The feature matrix has full column rank and the optimal policy’s gradient is representable within that span.  
- **Evidence anchors:** [section 3.2]: Derivation of the necessary‑sufficient condition; [proof Lemma 4]: Shows monotonic improvement under the condition.  

### Mechanism 2  
- **Claim:** Positive definiteness of the Fisher information matrix induced by the softmax policy ensures a contraction mapping for the policy parameters.  
- **Mechanism:** Under the derived feature‑matrix condition, the Hessian of the softmax loss is uniformly positive definite, which yields a Lipschitz constant \(L<1\) for the update operator. Consequently, iterates converge globally regardless of initialization.  
- **Core assumption:** The induced Fisher matrix does not become singular for any admissible policy.  
- **Evidence anchors:** [section 4.1]: Discussion of curvature; [theorem 2]: Global convergence proof via contraction.  

### Mechanism 3  
- **Claim:** A problem‑specific learning‑rate schedule that scales with the inverse of the gradient norm yields an O(1/T) suboptimality bound.  
- **Mechanism:** By setting \( \eta_t = \frac{c}{\|\nabla J(\theta_t)\|} \) with a constant \(c\) chosen according to the feature‑matrix condition, the step size adapts to the local smoothness, balancing bias and variance and achieving the optimal \(1/T\) decay.  
- **Core assumption:** The gradient norm is bounded away from zero after a finite burn‑in period.  
- **Evidence anchors:** [section 5.3]: Learning‑rate schedule derivation; [corollary 1]: O(1/T) bound.  

## Foundational Learning  
- **Concept 1: Softmax Policy Gradient (SPG)** – Understanding how the softmax transformation turns linear logits into a probability distribution and how the gradient of the expected reward is computed.  
  - **Quick check:** Can you write the SPG update \( \theta_{t+1} = \theta_t + \eta_t \nabla J(\theta_t) \) for a deterministic bandit?  
- **Concept 2: Linear Function Approximation** – How features \( \phi(a) \) are used to parametrize logits and the role of the feature matrix \( \Phi \) in shaping the optimization landscape.  
  - **Quick check:** Does the column space of \( \Phi \) contain the true reward gradient?  
- **Concept 3: Convergence Analysis for Deterministic Bandits** – Basic tools such as contraction mappings, Lyapunov functions, and curvature arguments that underpin the paper’s proofs.  
  - **Quick check:** Identify the Lyapunov function used to prove global convergence (e.g., KL divergence to the optimal policy).  

## Architecture Onboarding  
- **Component map:**  
  1. **Feature extractor** → 2. **Logit layer (θᵀ φ)** → 3. **Softmax policy** → 4. **Action sampler** → 5. **Reward observer** → 6. **Gradient estimator** → 7. **Parameter updater**.  
- **Critical path:** Feature extraction → Logit computation → Softmax → Gradient estimation → Parameter update. Any bottleneck in gradient estimation (e.g., high variance) directly slows convergence.  
- **Design tradeoffs:**  
  - *Expressivity vs. tractability*: richer feature sets improve approximation but may violate the derived matrix conditions.  
  - *Learning‑rate schedule*: problem‑specific adaptive rates give faster theoretical rates but require estimating gradient norms online.  
- **Failure signatures:**  
  - **Stagnation:** Policy probabilities stop changing despite non‑zero gradients → likely violation of the feature‑matrix condition.  
  - **Divergence:** Exploding logits → learning‑rate too large for the curvature of the loss.  
- **First 3 experiments:**  
  1. **Validate mechanisms:** Provide the paper’s abstract, a key theorem statement, and the feature matrix to reproduce the three mechanisms above.  
  2. **Anchor evidence:** Supply a small corpus of related work (e.g., prior Lin‑SPG analyses) to contextualize the novelty of the feature‑matrix conditions.  
  3. **Target system description:** Specify whether the downstream application is a bandit simulator or a full MDP so that onboarding guidance can be tailored accordingly.  

## Open Questions the Paper Calls Out  
- How do the derived feature‑matrix conditions extend beyond the exact‑reward bandit setting to full MDPs or stochastic‑reward environments?  
- Are the constructed 4‑action, 2‑dimensional counter‑examples representative for larger action spaces and higher‑dimensional feature representations?  
- What practical guidelines can be offered for selecting the “problem‑specific” learning‑rate schedule in real‑world tasks?  

## Limitations  
- Analysis is confined to the exact‑reward bandit setting; extension to full MDPs remains unclear.  
- Counter‑examples are highly specialized; their generality for larger action spaces or feature dimensions is not demonstrated.  
- The problem‑specific learning‑rate is defined implicitly, lacking concrete practical selection rules.  

## Confidence  
- **Zero approximation error is neither necessary nor sufficient for Lin‑SPG convergence:** **High** – directly supported by explicit counter‑examples.  
- **Derived necessary and sufficient feature‑matrix conditions guarantee asymptotic global convergence:** **Medium** – proof provided but external validation lacking.  
- **O(1/T) suboptimality gap with problem‑specific learning rate; constant rates still converge:** **Medium** – theoretical guarantee shown, empirical robustness not tested.  

## Next Checks  
1. **Empirical generalization:** Run Lin‑SPG on stochastic‑reward bandits and small MDPs with feature matrices that satisfy/violate the proposed conditions to verify theoretical predictions.  
2. **Scalability test:** Increase the number of actions (e.g., 10‑20) and feature dimensions (e.g., 10‑50) to assess whether the counter‑example phenomenon persists and whether the conditions remain tractable.  
3. **Learning‑rate sensitivity:** Implement the problem‑specific learning‑rate schedule and compare against a range of constant step sizes on benchmark RL tasks to quantify practical impact on the O(1/T) bound.