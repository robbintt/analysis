---
ver: rpa2
title: 'MCITlib: Multimodal Continual Instruction Tuning Library and Benchmark'
arxiv_id: '2508.07307'
source_url: https://arxiv.org/abs/2508.07307
tags:
- continual
- learning
- lora
- rank
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MCITlib, a comprehensive library for multimodal
  continual instruction tuning. It implements 8 representative multimodal continual
  instruction tuning algorithms and evaluates them across 3 benchmarks using 2 backbone
  models.
---

# MCITlib: Multimodal Continual Instruction Tuning Library and Benchmark

## Quick Facts
- arXiv ID: 2508.07307
- Source URL: https://arxiv.org/abs/2508.07307
- Reference count: 22
- Primary result: Comprehensive library implementing 8 multimodal continual instruction tuning algorithms across 3 benchmarks using 2 backbone models

## Executive Summary
This paper introduces MCITlib, a unified library for multimodal continual instruction tuning that implements 8 representative algorithms and evaluates them across 3 benchmarks using LLaVA-1.5-7b and InternVL-Chat-7b backbones. The library addresses key challenges in multimodal continual learning including catastrophic forgetting and cross-modal coordination. Experimental results reveal that while current methods partially mitigate forgetting on downstream tasks, they often degrade the model's original general-purpose capabilities, highlighting a fundamental tension in the field.

## Method Summary
The library implements 8 multimodal continual instruction tuning algorithms (LoRA-FT, O-LoRA, MoELoRA, ModalPrompt, CL-MoE, HiDe, SEFE, DISCO) on two 7B parameter models (LLaVA-1.5-7b, InternVL-Chat-7b) across three benchmarks (UCIT with 6 tasks, MLLM-DCL with 5 tasks, MLLM-ACL with 4 tasks). All methods use parameter-efficient fine-tuning with LoRA and expert modules, evaluated in a rehearsal-free setting. The evaluation framework measures both continual learning metrics (MFT, MFN, MAA, BWT) and general capability benchmarks (POPE, MME, MMBench, SEED-Bench).

## Key Results
- Current multimodal continual instruction methods partially mitigate forgetting on downstream tasks but show poor transfer to general-purpose benchmarks
- Methods that reduce catastrophic forgetting (improved BWT) still degrade original model capabilities on general benchmarks
- Cross-modal coordination emerges as a distinct challenge beyond standard catastrophic forgetting in MCL settings

## Why This Works (Mechanism)

### Mechanism 1: Expert Module Isolation
Parameter-efficient fine-tuning with task-specific expert modules can partially isolate knowledge across sequential tasks, reducing interference. Methods like O-LoRA, MoELoRA, CL-MoE, HiDe, and DISCO allocate orthogonal or specialized LoRA experts per task, creating subspaces where new task learning has limited overwrite on previous task parameters.

### Mechanism 2: Prompt-Based Modulation
Rehearsal-free continual learning with prompt-based modulation can maintain plasticity without storing exemplars. ModalPrompt uses learnable prompt prefixes that condition the frozen backbone, avoiding parameter overwrite entirely while steering task-specific behavior.

### Mechanism 3: General Capability Preservation Challenge
Optimizing only for continual learning metrics is insufficient; general-purpose capability degradation is a separate failure mode. The library evaluates on both CL metrics and general benchmarks, revealing that methods can reduce forgetting while still degrading broader capabilities.

## Foundational Learning

- **Catastrophic forgetting in neural networks**: Why needed - the entire library is designed to address this phenomenon where sequential task learning overwrites prior knowledge. Quick check - Can you explain why gradient descent on task B tends to degrade performance on previously learned task A?

- **Low-Rank Adaptation (LoRA)**: Why needed - all 8 implemented methods use LoRA or prompt-based PEFT; understanding low-rank decomposition is essential. Quick check - If a weight matrix W is 4096×4096 and LoRA rank is 16, how many trainable parameters does LoRA add per matrix?

- **Multimodal alignment in MLLMs**: Why needed - cross-modal coordination is identified as a core MCL challenge distinct from unimodal CL. Quick check - Why might forgetting visual representations not automatically imply forgetting the associated language concepts (and vice versa)?

## Architecture Onboarding

- **Component map**: `data_configs/` -> `model_configs/` -> `train_configs/` -> `MCIT_Algorithms/` -> `Evaluation/`
- **Critical path**: 1. Configure paths in data_configs and model_configs 2. Select algorithm directory (e.g., `MCIT_Algorithms/HiDe/`) 3. Run `sh scripts/MCITlib/Train/train_XX.sh` 4. Automated training across task sequence → inference → evaluation
- **Design tradeoffs**: Higher LoRA rank/expert num provides better capacity but increases parameters per task; more epochs improve convergence but extend runtime; rehearsal-free setting avoids exemplar storage but makes forgetting prevention harder
- **Failure signatures**: Negative BWT with high magnitude indicates catastrophic forgetting; high CL metrics but low general benchmark scores suggest over-fitting to task sequence; near-random accuracy on later tasks indicates plasticity loss
- **First 3 experiments**: 1. Reproduce LoRA-FT baseline on UCIT benchmark with LLaVA-1.5 to establish forgetting magnitude 2. Compare O-LoRA vs. MoELoRA on same setting to understand expert allocation trade-offs 3. Run ModalPrompt on MLLM-DCL and evaluate both CL metrics and POPE/MME to observe general capability preservation

## Open Questions the Paper Calls Out

- How can multimodal continual learning methods be designed to simultaneously mitigate catastrophic forgetting on downstream tasks while preserving or enhancing the model's original general-purpose capabilities? This gap is identified as a central direction for future work.

- What mechanisms can effectively address cross-modal coordination challenges in multimodal continual learning beyond merely preventing catastrophic forgetting? The paper identifies this as a distinct challenge but lacks specific metrics or validation.

- How do multimodal continual learning algorithms scale to larger foundation models (13B, 70B parameters), and does the relative effectiveness of different PEFT strategies change with model scale? Current evaluation is limited to 7B parameter models.

## Limitations

- All 8 evaluated algorithms show degradation on general benchmarks after sequential learning, revealing a fundamental trade-off between task-specific retention and general capability preservation
- The evaluation framework emphasizes accuracy-based CL metrics but lacks direct metrics for quantifying cross-modal coordination quality
- Results are limited to 7B parameter models, leaving scalability to larger foundation models as an open question

## Confidence

- **High**: The library provides a reproducible framework with specified algorithms, benchmarks, and evaluation metrics
- **Medium**: Experimental results showing partial forgetting mitigation are consistent across benchmarks, though absolute performance varies
- **Low**: The claim about general capability degradation being a fundamental limitation requires more extensive ablation studies to confirm

## Next Checks

1. Implement ablation studies that train on unimodal visual tasks vs. multimodal tasks to quantify whether cross-modal coordination specifically exacerbates forgetting beyond unimodal CL

2. Systematically vary LoRA rank and expert numbers across methods while monitoring both CL metrics and general benchmark performance to determine if the capability degradation trade-off is avoidable

3. Extend evaluation to measure positive transfer effects (forward and backward) across task sequences, as current metrics focus primarily on forgetting without quantifying knowledge transfer benefits