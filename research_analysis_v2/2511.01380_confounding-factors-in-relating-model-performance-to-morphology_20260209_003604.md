---
ver: rpa2
title: Confounding Factors in Relating Model Performance to Morphology
arxiv_id: '2511.01380'
source_url: https://arxiv.org/abs/2511.01380
tags:
- language
- languages
- arnett
- metrics
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies confounding factors in studies that analyze
  the relationship between morphological complexity and language modeling performance.
  The authors re-assess three hypotheses from Arnett and Bergen (2025) and find methodological
  issues, particularly the use of stem-suffix boundary recall (MorphScore) as a measure
  of morphological alignment, which does not accurately capture alignment for agglutinative
  languages.
---

# Confounding Factors in Relating Model Performance to Morphology

## Quick Facts
- arXiv ID: 2511.01380
- Source URL: https://arxiv.org/abs/2511.01380
- Reference count: 40
- Primary result: Morphological complexity impacts language modeling, but current evaluation methods and binary classifications are insufficient

## Executive Summary
This paper identifies methodological issues in studies analyzing the relationship between morphological complexity and language modeling performance. The authors critique the use of stem-suffix boundary recall (MorphScore) as a measure of morphological alignment, showing it fails to capture suffix-suffix boundaries critical for agglutinative languages. They propose token bigram metrics (accessor variety and entropic efficiency) as gradient proxies for morphological complexity that correlate with language modeling difficulty without requiring expert annotation. The analysis demonstrates that coarse morphological groupings obscure important variation and that continuous metrics provide better explanatory power for cross-linguistic modeling challenges.

## Method Summary
The authors introduce token bigram metrics (accessor variety and entropic efficiency) as gradient proxies for morphological complexity. They train monolingual Unigram Language Model tokenizers on ~1 GiB of data per language with 50k vocabulary size. For each token, they compute left/right accessor sets within a 1000-accessor window to calculate AV and η. The metrics are aggregated per language and correlated with language modeling difficulty. The analysis compares these metrics to traditional morphological alignment scores and evaluates their ability to explain performance differences across languages with different morphological typologies.

## Key Results
- MorphScore fails to capture morphological alignment quality for agglutinative languages due to ignoring suffix-suffix boundaries
- Token bigram metrics (AV and η) serve as gradient proxies that correlate with language modeling difficulty
- Agglutinative languages show higher token bigram variety and entropy, which correlates with higher perplexity in language models
- Coarse morphological groupings (binary classification) hide gradient variation and are insufficient for reliable conclusions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Token bigram metrics (accessor variety and entropic efficiency) serve as gradient proxies for morphological complexity, correlating with language modeling difficulty.
- Mechanism: For each subword token `t`, accessor variety (AV) counts unique predecessor/successor tokens. Entropic efficiency (η) measures how uniformly distributed these accessors are. Higher AV and η indicate more equally likely follow-up options per token, increasing prediction difficulty for causal language models. Metrics are computed on token bigrams within fixed 1000-accessor windows to control corpus size effects.
- Core assumption: Language modeling difficulty is proportional to the number and distribution of valid token continuations at each position.
- Evidence anchors: [abstract] "We introduce token bigram metrics (accessor variety and entropic efficiency) as gradient proxies for morphological complexity that do not require expert annotation and are directly related to language modeling difficulty." [section 5] "Table 5 shows that AV recovers the coarse groupings, with ALs having the highest AV... higher AV and η are causally linked to higher PPL." [corpus] Weak/missing. Related work (MorphBPE arXiv:2502.00894) discusses morphological alignment but does not validate AV/η specifically.
- Break condition: If token bigram distributions do not correlate with perplexity across languages in controlled experiments, the mechanism fails.

### Mechanism 2
- Claim: Stem-suffix boundary recall (MorphScore) does not accurately capture morphological alignment for agglutinative languages because it ignores suffix-suffix boundaries.
- Mechanism: MorphScore evaluates only the single stem-suffix boundary per word. In agglutinative languages with multiple morphemes (e.g., Turkish `arabaları`), missing suffix-suffix boundaries fragments morphs across tokens, making them potentially meaningless. This degradation is invisible to stem-suffix-only metrics.
- Core assumption: Missing suffix-suffix boundaries in agglutinative languages causes comparable or worse representational degradation than missing stem-suffix boundaries in fusional languages.
- Evidence anchors: [abstract] "methodological issues, particularly the use of stem-suffix boundary recall (MorphScore)... which does not accurately capture alignment for agglutinative languages." [section 4.1] "missing an agglutinative suffix-suffix boundary is potentially much worse: in wwwww/xxy/y/zz, the yy morph has lost half its length... stem-suffix boundaries are not explanatory for ALs underperforming to FLs." [corpus] Weak. Related papers discuss morphological alignment but don't specifically validate this critique.
- Break condition: If full morphological alignment (all boundaries) does not correlate better with language modeling performance than stem-suffix-only alignment for agglutinative languages, the critique loses force.

### Mechanism 3
- Claim: Coarse morphological groupings hide gradient variation in both morphological complexity and language modeling difficulty, necessitating continuous proxy metrics.
- Mechanism: Binary classifications obscure within-group variation. Languages vary continuously on AV (English 2.12 to Kannada 55.37, Table 9), with agglutinative languages generally higher but fusional languages showing wide ranges (French 2.39 to Polish 4.74 in EuroParl).
- Core assumption: Morphological complexity exists on a continuous spectrum that correlates with language modeling difficulty.
- Evidence anchors: [abstract] "coarse morphological groupings and current evaluation methods are insufficient to reliably answer this question." [section 5] "The expanded language coverage shows even more clearly that coarse groupings hide information... the middle shows the need for a gradient proxy." [corpus] Weak. Related surveys discuss modeling difficulty but don't validate the gradient approach.
- Break condition: If continuous metrics do not provide more predictive power for perplexity than binary groupings, the approach lacks utility.

## Foundational Learning

- Concept: **Subword Tokenization (BPE vs. Unigram LM)**
  - Why needed here: The paper evaluates morphological alignment and notes Unigram tends to be more aligned than BPE. Understanding tokenization is essential for interpreting alignment metrics.
  - Quick check question: Given vocabulary ["un", "happiness", "happy"], how would BPE likely tokenize "unhappiness" compared to a morphologically ideal segmentation?

- Concept: **Perplexity Limitations for Cross-Lingual Comparison**
  - Why needed here: The paper critiques comparing monolingual perplexities across languages as confounded by different tokenizations and test distributions.
  - Quick check question: If Model A achieves perplexity 20 on Spanish and Model B achieves 22 on English with different tokenizers, can you conclude Model A is better? Why not?

- Concept: **Confounding Variables in Causal Inference**
  - Why needed here: The paper identifies 6 confounding factors affecting conclusions. Understanding confounding is essential for valid experimental design.
  - Quick check question: You observe agglutinative languages have higher perplexity than fusional languages. List three alternative explanations (confounders) besides morphological complexity.

## Architecture Onboarding

- Component map:
  Tokenizer (Unigram LM) -> Tokenized Corpus -> Per-token Accessor Counting -> Left/Right Accessor Sets (AL, AR) -> AV, TA, AU, η per token (1000-accessor window) -> Aggregated metrics per language -> Correlation analysis

- Critical path:
  1. Train monolingual Unigram tokenizers on ~1 GiB language data with 50k vocabulary.
  2. Apply pretokenization (word boundaries) for morphological analysis mode.
  3. For each vocabulary token, compute accessor variety and entropic efficiency using sliding 1000-accessor windows.
  4. Filter punctuation/digit tokens and high boundary-ratio tokens (lexicalization filter).
  5. Aggregate per-language metrics and compare to morphological proxies.

- Design tradeoffs:
  - **With vs. without pretokenization**: With isolates intra-word token bigrams (morphology); without captures all transitions (LM difficulty).
  - **Window size (1000 accessors)**: Balances stability vs. sensitivity—empirically chosen.
  - **Token vs. word level**: Tokens match LM architecture; words match linguistic units.

- Failure signatures:
  - High lexicalization ratio (>50%) suggests tokenizer storing whole words—vocabulary/data mismatch.
  - Low accessor variety across all languages suggests insufficient corpus size.
  - High domain sensitivity (EuroParl vs. FineWeb differ in Figure 2)—ensure consistent domain for comparisons.
  - AU ≈ 1.0 with low TA indicates sparse sampling (unreliable), not true uniformity.

- First 3 experiments:
  1. **Reproduce AV/η**: Compute accessor variety and entropic efficiency for your tokenizer on 200k lines. Compare to Table 9 values.
  2. **Full alignment evaluation**: Using MorphyNet segmentations, compute full boundary F1 (not just stem-suffix). Compare suffix-suffix recall between language types.
  3. **Perplexity correlation test**: Train small LMs on controlled data for 10-15 languages spanning the morphological spectrum. Correlate AV/η with perplexity and compare to binary grouping predictive power.

## Open Questions the Paper Calls Out
None

## Limitations
- The critique of MorphScore relies on theoretical arguments about agglutinative suffix-suffix boundaries but lacks empirical validation showing full alignment correlates better with language modeling performance
- The proposed token bigram metrics show promise but their relationship to perplexity requires further empirical testing across controlled language modeling experiments
- Corpus size effects on these metrics are not fully characterized, and the 1000-accessor window implementation details remain underspecified for exact reproduction

## Confidence
**High Confidence:**
- The theoretical critique of MorphScore's inability to capture suffix-suffix boundaries in agglutinative languages
- The observation that coarse morphological groupings hide gradient variation in complexity
- The general principle that language modeling difficulty relates to prediction uncertainty

**Medium Confidence:**
- The proposed token bigram metrics as reliable gradient proxies for morphological complexity
- The causal link between higher AV/η and higher perplexity
- The claim that binary morphological classifications are insufficient for reliable conclusions

**Low Confidence:**
- The precise implementation details of the 1000-accessor windowing approach
- The universality of AV/η across different domains and corpus sizes
- The comparative predictive power of continuous metrics versus binary groupings without controlled experiments

## Next Checks
1. **Controlled perplexity correlation experiment**: Train small language models (2-10M parameters) on identical datasets for 10-15 languages spanning the morphological spectrum. Compute both AV/η and MorphScore for each language, then measure correlation between each metric and resulting perplexity. This will validate whether the proposed metrics better predict language modeling difficulty than existing approaches.

2. **Full alignment boundary evaluation**: Using MorphyNet segmentations, compute full morphological boundary F1-score (capturing all morpheme boundaries, not just stem-suffix) for the same set of languages. Compare the correlation between full alignment scores and perplexity versus the correlation between stem-suffix-only MorphScore and perplexity, specifically for agglutinative languages.

3. **Corpus size sensitivity analysis**: Systematically vary corpus size (100k, 500k, 1M tokens) for 3-4 languages representing different morphological types. Measure stability of AV, η, and perplexity across sizes to characterize the minimum viable corpus size and identify potential corpus-dependent biases in the metrics.