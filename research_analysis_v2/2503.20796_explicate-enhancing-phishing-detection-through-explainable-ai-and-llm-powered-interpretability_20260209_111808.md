---
ver: rpa2
title: 'EXPLICATE: Enhancing Phishing Detection through Explainable AI and LLM-Powered
  Interpretability'
arxiv_id: '2503.20796'
source_url: https://arxiv.org/abs/2503.20796
tags:
- phishing
- detection
- explanations
- explicate
- emails
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EXPLICATE is an explainable AI phishing detection framework combining
  machine learning, LIME/SHAP interpretability, and DeepSeek v3 LLM enhancement. The
  framework addresses the black-box nature of existing phishing detectors by providing
  transparent explanations alongside high-accuracy detection (98.4% accuracy, precision,
  recall, and F1-score).
---

# EXPLICATE: Enhancing Phishing Detection through Explainable AI and LLM-Powered Interpretability

## Quick Facts
- arXiv ID: 2503.20796
- Source URL: https://arxiv.org/abs/2503.20796
- Reference count: 16
- EXPLICATE achieves 98.4% accuracy in phishing detection with explainable AI using logistic regression and DeepSeek v3 LLM enhancement

## Executive Summary
EXPLICATE addresses the critical challenge of black-box phishing detection systems by integrating machine learning with explainable AI techniques. The framework combines logistic regression with domain-specific features, dual-explanation methods (LIME and SHAP), and LLM-powered natural language synthesis to achieve high detection accuracy (98.4%) while providing transparent explanations. The system is deployed as both a comprehensive GUI application and a lightweight Chrome extension, making it suitable for both detailed analysis and real-time protection. EXPLICATE demonstrates that interpretable phishing detection can achieve performance on par with deep learning approaches while enhancing user trust through meaningful explanations.

## Method Summary
EXPLICATE employs a logistic regression classifier using handcrafted phishing-specific features including linguistic patterns, URL characteristics, header anomalies, and contextual indicators. The framework implements dual-explanation techniques with LIME providing word-level attributions and SHAP offering concept-level feature contributions. A Feature Mapper component translates technical feature names into human-readable security terminology. The DeepSeek v3 LLM synthesizes natural language explanations from the technical XAI outputs, supporting four explanation modes (Detailed/Educational/Technical/Simple). The system processes emails through preprocessing, feature extraction, prediction, and explanation synthesis stages, delivering results with confidence scores and highlighted indicators.

## Key Results
- Achieves 98.4% accuracy, precision, recall, and F1-score in phishing detection
- Generates natural language explanations with 94.2% accuracy and 96.8% consistency to model predictions
- Successfully deployed as both GUI application (1.2 sec/email, 245 MB) and Chrome extension (0.8 sec/email, 128 MB)
- Error analysis identifies false negatives primarily in brand impersonation (40.9%) and social engineering narratives (33.9%)

## Why This Works (Mechanism)

### Mechanism 1: Domain-Specific Feature Engineering with Interpretable Classifier
The framework extracts four categories of features—linguistic patterns, URL characteristics, header anomalies, and contextual indicators—then applies logistic regression whose weights can be directly inspected to understand each feature's contribution. This achieves detection performance comparable to deep learning while maintaining full interpretability of decision boundaries.

### Mechanism 2: Complementary Dual-Explanation via LIME and SHAP Integration
LIME perturbs input emails by removing words/phrases and observes prediction changes to identify locally influential terms, while SHAP computes Shapley values representing each feature's marginal contribution to the prediction relative to a baseline. Together they provide stronger explanations combining word-level and concept-level insights.

### Mechanism 3: LLM-Powered Technical-to-Natural Language Translation
DeepSeek v3 transforms technical XAI outputs into accurate, actionable natural language explanations with 94.2% fidelity to model reasoning. The LLM receives structured inputs including original email content, LIME/SHAP attributions, explanation mode, and domain guidelines to synthesize context-appropriate natural language descriptions.

## Foundational Learning

- **Concept: SHAP (SHapley Additive exPlanations)**
  - Why needed here: SHAP provides mathematically-grounded feature importance by computing each feature's marginal contribution using cooperative game theory, enabling concept-level explanations like "urgency keywords contributed +0.15 toward phishing classification."
  - Quick check question: If SHAP assigns a positive value to a feature for a phishing prediction, does this mean the feature increases or decreases the probability of phishing classification?

- **Concept: LIME (Local Interpretable Model-agnostic Explanations)**
  - Why needed here: LIME generates word-level explanations by perturbing the input (removing words) and fitting a simple local linear model, revealing which specific terms drove the classification for that particular email.
  - Quick check question: Why does LIME create a "locally faithful" approximation rather than explaining the global model behavior?

- **Concept: Logistic Regression Decision Boundaries**
  - Why needed here: The paper deliberately selects logistic regression over neural networks because its linear decision boundary allows direct inspection of feature coefficients—each weight directly indicates a feature's influence on the final prediction.
  - Quick check question: If the coefficient for "suspicious_url" is +2.3, how does a one-unit increase in this feature affect the log-odds of phishing classification?

## Architecture Onboarding

- **Component map:**
  Input Layer (Email ingestion) → Preprocessing Pipeline (Text normalization, email component separation, feature standardization, duplicate removal) → Detection Module (Logistic regression classifier with TF-IDF + 4-category NLP features) → XAI Layer (LIME for word-level + SHAP for concept-level explanations) → Feature Mapper (Technical feature names → human-readable security terminology) → LLM Enhancement (DeepSeek v3 synthesizing natural language explanations in 4 modes) → Output Layer (GUI application or Chrome extension)

- **Critical path:**
  Email input → Preprocessing → Feature extraction → Logistic regression prediction → [If flagged] LIME + SHAP parallel analysis → Feature Mapper translation → LLM explanation synthesis → User-facing output with confidence scores and highlighted indicators

- **Design tradeoffs:**
  - **Accuracy vs. Interpretability**: Logistic regression (98.4%) vs. deep learning (~98.7%)—sacrifices ~0.3% accuracy for full weight interpretability
  - **Depth vs. Deployment Speed**: GUI app (1.2 sec/email, 245 MB) vs. Chrome extension (0.8 sec/email, 128 MB)—streamlined extension for real-time use
  - **Dual Explanations**: Increased computational overhead vs. comprehensive word+concept understanding

- **Failure signatures:**
  - **False negatives (1.2% / 230 cases)**: Brand impersonation with subtle domain variations (40.9%), novel social engineering narratives (33.9%), minimal-text link-only emails (25.2%)
  - **False positives (2.2% / 404 cases)**: Legitimate urgent notifications like password resets (38.6%), context-dependent sensitive information requests (21.3%)
  - **LLM explanation drift**: Consistency dropping below 96.8% indicates LLM generating content not grounded in actual model features

- **First 3 experiments:**
  1. **Feature ablation study**: Compare enhanced model (domain-specific features + TF-IDF) against TF-IDF-only baseline using the same train/test split to isolate the contribution of engineered features to the reported performance gain.
  2. **Explanation consistency validation**: Process the 50-email test set through the full LIME→SHAP→Feature Mapper→LLM pipeline; manually verify that LLM outputs maintain 96.8% consistency with model predictions and that Feature Mapper translations are semantically accurate.
  3. **Cross-dataset generalization test**: Train on subset of dataset sources (e.g., CEAS 08 + Enron + SpamAssassin), evaluate held-out sources (Nigerian Fraud, Original corpus) to assess overfitting to specific phishing patterns and identify transferability gaps.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can advanced feature engineering effectively reduce false negatives in sophisticated brand impersonation attacks where domain variations are subtle?
- Basis in paper: [explicit] The authors state future work will address "sophisticated brand impersonation," which Table VI identifies as the source of 40.9% of false negatives.
- Why unresolved: The current logistic regression model relies on feature extraction methods that may fail to distinguish legitimate corporate emails from phishing emails using subtle domain spoofing.
- What evidence would resolve it: Improved recall rates on a benchmark dataset specifically curated for brand impersonation and typo-squatting attacks.

### Open Question 2
- Question: How can the model distinguish legitimate urgent communications from phishing attempts that mimic urgency without increasing false negatives?
- Basis in paper: [explicit] The error analysis in Section IV.F highlights distinguishing "legitimate urgent notifications" as a necessary area for improvement.
- Why unresolved: LIME explanations indicate "urgency" is a high-weight feature for phishing; legitimate emails using urgent language (e.g., "password reset") are currently prone to misclassification.
- What evidence would resolve it: A sustained F1-score alongside a reduced false positive rate on a test set of legitimate time-sensitive business emails.

### Open Question 3
- Question: To what extent does the static feature set generalize to novel, AI-generated phishing strategies not present in the training data?
- Basis in paper: [inferred] The introduction identifies AI-generated phishing as a growing threat, yet the evaluation relies on historical datasets (e.g., Enron, CEAS 08) lacking modern LLM-crafted attacks.
- Why unresolved: The framework's dependence on pre-defined features and older corpora suggests potential fragility against the evolving linguistic patterns of generative AI phishing.
- What evidence would resolve it: High detection accuracy maintained when tested specifically against a dataset of phishing emails generated by modern LLMs.

## Limitations

- Feature engineering methodology lacks complete specification, particularly exact keyword lists and URL analysis parameters
- LIME/SHAP hyperparameter configuration and DeepSeek v3 prompt templates remain unspecified
- Evaluation focuses primarily on detection accuracy without systematic analysis of explanation quality across diverse phishing techniques or adversarial scenarios

## Confidence

- **High Confidence**: The logistic regression + domain features architecture achieving 98.4% accuracy (supported by explicit performance metrics and comparison to deep learning baselines).
- **Medium Confidence**: The LIME+SHAP dual-explanation effectiveness (mechanism described but lacks direct comparative validation against single-method approaches).
- **Medium Confidence**: The DeepSeek v3 translation accuracy (quantitative metrics reported but limited qualitative validation of explanation quality).
- **Low Confidence**: Generalization across novel phishing techniques (tested only on known attack patterns without adversarial evaluation).

## Next Checks

1. **Feature Ablation Study**: Systematically remove domain-specific features while maintaining TF-IDF baseline to quantify their contribution to the 98.4% performance versus simpler approaches.
2. **Cross-Dataset Generalization**: Train on combined CEAS 08 + Enron + SpamAssassin datasets, then test exclusively on Nigerian Fraud and Original corpus emails to assess transferability to unseen phishing patterns.
3. **Adversarial Robustness Test**: Generate minimal perturbations targeting high-weight features (e.g., slightly modify urgency keywords, domain variations) to measure detection degradation and identify exploitable weaknesses.