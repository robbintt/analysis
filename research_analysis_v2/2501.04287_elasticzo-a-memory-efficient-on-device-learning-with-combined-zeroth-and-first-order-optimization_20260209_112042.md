---
ver: rpa2
title: 'ElasticZO: A Memory-Efficient On-Device Learning with Combined Zeroth- and
  First-Order Optimization'
arxiv_id: '2501.04287'
source_url: https://arxiv.org/abs/2501.04287
tags:
- int8
- training
- memory
- full
- elasticzo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the memory inefficiency of traditional backpropagation-based
  training of deep neural networks (DNNs), which requires storing intermediate activations
  and gradients, making it challenging for deployment on memory-constrained edge devices.
  The authors propose ElasticZO, a novel method that combines zeroth-order (ZO) optimization
  with backpropagation (BP).
---

# ElasticZO: A Memory-Efficient On-Device Learning with Combined Zeroth- and First-Order Optimization

## Quick Facts
- **arXiv ID**: 2501.04287
- **Source URL**: https://arxiv.org/abs/2501.04287
- **Authors**: Keisuke Sugiura; Hiroki Matsutani
- **Reference count**: 40
- **Primary result**: ElasticZO achieves accuracy comparable to backpropagation while reducing memory usage by 2.22x on MNIST and executing 1.38x faster than pure zeroth-order methods

## Executive Summary
ElasticZO addresses the memory inefficiency of traditional backpropagation-based training of deep neural networks (DNNs), which requires storing intermediate activations and gradients, making it challenging for deployment on memory-constrained edge devices. The authors propose a novel method that combines zeroth-order (ZO) optimization with backpropagation (BP). ElasticZO trains most of the DNN using ZO, which only requires forward passes and minimal memory, but employs BP for the last few layers to improve accuracy. This hybrid approach effectively addresses the slow convergence and low accuracy of pure ZO methods while maintaining memory efficiency. The authors also introduce ElasticZO-INT8, an 8-bit integer-only version of ElasticZO, which further reduces memory usage and training time. Experimental results on image and point cloud classification datasets demonstrate that ElasticZO and ElasticZO-INT8 achieve accuracy comparable to BP-based training while consuming significantly less memory and executing faster than pure ZO methods.

## Method Summary
ElasticZO is a hybrid training method that combines zeroth-order (ZO) optimization with backpropagation (BP) to reduce memory usage during on-device DNN training. The method partitions an L-layer network at index C, training layers 1 through C using ZO optimization and layers C+1 through L using standard BP. For the ZO layers, ElasticZO uses the MeZO technique that stores only a random seed instead of the full perturbation vector, regenerating perturbations on-demand. The BP portion handles the final classification layers where accurate gradient signals are most critical. ElasticZO-INT8 extends this approach to 8-bit integer arithmetic using sign-based gradient estimation and power-of-2 exponential approximations, achieving inference-level memory usage. The method uses vanilla SGD without momentum or weight decay for both optimization parts.

## Key Results
- ElasticZO achieves 89.78% accuracy on MNIST compared to 89.89% for full BP, with 2.22x memory reduction
- ElasticZO-INT8 achieves 88.92% accuracy on MNIST while using only 8-bit integer operations
- ElasticZO executes 1.38x faster than pure ZO methods on MNIST classification tasks
- On point cloud classification (ModelNet40), ElasticZO maintains competitive accuracy while reducing memory footprint

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid ZO-BP partition improves accuracy over pure ZO with minimal memory overhead.
- Mechanism: Train feature extraction layers with ZO (no activation storage) and final classification layers with BP (accurate gradients for decision boundaries). The ZO gradient $\hat{\nabla}_\theta L(\theta; B) = \frac{L(\theta + \varepsilon z; B) - L(\theta - \varepsilon z; B)}{2\varepsilon} z$ estimates gradients via two forward passes, avoiding backprop overhead for early layers.
- Core assumption: Feature representations can be learned with noisy ZO gradients while final layers require precise BP gradients for task-specific discrimination.
- Evidence anchors:
  - [abstract] "ElasticZO lies in the middle between pure ZO- and pure BP-based approaches... employ BP for the last few layers and ZO for the remaining layers."
  - [Page 4] "ElasticZO can be viewed as an intermediate method between ZO and BP... with C=0 and C=L equivalent to pure BP- and ZO-based training."
  - [corpus] Related work on MCU-based ZO training (Zhao et al.) shows ~30x slower convergence than BP, suggesting pure ZO has accuracy-speed tradeoffs ElasticZO attempts to bridge.
- Break condition: If feature extraction layers require precise gradient signals for domain adaptation, ZO noise may harm transfer learning.

### Mechanism 2
- Claim: MeZO's random seed trick achieves inference-level memory usage by regenerating perturbations on-demand.
- Mechanism: Store only 4-byte random seed $s$ instead of full perturbation vector $z \sim \mathcal{N}(0, I)$. Regenerate $z$ from $s$ when needed for perturbation ($\theta \pm \varepsilon z$) and update ($\theta \leftarrow \theta - \eta g z$).
- Core assumption: Pseudorandom generator state can be efficiently reset without computational overhead dominating savings.
- Evidence anchors:
  - [Page 3] "MeZO saves a random seed s instead of a random vector z... Since z is of the same size as θ (or g), the memory usage of vanilla ZO equals activations plus twice parameters (θ, z)."
  - [Page 4] "PerturbParameters function initializes pseudorandom generator with seed s to regenerate z_l ∼ N(0, I)"
  - [corpus] MeZO (Malladi et al., 2023) is foundational but corpus does not provide independent validation of seed-regeneration overhead analysis.
- Break condition: If seed regeneration cost exceeds backward pass overhead on target hardware, memory savings won't translate to speed gains.

### Mechanism 3
- Claim: INT8-only training via sign-based ZO gradients eliminates floating-point dependency.
- Mechanism: Approximate loss difference sign via integer arithmetic: $g = \text{sgn}(\ell^+ - \ell^-) \in \{-1, 0, +1\}$. Use scaled 8-bit integers $(v_{\text{int8}}, s)$ with power-of-2 approximation of $\exp(x) \approx 2^{47274 \cdot x \cdot 2^{-15}}$.
- Core assumption: Ternary gradient $g \in \{-1, 0, +1\}$ preserves sufficient optimization signal when combined with sparse perturbations.
- Evidence anchors:
  - [Page 7] "We consider a sign of loss difference and use it as a ternary gradient g ∈ {−1, 0, +1}... The sign can be easily obtained by comparing $\sum_j 2^{\tilde{\alpha}_j}$, $\sum_j 2^{\tilde{\beta}_j}$."
  - [Page 9] Table 1 shows INT8* (integer-only) achieves 88.92%/71.02% vs 89.78%/73.98% (FP ZO) on MNIST/Fashion-MNIST.
  - [corpus] No corpus papers address integer-only ZO training; mechanism novelty is unvalidated externally.
- Break condition: If loss landscape has many shallow minima where sign-based updates oscillate, convergence may fail.

## Foundational Learning

### Concept: Zeroth-order optimization (ZO-SGD/SPSA)
- Why needed here: Understand how gradient estimation from loss differences enables memory-efficient training.
- Quick check question: Can you derive the SPSA gradient estimator and explain why $\mathbb{E}[gz] = \mathbb{E}[\nabla_\theta L]$ as $\varepsilon \to 0$?

### Concept: Backpropagation memory profile (activations, gradients, optimizer states)
- Why needed here: Quantify what ElasticZO avoids storing to assess memory savings on your hardware.
- Quick check question: For a 4-layer MLP with batch size 64, what are the relative sizes of activations vs. gradients vs. Adam momentum buffers?

### Concept: Quantization-aware training (QAT) and integer arithmetic
- Why needed here: Implementing ElasticZO-INT8 requires understanding scaling factors, pseudo-stochastic rounding, and overflow handling.
- Quick check question: Why does NITI use the $(v_{\text{int8}}, s)$ representation instead of static 8-bit quantization?

## Architecture Onboarding

### Component map:
- PerturbParameters() -> ZOUpdateParameters() -> ForwardPass() -> BPUpdateParameters() -> PerturbParameters()

### Critical path:
1. Seed generation and storage (4 bytes)
2. Two forward passes with perturbed parameters (dominant runtime: 84-97% per Figure 7)
3. ZO gradient computation (scalar division + sign extraction for INT8)
4. In-place parameter updates (no extra buffers for ZO layers)
5. BP for final layers (requires activation storage for $a_C, ..., a_L$)

### Design tradeoffs:
- Partition point C: Earlier C → more BP → higher accuracy, more memory
- Batch size B: Larger B → faster convergence, more activation memory (dominates at B=256)
- INT8 vs FP32: 1.46-1.60x memory savings, 1.38-1.42x speedup, but ~1-3% accuracy drop
- Sparsity $p_{\text{zero}}$: Higher sparsity reduces update noise but may slow convergence

### Failure signatures:
- Accuracy plateau: ZO gradient noise dominates → reduce perturbation scale $\varepsilon$ or increase BP layers
- Memory OOM at large batch sizes: Activations dominate → use gradient checkpointing or reduce B
- INT8 training divergence: Check scaling exponent stability, ensure $\max(|a_{\text{int32}}|)$ doesn't overflow
- Slow convergence vs BP: Expected (2 forward passes vs 1 forward+backward), but should match Full ZO speed

### First 3 experiments:
1. **Baseline comparison**: Run Full BP, Full ZO, ZO-Feat-Cls1, ZO-Feat-Cls2 on MNIST with LeNet-5. Measure accuracy, peak memory, and time per epoch. Verify ~5-9% accuracy gain of ElasticZO over Full ZO with <2% memory overhead.
2. **Partition sweep**: Vary C from L (Full ZO) to L-3 on Fashion-MNIST. Plot accuracy vs. memory to find elbow point. Expect diminishing returns beyond C=L-2.
3. **INT8 sanity check**: Compare ElasticZO-INT8 with floating-point ZO gradient computation. Measure sign agreement rate (should be ~95% per paper) and assess if accuracy gap is acceptable for your use case.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the optimal partition point C (number of layers trained via ZO vs BP) be automatically determined to balance accuracy and training cost for a given model and hardware?
- Basis in paper: [explicit] "it is an open problem to determine the number of layers trained by BP such that ElasticZO strikes the balance between accuracy and training cost."
- Why unresolved: The paper manually evaluates C = L−1 and C = L−2 configurations but provides no principled method for selecting C across different architectures or memory budgets.
- What evidence would resolve it: An algorithm or heuristic that, given model architecture and memory constraints, outputs an optimal or near-optimal C; validation across multiple DNN architectures showing consistent accuracy-memory tradeoffs.

### Open Question 2
- Question: Can ElasticZO close the remaining accuracy gap to full BP-based training on more complex DNN architectures (e.g., transformers, diffusion models)?
- Basis in paper: [explicit] "there is still an accuracy gap between ElasticZO and BP-based training that need to be addressed in a future work. We also plan to apply ElasticZO to large-scale DNNs including language models and image diffusion models to evaluate the practicality of our approach."
- Why unresolved: Experiments are limited to LeNet-5 and simple MLPs on classification tasks; scaling behavior and accuracy gaps on modern architectures remain unknown.
- What evidence would resolve it: Evaluation on large-scale models (LLMs, vision transformers, diffusion models) showing accuracy within a defined tolerance of BP baselines.

### Open Question 3
- Question: Can dynamic memory allocation based on variable lifetimes substantially reduce the memory footprint of ElasticZO-INT8 beyond the current theoretical analysis?
- Basis in paper: [explicit] "While we assume that all necessary variables are allocated at once and not released during training, they can be dynamically allocated and deallocated as necessary to further optimize the memory usage, which is a future work."
- Why unresolved: Memory analysis (Eqs. 13–15) assumes all buffers remain allocated throughout training; practical implementations could reuse buffers for non-overlapping variables.
- What evidence would resolve it: An implemented ElasticZO-INT8 system with lifetime-aware memory management showing measured memory reduction compared to the static allocation baseline.

## Limitations
- Hybrid partition effectiveness relies on empirical tuning rather than theoretical analysis for optimal layer selection
- INT8-only training stability depends on sign-based gradient estimation which may fail on complex loss landscapes
- Scalability to complex models (transformers, diffusion models) remains unproven with current experimental scope limited to LeNet-5

## Confidence

**High confidence**: Memory savings mechanism (MeZO seed trick) - directly observable and measurable; BP-for-final-layers strategy for accuracy - well-established practice in transfer learning

**Medium confidence**: Hybrid accuracy claims - supported by experiments but sensitive to architectural choices; INT8 implementation - novel approach with limited external validation

**Low confidence**: Generalization claims to arbitrary DNN architectures - insufficient experimental breadth

## Next Checks
1. **Cross-architecture validation**: Test ElasticZO on ResNet-18/34 for CIFAR-10 and MobileNetV2 for ImageNet-1K to assess scalability beyond LeNet-5. Measure accuracy-memory tradeoffs across architectures with different depths and computational patterns.
2. **INT8 stability analysis**: Implement ElasticZO-INT8 on a regression task (e.g., UCI datasets) where loss landscapes are smoother. Track sign agreement rates, gradient variance, and convergence trajectories across different learning rates to identify conditions where ternary updates fail.
3. **Perturbation scale sensitivity**: Systematically vary ε across 3 orders of magnitude on Fashion-MNIST while measuring accuracy, memory usage, and convergence speed. Identify the noise-accuracy tradeoff boundary and determine if there's a theoretically optimal scale based on gradient magnitude statistics.