---
ver: rpa2
title: 'HALO: Robust Out-of-Distribution Detection via Joint Optimisation'
arxiv_id: '2502.19755'
source_url: https://arxiv.org/abs/2502.19755
tags:
- detection
- attacks
- halo
- attack
- robust
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HALO addresses the vulnerability of out-of-distribution (OOD) detection
  systems to adversarial attacks. The core method adapts the TRADES framework to OOD
  detection by jointly optimizing classification and detection objectives with both
  in-distribution and outlier exposure attacks.
---

# HALO: Robust Out-of-Distribution Detection via Joint Optimisation

## Quick Facts
- **arXiv ID**: 2502.19755
- **Source URL**: https://arxiv.org/abs/2502.19755
- **Reference count**: 40
- **Primary result**: State-of-the-art OOD detection with average AUROC improvement of 3.15 (clean) and 7.07 (under attack) vs. next best method

## Executive Summary
HALO addresses the critical vulnerability of out-of-distribution (OOD) detection systems to adversarial attacks. The method extends the TRADES framework to OOD detection by jointly optimizing classification and detection objectives with both in-distribution and outlier exposure attacks. A novel helper-based regularization term improves the clean/robust accuracy trade-off. HALO achieves significant performance gains across multiple datasets and attack settings, demonstrating robustness to transferred attacks and compatibility with existing OOD detection frameworks.

## Method Summary
HALO trains a robust model using joint adversarial training on both in-distribution (ID) and outlier exposure (OE) data. During training, PGD perturbations are generated to maximize KL divergence between clean and perturbed outputs. ID data uses CE loss pushing toward one-hot labels, while OE data uses KL loss pushing toward uniform distribution. The helper term encourages the robust model to be invariant at perturbation strength ε but not beyond, reducing excessive decision boundary margins. The method uses entropy-based detection with the GEN post-processor by default.

## Key Results
- Achieves state-of-the-art AUROC performance across multiple datasets
- Average improvement of 3.15 AUROC points in clean settings
- Average improvement of 7.07 AUROC points under adversarial attacks
- Robust to transferred attacks and compatible with existing OOD frameworks

## Why This Works (Mechanism)

### Mechanism 1
Joint optimization of classification and detection objectives with both ID and OE adversarial training is necessary for robustness against both ID→OOD and OOD→ID attacks. Toy model experiments show that models trained with only one attack type exhibit robustness to that attack but vulnerability to the other.

### Mechanism 2
TRADES framework can be adapted to OOD detection by formulating detection as binary classification with entropy-based scoring. The theoretical bound extends from classification to detection by substituting the detection objective for the classification objective.

### Mechanism 3
Helper-based regularization improves the clean/robust accuracy trade-off, which indirectly improves OOD detection performance due to the correlation between classification accuracy and detection metrics. The helper term uses "helper examples" constructed from non-robust standard model predictions.

## Foundational Learning

- **Out-of-Distribution (OOD) Detection Attacks**: Understanding ID→OOD (causing false rejections) and OOD→ID (causing missed detections) attacks is prerequisite to interpreting robustness claims.
  - Quick check: Given a CIFAR-10 classifier with entropy-based OOD detection, describe how an adversary would craft an ID→OOD attack vs. an OOD→ID attack.

- **TRADES Framework**: HALO's objective function extends TRADES. The hyperparameter β controls the robustness/accuracy trade-off.
  - Quick check: In TRADES, what does the KL regularization term minimize? How does increasing β affect clean vs. robust accuracy?

- **Helper Adversarial Training (HAT)**: The helper term is key for improving the clean/robust trade-off. Understanding helper example and label construction is necessary for implementation.
  - Quick check: How is a helper example x̃ constructed, and where does the helper label ỹ come from? Why does this reduce excessive margin?

## Architecture Onboarding

- **Component map**: CIFAR-10 (ID) -> TIN597 (OE) -> TinyImageNet/CIFAR-100/MNIST/SVHN/Textures/Places365 (OOD) -> ResNet-18 (base classifier) -> Standard model (helper) -> PGD adversary -> HALO training -> GEN post-processor

- **Critical path**:
  1. Train standard (non-robust) model on ID data to generate helper labels
  2. Initialize robust model with same architecture
  3. For each training batch: sample ID/OE data, generate PGD attacks on both, construct helper examples, compute joint loss, update robust model
  4. Use adversarial validation to select final model

- **Design tradeoffs**:
  - η (OE weight): Higher improves AUROC (especially far-OOD) at cost of classification accuracy
  - β (robustness weight): Higher improves robust AUROC under combined attacks but hurts clean AUROC
  - γ (helper weight): Default 0.5; too high or too low degrades robust AUROC
  - Post-processor choice: GEN strongest in experiments; MSP is baseline

- **Failure signatures**:
  - Clean performance collapse: Robustness too high (β large), detection threshold calibration off
  - OOD detection near random (AUROC ~50): Complex datasets (CIFAR-100, TinyImageNet) or both attacks simultaneously
  - Transfer attack vulnerability: If robustness relies on gradient masking
  - Helper term backfire: On some datasets/settings, helper OE term worsens performance

- **First 3 experiments**:
  1. Reproduce toy model (Section IV): Train 4 models (standard, ID-robust, OE-robust, both-robust) on 2D data to verify joint attack training is necessary for dual robustness.
  2. Ablation on CIFAR-10 (Table VIII): Train HALO, HALO w/o HL, and TRADES with comparable robust accuracy. Compare AUROC across attack settings to isolate OE loss and helper loss contributions.
  3. Hyperparameter sweep (Figure 4/5): Vary η, γ, and β independently on CIFAR-10. Plot clean/robust accuracy and average AUROC to identify optimal regions.

## Open Questions the Paper Calls Out

### Open Question 1
What are the root causes of robust OOD detection performance degradation when scaling to larger, more complex datasets, and how can they be mitigated? The paper notes HALO struggles on CIFAR-100 and TinyImageNet, but the class cardinality experiment suggests factors beyond class count drive this degradation.

### Open Question 2
How does HALO perform under novel attack types and threat models beyond the ℓ∞ PGD attacks studied? Only ℓ∞ norm-bounded attacks were evaluated; other threat models remain unexplored.

### Open Question 3
Can HALO's joint optimisation approach be effectively adapted to domains beyond image classification? The method relies on entropy-based detection and PGD attacks tailored to image data; transferability to text, audio, tabular, or time-series domains is unknown.

### Open Question 4
Why does the helper-based outlier exposure loss term degrade performance, and could alternative formulations provide benefits? The helper term improves clean/robust trade-off for ID classification, yet the analogous OE term fails—understanding this asymmetry could unlock further improvements.

## Limitations

- Dataset construction ambiguity, particularly around exact TIN597 subset details
- Complex hyperparameter landscape requiring careful tuning for optimal performance
- Performance degradation on larger, more complex datasets (CIFAR-100, TinyImageNet)
- Theoretical gaps in justification for KL divergence as entropy invariance proxy

## Confidence

- **High Confidence**: Joint ID/OE adversarial training is necessary for robustness against both attack types (supported by toy model experiments)
- **Medium Confidence**: HALO achieves state-of-the-art performance with reported AUROC improvements (context-dependent results)
- **Medium Confidence**: Helper regularization improves clean/robust accuracy trade-off (supported by ablation but correlation needs validation)

## Next Checks

1. Reproduce the 2D toy model ablation to verify that only the model trained with both attack types achieves robust boundaries against both ID→OOD and OOD→ID attacks.

2. Conduct hyperparameter sensitivity analysis on CIFAR-10 to identify optimal regions for η, β, and γ while confirming asymmetric β₁/β₂ settings improve attack-specific robustness.

3. Evaluate HALO's resistance to transferred attacks across different model architectures to verify the approach is not relying on gradient masking.