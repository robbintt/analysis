---
ver: rpa2
title: 'GroundSight: Augmenting Vision-Language Models with Grounding Information
  and De-hallucination'
arxiv_id: '2509.25669'
source_url: https://arxiv.org/abs/2509.25669
tags:
- image
- question
- grounding
- object
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GroundSight, a method that improves Visual
  Question Answering (VQA) by combining text-grounded object localization with retrieval-augmented
  generation (RAG). The key innovation is using a bounding box to localize the object
  most relevant to the question, allowing targeted image cropping and focused retrieval.
---

# GroundSight: Augmenting Vision-Language Models with Grounding Information and De-hallucination

## Quick Facts
- **arXiv ID:** 2509.25669
- **Source URL:** https://arxiv.org/abs/2509.25669
- **Reference count:** 24
- **Primary result:** GroundSight achieves 25.64% VQA accuracy, improving from 22.19% baseline, while reducing hallucination from 65.79% to 13.88%.

## Executive Summary
GroundSight addresses the challenge of combining accurate visual understanding with reliable knowledge retrieval in Visual Question Answering (VQA). The system uses text-guided object localization (via Grounding DINO) to crop images to relevant regions before retrieval, reducing background noise and improving result relevance. A Chain-of-Spot prompting strategy anchors the model's reasoning to its visual interpretation before considering retrieved context. Additionally, selective fine-tuning enables the model to admit uncertainty rather than hallucinate, achieving a truthfulness score of -0.049 (best among tested configurations) while maintaining competitive accuracy.

## Method Summary
GroundSight combines three key innovations: (1) Grounding DINO localizes objects from question text and images, enabling targeted cropping that reduces background noise during retrieval; (2) Chain-of-Spot prompting forces the model to summarize visual understanding before exposure to retrieved context, preventing retrieval from overriding visual grounding; (3) selective fine-tuning teaches the model to output "I don't know" for question types requiring external knowledge, reducing hallucination at the cost of occasional abstention. The system uses Llama-3.2-Vision-11B as the reasoning engine, Meta's image search API for retrieval, and CLIP embeddings with 0.75 similarity threshold for filtering.

## Key Results
- VQA accuracy improved from 22.19% to 25.64% (3.45 percentage point gain)
- Hallucination rate reduced from 65.79% to 13.88%
- Truthfulness score improved to -0.049 (best among configurations tested)
- Chain-of-Spot prompting alone increased accuracy to 24.61% but also increased hallucination to 70.59%

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Targeted image cropping based on text-guided localization improves retrieval relevance by reducing background noise.
- **Mechanism:** The system uses a localizer (Grounding DINO) to predict a bounding box for the object mentioned in the query. By cropping the image to this Region of Interest (ROI) before performing visual search, the retrieval system (CLIP embeddings) focuses solely on the relevant object's features, ignoring background clutter that yields irrelevant results.
- **Core assumption:** The pre-trained localizer can correctly resolve the object of interest from the question text and image context.
- **Evidence anchors:**
  - [abstract] "enables the model to generate a bounding box... allowing for targeted image cropping and focused retrieval. This reduces background noise."
  - [section 4.1] "performing image retrieval directly on the full image would likely yield results focused on street scenes... if the question is 'How many passengers can the red car seat?', a retrieval system unaware of the object of interest will fail."
  - [corpus] "When RAG Hurts" (Neighbor) suggests retrieval can introduce distraction; GroundSight attempts to mitigate this via strict visual filtering.
- **Break condition:** If the localizer identifies the wrong object (e.g., background noise) or the bounding box is too large, the retrieval will fetch irrelevant context, potentially misleading the VLM.

### Mechanism 2
- **Claim:** Chain-of-Spot (CoS) prompting improves accuracy by anchoring the model to its own visual understanding before exposing it to potentially misleading retrieval results.
- **Mechanism:** Instead of answering immediately after retrieval, the model first generates a summary of the visual ROI. This intermediate step forces the model to "commit" to a visual interpretation. If the retrieval returns irrelevant data, the model is less likely to be swayed because it has already grounded its reasoning in the visual summary.
- **Core assumption:** The model's initial visual interpretation is more reliable than the retrieved external context in ambiguous cases.
- **Evidence anchors:**
  - [section 4.5.2] "Chain-of-Spot prompting mitigates this issue by first asking the model to describe what it sees... we have seen the model output the exact same answer as the summary, ignoring RAG results."
  - [table 8] Shows an example where CoS ignores a "Blood soup" retrieval error and answers correctly based on the visual summary.
  - [corpus] "Pixel-Grounded Retrieval" (Neighbor) emphasizes the need for fine-grained visual grounding to couple perception with knowledge.
- **Break condition:** If the model hallucinates during the initial ROI summary, this mechanism hardens the error, making the final answer confidently wrong (see Table 9).

### Mechanism 3
- **Claim:** Fine-tuning for selective abstention improves truthfulness scores by replacing confident hallucinations with explicit uncertainty.
- **Mechanism:** The model is fine-tuned to recognize specific question types (e.g., "who" questions requiring external knowledge) or low-confidence scenarios. Instead of generating a plausible but incorrect answer, it outputs "I don't know." This minimizes the penalty for incorrect answers in the truthfulness metric (range -1.0 to 1.0).
- **Core assumption:** In safety-critical or real-world scenarios, admitting uncertainty is preferable to a likely error.
- **Evidence anchors:**
  - [abstract] "de-hallucination method... reduced the hallucination rate from 65.79% to 13.88% and improves the truthfulness score."
  - [section 4.5.3] "This significant improvement demonstrates the impact of hallucination control... [the model] effectively generating 'I don't know' responses instead of hallucinations."
  - [section 4.5.5] Final GroundSight solution achieved highest truthfulness (-0.049) despite lower accuracy (8.98%) compared to the RAG-only version.
- **Break condition:** If the abstention threshold is too high, the system becomes unhelpful, refusing to answer questions it might have answered correctly.

## Foundational Learning

- **Concept: Intersection over Union (IoU)**
  - **Why needed here:** The paper explores training VLMs (like BLIP-2) to predict bounding boxes. IoU is the loss function/metric used to measure how well the predicted box aligns with the ground truth.
  - **Quick check question:** If a model predicts a box that perfectly contains the object but is twice the size, does the IoU go up or down? (Answer: Down, due to the Union term increasing).

- **Concept: Retrieval-Augmented Generation (RAG)**
  - **Why needed here:** The core architecture relies on fetching external metadata (e.g., price, history) that is not visible in the image. Understanding RAG is necessary to diagnose why retrieval might fail or mislead the model.
  - **Quick check question:** Why is RAG necessary for VQA in this context instead of just a larger VLM? (Answer: VLMs lack up-to-date or specific factual knowledge, e.g., the current price of a specific cereal box).

- **Concept: Hallucination in VLMs**
  - **Why needed here:** The paper explicitly targets the trade-off between helpfulness (answering) and honesty (de-hallucination). You must understand why VLMs generate plausible fictions to appreciate the "I don't know" fine-tuning strategy.
  - **Quick check question:** Does increasing the "temperature" of a VLM generally increase or decrease hallucination risk? (Answer: Generally increase).

## Architecture Onboarding

- **Component map:** Input (Question + Raw Image) -> Localizer (Grounding DINO -> Bounding Box) -> Preprocessor (Image Cropper -> Cropped Image) -> Retriever (Meta Search API -> Text Metadata/Entities) -> Reasoner (Llama-3.2-Vision-11B -> Summary (CoS) -> Final Answer)

- **Critical path:** The **Localization -> Retrieval** link. The paper notes that while localization helps, combining it with RAG can still mislead the model (Table 8/9). The Chain-of-Spot prompt acts as a guardrail here.

- **Design tradeoffs:**
  - **Accuracy vs. Truthfulness:** The "GroundSight (De-hallucination)" configuration achieved the best truthfulness score (-0.049) but dropped accuracy to ~9% because it frequently refused to answer. The "GDINO + CoS" configuration had higher accuracy (25.64%) but high hallucination (-0.45 truthfulness).
  - **Latency:** Chain-of-Spot requires two forward passes for the VLM (one for summary, one for answer), roughly doubling inference time (Section 4.5.2).

- **Failure signatures:**
  - **ROI Drift:** The localizer picks the wrong object (e.g., building instead of car), causing the retriever to return irrelevant city data.
  - **CoS Overconfidence:** The model hallucinates a detail in the summary phase (e.g., "This is a mural of Nat King Cole") and ignores correct retrieval or prior knowledge that would have corrected it (Table 9).

- **First 3 experiments:**
  1. **Ablate the Localizer:** Run the pipeline using the full image vs. the cropped image for retrieval to measure the signal gain from Grounding DINO specifically on "cluttered" vs. "simple" images.
  2. **Prompt Strategy Comparison:** Compare "Direct Answer" vs. "Chain-of-Spot" on a held-out set. Specifically, count how often CoS ignores relevant retrieval vs. ignores irrelevant retrieval (noise filtering).
  3. **Hallucination Threshold Sensitivity:** Retest the fine-tuned model with varying system prompts for "I don't know" to map the curve of Accuracy vs. Hallucination Rate and find a middle ground between the 8.98% accuracy and 25.64% accuracy baselines.

## Open Questions the Paper Calls Out

- **Can Chain-of-Spot prompting be combined with de-hallucination fine-tuning to maintain high accuracy without increasing hallucination?**
  - **Basis in paper:** [explicit] The Conclusion proposes exploring a combination of "Chain-of-Spot prompting with a fine-tuned, de-hallucinated VLM."
  - **Why unresolved:** The paper found CoS increased accuracy but also hallucination, whereas de-hallucination reduced accuracy by increasing the "I don't know" rate.
  - **What evidence would resolve it:** Evaluating a unified agent fine-tuned for both grounding and uncertainty to see if it retains the accuracy of CoS without the hallucination penalty.

- **Can LVLMs be fine-tuned to achieve localization performance (IoU > 0.5) sufficient to replace external detectors like Grounding DINO?**
  - **Basis in paper:** [explicit] Section 3.1.2 suggests "expanding the dataset and unfreezing more vision layers will yield substantial performance gains" but leaves "finetuning models for localization... for future work."
  - **Why unresolved:** The authors' proof-of-concept achieved only 0.45 IoU, necessitating the use of the external Grounding DINO model for the final system.
  - **What evidence would resolve it:** Training results on larger datasets with deeper fine-tuning showing if end-to-end grounding matches external detector performance.

- **Does Chain-of-Spot prompting cause models to over-rely on visual context and ignore correct retrieved information?**
  - **Basis in paper:** [inferred] Section 4.5.2 notes CoS can make models "overly confident" and "override correct prior knowledge," ignoring RAG results.
  - **Why unresolved:** The current system shows a trade-off where focusing on the visual region improves perception but may lead to ignoring external facts that contradict the visual summary.
  - **What evidence would resolve it:** Stress tests where the ground truth answer contradicts the visual ROI summary to measure the error rate.

## Limitations

- **API dependency uncertainty:** The Meta image search API access is unclear, potentially blocking replication if it's not a public endpoint.
- **De-hallucination fine-tuning details missing:** The paper lacks specific implementation details for the de-hallucination fine-tuning (training data, question-type taxonomy, hyperparameters).
- **Performance trade-off:** Chain-of-Spot prompting increases hallucination from 65.79% to 70.59% despite improving accuracy, creating a conflicting trade-off.

## Confidence

- **High Confidence:** The core localization + RAG pipeline (Grounding DINO + CLIP retrieval + Chain-of-Spot prompting) is well-documented and reproducible with available tools.
- **Medium Confidence:** The reported accuracy improvement (25.64% vs 22.19%) and hallucination reduction (13.88% vs 65.79%) are plausible given the methodology, but depend on correct implementation of the de-hallucination component.
- **Low Confidence:** The de-hallucination mechanism's effectiveness is uncertain without access to the training methodology and datasets.

## Next Checks

1. **De-hallucination Mechanism Validation:** Create a small synthetic dataset with question types known to require external knowledge, fine-tune a VLM to output "I don't know" for these, and measure hallucination reduction on a held-out set.

2. **API Dependency Assessment:** Test whether Grounding DINO and CLIP-based retrieval can be run locally or if Meta's API is essential, and benchmark performance impact of alternatives.

3. **Ablation Study Replication:** Reproduce the ablated experiments (full image vs. cropped image retrieval, Direct Answer vs. Chain-of-Spot) on a subset of CRAG-MM to validate the reported signal gains from each component.