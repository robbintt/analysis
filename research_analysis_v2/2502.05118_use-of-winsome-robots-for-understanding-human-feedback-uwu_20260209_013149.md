---
ver: rpa2
title: Use of Winsome Robots for Understanding Human Feedback (UWU)
arxiv_id: '2502.05118'
source_url: https://arxiv.org/abs/2502.05118
tags:
- robot
- feedback
- positive
- cute
- cuteness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how the perceived cuteness of a robot affects
  human feedback quality in reinforcement learning settings. The authors conducted
  a within-subject user study where participants provided binary feedback (positive/negative)
  on robot trajectories while critiquing both a "cute" and a "control" robot.
---

# Use of Winsome Robots for Understanding Human Feedback (UWU)

## Quick Facts
- arXiv ID: 2502.05118
- Source URL: https://arxiv.org/abs/2502.05118
- Reference count: 36
- Primary result: Cute robot faces elicit higher positive-to-negative feedback ratios; Stochastic TAMER mitigates bias in TAMER by injecting negative feedback when human feedback is suboptimally positive

## Executive Summary
This paper investigates how perceived robot cuteness affects human feedback quality in reinforcement learning settings. Through a within-subject user study, participants provided binary feedback on robot trajectories while interacting with both a "cute" robot (adhering to baby schema) and a control robot. The study found that the cute robot received higher cuteness ratings and a higher ratio of positive to negative feedback. To address the positive feedback bias that can arise from this leniency, the authors developed Stochastic TAMER, an adaptation of the TAMER algorithm that introduces negative feedback when users provide consistently suboptimal feedback, improving learning outcomes in simulated Wumpus World environments.

## Method Summary
The study employed a within-subject design where 14 participants critiqued 12 pre-recorded MOVO robot pick-and-place trajectories (6 successful, 6 unsuccessful) while viewing either a cute or control robot face displayed on an iPad. The cute face featured larger eyes positioned lower on the face, while the control had smaller, higher eyes. Participants provided binary feedback (positive/negative) using keyboard inputs. The core algorithmic contribution, Stochastic TAMER, extends TAMER by tracking a "user score" that penalizes positive feedback given to suboptimal actions, and when this score falls below a threshold, stochastically converts positive feedback to negative with increasing probability as suboptimal feedback persists.

## Key Results
- Cute robot received significantly higher cuteness ratings (4.0 ± 0.877) compared to control (2.79 ± 1.25)
- Participants gave a higher ratio of positive to negative feedback for the cute robot (trend toward significance, p=0.061)
- Stochastic TAMER achieved higher episode returns than standard TAMER when given skewed positive feedback distributions in Wumpus World

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Robot aesthetics adhering to the baby schema (larger eyes, lower face placement, larger forehead) increase perceived cuteness, which in turn elevates the ratio of positive to negative feedback from users.
- Mechanism: Visual features aligned with infant-like characteristics trigger protective/caregiving responses in humans, translating into more lenient evaluative feedback during training interactions.
- Core assumption: The affective response to cuteness generalizes from social judgment to quantitative feedback behavior in RL training contexts.
- Evidence anchors:
  - [abstract] "participants gave a higher ratio of positive to negative feedback for the cute robot"
  - [section III.A.2] "Studies have shown that 'cuteness' can illicit strong feelings of protectiveness and positive feelings from the viewer towards the subject"
  - [corpus] Weak direct evidence; neighbor paper "The Influence of Human-like Appearance on Expected Robot Explanations" supports appearance affecting user expectations but not specifically feedback ratios
- Break condition: If users are explicitly trained on feedback calibration or if task difficulty obscures performance assessment, the cuteness-feedback link may weaken.

### Mechanism 2
- Claim: Excessive positive feedback in TAMER-style learning creates "positive reward circuits" where agents become trapped in suboptimal state loops with inflated expected rewards.
- Mechanism: When human trainers provide positive feedback for poor trajectories, the agent's reward model assigns high value to non-goal states; standard TAMER maximizes this modeled reward without environmental grounding, causing policy fixation.
- Core assumption: The human reward model converges before the agent explores sufficiently to discover the bias.
- Evidence anchors:
  - [abstract] "Previous research has shown that humans tend to give more positive than negative feedback, which can cause failure to reach optimal robot behavior"
  - [section II.B] "Too much positive feedback may introduce positive reward circuits where an MDP agent can become stuck in a loop due to a circuit with an infinite expected reward"
  - [corpus] Not directly addressed in neighbors; mechanism is internal to TAMER literature
- Break condition: If the environment provides dense, unambiguous state signals that contradict human feedback, or if exploration noise is sufficiently high, agents may escape these circuits.

### Mechanism 3
- Claim: Stochastic TAMER mitigates positive feedback bias by monitoring user feedback quality and probabilistically injecting negative feedback when users are assessed as suboptimally positive.
- Mechanism: A user score tracks alignment between feedback and environmental reward; when the score falls below threshold, positive feedback is stochastically converted to negative with probability p, which increases as suboptimal feedback persists—breaking reward circuits through forced exploration.
- Core assumption: Environmental reward is available during training to assess feedback optimality (or a proxy ground-truth exists).
- Evidence anchors:
  - [abstract] "Stochastic TAMER achieved higher episode returns than standard TAMER when given skewed positive feedback distributions"
  - [section III.D] "if the user score dips below a pre-set threshold, the algorithm will stochastically swap positive feedback for negative with probability p"
  - [corpus] No direct corpus support; this is a novel contribution in the paper
- Break condition: If environmental reward is unavailable (pure human-only RLHF), the user score cannot be computed without alternative grounding.

## Foundational Learning

- Concept: **TAMER (Training an Agent Manually via Evaluative Reinforcement)**
  - Why needed here: The entire intervention modifies TAMER; understanding the baseline algorithm—human provides binary evaluative feedback, agent learns a reward model from this signal—is prerequisite.
  - Quick check question: Can you explain why TAMER does not require an environmental reward function during deployment, and what risk that creates?

- Concept: **Baby Schema (Kindchenschema)**
  - Why needed here: The independent variable manipulation (cute vs. control faces) is designed using Lorenz's baby schema; understanding why large eyes low on the face increases perceived cuteness explains the manipulation's theoretical grounding.
  - Quick check question: What visual features define high baby schema adherence, and why might they evoke caregiving behavior?

- Concept: **Positive Feedback Bias in Human Teachers**
  - Why needed here: The problem Stochastic TAMER addresses stems from documented human tendencies to over-reward; recognizing this as a systematic bias (not random noise) shapes solution design.
  - Quick check question: In Knox and Stone's Tetris experiments, what percentage of participants continued giving positive feedback despite poor agent performance?

## Architecture Onboarding

- Component map:
  Human Feedback Interface -> User Score Module -> Threshold Monitor -> Stochastic Injection Engine -> TAMER Reward Model -> Policy Selector

- Critical path: User feedback → User Score update → Threshold check → (if below) Stochastic swap → Modified feedback to TAMER reward model → Policy update

- Design tradeoffs:
  - **Threshold sensitivity**: Lower thresholds reduce intervention frequency (preserves user agency but risks bias); higher thresholds trigger more corrections (may frustrate users who see their feedback ignored)
  - **Probability p scheduling**: Fixed p is simpler but less adaptive; increasing p over time handles persistent bias but may overcorrect if user improves
  - **Environmental reward requirement**: Stochastic TAMER needs ground-truth signals to compute user score—this breaks pure RLHF settings where no environment reward exists

- Failure signatures:
  - **Overcorrection**: If user score threshold is too aggressive, even optimal feedback triggers swaps, degrading learning
  - **Feedback lag**: User score accumulates slowly; early biased feedback may already have corrupted the reward model before intervention
  - **Non-stationary users**: If users adapt their feedback style mid-session, the user score may be stale

- First 3 experiments:
  1. **Replicate Wumpus World with varying bias levels**: Test Stochastic TAMER at 30%, 50%, 70% positive feedback rates to map the algorithm's robustness range and identify breaking points
  2. **Ablate the user score mechanism**: Compare full Stochastic TAMER against (a) random stochastic injection without user score, and (b) fixed p injection to isolate the contribution of adaptive thresholding
  3. **Pilot with real users on a physical robot**: Deploy both standard TAMER and Stochastic TAMER with the cute robot condition; measure episode returns, user frustration (survey), and time-to-convergence to validate whether algorithmic correction compensates for aesthetic-induced bias in ecologically valid settings

## Open Questions the Paper Calls Out

- **Open Question 1**: Does perceived robotic cuteness cause a statistically significant increase in the ratio of positive to negative user feedback in larger cohorts?
  - Basis in paper: [explicit] The authors found a trend toward higher positive feedback for the cute robot ($p=0.061$) and explicitly state that this "warrants further exploration in future studies" as they could not confidently support their hypothesis H2.
  - Why unresolved: The pilot study had a small sample size ($N=14$), resulting in insufficient statistical power to confirm the hypothesis at the $p < 0.05$ threshold.
  - What evidence would resolve it: A replication of the user study with a significantly larger number of participants to determine if the observed trend holds statistical significance.

- **Open Question 2**: Can reinforcement learning algorithms be made robust against positive feedback bias without introducing instability or requiring knowledge of environmental rewards?
  - Basis in paper: [explicit] The authors note that "More investigation is needed to create a more robust algorithm" and that Stochastic TAMER relies on an environmental reward to calculate a "user score," which may not always be available.
  - Why unresolved: The current implementation of Stochastic TAMER penalizes the user score based on environmental rewards, which assumes access to ground truth that is often absent in real-world human-in-the-loop training scenarios.
  - What evidence would resolve it: A modified Stochastic TAMER algorithm that detects suboptimal feedback using unsupervised methods or self-supervised evaluation rather than relying on pre-defined environmental reward signals.

- **Open Question 3**: Can a wash-out task or alternative experimental design effectively eliminate the order effects and persistence bias observed in within-subject cuteness studies?
  - Basis in paper: [explicit] The authors discuss the confounding factor of order effects, noting that participants rated the second robot lower, and suggest that "there will most likely still be some persistence bias" even with a wash-out task.
  - Why unresolved: The study found that participants appeared to use their experience with the first robot as a baseline for rating the second, and it is unclear if a temporal separation task is sufficient to reset this psychological baseline.
  - What evidence would resolve it: A comparative study of within-subject designs with and without intervening wash-out tasks, or a shift to between-subjects designs, to isolate the aesthetic variable from order-based bias.

## Limitations

- **Manipulation ambiguity**: The paper claims cute robot aesthetics increase positive feedback ratios, but the visual features distinguishing "cute" from "control" faces are not fully specified (beyond "larger eyes, lower on face"). This limits reproducibility and theoretical generalization to other cute designs.
- **Algorithm transparency gaps**: Stochastic TAMER's user score computation and feedback injection parameters (threshold, p scheduling) are underspecified, making direct replication difficult without engineering assumptions.
- **Ecological validity concerns**: Results are based on simulated Wumpus World trajectories and pre-recorded robot demonstrations; real-time interaction with a physical robot and more complex tasks may yield different feedback patterns and algorithm performance.

## Confidence

- **High**: Cuteness manipulation effect (cute robot rated significantly higher on cuteness scale; participants gave more positive feedback).
- **Medium**: Positive feedback bias mechanism (theoretical grounding from TAMER literature, but untested for this specific cute-robot condition).
- **Medium**: Stochastic TAMER efficacy (simulation results show improved returns under bias, but real-user validation is absent).

## Next Checks

1. **Parameter sensitivity sweep**: Test Stochastic TAMER across a grid of user score thresholds and probability increase schedules to identify robustness ranges and breaking points.
2. **Real-robot pilot study**: Deploy both TAMER and Stochastic TAMER with the cute robot condition in a physical pick-and-place task; measure convergence time, returns, and user frustration (survey).
3. **Ablation on user score mechanism**: Compare full Stochastic TAMER against (a) random stochastic injection without user score, and (b) fixed p injection to isolate the contribution of adaptive thresholding.