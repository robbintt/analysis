---
ver: rpa2
title: Can large audio language models understand child stuttering speech? speech
  summarization, and source separation
arxiv_id: '2510.20850'
source_url: https://arxiv.org/abs/2510.20850
tags:
- speech
- audio
- child
- language
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper evaluates Large Audio-Language Models (LALMs) on child\
  \ stuttering speech, focusing on two tasks: single-channel source separation to\
  \ isolate the child's voice in mixed interviews, and child-only summarization that\
  \ preserves disfluencies while avoiding adult-speech leakage. Using the FluencyBank\
  \ corpus with 22 children who stutter, the study compares audio-first LALMs (Qwen2-Audio,\
  \ SALMONN, GAMA, Audio Flamingo-3, Kimi Audio) against a text-first baseline (Whisper\
  \ Large\u2192pyannote.audio\u2192Llama-3.2)."
---

# Can large audio language models understand child stuttering speech? speech summarization, and source separation

## Quick Facts
- arXiv ID: 2510.20850
- Source URL: https://arxiv.org/abs/2510.20850
- Authors: Chibuzor Okocha; Maya Bakri; Christan Grant
- Reference count: 32
- Large Audio-Language Models (LALMs) can effectively summarize child stuttering speech from mixed audio, with Audio Flamingo-3 achieving highest Purity (4.73±0.10) and overall quality (3.43±0.39) scores.

## Executive Summary
This paper evaluates Large Audio-Language Models (LALMs) on child stuttering speech summarization, comparing audio-first models against a text-first baseline. Using the FluencyBank corpus with 22 children who stutter, the study finds that modern LALMs can isolate child speech from mixed interviews and preserve clinically relevant disfluencies. Audio Flamingo-3 achieved the highest overall performance, demonstrating that audio-first approaches outperform traditional pipelines that rely on separate transcription and diarization steps.

## Method Summary
The study uses the FluencyBank corpus containing interviews and reading samples from 22 children who stutter. Five audio-first LALMs (Qwen2-Audio, SALMONN, GAMA, Audio Flamingo-3, Kimi Audio) are compared against a text-first baseline using Whisper Large for transcription followed by pyannote.audio for diarization and Llama-3.2 for summarization. Evaluation combines LLM-as-Judge ratings on Purity, Faithfulness, Fluency, and Usefulness metrics, human expert ratings, and BERTScore (F1). A clinical persona prompt is used to preserve disfluencies.

## Key Results
- Audio Flamingo-3 achieved the highest LLM-judge scores (Overall=3.43±0.39, Fluency=4.28±0.30, Faithfulness=3.07±0.30, Purity=4.73±0.10, Usefulness=3.43±0.15) for interview task
- BERTScore results showed Audio Flamingo-3 at F1=0.233 [0.202, 0.262] for interview and SALMONN at F1=0.299 [0.217, 0.383] for reading
- Inter-judge agreement between Llama-3.2 and Mistral reached Pearson r=0.80, κ=0.42 for interviews

## Why This Works (Mechanism)

### Mechanism 1: Implicit Source Separation via Cross-Modal Attention
Audio-first LALMs isolate child voice through attention mechanisms that distinguish acoustic profiles rather than explicit diarization. The model processes interleaved child-adult audio and learns to suppress adult waveforms during token generation, achieving high "Purity" scores. This works when the audio encoder can resolve overlapping speaker frequencies, but fails when speakers share similar acoustic characteristics.

### Mechanism 2: Clinical Persona Steering for Disfluency Preservation
Prompting with "You are a clinical Speech–Language Pathologist" shifts generation from fluency/normativity to clinical accuracy, reducing over-normalization of stuttering events. The model conditions generation on the clinical semantic field, treating disfluencies as relevant signal features rather than noise. This depends on pre-training data containing sufficient clinical text linking stuttering events to descriptions.

### Mechanism 3: Rubric-Grounded Evaluation via LLM-as-Judge
Decomposing evaluation into specific axes (Purity, Faithfulness, Fluency) allows text-only LLM to proxy human judgment more reliably than aggregate metrics like BERTScore. Forcing structured JSON output with explicit rationales for each metric approximates categorical reasoning of human experts. This assumes the judge LLM can adhere to strict definitions without being distracted by fluency.

## Foundational Learning

- **Concept: Speaker Diarization**
  - Why needed here: The text-first baseline relies on this (pyannote.audio) to separate speakers before summarization. Understanding its limitations explains why audio-first approaches performed better on purity.
  - Quick check question: How does the system identify *who* spoke *when* in a mixed audio file?

- **Concept: Disfluency Phenomena (Stuttering)**
  - Why needed here: The core domain challenge. One must distinguish between content words and disfluencies (blocks, prolongations) to know what to "preserve" vs. what is noise in standard ASR contexts.
  - Quick check question: What is the difference between a "block" and a "pause" in acoustic signal processing?

- **Concept: Audio-Text Alignment (Projection)**
  - Why needed here: LALMs function by projecting audio features into the LLM's embedding space. Understanding this "adapter" or "projector" layer is key to understanding how raw stuttering audio becomes a text token the LLM can reason about.
  - Quick check question: Does the model transcribe the stutter explicitly (e.g., "I-I-I want") or encode it as a latent feature?

## Architecture Onboarding

- **Component map:** Input Audio + System Prompt -> Audio Encoder -> Projector -> LLM Backbone -> Summary
- **Critical path:** 1) Verify audio sample rate (usually 16kHz) matches model requirements. 2) Inject "SLP Persona" prompt to enforce disfluency preservation. 3) Generate summary; parse for "adult-speech leakage" (content not attributable to child).
- **Design tradeoffs:**
  - Audio-First (AF3/Qwen): Higher "Purity" and context awareness, but opaque reasoning. Lower semantic overlap (BERTScore) despite higher human ratings.
  - Text-First (Whisper->Llama): Interpretable pipeline, but error propagation is severe; Whisper struggles with child acoustics.
- **Failure signatures:**
  - Adult-Speech Leakage: Summary includes interviewer questions as if child said them (Low Purity)
  - Over-Normalization: Summary says "The child spoke about X" without mentioning the stutter
  - Factual Drift: Summary contains details not present in the audio (Hallucination)
- **First 3 experiments:**
  1. Run the "Text-First" pipeline (Whisper Large) on FluencyBank Interview set and measure WER specifically on disfluencies vs. normal speech
  2. Run AF3 on Interview set with and without "Speech–Language Pathologist" system prompt; measure change in "Faithfulness" scores
  3. Compare Llama-3.2-as-Judge scores against Human Expert scores on subset (n=10) to verify "Purity" logic locally

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LALMs maintain high performance in child stuttering speech summarization across diverse accents and languages?
- Basis in paper: The conclusion states future work aims to "broaden the benchmark to include more accents and languages" to test generalizability.
- Why unresolved: The current study is restricted to the FluencyBank English corpus (specifically 22 children from the US), leaving cross-linguistic and cross-acoustic robustness untested.
- What evidence would resolve it: Evaluation of LALMs on multilingual child stuttering datasets showing comparable BERTScore and LLM-judge ratings to the English baseline.

### Open Question 2
- Question: Can specific prompting strategies or feedback mechanisms prevent factual drift and the erasure of natural disfluencies in LALM summaries?
- Basis in paper: The authors note that "challenges remain in preserving natural disfluencies and avoiding small factual drifts" and suggest refining prompting strategies as future work.
- Why unresolved: While Audio Flamingo-3 produced coherent summaries, it struggled with faithfulness and purity, and the study did not test specific interventions to mitigate these specific errors.
- What evidence would resolve it: Ablation studies comparing standard prompts against disfluency-aware constrained decoding or RLHF, showing improved preservation without loss of coherence.

### Open Question 3
- Question: Why do standard semantic similarity metrics like BERTScore fail to correlate with human expert ratings in disfluent child speech summarization?
- Basis in paper: The results show a "partial divergence" where Audio Flamingo-3 achieved high human/LLM-judge scores (Overall ~3.43) but very low BERTScore (F1 ~0.23), suggesting surface metrics miss perceptual qualities.
- Why unresolved: The paper reports the discrepancy but does not investigate the linguistic or acoustic features causing BERTScore to underestimate quality in this specific domain.
- What evidence would resolve it: Development of specialized evaluation metric for child disfluency that correlates strongly (Pearson r > 0.7) with human SLP ratings, potentially incorporating phonetic or prosodic features.

## Limitations
- Small sample size (22 children) and reliance on single FluencyBank corpus may limit generalizability to broader stuttering populations
- LLM-as-judge evaluation depends on judge model's understanding of clinical disfluency concepts, which may not align perfectly with human expert judgment
- Study does not address potential bias in audio encoders' ability to process diverse child voices across age, gender, or dialect variations

## Confidence

- **High Confidence:** Audio-first LALMs (specifically Audio Flamingo-3) demonstrate superior ability to isolate child speech from mixed interview audio compared to text-first baselines. The Purity score differences are statistically significant and reproducible.
- **Medium Confidence:** The clinical persona prompt effectively preserves disfluencies in summaries, though mechanism relies on unverified pre-training data content.
- **Medium Confidence:** LLM-as-judge evaluation correlates well with human experts (r=0.56, κ=0.36), but correlation may be task-specific and not generalize to other contexts.

## Next Checks

1. **Cross-Corpus Generalization Test:** Evaluate the same audio-first LALMs on at least two additional stuttering corpora to verify that high Purity and Faithfulness scores are not corpus-specific artifacts.

2. **Clinical Expert Validation Study:** Have 3-5 certified SLPs independently rate a random subset of 30 summaries (15 from AF3, 15 from text-first) on standardized clinical rubric. Compare these ratings against both LLM-as-judge scores and original human expert ratings.

3. **Disfluency Type Classification:** Use forced alignment or phonetic transcription tools to annotate specific disfluency types in original audio, then measure whether summaries actually preserve these distinctions or merely indicate "stuttering occurred."