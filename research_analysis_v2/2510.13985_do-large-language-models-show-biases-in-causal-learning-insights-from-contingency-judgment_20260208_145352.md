---
ver: rpa2
title: Do Large Language Models Show Biases in Causal Learning? Insights from Contingency
  Judgment
arxiv_id: '2510.13985'
source_url: https://arxiv.org/abs/2510.13985
tags:
- causal
- 'true'
- medicine
- recovered
- illness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether large language models (LLMs) exhibit
  causal illusions - the tendency to infer causal relationships without sufficient
  evidence. The authors construct 1,000 null-contingency scenarios in medical contexts
  and evaluate three LLMs (GPT-4o-Mini, Claude-3.5-Sonnet, and Gemini-1.5-Pro) on
  their ability to judge cause-effect relationships.
---

# Do Large Language Models Show Biases in Causal Learning? Insights from Contingency Judgment

## Quick Facts
- arXiv ID: 2510.13985
- Source URL: https://arxiv.org/abs/2510.13985
- Reference count: 40
- Key outcome: All three tested LLMs (GPT-4o-Mini, Claude-3.5-Sonnet, Gemini-1.5-Pro) systematically infer unwarranted causal relationships in null-contingency scenarios, showing strong susceptibility to illusion of causality

## Executive Summary
This study investigates whether large language models exhibit causal illusions - the tendency to infer causal relationships without sufficient evidence. The authors construct 1,000 null-contingency scenarios in medical contexts and evaluate three LLMs (GPT-4o-Mini, Claude-3.5-Sonnet, and Gemini-1.5-Pro) on their ability to judge cause-effect relationships. All three models systematically inferred unwarranted causal relationships, showing strong susceptibility to the illusion of causality. GPT-4o-Mini showed the highest illusion (mean rating of 75.74), followed by Claude-3.5-Sonnet (40.54), and Gemini-1.5-Pro (33.07). Statistical tests confirmed that all models' responses were significantly above zero (the correct answer for null contingencies), with p < 0.001 for each model. The results suggest that LLMs reproduce causal language without true understanding, raising concerns about their use in domains requiring accurate causal reasoning.

## Method Summary
The authors generated 1,000 null-contingency scenarios across medical contexts using 100 variable pairs in four categories (fabricated names, indeterminate variables, alternative medicine, established drugs). Each scenario contained 20-100 trials formatted as patient records with binary cause and outcome variables. The experiments tested three LLMs (GPT-4o-Mini, Claude-3.5-Sonnet, and Gemini-1.5-Pro) across three settings: temperature=1 with 10 repetitions, temperature=0 with 1 repetition, and default temperature with 1 repetition. Models were prompted to act as doctors or researchers evaluating experimental treatments and rate effectiveness on a 0-100 scale. The null-contingency structure ensured P(outcome|cause) = P(outcome|¬cause), making the normative response 0.

## Key Results
- All three models produced mean ratings significantly above zero in null-contingency scenarios (p < 0.001 for each model)
- GPT-4o-Mini showed highest susceptibility with mean rating of 75.74, followed by Claude-3.5-Sonnet (40.54) and Gemini-1.5-Pro (33.07)
- Models showed no reduction in bias when using invented medical terms versus real medical terms
- Gemini-1.5-Pro showed 20.5% zero-response rate versus 0% for GPT-4o-Mini, indicating different failure modes

## Why This Works (Mechanism)

### Mechanism 1
LLMs fail to compute normative contingency (ΔP) and instead rely on outcome frequency heuristics. In null-contingency scenarios, ΔP = P(outcome|cause) − P(outcome|¬cause) = 0. The paper constructed trials where recovery rates were equal with and without treatment (e.g., 40% vs. 40%). LLMs appear to weight "cause present + outcome present" trials disproportionately, ignoring the comparison with the baseline rate. This mirrors the human "outcome density" bias where frequent outcomes increase causal ratings regardless of contingency.

### Mechanism 2
LLMs lack shared or consistent internal criteria for causal judgment. The three models produced significantly different distributions (Friedman test χ² = 1516.99, p < 0.001), with GPT-4o-Mini at mean 75.74, Claude-3.5-Sonnet at 40.54, and Gemini-1.5-Pro at 33.07. Pairwise Wilcoxon tests confirmed no two models share the same central tendency. This suggests each model encodes different implicit criteria for causality rather than converging on the normative ΔP = 0 rule.

### Mechanism 3
Causal language reproduction substitutes for genuine causal reasoning. The paper argues LLMs reproduce statistical patterns from human-written text, where causal language often appears without rigorous contingency analysis. Humans form causal illusions through experience, but LLMs acquire them through text. The 20.5% zero-response rate for Gemini vs. 0% for GPT-4o-Mini suggests some models may encode partial safeguards, but high variance (SD 23.72) indicates instability rather than principled rejection.

## Foundational Learning

- Concept: **Contingency (ΔP)**
  - Why needed here: Understanding the core normative principle that causal strength = P(outcome|cause) − P(outcome|¬cause). The paper's null-contingency scenarios have ΔP = 0.
  - Quick check question: If 50/100 patients recover with treatment and 30/60 recover without it, what is ΔP?

- Concept: **Outcome Density Bias**
  - Why needed here: The human cognitive bias where higher base rates of the outcome (regardless of cause) inflate causal judgments. LLMs appear to exhibit this same pattern.
  - Quick check question: In a scenario with 80% recovery rate both with and without treatment, would outcome density predict high or low causal ratings?

- Concept: **Null-Contingency Detection**
  - Why needed here: Recognizing when statistical evidence is insufficient to claim causation. This is the correct response the models failed to produce.
  - Quick check question: What pattern in a 2×2 contingency table indicates a null contingency?

## Architecture Onboarding

- Component map: Scenario generation -> Prompt construction -> Model inference -> Response extraction -> Statistical analysis
- Critical path: 1. Scenario generation → 2. Prompt construction (role assignment + trial list + rating scale instruction) → 3. Model inference → 4. Response extraction → 5. Statistical analysis (Wilcoxon, Friedman, Cochran's Q)
- Design tradeoffs:
  - Temperature settings: Higher temperature (1.0) reveals response variance; lower (0) shows deterministic behavior—paper tested both
  - Variable categories (invented vs. real medical terms): Tests whether prior knowledge interferes; results showed no reduction in bias for invented terms
  - Scale format (0-100 vs. binary): Numeric scales capture gradations but may introduce anchoring biases (models may avoid extremes)
- Failure signatures:
  - Mean ratings significantly above 0 in null-contingency scenarios (primary failure mode)
  - High between-model variance (no shared criteria)
  - Zero-response rate as indicator: Gemini's 20.5% zeros vs. GPT-4o-Mini's 0% suggests different failure modes
  - Category-independent bias: Invented variables (e.g., "Glimber medicine") showed similar bias to real medical terms
- First 3 experiments:
  1. Replicate with temperature = 0 to confirm deterministic failure mode (paper shows consistent results across temperatures)
  2. Add chain-of-thought prompting: "First compute P(recovery|treatment), then P(recovery|no treatment), then compare" to test whether explicit reasoning improves performance
  3. Test positive and negative contingency scenarios (not just null) to assess whether models can distinguish genuine causation from null cases

## Open Questions the Paper Calls Out

- Would chain-of-thought (CoT) prompting reduce LLMs' susceptibility to causal illusions in contingency judgment tasks?
- How do LLMs perform on positive and negative contingency scenarios compared to null-contingency cases?
- Does the order of trial presentation influence LLM causal judgments in contingency tasks?
- Would alternative response formats (binary or multi-class) reduce the bias toward positive contingency judgments in LLMs?

## Limitations

- The paper only tests null-contingency scenarios and cannot assess whether models can correctly identify genuine causal relationships
- The exact algorithm for generating the 80/20 trial distribution across variable scenario sizes is not fully specified
- The complete list of 100 variable pairs is not provided, making exact replication challenging

## Confidence

**High confidence**: The finding that all three models systematically produce ratings above zero in null-contingency scenarios (p < 0.001 for each model). The statistical tests and effect sizes are robust across multiple experimental conditions.

**Medium confidence**: The claim that LLMs reproduce causal language without true understanding. While supported by the data, this interpretation assumes the observed patterns reflect genuine cognitive limitations rather than task-specific failures.

**Low confidence**: The assertion that LLMs acquire causal illusions through training data exposure. The paper provides plausible reasoning but lacks direct evidence about what patterns exist in training corpora.

## Next Checks

1. Chain-of-thought prompting experiment: Add explicit step-by-step reasoning instructions to test whether prompting can elicit normative contingency computation
2. Positive and negative contingency testing: Evaluate models on scenarios with clear positive (ΔP > 0) and negative (ΔP < 0) contingencies to determine whether the bias is specific to null-contingency detection
3. Training data analysis: Examine whether LLMs trained on causal language datasets (versus general text) show reduced susceptibility to causal illusions, helping distinguish between architectural limitations and learned biases