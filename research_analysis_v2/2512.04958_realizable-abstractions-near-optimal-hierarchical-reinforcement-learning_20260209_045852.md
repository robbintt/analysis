---
ver: rpa2
title: 'Realizable Abstractions: Near-Optimal Hierarchical Reinforcement Learning'
arxiv_id: '2512.04958'
source_url: https://arxiv.org/abs/2512.04958
tags:
- abstract
- abstraction
- learning
- which
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Realizable Abstractions, a new formal relation
  between generic low-level Markov Decision Processes (MDPs) and their associated
  high-level decision processes. The abstraction maps each abstract action to a sequence
  of ground actions realized through options, avoiding non-Markovianity issues.
---

# Realizable Abstractions: Near-Optimal Hierarchical Reinforcement Learning

## Quick Facts
- arXiv ID: 2512.04958
- Source URL: https://arxiv.org/abs/2512.04958
- Reference count: 40
- Authors: Roberto Cipollone, Luca Iocchi, Matteo Leonetti
- Primary result: RARL learns compositional, near-optimal low-level policies that are Probably Approximately Correct (PAC), converge in polynomial samples, and are robust to abstraction inaccuracies.

## Executive Summary
This paper introduces Realizable Abstractions, a formal framework linking low-level Markov Decision Processes (MDPs) to high-level abstractions through options. The key innovation is mapping abstract actions to sequences of ground actions realized through options while avoiding non-Markovianity issues. The authors develop RARL, a Hierarchical Reinforcement Learning algorithm that learns near-optimal low-level policies by iteratively refining the abstraction through constrained MDP formulations and updating overly optimistic rewards.

## Method Summary
RARL learns near-optimal policies by treating the realization of abstract actions as a Constrained MDP problem where occupancy measures serve as hard constraints while maximizing reward. The algorithm iteratively samples from the ground MDP to learn options, checking if realized values match abstract predictions and updating the abstraction when overly optimistic rewards are detected. The method requires an initial abstraction with admissible (optimistic) rewards and guarantees polynomial sample complexity under the Probably Approximately Correct framework.

## Key Results
- Proves any abstract policy for a realizable abstraction can be translated into near-optimal policies for the ground MDP
- Shows RARL is Probably Approximately Correct (PAC) and converges in polynomial samples
- Demonstrates robustness to inaccuracies in the abstraction through iterative refinement
- Establishes formal near-optimality guarantees (Corollary 3) when composition of options is applied

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If an abstraction satisfies the $(\alpha, \beta)$-realizability condition, abstract transitions can be mapped to concrete options with bounded error.
- **Mechanism:** The paper defines a Realizable Abstraction where an abstract tuple is realizable if there exists a $\phi$-relative option such that the difference between abstract transition probabilities/rewards and ground occupancy measures is bounded by $\alpha$ and $\beta$.
- **Core assumption:** The mapping function $\phi$ partitions the ground MDP into blocks where valid entry/exit states exist and dynamics allow options satisfying probability constraints.
- **Break condition:** If ground MDP dynamics don't support required transition probabilities between blocks, the mapping fails.

### Mechanism 2
- **Claim:** Composing realized options yields a near-optimal ground policy when abstraction is admissible.
- **Mechanism:** The authors use Theorem 1 to bound value loss between abstract optimal policy and ground realization. Admissibility (optimistic abstraction) ensures abstract planner doesn't underestimate ground values.
- **Core assumption:** Abstraction $\langle \bar{M}, \phi \rangle$ is admissible and discount factors are consistent with realizability constraints.
- **Break condition:** If abstract rewards aren't admissible or abstraction is overly coarse, value loss bound becomes too large.

### Mechanism 3
- **Claim:** RARL learns options using CMDP formulation and corrects overly optimistic abstract rewards.
- **Mechanism:** RARL treats realization as a CMDP problem with occupancy constraints as hard constraints and value as objective. It iteratively samples ground MDP to learn options and updates abstract MDP rewards when overly optimistic predictions are detected.
- **Core assumption:** CMDP solver is PAC-Safe and initial abstraction is admissible.
- **Break condition:** If initial abstract rewards are too inaccurate or realizability assumption is violated during sampling, algorithm may not converge.

## Foundational Learning

- **Concept:** Occupancy Measures
  - **Why needed here:** Realizability is defined by matching discounted probability of visiting states between abstract model and ground option. Understanding $d^\pi$ is essential for constraints in Eq. 8.
  - **Quick check question:** Can you explain how a policy's value can be expressed as the dot product of a reward vector and an occupancy measure?

- **Concept:** Constrained MDPs (CMDPs)
  - **Why needed here:** RARL solves realization problem by enforcing transition probabilities as hard constraints while maximizing reward. Requires understanding CMDPs differ from standard MDPs (feasible set $\Pi_c$).
  - **Quick check question:** In a CMDP, if a policy maximizes reward but violates a constraint by $\eta$, is it considered $\eta$-feasible?

- **Concept:** Options Framework (Precup & Sutton)
  - **Why needed here:** Paper extends standard options framework to $\phi$-relative options tied to specific state blocks. Understanding initiation sets and termination conditions is prerequisite.
  - **Quick check question:** How does a $\phi$-relative option differ from a standard option regarding its termination condition $\beta_o$?

## Architecture Onboarding

- **Component map:** Ground MDP $M$ -> State mapping $\phi$ -> Abstract MDP $\bar{M}$ -> Value Iteration -> Abstract policy $\bar{\pi}$ -> Realizer (CMDP solver) -> Options $\Omega$ -> ABSTRACT-ONER -> Updated abstract rewards

- **Critical path:**
  1. Define state mapping $\phi$ (partitioning ground state space)
  2. Initialize $\bar{M}$ (abstract transitions/rewards) to be admissible (optimistic)
  3. For each abstract state-action pair, run CMDP solver to find realizing option

- **Design tradeoffs:**
  - Abstract Resolution: Coarser abstraction reduces planning complexity but makes realizability constraints harder to satisfy
  - Realizability Parameters ($\alpha, \beta$): Tighter bounds ensure better near-optimality but might make CMDP infeasible if ground MDP is stochastic

- **Failure signatures:**
  - Infeasible CMDP: Solver cannot find option satisfying occupancy constraints
  - Reward Collapse: ABSTRACT-ONER continuously reduces abstract rewards, indicating overestimation of block reachability/value

- **First 3 experiments:**
  1. Grid World Validation: Implement 4-room grid with rooms as abstract states; verify RARL converges faster than flat Q-learning
  2. Stochasticity Stress Test: Increase transition noise; observe if occupancy constraints need relaxation (higher $\beta$) to find feasible options
  3. Ablation on Admissibility: Initialize abstract rewards with pessimistic values; confirm algorithm fails to explore sufficiently

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the sample complexity of RARL be expressed as an instance-dependent bound to better characterize efficiency gains over standard RL?
- **Basis in paper:** Conclusion states future work could express sample complexity as "instance-dependent bound" to highlight when HRL is more efficient.
- **Why unresolved:** Current theoretical bound (Theorem 8) is worst-case and relies on equivalence classes, potentially obscuring practical benefits in specific scenarios.
- **What evidence would resolve it:** Formal derivation of sample complexity depending on specific structure of ground MDP and accuracy of abstraction, rather than worst-case parameters.

### Open Question 2
- **Question:** Can the algorithm be extended to automatically learn the state partition $\phi$ rather than receiving it as fixed input?
- **Basis in paper:** Related work section notes "learning such a state partition remains outside the scope of this work."
- **Why unresolved:** Current framework assumes abstraction structure is provided a priori; learning it online introduces difficult joint optimization problem that may violate realizability constraints.
- **What evidence would resolve it:** Modified RARL that provably discovers state mapping function $\phi$ while maintaining PAC guarantees and near-optimality.

### Open Question 3
- **Question:** Can the requirement that realizations remain valid under changing entry distributions be relaxed or removed?
- **Basis in paper:** Authors note "nuanced dependency" regarding entry distributions which they "omit the treatment of this marginal issue."
- **Why unresolved:** As new options are learned, entry distribution into subsequent blocks shifts; current analysis assumes previously learned options remain valid without verification.
- **What evidence would resolve it:** Theoretical analysis proving robustness to distribution shift without Assumption 3, or mechanism to re-verify/re-learn options when distributions change.

## Limitations

- Requires pre-defined state abstractions without addressing how to discover optimal partitions
- Potential infeasibility of CMDP constraints in highly stochastic environments
- Limited empirical evaluation beyond simple grid-world scenarios

## Confidence

- **Theoretical Framework (Theorems 1-3):** High confidence - formal proofs follow directly from definitions and established MDP theory
- **RARL Algorithm Practical Performance:** Medium confidence - PAC sample complexity bounds provided but lacks extensive empirical validation
- **Practical Applicability:** Medium confidence - relies heavily on feasible CMDP solutions and accurate initial abstractions

## Next Checks

1. Implement RARL on a stochastic grid-world where abstract transitions require specific state visitations; verify that CMDP feasibility degrades gracefully as transition noise increases
2. Test RARL with intentionally inaccurate initial abstractions to measure the algorithm's claimed robustness to reward inaccuracies
3. Compare RARL's sample efficiency and final performance against standard HRL baselines (Option-Critic, HIRO) on a continuous control benchmark like Ant-v3