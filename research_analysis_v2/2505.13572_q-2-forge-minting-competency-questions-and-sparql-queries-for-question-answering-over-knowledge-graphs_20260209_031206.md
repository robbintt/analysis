---
ver: rpa2
title: 'Q${}^2$Forge: Minting Competency Questions and SPARQL Queries for Question-Answering
  Over Knowledge Graphs'
arxiv_id: '2505.13572'
source_url: https://arxiv.org/abs/2505.13572
tags:
- sparql
- query
- q2forge
- queries
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Q\xB2Forge is a pipeline that generates competency questions (CQs)\
  \ and SPARQL queries for knowledge graphs, addressing the challenge of limited training\
  \ data for text-to-SPARQL translation systems. The system iteratively validates\
  \ queries using human feedback and LLM as a judge, offering three core functions:\
  \ CQ generation, SPARQL query generation, and query refinement."
---

# Q${}^2$Forge: Minting Competency Questions and SPARQL Queries for Question-Answering Over Knowledge Graphs

## Quick Facts
- arXiv ID: 2505.13572
- Source URL: https://arxiv.org/abs/2505.13572
- Reference count: 26
- Generates competency questions and SPARQL queries for knowledge graphs using a modular pipeline with LLM-as-judge refinement

## Executive Summary
Q²Forge is a pipeline that generates competency questions (CQs) and SPARQL queries for knowledge graphs, addressing the challenge of limited training data for text-to-SPARQL translation systems. The system iteratively validates queries using human feedback and LLM as a judge, offering three core functions: CQ generation, SPARQL query generation, and query refinement. The approach uses configurable language models and services to create end-to-end question-query pairs tailored to any KG.

## Method Summary
Q²Forge is a modular pipeline that generates competency questions and SPARQL queries for knowledge graphs through a three-stage process: (1) KG configuration and schema pre-processing with class descriptions and embeddings, (2) LLM-based CQ generation, and (3) SPARQL query generation using similarity search over class embeddings with named entity extraction. The system supports iterative refinement where an LLM judges query-question match quality (0-10 scale with explanations) and users manually correct queries until satisfactory. Experiments used DeepSeek-v3 LLM, nomic-embed-text embeddings, and FAISS vector store on two domain-specific KGs (IDSM and D2KAB).

## Key Results
- CQ generation time: 45.6s for D2KAB, 132.4s for IDSM
- SPARQL query generation time: 40.1s for D2KAB, 51s for IDSM
- Supports efficient refinement with context-awareness
- Open-source, modular, and extensible for benchmarking and training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-stage context enrichment improves SPARQL query accuracy over direct LLM translation.
- Mechanism: Instead of passing a natural language question directly to an LLM, the pipeline extracts named entities (via SpaCy), retrieves similar ontology classes using embedding similarity, then fetches property/value-type patterns from actual KG instances before constructing the prompt.
- Core assumption: LLMs generate more accurate SPARQL when grounded in both schema definitions and observed instance patterns from the target KG.
- Evidence anchors:
  - [abstract] "The approach uses configurable language models and services to create end-to-end question-query pairs tailored to any KG."
  - [section 3.3] Scenario 5 workflow steps 3-6 describe entity extraction, class similarity search, context retrieval, and query generation as a staged process.
  - [corpus] Related work (FIRESPARQL, SPARQL-LLM) similarly adopts multi-stage LLM pipelines, but Q²Forge adds instance-level property sampling.
- Break condition: If ontology class descriptions are sparse or embedding quality is poor, class retrieval may return irrelevant schema elements, degrading query accuracy.

### Mechanism 2
- Claim: Iterative human-in-the-loop refinement with LLM-as-judge produces higher-quality question-query pairs than one-shot generation.
- Mechanism: After initial query generation, the system executes the query, interprets results via LLM, and provides a 0-10 relevance grade with explanations. Users manually amend queries and re-grade until satisfied.
- Core assumption: Non-expert users can correct queries when given interpretable feedback and schema labels, even without deep SPARQL expertise.
- Evidence anchors:
  - [abstract] "It iteratively validates those queries with human feedback and LLM as a judge."
  - [section 3.4] "The LLM is asked to judge whether the query matches the given question. It is requested to provide a grade between 0 and 10 along with explanations justifying the grade."
  - [corpus] "Learning to Refine" (arXiv:2511.11770) explores RL-based iterative refinement, suggesting iterative correction is a recognized pattern, though Q²Forge uses manual feedback.
- Break condition: If users lack sufficient domain or SPARQL knowledge to interpret grades and labels, refinement loops may stall without producing valid queries.

### Mechanism 3
- Claim: Pre-processing KG schema with instance sampling bridges the gap between ontology definitions and actual data representations.
- Mechanism: The pipeline extracts class labels/descriptions from ontologies and computes embeddings, but also samples real instances to analyze which properties and value types are actually used in practice, exposing vocabulary usage not explicit in the schema.
- Core assumption: KG instances often use properties from vocabularies not explicitly declared in the primary ontology, and exposing these patterns improves query generation.
- Evidence anchors:
  - [section 3.1.2] "Instances may use properties and resources from additional vocabularies that are not explicitly mentioned in the ontology."
  - [section 3.1.2] "Gen2KGBot samples class instances and analyzes the properties and value types they use."
  - [corpus] Weak direct evidence; no corpus neighbor explicitly compares schema-only vs. schema+instance approaches. This is a potential evaluation gap.
- Break condition: For extremely large KGs (e.g., IDSM with 36B triples), instance sampling may be computationally expensive or miss rare property patterns.

## Foundational Learning

- **SPARQL query structure (SELECT, WHERE, FILTER, OPTIONAL, prefixes)**
  - Why needed here: The pipeline generates, validates, and refines SPARQL queries; understanding basic syntax is required to interpret error messages and refinement suggestions.
  - Quick check question: Can you read a basic SELECT query with a FILTER clause and predict which triples it matches?

- **Knowledge graph fundamentals (RDF triples, ontologies, classes, properties, namespaces)**
  - Why needed here: The system extracts ontology classes, computes embeddings over class descriptions, and maps questions to schema elements.
  - Quick check question: Given a triple `<s> <p> <o>`, can you identify which part represents a class vs. an instance vs. a property?

- **Embedding-based similarity search (vector representations, nearest-neighbor retrieval)**
  - Why needed here: CQ-to-class matching and example retrieval rely on embedding similarity (nomic-embed-text + FAISS in experiments).
  - Quick check question: If two ontology class descriptions are semantically similar, should their embeddings be close in vector space?

## Architecture Onboarding

- **Component map:**
  - Frontend Web UI with CQ generation form, SPARQL editor (YasGUI), refinement interface with grade display
  - Backend Gen2KGBot server exposing REST endpoints for CQ generation, query generation/execution, refinement
  - LLM orchestration LangChain/LangGraph workflows implementing "scenarios" (Scenario 5 for zero-shot, Scenario 6 for few-shot)
  - KG connector rdflib + SPARQL endpoint client; reads ontology TBOX, samples instance ABOX
  - Vector store FAISS index over class description embeddings and few-shot example embeddings

- **Critical path:**
  1. Create KG configuration (endpoint, prefixes, description)
  2. Run pre-processing to extract class descriptions + compute embeddings + sample instances
  3. Generate CQs via LLM (streaming JSON output)
  4. For each CQ: run Scenario 5/6 → validate syntax → execute query → interpret results → LLM grading
  5. User refines query based on grade/explanation → re-execute → add to Q²set
  6. Export validated pairs in QALD-like JSON

- **Design tradeoffs:**
  - **Modularity vs. integration:** Each task (CQ gen, query gen, refinement) can run independently, but end-to-end pipeline requires state passing (browser cookies or server-side storage)
  - **LLM flexibility vs. reproducibility:** Config allows swapping models per step, but results may vary significantly across models; no standardized evaluation yet
  - **Instance sampling depth vs. latency:** Table 1 shows embedding computation takes 10s for D2KAB but 2,592.5s for IDSM (226K classes); sampling strategy is not fully described

- **Failure signatures:**
  - **No classes retrieved:** Embedding similarity threshold too strict or class descriptions missing labels; fallback to broader search needed
  - **Syntactically invalid SPARQL:** LLM hallucinates prefixes or constructs; retry prompt with error message should trigger (step 7 in Scenario 5)
  - **Empty query results:** Query syntactically valid but semantically mismatched; LLM-as-judge should flag low relevance grade
  - **Timeout on large KGs:** Instance sampling or embedding computation stalls; may need sampling limits or pre-computed indices

- **First 3 experiments:**
  1. **Zero-shot CQ generation on a small KG** (e.g., D2KAB subset): Measure time, manually assess CQ relevance and diversity
  2. **Query generation accuracy ablation:** Run Scenario 5 with and without instance-sampled property context; compare syntactic validity and result non-emptiness rates
  3. **Refinement iteration count:** For 20 CQs, track how many human-in-the-loop cycles are needed to reach a grade ≥8; identify common failure patterns (wrong property, missing FILTER, incorrect join)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Q²Forge's performance vary across different LLMs, and what are the resulting trade-offs in cost, latency, and output quality?
- Basis in paper: [explicit] Section 7 states authors will "evaluate Q²Forge's sensitivity to different LLM choices and analyze the resulting trade-offs in cost and latency."
- Why unresolved: Experiments used only DeepSeek-v3; no comparison across models was conducted.
- What evidence would resolve it: Systematic benchmarking across multiple LLMs (e.g., GPT-4, Llama, Mistral) measuring accuracy, execution time, and API costs on identical KGs and question sets.

### Open Question 2
- Question: What are the characteristic failure modes of the SPARQL query generator across complex query patterns involving different SPARQL operators?
- Basis in paper: [explicit] Section 7 commits to "identify and characterize the failure types in Gen2KGBot across a spectrum of challenging queries using the variety of operators available in SPARQL."
- Why unresolved: No failure analysis was performed; only successful query generation times were reported.
- What evidence would resolve it: Categorized error taxonomy from systematic testing with queries using OPTIONAL, UNION, subqueries, property paths, and aggregations.

### Open Question 3
- Question: Can the pipeline be extended to generate valid federated SPARQL queries across multiple knowledge graphs using the SERVICE clause?
- Basis in paper: [explicit] Section 7 notes the "current implementation is limited to single SPARQL endpoints and does not support federated queries" and proposes addressing this limitation.
- Why unresolved: Architecture currently assumes one endpoint; no federation logic implemented.
- What evidence would resolve it: Working prototype generating SERVICE-based queries validated against multi-endpoint test cases.

### Open Question 4
- Question: What is the quality of generated question-query pairs when evaluated against human expert judgments compared to hand-crafted benchmarks?
- Basis in paper: [inferred] Section 5 states "comprehensive evaluation remains as important future work" despite promising initial experiments with only two domain-specific KGs.
- Why unresolved: No systematic human evaluation protocol or comparison to gold-standard Q²sets was conducted.
- What evidence would resolve it: Human expert ratings of generated pairs on correctness, relevance, and complexity compared to existing benchmarks like QALD or LC-QuAD.

## Limitations
- Effectiveness depends heavily on KG metadata quality and LLM performance, which vary across models and domains
- Iterative refinement assumes non-expert users can interpret and act on LLM-as-judge feedback
- Claims about addressing "limited training data" are somewhat overstated as system generates synthetic pairs rather than solving fundamental data scarcity

## Confidence
- **High confidence:** Modular architecture design and implementation details are well-specified and reproducible; time performance metrics are directly measurable
- **Medium confidence:** Iterative refinement with LLM-as-judge is demonstrated but relies on user interpretation; instance sampling benefits are theoretically sound but lack direct empirical comparison
- **Low confidence:** Claims about addressing "limited training data" problem are somewhat overstated

## Next Checks
1. **Ablation study on instance sampling:** Compare SPARQL generation accuracy with and without instance-level property context on at least three diverse KGs to quantify the contribution of this feature
2. **User study on refinement effectiveness:** Recruit 10-15 non-expert users to perform refinement tasks; measure success rates, iteration counts, and qualitative feedback on judge explanations
3. **Scalability benchmark:** Test the pipeline on progressively larger KGs (10M, 100M, 1B+ triples) to identify performance bottlenecks in embedding computation and instance sampling