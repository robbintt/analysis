---
ver: rpa2
title: 'Reward Training Wheels: Adaptive Auxiliary Rewards for Robotics Reinforcement
  Learning'
arxiv_id: '2503.15724'
source_url: https://arxiv.org/abs/2503.15724
tags:
- reward
- learning
- auxiliary
- student
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Reward Training Wheels (RTW) is a teacher-student framework that
  automates auxiliary reward adaptation for robotics reinforcement learning, addressing
  the challenge of manually engineering reward functions for complex robotic tasks.
  The method employs a teacher agent that dynamically adjusts auxiliary reward weights
  based on the student robot's evolving capabilities, optimizing the learning process.
---

# Reward Training Wheels: Adaptive Auxiliary Rewards for Robotics Reinforcement Learning

## Quick Facts
- arXiv ID: 2503.15724
- Source URL: https://arxiv.org/abs/2503.15724
- Reference count: 29
- Primary result: RTW outperforms expert-designed rewards by 2.35% in navigation success rate and 122.62% in off-road mobility performance

## Executive Summary
Reward Training Wheels (RTW) introduces a teacher-student framework for robotics reinforcement learning that automatically adapts auxiliary reward weights based on student performance. The method addresses the challenge of manually engineering reward functions for complex robotic tasks by using a teacher agent to dynamically adjust reward weights as the student robot's capabilities evolve. RTW was evaluated on two challenging robotic tasks: navigation in highly constrained spaces and off-road vehicle mobility on vertically challenging terrain, demonstrating significant improvements over expert-designed reward functions while achieving faster training efficiency.

## Method Summary
RTW employs dual PPO agents where a teacher network adjusts auxiliary reward weights in real-time based on the student's performance history. The teacher observes a history window of past weights, primary rewards, and auxiliary values to determine which reward components need emphasis. The student robot learns from a composite reward signal that combines the primary task reward with weighted auxiliary rewards. The teacher's reward signal is the student's primary reward, creating an incentive to optimize the overall learning process. The framework uses standard PPO implementations with hyperparameters specified in Table I, including learning rates of 3e-4/5e-4 and PPO epochs of 5-10.

## Key Results
- RTW outperforms expert-designed rewards by 2.35% in navigation success rate
- Off-road mobility performance improved by 122.62% with RTW
- Training efficiency improved by 35% for navigation and 3X for off-road mobility
- Physical robot experiments achieved 5/5 success rate vs 2/5 for baseline

## Why This Works (Mechanism)

### Mechanism 1: Curriculum-like Weight Decay
RTW implicitly generates a curriculum by reducing reliance on safety-oriented auxiliary rewards as the student demonstrates competence. The teacher observes performance history (e.g., collision rates) and reduces weights for "training wheel" constraints as the student improves, allowing prioritization of speed and goal progress over conservative safety behaviors.

### Mechanism 2: Credit Assignment via History Windows
The teacher improves credit assignment by conditioning on a history window of past weights and performance metrics rather than instantaneous state. This allows correlation of specific weight configurations with subsequent performance changes, solving a temporal credit assignment problem.

### Mechanism 3: Intrinsic Correlation Discovery
The teacher autonomously identifies and exploits functional relationships between distinct auxiliary rewards without manual specification. By optimizing for the primary objective, the teacher learns to adjust seemingly separate variables (e.g., "speed reward" and "stall prevention reward") in tandem, discovering that both mechanistically encourage forward momentum.

## Foundational Learning

- **Partially Observable Markov Decision Process (POMDP)**: The student robot agent is modeled as a POMDP because it typically receives limited observations rather than full state information, necessitating policies that handle observation uncertainty. Quick check: If the student had access to full state vector, would the framework still require a POMDP formulation?

- **Proximal Policy Optimization (PPO)**: Both teacher and student agents use PPO, a policy gradient method. Understanding clipping mechanism and trust region is essential for tuning hyperparameters. Quick check: Why might high PPO epoch count cause instability in teacher's weight generation during early training?

- **Sparse vs. Dense Reward Formulation**: The paper's premise relies on difficulty of "sparse primary rewards" (e.g., +1 for goal). Understanding this sparsity is key to why auxiliary rewards are injected. Quick check: If primary reward were dense (e.g., negative distance to goal at every step), would need for adaptive auxiliary teacher increase or decrease?

## Architecture Onboarding

- **Component map**: Student Agent -> Reward Compositor -> Environment -> History Buffer -> Teacher Agent
- **Critical path**: Teacher samples weights → Student interacts using weighted rewards → Student updates policy → System logs rewards to History Buffer → Teacher updates policy using student's success as reward
- **Design tradeoffs**: History horizon H affects context vs computational overhead; teacher update frequency balances responsiveness vs noise; framework assumes predefined auxiliary rewards contain useful signals
- **Failure signatures**: Weight collapse (all weights dropping to zero), oscillation (rapid weight switching), dominance (one weight saturating at 1.0)
- **First 3 experiments**: 1) Implement Reward Randomization baseline to confirm adaptive weighting outperforms random; 2) Run RTW with H=1 vs H=10 to measure sensitivity to temporal context; 3) Plot weight evolution for simple navigation task to verify "training wheels" pattern

## Open Questions the Paper Calls Out

### Open Question 1
Can RTW automatically discover and incorporate new auxiliary reward components beyond pre-defined ones? Current RTW only adapts weights for manually specified auxiliary rewards; discovery of novel reward terms remains unexplored.

### Open Question 2
Do sophisticated teacher architectures (attention mechanisms, recurrent networks) improve RTW's ability to model long-term dependencies in student learning? Current teacher uses standard PPO without specialized architectures for temporal reasoning.

### Open Question 3
Does RTW transfer effectively to manipulation and human-robot interaction tasks? Experiments limited to navigation and mobility tasks; different task domains may have distinct reward structure dynamics.

### Open Question 4
How does RTW perform on physical robots for confined-space navigation task? Physical experiments only validated off-road mobility; sim-to-real transfer challenges for navigation in constrained spaces remain untested.

## Limitations

- Framework requires careful hyperparameter tuning for history horizon and teacher update frequency
- Method assumes predefined auxiliary rewards contain useful signals - cannot generate novel reward forms
- Results validated primarily in simulation with limited physical robot trials (5 trials per condition)
- Sim-to-real transfer challenges not fully explored, particularly for navigation tasks

## Confidence

**High Confidence**: Core mechanism of dynamic weight adjustment and performance improvements are well-supported by experimental results and Fig. 4 visualizations.

**Medium Confidence**: Claims about 35% and 3X faster training efficiency require careful interpretation of comparison baselines and training curves.

**Medium Confidence**: Physical robot validation showing 5/5 success vs 2/5 baseline is promising but based on very limited trials.

## Next Checks

1. Conduct additional physical robot trials (minimum 20-30 per condition) to establish statistical significance and confidence intervals for success rate differences.

2. Evaluate RTW's performance when deployed on previously unseen terrain configurations or navigation environments to test generalization beyond training distribution.

3. Systematically vary teacher's history horizon H and update frequency to quantify their impact on convergence speed and final performance, identifying optimal parameter ranges for different task complexities.