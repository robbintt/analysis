---
ver: rpa2
title: 'A Framework for Evaluating Vision-Language Model Safety: Building Trust in
  AI for Public Sector Applications'
arxiv_id: '2502.16361'
source_url: https://arxiv.org/abs/2502.16361
tags:
- noise
- adversarial
- attacks
- vlms
- robustness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework to evaluate adversarial vulnerabilities
  in Vision-Language Models (VLMs) for public sector applications. The authors propose
  a novel method that applies incremental noise (Gaussian, salt-and-pepper, uniform)
  to images until misclassification occurs, then averages these noise levels to create
  composite noise patches and saliency patterns.
---

# A Framework for Evaluating Vision-Language Model Safety: Building Trust in AI for Public Sector Applications

## Quick Facts
- arXiv ID: 2502.16361
- Source URL: https://arxiv.org/abs/2502.16361
- Authors: Maisha Binte Rashid; Pablo Rivas
- Reference count: 5
- Primary result: Novel noise-based adversarial evaluation achieves 66.54-67.54% accuracy degradation vs. 9.35% for FGSM on CLIP model

## Executive Summary
This paper introduces a framework to evaluate adversarial vulnerabilities in Vision-Language Models (VLMs) for public sector applications. The authors propose a novel method that applies incremental noise (Gaussian, salt-and-pepper, uniform) to images until misclassification occurs, then averages these noise levels to create composite noise patches and saliency patterns. These patterns are tested against the Fast Gradient Sign Method (FGSM) as a benchmark. The primary contribution is a new Vulnerability Score metric that combines the impact of random noise and targeted adversarial attacks. Experimental results on CLIP model using Caltech-256 dataset show that the proposed noise-based perturbations achieve accuracy of 66.54-67.54%, significantly better than FGSM's 9.35%, while maintaining computational efficiency by using only 1% of the dataset.

## Method Summary
The framework evaluates VLM robustness by applying incremental noise to images until misclassification, recording the threshold where model confidence drops below the true label. Three noise types (Gaussian, salt-and-pepper, uniform) are applied in 0.01 increments. Per-image thresholds are averaged to create composite noise patches that generalize as universal adversarial perturbations. These patches, along with derived saliency patterns, are tested against held-out images. The method is benchmarked against FGSM attacks, and a Vulnerability Score metric combines noise and adversarial impact using configurable weights (w1 + w2 = 1).

## Key Results
- Noise-based perturbations (Gaussian, salt-and-pepper, uniform) achieve 66.54-67.54% accuracy degradation on CLIP
- FGSM attack benchmark achieves only 9.35% accuracy degradation under same conditions
- Framework maintains computational efficiency using only 1% of Caltech-256 dataset (300 images)
- Vulnerability Score metric successfully combines random noise and targeted attack impact

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incremental noise application identifies per-image misclassification thresholds that reveal model vulnerability boundaries.
- Mechanism: The framework applies noise in 0.01 increments (Gaussian, salt-and-pepper, uniform) to clean images until the CLIP model misclassifies. Each noise type produces a threshold value N_final where F(I, N(i)) ≠ True Label. This creates a distribution of vulnerability points across the dataset.
- Core assumption: Misclassification under incremental random noise correlates with susceptibility to deliberate adversarial perturbations.
- Evidence anchors: [abstract]: "identifying misclassification thresholds and deriving composite noise patches"; [methodology]: "We tracked the specific noise level at which the model first misclassified the image, marking the threshold of the model's robustness"; [corpus]: Related work (DiffCAP) confirms VLM susceptibility to perturbations, supporting the premise that perturbation thresholds are meaningful vulnerability indicators.
- Break condition: If model exhibits non-monotonic robustness (accuracy improves with noise before degrading), threshold detection becomes unreliable.

### Mechanism 2
- Claim: Averaging per-image noise thresholds produces a composite noise patch that generalizes as a universal adversarial perturbation.
- Mechanism: The authors average all noise levels that caused misclassification across 300 images to create a single composite patch per noise type. This patch, when applied to unseen images, tests whether vulnerability patterns transfer across the dataset.
- Core assumption: Vulnerability is sufficiently uniform across images that an averaged pattern maintains adversarial effectiveness on new inputs.
- Evidence anchors: [abstract]: "averages these noise levels to create composite noise patches and saliency patterns"; [results]: Composite noise patches achieved 66.54-67.54% accuracy degradation vs. 95% baseline, outperforming FGSM's 9.35%; [corpus]: Zhao et al. (2024) found adversarial attacks transfer across VLM architectures, suggesting shared vulnerability patterns.
- Break condition: If model vulnerabilities are highly image-specific rather than structural, averaged patches will show near-baseline accuracy.

### Mechanism 3
- Claim: The Vulnerability Score metric captures combined robustness by weighting noise impact and targeted attack impact.
- Mechanism: The metric computes: Vulnerability Score = w1 × Noise Impact Score + w2 × FGSM Impact Score, where Impact Scores measure relative accuracy degradation from baseline. Configurable weights (w1 + w2 = 1) allow prioritizing random perturbations vs. adversarial attacks for different deployment contexts.
- Core assumption: A linear combination of noise and adversarial impact meaningfully represents overall model vulnerability.
- Evidence anchors: [abstract]: "new Vulnerability Score metric that combines the impact of random noise and targeted adversarial attacks"; [results]: "This metric provides a comprehensive metric for evaluating model robustness" with demonstrated efficiency using only 1% of dataset; [corpus]: Limited direct corpus validation of this specific metric formulation; related frameworks (SAIF, AutoTrust) propose alternative trustworthiness measures without direct comparability data.
- Break condition: If noise and adversarial vulnerabilities are uncorrelated or inversely related, the weighted combination obscures rather than clarifies risk assessment.

## Foundational Learning

- Concept: **Fast Gradient Sign Method (FGSM)**
  - Why needed here: Serves as the benchmark attack; understanding gradient-based perturbations is essential to interpret why noise-based attacks achieved different results.
  - Quick check question: Can you explain why FGSM computes perturbations aligned with the gradient of the loss function?

- Concept: **Vision-Language Models (VLMs) — specifically CLIP architecture**
  - Why needed here: The entire framework evaluates CLIP; understanding its dual-encoder design (image + text encoders with contrastive learning) explains how perturbations affect multimodal alignment.
  - Quick check question: How does CLIP's contrastive pre-training create shared embeddings for images and text?

- Concept: **Saliency Maps**
  - Why needed here: The framework generates saliency patterns to identify vulnerable image regions; these indicate which pixels most affect model confidence.
  - Quick check question: What does a high saliency value at a pixel indicate about its influence on the model's prediction?

## Architecture Onboarding

- Component map: Input layer (Caltech-256 images, 300 sampled) -> Perturbation engine (three noise generators with 0.01 increment controls) -> Target model (CLIP for zero-shot classification) -> Threshold detector (monitors classification output per noise level, records first misclassification) -> Pattern synthesizer (averages thresholds into composite patches; derives saliency patterns) -> Comparison module (FGSM attack implementation for benchmark) -> Score calculator (computes Noise Impact Score, FGSM Impact Score, and weighted Vulnerability Score)

- Critical path: 1. Sample selection (ensure class coverage across 300 images) 2. Per-image threshold detection (iterative noise injection → misclassification) 3. Composite pattern generation (average thresholds per noise type) 4. Cross-validation (apply patches to held-out images) 5. FGSM benchmark execution 6. Vulnerability Score computation

- Design tradeoffs: Sample size vs. representativeness (1% enables efficiency but may miss class-specific vulnerabilities); Noise types vs. coverage (three types provide diversity but don't cover structured perturbations); Weight flexibility vs. standardization (configurable w1, w2 allow domain tuning but reduce cross-study comparability)

- Failure signatures: Non-monotonic robustness (if accuracy doesn't monotonically decrease with noise level, threshold detection fails); Patch overfitting (if composite patches only affect training images but not held-out data, generalization is poor); Score instability (large variance in Vulnerability Score across different sample subsets indicates unreliable metric)

- First 3 experiments: 1. Baseline validation: Run CLIP on clean Caltech-256 subset to confirm ~95% baseline accuracy; verify class balance in 300-image sample. 2. Single noise type threshold mapping: Apply Gaussian noise to 50 images, record misclassification thresholds, compute mean and variance; check for outliers indicating image-specific vs. structural vulnerabilities. 3. Patch transfer test: Generate composite patch from first 150 images, apply to remaining 150; measure accuracy degradation to validate generalization claim.

## Open Questions the Paper Calls Out

- Question: What are optimal methods to reduce the computational complexity of the incremental noise application process for resource-constrained public sector deployments?
  - Basis in paper: [explicit] The Conclusions state: "Future work will optimize the framework's computational efficiency" and the Application Context acknowledges that "computational intensity might pose challenges for resource-constrained settings."
  - Why unresolved: The paper does not propose specific efficiency improvements; incremental noise application (adding 0.01 per iteration until misclassification) requires multiple forward passes per image.
  - What evidence would resolve it: Comparative runtime benchmarks showing reduced iterations or faster convergence while maintaining vulnerability detection accuracy.

- Question: How should the weights (w1, w2) in the Vulnerability Score be calibrated for different public sector application domains?
  - Basis in paper: [inferred] The paper states weights "can be adjusted to emphasize either random noise impact or adversarial perturbations" but provides no guidance on domain-specific calibration.
  - Why unresolved: Different domains (disaster response vs. medical diagnostics) may have distinct threat models requiring different weight configurations.
  - What evidence would resolve it: Empirical evaluation of weight configurations across multiple domains with domain-expert validation of resulting risk rankings.

- Question: Does the proposed framework generalize to other VLM architectures and multimodal models beyond CLIP?
  - Basis in paper: [explicit] The Conclusions state future work will "extend its applicability to other multimodal AI architectures."
  - Why unresolved: Experiments were conducted exclusively on CLIP with Caltech-256; architectural differences may affect noise sensitivity patterns.
  - What evidence would resolve it: Cross-model evaluation on architectures like BLIP-2, LLaVA, or MiniGPT-4 using the same noise-based methodology.

## Limitations

- Sample size concerns: The framework relies on only 300 images (~1% of Caltech-256), raising questions about whether vulnerability patterns generalize across all classes.
- Metric specificity: The Vulnerability Score formulation lacks direct validation against alternative trustworthiness frameworks and may not reflect real-world risk prioritization.
- Noise transfer assumption: The framework assumes averaged composite noise patches maintain adversarial effectiveness on unseen images, which may not hold if vulnerabilities are highly image-specific.

## Confidence

- High confidence: The incremental noise methodology for threshold detection is technically sound and reproducible.
- Medium confidence: The claim that noise-based attacks outperform FGSM (66.54-67.54% vs 9.35% accuracy degradation) is supported by the methodology.
- Low confidence: The generalizability of the Vulnerability Score metric and its effectiveness in real-world public sector applications remains to be validated.

## Next Checks

1. Class coverage validation: Test the framework on smaller class-specific subsets of Caltech-256 to determine if vulnerability patterns vary significantly by class.
2. Cross-dataset robustness: Apply the composite noise patches to entirely different datasets (e.g., CIFAR-100, ImageNet subsets) to test whether vulnerability patterns transfer across domains.
3. Alternative metric comparison: Implement and compare the Vulnerability Score against established trustworthiness metrics (SAIF, AutoTrust) using the same experimental setup.