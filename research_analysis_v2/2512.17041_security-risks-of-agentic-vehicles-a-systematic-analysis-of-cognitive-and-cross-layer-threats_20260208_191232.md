---
ver: rpa2
title: 'Security Risks of Agentic Vehicles: A Systematic Analysis of Cognitive and
  Cross-Layer Threats'
arxiv_id: '2512.17041'
source_url: https://arxiv.org/abs/2512.17041
tags:
- agentic
- agent
- reasoning
- these
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents the first structured framework for analyzing\
  \ security risks in Agentic Vehicles (AgVs)\u2014vehicles integrating reasoning-driven\
  \ AI agents with capabilities like memory-based personalization, goal interpretation,\
  \ and strategic reasoning. The work addresses the gap between traditional automotive\
  \ cybersecurity frameworks and the emerging cognitive vulnerabilities of agentic\
  \ AI systems."
---

# Security Risks of Agentic Vehicles: A Systematic Analysis of Cognitive and Cross-Layer Threats

## Quick Facts
- **arXiv ID:** 2512.17041
- **Source URL:** https://arxiv.org/abs/2512.17041
- **Reference count:** 25
- **Primary result:** First structured framework analyzing security risks in Agentic Vehicles, mapping OWASP Agentic AI Security Risks onto vehicle architecture with role-based decomposition and cross-layer attack analysis.

## Executive Summary
This paper presents the first structured framework for analyzing security risks in Agentic Vehicles (AgVs)—vehicles integrating reasoning-driven AI agents with capabilities like memory-based personalization, goal interpretation, and strategic reasoning. The work addresses the gap between traditional automotive cybersecurity frameworks and the emerging cognitive vulnerabilities of agentic AI systems. A role-based architecture decomposes the agentic layer into Personal Agent (managing user intent) and Driving Strategy Agent (generating policy-level behavioral proposals), both operating under authority and trust boundaries. The authors map fifteen OWASP Agentic AI Security Risks onto this architecture and evaluate their severity across different operational modes (manual vs. autonomous) and agency levels. They demonstrate that while traditional CAV attacks remain relevant, agentic-specific threats like memory poisoning, cascading hallucinations, and intent breaking can propagate through the reasoning pipeline to produce misaligned yet safety-validated behavior. Cross-layer attack analysis reveals how upstream perception, communication, and control layer attacks can mislead agentic reasoning without direct compromise.

## Method Summary
The research develops a role-based architecture for Agentic Vehicles that separates intent interpretation (Personal Agent) from behavioral proposal generation (Driving Strategy Agent), both operating under authority and trust boundaries. The framework maps all fifteen OWASP Agentic AI Security Risks onto this architecture and evaluates threat severity across operational modes and agency levels. Case studies demonstrate memory poisoning and infrastructure-induced policy misalignment using Llama 3.1 as the foundation model, showing how cognitive threats can propagate through the reasoning pipeline while passing deterministic safety validation.

## Key Results
- Role-based decomposition of agentic layer into Personal Agent (intent management) and Driving Strategy Agent (behavioral proposals) with bounded authority
- Mapping of 15 OWASP Agentic AI Security Risks onto vehicle architecture with severity evaluation across operational modes
- Demonstration that memory poisoning and infrastructure spoofing can produce semantically misaligned yet safety-validated behavior through the reasoning pipeline
- Cross-layer attack analysis showing upstream perception and communication layer attacks can mislead agentic reasoning without direct compromise

## Why This Works (Mechanism)

### Mechanism 1: Role-Based Decomposition with Bounded Authority
- Claim: Partitioning agentic functions into distinct roles with constrained authority limits adversarial propagation scope.
- Mechanism: The architecture separates intent interpretation (Personal Agent) from behavioral proposal generation (Driving Strategy Agent), ensuring no single agent can independently interpret intent, generate maneuvers, and authorize execution. Authority and trust boundaries regulate what inputs each role may rely upon and which outputs it may produce.
- Core assumption: Clear role separation can be enforced in practical deployments despite potential shared resources.
- Evidence anchors:
  - [abstract]: "A role-based architecture decomposes the agentic layer into Personal Agent (managing user intent) and Driving Strategy Agent (generating policy-level behavioral proposals), both operating under authority and trust boundaries."
  - [Section 3.1]: "A central design principle is the establishment of authority boundaries and trust boundaries... These boundaries prevent an agent from issuing commands or adopting objectives outside its domain."
  - [corpus]: Related work (arXiv:2510.23883) discusses agentic AI security but lacks vehicle-specific authority boundary modeling.
- Break condition: Shared resources, co-located execution, or optimization-driven coupling blur role boundaries, creating hidden dependencies.

### Mechanism 2: Cognitive Distortion Cascade Through Reasoning Pipeline
- Claim: Localized cognitive threats (memory poisoning, hallucinations, intent breaking) propagate through multi-step reasoning to produce behavior that passes safety validation while being semantically misaligned.
- Mechanism: A small distortion in the Personal Agent (e.g., poisoned preference memory) shapes intent descriptors that guide DSA planning. The resulting strategies appear coherent and satisfy physical constraints, so the Safety Check layer approves them—even though they optimize for corrupted objectives.
- Core assumption: Safety Check layers validate physical/regulatory constraints but do not verify semantic alignment with true user intent.
- Evidence anchors:
  - [abstract]: "Agentic-specific threats like memory poisoning, cascading hallucinations, and intent breaking can propagate through the reasoning pipeline to produce misaligned yet safety-validated behavior."
  - [Section 7.1]: Case study shows memory poisoning ("vehicle should not exceed ~45 km/h") reduces target speeds from 80–90 km/h to 30–60 km/h across scenarios without triggering SC rejection.
  - [Section 4.3]: Attack chain "Memory Poisoning → Intent Drift → Misaligned Strategy" demonstrates cross-role propagation.
  - [corpus]: Limited vehicle-specific evidence; related agentic AI security papers discuss similar cognitive threats in non-safety-critical domains.
- Break condition: Semantic provenance tracking or intent verification mechanisms detect distortion before DSA planning.

### Mechanism 3: Cross-Layer Indirect Compromise via Upstream Summary Corruption
- Claim: Adversarial manipulation of perception, communication, or control layers can mislead agentic reasoning without directly compromising agents.
- Mechanism: Spoofed V2X messages, adversarial perception perturbations, or falsified control feedback produce coherent-but-incorrect summaries. Agentic modules accept these as legitimate context, leading to policy proposals that are internally consistent with false premises and thus pass Safety Check validation.
- Core assumption: Agentic modules trust upstream summaries without independent multi-source verification.
- Evidence anchors:
  - [abstract]: "Cross-layer attack analysis reveals how upstream perception, communication, and control layer attacks can mislead agentic reasoning without direct compromise."
  - [Section 5]: "Attacks on these upstream layers can therefore propagate into the agentic pipeline without directly manipulating memory, goals, or reasoning."
  - [Section 7.2]: Infrastructure spoofing (false "40 km/h maintenance limit") causes DSA to propose significantly reduced speeds; SC approves because speeds remain physically safe.
  - [corpus]: Weak corpus evidence for vehicle-specific cross-layer attacks; general LLM system security papers exist but lack cyber-physical integration analysis.
- Break condition: Temporal consistency checks, cross-sensor fusion validation, or provenance-verified summaries detect upstream corruption.

## Foundational Learning

- **OWASP Agentic AI Security Risks (15 threats)**
  - Why needed: The paper maps all fifteen risks onto vehicle architecture; understanding their cognitive nature (memory-based, goal-driven, tool-mediated) is prerequisite to threat analysis.
  - Quick check question: Can you distinguish T1 (Memory Poisoning) from T5 (Cascading Hallucinations) in terms of where the corruption originates versus how it propagates?

- **CAV Layered Architecture (Perception → Communication → Control → Cloud)**
  - Why needed: Cross-layer attack analysis assumes understanding of how upstream layers produce summaries that agentic modules consume.
  - Quick check question: If an attacker spoofs LiDAR returns (perception layer) versus injects false V2X congestion warnings (communication layer), what is the common pathway by which both reach the Driving Strategy Agent?

- **Agency Levels vs. SAE Driving Automation Levels**
  - Why needed: The paper emphasizes these taxonomies are orthogonal; a manually driven vehicle can host high-agency AI for personalization, and an autonomous vehicle may use low-agency reasoning.
  - Quick check question: Why does a Level 3 Contextual Agent (long-horizon memory, goal adaptation) present a broader attack surface than a Level 1 Assisted Agent, even if both operate in the same SAE Level 2 vehicle?

## Architecture Onboarding

- **Component map:**
  - Personal Agent (PA): User intent interpretation, long-term preference memory, external service/tool queries; produces high-level intent descriptors; no direct path to control stack.
  - Driving Strategy Agent (DSA): Policy-level behavioral proposals, multi-step reasoning integrating PA intent + perception summaries + V2X context; outputs structured proposals to Safety Check.
  - Safety Check (SC) Layer: Deterministic, non-agentic, stateless rule-based validation (collision envelopes, acceleration limits, regulatory constraints); final gate before deterministic planner.
  - Upstream CAV Layers: Perception/fusion, V2X communication, control feedback—produce summaries that feed PA and DSA context.

- **Critical path:**
  User input → PA (encodes intent/preferences, may query tools) → DSA (generates behavioral proposals using PA context + environmental summaries) → SC (validates against physical/regulatory constraints) → Deterministic behavioral planner → Control stack.

- **Design tradeoffs:**
  1. Higher agency levels increase capability but expand attack surface (memory, tool use, multi-step reasoning).
  2. Adding a Safety Monitor Agent provides semantic oversight but introduces a new high-privilege component requiring its own security analysis.
  3. Orchestrator agents improve coordination but add latency and complexity; the paper avoids them to limit attack surface.

- **Failure signatures:**
  1. Behavior that passes SC validation but pursues incorrect objectives (semantic misalignment).
  2. Persistent behavioral drift across sessions (memory-based corruption).
  3. Coherent reasoning chains built on false environmental premises (upstream perception/V2X corruption).

- **First 3 experiments:**
  1. Inject a subtle preference into PA memory (e.g., speed constraint), measure behavioral shift in DSA proposals across scenarios; verify SC does not flag the change.
  2. Spoof V2X infrastructure messages (e.g., false road closure or speed limit), observe DSA policy adjustments; confirm SC approves resulting proposals.
  3. Introduce minor hallucinated context to PA input, trace amplification through DSA reasoning steps; quantify semantic drift versus baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can dynamic agency degradation strategies be securely implemented to maintain safety under threat without creating a new denial-of-service attack surface where attackers intentionally trigger lower-capability modes?
- Basis in paper: [explicit] Section 6.1 states that while dynamically lowering agency might help safety, "this can also open a new attack surface, when the attacker aims to degrade system performance by degrading its agency levels."
- Why unresolved: The authors identify the defensive potential of dynamic agency but explicitly flag the resulting trade-off as a vulnerability requiring further investigation.
- What evidence would resolve it: Formal verification of degradation protocols or empirical demonstration of systems that resist agency-degradation attacks while maintaining operational safety.

### Open Question 2
- Question: What architectural constraints and validation methods are required to ensure a Safety Monitor Agent (SMA) detects semantic drift without itself becoming a high-value target for compromise?
- Basis in paper: [explicit] Section 3.2.4 notes that designing an agentic safety monitor "lies outside the scope of the present work," and Section 8 lists "validation methods to assess the Safety Monitor Agent’s robustness" as a priority for future research.
- Why unresolved: While the paper proposes an SMA as an optional extension, it acknowledges that an agentic monitor introduces its own attack surface that must be bounded.
- What evidence would resolve it: A prototype SMA implementation showing it can contain adversarial reasoning while operating strictly under isolated memory and tool-access boundaries.

### Open Question 3
- Question: How do iterative feedback loops and joint state adaptation between the Personal Agent and Driving Strategy Agent lead to emergent vulnerabilities that static role-based taxonomies fail to capture?
- Basis in paper: [inferred] Section 6.2 states the current taxonomy "does not model emergent behaviors that arise when reasoning modules interact through iterative feedback or jointly adapt their internal states."
- Why unresolved: The framework relies on clear role separation; however, real-world implementations may involve coupling that creates complex, non-linear attack paths not mapped in the static severity matrices.
- What evidence would resolve it: Long-horizon multi-agent simulation results demonstrating adversarial behaviors emerging solely from inter-agent feedback dynamics rather than single-role compromise.

## Limitations

- The threat modeling relies heavily on conceptual scenarios rather than empirical validation across diverse vehicle platforms.
- The role-based architecture assumes clean separation between agents, but real implementations may exhibit emergent coupling through shared resources.
- Cross-layer attack analysis lacks quantified vulnerability assessments for specific vehicle architectures.
- The Safety Check layer is treated as an ideal deterministic validator without addressing conflicts between semantic alignment and physical safety requirements.

## Confidence

**High Confidence:** The foundational architecture decomposition into Personal Agent and Driving Strategy Agent roles, operating under bounded authority, is well-specified and theoretically sound. The conceptual separation of intent interpretation from behavioral proposal generation represents a meaningful advancement in understanding agentic vehicle vulnerabilities.

**Medium Confidence:** The mapping of OWASP Agentic AI Security Risks onto the vehicle architecture is methodologically rigorous, but the severity assessments across operational modes are based on theoretical reasoning rather than empirical measurements. The cross-layer attack analysis provides valuable insights but remains largely speculative regarding real-world propagation effectiveness.

**Low Confidence:** The quantitative claims in the case studies (specific speed reductions of 50% under memory poisoning and infrastructure spoofing) are presented without detailed implementation specifications, making independent verification challenging. The assumed independence of the Safety Check layer from semantic considerations may not hold in practical deployments.

## Next Checks

1. **Implementation Fidelity Verification:** Replicate the case study scenarios using the specified Llama 3.1 model with documented prompt engineering techniques to verify the claimed speed reductions under memory poisoning and infrastructure spoofing conditions.

2. **Cross-Layer Attack Quantification:** Conduct controlled experiments measuring the actual propagation effectiveness of perception and communication layer attacks through the agentic reasoning pipeline, including measurement of detection rates and behavioral impact severity.

3. **Authority Boundary Stress Testing:** Evaluate the resilience of the role-based architecture against sophisticated attacks that attempt to exploit coupling mechanisms between Personal Agent and Driving Strategy Agent, including shared memory access patterns and optimization-based coordination.