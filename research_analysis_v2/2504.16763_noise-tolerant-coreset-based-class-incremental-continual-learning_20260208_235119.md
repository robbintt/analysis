---
ver: rpa2
title: Noise-Tolerant Coreset-Based Class Incremental Continual Learning
arxiv_id: '2504.16763'
source_url: https://arxiv.org/abs/2504.16763
tags:
- noise
- continual
- learning
- data
- coreset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses continual learning (CL) in the presence of
  label and instance noise. The authors theoretically analyze and empirically validate
  CRUST, a Coreset-based method, for constructing noise-tolerant replay buffers.
---

# Noise-Tolerant Coreset-Based Class Incremental Continual Learning

## Quick Facts
- arXiv ID: 2504.16763
- Source URL: https://arxiv.org/abs/2504.16763
- Authors: Edison Mucllari; Aswin Raghavan; Zachary Alan Daniels
- Reference count: 40
- Primary result: Coreset-based replay buffers significantly improve continual learning under label and instance noise, achieving 89% accuracy at 0.5 label noise on MNIST vs 16% for Random Replay

## Executive Summary
This work addresses the challenge of continual learning (CL) under label and instance noise by introducing noise-tolerant coreset-based replay methods. The authors extend CRUST (Coreset-based Replay Using Spectral Theory) to the class-incremental learning (CIL) setting, creating Continual CRUST and Continual Cosine-CRUST. These methods incrementally train both the classifier and coresets, using gradient-based selection to filter out noisy samples from the replay buffer. Experiments on five diverse datasets demonstrate significant improvements in classification accuracy and minimal forgetting compared to standard memory-based CL methods under moderate to high noise conditions.

## Method Summary
The method maintains per-class coresets S[i] that store selected samples for each class. During each learning experience, the classifier is trained on combined coresets for m iterations to distill the new class, then gradients are computed for the current class data with respect to the last layer only. CRUST refines the coreset via greedy k-medoids optimization on gradient distances, while Cosine-CRUST first clusters gradients using cosine distance, filters small clusters (assumed noisy), then applies CRUST proportionally to cluster sizes. Training occurs in two phases: Phase 1 trains on full current+memory data, while Phase 2 trains on coresets only. The approach uses Avalanche library with 5 random seeds and models ranging from SimpleCNN to EfficientNetV2s depending on dataset complexity.

## Key Results
- Continual Cosine-CRUST achieves 89% accuracy at 0.5 label noise on MNIST compared to 16% for Random Replay
- Methods maintain high coreset purity (proportion of clean samples) even under high noise conditions
- Show minimal forgetting compared to baseline methods under label noise
- Demonstrate robustness to salt-and-pepper instance noise, maintaining performance across varying noise fractions
- Significant accuracy improvements over existing memory-based CL methods under moderate noise conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Selecting samples with similar gradients filters out mislabeled data
- Mechanism: CRUST formulates coreset selection as k-medoids optimization over gradient distances. Clean samples cluster in gradient space while mislabeled samples appear as outliers with divergent gradients, enabling their exclusion from the replay buffer.
- Core assumption: Clean samples exhibit consistent gradient patterns; noisy labels produce gradient outliers.
- Evidence anchors:
  - [abstract]: "Coresets found by CRUST are 'mostly clean' due to the underlying properties of the Jacobian spectrum"
  - [section 2.2]: "samples with clean labels tend to have similar gradients w.r.t. the NN parameters"
  - [corpus]: Weak direct corpus evidence for gradient-based coreset selection in noisy continual learning
- Break condition: Noise fraction ρ exceeds threshold (ρ ≥ δ/8 where δ is label margin) or model severely overparameterized

### Mechanism 2
- Claim: Cosine spectral clustering improves noise detection by filtering small gradient clusters
- Mechanism: CosineCRUST clusters gradients using cosine distance, filters clusters below size threshold nA (hypothesized noisy outliers), then applies CRUST within remaining clusters proportionally to cluster size.
- Core assumption: Noisy samples form small outlier clusters and are neither too similar nor too dissimilar to clean samples in Jacobian spectrum.
- Evidence anchors:
  - [section 4]: "noisy examples will form small clusters of outliers in the spectrum"
  - [section 4]: "we hypothesize that noisy examples will be neither too similar nor too dissimilar to clean samples"
  - [corpus]: No direct corpus evidence for cosine-based clustering in noisy CL
- Break condition: High noise regime where noisy samples form dense clusters comparable to clean clusters

### Mechanism 3
- Claim: Early stopping with bounded iterations provides theoretical noise robustness
- Mechanism: Theorem 4 bounds required iterations T based on noise fraction δ, learning rate η, and Jacobian properties. As noise increases, fewer iterations are needed to fit coreset, but approximation error grows—providing principled stopping criteria.
- Core assumption: Jacobian is L-smooth; coreset approximates Jacobian within error ε ≤ O(δα²/kβ log(δ)).
- Evidence anchors:
  - [section 3]: "as δ increases, the number of iterations required to fit the Coreset decreases"
  - [theorem 4]: "after T ≥ O(1/η(α² + EJ2α + EJ1α − ηEJ2β³/2 − ηEJ1β³) log(∥r₀∥²/ν) iterations, the NN classifies samples correctly"
  - [corpus]: Related work (arXiv 2503.10503) mentions learning guarantees for continual learning
- Break condition: Learning rate η > 1/(2β²) or iterations outside theoretical bounds

## Foundational Learning

- **Coresets**:
  - Why needed: Understanding how a weighted subset approximates full dataset loss (|L(X,θ) - L(S,θ)| ≤ ε|L(X,θ)|) is essential for replay buffer construction.
  - Quick check question: Can you explain why coreset size k affects approximation error ε?

- **Class-Incremental Learning (CIL)**:
  - Why needed: This paradigm requires balancing plasticity (new class adaptation) with stability (preventing forgetting)—the core tension the method addresses.
  - Quick check question: What constraint does CIL impose that task-incremental learning does not?

- **Jacobian Spectrum Analysis**:
  - Why needed: Theoretical guarantees rely on singular value properties (α = minimum, β = maximum) and L-smoothness of the Jacobian.
  - Quick check question: What does it mean for the Jacobian to be L-smooth, and why does this matter for convergence?

## Architecture Onboarding

- **Component map**: Per-class Coreset dictionary S -> Classifier fθ (neural network) -> Gradient computation module -> CRUST submodular optimizer OR CosineCRUST (spectral clustering + submodular optimization)

- **Critical path**:
  1. Initialize S[i] ← full dataset for new class i
  2. Train classifier on combined coresets for m iterations (distill new class)
  3. Compute gradients Gᵢ for current class data
  4. Apply CRUST or CosineCRUST to refine S[i]
  5. Update classifier using refined coresets for n iterations

- **Design tradeoffs**:
  - CRUST vs. CosineCRUST: CosineCRUST has O(n² log n) clustering overhead but generally outperforms; CRUST is O(nk) greedy
  - Two-phase training (m=40 epochs full data, n=20 epochs coreset-only) increases compute but improves noise filtering
  - Coreset size: 60-300 samples/class; larger coresets help with high noise but degrade purity

- **Failure signatures**:
  - CIFAR10 shows larger accuracy gap vs. joint learner (~40% vs. 78%), suggesting method struggles with unstructured data
  - CosineCRUST may pollute coreset when noisy samples form dense clusters (acknowledged in section 7)
  - Overparameterized models can overfit pure noise, reducing purity benefits

- **First 3 experiments**:
  1. Replicate MNIST label noise (noise=0.3, coreset=200) to validate baseline gradient selection works
  2. Ablate coreset size {100, 200, 300} at fixed noise=0.4 to measure memory-accuracy tradeoff
  3. Test instance noise (salt-and-pepper, noise=0.6) on FashionMNIST to validate Theorem 4 behavior under perturbed data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the clustering step in Continual CosineCRUST be modified to prevent performance degradation in worst-case scenarios where noisy samples form a dense cluster that pollutes the Coreset?
- Basis: [explicit] The authors state in the "Weaknesses and Future Work" section that "in the worst case scenario (high fraction of noisy samples), the clustering step of CosineCRUST may identify two dense clusters... Performing submodular optimization on the noisy cluster will pollute the global Coreset."
- Why unresolved: The current algorithm assumes clustering helps filter noise, but lacks a mechanism to distinguish between a dense cluster of valid samples and a dense cluster of noisy outliers in high-noise regimes.
- What evidence would resolve it: An algorithmic modification that successfully identifies and discards dense clusters composed primarily of noisy samples, validated by maintained Coreset purity and accuracy under high noise fractions (>0.5).

### Open Question 2
- Question: Why does the method exhibit reduced performance on unstructured, low-information datasets like CIFAR10, and how can this be mitigated?
- Basis: [explicit] The authors note, "We observed that Continual (Cosine)CRUST works better for well-structured data and struggles with unstructured low-information data (e.g., CIFAR10). Fully understanding this issue requires additional exploration."
- Why unresolved: The paper establishes the phenomenon empirically but does not offer a theoretical or empirical explanation for why the gradient-based Coreset selection fails to capture essential information in unstructured visual domains.
- What evidence would resolve it: An analysis of the Jacobian spectrum or gradient distributions in CIFAR10 compared to structured datasets (like MNIST/SAR), paired with architectural or selection-strategy changes that close the performance gap with upper-bound joint learners.

### Open Question 3
- Question: Do the noise-tolerance guarantees of Continual (Cosine)CRUST generalize to other continual learning scenarios, specifically domain-incremental or task-incremental learning?
- Basis: [explicit] The authors state in "Conclusions and Future Work" that "we only examined noise-tolerant learning in the context of class-incremental learning. Further experimentation is needed to validate that the approach generalizes to continual learning beyond the CIL classification problem."
- Why unresolved: The current evaluation is restricted to the Class-Incremental Learning (CIL) setting, leaving the method's efficacy in other standard CL paradigms unknown.
- What evidence would resolve it: Experimental results applying Continual CRUST to standard Domain-Incremental Learning (DIL) and Task-Incremental Learning (TIL) benchmarks with synthetic noise.

### Open Question 4
- Question: Is the proposed Coreset-based replay strategy robust to adversarial poisoning attacks, such as backdoor triggers or forgetting attacks?
- Basis: [explicit] The authors identify "Adversarial Poisoning" as a key noise type in the introduction but conclude by stating, "We have not yet explored Continual (Cosine)CRUST's susceptibility to adversarial attacks (e.g., backdoor triggers)."
- Why unresolved: While the method handles stochastic label and instance noise, it is unknown if the gradient-based selection criteria are spoofable by maliciously crafted adversarial samples.
- What evidence would resolve it: Empirical testing of Continual CRUST against established CL poisoning benchmarks to measure the success rate of backdoor injection or catastrophic forgetting induction.

## Limitations

- Theorem 4's noise tolerance bounds depend on specific Jacobian properties not fully verified across datasets
- Method assumes last-layer gradients suffice for selection, potentially missing deeper representation changes
- Method struggles with unstructured, low-information datasets like CIFAR10, showing large accuracy gaps vs joint learners

## Confidence

- High: Gradient-based filtering improves coreset purity vs random selection (Empirically validated across 5 datasets)
- Medium: Theoretical bounds on iterations and approximation error (Relies on assumptions about Jacobian properties)
- Low: Generalization to unstructured data like CIFAR10 (Large accuracy gap vs joint learner suggests limitations)

## Next Checks

1. Measure forgetting rates separately for clean vs noisy samples to verify method's noise tolerance mechanism
2. Test CRUST variants with different gradient aggregation (full network vs last layer only) to validate architectural assumption
3. Evaluate coreset selection stability across random seeds to assess reproducibility of noise filtering behavior