---
ver: rpa2
title: Label Poisoning is All You Need
arxiv_id: '2310.18933'
source_url: https://arxiv.org/abs/2310.18933
tags:
- flip
- backdoor
- labels
- training
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether backdoor attacks can be launched
  by corrupting only training labels, rather than both images and labels as in traditional
  attacks. It introduces FLIP, a novel label-only backdoor attack that matches the
  training trajectory of a traditionally backdoored model by optimizing label flips.
---

# Label Poisoning is All You Need

## Quick Facts
- arXiv ID: 2310.18933
- Source URL: https://arxiv.org/abs/2310.18933
- Reference count: 40
- One-line primary result: Label-only backdoor attacks can match or exceed the effectiveness of traditional image+label poisoning attacks.

## Executive Summary
This paper demonstrates that backdoor attacks can be launched by corrupting only training labels, without modifying the input images themselves. The authors introduce FLIP, a novel label-only backdoor attack that matches the training trajectory of a traditionally backdoored model by optimizing label flips. With just 2% of CIFAR-10 labels corrupted, FLIP achieves 99.4% Poison Test Accuracy while maintaining 98.2% Clean Test Accuracy, outperforming baseline attacks. The method is robust to partial knowledge of model architecture, optimizer, and training data. A modified version, softFLIP, extends this to knowledge distillation scenarios with even stronger backdoor capabilities.

## Method Summary
FLIP is a three-step process: (1) train an expert model using traditionally poisoned data and record checkpoints, (2) optimize soft labels such that training on clean images with these labels yields a training trajectory similar to the expert model, and (3) round the soft label solution to hard one-hot encoded labels for the attack. The optimization uses trajectory matching to make gradient steps from clean data with optimized labels align with gradient steps from poisoned data. softFLIP extends this to knowledge distillation by interpolating between ground-truth and corrupted soft labels with parameter α ∈ [0,1].

## Key Results
- FLIP achieves 99.4% Poison Test Accuracy (PTA) and 98.2% Clean Test Accuracy (CTA) on CIFAR-10 with only 2% label corruption
- FLIP outperforms baseline attacks across three datasets (CIFAR-10, CIFAR-100, Tiny-ImageNet) and four architectures
- softFLIP achieves even stronger backdoor attacks in knowledge distillation scenarios by allowing soft label corruption
- The attack remains effective even with partial knowledge of model architecture, optimizer, and training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FLIP optimizes labels to make training gradients mimic those of a traditionally backdoored model.
- Mechanism: FLIP generates soft labels for clean images so that gradient steps during training align with gradient steps from a poisoned dataset with the target trigger. This alignment is measured by a normalized squared distance in parameter space.
- Core assumption: The model training trajectory is smooth enough that short-term gradient matching leads to long-term behavior matching.
- Evidence anchors:
  - [abstract] "Our approach builds upon the recent advances in trajectory matching, originally introduced for dataset distillation."
  - [section 2] "Our objective is to produce a similar training trajectory to the traditionally-poisoned expert... Concretely, we randomly select an iteration k ∈ [K] and take two separate gradient steps starting from the expert checkpoint θk..."
  - [corpus] Weak or missing.
- Break condition: If training dynamics are highly chaotic or non-smooth, gradient alignment will not persist and the backdoor will fail.

### Mechanism 2
- Claim: FLIP selects high-scoring images whose logits indicate high confidence in an incorrect prediction, then flips their labels.
- Mechanism: Each image gets a score defined as the largest logit of the incorrect classes minus the logit of the correct class. The top-m scoring images have their labels flipped to the corresponding incorrect class.
- Core assumption: Images with high incorrect-class logits are those most likely to be influenced by trigger-like features in the data.
- Evidence anchors:
  - [abstract] "With only 2% of CIFAR-10 labels corrupted, FLIP achieves a near-perfect attack success rate of 99.4%..."
  - [section 2] "We define the score of an example as the largest logit of the incorrect classes minus the logit of the correct class. Then, to select m total label flips, we choose the m examples with the highest score..."
  - [corpus] Weak or missing.
- Break condition: If incorrect-class logits are not correlated with trigger-like features, the selection will not yield a strong backdoor.

### Mechanism 3
- Claim: softFLIP extends FLIP to the knowledge distillation setting by allowing corrupted soft labels interpolated with ground-truth labels.
- Mechanism: For each image, the final soft label is a linear interpolation between the one-hot ground-truth and the corrupted soft label found by FLIP, controlled by parameter α ∈ [0,1].
- Core assumption: Giving the attacker control over soft labels (not just hard labels) increases attack strength because the model can be trained on probabilistic supervision.
- Evidence anchors:
  - [abstract] "A modified version, softFLIP, is also introduced for knowledge distillation scenarios, achieving even stronger backdoor attacks by allowing soft label corruption."
  - [section 4] "To traverse the CTA-PTA trade-off, we regularize the attack by with a parameter α ∈ [0, 1], which measures how close the corrupted soft label is to the ground truths."
  - [corpus] Weak or missing.
- Break condition: If the distillation process is robust to soft label corruption or if α is set too high, the attack will lose effectiveness.

## Foundational Learning

- Concept: Trajectory matching in optimization
  - Why needed here: FLIP relies on matching the parameter update trajectory of a backdoored model to inject a backdoor without altering images.
  - Quick check question: If two models take identical gradient steps at every iteration, will they converge to the same parameters?

- Concept: Gradient-based label optimization
  - Why needed here: FLIP optimizes labels by minimizing the distance between gradients induced by clean data with optimized labels and gradients from poisoned data.
  - Quick check question: What is the effect on the loss if the gradient from the clean batch with optimized labels exactly matches the gradient from the poisoned batch?

- Concept: Soft labels and knowledge distillation
  - Why needed here: softFLIP exploits the ability to return soft labels in distillation, allowing interpolation between ground-truth and corrupted labels.
  - Quick check question: How does training on soft labels (probability distributions) differ from training on one-hot labels in terms of model calibration and robustness?

## Architecture Onboarding

- Component map: Expert model training -> Label optimization via trajectory matching -> Label selection and rounding -> Evaluation on user-trained models (softFLIP adds interpolation step)
- Critical path: Step 2 (label optimization) is the bottleneck; it requires backpropagating through a single gradient step and must be efficient enough to scale to large datasets.
- Design tradeoffs: More epochs K in expert training increase trajectory fidelity but also increase optimization cost and risk of drift. Fewer experts E reduce compute but may hurt robustness.
- Failure signatures: If CTA drops sharply with increasing PTA, the attack is too aggressive. If PTA stays low even with many label flips, the selected images are not trigger-correlated.
- First 3 experiments:
  1. Run FLIP on CIFAR-10 with sinusoidal trigger and ResNet-32, measure CTA/PTA at m=500, 1000, 1500 flips.
  2. Compare FLIP PTA vs inner-product baseline PTA across the same settings.
  3. Test softFLIP with α=0.8 on CIFAR-10 distillation setup and measure student model PTA.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can FLIP be extended to successfully bypass SPECTRE and other representation-based defenses?
- Basis in paper: [explicit] The authors note that SPECTRE is quite effective in mitigating FLIP, while other defenses fail, and suggest this as a rewarding future research direction.
- Why unresolved: While the authors demonstrate FLIP's effectiveness against other defenses, SPECTRE's success suggests that the current approach has limitations in bypassing representation-based defenses.
- What evidence would resolve it: Developing and testing new variants of FLIP specifically designed to evade SPECTRE's detection mechanisms, and comparing their performance to the original FLIP on the same datasets and architectures.

### Open Question 2
- Question: How does the choice of source label (ysource) affect the strength of FLIP attacks in more complex, real-world scenarios?
- Basis in paper: [explicit] The authors show that FLIP is effective even when the attacker uses images from any class for the attack (ysource = all but ytarget), but they only test this on CIFAR-10 with a single target class.
- Why unresolved: The paper's experiments focus on a simplified scenario with a single source class, but real-world datasets often have multiple source classes and more complex relationships between classes.
- What evidence would resolve it: Conducting experiments with FLIP on larger, more complex datasets (e.g., ImageNet) with multiple source classes and evaluating how the choice of source labels impacts the attack's success rate and CTA-PTA trade-off.

### Open Question 3
- Question: Can FLIP be adapted to target specific features or regions within images, rather than just the entire image?
- Basis in paper: [inferred] The authors mention that FLIP selects images based on a score that considers the largest logit of the incorrect classes minus the logit of the correct class, suggesting that the attack could potentially be more targeted.
- Why unresolved: The current FLIP approach focuses on flipping labels for entire images, but in practice, attackers might want to target specific features or regions within images to make the attack more stealthy.
- What evidence would resolve it: Developing a modified version of FLIP that allows for targeted label flipping based on specific features or regions within images, and evaluating its effectiveness compared to the original FLIP on the same datasets and architectures.

## Limitations
- The paper does not provide sufficient detail on how the trajectory matching is performed, which could impact reproducibility of the results.
- The assumption that model training trajectory is smooth enough for gradient matching to lead to long-term behavior matching is not rigorously validated.
- The effectiveness of the attack relies on the correlation between incorrect-class logits and trigger-like features, which is not explicitly demonstrated.

## Confidence

High confidence:
- FLIP achieves 99.4% Poison Test Accuracy and 98.2% Clean Test Accuracy on CIFAR-10 with 2% label corruption
- FLIP outperforms baseline attacks across multiple datasets and architectures

Medium confidence:
- The trajectory matching mechanism for label optimization
- The effectiveness of softFLIP in knowledge distillation scenarios
- The robustness of FLIP to partial knowledge of model architecture, optimizer, and training data

## Next Checks

1. Reproduce the trajectory matching step with different distance metrics and hyperparameters to assess robustness.
2. Analyze the correlation between incorrect-class logits and trigger-like features in the datasets used.
3. Evaluate softFLIP on a wider range of distillation scenarios to validate its generalizability.