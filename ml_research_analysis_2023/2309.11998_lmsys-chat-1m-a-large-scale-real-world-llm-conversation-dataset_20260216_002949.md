---
ver: rpa2
title: 'LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset'
arxiv_id: '2309.11998'
source_url: https://arxiv.org/abs/2309.11998
tags:
- content
- have
- user
- conversations
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The LMSYS-Chat-1M dataset contains one million real-world conversations
  with 25 state-of-the-art LLMs collected from 210K unique IP addresses. It enables
  multiple use cases including fine-tuning small LLMs for content moderation with
  performance matching GPT-4, creating a safety benchmark from jailbreak attempts,
  training instruction-following models comparable to Vicuna, and generating challenging
  benchmark questions.
---

# LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset

## Quick Facts
- **arXiv ID**: 2309.11998
- **Source URL**: https://arxiv.org/abs/2309.11998
- **Reference count**: 40
- **Key outcome**: Dataset contains 1 million real-world conversations with 25 state-of-the-art LLMs from 210K users across 154 languages, enabling fine-tuning of small LLMs to match GPT-4's content moderation performance.

## Executive Summary
The LMSYS-Chat-1M dataset provides one million real-world conversations collected from 210K unique IP addresses interacting with 25 state-of-the-art LLMs across more than 150 languages. The dataset enables multiple use cases including developing content moderation models that perform similarly to GPT-4, building safety benchmarks from jailbreak attempts, training instruction-following models comparable to Vicuna, and generating challenging benchmark questions. The dataset covers 154 languages and diverse topics, providing a rich resource for advancing LLM capabilities and safety research.

## Method Summary
The dataset was collected from Vicuna demo and Chatbot Arena interfaces, with conversations filtered using OpenAI moderation API for safety tagging. Content moderation models were fine-tuned using Llama2 base models on examples flagged by GPT-4. Safety benchmarks were compiled from jailbreak conversations, while instruction-following models were trained on high-quality conversation subsets. Challenging benchmark questions were identified through GPT-3.5-Turbo scoring of prompts requiring integration of diverse skills.

## Key Results
- Fine-tuned Vicuna-moderator-7B model surpasses GPT-3.5-turbo's performance and matches GPT-4 on content moderation tasks
- Instruction-following models trained on dataset subsets achieve performance comparable to Vicuna on MMLU and MT-bench evaluations
- Dataset enables creation of safety benchmarks from real-world jailbreak attempts and challenging evaluation questions
- Covers 154 languages with diverse topic distribution across technology, science, culture, and daily life domains

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning small LLMs on diverse, real-world conversations labeled by GPT-4 creates models that learn nuanced patterns of harmful content across multiple categories. Core assumption: GPT-4's labeling provides high-quality, reliable training data for content moderation.

### Mechanism 2
User-generated prompts requiring integration of diverse skills (problem-solving, creativity, knowledge) create more effective evaluation benchmarks than domain-specific tests. Core assumption: Human users naturally generate complex, multi-skill tasks when interacting with LLMs.

### Mechanism 3
Training on conversations in 154 languages exposes models to varied linguistic patterns, cultural contexts, and expression styles. Core assumption: Multilingual conversations provide representative coverage of each language's usage patterns.

## Foundational Learning

- **Content moderation categories (hate, self-harm, sexual, violence, harassment)**
  - Why needed: Essential for understanding safety use cases and evaluation criteria
  - Quick check: What are the five moderation categories used by OpenAI's API?

- **Instruction-following dataset curation**
  - Why needed: Critical for understanding how to create effective fine-tuning datasets
  - Quick check: How did the authors select "HighQuality" versus "Upvote" subsets?

- **Jailbreak prompt techniques**
  - Why needed: Important for safety research and understanding model vulnerabilities
  - Quick check: What are the key jailbreak techniques identified in the dataset?

## Architecture Onboarding

- **Component map**: Data collection pipeline (Vicuna demo + Chatbot Arena) -> Content filtering system (OpenAI moderation API tagging) -> Fine-tuning framework (Llama2 base models) -> Evaluation suite (MMLU, MT-bench, custom benchmarks)

- **Critical path**: 1) Collect conversations from live demos, 2) Apply moderation API for safety tagging, 3) Curate subsets for specific use cases, 4) Fine-tune base models on curated data, 5) Evaluate performance against baselines

- **Design tradeoffs**: Raw data vs. filtered data (keeping unsafe content enables safety research but requires careful handling), Scale vs. quality (1M conversations provide diversity but include lower-quality interactions), Multilingual coverage vs. depth (154 languages provide breadth but may lack depth in any single language)

- **Failure signatures**: Poor fine-tuning results (indicates data quality issues or insufficient filtering), Safety benchmark failures (suggests moderation API limitations or insufficient adversarial examples), Evaluation benchmark failures (points to prompt selection bias or insufficient diversity)

- **First 3 experiments**: 1) Fine-tune a small LLM on 10K content moderation examples and evaluate against GPT-4, 2) Create a challenging benchmark subset using top-scoring prompts and test GPT-4 vs GPT-3.5, 3) Train an instruction-following model on "HighQuality" subset and evaluate on MMLU/MT-bench

## Open Questions the Paper Calls Out

- **How does the dataset handle cases where the same user interacts with multiple models in the same conversation session?**
  - Basis: The paper mentions conversations from all three interfaces but doesn't explicitly state how it handles simultaneous multi-model interactions
  - Why unresolved: Understanding this would help researchers better understand dataset structure and potential biases

- **What is the distribution of prompt lengths and complexity across different languages in the dataset?**
  - Basis: While the paper mentions 154 languages, it doesn't provide detailed statistics on prompt characteristics across languages
  - Why unresolved: This information would be valuable for understanding potential biases and how different language communities interact with LLMs

- **How does the dataset ensure the anonymity and privacy of users when conversations contain potentially identifying information?**
  - Basis: The paper mentions best efforts to remove PII but doesn't specify exact methods or effectiveness
  - Why unresolved: The exact methods and effectiveness of PII removal are not specified, which is crucial for understanding privacy guarantees

- **What is the overlap between the LMSYS-Chat-1M dataset and existing benchmark datasets like MMLU or MT-Bench?**
  - Basis: The paper states the dataset may contain questions from existing benchmarks, potentially contaminating training data
  - Why unresolved: The extent of contamination and its implications for benchmarking are not quantified

- **How does the dataset handle cases where users attempt to jailbreak safety measures through indirect or subtle means?**
  - Basis: While the paper discusses jailbreak attempts, it focuses on direct violations and doesn't detail how it handles more subtle attempts
  - Why unresolved: Understanding this would help researchers better assess dataset completeness in capturing real-world safety challenges

## Limitations

- Content moderation performance relies entirely on GPT-4's labeling quality without human validation
- Safety benchmark construction may conflate model safety failures with false positive detections
- Multilingual claims lack systematic language quality assessment and distribution verification

## Confidence

- **High Confidence**: Dataset's basic characteristics (1M conversations, 210K users, 25 LLMs, 154 languages) are well-established
- **Medium Confidence**: Use case demonstrations show promising results but several technical details remain underspecified
- **Low Confidence**: Multilingual utility claims and safety benchmark construction quality lack sufficient validation evidence

## Next Checks

1. **Human Evaluation of Content Moderation Labels**: Recruit human annotators to independently label 500 conversations across all five moderation categories and compare against GPT-4's labels to establish reliability and identify systematic biases.

2. **Language Distribution and Quality Analysis**: Analyze actual conversation counts per language and identify the top 20 most-represented languages. Calculate average conversation length, response quality scores, and topic diversity metrics to verify adequate representation for meaningful model training.

3. **Jailbreak Classification Validation**: Create a blinded evaluation where human safety experts review 200 conversations flagged as potential jailbreak attempts. Classify each as genuine adversarial jailbreak, legitimate conversation with sensitive content, or false positive to recalibrate the safety benchmark.