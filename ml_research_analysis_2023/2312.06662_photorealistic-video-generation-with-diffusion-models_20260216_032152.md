---
ver: rpa2
title: Photorealistic Video Generation with Diffusion Models
arxiv_id: '2312.06662'
source_url: https://arxiv.org/abs/2312.06662
tags:
- video
- generation
- diffusion
- arxiv
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "W.A.L.T presents a transformer-based approach for video generation\
  \ via diffusion modeling, achieving state-of-the-art performance on UCF-101 (FVD:\
  \ 3.3\xB10.0), Kinetics-600 (FVD: 3.3\xB10.0), and ImageNet (FID: 2.56) without\
  \ classifier-free guidance. The method uses a causal encoder for joint image-video\
  \ compression and a window attention architecture for efficient spatial and spatiotemporal\
  \ modeling."
---

# Photorealistic Video Generation with Diffusion Models

## Quick Facts
- **arXiv ID**: 2312.06662
- **Source URL**: https://arxiv.org/abs/2312.06662
- **Reference count**: 40
- **Primary result**: Achieves state-of-the-art FVD of 3.3±0.0 on UCF-101 and Kinetics-600 without classifier-free guidance

## Executive Summary
W.A.L.T presents a transformer-based diffusion model approach for high-resolution video generation that achieves state-of-the-art performance across multiple benchmarks. The method uses a causal 3D CNN encoder for joint image-video compression and a window attention architecture for efficient spatiotemporal modeling. Through a three-stage cascade of diffusion models, W.A.L.T generates 512×896 resolution videos at 8 fps from text prompts, outperforming prior text-to-video methods on UCF-101 FVD (258.1) while using fewer parameters than leading systems.

## Method Summary
W.A.L.T employs a three-stage cascade of diffusion models: a base latent video diffusion model (128×128) followed by two video super-resolution diffusion models that upscale to 512×896 resolution. The system uses a causal 3D CNN encoder to jointly compress images and videos within a unified latent space, enabling static images to be treated as single-frame videos. A transformer backbone with alternating spatial and spatiotemporal window attention processes the latent representations, while AdaLN-LoRA reduces conditioning parameter count through parameter sharing. Text conditioning is achieved via cross-attention without classifier-free guidance, and generation is performed using DDIM sampling.

## Key Results
- Achieves FVD of 3.3±0.0 on UCF-101 and Kinetics-600 without classifier-free guidance
- Generates 512×896 resolution videos at 8 fps from text prompts
- Outperforms prior text-to-video methods on UCF-101 FVD (258.1)
- Uses significantly fewer parameters than leading text-to-video systems like Imagen Video (5.7B parameters)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Causal 3D CNN encoder enables unified latent space for images and videos by independently encoding the first frame
- Mechanism: The causal 3D convolution only operates on past frames, ensuring the first frame output depends solely on itself, allowing static images to be treated as single-frame videos
- Core assumption: Causal convolution design is necessary to process first frame independently
- Evidence anchors:
  - [abstract] "causal encoder to jointly compress images and videos within a unified latent space"
  - [section 4.1] "causal 3D CNN encoder-decoder architecture...ensures that the output for each frame is influenced solely by the preceding frames, enabling the model to tokenize the first frame independently"
  - [corpus] Weak - no direct corpus evidence found for causal 3D CNN design choice
- Break condition: If non-causal convolution is used, the first frame would depend on nonexistent future frames, breaking the unified representation

### Mechanism 2
- Claim: Window attention with alternating spatial and spatiotemporal configurations enables efficient joint training on images and videos
- Mechanism: Spatial windows process all tokens within a frame (images and video frames), while spatiotemporal windows model temporal dynamics across video frames. Identity attention masks pass image frames through without temporal processing
- Core assumption: Local window attention is sufficient to capture relevant spatial and temporal relationships
- Evidence anchors:
  - [abstract] "window attention architecture tailored for joint spatial and spatiotemporal generative modeling"
  - [section 4.2] "self-attention layers that alternate between non-overlapping, window-restricted spatial and spatiotemporal attention"
  - [corpus] Moderate - related works like "Align your Latents" use window attention but don't explicitly detail the alternating spatial/spatiotemporal design
- Break condition: If windows are too small, important long-range dependencies may be missed; if too large, efficiency gains are lost

### Mechanism 3
- Claim: AdaLN-LoRA reduces conditioning parameter count while maintaining performance through parameter sharing across layers
- Mechanism: Instead of learning separate conditioning parameters for each layer, AdaLN-LoRA uses a shared base conditioning (A1) and layer-specific residual projections (W_i^b W_i^a) to condition subsequent layers
- Core assumption: The shared base conditioning captures common information while residuals capture layer-specific variations
- Evidence anchors:
  - [section 4.3] "For each layer, we regress conditioning parameters as A1 = MLP(c + t), Ai = A1 + W_i^b W_i^a(c + t) ∀i ≠ 1"
  - [section 5.2] "AdaLN-LoRA layers...This reduces the number of trainable model parameters significantly"
  - [corpus] Weak - no direct corpus evidence for this specific AdaLN-LoRA formulation
- Break condition: If r is too small, conditioning capacity is insufficient; if too large, parameter savings diminish

## Foundational Learning

- Concept: Diffusion model training objective (Eq. 2)
  - Why needed here: Understanding the denoising objective and v-prediction target is crucial for implementing the model correctly
  - Quick check question: What is the difference between predicting noise epsilon vs v = sqrt(1-γ(t)) epsilon - sqrt(γ(t)) x_0?

- Concept: Transformer self-attention and window attention
  - Why needed here: The window attention mechanism is the core architectural innovation for efficient video processing
  - Quick check question: How does the attention mask differ between spatial windows (S) and spatiotemporal windows (ST)?

- Concept: Classifier-free guidance and its alternatives
  - Why needed here: The paper achieves strong results without classifier-free guidance, which is unusual for diffusion models
  - Quick check question: What conditioning mechanism does W.A.L.T use instead of classifier-free guidance for text-to-video?

## Architecture Onboarding

- Component map:
  - Causal 3D CNN encoder (stage 1) → learns unified image/video latent space
  - Transformer backbone with alternating window attention (stage 2) → generates from latents
  - Cascade of 3 diffusion models → produces final 512×896 resolution videos

- Critical path: Encoder → Transformer backbone (with window attention and AdaLN-LoRA) → Diffusion sampling (DDIM)

- Design tradeoffs:
  - Window size vs. computational efficiency vs. modeling capacity
  - Patch size vs. generation quality vs. memory usage
  - r parameter in AdaLN-LoRA vs. parameter efficiency vs. conditioning capacity

- Failure signatures:
  - Flickering artifacts: Likely issue with spatial-only autoencoder design
  - Poor text alignment: Issues with cross-attention or conditioning mechanism
  - Memory errors: Window sizes too large or patch size too small

- First 3 experiments:
  1. Train with default settings on UCF-101 and verify FVD score
  2. Compare spatial-only vs. spatiotemporal window attention configurations
  3. Test different patch sizes (1, 2, 4) to understand quality vs. efficiency tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of increasing the latent dimension c beyond the empirically determined "sweet spot" of c=8 on video generation quality?
- Basis in paper: [explicit] "As shown in Table 3f, increasing c significantly improves the reconstruction quality (lower rFVD) while keeping the same spatial fs and temporal compression ft ratios. Empirically, we found that both lower and higher values of c lead to poor FVD scores in generation, with a sweet spot of c=8 working well across most datasets and tasks we evaluated."
- Why unresolved: The paper only shows results for c=8 and mentions that both lower and higher values lead to poor FVD scores, but doesn't explore the specific behavior at values significantly above c=8.
- What evidence would resolve it: Generating videos with different c values (e.g., c=16, 32, 64) and comparing FVD scores would reveal if there's a plateau or decline in performance beyond c=8.

### Open Question 2
- Question: How does the performance of W.A.L.T scale with increasing model size, particularly when approaching or exceeding the parameter counts of leading text-to-video systems like Imagen Video (5.7B parameters)?
- Basis in paper: [explicit] "Scaling our base model size leads to significant improvements on both the metrics. It is important to note, however, that our base model is considerably smaller than leading text-to-video systems. For instance, Ho et al. [34] trained base model of 5.7B parameters. Hence, we believe scaling our models further is an important direction of future work."
- Why unresolved: The paper only shows results for the 3B parameter base model and doesn't explore scaling to the sizes of current state-of-the-art models.
- What evidence would resolve it: Training and evaluating W.A.L.T models with parameter counts ranging from 3B to 10B+ would quantify the relationship between model size and generation quality.

### Open Question 3
- Question: What is the effect of using full self-attention instead of window-restricted attention on video generation quality, and is the performance gap significant enough to justify the increased computational cost?
- Basis in paper: [explicit] "Full self-attention is not essential for good performance. sps is steps per sec." (Table 3b shows FVD and IS scores for different window sizes, including full self-attention)
- Why unresolved: While the paper shows that window attention performs competitively with full self-attention, it doesn't provide a direct comparison of the absolute performance difference or discuss whether the performance gap is worth the computational trade-off.
- What evidence would resolve it: Training and evaluating models with full self-attention and comparing their FVD and IS scores to the window attention variants would quantify the performance difference and allow for an informed decision about the computational trade-off.

## Limitations
- Limited ablation studies on individual architectural contributions
- Novel AdaLN-LoRA conditioning mechanism lacks extensive validation
- Performance comparisons only against models using classifier-free guidance

## Confidence
- **High confidence**: Performance metrics on standard benchmarks (FVD scores of 3.3±0.0 for UCF-101 and Kinetics-600, FID of 2.56 for ImageNet)
- **Medium confidence**: Window attention architecture's efficiency claims are plausible but lack strong corpus validation
- **Low confidence**: Effectiveness of causal 3D CNN encoder for unified image-video compression and specific AdaLN-LoRA formulation are novel with minimal external validation

## Next Checks
1. **Ablation study on causal vs. non-causal encoding**: Train W.A.L.T with a non-causal encoder variant to empirically verify that the causal design is essential for unified image-video representation and first-frame independence.

2. **Window attention configuration analysis**: Systematically compare spatial-only, spatiotemporal-only, and alternating window attention configurations on UCF-101 to quantify the contribution of each architectural choice to the reported FVD improvements.

3. **Classifier-free guidance comparison**: Implement and evaluate W.A.L.T with classifier-free guidance to determine whether the claimed superior performance without guidance is truly due to the model architecture or if guidance could further improve results.