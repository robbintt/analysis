---
ver: rpa2
title: 'Model Evaluation for Domain Identification of Unknown Classes in Open-World
  Recognition: A Proposal'
arxiv_id: '2312.05454'
source_url: https://arxiv.org/abs/2312.05454
tags:
- domain
- unknown
- classes
- learning
- birds
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an evaluation protocol for domain identification
  of unknown classes in open-world recognition tasks. The authors address the problem
  of separating unknown in-domain (ID) and out-of-domain (OOD) classes in domain-specific
  applications where not all unknown samples are relevant for learning.
---

# Model Evaluation for Domain Identification of Unknown Classes in Open-World Recognition: A Proposal

## Quick Facts
- arXiv ID: 2312.05454
- Source URL: https://arxiv.org/abs/2312.05454
- Reference count: 40
- Primary result: Pre-trained model representations enable effective domain identification of unknown classes, with MobileNetV3 achieving highest performance for garbage classification

## Executive Summary
This paper proposes an evaluation protocol for domain identification of unknown classes in open-world recognition tasks. The authors address the problem of separating unknown in-domain (ID) and out-of-domain (OOD) classes in domain-specific applications where not all unknown samples are relevant for learning. They propose three approaches: traditional transfer learning with linear classifiers, Automated Machine Learning (AutoML), and Nearest Class Mean (NCM) classifier with First Integer Neighbor Clustering Hierarchy (FINCH). The evaluation uses five domains (garbage, food, dogs, plants, birds) and measures performance using Balanced Accuracy (BACCU). Results show that all approaches yield good accuracy, with MobileNetV3 achieving the highest BACCU score for garbage classification.

## Method Summary
The study evaluates three approaches for domain identification of unknown classes: (1) Transfer learning using pre-trained models with frozen backbones and linear classifiers, (2) AutoML using AutoSklearn 2.0 with 30-minute budget, and (3) NCM classifier with FINCH clustering on pre-trained features. The evaluation uses five domains (garbage, food, dogs, plants, birds) with known classes for training and unknown classes for testing. Performance is measured using Balanced Accuracy (BACCU) to assess the ability to separate unknown in-domain from out-of-domain samples. The protocol involves extracting features from pre-trained models, applying the chosen classification method, and evaluating on held-out classes.

## Key Results
- All three approaches (transfer learning, AutoML, NCM+FINCH) achieved good domain identification accuracy across five tested domains
- MobileNetV3 achieved the highest BACCU score (86.05%) for garbage classification
- Strong feature representations from pre-trained models were found to be crucial for identifying unknown classes within the same domain
- The protocol successfully demonstrated that pre-trained models can distinguish between unknown in-domain and out-of-domain classes

## Why This Works (Mechanism)

### Mechanism 1
- Pre-trained model representations capture domain-specific discriminative features even for unknown classes
- Mechanism: Pre-trained models trained on large-scale datasets like ImageNet develop rich feature representations that generalize beyond their original training classes
- Core assumption: Domain-specific visual features learned during pre-training transfer effectively to new but related domains
- Evidence: Results suggest strong representations in pre-trained models are important for identifying unknown classes in the same domain

### Mechanism 2
- FINCH clustering followed by NCM classification provides effective domain separation without requiring labeled data
- Mechanism: FINCH algorithm creates clusters based on first-neighbor relationships, then NCM classifier assigns new samples to the nearest cluster mean
- Core assumption: Domain-specific features create meaningful cluster separations that correspond to domain boundaries
- Evidence: FINCH used to obtain clusters of training dataset, helping find appropriate representable clusters in feature space

### Mechanism 3
- AutoML ensembles provide robust baseline performance by automatically selecting optimal classifier configurations
- Mechanism: AutoML systems like AutoSklearn 2.0 explore multiple classifier configurations and ensemble them
- Core assumption: Feature space contains sufficient discriminative information for classifier ensembles to separate domains effectively
- Evidence: AutoML results in optimal ensembled machine learning models, avoiding biased justification in classifier selection

## Foundational Learning

- **Transfer Learning and Feature Extraction**: Understanding how feature representations transfer across domains is crucial since the paper relies on pre-trained models as feature extractors. Quick check: What happens to classification performance when you freeze pre-trained model weights versus fine-tuning them?

- **Open-World Recognition and Domain Adaptation**: The core problem is distinguishing unknown classes within vs outside the domain of interest, requiring understanding of open-world recognition principles. Quick check: How does open-world recognition differ from traditional closed-world classification?

- **Clustering and Nearest Class Mean Classification**: The FINCH+NCM approach forms the basis for one evaluation method, so understanding unsupervised clustering and centroid-based classification is essential. Quick check: What are the advantages and limitations of using nearest class mean classifiers compared to distance-based approaches?

## Architecture Onboarding

- **Component map**: Data Preparation (Domain datasets) -> Feature Extraction (Pre-trained models) -> Classification Methods (Transfer Learning, AutoML, NCM+FINCH) -> Evaluation (BACCU metric)

- **Critical path**: Load pre-trained model and extract features from training data → Apply chosen classification method → Train classifier on extracted features with ID vs OOD labels → Test on held-out classes from same and different domains → Calculate BACCU score

- **Design tradeoffs**: Pre-trained model choice vs. domain specificity, Simple classifiers vs. complex models for feature space, Computational cost of AutoML vs. simpler approaches, Number of clusters in FINCH vs. generalization capability

- **Failure signatures**: Low BACCU scores across all approaches indicate poor feature separability, High variance in BACCU across different pre-trained models suggests domain mismatch, FINCH clustering failure (too few or too many clusters) indicates poor feature structure

- **First 3 experiments**: Verify feature extraction works by visualizing t-SNE plots of ID vs OOD samples, Test simple logistic regression on extracted features to establish baseline, Compare BACCU scores across all pre-trained models for a single domain to identify best performer

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of domain identification vary when using different numbers of unknown classes for training and testing?
- Basis: The paper discusses training with unknown ID and OOD classes but does not explore varying the number of classes used
- Why unresolved: The study used a fixed number of classes (20) for each domain and did not investigate how performance changes with different numbers of unknown classes
- What evidence would resolve it: Systematic experiments varying the number of unknown classes in both training and testing phases across different domains would show how class count affects domain identification performance

### Open Question 2
- Question: What is the impact of domain relatedness on the ability to distinguish between unknown ID and OOD classes?
- Basis: The paper emphasizes the importance of identifying unknown classes within the domain of interest but does not explicitly study how domain relatedness affects performance
- Why unresolved: While the paper uses different domains, it does not analyze how the degree of similarity between domains influences the separation of unknown ID and OOD
- What evidence would resolve it: Experiments measuring domain identification performance when using domains with varying degrees of semantic or visual similarity would clarify the impact of domain relatedness

### Open Question 3
- Question: How do different clustering algorithms beyond FINCH affect the performance of NCM classifiers in domain identification?
- Basis: The paper uses FINCH for clustering in the NCM classifier approach but does not compare it with other clustering algorithms
- Why unresolved: The study only evaluates FINCH for clustering and does not explore whether other clustering methods might yield better or worse performance for domain identification
- What evidence would resolve it: Comparative experiments using various clustering algorithms (e.g., K-means, DBSCAN, hierarchical clustering) with the NCM classifier would reveal the impact of clustering method choice on domain identification accuracy

## Limitations
- Evaluation protocol relies on manually defined domain boundaries that may not generalize to ambiguous applications
- Limited domain diversity - only five visual object categories tested, potentially missing domain-specific challenges
- No ablation studies on impact of different pre-training datasets or fine-tuning strategies
- AutoML configuration details are sparse, making reproducibility difficult

## Confidence
- **High confidence** in core finding that pre-trained model representations enable domain identification for unknown classes
- **Medium confidence** in relative performance rankings between the three approaches
- **Low confidence** in generalizability to non-visual domains or significantly different characteristics

## Next Checks
1. Test the protocol on additional domains (e.g., medical imaging, satellite imagery) to assess domain generalization beyond object recognition
2. Conduct ablation studies varying pre-training datasets and fine-tuning strategies to quantify impact on domain identification performance
3. Implement statistical significance testing across multiple experimental runs to validate robustness of performance differences between approaches