---
ver: rpa2
title: 'Fair Diffusion: Instructing Text-to-Image Generation Models on Fairness'
arxiv_id: '2302.10893'
source_url: https://arxiv.org/abs/2302.10893
tags:
- fair
- diffusion
- fairness
- images
- gender
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates bias in large-scale text-to-image generation
  models, specifically focusing on gender occupation bias. The authors analyze Stable
  Diffusion, its underlying dataset (LAION-5B), and its pre-trained model (CLIP) to
  uncover biases.
---

# Fair Diffusion: Instructing Text-to-Image Generation Models on Fairness

## Quick Facts
- arXiv ID: 2302.10893
- Source URL: https://arxiv.org/abs/2302.10893
- Reference count: 40
- Primary result: Fair Diffusion reduces gender occupation bias in Stable Diffusion by 50%±4% through textual guidance during inference

## Executive Summary
This paper investigates gender occupation bias in large-scale text-to-image generation models, specifically analyzing Stable Diffusion, its training dataset (LAION-5B), and its underlying model (CLIP). The authors propose Fair Diffusion, a novel approach to mitigate biases at the deployment stage without requiring additional training or dataset filtering. Fair Diffusion uses textual instructions to guide the model toward fairer outcomes by balancing the representation of different gender identities in generated images.

The approach is evaluated using a dataset of over 1.8 million images and 37,000 generated images, demonstrating significant reduction in gender occupation bias. The results show that Fair Diffusion successfully shifts the gender proportion within a fair boundary, achieving fairness according to the proposed definition. The paper also explores extensions to address other biases such as heteronormativity and age discrimination, though these are presented as preliminary results requiring further research.

## Method Summary
Fair Diffusion is a deployment-stage approach that mitigates bias in text-to-image generation models by using semantic guidance during inference. The method employs classifier-free guidance extended with semantic guidance (Sega) to steer diffusion models by adding fair guidance terms to the noise prediction. It randomly alternates between promoting "female person" and "male person" with equal probability (50/50), ensuring balanced representation in output images. The approach measures fairness by computing the proportion of generated images classified as female-appearing versus male-appearing for each occupation, then adjusts generation to achieve the target proportion. Fair Diffusion uses textual instructions processed through a text encoder (CLIP) to influence image generation, followed by classification using FairFace to evaluate outcomes.

## Key Results
- Fair Diffusion reduces gender occupation bias in Stable Diffusion by shifting gender proportions toward 50%±4% target boundary
- The approach successfully mitigates biases in occupations with different baseline bias levels (amplification, reflection, mitigation)
- Fair Diffusion can be extended to address other bias types including heteronormativity and age discrimination through semantic guidance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fair Diffusion reduces gender occupation bias by conditioning image generation with alternating fair guidance prompts.
- Mechanism: The approach uses semantic guidance (Sega) to steer diffusion models by adding fair guidance terms to the noise prediction. It randomly alternates between promoting "female person" and "male person" with equal probability (50/50), ensuring balanced representation in the output images.
- Core assumption: The text encoder can properly encode and apply semantic concepts like "female person" and "male person" to influence image generation.
- Evidence anchors:
  - [abstract] "Fair Diffusion uses textual instructions to guide the model towards fairer outcomes by balancing the representation of different gender identities"
  - [section 3.3] "To ensure that both attributes are equally represented in the model's outcome (Eq. 4), we randomly choose the direction of the guidance with equal probability"
  - [corpus] Weak evidence - related papers mention similar fairness approaches but don't directly confirm this specific mechanism
- Break condition: If the text encoder cannot properly encode the fair guidance concepts, or if the diffusion model ignores the conditioning, the bias mitigation will fail.

### Mechanism 2
- Claim: Fair Diffusion achieves fairness according to Definition 2 by ensuring model-generated proportions match user-defined target distributions.
- Mechanism: The approach measures fairness by computing the proportion of generated images classified as female-appearing versus male-appearing for each occupation, then adjusts generation to achieve the target proportion (50%±4% for binary gender).
- Core assumption: Automated gender classification (FairFace) provides reliable binary gender labels for evaluating fairness.
- Evidence anchors:
  - [section 3.2] "We define fairness for Fair Diffusion... as algorithmic fairness for a dataset and model" with definitions 1 and 2
  - [section 4.1] "We employed FairFace as κ to classify facial (gender) attributes"
  - [corpus] Moderate evidence - related work confirms the importance of measurement in fairness evaluation
- Break condition: If gender classification is unreliable or if the target distribution is inappropriate for the context, the fairness metric becomes invalid.

### Mechanism 3
- Claim: Fair Diffusion can be applied to mitigate multiple types of bias beyond gender, including heteronormativity and age discrimination.
- Mechanism: The semantic guidance interface allows adding multiple concept descriptions simultaneously, enabling isolation and editing of different features like race, ethnicity, and age.
- Core assumption: The diffusion model has learned representations for these additional concepts that can be manipulated through textual guidance.
- Evidence anchors:
  - [section 4.4] "Another step toward a fairer model outcome is to go beyond gender editing" with examples of racial and age bias mitigation
  - [section 5] "Fair Diffusion can be employed again to generate homosexual couples and people of different ages"
  - [corpus] Limited evidence - the paper provides qualitative examples but lacks quantitative evaluation for these additional bias types
- Break condition: If the diffusion model lacks proper representations for these concepts, or if the guidance cannot effectively manipulate them, bias mitigation will be incomplete.

## Foundational Learning

- Concept: Diffusion Models and Classifier-Free Guidance
  - Why needed here: Understanding how diffusion models generate images and how classifier-free guidance works is essential to grasp how Fair Diffusion modifies the generation process.
  - Quick check question: How does classifier-free guidance extend the basic diffusion model to condition on text prompts?

- Concept: Fairness Definitions and Measurement
  - Why needed here: The paper uses specific definitions of dataset fairness and model fairness that are crucial for understanding how success is measured.
  - Quick check question: What is the difference between dataset fairness (Def. 1) and model fairness (Def. 2) in this context?

- Concept: Bias Inspection and Association Tests
  - Why needed here: Understanding how biases are measured in datasets, models, and outcomes through techniques like iEAT is key to evaluating Fair Diffusion's effectiveness.
  - Quick check question: How does the image Embedding Association Test (iEAT) measure bias in learned representations?

## Architecture Onboarding

- Component map: User prompt → Fair Diffusion guidance addition → Text encoder processing → Noise prediction with fair guidance → Image generation → Classification → Fairness evaluation
- Critical path: User prompt → Fair Diffusion guidance addition → Text encoder processing → Noise prediction with fair guidance → Image generation → Classification → Fairness evaluation
- Design tradeoffs: The approach trades perfect fairness for practical controllability, requiring binary gender classification and limiting to concepts the model can represent, while avoiding dataset filtering and additional training.
- Failure signatures: High variance in generated images, classifier uncertainty on ambiguous outputs, persistent bias in specific occupations, or poor performance when extending beyond binary gender concepts.
- First 3 experiments:
  1. Generate images for "firefighter" with default SD and measure female-appearing proportion, then repeat with Fair Diffusion to verify mitigation within fair boundary
  2. Test Fair Diffusion on occupations with different bias directions (amplification, reflection, mitigation) to verify consistent performance
  3. Experiment with different guidance concepts (age, race) on the "woman" prompt to verify multi-bias capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of using non-binary gender labels in Fair Diffusion, and how can the model be adapted to handle non-binary gender identities effectively?
- Basis in paper: [explicit] The paper acknowledges the limitation of binary gender labeling and suggests that current automated measures treat gender as a binary-valued attribute, which it is not. The authors advocate for research on encoding gender as non-binary in generative and predictive models.
- Why unresolved: The paper does not provide a concrete solution or evaluation of Fair Diffusion with non-binary gender labels. The authors only mention the limitation and suggest the need for further research in this area.
- What evidence would resolve it: An experimental evaluation of Fair Diffusion using non-binary gender labels, along with a comparison to the binary gender labeling approach, would provide insights into the effectiveness of the model in handling non-binary gender identities.

### Open Question 2
- Question: How can Fair Diffusion be extended to address biases beyond gender, such as heteronormativity and age discrimination, and what are the challenges in implementing such extensions?
- Basis in paper: [explicit] The paper presents some preliminary results on addressing heteronormativity and age discrimination using Fair Diffusion. However, the authors acknowledge that these results are preliminary and suggest that further research is needed in this area.
- Why unresolved: The paper does not provide a comprehensive evaluation of Fair Diffusion in addressing biases beyond gender. The authors only show some illustrative examples and suggest the need for more research in this area.
- What evidence would resolve it: A thorough evaluation of Fair Diffusion in addressing various types of biases, such as heteronormativity, age discrimination, and others, would provide insights into the effectiveness of the model in handling these biases. Additionally, an analysis of the challenges and limitations in implementing such extensions would be valuable.

### Open Question 3
- Question: What are the potential risks and ethical considerations associated with user interaction in Fair Diffusion, and how can these be mitigated to ensure responsible use of the technology?
- Basis in paper: [explicit] The paper discusses the dual-use dilemma of Fair Diffusion, acknowledging that while it can be used to promote fairness, it can also be misused to amplify biases. The authors suggest the need for further detection mechanisms for malicious interaction.
- Why unresolved: The paper does not provide a detailed analysis of the potential risks and ethical considerations associated with user interaction in Fair Diffusion. The authors only mention the dual-use dilemma and suggest the need for further research in this area.
- What evidence would resolve it: An in-depth analysis of the potential risks and ethical considerations associated with user interaction in Fair Diffusion, along with proposed mitigation strategies, would provide insights into the responsible use of the technology. Additionally, an evaluation of the effectiveness of these mitigation strategies would be valuable.

## Limitations

- The binary gender classification system used for evaluation cannot capture non-binary or gender-fluid representations, limiting the applicability of the fairness definition to real-world diversity.
- While Fair Diffusion shows promise for gender bias mitigation, the paper provides limited quantitative evidence for its effectiveness on other bias types (race, age, heteronormativity).
- The approach requires careful prompt engineering and guidance tuning for each bias type, making it less scalable than dataset-level interventions.

## Confidence

**High Confidence**: The effectiveness of Fair Diffusion in reducing gender occupation bias within the binary framework (50%±4% target), based on quantitative evaluation across 37,000+ generated images and 1.8M dataset images.

**Medium Confidence**: The mechanism by which semantic guidance influences diffusion model outputs through classifier-free guidance extension, though the exact mathematical relationship between guidance strength and bias mitigation is not fully characterized.

**Low Confidence**: Claims about Fair Diffusion's effectiveness on non-binary gender representation and other bias types beyond gender, due to lack of quantitative evaluation and reliance on qualitative examples only.

## Next Checks

1. **Bias Transfer Analysis**: Test Fair Diffusion's performance on occupations with different baseline bias levels (amplification, reflection, mitigation) to verify consistent behavior across the bias spectrum rather than just at the extremes.

2. **Concept Representation Validation**: Conduct ablation studies removing specific guidance concepts to measure their individual contributions to bias reduction, and test guidance effectiveness on concepts outside the binary gender framework.

3. **Long-term Stability Assessment**: Generate extended image sequences for the same occupation and measure variance in gender proportions over time to check for guidance drift or model adaptation that could undermine sustained fairness.