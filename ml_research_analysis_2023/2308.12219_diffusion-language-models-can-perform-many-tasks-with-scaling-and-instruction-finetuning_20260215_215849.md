---
ver: rpa2
title: Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning
arxiv_id: '2308.12219'
source_url: https://arxiv.org/abs/2308.12219
tags:
- language
- diffusion
- arxiv
- preprint
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that scaling diffusion language models
  w.r.t. data, model sizes, and tasks can effectively make them strong language learners.
---

# Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning

## Quick Facts
- arXiv ID: 2308.12219
- Source URL: https://arxiv.org/abs/2308.12219
- Authors: (not specified in source)
- Reference count: 40
- This paper demonstrates that scaling diffusion language models w.r.t. data, model sizes, and tasks can effectively make them strong language learners.

## Executive Summary
This paper presents diffusion language models as a viable alternative to autoregressive models for language tasks. By establishing an intrinsic connection between masked language models and discrete diffusion models, the authors show that pretrained MLMs can be directly adapted into diffusion models without expensive retraining. The key innovation is demonstrating that scaling diffusion models (up to 9.7B parameters) consistently improves performance across diverse downstream tasks, and that instruction finetuning enables zero-shot and few-shot in-context learning abilities. The results suggest diffusion language models can serve as competitive sequence generative models, achieving performance comparable to autoregressive models on translation and summarization tasks while offering the potential for more flexible generation strategies.

## Method Summary
The authors build diffusion language models by first pretraining with masked language modeling objectives on massive raw textual data, then applying diffusive adaptation to convert these pretrained MLMs into diffusion models. They use a decoder-only architecture based on XLM-RoBERTa, finetuning with a reparameterized discrete diffusion objective. The models are evaluated on downstream tasks including translation (IWSLT14 DE→EN, WMT14 EN→DE), summarization (Gigaword), and reasoning tasks (MMLU). Instruction finetuning is applied using the Flan 2022 Collection dataset to elicit zero-shot and few-shot abilities. Sampling is performed through iterative denoising with 50 steps using a top-k log-probability unmasking strategy.

## Key Results
- Scaling diffusion language models from 86M to 9.7B parameters consistently improves performance across translation, summarization, and classification tasks
- Instruction finetuning enables diffusion models to perform zero-shot and few-shot in-context learning on unseen tasks by following natural language instructions
- Diffusion models achieve competitive BLEU scores on translation tasks (e.g., 34.87 on WMT14 EN→DE with 9.7B parameters) compared to autoregressive baselines
- The intrinsic connection between MLMs and diffusion models allows direct utilization of massive pretrained language models without retraining

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diffusion language models can inherit knowledge from masked language model pretraining through their intrinsic connection.
- Mechanism: Absorbing diffusion with stationary distribution q(xT) = {1 if xT = [MASK]; 0 otherwise} naturally maps to the masked language modeling objective, allowing diffusion models to directly leverage pretrained MLMs without expensive retraining.
- Core assumption: The diffusion transition probability q(xt|xt−1) can be expressed as a convex combination between data and noise, aligning with MLM's masked token prediction.
- Evidence anchors:
  - [abstract]: "We first demonstrate the intrinsic connection between masked language models (Devlin et al., 2018) and discrete diffusion models"
  - [section]: "This connection allows us to establish diffusion language models by pretraining with MLM objectives from massive raw textual data."
  - [corpus]: Weak evidence - related papers focus on instruction tuning rather than the MLM-diffusion connection.
- Break condition: If the noise schedule βt or stationary distribution qnoise doesn't align with MLM masking ratios, the pretraining benefit diminishes.

### Mechanism 2
- Claim: Scaling diffusion language models improves performance across diverse downstream tasks.
- Mechanism: Larger models (up to 9.7B parameters) capture more complex linguistic patterns and generalize better to tasks like translation and summarization.
- Core assumption: The scaling law observed in autoregressive models applies similarly to diffusion models, where performance increases predictably with parameter count.
- Evidence anchors:
  - [abstract]: "Experiments show that scaling diffusion language models consistently improves performance across downstream language tasks."
  - [section]: "Fig. 3 shows the scaling curve of model performance with respect to model sizes. It demonstrates that the performance of the finetuned diffusion models substantially increases as the model scales increase."
  - [corpus]: Weak evidence - corpus neighbors focus on instruction tuning rather than scaling effects.
- Break condition: If the scaling law saturates or follows a different pattern than autoregressive models, additional scaling may not yield proportional benefits.

### Mechanism 3
- Claim: Instruction finetuning enables diffusion language models to perform zero-shot and few-shot in-context learning.
- Mechanism: Finetuning on instruction-formatted datasets teaches the model to follow natural language instructions, transferring learned task-solving patterns to unseen problems.
- Core assumption: The diffusion denoising process can learn to generate coherent sequences from masked inputs when trained on instruction-response pairs.
- Evidence anchors:
  - [abstract]: "We further discover that instruction finetuning can elicit zero-shot and few-shot in-context learning abilities that help tackle many unseen tasks by following natural language instructions"
  - [section]: "By leveraging instruction finetuning, we can further elicit zero-shot and few-shot abilities for diffusion language models to tackle unseen tasks by following natural language instructions."
  - [corpus]: Moderate evidence - related papers discuss instruction finetuning but focus on different model types.
- Break condition: If the instruction finetuning data lacks sufficient task diversity or the diffusion model architecture cannot handle complex reasoning steps, zero-shot performance degrades.

## Foundational Learning

- Concept: Discrete diffusion models and their connection to masked language modeling
  - Why needed here: Understanding this connection is crucial for leveraging pretrained MLMs as diffusion models without retraining
  - Quick check question: What stationary distribution in absorbing diffusion corresponds to the MLM objective?

- Concept: Non-autoregressive generation and iterative refinement
  - Why needed here: Diffusion models generate through iterative denoising rather than left-to-right prediction, requiring different sampling strategies
  - Quick check question: How does the top-k log-probability unmasking strategy differ from autoregressive sampling?

- Concept: Instruction finetuning and zero-shot learning
  - Why needed here: This is the mechanism by which diffusion models acquire general-purpose abilities to follow instructions
  - Quick check question: What distinguishes zero-shot from few-shot in-context learning in diffusion models?

## Architecture Onboarding

- Component map: Pretrained MLM → Diffusive adaptation (finetuning with diffusion objective) → Task-specific/instruction finetuning → Sampling via iterative denoising
- Critical path: Initialize with XLM-R → Apply diffusive adaptation → Evaluate on downstream tasks
- Design tradeoffs: Decoder-only architecture vs. encoder-decoder (global receptive field vs. conditional encoding), fixed masking ratios vs. adaptive schedules
- Failure signatures: Poor generation quality (stuck in local optima during denoising), catastrophic forgetting of pretraining knowledge, inability to follow instructions
- First 3 experiments:
  1. Apply diffusive adaptation to XLM-R-BASE on IWSLT14 DE→EN translation
  2. Scale up to XLM-R-XXL and repeat translation task
  3. Apply instruction finetuning on Flan 2022 and test zero-shot performance on MMLU

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can diffusion language models achieve reasoning abilities comparable to autoregressive models with further scaling and improved training recipes?
- Basis in paper: [inferred] The paper shows that current diffusion language models struggle with reasoning tasks like GSM8K and MGSM, despite showing promising structured reasoning behaviors in qualitative analysis.
- Why unresolved: The authors note that the largest XLM-R model used is around 10B parameters, which may be below the threshold needed for reasoning abilities. Additionally, the training recipe for XLM-R differs from that of state-of-the-art autoregressive models, lacking crucial data like code or scientific literature.
- What evidence would resolve it: Training diffusion language models with larger model sizes (e.g., 100B+ parameters) and improved pretraining recipes (including code and scientific literature) could potentially demonstrate reasoning abilities comparable to autoregressive models on benchmarks like GSM8K and MGSM.

### Open Question 2
- Question: Is instruction finetuning on diffusion language models an effective method for improving performance on unseen tasks, and how does it compare to autoregressive models?
- Basis in paper: [explicit] The paper demonstrates that instruction finetuning can elicit zero-shot and few-shot in-context learning abilities in diffusion language models, with performance improving as model size increases. However, the authors do not directly compare instruction finetuning effectiveness to autoregressive models.
- Why unresolved: While the paper shows promising results for diffusion language models, a direct comparison with autoregressive models finetuned with instructions is needed to determine if diffusion language models can match or exceed their performance on unseen tasks.
- What evidence would resolve it: Conducting a head-to-head comparison of instruction-finetuned diffusion and autoregressive language models on a diverse set of unseen tasks, including both generation and reasoning tasks, would provide insights into their relative effectiveness.

### Open Question 3
- Question: Can diffusion language models benefit from pretraining on tasks specifically designed to improve reasoning abilities, such as chain-of-thought reasoning?
- Basis in paper: [inferred] The authors note that the current pretraining recipe for XLM-R models lacks data like code or scientific literature, which are hypothesized to be crucial for reasoning ability. They also mention the potential of using specialized reasoning data for distillation from more capable models.
- Why unresolved: The paper does not explore the impact of pretraining diffusion language models on reasoning-specific tasks or data. It remains unclear whether such pretraining would lead to significant improvements in reasoning abilities.
- What evidence would resolve it: Pretraining diffusion language models on datasets containing code, scientific literature, or chain-of-thought reasoning examples, and then evaluating their performance on reasoning tasks like GSM8K and MGSM, would provide insights into the effectiveness of this approach.

## Limitations
- The performance on complex reasoning tasks remains limited, with diffusion models struggling on benchmarks like GSM8K and MGSM despite scaling efforts
- The reliance on XLM-RoBERTa architecture may limit generalizability, as performance gains may not transfer to other pretrained architectures like BERT or T5
- The computational cost of 50 denoising steps for sampling may be prohibitive compared to single-pass autoregressive generation

## Confidence
- High Confidence: The intrinsic connection between masked language models and discrete diffusion models is well-established theoretically and demonstrated empirically
- Medium Confidence: The scaling improvements observed are consistent but may not hold across all architectures and task types
- Low Confidence: The claim that diffusion language models can match autoregressive performance across all tasks is not fully supported, particularly for tasks requiring complex reasoning or long-form generation

## Next Checks
- Check 1: Architecture Transferability Test - Implement diffusive adaptation on a BERT-base model and evaluate on the same downstream tasks (IWSLT14 DE→EN translation, WMT14 EN→DE translation, Gigaword summarization). Compare performance with XLM-R results to assess architectural dependency.
- Check 2: Scaling Saturation Analysis - Extend the scaling experiments beyond 9.7B parameters to 50B+ parameter models to identify potential saturation points in the scaling curve. Test whether the observed scaling law holds at extreme scales or follows a different pattern.
- Check 3: Sampling Efficiency Benchmark - Compare generation quality vs. computational cost between diffusion models (varying denoising steps) and autoregressive models on representative tasks. Measure quality degradation at different step counts to identify optimal efficiency tradeoffs.