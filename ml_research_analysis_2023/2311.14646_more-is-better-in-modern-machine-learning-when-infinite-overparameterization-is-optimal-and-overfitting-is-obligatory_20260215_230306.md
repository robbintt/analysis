---
ver: rpa2
title: 'More is Better in Modern Machine Learning: when Infinite Overparameterization
  is Optimal and Overfitting is Obligatory'
arxiv_id: '2311.14646'
source_url: https://arxiv.org/abs/2311.14646
tags:
- uni00000156
- uni00000157
- uni000001ef
- will
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the benefits of overparameterization and
  overfitting in machine learning by studying random feature regression. The authors
  show that more features and more data improve test error for RF regression under
  optimal ridge regularization.
---

# More is Better in Modern Machine Learning: when Infinite Overparameterization is Optimal and Overfitting is Obligatory

## Quick Facts
- arXiv ID: 2311.14646
- Source URL: https://arxiv.org/abs/2311.14646
- Reference count: 40
- More features and data improve test error for RF regression under optimal ridge regularization; overfitting is obligatory for powerlaw tasks

## Executive Summary
This paper provides a theoretical framework explaining why overparameterization and overfitting benefit modern machine learning. The authors analyze random feature regression and kernel ridge regression for tasks with powerlaw eigenstructure, showing that more features and more data monotonically improve test error at optimal regularization. Crucially, they demonstrate that for a large class of powerlaw tasks, overfitting is obligatory: near-optimal performance requires training error much smaller than test error. Empirical results on image datasets with convolutional kernels support these theoretical predictions.

## Method Summary
The paper combines theoretical analysis of random feature regression with eigenstructure characterization of real datasets. The authors derive optimal ridge regularization for powerlaw tasks, analyze the convergence of random feature kernels to their deterministic limits, and compute fitting ratios on convolutional kernels applied to image datasets. They use both synthetic Gaussian data with controlled eigenstructure and real image datasets (CIFAR-10, MNIST) with convolutional neural tangent kernels to validate their theoretical predictions.

## Key Results
- Random feature regression test error decreases monotonically with both feature count and sample size under optimal ridge regularization
- For powerlaw tasks with exponents α > 1 and β ∈ (1, 2α + 1), optimal regularization forces aggressive fitting to training data
- Convolutional kernels on image datasets exhibit powerlaw eigenstructure with fitting ratios close to zero, confirming obligatory overfitting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimal generalization requires training error much smaller than test error (overfitting is obligatory)
- Mechanism: For tasks with powerlaw eigenstructure, the optimal ridge parameter is small or zero, forcing the model to fit the training data aggressively
- Core assumption: Task eigenvalues and target eigencoefficients follow powerlaw decay with exponents α > 1 and β ∈ (1, 2α + 1)
- Evidence anchors:
  - [abstract]: "for a large class of tasks characterized by powerlaw eigenstructure, training to near-zero training loss is obligatory"
  - [section 5]: "overfitting is obligatory: near-optimal performance can only be achieved when the training error is much smaller than the test error"
- Break condition: If the noise level is too high relative to the uncaptured signal (σ²_rel > (β - α)/(α(α - 1)), then zero ridge is no longer optimal

### Mechanism 2
- Claim: More features and more data monotonically improve test error for RF regression
- Mechanism: Additional features provide a better approximation to the infinite-width limiting process, while more data reduces the implicit regularization constant
- Core assumption: Gaussian universality ansatz holds for the eigenstructure of random features
- Evidence anchors:
  - [abstract]: "we first show that the test risk of RF regression decreases monotonically with both the number of features and the number of samples"
  - [section 4.2]: "Theorem 1 states that, for RF regression, more features (as well as more data) is better"
- Break condition: If the ridge parameter is not optimally tuned, additional features or data may not improve performance

### Mechanism 3
- Claim: Powerlaw eigenstructure in real datasets explains why interpolation is optimal
- Mechanism: Convolutional neural tangent kernels applied to image datasets exhibit powerlaw decay matching the theoretical criteria for obligatory overfitting
- Core assumption: The eigenstructure of real datasets can be characterized by two powerlaw exponents α and β
- Evidence anchors:
  - [section 5.1]: "standard image datasets with convolutional kernels clearly fall into this class" and "R*tr/te ≈ 0"
  - [section B]: "we find that the tasks we study can be characterized by two powerlaw exponents of similar value"
- Break condition: If real datasets do not exhibit powerlaw eigenstructure or have significantly different exponents, the theoretical predictions may fail

## Foundational Learning

- Concept: Kernel ridge regression and its eigenstructure
  - Why needed here: The paper builds on KRR theory and extends it to random features and powerlaw tasks
  - Quick check question: Can you derive the KRR risk estimate in terms of eigenvalues and ridge parameter?

- Concept: Random feature regression and its equivalence to KRR
  - Why needed here: RF regression is the primary model studied, and its connection to KRR enables the theoretical analysis
  - Quick check question: How does the random feature kernel converge to the deterministic kernel as the number of features increases?

- Concept: Powerlaw distributions and their properties
  - Why needed here: The paper characterizes tasks with powerlaw eigenstructure and derives optimal regularization for this class
  - Quick check question: What is the integral of x^(-α) from 1 to infinity, and for what values of α does it converge?

## Architecture Onboarding

- Component map:
  Data generation -> Feature extraction -> Ridge regression -> Evaluation -> Eigenstructure analysis

- Critical path:
  1. Generate or load data
  2. Compute kernel matrix (for NTK) or generate random features
  3. Perform ridge regression with varying ridge parameters
  4. Measure train/test error and compute fitting ratio
  5. Extract eigenstructure (α, β exponents)
  6. Compare experimental results with theoretical predictions

- Design tradeoffs:
  - Random vs. learned features: Random features enable theoretical analysis but may underperform learned features
  - Ridge parameter tuning: Too much regularization hurts performance; too little risks overfitting
  - Dataset size: Larger datasets improve performance but increase computational cost

- Failure signatures:
  - Poor agreement between theory and experiment: May indicate violation of powerlaw assumption or Gaussian universality
  - Optimal fitting ratio far from zero: May indicate high intrinsic noise or unfavorable eigenstructure
  - Double descent peak: Indicates under-regularized regime; should be avoided by tuning ridge parameter

- First 3 experiments:
  1. Synthetic Gaussian data with known powerlaw eigenstructure (α = β = 1.5)
  2. Random ReLU features on CIFAR-10 with varying ridge parameter
  3. Convolutional NTK on MNIST with artificial label noise at different levels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical explanation for why certain deep learning tasks exhibit powerlaw eigenstructure with exponents α, β that lead to obligatory overfitting?
- Basis in paper: [explicit] The paper demonstrates that tasks with powerlaw eigenstructure (Definition 2) exhibit obligatory overfitting, but does not provide a theoretical explanation for why natural data should have this structure.
- Why unresolved: The paper shows that powerlaw structure is observed empirically in image datasets but does not explain the underlying reason for this structure in natural data.
- What evidence would resolve it: A theoretical framework explaining why natural data distributions and target functions should have powerlaw eigenstructure with specific exponents.

### Open Question 2
- Question: How do the findings on overfitting in kernel ridge regression translate to deep neural networks beyond the kernel regime?
- Basis in paper: [explicit] The paper's results are derived for kernel ridge regression and random feature models, which are equivalent to shallow networks with only the last layer trained.
- Why unresolved: The paper does not extend its analysis to deeper neural networks or address how the results might change when feature learning occurs.
- What evidence would resolve it: Empirical studies or theoretical analysis of overfitting behavior in deep neural networks beyond the kernel regime.

### Open Question 3
- Question: What is the precise relationship between the noise scaling σ² = σ²_rel · Ete|σ²=δ=0 and real-world label noise in deep learning tasks?
- Basis in paper: [explicit] The paper introduces this scaling to prevent noise from dominating the signal at large n, but notes it's a choice to treat σ²_rel as an order-unity quantity.
- Why unresolved: The paper does not provide empirical evidence on how well this scaling matches real-world noise levels in deep learning tasks.
- What evidence would resolve it: Measurements of label noise levels in real deep learning datasets and their scaling with dataset size.

### Open Question 4
- Question: How does the "more is better" phenomenon for random features extend to other model architectures beyond RF regression?
- Basis in paper: [explicit] Theorem 1 shows that for RF regression, more features and more data strictly improve test error at optimal regularization.
- Why unresolved: The paper does not explore whether this property holds for other architectures like deep neural networks or other learning algorithms.
- What evidence would resolve it: Analysis of the "more is better" property for other model classes or empirical studies showing monotonic improvement with increased model capacity.

## Limitations
- The theoretical framework assumes powerlaw eigenstructure, which may not hold for all real-world datasets
- The analysis focuses on regression tasks, leaving classification and structured prediction unexplored
- The Gaussian universality ansatz requires empirical validation across diverse data distributions

## Confidence

- **High Confidence:** The monotonic improvement of test error with more features and data for RF regression under optimal ridge regularization
- **Medium Confidence:** The claim that overfitting is obligatory for powerlaw tasks, given empirical validation on image datasets
- **Medium Confidence:** The connection between kernel eigenstructure and optimal regularization, pending broader dataset validation

## Next Checks

1. **Dataset Diversity Test:** Apply the eigenstructure analysis to non-image datasets (text, tabular, molecular) to verify the powerlaw assumption holds broadly
2. **Architecture Scaling Test:** Examine whether the powerlaw eigenstructure persists as neural network architectures scale up, particularly for transformers and graph neural networks
3. **Optimization Algorithm Test:** Investigate whether gradient-based optimization of random features (as opposed to exact ridge regression) preserves the theoretical predictions about overfitting being obligatory