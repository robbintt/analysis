---
ver: rpa2
title: Learning to Optimise Wind Farms with Graph Transformers
arxiv_id: '2311.12750'
source_url: https://arxiv.org/abs/2311.12750
tags:
- wind
- graph
- farm
- turbines
- wake
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a data-driven model for accurate power prediction
  of all wind turbines in wind farms of arbitrary layout, yaw angle configurations
  and wind conditions. The model encodes wind farms into fully-connected graphs and
  processes them through a graph transformer, achieving an average relative accuracy
  of 99.8% on previously unseen layouts.
---

# Learning to Optimise Wind Farms with Graph Transformers

## Quick Facts
- arXiv ID: 2311.12750
- Source URL: https://arxiv.org/abs/2311.12750
- Reference count: 40
- Key outcome: Data-driven model achieves 99.8% accuracy on unseen wind farm layouts while accelerating yaw optimization

## Executive Summary
This paper presents a graph transformer-based approach for predicting wind turbine power output in arbitrary wind farm layouts. The model encodes wind farms as fully-connected graphs and uses self-attention to learn wake interactions without predefined adjacency matrices. The approach achieves high accuracy on unseen layouts and enables significantly faster optimization of yaw angles using genetic algorithms.

## Method Summary
The authors develop a graph transformer model that takes wind farm layouts, yaw angles, and wind conditions as input to predict individual turbine power outputs. The model is trained on PyWake-generated data using a Bastankhah-Gaussian deficit model with Jiménez deflection. Genetic algorithm runs during data generation expose the model to optimized yaw configurations. The transformer serves as a surrogate for PyWake in optimization tasks, enabling batched evaluation of yaw configurations.

## Key Results
- Achieves 99.8% mean relative accuracy on previously unseen wind farm layouts
- Reduces GA optimization time from ~25-44 minutes to ~1 minute per run
- Generalizes well to arbitrary layouts without requiring hand-crafted adjacency matrices

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The graph transformer outperforms GNNs by eliminating hand-crafted adjacency matrices and leveraging self-attention to model arbitrary wake interactions.
- Mechanism: Wind farms are encoded as fully-connected graphs where each turbine is a vertex and edges represent potential wake interactions. The transformer's self-attention layers learn which turbine pairs actually influence each other's power output, effectively discovering wake dependencies directly from data.
- Core assumption: Wake interactions can be fully captured through pairwise attention scores without explicit spatial or directional encoding.
- Evidence anchors:
  - [abstract] "The model encodes a wind farm into a fully-connected graph and processes the graph representation through a graph transformer."
  - [section] "The graph transformer surrogate is shown to generalise well and is able to uncover latent structural patterns within the graph representation of wind farms."
- Break condition: If wake interactions are highly directional and strongly dependent on precise geometric constraints, the fully-connected assumption could dilute meaningful relationships with noise.

### Mechanism 2
- Claim: Using genetic algorithm (GA) runs during data generation improves the surrogate's ability to identify optimal yaw configurations.
- Mechanism: Training data is sampled from GA-optimized yaw angle configurations, exposing the model to diverse and highly optimized scenarios. This allows the surrogate to learn the distinction between favorable and suboptimal yaw setups, even if it never sees the exact optimal configuration during training.
- Core assumption: Optimal yaw configurations exhibit patterns that are generalizable across different wind farm layouts and conditions.
- Evidence anchors:
  - [section] "Training data was randomly sampled from GA runs on wind farms of diverse layouts and wind conditions, enabling the model to discern between favorable and sub-optimal yaw angle configurations."
  - [section] "The trained model is able to accurately identify optimal yaw angle configurations despite not encountering them during training."
- Break condition: If optimal yaw configurations are highly layout-specific and not transferable, the surrogate's predictions could degrade when applied to unseen layouts.

### Mechanism 3
- Claim: The transformer surrogate accelerates GA optimization by processing batched yaw configurations in parallel.
- Mechanism: Unlike iterative evaluations in PyWake, the surrogate can evaluate all individuals in a GA generation as a single batch, drastically reducing computation time while maintaining accuracy.
- Core assumption: The transformer's predictions are sufficiently accurate to guide GA without requiring costly ground-truth evaluations at every step.
- Evidence anchors:
  - [section] "Genetic algorithm runs with the transformer surrogate with batched individuals were completed in approximately one minute for all three cases."
  - [section] "Genetic algorithm runs conducted with PyWake on the three exemplary wind farms... required 25.5, 30.4, and 44.3 minutes, respectively."
- Break condition: If the surrogate's prediction errors accumulate over generations, GA convergence could stall or diverge.

## Foundational Learning

- Concept: Graph neural networks and message passing
  - Why needed here: Understanding how GNNs aggregate information across graph structures helps explain why transformers can replace them for this problem.
  - Quick check question: In a message-passing GNN, what information flows along the edges during a single layer?

- Concept: Attention mechanisms and self-attention
  - Why needed here: The transformer's multi-head self-attention is central to learning wake interactions without predefined edges.
  - Quick check question: How does self-attention allow a model to focus on relevant parts of the input dynamically?

- Concept: Genetic algorithms and optimization
  - Why needed here: GA is used both for data generation and for optimizing yaw angles with the surrogate model.
  - Quick check question: What is the trade-off between exploration and exploitation in GA, and how does the surrogate affect it?

## Architecture Onboarding

- Component map: Input → Vertex encoder → Multi-head attention blocks → Decoder → Output turbine powers
- Critical path: Vertex feature extraction → Attention computation → Aggregation → Power prediction
- Design tradeoffs: Fully-connected graphs simplify implementation but increase computational load; fixed turbine limit (100) trades generality for efficiency
- Failure signatures: If attention scores are uniform, the model fails to distinguish wake effects; if validation loss plateaus early, capacity may be insufficient
- First 3 experiments:
  1. Train on a small dataset with known wake patterns and visualize attention scores to confirm they align with physical wake maps.
  2. Compare prediction accuracy on unseen layouts to baseline GNN models to quantify generalization gains.
  3. Benchmark GA optimization time with PyWake vs. transformer surrogate on a fixed layout to measure speedup.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well would the graph transformer model generalize to wind farm simulations with higher fidelity physics, such as RANS or LES calculations, compared to the current PyWake-based simulations?
- Basis in paper: [explicit] The authors suggest that the model could be improved by training or transfer learning on wind farm simulations featuring more intricate physics, such as that obtained from RANS or LES calculations.
- Why unresolved: The current model is trained on PyWake data, which uses simplified analytical wake models. The performance on more complex simulations remains untested.
- What evidence would resolve it: Comparative studies showing the model's accuracy and computational efficiency when trained on and tested against RANS or LES simulation data versus PyWake data.

### Open Question 2
- Question: How does the attention mechanism in the graph transformer model contribute to the interpretability of wake interactions, and can this interpretability be further enhanced?
- Basis in paper: [explicit] The authors demonstrate that the transformer model can accurately capture wake interactions and provide transparency into the model's inner workings through attention scores.
- Why unresolved: While the model shows promise in identifying wake interactions, the specific contributions of the attention mechanism to interpretability and potential enhancements are not fully explored.
- What evidence would resolve it: Detailed analysis of the attention scores and their correlation with physical wake patterns, along with experiments to test different attention mechanisms for improved interpretability.

### Open Question 3
- Question: What is the impact of using a fully connected graph representation versus predefined graph connectivities on the model's ability to learn and generalize across different wind farm layouts and yaw angle configurations?
- Basis in paper: [explicit] The authors adopt a fully connected graph approach, contrasting it with the use of predefined graph connectivities in other studies, to mitigate issues of over-smoothing and improve generalization.
- Why unresolved: The paper does not provide a direct comparison between fully connected and predefined graph connectivities in terms of model performance and generalization.
- What evidence would resolve it: Experimental results comparing the performance of the model using fully connected graphs versus predefined graph connectivities across various wind farm layouts and yaw angle configurations.

## Limitations

- Limited evaluation on a small set of wind farm layouts and wind conditions, raising questions about generalization to more diverse scenarios
- Performance gains demonstrated only for a specific GA setup and problem size, which may not translate directly to other optimization algorithms or larger wind farms
- Computational efficiency gains not quantified across different hardware configurations or implementations

## Confidence

- High: The transformer surrogate's ability to accurately predict power generation for unseen wind farm layouts, as demonstrated by the 99.8% mean relative accuracy.
- Medium: The claim that the transformer surrogate significantly accelerates GA optimization, as the speedup is demonstrated for a specific GA setup and problem size.
- Medium: The assertion that the graph transformer outperforms GNNs by eliminating the need for hand-crafted adjacency matrices, as the comparison is not explicitly shown in the paper.

## Next Checks

1. Evaluate the transformer surrogate's performance on a more diverse set of wind farm layouts and wind conditions to assess its generalization capabilities.
2. Compare the computational efficiency gains of the transformer surrogate across different optimization algorithms and problem sizes to determine its broader applicability.
3. Conduct an ablation study to quantify the impact of the fully-connected graph representation and self-attention mechanism on the model's performance compared to GNNs with hand-crafted adjacency matrices.