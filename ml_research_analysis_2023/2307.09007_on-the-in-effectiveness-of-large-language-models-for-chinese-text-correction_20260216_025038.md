---
ver: rpa2
title: On the (In)Effectiveness of Large Language Models for Chinese Text Correction
arxiv_id: '2307.09007'
source_url: https://arxiv.org/abs/2307.09007
tags:
- shot
- chinese
- correction
- llms
- gpt-3
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates large language models (LLMs) on Chinese text
  correction tasks, focusing on Chinese grammatical error correction (CGEC) and Chinese
  spelling check (CSC). The authors design task-specific prompts and in-context learning
  strategies to guide LLMs like ChatGPT in these tasks.
---

# On the (In)Effectiveness of Large Language Models for Chinese Text Correction

## Quick Facts
- arXiv ID: 2307.09007
- Source URL: https://arxiv.org/abs/2307.09007
- Authors: 
- Reference count: 18
- Key outcome: LLMs like ChatGPT show promise for Chinese text correction tasks but still lag behind fine-tuned models on automated metrics, though human evaluation reveals better performance

## Executive Summary
This paper evaluates large language models (LLMs) on Chinese text correction tasks, focusing on Chinese grammatical error correction (CGEC) and Chinese spelling check (CSC). The authors design task-specific prompts and in-context learning strategies to guide ChatGPT in these tasks. Despite improvements, LLMs still lag behind fine-tuned small models on automatic evaluation metrics, though human evaluation shows better performance. LLMs demonstrate better domain adaptability and data tolerance, and well-designed prompts improve their correction ability. Fine-grained analyses reveal that LLMs handle morphological errors better than phonological ones in CSC, and struggle more with complex grammatical errors in CGEC.

## Method Summary
The authors evaluate ChatGPT on Chinese text correction using task-specific prompts with explicit constraints (minimize changes, equal length for CSC) and in-context learning strategies. They employ three approaches: random sampling, correct+erroneous examples, and hard erroneous samples selected via BM25 and ROUGE-L ranking. The study tests both text-davinci-03 and gpt-3.5-turbo models across multiple CSC and CGEC datasets without fine-tuning, relying solely on prompt engineering and example selection to guide performance.

## Key Results
- LLMs show improved performance on Chinese text correction with task-specific prompts and in-context learning strategies
- Automated metrics show LLMs lag behind fine-tuned models, but human evaluation reveals more competitive performance
- LLMs demonstrate better domain adaptability and data tolerance compared to traditional fine-tuned models
- Fine-grained analysis shows LLMs handle morphological errors better than phonological ones in CSC

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task-specific prompts with explicit constraints improve LLM performance on Chinese text correction.
- Mechanism: By incorporating task-specific constraints into prompts (e.g., "minimize changes," "equal length for CSC"), LLMs are guided to align their outputs with evaluation metrics and dataset expectations, reducing creative deviations that lead to incorrect edits.
- Core assumption: LLMs can follow structured instructions and constraints when provided in the prompt.
- Evidence anchors:
  - [abstract] "We carefully design task-specific prompts to guide ChatGPT to behave like a CGEC model or a CSC model."
  - [section 2.1] "We ask ChatGPT to minimize the changes to the original input sentence in the prompt."
- Break condition: If LLM's training data does not include sufficient examples of constrained text correction or if the constraints conflict with the model's learned patterns.

### Mechanism 2
- Claim: In-context learning with carefully selected examples improves LLM performance on Chinese text correction.
- Mechanism: Providing relevant examples (correct/erroneous or hard erroneous samples) in the prompt helps LLMs understand the task better and improves their ability to handle complex errors by demonstrating patterns and expected outputs.
- Core assumption: LLMs can effectively learn from in-context examples without explicit fine-tuning.
- Evidence anchors:
  - [section 2.2] "ChatGPT possesses extraordinary in-context learning ability... by giving ChatGPT a small number of task examples to enhance its performance on specific tasks."
  - [section 3.5.2] "The model performance is the best when selecting hard erroneous samples... reflects the effectiveness of our in-context learning strategies designed for the Chinese Text Correction scenario."
- Break condition: If the selected examples are not representative of the task or if the model's in-context learning ability is limited for the specific error types.

### Mechanism 3
- Claim: Human evaluation provides a more realistic assessment of LLM performance than automated metrics in Chinese text correction.
- Mechanism: Human evaluation can account for nuances and context that automated metrics miss, such as semantic consistency and fluency, leading to a more accurate assessment of the model's correction ability.
- Core assumption: Human evaluators can reliably assess the quality of text corrections beyond what automated metrics capture.
- Evidence anchors:
  - [section 3.2] "Human evaluation is necessary because it can eliminate this kind of bias caused by automated metrics."
  - [section 3.4] "The human evaluation results show that the error correction ability of ChatGPT is not so far from that of traditional fine-tuned models."
- Break condition: If human evaluators are inconsistent or biased, or if the evaluation criteria are not well-defined.

## Foundational Learning

- Concept: Task-specific prompt engineering
  - Why needed here: LLMs require clear instructions to perform specialized tasks like Chinese text correction, as they may otherwise deviate from expected outputs.
  - Quick check question: What are the key constraints to include in a prompt for Chinese spelling correction?

- Concept: In-context learning strategies
  - Why needed here: LLMs can benefit from examples provided in the prompt to understand the task better and improve their performance, especially for complex error types.
  - Quick check question: How does selecting hard erroneous samples in in-context learning improve LLM performance?

- Concept: Evaluation metrics and human evaluation
  - Why needed here: Understanding the limitations of automated metrics and the value of human evaluation is crucial for accurately assessing LLM performance in subjective tasks like text correction.
  - Quick check question: Why might human evaluation be more reliable than automated metrics for assessing Chinese text correction?

## Architecture Onboarding

- Component map:
  - Task-specific prompt generator -> In-context learning example selector -> LLM API interface -> Automated metric calculator -> Human evaluation module

- Critical path:
  1. Generate task-specific prompt with constraints.
  2. Select and include in-context learning examples.
  3. Send prompt to LLM API and receive output.
  4. Evaluate output using automated metrics and human evaluation.
  5. Analyze results and iterate on prompt and example selection.

- Design tradeoffs:
  - Prompt complexity vs. LLM understanding: More complex prompts may improve performance but could also confuse the LLM.
  - Example selection vs. diversity: Selecting hard examples may improve performance on complex errors but could reduce overall accuracy.
  - Automated vs. human evaluation: Automated metrics are faster but may miss nuances; human evaluation is more accurate but slower and subjective.

- Failure signatures:
  - Low automated metric scores despite human evaluation showing good performance: Indicates a mismatch between automated metrics and the task requirements.
  - Inconsistent human evaluation results: Suggests a need for clearer evaluation criteria or more consistent evaluators.
  - LLM outputs that violate prompt constraints: Indicates the LLM is not following instructions as expected.

- First 3 experiments:
  1. Compare LLM performance with and without task-specific prompts to measure the impact of prompt constraints.
  2. Evaluate the effect of different in-context learning strategies (random, correct/erroneous, hard erroneous samples) on LLM performance.
  3. Analyze the correlation between automated metric scores and human evaluation results to identify potential biases in automated metrics.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLMs be effectively adapted to handle multiple errors within a single sentence in Chinese text correction tasks?
- Basis in paper: [explicit] The paper mentions that LLMs struggle with handling multiple errors in a sentence and require strong reasoning ability to address this issue.
- Why unresolved: The paper does not provide specific strategies or techniques for improving LLMs' ability to handle multiple errors within a sentence.
- What evidence would resolve it: Experiments demonstrating improved performance of LLMs on sentences with multiple errors after implementing specific techniques or training methods.

### Open Question 2
- Question: What are the key linguistic features or knowledge that LLMs lack, which hinders their performance in Chinese text correction tasks?
- Basis in paper: [explicit] The paper highlights that LLMs lack fine-grained knowledge such as specific location names and struggle with understanding morphological and phonological information about Chinese characters.
- Why unresolved: The paper does not provide a comprehensive analysis of all the linguistic features or knowledge that LLMs lack, nor does it suggest ways to incorporate this knowledge into LLMs.
- What evidence would resolve it: A detailed analysis of the linguistic features or knowledge that LLMs lack, along with experiments demonstrating improved performance after incorporating this knowledge into the models.

### Open Question 3
- Question: How can LLMs be made more controllable in their content generation to avoid changing the meaning of the input sentence in Chinese text correction tasks?
- Basis in paper: [explicit] The paper mentions that LLMs tend to play freely when performing text error correction, often altering the original meaning of the input sentence.
- Why unresolved: The paper does not provide specific methods or techniques for making LLMs more controllable in their content generation during text correction tasks.
- What evidence would resolve it: Experiments demonstrating improved performance of LLMs in maintaining the original meaning of the input sentence after implementing specific methods or techniques for controlling content generation.

## Limitations
- The evaluation relies heavily on automated metrics that may not capture semantic nuances critical for Chinese text correction
- The human evaluation sample size is limited (50 examples per task with 6 annotators), potentially affecting result reliability
- The prompt engineering approach is complex and may not generalize well to other languages or correction tasks

## Confidence
- High Confidence: LLMs demonstrate better domain adaptability and data tolerance compared to fine-tuned small models
- Medium Confidence: Task-specific prompts significantly improve LLM performance, though optimal structure may vary by model
- Low Confidence: Human evaluation shows better performance for LLMs than automated metrics suggests, but limited sample size may influence results

## Next Checks
1. Conduct a larger-scale human evaluation study with more annotators and examples to validate the reliability of human evaluation results and establish more robust correlations with automated metrics.

2. Test the prompt engineering approach across multiple LLM architectures (GPT-4, Claude, etc.) to determine if the effectiveness is model-dependent or generalizable.

3. Perform error analysis on a wider range of Chinese dialects and domain-specific texts to assess the robustness of LLMs in handling diverse linguistic variations and specialized terminology.