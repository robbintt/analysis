---
ver: rpa2
title: Improving Transformer Performance for French Clinical Notes Classification
  Using Mixture of Experts on a Limited Dataset
arxiv_id: '2303.12892'
source_url: https://arxiv.org/abs/2303.12892
tags:
- transformer
- clinical
- classi
- cation
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the effectiveness of Mixture of Experts (MoE)
  Switch Transformers for classifying small-scale French clinical narratives, addressing
  the challenge of limited data and computational resources in clinical NLP. By employing
  the MoE mechanism from the Switch Transformer, the model achieves high performance
  with an accuracy of 87%, precision of 87%, recall of 85%, and F1-score of 86%, outperforming
  pre-trained BERT-based models like DistillBERT, CamemBERT, FlauBERT, and FrALBERT.
---

# Improving Transformer Performance for French Clinical Notes Classification Using Mixture of Experts on a Limited Dataset

## Quick Facts
- arXiv ID: 2303.12892
- Source URL: https://arxiv.org/abs/2303.12892
- Reference count: 40
- Primary result: MoE Switch Transformer achieves 87% accuracy on small-scale French clinical classification, outperforming DistillBERT, CamemBERT, FlauBERT, and FrALBERT

## Executive Summary
This study evaluates Mixture of Experts (MoE) Switch Transformers for classifying small-scale French clinical narratives, addressing the challenge of limited data and computational resources in clinical NLP. The proposed approach achieves high performance with an accuracy of 87%, precision of 87%, recall of 85%, and F1-score of 86%, outperforming pre-trained BERT-based models. The MoE mechanism enables training at least 190 times faster than biomedical pre-trained BERT models, making it particularly valuable for hospital-based settings with limited computational resources. However, the study also highlights limitations including generalization gaps and sharp minima when training on small datasets, and notes that simpler frameworks combining statistical representation learning with multilayer perceptron networks can outperform the Switch Transformer in some cases.

## Method Summary
The study implements a Switch Transformer with Mixture of Experts mechanism for binary classification of French clinical narratives (cardiac failure detection) at CHU Sainte-Justine hospital. The dataset contains 5,444 short clinical narratives (1,941 positive, 3,503 negative) preprocessed with stopwords removed and negation handled. The model uses 4 hidden layers with 4 experts per layer, trained from scratch using AdamW optimizer with cosine annealed learning rate. Training runs for 70 epochs with batch size of 16, and early stopping is applied to prevent overfitting. The MoE routing mechanism dynamically selects one expert per token, enabling the model to capture diverse clinical patterns while maintaining computational efficiency.

## Key Results
- MoE Switch Transformer achieves 87% accuracy, 87% precision, 85% recall, and 86% F1-score on small-scale French clinical classification
- Model outperforms pre-trained BERT-based models (DistillBERT, CamemBERT, FlauBERT, FrALBERT) while requiring 190x less training time
- Early stopping effectively mitigates overfitting on small dataset, though generalization gaps and sharp minima remain challenges
- Integrated Gradients analysis reveals limitations in contextual understanding compared to simpler TF-IDF + MLP frameworks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MoE routing reduces effective parameters per token, enabling training with limited computational resources.
- Mechanism: The Switch Transformer dynamically routes each token to one of several expert sub-networks, keeping the activated parameter set small while scaling up total parameters.
- Core assumption: Routing function generalizes across tokens and doesn't introduce catastrophic forgetting.
- Evidence anchors:
  - [abstract] "By employing the MoE mechanism from the Switch Transformer, the model achieves high performance with an accuracy of 87%..."
  - [section] "The MoE mechanisms allow the model to divide the sequence into smaller, more manageable segments and apply different experts to each segment."
  - [corpus] Weak - corpus neighbors focus on MoE scaling, not resource constraints for small clinical data.
- Break condition: Routing collapse where a single expert dominates or routing becomes non-discriminative.

### Mechanism 2
- Claim: Multiple experts capture domain-specific linguistic patterns better than a single monolithic transformer.
- Mechanism: Each expert specializes in a subset of clinical narrative patterns (e.g., cardiac failure indicators, respiratory symptoms), and the gating function selects the relevant expert per token.
- Core assumption: The training data contains separable signal patterns that can be learned by distinct experts.
- Evidence anchors:
  - [abstract] "using a mixture of expert mechanisms from the Switch Transformer helps capture diverse patterns; hence, the proposed approach achieves better results than a conventional Transformer"
  - [section] "the MoE mechanism in the Switch Transformer can be represented by the following equation: zt = ∑ j gj(xt)*ej(xt)"
  - [corpus] Weak - corpus does not provide direct evidence of pattern separation in clinical NLP MoE models.
- Break condition: Insufficient diversity in the small dataset prevents experts from specializing.

### Mechanism 3
- Claim: Early stopping mitigates generalization gap and sharp minima on small datasets.
- Mechanism: Training is halted when validation loss begins to increase, preventing over-fitting to noise in the small dataset.
- Core assumption: The validation set is representative of unseen data distribution.
- Evidence anchors:
  - [abstract] "Although the MoE-Transformer addresses challenges of generalization gaps and sharp minima..."
  - [section] "Applying the early stopping at this point helped prevent the model's overfitting."
  - [corpus] Missing - corpus neighbors do not discuss early stopping strategies for small clinical datasets.
- Break condition: Validation set becomes stale or unrepresentative due to dataset drift.

## Foundational Learning

- Concept: Mixture of Experts (MoE) routing mechanism
  - Why needed here: Enables scaling transformer capacity without proportional computational cost, critical for hospital resource constraints.
  - Quick check question: If you have 4 experts and route 25% of tokens to each, what fraction of total parameters are active per token?

- Concept: Attention mechanism and multi-head attention
  - Why needed here: Understanding self-attention is prerequisite to grasping how MoE layers integrate with standard transformer blocks.
  - Quick check question: In scaled dot-product attention, why do we divide by sqrt(d_k)?

- Concept: Imbalanced classification and evaluation metrics
  - Why needed here: Clinical datasets often have skewed class distributions; proper metric interpretation is essential.
  - Quick check question: If you have 90% negative cases and 10% positive cases, which metric (accuracy, precision, recall, F1) best captures classifier performance?

## Architecture Onboarding

- Component map:
  Input embedding layer → MoE layer (gating + experts) → standard transformer encoder layers → classification head
- Critical path:
  Token embedding → MoE routing decision → Expert processing → Attention + FFN → Layer norm → Output
- Design tradeoffs:
  - Expert capacity vs. routing overhead: more experts increase capacity but require more routing computation
  - Sparsity level: top-1 routing maximizes sparsity but may underutilize expert diversity
  - Model size vs. generalization: larger total parameter count helps on small datasets but risks overfitting
- Failure signatures:
  - Single expert dominates routing (check routing distribution)
  - High variance in expert utilization across batches
  - Validation loss increases while training loss continues decreasing
  - Sharp accuracy drop on held-out data after certain epoch
- First 3 experiments:
  1. Compare routing distribution across tokens to ensure load balancing
  2. Test different expert counts (2, 4, 8) on validation set to find optimal sparsity
  3. Implement and compare early stopping vs. fixed epoch training on generalization gap

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do Switch Transformer models perform on larger clinical datasets compared to their performance on the small dataset used in this study?
- Basis in paper: [inferred] The paper notes that the Switch Transformer model outperformed other models on a small dataset but underperformed compared to simpler frameworks on the same dataset. It also discusses the generalization gap and sharp minima issues when training on small datasets.
- Why unresolved: The study only used a small dataset of French clinical notes, so it's unclear how the model would scale to larger datasets commonly found in clinical settings.
- What evidence would resolve it: Conducting experiments with larger clinical datasets from various languages and domains to compare the performance of Switch Transformer models against other approaches.

### Open Question 2
- Question: What specific modifications to the training methodology could mitigate the generalization gap and sharp minima issues observed in the Switch Transformer model?
- Basis in paper: [explicit] The paper discusses the generalization gap and sharp minima as limitations of the Switch Transformer model, particularly when trained on small datasets without early stopping.
- Why unresolved: While the paper identifies these issues, it doesn't provide specific solutions or modifications to the training process that could address them.
- What evidence would resolve it: Implementing and testing various training strategies (e.g., different learning rate schedules, regularization techniques, data augmentation) to determine their impact on reducing the generalization gap and sharp minima effects.

### Open Question 3
- Question: How does the interpretability of the Switch Transformer model compare to simpler frameworks like TF-IDF + MLP-NN in terms of providing clinically actionable insights?
- Basis in paper: [explicit] The paper uses Integrated Gradients to analyze misclassifications but notes that the model's contextual understanding of clinical data is limited compared to simpler frameworks.
- Why unresolved: The study provides some interpretability analysis but doesn't directly compare the clinical utility of insights gained from the Switch Transformer model versus simpler approaches.
- What evidence would resolve it: Conducting a comparative study where clinicians evaluate the interpretability and clinical actionability of explanations provided by both the Switch Transformer model and simpler frameworks on the same dataset.

## Limitations

- Small dataset size (5,444 samples) limits generalizability to larger clinical datasets and raises concerns about model performance at scale
- Generalization gaps and sharp minima issues persist even with early stopping, affecting model reliability on unseen data
- Underperformance compared to simpler TF-IDF + MLP frameworks suggests contextual understanding limitations in the Switch Transformer approach

## Confidence

- Method validity: High - clear implementation details and appropriate evaluation metrics
- Result reliability: Medium - strong performance on small dataset but limited external validation
- Generalization claims: Low - findings based on single dataset without cross-domain validation
- Computational efficiency claims: High - clear comparison of training times with pre-trained models

## Next Checks

1. Replicate the MoE Switch Transformer implementation on a larger French clinical dataset to verify scaling behavior and generalization performance
2. Conduct ablation studies varying expert counts and routing strategies to optimize the MoE configuration for clinical text
3. Compare interpretability methods (Integrated Gradients vs. attention visualization) to evaluate which provides more clinically actionable insights for model predictions