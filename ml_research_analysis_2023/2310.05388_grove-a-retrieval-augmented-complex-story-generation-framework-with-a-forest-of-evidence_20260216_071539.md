---
ver: rpa2
title: 'GROVE: A Retrieval-augmented Complex Story Generation Framework with A Forest
  of Evidence'
arxiv_id: '2310.05388'
source_url: https://arxiv.org/abs/2310.05388
tags:
- story
- stories
- evidence
- cats
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GROVE, a retrieval-augmented framework that
  leverages exemplary human-written stories and evidence to generate complex and credible
  plots. GROVE employs a retrieval repository of stories and an iterative "asking-why"
  prompting scheme to construct a forest of evidence, addressing ambiguities in the
  generated story.
---

# GROVE: A Retrieval-augmented Complex Story Generation Framework with A Forest of Evidence

## Quick Facts
- arXiv ID: 2310.05388
- Source URL: https://arxiv.org/abs/2310.05388
- Reference count: 16
- Key outcome: GROVE outperforms strong baselines in ensuring story complexity and creativity, with the highest average number of plots per story.

## Executive Summary
This paper introduces GROVE, a retrieval-augmented framework designed to generate complex and credible stories by leveraging exemplary human-written stories and evidence. GROVE employs a retrieval repository of stories and an iterative "asking-why" prompting scheme to construct a forest of evidence, addressing ambiguities in the generated story. The optimal evidence chains are then integrated into the story to enhance its complexity and credibility. Experimental results show that GROVE outperforms strong baselines in ensuring story complexity and creativity, with the highest average number of plots per story.

## Method Summary
GROVE is a retrieval-augmented story generation framework that uses a retrieval repository of human-written stories and an iterative "asking-why" prompting scheme to construct a forest of evidence. The framework first generates an initial story using few-shot examples from the retrieval repository and target conditions. It then builds an evidence forest by recursively asking the LLM to provide evidence for ambiguities in the initial story. Finally, the most relevant evidence chains are selected and integrated into the story to enhance its complexity and credibility.

## Key Results
- GROVE outperforms strong baselines in ensuring story complexity and creativity.
- The framework achieves the highest average number of plots per story.
- Experimental results demonstrate the effectiveness of the retrieval-augmented approach and the iterative "asking-why" prompting scheme.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval of relevant stories enables the LLM to learn diverse narrative patterns and plot structures.
- Mechanism: By building a retrieval repository with human-written stories and their associated target conditions, the LLM can draw inspiration and incorporate complex plots from existing examples.
- Core assumption: The retrieved stories contain useful narrative patterns and plot structures that can be learned and applied by the LLM.
- Evidence anchors:
  - [abstract] "We construct a retrieval repository that enables the LLM to learn diverse plots and common patterns from human-written stories."
  - [section 3.2] "The repository acts as a source of inspiration and provides the LLM with a rich set of story examples to draw from."
  - [corpus] Weak evidence; no direct citation found.

### Mechanism 2
- Claim: The iterative "asking-why" prompting scheme uncovers underlying story backgrounds and addresses ambiguities.
- Mechanism: By recursively asking the LLM to provide evidence for ambiguities in the generated story, the framework builds a forest of evidence that supplements and clarifies the narrative.
- Core assumption: The LLM can identify ambiguities in the story and provide relevant evidence to address them.
- Evidence anchors:
  - [abstract] "Additionally, we design an 'asking-why' prompting scheme that extracts a forest of evidence, providing compensation for the ambiguities that may occur in the generated story."
  - [section 3.3] "By asking why, we instruct the LLM to provide b pieces of evidence that compensate for the initial story."
  - [corpus] Weak evidence; no direct citation found.

### Mechanism 3
- Claim: Selecting the optimal evidence chains and integrating them into the story enhances its complexity and credibility.
- Mechanism: By choosing the most relevant evidence chains from the evidence forest and incorporating them into the generated story, the framework adds depth and detail to the narrative.
- Core assumption: The selected evidence chains are relevant to the target conditions and can be seamlessly integrated into the story.
- Evidence anchors:
  - [abstract] "Finally, we select the most fitting chains of evidence from the evidence forest and integrate them into the generated story, thereby enhancing the narrative's complexity and credibility."
  - [section 3.4] "We instruct the LLM to incorporate the information from { ¯E, A}N i=1 into the initial story to rewrite the final version of the story."
  - [corpus] Weak evidence; no direct citation found.

## Foundational Learning

- Concept: Retrieval-augmented generation
  - Why needed here: To provide the LLM with diverse narrative examples and inspire the generation of complex plots.
  - Quick check question: How does the retrieval repository help the LLM learn from existing stories?

- Concept: Iterative prompting and evidence generation
  - Why needed here: To uncover underlying story backgrounds, address ambiguities, and enrich the narrative with relevant details.
  - Quick check question: What is the purpose of the "asking-why" prompting scheme in the evidence forest construction?

- Concept: Story rewriting with evidence integration
  - Why needed here: To incorporate the most relevant evidence chains into the generated story, enhancing its complexity and credibility.
  - Quick check question: How does the selection and integration of evidence chains improve the final story?

## Architecture Onboarding

- Component map: Retrieval Repository -> Initial Story Generation -> Evidence Forest Construction -> Evidence Chains Selection -> Story Rewriting
- Critical path: Retrieval Repository → Initial Story Generation → Evidence Forest Construction → Evidence Chains Selection → Story Rewriting
- Design tradeoffs: Balancing the complexity of evidence chains with the coherence of the final story; selecting the most relevant evidence without introducing contradictions.
- Failure signatures: Irrelevant or low-quality retrieved stories, failure to identify or address ambiguities, poorly integrated evidence chains leading to inconsistencies.
- First 3 experiments:
  1. Test the retrieval repository with a small set of stories and target conditions to ensure relevant examples are retrieved.
  2. Verify the "asking-why" prompting scheme by generating an initial story and checking if the LLM can identify and address ambiguities.
  3. Evaluate the evidence chain selection and integration process by incorporating the chains into the initial story and assessing the coherence and complexity of the final output.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does GROVE's evidence forest approach compare to more established knowledge graphs in terms of maintaining logical consistency while enriching story complexity?
- Basis in paper: [inferred] The paper mentions GROVE's evidence forest addresses ambiguities and enhances story complexity, but does not directly compare its performance to knowledge graphs.
- Why unresolved: The paper does not provide empirical comparisons between GROVE's evidence forest and knowledge graphs for story generation.
- What evidence would resolve it: A controlled experiment comparing story quality, complexity, and logical consistency when using GROVE's evidence forest versus a knowledge graph for story generation.

### Open Question 2
- Question: What is the optimal number of evidence trees and evidence chains to include in the story rewriting process to maximize story quality without introducing contradictions or irrelevant information?
- Basis in paper: [explicit] The paper mentions GROVE selects optimal evidence chains, but does not explore the impact of varying the number of evidence trees or chains on story quality.
- Why unresolved: The paper does not systematically investigate the relationship between the number of evidence trees/chains and the resulting story quality.
- What evidence would resolve it: An ablation study that varies the number of evidence trees and chains included in the story rewriting process and measures the impact on story quality metrics like coherence, relevance, and complexity.

### Open Question 3
- Question: How well does GROVE generalize to other domains beyond the science fiction, literary fiction, and historical fiction genres explored in the paper?
- Basis in paper: [explicit] The paper mentions GROVE's retrieval repository and "asking-why" prompting scheme are generalizable, but does not provide evidence of GROVE's performance in other domains.
- Why unresolved: The paper only evaluates GROVE on a limited set of genres and does not demonstrate its effectiveness in other domains like mystery, romance, or fantasy.
- What evidence would resolve it: Applying GROVE to generate stories in a diverse set of genres and evaluating its performance using both human and automatic metrics to assess its generalizability.

## Limitations
- The framework's effectiveness is heavily dependent on the quality and diversity of the retrieval repository.
- The iterative "asking-why" prompting scheme relies on the LLM's ability to identify ambiguities and provide relevant evidence, which may not always be reliable.
- The selection and integration of evidence chains introduce potential risks of introducing inconsistencies or reducing the story's coherence if not done carefully.

## Confidence

- **High Confidence**: The overall retrieval-augmented approach and the iterative prompting scheme are well-established techniques in the literature.
- **Medium Confidence**: The specific implementation details, such as the prompt templates for extracting target conditions and the criteria for selecting optimal evidence chains, are not fully specified in the paper.
- **Low Confidence**: The claim that GROVE outperforms strong baselines in ensuring story complexity and creativity is based on experimental results, but the specific evaluation metrics and comparison methods are not clearly described.

## Next Checks

1. **Retrieval Repository Quality Assessment**: Evaluate the diversity and quality of the stories in the retrieval repository to ensure that it contains a wide range of narrative patterns and plot structures that can be learned by the LLM.

2. **Evidence Generation Reliability**: Test the LLM's ability to identify ambiguities in generated stories and provide relevant evidence through the "asking-why" prompting scheme. This can be done by manually inspecting the generated evidence and assessing its relevance and coherence with the initial story.

3. **Evidence Integration Coherence**: Assess the coherence and complexity of the final stories after integrating the selected evidence chains. This can be done through human evaluation or automatic evaluation using metrics such as BLEU, ROUGE, or perplexity to measure the story's quality and consistency.