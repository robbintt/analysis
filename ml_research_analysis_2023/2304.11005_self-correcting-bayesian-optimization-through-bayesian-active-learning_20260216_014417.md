---
ver: rpa2
title: Self-Correcting Bayesian Optimization through Bayesian Active Learning
arxiv_id: '2304.11005'
source_url: https://arxiv.org/abs/2304.11005
tags:
- learning
- scorebo
- bayesian
- optimization
- active
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces two methods for Bayesian active learning
  and optimization that explicitly learn GP hyperparameters by considering posterior
  disagreement under hyperparameter uncertainty. Statistical distance-based Active
  Learning (SAL) queries points where the average disagreement between posterior and
  conditional GPs is highest, using Wasserstein distance for exploration.
---

# Self-Correcting Bayesian Optimization through Bayesian Active Learning

## Quick Facts
- arXiv ID: 2304.11005
- Source URL: https://arxiv.org/abs/2304.11005
- Reference count: 40
- Primary result: SAL and SCoreBO methods learn GP hyperparameters faster and improve Bayesian optimization performance on synthetic benchmarks

## Executive Summary
This paper introduces two methods for Bayesian active learning and optimization that explicitly learn GP hyperparameters by considering posterior disagreement under hyperparameter uncertainty. SAL queries points where the average disagreement between posterior and conditional GPs is highest, using Wasserstein distance for exploration. SCoreBO extends SAL by conditioning on sampled optima to simultaneously optimize and learn hyperparameters, using Hellinger distance. Both methods outperform existing approaches on active learning tasks (Gramacy, Branin, Hartmann-6, Ishigami) and Bayesian optimization benchmarks (Rosenbrock, Hartmann). SCoreBO learns hyperparameters faster and better handles exotic setups like high-dimensional search spaces and additive decompositions.

## Method Summary
The paper proposes two methods for Bayesian active learning and optimization: SAL (Statistical distance-based Active Learning) and SCoreBO (Self-Correcting Bayesian Optimization). Both methods compute acquisition functions based on the statistical distance between conditional posteriors (given sampled hyperparameters) and the marginal posterior. SAL uses Wasserstein distance and focuses on hyperparameter learning, while SCoreBO uses Hellinger distance and conditions on sampled optima to balance optimization and hyperparameter learning. The methods employ moment matching to approximate the Gaussian mixture posterior, reducing computational complexity. Implementation uses MCMC sampling (NUTS in Pyro) for hyperparameter posteriors, with 256 warmup iterations, 16 thinning, and varying numbers of hyperparameter sets for different experiments.

## Key Results
- SAL-WS and SCoreBO outperform existing AL methods (BALM, BQBC, QBMGP) on 6 benchmark problems in terms of RMSE and MLL
- SCoreBO learns hyperparameters faster than SAL and improves Bayesian optimization performance on synthetic benchmarks (Rosenbrock, Hartmann)
- SAL-HR and SCoreBO achieve better performance on exploration-heavy tasks like Ishigami and Rosenbrock
- Moment matching approximation performs comparably to Monte Carlo sampling while reducing computational cost from O(N²M²) to O(NM)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Statistical distance-based acquisition functions accelerate hyperparameter learning by querying points where posterior disagreement is highest.
- Mechanism: SAL computes the expected statistical distance between conditional posteriors (given sampled hyperparameters) and the marginal posterior. High disagreement indicates regions where hyperparameter uncertainty causes model disagreement, so querying these points quickly reduces hyperparameter uncertainty.
- Core assumption: High posterior disagreement correlates with regions that will most efficiently reduce hyperparameter uncertainty when observed.
- Evidence anchors:
  - [abstract]: "SAL considers the average disagreement between samples from the posterior, as measured by a statistical distance."
  - [section 3.1]: "locations where the posterior distribution changes significantly as a result of model uncertainty will qualify as good points to sample to quickly learn the model hyperparameters."
- Break condition: If posterior disagreement does not correlate with hyperparameter uncertainty reduction, or if the statistical distance metric poorly captures meaningful disagreement.

### Mechanism 2
- Claim: Self-correction through conditioning on sampled optima creates additional posterior disagreement that favors promising regions.
- Mechanism: SCoreBO conditions on samples of the optimizer (px*, f*), creating truncated posteriors that are skewed upward in promising regions. This introduces additional disagreement between conditional and marginal posteriors, directing exploration toward likely optima while still learning hyperparameters.
- Core assumption: Conditioning on optimizer samples creates meaningful additional disagreement that balances exploration and exploitation.
- Evidence anchors:
  - [abstract]: "SCoreBO extends SAL by conditioning on sampled optima to simultaneously optimize and learn hyperparameters."
  - [section 3.2]: "sampling and conditioning on (x*, f*) introduces an additional source of disagreement between the marginal posterior and the conditionals."
- Break condition: If conditioning on optimizer samples creates too much or too little disagreement, or if the sampling of optima is unreliable.

### Mechanism 3
- Claim: Moment matching approximation of Gaussian mixture posteriors enables efficient computation of statistical distances.
- Mechanism: Instead of exact computation of distances between normal conditionals and Gaussian mixture marginals, moment matching approximates the mixture with a single Gaussian, reducing computational complexity from O(N²M²) to O(NM).
- Core assumption: The Gaussian approximation preserves enough information about posterior disagreement to maintain acquisition function quality.
- Evidence anchors:
  - [section 3.3]: "we propose to fully utilize the closed-form expressions of the involved distances for Gaussians, and approximate the full posterior mixture with a Gaussian distribution using moment matching."
- Break condition: If moment matching poorly approximates the true posterior disagreement, leading to suboptimal acquisition function performance.

## Foundational Learning

- Gaussian Processes and Hyperparameter Learning
  - Why needed here: Understanding how GP hyperparameters affect model performance and why learning them jointly with optimization is challenging.
  - Quick check question: What are the three main GP hyperparameters mentioned, and how does each affect the model?

- Bayesian Active Learning Acquisition Functions
  - Why needed here: Comparing SAL and SCoreBO to existing AL methods requires understanding the landscape of acquisition functions.
  - Quick check question: How does BALD differ from BQBC in terms of what uncertainty they target?

- Statistical Distances for Probability Distributions
  - Why needed here: The choice of Wasserstein vs Hellinger distance affects both the acquisition function behavior and computational efficiency.
  - Quick check question: When would you prefer Wasserstein distance over Hellinger distance for measuring posterior disagreement?

## Architecture Onboarding

- Component map:
  MCMC sampler (NUTS) -> Optimizer for candidate optima -> Statistical distance computation -> Moment matching module -> Acquisition function wrapper

- Critical path:
  1. Sample hyperparameters from posterior
  2. Sample optima from conditional posteriors
  3. Compute conditional posteriors given (θ, x*, f*)
  4. Approximate marginal posterior (moment matching)
  5. Compute statistical distances between conditionals and marginal
  6. Return acquisition value

- Design tradeoffs:
  - MC vs MM approximation: MC is asymptotically unbiased but O(N²M²), MM is biased but O(NM)
  - Wasserstein vs Hellinger: Wasserstein more exploratory, Hellinger more conservative
  - Number of optimizer samples per hyperparameter: More samples better exploration but higher cost

- Failure signatures:
  - Poor hyperparameter learning: Check if MCMC is mixing well and if acquisition function is querying diverse regions
  - Computational bottlenecks: Monitor moment matching accuracy vs MC computation time
  - Unstable optimization: Verify optimizer is finding valid optima and that conditioning is working correctly

- First 3 experiments:
  1. Run SAL-WS on Gramacy 1D with default priors, compare RMSE to BQBC
  2. Run SCoreBO-HR on Branin with broad lognormal prior, plot hyperparameter convergence vs EI
  3. Compare SAL-HR vs SAL-WS on Ishigami, focusing on MLL stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of statistical distance metric (Wasserstein vs Hellinger) affect the efficiency of hyperparameter learning in Bayesian optimization?
- Basis in paper: The paper mentions that SAL uses Wasserstein distance for active learning and SCoreBO uses Hellinger distance for Bayesian optimization, but notes that these distances have different exploratory properties. It states "SAL-WS is the overall more consistent approach" while also noting that "SCoreBO-HR is the overall more consistent approach."
- Why unresolved: While the paper demonstrates that different distances have different properties and that SAL-HR achieves better hyperparameter learning on some tasks, it doesn't systematically explore how different distance metrics affect the efficiency of hyperparameter learning across various problem types and dimensions.
- What evidence would resolve it: Systematic experiments comparing different statistical distance metrics (Wasserstein, Hellinger, KL divergence, etc.) across a range of problem types (high-dimensional, noisy, additive decomposition) would show which metrics are most effective for different scenarios and why.

### Open Question 2
- Question: What is the impact of the approximation strategy (Monte Carlo vs Moment Matching) on the performance of SAL and SCoreBO, particularly in high-dimensional problems?
- Basis in paper: The paper introduces both Monte Carlo and Moment Matching approximation strategies for computing statistical distances, noting that "performances are comparable" but showing that MC "notably outperforms slightly on the difficult Ishigami test function." It also mentions that MC "circumvents a quadratic cost OpN^2M^2q in the number of samples."
- Why unresolved: While the paper provides some comparison between MC and MM approximations, it doesn't explore how these approximations scale with problem dimensionality, how they affect computational efficiency, or whether there are regimes where one clearly outperforms the other.
- What evidence would resolve it: Comprehensive benchmarking of both approximation strategies across problems with varying dimensionality and complexity, including analysis of computational costs and accuracy trade-offs, would clarify when each approximation is preferable.

### Open Question 3
- Question: How does SCoreBO perform when conditioning on different quantities (x*, f*, or (x*, f*)) and how does this choice affect the balance between exploration and exploitation?
- Basis in paper: The paper states "SCoreBO is not restricted to employing that quantity alone" and mentions that choosing to condition on either x* or f* alone "introduces a smaller disagreement in the posterior" and "decreases the emphasis that SCoreBO puts on optimization, relative to hyperparameter learning."
- Why unresolved: The paper primarily uses the joint conditioning (JES-like) variant but doesn't systematically compare how different conditioning choices affect optimization performance, particularly on problems where the optimum is difficult to identify or where exploration is crucial.
- What evidence would resolve it: Comparative experiments using different conditioning choices (x*, f*, (x*, f*)) on a diverse set of benchmark problems, particularly those with challenging optimization landscapes, would reveal which conditioning strategy works best in different scenarios and why.

## Limitations

- The exact form of conditional GP posterior after optimizer conditioning is not fully specified
- The quasi-Monte Carlo sampling parameters for Wasserstein distance are underspecified
- The breakdown of SCoreBO performance between hyperparameter learning vs exploration remains unclear

## Confidence

- Core claims about SAL and SCoreBO accelerating hyperparameter learning through posterior disagreement: High confidence
- Claims about moment matching approximation preserving sufficient information: Medium confidence
- Claims about architectural choices (SAL-WS vs SAL-HR) affecting exploration: Medium confidence

## Next Checks

1. Implement and compare both MC and MM versions of SAL-WS on Gramacy1D to quantify approximation error
2. Run SCoreBO-HR on Branin with three different optimizer sample counts to assess trade-off between exploration and computational cost
3. Conduct ablation study isolating hyperparameter learning contribution by comparing SCoreBO to a version that only optimizes without learning