---
ver: rpa2
title: 'Navigating Healthcare Insights: A Birds Eye View of Explainability with Knowledge
  Graphs'
arxiv_id: '2309.16593'
source_url: https://arxiv.org/abs/2309.16593
tags:
- knowledge
- graph
- drug
- healthcare
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive overview of how knowledge graphs
  (KGs) enhance explainable AI (XAI) in healthcare. It presents a detailed workflow
  covering KG construction, feature extraction, and reasoning techniques.
---

# Navigating Healthcare Insights: A Birds Eye View of Explainability with Knowledge Graphs

## Quick Facts
- **arXiv ID**: 2309.16593
- **Source URL**: https://arxiv.org/abs/2309.16593
- **Reference count**: 40
- **Primary result**: Knowledge graphs offer a promising solution for interpretable and explainable AI in healthcare, but further research is needed to address existing challenges and improve their effectiveness in real-world applications.

## Executive Summary
This paper provides a comprehensive overview of how knowledge graphs (KGs) enhance explainable AI (XAI) in healthcare applications. The authors present a detailed workflow covering KG construction, feature extraction, and reasoning techniques, highlighting applications in drug-drug interactions, drug-target interactions, adverse drug reactions, and drug development. They emphasize knowledge-infused learning (K-iL) as a key approach for improving both predictive performance and explainability. The paper identifies several research challenges including open-world assumptions, knowledge integration, model complexity, benchmarking, user-friendly explanations, and privacy concerns. The key outcome is that while KGs offer significant promise for interpretable AI in healthcare, substantial work remains to address these challenges and demonstrate real-world effectiveness.

## Method Summary
The paper outlines a comprehensive approach to building and utilizing knowledge graphs for explainable AI in healthcare. The method involves constructing KGs from diverse biomedical data sources, extracting features using knowledge graph embeddings (KGEs), applying reasoning techniques for inference, and integrating this knowledge into deep learning models through knowledge-infused learning. The workflow includes defining objectives, gathering and preprocessing data, performing entity mapping and relation extraction, inferring missing links, and curating the final KG. Feature extraction employs techniques like node2vec and TransE to learn entity and relationship embeddings, which are then integrated into deep neural networks. Reasoning is performed using symbolic logic models and graph neural networks to discover new associations and validate predictions.

## Key Results
- Knowledge graphs provide structured representation that links biomedical entities via labeled relationships, enabling traceable decision paths for explainable AI
- Knowledge-infused learning injects background knowledge into deep learning models, improving both predictive performance and explainability by grounding predictions in established biomedical facts
- Knowledge graphs support reasoning over biomedical data, enabling discovery of new associations (e.g., drug-drug interactions, drug-target interactions) that are interpretable to domain experts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge graphs provide structured representation that links biomedical entities (drugs, diseases, proteins) via labeled relationships, enabling explainable AI by making the decision path traceable.
- Mechanism: The graph structure encodes domain knowledge as triplets (entity, relation, entity), which can be traversed to reveal why a prediction was made, satisfying both interpretability (understanding model logic) and explainability (justifying outputs).
- Core assumption: The entities and relationships in the KG are accurate, complete, and relevant to the task.
- Evidence anchors:
  - [abstract] "KGs offer a promising solution for integrating diverse health data, enhancing traceability, and facilitating profound interpretations, ultimately improving decision explainability"
  - [section II] "KGs can offer an interpretable representation of medical concepts, thereby facilitating context-aware insights"
  - [corpus] Weak/no direct evidence of KG-based explainability in the corpus neighbors; need to infer from domain literature
- Break condition: If the KG contains noisy, outdated, or irrelevant information, the explanations derived from it will be misleading or incorrect.

### Mechanism 2
- Claim: Knowledge-infused learning injects background knowledge into deep learning models, improving both predictive performance and explainability by grounding predictions in established biomedical facts.
- Mechanism: Embeddings derived from KGs are integrated into neural network layers or used as contextual features, allowing the model to leverage structured knowledge during training and inference.
- Core assumption: The KG embeddings capture meaningful semantic relationships that are beneficial for the target prediction task.
- Evidence anchors:
  - [section III] "integrating domain knowledge into deep neural models to enhance predictive capabilities and explanatory abilities"
  - [section III] "These approaches include integrating external knowledge as contextual information or features, infusing KG embeddings (KGEs) into neural model layers for explainability"
  - [corpus] No explicit KG-infused learning examples in corpus; evidence comes from the main paper's claims
- Break condition: If the KG knowledge is incomplete or biased, the model's predictions may become overconfident or incorrect, undermining explainability.

### Mechanism 3
- Claim: Knowledge graphs support reasoning over biomedical data, enabling discovery of new associations (e.g., drug-drug interactions, drug-target interactions) that are interpretable to domain experts.
- Mechanism: Symbolic logic models or graph neural networks traverse the KG to infer new facts or validate existing ones, providing human-understandable rules or paths that explain the reasoning.
- Core assumption: The reasoning methods (e.g., inductive logic programming, graph neural networks) can effectively capture and generalize the domain logic encoded in the KG.
- Evidence anchors:
  - [section II-C] "KGs enable accurate predictions and inference of new facts based on existing knowledge, benefiting clinicians seeking insights from data"
  - [section II-C] "Symbolic logic models, known for interpretability, mine logical rules from existing knowledge"
  - [corpus] Weak evidence; the corpus neighbors do not directly discuss reasoning over KGs for healthcare
- Break condition: If the reasoning method overfits to spurious correlations in the KG, the inferred associations may not generalize to real-world data.

## Foundational Learning

- Concept: Graph data structures and algorithms (nodes, edges, traversal, path finding)
  - Why needed here: KGs are fundamentally graph-based; understanding how to represent and query them is essential for both construction and reasoning.
  - Quick check question: What is the difference between a node and an edge in a knowledge graph?

- Concept: Embedding techniques for graphs (node2vec, TransE, RotatE, etc.)
  - Why needed here: KG embeddings are used to convert graph structure into vector representations that can be fed into machine learning models.
  - Quick check question: How does TransE represent a triplet (head, relation, tail) in embedding space?

- Concept: Explainable AI concepts (interpretability vs. explainability, post-hoc vs. intrinsic methods)
  - Why needed here: The paper distinguishes between making models interpretable (understandable) and explainable (providing justifications), which is critical for healthcare applications.
  - Quick check question: Give an example of a post-hoc explanation method and an intrinsic one.

## Architecture Onboarding

- Component map:
  - Data Ingestion Layer: Sources like DrugBank, KEGG, Hetionet → RDF/CSV formats
  - KG Construction Pipeline: Named Entity Recognition → Entity Linking → Relation Extraction → Ontology Mapping → Inference → Curation
  - Feature Extraction Module: KGE methods (TransE, DistMult, ComplEx) → Embeddings → Integration with DL models
  - Reasoning Engine: Rule mining (ILP, MLN) or GNN-based inference → Validation with experts
  - Explainability Interface: Path visualization, rule extraction, confidence scores → User-friendly explanations

- Critical path: Data → KG Construction → Feature Extraction → Model Training → Reasoning → Explanation → Clinical Decision Support

- Design tradeoffs:
  - Open-world vs. closed-world assumption: Open-world increases coverage but risks noise; closed-world is cleaner but may miss rare associations.
  - Symbolic vs. sub-symbolic reasoning: Symbolic is more interpretable but may not scale; sub-symbolic (GNNs) scales better but is less transparent.
  - Real-time vs. batch inference: Real-time requires optimized KG storage and indexing; batch allows deeper reasoning but is slower.

- Failure signatures:
  - Poor KG quality (missing entities, incorrect relations) → model predictions lack ground truth support.
  - Over-reliance on KG embeddings without domain validation → explanations may be misleading.
  - Complexity of KG traversal → explanations become too long or convoluted for end users.

- First 3 experiments:
  1. Load a small HKG (e.g., Hetionet) and verify basic graph statistics (nodes, edges, relation types).
  2. Train a simple TransE model on the HKG and evaluate link prediction performance on a held-out test set.
  3. Use a GNN (e.g., GCN or GAT) to predict drug-drug interactions, and extract the top-5 most influential paths from the KG that support each prediction.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can knowledge graphs be constructed under open-world assumptions while maintaining high quality in healthcare applications?
- Basis in paper: [explicit] The paper identifies this as a research challenge, noting that constructing KGs under open-world assumptions involves building them without pre-defined schemas or exhaustive entity normalization, which increases coverage but poses quality assurance challenges.
- Why unresolved: Balancing expanded coverage with quality assurance remains difficult because open-world KGs lack the constraints that help ensure data consistency and accuracy.
- What evidence would resolve it: Empirical studies demonstrating quality control methods for open-world KGs in healthcare, including validation metrics and error detection mechanisms that maintain accuracy while expanding coverage.

### Open Question 2
- Question: What are the most effective methods for integrating multiple healthcare knowledge graphs with different schemas and terminologies?
- Basis in paper: [explicit] The paper highlights knowledge integration as a challenge, specifically mentioning the need for robust ontology matching, schema alignment, and conflict resolution when merging different HKGs.
- Why unresolved: Different HKGs employ diverse terminologies, schemas, and data formats, making seamless integration difficult without losing information or introducing inconsistencies.
- What evidence would resolve it: Comparative studies of integration frameworks showing which methods best preserve information integrity and enable effective knowledge fusion across diverse healthcare KGs.

### Open Question 3
- Question: How can knowledge graph-based explanations be made more user-friendly for healthcare professionals without AI expertise?
- Basis in paper: [explicit] The paper identifies user-friendly explanations as a challenge, noting that many healthcare professionals lack technical knowledge to assess AI decisions, and while KGs offer visualization capabilities, further research is needed to optimize these methods.
- Why unresolved: Current visualization and semantic representation methods may still be too complex or technical for non-expert users to easily understand and trust.
- What evidence would resolve it: User studies demonstrating which explanation formats and interaction methods lead to better comprehension and trust among healthcare professionals with varying levels of technical expertise.

## Limitations

- Limited empirical validation: The paper describes methodologies but provides minimal quantitative results or case studies demonstrating real-world effectiveness
- Implementation gaps: Critical technical details for KG construction and integration into DL models are not fully specified
- No standardized evaluation framework: No standardized evaluation framework for comparing KG-based explainability methods is presented

## Confidence

- **High Confidence**: The general workflow of KG construction (data gathering → transformation → entity mapping → inference) and its theoretical benefits for explainability
- **Medium Confidence**: Claims about knowledge-infused learning improving both performance and explainability, as these depend heavily on implementation choices
- **Low Confidence**: Specific claims about reasoning methods discovering novel drug interactions, as these require empirical validation

## Next Checks

1. Implement a minimal KG construction pipeline using Hetionet data and evaluate link prediction accuracy
2. Compare two approaches: (a) standard DL model without KG, (b) KG-infused model using TransE embeddings, measuring both performance and explanation quality
3. Conduct a small user study with healthcare professionals to assess whether KG-derived explanations are interpretable and actionable in practice