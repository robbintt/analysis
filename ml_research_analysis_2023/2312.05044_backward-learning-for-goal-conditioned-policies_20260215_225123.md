---
ver: rpa2
title: Backward Learning for Goal-Conditioned Policies
arxiv_id: '2312.05044'
source_url: https://arxiv.org/abs/2312.05044
tags:
- learning
- goal
- world
- policy
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a reward-free method for learning goal-conditioned
  policies using backward world models. The core idea is to train a discrete backward
  world model that predicts previous states given current states and actions, then
  generate goal-reaching trajectories by starting at the goal and simulating backwards
  in time.
---

# Backward Learning for Goal-Conditioned Policies

## Quick Facts
- arXiv ID: 2312.05044
- Source URL: https://arxiv.org/abs/2312.05044
- Reference count: 4
- One-line primary result: 88-99% success rate in reaching goals from various starting positions in a 20x20 deterministic maze environment

## Executive Summary
This paper introduces a reward-free method for learning goal-conditioned policies using backward world models. The approach learns to predict previous states from current states and actions, then generates goal-reaching trajectories by simulating backwards from the goal state. These trajectories are improved using shortest path algorithms on a directed graph constructed from the simulations. Finally, a neural network policy is trained by imitation learning on the improved dataset, achieving high success rates in navigating to specific goal locations.

## Method Summary
The method consists of four main steps: (1) Train a discrete backward world model that predicts previous states given current states and actions using collected random forward trajectories, (2) Generate goal-reaching trajectories by starting at the goal and simulating backwards in time using the backward world model, (3) Improve these trajectories using shortest path algorithms (Dijkstra's) on a directed graph constructed from the simulations, and (4) Train a neural network policy by imitation learning on the improved dataset. The approach uses a discrete latent representation with entropy regularization to enable graph-based planning, and is evaluated on a 20x20 deterministic maze environment with 64x64 pixel observations.

## Key Results
- 88-99% success rate in reaching goals from various starting positions in a 20x20 deterministic maze
- Policies trained on one or four goals simultaneously achieve comparable performance
- Method works without requiring reward signals, using only goal-reaching trajectories

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The backward world model can simulate goal-reaching trajectories by starting from the goal state and simulating backward in time.
- Mechanism: The model learns to predict the previous state given a current state and action, allowing it to generate trajectories that naturally terminate at the goal state.
- Core assumption: The environment dynamics are deterministic or can be modeled as stochastic distributions that are learnable from random forward trajectories.
- Evidence anchors:
  - [abstract] "learn a world model that goes backward in time, secondly generates goal-reaching backward trajectories"
  - [section] "learn a backwards world model that takes a state-action pair (st, at−1) as input and outputs the previous state"
  - [corpus] Weak evidence - no direct comparison to forward-only approaches in related works
- Break condition: The backward dynamics become too stochastic to predict accurately, leading to divergent or looping trajectories.

### Mechanism 2
- Claim: Dijkstra's algorithm on the directed graph of simulated states finds shortest paths to the goal, creating high-quality training data.
- Mechanism: By treating each state as a node and backward transitions as edges, Dijkstra's algorithm computes shortest path estimates that are used to filter only transitions that bring us closer to the goal.
- Core assumption: The simulated backward trajectories form a connected graph that contains valid paths to the goal.
- Evidence anchors:
  - [section] "Apply a shortest path finding algorithm on the DG to find the shortest distances to the goal node"
  - [section] "if a forward transition (zt, at, zt+1) brings us closer to the goal, we include it"
  - [corpus] Missing - no comparison to other path-finding algorithms or heuristics
- Break condition: The graph becomes disconnected or contains cycles that Dijkstra cannot resolve effectively.

### Mechanism 3
- Claim: The discrete latent representation with entropy regularization enables graph-based planning by ensuring each observation maps to a single latent state.
- Mechanism: The entropy loss schedule reduces the entropy of the latent distribution over training, forcing the encoder to produce deterministic latent codes that can serve as graph nodes.
- Core assumption: A discrete latent space with low entropy is necessary for creating a well-defined directed graph.
- Evidence anchors:
  - [section] "the entropy of our latent representation decreases at the end of training to a minimum" and "to build directed graphs without multiple nodes for a single observation"
  - [section] "Each node is a state encoding zt" in the directed graph construction
  - [corpus] Weak evidence - no ablation studies on the impact of latent discretization
- Break condition: The latent space becomes too constrained, causing different observations to collapse to the same latent code.

## Foundational Learning

- Concept: Discrete latent variable modeling with categorical distributions
  - Why needed here: The method requires discrete states to construct a directed graph for Dijkstra's algorithm
  - Quick check question: How does the straight-through gradient estimator work for sampling from categorical distributions?

- Concept: Dijkstra's shortest path algorithm
  - Why needed here: To find the optimal sequence of actions from any state to the goal in the simulated backward trajectories
  - Quick check question: What is the time complexity of Dijkstra's algorithm on a graph with V vertices and E edges?

- Concept: Goal-conditioned reinforcement learning
  - Why needed here: The policy must learn to reach specific goal states rather than maximize cumulative rewards
  - Quick check question: How does goal-conditioned RL differ from standard reward-based RL in terms of policy conditioning?

## Architecture Onboarding

- Component map:
  - Encoder (CNN) → Discrete latent space (16 categoricals × 16 classes) → Decoder (CNN)
  - Backward world model (MLP) predicting previous latent state
  - Directed graph builder from simulated trajectories
  - Dijkstra's algorithm for shortest path computation
  - Imitation learning policy (MLP) trained on filtered transitions

- Critical path:
  1. Collect random forward trajectories
  2. Train encoder and backward world model jointly
  3. Generate backward simulations from goal state
  4. Build directed graph from simulations
  5. Apply Dijkstra's algorithm
  6. Filter transitions using shortest path estimates
  7. Train imitation learning policy

- Design tradeoffs:
  - Discrete vs continuous latent space: Discrete enables graph construction but may lose information
  - Backward vs forward world model: Backward ensures goal-reaching but may be less stable
  - Random action selection vs learned backward policy: Simpler but potentially less efficient

- Failure signatures:
  - Policy fails to reach goals consistently → Check if graph construction or Dijkstra filtering is working
  - High reconstruction loss → Encoder-decoder training issue
  - Backward model predicts poorly → Insufficient training data or model capacity

- First 3 experiments:
  1. Test backward model predictions on held-out trajectories to verify it learns reverse dynamics
  2. Visualize the directed graph to ensure it contains valid paths from various states to the goal
  3. Run Dijkstra's algorithm separately to confirm it finds reasonable shortest paths in the graph

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- Evaluated only on a single 20x20 deterministic maze environment, limiting generalizability to more complex or stochastic environments
- Method relies heavily on discrete latent representations and graph-based planning, which may not scale well to higher-dimensional state spaces
- No extensive ablation studies to isolate the contributions of each component (backward world model, Dijkstra's algorithm, entropy regularization) to the final performance

## Confidence
- High confidence in the mechanism of backward world model simulation and graph construction
- Medium confidence in the effectiveness of Dijkstra's algorithm for path improvement
- Low confidence in the scalability of discrete latent representations to more complex environments

## Next Checks
1. Test the method on stochastic versions of the maze environment to evaluate robustness to noise in dynamics
2. Compare the performance of the backward world model approach against a forward world model baseline on the same tasks
3. Conduct an ablation study removing the entropy regularization term to quantify its impact on policy performance