---
ver: rpa2
title: Direct Preference-Based Evolutionary Multi-Objective Optimization with Dueling
  Bandit
arxiv_id: '2311.14003'
source_url: https://arxiv.org/abs/2311.14003
tags:
- population
- learning
- dueling
- algorithm
- bandit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a direct preference-based evolutionary multi-objective
  optimization (PBEMO) framework using an active dueling bandit algorithm (RUCB-AL).
  The approach learns global optima directly from human feedback without requiring
  a fitness function, addressing the challenge of expensive sampling and consultation
  in PBEMO.
---

# Direct Preference-Based Evolutionary Multi-Objective Optimization with Dueling Bandit

## Quick Facts
- arXiv ID: 2311.14003
- Source URL: https://arxiv.org/abs/2311.14003
- Authors: 
- Reference count: 40
- Key outcome: RUCB-AL converges faster than baseline algorithms and achieves stable, accurate performance on benchmark problems with limited query budgets

## Executive Summary
This paper introduces a direct preference-based evolutionary multi-objective optimization (PBEMO) framework using an active dueling bandit algorithm (RUCB-AL). The approach learns global optima directly from human feedback without requiring a fitness function, addressing the challenge of expensive sampling and consultation in PBEMO. The method integrates with three types of MOEAs (NSGA-II, MOEA/D, R2-IBEA) and employs active learning to control query budgets while ensuring accurate preference prediction. Experiments demonstrate that RUCB-AL converges faster than baseline algorithms and achieves stable, accurate performance on benchmark problems with limited query budgets.

## Method Summary
The proposed method uses RUCB-AL to select pairs of solutions for pairwise comparison and learn the preference matrix P over time. The algorithm maintains a virtual fitness function Vs that is updated only when human feedback indicates a stable new recommendation. Active learning is incorporated through two constraints (c1 and c2) that determine when to consult the human oracle based on generation fraction and KL divergence thresholds. The approach integrates with three MOEA architectures: dominance-based (NSGA-II), decomposition-based (MOEA/D), and indicator-based (R2-IBEA). Human feedback is collected through pairwise comparisons of solutions, and the preference elicitation module updates the virtual fitness function accordingly.

## Key Results
- RUCB-AL achieves faster convergence compared to baseline preference learning algorithms on benchmark problems
- The active learning strategy effectively controls query budget while maintaining accurate preference prediction
- Superior performance on protein structure prediction problem compared to traditional fitness-based approaches
- Stable and consistent results across multiple benchmark problem sets with different dimensionalities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** RUCB-AL directly learns the global optima from human feedback without requiring a fitness function
- **Mechanism:** The active dueling bandit algorithm selects pairs of solutions for pairwise comparison and learns the preference matrix P over time. The virtual fitness function Vs is updated only when human feedback indicates a stable new recommendation.
- **Core assumption:** Human preferences are consistent and stationary over time
- **Evidence anchors:** [abstract] "sidesteps the need for calculating the fitness function, relying solely on human feedback"
- **Break condition:** If human preferences change over time or are inconsistent, the algorithm will not converge to a stable global optimum

### Mechanism 2
- **Claim:** Active learning controls the budget of query times while ensuring accurate preference prediction
- **Mechanism:** The algorithm uses two constraints (c1 and c2) to determine when to consult the human oracle. Constraint c1 triggers the first consultation after a fraction of generations, and constraint c2 triggers subsequent consultations only when the KL divergence between predicted distributions falls below a threshold.
- **Core assumption:** The information gain (KL divergence) between consecutive predicted distributions is a good proxy for the stability of human preferences
- **Evidence anchors:** [section 2.4] "When the recommendation of consultation module becomes stable, the predicted preference distribution will be almost the same, and thus DKL will have a small value"
- **Break condition:** If the KL divergence threshold is set too high or too low, the algorithm may either over-query or under-query the human oracle

### Mechanism 3
- **Claim:** The RUCB-AL algorithm has a regret bound of O(K), ensuring efficient exploration and exploitation
- **Mechanism:** The algorithm maintains a utility matrix U that measures the prediction accuracy for each arm. The first candidate arm c is selected if its row has the maximum cumulative loss, and the second candidate arm d is selected if, in the row corresponding to c, arm d contributes the most to the cumulative loss.
- **Core assumption:** The regret bound holds for the Copeland winner assumption
- **Evidence anchors:** [section 2.3.2] "Our proposed method has regret satisfying the following proposition"
- **Break condition:** If the Copeland winner assumption does not hold, the regret bound may not be achievable

## Foundational Learning

- **Concept:** Multi-objective optimization (MOO) and Pareto optimality
  - **Why needed here:** The paper deals with multi-objective optimization problems where the goal is to find solutions that are Pareto optimal
  - **Quick check question:** What is the difference between a Pareto optimal solution and a globally optimal solution in a single-objective optimization problem?

- **Concept:** Dueling bandit algorithms and preference learning
  - **Why needed here:** The paper uses a dueling bandit algorithm (RUCB-AL) to learn human preferences over solutions without requiring a fitness function
  - **Quick check question:** How does a dueling bandit algorithm differ from a traditional multi-armed bandit algorithm?

- **Concept:** Active learning and query strategies
  - **Why needed here:** The paper incorporates active learning to control the budget of query times while ensuring accurate preference prediction
  - **Quick check question:** What is the difference between uncertainty sampling and query-by-committee in active learning?

## Architecture Onboarding

- **Component map:** Optimization Module (MOEAs) -> Consultation Module (RUCB-AL) -> Preference Elicitation Module (Constraints c1/c2)
- **Critical path:** Generate initial population using MOEA -> Select 10 incumbent solutions using Vs -> Feed selected solutions to RUCB-AL for pairwise comparison -> Update Vs based on human feedback and KL divergence threshold -> Repeat until convergence or budget limit
- **Design tradeoffs:** Exploration vs. exploitation in the RUCB-AL algorithm, Query budget vs. accuracy of preference prediction, Number of incumbent solutions vs. computational complexity
- **Failure signatures:** High KL divergence between consecutive predicted distributions indicates unstable human preferences, Low cumulative regret indicates inefficient exploration by RUCB-AL, Large population size indicates poor convergence of MOEA
- **First 3 experiments:** Implement RUCB-AL on a simple toy problem with 10 arms and a small budget to verify the regret bound, Integrate RUCB-AL with NSGA-II on a ZDT test problem and compare with a fitness-based preference learning algorithm, Apply the complete PBEMO framework to a protein structure prediction problem and evaluate the RMSD of the predicted structure

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the performance of RUCB-AL scale with increasing number of arms K in the dueling bandit algorithm?
- **Basis in paper:** [explicit] The paper mentions RUCB-AL's regret bound of O(K) but does not provide empirical results on how performance scales with K
- **Why unresolved:** The experiments only tested K=10 and K=100, leaving scalability for larger K unexplored
- **What evidence would resolve it:** Empirical results showing RUCB-AL's performance on problems with K > 100 arms

### Open Question 2
- **Question:** How sensitive is RUCB-AL's performance to the choice of hyperparameters like κ (trade-off between exploration and exploitation)?
- **Basis in paper:** [inferred] The paper mentions κ controls the trade-off between random and greedy search but only tests one value (κ=0.3) in the experiments
- **Why unresolved:** The paper does not explore the sensitivity of performance to different κ values or provide guidelines for tuning
- **What evidence would resolve it:** A sensitivity analysis showing RUCB-AL's performance across a range of κ values

### Open Question 3
- **Question:** How does RUCB-AL compare to other active learning strategies (e.g., query-by-committee, expected model change) in the context of preference-based optimization?
- **Basis in paper:** [inferred] The paper mentions active learning has three core compositions but only uses uncertainty sampling in RUCB-AL
- **Why unresolved:** The paper does not compare RUCB-AL to other active learning strategies or justify the choice of uncertainty sampling
- **What evidence would resolve it:** Empirical comparison of RUCB-AL with other active learning strategies on preference-based optimization problems

## Limitations

- The approach assumes stationary human preferences, which may not hold in dynamic real-world scenarios
- Performance depends critically on KL divergence threshold and consultation budget settings that may not generalize across domains
- Lack of ablation studies on the consultation module's contribution to overall performance
- Claims about protein structure prediction are based on a single benchmark without comparison to state-of-the-art specialized methods

## Confidence

**High Confidence**: The core mechanism of using dueling bandits for preference learning in multi-objective optimization is technically sound and well-grounded in existing literature.

**Medium Confidence**: The active learning component's effectiveness is supported by experimental results but lacks theoretical justification for why KL divergence specifically is the optimal proxy for consultation timing.

**Low Confidence**: Claims about superior performance on the protein structure prediction problem are based on a single benchmark and lack comparison with state-of-the-art specialized PSP methods.

## Next Checks

1. **Consultation Module Ablation**: Run experiments with different KL divergence thresholds (0.1, 0.5, 1.0) and consultation budgets to quantify their impact on convergence speed and final solution quality.

2. **Preference Noise Sensitivity**: Introduce controlled noise into human preference feedback (10%, 30%, 50% random responses) to test algorithm robustness and identify breaking points.

3. **High-Dimensional Scalability**: Apply the method to DTLZ problems with m=15 and m=20 objectives to evaluate whether the active learning strategy remains effective as the objective space dimensionality increases.