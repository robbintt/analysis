---
ver: rpa2
title: 'TEQ: Trainable Equivalent Transformation for Quantization of LLMs'
arxiv_id: '2310.10944'
source_url: https://arxiv.org/abs/2310.10944
tags:
- quantization
- gptq
- arxiv
- methods
- ours
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TEQ introduces a trainable equivalent transformation for quantizing
  LLMs that maintains FP32 precision while enabling low-bit weight quantization. The
  method applies per-channel scaling to weights and activations, keeping the model
  output mathematically equivalent but improving quantization robustness.
---

# TEQ: Trainable Equivalent Transformation for Quantization of LLMs

## Quick Facts
- arXiv ID: 2310.10944
- Source URL: https://arxiv.org/abs/2310.10944
- Reference count: 22
- Primary result: TEQ achieves state-of-the-art 4-bit quantization performance across 12 scenarios when combined with GPTQ

## Executive Summary
TEQ introduces a trainable equivalent transformation for quantizing large language models that maintains FP32 precision while enabling low-bit weight quantization. The method applies per-channel scaling to weights and activations, keeping the model output mathematically equivalent but improving quantization robustness. Training is lightweight, requiring only 1K steps and less than 0.1% of the original model's parameters. For 4-bit quantization, TEQ outperforms round-to-nearest quantization in most cases and matches or exceeds GPTQ results on 6 of 12 evaluated scenarios. When combined with GPTQ, TEQ achieves new state-of-the-art performance in 8 of 12 scenarios.

## Method Summary
TEQ applies per-channel scaling to weights and activations in LLMs, preserving mathematical equivalence while optimizing the weight distribution before quantization. The method freezes original model weights and only trains per-channel scaling vectors, requiring minimal training (1K steps) with less than 0.1% of the original parameters. The transformation can be combined with other quantization methods like GPTQ, creating a two-stage optimization process. The approach works across different LLM architectures including LLaMA, BLOOM, and OPT models ranging from 3B to 13B parameters.

## Key Results
- Outperforms round-to-nearest quantization in most 4-bit scenarios
- Matches or exceeds GPTQ results on 6 of 12 evaluated scenarios
- Achieves state-of-the-art performance in 8 of 12 scenarios when combined with GPTQ
- Maintains mathematical equivalence while improving quantization robustness

## Why This Works (Mechanism)

### Mechanism 1
Per-channel scaling keeps the model output mathematically equivalent while reducing quantization loss. TEQ multiplies a per-channel scaling vector to weights and applies the inverse scaling to activations, preserving the FP32 output mathematically while optimizing the weight distribution before quantization. The optimal scaling vector exists such that the transformed weights and activations can be quantized with minimal error while maintaining mathematical equivalence.

### Mechanism 2
Training the scaling parameters is lightweight because it requires only 1K steps with less than 0.1% of the original model's parameters. Instead of training all model weights, TEQ freezes the original weights and only trains the per-channel scaling vectors, dramatically reducing parameter count and computational overhead.

### Mechanism 3
TEQ is orthogonal to existing quantization methods and can be combined to achieve better performance. The method modifies the weight distribution before quantization without changing the model architecture or inference process, allowing it to be applied before other quantization methods like GPTQ for complementary improvements.

## Foundational Learning

- Matrix multiplication and linear transformations: Understanding how per-channel scaling affects the output of matmul operations is fundamental to grasping TEQ's mechanism. Quick check: If wl is a cin x cout matrix and sl is a cin-dimensional vector, what is the dimension of diag(sl) and how does it transform wl?

- Quantization error and rounding: TEQ's effectiveness depends on understanding how quantization introduces error and how the scaling transformation can minimize this error. Quick check: In 4-bit quantization with symmetric ranges, what is the quantization step size for weights in [-1, 1]?

- Gradient computation through quantization: TEQ uses straight-through estimator (STE) to backpropagate through quantization operations during scaling parameter training. Quick check: What is the gradient of the round-to-nearest operation in the STE approximation?

## Architecture Onboarding

- Component map: Input layer → LayerNorm/BatchNorm → TEQ scaling application → Quantization → Model layers → Output
- Critical path: Load pre-trained model weights (frozen) → Initialize scaling vectors → Forward pass with scaling and quantization simulation → Compute loss and backpropagate through STE → Update scaling vectors → Repeat for 1K steps
- Design tradeoffs: TEQ vs QAT (minimal training vs full fine-tuning), TEQ vs PTQ (improved accuracy vs added training overhead), per-channel vs per-tensor scaling (finer control vs parameter count)
- Failure signatures: Poor performance on extreme quantization indicates scaling vectors aren't capturing distribution outliers, inconsistent improvements across layers suggest suboptimal scaling transformation, high memory usage despite few parameters indicates inefficient implementation
- First 3 experiments: 1) Baseline comparison on BLOOM-3B with 4-bit quantization vs round-to-nearest, 2) Scaling initialization ablation comparing ones vs 1.0/sqrt(wcin), 3) Combination validation applying TEQ followed by GPTQ on OPT-6.7B

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of TEQ compare to other state-of-the-art quantization methods when applied to even larger language models (e.g., models with over 100 billion parameters)? The paper discusses application to 3B-13B parameter models but doesn't explore performance on models with over 100 billion parameters.

### Open Question 2
What are the specific computational overheads introduced by TEQ during inference, and how do they compare to other quantization methods? While the paper mentions no computational overhead, it doesn't provide detailed analysis of computational requirements or comparisons to other methods.

### Open Question 3
How does the choice of initialization for the transformation scales (sl) affect the final performance of TEQ, and are there more optimal initialization strategies? The paper briefly mentions two initialization strategies but doesn't explore other potential strategies or provide comprehensive analysis of initialization impact.

## Limitations
- Limited theoretical guarantees for lightweight training claim across all model architectures
- Lack of rigorous proof for orthogonality claim with GPTQ
- No analysis of numerical stability when scaling vectors approach extreme values

## Confidence
**High confidence:** The basic mechanism of per-channel scaling and mathematical equivalence is well-established and clearly demonstrated.
**Medium confidence:** Claims about TEQ's effectiveness relative to baselines are supported by experimental results but sample size is relatively small.
**Low confidence:** Orthogonality claim with GPTQ lacks rigorous theoretical justification and edge cases aren't addressed.

## Next Checks
1. Conduct scaling vector sensitivity analysis with different initialization strategies and training step counts to determine minimum effective training budget.
2. Test TEQ's robustness to extreme quantization scenarios (2-bit and 3-bit) to identify break conditions and analyze scaling vector distributions in failure cases.
3. Evaluate TEQ's combination with other quantization methods beyond GPTQ to validate orthogonality claim across broader range of techniques.