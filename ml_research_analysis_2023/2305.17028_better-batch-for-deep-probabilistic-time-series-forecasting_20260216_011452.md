---
ver: rpa2
title: Better Batch for Deep Probabilistic Time Series Forecasting
arxiv_id: '2305.17028'
source_url: https://arxiv.org/abs/2305.17028
tags:
- uni00000013
- uni00000011
- time
- series
- forecasting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses error autocorrelation in deep probabilistic
  time series forecasting. The authors propose a novel training method that explicitly
  models error autocorrelation by constructing mini-batches of consecutive time series
  segments and learning a time-varying covariance matrix over each mini-batch.
---

# Better Batch for Deep Probabilistic Time Series Forecasting

## Quick Facts
- arXiv ID: 2305.17028
- Source URL: https://arxiv.org/abs/2305.17028
- Authors: 
- Reference count: 40
- Key outcome: Novel training method that models error autocorrelation in deep probabilistic time series forecasting by learning a time-varying covariance matrix over mini-batches of consecutive time series segments

## Executive Summary
This paper addresses a fundamental challenge in deep probabilistic time series forecasting: error autocorrelation. Traditional methods like DeepAR model errors as independent across time steps, which fails to capture the serial correlation commonly present in time series data. The authors propose a novel training approach that explicitly models error autocorrelation by constructing mini-batches of consecutive time series segments and learning a time-varying covariance matrix over each mini-batch. This covariance matrix encodes error correlation among adjacent time steps and is parameterized using a stationary kernel function, specifically a squared-exponential kernel.

The method is implemented as a modification to the DeepAR architecture, introducing minimal additional parameters while providing a statistically consistent framework for error correlation modeling. Experimental results on multiple datasets demonstrate that this approach effectively captures error autocorrelation and improves probabilistic forecasting accuracy, with notable performance gains across various prediction horizons. The approach maintains the scalability of deep learning methods while addressing a critical limitation in existing probabilistic forecasting models.

## Method Summary
The proposed method modifies the DeepAR training procedure by constructing mini-batches of D consecutive time series segments rather than randomly sampled sequences. For each mini-batch, the model learns a covariance matrix over the error terms using a squared-exponential (SE) kernel function parameterized by lengthscale l and weight v. The covariance matrix is computed as C = (1-v)K + vI, where K is generated from the SE kernel. During training, the model jointly optimizes the DeepAR parameters and the kernel parameters by maximizing the multivariate Gaussian likelihood of the mini-batch errors. For prediction, the learned correlation structure is used to condition future predictions on past errors within the mini-batch, effectively calibrating the prediction distribution at each step and mitigating error accumulation in multi-step forecasting.

## Key Results
- Significant improvements in CRPS across multiple datasets (traffic, electricity, m4-hourly, hospital, nn5) compared to standard DeepAR
- Performance gains become increasingly significant as prediction horizon extends further into the future
- The method introduces only two additional parameters to DeepAR while maintaining scalability
- Optimal mini-batch size D varies by dataset, with traffic and m4-hourly benefiting most from larger D values

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicit modeling of error autocorrelation within mini-batches improves probabilistic forecasting accuracy.
- Mechanism: By constructing mini-batches of D consecutive time series segments and learning a time-varying covariance matrix over each mini-batch, the method captures serial correlation among adjacent time steps. This covariance matrix encodes error correlation and is parameterized using a stationary kernel function, allowing the model to account for previously observed errors during prediction.
- Core assumption: Error terms in time series data exhibit significant autocorrelation that can be modeled by a stationary covariance structure.
- Evidence anchors:
  - [abstract]: "Our method constructs a mini-batch as a collection of D consecutive time series segments for model training. It explicitly learns a covariance matrix over each mini-batch, encoding error correlation among adjacent time steps."
  - [section 2.2]: "When there exists serial correlation in the error process, we will have ηT+1:T+Q = [ηi,T+1, . . . , ηi,T+Q]⊤ follows a multivariate Gaussian distribution with a certain covariance matrix N (0, Σ), where Σ is the autocovariance matrix that has non-zero elements on its off-diagonal entries."
- Break condition: If the error process is truly independent (no autocorrelation), the additional complexity of modeling covariance provides no benefit and may even hurt performance.

### Mechanism 2
- Claim: Jointly learning kernel parameters with the base model enables accurate uncertainty quantification.
- Mechanism: The covariance matrix is parameterized using a squared-exponential (SE) kernel function with parameters jointly optimized with the base DeepAR model. This approach ensures the covariance matrix is positive definite and allows the model to learn the appropriate correlation structure directly from data.
- Core assumption: A stationary kernel function (specifically SE kernel) can adequately represent the autocorrelation structure in the error process.
- Evidence anchors:
  - [section 4.1]: "We parameterize C as C = (1 − v)K + vI, where v ∈ (0, 1). Here, K is a covariance matrix generated from a squared-exponential (SE) kernel function, Kij = exp(− (i−j)2 / l2 )."
  - [section 5.3]: "Table 3 provides an overview of the weight parameters learned using different lengthscales. It is interesting to note that, in general, the value of v increases as the lengthscale l increases."
- Break condition: If the true error correlation structure is non-stationary or has complex patterns that cannot be captured by a simple kernel function, the model's performance will degrade.

### Mechanism 3
- Claim: The learned covariance matrix can be used during prediction to calibrate distributions and mitigate error accumulation.
- Mechanism: During multi-step forecasting, the model uses the learned correlation matrix to condition predictions on previously observed errors. This is done by computing the conditional distribution of the next error given past errors within the mini-batch, effectively calibrating the prediction distribution at each step.
- Core assumption: The time-invariant correlation structure learned during training remains approximately valid during the prediction horizon.
- Evidence anchors:
  - [section 4.2]: "Thus, for the first time step (t + 1) to be predicted, we have the conditional distribution of ϵt+1 given the past D − 1 observed errors: ϵt+1|ϵt, ϵt−1, . . . , ϵt−D+2 ∼ N(C∗C−1obs ϵobs t , 1 − C∗C−1obs C⊤∗)."
  - [section 5.2]: "This is particularly evident in multiple datasets, such as traffic, electricity, and m4-hourly, where the accuracy improvement becomes increasingly significant as we extend the prediction further into the future."
- Break condition: If the correlation structure changes rapidly over time or if the prediction horizon exceeds the range over which the correlation is meaningful, the calibration will become ineffective.

## Foundational Learning

- Concept: Multivariate Gaussian distribution and covariance matrices
  - Why needed here: The method explicitly models the joint distribution of errors across multiple time steps, requiring understanding of how covariance matrices parameterize correlations.
  - Quick check question: Given a covariance matrix with off-diagonal elements of 0.5 and diagonal elements of 1, what is the correlation coefficient between any two distinct variables?

- Concept: Kernel functions and positive definite matrices
  - Why needed here: The covariance matrix is constructed using a kernel function to ensure it is positive definite, which is required for valid multivariate Gaussian distributions.
  - Quick check question: What property must a kernel function satisfy to generate a valid covariance matrix?

- Concept: Conditional distributions of multivariate Gaussians
  - Why needed here: During prediction, the method computes conditional distributions of future errors given past errors, which requires understanding of how to partition and condition multivariate Gaussian distributions.
  - Quick check question: If x = [x1, x2] follows a multivariate Gaussian distribution, how do you express the conditional distribution of x1 given x2?

## Architecture Onboarding

- Component map:
  Data preprocessing -> Mini-batch construction (D consecutive segments) -> Base model (DeepAR) -> Covariance parameterization (SE kernel) -> Multivariate Gaussian likelihood -> Joint optimization

- Critical path:
  1. Construct mini-batches of D consecutive time series segments
  2. Compute base model outputs (mean and std) for each time step
  3. Parameterize covariance matrix using SE kernel and weight v
  4. Compute multivariate Gaussian likelihood for mini-batch
  5. Backpropagate through joint loss to update base model and kernel parameters

- Design tradeoffs:
  - Choice of D (mini-batch size): Larger D captures longer-range correlations but increases computational complexity and memory usage
  - Kernel selection: SE kernel is simple and ensures positive definiteness, but may not capture all correlation structures
  - Stationarity assumption: Simplifies implementation but may not hold for all datasets

- Failure signatures:
  - Performance similar to baseline: Likely indicates weak autocorrelation in the data
  - Training instability: May occur if the covariance parameterization becomes ill-conditioned
  - Degraded performance: Could happen if the kernel parameters overfit to training data

- First 3 experiments:
  1. Baseline comparison: Train standard DeepAR and compare CRPS on validation set
  2. Hyperparameter sweep: Test different values of D (1, 3, 6, 12) to find optimal mini-batch size
  3. Lengthscale sensitivity: Evaluate performance with l = 1, 2, 3 to understand correlation horizon

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed method scale with different dataset characteristics such as seasonality, trend, and noise levels?
- Basis in paper: [inferred] The paper mentions that the current method assumes a time-invariant and stationary correlation matrix, which may not hold for datasets with seasonal patterns.
- Why unresolved: The paper only tests the method on five datasets without explicitly analyzing how dataset characteristics affect performance. The authors acknowledge this limitation but don't provide empirical evidence.
- What evidence would resolve it: Systematic experiments on datasets with varying degrees of seasonality, trend strength, and noise levels, with detailed analysis of how these factors influence the method's effectiveness.

### Open Question 2
- Question: Can the proposed method be effectively extended to multivariate time series forecasting, and what modifications would be necessary?
- Basis in paper: [explicit] The authors mention in the conclusion that the method could be extended to multivariate models and suggest using a matrix Gaussian distribution instead of the current multivariate Gaussian distribution.
- Why unresolved: The paper only tests the method on univariate time series forecasting. The proposed extension to multivariate forecasting is speculative and not empirically validated.
- What evidence would resolve it: Implementation and testing of the proposed matrix Gaussian distribution approach on multivariate time series datasets, with comparison to existing multivariate forecasting methods.

### Open Question 3
- Question: How does the choice of kernel function for parameterizing the covariance matrix affect the model's performance and flexibility?
- Basis in paper: [explicit] The authors use a squared-exponential (SE) kernel function and mention that alternative autocorrelation structures like a moving-average process could be explored.
- Why unresolved: The paper only tests one kernel function and doesn't explore how different kernels might affect performance or whether more flexible autocorrelation structures could improve results.
- What evidence would resolve it: Experiments comparing different kernel functions (e.g., SE, Matérn, rational quadratic) and more flexible autocorrelation structures on various datasets, with analysis of their impact on forecasting accuracy.

## Limitations
- The method assumes error autocorrelation follows a stationary structure, which may not hold for datasets with changing correlation patterns over time
- Performance critically depends on the choice of mini-batch size D, requiring careful hyperparameter tuning for each dataset
- The squared-exponential kernel may not capture complex non-stationary correlation structures present in some time series

## Confidence
- **High confidence** in the core mathematical formulation and the mechanism by which the covariance matrix captures error autocorrelation
- **Medium confidence** in the empirical performance claims, as results show consistent improvements but effectiveness may vary with data characteristics
- **Medium confidence** in the practical utility, as the method adds minimal parameters but requires careful hyperparameter tuning

## Next Checks
1. **Ablation study on D**: Systematically evaluate performance across a wider range of mini-batch sizes (e.g., D = 1, 2, 4, 8, 16) to understand the sensitivity to this hyperparameter and identify the optimal range for different dataset characteristics.

2. **Stationarity test**: Apply statistical tests (e.g., Ljung-Box test) to measure autocorrelation in the error processes across all datasets, then correlate these measurements with the performance improvements to quantify the relationship between data characteristics and method effectiveness.

3. **Long-horizon prediction stability**: Evaluate the method's performance on extended prediction horizons (e.g., 24, 48, 72 steps ahead) to assess whether the time-invariant correlation assumption remains valid and whether error accumulation is effectively mitigated over longer time scales.