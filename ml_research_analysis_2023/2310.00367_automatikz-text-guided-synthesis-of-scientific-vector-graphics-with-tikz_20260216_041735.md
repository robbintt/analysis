---
ver: rpa2
title: 'AutomaTikZ: Text-Guided Synthesis of Scientific Vector Graphics with TikZ'
arxiv_id: '2310.00367'
source_url: https://arxiv.org/abs/2310.00367
tags:
- tikz
- language
- image
- caption
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors address the challenge of generating vector graphics
  for scientific figures by proposing TikZ as an intermediate representation. They
  introduce DaTikZ, a large-scale dataset of 120k TikZ drawings with captions, and
  fine-tune LLaMA and a new model CLiMA on this dataset.
---

# AutomaTikZ: Text-Guided Synthesis of Scientific Vector Graphics with TikZ

## Quick Facts
- arXiv ID: 2310.00367
- Source URL: https://arxiv.org/abs/2310.00367
- Reference count: 40
- Authors present a novel approach for generating scientific vector graphics using TikZ as an intermediate representation

## Executive Summary
This paper addresses the challenge of generating vector graphics for scientific figures by proposing TikZ as an intermediate representation for language models. The authors introduce DaTikZ, a large-scale dataset of 120k TikZ drawings with captions, and fine-tune LLaMA on this dataset. They further develop CLiMA, which integrates CLIP embeddings into LLaMA to improve text-image alignment. Through automatic and human evaluations, both CLiMA and LLaMA outperform commercial models like GPT-4 and Claude 2 in generating scientific figures similar to human-created ones. The framework and datasets are publicly available for research use.

## Method Summary
The approach uses TikZ, a high-level graphics language that compiles to vector graphics, as an intermediate representation between natural language captions and scientific figures. The authors create DaTikZ, a dataset of 120k paired TikZ drawings and captions, then fine-tune LLaMA on this specialized corpus. CLiMA extends this by integrating CLIP embeddings through a multimodal projection layer, enabling visual interpretation of input captions. The models are trained using AdamW optimizer with batch size 128 and learning rate 5e-4 for 12 epochs. Evaluation combines automatic metrics (CLIPScore, KID, CrystalBLEU, EED, CSR) with human best-worst scaling for caption and reference similarity.

## Key Results
- CLiMA and LLaMA outperform commercial models (GPT-4, Claude 2) in generating scientific figures similar to human-created ones
- CLiMA shows improved text-image alignment through CLIP integration
- All models demonstrate good generalization without memorization issues
- GPT-4 and Claude 2 tend to generate simpler figures and sometimes copy input captions (typographic attacks)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TikZ as an intermediate representation enables effective language modeling for scientific figure generation.
- Mechanism: By using TikZ, a high-level graphics language that compiles to vector graphics, the system can leverage existing LLM architectures to generate structured, programmable outputs rather than raw raster images.
- Core assumption: TikZ's human-oriented, high-level commands are learnable by language models and can capture the semantic intent of scientific figures.
- Evidence anchors:
  - [abstract]: "we propose the use of TikZ... as an intermediate representation of scientific figures. TikZ offers human-oriented, high-level commands, thereby facilitating conditional language modeling with any large language model."
  - [section 1]: "we focus on the graphics language TikZ in this work (Tantau, 2023). We aim to understand whether language models can capture the nuances of TikZ and automatically generate scientific figures based on image captions"
  - [corpus]: Weak evidence - no direct corpus support for TikZ-specific mechanisms, but related work on code generation exists.
- Break condition: If TikZ commands prove too complex or context-dependent for LLMs to learn effectively, or if compilation errors become too frequent.

### Mechanism 2
- Claim: Fine-tuning LLaMA on DaTikZ dataset improves scientific figure generation compared to general-purpose LLMs.
- Mechanism: Domain-specific fine-tuning on 120k paired TikZ drawings and captions allows the model to learn the specific patterns and structures of scientific figures, while general-purpose models lack this specialized training.
- Core assumption: The DaTikZ dataset captures sufficient diversity and complexity of scientific figures to enable effective learning.
- Evidence anchors:
  - [abstract]: "We fine-tune LLaMA on DaTikZ... In both human and automatic evaluation, CLiMA and LLaMA outperform commercial GPT-4 and Claude 2 in terms of similarity to human-created figures"
  - [section 4]: "we introduce DaTikZ, the first large-scale TikZ dataset to our knowledge, featuring approximately 120k paired TikZ drawings and captions. We fine-tune the large language model (LLM) LLaMA (Touvron et al., 2023a) on DaTikZ"
  - [corpus]: Weak evidence - no direct corpus support for fine-tuning effectiveness, but related work on code generation exists.
- Break condition: If the dataset is too small or lacks diversity, or if fine-tuning doesn't generalize beyond the training distribution.

### Mechanism 3
- Claim: Integrating CLIP embeddings into LLaMA (creating CLiMA) improves text-image alignment in generated figures.
- Mechanism: CLIP's multimodal embeddings provide visual understanding of captions, allowing the model to better align generated TikZ code with the semantic content of the description.
- Core assumption: CLIP's visual-semantic embeddings capture relevant visual concepts that can guide TikZ generation.
- Evidence anchors:
  - [abstract]: "We further develop CLiMA, a variant of LLaMA augmented with multimodal CLIP embeddings... This enhancement allows CLiMA to visually interpret input captions, thereby improving text-image alignment."
  - [section 4.1]: "we utilize the multimodal projection layer of CLIP, enabling us to extract visual information from both text and images within a common embedding space"
  - [corpus]: Weak evidence - no direct corpus support for CLIP integration effectiveness, but related work on multimodal models exists.
- Break condition: If CLIP embeddings don't provide meaningful visual information for the TikZ generation task, or if the integration degrades text-only performance.

## Foundational Learning

- Concept: Language modeling with code generation
  - Why needed here: The system generates TikZ code (a programming language) from natural language captions, requiring understanding of both programming syntax and scientific figure semantics.
  - Quick check question: What are the key differences between generating TikZ code versus natural language text?

- Concept: Multimodal embeddings and cross-modal alignment
  - Why needed here: CLiMA uses CLIP embeddings to bridge text and visual understanding, requiring knowledge of how multimodal models align different data types.
  - Quick check question: How do CLIP embeddings capture visual-semantic relationships that can guide code generation?

- Concept: Dataset construction and augmentation
  - Why needed here: The DaTikZ dataset was created from multiple sources with augmentation techniques, requiring understanding of data collection and quality improvement methods.
  - Quick check question: What are the trade-offs between using real versus synthetic data for training a code generation model?

## Architecture Onboarding

- Component map: Natural language caption -> CLIP (for CLiMA) -> LLaMA backbone -> TikZ code -> Vector graphics compilation

- Critical path:
  1. Caption processing through CLIP (for CLiMA) or directly to LLaMA
  2. TikZ code generation through LLaMA
  3. Compilation to vector graphics
  4. Evaluation against human-created figures

- Design tradeoffs:
  - Using TikZ vs. lower-level SVG: Higher abstraction enables better language modeling but requires compilation step
  - CLIP integration vs. text-only: Better visual alignment but increased complexity and computational cost
  - Fine-tuning vs. zero-shot: Better performance but requires dataset and training resources

- Failure signatures:
  - Compilation errors in generated TikZ code
  - Low CLIPScore or human evaluation scores
  - Memorized outputs rather than novel generation
  - Caption copying (typographic attacks)

- First 3 experiments:
  1. Generate TikZ code from simple captions and verify compilation success
  2. Compare text-only LLaMA vs. CLiMA performance on caption-image alignment
  3. Test generalization by evaluating on captions from outside the training distribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the integration of CLIP embeddings into LLaMA specifically improve text-image alignment in the generated TikZ drawings?
- Basis in paper: Explicit
- Why unresolved: While the paper mentions that CLiMA improves text-image alignment, it does not provide a detailed analysis of the mechanisms by which CLIP embeddings enhance this alignment. Understanding the specific contributions of CLIP embeddings could lead to further improvements in the model's performance.
- What evidence would resolve it: A detailed ablation study comparing the performance of CLiMA with and without CLIP embeddings on various text-image alignment metrics would provide insights into the specific benefits of CLIP integration.

### Open Question 2
- Question: To what extent do the generated TikZ drawings generalize to unseen scientific concepts, and how does this generalization compare to human-created figures?
- Basis in paper: Inferred
- Why unresolved: The paper demonstrates that the models generalize well and are not susceptible to memorization. However, it does not explicitly compare the generalization capabilities of the models to human-created figures or provide a detailed analysis of the types of scientific concepts the models can generate.
- What evidence would resolve it: A comprehensive evaluation of the models' ability to generate TikZ drawings for a diverse set of scientific concepts, including those not present in the training data, would provide insights into their generalization capabilities and how they compare to human performance.

### Open Question 3
- Question: How do the text-to-image generation capabilities of GPT-4 and Claude 2 differ from those of the fine-tuned LLaMA and CLiMA models, and what specific limitations do they exhibit?
- Basis in paper: Explicit
- Why unresolved: The paper highlights that GPT-4 and Claude 2 tend to generate simpler figures and sometimes produce degenerate solutions by copying input captions. However, it does not provide a detailed analysis of the specific limitations of these models or how their text-to-image generation capabilities differ from those of the fine-tuned models.
- What evidence would resolve it: A comparative study examining the strengths and weaknesses of GPT-4, Claude 2, LLaMA, and CLiMA in generating TikZ drawings for various scientific concepts would provide insights into the specific limitations of each model and how they differ in their text-to-image generation capabilities.

## Limitations

- Commercial models (GPT-4, Claude 2) underperform on scientific figure generation despite strong general capabilities, indicating specialized training is necessary
- CLIPScore evaluation may not fully capture semantic accuracy of scientific figures where precise geometric relationships matter
- CrystalBLEU metric for caption similarity could penalize creative but accurate visualizations since figures often require visual elements that cannot be perfectly described in text
- Dataset construction using artificial examples from GPT-4 could introduce bias or distributional shift
- Iterative resampling method for compilation errors may create selection bias in training data
- Evaluation focuses on similarity to human examples rather than functional utility for scientific communication

## Confidence

**High confidence**: TikZ as intermediate representation enables effective language modeling - well-supported by comparison with direct raster generation and success in code generation domains.

**Medium confidence**: CLiMA's CLIP integration improves text-image alignment - automatic metrics show improvement, but practical significance for scientific communication remains unclear and human evaluation focuses on similarity rather than utility.

**Medium confidence**: Generalization claims - analysis suggests no memorization, but evaluation primarily tests within-distribution generalization rather than true out-of-distribution performance on novel scientific concepts.

## Next Checks

1. **Functional validation**: Test whether generated figures are actually usable in scientific publications by having domain experts evaluate their publication readiness and scientific accuracy, not just their similarity to human examples.

2. **Cross-domain generalization**: Evaluate model performance on scientific domains completely absent from the training data (e.g., biology figures if trained primarily on physics/chemistry) to assess true generalization capability.

3. **Ablation study on data sources**: Systematically evaluate the contribution of each data source (curated examples, Stack Exchange, ArXiv, artificial examples) to identify whether the model is learning from high-quality scientific figures or relying heavily on lower-quality sources.