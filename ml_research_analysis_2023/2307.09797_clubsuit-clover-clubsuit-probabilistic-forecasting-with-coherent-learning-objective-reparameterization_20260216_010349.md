---
ver: rpa2
title: '$\clubsuit$ CLOVER $\clubsuit$: Probabilistic Forecasting with Coherent Learning
  Objective Reparameterization'
arxiv_id: '2307.09797'
source_url: https://arxiv.org/abs/2307.09797
tags:
- distribution
- forecasting
- forecasts
- base-level
- level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CLOVER, a probabilistic forecasting method
  that enforces coherence by construction for hierarchical time series. The core idea
  leverages a factor model structure, where base-level series are conditionally independent
  given latent factors.
---

# $\clubsuit$ CLOVER $\clubsuit$: Probabilistic Forecasting with Coherent Learning Objective Reparameterization

## Quick Facts
- arXiv ID: 2307.09797
- Source URL: https://arxiv.org/abs/2307.09797
- Authors: 
- Reference count: 40
- The paper introduces CLOVER, a probabilistic forecasting method that enforces coherence by construction for hierarchical time series.

## Executive Summary
This paper presents CLOVER, a novel approach to hierarchical probabilistic forecasting that enforces coherence by construction through a factor model structure. The method leverages exchangeability assumptions among base-level series and uses differentiable sampling to optimize sample-based loss functions like CRPS. CLOVER achieves significant improvements over state-of-the-art coherent forecasting methods, with average gains of 15% in CRPS accuracy across six datasets.

## Method Summary
CLOVER models hierarchical time series using a factor model structure where base-level series are conditionally independent given latent factors. A neural network outputs parameters for factor and base-level distributions, enabling differentiable sampling through reparameterization tricks. The model ensures coherence by construction since aggregates are linear combinations of base-level series. Training optimizes sample-based loss functions (CRPS, quantile loss) using generated samples, and the method provides a more compact forecast representation than previous approaches.

## Key Results
- Achieves average CRPS accuracy gains of 15% compared to state-of-the-art coherent forecasting methods
- Successfully handles six different datasets with varying hierarchical structures
- Provides more compact forecast representations than previous approaches
- Maintains coherence by construction through the factor model structure

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The factor model enforces coherence by construction because the base-level series are conditionally independent given the latent factors, and the aggregates are linear combinations of these series.
- Mechanism: By modeling the base-level forecasts as independent conditional on shared factors, and then aggregating these base-level forecasts, the resulting hierarchical forecasts are guaranteed to sum correctly across levels.
- Core assumption: Base-level series are exchangeable and can be modeled as conditionally independent given latent factors.
- Evidence anchors:
  - [abstract] "The core idea leverages a factor model structure, where base-level series are conditionally independent given latent factors."
  - [section] "Our model explicitly leverages exchangeability of the base-level targets using a factor model structure."
  - [corpus] Weak evidence; no direct mention of exchangeability or conditional independence in neighbor papers.
- Break condition: If base-level series are not exchangeable or share complex dependencies not captured by the factor structure, the model's coherence guarantee may not hold.

### Mechanism 2
- Claim: Differentiable sampling enables end-to-end optimization of arbitrary sample-based loss functions.
- Mechanism: The model outputs distribution parameters for factors and base-level series, which are then used with reparameterization tricks to generate differentiable samples. These samples can be used to compute losses like CRPS or quantile loss, and gradients flow back through the sampling process to optimize the model.
- Core assumption: The chosen distributions (e.g., Gamma, Normal) support differentiable sampling via reparameterization tricks.
- Evidence anchors:
  - [abstract] "The model parameters are outputs of a neural network, and differentiable sampling from the factor model enables optimization of arbitrary sample-based losses like CRPS and quantile loss."
  - [section] "Our method exploits these results: if we can compute differentiable samples from the factor distributions and from the base-level distributions, we can compute differentiable samples for our forecasts, at any level of aggregation."
  - [corpus] Weak evidence; no direct mention of differentiable sampling or reparameterization tricks in neighbor papers.
- Break condition: If the chosen distributions do not support differentiable sampling, or if the reparameterization trick is not applicable, end-to-end optimization becomes infeasible.

### Mechanism 3
- Claim: The neural network structure allows learning complex patterns in the data while maintaining the factor model structure.
- Mechanism: The neural network takes historical, static, and future features as input and outputs the parameters for the factor distributions, loadings, and base-level distributions. This allows the model to capture complex temporal and feature dependencies while still enforcing the factor model structure for coherence.
- Core assumption: The neural network architecture (e.g., MQCNN) is expressive enough to capture the relevant patterns in the data.
- Evidence anchors:
  - [abstract] "The model parameters are outputs of a neural network, and differentiable sampling from the factor model enables optimization of arbitrary sample-based losses like CRPS and quantile loss."
  - [section] "Let R be total number of samples we want to produce during the training process. For time t ∈ [T ], item u ∈ [U ], r ∈ [R], let the realized parameter-free noise η(l)t,u,r = ( η(l)t,u,r,1, · · · , η(l)t,u,r,K), where all η(l)s are i.i.d. sampled from ˜f (l)."
  - [corpus] Weak evidence; no direct mention of neural network structure or MQCNN in neighbor papers.
- Break condition: If the neural network architecture is not expressive enough to capture the relevant patterns in the data, the model's accuracy will suffer despite the coherence guarantee.

## Foundational Learning

- Concept: Exchangeability of random variables
  - Why needed here: The factor model structure relies on the assumption that base-level series are exchangeable, meaning their joint distribution is invariant to permutations. This allows modeling them as conditionally independent given latent factors.
  - Quick check question: What does it mean for a set of random variables to be exchangeable, and why is this property important for the factor model structure?

- Concept: Reparameterization trick for differentiable sampling
  - Why needed here: Differentiable sampling is crucial for end-to-end optimization of sample-based loss functions. The reparameterization trick allows generating samples from a distribution in a way that is differentiable with respect to the distribution parameters.
  - Quick check question: How does the reparameterization trick enable differentiable sampling from a distribution, and why is this important for optimizing sample-based loss functions?

- Concept: Factor models and their role in hierarchical forecasting
  - Why needed here: The factor model structure is the key to enforcing coherence by construction in hierarchical forecasting. Understanding how factors influence base-level series and how aggregates are formed from these series is essential for grasping the model's mechanism.
  - Quick check question: How does a factor model structure ensure coherence in hierarchical forecasting, and what are the key components of such a model?

## Architecture Onboarding

- Component map: Input features -> Encoding -> Decoding -> Sampling -> Aggregation -> Loss
- Critical path: Input features → Encoding → Decoding → Sampling → Aggregation → Loss
- Design tradeoffs:
  - Choice of factor and base-level distributions: Affects the model's expressiveness and computational efficiency
  - Number of factors: Balances model complexity and the ability to capture complex dependencies
  - Neural network architecture: Impacts the model's ability to learn relevant patterns in the data
- Failure signatures:
  - Poor accuracy: May indicate that the chosen distributions or neural network architecture are not well-suited to the data
  - Incoherent forecasts: Suggests a bug in the implementation of the factor model or aggregation logic
  - Slow convergence: Could be due to suboptimal learning rate or network architecture
- First 3 experiments:
  1. Train the model on a small, simple dataset (e.g., Traffic) with a single factor and Normal base distribution to verify the basic mechanism.
  2. Experiment with different base distributions (e.g., Clipped Normal, Truncated Normal, Log-Normal) on the Favorita dataset to understand their impact on accuracy.
  3. Vary the number of factors on the Traffic dataset to find the optimal balance between model complexity and accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of base-level distribution (e.g., Gamma vs. Truncated Normal vs. Clipped Normal) impact the model's accuracy across different datasets?
- Basis in paper: [explicit] The paper explicitly compares the performance of different base-level distributions (Gamma, Log-Normal, Truncated Normal, and Clipped Normal) on three datasets and finds that Clipped Normal performs best overall.
- Why unresolved: The paper only tests a limited set of distributions and does not explore other potential distributions or their impact on accuracy.
- What evidence would resolve it: A comprehensive comparison of various base-level distributions on a wider range of datasets would help determine the optimal choice for different scenarios.

### Open Question 2
- Question: What is the optimal number of factors for achieving the best forecast accuracy, and how does it vary across different datasets and hierarchical structures?
- Basis in paper: [explicit] The paper discusses the impact of the number of factors on accuracy and provides an example using the Traffic dataset, but does not explore the optimal number of factors for other datasets.
- Why unresolved: The optimal number of factors likely depends on the specific characteristics of the dataset and the hierarchical structure, which the paper does not fully investigate.
- What evidence would resolve it: A systematic analysis of the impact of the number of factors on accuracy across different datasets and hierarchical structures would help determine the optimal number of factors for each scenario.

### Open Question 3
- Question: How does the model's performance scale with the size of the hierarchy and the number of base-level series?
- Basis in paper: [inferred] The paper mentions that the model requires data for all base-level series to be fitted on a single GPU, which may be prohibitive for large-scale hierarchies.
- Why unresolved: The paper does not explore the model's performance on very large hierarchies or with a large number of base-level series.
- What evidence would resolve it: Evaluating the model's performance on datasets with varying sizes of hierarchies and base-level series would help determine its scalability limitations and potential solutions.

## Limitations

- The model assumes base-level series are exchangeable, which may not hold in all hierarchical structures
- The choice of distributions (Gamma, Normal) is not thoroughly explored across different data types
- Limited ablation studies on the neural network architecture's impact on performance

## Confidence

- **High confidence** in the coherence guarantee mechanism (Mechanism 1) - the factor model structure logically ensures coherent aggregation by construction
- **Medium confidence** in the differentiable sampling approach (Mechanism 2) - while theoretically sound, implementation details could affect practical performance
- **Medium confidence** in the neural network's ability to capture complex patterns (Mechanism 3) - the paper shows good results but doesn't thoroughly explore architecture sensitivity

## Next Checks

1. Test the model on hierarchical structures where base-level series exhibit strong non-exchangeability to evaluate robustness
2. Compare different base distribution choices (Normal, Gamma, Log-Normal) across datasets to determine optimal selection criteria
3. Conduct sensitivity analysis on the number of factors and neural network architecture to identify optimal configurations for different data characteristics