---
ver: rpa2
title: 'Multilingual estimation of political-party positioning: From label aggregation
  to long-input Transformers'
arxiv_id: '2310.12575'
source_url: https://arxiv.org/abs/2310.12575
tags:
- rile
- political
- right
- labels
- setting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses the challenge of multilingual scaling analysis
  of political-party positioning, specifically the automatic estimation of the RILE
  (Right-Left) score for party manifestos. The authors compare two approaches: label
  aggregation, which relies on predicting MARPOR categories for individual statements
  and then aggregating them, and direct prediction using long-input Transformer models
  (BigBird and Longformer) that predict RILE scores directly from raw text.'
---

# Multilingual estimation of political-party positioning: From label aggregation to long-input Transformers

## Quick Facts
- arXiv ID: 2310.12575
- Source URL: https://arxiv.org/abs/2310.12575
- Reference count: 40
- Key outcome: Label aggregation using multilingual encoders achieves best RILE prediction (0.73 X-COUNTRY, 0.9 X-TIME), outperforming direct LIT prediction and MT+monolingual approaches.

## Executive Summary
This work addresses the challenge of multilingual scaling analysis of political-party positioning, specifically the automatic estimation of the RILE (Right-Left) score for party manifestos. The authors compare two approaches: label aggregation, which relies on predicting MARPOR categories for individual statements and then aggregating them, and direct prediction using long-input Transformer models (BigBird and Longformer) that predict RILE scores directly from raw text. Experiments are conducted on the MARPOR dataset across 41 countries and 27 languages, using both leave-one-country-out (X-COUNTRY) and old-vs.-new (X-TIME) settings. The label aggregation approach, particularly when using a multilingual encoder, achieves the best results, with Spearman correlations of 0.73 (X-COUNTRY) and 0.9 (X-TIME) for RILE score prediction. While long-input Transformers show promise, their performance is currently lower, with the best model (BigBird) achieving 0.55 (X-COUNTRY) and 0.71 (X-TIME).

## Method Summary
The paper proposes two approaches for multilingual RILE estimation: label aggregation and direct prediction. Label aggregation involves training a sentence-level classifier on MARPOR categories, then aggregating predictions per manifesto to compute RILE scores. Direct prediction uses long-input Transformers (BigBird, Longformer) to predict RILE scores from raw text chunks. The MARPOR dataset is filtered to include only post-2000 manifestos in 27 supported languages. Experiments use leave-one-country-out (X-COUNTRY) and old-vs.-new (X-TIME) settings, with evaluation based on Spearman correlation between predicted and gold RILE scores.

## Key Results
- Label aggregation using a multilingual encoder (XLM-RoBERTa) achieves the highest RILE prediction accuracy (0.73 X-COUNTRY, 0.9 X-TIME).
- Long-input Transformers (BigBird, Longformer) show promise but underperform label aggregation (0.55 X-COUNTRY, 0.71 X-TIME for BigBird).
- Coarse-grained (3-way) label aggregation outperforms fine-grained (143-way) classification for RILE estimation, despite lower classification accuracy.
- Multilingual encoder outperforms MT+monolingual encoder combination in cross-country RILE prediction.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual label aggregation via multilingual encoders outperforms MT + monolingual encoders in cross-country RILE prediction.
- Mechanism: Multilingual encoder (XLM-RoBERTa) captures shared political semantics across languages, reducing noise from MT errors.
- Core assumption: Political stance signals are sufficiently language-agnostic to be represented in a shared multilingual space.
- Evidence anchors:
  - [abstract]: "label aggregation producing the best results" and multilingual encoder performance > MT in X-COUNTRY.
  - [section 4.2]: "using the multilingual encoder... produces the best results" for both X-COUNTRY and X-TIME.
  - [corpus]: No direct support; assumption about shared semantics not validated by corpus neighbors.
- Break condition: If political discourse is highly culturally/language-specific, shared multilingual representation will fail.

### Mechanism 2
- Claim: Aggregating coarse-grained (3-way) labels instead of fine-grained (143-way) labels yields better RILE estimation despite lower classification accuracy.
- Mechanism: Aggregation reduces label noise; errors in fine-grained classification are uncorrelated and cancel out in coarse aggregation.
- Core assumption: Classifier mistakes are random and not systematically biased toward a particular political direction.
- Evidence anchors:
  - [section 4.3.2]: "low accuracy... does not translate into low quality" and confusion matrix shows substitutions within same coarse category.
  - [section 4.2]: "Aggregating labels... does not improve the eventual RILE scores" but coarse aggregation beats fine-grained in X-COUNTRY.
  - [corpus]: No direct support; corpus neighbors focus on segmentation and decomposition, not coarse vs. fine aggregation.
- Break condition: If classifier systematically biases certain labels, aggregation will propagate errors.

### Mechanism 3
- Claim: Direct RILE prediction with long-input Transformers is feasible but underperforms label aggregation in cross-country setting due to insufficient training data.
- Mechanism: LIT models can process full manifestos in chunks, bypassing statement segmentation; regression to mean limits dispersion in predictions.
- Core assumption: LIT models can learn the relationship between text and RILE without explicit label supervision.
- Evidence anchors:
  - [section 3.1]: LIT input limit of 4096 tokens allows chunk-level RILE estimation.
  - [section 4.2]: "BigBird's predictions show a non-negligible correlation (0.55)" but "much worse than the label aggregation results."
  - [section 4.3.1]: "predicted values cluster closer to the mean" indicating regression to mean.
  - [corpus]: No direct support; corpus neighbors do not address LIT-based RILE prediction.
- Break condition: If training data is insufficient or text-RILE relationship is too complex, LIT regression will fail.

## Foundational Learning

- Concept: Political scaling and RILE score computation
  - Why needed here: Understanding how MARPOR categories map to RILE is essential for evaluating label aggregation vs. direct prediction.
  - Quick check question: How is the RILE score computed from left-wing, right-wing, and other category counts?

- Concept: Multilingual NLP and cross-lingual transfer
  - Why needed here: The paper compares multilingual encoders vs. MT for handling 27 languages; knowing strengths/weaknesses is key.
  - Quick check question: What are the main advantages and risks of using a multilingual encoder vs. MT for cross-lingual political text analysis?

- Concept: Long-input Transformer architectures (BigBird, Longformer)
  - Why needed here: These models enable direct RILE prediction from raw text; understanding their limitations is crucial.
  - Quick check question: What is the main architectural innovation that allows BigBird/Longformer to handle inputs longer than 512 tokens?

## Architecture Onboarding

- Component map:
  - Data pipeline: MARPOR manifestos → sentence-level labels → RILE score computation
  - Model options:
    - Label aggregation: sentence classifier (XLM-RoBERTa or MT+MPNet) → 3-way label prediction → aggregate → RILE
    - Direct prediction: LIT (BigBird/Longformer) → chunk-level RILE regression or 5-way stance classification → manifesto-level average
  - Evaluation: Spearman correlation with gold RILE; error analysis (absolute error, sign flips)

- Critical path:
  1. Preprocess manifestos (MT or use raw multilingual text)
  2. For label aggregation: sentence classification → aggregation → RILE
  3. For direct prediction: chunk the text → LIT prediction → average → RILE
  4. Evaluate on X-COUNTRY and X-TIME splits

- Design tradeoffs:
  - Label aggregation: requires statement segmentation (gold or heuristic), but leverages strong supervision; coarse aggregation reduces noise
  - Direct prediction: avoids segmentation, but needs large training data; regression to mean is a risk
  - Multilingual encoder: no preprocessing, but may miss language-specific cues; MT: easier for low-resource languages, but adds noise

- Failure signatures:
  - Low Spearman correlation despite high classification accuracy (label aggregation)
  - RILE predictions clustered near zero (LIT regression to mean)
  - Systematic sign flips in cross-country setting (bias in classifier)

- First 3 experiments:
  1. Train XLM-RoBERTa 3-way classifier on X-COUNTRY; evaluate sentence accuracy and manifesto RILE correlation
  2. Fine-tune BigBird for chunk-level RILE regression; compare performance to label aggregation baseline
  3. Implement 5-way stance classification with BigBird; analyze confusion matrix for extreme categories

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would multilingual encoder performance surpass machine translation when using larger, task-specific NMT models or when targeting languages with higher translation quality?
- Basis in paper: [explicit] "A cursory inspection of the translated sentences shows that the translation quality does vary across languages" and MT "still rather noisy, especially for non-WEIRD (Henrich et al., 2010) languages, which offsets the benefits of a stronger base model."
- Why unresolved: The paper only used off-the-shelf NMT systems and did not explore the potential of larger, specialized translation models or focus on higher-resource languages.
- What evidence would resolve it: Direct comparison using specialized NMT models for low-resource languages and evaluating performance on higher-resource languages.

### Open Question 2
- Question: How would the performance of long-input Transformers change with increased training data size or more advanced architectures specifically designed for political text analysis?
- Basis in paper: [inferred] "BigBird is especially affected by this... The training dataset was big enough to correctly estimate the mean of the distribution but not big enough to approximate the correct dispersion."
- Why unresolved: The paper suggests that training data limitations affect performance, but did not experiment with larger datasets or test more advanced long-input Transformer architectures.
- What evidence would resolve it: Performance evaluation using larger training datasets and newer long-input Transformer models specifically fine-tuned for political text analysis.

### Open Question 3
- Question: Would incorporating hierarchical label information or contextual features (e.g., party positions, country-specific political contexts) improve the accuracy of MARPOR category prediction?
- Basis in paper: [explicit] "Facing the same issues of label-frequency imbalance and rare labels, they mitigate them to some degree by using the hierarchical organisation of MARPOR labels"
- Why unresolved: While the paper acknowledges the label imbalance problem, it did not explore hierarchical label structures or contextual features that could help distinguish rare categories.
- What evidence would resolve it: Experiments incorporating hierarchical label structures and contextual features, with performance comparisons to the baseline model.

## Limitations
- Reliance on a single dataset (MARPOR) may not fully represent the diversity of political discourse across all 27 languages and 41 countries.
- Label aggregation's superiority assumes that political stance signals are sufficiently language-agnostic, which may not hold for culturally distinct political systems.
- Direct prediction approach's observed regression to mean suggests the model struggles to capture the full range of political positions, potentially limiting real-world applicability.

## Confidence
- High confidence: The superiority of label aggregation over direct prediction in the cross-country setting, supported by multiple experiments and error analysis.
- Medium confidence: The advantage of multilingual encoders over MT+monolingual encoders, as this is demonstrated but not extensively explored across different language pairs.
- Low confidence: The claim that coarse-grained aggregation beats fine-grained aggregation for RILE estimation, as this is only shown for the X-COUNTRY setting and may be dataset-specific.

## Next Checks
1. **Cross-linguistic robustness test**: Evaluate the multilingual encoder approach on manifestos from linguistically distant language families (e.g., Germanic vs. Slavic) to verify the assumption of shared political semantics.

2. **Data augmentation experiment**: Increase training data for the LIT models by including synthetic manifestos or leveraging multilingual corpora to test if performance improves beyond regression to mean.

3. **Fine-grained error analysis**: Conduct a systematic error analysis comparing label aggregation vs. direct prediction, focusing on manifestos from countries with unique political systems to identify potential cultural/language-specific biases.