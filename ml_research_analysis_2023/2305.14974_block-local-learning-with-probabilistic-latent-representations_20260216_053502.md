---
ver: rpa2
title: Block-local learning with probabilistic latent representations
arxiv_id: '2305.14974'
source_url: https://arxiv.org/abs/2305.14974
tags:
- network
- learning
- local
- training
- used
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new method called block-local learning
  (BLL) that addresses the locking and weight transport problems in deep neural networks
  during training. BLL splits a deep network into blocks and uses a twin backward
  network to propagate information from targets backwards, providing auxiliary local
  losses.
---

# Block-local learning with probabilistic latent representations

## Quick Facts
- arXiv ID: 2305.14974
- Source URL: https://arxiv.org/abs/2305.14974
- Authors:
- Reference count: 25
- Primary result: A new method called block-local learning (BLL) that addresses the locking and weight transport problems in deep neural networks during training.

## Executive Summary
This paper introduces block-local learning (BLL), a method that splits deep neural networks into blocks and uses a twin backward network to propagate information from targets backwards, providing auxiliary local losses. BLL treats network activations as parameters of probability distributions, enabling parallel forward and backward propagation with different weight sets. This addresses the locking and weight transport problems in traditional backpropagation while maintaining competitive performance on vision and sequence learning tasks.

## Method Summary
BLL divides a deep network into N blocks and introduces a feedback network that propagates information from the targets backwards to provide auxiliary local losses. Each block outputs natural parameters of an exponential family distribution for its latent representation. A backward twin network propagates messages from the targets using a similar distribution. Posterior messages combine forward and backward information, and local losses are computed from the KL divergence between these distributions. Forward and backward propagation can operate in parallel and with different sets of weights, addressing the problems of locking and weight transport. The method derives from a statistical interpretation of end-to-end training which treats activations of network layers as parameters of probability distributions.

## Key Results
- Block-local learning achieves competitive performance to backpropagation while avoiding locking and weight transport issues
- The method provides principled uncertainty estimates through the probabilistic interpretation of activations
- Experiments demonstrate state-of-the-art performance on vision and sequence learning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Block-local learning splits a deep network into forward and backward twin blocks that propagate probabilistic messages in parallel, eliminating locking and weight transport problems.
- Mechanism: The network is divided into N blocks. Each block k outputs natural parameters ϕk of an exponential family distribution for its latent representation zk. A backward twin network propagates βk messages from the targets using a similar distribution. Posterior messages ρk combine forward and backward information via ρk ∝ αk · βk. Local losses L(k)V are computed from the KL divergence between ρk and αk, allowing independent gradient updates within each block.
- Core assumption: Forward and backward networks can be trained independently because their outputs are treated as parameters of probability distributions rather than deterministic values, enabling local computation of matching losses.
- Evidence anchors:
  - [abstract] "Our method works by dividing a deep neural network into blocks and introduces a feedback network that propagates the information from the targets backwards to provide auxiliary local losses."
  - [section] "Forward and backward propagation can operate in parallel and with different sets of weights, addressing the problems of locking and weight transport."
  - [corpus] Weak. No direct evidence found in neighbor papers about parallel probabilistic message passing.
- Break condition: If the forward and backward distributions cannot be matched well (e.g., poor uncertainty estimation), local losses become uninformative and learning degrades.

### Mechanism 2
- Claim: The forward network's intermediate activations are interpreted as parameters of probability distributions, allowing the model to produce uncertainty estimates without explicit sampling.
- Mechanism: Each layer's output encodes natural parameters (e.g., means and variances for Gaussian, or log-odds for Bernoulli) of an implicit random variable zk. During training, the KL divergence between the forward distribution αk and posterior ρk acts as a local loss. This naturally yields uncertainty information in the form of posterior variance, as shown in the autoencoder example where variance channels predicted reconstruction uncertainty.
- Core assumption: The network's internal representations can be validly interpreted as parameters of well-behaved probability distributions (exponential family), making the probabilistic framework applicable.
- Evidence anchors:
  - [abstract] "Our approach derives from a statistical interpretation of end-to-end training which treats activations of network layers as parameters of probability distributions."
  - [section] "We have demonstrated a general purpose probabilistic framework for rigorously defining block-local losses for deep architectures."
  - [corpus] Weak. No neighbor papers discuss probabilistic interpretation of activations as uncertainty signals.
- Break condition: If the assumed exponential family form is a poor fit for the actual data distribution, the uncertainty estimates become unreliable.

### Mechanism 3
- Claim: Block-local learning achieves competitive performance to backpropagation by approximating the global likelihood loss with a sum of local variational losses that form an upper bound.
- Mechanism: The global log-likelihood L = −log p(y|x) is bounded by L1 = −log p(y|x) + (1/N) Σk DKL(qk||pk). Minimizing L1 is equivalent to minimizing the sum of local KL losses L(k)V over all blocks. Each block's loss uses its own forward and backward messages, so gradients are computed locally without global backprop. The experiments show this bound is tight enough in practice for good accuracy.
- Core assumption: The variational posterior qk can be well-approximated by combining forward αk and backward βk messages, so minimizing local KLs approximates global likelihood minimization.
- Evidence anchors:
  - [abstract] "Error backpropagation is then performed locally within each block, leading to 'block-local' learning."
  - [section] "We show that the derived block local losses and the resulting block local learning (BLL) are a general form of various existing local losses and provide an upper bound to a global loss."
  - [corpus] Weak. No neighbor papers discuss variational bounds or local approximation of global loss.
- Break condition: If the number of blocks is too large or the variational approximation is poor, the upper bound becomes loose and performance drops significantly.

## Foundational Learning

- Concept: Markov property and conditional independence in deep networks
  - Why needed here: The paper relies on the chain of conditional independence x → z1 → z2 → ... → y to factorize the joint probability and derive local losses. Without this, the probabilistic interpretation collapses.
  - Quick check question: In a chain x → z1 → z2 → y, is p(y|z1,z2) equal to p(y|z2)? Why or why not?

- Concept: Exponential family distributions and natural parameters
  - Why needed here: The model assumes activations encode natural parameters of exponential family distributions, enabling closed-form KL divergences and efficient message passing. This is central to the mathematical derivation.
  - Quick check question: For a Bernoulli distribution, what are the natural parameter and sufficient statistic? How does this relate to the sigmoid function?

- Concept: Variational inference and ELBO
  - Why needed here: The paper uses the Evidence Lower Bound (ELBO) to justify replacing intractable posterior inference with tractable local losses. Understanding ELBO is key to seeing why the method works.
  - Quick check question: Why is the KL term in the ELBO non-negative, and what does it mean when it is zero?

## Architecture Onboarding

- Component map:
  - Forward network NA: Standard deep network split into N blocks. Each block k outputs parameters ϕk for distribution p(zk|zk-1).
  - Backward network NB: Twin network with same architecture as NA but in reverse order. Outputs parameters βk for p(y|zk).
  - Local loss layer: For each block k, compute ρk ∝ αk · βk and KL divergence DKL(ρk||αk).
  - Mixing schedule: Parameter m that scales βk influence in ρk, annealed over epochs.

- Critical path:
  1. Forward pass through NA: compute activations and ϕk for each block.
  2. Forward pass through NB: compute βk from targets.
  3. Compute ρk = S(ak + m·bk) for Bernoulli case.
  4. Compute local loss L(k)V and backpropagate within each block independently.
  5. Update weights in both NA and NB using local gradients.

- Design tradeoffs:
  - More blocks → better parallelization but weaker local approximations and more hyperparameters.
  - Mixing parameter m → balances forward/backward influence; too high causes instability, too low loses backward guidance.
  - Choice of exponential family (Bernoulli vs Gaussian) → affects output activation and uncertainty representation.

- Failure signatures:
  - Training loss plateaus early → likely poor matching between forward and backward distributions.
  - High variance in uncertainty predictions → mixing schedule or distribution assumption mismatch.
  - Performance much worse than end-to-end → local losses not tight enough approximation.

- First 3 experiments:
  1. Implement forward/backward twin networks on MNIST with 4 blocks, verify parallel execution and local loss computation.
  2. Add mixing schedule, train and plot training/test accuracy vs m schedule; observe impact on convergence.
  3. Replace Bernoulli with Gaussian assumption, measure uncertainty prediction quality on Fashion-MNIST reconstruction task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of block-local learning scale with increasing network depth and complexity compared to end-to-end backpropagation?
- Basis in paper: [inferred] The paper shows results for ResNet-18, ResNet-50, and a 20-block transformer, but does not systematically explore scaling to much deeper networks or different architectures.
- Why unresolved: The experiments presented are limited in scope and do not provide a comprehensive analysis of how block-local learning performs as networks become significantly larger and more complex.
- What evidence would resolve it: Extensive experiments comparing block-local learning to backpropagation across a wide range of network depths, architectures (e.g., transformers with hundreds of layers, very deep CNNs), and tasks would provide insight into scalability and limitations.

### Open Question 2
- Question: What is the impact of different block sizes and placement on the performance of block-local learning?
- Basis in paper: [inferred] The paper uses fixed block sizes for ResNet architectures and a single split for the transformer, but does not explore the effect of varying block sizes or placements.
- Why unresolved: The choice of block size and placement could significantly impact the trade-off between parallelization benefits and performance degradation, but this is not systematically investigated.
- What evidence would resolve it: Experiments systematically varying block sizes and placements across different network architectures and tasks would reveal optimal strategies and limitations.

### Open Question 3
- Question: How does block-local learning perform on tasks requiring fine-grained feature extraction or precise localization, compared to end-to-end backpropagation?
- Basis in paper: [inferred] The experiments focus on classification tasks, which may not fully capture the challenges of tasks requiring detailed spatial information.
- Why unresolved: The paper does not explore tasks that demand high precision in feature extraction or localization, leaving open the question of whether block-local learning can match backpropagation in such scenarios.
- What evidence would resolve it: Experiments on tasks like object detection, semantic segmentation, or fine-grained image classification would provide insights into block-local learning's performance on spatially demanding tasks.

## Limitations

- The mixing schedule introduces significant hyperparameters without clear guidance for selection.
- The experimental comparison against strong baselines like full end-to-end backpropagation is limited in scope.
- The method's robustness to misspecification of the exponential family assumption is unclear.

## Confidence

- **High confidence**: The mathematical framework for deriving local losses from probabilistic interpretation is rigorous and internally consistent
- **Medium confidence**: The claim that BLL achieves competitive performance to backpropagation is supported by experiments but needs broader validation across architectures
- **Low confidence**: The assertion that BLL completely eliminates weight transport issues needs more careful examination, as the backward network still requires some form of weight copying or initialization

## Next Checks

1. Test BLL on deeper architectures (ResNet-101, EfficientNet) and larger datasets (ImageNet) to verify scalability claims
2. Compare BLL against other local learning methods like feedback alignment and synthetic gradients on identical tasks
3. Analyze the effect of different exponential family choices (e.g., categorical, Laplacian) on uncertainty estimation quality and training stability