---
ver: rpa2
title: Investigating Uncertainty Calibration of Aligned Language Models under the
  Multiple-Choice Setting
arxiv_id: '2310.11732'
source_url: https://arxiv.org/abs/2310.11732
tags:
- format
- calibration
- aligned
- uncertainty
- pre-trained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the impact of the alignment process on
  the uncertainty calibration of language models (LMs) under the multiple-choice setting.
  The authors conduct a thoughtful empirical study and find that aligned LMs are inherently
  overconfident with altered predictive distributions on both answer decision and
  response format.
---

# Investigating Uncertainty Calibration of Aligned Language Models under the Multiple-Choice Setting

## Quick Facts
- arXiv ID: 2310.11732
- Source URL: https://arxiv.org/abs/2310.11732
- Reference count: 40
- Primary result: Aligned LMs are inherently overconfident; proposed post-hoc calibration method outperforms out-of-the-box calibration

## Executive Summary
This paper investigates how alignment processes (SFT, LPF) affect the uncertainty calibration of language models under the multiple-choice setting. The authors find that aligned LMs are overconfident and propose a novel decomposition of uncertainty into answer and format uncertainty. They demonstrate that alignment processes conflate these two types of uncertainty, leading to miscalibration. The paper introduces a post-hoc calibration method that leverages the well-calibrated pre-trained LM's predictive distribution, which outperforms standard calibration approaches across seven diverse tasks.

## Method Summary
The paper evaluates pre-trained and aligned language models (Llama and Llama-2 families) on multiple-choice tasks using zero-shot and few-shot in-context learning. Tasks are formatted with choice letters and format identifiers to separate answer and format uncertainty. The authors propose a post-hoc calibration method using temperature scaling with the pre-trained LM's predictive distribution as a reference, trained on few-shot examples. Synthetic alignment schemes (Format, Choice, Mixed) are used to study the impact of different alignment objectives on calibration.

## Key Results
- Aligned LMs exhibit significantly higher ECE than pre-trained LMs under both zero-shot and few-shot settings
- The proposed post-hoc calibration method consistently outperforms standard temperature scaling and KDE baselines across all tasks
- Synthetic alignment experiments show that only the Mixed alignment scheme (conflating answer and format uncertainty) leads to significant miscalibration

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The alignment process distorts LM calibration by conflating two distinct uncertainties—answer uncertainty and format uncertainty.
- Mechanism: During SFT and LPF, the model simultaneously learns to prefer human-aligned formats and adjust answer distributions. Because these are optimized together, the model's confidence becomes tied to format preference rather than correctness, leading to overconfidence.
- Core assumption: Answer uncertainty and format uncertainty can be disentangled in the multiple-choice setting via choice letters and format identifiers.
- Evidence anchors: [abstract] "one reason for aligned LMs' overconfidence is the conflation of these two types of uncertainty"

### Mechanism 2
- Claim: Pre-trained LMs are well-calibrated in-context learners because ICL primarily adjusts format uncertainty while preserving answer uncertainty.
- Mechanism: In-context examples shift the model's format preference (e.g., starting with a choice letter) without altering its underlying confidence in the correctness of the answer. Thus, the model remains calibrated while adopting the desired response style.
- Core assumption: ICL examples can be constructed to isolate format preference changes without influencing answer probability distributions.
- Evidence anchors: [section 4.1] "the in-context examples SK in the multiple-choice setting operate primarily on format uncertainty"

### Mechanism 3
- Claim: Post-hoc calibration using the pre-trained LM's predictive distribution is effective because it directly models the shift in answer uncertainty rather than relying on accuracy in few-shot examples.
- Mechanism: By minimizing KL divergence between pre-trained and aligned LM predictive distributions (with temperature scaling), the method captures how alignment altered answer uncertainty. This avoids the noise and bias inherent in few-shot accuracy-based calibration.
- Core assumption: The pre-trained LM's distribution is a stable reference for the aligned LM's answer uncertainty.
- Evidence anchors: [abstract] "propose an easy-to-implement and sample-efficient method to calibrate aligned LMs by utilizing the well-calibrated pre-trained LMs"

## Foundational Learning

- Concept: Uncertainty calibration and Expected Calibration Error (ECE)
  - Why needed here: The paper's core contribution is diagnosing and correcting miscalibration in aligned LMs. Understanding calibration metrics is essential to interpret the experimental results.
  - Quick check question: What does an ECE of 0 imply about a model's predictions?

- Concept: Multiple-choice question formatting for LM evaluation
  - Why needed here: The paper evaluates calibration under a multiple-choice setting by formatting questions with choice letters and format identifiers. Knowing this setup is key to understanding the decomposition of uncertainties.
  - Quick check question: How does the format identifier "(“ in "(A)" help isolate format uncertainty?

- Concept: In-context learning (ICL) mechanics
  - Why needed here: The paper contrasts pre-trained and aligned LM behavior under ICL vs zero-shot settings. Understanding ICL is crucial for interpreting why pre-trained LMs stay calibrated while aligned LMs do not.
  - Quick check question: In ICL, what role do the demonstration examples play in the model's output formatting?

## Architecture Onboarding

- Component map: Pre-trained LM -> Alignment (SFT/LPF) -> Multiple-choice formatting layer -> Calibration evaluation -> Post-hoc calibrator (temperature scaling variants)
- Critical path:
  1. Load pre-trained LM
  2. Apply alignment (SFT/LPF) if aligned LM needed
  3. Format tasks as MCQs
  4. Generate predictions and extract logits
  5. Compute calibration metrics
  6. Apply post-hoc calibration if needed
  7. Re-evaluate calibration
- Design tradeoffs:
  - Using the format identifier to estimate format uncertainty vs directly modeling it—simpler but may be noisy
  - Few-shot post-hoc calibration vs full-dataset calibration—practical but less stable
  - Relying on pre-trained LM as a calibration reference vs learning from scratch—effective if pre-trained LM is well-calibrated
- Failure signatures:
  - High ECE that does not improve after ICL (aligned LM symptom)
  - Temperature scaling fails to improve calibration (few-shot method limitation)
  - Calibration-preserving alignment schemes do not improve accuracy (overfitting to synthetic schemes)
- First 3 experiments:
  1. Compare ECE of pre-trained vs aligned LM on MMLU under ZSL and ICL with choice format "(A)"
  2. Run the synthetic alignment schemes (Format, Choice, Mixed) on QQP and evaluate on MMLU
  3. Apply proposed TS with pre-trained LM distribution on Llama-2-Chat 70B across all tasks with 5-shot calibration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the alignment process impact the calibration of language models in open-ended question-answering settings, where it is unclear how to estimate answer uncertainty and format uncertainty?
- Basis in paper: [explicit] The paper discusses the impact of alignment processes on the calibration of language models under the multiple-choice setting and mentions that it is unclear how to estimate answer uncertainty and format uncertainty in open-ended question-answering settings.
- Why unresolved: The paper does not provide a clear answer on how to handle the estimation of answer and format uncertainty in open-ended question-answering settings, leaving this as an open question for future research.
- What evidence would resolve it: Evidence that demonstrates a method for estimating answer and format uncertainty in open-ended question-answering settings, or experimental results showing the impact of alignment processes on calibration in such settings, would help resolve this question.

### Open Question 2
- Question: How does the dialog wrapper affect the format preference and calibration of aligned language models in multiple-choice question answering?
- Basis in paper: [explicit] The paper mentions that the dialog wrapper affects the format preference and calibration of aligned language models in multiple-choice question answering, but does not provide a detailed analysis of the impact.
- Why unresolved: The paper does not provide a comprehensive analysis of how the dialog wrapper affects the format preference and calibration of aligned language models, leaving this as an open question for further investigation.
- What evidence would resolve it: Evidence that demonstrates the impact of the dialog wrapper on the format preference and calibration of aligned language models in multiple-choice question answering, such as experimental results comparing models with and without the dialog wrapper, would help resolve this question.

### Open Question 3
- Question: How does the prompt sensitivity of language models' calibration vary across different tasks and choice formats?
- Basis in paper: [explicit] The paper mentions that the prompt sensitivity of language models' calibration varies across tasks and choice formats, but does not provide a detailed analysis of the factors contributing to this variation.
- Why unresolved: The paper does not provide a comprehensive analysis of the factors contributing to the prompt sensitivity of language models' calibration across different tasks and choice formats, leaving this as an open question for further investigation.
- What evidence would resolve it: Evidence that demonstrates the factors contributing to the prompt sensitivity of language models' calibration across different tasks and choice formats, such as experimental results comparing models with different prompts and choice formats, would help resolve this question.

## Limitations

- Uncertainty conflation mechanism: The claim that alignment processes conflate answer and format uncertainty is primarily supported by synthetic experiments rather than real alignment data
- Pre-trained LM calibration assumption: The paper assumes pre-trained LMs are well-calibrated in-context learners, but this is not universally established across all tasks or model families
- Post-hoc calibration stability: The proposed method relies on few-shot calibration using the pre-trained LM as a reference, which may be sensitive to example selection

## Confidence

- Claim 1: Aligned LMs are inherently overconfident under multiple-choice settings. **Confidence: High**
- Claim 2: The proposed post-hoc calibration method outperforms out-of-the-box calibration. **Confidence: High**
- Claim 3: Alignment processes conflate answer and format uncertainty. **Confidence: Medium**

## Next Checks

1. Test the proposed calibration method on a held-out task not seen during few-shot calibration to assess generalization

2. Apply the synthetic alignment schemes (Format, Choice, Mixed) to a pre-trained model and verify that only the Mixed scheme induces significant miscalibration

3. Evaluate ECE of pre-trained Llama models on MMLU under ICL with choice format "(A)" across multiple random seeds to assess calibration stability