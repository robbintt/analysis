---
ver: rpa2
title: Diagnostic Reasoning Prompts Reveal the Potential for Large Language Model
  Interpretability in Medicine
arxiv_id: '2308.06834'
source_url: https://arxiv.org/abs/2308.06834
tags:
- patient
- reasoning
- diagnosis
- question
- clinical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed novel clinical reasoning prompts for large
  language models (LLMs) to improve interpretability of their diagnostic reasoning.
  GPT-3.5 and GPT-4 were evaluated on a modified MedQA USMLE dataset using traditional
  Chain-of-Thought prompting and four specialized diagnostic reasoning prompts (differential
  diagnosis, intuitive reasoning, analytic reasoning, and Bayesian inference).
---

# Diagnostic Reasoning Prompts Reveal the Potential for Large Language Model Interpretability in Medicine

## Quick Facts
- arXiv ID: 2308.06834
- Source URL: https://arxiv.org/abs/2308.06834
- Authors: 
- Reference count: 0
- GPT-4 maintained diagnostic accuracy while generating interpretable clinical reasoning rationales

## Executive Summary
This study develops and evaluates novel clinical reasoning prompts for large language models to improve interpretability of their diagnostic reasoning. GPT-3.5 and GPT-4 were tested on a modified MedQA USMLE dataset using traditional Chain-of-Thought prompting and four specialized diagnostic reasoning prompts. GPT-4 demonstrated similar performance across all prompting strategies (76-78% accuracy), while GPT-3.5 showed significant performance degradation with analytic reasoning, differential diagnosis, and Bayesian inference prompts compared to traditional prompting. The results suggest GPT-4 can mimic clinician cognitive processes without sacrificing diagnostic accuracy, offering potential for interpretable LLM use in medicine.

## Method Summary
The study evaluated GPT-3.5 and GPT-4 on a modified MedQA USMLE dataset using five prompting strategies: traditional Chain-of-Thought, differential diagnosis, intuitive reasoning, analytic reasoning, and Bayesian inference. Each prompt included two example questions with rationales using the target reasoning strategy (few-shot learning). Responses were evaluated by two blinded physician authors, with a third evaluator resolving disagreements. Accuracy was compared across prompting strategies using McNemar's test, with Cohen's Kappa measuring inter-rater agreement.

## Key Results
- GPT-4 maintained 76-78% accuracy across all prompting strategies
- GPT-3.5 performance dropped significantly with analytic reasoning (40%), differential diagnosis (38%), and Bayesian inference (42%) compared to traditional prompting (46%)
- GPT-4 achieved similar performance between traditional and diagnostic reasoning prompts, suggesting successful clinical reasoning without accuracy loss
- Intuitive reasoning was the only prompt that improved GPT-3.5 performance (+1.8%) compared to traditional CoT

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Specialized diagnostic reasoning prompts can elicit interpretable clinical reasoning in LLMs without sacrificing accuracy.
- Mechanism: The prompts structure the LLM's reasoning process to mirror clinical diagnostic workflows (differential diagnosis, intuitive associations, analytic pathophysiology, Bayesian updating), making the decision pathway explicit.
- Core assumption: LLMs possess latent clinical reasoning capabilities that can be activated through appropriate prompt engineering.
- Evidence anchors:
  - [abstract] "GPT-4 can be prompted to mimic the common clinical reasoning processes of clinicians without sacrificing diagnostic accuracy."
  - [section] "GPT-4 achieved an accuracy of 76% with traditional CoT, 77% with intuitive reasoning (+0.8%, CI -2.7% to +4.3%, P= 0.73), 78% with differential diagnosis (+2.2%, CI -1.4% to +5.8%, P= 0.24)"
  - [corpus] Weak - related papers focus on LLM evaluation but don't directly test prompt-specific interpretability claims
- Break condition: If prompts introduce noise or confusion, LLM performance degrades (as seen with GPT-3.5 on analytic, differential, and Bayesian prompts).

### Mechanism 2
- Claim: Chain-of-Thought prompting combined with clinical reasoning examples enables step-by-step justification of diagnoses.
- Mechanism: Few-shot learning with clinical reasoning examples teaches the LLM to decompose diagnostic tasks into interpretable reasoning steps rather than jumping directly to conclusions.
- Core assumption: LLMs can learn reasoning patterns from examples without explicit training on clinical reasoning.
- Evidence anchors:
  - [section] "Each prompt included two example questions (Table 1) with rationales employing the target reasoning strategy. This is a technique known as few-shot learning."
  - [section] "GPT-4 demonstrated similar performance between traditional and diagnostic reasoning prompts, suggesting it can be prompted to successfully perform clinical reasoning processes without sacrificing diagnostic accuracy."
  - [corpus] Weak - while related papers mention few-shot learning, none specifically demonstrate its use for clinical reasoning interpretability
- Break condition: If example quality is poor or reasoning patterns are not well-represented, few-shot learning fails to transfer.

### Mechanism 3
- Claim: Different diagnostic reasoning strategies have varying effectiveness depending on the LLM's reasoning capabilities.
- Mechanism: GPT-3.5's lower reasoning capacity means complex prompts (analytic, Bayesian) introduce confusion, while GPT-4's advanced capabilities allow it to benefit from or remain neutral to all strategies.
- Core assumption: LLM reasoning capabilities vary significantly between model versions and affect prompt strategy effectiveness.
- Evidence anchors:
  - [section] "GPT-3.5 performance was similar with traditional and intuitive reasoning CoT prompts, but significantly worse with differential diagnosis, analytical, and Bayesian inference CoT prompts."
  - [section] "GPT-4 demonstrated similar performance between traditional and diagnostic reasoning CoT prompts, suggesting it can be prompted to successfully perform clinical reasoning processes without sacrificing diagnostic accuracy."
  - [corpus] Weak - related papers evaluate LLM performance but don't systematically compare prompt effectiveness across model versions
- Break condition: If model architecture or training changes, the relationship between reasoning capabilities and prompt effectiveness may shift.

## Foundational Learning

- Concept: Clinical reasoning frameworks (differential diagnosis, intuitive associations, analytic pathophysiology, Bayesian inference)
  - Why needed here: The study evaluates LLM performance using these specific clinical reasoning strategies, so understanding them is essential for interpreting results and designing prompts
  - Quick check question: What are the four clinical reasoning strategies tested in this study, and how does each differ in approach to diagnosis?

- Concept: Chain-of-Thought prompting and few-shot learning
  - Why needed here: These prompting techniques are the foundation for eliciting interpretable reasoning from LLMs
  - Quick check question: How does Chain-of-Thought prompting differ from traditional prompting, and what role does few-shot learning play in this study?

- Concept: Statistical significance testing for paired proportions
  - Why needed here: The study uses McNemar's test to compare prompt performance, requiring understanding of appropriate statistical methods
  - Quick check question: Why is McNemar's test appropriate for comparing performance between different prompting strategies on the same test set?

## Architecture Onboarding

- Component map: Modified MedQA dataset -> Five prompting strategies -> LLM APIs (GPT-3.5, GPT-4) -> Response generation -> Physician evaluation -> Statistical analysis
- Critical path: Prompt → LLM API call → Response generation → Physician evaluation → Statistical analysis
- Design tradeoffs: The study chose free-response questions over multiple-choice to better assess reasoning, but this required physician grading and reduced automation. Using few-shot examples improves performance but increases prompt complexity and token usage.
- Failure signatures: Performance degradation with complex prompts (as seen with GPT-3.5), API errors causing missing data (21 GPT-4 errors), inter-rater disagreement in evaluation, or statistical non-significance when differences are expected.
- First 3 experiments:
  1. Test traditional CoT prompting on a small subset of MedQA questions to establish baseline performance and validate evaluation methodology
  2. Implement and test one diagnostic reasoning prompt (e.g., intuitive reasoning) on the same subset to compare performance and identify prompt issues
  3. Run all five prompting strategies on the full development set to optimize prompt wording and identify which strategies work best for which model version

## Open Questions the Paper Calls Out

- Can the clinical reasoning rationales generated by LLMs be systematically evaluated for logical consistency and absence of hallucinations?
- How do different prompt engineering strategies impact the interpretability and trustworthiness of LLM-generated clinical reasoning?
- Can LLM clinical reasoning capabilities be reliably transferred to different medical domains or clinical scenarios?

## Limitations

- Study relies on a single dataset (modified MedQA USMLE) and evaluation by a small team of physicians, limiting generalizability
- GPT-3.5's significant performance degradation with complex prompts highlights model-specific limitations
- Inconsistent evaluation framework between models due to lack of self-consistency for GPT-4 responses
- Only five prompting strategies tested, leaving open the possibility that other approaches might yield better results

## Confidence
- Confidence in the primary claims is Medium: while the results show GPT-4 can maintain accuracy with interpretable reasoning prompts, the performance differences between models and prompt types warrant further investigation.

## Next Checks
1. Test the same prompting strategies on additional clinical datasets to assess generalizability
2. Implement self-consistency for GPT-4 in future evaluations to ensure fair comparison
3. Conduct a systematic comparison of different prompt engineering approaches (beyond the five tested) to identify optimal strategies for clinical reasoning interpretability