---
ver: rpa2
title: 'Leveraging Human Feedback to Scale Educational Datasets: Combining Crowdworkers
  and Comparative Judgement'
arxiv_id: '2305.12894'
source_url: https://arxiv.org/abs/2305.12894
tags:
- comparative
- reliability
- were
- raters
- judgement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates using non-expert crowdworkers to evaluate
  complex educational data by comparing categorical versus comparative judgement methods.
  Crowdworkers assessed student responses to reading comprehension questions and oral
  reading fluency tasks.
---

# Leveraging Human Feedback to Scale Educational Datasets: Combining Crowdworkers and Comparative Judgement

## Quick Facts
- arXiv ID: 2305.12894
- Source URL: https://arxiv.org/abs/2305.12894
- Reference count: 0
- Primary result: Comparative judgement with crowdworkers improved inter-rater reliability from 0.66 to 0.80 for reading comprehension tasks

## Executive Summary
This study demonstrates that non-expert crowdworkers can reliably evaluate complex educational data when using comparative judgement rather than traditional categorical scoring. The research compared two evaluation methods for student responses to reading comprehension questions and oral reading fluency tasks, finding that comparative judgement significantly improved both inter-rater reliability and accuracy. The approach offers a scalable solution for creating large educational datasets while maintaining high data quality standards, addressing a key bottleneck in developing AI models for education.

## Method Summary
The study recruited 300 crowdworkers through Prolific to evaluate student responses to short-answer reading comprehension questions and oral reading fluency audio clips. Workers were randomly assigned to either categorical judgement (rating individual answers as correct/incorrect) or comparative judgement (comparing pairs of answers and selecting the better one). Inter-rater reliability was measured using Krippendorff's alpha with 99% confidence intervals, and accuracy was calculated for short-answer tasks against ground truth answers.

## Key Results
- Comparative judgement increased inter-rater reliability from 0.66 to 0.80 for short-answer questions
- Oral reading fluency reliability improved from 0.70 to 0.78 using comparative judgement
- Accuracy for short-answer tasks improved from 73% to 86% with comparative judgement
- Comparative judgement required no special expertise and maintained quality across different task types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Comparative judgement improves inter-rater reliability by reducing cognitive load and ambiguity in evaluation criteria
- Mechanism: Direct comparisons leverage natural comparative perception rather than attempting to quantify subjective qualities against fixed scales
- Core assumption: Human perceptual systems are better at relative comparisons than absolute measurements for complex qualities
- Evidence anchors: Comparative judgement "generally leads to high inter-rater reliability" by asking evaluators to make relative comparisons rather than scoring against predetermined standards

### Mechanism 2
- Claim: Crowdworkers can achieve expert-level reliability when evaluating educational data through comparative judgement
- Mechanism: The wisdom of crowds effect emerges through preference-based ranking, producing reliable consensus judgements even with non-expert evaluators
- Core assumption: Aggregated non-expert judgements can match individual expert performance when properly structured
- Evidence anchors: Accuracy for short-answer tasks improved from 73% to 86% using comparative judgement

### Mechanism 3
- Claim: The combination of crowdworkers and comparative judgement creates scalable, cost-effective educational data labelling
- Mechanism: This approach reduces per-rater training requirements while maintaining data quality, enabling rapid scaling of educational assessment datasets
- Core assumption: Quality can be maintained at scale through proper task design and quality control measures
- Evidence anchors: Crowdworkers can "reliably evaluate educational data when using comparative judgement, suggesting a scalable approach for creating large educational datasets"

## Foundational Learning

- Concept: Inter-rater reliability (IRR)
  - Why needed here: Core metric for evaluating data quality using Krippendorff's alpha
  - Quick check question: What does an alpha score of 0.8 indicate about rater agreement compared to chance agreement?

- Concept: Comparative judgement vs categorical judgement
  - Why needed here: The study directly compares these two evaluation methods
  - Quick check question: How does asking "which answer is more correct" differ fundamentally from asking "is this answer correct or incorrect"?

- Concept: Reinforcement learning from human feedback (RLHF)
  - Why needed here: The study builds on RLHF methodology using preference-based judgements
  - Quick check question: Why is comparative judgement particularly valuable when training AI models on subjective educational data?

## Architecture Onboarding

- Component map: Crowdworker platform (Prolific) -> Task assignment (categorical vs comparative) -> Data collection -> Reliability calculation (Krippendorff's alpha) -> Quality control checks
- Critical path: Task design -> Rater recruitment -> Quality control implementation -> Data collection -> Reliability analysis
- Design tradeoffs: Scale vs accuracy, cost vs quality, speed vs thoroughness in evaluation
- Failure signatures: Low alpha scores despite comparative judgement, inconsistent task completion times, high dropout rates from quality control checks
- First 3 experiments:
  1. Replicate short-answer comprehension task with modified difficulty levels
  2. Test comparative judgement on multiple-choice question evaluation
  3. Compare reliability across different crowdworker platforms using identical tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the minimum sample size needed to achieve statistically significant improvements in inter-rater reliability when using comparative judgement with crowdworkers?
- Basis in paper: The paper mentions Krippendorff's alpha confidence intervals but does not explore the relationship between sample size and statistical significance
- Why unresolved: The study provides point estimates and confidence intervals but does not conduct power analysis or explore how sample size affects the ability to detect reliability improvements
- What evidence would resolve it: A systematic study varying crowdworker sample sizes across different task types to determine minimum sample requirements for achieving statistically significant reliability improvements

### Open Question 2
- Question: How generalizable are the findings of comparative judgement improvements to other educational assessment tasks beyond reading comprehension and oral fluency?
- Basis in paper: The authors explicitly note that the effect was only demonstrated on two specific tasks and that it is yet to be determined whether this effect generalizes to other tasks
- Why unresolved: The study only tested two specific reading-related tasks, limiting the ability to make broader claims about comparative judgement's effectiveness across educational domains
- What evidence would resolve it: Replication studies testing comparative judgement across diverse educational assessment types (mathematics, science, writing, etc.) to establish generalizability patterns

### Open Question 3
- Question: What are the long-term effects of using comparative judgement on crowdworker performance and learning outcomes?
- Basis in paper: While the paper discusses immediate reliability improvements, it does not investigate whether crowdworkers improve with practice or how this might affect longitudinal data quality
- Why unresolved: The study used cross-sectional data without tracking individual crowdworker performance over time or examining whether comparative judgement leads to skill development
- What evidence would resolve it: Longitudinal studies tracking the same crowdworkers across multiple tasks and time periods to assess learning curves, performance stability, and potential skill transfer effects

## Limitations

- Generalizability concerns: Findings may not extend to other educational task types beyond reading comprehension and oral fluency assessment
- Crowdworker demographic limitations: Participants were limited to US/UK/Canada locations with >85% approval rates, creating potential selection bias
- Task design constraints: Comparative judgement implementation paired items within 10 points on the rubric scale, which may not reflect real-world evaluation scenarios

## Confidence

- High confidence: The finding that comparative judgement improves inter-rater reliability over categorical judgement is well-supported by statistical evidence
- Medium confidence: The scalability claim regarding crowdworker-based data labeling is plausible but requires additional validation
- Low confidence: The assertion that comparative judgement can fully replace expert evaluation in educational contexts is overstated

## Next Checks

- Validation Check 1: Test comparative judgement across diverse educational task types including creative writing, mathematical problem-solving, and scientific reasoning assessments to establish generalizability boundaries
- Validation Check 2: Conduct a multi-platform replication study using crowdworkers from different demographic backgrounds to assess the robustness of findings across populations
- Validation Check 3: Implement a longitudinal study examining the consistency of crowdworker performance over extended periods and multiple task iterations to evaluate sustainability of quality control measures