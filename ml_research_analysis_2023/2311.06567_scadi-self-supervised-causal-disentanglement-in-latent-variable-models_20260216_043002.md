---
ver: rpa2
title: 'SCADI: Self-supervised Causal Disentanglement in Latent Variable Models'
arxiv_id: '2311.06567'
source_url: https://arxiv.org/abs/2311.06567
tags:
- causal
- factors
- scadi
- observer
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes SCADI, a self-supervised causal disentanglement\
  \ model that learns semantic factors and their causal relationships without any\
  \ supervision. The key idea is to combine a masked structural causal model (SCM)\
  \ with a pseudo-label generator, where the observer generates pseudo-labels via\
  \ \u03B2-VAE and the interpreter performs causal disentanglement via masked SCM."
---

# SCADI: Self-supervised Causal Disanglement in Latent Variable Models

## Quick Facts
- arXiv ID: 2311.06567
- Source URL: https://arxiv.org/abs/2311.06567
- Reference count: 40
- Key outcome: SCADI achieves lower DAGness (0.1359 vs. 9.7837) and higher label quality scores (0.5698 vs. 0.6744) on synthetic pendulum dataset compared to fully unsupervised baselines.

## Executive Summary
SCADI introduces a self-supervised approach to causal disentanglement in latent variable models that eliminates the need for labeled data. The method combines a β-VAE-based observer that generates pseudo-labels with a masked structural causal model (SCM) interpreter that learns causal relationships. The observer and interpreter form a mutually beneficial relationship where the observer provides labels and receives DAGness regularization from the interpreter. Experiments on a synthetic pendulum dataset demonstrate superior performance compared to fully unsupervised baselines, with SCADI successfully recovering the ground truth causal structure while maintaining high-quality pseudo-labels.

## Method Summary
SCADI proposes a self-supervised causal disentanglement framework that combines a masked structural causal model with a pseudo-label generator. The observer uses a β-VAE architecture with two-step encoding to generate pseudo-labels, while the interpreter employs a masked SCM approach with adjacency matrix regularization to ensure DAGness. The two components form a mutual dependency loop where the observer receives DAGness regularization from the interpreter, and the interpreter receives labels from the observer. The method is evaluated on a synthetic pendulum dataset with 5482 training images and 1826 test images, using DAGness scores and label quality metrics as primary evaluation criteria.

## Key Results
- Achieves DAGness score of 0.1359 compared to 9.7837 for fully unsupervised baselines
- Obtains label quality score of 0.5698 compared to 0.6744 for fully unsupervised baselines
- Successfully recovers ground truth causal structure on synthetic pendulum dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The observer's DAGness regularization improves the quality of pseudo-labels by preventing bidirectional relationships in the adjacency matrix.
- Mechanism: By minimizing the DAGness of the adjacency matrix A, the observer is encouraged to disentangle factors, which in turn produces cleaner pseudo-labels for the interpreter.
- Core assumption: Lower DAGness in the adjacency matrix correlates with better disentanglement of factors.
- Evidence anchors:
  - [abstract] "The observer receives DAGness regularization from the interpreter, while the interpreter receives labels from the observer, forming a mutually beneficial relationship."
  - [section] "Therefore DAGness of A can assess the disentanglement in observer, and in the same context, minimize DAGness would assist disentangling factors by helping to suppress bidirectional relationships and anchor the factors in place."
- Break condition: If the DAGness regularization is too strong, it might over-constrain the model and lead to underfitting.

### Mechanism 2
- Claim: The interpreter's use of masked SCM allows it to learn causal relationships between factors by focusing on parental effects.
- Mechanism: By masking non-parental elements of Z using A in each semantic vector, the model learns the effects of individual factors while maintaining their connections with parental semantics.
- Core assumption: The masked SCM can effectively capture causal relationships between factors when provided with good pseudo-labels.
- Evidence anchors:
  - [abstract] "The interpreter performs causal disentanglement via masked SCM."
  - [section] "By masking out non-parental elements of Z using A in each semantic vector, the model is able to learn the effects of the individual factors while maintaining their connections with their parental semantics."
- Break condition: If the pseudo-labels provided by the observer are of poor quality, the interpreter's ability to learn causal relationships will be compromised.

### Mechanism 3
- Claim: The mutual dependency between observer and interpreter creates a self-supervised learning loop that improves both components over time.
- Mechanism: The observer generates pseudo-labels that are used by the interpreter to learn causal relationships, while the interpreter's DAGness regularization improves the observer's disentanglement ability.
- Core assumption: The mutual dependency between observer and interpreter leads to a positive feedback loop that improves both components.
- Evidence anchors:
  - [abstract] "The observer receives DAGness regularization from the interpreter, while the interpreter receives labels from the observer, forming a mutually beneficial relationship."
  - [section] "To summarize, the observer receives DAGness from the interpreter, while the interpreter obtains labels from the observer, establishing a mutually beneficial relationship."
- Break condition: If the balance between the observer and interpreter is not properly maintained, the mutual dependency might lead to a negative feedback loop that degrades performance.

## Foundational Learning

- Concept: Latent variable models (e.g., β-VAE)
  - Why needed here: SCADI builds upon β-VAE as its observer component, which is crucial for generating pseudo-labels.
  - Quick check question: What is the main difference between a standard VAE and a β-VAE, and how does it affect the disentanglement process?

- Concept: Causal inference and SCM
  - Why needed here: The interpreter component of SCADI relies on SCM to learn causal relationships between factors.
  - Quick check question: How does the concept of exogenous and endogenous latent variables relate to causal inference in SCM?

- Concept: DAGness and its role in causal structure learning
  - Why needed here: DAGness regularization is a key component of SCADI that helps improve the quality of pseudo-labels.
  - Quick check question: What is the relationship between DAGness and the identifiability of causal structures in latent variable models?

## Architecture Onboarding

- Component map: Observer → Interpreter (with mutual dependency)
- Critical path: Observer generates pseudo-labels → Interpreter learns causal relationships → Observer receives DAGness regularization
- Design tradeoffs:
  - Balancing the strength of DAGness regularization to avoid over- or under-constraining the model
  - Choosing the appropriate latent space dimensionality for the observer
- Failure signatures:
  - Poor disentanglement in the observer, leading to low-quality pseudo-labels
  - Failure to capture causal relationships in the interpreter, despite good pseudo-labels
- First 3 experiments:
  1. Train SCADI on the synthetic pendulum dataset and evaluate the quality of pseudo-labels using the label-finding process.
  2. Compare the causal relationships learned by SCADI with the ground truth causal graph.
  3. Analyze the impact of varying the DAGness regularization strength on the overall performance of SCADI.

## Open Questions the Paper Calls Out

The paper does not explicitly call out any open questions. However, based on the experimental setup and methodology, several implicit questions arise regarding the generalizability and limitations of the approach.

## Limitations

- The method relies heavily on the quality of pseudo-labels generated by the observer, which may not generalize well to more complex real-world datasets where semantic factors are not as clearly defined.
- The mutual dependency between observer and interpreter, while conceptually appealing, may create optimization challenges that are not fully explored in the paper.
- The evaluation is limited to a synthetic pendulum dataset, which may not capture the complexities of real-world causal disentanglement problems.

## Confidence

- High confidence in the novelty of combining masked SCM with pseudo-label generation for self-supervised causal disentanglement
- Medium confidence in the effectiveness of the proposed method on synthetic data
- Low confidence in the method's scalability and performance on real-world datasets with complex causal structures

## Next Checks

1. Evaluate SCADI on real-world datasets with known causal structures (e.g., dSprites or 3DShapes) to assess generalizability beyond synthetic data
2. Perform ablation studies to quantify the contribution of each component (DAGness regularization, masked SCM, mutual dependency) to overall performance
3. Test the method's robustness to varying degrees of factor correlation and confounding in the data generation process