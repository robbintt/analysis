---
ver: rpa2
title: 'AGaLiTe: Approximate Gated Linear Transformers for Online Reinforcement Learning'
arxiv_id: '2310.15719'
source_url: https://arxiv.org/abs/2310.15719
tags:
- arelit
- agent
- learning
- transformer
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Recurrent Linear Transformer (ReLiT) and Approximate
  Recurrent Linear Transformer (AReLiT), recurrent architectures that address the
  high computational and memory costs of transformers in online reinforcement learning.
  ReLiT uses a gating mechanism and a parameterized feature map to control the flow
  of past information and learn complex non-linear relationships.
---

# AGaLiTe: Approximate Gated Linear Transformers for Online Reinforcement Learning

## Quick Facts
- arXiv ID: 2310.15719
- Source URL: https://arxiv.org/abs/2310.15719
- Reference count: 40
- Primary result: ReLiT and AReLiT reduce transformer inference cost by ≥40% and memory usage by >50% while matching or outperforming GTrXL in partially observable RL tasks

## Executive Summary
This paper introduces Recurrent Linear Transformer (ReLiT) and Approximate Recurrent Linear Transformer (AReLiT) to address the high computational and memory costs of transformers in online reinforcement learning. ReLiT uses a gating mechanism and parameterized feature map to control information flow and learn complex non-linear relationships, while AReLiT further reduces space complexity through low-rank approximation. The approaches are evaluated on partially observable RL tasks including T-Maze, Mystery Path, and Memory Maze, demonstrating performance comparable to or better than GTrXL with significantly reduced computational overhead.

## Method Summary
The paper proposes ReLiT, which extends Linear Transformer with a gating mechanism (using βt and γt vectors) to control information flow and prevent unbounded growth of recurrent states. It also introduces a parameterized feature map based on outer products of ReLU activations to learn complex non-linear relationships. AReLiT builds on ReLiT by replacing the matrix-based recurrent state with a low-rank approximation using sum of cosines, reducing space complexity from O(ηd²) to O(d² + rηd). Both architectures are evaluated using A2C or PPO base RL algorithms on partially observable environments.

## Key Results
- ReLiT and AReLiT match or outperform GTrXL in partially observable RL tasks
- Inference cost reduced by at least 40% compared to GTrXL
- Memory usage decreased by more than 50%
- In Mystery Path, AReLiT outperforms GTrXL by more than 37% on harder tasks

## Why This Works (Mechanism)

### Mechanism 1
Gating mechanism in ReLiT controls flow of past information and prevents unbounded growth of recurrent states. Uses element-wise learned decay parameters (βt, γt) via sigmoid gating to smoothly reduce impact of past information. Each index of the recurrent state benefits from independent decay learning rather than uniform scalar decay.

### Mechanism 2
Parameterized feature map enables learning complex non-linear relationships and eliminates dependency on kernel choice. Uses outer products of ReLU activations to learn multiplicative interactions instead of relying on predefined kernels like ELU+1 or DPFP. Outer product feature maps provide sufficient expressiveness to approximate the softmax attention while being trainable.

### Mechanism 3
Low-rank approximation via sum of cosines reduces space complexity while maintaining approximation quality. Replaces matrix Ct with r vectors using Kronecker delta approximation via sum of cosines. Small r values (e.g., r=1 or r=7) provide sufficient approximation quality for practical performance.

## Foundational Learning

- **Self-attention mechanism and its computational complexity (O(N²) time, O(Nd) space)**: Understanding why transformers struggle with online RL due to expensive inference and memory requirements.
  - Quick check: What is the computational complexity of standard self-attention for a sequence of length N with embedding dimension d?

- **Kernel methods and their equivalence to softmax attention**: Linear Transformer replaces softmax with kernel function, enabling iterative updates.
  - Quick check: How does replacing softmax with a kernel function enable recurrent updates in transformers?

- **Gating mechanisms in recurrent networks**: Gating controls information flow and prevents vanishing/exploding gradients in RNNs.
  - Quick check: How do gating mechanisms like those in LSTMs control the flow of information in recurrent networks?

## Architecture Onboarding

- **Component map**: Input embedding layer (d-dimensional) -> Gating network (Wβ, Wγ parameters) producing βt, γt -> Feature map network (Wp1, Wp2, Wp3 parameters) producing kt, qt -> Value projection WV producing vt -> Recurrent state update (either matrix Ct for ReLiT or vector set for AReLiT) -> Attention output normalization and calculation

- **Critical path**: 
  1. Compute kt, qt, vt from input xt
  2. Generate gating vectors βt, γt
  3. Update recurrent state (Ct or {˜vk, ˜kk})
  4. Compute attention output at
  Time complexity: O(ηd²) for ReLiT, O(d² + rηd) for AReLiT

- **Design tradeoffs**:
  - η vs performance: Higher η increases capacity but also computational cost (O(ηd²))
  - r vs approximation quality: Higher r improves approximation but increases space from O(rηd) to O(rd²)
  - Number of layers/heads: Tradeoff between model capacity and computational efficiency

- **Failure signatures**:
  - Performance plateaus early: May indicate insufficient η or inappropriate gating parameters
  - Training instability: Could signal unbounded growth in recurrent states (gating not working)
  - Memory errors: Likely from r too large or insufficient GPU memory for matrix operations
  - Slow convergence: May need hyperparameter tuning of learning rate or entropy coefficient

- **First 3 experiments**:
  1. T-Maze corridor length 120-200 with varying η values to find sweet spot between performance and efficiency
  2. Ablation study comparing ReLiT gating vs scalar gating on T-Maze to validate gating mechanism importance
  3. AReLiT approximation quality test with synthetic data varying r to determine minimum acceptable r value

## Open Questions the Paper Calls Out

### Open Question 1
What is the theoretical relationship between the approximation parameter r and the quality of approximation in AReLiT? The paper mentions that limr→∞ ˜δmn = δmn and discusses the impact of r on the quality of approximation in a synthetic example, but it does not offer a theoretical derivation of the approximation error as a function of r.

### Open Question 2
How does the performance of AReLiT compare to GTrXL in larger-scale environments or with longer episode lengths? The paper evaluates AReLiT on several environments but focuses on relatively small-scale tasks. Evaluating AReLiT on environments with longer episode lengths or larger state spaces would provide insights into its scalability and performance in larger-scale settings.

### Open Question 3
What is the impact of different low-rank approximation methods on the performance of AReLiT? The paper only considers one alternative low-rank approximation method, leaving open the question of how other methods might perform in comparison to AReLiT's approach. Evaluating AReLiT with other low-rank approximation methods would provide a more comprehensive understanding of the impact of different approximation methods on its performance.

## Limitations

- Limited empirical evidence for the effectiveness of the parameterized feature map in learning complex non-linear relationships, as no direct comparisons are made with other feature map choices
- The approximation quality of the low-rank representation in AReLiT is not thoroughly validated, particularly for larger values of r where the space savings diminish
- The paper does not provide a comprehensive ablation study to isolate the contributions of the gating mechanism, parameterized feature map, and low-rank approximation to the overall performance

## Confidence

- **High Confidence**: The computational complexity analysis and the general framework of ReLiT and AReLiT architectures are well-defined and theoretically sound.
- **Medium Confidence**: The empirical results showing performance improvements over GTrXL are promising, but the lack of detailed hyperparameter tuning and ablation studies reduces confidence in the specific contributions of each component.
- **Low Confidence**: The claims about the parameterized feature map's ability to learn complex non-linear relationships and the effectiveness of the low-rank approximation for all values of r are not fully substantiated with empirical evidence.

## Next Checks

1. **Ablation Study**: Conduct an ablation study to isolate the effects of the gating mechanism, parameterized feature map, and low-rank approximation on the performance of ReLiT and AReLiT.

2. **Feature Map Comparison**: Compare the parameterized feature map with other commonly used feature maps (e.g., ELU+1, DPFP) to validate its effectiveness in learning complex non-linear relationships.

3. **Low-Rank Approximation Analysis**: Analyze the approximation quality of the low-rank representation in AReLiT for various values of r, particularly focusing on the trade-off between approximation error and space savings.